---
ver: rpa2
title: 'NIRANTAR: Continual Learning with New Languages and Domains on Real-world
  Speech Data'
arxiv_id: '2507.00534'
source_url: https://arxiv.org/abs/2507.00534
tags:
- learning
- data
- languages
- speech
- episodes
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Nirantar introduces a real-world continual learning framework for
  multilingual and multi-domain automatic speech recognition (ASR), featuring 3250
  hours of human-transcribed speech across 22 Indian languages and 208 districts.
  Unlike synthetic benchmarks, Nirantar leverages naturally episodic data collection,
  enabling evaluation across Language-Incremental (LIL), Domain-Incremental (DIL),
  and Language-Incremental Domain-Incremental Learning (LIDIL) scenarios.
---

# NIRANTAR: Continual Learning with New Languages and Domains on Real-world Speech Data

## Quick Facts
- arXiv ID: 2507.00534
- Source URL: https://arxiv.org/abs/2507.00534
- Reference count: 0
- Introduces first real-world continual learning framework for multilingual ASR across 22 Indian languages and 208 districts

## Executive Summary
Nirantar presents a real-world continual learning framework for automatic speech recognition that handles both new languages and new domains sequentially. The benchmark features 3,250 hours of human-transcribed speech across 22 Indian languages and 208 districts, collected naturally rather than synthetically constructed. The framework enables evaluation across three scenarios: Language-Incremental Learning (LIL), Domain-Incremental Learning (DIL), and combined Language-Incremental Domain-Incremental Learning (LIDIL). Experiments with established CL methods reveal that no single approach consistently excels across all scenarios, highlighting the need for more robust strategies that can handle both language and domain shifts dynamically.

## Method Summary
Nirantar employs a Conformer-L encoder with 120M parameters and 17 blocks, combined with a hybrid CTC-RNNT decoder architecture. The model uses separate decoder heads per language with 256-token BPE vocabularies. Training follows a sequential episode structure where base models are trained for 150K steps, then incrementally updated for 30K steps per episode using various CL methods. The framework implements Experience Replay with 3% buffer, EWC and MAS regularization, and adapter-based approaches. Evaluation metrics include AMER (Average Match Error Rate), Forward Transfer (FWT), Backward Transfer (BWT), and Intransigence Measure (IM) across all 12 training episodes.

## Key Results
- Experience Replay with just 3% buffer substantially outperforms regularization methods in language-incremental learning
- Regularization-based methods (EWC, MAS) show better performance in domain-incremental scenarios but struggle with language shifts
- Architecture-based methods like adapters prevent forgetting but become impractical at scale due to excessive parameter growth
- No single CL method consistently excels across all three evaluation scenarios (LIL, DIL, LIDIL)

## Why This Works (Mechanism)

### Mechanism 1: Experience Replay's Buffer Advantage
Experience Replay with a small memory buffer (3%) substantially outperforms regularization-based methods for language-incremental learning in ASR. The mechanism works by storing and replaying representative samples from prior episodes during sequential training, preventing catastrophic forgetting by ensuring gradient updates remain anchored to previous language distributions. The core assumption is that the sampled buffer sufficiently represents the linguistic diversity of prior episodes. Break conditions include buffer sampling failing to cover rare phonemes or dialectal variants, or if buffer maintenance becomes impossible due to privacy or storage constraints.

### Mechanism 2: Regularization Method Domain Specificity
Regularization-based methods (EWC, MAS) perform better in domain-incremental learning than in language-incremental learning, but exhibit inconsistent forward transfer across scenarios. These methods estimate parameter importance and penalize updates to weights deemed critical for prior tasks. In DIL, shared acoustic foundations across domains allow importance-weighted regularization to preserve useful features. In LIL, new languages may require modifying weights previously marked as important, leading to underfitting. Break conditions include significant divergence between new languages and prior language families, causing importance estimates to misclassify which parameters are safe to modify.

### Mechanism 3: Adapter Scalability Limits
Architecture-based methods (adapters) can prevent forgetting but become impractical at scale due to parameter growth per task. Adapters add small bottleneck modules to each transformer block, enabling task-specific specialization without modifying the backbone. The core assumption is that the number of incremental tasks remains bounded and infrastructure can handle growing model complexity. Break conditions include unbounded task space growth or exceeding memory/serving budgets, plus reduced knowledge sharing across tasks potentially harming forward transfer.

## Foundational Learning

**Concept: Catastrophic Forgetting**
Why needed here: Sequential training on new languages/domains overwrites representations learned for prior tasks. All CL methods in Nirantar are explicitly designed to mitigate this.
Quick check question: Can you explain why fine-tuning a multilingual ASR model on a new language might degrade performance on previously learned languages?

**Concept: Forward vs. Backward Transfer**
Why needed here: Nirantar evaluates CL methods using FWT (leveraging past knowledge for new tasks) and BWT (impact of new learning on old tasks). These metrics determine whether a method is truly "continual" or merely sequential.
Quick check question: If a model trained on Hindi domains then sees Bengali domains, what would positive BWT indicate about the learning process?

**Concept: Replay Buffer Sampling**
Why needed here: ER's effectiveness depends on which prior examples are retained. The 3% buffer in Nirantar must represent linguistic diversity across 22 languages and 208 domains.
Quick check question: Why might uniform sampling from the buffer fail for a dataset with highly skewed language hours (e.g., Assamese 241h vs. Gujarati 20h)?

## Architecture Onboarding

**Component map:**
Conformer-L encoder (120M params, 17 blocks, 512 dim) -> Hybrid CTC-RNNT decoder -> Per-language decoder heads (256-token BPE) -> CL method modules (ER buffer, EWC/MAS regularization, or Adapter insertion) -> Episode scheduler

**Critical path:**
1. Load base model (m₀) trained on seed episode (E₀)
2. For each incremental episode (Eₜ): apply CL method → produce mₜ
3. Evaluate on test set spanning all seen language-domain pairs using AMER, FWT, BWT, IM

**Design tradeoffs:**
- ER: Strong BWT, requires storage and sampling strategy; FWT may suffer
- EWC/MAS: No storage overhead, but sensitive to hyperparameters; scenario-dependent performance
- Adapters: Best isolation, poor scalability, limited cross-task transfer

**Failure signatures:**
- Sharp AMER spikes at specific episodes (e.g., Manipuri at episode 9) indicate low-resource or linguistically distant data causing collapse
- Consistently negative BWT indicates catastrophic forgetting not mitigated
- High Intransigence Measure (IM) suggests model cannot adapt to new tasks (plasticity loss)

**First 3 experiments:**
1. Reproduce LIL baseline with ER (3% buffer) vs. Incremental FT on first 3 episodes; confirm ER's BWT advantage
2. Run DIL with MAS and compare FWT/BWT to LIL results; verify regularization methods perform better in domain-only scenarios
3. Add adapters for a single new language in LIL; measure parameter overhead and AMER vs. ER to confirm tradeoff between isolation and scalability

## Open Questions the Paper Calls Out

**Open Question 1:** Can a unified continual learning method be developed that performs consistently well across LIL, DIL, and LIDIL scenarios?
Basis in paper: The authors state "no single method performs consistently well" across the three scenarios, with ER excelling in LIL but MAS performing better in DIL.
Why unresolved: Current CL methods are designed for specific incremental learning settings; LIDIL's hybrid nature with simultaneous language and domain shifts presents novel challenges.
What evidence would resolve it: A method achieving competitive AMER, BWT, FWT, and IM scores across all three scenarios on Nirantar.

**Open Question 2:** How can architecture-based continual learning be made scalable for large-scale multilingual, multi-domain ASR?
Basis in paper: The paper notes adapters require "excessive parameter growth" (11M additional parameters for 11 languages) and are "impractical for large-scale CL applications."
Why unresolved: Current architecture-based methods require dedicated parameters per task/language/domain; the combinatorial explosion when scaling to hundreds of domains makes this approach infeasible.
What evidence would resolve it: A parameter-efficient architecture modification that maintains adapter-level performance while adding sub-linear parameters that scales gracefully to 208+ domains.

**Open Question 3:** What memory buffer management strategies can improve Experience Replay beyond simple random sampling for multilingual CL?
Basis in paper: The paper tests ER with only 3% buffer size using random sampling; while ER outperforms other methods in LIL, it shows poor forward transfer.
Why unresolved: The optimal selection, prioritization, and updating of memory samples remains unexplored, particularly for handling the non-uniform data distributions and diverse linguistic characteristics in Nirantar.
What evidence would resolve it: A principled buffer management strategy that demonstrates improved FWT and BWT compared to random sampling at equivalent buffer sizes.

## Limitations

- Single dataset scope focused on Indian languages and districts may not generalize to other language families
- 3% replay buffer size was not systematically explored across different fractions
- Does not address privacy or data access constraints that could prevent buffer retention in real-world deployments
- Adapter approach demonstrates impracticality at scale without proposing alternatives or hybrid solutions

## Confidence

**High confidence:** ER with 3% buffer outperforming regularization methods in LIL; adapter scalability issues becoming prohibitive with 22 languages and 208 domains; no single CL method excelling across all scenarios

**Medium confidence:** EWC/MAS superiority in DIL over LIL; performance drops for low-resource languages (e.g., Manipuri) indicating family-level transfer challenges

**Low confidence:** Optimal buffer size determination; hyperparameter sensitivity across different language families; long-term forgetting beyond 12 episodes

## Next Checks

1. **Buffer Size Sensitivity:** Systematically vary the replay buffer from 1% to 10% in 1% increments for LIL scenario to identify optimal memory allocation and determine diminishing returns

2. **Language Family Transfer:** Design experiments isolating language family transitions (Indo-Aryan → Dravidian → Tibeto-Burman) to quantify how linguistic distance affects forward and backward transfer across all CL methods

3. **Privacy-Aware CL:** Implement gradient-based replay or synthetic data generation to approximate ER benefits without storing raw audio, validating performance against the 3% buffer baseline under realistic privacy constraints