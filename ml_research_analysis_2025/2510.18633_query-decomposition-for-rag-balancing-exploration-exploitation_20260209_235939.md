---
ver: rpa2
title: 'Query Decomposition for RAG: Balancing Exploration-Exploitation'
arxiv_id: '2510.18633'
source_url: https://arxiv.org/abs/2510.18633
tags:
- documents
- query
- retrieval
- document
- relevance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of efficiently selecting relevant
  documents in retrieval-augmented generation (RAG) systems when handling complex
  queries. The authors formulate query decomposition and document retrieval as an
  exploitation-exploration problem using a multi-armed bandit framework, where each
  sub-query is treated as an arm and document relevance observations build beliefs
  about sub-query utility.
---

# Query Decomposition for RAG: Balancing Exploration-Exploitation

## Quick Facts
- **arXiv ID:** 2510.18633
- **Source URL:** https://arxiv.org/abs/2510.18633
- **Reference count:** 25
- **Primary result:** 35% gain in document-level precision, 15% increase in α-nDCG, and improved downstream long-form generation performance with 6.0–9.9% higher citation support, 6.7–8.5% better nugget coverage, and 7.3–9.6% stronger sentence support

## Executive Summary
This paper addresses the challenge of efficiently selecting relevant documents in retrieval-augmented generation (RAG) systems when handling complex queries. The authors formulate query decomposition and document retrieval as an exploitation-exploration problem using a multi-armed bandit framework, where each sub-query is treated as an arm and document relevance observations build beliefs about sub-query utility. They propose a rank-aware top-k Bernoulli policy with upper confidence bounds and diversity constraints that dynamically selects the most informative sub-queries under a fixed budget. Experimental results show this approach achieves significant improvements in document-level precision, ranking quality, and downstream generation performance compared to baseline methods.

## Method Summary
The method frames query decomposition and document retrieval as a multi-armed bandit problem where sub-queries are arms and documents are pulls. Thompson sampling with Beta priors updates beliefs about sub-query relevance based on document observations. The main policy is a Bernoulli top-k UCB diversity reward that combines rank information, novelty penalties, and exploration bounds. The framework supports hierarchical query decomposition where promising sub-queries can be expanded into child sub-queries with inherited posteriors. Experiments use NeuCLIR corpus with complex user requests decomposed into 16 sub-queries each, and ResearchyQuestions with hierarchical 2-level decomposition.

## Key Results
- 35% gain in document-level precision over baseline methods
- 15% increase in α-nDCG for document ranking quality
- 6.0–9.9% higher citation support, 6.7–8.5% better nugget coverage, and 7.3–9.6% stronger sentence support in downstream long-form generation

## Why This Works (Mechanism)

### Mechanism 1: Multi-Armed Bandit Formulation for Sub-Query Selection
Treating sub-queries as arms in a multi-armed bandit problem enables adaptive allocation of retrieval budget across sub-queries based on observed relevance. Each sub-query q_i is modeled as an arm with an unknown relevance distribution. Thompson sampling with Beta priors samples from posteriors to select which sub-query to pull next. Observing a document updates the posterior (α for relevant, β for non-relevant). This dynamically shifts budget toward high-utility sub-queries.

### Mechanism 2: Rank-Aware Top-k Bernoulli UCB Diversity Reward
A composite reward incorporating rank information, novelty, and exploration bounds improves document selection precision over naive relevance-only rewards. The reward averages relevance over a local window of k documents, penalizes redundancy via cosine similarity to prior selections, and adds a UCB term to ensure all arms are explored. This jointly optimizes relevance, diversity, and coverage.

### Mechanism 3: Hierarchical Correlated Bandits for Multi-Level Decomposition
Modeling hierarchical sub-query relationships with inherited posteriors improves early-budget precision by focusing exploration on promising branches. When a sub-query exceeds an informativeness threshold, it can be expanded into child sub-queries. Children inherit scaled priors from parents, propagating beliefs down the hierarchy and enabling focused sampling.

## Foundational Learning

- **Concept: Multi-Armed Bandit & Thompson Sampling**
  - **Why needed here:** The core algorithm selects sub-queries under uncertainty using posterior sampling.
  - **Quick check question:** Can you explain why Thompson sampling naturally balances exploration and exploitation without explicit ε parameters?

- **Concept: Beta-Bernoulli Conjugate Priors**
  - **Why needed here:** The paper models sub-query relevance as Bernoulli with Beta priors; understanding posterior updates is essential.
  - **Quick check question:** Given α=3, β=2 after 5 observations, what is the expected probability of relevance for the next pull?

- **Concept: α-nDCG (Novelty-Adjusted Ranking Metric)**
  - **Why needed here:** Used to evaluate whether selected documents provide diverse, non-redundant information.
  - **Quick check question:** Why does α-nDCG penalize retrieving multiple documents with highly similar content?

## Architecture Onboarding

- **Component map:** Query Decomposer (LLM) -> Retriever (PLAID-X + LSR + Qwen) -> Bandit Policy Engine -> Document Assessor -> Aggregator + LLM Generator

- **Critical path:** Query decomposition → parallel retrieval → sequential bandit selection (budget-constrained) → aggregated context → generation. The bandit loop is the bottleneck: each selection requires one relevance assessment.

- **Design tradeoffs:**
  - Budget vs. Coverage: Lower budgets favor bandit policies; at 100% budget, all policies converge
  - Assessment cost: Human/LLM relevance judgments are accurate but expensive; rank-only rewards are cheap but noisier
  - Hierarchy depth: Deeper decomposition improves precision at low budgets but increases decomposition cost and noise

- **Failure signatures:**
  - Low precision at high budget: Suggests retriever ranking is poorly calibrated
  - All arms converge to similar posteriors: Diversity penalty may be too aggressive
  - Hierarchical expansion fails to trigger: Informativeness threshold may be too high

- **First 3 experiments:**
  1. Baseline comparison: Run random, ε-greedy, and full exploitation policies on a held-out subset; confirm bandit policies achieve ≥15% precision gain at 20–30% budget
  2. Ablation on reward components: Disable rank, diversity, and UCB terms one at a time; measure precision impact to validate Equation 1 contributions
  3. Hierarchical validation: On ResearchyQuestions, compare serial vs. hierarchical policies at b=10% budget; confirm ≥20% precision gain with τ=0.77, λ=0.91, n=4

## Open Questions the Paper Calls Out
None

## Limitations
- Bandit assumptions may fail if retriever ranking is poorly calibrated, leading to misleading posterior updates
- Relevance assessment cost is not specified (human vs. LLM judges), which impacts practical deployment
- Hierarchy construction method is not detailed, making it unclear how to generalize the hierarchical approach

## Confidence
- **High Confidence:** Multi-armed bandit formulation as a framework for adaptive sub-query selection; experimental setup and 35% precision gain at 20-30% budget
- **Medium Confidence:** Specific reward function (Equation 1) and its components; 15% α-nDCG improvement depends on exact implementation
- **Low Confidence:** Hierarchical correlated bandit extension; inheritance mechanism and thresholds may be dataset-specific

## Next Checks
1. **Ablation Study on Reward Components:** Disable rank, diversity, and UCB terms one at a time in Equation 1. Measure precision and α-nDCG impact to isolate which component drives the improvements.

2. **Hierarchy Sensitivity Analysis:** On ResearchyQuestions, vary the informativeness threshold τ (e.g., 0.5, 0.77, 0.9) and inheritance factor λ (e.g., 0.5, 0.91, 1.0). Plot precision vs. these hyperparameters to determine if the reported values are optimal or dataset-specific.

3. **Retriever Calibration Check:** For NeuCLIR, compute rank-relevance correlation (Spearman) for each query's ranked list. If many queries show negative or near-zero correlation, this suggests the Bernoulli assumption is violated and the bandit framework may need modification.