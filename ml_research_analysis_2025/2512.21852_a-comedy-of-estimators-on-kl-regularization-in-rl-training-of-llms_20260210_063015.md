---
ver: rpa2
title: 'A Comedy of Estimators: On KL Regularization in RL Training of LLMs'
arxiv_id: '2512.21852'
source_url: https://arxiv.org/abs/2512.21852
tags:
- gradient
- training
- reward
- loss
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper systematically analyzes how different implementations
  of KL regularization affect RL fine-tuning of LLMs. It identifies that widely-used
  practices, such as adding the K3 estimator to the loss or reward, introduce biased
  gradient estimates that can destabilize training or reduce performance.
---

# A Comedy of Estimators: On KL Regularization in RL Training of LLMs

## Quick Facts
- arXiv ID: 2512.21852
- Source URL: https://arxiv.org/abs/2512.21852
- Reference count: 31
- Key outcome: KL estimator placement dramatically affects RL training stability and generalization; K1-in-reward (unbiased) outperforms K3-in-loss (biased but stable) on out-of-domain tasks

## Executive Summary
This paper systematically analyzes how different implementations of KL regularization affect RL fine-tuning of LLMs. It identifies that widely-used practices, such as adding the K3 estimator to the loss or reward, introduce biased gradient estimates that can destabilize training or reduce performance. In contrast, configurations producing unbiased gradients—like using the K1 estimator in the reward—lead to more stable training and better generalization on both in-domain and out-of-domain tasks. The findings highlight the importance of correct gradient estimation in KL-regularized objectives, especially in on-policy settings, and show that KL regularization can also stabilize asynchronous (off-policy) RL training. Overall, unbiased gradient implementations should be the default for effective and robust LLM reasoning enhancement.

## Method Summary
The paper investigates RL fine-tuning of LLMs on verifiable reasoning tasks using KL regularization. It compares four configurations of KL estimator placement (K1/K3 in loss/reward) using the RLOO baseline algorithm. The analysis decomposes gradient estimation into score function and path-wise components, revealing that K1-in-reward provides unbiased gradient estimates of sequence-level reverse KL divergence while K3-in-loss implicitly optimizes forward KL distillation. Experiments train Qwen2.5-7B and Llama-3.1-8B on MATH dataset with β values {0.05, 0.1, 0.3, 1.0}, evaluating on in-domain (MATH500, MATH2) and out-of-domain (MMLU physics/chemistry/biology) tasks.

## Key Results
- K3-in-reward causes training collapse across all tested β values due to high-gradient bias
- K1-in-reward (unbiased) outperforms K3-in-loss (biased but stable) by 19.06% on out-of-domain tasks
- K3-in-loss is stable because it approximates forward KL distillation rather than reverse KL regularization
- KL regularization stabilizes asynchronous (off-policy) RL training when using K3-in-loss

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Adding the K1 estimator to the reward (with stop-gradient) produces unbiased gradient estimates of the sequence-level reverse KL divergence.
- Mechanism: The K1 estimator (∑ log π_θ/π_ref) when placed in reward uses only the score function derivative, which recovers the true gradient: E[log(π_θ/π_ref) ∇log π_θ]. The path-wise derivative (∇K1) is zero in expectation, so adding K1 to the loss provides no gradient signal and only adds variance.
- Core assumption: Assumes on-policy sampling (importance ratio ω = 1); off-policy settings introduce additional bias via token-level importance sampling ratios.
- Evidence anchors:
  - [section 3.2.1]: "The expected gradient (under π_θ) of the K1 estimator when used in the reward is unbiased with respect to the reverse KL gradient."
  - [table 1]: K1-in-reward is the only configuration marked as having unbiased gradient estimates in on-policy settings.
  - [corpus]: Limited direct corroboration; neighboring papers focus on different gradient estimation contexts (SNNs, risk-seeking optimization) rather than KL estimator placement.
- Break condition: Off-policy training with multiple minibatch updates breaks the on-policy assumption, potentially invalidating the unbiasedness guarantee.

### Mechanism 2
- Claim: K3 estimator in loss produces stable training despite biased gradients because it approximates a forward KL distillation objective.
- Mechanism: The path-wise derivative of K3-in-loss yields: -E[π_ref/π_θ ∇log π_θ], which is mathematically equivalent to gradient estimates of token-level forward KL divergences. This behaves as stable on-policy distillation with the reference model as teacher, rather than optimizing the intended reverse KL-regularized objective.
- Core assumption: Assumes the reference policy π_ref remains fixed during training; if π_ref were updated, the distillation interpretation would change.
- Evidence anchors:
  - [section 4.1, observation 3]: "This may be explained by the observation that the gradient estimate (16) in this case is a sum of unbiased gradient estimates of the forward KL divergences computed at the token level, making this configuration equivalent to a forward KL divergence based stable on-policy distillation objective."
  - [figure 4]: K3-in-loss shows stable training curves across all β values tested.
  - [corpus]: RSPO paper discusses risk-seeking policy optimization for LLMs but does not address KL estimator mechanics.
- Break condition: If the true goal is reverse KL regularization (mode-seeking behavior), K3-in-loss may not provide the intended regularization properties.

### Mechanism 3
- Claim: Configurations with unbiased gradient estimates produce better generalization on both in-domain and out-of-domain tasks.
- Mechanism: Unbiased gradients ensure the optimization actually follows the intended objective landscape. Biased gradients may converge to different optima or create instabilities that harm generalization. The paper shows K1-in-reward outperforms K3-in-loss by 19.06% on OOD tasks (Qwen2.5-7B, β=0.05) despite both being stable.
- Core assumption: Assumes evaluation tasks are representative of desired generalization; assumes β is appropriately tuned.
- Evidence anchors:
  - [section 4.1, observation 3]: "Using K1-in-reward (i.e. unbiased gradient estimate) outperforms using K3-in-loss (i.e. biased gradient estimate)... the gains are more pronounced in out-of-domain tasks for Qwen-2.5-7B, with an average relative improvement of 19.06%."
  - [figures 5-6]: K1-in-reward consistently outperforms K3-in-loss across MATH500, MATH2, and MMLU subsets.
  - [corpus]: No direct corpus evidence on this specific generalization claim.
- Break condition: Very high β values may over-constrain the policy regardless of estimator choice, potentially masking differences.

## Foundational Learning

- Concept: **Score function vs. path-wise derivatives**
  - Why needed here: The paper's core analysis hinges on decomposing KL gradient estimation into these two components. Understanding that "reward" placement uses score function derivatives while "loss" placement uses path-wise derivatives is essential for interpreting Table 1.
  - Quick check question: If you add a KL estimator directly to the loss with autodiff, which derivative type are you computing?

- Concept: **Reverse vs. forward KL divergence**
  - Why needed here: The paper emphasizes reverse KL for regularization (mode-seeking), but reveals K3-in-loss implicitly optimizes forward KL (mode-covering). This distinction explains why K3-in-loss is stable despite being "wrong."
  - Quick check question: Which KL variant would you expect to produce more diverse outputs from a constrained policy?

- Concept: **Stop-gradient operator**
  - Why needed here: Adding KL to reward requires stop-gradient on the KL estimate to prevent backprop through the sampling process while still allowing gradient flow through the score function term.
  - Quick check question: What happens if you forget the stop-gradient when adding K1 to the reward?

## Architecture Onboarding

- Component map:
  - Policy network (π_θ) -> KL estimator module -> Reward computation (with stop-gradient) -> Advantage estimator -> Policy gradient update

- Critical path:
  1. Sample sequences y_{1:T} from π_θ given prompt x
  2. Compute token-level log-probabilities under both π_θ and π_ref
  3. Compute KL estimator (K1: log ratios; K3: ratio - 1 - log ratio)
  4. Either: (a) add to reward with stop-gradient, compute advantage, then policy gradient; or (b) add directly to loss
  5. Backpropagate and update π_θ

- Design tradeoffs:
  - K1-in-reward (recommended): Unbiased gradients, best generalization, requires understanding stop-gradient
  - K3-in-loss (common but suboptimal): Biased but stable, implicitly does forward KL distillation, underperforms on OOD
  - K3-in-reward: Avoid—causes training collapse due to high gradient bias
  - K1-in-loss: Avoid—adds variance without gradient signal, can cause instability
  - β tuning: Lower values (0.05-0.1) generally better; higher values degrade performance

- Failure signatures:
  - Training collapse with sudden accuracy drop to near-zero → likely using K3-in-reward
  - Training instability with oscillating accuracy → likely using K1-in-loss with high β or off-policy updates
  - Poor OOD performance despite stable training → likely using K3-in-loss instead of K1-in-reward
  - No regularization effect despite non-zero β → check that KL is actually being computed and added correctly

- First 3 experiments:
  1. Validate gradient bias in isolation: Implement a minimal autoregressive model (as in Section 3.3) and measure squared bias of gradient estimates for each configuration against analytical true gradient. Confirm K1-in-reward has lowest bias.
  2. Ablate estimator placement: Train a small LLM (e.g., 1B params) on a reasoning task with K1-in-reward vs. K3-in-loss vs. K1-in-loss vs. K3-in-reward. Monitor training stability and final in-domain accuracy to reproduce Figure 2-4 patterns.
  3. Measure OOD generalization gap: After establishing stable configurations (K1-in-reward and K3-in-loss), evaluate both on held-out tasks from different domains (e.g., train on math, evaluate on physics/chemistry/biology) to quantify the generalization benefit of unbiased gradients.

## Open Questions the Paper Calls Out

- **Question**: How can an unbiased sequence-level reverse KL gradient estimate be effectively implemented and analyzed in off-policy RL settings?
- **Basis in paper**: [explicit] The authors state, "we leave the implementation and analysis of an unbiased sequence-level reverse KL gradient estimate in off-policy settings for future work."
- **Why unresolved**: The paper identifies that token-level importance sampling ratios ($\omega_t$) are required for unbiased estimates in off-policy settings, but restricts its own primary analysis and implementation to the on-policy setting ($\omega_t = 1$).
- **What evidence would resolve it**: An empirical study showing stable training and performance metrics for an estimator incorporating token-level importance sampling ratios in asynchronous or off-policy LLM fine-tuning.

- **Question**: What specific mechanisms drive the performance difference between the unbiased K1-in-reward and the biased K3-in-loss configurations?
- **Basis in paper**: [explicit] In Appendix D.3, the authors note that observations regarding forward KL divergence and entropy "do not directly explain the performance differences between K1-in-reward and K3-in-loss and further analysis needs to be carried out."
- **Why unresolved**: While the paper demonstrates that K1-in-reward outperforms K3-in-loss, standard metrics like entropy and KL divergence did not correlate strongly enough to explain the gap, particularly in out-of-domain tasks.
- **What evidence would resolve it**: A detailed analysis of gradient dynamics or learned representations that identifies the distinct optimization behaviors causing the generalization gap.

- **Question**: Does the observed stability of the K3-in-loss configuration result from its equivalence to a forward KL distillation objective?
- **Basis in paper**: [inferred] The paper hypothesizes in Section 4.1 that the stability of K3-in-loss "may be explained by the observation that the gradient estimate... is a sum of unbiased gradient estimates of the forward KL divergences," equating it to knowledge distillation, but it does not empirically verify this specific theoretical link.
- **Why unresolved**: The stability is empirically observed, but the theoretical connection to forward KL distillation is presented as a potential explanation rather than a proven mechanism.
- **What evidence would resolve it**: Experiments comparing the trajectory and performance of K3-in-loss against an explicit on-policy knowledge distillation baseline to confirm behavioral equivalence.

## Limitations

- The analysis of unbiased gradient estimators relies critically on the on-policy assumption (ω=1), which rarely holds in practical RL training pipelines
- The causal mechanism for why unbiased gradients improve OOD generalization remains unclear, with alternative explanations not fully ruled out
- Key implementation details about RLOO baseline normalization and chat template handling are unspecified, potentially affecting reproducibility

## Confidence

- **High confidence**: K3-in-reward causes training collapse; K1-in-loss provides no gradient signal; K3-in-loss is stable due to forward KL distillation
- **Medium confidence**: K1-in-reward produces unbiased gradients in on-policy settings; unbiased gradients lead to better OOD generalization
- **Low confidence**: Off-policy bias magnitude is negligible; 19.06% OOD improvement is solely due to gradient unbiasedness

## Next Checks

1. **Off-policy bias quantification**: Implement a controlled experiment comparing K1-in-reward gradient estimates under strict on-policy vs. practical off-policy sampling (with multiple minibatch updates). Measure the squared bias of gradient estimates against analytical true gradients to quantify how much the off-policy assumption violation degrades performance.

2. **Forward vs. reverse KL behavior analysis**: Train two identical models on the same task, one with K3-in-loss (biased but stable) and one with K1-in-reward (unbiased), then systematically analyze their output distributions. Measure diversity metrics, mode-covering vs. mode-seeking behavior, and sensitivity to reference model perturbations to validate whether K3-in-loss actually implements forward KL distillation as claimed.

3. **β scaling and bias interaction study**: Conduct a systematic ablation across multiple β values (0.01, 0.05, 0.1, 0.3, 1.0) for both K1-in-reward and K3-in-loss configurations. Measure not just final accuracy but training stability metrics (gradient norm variance, KL divergence magnitude, collapse frequency) to determine whether the observed OOD generalization benefits persist at extreme β values where bias effects might dominate.