---
ver: rpa2
title: Towards Reliable Evaluation of Large Language Models for Multilingual and Multimodal
  E-Commerce Applications
arxiv_id: '2510.20632'
source_url: https://arxiv.org/abs/2510.20632
tags:
- e-commerce
- tasks
- product
- multimodal
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces EcomEval, a multilingual and multimodal benchmark
  for evaluating large language models (LLMs) in e-commerce. EcomEval covers six categories
  and 37 tasks across seven languages, including five low-resource Southeast Asian
  languages, and includes eight multimodal tasks.
---

# Towards Reliable Evaluation of Large Language Models for Multilingual and Multimodal E-Commerce Applications

## Quick Facts
- arXiv ID: 2510.20632
- Source URL: https://arxiv.org/abs/2510.20632
- Reference count: 31
- Key outcome: Introduces EcomEval, a multilingual and multimodal benchmark for evaluating LLMs in e-commerce, revealing significant performance gaps especially in low-resource languages and domain-specific tasks.

## Executive Summary
This paper introduces EcomEval, a comprehensive benchmark for evaluating large language models in e-commerce scenarios across multiple languages and modalities. The benchmark covers six categories and 37 tasks derived from authentic customer interactions, including five low-resource Southeast Asian languages. A semi-automatic pipeline combining LLM-generated drafts with expert human review ensures high-quality reference answers at scale. Evaluation of 19 state-of-the-art LLMs reveals significant performance disparities, particularly in product guidance, after-sales queries, and complex reasoning tasks, demonstrating the benchmark's ability to expose domain-specific weaknesses that general-purpose evaluations miss.

## Method Summary
EcomEval is constructed through a four-step pipeline: (1) collecting authentic customer queries and transaction logs, (2) classifying tasks using prefix clustering and a fine-tuned classifier, (3) verifying questions, and (4) generating reference answers via a semi-automatic approach where LLMs draft responses reviewed by over 50 expert annotators. The benchmark includes 37 tasks across six categories in seven languages, with eight multimodal tasks incorporating images. Evaluation uses GPT-4.1 as an LLM judge to score responses on a 0-3 scale (scaled to 0-100), with human expert review ensuring quality. Task difficulty is calibrated by averaging scores across 19 diverse LLMs ranging from 7B to several hundred billion parameters.

## Key Results
- EcomEval reveals significant performance gaps between models on e-commerce-specific tasks, particularly in product guidance and after-sales service
- Models show substantial performance drops on low-resource Southeast Asian languages compared to English and Chinese
- Multimodal tasks expose weaknesses in fine-grained product understanding, with models often relying on textual titles rather than visual features
- Difficulty levels calibrated through model averaging successfully differentiate task complexity, with some tasks consistently scoring below 70 (hard)

## Why This Works (Mechanism)

### Mechanism 1: Semi-Automatic Data Construction Pipeline
- Claim: Combining LLM drafts with expert annotator review produces higher-quality, scalable benchmark data than purely synthetic or manually curated datasets alone.
- Mechanism: Large models generate initial candidate responses, leveraging broad knowledge for scalability. These drafts then undergo mandatory verification and modification by over 50 human experts with e-commerce and multilingual expertise. This hybrid approach captures LLM breadth while correcting hallucinations, domain inaccuracies, and cultural nuances through human oversight.
- Core assumption: LLMs can produce sufficiently good drafts that experts can efficiently refine rather than writing from scratch, and expert annotators possess the necessary domain and linguistic knowledge to identify and correct errors.
- Evidence anchors:
  - [abstract] "To ensure both quality and scalability of reference answers, we adopt a semi-automatic pipeline in which large models draft candidate responses subsequently reviewed and modified by over 50 expert annotators..."
  - [section 3.3, Step 4] "For the high-quality questions collected, we use LLMs to generate corresponding answers... This may involve consulting existing blogs... Finally, human reviewers examine whether the generated answers follow all the instructions..."
  - [corpus] Related work (e.g., Compass-v3) notes challenges with noisy, heterogeneous, multilingual e-commerce data; this pipeline directly addresses such quality challenges, though direct comparative evidence of superiority isn't in the provided corpus.
- Break condition: If LLM drafts are consistently too poor for efficient human review, or if expert annotator quality cannot be maintained at scale (cost, fatigue, insufficient expertise), the pipeline's efficiency and quality advantages will degrade.

### Mechanism 2: Difficulty Calibration via Model Averaging
- Claim: Averaging evaluation scores across multiple models with different sizes and capabilities provides a robust, model-agnostic measure of task and item difficulty.
- Mechanism: A diverse set of 19 LLMs (from 7B to several hundred billion parameters) is evaluated. Their performance scores (0-3, scaled to 0-100) on each item and task category are averaged. This aggregate performance classifies difficulty (e.g., average score < 70 is hard). This averages out individual model-specific quirks.
- Core assumption: The collective performance of a diverse set of current LLMs serves as a valid proxy for the intrinsic difficulty of an e-commerce task. If all models struggle, the task is inherently hard.
- Evidence anchors:
  - [abstract] "We define difficulty levels for each question and task category by averaging evaluation scores across models with different sizes and capabilities..."
  - [section 3.2] "...we evaluate each response from the tested models shown in section 4.2 (ranging from 7B to several hundred billion parameters) on a 0–3 point scale. We then compute average scores of each task’s questions..."
  - [corpus] No direct evidence for this specific difficulty calibration method in the provided corpus.
- Break condition: The calibration becomes unreliable if the model set is not sufficiently diverse or representative of general LLM capabilities, or if future model architectures render the current calibration obsolete.

### Mechanism 3: Domain-Specific Taxonomy Grounded in Real-World Interactions
- Claim: A hierarchical task taxonomy derived from authentic user logs and business scenarios exposes domain-specific model weaknesses that general-purpose benchmarks miss.
- Mechanism: The benchmark's six categories and 37 tasks are mapped from real-world e-commerce interactions, including authentic customer queries and transaction logs. This captures noisy, heterogeneous, complex scenarios like after-sales service and product guidance. Models evaluated on this taxonomy reveal performance gaps specific to these business-critical but under-explored tasks.
- Core assumption: The included tasks are representative of the most frequent and important challenges in real-world e-commerce. Real-world data contains complexities (noise, specific terminology, multi-step reasoning) not found in synthetic datasets.
- Evidence anchors:
  - [abstract] "EcomEval covers six categories and 37 tasks... sourced primarily from authentic customer queries and transaction logs, reflecting the noisy and heterogeneous nature of real business interactions."
  - [section 3.1] "Our task tree includes 6 primary categories and dozens of tasks... The e-commerce tasks in EcomEval mainly come from our internal application scenarios."
  - [section 4.3.3] "Moreover, we observe that in Ecom QA, which includes the tasks of shopping guide and after-sales service (absent in existing e-commerce benchmarks), it is challenging for LLMs..."
  - [corpus] Related work (ChineseEcomQA, EcomBench) also emphasizes domain-specific evaluation, supporting the premise that general benchmarks are insufficient.
- Break condition: The taxonomy may fail to predict real-world performance if sampled logs are unrepresentative, if the e-commerce landscape shifts rapidly making tasks obsolete, or if translation of logs to benchmark tasks loses critical nuance.

## Foundational Learning

- **LLM-as-a-Judge Evaluation Paradigm**
  - Why needed here: EcomEval uses GPT-4.1 as an automated judge to score model responses on a 0-3 scale, which is then reviewed by human experts. Understanding this paradigm is critical for interpreting the reported performance scores.
  - Quick check question: How does using an LLM as a judge introduce potential bias into the evaluation, and how does the paper attempt to mitigate this?

- **Cross-Lingual Transfer in LLMs**
  - Why needed here: The benchmark spans 7 languages, including 5 low-resource Southeast Asian languages. Results show significant performance disparities between English and these languages (e.g., Table 3). Understanding cross-lingual capabilities is essential for interpreting these gaps.
  - Quick check question: Why might a model that performs well on general English tasks struggle on e-commerce queries in Indonesian or Vietnamese?

- **E-commerce Domain Specifics**
  - Why needed here: The paper identifies specific failure modes in tasks like "product tag generation" and "after-sales service." These require knowledge of e-commerce concepts (e.g., selling points, return policies) beyond general reasoning.
  - Quick check question: What is an example of e-commerce domain knowledge that a general-purpose LLM might lack, leading to failure on a task like "shopping guide"?

## Architecture Onboarding

- **Component map:**
  Data Collection -> Task Classification -> Question Verification -> Answer Generation & Verification -> Model Evaluation -> LLM-as-Judge Scoring -> Human Expert Review

- **Critical path:**
  The most critical step for data quality is **Step 4 (Answer Generation)**. This is where LLM drafts are created and verified against external sources (search engines, e-commerce platforms) and by human experts. Errors not caught here become the ground truth for evaluation.

- **Design tradeoffs:**
  - **Synthetic vs. Authentic Data:** The paper prioritizes authentic logs but supplements with data adapted from open-source datasets (e.g., Shopping MMLU) and machine-translated into low-resource languages, trading some purity for breadth.
  - **Human vs. LLM Judgement:** Uses a hybrid approach. LLM-as-a-judge provides scalability for evaluating 19 models on ~3,100 items, but adds a layer of potential bias mitigated by human expert review.

- **Failure signatures:**
  - **Low-resource language performance drop:** A clear signal of insufficient multilingual training data in the evaluated models (Table 3).
  - **Poor performance on "Ecom Generative Ability":** Indicates models struggle with structured, instruction-heavy output (e.g., title generation, Table D2).
  - **Weakness in "User Understanding":** Suggests models fail to grasp implicit context from user queries (e.g., query-product matching, Table D1).

- **First 3 experiments:**
  1. **Baseline a new model:** Run your model on the EcomEval benchmark (released on GitHub upon acceptance) using the provided evaluation harness. Compare performance against the 19 reported models to identify overall strengths and weaknesses.
  2. **Ablation on Difficulty:** Analyze your model's performance specifically on "Hard" tasks (as defined in the dataset). This pinpoints where reasoning or domain knowledge is most deficient.
  3. **Error Analysis on Specific Categories:** Focus on the "Ecom QA" category (shopping guide & after-sales). Manually inspect failure cases (examples in Appendix D) to understand if failures are due to lack of knowledge, poor instruction following, or reasoning errors. Use this to fine-tune or RAG-enhance your model.

## Open Questions the Paper Calls Out
- How does multi-turn conversational capability affect LLM performance on complex e-commerce tasks such as shopping guidance and after-sales support?
- To what extent does LLM-as-a-judge (GPT-4.1) introduce systematic bias when evaluating multilingual responses in low-resource Southeast Asian languages?
- What architectural or training improvements can mitigate models' tendency to overestimate product similarity based on textual titles while ignoring visual differences?
- Does translating existing benchmark items from English into low-resource languages preserve authentic e-commerce intent and cultural context?

## Limitations
- The semi-automatic data construction pipeline's efficiency and quality advantages are plausible but lack direct comparative evidence against purely synthetic or fully manual approaches.
- Difficulty calibration via model averaging is methodologically sound but may become less reliable as newer model architectures emerge.
- Performance disparities between English and low-resource languages are documented but the underlying causes (insufficient training data vs. linguistic complexity vs. evaluation artifacts) aren't systematically disentangled.

## Confidence
- **High confidence:** The benchmark's architecture and evaluation methodology are well-specified and reproducible. The claim that EcomEval reveals domain-specific weaknesses not captured by general benchmarks is strongly supported by comparative analysis.
- **Medium confidence:** The semi-automatic data construction pipeline's efficiency and quality advantages are plausible based on design, but direct comparative evidence is limited.
- **Medium confidence:** The difficulty calibration approach is methodologically sound, but its validity as a model-agnostic measure requires ongoing validation as LLM capabilities evolve.

## Next Checks
1. **Cross-lingual ablation study:** Evaluate models on English vs. low-resource language tasks with controlled difficulty levels to isolate whether performance gaps stem from linguistic complexity or insufficient training data.
2. **Human vs. LLM judge consistency:** Conduct a small-scale comparison where human experts score a subset of responses independently of the LLM judge to quantify potential bias in the automated evaluation pipeline.
3. **Domain generalization test:** Assess whether improvements on EcomEval translate to real-world e-commerce performance by deploying top-performing models on actual customer support or product recommendation tasks and measuring business metrics.