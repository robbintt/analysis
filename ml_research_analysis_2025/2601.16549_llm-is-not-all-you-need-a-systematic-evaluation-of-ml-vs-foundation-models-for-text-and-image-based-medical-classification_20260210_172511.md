---
ver: rpa2
title: 'LLM is Not All You Need: A Systematic Evaluation of ML vs. Foundation Models
  for text and image based Medical Classification'
arxiv_id: '2601.16549'
source_url: https://arxiv.org/abs/2601.16549
tags:
- classification
- medical
- text
- image
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study provides a systematic benchmark comparing traditional\
  \ machine learning models with modern foundation models for medical classification\
  \ across text and image modalities. The authors evaluate three model classes\u2014\
  classical ML (Logistic Regression, LightGBM, ResNet-50), prompt-based LLMs/VLMs\
  \ (Gemini 2.5), and fine-tuned PEFT models (LoRA-adapted Gemma3 variants)\u2014\
  on four public medical datasets covering binary and multiclass tasks."
---

# LLM is Not All You Need: A Systematic Evaluation of ML vs. Foundation Models for text and image based Medical Classification

## Quick Facts
- arXiv ID: 2601.16549
- Source URL: https://arxiv.org/abs/2601.16549
- Authors: Meet Raval; Tejul Pandit; Dhvani Upadhyay
- Reference count: 38
- Primary result: Traditional ML models outperformed foundation models on medical classification tasks

## Executive Summary
This study provides a systematic benchmark comparing traditional machine learning models with modern foundation models for medical classification across text and image modalities. The authors evaluate three model classes—classical ML (Logistic Regression, LightGBM, ResNet-50), prompt-based LLMs/VLMs (Gemini 2.5), and fine-tuned PEFT models (LoRA-adapted Gemma3 variants)—on four public medical datasets covering binary and multiclass tasks. Results show that traditional ML models consistently outperformed both zero-shot and fine-tuned LLMs/VLMs, particularly on structured text tasks, while the zero-shot Gemini 2.5 achieved competitive performance only on the multiclass image task. The fine-tuned LoRA models performed worst across all tasks, indicating that minimal fine-tuning was ineffective.

## Method Summary
The study evaluates three model classes on four public medical datasets: traditional ML models (Logistic Regression, LightGBM, ResNet-50), prompt-based LLMs/VLMs (Gemini 2.5), and fine-tuned PEFT models (LoRA-adapted Gemma3 variants). The datasets include binary and multiclass classification tasks across text and image modalities. Models are compared using standard metrics including accuracy, F1-score, and AUC-ROC. The fine-tuning approach uses LoRA with minimal training (3 epochs), while prompt-based models rely on basic prompting without advanced techniques.

## Key Results
- Traditional ML models (Logistic Regression, LightGBM, ResNet-50) consistently outperformed both zero-shot and fine-tuned LLMs/VLMs across all tasks
- Zero-shot Gemini 2.5 achieved competitive performance only on the multiclass image task
- Fine-tuned LoRA models performed worst across all tasks, indicating minimal fine-tuning was ineffective

## Why This Works (Mechanism)
The superior performance of traditional ML models likely stems from their ability to effectively capture patterns in structured medical data with fewer parameters and less computational overhead. These models are particularly well-suited for binary classification tasks common in medical diagnosis, where clear decision boundaries exist. The poor performance of fine-tuned foundation models with minimal training (3 epochs) suggests that adaptation strategies for these models in medical domains require more sophisticated approaches than basic PEFT methods.

## Foundational Learning

### Key Concepts
1. **Medical Classification Tasks** - Understanding binary vs. multiclass classification in clinical contexts; needed for proper task formulation and evaluation
2. **Traditional ML vs. Foundation Models** - Recognizing fundamental architectural differences and their implications for medical data; needed for appropriate model selection
3. **Fine-tuning Strategies** - Knowledge of PEFT methods (LoRA, QLoRA) and their training requirements; needed to interpret adaptation effectiveness
4. **Prompt Engineering** - Basic prompting techniques for LLMs/VLMs; needed to understand zero-shot performance limitations
5. **Medical Dataset Characteristics** - Structured vs. unstructured data in healthcare; needed for interpreting modality-specific results
6. **Evaluation Metrics** - Accuracy, F1-score, AUC-ROC in medical contexts; needed for proper performance assessment

## Architecture Onboarding

### Component Map
Traditional ML Models (Logistic Regression, LightGBM, ResNet-50) -> Prompt-based LLMs/VLMs (Gemini 2.5) -> Fine-tuned PEFT Models (LoRA-adapted Gemma3)

### Critical Path
Dataset preprocessing → Model training/adaptation → Performance evaluation → Comparison analysis

### Design Tradeoffs
The study chose minimal fine-tuning (3 epochs) to test the effectiveness of lightweight adaptation, potentially at the cost of suboptimal foundation model performance. The focus on basic prompting without advanced techniques may have limited LLM/VLM capabilities. Traditional ML models were selected for their proven track record in medical applications rather than exploring newer classical approaches.

### Failure Signatures
Consistently poor performance across all fine-tuned LoRA models suggests insufficient training duration or inappropriate adaptation strategy for medical classification tasks. The gap between traditional ML and foundation models indicates that current adaptation methods may not effectively leverage the strengths of LLMs/VLMs in structured medical data contexts.

### First Experiments
1. Increase LoRA fine-tuning duration to 10-20 epochs and compare performance
2. Implement structured prompt engineering (chain-of-thought, few-shot examples) for Gemini 2.5
3. Test alternative PEFT methods (QLoRA, Prefix Tuning) to evaluate different adaptation strategies

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation limited to four public datasets, which may not represent full complexity of real-world medical applications
- Only basic prompting and minimal fine-tuning (3 epochs) were tested, potentially underutilizing foundation model capabilities
- Binary nature of most tasks may have disadvantaged LLMs/VLMs, which often show relative strength on more complex, open-ended problems

## Confidence
- Traditional ML superiority: High confidence for evaluated datasets and tasks
- Fine-tuned LoRA failure: High confidence, though may reflect insufficient training rather than inherent limitation
- Zero-shot performance: Medium confidence, as basic prompting may not represent optimal usage

## Next Checks
1. Extended fine-tuning evaluation: Systematically test foundation models with varying fine-tuning durations (5, 10, 20 epochs) and alternative PEFT methods (QLoRA, Prefix Tuning) to determine optimal adaptation strategies.

2. Complex task expansion: Evaluate all model classes on multi-label classification tasks, ordinal regression problems, and longitudinal prediction scenarios to assess performance across clinically relevant task types.

3. Prompt engineering benchmarking: Implement structured prompt engineering strategies (chain-of-thought, few-shot examples, role prompting) for LLMs/VLMs to establish whether improved prompting can close the performance gap observed with basic prompts.