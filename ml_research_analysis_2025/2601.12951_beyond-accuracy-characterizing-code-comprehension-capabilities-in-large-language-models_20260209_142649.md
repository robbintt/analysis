---
ver: rpa2
title: 'Beyond Accuracy: Characterizing Code Comprehension Capabilities in (Large)
  Language Models'
arxiv_id: '2601.12951'
source_url: https://arxiv.org/abs/2601.12951
tags:
- code
- metrics
- performance
- shadow
- auroc
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study introduces a diagnostic framework to evaluate code comprehension
  in large language models (LLMs), reframing it as a binary input-output consistency
  task. It compares human-centric software metrics (e.g., code length, AST structure)
  against a feature-free shadow model trained on raw code and inputs.
---

# Beyond Accuracy: Characterizing Code Comprehension Capabilities in (Large) Language Models

## Quick Facts
- arXiv ID: 2601.12951
- Source URL: https://arxiv.org/abs/2601.12951
- Reference count: 10
- One-line primary result: Human-centric software metrics weakly predict LLM code comprehension success (AUROC 0.63), while shadow models trained on raw code achieve significantly higher accuracy (AUROC 0.86), revealing model-specific patterns beyond traditional benchmarks.

## Executive Summary
This study reframes code comprehension evaluation as a binary input-output consistency task, enabling systematic comparison of human-designed software metrics against learned shadow predictors. The framework diagnoses why LLMs succeed or fail on specific code comprehension instances rather than relying solely on aggregate accuracy. Human-centric metrics (code length, AST structure, complexity measures) show only weak correlation with LLM performance, while a feature-free shadow model fine-tuned on raw code sequences achieves substantially higher predictive accuracy. These findings suggest current benchmarks may miss crucial performance nuances and that LLM comprehension involves complex, non-human regularities that resist simple characterization.

## Method Summary
The framework generates a dataset from CodeNet Python subset, using type-aware fuzzing to create input-output pairs and executing programs to establish ground truth labels. Negative examples are constructed via in-program output shuffling while preserving lexical properties. For each target LLM, the system predicts success using two approaches: (1) an XGBoost classifier on ~300 human-designed static features (size, opcodes, AST/graph metrics, control-flow complexity) with SAGE-based feature pruning, and (2) a shadow model fine-tuning UniXcoder on raw code-input-output sequences. The comparison focuses on AUROC metrics, with feature importance analysis revealing that pruned models retain only ~5% of features while maintaining or improving performance.

## Key Results
- Human-centric metrics predict LLM success with AUROC 0.63, while shadow models achieve AUROC 0.86
- Pruned models with ~17 features (5.8% of original) achieve mean AUROC 0.634 versus 0.554 for full-feature baselines
- Top predictive features vary significantly across target models, indicating model-specific comprehension patterns
- Shadow model performance consistently exceeds human-metric predictors across all evaluated LLMs

## Why This Works (Mechanism)

### Mechanism 1
Reframing code comprehension as binary I/O consistency enables systematic evaluation across model types. Given program `p`, input `x`, and candidate output `y`, the task asks whether `y = f_p(x)` (ground truth via execution). Negative examples are constructed by shuffling outputs within the same program, preserving lexical and stylistic properties while breaking semantic correctness. Core assumption: program execution is deterministic and reproducible for label generation. Evidence: abstract defines the binary consistency task; section 3.1 specifies `t(p, x, y) = [y = f_p(x)]` and negative construction via shuffling; LoCoBench and CRUXEval validate the I/O consistency paradigm. Break condition: non-deterministic programs or programs with side effects producing different outputs across runs.

### Mechanism 2
Shadow models capture predictive patterns in LLM success that human-designed metrics miss. Fine-tuning UniXcoder on raw sequences `<p[SEP]x[SEP]y>` learns representations encoding high-dimensional interactions between code structure, I/O context, and model-specific behavior without handcrafted features. Core assumption: patterns predicting target LLM behavior are learnable from raw input representations. Evidence: abstract states shadow models achieve AUROC 0.86; section 4.3 reports mean AUROC 0.859 for shadow models vs. 0.634 for human-metric classifiers; Lachesis similarly predicts LLM inference success from structural properties. Break condition: target model behavior becomes highly context-dependent or distribution shifts between shadow model training and deployment.

### Mechanism 3
SAGE-based feature pruning reveals that human metrics are fragmented and model-specific. SAGE computes feature contributions averaged over all feature subsets, with pruning retaining features accounting for 95% of positive SAGE mass, yielding compact models with ~17 features. Core assumption: feature importance rankings generalize across instances and capture meaningful predictive structure. Evidence: section 3.3 describes SAGE computation and 95% coverage criterion; section 4.2 shows pruned models achieved mean AUROC of 0.634 versus 0.554 for full-feature baselines; top features vary by model (UniXcoder prioritizes lexical cues; GPT-OSS emphasizes opcodes and graph density). Break condition: feature correlations are spurious or dataset-specific, failing to transfer to new code domains.

## Foundational Learning

- **Binary Classification Metrics (AUROC, F1, Precision/Recall)**: Why needed here - the framework evaluates predictors of LLM success as binary classifiers; AUROC 0.63 vs. 0.86 is the central comparison. Quick check: Given a classifier with AUROC 0.86, what does this imply about true positive rate at different false positive thresholds?

- **Static Code Analysis (ASTs, CFGs, Bytecode, Cyclomatic Complexity)**: Why needed here - human-centric features include AST/graph structure, opcode statistics, and control-flow complexity; understanding these is essential to interpret feature importance results. Quick check: How does cyclomatic complexity differ from AST node count as a code complexity measure?

- **Transformer Code Encoders (UniXcoder)**: Why needed here - the shadow model fine-tunes UniXcoder; understanding its encoder-only architecture and pre-training objectives clarifies how it captures code patterns. Quick check: What does the `[SEP]` token do in the serialized input `<p[SEP]x[SEP]y>`?

- **Feature Importance via Shapley Values**: Why needed here - SAGE is used to rank features and prune the model; understanding cooperative game theory foundations helps interpret why certain features are retained. Quick check: Why does SAGE average over all feature subsets rather than computing importance for each feature independently?

## Architecture Onboarding

- **Component map**: CodeNet Python subset -> type-aware fuzzer -> executed I/O pairs -> filtered triples (p, x, y) with positive/negative labels -> dual prediction pipeline (XGBoost on features + UniXcoder on raw sequences) -> target LLM evaluation

- **Critical path**: Dataset construction (execute programs, generate I/O pairs, construct negative examples, filter by length, partition by problem); target model labeling (query each LLM, record success/failure); dual prediction pipeline (train XGBoost on features with SAGE pruning, fine-tune UniXcoder on raw inputs); compare AUROCs between human-metric and shadow models

- **Design tradeoffs**: Interpretability vs. accuracy (human-metric classifier interpretable but weak at AUROC 0.63; shadow model accurate but opaque); feature coverage vs. parsimony (full ~300 features underperform pruned ~17-feature models at 0.554 vs. 0.634 AUROC); generalization vs. model-specificity (shadow models trained per-target LLM; feature rankings vary across models suggesting limited cross-model transfer); aggregate vs. instance-level (framework enables per-sample diagnostics but requires more compute than aggregate benchmarks)

- **Failure signatures**: Human-metric predictor AUROC near 0.5 (features contain no signal; check feature extraction pipeline or data leakage); shadow model AUROC near 1.0 (possible train/test overlap; verify problem-level partitioning); large discrepancy between target LLM accuracy and shadow model AUROC (target LLM may have systematic biases like always predicting "yes"); feature importance concentrated in single feature (model may exploit trivial pattern like output length)

- **First 3 experiments**: 1) Reproduce baseline comparison: train XGBoost on all human metrics, compute SAGE, prune to 95% coverage, report AUROC for each target LLM (target ~0.63); 2) Train shadow model for one target LLM: fine-tune UniXcoder on (p, x, y) sequences for Mistral Small 24B, evaluate on held-out test split (target AUROC ~0.87); 3) Feature ablation study: train shadow models on subsets of human metrics (size-only, AST-only, opcode-only) to quantify contribution of each feature family, compare to full shadow model to confirm raw-input advantage

## Open Questions the Paper Calls Out

### Open Question 1
Can hybrid diagnostic frameworks effectively combine human-interpretable metrics with learned shadow predictors to balance explainability with higher predictive accuracy? Basis: conclusion explicitly calls for "hybrid approaches that combine human-interpretable diagnostics with learned predictors." Unresolved because current trade-offs force choice between weak-but-interpretable human metrics (AUROC 0.63) and strong-but-opaque shadow models (AUROC 0.86). Evidence to resolve: model integrating both feature types achieving high AUROC (>0.85) while providing feature attribution for specific failure modes.

### Open Question 2
Can the non-human, model-specific regularities captured by shadow models be reverse-engineered into concrete, human-understandable code properties? Basis: section 4.3 states shadow models outperform human metrics but "offer no interpretable explanation of why certain samples fail." Unresolved because specific latent features or high-dimensional interactions driving shadow model success remain opaque. Evidence to resolve: interpretable model using extracted features to match shadow model's performance, or successful attribution techniques applied to shadow model.

### Open Question 3
Do the weak correlations between static complexity metrics and LLM success generalize to diverse programming languages and paradigms? Basis: methodology restricts analysis to "Python portion of CodeNet," leaving other languages unexplored. Unresolved because Python-specific features (dynamic typing, specific AST structures) may drive observed weak correlations that might differ in static languages like Java or C++. Evidence to resolve: replication across multi-language subsets of CodeNet showing consistent or diverging metric correlations.

## Limitations
- Dataset representativeness: CodeNet Python subset may not reflect real-world code complexity or security-critical scenarios; synthetic inputs from type-aware fuzzer might miss edge cases or malicious patterns common in production
- Target model accessibility: GPT-OSS 120B not publicly documented; some models (GPT-3.5-Turbo, GPT-4) proprietary; limits reproducibility and raises questions about generalizability to other LLM architectures
- Metric validity: AUROC provides calibration-free evaluation but doesn't capture calibration quality or model calibration-accuracy tradeoffs; shadow model's superior AUROC might come at cost of poor calibration in deployment scenarios

## Confidence

- **High confidence**: Core finding that human-centric software metrics weakly predict LLM success (AUROC 0.63) is robust; experimental methodology is sound; result aligns with prior work showing limited correlation between traditional metrics and model performance
- **Medium confidence**: Shadow model's superior predictive accuracy (AUROC 0.86) is well-supported but depends on UniXcoder architecture's ability to capture relevant patterns; generalizability to other domains or model families remains uncertain
- **Low confidence**: Interpretation that LLM success reflects "complex, non-human regularities" is speculative; shadow model might exploit statistical patterns in specific CodeNet dataset rather than capturing genuine code comprehension capabilities

## Next Checks
1. Cross-domain generalization: Apply framework to code from different sources (GitHub, Stack Overflow, security benchmarks) to test whether shadow model superiority persists across diverse code distributions
2. Calibration analysis: Evaluate whether shadow models maintain predictive accuracy when tested on code from different programming languages or when target LLMs undergo fine-tuning on code-specific datasets
3. Feature stability validation: Conduct ablation studies to determine which feature families contribute most to predictive accuracy, and test whether these rankings remain stable when training data is augmented with adversarial code examples