---
ver: rpa2
title: Multi-Sourced Compositional Generalization in Visual Question Answering
arxiv_id: '2505.23045'
source_url: https://arxiv.org/abs/2505.23045
tags:
- primitive
- compositions
- visual
- novel
- primitives
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses multi-sourced compositional generalization
  (MSCG) in visual question answering (VQA), where compositions involve primitives
  from different modalities (linguistic and visual). The authors propose a retrieval-augmented
  training framework that learns unified representations for semantically equivalent
  primitives across modalities.
---

# Multi-Sourced Compositional Generalization in Visual Question Answering

## Quick Facts
- arXiv ID: 2505.23045
- Source URL: https://arxiv.org/abs/2505.23045
- Authors: Chuanhao Li; Wenbo Ye; Zhen Li; Yuwei Wu; Yunde Jia
- Reference count: 9
- Primary result: Retrieval-augmented training framework significantly improves VQA models' multi-sourced compositional generalization across linguistic, visual, and cross-modal compositions while maintaining IID performance

## Executive Summary
This paper addresses multi-sourced compositional generalization (MSCG) in visual question answering, where compositions involve primitives from different modalities (linguistic and visual). The authors propose a retrieval-augmented training framework that learns unified representations for semantically equivalent primitives across modalities. The framework constructs primitive databases for both linguistic and visual primitives, retrieves semantically similar features during training, and aggregates these with original features to refine the model. Experiments show significant improvements in MSCG ability across all composition types while maintaining IID generalization performance, with the framework being effective across different baseline models including CFR and Qwen-VL.

## Method Summary
The framework constructs primitive databases (D_q for linguistic, D_v for visual) from training data, retrieving semantically similar features during training using cosine similarity. For each training sample, it extracts primitive-level features, retrieves top-K similar features from databases, and aggregates them via weighted averaging. The refined features replace original primitive features before passing through the VQA model, which trains with its standard loss function. The framework operates during training only, requiring no changes at inference. Database features are updated throughout training, creating a dynamic refinement loop that encourages consistent representations for the same semantic primitive across modalities.

## Key Results
- Retrieval-augmented framework achieves significant MSCG improvements on GQA-MSCG across LL, VV, and LV composition types
- Maintains or improves IID generalization performance on GQA test-dev and VQA v2
- Effective across different baseline models including small models (CFR) and large multimodal models (Qwen-VL)
- Shows optimal performance with w_q=0.6, w_v=0.4, indicating linguistic primitives benefit more from cross-modal retrieval

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Retrieving semantically equivalent primitives across modalities during training improves generalization to novel cross-modal compositions
- **Mechanism:** Framework constructs separate databases for linguistic and visual primitives, retrieves top-K similar features using cosine similarity, and aggregates them with original features via weighted averaging. This encourages the model to learn consistent representations for the same semantic primitive regardless of modality
- **Core assumption:** Primitives with identical semantics (e.g., word "dog" and visual regions labeled "dog") should share similar representations in a well-generalizing model
- **Evidence anchors:** [abstract] "semantically equivalent primitives are retrieved for each primitive in the training samples"; [section 3.3] Aggregation formula; [corpus] Jing et al., 2024 shows retrieval-augmented primitive representations help compositional zero-shot learning
- **Break condition:** If retrieved primitives are noisy or databases lack diversity, aggregation may introduce conflicting signals rather than helpful alignment

### Mechanism 2
- **Claim:** Storing multiple contextual instances per primitive enables context-invariant feature learning
- **Mechanism:** Linguistic database stores T_q questions containing each primitive; visual database stores T_v images containing each visual entity label. During training, the same primitive appears in varied contexts, and retrieved features reflect this diversity, helping the model abstract away context-specific variations
- **Core assumption:** Feature extractor's representations of the same primitive vary across contexts, and averaging over diverse instances smooths this variability
- **Evidence anchors:** [section 3.2] "different questions provide different contexts"; [section 3.2] words with same prototype treated as same primitive; [corpus] Compositional-ARC finds systematic generalization requires exposure to diverse combinatorial patterns
- **Break condition:** If T_q or T_v is too small, retrieved features may not cover sufficient context variation

### Mechanism 3
- **Claim:** Feature-level aggregation preserves end-to-end differentiability while incorporating external knowledge
- **Mechanism:** Retrieved features are aggregated and replace original primitive features before passing through VQA model. Model trains with standard loss, requiring no additional loss terms. Database features are updated throughout training, creating dynamic refinement loop
- **Core assumption:** Model's feature extractors can be refined through gradient flow even when input features are partially composed of retrieved, evolving representations
- **Evidence anchors:** [section 3.4] "using same optimization approach as baseline model without introducing additional training losses"; [section 3.1] "VQA model continuously refines its feature extractor"; [corpus] SCRAMBLe uses synthetic preference data to enhance compositionality
- **Break condition:** If feature extractor's gradients are blocked or databases are not updated, refinement loop fails and representations stagnate

## Foundational Learning

- **Concept: Compositional Generalization**
  - **Why needed here:** Entire framework targets ability to generalize to novel compositions of seen primitives. Without understanding that "white dog" is composed of primitives "white" and "dog," motivation for retrieval-based alignment is unclear
  - **Quick check question:** Given training examples "red car," "blue car," "red truck," can a model correctly answer questions about "blue truck"? If you can't articulate why this is non-trivial, review compositional generalization literature

- **Concept: Cross-modal Alignment**
  - **Why needed here:** Paper explicitly addresses multi-sourced compositions where primitives come from different modalities. Understanding alignment is essential to grasp why retrieving visual "dog" features when processing linguistic "dog" should help
  - **Quick check question:** Why might a model that correctly answers "Is there a dog?" fail when asked to identify visual region corresponding to "dog"? If unclear, study vision-language alignment methods (CLIP, ALIGN)

- **Concept: Retrieval-Augmented Learning**
  - **Why needed here:** Framework's core intervention is retrieving similar primitives from databases during training. Understanding how retrieval can augment representation learning (vs. inference-only RAG) is critical
  - **Quick check question:** How does training-time retrieval differ from inference-time RAG? What are tradeoffs in terms of compute, generalization, and distribution shift?

## Architecture Onboarding

- **Component map:** Primitive Extraction -> Database Construction -> Feature Extraction -> Retrieval Module -> Aggregation Module -> VQA Backbone
- **Critical path:** 1. Pre-compute primitive databases from training set; 2. For each training batch: extract features → retrieve from D_q and D_v → aggregate → forward pass → backward pass (updates model AND database features); 3. Inference: standard forward pass (no retrieval)
- **Design tradeoffs:**
  - T_q, T_v: Larger values increase context diversity but slow retrieval and memory
  - K_q, K_v: More retrieved features provide smoother estimates but risk diluting signal with noise
  - w_q, w_v: Control modality emphasis; paper found w_q=0.6, w_v=0.4 optimal, suggesting linguistic primitives benefit more from cross-modal retrieval
  - Database update frequency: Updated every batch (implicit) vs. periodic refresh; not explicitly studied
- **Failure signatures:**
  - No improvement on IID tasks: May indicate retrieval is adding noise; check database quality and retrieval accuracy
  - Degraded performance on specific composition types: Check if corresponding database (D_q or D_v) is undersized or contains mislabeled primitives
  - Training instability: Large w_q/w_v may cause gradient issues; reduce aggregation weights
  - Memory explosion: Storing features for all primitives in databases; consider on-the-fly feature computation or pruning
- **First 3 experiments:**
  1. Sanity check: Train CFR on GQA train-balanced, evaluate on GQA-MSCG LL/VV/LV splits without retrieval. Confirm baseline underperforms on novel compositions vs. IID
  2. Ablation on database necessity: Enable only D_q or only D_v. Verify each provides complementary benefits and that joint use is best
  3. Hyperparameter sweep: Vary w_q ∈ {0.2, 0.4, 0.6, 0.8} with w_v = 1 - w_q. Confirm optimal is near w_q=0.6, w_v=0.4 and document sensitivity

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the retrieval-augmented framework generalize effectively to other vision-and-language tasks beyond VQA, such as image captioning, visual dialogue, or temporal video grounding?
- **Basis in paper:** [inferred] Framework claims to be "versatile" and "can be seamlessly incorporated into existing VQA models," but evaluation is limited to VQA on GQA, GQA-MSCG, and VQA v2 datasets
- **Why unresolved:** No experiments were conducted on other V&L tasks to validate cross-task generalization
- **What evidence would resolve it:** Experiments applying framework to image captioning (COCO Captions), visual dialogue (VisDial), or temporal video grounding (Charades-CG) showing consistent improvements

### Open Question 2
- **Question:** What is the computational overhead of the retrieval-augmented training process, and how does it scale with database size and model parameters?
- **Basis in paper:** [inferred] Framework retrieves top-K features (Kq=4, Kv=16) from databases containing Tq=8 and Tv=32 samples per primitive, but no analysis of training time, memory usage, or efficiency trade-offs is provided
- **Why unresolved:** Paper focuses on accuracy improvements without discussing practical training costs or scalability
- **What evidence would resolve it:** Comparative training time measurements, memory profiling, and analysis of performance vs. database size trade-offs for larger-scale deployments

### Open Question 3
- **Question:** Why does weighting the linguistic primitive database higher than the visual database (wq=0.6, wv=0.4) yield optimal performance, and is this asymmetry consistent across different datasets or model architectures?
- **Basis in paper:** [inferred] Parameter analysis (Figure 5) shows wq=0.6, wv=0.4 performs best, but paper provides no theoretical or empirical explanation for why linguistic primitives should contribute more
- **Why unresolved:** Asymmetry is observed but not explained; may be dataset-specific to GQA's compositional question structure
- **What evidence would resolve it:** Analysis across datasets with different linguistic/visual complexity ratios, or probing experiments examining feature alignment quality under different weight configurations

### Open Question 4
- **Question:** How does the framework perform on architectures other than pretrain-based models, such as attention-based (MAC), graph-based (LCGN), or neural module networks (MMN)?
- **Basis in paper:** [inferred] Framework tested on CFR (small pretrain-based) and Qwen-VL (large multimodal), but Table 2 shows other architecture types exist where relative improvement may differ
- **Why unresolved:** Architectural flexibility claim is partially validated but not comprehensively tested across diverse model families
- **What evidence would resolve it:** Experiments integrating framework with MAC, LCGN, or modular networks to assess whether retrieval-augmented primitive alignment benefits are architecture-agnostic

## Limitations

- Retrieval-augmented approach depends heavily on quality and diversity of primitive databases, which may not fully capture all possible compositional contexts
- Method introduces computational overhead during training due to retrieval operations, though this does not affect inference time
- Framework's effectiveness across CFR and Qwen-VL suggests robustness, but generalization to other VQA models remains untested

## Confidence

- **High Confidence:** Framework's ability to improve MSCG performance across all three composition types (LL, VV, LV) while maintaining IID generalization
- **Medium Confidence:** Claim that framework is effective across different baseline models (CFR and Qwen-VL), as demonstrated on only two specific architectures
- **Medium Confidence:** Assertion that aggregation weights (wq=0.6, wv=0.4) represent optimal balance, as determined through limited experimentation

## Next Checks

1. **Database Quality Analysis:** Systematically evaluate impact of database size (T_q, T_v) and retrieval count (K_q, K_v) on MSCG performance to identify optimal configurations and assess sensitivity to database quality
2. **Cross-Architecture Generalization:** Test framework on additional VQA architectures beyond CFR and Qwen-VL to validate versatility and identify any architecture-specific limitations
3. **Temporal Generalization:** Evaluate framework's performance when training and test data come from different distributions or time periods to assess robustness to distribution shift beyond controlled MSCG splits