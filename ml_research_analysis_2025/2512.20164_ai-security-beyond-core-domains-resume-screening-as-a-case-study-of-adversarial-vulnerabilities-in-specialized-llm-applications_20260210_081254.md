---
ver: rpa2
title: 'AI Security Beyond Core Domains: Resume Screening as a Case Study of Adversarial
  Vulnerabilities in Specialized LLM Applications'
arxiv_id: '2512.20164'
source_url: https://arxiv.org/abs/2512.20164
tags:
- attack
- resume
- adversarial
- https
- match
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the vulnerability of Large Language Models
  (LLMs) to adversarial resume injection attacks, where malicious candidates embed
  hidden instructions or manipulated content in resumes to manipulate automated screening
  decisions. The study introduces a systematic benchmark framework evaluating four
  attack types (instruction injection, invisible keywords, fabricated experience,
  and job manipulation) across four injection positions within candidate profiles.
---

# AI Security Beyond Core Domains: Resume Screening as a Case Study of Adversarial Vulnerabilities in Specialized LLM Applications

## Quick Facts
- arXiv ID: 2512.20164
- Source URL: https://arxiv.org/abs/2512.20164
- Authors: Honglin Mu; Jinghao Liu; Kaiyang Wan; Rui Xing; Xiuying Chen; Timothy Baldwin; Wanxiang Che
- Reference count: 18
- Primary result: Attack success rates exceeding 80% for certain adversarial techniques in LLM-based resume screening

## Executive Summary
This paper exposes significant security vulnerabilities in LLM-based resume screening systems, demonstrating how malicious candidates can manipulate automated hiring decisions through adversarial injection attacks. The research systematically evaluates four attack types across nine state-of-the-art models, revealing that job manipulation attacks are particularly effective at deceiving screening algorithms. The study introduces FIDS, a training-time defense mechanism using LoRA adaptation that outperforms traditional prompt-based approaches by achieving better attack reduction while maintaining lower false rejection rates. The findings highlight critical security gaps in specialized LLM applications beyond traditional core domains.

## Method Summary
The research establishes a comprehensive benchmark framework for evaluating adversarial attacks against LLM-based resume screening systems. Four distinct attack types were designed: instruction injection (embedding hidden instructions within resumes), invisible keywords (using invisible characters or similar Unicode characters), fabricated experience (inflating credentials), and job manipulation (misrepresenting job requirements). These attacks were tested across four injection positions within candidate profiles. The evaluation was conducted on nine state-of-the-art LLMs using a dataset of 6,000 real-world resumes and 2,000 job descriptions. Two defense mechanisms were assessed: prompt-based defenses and FIDS (Foreign Instruction Detection through Separation), which employs LoRA adaptation for detecting and mitigating malicious content.

## Key Results
- Attack success rates exceeded 80% for certain attack types, with job manipulation being particularly effective
- FIDS achieved 15.4% attack reduction with 10.4% false rejection increase, outperforming prompt-based defenses
- Combined approach (FIDS + prompt-based) provided 26.3% attack reduction
- Training-time defenses (FIDS) outperformed inference-time mitigations in both security and utility preservation

## Why This Works (Mechanism)
The effectiveness of adversarial attacks stems from LLMs' fundamental vulnerability to instruction injection and content manipulation. These models process textual input holistically, making them susceptible to hidden instructions embedded within seemingly legitimate content. The success of job manipulation attacks specifically exploits the model's tendency to align candidate profiles with job requirements, allowing attackers to subtly shift evaluation criteria. FIDS addresses these vulnerabilities through LoRA adaptation, which modifies the model's behavior at the parameter level rather than relying on surface-level prompt engineering, enabling more robust detection of malicious patterns that may be imperceptible to human reviewers.

## Foundational Learning
**LLM adversarial vulnerabilities** - Understanding how LLMs can be manipulated through carefully crafted inputs is essential for developing robust AI systems. Quick check: Test model responses to increasingly sophisticated prompt injections.
**Instruction injection techniques** - Knowledge of how hidden instructions can be embedded in legitimate text is crucial for both attack development and defense design. Quick check: Verify detection of invisible Unicode characters and formatting tricks.
**LoRA adaptation** - Fine-tuning specific layers of LLMs using low-rank adaptations provides efficient parameter-level modifications without full model retraining. Quick check: Measure performance impact when applying LoRA to different model layers.
**Resume screening workflows** - Understanding how automated systems evaluate candidate-job fit helps identify attack vectors and defense opportunities. Quick check: Map screening criteria to potential manipulation points.
**Adversarial benchmark design** - Creating realistic attack scenarios requires understanding both attacker capabilities and defender limitations. Quick check: Validate synthetic attack patterns against real-world examples.

## Architecture Onboarding

Component map:
Synthetic resume corpus -> Attack generation engine -> Injection positioning module -> LLM evaluation framework -> Defense mechanisms (FIDS, prompt-based) -> Performance metrics (attack success, false rejection)

Critical path:
Resume generation → Attack injection → LLM screening → Defense evaluation → Performance measurement

Design tradeoffs:
- Synthetic vs. real data: Synthetic data enables controlled experiments but may miss real-world complexity
- Training-time vs. inference-time defenses: FIDS requires upfront adaptation but provides better performance than prompt-based approaches
- Attack realism vs. reproducibility: Highly realistic attacks may be harder to consistently reproduce across models

Failure signatures:
- High false rejection rates indicate overly aggressive defense mechanisms
- Low attack detection suggests insufficient training data or inadequate defense parameters
- Inconsistent performance across models reveals fundamental architecture vulnerabilities

First experiments to run:
1. Test attack success rates on a single model with varying injection positions
2. Evaluate FIDS performance with different LoRA rank values
3. Compare prompt-based defense variations with minimal parameter tuning

## Open Questions the Paper Calls Out
None identified in the source material.

## Limitations
- Benchmark uses synthetic resume data that may not capture full complexity of real candidate profiles
- Attack success rates in controlled experiments may not translate directly to production environments
- FIDS effectiveness depends heavily on quality and diversity of synthetic malicious resume corpus
- 10.4% increase in false rejections could significantly impact hiring outcomes and fairness

## Confidence
- High confidence in demonstrating specific attack vulnerabilities across multiple models
- Medium confidence in defense effectiveness given synthetic data limitations
- Medium confidence in comparative defense performance between FIDS and prompt-based approaches

## Next Checks
1. Conduct field experiments with real-world resume screening workflows to validate attack success rates and defense performance under operational conditions
2. Test defense robustness against adaptive attackers who modify techniques based on deployed countermeasures
3. Evaluate the fairness impact of increased false rejections across different demographic groups to ensure non-discrimination compliance