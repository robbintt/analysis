---
ver: rpa2
title: A Modern Look at Simplicity Bias in Image Classification Tasks
arxiv_id: '2509.12265'
source_url: https://arxiv.org/abs/2509.12265
tags:
- bias
- simplicity
- tasks
- sensitivity
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates how simplicity bias in CLIP models relates
  to performance across various image classification tasks. The authors propose a
  frequency-aware metric to measure simplicity bias, addressing limitations of prior
  methods that only considered output sensitivity.
---

# A Modern Look at Simplicity Bias in Image Classification Tasks

## Quick Facts
- arXiv ID: 2509.12265
- Source URL: https://arxiv.org/abs/2509.12265
- Reference count: 40
- Key outcome: This paper investigates how simplicity bias in CLIP models relates to performance across various image classification tasks. The authors propose a frequency-aware metric to measure simplicity bias, addressing limitations of prior methods that only considered output sensitivity. This new metric captures differences by frequency bands and shows that models with stronger simplicity bias tend to be more sensitive to low-frequency features. The study examines two methods to modulate simplicity bias: BetaReLU for ResNets and LayerNorm scaling for ViTs. Results show that task performance tends to be higher when the simplicity bias aligns with task-specific preferences. For example, stronger simplicity bias correlates with better performance on OOD generalization and transfer attacks, while higher complexity correlates with better adversarial robustness. The optimal simplicity bias varies across tasks and architectures, with ViT generally requiring less simplicity bias than ResNet. These findings suggest that aligning a model's inductive biases with target task characteristics can improve performance across diverse image classification scenarios.

## Executive Summary
This paper investigates simplicity bias (SB) in CLIP models and its relationship to image classification performance. The authors propose a novel frequency-aware metric to measure SB, decomposing inputs into low-, mid-, and high-frequency bands and measuring output sensitivity along interpolation paths within each band. They examine two methods to modulate SB: BetaReLU for ResNets and LayerNorm scaling for ViTs. The study finds that task performance tends to be higher when SB aligns with task-specific preferences - stronger SB correlates with better OOD generalization and transfer attacks, while higher complexity correlates with better adversarial robustness. The optimal SB varies across tasks and architectures, with ViT generally requiring less SB than ResNet. These findings suggest that aligning a model's inductive biases with target task characteristics can improve performance across diverse image classification scenarios.

## Method Summary
The authors propose a frequency-aware measure to capture finer-grained simplicity bias (SB) differences by decomposing inputs into low-, mid-, and high-frequency bands via masked inverse Fourier transforms. They measure output sensitivity (logit variation) along interpolation paths within each band, using the ratio of low-to-high frequency sensitivity as a SB proxy. To modulate SB, they replace ReLU with BetaReLU (parameterized by β) for ResNets and scale LayerNorm weights by a scalar γ_s for ViTs. They fine-tune CLIP models on CIFAR-10 and evaluate zero-shot performance across multiple datasets, measuring SB and task performance across different SB modulation parameters.

## Key Results
- The frequency-aware SB metric captures finer-grained differences than uniform interpolation methods
- Stronger SB (lower β or γ_s) correlates with better OOD generalization and transfer attack performance
- Higher complexity (higher β or γ_s) correlates with better adversarial robustness
- Optimal SB varies across tasks and architectures, with ViT generally requiring less SB than ResNet

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Frequency-aware sensitivity metrics capture simplicity bias (SB) more reliably than uniform interpolation methods.
- Mechanism: Decompose inputs into low-, mid-, and high-frequency bands via masked inverse Fourier transforms. Measure output sensitivity (logit variation) along interpolation paths within each band. The ratio S_l / S_h serves as a SB proxy: higher ratios indicate stronger bias toward low-frequency (simpler) features.
- Core assumption: Low-frequency features correspond to simpler functions; high-frequency sensitivity reflects complexity.
- Evidence anchors:
  - [abstract] "We propose a frequency-aware measure capturing finer-grained SB differences."
  - [section 4.2, Eq. 14-15] Formal definition of band-specific sensitivity and ratio metric.
  - [corpus] Weak support: neighboring papers discuss SB in general terms but lack frequency-aware formulations.
- Break condition: If low-/high-frequency bands do not separate meaningful feature classes for a given dataset, the ratio may not reflect functional simplicity.

### Mechanism 2
- Claim: Activation smoothness modulation (BetaReLU) strengthens simplicity bias in ResNet-based CLIP encoders.
- Mechanism: Replace ReLU with BetaReLU parameterized by β ∈ (0,1]. Lower β yields smoother activations, reducing sensitivity to high-frequency components more than low-frequency ones during fine-tuning, effectively increasing S_l / S_h.
- Core assumption: Smoother activations suppress high-frequency feature learning preferentially.
- Evidence anchors:
  - [abstract] "models with stronger simplicity bias tend to be more sensitive to low-frequency features"
  - [section 5, Eq. 16] BetaReLU definition; [section 6.2, Figure 3] Sensitivity decay ratios showing faster HFC suppression at lower β.
  - [corpus] No direct evidence; related work [11, 15–17] cites periodic/smooth activations for other domains.
- Break condition: If the network's expressivity is already constrained (e.g., very small capacity), smoothing activations may not further shift SB measurably.

### Mechanism 3
- Claim: LayerNorm scaling modulates SB in ViT-based CLIP encoders, with larger γ_s increasing complexity.
- Mechanism: Multiply the learnable LayerNorm weight γ by a scalar γ_s. Larger γ_s amplifies normalized activations, potentially increasing model capacity to represent complex functions; smaller γ_s suppresses them, strengthening SB.
- Core assumption: LayerNorm magnitude directly influences the effective complexity of representable functions.
- Evidence anchors:
  - [section 5, Eq. 17] LayerNorm scaling formulation.
  - [section 6.2, Table 1] ViT-B/16 shows decreasing S_l / S_h as γ_s increases.
  - [corpus] Teney et al. [4] is cited for LayerNorm magnitude effects on MLP complexity.
- Break condition: If γ_s is too small (<0.9 per paper), model capacity may be excessively restricted, harming all performance metrics.

## Foundational Learning

- Concept: Simplicity bias (inductive bias toward simple functions)
  - Why needed here: The entire framework assumes SB is measurable and task-dependent; understanding that SB stems from architecture (activations, normalization) and optimization is prerequisite.
  - Quick check question: Can you explain why a constant function has complexity 0 per Definition 3.1?

- Concept: Frequency-domain image analysis (Fourier transform, band-pass filtering)
  - Why needed here: The proposed metric relies on decomposing images into frequency bands; practitioners must interpret low/mid/high-frequency components.
  - Quick check question: Given a 224×224 image, what spatial frequencies fall into the 0–25% normalized band?

- Concept: CLIP architecture (dual encoder, contrastive pre-training)
  - Why needed here: Experiments fine-tune and evaluate CLIP; understanding logits as f_I(x) · f_T(T) and zero-shot inference is essential.
  - Quick check question: How are zero-shot classifications performed without a task-specific classifier head?

## Architecture Onboarding

- Component map:
  - Input: Image pairs (x_1, x_2) from different classes
  - Frequency decomposition: F(x), band-pass mask m(·; r_k), inverse F⁻¹ → x_l, x_m, x_h
  - Interpolated paths: X^k_{x1,x2} = x_1 + [(1−λ)x^k_1 + λx^k_2], k∈{l,m,h}
  - Encoder: CLIP vision backbone (ResNet or ViT) with SB modulation (BetaReLU or LayerNorm scaling)
  - Logit extraction: Z^k = f_I(X^k) · f_T(T)
  - Sensitivity computation: S^k via total variation along path (Eq. 14)
  - SB metric: Ratio = S_l / S_h

- Critical path:
  1. Implement frequency decomposition (Eq. 10–11) with correct band thresholds.
  2. Generate interpolation paths preserving non-target frequencies (Eq. 12).
  3. Compute logit trajectories and apply TV-based sensitivity metric.
  4. Verify that BetaReLU/LayerNorm scaling modulates the ratio as expected.

- Design tradeoffs:
  - Band thresholds (25%, 80%, 100%) are heuristic; dataset-specific tuning may improve discrimination.
  - Metric depends on choice of image pairs and interpolation resolution n; higher n increases compute but smooths estimates.
  - SB modulation via activation/norm changes may interact with pre-trained weights unpredictably.

- Failure signatures:
  - Baseline sensitivity metric shows no trend with β or γ_s (Table 1) → indicates frequency-aware decomposition is necessary.
  - Sensitivity ratio spikes or instability (e.g., RN101 at β=0.8, Table 1) → may indicate numerical issues or dataset-specific anomalies.
  - ViT requires opposite SB modulation direction from ResNet for same task → suggests architecture-specific inherent SB.

- First 3 experiments:
  1. Reproduce Figure 3 on a held-out dataset to validate frequency-aware metric sensitivity to β.
  2. Fine-tune CLIP (RN50) on CIFAR-10 with β∈{1.0,0.9,0.8,0.7,0.6} and measure ID/OOD accuracy to confirm SB-benefit correlation.
  3. Apply LayerNorm scaling to ViT-B/16 on CIFAR-10-C and verify that optimal γ_s differs from ResNet's optimal β, testing architecture-dependent SB.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the key inductive biases, beyond simplicity bias, that significantly influence generalization and robustness in deep neural networks?
- Basis in paper: [explicit] The authors state in the conclusion: "Identifying other influential biases would further contribute to the understanding of generalization and robustness."
- Why unresolved: The paper focuses solely on simplicity bias, acknowledging it as one aspect of inductive bias, leaving other biases unexplored in the context of CLIP models.
- What evidence would resolve it: Systematic studies comparing multiple inductive biases across diverse models and tasks, potentially using controlled experiments to isolate their effects.

### Open Question 2
- Question: Can the proposed frequency-aware simplicity bias metric be effectively integrated as a supervisory signal during training to dynamically adjust model bias for improved downstream task performance?
- Basis in paper: [explicit] The authors suggest in future work: "Our simplicity bias metric can serve as a supervisory signal during training, allowing learnable adjustment of the bias to better suit downstream tasks."
- Why unresolved: The paper only uses the metric for post-hoc analysis; its utility in training as a loss component or regularizer remains untested.
- What evidence would resolve it: Experiments incorporating the metric into the training objective of CLIP or similar models, comparing performance on zero-shot and fine-tuning tasks against baselines.

### Open Question 3
- Question: How does the relationship between simplicity bias and task performance generalize to other large-scale vision models or non-CLIP architectures?
- Basis in paper: [inferred] The study is limited to CLIP models with ResNet and ViT backbones; the authors note that prior work focused on small models, implying a gap in understanding for other modern architectures.
- Why unresolved: Without empirical validation on models like Swin Transformers, MLP-Mixers, or self-supervised frameworks, it is unclear if findings on simplicity bias modulation and frequency sensitivity are architecture-specific.
- What evidence would resolve it: Replicating the frequency-aware measurement and modulation experiments on diverse vision models, assessing whether optimal bias levels correlate with task characteristics similarly.

## Limitations
- The frequency-aware metric depends on heuristic frequency band thresholds that may not generalize across all datasets
- The study focuses on CLIP models, limiting generalizability to other vision architectures
- The fine-tuning procedure uses fixed hyperparameters without ablation studies

## Confidence

- **High confidence**: The correlation between SB and task performance (stronger SB benefits OOD generalization and transfer attacks, while higher complexity benefits adversarial robustness) is well-supported by empirical results across multiple datasets.
- **Medium confidence**: The mechanism by which BetaReLU and LayerNorm scaling modulate SB is theoretically sound but lacks direct mechanistic validation beyond sensitivity measurements.
- **Medium confidence**: The claim that SB-optimal values vary across architectures is supported by results but requires more extensive architectural ablation to confirm it's not dataset-specific.

##