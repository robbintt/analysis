---
ver: rpa2
title: How to Make Large Language Models Generate 100% Valid Molecules?
arxiv_id: '2509.23099'
source_url: https://arxiv.org/abs/2509.23099
tags:
- smiles
- molecule
- invalid
- llms
- selfies
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the challenge of generating valid molecules
  using large language models (LLMs), which often struggle with strict syntax rules
  in representations like SMILES. The authors propose SmiSelf, a cross-chemical language
  framework that converts invalid SMILES strings into SELFIES and back, leveraging
  SELFIES' inherent robustness to correct errors.
---

# How to Make Large Language Models Generate 100% Valid Molecules?

## Quick Facts
- arXiv ID: 2509.23099
- Source URL: https://arxiv.org/abs/2509.23099
- Reference count: 26
- Large language models can generate 100% valid molecules by converting invalid SMILES strings to SELFIES and back

## Executive Summary
This paper addresses the challenge of generating valid molecular structures using large language models (LLMs), which often produce syntactically invalid SMILES strings. The authors propose SmiSelf, a cross-chemical language framework that converts invalid SMILES to SELFIES and back, leveraging SELFIES' inherent robustness to correct errors. The method achieves 100% validity while preserving molecular characteristics and improving performance on various similarity metrics. SmiSelf is compatible with all SMILES-based generative models and expands LLMs' practical applications in biomedicine without requiring model fine-tuning.

## Method Summary
SmiSelf is a post-hoc correction pipeline that ensures 100% validity of SMILES strings generated by LLMs. The method consists of four steps: (1) parsing invalid SMILES to molecular graphs using a custom error-tolerant parser, (2) converting graphs to SELFIES using Graph-SELFIES rules, (3) decoding SELFIES to semantically valid molecular graphs using SELFIES-Graph derivation rules, and (4) converting back to valid SMILES. The framework leverages SELFIES' grammar-enforced validity to correct both syntactic and semantic errors in LLM-generated SMILES. No model training is requiredâ€”it's a pure post-processing approach that works with any SMILES-based generative model.

## Key Results
- SmiSelf achieves 100% validity for LLM-generated molecules while preserving BLEU, Levenshtein, and fingerprint similarity scores
- The method outperforms LLM-based correctors and constrained decoding approaches in validity while maintaining or improving other metrics
- SmiSelf enables practical applications of LLMs in biomedicine by ensuring chemically valid outputs without sacrificing diversity

## Why This Works (Mechanism)

### Mechanism 1: Syntactic Recovery via Robust Parsing
The framework recovers valid molecular graphs from syntactically corrupted strings by decoupling strict SMILES grammar from graph topology during parsing. A custom parser reads invalid SMILES, extracting atoms and bonds into a graph structure while skipping syntax violations. This converts broken strings into syntactically valid intermediate graphs, preserving the raw atomic connectivity the LLM intended. The core assumption is that LLM outputs contain sufficient correct atomic tokens and bond operators to reconstruct chemically coherent topology. Break condition: If input contains too ambiguous or destructive token sequences, the resulting graph may be empty or topologically disconnected.

### Mechanism 2: Semantic Correction via SELFIES Derivation Rules
Converting the intermediate graph into SELFIES and back forces compliance with chemical valence rules through grammar-derived constraints. The SELFIES derivation rules function as a state machine that guarantees semantic validity. When re-parsed, if a derived bond would violate valence (e.g., carbon with 5 bonds), the SELFIES grammar automatically adjusts the bond order. The core assumption is that SELFIES grammar is sufficiently expressive to represent the "intended" molecule without excessively distorting structure. Break condition: If correction requires changing fundamental atom count or connectivity to satisfy valence, the resulting molecule may drift significantly from the semantic intent.

### Mechanism 3: Distribution Preservation via Deterministic Post-Hoc Processing
Unlike stochastic LLM correction, this deterministic pipeline preserves the statistical distribution of generated molecules relative to the prompt. By fixing errors via fixed algorithmic rules rather than regenerating parts of the string via probability, SmiSelf minimizes introduction of new, unrelated features. It acts as a "filter" or "projector" onto the manifold of valid molecules, maintaining fingerprints close to original generation. The core assumption is that invalid SMILES generated by LLM is closer to ground truth in latent space than randomly sampled valid molecule would be.

## Foundational Learning

- **Concept: SMILES vs. SELFIES Representations**
  - Why needed here: The method relies on brittleness of SMILES versus robustness of SELFIES. Understanding this duality is essential to grasp why translation works.
  - Quick check question: Why does random modification of SMILES string often yield invalid molecule, while same modification in SELFIES typically yields valid one?

- **Concept: Syntactic vs. Semantic Validity**
  - Why needed here: Paper distinguishes between fixing "grammar" of string (parentheses, ring numbers) and fixing "chemistry" (valence). Two-step process addresses these sequentially.
  - Quick check question: Is string `C#C=C` syntactically valid? Semantically valid? How would SELFIES handle middle carbon?

- **Concept: Post-Hoc Correction vs. Constrained Decoding**
  - Why needed here: Paper argues for post-hoc correction as superior to forcing LLM to generate perfectly or iteratively correcting. Understanding this architectural choice frames efficiency.
  - Quick check question: Why might "fixing" invalid molecule be computationally cheaper or more accurate than "forcing" LLM to generate perfectly first time?

## Architecture Onboarding

- **Component map:** Input (Invalid SMILES) -> Robust Parser (Invalid SMILES -> Intermediate Molecular Graph) -> SELFIES Encoder (Graph -> SELFIES String) -> SELFIES Decoder (SELFIES -> Validated Molecular Graph) -> SMILES Writer (Validated Graph -> Valid SMILES)

- **Critical path:** The Robust Parser. If parser cannot salvage invalid string into graph, entire pipeline fails. Quality of final molecule heavily contingent on how well parser interprets "intent" of broken syntax.

- **Design tradeoffs:**
  - SmiSelf vs. LLM Correction: SmiSelf is deterministic and cheap (no API calls), but strictly edits given string. LLM Corrector might "hallucinate" better molecule but at high cost and lower stability.
  - SmiSelf vs. Constrained Decoding: SmiSelf allows LLM to generate freely (higher creativity/diversity) and fixes mess later. Constrained decoding restricts LLM during generation, potentially stifling diversity.

- **Failure signatures:**
  - High Validity, Low Similarity: Output is valid chemistry but correction fundamentally changed molecule's structure, breaking link to input description.
  - Empty Output: Parser encountered unrecoverable syntax errors and produced empty graph.

- **First 3 experiments:**
  1. Baseline Error Analysis: Generate 1000 molecules with base LLM. Categorize errors using RDKit (Parentheses vs. Valence) to confirm distribution shown in Figure 5.
  2. Pipeline Integration Unit Test: Construct minimal "Robust Parser" handling most common error (parentheses). Feed example `CC(=O)OC1=CC=CC=C1=C(=O)O)` and verify intermediate graph matches Aspirin's topology partially.
  3. Metric Impact Analysis: Run full SmiSelf pipeline on validation set. Compare "Semantic Validity" of SmiSelf outputs against "LLM Corrector" baseline to quantify trade-off in BLEU/Levenshtein scores described in Table 6.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can distortion introduced by SmiSelf correction process be minimized to ensure corrected molecules remain strictly aligned with input textual descriptions?
- Basis in paper: [explicit] "Limitations" section states SmiSelf introduces distortion in correction process for text-based molecule generation task, leading to corrected molecules deviating further from ground truth and being less aligned with given descriptions.
- Why unresolved: While SmiSelf guarantees 100% validity, results in Table 4 show MACCS and RDK Fingerprint Tanimoto Similarity (FTS) scores can slightly decrease after correction, indicating trade-off between validity and fidelity to original generation intent.
- Evidence: Modified correction framework maintaining 100% validity rate while achieving statistically higher FTS scores compared to current SmiSelf baseline.

### Open Question 2
- Question: Can targeted training or prompting strategies improve ability of LLMs to act as post-hoc correctors for specific chemical syntax errors, such as valence violations?
- Basis in paper: [inferred] Section 3.3 notes while LLMs can correct some invalid SMILES, there are notable variations in correction rates across different models and error types, with Figure 6 showing distinct performance gaps between correcting parentheses errors versus valence errors.
- Why unresolved: Paper identifies inefficiency of LLMs as correctors and variance in performance, but does not propose method to stabilize or improve this capability to level of rule-based methods.
- Evidence: Study demonstrating that LLM, when fine-tuned on dataset of (invalid SMILES, valid SMILES) pairs, can match or exceed correction consistency of SmiSelf rule-based approach.

### Open Question 3
- Question: Is inferior performance of LLMs on SELFIES compared to SMILES primarily due to scarcity of training data, or do inherent grammatical complexities of SELFIES pose fundamental barrier for current architectures?
- Basis in paper: [inferred] Section 2.5 hypothesizes poor SELFIES performance stems from SMILES appearing much earlier in scientific literature (Figure 4), implying LLMs have seen significantly less SELFIES data during pre-training.
- Why unresolved: Paper evaluates current LLMs without conducting controlled experiment to determine if performance parity is achievable by simply scaling up SELFIES-specific pre-training data.
- Evidence: Comparative benchmark where LLM is pre-trained on balanced corpus of SMILES and SELFIES strings, showing performance gap can be closed or reversed.

## Limitations
- The method introduces some distortion in the correction process, potentially causing corrected molecules to deviate from the ground truth and be less aligned with given descriptions
- The custom error-tolerant parser implementation details are not fully specified, making faithful reproduction challenging
- The approach may systematically bias outputs toward certain structural motifs when SELFIES valence correction rules require changes

## Confidence
- **High confidence:** The core mechanism of using SELFIES as an interlingua for semantic validation is well-established in literature and theoretically sound
- **Medium confidence:** The syntactic recovery mechanism is plausible but depends on undocumented parser implementation details that could significantly impact performance
- **Medium confidence:** The claim of preserving molecular characteristics is supported by similarity metrics but may not hold for specialized chemical classes requiring strict structural features

## Next Checks
1. **Parser Robustness Analysis:** Systematically generate SMILES strings with increasing levels of corruption (from minor syntax errors to severely broken strings) and measure percentage of inputs that produce valid intermediate graphs versus empty outputs or disconnected fragments.

2. **Semantic Drift Quantification:** For dataset of known valid molecules, deliberately corrupt their SMILES representations, apply SmiSelf, and measure how often corrected output fundamentally changes molecular structure (e.g., removes rings, changes functional groups) versus making minimal corrections.

3. **Domain-Specific Performance:** Test SmiSelf on specialized chemical classes (e.g., organometallics, macrocycles, strained ring systems) where SELFIES' valence correction rules might introduce systematic biases, comparing validity rates and structural similarity to ground truth molecules.