---
ver: rpa2
title: 'DEEPQUESTION: Systematic Generation of Real-World Challenges for Evaluating
  LLMs Performance'
arxiv_id: '2505.24532'
source_url: https://arxiv.org/abs/2505.24532
tags:
- question
- questions
- prompt
- reasoning
- original
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces DeepQuestion, a scalable automated framework\
  \ that generates cognitively diverse benchmarks for evaluating large language models\
  \ (LLMs). DeepQuestion leverages Bloom\u2019s taxonomy to create two types of deeper\
  \ questions: scenario-based questions (Q2S) that embed problems in realistic narratives,\
  \ and instruction-based question generation (Q2I) that tests higher-order skills."
---

# DEEPQUESTION: Systematic Generation of Real-World Challenges for Evaluating LLMs Performance

## Quick Facts
- arXiv ID: 2505.24532
- Source URL: https://arxiv.org/abs/2505.24532
- Reference count: 29
- Primary result: DeepQuestion framework generates cognitively diverse benchmarks that expose reasoning gaps in LLMs, with up to 70% accuracy drops on higher-order tasks.

## Executive Summary
DeepQuestion is an automated framework that systematically generates cognitively diverse benchmarks for evaluating large language models using Bloom's taxonomy. The framework creates two types of deeper questions: scenario-based questions (Q2S) that embed problems in realistic narratives with distractors, and instruction-based question generation (Q2I) that tests higher-order cognitive skills by requiring models to generate questions from given instructions. Applied to GSM8K and physics exam questions, DeepQuestion reveals substantial performance degradation in LLMs when facing contextualized problems or generation tasks, despite strong performance on original benchmarks. The findings highlight persistent reasoning gaps in LLMs and demonstrate the need for cognitively rich benchmarks to advance model evaluation.

## Method Summary
DeepQuestion uses an iterative dual-LLM pipeline where a prompt generator proposes transformations and a prompt evaluator scores them on a 0-10 scale, iterating until a threshold (≥8) is reached. The framework creates two transformation types: Q2S wraps core problems in realistic narratives with extraneous details to test application-level reasoning, while Q2I provides target answers and requires models to construct valid questions, testing evaluation and creation skills. The method was applied to 60 GSM8K questions and 60 physics questions from Iranian University Entrance Exam, using Gemini 2.5 Pro for all pipeline roles. Evaluation involved ten models across three task types (original, Q2S, Q2I) with answerability checks and LLM-as-judge quality assessment for Q2I outputs.

## Key Results
- Models showed up to 70% accuracy drop on higher-order tasks despite strong performance on original benchmarks
- No model surpassed 38% accuracy on instruction-based question generation despite over 95% on original questions
- Q2S and Q2I transformations successfully exposed reasoning limitations in both reasoning-focused and general-purpose LLMs

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Embedding questions within realistic scenarios with distractors exposes brittle pattern-matching versus genuine comprehension.
- Mechanism: Q2S transformation wraps core problems in narrative contexts containing irrelevant numerical values and contextual noise. Models must isolate signal from noise, simulating real-world information overload where not all data is actionable.
- Core assumption: Performance degradation under distraction indicates insufficient conceptual grounding rather than task unfairness.
- Evidence anchors:
  - [section 2.1]: "embedding the core problem within a realistic narrative, often including extraneous details and some distractions to simulate real-world complexity"
  - [corpus]: GSM-Symbolic (Mirzadeh et al., 2025) shows similar degradation when irrelevant information is added, supporting the distraction-sensitivity pattern.
- Break condition: If models trained on noisy, real-world data show minimal Q2S degradation, the mechanism may reflect distribution mismatch rather than reasoning depth.

### Mechanism 2
- Claim: Reversing the solve-to-generate direction tests deeper cognitive levels (evaluation/creation) that standard benchmarks bypass.
- Mechanism: Q2I provides target answers and requires models to construct valid questions. This forces backward reasoning: understanding solution structure, selecting appropriate variables, and ensuring numerical coherence—all without explicit formula guidance.
- Core assumption: Question-generation ability correlates with deeper conceptual mastery than question-answering ability.
- Evidence anchors:
  - [section 2.2]: "Designing questions based on instruction implicitly tests three levels of the model's reasoning: conceptual understanding of the domain and relevant equations, the ability to define appropriate variables and their interrelations"
  - [results section 4.1]: "No model surpassed 38% accuracy on instruction-based question generation... despite over 95% on original questions"
- Break condition: If fine-tuning on generation tasks rapidly closes the gap without improving solving performance, the task may be skill-specific rather than depth-indicating.

### Mechanism 3
- Claim: Automated dual-LLM prompt refinement produces domain-adapted transformations without manual engineering.
- Mechanism: A generator LLM proposes prompts; an evaluator LLM scores them (0-10) with qualitative feedback. Iteration continues until threshold (≥8). This creates benchmark-specific prompts that respect domain conventions and difficulty calibration.
- Core assumption: LLM-as-evaluator provides reliable signal for prompt quality optimization.
- Evidence anchors:
  - [section 2.3]: "The pipeline operates through an iterative dialogue between two LLMs: a prompt generator and a prompt evaluator"
  - [corpus]: Limited direct evidence for dual-LLM optimization efficacy in related benchmarks; this mechanism is less validated externally.
- Break condition: If generated prompts produce inconsistent transformations across random seeds or evaluator choice significantly changes outcomes, the automation is unstable.

## Foundational Learning

- **Bloom's Taxonomy (six cognitive levels)**
  - Why needed here: The entire framework maps transformations to taxonomy levels (Q2S → Apply, Q2I → Evaluate/Create). Without this, you cannot interpret why different task types produce different failure modes.
  - Quick check question: Which taxonomy level does "design a physics problem yielding x = 3t − 4" target?

- **Distribution shift in evaluation**
  - Why needed here: The paper's core claim is that standard benchmarks suffer from construct invalidity—they test recall/understanding but not real-world application. Understanding distribution shift helps distinguish model limitations from benchmark limitations.
  - Quick check question: Why might a model scoring 95% on GSM8K drop to 25% on Q2I versions of the same questions?

- **LLM-as-judge evaluation paradigm**
  - Why needed here: Q2I quality assessment relies on O4-mini comparing generated vs. original questions across five criteria. Understanding judge limitations (bias, calibration) is critical for interpreting results.
  - Quick check question: What failure modes might occur when using one LLM to evaluate another's creative output?

## Architecture Onboarding

- **Component map:**
  - Source Benchmark → Sample Batch → Prompt Generator LLM → Candidate Prompt → Prompt Evaluator LLM → (score < 8? loop) → Accepted Prompt → Question Generator LLM → Transformed Questions
  - Evaluation: Answerability Checker (O4-mini) + LLM-as-Judge (O4-mini) across 5 criteria

- **Critical path:**
  1. Prompt generation loop convergence (most variable step)
  2. Q2S/Q2I transformation quality
  3. Answerability verification (binary)
  4. Qualitative judging (comparative)

- **Design tradeoffs:**
  - Using Gemini-2.5-Pro for all framework roles vs. mixing models: consistency vs. potential systematic bias
  - Temperature=0 for reproducibility vs. exploration of generation diversity
  - 60 questions per benchmark: statistical power vs. annotation cost

- **Failure signatures:**
  - Prompts stuck in iteration loop (score never reaches 8): evaluator criteria may be too strict or contradictory
  - Generated questions unsolvable by any model: transformation corrupted core logic
  - Judge rates all generated questions as inferior regardless of model: criterion bias or framing issues

- **First 3 experiments:**
  1. Replicate Q2S on 20 GSM8K questions with manual prompt inspection—verify distractors are truly irrelevant and core solution path preserved.
  2. Ablate the dual-LLM prompt optimization by using a fixed template—quantify how much automated refinement improves transformation quality.
  3. Cross-evaluate: use prompts generated for physics on a chemistry dataset to test domain specificity of the prompt-generation mechanism.

## Open Questions the Paper Calls Out

- **Open Question 1**: How does DeepQuestion perform on open-ended questions or domains beyond mathematics and physics?
  - Basis in paper: [explicit] The authors state in the Limitations section: "Although our proposed framework is capable of handling open-ended questions, in this study we restrict our focus to questions with definite correct answers... our framework is not specifically tailored to these benchmarks and can be extended to other domains as well."
  - Why unresolved: The current study only evaluated two domains with well-defined answers; generalizability to other domains (e.g., humanities, medicine) and open-ended tasks remains untested.
  - What evidence would resolve it: Experiments applying DeepQuestion to benchmarks with open-ended responses (e.g., essays, clinical reasoning scenarios) and diverse domains, with appropriate evaluation metrics.

- **Open Question 2**: How sensitive is the framework's output quality to the choice of LLM used in the prompt generation pipeline?
  - Basis in paper: [inferred] The paper exclusively uses Gemini 2.5 Pro for all three pipeline roles (prompt generator, prompt evaluator, question generator), without ablation or comparison to other models.
  - Why unresolved: Different LLMs may produce varying prompt quality, affecting the resulting deep questions and potentially biasing evaluation toward models similar to the pipeline generator.
  - What evidence would resolve it: Ablation studies varying each pipeline component's LLM independently and measuring resulting question quality and downstream model performance.

- **Open Question 3**: Can training interventions (e.g., fine-tuning on higher-order cognitive tasks) close the performance gap between original and DeepQuestion-enhanced benchmarks?
  - Basis in paper: [inferred] Results show 8–70% accuracy drops on DeepQuestion tasks, with generated questions scoring lower than originals on quality criteria. The paper concludes that "a significant quality gap persists" but does not explore remediation.
  - Why unresolved: The paper identifies the limitation but does not investigate whether models can be improved to handle higher-order cognitive tasks better.
  - What evidence would resolve it: Fine-tuning experiments on Q2S/Q2I-style tasks, followed by re-evaluation on DeepQuestion benchmarks to measure improvement.

## Limitations

- The automated evaluation pipeline relies heavily on LLM-as-judge mechanisms, which may introduce systematic bias without external validation
- Performance degradation claims may conflate reasoning limitations with distributional sensitivity rather than genuine cognitive gaps
- The Persian physics dataset used for evaluation is not publicly available, limiting reproducibility and independent verification

## Confidence

**High Confidence** (Mechanistic understanding validated):
- The Q2S transformation mechanism (scenario embedding with distractors) is well-specified and reproducible given the prompt templates in the appendix
- The Q2I transformation logic (question generation from instructions) follows a clear backward-reasoning paradigm

**Medium Confidence** (Core claims supported but with caveats):
- Performance degradation claims (70% accuracy drop) are methodologically sound but may conflate reasoning limitations with distributional sensitivity
- The taxonomy mapping (Q2S→Apply, Q2I→Evaluate/Create) provides useful framework but requires empirical validation that transformed tasks actually demand those cognitive levels

**Low Confidence** (Limited external validation):
- The dual-LLM prompt optimization mechanism's effectiveness beyond tested domains remains unproven
- The claim that Q2I tasks test "Evaluate/Create" levels specifically lacks independent verification that generated questions meet these criteria

## Next Checks

1. **Distribution Shift Validation**: Test whether performance drops persist when irrelevant distractors are semantically coherent but mathematically irrelevant (e.g., weather data in a physics problem). If drops remain, this strengthens the reasoning-gap claim; if not, the effect may be distributional rather than cognitive.

2. **Cross-Domain Prompt Transfer**: Apply prompts optimized for GSM8K to a chemistry or biology dataset. Consistent transformation quality across domains would validate the automation mechanism; domain-specific failures would reveal brittleness.

3. **External Judge Validation**: Have human experts rate a sample of Q2S and Q2I outputs against the five evaluation criteria. Compare human vs. O4-mini judgments to quantify judge bias and establish ground truth for future automated evaluations.