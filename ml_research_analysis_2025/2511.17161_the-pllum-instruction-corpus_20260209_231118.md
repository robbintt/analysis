---
ver: rpa2
title: The PLLuM Instruction Corpus
arxiv_id: '2511.17161'
source_url: https://arxiv.org/abs/2511.17161
tags:
- text
- prompt
- instructions
- data
- polish
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The PLLuM project developed a large instruction corpus to fine-tune
  Polish language models. The corpus combines human-authored (organic), synthetic,
  and converted instructions, totaling 84,795 samples.
---

# The PLLuM Instruction Corpus

## Quick Facts
- arXiv ID: 2511.17161
- Source URL: https://arxiv.org/abs/2511.17161
- Reference count: 40
- The PLLuM project developed a large instruction corpus to fine-tune Polish language models, achieving strong performance on Polish benchmarks.

## Executive Summary
The PLLuM project developed a comprehensive instruction corpus for fine-tuning Polish language models, consisting of 84,795 samples combining organic, synthetic, and converted instructions. The project's key finding is that effective instruction fine-tuning requires prior continual pre-training on the target language, as models fine-tuned directly on instructions without this foundation show degraded performance. The final models achieved strong results on Polish linguistic and cultural competency benchmarks (PLCC scores up to 69.17) while maintaining safety through alignment on human preference datasets.

## Method Summary
The project employed a three-phase pipeline: (1) continual pre-training on ~150B Polish tokens to build language-specific representations, (2) supervised fine-tuning (SFT) on the PLLuMIC corpus using AdamW optimizer with 1e-5 learning rate for 3 epochs, and (3) alignment using ORPO on 40,000+ preference pairs. The instruction corpus contains 84,795 samples with organic instructions comprising 47.12%, converted 43.56%, and synthetic 7.32% of the total. Base models included Mistral-Nemo-Base-2407, Mixtral-8x7B-v0.1, and Llama-3.1 variants, fine-tuned using DeepSpeed ZeRO-3 across multi-node NVIDIA H100 infrastructure.

## Key Results
- Fine-tuning on Polish instructions only effective after continual pre-training (otherwise degrades performance)
- Synthetic instructions induce negative linguistic transfer, introducing English-influenced stylistic patterns in Polish outputs
- Small high-quality organic instruction sets (~100 samples) can effectively teach specific Polish language conventions
- Final models achieved PLCC scores up to 69.17 and LLMzSzŁ scores up to 64.42 on Polish benchmarks
- Alignment improved safety but increased verbosity and false-refusal rates

## Why This Works (Mechanism)

### Mechanism 1: Continual Pre-training Enables Effective Fine-tuning
- Claim: Fine-tuning on language-specific instructions only produces gains when the model has first undergone continual pre-training on the target language.
- Mechanism: Continual pre-training on ~150B Polish tokens builds language-specific representations that allow the model to properly absorb instruction-following behaviors. Without this foundation, the instruction signal cannot be integrated effectively—the model lacks the linguistic substrate to apply the instructions to.
- Core assumption: Base multilingual models have insufficient target-language representation depth to benefit from instruction tuning without additional exposure.
- Evidence anchors: [section 5.3]: "fine-tuning on PLLuMIC is only effective for models that have undergone continual pretraining; otherwise, fine-tuning even degrades the model performance"; [section 5.3, Table 4]: Base models fine-tuned directly on PLLuMIC scored lower than continually pre-trained then fine-tuned versions.

### Mechanism 2: Synthetic Data Induces Negative Linguistic Transfer
- Claim: Synthetic instructions distilled from English-dominant teacher models introduce non-idiomatic patterns into target-language outputs.
- Mechanism: Transformer models transfer stylistic conventions from dominant pre-training languages. When generating synthetic Polish instructions, GPT-4 (predominantly English-trained) injects English email conventions even while producing grammatically correct Polish text. This interference occurs because the model's stylistic priors are shaped by English patterns.
- Core assumption: Teacher models encode and transfer language-specific stylistic conventions, not just semantic content.
- Evidence anchors: [section 5.1]: Provides concrete example—"Mam nadzieję, że ten email zastanie Pana w dobrym zdrowiu" is a direct translation of English formula, sounding unidiomatic in Polish; [section 5.1]: "Transformer-based models tend to transfer stylistic conventions from languages best represented in the pre-training phase, leading to non-idiomatic outputs."

### Mechanism 3: Small High-Quality Organic Sets Imprint Idiomatic Conventions
- Claim: Focused organic instruction sets (~100 samples) can effectively teach specific language conventions that differ from dominant-language patterns.
- Mechanism: Fine-tuning on high-quality exemplars provides explicit signal that overrides implicit stylistic priors from pre-training. The paper shows that specific Polish email conventions required only ~100 curated examples to establish consistently.
- Core assumption: Human annotators with linguistic expertise can produce consistently correct idiomatic examples.
- Evidence anchors: [section 5.1]: "a subset of less than 100 high-quality e-mail writing instructions was sufficient to imprint the above-mentioned (and several other) guidelines"; [section 4.1]: Organic instructions underwent "rigorous quality control" with trained annotators.

## Foundational Learning

- **Concept: Continual Pre-training vs. Fine-tuning Sequence**
  - Why needed here: The paper's central finding is that training order matters—fine-tuning on instructions fails or degrades performance without prior language adaptation.
  - Quick check question: Why would fine-tuning on Polish instructions decrease performance if applied to a base model without continual pre-training?

- **Concept: Negative Linguistic Transfer**
  - Why needed here: Understanding this phenomenon explains why synthetic data from English-dominant models produces unidiomatic outputs in other languages.
  - Quick check question: What are two specific examples of English-style email conventions that incorrectly appeared in Polish outputs?

- **Concept: Instruction Typology (Organic/Converted/Synthetic)**
  - Why needed here: The paper's 47%/44%/7% composition reflects deliberate tradeoffs between cost, scale, and linguistic quality.
  - Quick check question: Why did the project limit converted instructions to maximum 1,000 per source dataset?

## Architecture Onboarding

- **Component map**: Base models (Mistral-Nemo-Base-2407, Mixtral-8x7B-v0.1, Llama-3.1-8B/70B) -> Continual pre-training (~150B Polish tokens) -> SFT (3 epochs, AdamW, lr=1e-5, cosine scheduler, max seq 16,384) -> Alignment (ORPO)

- **Critical path**: 1. Continual pre-training on Polish corpus—blocking prerequisite for downstream effectiveness; 2. SFT with organic instruction majority to establish idiomatic baseline; 3. ORPO alignment on 40K+ preference pairs for safety/helpfulness balance

- **Design tradeoffs**: Organic vs. synthetic: Organic ensures idiomatic quality but is expensive; synthetic scales but risks negative transfer; Converted data: Efficient but risks repetitiveness—limited to 1K per resource to preserve conversational versatility; Alignment method: ORPO post-SFT outperformed DPO/KTO/PPO, but increases verbosity and false-refusal rates

- **Failure signatures**: Fine-tuning without pre-training: PLCC scores drop below baseline; Synthetic-heavy training: English punctuation/conventions in Polish outputs; Over-alignment: Increased false-refusal rates (FRR rose from ~0.6% to 3-8% after alignment)

- **First 3 experiments**: 1. Validate pre-training prerequisite with controlled ablation: Compare (a) base + SFT, (b) pre-trained + SFT, (c) pre-trained + SFT + alignment across all model sizes; 2. Measure organic/synthetic ratio effects: Vary proportions systematically while tracking both benchmark scores and human evaluation of idiomatic quality; 3. Quantify negative transfer sources: Generate synthetic instructions from teachers with varying English dominance and correlate with stylistic error rates

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can negative linguistic transfer from dominant languages (e.g., English) be mitigated during the alignment and distillation phases for non-English LLMs?
- Basis in paper: [explicit] Section 5.2 notes that without adjustments, alignment on preference datasets occasionally resulted in grammatical and stylistic inconsistencies reflecting English-language rules.
- Why unresolved: The authors observed that safety alignment often propagates stylistic interference from models predominantly trained on English, compromising idiomatic output.
- What evidence would resolve it: Development of alignment algorithms that disentangle safety preferences from language-specific stylistic conventions.

### Open Question 2
- Question: What is the optimal interaction between the volume of continual pre-training and the efficacy of subsequent instruction fine-tuning?
- Basis in paper: [explicit] Section 5.3 concludes that fine-tuning on Polish instructions was only effective for models that had first undergone sufficient continual pre-training; otherwise, performance degraded.
- Why unresolved: The study establishes the necessity of pre-training priming but leaves the precise "sufficiency" threshold and the balance between data volume and quality undefined.
- What evidence would resolve it: Ablation studies measuring model performance across varying scales of continual pre-training data prior to instruction tuning.

### Open Question 3
- Question: To what extent does preference optimization (e.g., ORPO) inadvertently increase verbosity and false refusal rates in low-resource language models?
- Basis in paper: [explicit] Section 5.2 states that while alignment improved safety, it also caused verbosity and a "tendency for models to overly refuse to respond, even in the case of non-adversarial prompts."
- Why unresolved: The trade-off between enforcing safety constraints and maintaining helpfulness/conciseness is noted as a side effect of the alignment process.
- What evidence would resolve it: Comparative evaluations of alignment methods specifically tracking verbosity and refusal rates on benign prompts.

## Limitations

- The core mechanism showing continual pre-training is prerequisite for effective instruction fine-tuning is primarily observational rather than experimentally isolated
- Negative linguistic transfer mechanism lacks quantitative measurement of stylistic divergence or systematic comparison across different synthetic instruction sources
- Effectiveness of small organic instruction sets is based on project-specific observations rather than systematic experimentation with varying set sizes

## Confidence

**High Confidence**: The empirical results showing that continually pre-trained models achieve superior PLCC and LLMzSzŁ scores compared to base models fine-tuned without pre-training.

**Medium Confidence**: The assertion that continual pre-training is a blocking prerequisite for instruction fine-tuning effectiveness.

**Low Confidence**: The specific claims about negative linguistic transfer being primarily caused by English stylistic conventions in synthetic instructions.

## Next Checks

1. **Controlled Pre-training Ablation Study:** Systematically compare model performance across (a) base + SFT, (b) continually pre-trained + SFT, and (c) continually pre-trained + SFT + alignment, using identical instruction datasets and hyperparameters to isolate the pre-training effect.

2. **Synthetic Instruction Source Analysis:** Generate synthetic instructions from multiple teacher models with varying language dominance and measure corresponding rates of stylistic errors in Polish outputs using both automated stylistic metrics and human evaluation.

3. **Organic Instruction Set Size Experiment:** Systematically vary the proportion of organic instructions in the fine-tuning dataset while keeping total dataset size constant, measuring both benchmark performance and human-rated idiomatic quality to determine the minimum effective organic instruction threshold.