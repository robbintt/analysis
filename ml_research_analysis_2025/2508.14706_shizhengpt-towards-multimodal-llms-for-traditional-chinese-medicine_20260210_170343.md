---
ver: rpa2
title: 'ShizhenGPT: Towards Multimodal LLMs for Traditional Chinese Medicine'
arxiv_id: '2508.14706'
source_url: https://arxiv.org/abs/2508.14706
tags:
- data
- arxiv
- multimodal
- shizhengpt
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ShizhenGPT is a multimodal large language model tailored for Traditional
  Chinese Medicine (TCM), addressing data scarcity and multimodal diagnostic needs.
  It integrates vision, audio, and physiological signal encoders with a 32B-parameter
  LLM backbone, trained on 100GB+ text and 200GB+ multimodal data.
---

# ShizhenGPT: Towards Multimodal LLMs for Traditional Chinese Medicine

## Quick Facts
- arXiv ID: 2508.14706
- Source URL: https://arxiv.org/abs/2508.14706
- Authors: Junying Chen; Zhenyang Cai; Zhiheng Liu; Yunjin Yang; Rongsheng Wang; Qingying Xiao; Xiangyi Feng; Zhan Su; Jing Guo; Xiangyi Wan; Guangjun Yu; Haizhou Li; Benyou Wang
- Reference count: 40
- Primary result: 78.1 average score on national TCM licensing exams, leading multimodal models on TCM-Vision benchmark (63.6 accuracy)

## Executive Summary
ShizhenGPT is a multimodal large language model designed for Traditional Chinese Medicine that integrates vision, audio, and physiological signal processing into a unified diagnostic framework. The model addresses data scarcity in TCM by combining extensive text corpora with multimodal datasets including images, audio, and physiological signals like pulse and ECG. Trained on a 32B-parameter backbone with specialized encoders for each modality, ShizhenGPT achieves state-of-the-art performance on TCM licensing exams and visual diagnostic tasks while demonstrating strong unified perception across the four traditional diagnostic methods of TCM.

## Method Summary
ShizhenGPT employs a two-stage pre-training approach using a Qwen-2.5-32B backbone with Qwen-2.5-VL vision encoder and Whisper-large-v3 signal encoder. Stage 1 involves text-only pre-training on 11.9B tokens from TCM books and web sources, followed by Stage 2 multimodal pre-training on 3.8B tokens combining images, audio, and resampled text to preserve language grounding. A 1-layer MLP adapter projects mel-spectrogram representations to LLM embedding space for unified signal processing. Instruction tuning uses 83K text, 65K vision, 57K speech, and signal datasets across 3 epochs. All training uses full-parameter tuning on 16×A100 GPUs with batch sizes of 256 for text and 128 for multimodal stages.

## Key Results
- Achieves 78.1 average score on national TCM licensing exams, outperforming comparable models by 5.5 points
- Leads TCM-Vision benchmark with 63.6 accuracy on 7,204 multiple-choice questions
- Strong unified perception across modalities: 80.5% on pregnancy detection via pulse, 83.1% on ECG beat classification, 48.8% on smell disease classification
- Outperforms baselines on general audio benchmarks with 64.0 on Llama Question Speech vs Qwen2-Audio's 60.0

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Domain-specific pre-training substantially improves TCM expertise and multimodal understanding
- Mechanism: Large-scale TCM text corpus (11.9B tokens in Stage 1) infuses domain knowledge into the LLM backbone before multimodal alignment, enabling better grounding of visual and signal inputs in TCM diagnostic concepts
- Core assumption: TCM domain knowledge is sufficiently encoded in text to support downstream multimodal reasoning
- Evidence anchors: Ablation shows 70.1 vs 66.9 on Pharmacist exam, 76.4 vs 68.8 on Physician exam with/without pretraining

### Mechanism 2
- Claim: Unified waveform-to-spectrogram encoding enables cross-modal signal perception
- Mechanism: Non-audio physiological signals (pulse, smell, ECG) are converted to waveforms via linear interpolation, resampled to 16kHz, transformed to 128-channel mel-spectrograms, then projected into LLM embedding space via a 1-layer MLP adapter—allowing a single Whisper-based encoder to process diverse modalities
- Core assumption: Physiological signals share sufficient temporal structure with audio for Whisper's encoder to extract meaningful features
- Evidence anchors: Shows 80.5% on pregnancy detection via pulse, 83.1% on ECG beat classification, 48.8% on smell disease classification—above random baselines

### Mechanism 3
- Claim: Two-stage pre-training with text resampling preserves language grounding during multimodal learning
- Mechanism: Stage 2 multimodal pre-training includes 1.8B resampled textual tokens from Stage 1 alongside image-text and audio-text data, preventing catastrophic forgetting of text-only capabilities while aligning modalities
- Core assumption: Mixed-modality training with text resampling maintains base LLM capabilities
- Evidence anchors: General audio benchmarks show ShizhenGPT-7B achieves 64.0 on Llama Question Speech vs Qwen2-Audio's 60.0, suggesting general capabilities preserved

## Foundational Learning

- Concept: **Vision-Language Adapter Architecture**
  - Why needed here: Understanding how visual patches are grouped, concatenated, and projected via 2-layer MLP into LLM embedding space is essential for debugging visual task failures
  - Quick check question: Can you explain why adjacent visual patches are grouped (4 patches) before projection?

- Concept: **Mel-Spectrogram Signal Representation**
  - Why needed here: All physiological signals (pulse, smell, ECG) are normalized to this format; understanding the 25ms window / 10ms hop / 128-channel parameters is critical for signal preprocessing
  - Quick check question: What is the resulting temporal resolution per token after stride-2 pooling?

- Concept: **TCM Four Diagnostic Methods (四诊)**
  - Why needed here: The model's modality coverage maps directly to TCM's "Four Examinations": looking (vision), listening/smelling (audio + smell), questioning (text), pulse-taking (pulse signals)
  - Quick check question: Which diagnostic method does the "smell" modality correspond to in TCM theory?

## Architecture Onboarding

- Component map:
  - Qwen-2.5-32B backbone -> Vision encoder (Qwen-2.5-VL) with 2-layer MLP adapter -> LLM embedding space
  - Qwen-2.5-32B backbone -> Whisper-large-v3 signal encoder with 1-layer MLP adapter -> LLM embedding space

- Critical path:
  1. Signal preprocessing: Raw signal → linear interpolation to waveform → 16kHz resample → mel-spectrogram (128-channel, 25ms window, 10ms hop) → stride-2 pooling → 40ms-per-token
  2. Vision preprocessing: Image → ViT patches → group 4 adjacent patches → concatenate → 2-layer MLP
  3. Both paths converge at LLM embedding space for unified token sequence processing

- Design tradeoffs:
  - 32B backbone vs 7B: 5.5-point average gain on TCM exams (78.1 vs 72.9) but significantly higher inference cost
  - Single Whisper encoder for all signals vs specialized encoders: Simpler but may lose modality-specific nuance
  - Full-parameter tuning vs adapter-only: Paper uses full-parameter; higher cost but better performance per ablation

- Failure signatures:
  - Vision tasks fail but text works → Check Vision Adapter projection or encoder initialization
  - Signal tasks at random baseline → Verify mel-spectrogram conversion, check if modality token (<Pulse>, <Smell>) prepended correctly
  - General language capabilities degraded → May indicate insufficient text resampling in Stage 2

- First 3 experiments:
  1. Reproduce signal encoding pipeline: Take a pulse waveform, convert to mel-spectrogram, verify 40ms-per-token temporal resolution matches paper specification
  2. Ablate modality tokens: Test pulse/ECG tasks with and without <Pulse>/<ECG> tokens to confirm their role in modality disambiguation
  3. Verify vision-language alignment: Run TCM-Vision benchmark subset on medicinal recognition; if significantly below 74.2% (32B), check Vision Adapter initialization from Qwen-2.5-VL

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can ShizhenGPT's performance on physiological signals (pulse, smell, ECG) be improved with larger, higher-quality datasets beyond the currently scarce public sources?
- Basis in paper: The authors state "Despite the large-scale collection of text and image data, high-quality signal data (e.g., smell, pulse) remains scarce. Most of it comes from limited public datasets, which constrains the model's ability to fully develop and generalize in these modalities."
- Why unresolved: The current signal datasets are small (e.g., 4,101 pulse samples, 672 smell samples), limiting model generalization; it remains unclear whether performance bottlenecks stem from data scarcity or architectural limitations.
- What evidence would resolve it: Systematic experiments varying signal dataset size while holding architecture constant, showing performance scaling curves.

### Open Question 2
- Question: Does adapting the Whisper speech encoder for non-audio physiological signals (pulse, smell, ECG via waveform conversion) result in suboptimal feature extraction compared to modality-specific encoders?
- Basis in paper: The paper converts non-audio signals to 16kHz waveforms processed through Whisper, noting that "although pulse signals are low-frequency, the resulting 40ms-per-token resolution retains sufficient temporal granularity"—this claim is not empirically validated against specialized encoders.
- Why unresolved: No ablation compares Whisper-based signal encoding against dedicated time-series encoders for physiological data.
- What evidence would resolve it: Ablation study comparing Whisper encoder vs. specialized signal encoders (e.g., CNN/Transformer for ECG) on the same pulse/smell/ECG tasks.

### Open Question 3
- Question: How does ShizhenGPT perform in real-world clinical deployment with actual patient feedback, compared to its strong benchmark and expert evaluation results?
- Basis in paper: The authors state "ShizhenGPT has not been tested in actual clinical settings. The absence of patient-level feedback limits our understanding of its practical effectiveness and safety."
- Why unresolved: All evaluations (licensing exams, visual benchmarks, human evaluation) use curated test sets or expert preference ratings, not live patient interactions with outcome tracking.
- What evidence would resolve it: Prospective clinical trial measuring diagnostic accuracy, treatment recommendation appropriateness, and patient outcomes in operational TCM clinics.

### Open Question 4
- Question: How does synthetic TTS-generated audio data affect the model's ability to handle real patient speech with acoustic variations (dialects, dyspnea, emotional states)?
- Basis in paper: The paper synthesizes 58K audio-text pairs from dialogues "via a high-fidelity TTS system," but real auscultation involves pathological voice changes not captured by standard TTS.
- Why unresolved: No evaluation compares model performance on synthesized training audio versus authentic patient recordings with acoustic pathology.
- What evidence would resolve it: Benchmark comparing model performance on TTS-synthesized vs. real patient speech samples, measuring accuracy degradation on pathological voice characteristics.

## Limitations

- Data access constraints: Performance relies on proprietary TCM corpora and specialized signal datasets not publicly available
- Modality-specific limitations: Smell classification achieves only 48.8% accuracy, marginally above random baselines
- No clinical validation: Model excels on exams and benchmarks but lacks real-world patient outcome data

## Confidence

- **High Confidence**: Domain-specific pretraining improves TCM exam performance (strong ablation evidence: 70.1 vs 66.9 on Pharmacist exam)
- **Medium Confidence**: Unified multimodal encoding architecture works across vision, audio, and physiological signals (consistent performance above baselines, but modality-specific limitations noted)
- **Low Confidence**: The model achieves "holistic" TCM diagnosis comparable to human practitioners (no clinical deployment data, only exam and benchmark performance reported)

## Next Checks

1. **Modality Token Ablation Study** - Test pulse and ECG classification tasks with and without modality tokens (<Pulse>, <ECG>) to quantify their contribution to signal disambiguation and performance.

2. **Temporal Resolution Impact Analysis** - Vary the mel-spectrogram stride/pooling parameters to generate models with 20ms, 40ms, and 80ms temporal resolution per token. Compare diagnostic accuracy on pulse and ECG tasks.

3. **Cross-Institutional Data Validation** - Replicate the signal encoding and classification pipeline using independent pulse and ECG datasets from different TCM hospitals or research institutions.