---
ver: rpa2
title: Estimating the Empowerment of Language Model Agents
arxiv_id: '2509.22504'
source_url: https://arxiv.org/abs/2509.22504
tags:
- empowerment
- agent
- eelma
- state
- states
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "EELMA (Estimating Empowerment of Language Model Agents) introduces\
  \ an information-theoretic framework for evaluating LM-agent capability without\
  \ requiring task-specific goals. The method estimates effective empowerment\u2014\
  the mutual information between agent actions and future states\u2014from multi-turn\
  \ text interactions using contrastive learning on state-action embeddings."
---

# Estimating the Empowerment of Language Model Agents

## Quick Facts
- arXiv ID: 2509.22504
- Source URL: https://arxiv.org/abs/2509.22504
- Authors: Jinyeop Song; Jeff Gore; Max Kleiman-Weiner
- Reference count: 40
- Primary result: EELMA correlates strongly with task performance (r=0.70-0.94) across language games and web-browsing tasks

## Executive Summary
EELMA (Estimating Empowerment of Language Model Agents) introduces an information-theoretic framework for evaluating LM-agent capability without requiring task-specific goals. The method estimates effective empowerment—the mutual information between agent actions and future states—from multi-turn text interactions using contrastive learning on state-action embeddings. In controlled language games (Gridworld, Tower of Hanoi) and realistic web-browsing tasks (WebArena), EELMA estimates strongly correlate with task performance, capture the impact of model scale, chain-of-thought, and memory, and identify pivotal agent behaviors like successful authentication. The approach is robust to natural-language variability and outperforms prompt-only LLM estimators that systematically overpredict empowerment. EELMA provides a scalable, goal-agnostic metric for monitoring agent capability and detecting anomalous behavior in complex, open-ended settings.

## Method Summary
EELMA estimates the mutual information between agent actions and future states using contrastive learning on state-action embeddings. The method trains encoder networks to maximize InfoNCE loss on state-action-future state tuples extracted from multi-turn text trajectories. Two encoder networks project state and (state, action) pairs into a shared embedding space where empowerment is computed as the difference in mutual information between future states given states alone versus states plus actions. The approach is validated on three domains: Gridworld (5×5 navigation), Tower of Hanoi (4 disks, 3 rods), and WebArena (web-browsing tasks including GitLab, Reddit, and shopping). EELMA demonstrates strong correlation with task performance across agent ablations and is robust to natural language variability.

## Key Results
- EELMA estimates correlate strongly with task performance (r=0.70-0.94) across Gridworld, Tower of Hanoi, and WebArena tasks
- EELMA captures the impact of model scale, chain-of-thought, and memory on agent capability
- EELMA identifies pivotal agent behaviors like successful authentication in web tasks
- EELMA outperforms prompt-only LLM estimators that systematically overpredict empowerment by 10-25×

## Why This Works (Mechanism)
EELMA works by estimating the mutual information between agent actions and future states using contrastive learning on state-action embeddings. The key insight is that empowerment—the agent's ability to influence its future states—can be captured as the information gain from knowing the agent's action. By training encoder networks to maximize the similarity between (state, action) embeddings and future state embeddings while minimizing similarity with unrelated future states, EELMA learns a representation space where empowerment can be computed as the difference in mutual information. This approach avoids the need for reward functions or goal specifications, making it applicable to open-ended environments where traditional evaluation metrics fail.

## Foundational Learning
- **Mutual Information Estimation**: Mutual information measures the dependence between variables; here between actions and future states. Why needed: Empowerment is fundamentally an information-theoretic quantity. Quick check: Verify MI estimation matches analytical values on synthetic data.
- **Contrastive Learning**: Uses InfoNCE loss to learn representations by contrasting positive and negative samples. Why needed: Enables learning of the embedding space without explicit labels. Quick check: Confirm contrastive loss decreases during training.
- **State-Action Embedding**: Maps state-action pairs to a shared representation space with future states. Why needed: Creates the mathematical foundation for computing empowerment. Quick check: Ensure embeddings preserve semantic similarity.

## Architecture Onboarding

**Component Map**: Text Trajectories -> State-Action Extraction -> Embedding Projection -> Encoder Networks (ϕ, ψ) -> InfoNCE Loss -> Empowerment Estimation

**Critical Path**: Multi-turn text trajectories → State-action-future state tuples → Contrastive training → Encoder networks → Empowerment computation → Correlation with task performance

**Design Tradeoffs**: 
- Uses pretrained embeddings vs. training from scratch (better generalization vs. task-specific optimization)
- InfoNCE contrastive loss vs. other MI estimators (sample efficiency vs. potential bias)
- State-only vs. state-action contrastive terms (captures action influence vs. computational cost)

**Failure Signatures**:
- Low correlation with task performance indicates poor empowerment estimation
- Systematic overestimation by prompt-only estimators reveals limitations of naive approaches
- Degradation on paraphrased observations indicates sensitivity to language variability

**First Experiments**:
1. Train on Gridworld with known ground truth to verify correlation with task performance
2. Test robustness by evaluating on paraphrased observations from the same trajectories
3. Compare against prompt-only LLM estimators on the same task set

## Open Questions the Paper Calls Out

**Open Question 1**: Can EELMA be extended to quantify empowerment in multi-agent settings where agents influence each other's states and beliefs? The current formulation models single-agent MDPs; multi-agent interactions introduce game-theoretic complexity. Future research could extend this method to multi-agent scenarios.

**Open Question 2**: Does integrating visual or audio embeddings into EELMA preserve its correlation with task performance for multimodal agents? EELMA has only been tested on text-based observations; cross-modal alignment may introduce noise. Applicability to Multimodal Models identifies this as a promising direction for future research.

**Open Question 3**: How can empowerment-based metrics account for option quality rather than just quantity, to better predict performance in tasks requiring precise reasoning? The Shopping domain showed flat empowerment-reward relationship; having more options does not always translate directly into greater power.

## Limitations
- Validated only on synthetic and web-browsing tasks, leaving open whether the method generalizes to more complex domains like robotics or multi-agent environments
- Relies on fixed embedding models without fine-tuning, potentially limiting adaptation to domain-specific language patterns
- Temperature parameter τ initialization and update mechanism are underspecified, creating uncertainty about implementation details

## Confidence

**High confidence** in the correlation results (r=0.70-0.94) between estimated empowerment and task performance, as these are demonstrated across multiple tasks and ablations.

**Medium confidence** in the robustness claims to natural language variability, since the comparison is limited to a single paraphrasing dataset (ParaSCI).

**Medium confidence** in the superiority over prompt-only estimators, given the systematic overestimation by 10-25× is shown but the comparison methodology for more sophisticated estimators is not explored.

## Next Checks
1. Test EELMA on additional domains beyond Gridworld, Tower of Hanoi, and WebArena (e.g., robotic control or multi-agent coordination) to assess generalization
2. Implement and evaluate alternative temperature update mechanisms for τ to verify the robustness of results to this hyperparameter
3. Compare against other information-theoretic estimators or learned reward models that incorporate future state information, not just prompt-only approaches