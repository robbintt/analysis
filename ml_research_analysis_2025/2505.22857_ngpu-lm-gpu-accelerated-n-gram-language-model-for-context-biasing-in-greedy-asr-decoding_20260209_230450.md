---
ver: rpa2
title: 'NGPU-LM: GPU-Accelerated N-Gram Language Model for Context-Biasing in Greedy
  ASR Decoding'
arxiv_id: '2505.22857'
source_url: https://arxiv.org/abs/2505.22857
tags:
- greedy
- decoding
- language
- beam
- states
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the inefficiency of traditional n-gram language
  models (LMs) in GPU-accelerated speech recognition, particularly during greedy decoding.
  The core contribution is NGPU-LM, a novel GPU-optimized data structure and algorithm
  for n-gram LMs that enables fast, parallelized full-vocabulary scoring.
---

# NGPU-LM: GPU-Accelerated N-Gram Language Model for Context-Biasing in Greedy ASR Decoding

## Quick Facts
- arXiv ID: 2505.22857
- Source URL: https://arxiv.org/abs/2505.22857
- Reference count: 0
- Primary result: GPU-optimized n-gram LM enables fast full-vocabulary scoring for greedy ASR decoding with up to 10.6% relative WER reduction

## Executive Summary
This paper addresses the inefficiency of traditional n-gram language models (LMs) in GPU-accelerated speech recognition, particularly during greedy decoding. The core contribution is NGPU-LM, a novel GPU-optimized data structure and algorithm for n-gram LMs that enables fast, parallelized full-vocabulary scoring. This is achieved through a batched trie-based representation that supports efficient range-based token probability retrieval, avoiding the slow sequential lookups typical of CPU-based implementations. NGPU-LM is integrated into greedy decoding pipelines for major ASR architectures (CTC, Transducers, AED) with minimal overhead.

## Method Summary
NGPU-LM transforms n-gram LMs into GPU-optimized tensors representing arcs (tokens, weights, states) with range indices for parallel lookup. The method uses batched trie-based representation with start_arcs and end_arcs tensors to define transition ranges per state, enabling efficient range-based token probability retrieval. A Triton kernel implementation supports CUDA graphs compatibility, with PyTorch fallback for CPU/non-Triton environments. Architecture-specific token grouping preserves special token behavior during fusion (blank/repeat/other tokens handled separately). The approach enables full-vocabulary scoring in greedy decoding without beam search overhead, maintaining high decoding speed while improving out-of-domain recognition accuracy.

## Key Results
- Achieves up to 10.6% relative WER reduction in out-of-domain recognition
- Maintains decoding speed with <7% RTFx overhead compared to baseline greedy decoding
- Successfully integrated with CTC, RNN-T, TDT, and AED ASR architectures
- Open-sourced implementation available in NeMo toolkit

## Why This Works (Mechanism)

### Mechanism 1: Range-Based Trie Tensorization for Parallel Lookup
Representing the n-gram trie as sorted tensors with range indices enables GPU-parallel probability retrieval across the full vocabulary. Arcs are sorted by (from_state, token) tuples; auxiliary start_arcs and end_arcs tensors define transition ranges per state. Probability retrieval becomes a parallel "index-select" operation rather than sequential binary search or hash lookup. The n-gram order bounds backoff chain depth (≤10 in experiments), keeping iteration count small enough for GPU efficiency.

### Mechanism 2: Iterative Backoff Accumulation for Full-Vocabulary Scoring
Full-vocabulary scores can be computed by bounded iteration through backoff transitions, accumulating weights until all tokens are resolved. Initialize output tensors for all V tokens. For each state, retrieve direct transition scores. Unresolved tokens trigger backoff traversal with accumulated discount weights, repeating until all scores are filled or the unigram state is reached. The n-gram LM has a well-formed backoff structure where every token eventually resolves to a unigram weight (or <unk>).

### Mechanism 3: Architecture-Specific Token Grouping During Fusion
Preserving special token behavior (blank, repeat, EOS) while applying LM scores only to relevant tokens prevents fusion-induced errors. Transducers use two-stage selection (blank preserved; non-blanks rescored). CTC maintains three groups (blank, repeated, other). AED precomputes final weights for EOS handling. This prevents blank dominance from inflating deletions.

## Foundational Learning

- Concept: N-gram Backoff and Smoothing
  - Why needed here: The algorithm's core loop traverses backoff chains; understanding how n-gram models fall back to shorter contexts is prerequisite.
  - Quick check question: In a 4-gram model, if P(D|A B C) is not stored, what context does the model query next?

- Concept: Trie/Graph Representation of LMs
  - Why needed here: NGPU-LM is fundamentally a trie-to-tensor transformation; understanding states, arcs, and suffix links is required to follow Algorithm 1.
  - Quick check question: In the paper's example (Figure 1), which state does the "sat" node backoff to?

- Concept: Shallow Fusion in ASR Decoding
  - Why needed here: The method applies LM scores at inference time without retraining; understanding log-probability summation is essential.
  - Quick check question: Why are ASR and LM log-probabilities summed rather than multiplied directly during shallow fusion?

## Architecture Onboarding

- Component map: ARPA file → KenLM parsing → Tensor conversion (arcs, weights, state ranges, backoff tensors) → NGPU-LM module (Triton kernel primary; PyTorch fallback) → ASR encoder → decoder → log-probabilities → Fusion layer with architecture-specific token grouping → Greedy selection over combined scores → state update

- Critical path: Parse ARPA; build sorted arc tensors and range indices. For each decoding step: ASR produces vocab log-probs; NGPU-LM retrieves LM scores for batch of states. Fuse scores respecting token groups (blank/repeat/other); apply greedy selection; update LM state.

- Design tradeoffs: Triton kernel vs PyTorch (CUDA graphs vs CPU support); Full-vocabulary vs selective scoring (memory vs complexity); N-gram order (context capture vs memory/iterations).

- Failure signatures: Transducer deletion increase (direct LM fusion without blank separation → use two-stage selection); Unexpected slowdown (CPU-bound PyTorch loop → switch to Triton kernel); AED EOS generation failure (without LM, beam search may not emit end-of-sequence → precompute final weights); OOM on large vocabs (full-vocab tensors exceed GPU memory → consider pruning or batching).

- First 3 experiments: Baseline validation (greedy with/without NGPU-LM on in-domain and out-of-domain sets; verify WER improvement and <7% RTFx overhead); Architecture sweep (test CTC, RNN-T, TDT, AED with identical LM; confirm blank/repeat handling prevents deletion issues); Scaling profile (measure memory and latency across vocabulary sizes 1024→5000→10000; identify breakpoints and iteration variance).

## Open Questions the Paper Calls Out
None

## Limitations
- Method assumes bounded backoff chain depth (≤10 in experiments) and well-formed backoff structures, which may break for very high-order n-grams or malformed LMs with cycles or missing unigrams.
- Full-vocabulary scoring approach may become memory-prohibitive for vocabularies significantly larger than 1024 tokens.
- WER improvements depend on unspecified LM weight tuning and may be sensitive to hyperparameters across different ASR architectures and domains.

## Confidence

**High Confidence** - The core GPU acceleration mechanism (range-based trie tensorization with start_arcs/end_arcs for parallel lookup) is well-specified and directly supported by the code implementation.

**Medium Confidence** - The claimed WER improvements (up to 10.6% relative reduction) are supported by experimental results, but depend on unspecified LM weight tuning and may not generalize to all ASR architectures or domains without careful hyperparameter adjustment.

**Low Confidence** - The method's behavior under malformed backoff structures, extremely large vocabularies, or very high n-gram orders remains untested. The scalability claims are based on limited testing with 10-gram LMs and 1024-token vocabularies.

## Next Checks

1. **Backoff Chain Robustness Test**: Construct malformed n-gram LMs with cycles or missing unigrams and verify that NGPU-LM's iterative backoff algorithm either handles these gracefully or fails with appropriate error messages, rather than causing non-termination or incorrect scoring.

2. **Vocabulary Scaling Benchmark**: Measure memory usage and decoding latency as vocabulary size scales from 1024 to 5000 to 10000 tokens, identifying the exact breakpoints where the full-vocabulary approach becomes impractical and quantifying the iteration overhead increase.

3. **Architecture Weight Sensitivity Analysis**: Systematically vary the LM fusion weight across a wide range (0.0 to 2.0) for each ASR architecture (CTC, RNN-T, TDT, AED) on out-of-domain test sets to determine the sensitivity of WER improvements to this hyperparameter and identify optimal ranges for each architecture.