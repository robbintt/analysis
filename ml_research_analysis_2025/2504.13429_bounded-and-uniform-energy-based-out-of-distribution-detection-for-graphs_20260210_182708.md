---
ver: rpa2
title: Bounded and Uniform Energy-based Out-of-distribution Detection for Graphs
arxiv_id: '2504.13429'
source_url: https://arxiv.org/abs/2504.13429
tags:
- detection
- data
- energy
- gnnsafe
- scores
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper tackles the problem of detecting out-of-distribution
  (OOD) data at the node level in graph neural networks (GNNs), where existing methods
  suffer from extreme score variance during negative energy score aggregation. The
  authors propose NODESAFE, which introduces two optimization terms to address this
  issue: one that bounds the variance of logit 2-norms (Lbound) to constrain negative
  energy scores, and another that mitigates logit shift by reducing the variance of
  logit sums (Luniform).'
---

# Bounded and Uniform Energy-based Out-of-distribution Detection for Graphs

## Quick Facts
- arXiv ID: 2504.13429
- Source URL: https://arxiv.org/abs/2504.13429
- Reference count: 32
- Primary result: NODESAFE reduces FPR95 by up to 28.4% (without OOD exposure) and 22.7% (with OOD exposure) compared to GNNSAFE

## Executive Summary
This paper tackles the problem of detecting out-of-distribution (OOD) data at the node level in graph neural networks (GNNs), where existing methods suffer from extreme score variance during negative energy score aggregation. The authors propose NODESAFE, which introduces two optimization terms to address this issue: one that bounds the variance of logit 2-norms (Lbound) to constrain negative energy scores, and another that mitigates logit shift by reducing the variance of logit sums (Luniform). Experiments across multiple real-world datasets (Cora, Citeseer, PubMed, Twitch, and ogbn-Arxiv) show significant improvements in OOD detection performance.

## Method Summary
NODESAFE combines a GCN backbone with a specialized loss function that includes two variance-reduction terms. Lbound minimizes the variance of logit 2-norms across samples, bounding negative energy scores between log(C) ± Mnorm/√C. Luniform reduces the variance of logit sums separately for in-distribution (ID) and OOD samples, mitigating the effect of arbitrary logit shifts that preserve softmax but alter energy scores. The combined loss L_ALL = L_OOD + λ_2 · L_UB is added after the model learns basic classification. Energy scores are aggregated across graph edges using a label-propagation-style mechanism from GNNSAFE, but with reduced variance due to the constraints.

## Key Results
- NODESAFE reduces FPR95 by up to 28.4% (without OOD exposure) and 22.7% (with OOD exposure) compared to GNNSAFE
- Significant improvements across five datasets: Cora, Citeseer, PubMed, Twitch, and ogbn-Arxiv
- Faster convergence than GNNSAFE, with negligible additional training cost
- Ablation shows Lbound alone contributes substantially while Luniform alone shows minimal improvement

## Why This Works (Mechanism)

### Mechanism 1: Bounding Negative Energy via Logit Norm Constraint (Lbound)
Constraining the 2-norm of logits bounds negative energy scores, preventing extreme values that corrupt score propagation. Lbound minimizes the variance of logit 2-norms across samples. When all logits lie on a hypersphere of radius Mnorm, negative energy scores are bounded between log(C) ± Mnorm/√C. This prevents unbounded amplification during neighbor score aggregation.

### Mechanism 2: Mitigating Logit Shift via Sum Variance (Luniform)
Reducing variance of logit sums stabilizes energy scores against arbitrary constant shifts that preserve softmax but alter energy. Since adding constant s to all logits leaves softmax unchanged, supervised training permits logit shift. Luniform penalizes variance of logit sums, pushing all samples toward a consistent mean. This reduces score variance within ID/OOD groups separately.

### Mechanism 3: Variance-Reduced Score Aggregation
Bounded and uniform energy scores reduce outlier contamination during label-propagation-style aggregation across graph edges. Extreme neighbor scores propagate outward. NODESAFE's constraints reduce variance, preventing a few extreme nodes from dominating their neighborhoods.

## Foundational Learning

- **Energy-based models and negative energy scores**: Why needed here: NODESAFE uses negative energy -log(Σ exp(z_c)) as OOD scoring function; understanding why lower energy = higher confidence is essential. Quick check: Given logits [2.0, 1.0, 0.5] for 3 classes, compute the negative energy score.

- **Message passing in GNNs**: Why needed here: Score aggregation follows neighbor averaging (D^(-1)AE); must understand how information propagates across graph structure. Quick check: For a node with degree 3 and neighbors with scores [0.8, 0.6, 0.4], what is the aggregated neighbor contribution with η=0.5?

- **Softmax invariance and logit shift**: Why needed here: Core insight is that supervised loss permits arbitrary logit shifts; understanding this degeneracy motivates Luniform. Quick check: If logits are [3, 2, 1] and you add 5 to each, does the softmax output change? Does the negative energy change?

## Architecture Onboarding

- **Component map**: Input Graph → GNN Encoder (GCN/GAT/etc.) → Logits z_v → Standard Classification (Cross-Entropy) → LUB Loss → Combined Loss: L_ALL = L_OOD + λ_2 · L_UB → Inference: Logits → Negative Energy → Score Aggregation (k hops) → OOD Score

- **Critical path**: Lbound computation (logit 2-norm variance) → Luniform computation (logit sum variance) → Combined with base loss → Backprop. Mnorm and Msum are scaling coefficients (no gradients).

- **Design tradeoffs**:
  - λ_1 (Luniform vs Lbound balance): Paper finds 10^(-3) optimal; Lbound contributes more
  - λ_2 (LUB vs L_OOD scale): Paper finds 1.0 optimal (range 10^(-1) to 10^1 works)
  - When to add LUB: After model learns reasonable classification; early addition harms both tasks

- **Failure signatures**:
  - ID accuracy drops significantly: Lbound may be over-constraining logits needed for class separation
  - OOD detection worse than baseline: Check if Luniform dominating (λ_1 too high) or LUB added too early
  - Slower convergence than reported: Verify hyperparameters; paper shows NODESAFE converges faster than GNNSAFE

- **First 3 experiments**:
  1. **Reproduce Cora Structure Manipulation baseline**: Train GNNSAFE (without LUB), measure FPR95. Then add LUB with default λ_1=0.001, λ_2=1. Target: ~25-26 FPR95.
  2. **Ablate Lbound vs Luniform**: Run NODESAFE w/o Luniform and w/o Lbound on Cora. Expect Lbound removal to hurt more.
  3. **Hyperparameter sweep on Twitch**: Vary λ_1 ∈ {0.0001, 0.001, 0.01, 0.1} and λ_2 ∈ {0.1, 1, 10}. Plot FPR95 surface. Expect best near λ_1=0.001, λ_2=1.

## Open Questions the Paper Calls Out

### Open Question 1
How can the NODESAFE framework be adapted to achieve significant performance improvements for temporal distribution shifts, similar to its success with structure manipulation? The authors note that performance improvement for OOD data based on temporal splits is "more limited" compared to other methods and identify this as a target for future research.

### Open Question 2
Why does the Luniform optimization term fail to significantly enhance OOD detection when applied in isolation? The paper observes that direct use of Luniform does not significantly improve OOD detection and speculates this may be due to the lack of upper and lower bound constraints.

### Open Question 3
What is the optimal epoch or training interval to introduce the LUB regularization term to maximize OOD detection without degrading classification accuracy? The paper discusses that LUB works better when the model is good at the supervised classification task and that adding it too early affects performance, but does not formalize a strategy.

## Limitations
- Performance on temporal distribution shifts is more limited compared to structure manipulation
- Theoretical bounds assume uniform distribution of logits on hypersphere which may not hold in practice
- Generalization to heterophilic graphs beyond tested datasets remains unproven

## Confidence

**High Confidence**: The core mechanism of variance reduction in negative energy scores improving OOD detection is well-supported by ablation studies and theoretical analysis. Experimental results showing FPR95 reductions of 22.7-28.4% are statistically significant and reproducible.

**Medium Confidence**: The claim that Lbound addresses the root cause while Luniform provides complementary benefits is supported but not definitively proven. The ablation showing Luniform alone is weak doesn't conclusively demonstrate that logit shift is the primary mechanism.

**Low Confidence**: The generalization claims to other graph types are based on limited evidence. The theoretical analysis assumes specific distributions that may not reflect real-world graph data. The comparison with energy-based methods outside the GNNSAFE family is absent.

## Next Checks

1. **Logit Shift Isolation Experiment**: Design an experiment that isolates the effect of logit shift by training models with identical classification performance but different shift amounts, then measure OOD detection performance to validate whether shift directly causes variance.

2. **Heterophilic Graph Testing**: Apply NODESAFE to additional heterophilic datasets (e.g., Chameleon, Squirrel from WebKB) to verify the claim of robust performance across graph types, measuring both accuracy and OOD detection metrics.

3. **Theoretical Validation**: Empirically test Proposition 3.3's bounds by sampling logits from various distributions (uniform, normal, skewed) on hyperspheres and measuring actual vs. predicted negative energy score ranges during training.