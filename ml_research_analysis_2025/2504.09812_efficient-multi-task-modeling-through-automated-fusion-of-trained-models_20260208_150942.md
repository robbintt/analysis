---
ver: rpa2
title: Efficient Multi-Task Modeling through Automated Fusion of Trained Models
arxiv_id: '2504.09812'
source_url: https://arxiv.org/abs/2504.09812
tags:
- task
- trained
- tasks
- components
- multi-task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes EMM, a method for efficient multi-task modeling
  by automatically fusing trained single-task models. The core idea is to decompose
  various trained models into hierarchical components and integrate them using an
  Adaptive Knowledge Fusion (AKF) module based on the Transformer architecture.
---

# Efficient Multi-Task Modeling through Automated Fusion of Trained Models

## Quick Facts
- arXiv ID: 2504.09812
- Source URL: https://arxiv.org/abs/2504.09812
- Reference count: 36
- Key outcome: Automated fusion of heterogeneous trained models into one multi-task model, mitigating negative transfer and scaling to more tasks with improved AUC scores.

## Executive Summary
This paper proposes EMM, a method for efficient multi-task modeling by automatically fusing trained single-task models. The core idea is to decompose various trained models into hierarchical components and integrate them using an Adaptive Knowledge Fusion (AKF) module based on the Transformer architecture. The AKF module uses Mixture-of-Experts for intra-task fusion and a self-attention-based Multi-Task Merge (MTM) for inter-task fusion, addressing the heterogeneity of different models. Experiments on three datasets show that EMM significantly outperforms traditional multi-task learning methods, achieving better AUC scores across multiple tasks. Notably, EMM effectively mitigates negative transfer and handles increasing task numbers with consistent performance improvements, validating its efficiency and adaptability.

## Method Summary
EMM fuses pre-trained single-task models into a multi-task architecture by first decomposing them into hierarchical components at shared structural layers. These components are then integrated using an Adaptive Knowledge Fusion (AKF) module. The AKF performs intra-task fusion via Mixture-of-Experts gating to select the best components per task, and inter-task fusion via a Multi-Task Merge (MTM) self-attention block that selectively merges the two most relevant tasks to mitigate negative transfer. The method trains only the fusion modules while keeping the original model components frozen, reducing computational cost.

## Key Results
- EMM achieves higher AUC scores across multiple tasks compared to traditional multi-task learning methods like MMoE, PLE, and AITM.
- The method effectively mitigates negative transfer, a common issue in multi-task learning.
- EMM's performance improves consistently as the number of tasks increases, validating its scalability.

## Why This Works (Mechanism)

### Mechanism 1: Structural Decomposition for Heterogeneity Alignment
- **Claim:** Decomposing trained models into hierarchical components based on shared structural layers appears to resolve input/output incompatibilities between heterogeneous architectures.
- **Mechanism:** The algorithm scans a collection of trained models to identify an intersection of identical layer structures (e.g., specific dense or normalization layers). It splits models at these points, creating "model components" that share consistent input/output dimensions at the same hierarchical level, allowing them to be stacked and processed uniformly.
- **Core assumption:** The trained models in the pool must share at least one identical layer structure; otherwise, the intersection is empty and decomposition fails.
- **Evidence anchors:**
  - [section III-A]: "The core idea of this method is to split each trained model into several hierarchical model components... based on these identical layers."
  - [Algorithm 1]: Defines the intersection operation $(l_1 \cap l_2 \cap \ldots \cap l_N)$ to find split points.
  - [corpus]: Adjacent work like *MeTA-LoRA* focuses on parameter-efficient tuning, but EMM's specific mechanism of structural slicing for fusion is distinct and relies heavily on the modularity of standard deep learning layers.
- **Break condition:** If model architectures are entirely distinct (e.g., a pure CNN vs. a pure Transformer with no shared layer types), the method cannot identify alignment anchors.

### Mechanism 2: Intra-Task Gating (MoE) for Knowledge Distillation
- **Claim:** Applying a Mixture-of-Experts (MoE) gating network to components within the same task likely filters noise and amplifies the most useful features from diverse model pools.
- **Mechanism:** For a specific task, the outputs of its multiple decomposed components are weighted by a learned "Task Gating Network" ($G_i(x)$). This produces a weighted sum, effectively selecting the best internal representation for that task before attempting cross-task fusion.
- **Core assumption:** There is variance in quality or feature representation among the trained single-task models, making a weighted sum superior to a simple average.
- **Evidence anchors:**
  - [abstract]: "...AKF module... uses Mixture-of-Experts for intra-task fusion."
  - [section III-B]: Eq. (2) defines the fusion as $H = [G_i(x) \cdot h^*_i]$.
  - [corpus]: *Corpus evidence for this specific internal gating mechanism is weak; adjacent papers focus on high-level multi-task learning (MTL) efficiency rather than the internal component weighting of fused models.*
- **Break condition:** If all trained models for a task are identical or equally poor, the gating network offers no marginal gain over a simple average.

### Mechanism 3: Selective Inter-Task Fusion (MTM) for Negative Transfer Mitigation
- **Claim:** Restricting inter-task fusion to only the most relevant task pair via attention mechanisms seems to prevent performance degradation caused by unrelated task interference (negative transfer).
- **Mechanism:** A "Fusion Gating Network" scores the relevance of all other tasks to the current one. It selects the top-scoring task and uses a self-attention block (Multi-Task Merge, MTM) to fuse the two. This forces sparse, high-utility knowledge transfer rather than dense, noisy mixing.
- **Core assumption:** Task relationships are sparse or directional; merging all tasks simultaneously introduces more noise than signal.
- **Evidence anchors:**
  - [section III-B]: "To mitigate the impact of negative transfer... we propose a Multi-Task Merge (MTM) method... [selecting] the two most relevant tasks."
  - [abstract]: "Notably, EMM effectively mitigates negative transfer."
  - [corpus]: *Upcycling Text-to-Image Diffusion Models* notes resource intensity in multi-task adaptation, supporting the efficiency of selective/sparse fusion strategies.
- **Break condition:** If tasks are highly correlated and mutually beneficial, the restrictive top-1 selection might limit potential performance gains achievable through denser connections.

## Foundational Learning

- **Concept: Mixture-of-Experts (MoE)**
  - **Why needed here:** EMM relies on MoE not for scaling parameters, but for soft-selection of model components. You must understand how softmax-gated weighted sums work to interpret the intra-task fusion.
  - **Quick check question:** How does the gating network $G(x)$ in Eq. (2) ensure differentiable selection of components?

- **Concept: Self-Attention (Query-Key-Value)**
  - **Why needed here:** The Multi-Task Merge (MTM) module uses Q/K/V projections to fuse task features. Understanding attention weights as "relevance scores" is critical for debugging inter-task fusion.
  - **Quick check question:** In Eq. (6), why is the dot product $\langle Q(w), K(w) \rangle$ normalized by $\sqrt{d}$?

- **Concept: Soft Parameter Sharing**
  - **Why needed here:** EMM is classified as a soft parameter sharing method (distinct from hard sharing). Understanding that tasks have their own parameters (components) but communicate via fusion is key to the system architecture.
  - **Quick check question:** How does keeping task-specific components frozen while training the AKF module differ from standard Multi-Task Learning fine-tuning?

## Architecture Onboarding

- **Component map:**
  Model Pool -> Deconstructor (Alg 1) -> Model Components (MC) -> AKF Module (Task Gating -> Fusion Gating -> MTM Block) -> Task Towers

- **Critical path:** The success of the entire system hinges on the **Deconstructor** finding non-empty intersections of layer structures. If the intersection is empty, the pipeline halts immediately.

- **Design tradeoffs:**
  - **Flexibility vs. Uniformity:** EMM accepts heterogeneous models but forces them into a uniform hierarchy, potentially discarding unique structural benefits of complex architectures if they don't align with the intersection set.
  - **Stability vs. Transfer:** The MTM selects only the single best related task. This sacrifices potential positive transfer from multiple tasks for stability against negative transfer.

- **Failure signatures:**
  - **Zero Intersection Error:** Logs show empty set for identical layers (models are too structurally different).
  - **Dominant Gate:** If the Fusion Gating Network always picks the same auxiliary task regardless of input, the "adaptive" nature has collapsed.
  - **Negative Transfer Spikes:** If AUC drops sharply when adding a new task, the Fusion Gating Network may be selecting a conflicting task incorrectly.

- **First 3 experiments:**
  1. **Intersection Validation:** Run Algorithm 1 on your selected model pool. Verify that $MC$ is populated and components have matching tensor shapes.
  2. **Ablation on Gating:** Disable the Fusion Gating Network (force a random or fixed task pair) and compare AUC against the adaptive selection to measure the value of the "relevance filter."
  3. **Scaling Stress Test:** Incrementally increase the number of tasks (as in Fig 3) to verify that the sparse MTM fusion maintains performance where dense baselines fail.

## Open Questions the Paper Calls Out

- **Open Question 1**
  - **Question:** How can the EMM framework be generalized to fuse models that possess completely distinct architectural structures without relying on identical layer intersections?
  - **Basis in paper:** [explicit] Section III-A explicitly states the method relies on a "core assumption that there are identical layer structures in all selected trained models" and acknowledges this imposes limitations.
  - **Why unresolved:** The current Algorithm 1 fails if the intersection of layer structures is empty, restricting the framework to models that share at least some structural similarities.
  - **What evidence would resolve it:** A modified fusion mechanism that utilizes adapters or projection layers to align components from models with zero identical layers (e.g., a CNN and an MLP) without performance degradation.

- **Open Question 2**
  - **Question:** Is the heuristic of selecting exactly two tasks for inter-task fusion (via the MTM module) optimal, or does it create a bottleneck in scenarios with high task density?
  - **Basis in paper:** [inferred] Section III-B describes the MTM method as identifying the "two most relevant tasks" to merge, but provides no justification for why the selection is limited to two rather than a dynamic number.
  - **Why unresolved:** In complex systems with many strongly correlated tasks, restricting the fusion to a single pair might ignore valuable multi-way dependencies (e.g., a sequence of click -> cart -> purchase).
  - **What evidence would resolve it:** An ablation study evaluating EMM performance when the MTM module selects the top $k$ relevant tasks (where $k > 2$) compared to the fixed top-2 approach.

- **Open Question 3**
  - **Question:** Does the constraint of keeping the trained model components frozen limit the multi-task model's ability to correct feature misalignments or negative transfer?
  - **Basis in paper:** [inferred] Section III-B states that during the training process, "the parameters of these model components remain frozen," relying only on the AKF module for adaptation.
  - **Why unresolved:** While freezing parameters reduces training costs, it prevents the foundational features from adjusting to the new multi-task objectives, which might explain why some baseline multi-task methods underperformed compared to single-task models in the results.
  - **What evidence would resolve it:** A comparative analysis of the proposed frozen-component EMM against a variant that allows for fine-tuning of the decomposed components during the fusion process.

## Limitations
- The method is restricted to model pools with shared structural layers; it fails if no intersection exists.
- Freezing the original model components may prevent the system from correcting feature misalignments or negative transfer.
- The selective top-1 task fusion in MTM may limit positive transfer in highly correlated task scenarios.

## Confidence
- **High confidence:** The mechanism of decomposing models at shared layers to enable fusion is well-specified and logically sound.
- **Medium confidence:** The adaptive gating and MTM-based inter-task fusion will consistently mitigate negative transfer and scale to many tasks; the evidence is primarily empirical from three datasets.
- **Low confidence:** The method will generalize to entirely different architectures (e.g., CNNs, Transformers) or domains beyond tabular data, given the strong dependence on shared structural layers.

## Next Checks
1. **Intersection Validation:** Execute Algorithm 1 on your chosen model pool. Verify that the set of model components is non-empty and that each component at a given level has matching input/output dimensions.
2. **Gating Ablation:** Train EMM with and without the adaptive Fusion Gating Network (fix the task pair or select randomly). Compare per-task AUC to quantify the benefit of relevance-based selection.
3. **Scaling Stress Test:** Incrementally add tasks and monitor AUC trends, especially when approaching or exceeding the number of tasks tested in the original experiments, to confirm consistent performance improvement.