---
ver: rpa2
title: Hessian Geometry of Latent Space in Generative Models
arxiv_id: '2506.10632'
source_url: https://arxiv.org/abs/2506.10632
tags:
- space
- latent
- dlog
- distribution
- generative
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a method to analyze the latent space geometry
  of generative models by reconstructing the Fisher information metric. The approach
  estimates the posterior distribution of latent variables given generated samples
  and uses this to learn the log-partition function, which defines the Fisher metric
  for exponential families.
---

# Hessian Geometry of Latent Space in Generative Models

## Quick Facts
- **arXiv ID:** 2506.10632
- **Source URL:** https://arxiv.org/abs/2506.10632
- **Reference count:** 40
- **One-line primary result:** Introduces a method to reconstruct Fisher information metrics in generative model latent spaces, revealing fractal phase transition structures in diffusion models.

## Executive Summary
This paper presents a novel approach to analyze the intrinsic geometry of latent spaces in generative models by reconstructing the Fisher information metric through posterior concentration and log-partition function learning. The method is theoretically grounded in exponential family statistics and validated on exactly solvable statistical physics models (Ising model and TASEP), where it accurately recovers thermodynamic quantities. When applied to diffusion models, it reveals a previously unknown fractal structure of phase transitions in the latent space, characterized by abrupt changes in the Fisher metric and divergent Lipschitz constants at phase boundaries.

## Method Summary
The approach estimates the posterior distribution of latent parameters given generated samples, then learns the log-partition function whose Hessian defines the Fisher metric. For physics models, exact samples are generated and the posterior is computed directly. For diffusion models, CLIP embeddings approximate posterior distances in semantic space. A 5-layer MLP learns the log-partition function via Jensen-Shannon divergence loss against the empirical posterior. The Fisher metric is extracted as the Hessian of the trained model, enabling geodesic computation and phase boundary detection.

## Key Results
- Outperforms baseline methods (Mean-as-Stat, PCA-VAE) in reconstructing thermodynamic quantities for Ising and TASEP models
- Reveals fractal phase transition boundaries in diffusion model latent space at scales down to 10⁻⁸
- Shows that while geodesic interpolations are approximately linear within individual phases, they exhibit sharp curvature at phase boundaries
- Demonstrates divergent Lipschitz constant in diffusion models at phase transitions, linked to Lyapunov exponent divergence

## Why This Works (Mechanism)

### Mechanism 1: Posterior Concentration Maps to Bregman Divergence
The posterior distribution over latent parameters, when raised to power 1/N, converges to the exponential of negative Bregman divergence from the true parameter. This follows from the law of large numbers applied to sufficient statistics combined with Bayes' theorem, where the Bregman divergence equals KL divergence for exponential families.

### Mechanism 2: Log-Partition Function Recovery via Normalized Distribution Matching
Minimizing Jensen-Shannon divergence between empirical posterior and the normalized exponential distribution derived from log Z recovers the true Fisher metric. The normalized distribution depends only on log Z and its gradient, forcing the Hessian ∇² log Z_θ to match the true Fisher metric during training.

### Mechanism 3: Phase Transitions Appear as Metric Singularities and Fractal Boundaries
Diffusion model latent space contains distinct phases separated by fractal boundaries where the Fisher metric is non-analytic and the Lipschitz constant diverges. Small latent changes cause large output shifts at boundaries, resulting in divergent Lyapunov exponents as mode variance approaches zero in bimodal targets.

## Foundational Learning

- **Fisher Information Metric**: Defines intrinsic geometry of probability distributions; the paper reconstructs this metric as ∇² log Z(t) to study latent space structure. Quick check: Given a parametric family p(x|θ), what does the Fisher metric g_ij(θ) = E[∂log p/∂θᵢ · ∂log p/∂θⱼ] measure about nearby distributions?

- **Bregman Divergence**: Connects posterior concentration to the log-partition function; enables training log Z via distribution matching. Quick check: For convex function φ, what is the Bregman divergence D_φ(x, y) and why does it equal KL divergence for exponential families?

- **Exponential Family and Sufficient Statistics**: The theoretical guarantees require exponential family structure; diffusion models are approximated via 2D Hessianizability. Quick check: In p(x|t) = exp(⟨t, f(x)⟩ - log Z(t)), what role does f(x) play and why does ∇ log Z(t) = E[f(x)]?

## Architecture Onboarding

- **Component map**: Posterior estimator (U²-Net/CLIP) -> Log-partition network (5-layer MLP) -> Fisher metric extraction (Hessian) -> Geodesic solver (Adam optimization)
- **Critical path**: (1) Generate samples p(x|t) across parameter grid -> (2) Compute posterior approximation -> (3) Train log Z_θ via JSD loss -> (4) Extract Fisher metric via Hessian -> (5) Compute geodesics and detect phase boundaries via metric discontinuities
- **Design tradeoffs**: CLIP vs. U²-Net posterior captures semantics but ignores pixel-level determinism; deterministic (η=0) vs. stochastic (η>0) sampling reveals sharp vs. smoothed boundaries; MSE vs. JSD loss has theoretical guarantees but vanishing gradients vs. practical utility
- **Failure signatures**: Smooth log Z where fractal boundaries expected (check η > 0 or insufficient posterior resolution); vanishing gradients early in training (switch from MSE to normalized JSD loss); curved geodesics within single phase (increase N or improve feature extractor)
- **First 3 experiments**: 1) Replicate Table 1 results for Ising/TASEP RMSE validation; 2) Generate 60K images from fixed triplet z₀, z₁, z₂ with η=0 and visualize fractal boundaries at 10⁻⁵ scale; 3) Compute and compare geodesic vs. linear interpolation across phase boundary measuring curvature and path lengths

## Open Questions the Paper Calls Out

### Open Question 1
Does the fractal structure of phase boundaries persist in the full high-dimensional latent space, or is it an artifact of analyzing 2D sections? The guarantee that a Riemannian metric admits a Hessian representation is local for 2D analytic manifolds but not guaranteed for higher dimensions.

### Open Question 2
Is the divergent Lipschitz constant at phase boundaries strictly caused by the mapping from a unimodal latent distribution to disjoint data manifolds? The paper supports this with a 1D proposition, but a formal proof for high-dimensional diffusion models is lacking.

### Open Question 3
To what extent does the choice of feature extractor determine the observed phase structure versus revealing intrinsic properties of the diffusion model? Different extractors (U²-Net vs. CLIP) produce contradictory geometries, suggesting the feature extractor acts as an approximate sufficient statistic.

## Limitations
- Theoretical guarantees rely on exponential family structure, which diffusion models only approximate through 2D Hessianizability
- Fractal phase transition claims rest on simplified bimodal Gaussian targets rather than full diffusion model complexity
- CLIP-based posterior approximation may miss fine-grained metric structure at small scales where fractal features emerge

## Confidence
- **High Confidence**: Theoretical framework for exponential families (Theorems 3.1, 3.2) and physics model validation (Table 1 RMSE results)
- **Medium Confidence**: Fisher metric reconstruction methodology and its application to 2D diffusion slices
- **Low Confidence**: Fractal phase transition claims and their implications for Lipschitz divergence in high-dimensional diffusion models

## Next Checks
1. Validate metric reconstruction on controlled toy models by testing the pipeline on a simple 2D exponential family with known Fisher metric
2. Perform sensitivity analysis of phase boundaries by systematically varying CLIP embedding distance threshold and posterior sample size N
3. Compare deterministic vs. stochastic sampling quantitatively by measuring Fisher metric smoothness and geodesic curvature variance to establish the scale at which stochasticity obscures phase transitions