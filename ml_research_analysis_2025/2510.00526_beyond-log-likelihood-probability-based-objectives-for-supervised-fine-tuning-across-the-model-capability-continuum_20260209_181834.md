---
ver: rpa2
title: 'Beyond Log Likelihood: Probability-Based Objectives for Supervised Fine-Tuning
  across the Model Capability Continuum'
arxiv_id: '2510.00526'
source_url: https://arxiv.org/abs/2510.00526
tags:
- objectives
- tokens
- objective
- training
- preprint
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper challenges the standard use of negative log-likelihood
  (NLL) as the training objective in supervised fine-tuning (SFT) of large language
  models. The authors argue that NLL, optimal when training from scratch, can be suboptimal
  in post-training settings where models already encode priors and supervision can
  be noisy.
---

# Beyond Log Likelihood: Probability-Based Objectives for Supervised Fine-Tuning across the Model Capability Continuum

## Quick Facts
- arXiv ID: 2510.00526
- Source URL: https://arxiv.org/abs/2510.00526
- Reference count: 40
- Primary result: Prior-leaning objectives outperform NLL in model-strong domains; NLL dominates in model-weak domains

## Executive Summary
This paper challenges the standard use of negative log-likelihood (NLL) as the training objective in supervised fine-tuning (SFT) of large language models. The authors argue that NLL, optimal when training from scratch, can be suboptimal in post-training settings where models already encode priors and supervision can be noisy. They propose a general family of probability-based objectives and identify a key factor governing their effectiveness: the model-capability continuum. This continuum reflects the strength of priors inherited from pretraining, with strong priors in domains like math and weak priors in tasks like novel puzzles. Experiments across 7 model backbones, 14 benchmarks, and 3 domains show that prior-leaning objectives (e.g., -p, -p^10) consistently outperform NLL in model-strong settings, while NLL dominates in model-weak settings. In intermediate regions, no single objective prevails. Theoretical analysis provides a principled foundation for adapting objectives to model capability, highlighting the trade-offs between exploiting model priors and learning from unlikely predictions.

## Method Summary
The paper proposes a general family of probability-based objectives for SFT, parameterized by α in f_α(p). The standard NLL corresponds to α → 0, while prior-leaning objectives like -p use α = 1. The core innovation is diagnosing the "model-capability continuum" before training by computing the mean predicted probability of training tokens under the base model. This diagnostic determines whether to use NLL (Model-Weak/low prior) or prior-leaning objectives (Model-Strong/high prior). The framework is implemented in the verl library with AdamW optimizer, cosine decay learning rate schedules, and standard SFT procedures across multiple model families and datasets.

## Key Results
- Prior-leaning objectives (-p, -p^10) consistently outperform NLL in model-strong domains (Math)
- NLL dominates in model-weak domains (Puzzles) while prior-leaning objectives fail to learn
- No single objective prevails in intermediate regions, suggesting dynamic adaptation potential
- The mean predicted probability proxy successfully predicts optimal objective selection across tested domains

## Why This Works (Mechanism)

### Mechanism 1: Gradient Mass Redistribution via Objective Convexity
Switching from NLL to prior-leaning objectives appears to improve performance on pre-trained domains by shifting gradient mass from low-probability tokens to high-probability tokens. The gradient contribution of a token is weighted by W_f(p) = -f'(p)p(1-p). NLL (-log p) is convex and maximizes W_f at low probabilities (p → 0), forcing the model to correct "surprising" tokens. Concave objectives (-p) maximize W_f in [0.5, 1.0], reinforcing tokens the model already predicts well. In "Model-Strong" domains, low-probability tokens often constitute noise or supervision errors; downweighting them prevents the degradation of valid priors.

### Mechanism 2: The Model-Capability Continuum Proxy
The effectiveness of the training objective is conditional on the strength of the model's pre-existing priors, which can be quantitatively estimated before training. The paper defines a continuum from Model-Strong (high prior) to Model-Weak (low prior). The authors use the mean predicted probability of the training set tokens under the base model as a proxy. High mean probability (~0.8) indicates Model-Strong (Math), allowing prior-leaning objectives to succeed. Low mean probability (~0.01) indicates Model-Weak (Puzzles), requiring NLL to force learning.

### Mechanism 3: Risk Reduction Dynamics Reversal
Theoretical analysis suggests that the rate of population risk reduction for different objectives reverses depending on the alignment between model predictions and ground truth. Theorem 1 analyzes gradient flow, postulating that in the Model-Strong end, prior-leaning objectives achieve a larger initial reduction in risk compared to NLL. Conversely, in the Model-Weak end, NLL provides superior risk reduction. This provides a theoretical basis for the continuum hypothesis.

## Foundational Learning

- **Concept: Negative Log-Likelihood (NLL) / Cross-Entropy**
  - Why needed here: This is the standard baseline the paper argues is conditionally suboptimal. You must understand that NLL heavily penalizes low-probability predictions (-log(p) → ∞ as p → 0), driving the model to correct "surprises."
  - Quick check question: Does NLL apply more gradient weight to tokens the model predicts with low probability or high probability? (Answer: Low probability)

- **Concept: Gradient Weighting Function (W_f(p))**
  - Why needed here: The core mathematical lever of the paper. It explains how the proposed objectives change learning dynamics. It is the function that maps a token's probability to its gradient magnitude.
  - Quick check question: For a prior-leaning objective like -p, where is the gradient mass concentrated compared to NLL? (Answer: Shifted toward higher probability tokens)

- **Concept: Calibration in LLMs**
  - Why needed here: The paper's diagnostic for the "Model-Capability Continuum" relies on the model's predicted probabilities being meaningful (calibrated).
  - Quick check question: If a model is "confidently wrong" (miscalibrated), would the mean predicted probability still be a reliable proxy for selecting an objective? (Answer: No)

## Architecture Onboarding

- **Component map:** Base Model -> Diagnostic Module -> Loss Adapter -> Trainer
- **Critical path:**
  1. Run inference on training data with frozen base model to get probabilities
  2. Calculate Eq. 5 (Mean Likelihood). If >0.7 (Model-Strong), flag for prior-leaning objective. If <0.2 (Model-Weak), flag for NLL
  3. Configure Loss function f(p) accordingly
  4. Train

- **Design tradeoffs:**
  - Robustness vs. Efficiency: Prior-leaning objectives (-p) converge faster on strong domains by ignoring noise, but fail catastrophically on novel domains. NLL is robust but inefficient/noisy when priors are already strong
  - Implementation: Implementing the parametric family f_α(p) requires a custom loss kernel; standard libraries usually hardcode NLL

- **Failure signatures:**
  - Collapse on Novel Data: Using a prior-leaning objective on "Model-Weak" data results in near-zero gradients (because p is low everywhere), causing the model to fail to learn
  - Overfitting to Noise: Using NLL on "Model-Strong" data forces the model to memorize idiosyncrasies of long CoT supervision, degrading generalization

- **First 3 experiments:**
  1. Diagnostic Validation: Take a base model (e.g., Llama-3-8B) and a dataset (e.g., Math). Calculate the mean predicted probability (Eq. 5) before training. Confirm it is high (~0.7+)
  2. Ablation on Continuum: Train two instances on the Math set: one with NLL (-log p) and one with -p. Compare accuracy on benchmarks (e.g., MATH-500). Expect -p to win
  3. Reversal Check: Train two instances on a synthetic puzzle task (Model-Weak). Compare NLL vs -p. Expect NLL to win and -p to fail/flatline

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can dynamic or curriculum-style objective adaptation—where the objective function evolves alongside model improvement—outperform static objective selection?
- **Basis in paper:** Section 6 identifies "dynamic or curriculum-style adaptation, where the objective evolves with model improvement during training" as a promising avenue
- **Why unresolved:** The study focuses on comparing static objective functions (-log p vs. -p) fixed prior to training
- **What evidence would resolve it:** Experiments showing a training schedule that adjusts α (e.g., shifting from prior-averse to prior-leaning) yields better generalization than fixed objectives

### Open Question 2
- **Question:** Do the dynamics of the model-capability continuum hold for models with 30B+ parameters, or do larger base models exhibit different saturation behaviors?
- **Basis in paper:** Appendix D states the authors "did not extend our experiments to excessively large models (e.g., 30B–70B parameters) due to computational resource constraints"
- **Why unresolved:** It is unclear if the trade-offs between NLL and prior-leaning objectives scale linearly with model size or if larger models require different handling
- **What evidence would resolve it:** Replication of the main results (Table 2 and 4) on models ranging from 30B to 70B parameters

### Open Question 3
- **Question:** Can a more principled, fine-grained measure of "model capability" be designed to better predict the optimal objective function prior to training?
- **Basis in paper:** Appendix D notes the current metric (mean predicted probability) is a "first attempt" and suggests "future work may design more principled or fine-grained measures"
- **Why unresolved:** The current proxy may not fully capture the complexity of priors across diverse domains, leading to suboptimal objective selection
- **What evidence would resolve it:** A new capability metric that correlates more strongly with the empirically optimal α value than the current likelihood estimation

## Limitations

- The framework assumes base models are well-calibrated, but systematic evaluation of calibration quality across diverse model families remains incomplete
- The experimental validation uses a limited set of representative domains, with the intermediate region under-characterized
- The theoretical analysis depends on simplifying assumptions that may not hold in realistic fine-tuning scenarios

## Confidence

**High Confidence:** The empirical demonstration that prior-leaning objectives outperform NLL in model-strong domains (Math) and that NLL dominates in model-weak domains (Puzzles). The gradient weighting mechanism (W_f(p)) provides a mathematically sound explanation.

**Medium Confidence:** The theoretical characterization of risk reduction dynamics across the capability continuum. While Theorem 1 provides a principled foundation, the proof relies on assumptions that simplify real-world conditions.

**Low Confidence:** The claim that the proposed objectives represent a universal improvement over NLL. The paper acknowledges that no single objective dominates in intermediate regions.

## Next Checks

1. **Calibration Robustness Testing:** Systematically evaluate the mean probability proxy across diverse model families (including smaller models, specialized domains, and models with known calibration issues) to determine conditions under which the continuum framework fails.

2. **Intermediate Region Characterization:** Conduct comprehensive experiments across a wider spectrum of task difficulties to map the full capability continuum more precisely, including tasks with gradual shifts in model prior strength.

3. **Cross-Domain Generalization Analysis:** Test the framework's robustness when fine-tuning on data that combines model-strong and model-weak characteristics, including mixed-domain datasets and curriculum learning scenarios.