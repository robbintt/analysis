---
ver: rpa2
title: 'PARSE: LLM Driven Schema Optimization for Reliable Entity Extraction'
arxiv_id: '2510.08623'
source_url: https://arxiv.org/abs/2510.08623
tags:
- schema
- extraction
- schemas
- type
- json
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: PARSE addresses the challenge of reliable structured information
  extraction from unstructured text for LLM agent systems by optimizing JSON schemas
  for LLM consumption and implementing reflection-based guardrails. The system features
  ARCHITECT, which iteratively refines schemas through synthetic test data generation
  and automated code generation for backward compatibility, and SCOPE, which performs
  systematic validation with missing attribute checks, grounding verification, and
  rule compliance.
---

# PARSE: LLM Driven Schema Optimization for Reliable Entity Extraction

## Quick Facts
- arXiv ID: 2510.08623
- Source URL: https://arxiv.org/abs/2510.08623
- Reference count: 40
- Primary result: Up to 64.7% accuracy improvement on SWDE dataset with 92% reduction in extraction errors within first retry

## Executive Summary
PARSE addresses the challenge of reliable structured information extraction from unstructured text for LLM agent systems by optimizing JSON schemas for LLM consumption and implementing reflection-based guardrails. The system features ARCHITECT, which iteratively refines schemas through synthetic test data generation and automated code generation for backward compatibility, and SCOPE, which performs systematic validation with missing attribute checks, grounding verification, and rule compliance. Evaluation on three datasets (Retail-Conv, SGD, SWDE) demonstrates significant accuracy improvements while maintaining practical latency.

## Method Summary
PARSE operates in two phases: (1) BUILD PHASE - ARCHITECT iteratively refines input JSON schemas using synthetic adversarial test data generation, LLM-based extraction evaluation, and schema modification until accuracy thresholds are met; (2) EXTRACT PHASE - SCOPE performs three-stage validation (missing attribute check → grounding verification → rule compliance) with structured reflection-based retries. RELAY automatically generates Python transformation code to maintain backward compatibility with original schema formats. The system was evaluated on three datasets using multiple LLM models including Claude 3.5/3.7 Sonnet, Claude 3.5 Haiku, Llama 4-Maverick, and DeepSeek-R1-671B.

## Key Results
- Up to 64.7% accuracy improvement on SWDE dataset compared to baseline
- 92% reduction in extraction errors within first retry using SCOPE guardrails
- Combined framework improvements reaching 10% across models while maintaining practical latency
- Schema optimization generalizes across different LLM models (model-agnostic improvements)

## Why This Works (Mechanism)

### Mechanism 1: Iterative Schema Optimization for LLM Comprehension
Treating JSON schemas as optimizable natural language contracts rather than static human-centric artifacts improves LLM extraction accuracy. ARCHITECT iteratively refines schemas through entity description enhancement (34% of changes), structural reorganization including flattening nested structures (55%), and validation rule/pattern addition (~3%). Each iteration generates synthetic adversarial test data, evaluates extraction performance, and uses failure analysis to guide schema modifications. Core assumption: LLMs process schema descriptions as natural language instructions; clearer, more explicit constraints reduce interpretation ambiguity and hallucination.

### Mechanism 2: Reflection-Based Guardrails with Structured Error Feedback
Multi-stage validation combined with structured reflection enables systematic self-correction, reducing errors by 92% within the first retry. SCOPE implements three sequential guardrails: Missing Attribute Check verifies required fields exist, Grounding Verification confirms extracted values appear in source text, and Rule Compliance validates format constraints. On failure, structured reflections—not generic retries—guide the LLM to specific corrections. Core assumption: LLMs can effectively use structured error feedback to correct specific extraction mistakes when provided with precise failure information.

### Mechanism 3: Backward Compatibility via Automated Transformation Code Generation
RELAY maintains production system compatibility while allowing aggressive schema optimization by automatically generating and validating Python transformation code. When ARCHITECT creates an optimized schema S*, RELAY simultaneously generates Python functions mapping O(S*) → O(S_user). It validates transformations through sample data pair testing and iteratively refines code until semantic preservation is verified. Core assumption: Schema structural changes can be reversibly mapped; the transformation logic is simpler than the extraction task itself.

## Foundational Learning

- **Concept: JSON Schema Specification (draft-07 or later)**
  - Why needed here: ARCHITECT modifies schema structure, adds pattern constraints, enums, and validation rules. Understanding `required`, `pattern`, `minLength`, `properties`, and nested object syntax is essential for debugging schema iterations.
  - Quick check question: Given `{"type": "object", "properties": {"price": {"type": "string", "pattern": "^[$][0-9]+"}}}`, what inputs validate? What fails?

- **Concept: Constrained Decoding vs. Post-hoc Validation**
  - Why needed here: SCOPE uses post-hoc validation with reflection rather than grammar-guided generation. Understanding the tradeoff—constrained decoding guarantees syntax but may sacrifice quality—clarifies why PARSE chose reflection-based correction.
  - Quick check question: Why might constrained decoding produce valid JSON but incorrect extraction values?

- **Concept: Synthetic Data Generation for Adversarial Testing**
  - Why needed here: ARCHITECT's optimization loop depends on synthetic test data quality. Understanding how to generate challenging edge cases (contextual ambiguity, structural challenges, semantic traps) directly affects schema refinement effectiveness.
  - Quick check question: If synthetic data only covers happy-path scenarios, what failure mode will ARCHITECT miss?

## Architecture Onboarding

- **Component map:**
  ```
  BUILD PHASE (one-time):
  Original Schema → Schema Generator → Initial Schema S₀
                       ↓
  Synthetic Data Generator ← (uses D_seed, task description)
                       ↓
  Extraction Evaluator → Error Analysis
                       ↓
  Schema Refinement Agent → S_{i+1}
       ↑_______________| (iterates until τ or K)
                       ↓
  RELAY Code Generator → transformation.py

  EXTRACT PHASE (runtime):
  Input Text + Optimized Schema S* → SCOPE Extractor → Raw Output
                                            ↓
                                    Missing Attr Check → Fail? → Reflection
                                            ↓ Pass
                                    Grounding Verification → Fail? → Reflection
                                            ↓ Pass
                                    Rule Compliance → Fail? → Reflection
                                            ↓ Pass
                                    RELAY Transform → Final Output (original schema format)
  ```

- **Critical path:** The extraction latency (Table 3) shows SCOPE adds ~10-16s on Claude models. The critical path is the reflection loop—if grounding or rule checks fail repeatedly, latency compounds. ARCHITECT-optimized schemas reduce retries, cutting this path.

- **Design tradeoffs:**
  - Latency vs. Accuracy: SCOPE increases latency 2-3x but achieves +64.7% on SWDE. ARCHITECT schemas reduce this penalty by ~4s average.
  - Schema Complexity vs. LLM Comprehension: Flattening nested structures improves extraction but may complicate downstream consumer logic—RELAY must compensate.
  - Synthetic Data Quality vs. Optimization Generalization: Overfitting to synthetic examples risks (Section 5.4 notes performance dips after 5-6 iterations).

- **Failure signatures:**
  - High retry count with no convergence: Likely schema ambiguity—check if descriptions conflict or patterns are underspecified.
  - Grounding failures on valid extractions: May indicate over-aggressive grounding check or multi-turn context not being passed correctly.
  - RELAY transformation errors: Check if optimized schema split/merged fields in ways that lose information (e.g., concatenation without delimiter).

- **First 3 experiments:**
  1. **Baseline extraction comparison:** Run baseline agent vs. SCOPE (without ARCHITECT) on 20 samples from your domain. Measure accuracy and retry distribution. This isolates SCOPE's contribution.
  2. **Schema iteration profiling:** Run ARCHITECT on one schema with verbose logging. Track which modification types (description, structure, pattern) correlate with accuracy jumps. Stop at iteration 5-6 to avoid overfitting.
  3. **Cross-model schema transfer test:** Optimize schema using Model A, extract using Model B. Compare against schema optimized on Model B. If performance gap <2%, schemas are model-agnostic (confirming Table 2 findings).

## Open Questions the Paper Calls Out

- **Can ARCHITECT's iterative schema optimization scale efficiently to schemas with hundreds or thousands of attributes without prohibitive computational costs?**
  - Basis: The iterative refinement process can be computationally expensive for complex schemas with many attributes, creating potential scalability bottlenecks for large-scale deployments.
  - Why unresolved: Evaluation only tested schemas with relatively few attributes; no experiments examined schema complexity scaling.

- **How does PARSE perform when schemas must evolve dynamically in real-time production environments rather than being optimized offline?**
  - Basis: Approach assumes relatively static schema structures that can be optimized offline, which can be challenging where schemas evolve continuously.
  - Why unresolved: ARCHITECT operates as a one-time "Build Phase" process with no mechanism for incremental re-optimization or handling schema version changes in live systems.

- **How robust is PARSE when seed datasets are unavailable or unrepresentative for entirely new domains?**
  - Basis: ARCHITECT's optimization quality depends heavily on the availability and representativeness of seed datasets. For entirely new domains, obtaining sufficient high-quality seed data can be challenging.
  - Why unresolved: All three evaluation datasets had ground-truth annotations available; no ablation tested performance with reduced or absent seed data.

## Limitations

- **Internal Dataset Dependency**: Retail-Conv dataset is proprietary, limiting reproducibility of the full three-dataset evaluation. The reported 92% error reduction relies partially on this internal data.
- **Synthetic Data Generalization**: ARCHITECT's optimization depends heavily on synthetic test data quality. While cross-model schema transfer shows promise, no experiments test performance on truly out-of-distribution inputs.
- **Latency-Accuracy Tradeoff Boundary**: SCOPE adds 10-16s latency on Claude models, but the paper doesn't explore whether simpler validation strategies could achieve similar accuracy gains with lower latency.

## Confidence

- **High Confidence**: The mechanism of iterative schema refinement improving extraction accuracy (supported by quantitative results across multiple models and datasets).
- **Medium Confidence**: The reflection-based guardrail system achieving 92% error reduction within first retry (empirical but lacks ablation studies isolating reflection vs. simple retry effects).
- **Low Confidence**: The RELAY transformation code generation maintaining semantic preservation in all cases (minimal empirical validation shown; corpus support is weak).

## Next Checks

1. **Synthetic Data Robustness Test**: Run ARCHITECT-optimized schemas on a held-out test set containing adversarial examples not present in synthetic training data. Measure accuracy drop to quantify overfitting risk.

2. **Ablation of Reflection Detail**: Compare SCOPE's performance with structured reflections vs. simple "please retry" prompts across 100 samples. This isolates whether reflection detail drives the 92% error reduction.

3. **Latency-Optimized SCOPE Variant**: Implement a minimal validation pipeline (e.g., only missing attribute check + one grounding verification) and compare accuracy-latency tradeoff against full SCOPE on SWDE. Determine if 2-3x latency increase is strictly necessary.