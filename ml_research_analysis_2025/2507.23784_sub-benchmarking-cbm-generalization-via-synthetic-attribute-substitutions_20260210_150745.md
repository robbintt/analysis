---
ver: rpa2
title: 'SUB: Benchmarking CBM Generalization via Synthetic Attribute Substitutions'
arxiv_id: '2507.23784'
source_url: https://arxiv.org/abs/2507.23784
tags:
- attribute
- images
- bird
- concept
- color
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper addresses the problem of evaluating concept-based interpretable\
  \ models (CBMs) by creating a synthetic benchmark where single attributes are substituted\
  \ in bird images. The core method, Tied Diffusion Guidance (TDG), generates these\
  \ attribute substitutions by tying two diffusion processes\u2014one for the reference\
  \ bird class and one for a guidance bird with the target attribute\u2014ensuring\
  \ faithful modifications."
---

# SUB: Benchmarking CBM Generalization via Synthetic Attribute Substitutions

## Quick Facts
- **arXiv ID**: 2507.23784
- **Source URL**: https://arxiv.org/abs/2507.23784
- **Reference count**: 40
- **Primary result**: CBMs achieve only 45.7% accuracy on substituted attributes in SUB benchmark, well below human baseline of 94.0%, revealing concept predictions are not grounded in image content

## Executive Summary
This paper addresses the critical challenge of evaluating whether concept-based interpretable models (CBMs) genuinely detect visual concepts or merely memorize class-concept associations. The authors create SUB, a synthetic benchmark using Tied Diffusion Guidance (TDG) to generate bird images with single-attribute substitutions. When evaluated on SUB, CBMs and vision-language models fail to reliably identify substituted concepts, with the best CBM achieving only 45.7% accuracy on target attributes despite high accuracy on training classes. This demonstrates that current CBMs ground predictions in class identity rather than actual image content, undermining their interpretability claims.

## Method Summary
The authors develop Tied Diffusion Guidance (TDG) to create synthetic images where single attributes are substituted in bird images. TDG couples two diffusion processes—one generating the reference bird class and one generating a guidance bird with the target attribute—using adaptive noise tying. Generated images undergo multi-stage filtering: VQA model filtering retains top 10% by confidence, followed by human validation ensuring attribute presence and reference bird faithfulness. The resulting SUB dataset contains 38,400 synthetic images from 33 bird classes with 45 substituted attributes. CBMs and VLMs are evaluated on their ability to detect substituted concepts (S+ accuracy) and removed original attributes (S- accuracy).

## Key Results
- CBMs achieve only 45.7% accuracy on detecting substituted attributes in SUB, below random chance and far below human baseline of 94.0%
- Best CBM achieves 96.7% accuracy on training classes but fails to generalize to novel attribute combinations
- Concept Embedding Models (CEMs) perform better than standard CBMs but still fail to reliably detect substitutions
- SUB annotations show 98.90% accuracy vs. CUB's 57.50% human agreement, validating synthetic data quality

## Why This Works (Mechanism)

### Mechanism 1: Tied Diffusion Guidance (TDG) for Compositional Control
TDG enables single-attribute substitution by coupling two diffusion processes with adaptive noise tying. Two images are generated from related prompts (reference class R and guidance class G with target attribute S⁺). At each denoising step, noise predictions are compared element-wise; where predictions differ below threshold η, they are averaged; where they differ significantly, the original prediction is retained. An η schedule starts strict (tying enforced) and loosens toward independent generation. The guidance class G provides a stronger semantic anchor for the target attribute than the reference class R, enabling attribute transfer while preserving R's other characteristics.

### Mechanism 2: CBM Concept Prediction Grounding Failure
CBMs predict class-associated concept vectors rather than image-observed concepts, causing generalization failure on novel concept combinations. CBMs trained on class-level concept labels (via majority voting) learn a mapping from visual features to class identity, then output the memorized class concept vector. When presented with an image containing a modified attribute, the model still predicts the original class's concept vector. CBMs exploit spurious correlations between class identity and concept labels rather than learning to detect concepts from visual features.

### Mechanism 3: VQA-Human Filtering Pipeline for Synthetic Data Quality
Multi-stage filtering (VQA + human validation) produces synthetic images with higher label accuracy than real datasets. (1) Generate N images per bird-attribute pairing with TDG; (2) VQA model (InternVL-2.5-8B) filters incorrect attribute predictions, retaining top 10% by confidence; (3) Human annotators validate attribute presence and reference bird faithfulness on a subset; (4) Discard pairings with <90% human agreement. VQA model confidence correlates with actual attribute presence; human validation on subset generalizes to full dataset.

## Foundational Learning

- **Latent Diffusion Models (LDMs)**: Why needed here: TDG operates on the denoising process of LDMs; understanding classifier-free guidance (Eq. 2) is prerequisite for grasping how noise tying modifies generation. Quick check: Can you explain why classifier-free guidance scales the conditional vs. unconditional noise predictions?
- **Concept Bottleneck Models (CBMs)**: Why needed here: The benchmark evaluates CBM generalization; understanding the image-to-concept → concept-to-label pipeline clarifies why class-level concept labels create shortcut learning. Quick check: What is the difference between joint, sequential, and independent CBM training?
- **Compositional Generalization**: Why needed here: The core failure mode is poor generalization to novel combinations of known concepts (known attributes in unseen bird-attribute pairings). Quick check: Why is compositionality harder for models than learning individual concepts in isolation?

## Architecture Onboarding

- **Component map**: TDG Generator (FLUX.1-dev + dual-prompt noise tying) -> VQA Filter (InternVL-2.5-8B) -> Human Validation Interface (binary questions) -> Evaluation Pipeline (CBM/VLM inference)
- **Critical path**: 1. Select reference bird R and target attribute S⁺ (must be in different attribute group than R's natural attribute); 2. Choose guidance bird G (required for shape/texture; "bird" suffices for color); 3. Generate 500 images with TDG, filter to top 50 via VQA; 4. Run human validation on subset; discard pairings with <90% agreement; 5. Evaluate CBM: measure S⁺ accuracy (should be ~50% if failing) and S⁻ accuracy (high = model correctly removes original attribute)
- **Design tradeoffs**: t_max setting: higher (0.9) for color/shape allows more divergence; lower (0.6) for texture requires tighter coupling but risks convergence; VQA threshold: stricter filtering (top 10%) improves precision but reduces dataset coverage (1,485 → 831 pairings survived); Per-image vs. class-level labels: per-image labels may improve concept grounding but were not tested
- **Failure signatures**: TDG generates identical reference and guidance images (t_max too low or k too high); VQA confidence uniformly low (attribute ambiguous or prompt poorly specified); CBM achieves high S⁺ accuracy on SUB but low S⁻ accuracy (model predicting all attributes as present); CBM achieves low S⁺ but high S⁻ accuracy (model predicting class-associated concepts, ignoring image content)
- **First 3 experiments**: 1. Reproduce baseline failure: Train standard CBM on CUB with class-level concept labels, evaluate on SUB—expect S⁺ < 50%; 2. Ablate TDG components: Compare TDG vs. naive prompting for attribute substitution—measure VQA pass rate on generated images; 3. Test per-concept training: Train separate binary classifiers per attribute (no shared weights) to isolate whether weight sharing causes shortcut learning—expect modest improvement but still below human baseline

## Open Questions the Paper Calls Out

- Can architectural modifications or training regularization force CBMs to ground predictions in local image evidence rather than memorized class-level concept vectors? The paper demonstrates that CBMs ground predictions in class identity rather than image content, calling for more robust and well-grounded models, but does not propose a solution.
- Can the Tied Diffusion Guidance (TDG) pipeline be fully automated to generate high-fidelity benchmarks for complex domains outside of natural bird images? TDG requires human intervention for prompt creation and filtering verification, suggesting future work should focus on automation for broader applicability.
- Does the correlation between class labels and concepts in training data inherently prevent compositional generalization, even in models with continuous concept embeddings? CEMs outperform standard CBMs but still fail to reliably detect substitutions, suggesting the issue may stem from training data correlation structure rather than just the binary bottleneck.

## Limitations

- TDG requires human intervention for prompt creation and filtering verification, limiting automation potential
- SUB covers only 33/200 CUB classes and 45/312 attributes, restricting generalizability to full dataset
- Claims about VQA model quality and SUB superiority over real datasets rely on limited human validation samples

## Confidence

- **High Confidence**: SUB dataset construction methodology (TDG implementation, VQA filtering pipeline, human validation protocol) is well-specified and reproducible
- **Medium Confidence**: The failure of CBMs to generalize to SUB is demonstrated, but the specific mechanism (class-level shortcut learning vs. architectural limitations) requires further investigation
- **Low Confidence**: Claims about VQA model quality and the superiority of SUB over real datasets rely on limited human validation samples and indirect comparisons

## Next Checks

1. **Mechanism Dissection**: Train CBMs on CUB with per-image concept labels (instead of class-level) and evaluate on SUB to determine if the failure stems from label aggregation or fundamental concept detection inability
2. **TDG Fidelity Analysis**: Conduct systematic human evaluation of TDG-generated images (not just samples) to verify reference class preservation and attribute substitution accuracy across all bird-attribute pairings
3. **Broader Generalization Test**: Extend SUB to additional CUB classes and attributes, and evaluate whether CBMs fail similarly on novel combinations within their training distribution (e.g., birds with rare attribute pairings not seen during training)