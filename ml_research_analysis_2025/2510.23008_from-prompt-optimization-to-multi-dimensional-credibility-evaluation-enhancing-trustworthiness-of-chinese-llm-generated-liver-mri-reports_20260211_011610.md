---
ver: rpa2
title: 'From Prompt Optimization to Multi-Dimensional Credibility Evaluation: Enhancing
  Trustworthiness of Chinese LLM-Generated Liver MRI Reports'
arxiv_id: '2510.23008'
source_url: https://arxiv.org/abs/2510.23008
tags:
- prompt
- diagnostic
- reports
- clinical
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study introduces a Multi-Dimensional Credibility Assessment
  (MDCA) framework to evaluate and enhance the trustworthiness of LLM-generated liver
  MRI reports. The framework assesses reports across three dimensions: Semantic Coherence,
  Diagnostic Correctness, and Clinical Prioritization Alignment.'
---

# From Prompt Optimization to Multi-Dimensional Credibility Evaluation: Enhancing Trustworthiness of Chinese LLM-Generated Liver MRI Reports

## Quick Facts
- **arXiv ID**: 2510.23008
- **Source URL**: https://arxiv.org/abs/2510.23008
- **Reference count**: 32
- **Primary result**: Multi-Dimensional Credibility Assessment (MDCA) framework improves trustworthiness of Chinese LLM-generated liver MRI reports through systematic prompt optimization.

## Executive Summary
This study introduces a Multi-Dimensional Credibility Assessment (MDCA) framework to evaluate and enhance the trustworthiness of LLM-generated liver MRI reports. The framework assesses reports across three dimensions: Semantic Coherence, Diagnostic Correctness, and Clinical Prioritization Alignment. Through systematic prompt optimization—including role definition, diagnostic taxonomy, verification checkpoints, and example-based guidance—the study evaluated four Chinese LLMs (Kimi-K2, DeepSeek-V3, ByteDance-Seed, Qwen3) on 15,127 liver MRI reports. Results showed that structured prompts with ~10-15 examples achieved optimal performance, with Kimi-K2 and DeepSeek-V3 demonstrating the highest overall MDCA scores of 76.15 and 75.41, respectively.

## Method Summary
The study used 15,127 de-identified liver MRI reports from Gd-EOB-DTPA-enhanced examinations at a single Chinese hospital. Researchers developed 11 prompt configurations varying instruction-based components (role definition, TOP diagnostic taxonomy, verification checkpoints, report structure standards, diagnostic principles) and example-based guidance (0-25 radiologist-written reports). Four Chinese LLMs were evaluated via standardized API calls with fixed parameters. The MDCA framework computed three scores: Semantic Coherence (BERT cosine similarity), Diagnostic Correctness (keyword matching), and Clinical Prioritization Alignment (position-weighted scoring), combined as a weighted composite (0.2×SC + 0.4×DC + 0.4×CPA). Radiologist validation confirmed framework alignment with clinical judgment.

## Key Results
- Structured prompts with 10-15 examples achieved optimal performance across all evaluated models
- Kimi-K2 and DeepSeek-V3 achieved the highest MDCA scores of 76.15 and 75.41, respectively
- MDCA framework scores correlated strongly with radiologist three-tier grading (A/B/C)
- Performance plateaued beyond 20 examples due to contextual noise and redundancy

## Why This Works (Mechanism)

### Mechanism 1: Multi-Dimensional Credibility Assessment Decomposition
The MDCA framework decomposes trustworthiness into three orthogonal dimensions, enabling model-agnostic evaluation without relying on LLM self-assessment. It combines deterministic algorithms: Semantic Coherence uses BERT embeddings for similarity between target and synthesized diagnosis sentences; Diagnostic Correctness applies keyword matching between term sets; Clinical Prioritization Alignment uses position-based scoring to verify high-priority diagnoses appear first. The weighted composite (0.2×SC + 0.4×DC + 0.4×CPA) emphasizes clinical relevance over fluency. This approach assumes keyword matching and position-based scoring adequately capture diagnostic correctness within institutional reporting conventions.

### Mechanism 2: Synergistic Prompt Component Integration
Instruction-based and example-based prompt components produce complementary improvements across MDCA dimensions. Instruction-based components (role definition, TOP taxonomy, verification checkpoints, diagnostic principles) enforce structured reasoning and domain-specific constraints, primarily improving Diagnostic Correctness and Clinical Prioritization Alignment by guiding attention to clinically relevant patterns. Example-based guidance provides reference style and reasoning patterns, primarily improving Semantic Coherence through in-context learning of radiological writing conventions. The combination yields higher composite scores than either approach alone.

### Mechanism 3: Example Count Saturation Effect
Moderate example augmentation (10-20 examples) improves performance through in-context learning, but excessive examples introduce contextual noise that degrades or plateaus performance. In-context learning benefits from diverse representative examples that establish output patterns. Beyond ~20 examples, attention mechanisms may dilute focus across redundant or conflicting patterns, increasing token consumption without proportional gains. This reflects the hypothesis that excessive examples create "contextual noise and redundancy that hinder performance."

## Foundational Learning

### Concept: BERT Sentence Embeddings for Semantic Similarity
- **Why needed here**: The Semantic Coherence dimension uses pre-trained BERT to compute pairwise sentence embeddings between target and synthesized diagnosis sentences, where higher scores indicate better fluency and alignment with radiological writing conventions.
- **Quick check question**: How does BERT's bidirectional attention mechanism enable semantic comparison between two diagnostic sentences, and what are its limitations for domain-specific medical terminology?

### Concept: In-Context Learning with Few-Shot Examples
- **Why needed here**: The study relies on providing 0-25 example reports in prompts to guide LLM behavior without fine-tuning weights, demonstrating that performance gains are achievable through prompt engineering alone.
- **Quick check question**: What is the hypothesized mechanism by which LLMs learn from examples provided in context, and how does example diversity versus quantity affect this learning?

### Concept: Tiered Diagnostic Taxonomy (TOP System)
- **Why needed here**: The TOP system (TOP1: HCC/metastases → TOP5: renal cysts/effusions) structures clinical prioritization for both prompt guidance and CPA evaluation, ensuring malignant/urgent findings are prioritized.
- **Quick check question**: Why would a hierarchical taxonomy improve LLM diagnostic output compared to unstructured lists of possible conditions?

## Architecture Onboarding

### Component map:
- **Input Layer**: 15,127 de-identified liver MRI reports (findings section only, conclusions removed) from Gd-EOB-DTPA-enhanced examinations
- **Prompt Engineering Layer**: 11 configurations (P0-P11) combining instruction-based components (role definition, core task, TOP taxonomy, verification checkpoints, formatting standards, diagnostic principles) with example-based guidance (0-25 radiologist-written reports)
- **LLM Processing Layer**: Four Chinese LLMs accessed via SiliconFlow APIs with standardized parameters (temperature=0.5, top_p=0.95, max_tokens=1024, thinking disabled)
- **MDCA Evaluation Layer**: Three deterministic scoring algorithms—SC (BERT cosine similarity), DC (keyword matching with term set comparison), CPA (position-weighted priority matching with Top-1 verification)
- **Validation Layer**: Radiologist three-tier grading (A/B/C) on sampled reports for framework alignment verification

### Critical path:
1. **Data preprocessing**: Apply exclusion criteria (surgical history, extrahepatic malignancy, absent lesions, poor image quality) to derive 15,127 eligible reports from 24,797 raw records
2. **Prompt construction**: Select appropriate instruction components and example count based on target evaluation dimension (DC/CPA prioritize instruction-based; SC benefits from examples)
3. **Inference execution**: Generate diagnostic conclusions via standardized API calls, controlling for temperature and token limits
4. **MDCA computation**: Calculate SC, DC, CPA using deterministic algorithms; aggregate into composite score
5. **Human validation**: Sample 100 reports per prompt configuration for radiologist grading to verify MDCA alignment

### Design tradeoffs:
- **SC weight (0.2) vs DC/CPA (0.4)**: Deliberately deprioritizes fluency since modern LLMs generate coherent text easily; clinical accuracy matters more
- **Keyword matching for DC**: Simpler and faster than semantic matching, but may miss paraphrased diagnoses or novel terminology
- **Single-center training data**: Enables consistent evaluation criteria but may limit generalizability to institutions with different reporting styles
- **Example count (10-15 optimal)**: Balances performance gains against API token costs and latency

### Failure signatures:
- **High SC + Low DC**: Model generates fluent but diagnostically inaccurate reports → strengthen instruction-based components (TOP taxonomy, verification checkpoints, diagnostic principles)
- **Low SC + High DC**: Model generates accurate but awkward reports → increase example count or improve example quality
- **Low CPA specifically**: Priority diagnoses not appearing first → verify TOP taxonomy is correctly specified in prompt; check CPA scoring logic against institutional guidelines
- **Performance plateau at >20 examples**: Contextual noise from redundant examples → reduce to 10-15 representative cases
- **Model-specific performance gaps**: Some models (ByteDance-Seed, Qwen3) showed weaker adaptability to complex prompts → consider model-specific prompt simplification

### First 3 experiments:
1. **Validate MDCA scoring algorithms**: Reproduce SC (BERT embedding similarity), DC (keyword matching), and CPA (position scoring) on 500 held-out reports with manual verification of intermediate calculations to ensure implementation correctness
2. **Ablate prompt components systematically**: Test P0→P1→P2→P4→P6 progression on a single model (DeepSeek-V3) to isolate contribution of each instruction-based component; measure delta in each MDCA dimension
3. **Establish example count curve**: Run inference with example counts of 0, 3, 5, 10, 15, 20, 25 on 1,000 reports to reproduce the saturation effect and identify optimal count for your target model and computational budget

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: Can the MDCA framework be effectively adapted to evaluate multimodal models that process raw imaging data directly?
- **Basis in paper**: [explicit] The authors note the study focused exclusively on text-to-text LLMs and identify multimodal systems as a distinct future direction currently facing deployment barriers.
- **Why unresolved**: The current MDCA relies on text-based semantic coherence (BERT embeddings), which may not capture the visual reasoning fidelity of multimodal inputs.
- **What evidence would resolve it**: Validation of the MDCA framework on multimodal models (e.g., those ingesting DICOMs) demonstrating correlation with radiologist assessments.

### Open Question 2
- **Question**: How does the inclusion of multi-specialty clinical feedback alter the weightings or structure of the MDCA dimensions?
- **Basis in paper**: [explicit] The authors state the evaluation involved only radiologists and suggest future studies could include multi-specialty panels (e.g., surgeons, oncologists) to refine the framework.
- **Why unresolved**: Different specialists prioritize report elements differently (e.g., surgical resectability vs. diagnostic precision), which may require adjusting the Clinical Prioritization Alignment (CPA) metric.
- **What evidence would resolve it**: A study recalibrating MDCA weights based on assessments from diverse clinical panels to ensure broader clinical utility.

### Open Question 3
- **Question**: Do the prompt optimization strategies generalize to institutions with distinct reporting styles and linguistic norms?
- **Basis in paper**: [explicit] The authors acknowledge the single-center design may restrict generalizability and warrant larger multicenter studies to validate reproducibility.
- **Why unresolved**: Prompts optimized for a specific institutional style (The First Affiliated Hospital of Army Medical University) may yield lower Semantic Coherence or Diagnostic Correctness in different clinical environments.
- **What evidence would resolve it**: Multicenter trials confirming that the 10-15 example heuristic maintains high MDCA scores across hospitals with varying reporting standards.

## Limitations

- **Single-center dataset**: Results may not generalize to institutions with different reporting conventions or imaging protocols
- **Deterministic evaluation**: MDCA framework cannot capture nuanced clinical judgment or assess actual clinical utility in patient care
- **Language-specific focus**: Framework optimized for Chinese medical terminology may not translate directly to other languages

## Confidence

| Claim | Confidence |
|-------|------------|
| Structured prompts with 10-15 examples optimize performance | **High** |
| MDCA framework aligns with radiologist judgments | **High** |
| Mechanisms (dimensional decomposition, prompt synergy, example saturation) are valid | **Medium** |
| Generalizability to other institutions and specialties | **Low** |

## Next Checks

1. **Cross-institutional validation**: Apply the MDCA framework to liver MRI reports from multiple hospitals with different reporting styles to assess framework generalizability and potential need for institution-specific calibration.
2. **Clinical utility assessment**: Conduct a prospective study where clinicians use LLM-generated reports in actual diagnostic workflows, measuring impact on diagnostic accuracy, time efficiency, and clinical decision-making compared to standard reporting.
3. **Framework adaptability test**: Apply the MDCA framework to a different imaging modality (e.g., CT scans or chest radiographs) to evaluate its scalability beyond liver MRI and identify necessary modifications for other clinical contexts.