---
ver: rpa2
title: 'Realizing Scaling Laws in Recommender Systems: A Foundation-Expert Paradigm
  for Hyperscale Model Deployment'
arxiv_id: '2508.02929'
source_url: https://arxiv.org/abs/2508.02929
tags:
- expert
- user
- embeddings
- data
- paradigm
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of efficiently deploying hyperscale
  recommender models at production scale, where traditional one-stage approaches are
  resource-intensive and difficult to maintain across diverse recommendation surfaces.
  The authors propose a Foundation-Expert paradigm that decouples general knowledge
  learning from surface-specific adaptation.
---

# Realizing Scaling Laws in Recommender Systems: A Foundation-Expert Paradigm for Hyperscale Model Deployment

## Quick Facts
- arXiv ID: 2508.02929
- Source URL: https://arxiv.org/abs/2508.02929
- Reference count: 40
- Primary result: Deployment of a two-stage recommender system at Meta serving tens of billions of daily requests with improved generalization and efficiency

## Executive Summary
This paper addresses the challenge of efficiently deploying hyperscale recommender models at production scale, where traditional one-stage approaches are resource-intensive and difficult to maintain across diverse recommendation surfaces. The authors propose a Foundation-Expert paradigm that decouples general knowledge learning from surface-specific adaptation. A central, compute-heavy Foundation Model (FM) is trained on lifelong, cross-surface, multi-modal user data and generates target-aware embeddings for each candidate item. These embeddings are then consumed by lightweight, surface-specific "expert" models, which optimize for local objectives with minimal overhead. The approach is enabled by HyperCast, a production-grade infrastructure that supports decoupled training, high-frequency model updates, and efficient two-tier inference serving.

## Method Summary
The proposed Foundation-Expert paradigm consists of two stages: a central Foundation Model (FM) that learns generalizable user intent from lifelong, cross-surface interaction sequences, and surface-specific Expert models that consume FM-generated embeddings to optimize local objectives. The FM uses an HSTU architecture with multi-modal input embeddings summed together, trained with a multi-task multi-label loss that masks invalid tasks per surface. Experts are lightweight models that fuse FM embeddings with short-term user history via an MLP-based fusion module. The system decouples training, with Experts consuming materialized FM embeddings logged in real-time. This architecture enables efficient updates, high model freshness, and transfer of knowledge from the FM to surface-specific tasks.

## Key Results
- Transfer ratios from FM to experts of 0.64–1.0, significantly higher than traditional knowledge distillation methods
- Generalization to unseen tasks with statistically significant improvements
- Online A/B tests showing user experience improvements without sacrificing latency or compute efficiency
- Current deployment at Meta serving tens of billions of daily requests

## Why This Works (Mechanism)
The Foundation-Expert paradigm works by separating the computationally expensive task of learning generalizable user intent patterns from the efficient adaptation to surface-specific objectives. The FM acts as a knowledge repository, capturing cross-surface patterns from lifelong user histories and multi-modal data. By generating target-aware embeddings for candidate items, the FM encodes rich, generalizable representations that can be efficiently consumed by lightweight Experts. This decoupling allows the computationally intensive FM to be updated less frequently while enabling Experts to rapidly adapt to local objectives and maintain high freshness. The HyperCast infrastructure supports this architecture by enabling decoupled training, efficient serving, and high-frequency updates.

## Foundational Learning
- **HSTU Architecture**: Hierarchical Sequential Transduction Units for processing sequential user interactions. Needed for efficient processing of long user histories. Quick check: Verify that the architecture can handle variable-length sequences and produce meaningful embeddings.
- **Multi-Task Multi-Label (MTML) Loss**: Joint training across multiple surfaces with task-specific weighting and masking. Needed to enable the FM to learn cross-surface patterns while handling surface-specific objectives. Quick check: Confirm that the loss function correctly masks invalid tasks per surface.
- **Target-Aware Embeddings**: FM-generated embeddings that encode both user intent and candidate item properties. Needed to enable efficient knowledge transfer to surface-specific experts. Quick check: Validate that embeddings capture relevant information for downstream tasks.
- **Decoupled Training**: Separation of FM training from Expert training with materialized embeddings. Needed to enable efficient updates and high model freshness. Quick check: Ensure embedding materialization doesn't create bottlenecks.
- **FM Fusion Module**: MLP-based module for combining FM embeddings with short-term context in Experts. Needed to integrate long-term FM knowledge with immediate context. Quick check: Verify that fusion produces meaningful representations for Experts.

## Architecture Onboarding

**Component Map**: User Interaction Sequences -> Foundation Model (HSTU) -> Target-Aware Embeddings -> Expert Models (FM Fusion Module + Surface-Specific Layers) -> Recommendations

**Critical Path**: User history → FM inference → Embedding materialization → Expert inference → Ranking

**Design Tradeoffs**: The two-stage architecture trades additional serving complexity for computational efficiency and improved generalization. While the naive implementation doubles inference time, materialization of FM embeddings and efficient serving infrastructure mitigate this overhead.

**Failure Signatures**: Low transfer ratio indicates poor knowledge transfer from FM to Experts, suggesting the FM embeddings lack expressiveness. High serving latency indicates inefficient implementation of the two-stage inference pipeline.

**First Experiments**: 1) Implement a minimal FM as a Transformer with summed embeddings and train on a public sequential dataset; 2) Generate target-aware embeddings for expert consumption; 3) Train a simple expert model on surface-specific labels using pre-computed FM embeddings.

## Open Questions the Paper Calls Out
None

## Limitations
- Missing architectural details for HSTU and Fusion modules make precise reproduction difficult
- Sampling strategies and task weighting details are not provided, which could impact training stability
- Methodology for defining and evaluating "unseen" task generalization is not detailed

## Confidence
- High: Existence of working, deployed two-stage system at Meta scale (production statistics and online A/B test results)
- Medium: Specific performance claims (transfer ratios, MTML losses) due to missing architectural hyperparameters
- Low: Claims about unseen task generalization due to lack of methodology details

## Next Checks
1. Reproduce a minimal end-to-end pipeline: Implement the FM as a standard Transformer with summed embeddings and train on a public sequential dataset; generate embeddings for expert consumption.
2. Validate transfer ratio definition: Confirm that the formula in Equation 6 correctly measures knowledge transfer from FM to experts, and test on held-out tasks.
3. Profile serving overhead: Measure inference latency for the two-stage approach under realistic load to verify claimed efficiency gains.