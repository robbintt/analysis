---
ver: rpa2
title: Projection-free Algorithms for Online Convex Optimization with Adversarial
  Constraints
arxiv_id: '2501.16919'
source_url: https://arxiv.org/abs/2501.16919
tags:
- algorithm
- convex
- online
- regret
- cost
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of Online Convex Optimization
  with time-varying adversarial constraints, aiming to design a computationally efficient
  learning policy that minimizes both regret and cumulative constraint violation.
  The authors propose a projection-free algorithm that leverages linear optimization
  instead of expensive Euclidean projections.
---

# Projection-free Algorithms for Online Convex Optimization with Adversarial Constraints

## Quick Facts
- arXiv ID: 2501.16919
- Source URL: https://arxiv.org/abs/2501.16919
- Reference count: 40
- Primary result: Projection-free algorithm achieving $\tilde{O}(T^{3/4})$ regret and cumulative constraint violation in online convex optimization with adversarial constraints

## Executive Summary
This paper addresses Online Convex Optimization with time-varying adversarial constraints, proposing a computationally efficient learning policy that minimizes both regret and cumulative constraint violation. The authors develop a projection-free algorithm that leverages linear optimization instead of expensive Euclidean projections, achieving improved bounds of $\tilde{O}(T^{3/4})$ for both metrics. The approach involves constructing a surrogate cost function as a non-negative combination of original costs and constraint functions, then applying an adaptive version of the Online Conditional Gradient algorithm. The method is also extended to the bandit feedback setting while maintaining the same performance guarantees.

## Method Summary
The algorithm combines two key components: a surrogate cost construction and an adaptive Online Conditional Gradient (OCG) method. For each round t, the algorithm builds a surrogate cost $\hat{z}_t(x) = Vf_t(x) + \Phi'(Q(t))g^+_t(x)$, where $V$ is a scaling parameter, $\Phi(x) = x^m$ with $m = \log T$ is a Lyapunov function, and $Q(t)$ tracks cumulative constraint violation. This surrogate is passed to an adaptive OCG algorithm that uses time-varying step sizes $\eta_t = D/(2L_t T^{3/4})$, where $L_t$ is the Lipschitz constant computed online. The algorithm maintains iterate $x_t$ through convex combinations with LP oracle calls, achieving $\tilde{O}(T^{3/4})$ bounds for both regret and cumulative constraint violation through regret decomposition.

## Key Results
- Achieves $\tilde{O}(T^{3/4})$ regret and cumulative constraint violation bounds, improving upon previous projection-free methods
- Extends to bandit feedback setting with same performance guarantees using gradient estimation techniques
- Demonstrates 10-100x computational speedup compared to projection-based methods in numerical experiments
- Handles time-varying adversarial constraints without requiring convexity in constraint evolution

## Why This Works (Mechanism)

### Mechanism 1
- Claim: A surrogate cost function combining original costs and constraint violations enables joint optimization of both objectives.
- Mechanism: The algorithm constructs a surrogate function $\hat{z}_t(x) = Vf_t(x) + \Phi'(Q(t))g^+_t(x)$, where $V$ is a scaling parameter, $\Phi$ is a Lyapunov function (specifically $\Phi(x) = x^m$ with $m = \log T$), and $Q(t)$ tracks cumulative constraint violation. This transforms the constrained problem into an unconstrained one amenable to projection-free methods.
- Core assumption: Constraint functions are convex and G-Lipschitz; a feasible benchmark $x^*$ exists satisfying all constraints (Assumption 3.2).
- Evidence anchors: [abstract] "it first constructs a surrogate cost function as a non-negative linear combination of the cost and constraint functions"; [section 3.2] Equation (13) defines $\hat{z}_t(x) = Vf_t(x) + \Phi'(Q(t))g^+_t(x)$

### Mechanism 2
- Claim: Adaptive learning rates eliminate the need for a priori knowledge of Lipschitz constants while maintaining regret bounds.
- Mechanism: The adaptive OCG algorithm uses time-varying step sizes $\eta_t = D/(2L_t T^{3/4})$, where $L_t$ is the Lipschitz constant of the current surrogate cost (computed online). This handles the fact that surrogate costs have data-dependent Lipschitz constants that cannot be bounded beforehand.
- Core assumption: Lipschitz constants $L_t$ are monotonically non-decreasing (satisfied by the surrogate construction with convex $\Phi$).
- Evidence anchors: [abstract] "adaptive version of the Online Conditional Gradient algorithm"; [section 2.1] Algorithm 1 defines $\eta_t = D/(2L_t T^{3/4})$

### Mechanism 3
- Claim: The regret decomposition inequality enables separate bounding of regret and cumulative constraint violation from a single surrogate regret analysis.
- Mechanism: The key inequality $\Phi(Q(T)) + V\cdot\text{Regret}_T \leq \Phi(Q(0)) + \text{Regret}'_T$ separates the original regret from the surrogate regret. By choosing appropriate $V$ and analyzing the surrogate regret via adaptive OCG, both quantities can be bounded to $\tilde{O}(T^{3/4})$.
- Core assumption: The feasible benchmark $x^*$ satisfies $g_t(x^*) \leq 0$ for all $t$ (Assumption 3.2).
- Evidence anchors: [section 3.2] Equation (14) states the regret decomposition inequality; [section 3.3] Analysis derives $Q(T)$ and $\text{Regret}_T$ bounds from the inequality

## Foundational Learning

- **Concept**: Online Conditional Gradient (Frank-Wolfe) algorithm
  - Why needed here: This is the core subroutine that replaces expensive projections with LP oracle calls. Understanding why line optimization suffices (weak duality, local linear approximation) is essential.
  - Quick check question: Why does minimizing $\langle\nabla F(x_t), x\rangle$ over $X$ give a feasible direction without projection?

- **Concept**: Regret and Cumulative Constraint Violation (CCV) in online learning
  - Why needed here: These are the two performance metrics the algorithm optimizes. Understanding their definitions and tradeoffs is necessary to interpret the $\tilde{O}(T^{3/4})$ bounds.
  - Quick check question: Why is $\tilde{O}(T^{3/4})$ for both metrics considered an improvement over $O(T^{3/4})$ regret with $O(T^{7/8})$ CCV?

- **Concept**: Drift-plus-penalty (Lyapunov drift) methodology
  - Why needed here: The surrogate cost construction and Lyapunov function $\Phi$ derive from this framework for handling long-term constraints.
  - Quick check question: How does the choice of power-law potential $\Phi(x) = x^{\log T}$ affect the CCV bound?

## Architecture Onboarding

- **Component map**: LP Oracle -> Surrogate Cost Builder -> Adaptive OCG Core -> Violation Tracker

- **Critical path**: 
  1. Initialize $x_1 \in X$ arbitrarily, $Q(0) = GD \log T$
  2. Each round: Play $x_t$ → Observe $f_t, g_t$ → Compute surrogate gradient $\nabla\hat{z}_t(x_t)$ → Call LP oracle → Update $x_{t+1}$
  3. Key constraint: LP oracle call must be computationally tractable for your decision set

- **Design tradeoffs**:
  - Projection-free vs. optimal rates: You gain computational efficiency (LP vs. quadratic projection) but sacrifice optimal $\tilde{O}(\sqrt{T})$ bounds for $\tilde{O}(T^{3/4})$
  - Full-information vs. bandit: Bandit setting requires blocking technique (Section 4) and gradient estimation, adding variance complexity
  - LP-only vs. CONV-OPT: This algorithm uses only LP calls; some competitors (Garber & Kretzu Algorithm 3) require convex optimization over Euclidean balls

- **Failure signatures**:
  - $Q(t)$ grows linearly rather than sublinearly → Check if benchmark $x^*$ is truly feasible for all constraints
  - Regret bound has large multiplicative constant → Check if $L_t/L_{t-1}$ ratio is bounded; may need different $\Phi$
  - LP oracle too slow → Decision set may not have efficient linear optimization structure; reconsider problem formulation

- **First 3 experiments**:
  1. **Sanity check on synthetic data**: Implement Algorithm 3 on a simple flow polytope with known optimal solution; verify $\tilde{O}(T^{3/4})$ scaling for both metrics over $T \in \{100, 500, 1000, 2000\}$
  2. **Comparison against projection-based baseline**: Run against Sinha & Vaze (2024) algorithm on same problem; plot regret, CCV, and wall-clock time to validate computational efficiency claim (Figure 3 shows 10-100x speedup)
  3. **Ablation on constraint adaptivity**: Test with fixed vs. time-varying adversarial constraints to verify the algorithm handles the more general setting; check CCV bounds degrade gracefully under adversarial constraint sequences

## Open Questions the Paper Calls Out

- **Question**: Can a tight lower bound be established for projection-free methods solving Constrained OCO, or can either the regret or CCV rate be improved beyond $\tilde{O}(T^{3/4})$?
  - **Basis in paper**: [explicit] The conclusion states: "Future work may investigate whether either of the metrics could be improved further or whether a tight lower bound could be established for a class of projection-free methods."
  - **Why unresolved**: The paper provides upper bounds but no matching lower bounds; the gap between projection-based ($O(\sqrt{T})$) and projection-free ($\tilde{O}(T^{3/4})$) rates remains unexplained.
  - **What evidence would resolve it**: A lower bound proof showing $\Omega(T^{3/4})$ is necessary, or an improved algorithm achieving $O(T^{\alpha})$ with $\alpha < 3/4$.

- **Question**: Can projection-free algorithms for COCO achieve the same $O(\sqrt{T})$ regret and CCV rates as projection-based methods?
  - **Basis in paper**: [inferred] Table 1 shows projection-based methods achieve $O(\sqrt{T})$ regret and $\tilde{O}(\sqrt{T})$ CCV, while the proposed projection-free method achieves $\tilde{O}(T^{3/4})$—a fundamental gap that is not addressed theoretically.
  - **Why unresolved**: The paper improves upon prior projection-free bounds ($O(T^{7/8})$ CCV) but does not determine if the $\sqrt{T}$ rate is achievable without projections.
  - **What evidence would resolve it**: Either a projection-free algorithm with $\tilde{O}(\sqrt{T})$ bounds, or a lower bound proving this rate is unattainable for projection-free methods.

- **Question**: Can the bandit feedback results be extended to fully adaptive adversaries (where cost functions depend on all past actions including the current one)?
  - **Basis in paper**: [inferred] Section C.3 notes that "in the bandit setting, such an adaptive adversary is not feasible" because gradient estimates would be rendered useless, limiting analysis to non-oblivious adversaries where costs depend only on $x_{<t}$.
  - **Why unresolved**: The bandit extension relies on overestimation of cumulative violation to handle non-oblivious adversaries, but fully adaptive adversaries remain outside the current framework.
  - **What evidence would resolve it**: An algorithm with provable guarantees against fully adaptive adversaries in the bandit setting, or a negative result showing impossibility.

## Limitations

- The algorithm requires a strictly feasible benchmark $x^*$ satisfying all constraints for all rounds; no guarantees exist if this assumption is violated
- Performance bounds depend on maintaining a bounded Lipschitz constant ratio $L_t/L_{t-1}$, which may not hold for all problem instances
- The bandit feedback extension introduces additional variance through gradient estimation that may degrade empirical performance

## Confidence

- **High confidence**: The surrogate cost construction mechanism and its role in enabling projection-free optimization (Mechanism 1)
- **Medium confidence**: The adaptive OCG algorithm's ability to handle unknown Lipschitz constants while maintaining bounds (Mechanism 2)
- **Medium confidence**: The regret decomposition framework enabling joint regret and CCV bounds (Mechanism 3)

## Next Checks

1. **Feasibility robustness test**: Implement the algorithm on instances where the benchmark $x^*$ violates constraints on a small fraction of rounds (ε-feasible case) to determine the degradation in CCV bounds.

2. **Lipschitz ratio sensitivity analysis**: Systematically vary problem parameters to measure how the $L_t/L_{t-1}$ ratio affects the multiplicative factor in regret bounds, identifying the threshold where bounds become impractical.

3. **Computational complexity validation**: Implement the LP oracle for the flow polytope using different graph sizes and densities to empirically verify the claimed 10-100x speedup over projection-based methods across the full problem spectrum.