---
ver: rpa2
title: 'Dyna-Mind: Learning to Simulate from Experience for Better AI Agents'
arxiv_id: '2510.09577'
source_url: https://arxiv.org/abs/2510.09577
tags:
- action
- reasoning
- simulation
- training
- agent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Dyna-Mind, a two-stage training framework
  that enhances AI agents' simulation and reasoning capabilities for long-horizon,
  planning-intensive tasks. The first stage, Reasoning with Simulations (RESIM), constructs
  training data by generating structured reasoning traces from search trees built
  using real environment interactions, grounding the agent's reasoning in accurate
  world dynamics.
---

# Dyna-Mind: Learning to Simulate from Experience for Better AI Agents
## Quick Facts
- arXiv ID: 2510.09577
- Source URL: https://arxiv.org/abs/2510.09577
- Reference count: 40
- Outperforms baselines by 3-13% on Sokoban, ALFWorld, and AndroidWorld tasks

## Executive Summary
This paper introduces Dyna-Mind, a two-stage training framework that enhances AI agents' simulation and reasoning capabilities for long-horizon, planning-intensive tasks. The first stage, Reasoning with Simulations (RESIM), constructs training data by generating structured reasoning traces from search trees built using real environment interactions, grounding the agent's reasoning in accurate world dynamics. The second stage, Dyna-GRPO, employs online reinforcement learning to further improve simulation and decision-making by leveraging both outcome rewards and intermediate state feedback from rollouts. Experiments on Sokoban, ALFWorld, and AndroidWorld show that RESIM effectively teaches agents to simulate future states, while Dyna-GRPO improves performance by refining simulation ability during training.

## Method Summary
Dyna-Mind is a two-stage framework for training (V)LM agents on simulation-guided reasoning tasks. Stage 1 (RESIM) collects real environment trajectories, builds search trees via DFS, and aggregates them into structured reasoning responses using an aggregator model. This data is used to finetune the agent via SFT. Stage 2 (Dyna-GRPO) performs online RL with alternating phases: simulation improvement using SimRollout with future state feedback, and policy improvement using standard GRPO. The framework uses both outcome rewards and intermediate state feedback to train agents that can accurately simulate future states while making effective decisions.

## Key Results
- RESIM significantly outperforms baselines like REACT and R1 across all three environments
- Dyna-GRPO improves performance by 3-13% over SFT-only baselines
- Strong positive correlation (r=0.64-0.78) between simulation score improvement and task success
- Achieves 77.1% success rate on Sokoban compared to 73.1% for GRPO baseline

## Why This Works (Mechanism)

### Mechanism 1
Grounding simulation in real environment interactions improves reasoning accuracy over purely synthetic simulation. ReSim constructs reasoning traces from search trees built via actual environment rollouts, then aggregates them into structured responses that contain ground-truth future state information. This bypasses hallucination from reasoning models generating synthetic simulation data. Core assumption: The rollout model and value function can generate sufficiently diverse and informative trajectories for distillation. Evidence anchors: RESIM grounds the agent's reasoning in faithful world dynamics and equips it with the ability to anticipate future states. Break condition: If the rollout model is too weak to discover useful trajectories, the distilled reasoning traces will contain poor-quality simulations regardless of grounding.

### Mechanism 2
Intermediate state feedback during RL training provides richer learning signal than outcome-only rewards. SimRollout executes the agent's planned actions in the environment, retrieves actual next-states, then prompts the agent to refine its response using this ground-truth feedback. The refined trajectory receives modified advantage rewards that incentivize both correctness and improvement over standard rollouts. Core assumption: The agent can leverage textual future-state information to improve its reasoning when prompted. Evidence anchors: Dyna-GRPO uses both outcome rewards and intermediate states as feedback from real rollouts. SimRollout improves performance over REACT baseline across all tested models. Break condition: If the policy cannot meaningfully incorporate the future-state information, refinement provides no benefit.

### Mechanism 3
Alternating between simulation improvement and policy improvement separates world-modeling from decision-making optimization. Dyna-GRPO iterates between (1) simulation improvement using refined trajectories with future-state access, and (2) policy improvement using standard rollouts without future-state access. This prevents the model from overfitting to the privileged information while still improving its simulation ability. Core assumption: The alternating curriculum transfers simulation ability to the unassisted policy. Evidence anchors: Dyna-GRPO iterates between simulation improvement and direct policy improvement. Dyna-GRPO outperforms GRPO and RLOO baselines. Break condition: If n_T or n_π are poorly balanced, the model may either overfit to future-state access or fail to transfer simulation gains.

## Foundational Learning

- Concept: **Markov Decision Processes (MDPs) and rollout-based planning**
  - Why needed here: The entire framework models environments as MDPs (S, A, T, R). Understanding how rollouts generate state-action sequences and how value functions estimate trajectory quality is essential for comprehending both ReSim data collection and Dyna-GRPO optimization.
  - Quick check question: Given a state s and policy π, can you trace how a rollout generates a trajectory τ = {s₀, a₀, s₁, a₁, ..., s_T}?

- Concept: **Group Relative Policy Optimization (GRPO)**
  - Why needed here: Dyna-GRPO builds directly on GRPO, which computes advantages relative to a group of trajectories rather than using a separate value network. Understanding the baseline computation (mean/std normalization within groups) is necessary to modify the advantage function for simulation refinement.
  - Quick check question: How does GRPO's advantage function A(τ) differ from standard PPO's advantage estimation using GAE?

- Concept: **Search tree construction (DFS/BFS/MCTS)**
  - Why needed here: ReSim uses depth-first search to expand search trees from real environment interactions. The branching factor (b) and depth (d) hyperparameters directly control the quality and coverage of simulation data.
  - Quick check question: Given a state with branching factor b=16 and depth d=5, how many partial trajectories are evaluated before aggregation?

## Architecture Onboarding

- Component map: Environment → Rollout Model → Search Tree (DFS) → Value Function → Aggregator VLM → ReSim Response → SFT Training → Dyna-GRPO Loop: Policy π_θ → SimRollout (with future states) → Refined Trajectory τ'_refine → Combined Group → GRPO Update → Simulation Improvement Step → Policy Improvement Step (no future states) → Final Policy

- Critical path: 1) Instantiate rollout model (finetuned or prompted LLM/VLM), 2) Configure value function (finetuned value head or LLM-as-judge), 3) Collect ReSim trajectories by executing Algorithm 3 in environment, 4) Perform SFT distillation on collected trajectories, 5) Initialize Dyna-GRPO with distilled model, set n_T and n_π iteration counts, 6) Implement modified advantage A_refine for simulation improvement phase

- Design tradeoffs:
  - Rollout model strength vs. compute: Weaker rollout models reduce inference cost but may miss valuable trajectories
  - Branch/depth (b, d) vs. response length: Higher values improve simulation coverage but produce longer reasoning traces
  - n_T vs. n_π balance: More simulation steps improve world modeling but may slow policy adaptation

- Failure signatures:
  - Low simulation score (Sim Score < 0.4) despite high task success → policy may be succeeding via shortcuts, not simulation
  - SimRollout refinement shows no improvement over base response → future-state information not being utilized
  - Large ID-OOD gap → overfitting to training distribution; increase trajectory diversity or reduce b_train subsampling

- First 3 experiments:
  1. Baseline validation: Run REACT, DISTILL(R1), and DISTILL(ReSim) on a held-out set; verify DISTILL(ReSim) achieves comparable or better performance with fewer generated tokens
  2. Simulation score correlation: Compute Sim Score for multiple checkpoints during training; confirm positive correlation between Sim Score improvement and success rate
  3. Ablate n_T/n_π ratio: Train with n_T ∈ {0, 5, 10, 20} and fixed n_π=10; identify the point where simulation improvement stops providing marginal gains

## Open Questions the Paper Calls Out
None

## Limitations
- The aggregation process in RESIM lacks detailed specification, particularly the prompts used to guide the aggregator model
- Computational costs and runtime efficiency are not addressed, despite requiring multiple model invocations
- Empirical validation focuses primarily on success rates without deep analysis of failure cases or ablation studies for critical hyperparameters

## Confidence
**High Confidence**: The core claim that grounding simulation in real environment interactions improves reasoning accuracy is well-supported by experimental results.
**Medium Confidence**: The mechanism of intermediate state feedback providing richer learning signals is demonstrated but could have stronger theoretical justification.
**Low Confidence**: The assertion that the alternating curriculum effectively transfers simulation ability to unassisted policies is the weakest claim with minimal empirical support.

## Next Checks
1. Systematically vary b (branching factor), d (depth), n_T (simulation steps), and n_π (policy steps) across multiple runs to identify optimal configurations and assess robustness
2. Conduct detailed analysis of failed trajectories to determine whether failures stem from simulation errors, reasoning errors, or environment complexity
3. Measure wall-clock time, inference cost, and memory usage for RESIM data collection and Dyna-GRPO training compared to baselines