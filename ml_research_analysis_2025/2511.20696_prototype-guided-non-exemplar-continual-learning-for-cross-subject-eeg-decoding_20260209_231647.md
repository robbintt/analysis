---
ver: rpa2
title: Prototype-Guided Non-Exemplar Continual Learning for Cross-subject EEG Decoding
arxiv_id: '2511.20696'
source_url: https://arxiv.org/abs/2511.20696
tags:
- learning
- continual
- subjects
- subject
- prototype
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of continual learning for EEG-based
  brain-computer interfaces (BCIs), where high inter-subject variability leads to
  catastrophic forgetting when new subjects are introduced. Existing methods relying
  on replay buffers are impractical due to privacy and memory constraints.
---

# Prototype-Guided Non-Exemplar Continual Learning for Cross-subject EEG Decoding

## Quick Facts
- arXiv ID: 2511.20696
- Source URL: https://arxiv.org/abs/2511.20696
- Reference count: 35
- Primary result: ProNECL achieves 77.18% (BCI Comp IV 2a) and 81.15% (BCI Comp IV 2b) accuracy while mitigating catastrophic forgetting without exemplar replay

## Executive Summary
This paper addresses the challenge of continual learning for EEG-based brain-computer interfaces where high inter-subject variability causes catastrophic forgetting when new subjects are introduced. The proposed Prototype-guided Non-Exemplar Continual Learning (ProNECL) framework preserves prior knowledge without storing historical EEG samples by constructing class-level prototypes as compact summaries of discriminative knowledge. Through prototype-based feature regularization and cross-subject alignment, ProNECL incrementally adapts to new subjects while maintaining performance on previously seen subjects. Experiments demonstrate superior performance compared to baselines, achieving high accuracy with backward transfer close to zero.

## Method Summary
ProNECL employs a DeepConvNet encoder with 3-layer MLP classifier and introduces a global prototype memory updated via exponential moving average (EMA). For each new subject, the model computes class-level prototypes from embeddings, applies a combined loss function (classification + prototype consistency + cross-subject alignment), and updates the global prototypes. The prototype consistency loss constrains new subject embeddings to remain close to established class prototypes, while cross-subject alignment normalizes subject-level features toward a global centroid. This approach enables continual learning without exemplar replay, addressing privacy and memory constraints in real-world BCI applications.

## Key Results
- Achieves 77.18% average accuracy on BCI Competition IV 2a dataset (4-class motor imagery)
- Achieves 81.15% average accuracy on BCI Competition IV 2b dataset (2-class motor imagery)
- Maintains backward transfer close to zero, effectively mitigating catastrophic forgetting
- Outperforms finetuning baseline by 12.85% and 12.48% on respective datasets
- Demonstrates superior performance to EWC and LWF regularization methods

## Why This Works (Mechanism)

### Mechanism 1: Prototype-Based Knowledge Compression
Summarizing class-level distributions as mean embeddings preserves discriminative information while eliminating raw data storage requirements. For each class $c$, the prototype $P_c^k = \frac{1}{|D_c^k|} \sum Z_i^k$ aggregates all embeddings into a single centroid, updated via EMA: $P_c \leftarrow \alpha P_c + (1-\alpha) P_c^k$. Core assumption: Class centroids sufficiently capture discriminative boundary information without access to variance or outlier information.

### Mechanism 2: Prototype Consistency Regularization
Constraining new subject embeddings to remain close to their corresponding class prototypes prevents feature drift and catastrophic forgetting. The prototype consistency loss $\mathcal{L}_{pro} = \mathbb{E}[\|E_\phi(x^k) - P_{y^k}\|_2^2]$ explicitly penalizes deviations from established prototype positions in embedding space, anchoring new learning to prior knowledge.

### Mechanism 3: Cross-Subject Centroid Alignment
Aligning subject-level mean embeddings with the global prototype centroid promotes domain-invariant representations across subjects. The alignment loss $\mathcal{L}_{align} = \|\frac{1}{m_k}\sum E_\phi(X_i^k) - \frac{1}{C}\sum P_c\|_2^2$ pulls the current subject's average feature toward the global centroid, reducing inter-subject variability without storing samples.

## Foundational Learning

- **Concept: Catastrophic Forgetting in Sequential Learning**
  - Why needed here: The entire framework is designed to mitigate CF; understanding why neural networks overwrite prior knowledge when learning sequentially is essential.
  - Quick check question: Can you explain why fine-tuning on new subjects degrades performance on previous subjects (BWT = -42.70% in Table I)?

- **Concept: Prototype/Centroid Representations**
  - Why needed here: The core innovation replaces exemplar replay with prototype memory; understanding what information centroids preserve (and discard) is critical.
  - Quick check question: What information is lost when representing a class distribution solely by its mean embedding?

- **Concept: EEG Inter-Subject Variability**
  - Why needed here: The paper explicitly targets cross-subject decoding where physiological differences cause domain shift; this motivates the alignment mechanism.
  - Quick check question: Why do traditional transfer learning approaches struggle with sequential subject data in BCI applications?

## Architecture Onboarding

- **Component map:**
  Input EEG (X) → Encoder E_φ (DeepConvNet) → Embedding Z ∈ R^d → Classifier C_ψ (3-layer MLP) → Prediction Ŷ
                                              ↓
                                    Prototype Memory P = {P_c}^{C}_{c=1} (global, updated via EMA)
                                              ↓
  Loss Computation: L_total = L_ce + λ_p·L_pro + λ_a·L_align

- **Critical path:**
  1. Base Phase: Pre-train $F_0$ on $D_0$, compute initial prototypes $P_c$ from first subject
  2. Incremental Phase (each new subject): Load prototype memory, train with combined loss, update global prototypes via EMA
  3. Evaluation: Test on all seen subjects to compute ACC and BWT

- **Design tradeoffs:**
  - $\alpha$ (EMA coefficient): Higher values retain more prior knowledge but adapt slowly; paper doesn't specify value used
  - $\lambda_p = 0.1, \lambda_a = 0.3$: Alignment weighted higher than prototype consistency—suggests cross-subject normalization is prioritized
  - Prototype dimension $d$: Must balance representational capacity against memory footprint

- **Failure signatures:**
  - Negative BWT with large magnitude: Catastrophic forgetting not mitigated
  - High ACC but poor per-class balance: Prototypes may be dominated by majority classes
  - Training instability with $\mathcal{L}_{align}$: Subject features may be too far from global centroid

- **First 3 experiments:**
  1. Reproduce baseline comparison: Implement Finetuning and EWC on BCI Competition IV 2a to verify ACC gap before implementing ProNECL
  2. Ablation on loss components: Test ProNECL with $\lambda_p=0$ and $\lambda_a=0$ separately to isolate contribution of prototype consistency vs. alignment
  3. Prototype capacity test: Vary $\alpha \in \{0.1, 0.3, 0.5, 0.7, 0.9\}$ to characterize sensitivity to EMA update rate

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the ProNECL framework be adapted to handle multimodal data streams where different physiological signals (e.g., EEG combined with EMG or fNIRS) possess heterogeneous feature dimensions and distributions?
- Basis in paper: [explicit] The authors state in the conclusion: "In future work, we plan to extend this framework to multimodal... scenarios to further enhance adaptability in real-world BCI applications."
- Why unresolved: The current method computes prototypes by averaging embeddings from a single encoder. It is unclear how a global prototype memory can simultaneously represent and align distinct modalities that may have conflicting optimal latent spaces.
- What evidence would resolve it: A demonstration of ProNECL on a multimodal dataset, showing that cross-subject alignment improves (or maintains) performance when fusing modalities compared to single-modality baselines.

### Open Question 2
- Question: Can the prototype construction mechanism be modified to function effectively in unsupervised or semi-supervised scenarios where class labels for new subjects are partially missing or entirely unavailable?
- Basis in paper: [explicit] The conclusion explicitly identifies "unsupervised continual decoding scenarios" as a target for future extensions.
- Why unresolved: The current methodology relies on Eq. (2) to compute class-level prototypes using the ground truth labels of the current subject. Without these labels, the mechanism for updating the global prototype memory is undefined.
- What evidence would resolve it: Implementing a clustering-based or pseudo-labeling approach within ProNECL that maintains classification accuracy on unlabeled subjects without catastrophic forgetting of previous labeled subjects.

### Open Question 3
- Question: Is the global prototype memory susceptible to irreversible drift or corruption when the sequential data stream includes subjects with poor signal quality or non-discriminative features?
- Basis in paper: [inferred] The method updates the global prototype memory using an Exponential Moving Average (Eq. 3). The paper does not analyze the impact of "noisy" or outlier subjects on the stability of the prototype centroid.
- Why unresolved: If a new subject's features are non-separable or noisy, their prototypes ($P_c^k$) will be poor representations. Averaging these into the global memory ($P_c$) could degrade the reference anchor for all subsequent alignments.
- What evidence would resolve it: An ablation study introducing subjects with high noise levels or low classification accuracy into the training sequence, measuring the resulting degradation in the global prototype's discriminability.

### Open Question 4
- Question: How does ProNECL perform in a class-incremental setting where new motor imagery tasks (classes) are introduced over time, rather than just new subjects performing the same tasks?
- Basis in paper: [inferred] The problem definition fixes the label space $Y$, and the prototype memory initializes based on the base phase. The paper does not discuss mechanisms for expanding the prototype memory to accommodate classes unseen in the initial training phase.
- Why unresolved: To handle new classes, the model would need to expand the output layer and create new prototypes without access to old data, risking the re-mapping of old class features to new prototype slots.
- What evidence would resolve it: Experimental results on a dataset stream where new classes are introduced incrementally, evaluating the model's ability to distinguish both old and new classes.

## Limitations

- The EMA coefficient α for prototype updates is not specified, which is critical for balancing prior and new knowledge retention
- The ablation study lacks analysis of individual loss component contributions (λ_p vs λ_a), making it difficult to attribute performance gains to specific mechanisms
- The prototype-based approach may lose discriminative information from multi-modal class distributions or high intra-class variance that cannot be captured by centroids alone

## Confidence

- Prototype-based knowledge compression mechanism: Medium confidence - Theoretical framework is sound but assumption about centroid sufficiency lacks empirical validation
- Prototype consistency regularization effectiveness: High confidence - Loss formulation is explicit with supporting t-SNE visualization evidence
- Cross-subject alignment contribution: Medium confidence - Theoretically justified but isolated contribution not quantified through ablation studies

## Next Checks

1. **Ablation study**: Implement ProNECL variants with λ_p=0 and λ_a=0 separately to quantify the individual contribution of prototype consistency versus cross-subject alignment mechanisms.

2. **EMA sensitivity analysis**: Systematically vary α ∈ {0.1, 0.3, 0.5, 0.7, 0.9} to characterize the sensitivity of prototype memory updates and identify the optimal balance between prior knowledge retention and adaptation speed.

3. **Distribution shape validation**: Test ProNECL on synthetic EEG-like data with controlled multi-modal class distributions to empirically verify whether prototype centroids capture sufficient discriminative information when intra-class variance is high.