---
ver: rpa2
title: Towards aligned body representations in vision models
arxiv_id: '2512.00365'
source_url: https://arxiv.org/abs/2512.00365
tags:
- representations
- human
- body
- vision
- physical
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper examines whether vision models trained for segmentation
  develop human-like "coarse" body representations used for physical reasoning. Researchers
  adapted a psychophysical experiment (50 human participants) into a semantic segmentation
  task, testing seven segmentation networks of varying sizes.
---

# Towards aligned body representations in vision models

## Quick Facts
- arXiv ID: 2512.00365
- Source URL: https://arxiv.org/abs/2512.00365
- Authors: Andrey Gizdov; Andrea Procopio; Yichen Li; Daniel Harari; Tomer Ullman
- Reference count: 9
- Primary result: Smaller vision models naturally develop human-like coarse body representations, simplifying concavities and "filling in" missing regions, while larger models produce overly detailed encodings due to computational resource constraints.

## Executive Summary
This paper investigates whether vision models trained for segmentation develop human-like "coarse" body representations used for physical reasoning. Researchers adapted a psychophysical experiment (50 human participants) into a semantic segmentation task, testing seven segmentation networks of varying sizes. They found that smaller models naturally form human-like coarse body representations—simplifying concavities and "filling in" missing regions—while larger models produce overly detailed encodings. This effect emerged as a consequence of limited computational resources, with smaller networks favoring efficient, convex-like encodings that balance accuracy with computational efficiency. The results suggest that coarse representations can emerge under resource constraints, and that machine models may provide a scalable path toward understanding how humans encode and reason about the physical world.

## Method Summary
The study used SegFormer B0-B5 models (3.8M-84.7M parameters) pretrained on ADE20K, fine-tuned on synthetic polygon datasets (5-12 vertices, 0-3 concavities, 24 colors) for 15 epochs with AdamW optimizer (lr=5×10⁻⁵, cosine schedule, batch size 4). The key metric was Relative Area Change (RAC_seg) comparing initial and modified shapes. Researchers tested three change types: CONCAVE, NOFILL, and CONVEX modifications, measuring how well model segmentations matched human psychophysical data from Li et al. (2023a). The optimal change detection threshold was τ=1% based on minimal RMSE versus human responses.

## Key Results
- Smaller models (B0-B2) exhibit stronger concavity-smoothing effects, matching human psychophysical data with RAC_seg gaps between concave and convex changes.
- Larger models (B4-B5) show reduced concavity smoothing, producing finer-grained segmentations with smaller RAC_seg differences between change types.
- Extended training (15 epochs vs 5) reduces coarse representation bias in smaller models, while shorter training enhances concavity smoothing effects.

## Why This Works (Mechanism)

### Mechanism 1: Resource-Constraint-Induced Coarse Encoding
Smaller models develop human-like coarse body representations as an efficient solution under limited capacity. Under parameter constraints, networks favor compact, convex-like encodings that reduce vertices and spatial detail, smoothing concavities while preserving convex boundaries. This creates a "filling-in" effect analogous to human psychophysical data. The human brain's resource constraints similarly favor efficient coarse representations.

### Mechanism 2: Differential Geometric Sensitivity (Concavity vs. Convexity)
Models and humans both exhibit reduced sensitivity to changes in concave regions compared to convex ones. Probability maps show activations spreading beyond concave boundaries while corners and convex edges remain stable. The Relative Area Change (RAC) metric remains lower for concave modifications even with extended training, reflecting a representational bias toward compact encodings.

### Mechanism 3: Scale-Dependent Detail Tradeoff
Model capacity and training compute control a spectrum from coarse to fine-grained representations. As models scale from B0 (~3.8M) to B5 (~84.7M) parameters, the gap between concave and convex sensitivity diminishes. Larger models can "afford" finer geometric detail, while smaller models prioritize computational efficiency through coarser encodings.

## Foundational Learning

- **Concept: Relative Area Change (RAC) metric**
  - Why needed here: Quantifies local sensitivity to shape modifications; enables comparison between model outputs and human change-detection behavior.
  - Quick check question: Can you compute RAC_seg given A_init, A_out, and A_seg^(gt)?

- **Concept: Convex hull / coarse body approximation**
  - Why needed here: Provides the theoretical basis for why "filling-in" concavities is computationally efficient (fewer vertices, lower memory).
  - Quick check question: Why might a convex approximation be sufficient for collision detection but not for object recognition?

- **Concept: SegFormer architecture hierarchy**
  - Why needed here: Understanding B0-B5 variants (3.8M–84.7M parameters) is essential for reproducing the scale-dependence experiments.
  - Quick check question: Which model size would you expect to show the strongest human-like coarse representation?

## Architecture Onboarding

- **Component map:** Pretrained SegFormer (ADE20K) -> Synthetic polygon fine-tuning -> Mask extraction -> RAC_seg computation -> Human data comparison

- **Critical path:**
  1. Load pretrained SegFormer (ADE20K weights)
  2. Generate synthetic polygon dataset (5–12 vertices, 0–3 concavities, 24 colors)
  3. Fine-tune (AdamW, lr=5×10⁻⁵, cosine schedule, 15 epochs, batch size 4)
  4. Extract masks → compute RAC_seg across CONCAVE/NOFILL/CONVEX conditions
  5. Compare RAC_seg profiles to human psychophysical data

- **Design tradeoffs:**
  - Smaller models (B0–B2): Better human alignment, lower absolute accuracy
  - Larger models (B4–B5): Higher segmentation fidelity, reduced human-like coarse bias
  - Training duration: Longer training reduces coarse representation bias

- **Failure signatures:**
  - RAC_seg ≈ 1 for all conditions → model too fine-grained (try smaller model or early stopping)
  - RAC_seg ≈ 0 for all conditions → model failing to segment (check training convergence)
  - Large variance across τ thresholds → unstable representations (increase training data diversity)

- **First 3 experiments:**
  1. Replicate RAC_seg curves for B0 vs B5 across 15 epochs to confirm scale-dependence.
  2. Ablate training compute: train B3 for 5 vs 15 epochs and measure concavity smoothing.
  3. Test transfer: fine-tune on real-world segmentation datasets (e.g., COCO) and measure whether coarse bias persists.

## Open Questions the Paper Calls Out
None

## Limitations
- Synthetic training data may not capture real-world shape complexity, potentially limiting generalization to naturalistic images.
- The paper does not establish whether resource constraints are necessary versus merely sufficient for coarse representations—larger models might develop similar patterns under different training regimes.
- Comparison with human data is correlational rather than mechanistic, leaving open whether both systems converge on coarse encodings for the same computational reasons.

## Confidence

**High confidence**: That smaller SegFormer models exhibit concavity-smoothing effects and that this effect diminishes with model size. The RAC_seg metric and training curves are well-defined and reproducible.

**Medium confidence**: That resource constraints are the primary driver of coarse representations. While the scale-dependence is clear, alternative explanations (architecture inductive biases, pretraining data influence) are not ruled out.

**Low confidence**: That this represents a true parallel with human cognition. The comparison is behavioral (similar RAC patterns) but does not establish whether the underlying mechanisms are shared.

## Next Checks
1. **Generalization test**: Fine-tune SegFormer B0–B5 on a real-world shape dataset (e.g., Pascal VOC with shape annotations) and measure whether coarse bias persists on Li et al. stimuli. This would validate whether the effect transfers beyond synthetic training.

2. **Resource manipulation experiment**: Hold model architecture constant (e.g., B3) but vary effective compute through early stopping, gradient checkpointing, or smaller batch sizes. If coarse representations emerge under compute constraints alone, this strengthens the resource hypothesis.

3. **Alternative architecture test**: Train a non-transformer architecture (e.g., DeepLabV3+) with identical parameter budgets. If coarse representations emerge regardless of architecture, this suggests a more general principle; if not, the effect may be SegFormer-specific.