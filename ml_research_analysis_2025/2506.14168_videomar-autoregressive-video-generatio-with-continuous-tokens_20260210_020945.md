---
ver: rpa2
title: 'VideoMAR: Autoregressive Video Generatio with Continuous Tokens'
arxiv_id: '2506.14168'
source_url: https://arxiv.org/abs/2506.14168
tags:
- video
- generation
- videomar
- autoregressive
- temporal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces VideoMAR, a decoder-only autoregressive video
  generation model with continuous tokens. It proposes next-frame diffusion loss for
  mask-video integration, temporal short-to-long curriculum learning, spatial progressive
  resolution training, and progressive temperature strategy to mitigate accumulation
  error.
---

# VideoMAR: Autoregressive Video Generatio with Continuous Tokens

## Quick Facts
- **arXiv ID**: 2506.14168
- **Source URL**: https://arxiv.org/abs/2506.14168
- **Reference count**: 40
- **Primary result**: Achieves SOTA on VBench-I2V with 9.3% fewer parameters than Cosmos I2V

## Executive Summary
VideoMAR introduces a decoder-only autoregressive video generation model that operates with continuous tokens rather than discrete ones. The model incorporates several key innovations including next-frame diffusion loss for mask-video integration, temporal short-to-long curriculum learning, spatial progressive resolution training, and a progressive temperature strategy to mitigate accumulation error. VideoMAR achieves state-of-the-art performance on the VBench-I2V benchmark while using significantly fewer parameters, less training data, and reduced GPU resources compared to competing models.

## Method Summary
VideoMAR employs a continuous token-based autoregressive framework that generates video frames sequentially. The architecture leverages 3D rotary embeddings to capture both spatial and temporal relationships. Key innovations include a next-frame diffusion loss that improves mask-video integration, curriculum learning strategies that progress from short to long temporal sequences and from low to high spatial resolutions, and a progressive temperature schedule designed to reduce error accumulation during long sequence generation. The model demonstrates capabilities in both spatial and temporal extrapolation while maintaining efficiency through reduced parameter counts and resource requirements.

## Key Results
- Achieves state-of-the-art performance on VBench-I2V benchmark
- Outperforms Cosmos I2V with 9.3% fewer parameters, 0.5% less training data, and 0.2% fewer GPU resources
- Demonstrates spatial and temporal extrapolation capabilities via 3D rotary embeddings

## Why This Works (Mechanism)
VideoMAR's continuous token approach enables smoother transitions between video frames compared to discrete token methods. The progressive temperature strategy effectively mitigates error accumulation that typically plagues autoregressive models during long sequence generation. The next-frame diffusion loss provides better integration between masked regions and existing video content, while the curriculum learning approach allows the model to gradually learn complex temporal and spatial relationships without being overwhelmed initially.

## Foundational Learning
- **Autoregressive video generation**: Sequentially predicts frames conditioned on previous ones - needed for temporal coherence, check by verifying frame-to-frame consistency
- **Continuous vs discrete tokens**: Uses continuous representations rather than quantized tokens - needed for smoother transitions, check by comparing output smoothness metrics
- **3D rotary embeddings**: Extends relative position encoding to three dimensions - needed for joint spatial-temporal modeling, check by validating positional encoding effectiveness
- **Curriculum learning**: Gradually increases sequence length and resolution during training - needed to manage complexity, check by examining learning curves across stages
- **Diffusion loss for integration**: Uses diffusion-based objectives for mask-video fusion - needed for seamless content blending, check by evaluating mask boundary artifacts
- **Progressive temperature scheduling**: Gradually adjusts sampling temperature - needed to balance creativity and stability, check by monitoring error accumulation over sequence length

## Architecture Onboarding

**Component map**: Continuous token encoder -> 3D rotary embedding layer -> Temporal/Spatial curriculum modules -> Progressive temperature scheduler -> Next-frame diffusion loss module -> Frame generator

**Critical path**: Input prompt -> Continuous token encoding -> 3D rotary embeddings -> Curriculum learning progression -> Temperature scheduling -> Frame-by-frame generation with diffusion loss integration

**Design tradeoffs**: The model prioritizes efficiency (fewer parameters/resources) over absolute maximum performance, uses continuous tokens for smoothness at the cost of potential quantization benefits, and employs curriculum learning to manage training complexity while potentially extending training time.

**Failure signatures**: Error accumulation in long sequences despite progressive temperature strategy, potential quality degradation when extrapolating beyond training resolution/time bounds, and possible limitations in generating highly complex motions due to data scale constraints.

**3 first experiments**:
1. Validate continuous token smoothness by comparing against discrete token baseline on frame interpolation tasks
2. Test progressive temperature effectiveness by measuring error accumulation across sequence lengths with and without temperature scheduling
3. Evaluate curriculum learning impact by training with flat (non-curriculum) schedules and comparing final performance metrics

## Open Questions the Paper Calls Out
- Can VideoMAR naturally function as an interactive world model by replacing text prompts with frame-level action conditions?
- Can VideoMAR effectively unify text-to-image, text-to-video, and video editing tasks within a single continuous-token autoregressive framework?
- Can scaling training data resolve the observed limitation in generating large, complex motions without exacerbating accumulation errors?

## Limitations
- Long-term generation stability remains uncertain despite progressive temperature strategy
- Performance claims rely on a single benchmark (VBench-I2V), limiting generalizability
- Practical significance of spatial and temporal extrapolation capabilities for real-world applications is unclear

## Confidence
- **High**: Architectural innovations (continuous tokens, 3D rotary embeddings) and their technical implementation
- **Medium**: Performance claims on VBench-I2V benchmark and resource efficiency comparisons with Cosmos I2V
- **Low**: Long-term generation stability and real-world applicability of extrapolation capabilities

## Next Checks
1. Test VideoMAR's generation stability on video sequences longer than those evaluated in the paper (e.g., 30+ seconds at 30fps) to assess progressive temperature strategy effectiveness
2. Evaluate VideoMAR on multiple video generation benchmarks beyond VBench-I2V to verify generalization of claimed state-of-the-art performance
3. Compare VideoMAR's mask-video integration performance against alternative methods using ablation studies with identical training configurations