---
ver: rpa2
title: A Continual Offline Reinforcement Learning Benchmark for Navigation Tasks
arxiv_id: '2506.02883'
source_url: https://arxiv.org/abs/2506.02883
tags:
- uni00000037
- learning
- uni00000030
- uni00000031
- uni00000028
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Continual NavBench, a novel benchmark for
  Continual Offline Reinforcement Learning (CRL) in navigation tasks within video
  game environments. The benchmark addresses the challenge of evaluating CRL algorithms
  that must adapt to changing tasks without catastrophic forgetting.
---

# A Continual Offline Reinforcement Learning Benchmark for Navigation Tasks

## Quick Facts
- arXiv ID: 2506.02883
- Source URL: https://arxiv.org/abs/2506.02883
- Reference count: 32
- Introduces Continual NavBench, a benchmark for Continual Offline Reinforcement Learning in navigation tasks

## Executive Summary
This paper introduces Continual NavBench, a novel benchmark for Continual Offline Reinforcement Learning (CRL) in navigation tasks within video game environments. The benchmark addresses the challenge of evaluating CRL algorithms that must adapt to changing tasks without catastrophic forgetting. The authors provide standardized offline datasets from 10 hours of human gameplay across diverse Godot mazes, evaluation protocols, and metrics. They evaluate various CRL methods including PNN, HiSPO, EWC, L2 regularization, and replay-based approaches on these datasets. PNN achieves the highest performance but with significant memory and computational costs, while HiSPO shows good performance with moderate overhead. The benchmark provides a reproducible framework for comparing CRL methods in navigation tasks, filling a gap in the literature for standardized evaluation in this domain.

## Method Summary
The paper introduces Continual NavBench, a benchmark designed to evaluate Continual Offline Reinforcement Learning (CRL) algorithms in navigation tasks. The benchmark consists of standardized offline datasets collected from 10 hours of human gameplay in Godot maze environments, evaluation protocols, and metrics. The authors evaluate several CRL methods including Progressive Neural Networks (PNN), HiSPO, Elastic Weight Consolidation (EWC), L2 regularization, and replay-based approaches. The evaluation focuses on measuring performance across changing tasks while preventing catastrophic forgetting, with particular attention to computational efficiency and memory usage. The benchmark aims to provide a reproducible framework for comparing CRL methods in navigation tasks.

## Key Results
- PNN achieves highest performance but requires significant memory and computational resources
- HiSPO demonstrates good performance with moderate overhead
- The benchmark reveals substantial limitations in scalability and computational efficiency of current CRL methods

## Why This Works (Mechanism)
The benchmark works by providing standardized datasets and evaluation protocols that isolate the challenge of catastrophic forgetting in CRL. By using human gameplay data from controlled maze environments, it creates reproducible scenarios where algorithms must adapt to new tasks while retaining performance on previous ones. The diversity of mazes and tasks in the dataset ensures that evaluated methods face realistic challenges in balancing exploration of new tasks with preservation of learned knowledge.

## Foundational Learning

**Offline Reinforcement Learning**
*Why needed:* Allows learning from pre-collected datasets without environment interaction
*Quick check:* Verify data collection methodology and distribution coverage

**Continual Learning**
*Why needed:* Addresses adaptation to changing tasks while preventing catastrophic forgetting
*Quick check:* Confirm task diversity and transition patterns in benchmark

**Catastrophic Forgetting**
*Why needed:* Central challenge in CRL where models lose performance on previous tasks
*Quick check:* Measure performance degradation across task sequences

## Architecture Onboarding

**Component Map**
Offline Datasets -> CRL Methods (PNN, HiSPO, EWC, L2, Replay) -> Evaluation Metrics

**Critical Path**
Data Collection → Method Implementation → Training → Evaluation → Analysis

**Design Tradeoffs**
- PNN: Highest performance but prohibitive memory/computation costs
- HiSPO: Good balance of performance and efficiency
- EWC/L2: Simpler but potentially less effective at preventing forgetting
- Replay: Memory-efficient but may struggle with long task sequences

**Failure Signatures**
- Performance degradation on early tasks indicates catastrophic forgetting
- High computational costs suggest scalability issues
- Inconsistent performance across maze types indicates poor generalization

**3 First Experiments**
1. Baseline comparison of all methods on single-task navigation
2. Progressive task addition to measure forgetting rates
3. Memory usage profiling during multi-task training

## Open Questions the Paper Calls Out
None

## Limitations
- PNN's high memory and computational costs limit practical applicability
- Game environment focus may not generalize to real-world navigation scenarios
- 10 hours of human gameplay may not capture sufficient task diversity

## Confidence

**High confidence** in methodology for creating benchmark and evaluation protocols
**Medium confidence** in comparative analysis of CRL methods given computational constraints
**Low confidence** in benchmark's applicability to real-world navigation scenarios

## Next Checks

1. Evaluate the benchmark's performance using real-world navigation datasets to assess generalizability beyond game environments

2. Conduct ablation studies to determine the impact of specific architectural components in PNN on both performance and computational efficiency

3. Compare the benchmark results with online RL approaches to quantify the trade-offs between offline and online learning in navigation tasks