---
ver: rpa2
title: 'GraspCorrect: Robotic Grasp Correction via Vision-Language Model-Guided Feedback'
arxiv_id: '2503.15035'
source_url: https://arxiv.org/abs/2503.15035
tags:
- grasp
- manipulation
- graspcorrect
- robotic
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: GraspCorrect addresses the challenge of unstable grasping in robotic
  manipulation, which remains a bottleneck even for state-of-the-art policies. The
  proposed method uses vision-language model (VLM) guidance for grasp detection, combined
  with visual goal generation and goal-conditioned behavioral cloning to predict corrective
  joint-level actions.
---

# GraspCorrect: Robotic Grasp Correction via Vision-Language Model-Guided Feedback

## Quick Facts
- **arXiv ID:** 2503.15035
- **Source URL:** https://arxiv.org/abs/2503.15035
- **Reference count:** 12
- **Primary result:** Improved robotic grasp correction using VLM-guided feedback and goal-conditioned behavioral cloning, achieving up to 26.4% success rate gains

## Executive Summary
GraspCorrect addresses the challenge of unstable grasping in robotic manipulation by introducing a vision-language model (VLM)-guided feedback system. The method uses iterative visual question answering to select grasp points, composite goal image generation to represent target configurations, and goal-conditioned behavioral cloning with diffusion models to predict corrective actions. When integrated with existing policies like 3D Diffuser Actor and RVT-2, GraspCorrect significantly improves task success rates and shows particular effectiveness on long-horizon manipulation tasks.

## Method Summary
GraspCorrect is a plug-and-play module that activates during the grasping phase of robotic manipulation. It uses an iterative VQA framework with a VLM (ChatGPT-4o) over 4 iterations to select grasp points from segmented object contours, incorporating grasp-guided prompting and object-aware sampling. Visual goals are generated through LaMa inpainting and image composition, blending the gripper, transformed object, and restored background. The action generation module uses goal-conditioned behavioral cloning with a DDPM, where a ResNet-34 encoder processes stacked current and goal images, and an MLP outputs noise predictions for denoising position and rotation components. The system is trained on synthetically perturbed grasp poses and integrated with baseline policies without retraining.

## Key Results
- Integrating GraspCorrect with 3D Diffuser Actor increased success rates by up to 26.4% on RLBench tasks
- Average length scores improved by 0.7-0.8 on long-horizon tasks in CALVIN dataset
- Ablation studies showed success rate improvements from 30.6% to 73.8% through iterative refinement and object-aware sampling
- Method is architecture-agnostic, enhancing existing policies without retraining

## Why This Works (Mechanism)

### Mechanism 1
Iterative VQA with constrained sampling produces more physically feasible grasp candidates than direct VLM spatial prediction. Rather than asking VLMs to output continuous coordinates (which they struggle with), the system samples candidate grasp points along segmented object contours, presents them visually with numbered annotations, and iteratively refines based on VLM selection. Grasp-guided prompting injects task constraints; object-aware sampling via LangSAM masks prevents proposals on background regions. Core assumption: VLMs can reliably rank visual grasp candidates even if they cannot generate precise coordinates directly. Evidence: [abstract] "grasp-guided prompting, which incorporates task-specific constraints, and object-aware sampling, which ensures the selection of physically feasible grasp candidates"; [Section 3.1] iterative refinement approach; [Section 4.1, Table 3] ablation showing 30.6% → 42.5% → 73.8% success rate improvements.

### Mechanism 2
Composite goal images outperform diffusion-based image generation for representing target grasp configurations. The system uses LaMa inpainting to restore occluded backgrounds, then blends the gripper and transformed object via conventional image transformations guided by detected contact points. This preserves structural integrity better than generative models. Core assumption: Task-relevant spatial relationships can be captured through 2D image composition without explicit 3D reasoning in the goal representation. Evidence: [Section 3.2] composite image construction process; [Section 3.4, Figure 4] testing of DALL-E, SuSIE, DiffEdit, and Imagic showing failures. Break condition: If grasp corrections require complex 3D reasoning beyond top-down 2D projection, composite images may not capture necessary spatial constraints.

### Mechanism 3
GCBC with diffusion models translates visual goals into executable joint actions while maintaining trajectory coherence. A ResNet-34 encoder processes stacked current and goal images; an MLP outputs noise predictions for DDPM denoising over position and rotation. Current action state conditions the model for smooth integration. The loss separately weights position (λ=0.2) and rotation noise terms. Core assumption: The training distribution of perturbed grasp poses generalizes to real-world correction scenarios encountered during policy execution. Evidence: [Section 3.3, Equation 2] loss formulation with separate noise terms and weighting λ=0.2; [Section 4.1, Table 4] λ=0.2 achieves 82.7% success vs. 77.3–78.7% for other values.

## Foundational Learning

- **Vision-Language Models for Spatial Reasoning**
  - Why needed: Understanding that VLMs excel at semantic understanding but struggle with precise spatial output; the workaround is visual prompting with discrete choices rather than continuous prediction.
  - Quick check: Can you explain why asking a VLM to output coordinates directly fails, while asking it to choose among annotated points succeeds?

- **Goal-Conditioned Behavioral Cloning**
  - Why needed: The action generation module learns to map (observation, goal_image) pairs to actions, requiring understanding of how goal-conditioning shapes policy learning.
  - Quick check: How does providing a goal image during training differ from standard behavioral cloning without goal conditioning?

- **Diffusion Models for Action Generation**
  - Why needed: DDPM is used to denoise action distributions; understanding the iterative refinement process is essential for debugging action quality.
  - Quick check: What does the noise prediction network learn, and how does multi-step denoising produce the final action?

## Architecture Onboarding

- **Component map:** Grasp Detection (pre-grasp frame → LangSAM segmentation → VLM iterative selection → grasp point) → Goal Generation (LaMa inpainting → image composition → goal image) → Action Generation (ResNet-34 encoder → 3-layer MLP → DDPM denoising → joint action) → Integration point (activates at grasping moment t(g) when gripper contacts target)
- **Critical path:** VLM grasp point selection → goal image quality → action prediction accuracy. Errors in grasp detection propagate through the entire pipeline; ablation shows object-aware sampling alone contributes ~31 percentage points (42.5% → 73.8%).
- **Design tradeoffs:** Top-down imagery simplifies grasp guidance but may miss 3D geometry (acknowledged limitation); image-based goals are interpretable but struggle with occlusion and dynamic properties; plug-and-play design avoids retraining but cannot address pre-grasp or post-grasp failures.
- **Failure signatures:** GraspCorrect inactive on tasks without object grasping (Table 1, "-" entries); limited impact when baseline models fail before reaching grasp phase (PerAct/Act3D height prediction failures); no degradation observed, but gains are modest when baselines already perform well (>95%).
- **First 3 experiments:** 1) Validate VLM grasp selection by running iterative VQA on held-out RLBench tasks and measuring candidate ranking accuracy against ground-truth grasp annotations; 2) Test goal image fidelity by generating composite goals for 50 grasp scenarios and comparing perceptual similarity and downstream action accuracy against diffusion-based alternatives; 3) End-to-end integration by testing GraspCorrect with 3D Diffuser Actor on 5 challenging RLBench tasks and reporting success rate delta and failure mode analysis.

## Open Questions the Paper Calls Out

- **Performance in physical real-world environments:** The paper restricts experiments to RLBench and CALVIN simulation datasets, yet frames the problem around real-world application bottlenecks. No empirical evidence is provided for efficacy on physical robots where factors like lighting noise, VLM API latency, and calibration errors differ significantly from simulation.

- **Incorporation of tactile or force feedback:** The Limitations section states that incorporating force and tactile feedback could further refine grasp correction, allowing the system to adjust its grip based on material properties. The current architecture relies exclusively on visual inputs, making it blind to haptic properties like surface friction, weight distribution, or deformability that dictate grasp stability.

- **Reliance on top-down imagery limitations:** The authors note that reliance on top-view imagery may overlook crucial geometric features in complex 3D environments. While top-down views simplify grasp guidance, they struggle with occlusions and structural details of tall objects or those requiring side-access, a constraint not quantified in the current evaluation.

## Limitations

- **Top-down imagery constraints:** Reliance on top-view imagery simplifies grasp guidance but may miss crucial 3D geometric features in complex environments
- **VLM dependence on segmentation quality:** Grasp detection depends on LangSAM segmentation quality and VLM ranking reliability under occlusion or unusual object geometries
- **Limited to grasp-phase corrections:** Method specifically designed for grasp-phase corrections and cannot address pre-grasp planning failures or post-grasp object manipulation issues

## Confidence

- **High Confidence:** Ablation study results showing grasp-guided prompting (30.6% → 42.5%) and object-aware sampling (42.5% → 73.8% success rate) are well-supported by direct experimental evidence. Architectural description and integration with baseline policies is clearly specified.
- **Medium Confidence:** Claim that composite goal images outperform diffusion-based generation relies on qualitative comparisons; systematic quantitative comparison of goal image quality metrics would strengthen this claim.
- **Medium Confidence:** Assertion that GraspCorrect is architecture-agnostic is supported by successful integration with two different policy architectures, but broader validation across more diverse policy types would be needed for stronger generalization claims.

## Next Checks

1. **VLM Spatial Reasoning Robustness:** Evaluate grasp candidate ranking accuracy under increasing levels of occlusion and object complexity, comparing against ground-truth grasp annotations to quantify performance degradation.
2. **Goal Image Quality Impact:** Conduct controlled experiments isolating goal image quality effects by testing GCBC performance with synthetic perfect goal images versus composite-generated goals across varying object geometries.
3. **Distribution Shift Analysis:** Measure the performance gap between synthetically perturbed training grasps and real policy failure cases to quantify the extent of distribution shift affecting correction accuracy.