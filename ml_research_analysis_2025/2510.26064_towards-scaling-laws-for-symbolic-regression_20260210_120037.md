---
ver: rpa2
title: Towards Scaling Laws for Symbolic Regression
arxiv_id: '2510.26064'
source_url: https://arxiv.org/abs/2510.26064
tags:
- scaling
- training
- expressions
- compute
- size
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work presents the first systematic scaling study of symbolic
  regression (SR) using transformer models. The authors investigate how SR performance
  scales with model size and compute across five model sizes (6.5M-93M parameters)
  and three orders of magnitude in compute.
---

# Towards Scaling Laws for Symbolic Regression

## Quick Facts
- arXiv ID: 2510.26064
- Source URL: https://arxiv.org/abs/2510.26064
- Authors: David Otte; JÃ¶rg K. H. Franke; Frank Hutter
- Reference count: 40
- First systematic scaling study of symbolic regression using transformer models

## Executive Summary
This work presents the first systematic scaling study of symbolic regression (SR) using transformer models, investigating how performance scales with model size and compute across five model sizes (6.5M-93M parameters) and three orders of magnitude in compute. The authors develop a controlled synthetic data generation approach that produces cleaner training data by recursively generating expressions and filtering duplicates, combined with an improved tabular transformer architecture featuring cell-level embeddings and row/column attention.

Key findings show that both validation loss and solved rate follow clear power-law scaling with compute, with Accsolved increasing from 0.03 to 0.6 across the tested range. The study identifies an optimal token-to-parameter ratio of approximately 15 in their regime, with a slight upward trend as compute increases. These results demonstrate that SR performance is largely predictable from compute and provide practical heuristics for training future SR models.

## Method Summary
The authors systematically studied symbolic regression scaling by training transformer models across five sizes (6.5M to 93M parameters) with three orders of magnitude variation in compute. They developed a novel synthetic data generation approach that recursively generates expressions and filters duplicates to produce cleaner training data compared to existing methods. The improved tabular transformer architecture incorporates cell-level embeddings and row/column attention mechanisms. The experimental design controlled for data quality while varying model size and compute budget to isolate scaling relationships.

## Key Results
- Validation loss and solved rate follow clear power-law scaling with compute across the tested range
- Accsolved increases from 0.03 to 0.6 as compute increases across three orders of magnitude
- Optimal token-to-parameter ratio of approximately 15 identified in their training regime
- Optimal batch size and learning rate both grow with model size, contrasting with language modeling findings

## Why This Works (Mechanism)
The power-law scaling observed in symbolic regression follows similar principles to language modeling scaling, where performance improves predictably with increased compute and model size. The controlled synthetic data generation provides cleaner supervision signals compared to noisy real-world data, enabling clearer observation of scaling relationships. The improved transformer architecture with cell-level embeddings and row/column attention better captures the structural properties of symbolic expressions, allowing models to leverage increased capacity more effectively.

## Foundational Learning
- Power-law scaling: Performance improvements in machine learning models often follow predictable power-law relationships with compute and model size, allowing for extrapolation of future performance
  - Why needed: Understanding scaling relationships enables prediction of model performance and efficient resource allocation
  - Quick check: Plot performance metrics against compute on log-log scale to verify linear relationship

- Synthetic data generation for SR: Recursive generation of symbolic expressions with duplicate filtering creates cleaner training data than real-world datasets
  - Why needed: Clean, controlled data enables isolation of scaling effects from data quality variations
  - Quick check: Compare duplicate rates and expression complexity distributions between synthetic and real SR datasets

- Tabular transformer architecture: Cell-level embeddings combined with row/column attention capture structural relationships in tabular symbolic expressions
  - Why needed: Symbolic regression requires modeling hierarchical expression structures, not just sequence patterns
  - Quick check: Measure attention weights distribution across row/column dimensions during inference

## Architecture Onboarding

Component map: Data Generation -> Model Training -> Performance Evaluation -> Scaling Analysis

Critical path: The core experimental pipeline involves generating synthetic symbolic expressions, training transformer models with varying sizes and compute budgets, evaluating solved rates and validation loss, then analyzing scaling relationships through power-law fitting.

Design tradeoffs: The study prioritizes controlled experimental conditions over real-world data complexity, trading data authenticity for cleaner scaling observations. The tabular transformer architecture trades off increased parameter count for better structural modeling capabilities.

Failure signatures: Scaling law violations would manifest as deviations from power-law relationships in performance metrics, plateaus in solved rates despite increased compute, or unstable training dynamics with batch size/parameter ratio mismatches.

First experiments:
1. Train smallest model (6.5M parameters) with minimum compute to establish baseline performance
2. Vary batch size around predicted optimal ratio to verify batch size scaling relationship
3. Generate and visualize synthetic data distribution to confirm quality improvements over baseline methods

## Open Questions the Paper Calls Out
None

## Limitations
- Scaling laws may break down when training beyond the compute-optimal frontier or with extreme model sizes
- Synthetic data generation, while cleaner, may not fully capture real-world SR problem complexity and noise characteristics
- The optimal token-to-parameter ratio of 15 may shift in different training configurations or with alternative model architectures

## Confidence
- Scaling law findings: High - systematic variation across model sizes and compute budgets with controlled data generation
- Practical heuristics (batch size, learning rate, token ratio): Medium - derived from limited parameter range, may require adjustment for larger models

## Next Checks
1. Test scaling laws with parameter sizes beyond 100M to validate extrapolation predictions
2. Evaluate model performance on real-world SR benchmarks to assess transfer from synthetic data
3. Experiment with alternative transformer architectures (e.g., different attention mechanisms) to verify architecture independence of scaling laws