---
ver: rpa2
title: Sentiment Analysis on the young people's perception about the mobile Internet
  costs in Senegal
arxiv_id: '2504.13284'
source_url: https://arxiv.org/abs/2504.13284
tags:
- sentiment
- data
- internet
- twitter
- comments
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper investigates young people\u2019s sentiment towards\
  \ mobile Internet costs in Senegal using social media data. The authors collected\
  \ over 10,000 comments from Twitter and Facebook, focusing on discussions about\
  \ Internet package prices and perceived network quality."
---

# Sentiment Analysis on the young people's perception about the mobile Internet costs in Senegal

## Quick Facts
- **arXiv ID:** 2504.13284
- **Source URL:** https://arxiv.org/abs/2504.13284
- **Reference count:** 0
- **Primary result:** Sentiment analysis of 10,184 social media comments reveals predominantly negative sentiment toward Orange, Senegal's dominant telecom operator, while Free and Expresso receive more positive feedback despite network quality concerns.

## Executive Summary
This study analyzes young people's sentiment about mobile Internet costs in Senegal using social media data from Twitter and Facebook. The authors collected over 10,000 comments discussing Internet package prices and network quality, then applied multilingual language models to classify sentiment. Due to low-resource language challenges with Wolof, they translated texts to French before classification. Results show significant dissatisfaction with Orange's cost-quality trade-off, while other operators received more positive sentiment despite network issues. The work highlights the need for better telecom regulation and infrastructure development in Senegal.

## Method Summary
The researchers collected 10,184 social media comments about mobile Internet costs in Senegal, using Bright Data scraping for Facebook and Twitter API for tweets. They applied language identification (GlotLID) to separate French and Wolof texts, then translated Wolof to French using Google Translate API. After preprocessing with regex and stopword removal (NLTK for French, manually extended for Wolof), they used XLM-T, a domain-specific multilingual model pre-trained on social media data, to classify sentiment into positive, negative, or neutral categories. Word frequency analysis validated results by revealing operator-specific complaint vocabulary.

## Key Results
- XLM-T outperformed GPT-4o on Wolof/French social media sentiment, avoiding neutral overclassification
- Orange received predominantly negative sentiment despite being the dominant operator
- Free and Expresso operators received more positive sentiment despite network quality concerns
- Word cloud analysis revealed hostile terms like "voleur" (thief) and "arnaque" (scam) for Orange comments

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Domain-specific multilingual models outperform general-purpose LLMs for social media sentiment in low-resource language contexts.
- Mechanism: XLM-T, pre-trained on ~200 million tweets across 30 languages, captures platform-specific linguistic patterns (e.g., informal syntax, emoticons, code-switching) that general models like GPT-4o misclassify as neutral.
- Core assumption: Social media text distributions differ sufficiently from general corpora that domain pre-training provides measurable gains.
- Evidence anchors:
  - [section 5]: "We observed the same behavior with an overclassification to the neutral sentiment. To mitigate this aspect, we used XLM-T... a domain-specific model (in this case, social media) is more effective than its general counterpart."
  - [corpus]: Related work [2] (AfriSenti) confirms benchmark challenges for African languages on Twitter, showing domain-specific models improve performance.
- Break condition: If target language has zero representation in pre-training data, domain-specific gains may not transfer.

### Mechanism 2
- Claim: Translate-test pipelines enable sentiment analysis for low-resource languages when direct models are unavailable.
- Mechanism: Wolof text → Google Translate API → French → XLM-T classifier. Google's translation model handles non-standard Wolof orthography better than specialized models trained on formal scripts.
- Core assumption: Translation preserves sentiment-bearing lexical and syntactic features with tolerable error propagation.
- Evidence anchors:
  - [section 5]: "Google's translation model offers greater robustness to variations in Wolof writing than specialized models."
  - [section 6]: "Our approach also relies mainly on the translate-test principle... errors in the translation process tend to propagate to the subsequent steps, potentially inducing additional bias."
  - [corpus]: Limited direct evidence in corpus; related work [24] cited supports translate-test efficacy in low-resource settings.
- Break condition: High translation error rates on idiomatic or sentiment-heavy expressions (e.g., "beugouniou") will corrupt downstream classification.

### Mechanism 3
- Claim: Word frequency analysis validates sentiment model outputs by revealing domain-specific complaint vocabulary.
- Mechanism: After stopword removal (French via NLTK; Wolof manually extended), word clouds surface high-frequency sentiment-bearing terms (e.g., "voleur," "arnaques," "boycotte" for Orange) that correlate with negative classification distributions.
- Core assumption: Frequent terms in operator-specific corpora reflect genuine user sentiment rather than noise or sarcasm.
- Evidence anchors:
  - [section 4]: "We see very hostile terms like voleur (thief), arnaque (scam), boycotte (boycott) or beugouniou... Orange is generally criticized by the population for the high cost of its packages."
  - [section 5]: Sentiment distributions (Fig. 11–14) align with word cloud hostility levels per operator.
  - [corpus]: No direct corpus validation; related sentiment analysis papers [3, 4, 5] use similar lexicon-confirmation approaches.
- Break condition: If sarcasm or bot activity drives frequent terms, word cloud signals decouple from genuine sentiment.

## Foundational Learning

- **Code-switching / Code-mixing**:
  - Why needed here: Comments mix French and Wolof in single posts; monolingual models fail to capture cross-lingual sentiment cues.
  - Quick check question: Can you identify why standard stopword lists would miss mixed-language noise?

- **Low-resource language processing**:
  - Why needed here: Wolof lacks standardized stopword lexicons and formal orthography on social media, requiring manual augmentation and translation-based workarounds.
  - Quick check question: What are two failure modes when applying high-resource NLP pipelines to low-resource languages?

- **Class imbalance in sentiment datasets**:
  - Why needed here: Orange dominates data volume (largest operator); sentiment results may reflect operator-specific rather than market-wide patterns.
  - Quick check question: How would you detect whether negative sentiment toward Orange is a data artifact or genuine signal?

## Architecture Onboarding

- **Component map**:
  - Data ingestion: Bright Data scraping API (Facebook) + Twitter API → raw CSVs
  - Language ID: GlotLID (>1,500 languages, including low-resource)
  - Translation: Google Translate API (Wolof → French)
  - Preprocessing: Regex (hashtags, mentions, URLs), NLTK stopword removal + manual Wolof extension
  - Classification: XLM-T (XLM-Roberta fine-tuned on tweets)
  - Validation: Word cloud frequency analysis per operator

- **Critical path**:
  - Scraping → Language detection → (Wolof?) → Translation → Preprocessing → XLM-T inference → Sentiment distribution per operator

- **Design tradeoffs**:
  - Twitter vs. Facebook: Twitter provides cleaner text but API restrictions limit volume (250 tweets vs. 10,000+ Facebook comments); Facebook introduces platform-specific noise.
  - Translate-test vs. native Wolof model: Translation introduces error but avoids costly annotation; native model requires labeled Wolof corpus (planned future work).
  - GPT-4o vs. XLM-T: GPT-4o offers broader capabilities but overclassifies neutral; XLM-T is narrower but domain-optimized.

- **Failure signatures**:
  - GPT-4o systematic neutral overclassification on Wolof/French social text.
  - Translation errors on idiomatic Wolof terms (e.g., "beugouniou" may lose sentiment intensity).
  - Data imbalance across operators distorting comparative sentiment (Orange has more data but also more negative sentiment).

- **First 3 experiments**:
  1. **Baseline validation**: Run XLM-T on a held-out manually annotated subset (100–200 comments) to quantify precision/recall per sentiment class; document translation-induced error rate.
  2. **Operator-balanced sampling**: Resample to equalize comment counts across operators; compare sentiment distributions to detect whether Orange negativity is robust or volume-driven.
  3. **Ablation on translation**: Compare (a) Wolof→French→XLM-T vs. (b) direct XLM-T on raw Wolof vs. (c) GPT-4o on raw Wolof to isolate translation vs. model effects.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can a native Wolof sentiment classification model outperform the translate-test approach for Senegalese social media data?
- Basis in paper: [explicit] "We also intend to carry out an annotation of the final corpus in order to build up an open sentiment analysis dataset for the Wolof language. This will ultimately provide a more suitable and therefore more powerful classification model..."
- Why unresolved: The authors relied on translating Wolof to French via Google Translate before classification, which introduces potential meaning loss; no native Wolof sentiment model exists.
- What evidence would resolve it: A manually annotated Wolof sentiment corpus and comparison experiments between native Wolof models versus translate-test baselines on held-out data.

### Open Question 2
- Question: To what extent do translation errors propagate and bias sentiment predictions in low-resource language settings?
- Basis in paper: [explicit] "This method has proved highly effective in low-resource environments... but errors in the translation process tend to propagate to the subsequent steps, potentially inducing additional bias."
- Why unresolved: The paper does not quantify translation-induced error rates or their downstream impact on sentiment classification accuracy.
- What evidence would resolve it: Error analysis of Google Translate output on Wolof social media text, coupled with human evaluation of sentiment labels before and after translation.

### Open Question 3
- Question: Do sentiment patterns on Twitter differ systematically from those on Facebook for telecom service discussions in Senegal?
- Basis in paper: [explicit] "The over-representation of Facebook data may induce biases, as users may have a different behavior on Twitter."
- Why unresolved: Twitter API restrictions limited collection to only 250 tweets versus over 10,000 Facebook comments, preventing meaningful cross-platform comparison.
- What evidence would resolve it: A balanced corpus with comparable sample sizes from both platforms, enabling statistical comparison of sentiment distributions per operator.

### Open Question 4
- Question: How does code-switching between French and Wolof affect sentiment classification performance?
- Basis in paper: [inferred] The paper notes code-mixing is "typical of countries where several languages are present" but does not handle mixed-language comments explicitly, preprocessing each language separately.
- Why unresolved: No analysis was conducted on comments containing both French and Wolof in the same utterance, which may confuse language identification and sentiment models.
- What evidence would resolve it: Identification and separate evaluation of code-switched comments, comparing performance of monolingual versus code-switch-aware models.

## Limitations
- Limited Twitter data (250 tweets) versus 10,000+ Facebook comments creates potential platform bias
- Translation from Wolof to French introduces error propagation that wasn't quantitatively measured
- No ground truth labels available for quantitative performance evaluation of the sentiment classifier

## Confidence
- **Method design**: High - clear rationale for translate-test approach in low-resource settings
- **Model choice**: High - XLM-T domain-specific advantages demonstrated empirically
- **Results interpretation**: Medium - limited by lack of ground truth and data imbalance
- **Reproducibility**: Medium - key parameters (stopword lists, exact keywords) not fully specified

## Next Checks
1. Manually annotate 100-200 comments to establish ground truth for quantitative evaluation
2. Conduct translation error analysis by comparing Wolof sentiment before/after Google Translate
3. Resample data to balance operator representation and verify Orange sentiment isn't volume-driven artifact