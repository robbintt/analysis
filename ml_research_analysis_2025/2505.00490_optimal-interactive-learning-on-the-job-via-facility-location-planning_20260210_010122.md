---
ver: rpa2
title: Optimal Interactive Learning on the Job via Facility Location Planning
arxiv_id: '2505.00490'
source_url: https://arxiv.org/abs/2505.00490
tags:
- human
- coil
- cost
- robot
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: COIL addresses the problem of minimizing human effort during multi-task
  human-robot collaboration by strategically planning when to learn new skills, request
  preferences, or ask for human assistance. The core method formulates interactive
  learning as an uncapacitated facility location problem, enabling bounded-suboptimal
  planning in polynomial time.
---

# Optimal Interactive Learning on the Job via Facility Location Planning

## Quick Facts
- arXiv ID: 2505.00490
- Source URL: https://arxiv.org/abs/2505.00490
- Authors: Shivam Vats, Michelle Zhao, Patrick Callaghan, Mingxi Jia, Maxim Likhachev, Oliver Kroemer, George Konidaris
- Reference count: 40
- Key outcome: Reduces human effort by 12%-23% through strategic planning of skill learning, preference queries, and human assistance

## Executive Summary
COIL addresses the problem of minimizing human effort during multi-task human-robot collaboration by strategically planning when to learn new skills, request preferences, or ask for human assistance. The core method formulates interactive learning as an uncapacitated facility location problem, enabling bounded-suboptimal planning in polynomial time. COIL extends this formulation to handle preference uncertainty through one-step belief space planning. Experiments across gridworld, simulated manipulation, and real-world conveyor domains show COIL reduces human effort by 12%-23% compared to baselines while maintaining task completion, and adapts online to teaching failures by updating plans based on observed skill learnability.

## Method Summary
COIL formulates interactive learning as an uncapacitated facility location (UFL) problem where learning a skill is a "facility" with opening cost, and each task instance is a "demand" that must be served. The UFL planner finds the minimal-cost set of actions (learn skills, ask preferences, delegate to human) to complete all tasks. COIL extends this with one-step belief space planning to decide when to query for preferences, and Bayesian inference to update teaching success probabilities online. The system replans after each action based on observed outcomes, adapting to failures and changing beliefs.

## Key Results
- Reduces human effort by 12%-20% in gridworld and 7DoF manipulation simulations compared to baselines
- Achieves 23% lower plan cost in physical conveyor experiments versus state-of-the-art methods
- Adapts online to teaching failures, updating plans to avoid wasting human effort on unlearnable tasks
- Maintains task completion while significantly reducing human interaction costs

## Why This Works (Mechanism)

### Mechanism 1
- Claim: A multi-task, multi-query planning problem can be formulated as an uncapacitated facility location (UFL) problem, enabling efficient, bounded-suboptimal planning.
- Mechanism: The robot's possible actions (learning a skill, asking for help, executing a known skill) are modeled as "facilities" with opening costs. Each task in a sequence is a "demand" that must be served. Opening a "skill facility" has a one-time cost (teaching) but can serve all future instances of that task type at a lower service cost (robot execution). Opening a "human help Facility" serves only the current task at a high cost. The UFL algorithm finds the set of facilities to "open" (skills to learn, tasks to delegate) that minimizes the total cost of serving all demands (completing all tasks).
- Core assumption: The task sequence is known or can be estimated beforehand. The costs of human interaction (`c_skill`, `c_pref`, `c_hum`) can be accurately modeled.
- Evidence anchors:
  - [abstract] "...formulate COIL as an uncapacitated facility location (UFL) problem, which enables bounded-suboptimal planning in polynomial time..."
  - [Page 4, Section IV-A] "We formulate the interaction planning problem as a facility location problem by defining demands, facilities, facility costs and service costs... Intuitively, the challenge of identifying the minimal cost set of interactive actions that cover the full task sequence maps nicely onto the UFL problem..."
  - [corpus] Corpus provides weak supporting evidence on LLM-assisted facility location planning but lacks direct evidence for this specific robotic UFL formulation.
- Break condition: The task sequence is completely unknown or cannot be predicted ahead of time, making the UFL formulation impossible. The costs for human interaction are highly dynamic and cannot be parameterized.

### Mechanism 2
- Claim: A one-step belief space planning extension allows the robot to efficiently decide when to query for user preferences to reduce uncertainty.
- Mechanism: The planner first generates a cost-optimal plan assuming user preferences are known (Mechanism 1). It then compares this plan's cost to the expected cost of a new plan that would be generated after asking a preference query and receiving the user's answer. If the expected cost of the new plan plus the cost of asking the query (`c_pref`) is lower than the original plan's cost, the robot asks for the preference. This is done only for the current task, keeping the computation tractable.
- Core assumption: User preferences can be modeled with a discrete set of parameters. The one-step lookahead is sufficient to capture the value of reducing preference uncertainty.
- Evidence anchors:
  - [abstract] "...extend our formulation to handle uncertainty in user preferences by incorporating one-step belief space planning, which uses these approximation algorithms as subroutines..."
  - [Page 5, Section IV-B] "COIL then determines when preference requests are needed to clarify user preferences before execution... If the expected plan cost plus preference request cost is lower than the current plan (line 22), COIL elects to first reduce uncertainty..."
  - [corpus] Corpus has weak or missing evidence for this specific one-step belief planning mechanism in interactive learning.
- Break condition: The space of possible user preferences is continuous or extremely large, making the one-step belief update and re-planning impractical. The value of preference information requires multi-step lookahead to be accurately assessed.

### Mechanism 3
- Claim: Modelling the success of teaching as a learnable probability (`λ_teach`) allows the robot to adapt its plan online and avoid wasting human effort on unlearnable tasks.
- Mechanism: The robot uses a Bayesian model to estimate the probability that a skill will be successfully learned from a human demonstration (`λ_teach`). This probability is incorporated into the skill return model (Equation 4). If a teaching attempt fails, the robot updates its belief and replans, likely deciding to delegate that task or similar ones to the human rather than asking for more demonstrations.
- Core assumption: The success or failure of teaching a skill provides information about the feasibility of learning similar skills in the future.
- Evidence anchors:
  - [Page 3, Section IV] "λ_teach estimates the probability that the robot will successfully learn a skill from the human. We model the success of teaching as a Bernoulli process and use Bayesian inference to compute the posterior distribution... This enables the robot to identify difficult-to-learn tasks and adapt its plan online if teaching fails..."
  - [Page 7, Section VI-C] "COIL Adaptivity in Simulated 7DoF Manipulation: The mug object is too wide for the robot gripper which results in a teaching failure... COIL takes this failure into account by updating λ_teach for all mugs and adapts its plan to assign mugs to the human."
  - [corpus] Corpus has weak or missing evidence for this specific adaptive failure handling in interactive robot teaching.
- Break condition: The success of teaching is highly stochastic and cannot be reliably predicted by a simple probabilistic model. Failures are caused by novel, unrelated reasons each time.

## Foundational Learning

- **Uncapacitated Facility Location (UFL) Problem**
  - Why needed here: This is the core combinatorial optimization problem COIL reduces its planning task to. Understanding the UFL formulation (demands, facilities, costs) is essential to understand how COIL generates a plan.
  - Quick check question: Can you explain the difference between the "facility cost" and the "service cost" in the UFL formulation and how they map to robot learning and execution?

- **Hidden-Parameter Markov Decision Process (HiP-MDP)**
  - Why needed here: Each task is modeled as a HiP-MDP, where the hidden parameters represent the user's unknown preferences. This formalism defines the structure of the problem the robot is trying to solve.
  - Quick check question: What do the hidden parameters (z) in the HiP-MDP represent in the context of this paper?

- **Belief Space Planning**
  - Why needed here: The robot must plan not just in the space of actions, but in the space of its beliefs about user preferences. The one-step extension to the UFL planner is a form of belief space planning.
  - Quick check question: Why does the paper use a *one-step* belief space planning approach instead of planning over the entire sequence of future uncertainties?

## Architecture Onboarding

- Component map:
  1. **HiP-MDP Task Model:** Represents each task with uncertain preference parameters.
  2. **Preference Belief Estimator:** A Bayesian filter that maintains a probability distribution (`b_Θ`) over user preferences for all tasks.
  3. **Skill Return Model:** A function that predicts the expected return (and probability of success) of executing a skill based on a task and a preference parameter.
  4. **UFL Planner:** The core component that solves the facility location problem to find the cost-optimal sequence of actions.
  5. **One-Step Belief Planner:** A module that decides whether to issue a preference query by comparing the current plan to an expected re-plan.
  6. **Interaction Manager:** The loop that executes actions, gets human feedback, and triggers replanning.

- Critical path: The performance of the entire system hinges on the **Skill Return Model**'s ability to predict generalization (`ρ_safe_π`). If this function is inaccurate, the UFL planner will make poor decisions about which skills to learn. The second critical point is the accurate specification of human **costs** (`c_skill`, `c_pref`, `c_hum`), which directly drive the UFL optimization.

- Design tradeoffs:
  - **Approximation Quality vs. Speed:** The paper uses a polynomial-time approximation algorithm for UFL, which guarantees a bounded-suboptimal solution. The tradeoff is that the plan is not guaranteed to be perfectly optimal.
  - **One-step Lookahead vs. Full Belief Planning:** The paper uses a one-step belief space planning approach for preference queries. This is far more tractable than solving the full POMDP but may be myopic and miss the value of information for future tasks.
  - **Pre-planning vs. Online Replanning:** The system requires knowing the task sequence to form an initial plan. It then replans online to handle failures, balancing upfront computation with real-world adaptation.

- Failure signatures:
  - **Excessive Queries:** The robot asks for a demonstration for a task that appears only once, or asks for a preference for a task it then delegates to the human. (Breaks cost-benefit reasoning).
  - **Repeated Failures:** The robot repeatedly asks for a demonstration on a task that has already failed to be taught, wasting human time. (Failure of the `λ_teach` update and replanning).
  - **Preference Misalignment:** The robot executes a skill according to a preference it believes is correct, but violates the user's true preference, incurring a high penalty. (Failure of belief estimation).

- First 3 experiments:
  1. **Ablation on Skill Generalization:** Systematically vary the accuracy of the `ρ_safe_π` function (the skill return model) and measure the resulting plan cost. This quantifies the sensitivity of the UFL planner to this core assumption.
  2. **Sensitivity to Human Cost Profiles:** Run COIL across a wide range of simulated cost profiles, not just the three in the paper. Identify breaking points where the system's behavior becomes irrational (e.g., where it always chooses the human or always tries to learn).
  3. **Robustness to Sequence Noise:** Perturb the task sequence given to the planner with random additions, deletions, and reorderings. Measure the degradation in plan quality and human workload compared to the ideal known sequence. This tests a key limitation mentioned in the paper.

## Open Questions the Paper Calls Out

- **Question:** Can the COIL framework be extended to plan effectively for multi-step tasks without relying solely on decomposition into independent sub-tasks?
  - **Basis in paper:** [explicit] The authors state, "In our future work, we are interested in extending our planner to multi-step tasks," and suggest decomposition as a potential approach.
  - **Why unresolved:** Decomposing complex tasks into sub-tasks may ignore temporal dependencies or context, potentially violating the "facility location" assumptions if the cost of one sub-task depends heavily on the state resulting from a previous one.
  - **What evidence would resolve it:** A modified UFL formulation or a hierarchical planner that handles sequential dependencies while maintaining polynomial-time performance guarantees.

- **Question:** How can the planner optimally schedule interactions for unordered task sets where the robot must determine the execution order?
  - **Basis in paper:** [explicit] The authors identify "plan[ning] for unordered sets of tasks" as a direction of interest, noting that it "introduces an additional challenge of scheduling."
  - **Why unresolved:** The current formulation assumes a fixed sequence of demands $\tau_1, \dots, \tau_n$. Allowing the algorithm to select the order increases the action space combinatorially, potentially rendering current approximation algorithms inefficient.
  - **What evidence would resolve it:** An algorithm that jointly optimizes task ordering and query selection, demonstrating scalability and cost reductions compared to fixed-sequence baselines.

## Limitations

- Requires knowing task sequences beforehand, limiting applicability in highly dynamic environments where future tasks are unpredictable
- One-step belief planning may be overly myopic, missing the compounding value of preference information across multiple future tasks
- Teaching success modeling depends on accurate task similarity metrics, with implementation details deferred to appendices

## Confidence

- **High confidence:** UFL formulation correctness, polynomial-time bounded-suboptimal planning guarantees, and comparative performance gains (12-23% reduction) against baselines in controlled experiments
- **Medium confidence:** One-step belief planning effectiveness and teaching adaptation mechanism, as supporting evidence is primarily from ablation studies rather than direct comparison to multi-step alternatives
- **Low confidence:** Generalization to continuous or high-dimensional preference spaces, and performance in environments with unpredictable task sequences or highly stochastic teaching outcomes

## Next Checks

1. **Multi-step lookahead comparison**: Implement and compare COIL against a version using full belief space planning (POMDP solver) on small-scale problems to quantify the performance gap from the one-step approximation

2. **Sequence prediction robustness**: Systematically corrupt the input task sequence with various noise models (deletions, reorderings, additions) and measure plan quality degradation versus oracle knowledge of the true sequence

3. **Teaching model sensitivity**: Conduct an ablation study varying the task similarity threshold (ε) and observe its impact on λ_teach adaptation quality and overall plan cost, particularly in cases where teaching fails intermittently