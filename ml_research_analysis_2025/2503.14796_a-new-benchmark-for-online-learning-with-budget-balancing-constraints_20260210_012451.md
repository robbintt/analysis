---
ver: rpa2
title: A New Benchmark for Online Learning with Budget-Balancing Constraints
arxiv_id: '2503.14796'
source_url: https://arxiv.org/abs/2503.14796
tags:
- regret
- time
- budget
- benchmark
- action
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces a new benchmark for the adversarial Bandit\
  \ with Knapsack (BwK) problem that overcomes the \"spend-or-save\" impossibility\
  \ result. The key innovation is using Earth Mover's Distance (EMD) to measure similarity\
  \ between spending patterns, defining the benchmark as the best strategy within\
  \ EMD o(T\xB2) of any sub-pacing spending pattern."
---

# A New Benchmark for Online Learning with Budget-Balancing Constraints

## Quick Facts
- **arXiv ID**: 2503.14796
- **Source URL**: https://arxiv.org/abs/2503.14796
- **Reference count**: 28
- **Primary result**: Introduces EMD-based benchmark for adversarial BwK that achieves sublinear regret O(√(T log|F| + D))

## Executive Summary
This paper addresses the fundamental impossibility of achieving sublinear regret in the adversarial Bandit with Knapsack (BwK) problem when competing against arbitrary spending patterns. The authors introduce a new benchmark framework based on Earth Mover's Distance (EMD) that allows algorithms to compete with spending patterns that are similar to a sub-pacing strategy. The key innovation is defining the benchmark as the best strategy within EMD o(T²) of any sub-pacing spending pattern, effectively relaxing the impossibility result while maintaining meaningful performance guarantees.

The proposed LagrangianEMD algorithm achieves sublinear regret against this EMD-based benchmark, with regret bounds of O(√(T log|F| + D)) where F is the set of strategies and D is the permitted EMD distance. This framework captures natural pacing benchmarks and resolves an open question about finding meaningful weaker benchmarks for BwK. The authors also prove a matching lower bound showing this regret is optimal, and demonstrate that EMD is the right metric for characterizing viable spending patterns in budget-constrained online learning.

## Method Summary
The authors introduce a novel benchmark framework for adversarial BwK using Earth Mover's Distance to measure similarity between spending patterns. The benchmark is defined as the best strategy within EMD o(T²) of any sub-pacing spending pattern, circumventing the spend-or-save impossibility result. They propose LagrangianEMD, a primal-dual algorithm that achieves sublinear regret O(√(T log|F| + D)) against this benchmark. The algorithm works by maintaining dual variables that balance reward maximization with budget constraints while allowing controlled deviation from any fixed pacing strategy. The framework generalizes to various pacing constraints and captures natural benchmarks like "pacing over windows" where strategies can change distributions every w rounds, achieving regret Õ(T/√w + √(wT)).

## Key Results
- Introduces EMD-based benchmark framework that circumvents the BwK impossibility result
- Proposes LagrangianEMD algorithm achieving sublinear regret O(√(T log|F| + D))
- Proves matching lower bound showing the regret bound is optimal
- Demonstrates EMD is the right metric for characterizing viable spending patterns
- Framework captures natural pacing benchmarks like "pacing over windows"

## Why This Works (Mechanism)
The EMD-based benchmark works by relaxing the requirement to compete with arbitrary spending patterns. Instead of competing with all possible spending patterns, the algorithm only needs to compete with patterns that are close (in EMD) to some sub-pacing strategy. This relaxation is sufficient to circumvent the impossibility result while still providing meaningful performance guarantees. The LagrangianEMD algorithm achieves this by maintaining dual variables that balance the trade-off between reward maximization and budget constraints, allowing controlled deviation from fixed pacing strategies.

## Foundational Learning
**Earth Mover's Distance (EMD)**: Measures the minimum cost of transforming one distribution into another. Needed to quantify similarity between spending patterns in a way that captures meaningful pacing constraints. Quick check: Verify EMD satisfies triangle inequality and is a valid metric.

**Bandit with Knapsack (BwK)**: Online learning problem where actions have both rewards and resource consumption. Needed as the foundational problem being addressed. Quick check: Confirm budget constraints are hard (not soft) and must be satisfied exactly.

**Primal-Dual Algorithms**: Optimization approach maintaining dual variables to balance competing objectives. Needed to implement the EMD-based benchmark algorithm. Quick check: Verify complementary slackness conditions hold at optimality.

**Sub-pacing Strategies**: Fixed spending patterns that allocate budget at constant rate. Needed as the reference point for measuring EMD distance. Quick check: Confirm sub-pacing strategies achieve optimal competitive ratio in certain settings.

## Architecture Onboarding

**Component Map**: LagrangianEMD -> EMD computation -> Dual variable update -> Action selection -> Reward/resource feedback loop

**Critical Path**: Action selection -> Resource consumption measurement -> EMD distance computation -> Dual variable update -> Next action selection

**Design Tradeoffs**: The EMD threshold O(T²) balances expressiveness of the benchmark against algorithmic complexity. Higher thresholds allow competing with more diverse spending patterns but increase computational overhead and require more careful dual variable management.

**Failure Signatures**: Large EMD distances between actual and benchmark spending patterns indicate poor algorithm performance. Persistent dual variable oscillation suggests instability in the primal-dual updates. Budget violations indicate the algorithm is not properly balancing reward maximization with resource constraints.

**First Experiments**:
1. Test LagrangianEMD on synthetic BwK instances with known optimal spending patterns to verify sublinear regret guarantees
2. Compare performance against baseline algorithms on varying EMD threshold values to identify optimal settings
3. Evaluate robustness to adversarial reward sequences and resource consumption patterns

## Open Questions the Paper Calls Out
None

## Limitations
- EMD metric choice may not generalize to all budget-constrained online learning scenarios
- O(T²) threshold for EMD distance is somewhat arbitrary and may not be optimal for all problem instances
- Framework focuses on adversarial setting, leaving open questions about stochastic environments
- Practical implementation details may be sensitive to problem parameters

## Confidence

**High Confidence**: The impossibility result for sublinear regret against general spending patterns in adversarial BwK is well-established. The EMD framework successfully circumvents this fundamental limitation. The matching upper and lower bounds for the EMD-based benchmark appear technically sound.

**Medium Confidence**: The specific algorithmic instantiation (LagrangianEMD) and its regret guarantees depend on careful implementation details that may be sensitive to problem parameters. The choice of EMD as the "right" metric, while theoretically justified, may have practical limitations in certain applications.

**Medium Confidence**: The characterization of viable spending patterns and the claim that EMD Ω(T²) patterns cannot both be benchmarked with sublinear regret requires careful examination of edge cases and potential pathological spending patterns.

## Next Checks

1. Implement LagrangianEMD on real-world bidding datasets (e.g., online advertising auctions) to empirically validate the sublinear regret guarantees and assess practical performance across different EMD threshold values.

2. Conduct experiments varying the EMD threshold O(T²) to identify problem-dependent optimal values and test whether the framework remains robust when the threshold is adjusted based on problem characteristics.

3. Extend the theoretical analysis to stochastic reward settings where reward distributions have known or learnable structure, testing whether the EMD-based benchmark framework can leverage this additional information to achieve improved performance.