---
ver: rpa2
title: Predicting Biased Human Decision-Making with Large Language Models in Conversational
  Settings
arxiv_id: '2601.11049'
source_url: https://arxiv.org/abs/2601.11049
tags:
- dialogue
- framing
- human
- choice
- cognitive
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Large language models (LLMs) can simulate human decision-making
  in conversational settings, including well-documented cognitive biases like Framing
  and Status Quo effects. In a pre-registered study (N = 1,648), participants completed
  six classic decision-making tasks via a chatbot with dialogues of varying complexity.
---

# Predicting Biased Human Decision-Making with Large Language Models in Conversational Settings

## Quick Facts
- arXiv ID: 2601.11049
- Source URL: https://arxiv.org/abs/2601.11049
- Reference count: 40
- Large language models can simulate human decision-making in conversational settings, including cognitive biases like Framing and Status Quo effects.

## Executive Summary
This pre-registered study (N=1,648) demonstrates that large language models (LLMs) can predict human decision-making in conversational settings while reproducing cognitive biases and their interactions with cognitive load. Participants completed six classic decision-making tasks via a chatbot with dialogues of varying complexity. Results showed that increased dialogue complexity selectively amplified Framing effects, demonstrating a load-bias interaction. LLM experiments revealed that models incorporating dialogue context predicted individual decisions more accurately than those using demographics alone, particularly for Goal Framing and Investment Decision Making. GPT-4.1 consistently outperformed other models in both predictive accuracy and fidelity to human-like bias patterns.

## Method Summary
The study combined human experiments with LLM simulations. Human participants (N=1,648) completed six choice problems (Risky-choice framing, Attribute framing, Goal framing, Budget allocation, Investment, College jobs) via a chatbot with either simple or complex prior dialogue. Cognitive load was measured using NASA-TLX mental demand scores. Six LLMs (GPT-4.1, GPT-4.1-mini, GPT-5, GPT-5-mini, gpt-oss-120b, llama4, qwen3) were tested across three prompting strategies (minimal role, natural response, explicit bias susceptibility) and three input conditions (choice problem only, with demographics, with dialogue context). Individual prediction accuracy and population-level bias reproduction were evaluated.

## Key Results
- Increased dialogue complexity selectively amplified Framing effects (Risky-choice and Goal framing) but not Status Quo bias, demonstrating load-bias interaction
- LLMs incorporating dialogue context predicted individual decisions more accurately than those using demographics alone, particularly for Goal Framing (47% → 63%) and Investment (62% → 76%)
- GPT-4.1 consistently outperformed GPT-5 and open-source models in both predictive accuracy and fidelity to human-like bias patterns
- Explicit bias prompting (HL3) caused false positives across all scenarios, while neutral prompting (HL1/HL2) better reproduced human bias patterns

## Why This Works (Mechanism)

### Mechanism 1: Load-Bias Amplification Through Prior Dialogue Complexity
- Claim: Prior dialogue complexity increases cognitive load, which selectively amplifies susceptibility to Framing effects but not Status Quo bias.
- Mechanism: Complex dialogues with nested referential structures and memory demands increase working memory load, reducing cognitive resources available for deliberative processing and increasing reliance on heuristic-driven biases like framing.
- Core assumption: NASA-TLX mental demand scores and recall accuracy are valid proxies for cognitive load; the effect generalizes beyond specific dialogue designs tested.
- Evidence anchors: Increased mental demand (d=0.85-1.08); significant interaction for Risky-choice and Goal framing; no interaction for Budget allocation, Investment, or College jobs.

### Mechanism 2: Dialogue Context Improves LLM Prediction Accuracy
- Claim: LLMs predict individual human decisions more accurately when provided with prior dialogue transcripts in addition to demographic information, particularly for Goal Framing and Investment decisions.
- Mechanism: Prior dialogue contains behavioral signals and contextual cues that inform the model about the participant's decision-making style, preferences, or cognitive state, enabling more personalized predictions than demographics alone.
- Core assumption: Improvement is due to contextual information rather than spurious correlations; ablation results support this interpretation.
- Evidence anchors: Accuracy improved from 47% to 63% for Goal Framing and 62% to 76% for Investment when adding dialogue context.

### Mechanism 3: Human-Likeness Prompting Aligns LLM Bias Patterns
- Claim: Neutral or naturalistic prompting (HL1, HL2) better reproduces human bias patterns than explicit bias instructions (HL3), which causes overestimation and false positives.
- Mechanism: Explicit instructions to exhibit cognitive biases cause models to over-apply bias patterns across all scenarios, including those where humans showed no bias, whereas neutral prompts allow the model's implicit patterns to emerge more naturally.
- Core assumption: Observed alignment reflects the model's internal representations rather than memorization of common experimental stimuli.
- Evidence anchors: HL3 accuracy was only 58% with strong biases across all problems; HL1/HL2 accuracy improved to 75% with better bias pattern alignment.

## Foundational Learning

- **Concept: Cognitive Load Theory (CLT)**
  - Why needed here: The paper uses dialogue complexity to manipulate cognitive load and tests its interaction with cognitive biases; understanding intrinsic, extraneous, and germane load is essential for interpreting the manipulation.
  - Quick check question: Can you explain why nested referential structures in dialogue increase germane load more than simple binary questions?

- **Concept: Framing Effect and Status Quo Bias**
  - Why needed here: These are the two primary cognitive biases investigated; the paper adapts classic behavioral economics choice problems to conversational settings.
  - Quick check question: What distinguishes Risky-choice framing from Attribute framing, and why might they interact differently with cognitive load?

- **Concept: Dual-Process Theory**
  - Why needed here: The paper interprets load-bias interactions through the lens of System 1 (fast, heuristic) versus System 2 (slow, deliberative) processing, with cognitive load shifting reliance toward System 1.
  - Quick check question: How would increased cognitive load theoretically affect the balance between System 1 and System 2 processing?

## Architecture Onboarding

- **Component map**: Dialogue Design Layer (Simple vs Complex) -> Choice Problem Layer (3 Framing + 3 Status Quo) -> LLM Simulation Layer (Prompt construction) -> Evaluation Layer (Individual accuracy + Sample-level bias reproduction)
- **Critical path**: Dialogue design → Human data collection (N=1,648) → NASA-TLX validation of load manipulation → LLM simulation with prompting variants → Accuracy and bias alignment evaluation → Ablation studies (demographics, memory, arithmetic components)
- **Design tradeoffs**: (1) Generalizability vs. control - Standardized dialogues enable experimental control but may limit ecological validity; (2) Model coverage vs. depth - Testing multiple models provides breadth but limited insight into why performance differs; (3) Prompt transparency vs. naturalness - HL3 explicitly mentions biases, improving load-bias alignment but causing false positives
- **Failure signatures**: (1) Low correlation with human z-scores - Model fails to reproduce load-bias interaction direction; (2) High accuracy but wrong bias direction - Model predicts choices accurately but fails to reproduce population-level bias patterns; (3) Prompt overfitting - HL3 produces strong biases across all problems, including those where humans show no bias
- **First 3 experiments**: 
  1. Validate cognitive load manipulation - Run Simple vs. Complex dialogue conditions and verify NASA-TLX mental demand scores differ significantly (target: d > 0.8)
  2. Establish baseline prediction accuracy - Test GPT-4.1 with Choice Problem Only, Without Prior Dialogue, and With Prior Dialogue conditions; confirm accuracy improves with dialogue context for Goal Framing and Investment decisions
  3. Test prompting strategies - Compare HL1, HL2, HL3 across all choice problems; verify HL3 causes false positives in Attribute Framing and Investment scenarios where humans showed no bias

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: Does increasing the number of decision alternatives (choice set size) induce a cognitive load interaction for Status Quo bias in conversational settings?
- **Basis in paper**: [Explicit] The authors speculate that the observed lack of interaction between dialogue complexity and Status Quo bias may be due to the "simplicity of our binary choice task," suggesting that more complex scenarios might reveal an interaction.
- **Why unresolved**: The study only utilized binary choices, which may not trigger the "choice overload" or extensive working memory demands typically associated with Status Quo bias under cognitive load.
- **What evidence would resolve it**: Replicating the experiment with choice problems containing 3 or more alternatives while manipulating prior dialogue complexity.

### Open Question 2
- **Question**: Can LLMs predict and reproduce cognitive biases that operate through distinct psychological mechanisms (e.g., Anchoring or Confirmation Bias) in conversational settings?
- **Basis in paper**: [Explicit] The authors state that because biases like Anchoring and Confirmation "may operate through distinct psychological mechanisms," the findings for Framing and Status Quo effects "should not be assumed to generalize to other biases without further targeted research."
- **Why unresolved**: The current study was restricted to Framing and Status Quo effects. It is unknown if the dialogue context aids prediction for biases that rely more heavily on specific memory retrieval or selective attention.
- **What evidence would resolve it**: Conducting parallel human and LLM experiments using conversational versions of classic Anchoring or Confirmation Bias tasks.

### Open Question 3
- **Question**: Do LLMs simulate biased decision-making via statistical pattern matching of known problems or through a structural emulation of cognitive load effects?
- **Basis in paper**: [Inferred] The Discussion notes that it "remains possible that these models are simply matching patterns based on learned statistical associations, especially given the widespread use of these choice problems in existing datasets."
- **Why unresolved**: High fidelity to human behavior on classic problems might indicate memorization/training data contamination rather than the LLM developing an internal model of human cognitive constraints.
- **What evidence would resolve it**: Testing LLMs on novel, unpublished decision-making scenarios designed to trigger biases, comparing their performance against human subjects with no prior exposure.

### Open Question 4
- **Question**: Can advanced prompting strategies improve LLM fidelity to "load-bias" interactions without inducing "alignment overfitting" (false positives)?
- **Basis in paper**: [Explicit] The authors note that while explicit bias prompting (HL3) improved alignment with load-bias interactions, it caused "alignment overfitting," leading to false positives where humans showed no bias.
- **Why unresolved**: There is a trade-off between instructing a model to be "human-like" (and thus biased) and over-instructing it to the point where it hallucinates bias in neutral scenarios.
- **What evidence would resolve it**: Developing prompts that encode cognitive parameters (e.g., "you have low working memory capacity") rather than explicit bias instructions, followed by an analysis of whether this induces bias selectively under load conditions.

## Limitations
- External validity is constrained by artificial conversational dialogues rather than real-world interactions
- The human dataset lacks demographic diversity information, limiting conclusions about equity in predictive performance
- The human-likeness prompting mechanism remains incompletely understood and may not generalize to other models or domains

## Confidence
- **High confidence**: The load-bias interaction effect is supported by significant NASA-TLX mental demand differences and statistical interaction patterns
- **Medium confidence**: GPT-4.1's superiority in predictive accuracy and bias reproduction is demonstrated, though reasons for its advantage remain unclear
- **Medium confidence**: The mechanism linking dialogue context to improved prediction accuracy is plausible but exact information pathways require further validation

## Next Checks
1. Replicate the load manipulation validation by testing whether the Complex dialogue condition produces significantly higher NASA-TLX mental demand scores than Simple (target d > 0.8)
2. Conduct perturbation analysis by replacing human utterances with random text to determine whether dialogue context improves predictions through behavioral signals or structural patterns
3. Test additional prompting strategies beyond HL1-3 to identify whether explicit bias instructions can be reformulated to reproduce human bias patterns without causing false positives