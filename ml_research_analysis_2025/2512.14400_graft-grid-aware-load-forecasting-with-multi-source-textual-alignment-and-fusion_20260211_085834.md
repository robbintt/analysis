---
ver: rpa2
title: 'GRAFT: Grid-Aware Load Forecasting with Multi-Source Textual Alignment and
  Fusion'
arxiv_id: '2512.14400'
source_url: https://arxiv.org/abs/2512.14400
tags:
- load
- news
- forecasting
- source
- external
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes GRAFT (GRid-Aware Forecasting with Text), a
  STanHOP-based model that improves grid-aware load forecasting by incorporating multi-source
  textual information (news, social media, and policy) through strict temporal alignment
  and position-aware cross-attention fusion. The key innovation is a plug-and-play
  external memory interface that encodes daily-aggregated texts as memory vectors,
  enabling dynamic retrieval conditioned on both time and region.
---

# GRAFT: Grid-Aware Load Forecasting with Multi-Source Textual Alignment and Fusion

## Quick Facts
- arXiv ID: 2512.14400
- Source URL: https://arxiv.org/abs/2512.14400
- Reference count: 40
- Key outcome: GRAFT achieves significant accuracy gains over strong baselines and the state of the art across multiple Australian states and forecasting horizons, with interpretable text-to-load attribution and a released open benchmark.

## Executive Summary
This paper introduces GRAFT, a grid-aware load forecasting model that integrates multi-source textual information (news, social media, policy) with half-hourly electric load data. The key innovation is a plug-and-play external memory interface that encodes daily-aggregated texts as memory vectors, enabling dynamic retrieval conditioned on both time and region. GRAFT leverages strict temporal alignment and position-aware cross-attention fusion to achieve significant accuracy gains over strong baselines and the state of the art across multiple Australian states and forecasting horizons. The model also provides interpretable attribution of text-to-load effects via source-gating and attention weight visualization.

## Method Summary
GRAFT extends STanHOP with an external memory interface that encodes daily-aggregated textual information (news, social media, policy) into fixed-length vectors. These text memories are aligned with half-hour load data by date-region pairs through same-day broadcasting with explicit masking for missing entries. Cross-attention retrieval uses α-EntMax sparse attention between load queries and gated text memories, with source gating enabling adaptive multi-source fusion weighted by learned sparse attention. The model is trained end-to-end with MSE, quantile loss, and entropy regularization, achieving significant accuracy gains over baselines.

## Key Results
- Significant RMSE/MAE/MAPE improvements over NoExt baseline and state-of-the-art models across all 5 Australian states
- Source gating enables interpretable attribution: News excels for hourly event corrections, Reddit for daily behavioral rhythms, Policy for monthly trends
- Visual attention analysis shows clear text-to-load effects during extreme events and holidays
- Released an open, aligned benchmark for reproducible evaluation of text-guided load forecasting

## Why This Works (Mechanism)

### Mechanism 1: Temporally-Aligned External Memory Encoding
Daily-aggregated textual information (news, social media, policy) enhances load forecasting when strictly aligned to load data by date-region pairs. Texts are encoded via Sentence-BERT into fixed-length vectors, aggregated daily with relevance weights, and aligned to the half-hour load axis through "same-day broadcasting" with explicit masking for missing entries to prevent spurious signals from entering attention readout.

### Mechanism 2: Sparse Cross-Attention Retrieval via α-EntMax
Sparse attention (α-EntMax instead of softmax) enables selective retrieval of relevant text memory entries, reducing noise propagation from irrelevant or weakly-related sources. Generalized Sparse Hopfield attention replaces softmax with α-EntMax, producing row-sparse retrieval distributions that concentrate mass on dominant entries.

### Mechanism 3: Source Gating for Adaptive Multi-Source Fusion
Different text sources (News, Reddit, Policy) have time-scale-specific strengths—News excels at hourly event-driven corrections, Reddit captures daily behavioral rhythms, Policy informs monthly trends—and can be adaptively weighted via learned gating. A gating vector γ ∈ Δ³ is computed from aggregated load and text representations via α-EntMax, producing sparse weights for each source.

## Foundational Learning

- Concept: **Hopfield Networks and Modern Sparse Variants**
  - Why needed here: GRAFT builds on STanHOP, which uses Generalized Sparse Hopfield (GSH) networks as its attention mechanism. Understanding the energy-based retrieval formulation, α-EntMax sparsity, and Lyapunov convergence is essential to debug retrieval behavior.
  - Quick check question: Why does α-EntMax produce sparser attention weights than softmax, and what does Theorem 2 (Lyapunov monotonicity) guarantee about the retrieval iteration?

- Concept: **Cross-Attention for Multi-Modal Fusion**
  - Why needed here: GRAFT uses load representations as queries (Q) and text memories as keys (K) and values (V). Understanding cross-attention mechanics—distinct from self-attention—is necessary to implement and debug the retrieval and injection pipeline.
  - Quick check question: In Eq. 26-27, what happens if the query dimension (from STanHOP backbone) differs from the key dimension (from text projection), and how do W_Q, W_K matrices resolve this?

- Concept: **Temporal Alignment in Multi-Resolution Data**
  - Why needed here: The paper aggregates text to daily resolution and broadcasts to half-hourly load via "same-day broadcasting." Understanding alignment strategies, timezone handling (AEST/AEDT), and masking for missing data is critical for real-world deployment.
  - Quick check question: A policy document is released on Friday but references a rule change effective the following Monday. How should the time-decay factor ρ^Δt (Eq. 3) handle this, and what mask value should be assigned?

## Architecture Onboarding

- Component map: Text (raw) → [SBERT + daily aggregation] → Y_text → [source gating via α-EntMax] → ỹ_text → [cross-attention: Q from load, K/V from text] → Z_text → [residual fusion with STanHOP output] → [decoder] → ŷ

- Critical path: Load (raw) → [normalization, timezone alignment] → [STanHOP: TimeGSH → SeriesGSH → coarsening] → R (queries) → [cross-attention with text memories] → Z_text → [residual fusion] → [decoder] → ŷ

- Design tradeoffs:
  1. Daily vs. intra-day text resolution: Daily aggregation simplifies alignment and reproducibility but may miss fine-grained event timing within a day.
  2. Sparse vs. dense retrieval: α-EntMax reduces noise but requires tuning α (paper uses 1≤α≤2); too sparse may miss relevant signals.
  3. PlugMemory vs. TuneMemory: PlugMemory (Eq. 19) is zero-shot with frozen backbone; TuneMemory (Eq. 20-22) uses weakly labeled "event-response" memories for rare scenarios but adds complexity.
  4. Multi-source vs. single-source: "All" configuration gives best robustness (lowest RankRMSE, highest Wins) but may dilute strong single-source signals; use gating to adaptively emphasize News/Reddit during eventful periods.

- Failure signatures:
  1. NaN/Inf in loss or outputs: Likely numerical instability in α-EntMax with extreme β or unnormalized inputs. Apply L2 gradient clipping (threshold 1.0 per paper).
  2. Gating collapse to single source: γ converges to near-deterministic weights. Increase entropy regularization λ_γ or check source diversity.
  3. No improvement over NoExt baseline: Verify text-load alignment (timezone, masking), ensure non-empty text for test dates, check that cross-attention is actually retrieving non-zero attention weights.
  4. High MAPE but low RMSE: Model over-fits high-load periods at expense of relative error in low-load segments. Adjust quantile loss weights or constrain text injection magnitude.

- First 3 experiments:
  1. Ablation on text sources: Train GRAFT with News-only, Reddit-only, Policy-only, and All configurations. Compare RMSE/MAE/MAPE across hourly, daily, and monthly horizons to replicate Table 1a-1d and validate source-scale correspondence.
  2. Cross-attention vs. static concatenation: Replace position-aware cross-attention (Eq. 26) with early concatenation of text embeddings to load features. Expect performance degradation, especially in event-driven windows (Figure 8-10).
  3. Alignment sensitivity test: Simulate publication delays by shifting text availability forward by 1-7 days with appropriate decay. Measure Skill degradation to quantify robustness of alignment mechanism.

## Open Questions the Paper Calls Out

- **Cross-modal generalization**: Can GRAFT's architecture achieve comparable performance when transferred to power grids with different market structures, regulatory frameworks, and multilingual text environments (e.g., European or Asian markets)?

- **Fine-grained text encoding**: Would incorporating higher temporal resolution text encoding (hourly or finer) with timestamp-retrieval mechanisms yield significant accuracy gains for very short-term forecasting?

- **Probabilistic forecasting**: Can GRAFT be extended to probabilistic load forecasting with well-calibrated prediction intervals that incorporate textual uncertainty signals?

- **Operational integration**: How should text-guided load forecasts be integrated into downstream operational decisions (demand response dispatch, ancillary service allocation) to achieve measurable economic or reliability improvements?

## Limitations
- Cross-modal generalization beyond Australian grid contexts remains unproven; performance may degrade in regions with different text-load relationships or temporal patterns
- Cross-attention parameter tuning (α for α-EntMax and β temperature) is not explicitly specified, introducing variability in retrieval behavior across implementations
- Regularization weights for quantile loss and entropy gating are missing from the paper, affecting convergence and source selection stability

## Confidence
- **High confidence** in the alignment mechanism and source-gating architecture based on detailed equations and ablation studies
- **Medium confidence** in the sparse attention formulation due to lack of direct baseline comparisons for α-EntMax in load forecasting
- **Medium confidence** in the benchmark claims pending independent reproduction of the full pipeline with exact hyperparameters

## Next Checks
1. Hyperparameter sensitivity analysis: Systematically vary α (1.0-2.0) and β in α-EntMax to identify optimal sparsity levels and test robustness to tuning choices
2. Zero-shot transfer test: Apply the pre-trained model to load forecasting in a different geographic region (e.g., US ISOs) without fine-tuning to assess cross-domain generalization
3. Publication delay robustness: Simulate 1-7 day text availability delays with varying decay factors to quantify alignment mechanism tolerance to temporal offsets