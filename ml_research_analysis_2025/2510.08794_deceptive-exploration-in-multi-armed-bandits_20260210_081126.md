---
ver: rpa2
title: Deceptive Exploration in Multi-armed Bandits
arxiv_id: '2510.08794'
source_url: https://arxiv.org/abs/2510.08794
tags:
- agent
- best
- public
- private
- algorithm
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses deceptive exploration in multi-armed bandits,
  where an agent aims to identify the best private arm while appearing to optimize
  public rewards under a Kullback-Leibler (KL) divergence constraint. The authors
  formalize this as a constrained best arm identification problem with history-dependent
  costs, where pulling suboptimal arms becomes increasingly difficult.
---

# Deceptive Exploration in Multi-armed Bandits

## Quick Facts
- arXiv ID: 2510.08794
- Source URL: https://arxiv.org/abs/2510.08794
- Reference count: 40
- Primary result: Under KL divergence constraints, deceptive exploration achieves Θ(√T) rate for pulling public suboptimal arms versus O(log T) for Thompson Sampling.

## Executive Summary
This paper addresses deceptive exploration in multi-armed bandits where an agent aims to identify the best private arm while appearing to optimize public rewards under a KL divergence constraint. The authors formalize this as a constrained best arm identification problem where pulling suboptimal arms becomes increasingly difficult. They prove that under the KL constraint, the agent can explore public suboptimal arms at most at a Θ(√T) rate, significantly improving upon the O(log T) rate of Thompson Sampling. The paper formulates a maximin problem to characterize the optimal error exponent for private best arm identification and proposes a top-two sampling algorithm that adapts exploration based on public suboptimality gaps.

## Method Summary
The method uses Thompson Sampling as a reference algorithm, with the agent computing actual pull probabilities under a KL divergence constraint. For each arm, the agent solves a KL-constrained optimization to boost the probability of pulling that arm from exponentially small to harmonically small. The approach maintains separate posteriors for public and private rewards, uses Gauss-Hermite quadrature for numerical integration, and implements a top-two sampling strategy where the leader is sampled from the private posterior and the challenger is selected proportionally to optimality under the private posterior. A maximin problem determines optimal long-term boosting proportions to achieve information balance across competitor arms.

## Key Results
- Under KL constraints, the agent can pull public suboptimal arms at most at a Θ(√T) rate, with the constant inversely proportional to the public suboptimality gap
- The achievable error exponent for best private arm identification is characterized by a maximin problem whose solution determines optimal boosting proportions
- Numerical experiments validate the theoretical rates and demonstrate the algorithm's effectiveness in balancing information gain with pulling difficulty across asymmetric arms

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** The agent can boost the probability of pulling a public suboptimal arm from exponentially small (exp(-N)) to harmonically small (1/N) using a per-step KL divergence budget.
- **Mechanism:** Thompson Sampling assigns strictly positive probability to every arm due to posterior uncertainty. By solving max_q q subject to d_KL(Ber(q), Ber(p)) ≤ ε, the agent can amplify a reference probability p to approximately q*(p) ≈ ε/log(ε/p), converting exponential decay into harmonic decay.
- **Core assumption:** The reference algorithm is Thompson Sampling (randomized), and each arm is pulled infinitely often so posterior concentration holds.
- **Evidence anchors:**
  - [abstract] "We formalize detectability as a stepwise Kullback-Leibler (KL) divergence constraint between the actual pull probabilities used by the agent and the anticipated pull probabilities by the observer."
  - [section] Lemma 1 proves lim_{p→0} log(ε/p) / (ε/q*(p)) = 1, showing the boosting capacity under KL constraint.
  - [corpus] Related work on Thompson Sampling dynamics (arXiv:2601.21131) provides context on arm-pull dynamics but does not address KL-constrained deception.
- **Break condition:** If the reference algorithm becomes deterministic (e.g., pure UCB without randomization), the agent cannot boost probabilities without detectable deviation.

### Mechanism 2
- **Claim:** The achievable rate of pulling a public suboptimal arm under repeated boosting is Θ(√T), with the constant inversely proportional to the public suboptimality gap.
- **Mechanism:** Each successful pull of arm a increases N_{a,t}, which reduces future pull probability. Model this as Bernoulli trials with success probability ≈ c/(M_t + M_0) where M_t is prior successes. Lemma 2 shows M_T/(√(2cT)) → 1 almost surely, yielding the √T rate.
- **Core assumption:** The KL budget ε is fixed and small; the public optimal arm is pulled almost exclusively as T → ∞ so N_{1,t}/N_{a,t} diverges.
- **Evidence anchors:**
  - [abstract] "We show these pulls can happen at most at a Θ(√T) rate under the KL constraint."
  - [section] Theorem 1: lim_{T→∞} N_{a,T}/√T = √(4εσ²/(μ_1 - μ_a)²) a.s.
  - [corpus] No direct corpus evidence on √T exploration rates under KL constraints; existing bandit literature focuses on O(log T) regret minimization.
- **Break condition:** If ε scales with T or if public/private means are perfectly aligned, the asymmetry assumption fails and the rate analysis changes.

### Mechanism 3
- **Claim:** Optimal deceptive exploration requires information balance—equal evidence across competitor arms—achieved via a maximin problem whose solution determines long-term boosting proportions.
- **Mechanism:** The error exponent for best private arm identification is governed by min_{a≠1} √(w₁w_a)(Δ_priv^a)²/(√(w₁)d_a + √(w_a)d_1). Maximizing this concave objective yields optimal allocation w* satisfying g_a(x*_a) = y* for all competing arms, formalizing information balance.
- **Core assumption:** The best public arm i* ≠ best private arm 1; otherwise, no deception is needed. Gaussian rewards with known variance.
- **Evidence anchors:**
  - [abstract] "We formulate a maximin problem whose solution characterizes the optimal error exponent for best private arm identification."
  - [section] Eq. (8)-(9) define Γ(w) and the concave maximization; Algorithm 1 implements top-two sampling with information-directed selection.
  - [corpus] Related work on top-two algorithms (Russo 2016, You et al. 2023 cited in paper) supports information-directed selection but not under KL constraints.
- **Break condition:** If public and private reward structures are highly misaligned, the optimization may require boosting the public optimal arm more than analytically predicted.

## Foundational Learning

- **Concept:** Thompson Sampling posterior dynamics
  - **Why needed here:** Understanding how reference probabilities π_{a,t} decay exponentially with pulls is essential to analyze what boosting achieves.
  - **Quick check question:** Given a Gaussian posterior N(μ̂, σ²/N), what is the probability that a sample exceeds the public optimal arm's mean by β standard deviations?

- **Concept:** KL divergence between Bernoulli distributions
  - **Why needed here:** The detectability constraint is formalized as d_KL(Ber(q), Ber(p)) ≤ ε; understanding this geometry is critical for the boosting optimization.
  - **Quick check question:** For fixed ε, what is the maximum achievable q when p = 10⁻⁶?

- **Concept:** Best arm identification vs. regret minimization
  - **Why needed here:** Deceptive exploration targets pure exploration (identify best private arm) while appearing to minimize regret; the O(log T) vs. Θ(√T) distinction is central.
  - **Quick check question:** Why does Thompson Sampling explore suboptimal arms at O(log T) rate, and why is this insufficient for best arm identification?

## Architecture Onboarding

- **Component map:** Posterior tracker -> Reference probability estimator -> Top-two selector -> Boosting optimizer -> Stopping criterion
- **Critical path:** 1. Pull arm A_t ~ π'_{ref,t} (boosted distribution). 2. Observe (X_t^{pub}, X_t^{priv}), update both posteriors. 3. Recompute π_{ref,t+1} and check stopping criterion. The bottleneck is computing π_{ref,t} at each step; Gauss-Hermite quadrature provides O(K²) complexity per step.
- **Design tradeoffs:**
  - Numerical integration vs. sampling: Gauss-Hermite quadrature is deterministic and accurate for Gaussian posteriors; Monte Carlo sampling is more general but noisier.
  - Exact q*(p) vs. approximation: Exact solution requires bisection; analytical approximation q̂(p) = max{ε/W(ε/p), p} speeds up experiments but may under-boost for moderate p.
  - Tracking-based vs. top-two: Tracking-based algorithms (solve w* at each step) are theoretically optimal but computationally heavier; top-two is simpler and asymptotically equivalent.
- **Failure signatures:**
  - Insufficient exploration: If ε is too small, error probability 1 - p*_t decays too slowly; verify by checking that Γ(w_t) converges to Γ*.
  - Detectability breach: If actual pull frequencies deviate significantly from π_{ref,t}, observer may detect deception; monitor empirical KL divergence.
  - Non-convergence: If posteriors do not concentrate (e.g., variance underestimated), leader/challenger selection becomes unstable.
- **First 3 experiments:**
  1. Validate √T rate: Fix ε = 0.1, uniform boosting proportions w_a = 1/(K-1), plot N_{a,T}/φ_{a,T} vs. time for multiple arms to confirm Theorem 1.
  2. Vary KL budget: Run Algorithm 1 with ε ∈ {0, 10⁻³, 10⁻², 10⁻¹, 1, ∞} on a fixed bandit instance; plot 1 - p*_t vs. time to show faster decay with larger ε.
  3. Test asymmetry adaptation: Fix private means, vary public suboptimality gaps; verify that Algorithm 1 compensates for harder-to-pull arms by boosting easier arms more frequently, matching the information balance condition.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do the deceptive exploration rates and optimal strategies change when extending from best arm identification to regret minimization objectives under the same KL constraint?
- Basis in paper: [explicit] "Future work involves extending our results to regret minimization or other pure exploration problems."
- Why unresolved: The current analysis and algorithm are specifically designed for pure exploration, where the objective is to identify the best private arm with high confidence. Regret minimization requires balancing exploration and exploitation differently, and the Θ(√T) exploration rate may not directly translate.
- What evidence would resolve it: A theoretical characterization of achievable regret under KL constraints, potentially with different rate behavior than the √T exploration rate established for best arm identification.

### Open Question 2
- Question: What are the optimal deceptive exploration strategies when the agent does not directly observe private rewards, but public and private rewards are correlated?
- Basis in paper: [explicit] "Another possible direction is studying deceptive exploration in scenarios with more complex information structures, for instance, a case in which the agent does not directly observe private rewards, but public and private rewards are correlated."
- Why unresolved: The current framework assumes the agent observes both public and private rewards directly. Correlated structures create different inference problems and may require fundamentally different exploration strategies.
- What evidence would resolve it: A modified algorithm and theoretical analysis for correlated reward structures, with characterization of how correlation strength affects the achievable error exponent.

### Open Question 3
- Question: What are the finite-time guarantees for Algorithm 1, beyond the asymptotic convergence results presented?
- Basis in paper: [inferred] The paper provides asymptotic results for the Θ(√T) rate and shows Γ(w_t) converges to Γ* numerically (Figure 4), but does not provide finite-time bounds on the error probability or convergence rate of the algorithm.
- Why unresolved: The analysis focuses on asymptotic behavior as T→∞. Finite-time performance bounds are essential for practical deployment but require different analytical techniques.
- What evidence would resolve it: Non-asymptotic bounds on the error probability and sample complexity for Algorithm 1 under the KL constraint.

## Limitations
- The analysis assumes Gaussian rewards with known variance, limiting applicability to other reward distributions
- The paper provides asymptotic guarantees but lacks finite-time performance bounds for practical deployment
- The relationship between the KL budget ε and the overall detectability of the agent's behavior over entire trajectories is not fully characterized

## Confidence

- **High confidence:** The theoretical framework connecting KL-constrained boosting to the Θ(√T) exploration rate (Theorem 1) is well-founded given the supporting Lemma 2 and empirical validation in Figure 2.
- **Medium confidence:** The maximin formulation for optimal information balance (Γ(w) maximization) provides a principled approach, though its practical performance relative to tracking-based algorithms requires further comparison.
- **Low confidence:** The claim about Algorithm 1's asymptotic equivalence to tracking-based methods lacks rigorous proof, and the computational complexity of tracking-based approaches is not fully explored.

## Next Checks

1. **Rate verification:** Run Algorithm 1 with K=4, μ^pub=(0.6,0.3,0.0,0.2), ε=0.1, equal boosting proportions, and 50 seeds. Plot N_a,T/√T for each arm to confirm Theorem 1's Θ(√T) prediction.

2. **KL budget impact:** Test Algorithm 1 with ε ∈ {0, 10⁻³, 10⁻², 10⁻¹, 1, ∞} on μ^priv=(0.2,0.5,0.1,0.0). Measure error probability decay 1-p*_t and compare against O(log T) Thompson Sampling baseline.

3. **Asymmetry adaptation:** Fix private means and vary public suboptimality gaps. Verify Algorithm 1's boosting proportions satisfy the information balance condition by checking that g_a(x*_a) = y* across all competing arms.