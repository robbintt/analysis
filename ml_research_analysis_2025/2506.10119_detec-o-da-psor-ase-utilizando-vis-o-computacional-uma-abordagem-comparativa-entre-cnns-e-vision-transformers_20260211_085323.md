---
ver: rpa2
title: "Detec\xE7\xE3o da Psor\xEDase Utilizando Vis\xE3o Computacional: Uma Abordagem\
  \ Comparativa Entre CNNs e Vision Transformers"
arxiv_id: '2506.10119'
source_url: https://arxiv.org/abs/2506.10119
tags:
- para
- dados
- modelo
- imagens
- psor
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper compares CNNs and Vision Transformers for multi-classifying
  psoriasis and similar skin diseases. Pre-trained models on ImageNet were adapted
  to a dataset of 3,843 images.
---

# Detecção da Psoríase Utilizando Visão Computacional: Uma Abordagem Comparativa Entre CNNs e Vision Transformers

## Quick Facts
- **arXiv ID**: 2506.10119
- **Source URL**: https://arxiv.org/abs/2506.10119
- **Reference count**: 0
- **Primary result**: Vision Transformers outperform CNNs for psoriasis classification, with DaViT-B achieving 96.4% F1-score and 97% accuracy on psoriasis cases

## Executive Summary
This study compares Convolutional Neural Networks (CNNs) and Vision Transformers (ViTs) for multi-classifying psoriasis and similar skin diseases using a dataset of 3,843 images. Pre-trained models on ImageNet were adapted for this dermatological classification task. Both model families demonstrated high predictive performance, but Vision Transformers showed superior efficiency with smaller model sizes. The DaViT-B model achieved the highest performance with an F1-score of 96.4% and correctly classified 97% of psoriasis cases, establishing ViTs as more efficient for automated psoriasis detection.

## Method Summary
The study employed pre-trained CNN and Vision Transformer models on ImageNet, which were then fine-tuned on a dermatological dataset containing 3,843 images of psoriasis and similar skin diseases. The experimental framework compared multiple model architectures from both families, evaluating their performance using standard classification metrics. The focus was on adapting existing pre-trained models rather than training from scratch, leveraging transfer learning to achieve high performance in medical image classification.

## Key Results
- DaViT-B achieved the best performance with 96.4% F1-score
- Vision Transformers demonstrated superior performance with smaller model sizes compared to CNNs
- 97% accuracy in correctly classifying psoriasis cases
- Both CNNs and ViTs achieved high predictive performance overall

## Why This Works (Mechanism)
The superior performance of Vision Transformers for psoriasis detection can be attributed to their ability to capture global contextual information through self-attention mechanisms, which is particularly valuable for understanding the spatial patterns and characteristics of skin lesions. Unlike CNNs that process images through local receptive fields, ViTs can establish relationships between distant regions of the skin image, enabling better differentiation between psoriasis and other similar dermatological conditions.

## Foundational Learning
- **Transfer Learning**: Adapting pre-trained models from ImageNet to dermatological tasks reduces data requirements and improves performance. *Why needed*: Medical imaging datasets are often limited; *Quick check*: Verify model performance with and without pre-training.
- **Multi-class Classification**: Distinguishing between multiple similar skin conditions requires robust feature extraction and discriminative power. *Why needed*: Psoriasis shares visual characteristics with other skin diseases; *Quick check*: Confusion matrix analysis.
- **Vision Transformer Architecture**: Self-attention mechanisms capture global image context effectively. *Why needed*: Skin lesions require understanding of spatial relationships; *Quick check*: Compare with CNN performance on same task.

## Architecture Onboarding
**Component Map**: Input Image -> Transformer Encoder (Self-Attention + Feed-Forward) -> Classification Head
**Critical Path**: Image tokenization → Self-attention computation → Multi-head attention aggregation → Feed-forward networks → Classification output
**Design Tradeoffs**: ViTs require more computational resources during training but achieve better performance with smaller models, while CNNs are more computationally efficient at inference but need larger architectures for comparable performance.
**Failure Signatures**: ViTs may struggle with very small datasets due to their parameter count; CNNs may fail to capture long-range dependencies crucial for lesion characterization.
**First Experiments**: 1) Fine-tune DaViT-B on the dataset with different learning rates; 2) Compare attention map visualizations between successful and failed classifications; 3) Test model robustness to image quality variations.

## Open Questions the Paper Calls Out
None

## Limitations
- Dataset composition details are unclear, including class balance and demographic representation
- Performance metrics lack confidence intervals and statistical significance testing
- No external validation on independent datasets for clinical reliability
- Absence of explainability analysis or attention visualization

## Confidence
- Superior ViT performance for psoriasis classification: **High**
- CNN vs ViT comparison validity: **Medium**
- Generalizability to other medical imaging tasks: **Low**

## Next Checks
1. External validation on multiple independent dermatological datasets with different acquisition protocols
2. Ablation studies comparing ImageNet pre-training versus dermatology-specific pre-training
3. Clinical workflow integration testing including inference speed, resource requirements, and user acceptance studies