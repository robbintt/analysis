---
ver: rpa2
title: Majorization-Minimization Dual Stagewise Algorithm for Generalized Lasso
arxiv_id: '2501.02197'
source_url: https://arxiv.org/abs/2501.02197
tags:
- problem
- dual
- step
- lasso
- algorithm
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper addresses efficient path-following algorithms for generalized
  lasso problems with general convex loss functions, such as logistic regression and
  Cox regression. The proposed MM-DUST algorithm combines majorization-minimization
  (MM) with dual-stagewise updates: it majorizes the original loss using quadratic
  approximations, then solves the resulting problem in the dual space via simple coordinate-wise
  updates with a small step size.'
---

# Majorization-Minimization Dual Stagewise Algorithm for Generalized Lasso

## Quick Facts
- arXiv ID: 2501.02197
- Source URL: https://arxiv.org/abs/2501.02197
- Authors: Jianmin Chen; Kun Chen
- Reference count: 40
- The paper proposes MM-DUST, an efficient path-following algorithm for generalized lasso with general convex losses, achieving comparable prediction performance to SPG while substantially reducing execution time.

## Executive Summary
This paper addresses efficient path-following algorithms for generalized lasso problems with general convex loss functions, such as logistic regression and Cox regression. The proposed MM-DUST algorithm combines majorization-minimization (MM) with dual-stagewise updates: it majorizes the original loss using quadratic approximations, then solves the resulting problem in the dual space via simple coordinate-wise updates with a small step size. This slow-brewing approach enables a trade-off between statistical accuracy and computational efficiency. Theoretical analysis establishes uniform convergence of the approximate paths to the exact ones as the step size decreases, and computational complexity is reduced compared to gradient-based methods.

## Method Summary
MM-DUST solves generalized lasso problems by first majorizing the convex loss with a quadratic surrogate, then transforming to the dual space where the non-separable ℓ₁ penalty becomes simple box constraints. The algorithm performs coordinate-wise stagewise updates in the dual space with a small step size ε, then recovers the primal solution via stationarity conditions. Key parameters include the number of majorization steps per λ (Nm), dual iterations (Nd), and step size ε. The method includes early stopping based on AIC/BIC criteria and supports various loss functions through appropriate majorizers.

## Key Results
- MM-DUST achieves comparable or better prediction performance than SPG across simulations and real data applications
- The algorithm substantially reduces execution time, especially in high-dimensional or sparse scenarios
- Theoretical analysis establishes uniform convergence of approximate paths to exact generalized lasso paths as step size decreases
- Computational complexity is reduced compared to gradient-based methods, with O(Nm p(nk + Nd m)) scaling for large-scale problems

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Quadratic majorization enables handling arbitrary convex loss functions through surrogate optimization.
- **Mechanism:** The algorithm constructs an upper-bounding quadratic function $\tilde{f}(\beta|\beta_0) = f(\beta_0) + \nabla f(\beta_0)^T(\beta - \beta_0) + \frac{1}{2}(\beta - \beta_0)^T M(\beta - \beta_0)$ where $M \succeq \nabla^2 f(\beta)$ for all $\beta$. This transforms non-quadratic losses (logistic, Cox) into solvable least-squares subproblems.
- **Core assumption:** The loss function is convex and twice continuously differentiable with Lipschitz-continuous gradient.
- **Evidence anchors:**
  - [abstract] "the majorization technique is incorporated to handle different convex loss functions through their quadratic majorizers"
  - [section 3.2] "we approximate the original loss function with a quadratic majorizer and optimize the approximated problem"
  - [corpus] Related work on MM algorithms for structured sparse regression validates this approach
- **Break condition:** When the loss function is non-convex or non-smooth, the majorizer construction may fail to guarantee descent.

### Mechanism 2
- **Claim:** Dual-space transformation converts the non-separable ℓ₁ penalty into a simple box constraint.
- **Mechanism:** The primal problem $\min_\beta f(\beta) + \lambda\|D\beta\|_1$ transforms to dual $\min_u f^*(-D^T u)$ subject to $\|u\|_\infty \leq \lambda$. The non-separable penalty becomes separable box constraints, enabling coordinate-wise optimization.
- **Core assumption:** The convex conjugate $f^*(\cdot)$ exists and is tractable (or can be majorized when intractable).
- **Evidence anchors:**
  - [abstract] "utilizing the connection between primal and dual problems... the minimization step is carried out in the dual space"
  - [section 2.1] "the dual problem of (3) has the following form minimize_u f^*(-D^T u), s.t. $\|u\|_\infty \leq \lambda$"
  - [corpus] Standard duality for generalized lasso established in Tibshirani & Taylor (2011); corpus lacks direct validation for general loss functions
- **Break condition:** When $D$ is not full column rank and $\nabla f$ cannot be easily inverted, recovering primal from dual becomes ill-posed.

### Mechanism 3
- **Claim:** Small step-size stagewise updates trade solution accuracy for computational efficiency with provable convergence.
- **Mechanism:** Rather than fully solving each dual subproblem, the algorithm takes $N_d$ coordinate descent steps with step size $\varepsilon$, selecting the direction of steepest descent. As $\varepsilon \to 0$, the approximate path uniformly converges to the exact solution path (Theorem 3.1).
- **Core assumption:** The dual iterations reach a neighborhood where no descent direction exists within the $\varepsilon$-ball.
- **Evidence anchors:**
  - [abstract] "selecting an appropriate step size enables a trade-off between statistical accuracy and computational efficiency"
  - [section 3.3, Theorem 3.1] "as the step size $\varepsilon \to 0$, $|N_0 - N_d| = O(1)$ and when we take $N_m = 1$, the MM-DUST paths converge to the generalized lasso paths uniformly"
  - [corpus] Stagewise learning trade-offs validated in Zhao & Yu (2007), Tibshirani (2015); corpus confirms slow-brewing is established but not for this specific algorithm
- **Break condition:** If $\varepsilon$ is too large relative to curvature, the path may diverge from the true solution; if $N_d$ is too small, primal-dual stationarity is violated.

## Foundational Learning

- **Concept: Majorization-Minimization (MM) principle**
  - Why needed here: Core algorithmic strategy for handling non-quadratic losses
  - Quick check question: Can you explain why $f(\beta) \leq \tilde{f}(\beta|\beta_0)$ guarantees descent when minimizing the majorizer?

- **Concept: Convex conjugate (Fenchel dual)**
  - Why needed here: Required to understand the primal-dual transformation in Section 2
  - Quick check question: For $f(\beta) = \frac{1}{2}\|y - X\beta\|^2$, what is $f^*(\cdot)$?

- **Concept: Primal-dual stationarity in constrained optimization**
  - Why needed here: Recovering $\beta$ from $u$ via $\nabla f(\beta) + D^T u = 0$
  - Quick check question: Why does stationarity at optimal points enable recovery of primal from dual?

## Architecture Onboarding

- **Component map:**
  - Majorizer Module -> Dual-Solver -> Primal Recovery -> Path Controller

- **Critical path:** Initial $\hat{\beta}_0$ from null-space of $D$ → Majorization → Dual-Solver ($N_d$ iterations) → Primal update → Check objective decrease → Repeat

- **Design tradeoffs:**
  - $\varepsilon$ (step size): Smaller → more accurate paths, more $\lambda$ points, higher cost
  - $N_m$ (majorizations per $\lambda$): Larger → better convergence per $\lambda$, higher cost per point
  - $N_d$ (dual iterations): Larger → better dual optimality, but primal gradient computed only at majorization

- **Failure signatures:**
  - Primal objective not decreasing: $N_d$ too small or $\varepsilon$ too large
  - Path oscillation: $L$ (Lipschitz constant) underestimated
  - Non-termination: Early stopping criterion (AIC/BIC) never triggered

- **First 3 experiments:**
  1. **Sanity check:** Run MM-DUST on lasso ($D = I$, squared loss) comparing paths to `glmnet`; verify $\varepsilon \to 0$ converges
  2. **Step size sensitivity:** For logistic regression with fused lasso, sweep $\varepsilon \in \{0.5, 0.1, 0.05, 0.01\}$ and plot accuracy vs. time
  3. **Scalability test:** Compare execution time vs. SPG on increasing dimensions $(p, m)$ under Cox regression; verify $O(N_m p(nk + N_d m))$ scaling

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can adaptive step sizes be systematically incorporated into MM-DUST to achieve faster computation without sacrificing path accuracy?
- Basis in paper: [explicit] "the overall computational time can be further reduced by incorporating adaptive step sizes. Specifically, one could use a relatively larger step size at the beginning of the path to rapidly explore sparser models, and gradually shrink the step size as the model becomes denser."
- Why unresolved: The current algorithm uses a fixed step size ε throughout, which balances accuracy and efficiency but does not adapt to model complexity along the path.
- What evidence would resolve it: Development of an adaptive scheme with theoretical convergence guarantees and empirical demonstration of speedup over fixed step sizes.

### Open Question 2
- Question: Can alternative dual-space optimization strategies (e.g., ADMM) match or exceed MM-DUST's efficiency when the "slow-brewing" property is unnecessary?
- Basis in paper: [explicit] "alternative optimization strategies such as ADMM could be explored in the dual space if the 'slow-brewing' feature is not suitable for specific applications."
- Why unresolved: MM-DUST's stagewise framework was specifically designed to exploit slow-brewing; it remains unclear whether standard methods in the dual space retain comparable efficiency.
- What evidence would resolve it: A comparative study of dual ADMM vs. MM-DUST on tasks where full solution paths are unnecessary, reporting time and solution quality.

### Open Question 3
- Question: How can MM-DUST be extended to non-convex penalties (e.g., SCAD, MCP) or overlapping group lasso structures?
- Basis in paper: [explicit] "it may be interesting to investigate extensions to other penalty structures, including overlapping group lasso or even non-convex penalties."
- Why unresolved: The current theory and algorithm rely on convexity for the duality formulation and uniform convergence guarantees; non-convex penalties break these foundations.
- What evidence would resolve it: Modified algorithm with convergence analysis for specific non-convex cases, and empirical evaluation showing benefit over convex baselines.

### Open Question 4
- Question: Can a principled, data-driven method be developed for selecting the step size ε to automate the accuracy-efficiency trade-off?
- Basis in paper: [inferred] The paper notes ε enables a trade-off between statistical accuracy and computational efficiency, but only provides empirical guidance (e.g., ε = 0.1 or 0.05) without a systematic selection criterion.
- Why unresolved: Current practice relies on manual tuning or comparing multiple ε values, which is inefficient for large-scale applications.
- What evidence would resolve it: A method (e.g., based on problem dimensions, loss curvature, or cross-validation) that adaptively selects ε with performance comparable to manually tuned values.

## Limitations

- The theoretical analysis relies on convexity assumptions that may not hold in all practical scenarios
- Several implementation details are not fully specified, including the exact Cox regression Lipschitz constant calculation and rounding procedures
- The computational complexity claims need more rigorous empirical validation across diverse problem structures

## Confidence

**High Confidence**
- The majorization-minimization framework is a valid approach for handling general convex loss functions through quadratic majorization
- The dual-space transformation correctly converts the non-separable ℓ₁ penalty into box constraints
- The path-following framework with small step sizes is theoretically sound and produces valid solution paths

**Medium Confidence**
- The uniform convergence guarantee holds for all convex losses (theorem stated but proof details limited)
- The computational complexity reduction is significant in practice (empirically demonstrated but not rigorously analyzed)
- The early stopping criteria (AIC/BIC) consistently select good models (shown in simulations but not systematically studied)

**Low Confidence**
- The algorithm performs well on high-dimensional sparse problems specifically (only one real data example)
- The performance gap to SPG is consistently large across all scenarios (limited comparison scope)
- The choice of parameters (Nm=1, Nd=15-70) is universally optimal (not systematically explored)

## Next Checks

1. **Theoretical gap analysis:** Examine the convergence proof more carefully to identify any unstated assumptions or conditions where the uniform convergence guarantee might fail. Test edge cases with ill-conditioned D matrices.

2. **Parameter sensitivity study:** Systematically explore the three key parameters (ε, Nm, Nd) across different problem regimes (high-dimensional vs. low-dimensional, sparse vs. dense) to understand their impact on both statistical accuracy and computational efficiency.

3. **Scalability benchmark:** Implement a comprehensive scalability test comparing MM-DUST against SPG and other path-following methods across a wide range of dimensions (n, p, m) and data structures, measuring both wall-clock time and memory usage to validate the claimed complexity advantages.