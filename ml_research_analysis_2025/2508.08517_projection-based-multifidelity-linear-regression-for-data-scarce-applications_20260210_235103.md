---
ver: rpa2
title: Projection-based multifidelity linear regression for data-scarce applications
arxiv_id: '2508.08517'
source_url: https://arxiv.org/abs/2508.08517
tags:
- data
- regression
- linear
- training
- augmentation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents two projection-based multifidelity linear regression
  methods that combine low-fidelity and high-fidelity data through data augmentation
  to address regression problems with high-dimensional outputs and limited high-fidelity
  data. The methods leverage principal component analysis for dimensionality reduction
  and employ weighted least squares with fidelity-specific weights to train the regression
  model.
---

# Projection-based multifidelity linear regression for data-scarce applications

## Quick Facts
- **arXiv ID:** 2508.08517
- **Source URL:** https://arxiv.org/abs/2508.08517
- **Reference count:** 40
- **Primary result:** Two projection-based multifidelity linear regression methods achieve 3-12% improvement in median accuracy compared to single-fidelity methods with no more than ten high-fidelity samples

## Executive Summary
This paper introduces two projection-based multifidelity linear regression methods that combine low-fidelity and high-fidelity data through data augmentation to address regression problems with high-dimensional outputs and limited high-fidelity data. The methods leverage principal component analysis for dimensionality reduction and employ weighted least squares with fidelity-specific weights to train the regression model. Two approaches for data augmentation are introduced: direct data augmentation using low-fidelity data, and data augmentation incorporating explicit linear corrections between low-fidelity and high-fidelity data. The proposed methods are demonstrated on approximating the surface pressure field of a hypersonic vehicle in flight, achieving approximately 3% - 12% improvement in median accuracy compared to single-fidelity methods in a low-data regime of no more than ten high-fidelity samples.

## Method Summary
The method combines dimensionality reduction via PCA with multifidelity data augmentation to enable effective linear regression when high-fidelity data is scarce. The approach projects high-dimensional outputs onto a low-dimensional subspace, augments the limited high-fidelity training set with synthetic data derived from abundant low-fidelity samples, and trains the model using weighted least squares with fidelity-specific weights. Two data augmentation strategies are presented: direct concatenation of low-fidelity data, and explicit mapping that learns a linear correction between low-fidelity and high-fidelity data in the reduced space. The optimal synthetic data weight is determined through leave-one-out cross-validation.

## Key Results
- Projection-based multifidelity methods achieve 3-12% improvement in median accuracy over single-fidelity approaches
- Explicit mapping method is less sensitive to synthetic data weighting than direct augmentation
- In ultra-low data regime (N_HF ≤ 3), explicit mapping shows significant performance gains
- Low-fidelity data provides equivalent benefit to 0.63 additional high-fidelity samples

## Why This Works (Mechanism)

### Mechanism 1
Projecting high-dimensional outputs onto a low-dimensional subspace via Principal Component Analysis (PCA) may allow linear regression to function in regimes where the number of samples (N) is far smaller than the output dimension (m). The method computes a reduced basis U_k from the high-fidelity (HF) training data. It maps the full output space R^m to a reduced space R^k (where k << m), performs regression in this reduced space, and reconstructs predictions via lifting. Core assumption: The output variance is concentrated in a low-dimensional linear subspace, and the first k singular values capture the essential system dynamics (cumulative energy > ε). Evidence anchors: [abstract] "leverage principal component basis vectors for dimensionality reduction", [section 2.1] Eq. (3) defines the dimension k based on a cumulative energy tolerance. Break condition: If the HF data is rank-deficient or exhibits highly non-linear manifold structure not captured by linear PCA, the reconstruction error will dominate.

### Mechanism 2
Augmenting the scarce HF training set with synthetic data derived from abundant low-fidelity (LF) samples may better constrain the regression coefficients, specifically in input space regions uncovered by HF data. The approach constructs an augmented dataset (X_MF, Y_MF) by combining HF data with either raw LF data (Direct) or LF data corrected by a learned linear map g (Explicit Mapping). This larger dataset supports higher-order polynomial bases (e.g., quadratic vs. linear) that would otherwise overfit the HF data alone. Core assumption: A meaningful correlation exists between the LF and HF model responses, such that LF trends provide informative priors for HF behavior. Evidence anchors: [abstract] "combine low-fidelity and high-fidelity data through data augmentation", [section 3.1] "The MF regression via data augmentation provides additional information about the underlying system response in regions... insufficiently covered by HF samples." Break condition: If the LF model physics diverge significantly from HF physics (e.g., different flow regimes), the synthetic data injects bias rather than signal.

### Mechanism 3
Assigning fidelity-specific weights via Weighted Least Squares (WLS), particularly down-weighting LF samples near HF samples, mitigates the noise introduced by lower-fidelity data. A proximity-based weighting scheme assigns weights w_i=1 to HF samples and w_i < 1 to LF samples. It explicitly lowers the influence of LF data where HF data already exists (redundancy) while utilizing LF data to fill gaps (epistemic uncertainty reduction). Core assumption: LF data is heteroscedastic; its value diminishes in the immediate neighborhood of HF samples where the more accurate HF data dominates. Evidence anchors: [section 3.2] "...LF samples located near HF samples in the input space can be considered redundant... The LF data may therefore introduce noise rather than useful information.", [abstract] "train the linear regression model through weighted least squares with fidelity-specific weights." Break condition: If the distance metric ρ fails to capture the true similarity in the response surface, or if the optimal weight w*_syn is misspecified (Fig. 4 shows sensitivity).

## Foundational Learning

- **Concept:** Principal Component Analysis (PCA) / Singular Value Decomposition (SVD)
  - **Why needed here:** The method relies on reducing ~56,000 dimensional output vectors to ~7 dimensions before regression. Without this, the problem is ill-posed.
  - **Quick check question:** Given a data matrix Y, can you calculate the cumulative energy captured by the first k singular vectors?

- **Concept:** Weighted Least Squares (WLS)
  - **Why needed here:** The core training mechanism uses WLS to mix data of varying fidelity. You must understand how diagonal weight matrices influence the loss landscape compared to Ordinary Least Squares (OLS).
  - **Quick check question:** How does assigning a weight w < 1 to a specific training sample affect the resulting regression hyperplane compared to w=1?

- **Concept:** Cross-Validation (LOOCV)
  - **Why needed here:** The method uses Leave-One-Out Cross-Validation to tune the critical hyperparameter w_syn. This automated selection is cited as key to robust performance.
  - **Quick check question:** Why is LOOCV particularly suitable for the "ultra low-data regime" (N ≤ 10) compared to k-fold CV?

## Architecture Onboarding

- **Component map:** Data Inputs -> Projection Layer -> Synthetic Generator -> Optimizer -> Hyperparameter Tuner
- **Critical path:** The generation of the coordinate-transformed reduced states Ĉ_LF (Eq. 5) and the subsequent training of the map g is the most complex step in the "Explicit Mapping" approach. Errors here propagate directly to the synthetic data quality.
- **Design tradeoffs:**
  - Direct vs. Explicit Mapping: Direct is simpler but highly sensitive to weight choice; Explicit Mapping is more robust to weight choice (Fig. 4) but requires training an intermediate mapping model g, adding complexity.
  - Cost vs. Accuracy: Using 80 LF samples costs ≈ 0.63 HF samples. The tradeoff is strictly favorable in this paper (higher accuracy for less than 1 extra equivalent HF sample), assuming LF evaluations are available.
- **Failure signatures:**
  - High Sensitivity: If test accuracy varies wildly across random training seeds, check the LOOCV initialization (currently 10^-1) or broaden the search range for w_syn.
  - Additive Stagnation: If the "Additive" baseline (Appendix A) matches the "Explicit Map" results, the LF-HF correlation might be too complex for a simple linear discrepancy model.
- **First 3 experiments:**
  1. Reconstruction Baseline: Verify PCA implementation. For N_HF=10, plot the singular value decay (Fig. 2/3 style) and confirm k captures >99.5% energy.
  2. Weight Sensitivity Sweep: Implement the Direct Augmentation method. Run a sweep of fixed w_syn ∈ [0.01, 0.9] to replicate the sensitivity curve in Fig. 4a.
  3. Mapping Comparison: Implement the Explicit Mapping (Alg 1). Compare its performance against the Direct method specifically on the N_HF=3 case to verify the "low-data" advantage claimed in Section 4.3.

## Open Questions the Paper Calls Out

### Open Question 1
Can the data augmentation framework be effectively extended to non-linear regression techniques, such as neural networks and regression trees? Basis in paper: [explicit] The conclusion states, "Future work can expand these MF regression methods to different underlying regression techniques, such as neural networks and regression trees." Why unresolved: The current methodology relies on linear regression (linear in parameters) and closed-form weighted least squares solutions, which do not directly transfer to non-linear optimization schemes. What evidence would resolve it: Successful integration of the fidelity-weighting and data augmentation strategies into the training loss of a neural network, demonstrating accuracy gains on the same high-dimensional benchmarks.

### Open Question 2
Do alternative coordinate transformation techniques, such as manifold alignment, improve the mapping between low-fidelity and high-fidelity subspaces compared to the current projection approach? Basis in paper: [explicit] Section 3.1 and Section 5 note that one could "explore methods such as manifold alignment to align the two subspaces and potentially provide better mappings between the two reduced states." Why unresolved: The current method relies on a linear projection (Eq. 5) to express LF data in the HF basis, which may be suboptimal if the intrinsic manifolds of the fidelity levels differ significantly. What evidence would resolve it: A comparative study quantifying the reconstruction error of the explicit mapping method when using manifold alignment versus the standard PCA projection utilized in Algorithm 1.

### Open Question 3
How does the performance of the explicit mapping method degrade when the underlying assumption of a linear relationship between low-fidelity and high-fidelity outputs is violated? Basis in paper: [inferred] Section 3.1 states the authors "choose to model this as a low-rank linear relationship," implying potential limitations if the true correlation is non-linear. Why unresolved: The paper does not evaluate the method's sensitivity to the validity of this linearity assumption, which is critical for problems with complex physics discrepancies between fidelities. What evidence would resolve it: Application of the method to a synthetic problem where the LF-HF correlation is parameterized from linear to highly non-linear, observing the change in prediction accuracy.

## Limitations
- Method's performance tightly coupled to quality of low-fidelity model and success of linear correction mapping
- Optimal synthetic weight hyperparameter (w_syn) requires expensive LOOCV optimization that scales poorly with larger datasets
- Assumes sufficient low-fidelity data availability (80 samples used here), which may not be feasible for all applications

## Confidence

**High Confidence:** The core PCA-based dimensionality reduction mechanism (Mechanism 1) is well-established and mathematically sound. The performance improvements (3-12% median accuracy) are demonstrated through systematic testing across multiple data regimes.

**Medium Confidence:** The weighted least squares approach with proximity-based weights (Mechanism 3) shows effectiveness in the presented case but may be sensitive to the choice of distance metric and weight function parameters. The explicit mapping approach's superiority over direct augmentation requires further validation across different problem domains.

**Low Confidence:** The assumption that second-order polynomial bases are appropriate for all multifidelity regression problems in the augmented space. This choice appears justified by the dataset size but may not generalize to problems with different characteristics.

## Next Checks

1. **Cross-Problem Generalization Test:** Apply the methodology to a different physical system (e.g., thermal simulation or structural analysis) to verify that the explicit mapping approach consistently outperforms direct augmentation across diverse domains, not just the hypersonic vehicle case.

2. **Robustness to LF Model Quality:** Systematically degrade the LF model quality (increase bias while maintaining computational efficiency) to identify the threshold where the multifidelity approach ceases to provide benefits over single-fidelity methods.

3. **Weight Sensitivity Analysis with Alternative Metrics:** Replace the current distance-based weighting scheme with alternative approaches (e.g., uncertainty-based weights or influence functions) to determine if the performance improvements are robust to different weighting strategies.