---
ver: rpa2
title: 'KL-Regularised Q-Learning: A Token-level Action-Value perspective on Online
  RLHF'
arxiv_id: '2508.17000'
source_url: https://arxiv.org/abs/2508.17000
tags:
- policy
- action-value
- function
- rlhf
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces KL-regularised Q-Learning (KLQ), a novel\
  \ action-value method for language model reinforcement learning from human feedback\
  \ (LM-RLHF). KLQ leverages the discrete state-action space and KL-regularisation\
  \ against a base policy, using \u03BB-returns to construct regression targets and\
  \ exploiting an analytic correspondence between optimal policies and action-values\
  \ in the KL-regularised setting."
---

# KL-Regularised Q-Learning: A Token-level Action-Value perspective on Online RLHF

## Quick Facts
- arXiv ID: 2508.17000
- Source URL: https://arxiv.org/abs/2508.17000
- Reference count: 40
- Achieves similar reward optimization to PPO while consistently achieving higher LLM-as-a-judge win-rates

## Executive Summary
This paper introduces KL-regularised Q-Learning (KLQ), an action-value method for language model reinforcement learning from human feedback (LM-RLHF). KLQ uses λ-returns to construct regression targets and exploits an analytic correspondence between optimal policies and action-values in the KL-regularised setting. The authors show that KLQ's updates are equivalent to a modified version of PPO in a specific sense. Empirical results on summarisation and dialogue tasks demonstrate that KLQ achieves similar reward optimisation performance to PPO while consistently achieving higher win-rates in LLM-as-a-judge evaluations, suggesting better qualitative performance.

## Method Summary
KLQ is an action-value method for LM-RLHF that parametrizes the Q-function as $Q_\theta(s,a) = \tau \log(\pi_\theta(a|s)/\pi_b(a|s)) + V_\theta(s)$, where $\pi_b$ is a frozen reference policy (the SFT model). The algorithm constructs regression targets using λ-returns computed via TD errors, then minimizes an L2 loss between the Q-function output and these targets. The theoretical contribution shows this update is equivalent to optimizing a modified PPO-penalty objective. The method is implemented as an on-policy algorithm with hyperparameters including $\gamma=1$, $\tau=0.05$, $\lambda=0.95$, and learning rate 1.41e-5.

## Key Results
- On TL;DR summarisation, KLQ achieves similar reward optimization to PPO while consistently achieving higher LLM-as-a-judge win-rates across different KL penalty coefficients
- On Anthropic-HH dialogue, KLQ shows comparable performance to PPO in reward optimization, though with less consistent win-rate advantages
- The theoretical analysis establishes equivalence between KLQ updates and a modified PPO objective under on-policy sampling

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: KLQ achieves stable policy updates by framing them as regression toward a conservative, multi-step value estimate, which implicitly performs a form of proximal policy optimization.
- **Mechanism**: The algorithm trains an action-value function $Q_\theta(s,a)$ by minimizing an $\ell_2$ loss against a conservative $\lambda$-return target $G^{\lambda,\alpha}_t$. This target is a weighted average of n-step returns. The action-value function is explicitly parameterized as $Q_\theta(s,a) = \tau \log(\pi_\theta(a|s)/\pi_b(a|s)) + V_\theta(s)$. A theoretical analysis (Proposition 4.1) shows this update is equivalent to optimizing a modified PPO-penalty objective with a reverse KL penalty against the previous policy iterate. The policy improvement is thus implicit in the Q-function update.
- **Core assumption**: The theoretical equivalence relies on states being sampled on-policy. The algorithm assumes the specific Q-function parameterization can be effectively learned by the neural network via regression.
- **Evidence anchors**:
  - [abstract] "KLQ leverages... λ-returns to construct regression targets and exploiting an analytic correspondence between optimal policies and action-values..."
  - [section 4] "Proposition 4.1. The sequence (Qk)k defined by Equation (15) corresponds via the mapping from Section 3.2 to the sequence (πk, Vk)k defined above by optimising Equations (17) and (19)."
  - [corpus] Paper `2511.21638` ("Aligning LLMs...") discusses multi-turn PPO, suggesting the stability of proximal methods is a key area of research. Paper `2506.02553` ("Response-Level Rewards...") analyzes RL under sparse rewards, the problem KLQ's λ-returns are designed to solve.
- **Break condition**: The mechanism assumes the theoretical contraction properties of the Bellman operator hold in practice with function approximation. Instability or divergence in the Q-loss could occur if the network fails to approximate the Q-function well, breaking the link to the PPO objective.

### Mechanism  2
- **Claim**: Effective credit assignment for sparse reward signals is achieved by propagating the final reward backward through the token sequence using TD(λ) error accumulation.
- **Mechanism**: The reward model provides a single preference score at the end of an episode. KLQ uses a λ-return estimator (Eq. 7), which is a weighted sum of n-step returns. This propagates the final reward signal to earlier tokens, allowing the Q-function to learn which token-level actions contribute to the final score, while managing the bias-variance trade-off via the λ hyperparameter.
- **Core assumption**: The intermediate token-level actions have a predictable relationship with the final outcome, which the TD(λ) estimator can capture. The bias introduced by bootstrapping (λ < 1) is acceptable for variance reduction.
- **Evidence anchors**:
  - [section 3.1] "However, one-step methods are inappropriate for the LM-RLHF setting, since the non-KL-regularisation portion of the reward signal... is very sparse... We therefore choose a value formulation based on the λ-return framework."
  - [section 5.1] Training curves (Figure 1) show KLQ successfully optimizing the RLHF reward, suggesting effective learning from the sparse reward.
  - [corpus] Paper `2506.02553` ("Response-Level Rewards...") explicitly analyzes the "Zero-Reward Assumption" in this setting, confirming the centrality of this challenge.
- **Break condition**: If the discount factor γ is too low or the sequence length is too long, the reward signal may vanish before reaching early tokens. Conversely, if λ is too high, variance from the sparse reward could destabilize training.

### Mechanism 3
- **Claim**: A principled initialization for the Q-function is derived from the pre-trained policy, stabilizing the start of learning.
- **Mechanism**: Standard Q-learning suffers from cold-start issues. KLQ leverages a theoretical mapping from any policy π and value function V to a corresponding Q-function: $Q[\pi, V](s, a) = \tau \log(\pi(a|s)/\pi_b(a|s)) + V(s)$. By initializing the policy π_θ as the SFT policy π_b, the Q-function starts in a known, stable state, avoiding random exploration at the outset.
- **Core assumption**: The SFT policy provides a reasonable baseline for action values. The value head V_θ, even if randomly initialized, will be quickly adjusted by the regression targets without destabilizing the policy component.
- **Evidence anchors**:
  - [section 3.2] "In order to use action-value methods for LM-RLHF, we develop a principled way to convert between a policy initialisation and an action-value initialisation."
  - [section 3.2] "The policy πθ(a|s) is initialised as the SFT-policy, πb, and the Boltzmann state-value function V is a randomly initialised value head..."
  - [corpus] Paper `2502.19255` ("Can RLHF be More Efficient...") explores using imperfect reward models, which relates to the broader theme of leveraging existing knowledge for initialization.
- **Break condition**: If the value head initialization is extremely poor, the initial Q-estimates could be wildly incorrect, leading to destructive policy updates in the first few steps before the value head adapts.

## Foundational Learning

- **Concept**: **KL-Regularized Reinforcement Learning**
  - **Why needed here**: This is the core theoretical framework for the paper. Understanding that the objective is to maximize reward while staying close (in KL-divergence) to a base reference policy is essential.
  - **Quick check question**: In the KL-regularized objective $G_t$, what two competing terms are being traded off at each step?

- **Concept**: **Temporal Difference (TD) Learning & λ-returns**
  - **Why needed here**: The paper builds its core algorithm on TD learning, specifically using λ-returns to handle sparse rewards. One must understand bootstrapping and the bias-variance trade-off controlled by λ.
  - **Quick check question**: When λ=1, the λ-return becomes equivalent to the full Monte Carlo return. What are the pros and cons of using this setting compared to λ=0?

- **Concept**: **Action-Value (Q) vs. State-Value (V) Functions**
  - **Why needed here**: KLQ is an action-value method. A key contribution is the explicit, learnable mapping between the policy π, the value function V, and the action-value function Q. Understanding this relationship is critical.
  - **Quick check question**: According to the paper's parameterization (Eq. 13), what two components make up the Q-value for a given state and action?

## Architecture Onboarding

- **Component map**:
    1.  **Policy Network**: A transformer-based LLM that outputs logits for the next token (the policy π_θ).
    2.  **Value Head**: A small neural network (e.g., a linear layer) attached to the transformer body that outputs a scalar state-value V_θ(s).
    3.  **Reference Model**: A frozen copy of the initial SFT model (π_b), used to compute KL-divergence penalties.
    4.  **Reward Model**: A separate, frozen model that provides the scalar preference score at the end of an episode.

- **Critical path**:
    1.  **Rollout**: The policy π_θ auto-regressively generates a completion for a given prompt.
    2.  **Reward Calculation**: The full completion is scored by the reward model. A KL penalty is computed at each token step against the reference model.
    3.  **Target Construction**: For each token in the trajectory, a λ-return target G_t is computed by backward recursion using the TD error δ_t = r_{t+1} + γV_θ(s_{t+1}) - Q_θ(s_t, a_t).
    4.  **Update**: The main network is trained by minimizing the L2 loss between its Q-output and the target G_t. This single loss updates both the policy and value head components.

- **Design tradeoffs**:
    - **On-policy vs. Off-policy**: The authors implement KLQ as an on-policy method (like PPO), simplifying the design but potentially wasting data. They note a future off-policy version could reuse data.
    - **Parameterization**: The Q-function is explicitly constructed from the policy and value head. This enforces a specific Boltzmann policy structure but simplifies the loss function to a single regression term.

- **Failure signatures**:
    - **KL Collapse**: If τ (KL penalty coefficient) is too low, the policy may deviate too far from the reference model, leading to degenerate outputs and reward hacking.
    - **Unstable Value Head**: If the value head's learning is unstable, the TD errors and λ-return targets will be noisy, leading to erratic policy updates.
    - **Reward Hacking**: The model may exploit flaws in the reward model to achieve high scores with low-quality text. The consistent LLM-as-a-judge wins for KLQ suggest it may be somewhat more robust to this than PPO.

- **First 3 experiments**:
    1.  **Sanity Check - Imitation**: Run KLQ with the reward model replaced by a constant reward. Verify that the policy remains close to the SFT initialization.
    2.  **Hyperparameter Scan - λ**: Train on a small dataset with λ ∈ {0.0, 0.5, 0.95, 1.0}. Plot reward curves and KL divergence to understand the bias-variance trade-off in this specific domain.
    3.  **Benchmark vs. PPO**: On a standard task like TL;DR summarization, compare KLQ against a PPO baseline. Track not just the reward model score, but also the KL-divergence from the SFT model and perform a qualitative "win-rate" evaluation using an independent LLM judge, as done in the paper.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can KLQ be effectively adapted to an off-policy setting using prioritized experience replay or expert demonstrations, while retaining its theoretical grounding?
- Basis in paper: [explicit] Section 7.1: "In future work we hope to explore the use of off-policy sampling as an extension to KLQ. In particular, off-policy data can include expert demonstrations—obviating the need for an SFT stage altogether—or prioritised experience replay"
- Why unresolved: KLQ is currently on-policy. While action-value methods can theoretically work off-policy, the practical implementation with language models and the specific λ-return formulation used in KLQ has not been validated.
- What evidence would resolve it: Empirical comparison of off-policy KLQ variants against on-policy KLQ and PPO on standard LM-RLHF benchmarks, demonstrating comparable or superior performance while reusing experience.

### Open Question 2
- Question: How can the token-level attribution benefits of explicit value heads be combined with the bootstrapping bias elimination offered by group-rollout methods?
- Basis in paper: [explicit] Section 7.2: "In future work we hope to investigate ways to combine this with the benefits of group-rollouts."
- Why unresolved: Group rollout methods like RLOO and GRPO eliminate the need for value heads but work at the completion level; KLQ uses token-level value heads for explicit attribution but relies on bootstrapping which introduces bias.
- What evidence would resolve it: A hybrid algorithm design with ablations showing (1) retained token-level attribution capabilities and (2) reduced bias from bootstrapping, validated on tasks requiring fine-grained credit assignment.

### Open Question 3
- Question: Is there an appropriate advantage normalisation or whitening procedure for KLQ's combined (π, V) objective that could yield significant empirical improvements?
- Basis in paper: [explicit] Section 7.3: "It is not clear how to establish an analogue of this procedure for KLQ due to the combined π, V objective. There may be significant empirical improvements from establishing an appropriate whitening procedure for KLQ."
- Why unresolved: PPO typically uses advantage whitening, claimed to be important by the TRL implementation. KLQ's parametrisation intertwines policy and value components, making standard whitening approaches potentially inapplicable.
- What evidence would resolve it: Systematic ablation studies comparing KLQ with and without various normalisation schemes, showing convergence speed and final performance differences across multiple seeds and tasks.

### Open Question 4
- Question: Does KLQ outperform PPO on multi-step reasoning tasks where credit assignment over long token sequences is critical?
- Basis in paper: [explicit] Section 7.4: "We would also be excited to evaluate our methodology on multi-step reasoning tasks, which are currently an exciting application area for language model RL."
- Why unresolved: Current experiments only cover summarisation and single-turn dialogue. Multi-step reasoning may have different reward landscapes and require different KL-regularisation trade-offs.
- What evidence would resolve it: Benchmarks on math reasoning (e.g., GSM8K) or code generation tasks, comparing reward optimisation and qualitative performance between KLQ and PPO.

## Limitations
- **Theoretical Generalization**: The theoretical equivalence to PPO assumes on-policy sampling and may not hold under function approximation or off-policy regimes
- **Empirical Scope**: Evaluation focuses on two narrow tasks (TL;DR summarization and single-turn dialogue) using similar Pythia-1B model scales
- **Implementation Details**: Critical implementation specifics remain unspecified, including value head architecture and reward model normalization

## Confidence
- **Mechanism 1**: High confidence - theoretical framework is well-specified and mathematically rigorous
- **Mechanism 2**: High confidence - λ-returns are standard technique with clear empirical support
- **Mechanism 3**: Medium confidence - principled initialization is theoretically sound but practical impact depends on unspecified details
- **Empirical Performance**: Medium confidence - TL;DR results are compelling but Anthropic-HH results are less conclusive

## Next Checks
1. **Off-policy Validation**: Implement an off-policy variant of KLQ that stores and reuses trajectories to test whether the theoretical equivalence to PPO extends beyond the on-policy setting assumed in the paper's analysis.

2. **Scale Transfer Test**: Apply KLQ to a significantly larger model scale (e.g., 8B+ parameters) and evaluate whether the qualitative performance improvements observed with Pythia-1B transfer to larger models, where reward hacking becomes more prevalent.

3. **Generalization Benchmark**: Evaluate KLQ across a diverse set of generation tasks including story completion, code synthesis, and multi-turn dialogue to determine whether the LLM-as-a-judge performance advantages are task-specific or represent a general improvement in alignment quality.