---
ver: rpa2
title: 'Agent-Omni: Test-Time Multimodal Reasoning via Model Coordination for Understanding
  Anything'
arxiv_id: '2511.02834'
source_url: https://arxiv.org/abs/2511.02834
tags:
- reasoning
- multimodal
- agent
- video
- omni
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Agent-Omni introduces a master-agent framework that coordinates
  existing foundation models to achieve test-time multimodal reasoning without retraining.
  By dynamically delegating tasks to modality-specific agents and iteratively refining
  outputs, the system integrates text, image, audio, and video inputs into coherent
  answers.
---

# Agent-Omni: Test-Time Multimodal Reasoning via Model Coordination for Understanding Anything

## Quick Facts
- arXiv ID: 2511.02834
- Source URL: https://arxiv.org/abs/2511.02834
- Reference count: 40
- Key outcome: Achieves state-of-the-art accuracy on diverse multimodal benchmarks through test-time model coordination without retraining.

## Executive Summary
Agent-Omni introduces a master-agent framework that coordinates existing foundation models to achieve test-time multimodal reasoning without retraining. By dynamically delegating tasks to modality-specific agents and iteratively refining outputs, the system integrates text, image, audio, and video inputs into coherent answers. Experiments show Agent-Omni achieves state-of-the-art accuracy on diverse benchmarks, outperforming both unified omni models and DSPy-CoT, especially on complex reasoning tasks like MMMU-Pro and Daily-Omni. The modular design enables seamless integration of specialized models and iterative self-correction, delivering robust omni-modal understanding while avoiding trade-offs seen in joint training approaches.

## Method Summary
Agent-Omni uses a master-agent framework with a 4-stage loop: Perception (input summarization to JSON), Reasoning (query decomposition and model delegation), Execution (model invocation), and Decision (answer synthesis and completeness evaluation). The system delegates subtasks to specialized models—Claude 3.7 Sonnet for image/video, Deepseek R1 for text, Qwen2.5 Omni for audio—coordinated through structured JSON communication. A maximum of 3 iterative loops refine outputs based on master agent feedback. The framework is evaluated on 15 benchmarks across text, image, video, audio, and omni-modal tasks using accuracy as the primary metric.

## Key Results
- Outperforms unified omni models (Phi-4, Qwen2.5 Omni) and DSPy-CoT baselines on MMMU-Pro, Daily-Omni, and other multimodal benchmarks.
- Achieves 83.21% on MMLU-Pro and 60.23% on MMMU-Pro, with significant gains on complex reasoning tasks.
- Demonstrates modularity by integrating specialized models without retraining, achieving 60.03% on the challenging Daily-Omni benchmark.

## Why This Works (Mechanism)

### Mechanism 1
Iterative self-correction through the master loop improves answer quality for complex multimodal queries. The Decision stage evaluates completeness via the `is_final` flag; if false, it generates suggestions that feed back into the next Reasoning iteration, targeting identified gaps. Core assumption: The master agent can reliably detect incomplete or inconsistent outputs and generate actionable refinement suggestions. Evidence anchors: [abstract] "iteratively refining outputs"; [section 2.2] "if gaps or inconsistencies are detected, the agent appends feedback instructions to the JSON and triggers another round"; [corpus] Related work on inference-time scaling (Forest-of-Thought) supports iterative refinement as a reasoning strategy, but corpus lacks direct validation of this specific loop design. Break condition: The mechanism degrades if suggestions are uninformative or if the master agent cannot distinguish genuine gaps from noise; maximum loop limit L caps iterations.

### Mechanism 2
Modality-specific delegation to specialized models outperforms single unified omni-models. The Reasoning stage decomposes queries into sub-questions routed to the best-available model per modality (e.g., Deepseek R1 for text, Claude 3.7 Sonnet for video), preserving each model's expertise. Core assumption: Specialized models maintain higher per-modality accuracy than jointly trained omni-models, and errors do not cascade destructively. Evidence anchors: [abstract] "delegates subtasks to modality-specific agents"; [section 3.7, Table 8] Agent-Omni outperforms Phi-4 Multimodal Instruct and Qwen2.5 Omni across nearly all benchmarks; [corpus] Unified Multi-Agent Framework paper shows similar agent-based coordination benefits, supporting generalization of this approach. Break condition: Fails if delegation logic routes to suboptimal models or if cross-modal dependencies require tighter integration than sequential delegation allows.

### Mechanism 3
Structured JSON-based communication enables interpretable, traceable reasoning pipelines. Each stage outputs to a typed JSON schema (e.g., `MasterReasoningStructure`, `MasterDecisionStructure`), making intermediate states explicit and debuggable. Core assumption: The master agent can reliably generate valid structured outputs; downstream agents parse and act on them correctly. Evidence anchors: [section 2.2] "organized in a structured JSON format, where the user intent is explicitly represented"; [Appendix B] Detailed JSON schemas for reasoning and decision stages provided; [corpus] No direct corpus evidence on JSON-structured agent communication; this is a framework-specific design choice. Break condition: JSON parsing failures or schema violations halt the pipeline; prompt engineering must enforce format compliance.

## Foundational Learning

- **Agent orchestration patterns**
  - Why needed here: Agent-Omni coordinates multiple models through a central controller; understanding delegation, tool-use, and feedback loops is essential.
  - Quick check question: Can you sketch how a master agent decides when to invoke a sub-agent versus answering directly?

- **Multimodal foundation model capabilities and limitations**
  - Why needed here: Selecting the right model per modality requires knowing their strengths (e.g., Claude 3.7 Sonnet for visual reasoning, Qwen2.5 Omni for audio).
  - Quick check question: For a query combining dashboard video and engine audio, which models would you assign and why?

- **Iterative refinement / inference-time compute scaling**
  - Why needed here: The master loop trades latency for accuracy; understanding when additional iterations help is critical for tuning L.
  - Quick check question: What signals would indicate that a query needs >1 iteration versus exiting after the first?

## Architecture Onboarding

- **Component map**: User query -> Perception (modality detection) -> Reasoning (decompose + delegate) -> Execution (invoke models) -> Decision (integrate + evaluate) -> Check `is_final` -> Loop or return
- **Critical path**: User query → Perception (modality detection) → Reasoning (decompose + delegate) → Execution (invoke models) → Decision (integrate + evaluate) → Check `is_final` → Loop or return
- **Design tradeoffs**:
  - Latency vs. accuracy: More iterations improve complex tasks (MMLU-Pro: 82.20% → 83.21%) but increase latency (4–20s)
  - Model pool flexibility vs. integration complexity: Swapping models is easy, but performance depends on each model's API stability and output format
  - Transparency vs. overhead: JSON-structured outputs aid debugging but require prompt enforcement and validation
- **Failure signatures**:
  - Exit after 1 iteration on complex tasks (check exit rates in Table 10; video tasks show higher multi-iteration usage)
  - JSON parsing errors from master agent (review prompts in Appendix B for format constraints)
  - Modality mismatch (e.g., audio query routed to text model due to misclassification in Perception)
- **First 3 experiments**:
  1. **Single-iteration vs. multi-iteration ablation**: Run on MMMU-Pro and Daily-Omni with L=1 vs. L=3; compare accuracy and latency.
  2. **Model pool swap test**: Replace Claude 3.7 Sonnet (video) with Llava Video 7B; measure performance drop on VideoMathQA and STI-Bench.
  3. **Error trace on failure cases**: Identify queries where Agent-Omni underperforms baselines; inspect JSON outputs to diagnose whether delegation, integration, or iteration is the bottleneck.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can parallelized execution of modality-specific agents significantly reduce the high latency observed in video and complex omni tasks without compromising reasoning accuracy?
- Basis in paper: [explicit] Page 7 notes, "Future improvements such as parallelized execution could further reduce latency while preserving robustness," addressing the observed latency of up to 20.53s on video benchmarks.
- Why unresolved: The current implementation implies a degree of sequential coordination or aggregation overhead that creates a "trade-off between speed and reasoning quality."
- What evidence would resolve it: A study comparing the current sequential/coordinated pipeline against a fully parallelized execution model on the STI-Bench or VideoMathQA benchmarks, reporting latency reduction percentages and accuracy deltas.

### Open Question 2
- Question: How does the framework's performance degrade when exposed to noisy, adversarial, or safety-critical inputs compared to the curated benchmarks used in the study?
- Basis in paper: [explicit] Page 8 states, "generalization to noisy, adversarial, or safety-critical settings is unverified," as the evaluation relied primarily on curated benchmarks.
- Why unresolved: The robustness of the Master Agent's "Decision" stage against malicious or low-quality inputs from the "Perception" stage remains untested.
- What evidence would resolve it: Evaluation results on adversarial multimodal benchmarks (e.g., perturbed images or conflicting audio/text pairs) to measure the system's resilience to hallucination or incorrect synthesis.

### Open Question 3
- Question: Can the Agent-Omni architecture be extended to support non-textual output generation (e.g., generating images or audio) while maintaining the current modular coordination?
- Basis in paper: [explicit] Page 8 lists as a limitation: "the system currently only produces textual outputs and does not handle generation in other modalities."
- Why unresolved: The current "Decision" stage and JSON schemas (Appendix B) are designed strictly for synthesizing text, lacking mechanisms to integrate generative models into the output loop.
- What evidence would resolve it: A modified framework architecture that includes generative agents in the model pool and successful qualitative/quantitative results on an any-to-any generation task.

### Open Question 4
- Question: To what extent does the iterative self-correction loop mitigate the propagation of errors or biases inherent in the specialized foundation models?
- Basis in paper: [explicit] Page 8 warns that "Errors or biases from individual models can propagate through the coordination process."
- Why unresolved: While the master loop allows for refinement, it is unclear if the Master Agent possesses sufficient ground-truth knowledge to detect confident but incorrect outputs from specialized agents (e.g., a vision model hallucinating details).
- What evidence would resolve it: An ablation study measuring the Master Agent's success rate in detecting and correcting synthetic hallucinations injected into the outputs of the modality-specific agents.

## Limitations

- Generalization to noisy, adversarial, or safety-critical settings is unverified as the evaluation relied primarily on curated benchmarks.
- Errors or biases from individual models can propagate through the coordination process, and the master agent's ability to detect and correct them is untested.
- The system currently only produces textual outputs and does not handle generation in other modalities.

## Confidence

- **High confidence**: Claims about achieving state-of-the-art accuracy on benchmarks (MMM-Pro, Daily-Omni) are well-supported by comparative tables.
- **Medium confidence**: The mechanism of iterative self-correction improving complex task performance is plausible but lacks direct validation of the master agent's gap-detection capability.
- **Medium confidence**: Modality-specific delegation outperforming unified models is supported by benchmark comparisons, but cross-modal integration quality is not rigorously tested.

## Next Checks

1. **Gap-detection capability test**: Run ablation studies isolating the master agent's ability to identify incomplete answers vs. noise; compare accuracy gains from informed vs. random feedback suggestions.
2. **Cross-modal dependency stress test**: Design queries requiring tight audio-video integration (e.g., "Describe the scene and its sound source"); compare Agent-Omni's sequential delegation vs. a unified model's joint processing.
3. **JSON schema robustness audit**: Intentionally perturb model outputs to violate JSON schemas; measure failure rates and assess whether the pipeline recovers or halts, quantifying the brittleness of structured communication.