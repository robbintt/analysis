---
ver: rpa2
title: 'Towards Aligning Multimodal LLMs with Human Experts: A Focus on Parent-Child
  Interaction'
arxiv_id: '2511.04366'
source_url: https://arxiv.org/abs/2511.04366
tags:
- child
- parent
- interaction
- slps
- attention
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study explores aligning multimodal large language models (MLLMs)
  with speech-language pathologists (SLPs) in analyzing joint attention in parent-child
  interactions. Through interviews and video annotations with three SLPs, the researchers
  identified key behavioral cues (gaze, action, vocalisation) used to assess interaction
  quality.
---

# Towards Aligning Multimodal LLMs with Human Experts: A Focus on Parent-Child Interaction

## Quick Facts
- arXiv ID: 2511.04366
- Source URL: https://arxiv.org/abs/2511.04366
- Reference count: 40
- Two-stage MLLM system achieves 85% accuracy in behavioral description and 64% accuracy in joint attention evaluation

## Executive Summary
This study explores aligning multimodal large language models (MLLMs) with speech-language pathologists (SLPs) for analyzing joint attention in parent-child interactions. Through interviews and video annotations with three SLPs, researchers identified key behavioral cues (gaze, action, vocalisation) used to assess interaction quality. A two-stage MLLM system was developed: first extracting behavioral descriptions from videos using structured prompting, then evaluating interaction quality using few-shot examples. The MLLM achieved up to 85% accuracy in behavioral description and over 64% accuracy in joint attention evaluation compared to expert labels. The findings demonstrate that alignment is more robust at the observation level where experts share common descriptors than at the judgment level where interpretive criteria diverge.

## Method Summary
The researchers employed a two-stage MLLM framework to analyze parent-child interaction videos. In Stage 1, they extracted behavioral descriptions from video frames using structured prompting with four reasoning steps: identifying joint attention behaviors, labeling behavior types (gaze, action, vocalisation), counting behaviors, and summarizing descriptions. In Stage 2, they evaluated interaction quality using few-shot examples based on extracted descriptions. The system was trained and tested on 27 video clips from the P-ADL dataset, annotated by three SLPs who also participated in semi-structured interviews to understand their assessment criteria.

## Key Results
- MLLM achieved up to 85% accuracy in behavioral description task compared to expert annotations
- Joint attention evaluation accuracy reached over 64% compared to expert labels
- Experts showed higher agreement on observation-level descriptors than on judgment-level interpretations
- Alignment was more robust at the observation level where experts shared common descriptors

## Why This Works (Mechanism)
The two-stage approach works by first establishing a common ground for describing observable behaviors (gaze, action, vocalisation) through structured prompting, then applying expert judgment to evaluate interaction quality. The MLLM successfully mimics the SLPs' process of breaking down complex social interactions into quantifiable behavioral components before making interpretive assessments about joint attention.

## Foundational Learning
1. **Joint attention assessment** - Understanding how SLPs evaluate coordinated attention between parent and child
   - Why needed: Forms the basis for what the MLLM needs to recognize and evaluate
   - Quick check: Can you list the three main behavioral cues used by experts?

2. **Multimodal prompt engineering** - Using structured prompting with specific reasoning steps for video analysis
   - Why needed: Enables systematic extraction of behavioral descriptions from video frames
   - Quick check: What are the four reasoning steps in the Stage 1 prompting process?

3. **Few-shot learning for evaluation** - Using limited examples to train judgment models
   - Why needed: Allows the MLLM to learn expert evaluation criteria without extensive training data
   - Quick check: How many few-shot examples were used for the joint attention evaluation?

4. **Expert consensus building** - Understanding how SLPs agree or disagree on interpretations
   - Why needed: Reveals where MLLM alignment is strongest and where challenges remain
   - Quick check: At which level (observation or judgment) did experts show higher agreement?

5. **Behavioral cue taxonomy** - Categorizing observable behaviors into gaze, action, and vocalisation types
   - Why needed: Provides a structured framework for both human and MLLM analysis
   - Quick check: What are the three main behavior categories identified by experts?

## Architecture Onboarding

**Component Map:**
Video Input -> Stage 1 MLLM (Behavioral Description) -> Stage 2 MLLM (Joint Attention Evaluation) -> Expert Label Comparison

**Critical Path:**
The critical path flows from video input through structured behavioral description extraction to joint attention evaluation, with the bottleneck being the transition from objective behavioral descriptions to subjective quality judgments.

**Design Tradeoffs:**
The two-stage approach trades computational efficiency for accuracy by separating the descriptive and evaluative tasks, allowing each stage to specialize rather than attempting holistic analysis in a single pass.

**Failure Signatures:**
- Low behavioral description accuracy indicates problems with prompt engineering or visual feature extraction
- Mismatch between MLLM and expert evaluation suggests either insufficient few-shot examples or fundamental differences in interpretive criteria
- High variability in expert annotations reveals ambiguous evaluation criteria that the MLLM cannot resolve

**First 3 Experiments to Run:**
1. Test the MLLM on a held-out set of videos not seen during training to verify generalization
2. Compare MLLM performance against a simple baseline that uses only gaze cues (the most reliable expert indicator)
3. Conduct ablation study removing the few-shot examples to determine their impact on joint attention evaluation accuracy

## Open Questions the Paper Calls Out
None

## Limitations
- Small sample size (only three expert annotators and 27 video clips) limits generalizability
- Narrow domain focus on joint attention in autism assessment may not apply to broader interaction contexts
- Expert disagreement at the judgment level creates challenges for establishing ground truth
- Few-shot learning approach for evaluation may not scale well to more complex or varied interaction scenarios

## Confidence

**Major claim confidence labels:**
- Behavioral description accuracy (85%): **High** - This is a more straightforward, observation-based task where experts showed higher agreement
- Joint attention evaluation accuracy (64%): **Medium** - This interpretive task showed more expert disagreement, making it harder to establish ground truth
- Two-stage MLLM framework effectiveness: **Medium** - While the approach shows promise, limited testing prevents strong conclusions about scalability

The finding that alignment is stronger at the observation level than the judgment level is well-supported by the data, as experts showed higher consensus on describing behaviors than on evaluating interaction quality. However, the study's limited scope and sample size mean these results should be interpreted cautiously.

## Next Checks

1. Test the MLLM system with a larger, more diverse set of parent-child interaction videos (minimum 100 clips) and multiple expert groups to assess generalizability

2. Conduct inter-rater reliability analysis among a larger pool of SLPs to establish baseline expert agreement rates for both behavioral descriptions and joint attention judgments

3. Implement cross-validation with temporal segmentation to evaluate whether the MLLM can maintain accuracy across different interaction phases and contexts