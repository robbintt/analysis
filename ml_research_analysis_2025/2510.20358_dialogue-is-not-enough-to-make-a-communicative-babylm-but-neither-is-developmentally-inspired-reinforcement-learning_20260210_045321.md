---
ver: rpa2
title: Dialogue Is Not Enough to Make a Communicative BabyLM (But Neither Is Developmentally
  Inspired Reinforcement Learning)
arxiv_id: '2510.20358'
source_url: https://arxiv.org/abs/2510.20358
tags:
- language
- reward
- data
- learning
- fine-tuning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study investigates whether small language models pre-trained
  exclusively on child-caregiver dialogue data can acquire communicative competence,
  and whether reinforcement learning can further enhance this ability. The authors
  train a 135M-parameter model (llamalogue) on 10M lexical tokens of CHILDES dialogue
  triplets, then fine-tune it using two approaches: Direct Preference Optimization
  (DPO) with either naturalistic or synthetic dialogue pairs, and Proximal Policy
  Optimization (PPO) with various reward functions.'
---

# Dialogue Is Not Enough to Make a Communicative BabyLM (But Neither Is Developmentally Inspired Reinforcement Learning)

## Quick Facts
- arXiv ID: 2510.20358
- Source URL: https://arxiv.org/abs/2510.20358
- Reference count: 39
- Primary result: Dialogue pre-training and DPO fine-tuning improve dialogue continuation prediction but do not generalize to formal language benchmarks

## Executive Summary
This study investigates whether small language models pre-trained exclusively on child-caregiver dialogue data can acquire communicative competence, and whether reinforcement learning can further enhance this ability. The authors train a 135M-parameter model (llamalogue) on 10M lexical tokens of CHILDES dialogue triplets, then fine-tune it using two approaches: Direct Preference Optimization (DPO) with either naturalistic or synthetic dialogue pairs, and Proximal Policy Optimization (PPO) with various reward functions. The base model underperforms on most BabyLM benchmarks but excels at dialogue continuation prediction in minimal pair settings (63-64% accuracy). DPO fine-tuning on naturalistic data further improves dialogue prediction accuracy to 67-68%, while PPO shows mixed results with no consistent gains on formal benchmarks. None of the fine-tuning strategies improve performance on syntactic or semantic benchmarks, suggesting that current reinforcement learning approaches are not effective at generalizing communicative competence to broader linguistic abilities.

## Method Summary
The authors train llamalogue, a 135M-parameter Llama model, on 10M lexical tokens of CHILDES dialogue triplets (preserving conversational context with speaker tags). The model is pre-trained for 10 epochs using an autoregressive objective. Two fine-tuning approaches are then applied: DPO using either naturalistic (real caregiver-child pairs) or synthetic (LLM-generated) dialogue pairs for 10 epochs, and PPO with four different reward functions (BLEU, semantic similarity, LLM score, teacher confidence) for 3 epochs. The model is evaluated on BabyLM benchmarks (BLiMP, COMPS, EWoK, etc.) and custom dialogue minimal pairs comparing real vs. random child responses.

## Key Results
- Base model achieves 63-64% accuracy on dialogue continuation minimal pairs but near-chance performance on most BabyLM benchmarks
- DPO fine-tuning on naturalistic dialogue pairs improves dialogue MP accuracy to 67-68%, outperforming synthetic data by ~4%
- PPO fine-tuning shows mixed to adversarial effects with no consistent improvements on formal benchmarks
- No fine-tuning method improves performance on syntactic or semantic benchmarks despite improved dialogue prediction

## Why This Works (Mechanism)

### Mechanism 1: Dialogue Triplet Pre-training for Contingent Structure Learning
- Claim: Pre-training on dialogue triplets enables the model to learn contingent conversational patterns and turn-level coherence that transfer to dialogue continuation tasks.
- Mechanism: By preserving three consecutive turns with speaker tags, the autoregressive objective forces the model to predict contextually appropriate responses that depend on prior turns.
- Core assumption: Contingent dialogue structure provides implicit learning signals for communicative competence beyond what isolated sentences can offer.
- Evidence anchors:
  - [abstract] "Although our models underperform on most standard BabyLM benchmarks, they excel at dialogue continuation prediction in a minimal pair setting."
  - [section 3.1] "By using dialogue data only, we assume that the autoregressive pretraining process pushes our BabyLM to model contingent structure (responses depend on previous turns), learn turn-level coherence, and acquire some knowledge about implicit expectations in communication"
  - [corpus] ContingentChat (arXiv:2510.20411) explores multi-turn contingency in BabyLMs via teacher-student frameworks, complementing this pre-training-only approach.
- Break condition: If dialogue triplets are preprocessed into isolated sentences (standard BabyLM practice), communicative context is destroyed and this mechanism fails.

### Mechanism 2: DPO for Preference Learning on Naturalistic Dialogue Pairs
- Claim: Direct Preference Optimization on authentic caregiver-child pairs improves dialogue continuation accuracy more effectively than synthetic LLM-generated pairs.
- Mechanism: DPO optimizes the model to assign higher probability to real child responses compared to randomly sampled distractors. Length-matching controls for surface confounds; the preference signal captures genuine interactional appropriateness.
- Core assumption: Real human dialogue contains interactional patterns (fragmentation, contingency, pragmatics) that synthetic LLM generation cannot faithfully reproduce.
- Evidence anchors:
  - [abstract] "DPO fine-tuning on naturalistic data further improves their performance on our custom dialogue benchmark."
  - [section 4.2.2] "the model fine-tuned on real caregiver–child interaction data scores approximately 4% higher than the base model and the model fine-tuned on artificially generated child utterances... natural data is clearly superior to synthetic data when trying to optimize for this task."
  - [corpus] Weak corpus support—no directly comparable DPO-on-dialogue studies found in neighbors.
- Break condition: If synthetic data diverges from authentic interactional patterns (verbose, non-fragmentary, over-grammatical), the learned preferences will not transfer to real dialogue evaluation.

### Mechanism 3: PPO Reward Functions and Limited Transfer to Formal Competence
- Claim: PPO with various reward formulations can shape model behavior but does not consistently improve formal linguistic benchmarks; rewards optimized for communication do not generalize to syntax/semantics.
- Mechanism: PPO uses policy gradient updates with rewards computed by comparing model outputs to teacher-generated references (BLEU, semantic similarity, LLM score, confidence ranking). The reward signal targets communicative appropriateness but this learning does not transfer.
- Core assumption: A scalar reward derived from teacher comparisons encodes aspects of communicative competence that would transfer to broader linguistic evaluation.
- Evidence anchors:
  - [abstract] "While PPO fine-tuning has mixed to adversarial effects on our models, DPO fine-tuning further improves their performance on our custom dialogue benchmark."
  - [section 5] "Such fine-tuning with a specific, pragmatics- or communication-based goal in mind has so far only shown to improve performance on benchmarks that also test for this goal."
  - [corpus] Stöpler et al. (arXiv:2505.05970) found similar results: communicative success rewards changed speaker behavior but did not improve linguistic benchmarks.
- Break condition: If the single teacher reference is semantically or pragmatically distant from model outputs, the reward provides noisy or misleading gradients, potentially causing training instability.

## Foundational Learning

- Concept: **Autoregressive Language Modeling**
  - Why needed here: The base llamalogue model uses next-token prediction on dialogue triplets. Understanding this explains why the model excels at dialogue continuation but underperforms on broader benchmarks that require different generalizations.
  - Quick check question: Given "*MOT: where's the kitty? *CHI:", what would an autoregressive model predict, and how does dialogue context constrain predictions differently than isolated sentence modeling?

- Concept: **Direct Preference Optimization (DPO)**
  - Why needed here: DPO is the more successful fine-tuning approach in this study. Understanding how DPO uses preference pairs rather than scalar rewards clarifies its advantage over PPO for this task.
  - Quick check question: Given a prompt and two responses (preferred vs. dispreferred), how does DPO's loss function differ from standard supervised fine-tuning?

- Concept: **Minimal Pair Evaluation**
  - Why needed here: The paper's positive result (63-68% on dialogue MP) relies on minimal pair methodology. Understanding this paradigm explains what above-chance accuracy means.
  - Quick check question: In a dialogue minimal pair task with a real vs. random child response, what does 64% accuracy indicate about the model's learned preferences vs. syntactic knowledge?

## Architecture Onboarding

- Component map:
  - CHILDES dialogue triplets -> llamalogue 135M model (16 layers, 16 heads, hidden=1024) -> DPO fine-tuning (naturalistic/synthetic pairs) or PPO fine-tuning (4 reward variants) -> BabyLM benchmarks + dialogue minimal pairs

- Critical path:
  1. Preprocess CHILDES -> extract triplets (>=2 speakers, >=5 lexical words, keep speaker tags)
  2. Pre-train llamalogue (10 epochs)
  3. Build DPO pairs (length-matched natural/synthetic) OR configure PPO rewards
  4. Fine-tune with selected method
  5. Evaluate on BabyLM benchmarks + dialogue MP

- Design tradeoffs:
  - Small vocab (8k) vs. larger: Reduces sparsity for CDS but may miss rare words
  - Triplets vs. isolated sentences: Preserves interactional context but limits effective data
  - Naturalistic vs. synthetic DPO: Authenticity vs. quantity/grammaticality
  - PPO reward design: Different operationalizations of "communicative quality" may not align

- Failure signatures:
  - PPO training instability (reward drops, crashes per Section 4.2.3)
  - DPO synthetic data underperforming naturalistic by ~4% on dialogue MP
  - All fine-tuning methods showing near-chance on BLiMP while improving dialogue MP
  - Negative or near-zero correlations on morphological generalization (Wug tasks)

- First 3 experiments:
  1. **Dialogue structure ablation**: Train two models—one on triplets, one on isolated sentences from the same data. Evaluate on both dialogue MP and BLiMP to isolate the contribution of preserved interactional context.
  2. **DPO data mixing study**: Run DPO with 100% natural, 100% synthetic, and 50/50 mixed data. Measure dialogue MP accuracy to quantify the authenticity gap and test whether synthetic data can supplement limited naturalistic pairs.
  3. **PPO reward correlation analysis**: Run one epoch of PPO with each reward type, logging per-example rewards and downstream dialogue MP performance. Identify which reward formulations correlate with desired outputs before instability emerges.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can curriculum-based reward schedules that alternate between exploration and exploitation phases, inspired by human developmental patterns, improve PPO fine-tuning effectiveness for communicative language learning?
- Basis in paper: [explicit] "Notably, our study did not incorporate such a curriculum in the reward design... It would be interesting for future work to explore this direction and assess whether exploration/exploitation reward patterns inspired by human developmental trends could yield greater benefits."
- Why unresolved: The study used static reward functions throughout training, while child development involves dynamic alternation between imitation-based exploitation and self-generated exploration phases.
- What evidence would resolve it: Experiments comparing static vs. developmentally-inspired curriculum reward schedules across PPO fine-tuning runs, measuring both dialogue coherence and formal benchmark performance.

### Open Question 2
- Question: Would incorporating adult–adult dialogue data alongside child-caregiver dialogue improve lexical diversity and syntactic generalization while maintaining communicative competence?
- Basis in paper: [explicit] "The incorporation of adult–adult dialogue into our training regimen might be a promising direction for future research."
- Why unresolved: Models trained exclusively on CDS exhibit limited vocabulary and syntactic variety, constraining generalization to benchmarks like BLiMP that target broader grammatical phenomena.
- What evidence would resolve it: Comparative experiments training models on varying proportions of adult-adult vs. child-caregiver dialogue, evaluated on both dialogue minimal pairs and formal syntactic benchmarks.

### Open Question 3
- Question: To what extent do suboptimal hyperparameter choices vs. fundamental reward function design explain the ineffectiveness of PPO fine-tuning observed in this study?
- Basis in paper: [explicit] "our fine-tuning phases with DPO and PPO were conducted without a previous extensive hyperparameter search... future work should place greater emphasis on systematically identifying the optimal hyperparameters for each reward function prior to training"
- Why unresolved: PPO showed mixed to adversarial effects, but it remains unclear whether this reflects intrinsic limitations of the reward functions or insufficient hyperparameter optimization.
- What evidence would resolve it: Systematic hyperparameter sweeps for each PPO reward type (BLEU, semantic similarity, LLM score, teacher confidence) before drawing conclusions about reward function efficacy.

## Limitations
- Small dataset size (10M tokens) and vocabulary constraints (8465 tokens) limit generalization to broader linguistic tasks
- PPO training instability and reward collapse across all four reward formulations suggest fundamental issues with current RL approach
- Minimal pair evaluation uses a relatively small evaluation set (8K pairs) which may not provide robust performance estimates

## Confidence
- High Confidence: Dialogue triplet pre-training enables above-chance dialogue continuation prediction (63-64%) with well-supported minimal pair methodology
- Medium Confidence: DPO fine-tuning on naturalistic data improves dialogue prediction (67-68%) while synthetic data underperforms
- Low Confidence: Claim that current RL approaches cannot improve formal language competencies due to lack of systematic exploration of alternative reward designs and hyperparameter settings

## Next Checks
1. **Reward Design Exploration**: Systematically test a broader range of reward formulations including pairwise ranking rewards, curriculum-based rewards, and multi-task rewards that combine communicative success with syntactic/semantic correctness.

2. **Model Scale and Data Augmentation**: Evaluate whether scaling up to a 350M or 1.3B parameter model, or augmenting the 10M token dataset with additional dialogue data (while maintaining the triplet structure), can improve performance on formal benchmarks without sacrificing dialogue abilities.

3. **Transfer Learning Analysis**: Conduct a controlled study comparing models fine-tuned with DPO on naturalistic pairs versus models fine-tuned on synthetic pairs but with additional syntactic/semantic supervision.