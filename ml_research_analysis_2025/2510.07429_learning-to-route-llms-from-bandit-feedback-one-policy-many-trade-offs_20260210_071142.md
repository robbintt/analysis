---
ver: rpa2
title: 'Learning to Route LLMs from Bandit Feedback: One Policy, Many Trade-offs'
arxiv_id: '2510.07429'
source_url: https://arxiv.org/abs/2510.07429
tags:
- cost
- performance
- arxiv
- tasks
- score
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of efficiently routing queries
  to the most suitable large language model (LLM) from a pool of candidates, balancing
  performance and cost. The core method, BARP (Bandit-feedback Routing with Preferences),
  formulates LLM routing as a multi-objective contextual bandit problem.
---

# Learning to Route LLMs from Bandit Feedback: One Policy, Many Trade-offs

## Quick Facts
- arXiv ID: 2510.07429
- Source URL: https://arxiv.org/abs/2510.07429
- Authors: Wang Wei; Tiankai Yang; Hongjie Chen; Yue Zhao; Franck Dernoncourt; Ryan A. Rossi; Hoda Eldardiry
- Reference count: 19
- One-line primary result: BARP consistently outperforms strong offline routers by at least 12.46% and the largest LLM by at least 2.45%, while generalizing robustly to unseen tasks.

## Executive Summary
This paper addresses the challenge of efficiently routing queries to the most suitable large language model (LLM) from a pool of candidates, balancing performance and cost. The core method, BARP (Bandit-feedback Routing with Preferences), formulates LLM routing as a multi-objective contextual bandit problem. BARP learns from bandit feedback—only observing the outcome of the chosen model—while conditioning on a user preference vector that specifies the trade-off between accuracy and cost. This allows the router to adapt to different user priorities without retraining. Comprehensive experiments show that BARP consistently outperforms strong offline routers by at least 12.46% and the largest LLM by at least 2.45%, while generalizing robustly to unseen tasks.

## Method Summary
BARP trains a routing policy under bandit-feedback constraints, where only the reward from the chosen LLM is observed. The policy conditions on both the prompt and a preference vector specifying the performance-cost trade-off, enabling test-time adjustment without retraining. It uses REINFORCE with entropy regularization for exploration, and an MLP-based preference encoder to learn representations that enable controllable trade-offs at inference.

## Key Results
- BARP outperforms offline routers by at least 12.46% in average score while reducing cost
- BARP outperforms the largest LLM by at least 2.45% in average score
- REINFORCE with entropy regularization outperforms classical bandit algorithms (LinTS, LinUCB, ε-greedy) due to non-linear policy capacity

## Why This Works (Mechanism)

### Mechanism 1: Bandit-Feedback Alignment Between Training and Deployment
- Claim: Training under partial-feedback constraints produces routers that generalize better to deployment conditions than full-information offline supervision.
- Mechanism: During training, after sampling action $a_t \sim \pi_\theta(\cdot|x_t, w_t)$, the supervision signal is restricted to only the outcome (score $q_t$ and cost $c_t$) of the selected LLM. This simulates deployment where alternatives' outcomes are unobservable. The reward $r_t = w_q^t q_t - w_c^t \tilde{c}_t$ drives policy updates via REINFORCE.
- Core assumption: The bandit-feedback training distribution sufficiently approximates the true deployment distribution for learned policies to transfer.
- Evidence anchors:
  - [abstract] "trains under the same partial-feedback restriction as deployment"
  - [Section 2.1] Defines context as $(x_t, w_t)$, action as LLM selection, and reward as bandit feedback from chosen arm only
  - [corpus] Related work "Adaptive LLM Routing under Budget Constraints" addresses routing but doesn't explicitly use bandit formulation; weak direct corpus support for this specific mechanism
- Break condition: If deployment has systematic distribution shift (e.g., new query types, cost structure changes) not captured in training logs, bandit alignment may not transfer.

### Mechanism 2: Preference-Conditioned Policy for Test-Time Trade-off Control
- Claim: Conditioning the routing policy on a preference vector enables runtime adjustment of performance-cost trade-offs without retraining.
- Mechanism: The preference vector $w = (w_q, w_c)$ on the 1-simplex is encoded via MLP $\phi(w)$ and concatenated with prompt embedding $h(x)$. During training, $w_t$ is uniformly sampled; at inference, operators set $w$ to prioritize performance ($w_c$ low) or cost ($w_c$ high). The policy $\pi_\theta(a|x,w)$ adapts its distribution accordingly.
- Core assumption: Random sampling during training covers the preference space sufficiently for interpolation at test time.
- Evidence anchors:
  - [abstract] "operators can dial the performance–cost trade-off at test time without retraining"
  - [Section 4.4.1, Figure 3] Shows varying $w_c$ from 0.2 to 0.8 systematically reduces cost from $0.074 to $0.015 while predictably affecting scores
  - [corpus] No direct corpus comparison for preference-tunable inference; "Reward Model Routing in Alignment" addresses routing but for reward models, not LLMs
- Break condition: If requested preference falls outside training distribution (e.g., extreme weights not sampled), policy behavior may be unpredictable.

### Mechanism 3: Entropy-Regularized Policy Gradient for Exploration Under Partial Feedback
- Claim: REINFORCE with entropy regularization outperforms classical bandit algorithms for LLM routing due to non-linear policy capacity.
- Mechanism: Loss $L_t(\theta) = -(r_t - b_t)\log\pi_\theta(a_t|s_t) - \beta H(\pi_\theta)$ where $b_t$ is batch-mean baseline for variance reduction and $\beta=0.05$ encourages exploration. The MLP decision head captures non-linear relationships between (prompt, preference) context and optimal LLM choice.
- Core assumption: The reward function $r_t$ adequately captures user utility, and entropy coefficient $\beta$ balances exploration/exploitation appropriately.
- Evidence anchors:
  - [Section 2.3] Defines the entropy-regularized objective and REINFORCE updates
  - [Section 4.6, Table 7] REINFORCE achieves 0.7432 avg score vs. 0.6430 (LinTS), 0.6166 (LinUCB), 0.6556 ($\epsilon$-greedy), attributed to non-linear policy learning
  - [corpus] "Efficient Adversarial Attacks on High-dimensional Offline Bandits" discusses bandit vulnerabilities but doesn't compare learning algorithms
- Break condition: If entropy regularization is too weak, insufficient exploration may cause suboptimal convergence; if too strong, policy may fail to exploit learned patterns.

## Foundational Learning

- Concept: Contextual Bandits
  - Why needed here: The paper formulates routing as a contextual bandit where context $(x_t, w_t)$ is observed, action $a_t$ is selected, and only that action's reward is observed. Understanding the exploration-exploitation trade-off is essential.
  - Quick check question: Given context $(x, w)$ and three candidate LLMs, if you only observe the reward from your chosen LLM, how do you learn which LLM is best for similar contexts?

- Concept: Policy Gradient Methods (REINFORCE)
  - Why needed here: The router is trained via REINFORCE, which estimates $\nabla_\theta J(\theta)$ using sampled trajectories. Understanding baseline subtraction and entropy regularization is critical for debugging training.
  - Quick check question: Why does subtracting a baseline $b_t$ from reward reduce variance without introducing bias in policy gradient estimation?

- Concept: Multi-Objective Optimization via Scalarization
  - Why needed here: The reward $r_t = w_q^t q_t - w_c^t \tilde{c}_t$ scalarizes two objectives (performance, cost) using preference weights. This enables controllable trade-offs but assumes linear utility structure.
  - Quick check question: If a user's true utility is $U(q, c) = \sqrt{q} - \alpha c^2$, would the linear scalarization still enable optimal trade-off control?

## Architecture Onboarding

- Component map:
  Input: (prompt x, preference w)
  ↓
  Prompt Encoder h(x): frozen all-MiniLM-L6-v2 → 384-dim embedding
  ↓
  Preference Encoder ϕ(w): 2-dim → d_p-dim MLP with ReLU
  ↓
  Concatenation: z = [h(x); ϕ(w)]
  ↓
  Decision Head g_θ(z): MLP with ReLU → K logits (one per LLM)
  ↓
  Softmax → π_θ(a|x, w)
  ↓
  Inference: argmax_a π_θ(a|x, w)

- Critical path: The preference encoder is the key trainable component enabling test-time control. If it fails to learn meaningful representations, preference conditioning collapses. Monitor $\phi(w)$ output variance across different $w$ values during training.

- Design tradeoffs:
  - Prompt encoder: Larger encoders (E5-large-v2, 1024-dim) showed no significant improvement over MiniLM (384-dim) in Table 5, suggesting compact encoders suffice for routing semantics.
  - Decision head: MLP outperforms linear and bilinear heads (Table 6), but adds parameters. Linear head is a strong baseline if parameter efficiency is critical.
  - Learning algorithm: Classical bandit algorithms (LinUCB, LinTS) are simpler but underperform REINFORCE (Table 7) due to linear assumptions.

- Failure signatures:
  - **Preference-insensitive routing**: If scores don't vary with $w_c$ during inference, $\phi(w)$ may have collapsed (check gradient flow to preference encoder).
  - **Cost myopia**: If router always selects cheapest LLM regardless of preference, entropy regularization may be too strong or cost scaling $\tau$ may be poorly calibrated.
  - **OOD collapse**: If performance drops sharply on unseen tasks (like RouterDC/GraphRouter in Table 3), the prompt encoder may lack generalizable representations.

- First 3 experiments:
  1. **Preference sensitivity test**: Run inference with $w_c \in \{0.0, 0.25, 0.5, 0.75, 1.0\}$ on held-out prompts. Plot score vs. cost curves. Expected: monotonic cost decrease as $w_c$ increases (Figure 3 pattern).
  2. **Bandit vs. full-information baseline**: Train a variant with access to all LLM scores (full supervision) using same architecture. Compare to BaRP to quantify the cost of bandit feedback restriction.
  3. **Encoder ablation**: Replace MiniLM with BERT-base and E5-large-v2 (Table 5 replication). If MiniLM underperforms on your domain-specific queries, consider domain-adapted encoders.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can BaRP be adapted to learn continuously from live feedback in a truly online setting, rather than training on static, offline logs?
- Basis in paper: [explicit] The authors state their method "trains on static, offline logs, which is practical but differs from a truly online setting where a router could learn continuously from live feedback."
- Why unresolved: The current implementation simulates a bandit environment using pre-existing logs but does not handle the non-stationarity of a live deployment where model performance or availability may shift.
- What evidence would resolve it: A deployment study showing the policy successfully adapting to sudden changes in model pricing or performance drift without manual retraining.

### Open Question 2
- Question: How can the contextual bandit formulation be extended to optimize routing for multi-turn, conversational scenarios?
- Basis in paper: [explicit] The authors note the formulation "models routing as a single-step decision... not explicitly designed for multi-turn, conversational scenarios."
- Why unresolved: The current state representation ($s_t$) relies on a single prompt and preference vector, lacking the history mechanisms needed to maintain coherence across a dialogue session.
- What evidence would resolve it: Modifying the state space to include dialogue history and demonstrating that the router maintains context consistency better than single-step baselines.

### Open Question 3
- Question: Can the reward function be generalized to incorporate richer constraints such as latency or task-specific requirements beyond performance and cost?
- Basis in paper: [explicit] The authors acknowledge they "only consider performance and monetary cost, while real deployments may require richer, possibly task-specific preferences and constraints (e.g., latency)."
- Why unresolved: The scalar reward (Eq. 1) is currently limited to a weighted sum of accuracy and normalized cost, ignoring operational constraints that often dictate routing decisions in production.
- What evidence would resolve it: Integrating latency as a variable in the preference vector and showing the router can satisfy strict Service Level Agreements (SLAs) while balancing accuracy.

## Limitations
- The method depends on accurate cost feedback signals, which may be noisy or delayed in multi-tenant serving systems
- The bandit training distribution may not cover deployment conditions, leading to performance degradation with systematic distribution shifts
- The linear utility scalarization may not capture complex user utility functions with non-linear preferences

## Confidence
- **High Confidence**: Claims about BARP's empirical performance (12.46% improvement over offline routers, 2.45% improvement over largest LLM) are supported by comprehensive experiments on multiple datasets. The mechanism of preference-conditioned routing for test-time control is well-validated through systematic experiments varying $w_c$ and observing predictable score/cost trade-offs.
- **Medium Confidence**: The bandit-feedback alignment mechanism is theoretically sound, but the paper lacks ablation studies showing how much performance degrades if deployment distribution shifts beyond training distribution. The superiority of REINFORCE over classical bandit algorithms is demonstrated, but the analysis focuses on aggregate performance rather than failure mode characterization.
- **Low Confidence**: The claim that MiniLM-6 embeddings are sufficient for routing decisions is based on limited ablation (Table 5), and domain-specific applications may require larger or fine-tuned encoders. The paper doesn't explore the robustness of the preference encoder $\phi(w)$ to extreme or out-of-distribution preference vectors.

## Next Checks
1. **Distribution Shift Robustness Test**: Hold out an entire domain or task type during training and evaluate BARP's performance on this unseen distribution. Compare to offline routers to quantify generalization failure modes.

2. **Preference Encoder Stability Analysis**: During training, systematically probe $\phi(w)$ responses to preference vectors increasingly far from the training distribution. Monitor for output collapse or saturation, and test inference behavior on these extreme preferences.

3. **Cost Feedback Noise Sensitivity**: Introduce varying levels of noise to cost signals $\tilde{c}_t$ during training (Gaussian noise with increasing variance) and measure degradation in routing performance. This validates the robustness of the bandit learning algorithm to cost estimation errors.