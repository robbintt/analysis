---
ver: rpa2
title: 'CABENCH: Benchmarking Composable AI for Solving Complex Tasks through Composing
  Ready-to-Use Models'
arxiv_id: '2508.02427'
source_url: https://arxiv.org/abs/2508.02427
tags:
- tasks
- task
- solution
- composable
- each
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CABENCH, the first benchmark for Composable
  AI (CA), which tackles complex AI tasks by decomposing them into sub-tasks and solving
  each using ready-to-use models. CABENCH includes 70 realistic tasks and a curated
  pool of 700 models spanning multiple modalities.
---

# CABENCH: Benchmarking Composable AI for Solving Complex Tasks through Composing Ready-to-Use Models

## Quick Facts
- arXiv ID: 2508.02427
- Source URL: https://arxiv.org/abs/2508.02427
- Authors: Tung-Thuy Pham; Duy-Quan Luong; Minh-Quan Duong; Trung-Hieu Nguyen; Thu-Trang Nguyen; Son Nguyen; Hieu Dinh Vo
- Reference count: 18
- One-line primary result: CABENCH is the first benchmark for Composable AI (CA), revealing that automated LLM-based approaches lag significantly behind human-designed solutions, with gaps up to 90% on complex tasks.

## Executive Summary
This paper introduces CABENCH, the first benchmark designed to evaluate Composable AI (CA) methods that solve complex tasks by decomposing them into sub-tasks and composing ready-to-use models into executable pipelines. CABENCH includes 70 realistic tasks and a curated pool of 700 models spanning multiple modalities. The authors propose a full-pipeline evaluation framework for end-to-end assessment of CA solutions. Experimental results compare two LLM-based approaches—Prompt-to-Solve and Prompt-to-Pipeline—against human-designed reference solutions, showing that Prompt-to-Solve achieves an average accuracy of 0.56, more than double Prompt-to-Pipeline, but still falls short of human-designed solutions by up to 90%.

## Method Summary
CABENCH is a benchmark suite for Composable AI that includes 70 tasks derived from popular datasets and a curated pool of ~700 models from Hugging Face. Each task is defined by its input/output spaces, objective, and query set. The evaluation framework executes generated pipelines (structured as DAGs) and computes normalized scores using task-specific metrics. The paper proposes two LLM-based baselines—Prompt-to-Solve (direct answers) and Prompt-to-Pipeline (decompose, select models, compose)—tested across three prompting patterns (Zero-shot, Few-shot, CoT) using GPT-4o-mini. Results show automated methods lag significantly behind human-designed solutions, especially on complex, graph-structured tasks.

## Key Results
- Prompt-to-Solve achieves an average accuracy of 0.56, more than double Prompt-to-Pipeline's performance.
- Both automated approaches fall short of human-designed reference solutions by up to 90%.
- The performance gap is largest on complex "Graph Solution" tasks requiring compositional reasoning and structured execution.
- Prompt-to-Solve fails on tasks requiring specialized models (e.g., Speech Recognition), while Prompt-to-Pipeline struggles with pipeline orchestration and context limitations.

## Why This Works (Mechanism)
CABENCH enables systematic evaluation of CA methods by providing a standardized benchmark with diverse tasks and a fixed model pool. The framework supports end-to-end assessment by executing generated pipelines and comparing outputs to ground truth. This setup allows researchers to measure both the decomposition and composition capabilities of CA systems, highlighting the gap between automated and human-designed solutions.

## Foundational Learning
- **Concept**: **Directed Acyclic Graphs (DAGs) for Workflow Orchestration**
  - **Why needed here**: The core definition of a Composable AI solution in CABENCH is an executable pipeline structured as a DAG. Understanding nodes (computational units) and edges (data flow) is fundamental to designing or generating a valid solution.
  - **Quick check question**: Can you explain why a pipeline with a cycle might cause an execution failure?

- **Concept**: **Model Modality and Interface Compatibility**
  - **Why needed here**: Selecting models requires matching their input/output modalities (e.g., audio, text, image) to the task's needs and to each other. A breakdown here necessitates "glue code" or invalidates the solution.
  - **Quick check question**: If Model A outputs a JSON string and Model B expects a Python dictionary, what component is required to connect them?

- **Concept**: **Prompt Engineering Strategies (Zero-shot, Few-shot, Chain-of-Thought)**
  - **Why needed here**: The paper uses these as key variables in its baseline experiments. Understanding their trade-offs is crucial for interpreting the LLM-based results and for future method development.
  - **Quick check question**: Which prompting strategy provides the LLM with examples before asking it to solve the actual query?

## Architecture Onboarding
- **Component map**:
  1. **CABENCH Benchmark Suite**: Contains 70 realistic tasks (with descriptions, query sets, ground truth) and a curated pool of ~700 ready-to-use models.
  2. **Task Specification (T)**: Formal definition of a task, including input space (X), output space (Y), objective (O), and query set (Q).
  3. **Solution Pipeline (G)**: A Directed Acyclic Graph (DAG) of computational nodes (models or glue code) connected by edges defining data flow.
  4. **Evaluation Framework**: An automated system that takes a pipeline G and a query set Q, executes the pipeline, and compares the output against ground truth using task-appropriate metrics (e.g., Accuracy, F1).

- **Critical path**:
  1. **Input**: A complex task description and query (e.g., audio file + image file → verify claim).
  2. **Decomposition (Mental or LLM-generated)**: Break into sub-tasks (e.g., transcribe audio, extract image text, compare similarity).
  3. **Model Selection**: Pick models from the pool for each sub-task (e.g., `openai/whisper-large-v3` for transcription).
  4. **Glue Code Generation**: Write code to handle data transformations between models (e.g., standardizing text format, mapping a score to a verdict).
  5. **Execution**: The framework runs the composed pipeline on the input queries.
  6. **Evaluation**: Framework computes normalized scores (0-1) by comparing outputs to ground truth.

- **Design tradeoffs**:
  - **Prompt-to-Solve vs. Prompt-to-Pipeline**: The former is simpler and currently more effective for tasks within an LLM's knowledge, but fails on tasks requiring specialized external models. The latter is more generalizable but is currently brittle due to complex orchestration and context length limits.
  - **Fixed Model Pool vs. Open Repositories**: The benchmark uses a fixed pool of 700 models for reproducibility, which trades off the breadth of available solutions for experimental control.
  - **Automation vs. Human Design**: Fully automated solutions currently lag significantly in performance (up to 90% gap), indicating that human intuition for decomposition and composition is still superior.

- **Failure signatures**:
  - **Prompt-to-Solve Failure**: Score of 0.00 on tasks requiring specialized perception (e.g., Speech Recognition, Sentence Similarity in Table I) because the general LLM lacks the necessary domain-specific capabilities.
  - **Prompt-to-Pipeline Failure**: Low or zero scores across complex task types (Table I) due to invalid pipeline generation, context truncation limiting model knowledge, or failure to generate correct glue code.
  - **Format Mismatch**: A pipeline fails to execute if the output of one node does not match the input format expected by the next node, and no glue code handles the transformation.

- **First 3 experiments**:
  1. **Reproduce Baselines**: Implement and run the `Prompt-to-Solve` (Zero-shot) and `Prompt-to-Pipeline` (Zero-shot) strategies on a subset of tasks to replicate the performance gap reported in Table I.
  2. **Ablate Model Pool Context**: For `Prompt-to-Pipeline`, experiment with different strategies for truncating or ordering the model pool in the prompt to measure the impact of context limitations on pipeline generation quality.
  3. **Test on "Easy" vs. "Hard" Task Types**: Compare the performance of both strategies on "Atomic" tasks (single node) vs. "Graph" tasks (complex pipeline) as categorized in Table II to quantify the difficulty posed by structural complexity.

## Open Questions the Paper Calls Out
### Open Question 1
- Question: What specific computational methods are required to bridge the performance gap between automated LLM approaches and human-designed pipelines, which currently differs by up to 90%?
- Basis in paper: [explicit] The conclusion states the results "emphasize the need to develop methods for unlocking the full potential of composable AI systems by automatically generating effective pipelines."
- Why unresolved: Current LLM baselines (Prompt-to-Solve and Prompt-to-Pipeline) fail to match the compositional reasoning and structured execution capabilities of humans.
- What evidence would resolve it: A new automated method achieving performance comparable to the Human-designed Reference Solution on CABENCH.

### Open Question 2
- Question: How can automated agents be improved to effectively decompose tasks and compose pipelines for "Graph Solution" problems where multiple nodes have interdependent data flows?
- Basis in paper: [explicit] The results show the performance gap is highest (up to 85%) on complex tasks requiring "graph-structured pipelines," where LLMs struggle significantly compared to simpler chain structures.
- Why unresolved: The paper notes that LLMs lack the capability to manage the topological complexity and interoperability required for graph-based compositions.
- What evidence would resolve it: An automated agent demonstrating high accuracy on the "Graph Solution" subset of CABENCH tasks.

### Open Question 3
- Question: How can the prompt context limitation be overcome to allow LLMs to perform optimal model selection from large pools without random truncation?
- Basis in paper: [inferred] The experimental setup notes that "prompt length limitation... prevents the LLM from accessing the full context of the model pool," forcing the authors to randomly shuffle and truncate the list, which introduces bias.
- Why unresolved: LLMs cannot currently process the descriptions of all 700 available models simultaneously, leading to sub-optimal selection.
- What evidence would resolve it: A retrieval or indexing mechanism that allows an LLM to identify the best model from the full pool without truncation errors.

## Limitations
- The specific performance gap percentages (e.g., "up to 90%") may vary with different model pools or task distributions.
- Prompt templates and complete task specifications are not fully disclosed, limiting reproducibility.
- The fixed 700-model pool may not represent the full diversity of available models, constraining generalizability.

## Confidence
- High confidence: The benchmark framework's validity, the performance gap between automated and human-designed solutions, and the effectiveness of Prompt-to-Solve over Prompt-to-Pipeline for certain task types.
- Medium confidence: The generalizability of the 90% performance gap across different model pools or task distributions, as these depend on specific experimental conditions.
- Low confidence: The completeness of the model pool in representing real-world model diversity, and the exact reproducibility of results without access to complete prompt templates and task specifications.

## Next Checks
1. Validate the performance gap by implementing the two baseline strategies (Prompt-to-Solve and Prompt-to-Pipeline) on a subset of tasks using the provided benchmark framework.
2. Experiment with different model pool sizes and compositions to assess the impact on automated solution performance and identify potential bottlenecks.
3. Test the framework's robustness by introducing controlled variations in task descriptions and query formats to evaluate the adaptability of both automated and human-designed solutions.