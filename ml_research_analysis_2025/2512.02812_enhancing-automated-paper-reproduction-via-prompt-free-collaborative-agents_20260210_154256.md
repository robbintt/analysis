---
ver: rpa2
title: Enhancing Automated Paper Reproduction via Prompt-Free Collaborative Agents
arxiv_id: '2512.02812'
source_url: https://arxiv.org/abs/2512.02812
tags:
- refinement
- code
- arxiv
- agent
- prompts
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper introduces a prompt-free collaborative agent framework\
  \ to enhance automated paper reproduction by systematically verifying and refining\
  \ outputs at each generation step without requiring manually designed refinement\
  \ prompts. The approach uses two agents\u2014verification and refinement\u2014that\
  \ leverage original system prompts to automatically improve code quality and completeness."
---

# Enhancing Automated Paper Reproduction via Prompt-Free Collaborative Agents

## Quick Facts
- **arXiv ID:** 2512.02812
- **Source URL:** https://arxiv.org/abs/2512.02812
- **Reference count:** 33
- **Primary result:** ~15% accuracy and ~13% completeness improvement over baselines on PaperBench Code-Dev and Paper2CodeBench datasets

## Executive Summary
This paper introduces a prompt-free collaborative agent framework to enhance automated paper reproduction by systematically verifying and refining outputs at each generation step without requiring manually designed refinement prompts. The approach uses two agents—verification and refinement—that leverage original system prompts to automatically improve code quality and completeness. Experimental results show significant performance gains: approximately 15% and 13% improvements in accuracy and completeness, respectively, on PaperBench Code-Dev and Paper2CodeBench datasets compared to baseline methods. The framework also demonstrates superior robustness and efficiency over Self-Refine and RePro, achieving comparable or better results with fewer computational iterations, making it a scalable solution for automated code generation from scientific papers.

## Method Summary
The paper presents a prompt-free collaborative agent framework that enhances automated paper reproduction by integrating verification and refinement agents into existing generation workflows. The method operates by having a Verification Agent examine outputs at each step against the corresponding system prompt, producing structured reports identifying missing information and action items. A Refinement Agent then uses these reports to revise the outputs, improving completeness and correctness. The framework is applied sequentially across multiple stages (Overall Plan → Architecture Design → Logic Design → Configuration → Code Generation), with each refined artifact serving as context for subsequent steps. The approach is evaluated on two datasets: PaperBench Code-Dev (20 ICML 2024 papers) and Paper2CodeBench (90 papers from ICLR/ICML/NeurIPS 2024), demonstrating significant improvements over baseline methods.

## Key Results
- Achieves approximately 15% improvement in accuracy and 13% improvement in completeness on PaperBench Code-Dev dataset
- Demonstrates 4.34 average score improvement on Paper2CodeBench dataset
- Shows superior robustness compared to Self-Refine and RePro baselines with fewer computational iterations
- Outperforms baselines across both planning and coding stages when optimization is applied to both

## Why This Works (Mechanism)

### Mechanism 1: Prompt-Free Verification via System Prompt Alignment
The paper uses the original system prompt as the evaluation standard, ensuring alignment between generation objectives and verification criteria without manual intervention. A Verification Agent ($V$) takes the system prompt ($S_i$) and the generated output ($O_i$) to produce a structured report ($R_i$), checking if the output satisfies requirements already present in $S_i$. The core assumption is that system prompts provided in the original workflow are sufficiently comprehensive to serve as both generation guides and verification checklists.

### Mechanism 2: Closed-Loop Refinement via Structured Feedback
Performance gains appear driven by a closed-loop where a Refinement Agent ($F$) systematically addresses specific "missing information" items identified by the Verification Agent. The Refinement Agent takes the original output ($O_i$), the verification report ($R_i$), and the system prompt ($S_i$) to produce $O^*_i$, following explicit instructions to preserve correct parts and address listed "action items." The underlying LLM must be capable of correctly interpreting the verification report and successfully modifying the code or plan to close identified gaps.

### Mechanism 3: Sequential Dependency Consistency
The framework improves overall robustness by refining artifacts sequentially, ensuring that later steps (e.g., coding) utilize corrected inputs from earlier steps (e.g., planning). The process operates sequentially (Overall Plan → Architecture → Logic → Configuration), with refined output $O^*_i$ passed as context for subsequent generation and refinement steps. The core assumption is that errors in early stages are the primary driver of final code failures, and correcting them propagates benefits linearly.

## Foundational Learning

- **Concept: System Prompts vs. Refinement Prompts**
  - Why needed here: The paper's core innovation is collapsing these two categories. Standard "Self-Refine" methods use distinct prompts for generation and correction, whereas this method uses the *same* prompt for both to save engineering effort and reduce overfitting.
  - Quick check question: If a user changes the desired output format in the system prompt, does the verification agent require a separate update to its evaluation logic? (Answer: No, per this architecture).

- **Concept: Multi-Stage Code Generation Workflows**
  - Why needed here: The agents operate within a specific pipeline (Plan → Code). You need to grasp that the "Verification Agent" is not just checking syntax, but checking semantic alignment with the phase requirements (e.g., "Does the Logic Design include dependency analysis?").
  - Quick check question: In the Paper2Code workflow, which artifact is generated first: the Configuration or the Logic Design?

- **Concept: LLM-Based Evaluation (LLM-as-Judge)**
  - Why needed here: The experimental results rely on LLM-based judges (PaperBench/Paper2CodeBench) to score correctness. Understanding the inherent noise and bias in this evaluation method is critical for interpreting the ~15% improvement claims.
  - Quick check question: What are the risks of using an LLM to grade the output of another LLM in a closed loop?

## Architecture Onboarding

- **Component map:** Paper2Code (Orchestrator) → Verification Agent ($V$: P, S_i, O_i → JSON Report R_i) → Refinement Agent ($F$: P, S_i, O_i, R_i, {O*_j}_{j<i} → Refined Artifact O*_i) → Sequential Accumulation

- **Critical path:** Planning Stage: Verify/Refine Plan → Arch → Logic → Config → Coding Stage: Verify/Refine Python files based on refined Logic and Config

- **Design tradeoffs:** Robustness vs. Granularity (verification prompts prioritize high-level completeness over fine-grained details); Efficiency vs. Iterations (uses 1 iteration vs. RePro's 5)

- **Failure signatures:** Task "mechanistic-understanding" (-26.6%): indicates failure when specific implementation details are critical but verification agent prioritized high-level structure; Self-Refine Overfitting: improvements on one dataset but regressions on another suggest prompt overfitting

- **First 3 experiments:**
  1. Baseline Integration: Run Verification Agent on a single planning step (e.g., Logic Design) with a naive LLM to confirm it can produce a valid JSON report (R_i) from the system prompt
  2. Ablation Study: Compare "Plan-Only Optimization" vs. "Code-Only Optimization" on a small sample to identify which stage drives the majority of the 15% accuracy gain
  3. Robustness Check: Intentionally inject a specific error (e.g., missing dependency in Logic Design) and verify if the sequential Refinement Agent catches it in subsequent steps or if it hallucinates a fix

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions in the text provided.

## Limitations
- The framework shows task-specific weaknesses, particularly for mechanistic-understanding tasks where fine-grained details matter
- Performance relies on undocumented prompt templates and system prompts that were not shared
- Sequential dependency mechanism may propagate hallucinations from early stages to all subsequent artifacts

## Confidence
- **High Confidence:** The sequential agent architecture and general mechanism of using structured verification feedback for refinement are clearly specified and experimentally validated
- **Medium Confidence:** The prompt-free innovation (using system prompts for verification) is logically sound but relies on undocumented prompt templates and system prompts
- **Low Confidence:** The claim that this approach is universally more efficient and scalable than baselines lacks detailed cost analysis comparing iteration counts, token usage, and wall-clock time across all methods

## Next Checks
1. **Prompt Template Validation:** Reconstruct the verification and refinement agent prompts based on the described components (completeness_summary, missing_information, action_items) and test on a small sample to confirm they produce valid structured outputs
2. **Ablation Study Replication:** Independently verify which stage (Planning vs. Coding) contributes most to the ~15% accuracy improvement by running "Plan-Only Optimization" and "Code-Only Optimization" separately on a subset of tasks
3. **Robustness Analysis:** Systematically inject specific errors (e.g., missing dependencies, incorrect hyperparameters) into planning artifacts and trace whether the sequential refinement catches them or propagates them to downstream code generation