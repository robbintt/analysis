---
ver: rpa2
title: 'Green MLOps to Green GenOps: An Empirical Study of Energy Consumption in Discriminative
  and Generative AI Operations'
arxiv_id: '2503.23934'
source_url: https://arxiv.org/abs/2503.23934
tags:
- energy
- consumption
- inference
- training
- power
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study empirically evaluates energy consumption across both
  Discriminative and Generative AI models within MLOps pipelines. For Discriminative
  AI, it analyzes training and inference across various architectures and hyperparameters,
  finding that MACs per parameter strongly correlate with energy usage.
---

# Green MLOps to Green GenOps: An Empirical Study of Energy Consumption in Discriminative and Generative AI Operations

## Quick Facts
- **arXiv ID:** 2503.23934
- **Source URL:** https://arxiv.org/abs/2503.23934
- **Reference count:** 40
- **Primary result:** Empirical study finds energy-efficient ML deployment requires balancing model complexity, hardware utilization, and operational parameters across both discriminative and generative AI workloads.

## Executive Summary
This study empirically evaluates energy consumption across both Discriminative and Generative AI models within MLOps pipelines. For Discriminative AI, it analyzes training and inference across various architectures and hyperparameters, finding that MACs per parameter strongly correlate with energy usage. For Generative AI, it measures inference energy consumption across different LLM sizes and request rates, revealing that smaller models with efficient batching minimize per-request energy costs. The research demonstrates that energy-efficient ML deployment requires balancing model complexity, hardware utilization, and operational parameters, providing practical guidelines for reducing energy consumption while maintaining performance.

## Method Summary
The study employs software-based power measurement using NVML (GPU) and RAPL (CPU/DRAM) to capture energy consumption during training and inference. Discriminative models (CNNs) are trained on CIFAR-10 with fixed hyperparameters, while generative models (LLMs) are deployed using vLLM with 8-bit quantization. Energy is calculated by integrating power readings over time and subtracting baseline idle power. The methodology measures both total energy and energy-per-request metrics across varying batch sizes, model complexities, and request rates.

## Key Results
- MACs per parameter ratio strongly correlates with energy usage in discriminative models
- Smaller LLMs with efficient batching minimize per-request energy costs
- Optimal batch size exists for each hardware configuration that minimizes power consumption
- Energy-per-request decreases with increasing request rates until hardware saturation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Operating AI models at hardware utilization levels near (but not exceeding) saturation minimizes energy per unit of work.
- Mechanism: As hardware utilization increases, the fixed power cost of running the device is amortized over a larger number of computations (e.g., more inference requests, larger batches). This reduces the energy-per-request metric. Beyond the saturation point, resource contention causes throughput gains to plateau, potentially increasing energy-per-request due to inefficiency.
- Core assumption: The hardware's power-performance curve is non-linear and possesses an optimal utilization "sweet spot" that balances high throughput with amortized fixed power costs.
- Evidence anchors:
  - [abstract] "...finding that smaller models with efficient batching minimize per-request energy costs."
  - [section] "For every HC, an optimal batch size exists that minimises power consumption; any further increase in the batch size does not yield additional improvements."
  - [section] "...a placement leading to the hardware being close to its saturation point (but not exceeding that) can lead to the best energy-performance result."
  - [corpus] Corpus evidence is weak; related papers mention energy-aware inference but do not explicitly detail this utilization threshold mechanism.
- Break condition: If hardware power scales perfectly linearly with zero fixed cost, or if contention at high utilization causes throughput to collapse, this mechanism would not hold.

### Mechanism 2
- Claim: Batching inference requests significantly reduces the total energy consumed per individual request.
- Mechanism: Processing requests in batches allows the GPU to leverage parallelism, amortizing fixed energy costs like kernel launches and data transfers across multiple requests. This increases hardware utilization and lowers the energy-per-request.
- Core assumption: The inference engine and hardware can effectively parallelize batched computation, assuming the model architecture is conducive to it.
- Evidence anchors:
  - [abstract] "...finding that smaller models with efficient batching minimize per-request energy costs."
  - [section] LLM analysis shows a strong negative correlation between `request_rate` and energy per sample (-0.95).
  - [section] "...the per-request energy cost initially decreases due to more efficient utilisation of GPU resources."
  - [corpus] Corpus evidence is weak; related papers focus on prompting or code generation but do not contradict the batching mechanism.
- Break condition: If batch size exceeds available memory (OOM errors) or if overhead outweighs parallelism gains for extremely small batches.

### Mechanism 3
- Claim: The ratio of Multiply-Accumulate operations to parameters (`macs_param`) is a stronger predictor of energy use for discriminative models than either metric alone.
- Mechanism: The `macs_param` ratio captures a model's computational intensity per parameter, which correlates better with the combined energy profile of compute and memory access operations than total MACs or parameter count in isolation.
- Core assumption: Model energy cost is a function of both compute-bound (MACs) and memory-bound (parameters) characteristics.
- Evidence anchors:
  - [abstract] "For Discriminative AI... finding that MACs per parameter strongly correlate with energy usage."
  - [section] Table 3 shows `macs_param` has a strong Spearman correlation (ρ ≈ 0.9) with total energy for discriminative models.
  - [section] "Our findings... suggest that the ratio of MACs to model parameters (macs_param) offers a more consistent and reliable predictor..."
  - [corpus] Corpus evidence is absent; related papers do not mention `macs_param`.
- Break condition: May break for models with atypical layers where standard MACs don't reflect hardware FLOPs, or on hardware with vastly different compute-to-memory bandwidth ratios.

## Foundational Learning

- Concept: **Multiply-Accumulate Operations (MACs)**
  - Why needed here: The paper identifies `macs_param` as a key predictor of energy consumption. Understanding MACs is essential for calculating this metric and interpreting its relationship to model complexity.
  - Quick check question: A model has 10 million parameters and requires 200 million MACs for a forward pass. What is its `macs_param` ratio? (Answer: 20)

- Concept: **Amortization of Fixed Costs**
  - Why needed here: This principle explains why higher utilization and batching improve energy efficiency. The fixed energy cost of running hardware is distributed across more tasks, lowering the cost per task.
  - Quick check question: Why does processing 10 requests in a batch typically use less total energy than 10 sequential requests? (Answer: Fixed costs for kernel launches and data transfers are amortized over the batch.)

- Concept: **Correlation vs. Causation**
  - Why needed here: The study is empirical and identifies statistical relationships. It is crucial to interpret these findings as strong evidence-based trends rather than absolute, universal causal laws.
  - Quick check question: If energy per sample decreases with RPS, does doubling RPS guarantee half the energy per sample? (Answer: No, the relationship is non-linear and saturates.)

## Architecture Onboarding

- Component map: Input Data/Requests -> Model (Discriminative/LLM) -> Training/Inference -> Output -> Power Measurement Toolkit (NVML/RAPL) -> Energy Calculation Engine

- Critical path:
  1. Configure the experiment (model, hyperparameters, hardware)
  2. Synchronize the start of the ML workload and the power measurement toolkit
  3. Execute workload to completion
  4. Toolkit records power, calculates total energy, and logs results for analysis

- Design tradeoffs:
  1. **Software vs. Hardware Measurement**: Software tools (NVML/RAPL) are easy to replicate and cost-effective but have a reported variance of ±5W. Hardware meters are precise but complex and expensive.
  2. **Batch Size vs. Energy**: Increasing batch size reduces energy per sample but consumes more memory. An optimal point exists before saturation.
  3. **LLM Size vs. Request Rate**: Larger models offer higher capability but are far less energy-efficient, especially at low request rates. Batching can mitigate this but increases latency.

- Failure signatures:
  1. **OOM (Out of Memory)**: Caused by batch size or model size exceeding available VRAM when pushing for high utilization.
  2. **Latency Spikes**: Occur when request rate pushes the system beyond saturation, leading to queuing delays and potential timeouts.
  3. **Measurement Drift**: Inaccurate energy readings if the sampling interval is too long for short-lived experiments.

- First 3 experiments:
  1. **Baseline Profile**: Measure energy for a single epoch of training and inference for a discriminative model (e.g., ResNet-18) to establish `E_tr` and `E_in`.
  2. **Batch Optimization**: Run inference with increasing batch sizes and plot energy per sample to find the optimal batch size that minimizes energy on your hardware.
  3. **LLM Request Profiling**: Deploy a small and large LLM, send requests at varying rates (RPS), and measure energy per request to observe the efficiency gains from batching.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does energy consumption vary across different specific model layer types (e.g., convolutional, activation, pooling) within neural network architectures?
- Basis in paper: [explicit] The authors state in the conclusion that an "investigation targeting the energy consumption of different model layer types... will give more practical guidelines to ML practitioners who aim to build energy-efficient models."
- Why unresolved: The current study focused on model-level aggregates (total parameters, MACs) rather than isolating the energy cost of individual structural components or layer operations.
- What evidence would resolve it: Fine-grained energy profiling logs that isolate the power draw of individual layer executions during training and inference across standard architectures.

### Open Question 2
- Question: Do the energy efficiency correlations observed on server-grade hardware (e.g., MACs per parameter) hold true for edge devices and ARM-based architectures?
- Basis in paper: [explicit] The authors acknowledge limitations regarding their hardware set, noting that "considering other architectures (e.g., edge devices or ARM-based architectures) and a larger set of hardware configurations will provide more comprehensive results."
- Why unresolved: The empirical data was derived exclusively from high-performance Intel CPUs and Nvidia GPUs, leaving the behavior of low-power or mobile architectures untested.
- What evidence would resolve it: A replication of the study's profiling methodology on edge hardware (e.g., Nvidia Jetson, ARM-based CPUs) to compare the resulting energy-to-accuracy trade-offs.

### Open Question 3
- Question: Can real-time adaptive strategies, such as dynamic model scaling or quantization-aware training, maintain performance while significantly reducing energy consumption in live deployments?
- Basis in paper: [explicit] The authors list "adaptive strategies for energy management" and "adaptive model scaling at runtime" as specific areas for future work, noting their current study did not account for these optimizations.
- Why unresolved: The experiments utilized static model configurations and hyperparameters; they did not evaluate mechanisms that adjust model complexity or precision dynamically based on incoming request loads.
- What evidence would resolve it: Comparative experiments measuring the energy consumption of a static baseline versus a system implementing dynamic, runtime-adaptive inference policies.

### Open Question 4
- Question: What is the net energy impact when integrating energy-efficient practices into a unified, real-world MLOps or GenOps pipeline compared to optimizing components in isolation?
- Basis in paper: [explicit] The conclusion suggests that "integrating all the above practices in a real-world MLOps or GenOps pipeline will reveal more areas of consideration," implying the systemic interaction of these practices is currently unknown.
- Why unresolved: The paper analyzes training and inference mostly as isolated stages or component-level tasks, rather than measuring the overhead and cumulative energy usage of a fully integrated operational pipeline.
- What evidence would resolve it: End-to-end energy measurements of a comprehensive system that integrates the paper's recommended batching, hardware utilization, and model selection strategies.

## Limitations
- Hardware specificity: Results are based on H100/A100 GPUs and may not generalize to consumer hardware
- Measurement variance: Software-based sampling has ±5W variance, introducing potential noise
- Architecture scope: Focuses on CNNs for image classification, limiting applicability to other domains

## Confidence
- **High confidence**: The correlation between MACs-per-parameter ratio and energy consumption for discriminative models is well-supported by experimental data and statistical analysis.
- **Medium confidence**: The utilization threshold mechanism for minimizing energy-per-request requires further validation across different hardware types and workload patterns.
- **Low confidence**: The generalizability of LLM-specific findings to other generative model families (diffusion models, transformers for other tasks) remains uncertain.

## Next Checks
1. **Hardware Generalization**: Reproduce the core findings on consumer-grade GPUs (RTX 4090/3080) to validate whether the saturation utilization principle and batch size optimization hold across different hardware tiers.

2. **Architecture Extension**: Test the MACs-per-parameter correlation on transformer-based discriminative models (BERT, ViT) to assess whether the metric generalizes beyond CNNs.

3. **Measurement Validation**: Implement cross-validation of software-based energy measurements using hardware power meters for at least one model size and workload pattern to quantify systematic measurement biases.