---
ver: rpa2
title: Learning Reward Functions for Cooperative Resilience in Multi-Agent Systems
arxiv_id: '2601.22292'
source_url: https://arxiv.org/abs/2601.22292
tags:
- reward
- resilience
- learning
- agents
- cooperative
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the problem of designing reward functions for
  multi-agent systems that operate in dynamic, uncertain environments and must sustain
  cooperation under disruptions. The core idea is to use inverse reinforcement learning
  (IRL) to infer reward functions from ranked agent trajectories, where rankings are
  based on a cooperative resilience metric.
---

# Learning Reward Functions for Cooperative Resilience in Multi-Agent Systems

## Quick Facts
- arXiv ID: 2601.22292
- Source URL: https://arxiv.org/abs/2601.22292
- Reference count: 40
- One-line primary result: Reward functions learned from resilience-ranked trajectories improve cooperative resilience and reduce catastrophic outcomes under disruptions

## Executive Summary
This paper introduces a framework for learning reward functions that promote cooperative resilience in multi-agent systems operating under disruptions. The approach uses inverse reinforcement learning to infer rewards from ranked agent trajectories, where rankings are based on a cooperative resilience metric that evaluates collective performance across multiple indicators. The method is evaluated in a social dilemma environment with two agents sharing a resource, showing that hybrid rewards combining resilience-based and individual consumption signals significantly improve system performance and sustainability under various disruption scenarios.

## Method Summary
The framework collects trajectories under disruption, ranks them by a cooperative resilience metric (harmonic mean of consumption, resource availability, inequality, and hunger indicators), and uses preference-based inverse reinforcement learning to infer reward functions. Three reward parameterizations are compared: handcrafted features, state-based linear models, and neural networks, with two preference learning methods (margin-based and probabilistic). Agents are trained using PPO or QMIX with hybrid rewards combining the learned resilience signal and individual consumption rewards, then evaluated under expanded disruption protocols.

## Key Results
- Hybrid rewards combining resilience-inferred and individual consumption signals outperform traditional reward strategies
- Handcrafted reward parameterizations achieve higher data efficiency and performance than neural networks with limited trajectory data
- Learned rewards significantly reduce catastrophic outcomes (e.g., last-apple consumption) while maintaining high cumulative consumption
- The approach improves cooperative resilience across multiple disruption types including resource removal and regrowth reduction

## Why This Works (Mechanism)

### Mechanism 1: Resilience-Guided Preference Extraction
Using cooperative resilience scores to rank trajectories creates a structured preference signal that guides reward learning toward sustained collective welfare under disruption. A cooperative resilience metric evaluates trajectories based on performance curves across multiple collective well-being indicators under both baseline and disrupted conditions, providing pairwise preferences that feed into preference-based IRL to learn reward parameters that assign higher cumulative rewards to higher-ranked trajectories.

### Mechanism 2: Hybrid Reward Alignment of Individual and Collective Incentives
Combining inferred resilience-based rewards with individual consumption rewards preserves task performance while promoting cooperative outcomes. The hybrid reward balances exploitation (individual rewards) with sustainability (resilience rewards), preventing overly conservative policies that preserve resources at zero consumption, thus maintaining high consumption while improving resilience.

### Mechanism 3: Handcrafted Feature Priors for Data-Efficient Reward Learning
Handcrafted linear reward parameterizations encode domain-relevant priors that enable effective learning from limited ranked trajectories. Manually designed features capture resilience-relevant state features, making this low-dimensional parameterization more data-efficient than neural networks, which struggle with high-dimensional joint state spaces and limited data.

## Foundational Learning

- **Cooperative Resilience Metrics**
  - Why needed here: The framework relies on quantifying resilience via multi-indicator performance curves to rank trajectories. Understanding how the metric is computed from indicators like Gini index, hunger index, etc., is essential for designing metrics for new domains.
  - Quick check question: Can you compute a resilience score given baseline and disrupted performance curves over time?

- **Preference-Based Inverse Reinforcement Learning (IRL)**
  - Why needed here: The core reward learning uses preference rankings (not optimal demonstrations) to infer rewards. Understanding margin-based and probabilistic formulations is critical to implementing the learning pipeline.
  - Quick check question: Given two trajectories with cumulative rewards $R(\tau_i)$ and $R(\tau_j)$, can you write the loss for ensuring $\tau_i \succ \tau_j$ in both margin and probabilistic formulations?

- **Mixed-Motive Social Dilemmas**
  - Why needed here: The environment balances individual consumption incentives against collective resource preservation. Understanding the tension helps in designing reward structures that avoid tragedy-of-the-commons outcomes.
  - Quick check question: In a shared-resource environment, what reward structures lead to over-exploitation vs. sustainability?

## Architecture Onboarding

- **Component map:** Trajectory Collector -> Resilience Scorer -> Preference Ranker -> Reward Learner (IRL) -> MARL Trainer -> Evaluation Suite
- **Critical path:** 1) Define collective well-being indicators for the domain; 2) Collect trajectories under disruption; compute resilience scores; 3) Run preference-based IRL to learn $\hat{R}_{resilience}$; 4) Combine with individual rewards and train MARL agents; 5) Evaluate under novel disruptions
- **Design tradeoffs:** Handcrafted vs. neural rewards (interpretability vs. flexibility); Margin vs. probabilistic preference learning (simplicity vs. smoothness); Resilience-only vs. hybrid rewards (conservatism vs. performance)
- **Failure signatures:** High resilience, low consumption (overly conservative); High consumption, low resilience (over-exploitation); Random-like behavior (poorly differentiated rewards)
- **First 3 experiments:** 1) Baseline resilience ranking with 500 random trajectories and single disruption; 2) Reward learning ablation comparing handcrafted, linear, and neural models; 3) Hybrid vs. resilience-only comparison under expanded disruption protocol

## Open Questions the Paper Calls Out

- Can the data efficiency of neural network reward parameterizations be improved to match or exceed the performance of handcrafted features without relying on domain-specific prior knowledge?
- How does the inferred resilience-based reward function interact with state-of-the-art cooperative MARL algorithms that were excluded from the initial validation?
- Can the framework effectively leverage human-generated trajectories as preference signals to align artificial agents with natural cooperative resilience strategies?

## Limitations

- The neural network reward models were underpowered with limited data, raising questions about scalability to more complex environments
- The approach was only validated with PPO and QMIX, leaving compatibility with modern cooperative MARL algorithms unproven
- The method relies on expert-designed features for data efficiency, limiting generalizability to domains without clear feature engineering guidelines

## Confidence

- **High Confidence:** The core mechanism of using cooperative resilience metrics to rank trajectories and guide preference-based IRL is well-specified and theoretically sound
- **Medium Confidence:** The empirical superiority of the MPL-M1 Hybrid approach with handcrafted features is demonstrated, but comparisons may be confounded by unspecified neural network architectures
- **Low Confidence:** Claims about generalizability to novel disruption types and stability across different hyperparameter settings are not sufficiently supported

## Next Checks

1. **Architectural Sensitivity Test:** Re-run neural reward learning experiments with multiple architectures to determine if poor performance stems from data limitations or architectural choices
2. **Transfer Disruption Test:** Evaluate learned rewards under at least two disruption types not seen during training to assess robustness to distribution shifts
3. **Hyperparameter Sweep:** Conduct systematic sweep over resilience-individual reward weight and preference margin parameters to quantify sensitivity and identify stable operating regions