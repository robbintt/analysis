---
ver: rpa2
title: On the Potential of Large Language Models to Solve Semantics-Aware Process
  Mining Tasks
arxiv_id: '2504.21074'
source_url: https://arxiv.org/abs/2504.21074
tags:
- process
- tasks
- llms
- order
- activities
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Large language models (LLMs) struggle with semantics-aware process
  mining tasks when used out of the box or with minimal in-context examples. This
  paper defines five semantics-aware process mining tasks (semantic anomaly detection,
  next activity prediction, and process discovery) and provides benchmarking datasets
  for evaluation.
---

# On the Potential of Large Language Models to Solve Semantics-Aware Process Mining Tasks

## Quick Facts
- **arXiv ID:** 2504.21074
- **Source URL:** https://arxiv.org/abs/2504.21074
- **Reference count:** 40
- **Primary result:** Fine-tuned LLMs achieve strong performance on semantics-aware process mining tasks, with F1-scores up to 0.88 and fitness scores up to 0.84, while out-of-the-box models struggle.

## Executive Summary
This paper investigates whether large language models (LLMs) can effectively address semantics-aware process mining tasks that require understanding the meaning of activities and their relationships. The authors define five such tasks: semantic anomaly detection (trace and activity levels), semantic next activity prediction, and semantic process discovery (directly-follows graphs and process trees). Through comprehensive benchmarking on a process behavior corpus derived from the SAP-SAM collection, the study reveals that LLMs fail dramatically when used out-of-the-box with few-shot in-context learning, achieving near-random performance. However, when fine-tuned on task-specific datasets, LLMs significantly outperform both rule-based baselines and smaller encoder-based models like RoBERTa, demonstrating their potential as powerful tools for semantics-aware process mining when properly adapted.

## Method Summary
The authors created five semantics-aware process mining tasks and corresponding datasets from the SAP-SAM process model collection. They evaluated decoder LLMs (Llama-3-8B, Mistral-7B) using both few-shot in-context learning and fine-tuning approaches. For classification tasks, they employed constrained text generation where models generate specific class tokens. Fine-tuning was performed using LoRA adapters with AdamW optimizer, learning rate of 1e-5, batch size of 2 with gradient accumulation of 16, and 3 epochs for LLMs. The models were compared against rule-based baselines and RoBERTa-large encoder models. Generation tasks were evaluated using footprint-based fitness scores comparing predicted behavioral relations to ground truth.

## Key Results
- LLMs achieve near-random performance (F1 ~0.5) on semantic anomaly detection tasks using few-shot in-context learning
- Fine-tuned LLMs achieve F1-scores up to 0.88 for classification tasks and fitness scores up to 0.84 for process discovery tasks
- Fine-tuned LLMs outperform both rule-based baselines and smaller encoder-based models (RoBERTa) across all five tasks
- Performance gains from fine-tuning range from 20 to 28 points compared to best in-context learning performance

## Why This Works (Mechanism)

### Mechanism 1: Task-Specific Fine-Tuning Bridges the Distribution Gap
Few-shot in-context learning yields near-random performance (F1 ~0.50) on anomaly detection because pretraining objectives don't encode the specific logic required. Fine-tuning with labeled examples forces the model to learn conditional probabilities between process semantics and task labels, creating a specialized mapping from process context to classification or generation. Core assumption: the fine-tuning data is representative of the semantic rules and process structures the model will encounter. Break condition: Performance will degrade on process domains not well-represented in the training corpus.

### Mechanism 2: Constrained Generative Decoding for Structured Output
For classification tasks, LLMs are fine-tuned to generate specific tokens ('true'/'false') at the end of prompts containing traces and activity sets. This operates within the LLM's generative framework but is constrained to allowed class tokens. The mechanism posits that the LLM's larger pretraining corpus provides a richer semantic representation of activities and process flows than smaller encoder models. Core assumption: the LLM's autoregressive nature and larger pretraining corpus provide richer semantic representation than encoders. Break condition: Performance advantage would diminish on tasks where semantic understanding is less critical than pattern matching.

### Mechanism 3: Process Knowledge Encoded in Pretraining Supports Generation Tasks
Few-shot ICL achieves substantially better-than-random fitness scores (~0.60) for discovery tasks, suggesting models possess a "prior" on how typical business processes are structured. Fine-tuning then sharpens this prior, aligning it with specific syntax and definitions of the task. Core assumption: pretraining corpus contains significant text describing business processes and workflows. Break condition: ICL performance should drop to near-random on generating process models from completely alien domains or nonsensical activity sets.

## Foundational Learning

- **Concept: Autoregressive vs. Masked Language Modeling**
  - Why needed here: The paper uses decoder LLMs (autoregressive) for generation and encoder models (masked) as classification baselines. Their architectures dictate how they're adapted for tasks.
  - Quick check question: Can a standard encoder-only model (like BERT) be directly used for unconstrained text generation, such as producing a process tree from scratch?

- **Concept: Fine-Tuning vs. In-Context Learning (ICL)**
  - Why needed here: The paper's central finding hinges on the dramatic performance gap between these two adaptation methods.
  - Quick check question: Which method requires access to the model's internal weights and a training loop, and which operates purely at inference time?

- **Concept: Process Model Representations (DFGs & Process Trees)**
  - Why needed here: The paper's generation tasks require the LLM to output specific, structured representations of a process.
  - Quick check question: In a Process Tree, what is the semantic difference between the '×' (exclusive choice) and '∧' (parallel) operators?

## Architecture Onboarding

- **Component map:** Prompt constructor -> Pre-trained decoder LLM with LoRA adapters -> Constrained decoding layer (classification) or autoregressive head (generation) -> Output parser

- **Critical path:**
  1. Data Preparation: Convert process behavior corpus into task-specific datasets with clear train/validation/test splits
  2. Prompt Engineering: Design few-shot prompts for ICL baseline and formatted instruction-tuning data for fine-tuning
  3. Parameter-Efficient Fine-Tuning: Train LoRA adapters on training split for each task, monitoring validation performance
  4. Constrained Decoding: Implement logic to restrict vocabulary at final token step to only valid class tokens
  5. Evaluation: Calculate macro F1-scores for classification and footprint-based fitness for generation tasks

- **Design tradeoffs:**
  - ICL vs. Fine-Tuning: ICL is faster but yields poor performance; fine-tuning requires significant compute but yields state-of-the-art results
  - Decoder LLM vs. Encoder Model: Fine-tuned decoder LLMs slightly outperform encoders but require more training time (~25x more per epoch)
  - Generalist vs. Specialist: Current work creates task-specific fine-tunes; future direction is a "Large Process Model" fine-tuned on many tasks

- **Failure signatures:**
  - ICL on Classification: Near-random performance (F1 ~0.5) on anomaly detection tasks
  - Hallucination in Generation: Model may generate semantically incorrect relations, especially on ambiguous or small activity sets
  - Overfitting: High validation performance but poor generalization to unseen activity types or industries
  - Constraint Violation: Model might generate invalid tokens if constrained decoding is not properly implemented

- **First 3 experiments:**
  1. Baseline ICL Assessment: Run pre-trained Llama and Mistral models on test sets using optimized few-shot prompts to establish out-of-the-box performance
  2. Fine-Tuning on a Single Task (e.g., T-SAD): Implement full fine-tuning experiment for T-SAD task using LoRA on Llama-3-8B and evaluate on test set
  3. Cross-Task Generalization Check: Evaluate model fine-tuned on S-DFD task on S-PTD task to assess transfer of learned process semantics

## Open Questions the Paper Calls Out

- **Can hybrid architectures that combine traditional process mining algorithms with LLM-based semantic checks outperform standalone LLMs or frequency-based methods?**
  - Basis in paper: Section 9 proposes integrating existing approaches, such as using LLMs to reject nonsensical predictions in next-activity forecasting
  - Why unresolved: Current experiments only evaluated standalone LLMs against rule-based or encoder baselines, not integrated hybrid systems
  - What evidence would resolve it: Comparative benchmarks of a hybrid model against the single-task fine-tuned LLMs presented in the paper

- **Do the performance gains observed in fine-tuned LLMs on academic model collections transfer to noisy, real-world industrial process models?**
  - Basis in paper: Section 9 highlights the need to verify results beyond the "academic process model collection" using cross-domain real-world reference models
  - Why unresolved: The current corpus (sap-sam) consists of curated diagrams; real-world logs often contain noise and semantics not captured in academic sets
  - What evidence would resolve it: Evaluation of the fine-tuned models on a newly established benchmark derived from industrial reference models

- **Can a single "Large Process Model" fine-tuned on multiple tasks simultaneously generalize better than models fine-tuned for individual tasks?**
  - Basis in paper: Section 9 suggests creating LLMs specialized for "many process analysis tasks" rather than specific ones
  - Why unresolved: The paper evaluated models fine-tuned independently for each of the five tasks, not a unified model
  - What evidence would resolve it: Training a single LLM on a mixture of all five tasks and comparing its performance against the specific-task models

## Limitations

- The study relies on an academic process model collection (SAP-SAM) that may not fully represent the noise and complexity of real-world industrial process data
- Performance gains from fine-tuning are evaluated on the same corpus used for training, raising questions about true generalization to unseen domains
- The paper doesn't explore hybrid approaches that could potentially combine the strengths of traditional process mining algorithms with LLM-based semantic understanding

## Confidence

- **High**: LLMs achieve strong performance when fine-tuned for semantics-aware process mining tasks (F1 up to 0.88, fitness up to 0.84)
- **Medium**: LLMs outperform smaller encoder-based models due to richer pretraining corpus representation
- **Medium**: Pretraining provides a non-trivial prior for generative tasks, refined through fine-tuning

## Next Checks

1. **Domain Generalization Test**: Evaluate fine-tuned models on process models from industries not well-represented in the sap-sam corpus (e.g., Government) to test true semantic understanding vs. overfitting

2. **Encoder Pretraining Parity**: Pre-train RoBERTa on a comparable corpus of process descriptions and re-evaluate classification performance to isolate the effect of pretraining corpus size vs. architecture

3. **Zero-Shot Semantic Transfer**: Test if fine-tuning on S-DFD (graph discovery) enables zero-shot performance on S-PTD (tree discovery) to validate transfer of learned process semantics