---
ver: rpa2
title: Effective Quantization of Muon Optimizer States
arxiv_id: '2509.23106'
source_url: https://arxiv.org/abs/2509.23106
tags:
- muon
- quantization
- arxiv
- adamw
- optimizer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The authors propose 8-bit quantization of the Muon optimizer for
  large language model training, addressing the high memory footprint of optimizer
  states. They demonstrate that Muon is surprisingly robust to both linear and dynamic
  quantization schemes, unlike 8-bit AdamW which is unstable under linear quantization.
---

# Effective Quantization of Muon Optimizer States

## Quick Facts
- arXiv ID: 2509.23106
- Source URL: https://arxiv.org/abs/2509.23106
- Reference count: 40
- Authors demonstrate 8-bit Muon variants achieve within 1-2% of full-precision performance while reducing optimizer memory by up to 86% compared to AdamW

## Executive Summary
The paper addresses the high memory footprint of optimizer states in large language model training by proposing 8-bit quantization of the Muon optimizer. Unlike AdamW which is unstable under linear quantization, Muon proves surprisingly robust to both linear and dynamic quantization schemes. Through theoretical analysis and extensive experiments on GPT models (1.6B parameters) and Llama 3.2 3B fine-tuning, the authors demonstrate that 8-bit Muon variants achieve performance within 1-2% of full-precision Muon while significantly reducing memory requirements.

## Method Summary
The method involves quantizing Muon optimizer states using blockwise quantization (block size 2048) with both linear and dynamic schemes. The Muon optimizer uses orthogonalization via Newton-Schulz iteration to maintain spectral control of updates. For matrix-valued parameters (weight matrices), Muon is applied with 8-bit quantized momentum buffers, while embedding and LM head parameters use AdamW with either 8-bit dynamic quantization or 32-bit precision. The quantization occurs at storage level with dequantize-reorthogonalize-update-quantize cycles during training.

## Key Results
- 8-bit Muon maintains stability under both linear and dynamic quantization, unlike AdamW which diverges under linear quantization
- 8-bit Muon variants achieve within 1-2% of full-precision Muon performance across pre-training and fine-tuning tasks
- Memory reduction of up to 86% compared to AdamW-32 and 74% compared to full-precision Muon
- GPT-Small (124M) and GPT-XL (1.6B) pre-training on FineWeb-Edu shows competitive validation loss curves

## Why This Works (Mechanism)

### Mechanism 1
AdamW diverges under 8-bit linear quantization due to error amplification in the second-moment denominator. Linear quantization allocates insufficient resolution to small-magnitude values where optimizer states concentrate. When these quantized values appear in AdamW's update rule (m/√v + ε), small errors in v produce large errors in the update direction. Theorem 1 shows expected squared error scales as O(1/ε²), diverging as ε → 10⁻⁸.

### Mechanism 2
SGD with momentum and Muon admit uniform error bounds under linear quantization because they lack a denominator term. Without division by an accumulated moment, quantization error propagates linearly rather than being amplified. Theorem 2 proves ∥θ̃ - θ∥² ≤ dη²ρ²(∥m∥∞/127)² for SGD—a bounded, dimension-scaled error independent of ε.

### Mechanism 3
Muon's orthogonalization step provides additional error robustness through singular value conditioning. The Newton-Schulz iteration produces orthonormal updates by collapsing all singular values to 1. Theorem 3 shows quantization error scales with 1/σ_min where σ_min is the minimum singular value of momentum. Since orthogonalization naturally prevents small singular values, this factor remains bounded.

## Foundational Learning

- **Blockwise quantization**: Divides tensors into blocks (2048 elements) for per-block scaling, handling outliers without global range distortion. Essential for understanding why 8-bit works at all. Quick check: Given a tensor with values [0.001, 0.002, 1000.0], would linear quantization preserve the small values better with block size 3 or block size 1?

- **Dynamic vs linear quantization**: Dynamic quantization allocates more codes to high-density regions, providing insurance against outliers. Understanding why Muon works with linear (cheaper) while AdamW requires dynamic is central to the paper. Quick check: Why does dynamic quantization help AdamW but provides no measurable benefit for Muon?

- **Polar decomposition / Newton-Schulz iteration**: Muon's core operation—orthogonalizing gradient momentum. This is what distinguishes Muon from SGD+M and enables spectral control of updates. Quick check: If M has SVD UΣV^T, what does the polar factor UV^T achieve that Σ doesn't?

## Architecture Onboarding

- **Component map**: Model Parameters → Hidden matrix-valued (weight matrices) → Muon optimizer → Momentum buffer M → 8-bit quantized (Z, S) | Embeddings → AdamW optimizer → First/second moments → 8-bit dynamic OR 32-bit | LM head → AdamW optimizer

- **Critical path**: Gradient → Momentum accumulation → Dequantize M from (Z, S) → Newton-Schulz orthogonalization → Weight update → Quantize new M → Store (Z, S). The quantize/dequantize round-trip at each step is the memory-accuracy trade-off.

- **Design tradeoffs**: Muon-8D vs Muon-8L (Dynamic offers insurance but linear works nearly as well); Muon-8D vs Muon-8D/A-32 (Quantize AdamW parameters or not?); Block size (2048 chosen; smaller blocks = better precision but more scaling factors to store)

- **Failure signatures**: Muon-8L with 8-bit AdamW everywhere (Underperforms on smaller models); Early divergence (likely embedding/LM-head instability with linear quantization—switch to Muon-8D/A-32 variant); Memory not reducing as expected (check embedding layer ratio)

- **First 3 experiments**: 1) Sanity check: Train from initialization with Muon-8D for 1B tokens vs AdamW-32 baseline; 2) Linear vs dynamic ablation: Train GPT-Small with Muon-8L, Muon-8D, Muon-8L/A-32; 3) Memory profiling: Measure HBM usage for Llama-3.2-3B with AdamW-32 vs Muon-8D

## Open Questions the Paper Calls Out

- Can Muon be effectively quantized to bit-widths lower than 8-bit (e.g., 4-bit) while maintaining training stability? (Future work on quantization to even lower bits)

- What are the performance and memory impacts of combining 8-bit Muon with low-rank optimization techniques like GaLore or LoRA? (Future work on combination with techniques like low-rank matrices)

- How does the Newton-Schulz approximation error interact with quantization noise compared to the exact polar decomposition analyzed in Theorem 3? (The theoretical analysis relies on exact orthogonalization vs actual iterative approximation)

## Limitations

- 8-bit AdamW instability is theoretically proven but empirically narrow - demonstrated on ImageNet but not extensively validated on LLM pre-training tasks
- Momentum scaling factor justification - the "0.2 * sqrt(max(m,n))" scaling is introduced without derivation or sensitivity analysis
- Block size selection - choice of 2048 lacks theoretical justification through ablation studies

## Confidence

**High confidence**: Muon's robustness to 8-bit linear quantization, memory reduction calculations, and core theoretical bounds (Theorems 2 and 3)

**Medium confidence**: Comparative advantage over AdamW-8D in terms of memory savings and stability, though ImageNet divergence test is narrow validation

**Low confidence**: Specific momentum scaling factor (0.2 * sqrt(max(m,n))) and its impact on different model scales; block size choice of 2048

## Next Checks

1. **Cross-architecture robustness test**: Apply 8-bit Muon to a transformer variant with different attention mechanisms (e.g., RWKV or Mamba) and measure stability compared to AdamW-8D

2. **Scaling factor sensitivity analysis**: Systematically vary the momentum scaling factor (0.2 * sqrt(max(m,n))) across [0.1, 0.15, 0.2, 0.25, 0.3] and measure impact on convergence speed and final performance for models of different scales

3. **Block size ablation study**: Train the same model with block sizes [512, 1024, 2048, 4096] for 8-bit quantization, measuring both memory savings and performance impact