---
ver: rpa2
title: Complex LLM Planning via Automated Heuristics Discovery
arxiv_id: '2502.19295'
source_url: https://arxiv.org/abs/2502.19295
tags:
- heuristic
- state
- cube
- block
- function
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces AutoHD, a method that enables LLMs to generate
  heuristic functions for planning tasks, eliminating the need for unreliable self-verification
  or costly external models. AutoHD prompts LLMs to create Python-based heuristic
  functions, which guide inference-time search, and refines them through an iterative
  evolution process.
---

# Complex LLM Planning via Automated Heuristics Discovery

## Quick Facts
- arXiv ID: 2502.19295
- Source URL: https://arxiv.org/abs/2502.19295
- Reference count: 40
- Primary result: AutoHD achieves nearly double the planning accuracy on Blocksworld, Game of 24, and Rubik's Cube tasks by generating and evolving Python-based heuristic functions

## Executive Summary
AutoHD introduces a method for enabling LLMs to generate heuristic functions for planning tasks, eliminating the need for unreliable self-verification or costly external models. The approach prompts LLMs to create Python-based heuristic functions that guide inference-time search, then refines them through an iterative evolution process. Experiments on three diverse planning problems demonstrate significant accuracy gains over baselines, with performance improvements of up to 96% on Blocksworld tasks. The method is training-free, interpretable, and generalizable across different planning domains.

## Method Summary
AutoHD prompts LLMs to generate Python heuristic functions that estimate the cost to reach a goal state, then evaluates and evolves these functions using a small validation set. The process involves generating an initial pool of heuristics, selecting the best performers, and refining them through exploration and modification prompts over multiple generations. The final heuristic is used with search algorithms like Greedy BFS or A* during inference, eliminating the need for LLM-based state evaluation at every step.

## Key Results
- AutoHD achieves up to 96% accuracy on Blocksworld tasks, nearly doubling baseline performance
- The method shows consistent improvements across diverse planning problems including Game of 24 and 2x2 Rubik's Cube
- Heuristic evolution over 5 generations demonstrates clear improvement in validation accuracy before plateauing
- The approach maintains training-free operation while providing interpretable, code-based heuristics

## Why This Works (Mechanism)

### Mechanism 1
Replacing LLM self-verification with deterministic code execution mitigates the unreliability of probabilistic state evaluation. Instead of asking the LLM to rate a state, AutoHD prompts it to generate a Python function $H(s)$ that is executed by a deterministic interpreter. This shifts the LLM's role from an unreliable critic to a code generator, leveraging its coding proficiency while bypassing reasoning inconsistencies.

### Mechanism 2
Iterative evolution on a validation set improves heuristic quality by filtering out initially plausible but ineffective functions. The system generates an initial pool of heuristics, evaluates them on a small validation set (approx. 10 instances), and refines the best performers through exploration and modification. This evolutionary pressure forces the LLM to converge on robust evaluation logic.

### Mechanism 3
Decoupling heuristic generation from inference-time search reduces computational overhead compared to step-by-step LLM evaluation. Methods like Tree-of-Thought query the LLM for every state evaluation, while AutoHD generates the heuristic once before the main inference. During search, the algorithm calls the lightweight Python function, amortizing the LLM generation cost over the entire search process.

## Foundational Learning

- **Concept: A* Search and Heuristic Functions**
  - **Why needed here:** Understanding that a "heuristic" estimates the cost to reach the goal explains why the Python code guides the search.
  - **Quick check question:** Can you explain why an "admissible" heuristic (one that never overestimates the cost) guarantees an optimal solution in A* search?

- **Concept: LLM Code Generation vs. Reasoning**
  - **Why needed here:** The core pivot is that LLMs are better at writing code to solve a problem than "thinking" through the problem step-by-step.
  - **Quick check question:** Why might an LLM write a correct Python function for a puzzle but fail to solve the same puzzle using only text generation?

- **Concept: Evolutionary Algorithms (Selection & Mutation)**
  - **Why needed here:** The "Heuristic Evolution" component uses principles from genetic algorithmsâ€”keeping the "fittest" heuristics and mutating them.
  - **Quick check question:** In the context of this paper, what corresponds to the "mutation" operator in a standard genetic algorithm?

## Architecture Onboarding

- **Component map:** LLM Generator -> Sandbox/Executor -> Search Engine -> Validator
- **Critical path:** The Prompt Engineering for the Proposer is the highest leverage component. If the prompt does not clearly define the state representation, the LLM will write syntactically correct but functionally useless code.
- **Design tradeoffs:** Interpretability vs. Black Box (code is interpretable but can become opaque), Setup Cost vs. Inference Speed (high setup cost for faster, cheaper inference later)
- **Failure signatures:** Static Heuristic (always returns constant), Syntax Drift (introduces unavailable library dependencies), State Mismatch (expects different input format than provided)
- **First 3 experiments:**
  1. Implement the "Game of 24" experiment to verify generated code runs correctly
  2. Run AutoHD with 0 generations vs. 5 generations to measure evolution's contribution
  3. Swap Greedy BFS for A* with constant heuristic to validate compatibility with different search strategies

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions in the text provided.

## Limitations
- Effectiveness depends heavily on the representativeness of the small validation set, risking overfitting
- Missing specific implementation details (pool sizes, generation counts, termination conditions) make exact replication challenging
- Performance gains, while significant, are not consistently "nearly double" across all benchmarks

## Confidence
- **High Confidence:** The core mechanism of using LLM-generated Python code for heuristics and iterative evolution is well-supported by experimental results
- **Medium Confidence:** Claims about interpretability and generalizability are plausible but not extensively validated across diverse domains
- **Low Confidence:** Specific numerical claims (e.g., "nearly double the accuracy") require precise replication details not fully provided

## Next Checks
1. **Prompt Sensitivity Analysis:** Test how sensitive heuristic quality is to minor changes in the generation prompt to validate prompt engineering robustness
2. **Dataset Shift Test:** Run AutoHD on a perturbed validation set to check if evolved heuristics overfit to the original validation distribution
3. **Cross-Domain Transfer:** Apply the best heuristic from Blocksworld to Rubik's Cube (or vice versa) to test generalizability claims