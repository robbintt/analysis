---
ver: rpa2
title: 'Neural Networks Remember More: The Power of Parameter Isolation and Combination'
arxiv_id: '2502.10966'
source_url: https://arxiv.org/abs/2502.10966
tags:
- task
- learning
- tasks
- https
- continual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles catastrophic forgetting in continual learning
  for pre-trained language models (PLMs), where models lose previously acquired knowledge
  when sequentially trained on new tasks. The authors propose a novel approach that
  combines parameter isolation and combination strategies.
---

# Neural Networks Remember More: The Power of Parameter Isolation and Combination

## Quick Facts
- arXiv ID: 2502.10966
- Source URL: https://arxiv.org/abs/2502.10966
- Authors: Biqing Zeng; Zehan Li; Aladdin Ayesh
- Reference count: 32
- Primary result: Achieves state-of-the-art continual learning performance with 77.2% average accuracy without task identification or historical data storage

## Executive Summary
This paper addresses catastrophic forgetting in continual learning for pre-trained language models (PLMs), where models lose previously acquired knowledge when sequentially trained on new tasks. The authors propose a novel approach combining parameter isolation and combination strategies. Initially, they use Parameter-Efficient Fine-Tuning (PEFT) methods like Adapter and LoRA to adapt to each downstream task while keeping the backbone model frozen. After training on all tasks, they combine the trained parameters using task arithmetic and apply them to the backbone model. The method is evaluated on continual language learning benchmarks and achieves state-of-the-art performance, outperforming existing methods like EPI and IDBR without requiring task identification or historical data storage.

## Method Summary
The proposed method tackles catastrophic forgetting through a two-phase approach. First, it uses Parameter-Efficient Fine-Tuning (PEFT) methods like Adapter and LoRA to adapt the PLM to each downstream task while keeping the backbone model frozen. This parameter isolation strategy allows each task to be learned independently. Second, after training on all tasks, the method employs task arithmetic to combine the trained parameters from different tasks. This combination is then applied to the backbone model, enabling the model to leverage knowledge from all tasks simultaneously. The approach effectively balances model stability and plasticity, mitigating catastrophic forgetting while facilitating knowledge transfer between tasks. Notably, the method operates without requiring task identification during inference or storage of historical data.

## Key Results
- Achieves 77.2% average accuracy in the full-shot setting, surpassing previous approaches
- Outperforms existing methods like EPI and IDBR on continual language learning benchmarks
- Operates without requiring task identification or historical data storage
- Effectively balances model stability and plasticity, mitigating catastrophic forgetting

## Why This Works (Mechanism)
The method works by combining parameter isolation during training with parameter combination during inference. Parameter isolation through PEFT adapters allows each task to be learned independently without interfering with the backbone model, preserving existing knowledge. The task arithmetic approach then combines these isolated parameter sets, creating a unified model that can handle all tasks. This dual strategy addresses the stability-plasticity dilemma in continual learning: isolation provides stability by protecting learned knowledge, while combination provides plasticity by enabling knowledge transfer and integration across tasks. The approach leverages the PLM's pre-trained knowledge while adapting to new tasks efficiently, achieving state-of-the-art performance without the need for task identification or storing historical data.

## Foundational Learning

### Catastrophic Forgetting
- **Why needed**: Understanding why neural networks lose previously learned knowledge when trained on new tasks is fundamental to continual learning
- **Quick check**: Models show degraded performance on earlier tasks after sequential training on new tasks

### Parameter-Efficient Fine-Tuning (PEFT)
- **Why needed**: PEFT methods like Adapter and LoRA allow task-specific adaptation without modifying the entire model, reducing computational cost and interference
- **Quick check**: Only a small percentage of parameters are updated during task adaptation

### Task Arithmetic
- **Why needed**: The linear combination of parameters from different tasks enables knowledge integration across multiple learning experiences
- **Quick check**: Adding or subtracting parameter updates can produce meaningful intermediate models

### Continual Learning
- **Why needed**: Understanding the broader framework of learning from sequential data streams without catastrophic forgetting
- **Quick check**: Models must maintain performance on all seen tasks while learning new ones

## Architecture Onboarding

### Component Map
PLM Backbone -> PEFT Adapters (Task 1, Task 2, ..., Task N) -> Parameter Combination -> Unified Model

### Critical Path
1. Initialize PLM backbone
2. Apply PEFT adapters for each task sequentially
3. Store adapter parameters for each task
4. Combine adapter parameters using task arithmetic
5. Apply combined parameters to backbone model

### Design Tradeoffs
- **Isolation vs. Integration**: Complete isolation prevents forgetting but limits knowledge transfer; the method balances this by combining parameters after training
- **Efficiency vs. Performance**: PEFT methods reduce computational cost but may limit model capacity compared to full fine-tuning
- **Task-agnostic vs. Task-specific**: Avoiding task identification simplifies deployment but may miss optimization opportunities

### Failure Signatures
- Degraded performance on early tasks indicates insufficient parameter isolation
- Poor overall performance suggests ineffective parameter combination or task arithmetic
- High computational overhead during combination phase indicates scalability issues

### First 3 Experiments
1. Evaluate catastrophic forgetting by testing on all tasks after sequential training
2. Compare performance with and without parameter combination to isolate its contribution
3. Test the method on different PLM architectures to assess generalizability

## Open Questions the Paper Calls Out
None

## Limitations
- Scalability challenges with very large numbers of tasks due to parameter storage and combination overhead
- Assumption of linear separability of task representations may not hold for all downstream tasks or PLM architectures
- Evaluation primarily focuses on classification benchmarks, leaving uncertainty about performance on generation or reasoning tasks

## Confidence
- Performance claims on continual learning benchmarks: Medium
- Effectiveness of stability-plasticity balance: Medium
- Task-agnostic operation without identification: High
- Scalability to very large task sequences: Low

## Next Checks
1. Conduct ablation studies varying the number of tasks to quantify parameter storage and combination overhead scaling
2. Test the method on non-classification downstream tasks (e.g., text generation, QA) to assess generalizability beyond the evaluated benchmarks
3. Implement and measure the computational cost of the parameter combination phase during inference to establish practical deployment requirements