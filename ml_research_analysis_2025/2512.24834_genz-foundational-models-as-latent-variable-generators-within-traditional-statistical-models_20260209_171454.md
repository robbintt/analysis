---
ver: rpa2
title: 'GenZ: Foundational models as latent variable generators within traditional
  statistical models'
arxiv_id: '2512.24834'
source_url: https://arxiv.org/abs/2512.24834
tags:
- feature
- features
- items
- house
- semantic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces GenZ, a hybrid approach that combines foundational
  models with traditional statistical modeling through interpretable semantic features.
  The key innovation is an iterative algorithm that discovers dataset-specific semantic
  features by contrasting groups of items identified through statistical modeling
  errors, rather than relying solely on the foundational model's domain knowledge.
---

# GenZ: Foundational models as latent variable generators within traditional statistical models

## Quick Facts
- arXiv ID: 2512.24834
- Source URL: https://arxiv.org/abs/2512.24834
- Authors: Marko Jojic; Nebojsa Jojic
- Reference count: 35
- Primary result: 12% median relative error on house price prediction (vs 38% for GPT-5 baseline)

## Executive Summary
GenZ introduces a hybrid approach that combines foundational models with traditional statistical modeling through interpretable semantic features. The key innovation is an iterative algorithm that discovers dataset-specific semantic features by contrasting groups of items identified through statistical modeling errors, rather than relying solely on the foundational model's domain knowledge. The method treats the foundational model's judgments as noisy observations of latent binary features that predict real-valued targets through learned statistical relationships. Experiments demonstrate substantial improvements over GPT-5 baselines: 12% median relative error on house price prediction and 0.59 cosine similarity for predicting Netflix movie embeddings from semantic descriptions alone.

## Method Summary
GenZ uses a generalized EM framework with mean-field variational inference to combine foundational models with statistical modeling. The approach treats LLM feature classifications as noisy observations of latent binary variables, enabling joint optimization of feature descriptors and model parameters. Features are discovered through an iterative mining process that contrasts items grouped by statistical modeling errors, with conditional feature discovery preventing contradictory splits. The model uses either linear or neural network mappings from binary features to targets, with a pruning mechanism to remove low-impact features. Training involves alternating between updating posteriors, learning statistical parameters, and mining new features until convergence.

## Key Results
- House price prediction: 12% median relative error vs 38% for GPT-5 baseline
- Netflix embedding prediction: 0.59 cosine similarity, matching performance of ~4000 user ratings
- Discovered features reveal dataset-specific patterns (architectural details, franchise membership) that diverge from general domain knowledge
- Linear models outperform neural networks for embedding prediction, while neural networks better capture interactions for house prices

## Why This Works (Mechanism)

### Mechanism 1
Dataset-specific features can be discovered by contrasting items grouped according to statistical modeling errors rather than LLM domain knowledge. The posterior identifies items that should be reassigned to improve prediction, and sampling these posteriors creates positive/negative groups that an LLM then explains semantically. This closes the loop between statistical signal and semantic interpretation.

### Mechanism 2
Treating LLM feature classifications as noisy observations of latent binary variables enables joint optimization. The model defines p(zi|s,θf,pe) with error probability pe, allowing posteriors to override LLM judgments when observations strongly contradict them. This enables iterative refinement of feature descriptors.

### Mechanism 3
Conditional feature discovery prevents contradictory splits by conditioning on existing feature assignments. The algorithm selects the feature combination with largest total error and mines new features only within that subgroup, preventing scenarios where contradictory features are discovered for different subgroups.

## Foundational Learning

- **Concept: Variational EM / Mean-field approximation**
  - Why needed here: The bound in Eq. (1) and factorized posterior q = ∏i q(zi) are standard variational inference. Understanding why entropy terms cancel in Eq. (11) requires familiarity with mean-field updates.
  - Quick check question: Given Eq. (1), why does setting q = p(z|y,s) make the bound tight?

- **Concept: Concept Bottleneck Models (CBMs)**
  - Why needed here: GenZ extends CBMs by making the "concepts" (features z) learnable from data rather than pre-defined. Section 1 explicitly positions GenZ relative to CBM literature.
  - Quick check question: In a standard CBM, who provides the concept labels? In GenZ, who provides them?

- **Concept: Cold-start problem in collaborative filtering**
  - Why needed here: Section 5.3 frames embedding prediction as a cold-start solution. Understanding why cosine similarity of 0.59 is meaningful requires knowing that collaborative filtering embeddings normally require thousands of user interactions to stabilize.
  - Quick check question: Why can't a new movie be recommended immediately in a pure collaborative filtering system?

## Architecture Onboarding

- **Component map:**
  - Semantic encoder (frozen LLM) -> Feature store -> Statistical head -> Mining prompter -> Posterior tracker
  - Semantic encoder executes extraction prompts → h(s, θf) ∈ {0,1}^nf
  - Feature store maintains list of natural language feature descriptors
  - Statistical head maps binary features to y (linear or neural net)
  - Mining prompter generates new feature descriptors from exemplar groups
  - Posterior tracker maintains soft feature assignments per item during training

- **Critical path:**
  1. Initialize θf (empty or seed features)
  2. Run extraction prompts → h values for all items
  3. E-step: Compute q(zi) for all items via Eq. (11)
  4. M-step: Update θm, σ²y, pe via Eqs. (12-14)
  5. Mining: Sample q to form groups → generate new θf
  6. Prune low-impact features (Algorithm 3)
  7. Iterate until convergence

- **Design tradeoffs:**
  - Linear vs. neural f(z): Linear generalizes better for embeddings; neural captures interactions for house prices but overfits more
  - Number of inner iterations: Too few → unstable posteriors; too many → wasted computation
  - Feature cardinality constraints: Discarding features active <2% or >98% prevents rare/ubiquitous features

- **Failure signatures:**
  - Test error rises while train error drops → overfitting; reduce f(z) capacity
  - Discovered features semantically incoherent → LLM may not understand item semantics
  - Feature count grows unbounded → increase pruning threshold

- **First 3 experiments:**
  1. Replicate "binary representation of numbers 0–511" experiment to validate mining loop
  2. Compare fixing pe = 0 vs. learning pe on house price MAE
  3. Run on book ratings domain and manually inspect discovered features

## Open Questions the Paper Calls Out

### Open Question 1
How does the full two-item-type model (modeling co-occurrence between semantic item sets, e.g., users and movies jointly) compare to the simplified single-item-type approach using precomputed embeddings? The full joint model was derived but not empirically validated.

### Open Question 2
What principled criteria determine whether a linear or nonlinear mapping f(z) is appropriate for a given prediction task? The paper observes different optimal choices across domains without establishing diagnostic criteria.

### Open Question 3
How sensitive are discovered features to the choice and capabilities of the foundational model used for mining and extraction? The paper uses GPT-5 and GPT-4 but does not ablate this choice or test alternatives.

### Open Question 4
Can the approach effectively integrate predicted embeddings with incrementally observed user ratings as cold-start items gain interaction data? The paper demonstrates semantic predictions but doesn't address the warm-start transition.

## Limitations
- Computational cost: Iterative mining process requires multiple LLM queries per feature addition
- Feature drift: Later features may become less interpretable or relevant to the overall task
- LLM dependency: Performance is bounded by the foundational model's understanding of the domain

## Confidence
- **High confidence**: Mathematical framework (variational EM, mean-field approximation) is sound and well-established
- **Medium confidence**: Netflix embedding prediction results (0.59 cosine similarity) are compelling but may depend on dataset-specific factors
- **Low confidence**: Generalizability of mining prompts and exact hyperparameters across different domains remain uncertain

## Next Checks
1. Apply GenZ to a third domain (e.g., medical diagnosis prediction) to test cross-domain generalization
2. Compare GenZ against pure statistical models with handcrafted features and pure LLM predictions to isolate the mining algorithm's contribution
3. Perform error analysis on discovered features, categorizing them as obvious domain patterns, surprising insights, or spurious correlations