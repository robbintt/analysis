---
ver: rpa2
title: Convergence Rates for Gradient Descent on the Edge of Stability in Overparametrised
  Least Squares
arxiv_id: '2510.17506'
source_url: https://arxiv.org/abs/2510.17506
tags:
- which
- convergence
- then
- lemma
- gradient
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper provides the first convergence rate analysis for gradient
  descent at the edge of stability in overparameterized least squares. The key insight
  is that overparameterization makes the global minimizers form a Riemannian manifold,
  allowing decomposition of the dynamics into parallel (Riemannian gradient descent
  on sharpness) and orthogonal (bifurcating dynamical system) components.
---

# Convergence Rates for Gradient Descent on the Edge of Stability in Overparametrised Least Squares

## Quick Facts
- arXiv ID: 2510.17506
- Source URL: https://arxiv.org/abs/2510.17506
- Reference count: 40
- Primary result: First convergence rate analysis for gradient descent at the edge of stability in overparameterized least squares, identifying three distinct convergence regimes

## Executive Summary
This paper provides the first convergence rate analysis for gradient descent at the edge of stability in overparameterized least squares problems. The key insight is that overparameterization creates a Riemannian manifold of global minimizers, allowing decomposition of the dynamics into parallel (Riemannian gradient descent on sharpness) and orthogonal (bifurcating dynamical system) components. The analysis identifies three convergence regimes: subcritical (linear convergence to suboptimal flat minimum), critical (power-law convergence to optimal flat minimum), and supercritical (linear convergence to period-two orbit centered at optimal minimum). The work is verified on multilayer scalar factorization problems, demonstrating the predicted convergence rates and implicit bias toward flat minima.

## Method Summary
The paper analyzes gradient descent on codimension-1 least squares problems where the global minimizers form a Riemannian manifold M. By constructing a tubular neighborhood around M, the parameter update is split into parallel (θ_∥) and orthogonal (θ_⊥) coordinates. The parallel component performs Riemannian gradient descent on the sharpness λ, while the orthogonal component follows a normal form of a period-doubling bifurcation. Three regimes emerge based on the learning rate η relative to the critical value 2/λ(θ*): subcritical (η < 2/λ*) for transient instability followed by linear convergence to suboptimal minimum, critical (η = 2/λ*) for power-law convergence to optimally flat minimum, and supercritical (η > 2/λ*) for linear convergence to a period-two orbit centered at optimal minimum.

## Key Results
- Gradient descent at the edge of stability decomposes into Riemannian gradient descent on sharpness and a bifurcating dynamical system
- Three convergence regimes identified: subcritical (linear to suboptimal), critical (power-law to optimal), and supercritical (linear to period-two orbit)
- Implicit bias toward flat minima occurs across all three regimes
- Analysis verified on multilayer scalar factorization with depth p=5 and target y=1
- Power-law convergence rate Θ(t^{-1/2}) in critical regime matches theoretical prediction

## Why This Works (Mechanism)

### Mechanism 1: Manifold Decomposition of Gradient Descent
- **Claim:** In overparameterized least squares, the dynamics of Gradient Descent (GD) decompose into two distinct systems: Riemannian gradient descent (RGD) on the sharpness along the solution manifold, and a bifurcating dynamical system orthogonal to it.
- **Mechanism:** Overparameterization ensures global minimizers form a Riemannian manifold M. By constructing a "tubular neighbourhood" around M, the parameter update θ is split into parallel (θ_∥) and orthogonal (θ_⊥) coordinates. The parallel update drives the iterates toward flatter regions of M, while the orthogonal update determines stability.
- **Core assumption:** The solution set forms a smooth manifold M (Assumption 3.1), and the problem is codimension-1.
- **Evidence anchors:**
  - [abstract] "The key insight behind our analysis is that... the set of global minimisers forms a Riemannian manifold M..."
  - [section 4] Theorem 4.1 establishes the normal form: "GD... takes the form... GD_∥... GD_⊥..."
  - [corpus] Neighbor papers confirm general interest in EOS dynamics but lack this specific Riemannian decomposition.
- **Break condition:** Fails if the loss landscape has no smooth manifold of solutions (e.g., non-overparameterized settings) or if codimension > 1.

### Mechanism 2: Orthogonal Bifurcation Dynamics
- **Claim:** The behavior of the iterates orthogonal to the solution manifold (θ_⊥) is governed by the normal form of a period-doubling bifurcation, determining whether the trajectory converges or oscillates.
- **Mechanism:** The orthogonal update follows θ_⊥ → (1 - ηλ)θ_⊥ + θ_⊥³. Depending on the learning rate η relative to the sharpness λ, the fixed point θ_⊥=0 changes stability. This drives the system into subcritical (stable), critical (parabolic), or supercritical (oscillatory) regimes.
- **Core assumption:** A "genericity" condition holds regarding the 3rd and 4th derivatives of the loss (Assumption 3.3).
- **Evidence anchors:**
  - [section 4] Equation (13) defines the orthogonal update as "(1 - eta lambda) theta_perp + (theta_perp)^3".
  - [section 4] "The orthogonal component is a bifurcating dynamical system."
  - [corpus] Evidence in corpus regarding specific normal forms is weak; most works focus on empirical EOS rather than bifurcation proofs.
- **Break condition:** If higher-order terms (O(θ_⊥⁴)) dominate (e.g., extreme learning rates), the simple bifurcation model may fail to predict chaotic dynamics.

### Mechanism 3: Implicit Sharpness Minimization
- **Claim:** GD with a large learning rate implicitly performs Riemannian gradient descent on the objective sharpness λ, biasing the solution toward flatter minima.
- **Mechanism:** The parallel component GD_∥ acts as RGD on λ with a step size proportional to the squared magnitude of the orthogonal oscillation (θ_⊥)². In the critical and supercritical regimes, the persistent oscillation provides the "energy" to drive the parallel component toward the flattest point θ_∥*.
- **Core assumption:** The sharpness λ is geodesically strongly convex on the manifold (Assumption 3.4).
- **Evidence anchors:**
  - [page 2] "GD with a large learning rate implicitly performs Riemannian GD on the sharpness..."
  - [section 5] Theorems 5.2 and 5.3 show convergence to the "optimally flat" minimum in critical/supercritical regimes.
  - [corpus] Corpus aligns on the "implicit bias toward flat minima" observation.
- **Break condition:** Fails if sharpness is not geodesically convex, potentially leading to convergence to suboptimal sharp minima.

## Foundational Learning

- **Concept: Riemannian Manifolds & Tubular Neighbourhoods**
  - **Why needed here:** The entire theoretical framework relies on viewing the solution set not as discrete points but as a continuous manifold M with tangent spaces.
  - **Quick check question:** Can you explain the difference between a standard gradient in Euclidean space and a Riemannian gradient on a curved manifold?

- **Concept: Bifurcation Theory (Flip/Pitchfork)**
  - **Why needed here:** To understand why increasing the learning rate causes the system to transition from stable convergence to a period-2 oscillation (supercritical regime).
  - **Quick check question:** In the map x → ax + x³, how does the stability of the fixed point x=0 change as parameter a crosses 1?

- **Concept: Sharpness (Hessian Eigenvalues)**
  - **Why needed here:** The "Edge of Stability" is defined by the relationship between the learning rate η and the maximum eigenvalue λ_max (sharpness).
  - **Quick check question:** Why does classical optimization theory require the learning rate η to be less than 2/λ_max?

## Architecture Onboarding

- **Component map:**
  - Global Minimizers (M): The "backbone" manifold where loss is zero
  - Normal Vector (n(θ_∥)): The direction orthogonal to M, determining the "sharpness" landscape
  - Bifurcation Parameter (ηλ): The ratio controlling the stability of the orthogonal dynamics

- **Critical path:**
  1. Verify overparameterization ensures a smooth solution manifold M (check rank of Df)
  2. Compute the Hessian ∇²ℓ along M to determine sharpness λ
  3. Select learning rate η relative to 2/λ(θ*) to target a specific regime (Sub/Critical/Supercritical)

- **Design tradeoffs:**
  - **Subcritical (η < 2/λ*):** Guaranteed linear convergence, but stops at a suboptimally flat point (worse generalization potential)
  - **Supercritical (η > 2/λ*):** Reaches the flattest point (best generalization potential), but results in a period-2 oscillation (requires weight averaging or specific stopping)

- **Failure signatures:**
  - **Progressive Sharpening Failure:** If initialization is outside the "tubular neighbourhood," the theory does not guarantee convergence to M
  - **High Codimension:** If the problem has codimension > 1 (e.g., multi-output regression), the bifurcation analysis changes fundamentally

- **First 3 experiments:**
  1. **Scalar Factorization Validation:** Implement f(θ) = ∏θ_i and plot |θ_⊥| and λ(θ_∥) to verify the predicted power-law vs. linear convergence rates across regimes
  2. **Regime Transition Sweep:** Systematically vary η across the 2/λ* threshold to observe the transition from stable fixed points to period-2 orbits (Figure 2 vs Figure 4)
  3. **Initialization Robustness:** Test how far from M initialization can be while still observing the "progressive sharpening" into the tubular neighbourhood

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Is the geodesic strong convexity of the loss sharpness (Assumption 3.4) a general property of overparameterized deep learning loss landscapes, or is it specific to scalar factorization problems?
- **Basis in paper:** [explicit] Section 6 states: "One of the most surprising results of our work is that the sharpness of the loss is geodesically strongly convex... Should this prove to be a more general fact... We leave a more thorough investigation of this question to future work."
- **Why unresolved:** The non-convexity of DNN landscapes makes this non-trivial; the paper only verifies this property for multilayer scalar factorization.
- **What evidence would resolve it:** A theoretical proof that ∇²_M λ ≽ μI holds for the solution manifolds of general neural network architectures or a counterexample.

### Open Question 2
- **Question:** Can the convergence rate analysis and normal form decomposition be extended to higher codimension least squares problems, such as regression on multiple datapoints?
- **Basis in paper:** [explicit] Section 6 states: "Our theory only allows us to treat codimension 1 problems... It is not difficult to extend our definitions to higher codimension, however proving anything in this setting seems very challenging... We leave this question to future work."
- **Why unresolved:** Higher codimensions involve orthogonal dynamics of dimension greater than one, requiring an understanding of higher-dimensional bifurcating dynamical systems.
- **What evidence would resolve it:** Deriving convergence theorems for a system where the orthogonal component θ_⊥ is a vector rather than a scalar.

### Open Question 3
- **Question:** Does gradient descent converge rapidly to the tubular neighbourhood of the solution manifold (where the normal form holds) from standard random initialization during the progressive sharpening phase?
- **Basis in paper:** [explicit] Section 6 states: "We conjecture that GD does converge rapidly to a tubular neighbourhood from a standard initialisation... We leave exploration of this question to future work."
- **Why unresolved:** The current theoretical results are premised on the assumption that the initialization already lies within this specific neighbourhood.
- **What evidence would resolve it:** A theorem bounding the time required for iterates starting from a standard initialization to enter the tubular neighbourhood N.

## Limitations

- **Codimension restriction:** The theory only applies to codimension-1 problems, excluding most practical regression tasks with multiple data points
- **Smooth manifold assumption:** Requires the solution set to form a smooth Riemannian manifold, which may not hold for all overparameterized models
- **Initialization dependency:** The theoretical analysis assumes initialization within the tubular neighborhood, which may not hold for standard random initialization

## Confidence

- **Riemannian decomposition mechanism:** High confidence - the manifold structure is well-established in overparameterized settings
- **Bifurcation dynamics:** Medium confidence - the normal form analysis is rigorous but the genericity assumptions may be restrictive
- **Sharpness minimization property:** Medium confidence - verified for scalar factorization but generalization to complex networks requires further validation
- **Practical applicability:** Low confidence - the codimension-1 restriction significantly limits real-world applications

## Next Checks

1. **Verify manifold projection implementation:** Carefully implement and test the KKT-based projection logic to ensure iterates are correctly mapped to the solution manifold for the scalar factorization problem

2. **Validate regime transitions empirically:** Systematically sweep learning rates around the critical threshold (2/λ*) to observe the predicted transition from stable convergence to period-2 orbits

3. **Test initialization robustness:** Evaluate how initialization distance from the solution manifold affects convergence behavior and whether the progressive sharpening conjecture holds