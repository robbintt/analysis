---
ver: rpa2
title: Alzheimer's Dementia Detection Using Perplexity from Paired Large Language
  Models
arxiv_id: '2506.09315'
source_url: https://arxiv.org/abs/2506.09315
tags:
- perplexity
- language
- alzheimer
- detection
- adress
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper extends the paired perplexity approach to detect Alzheimer's
  dementia (AD) using a more advanced large language model (LLM), the instruction-following
  Mistral-7B. The method improves accuracy by an average of 3.33% over the best current
  paired perplexity method and by 6.35% over the top-ranked method from the ADReSS
  2020 challenge benchmark.
---

# Alzheimer's Dementia Detection Using Perplexity from Paired Large Language Models

## Quick Facts
- **arXiv ID:** 2506.09315
- **Source URL:** https://arxiv.org/abs/2506.09315
- **Reference count:** 0
- **Primary result:** 93.75% accuracy on ADReSS using paired perplexity with Mistral-7B-Instruct

## Executive Summary
This paper introduces a paired perplexity approach using Mistral-7B-Instruct to detect Alzheimer's dementia from speech transcripts. By fine-tuning two separate LLMs—one on AD transcripts and one on healthy control (HC) transcripts—the method computes normalized log-perplexity differences that distinguish between the two classes. The approach achieves state-of-the-art accuracy of 93.75% on the ADReSS benchmark, improving upon previous paired perplexity methods by 3.33% and the top-ranked baseline by 6.35%. The method offers interpretable decision boundaries through analysis of generated text that reveals learned AD language patterns.

## Method Summary
The method uses paired perplexity with Mistral-7B-Instruct fine-tuned via LoRA adapters on AD and HC transcripts separately. Training uses LoRA rank=16, α=32, AdamW optimizer, learning rate 5e-5, batch size 8, and 6 epochs with early stopping. The classification score is the normalized logarithmic perplexity difference between the two models. For interpretability, fine-tuned models are prompted to generate text describing the Cookie Theft picture, and the generated language features are compared to human transcripts to validate that the models have learned AD-specific patterns.

## Key Results
- Achieves 93.75% accuracy on ADReSS benchmark, the highest reported result
- Improves upon previous paired perplexity methods by 3.33% average accuracy
- Improves upon top-ranked ADReSS 2020 baseline by 6.35% average accuracy
- Generated text from fine-tuned models shows 30 out of 41 linguistic features significantly differ between AD and HC classes

## Why This Works (Mechanism)

### Mechanism 1: Paired Perplexity Differential
Fine-tuned LLMs trained on AD vs HC transcripts produce measurably different perplexity scores on test samples, enabling binary classification. When MAD encounters AD speech, its learned probability distribution assigns higher probability to observed word sequences → lower perplexity. The same sample yields higher perplexity on MC. The difference isolates class-specific patterns from general linguistic variation.

### Mechanism 2: Instruction-Following Enables Pattern Probing
Fine-tuned instruction-following LLMs can generate synthetic speech that reveals learned class-specific linguistic features. Unlike GPT-2, Mistral-7B-Instruct can be prompted to "describe the Cookie Theft picture." The fine-tuned MAD generates text with AD markers (disfluencies, incomplete sentences); MC generates fluent, grammatical descriptions.

### Mechanism 3: Normalized Log-Difference Stabilizes Classification
Normalized logarithmic perplexity difference reduces sensitivity to raw perplexity magnitude variations. Formula D̃_log = (log(PPL_HC) - log(PPL_AD)) / log(PPL_HC) normalizes by HC perplexity, making the score less dependent on absolute perplexity values which vary with sequence length and vocabulary.

## Foundational Learning

- **Perplexity as sequence prediction quality**
  - Why needed: The entire method hinges on interpreting perplexity differences. Without this, you cannot understand why two models' scores are compared.
  - Quick check: Given a model that assigns probability 0.01 to each word in a 10-word sequence, what is the perplexity?

- **LoRA (Low-Rank Adaptation) fine-tuning**
  - Why needed: Mistral-7B is fine-tuned using LoRA (rank=16, α=32). Understanding this explains how large models are adapted efficiently without full weight updates.
  - Quick check: What does merging the LoRA adapter with the base model mean, and why is it done?

- **Instruction-tuned LLMs vs base LLMs**
  - Why needed: The choice of Mistral-7B-Instruct over base Mistral enables the interpretability probing. Base models cannot reliably follow "describe this image" prompts.
  - Quick check: Why would GPT-2 struggle with the probing experiments that Mistral-7B-Instruct handles?

## Architecture Onboarding

- **Component map:** ADReSS/DementiaBank → preprocessing → instruction dataset (prompt + transcript pairs) → LoRA fine-tuning (MAD on AD, MC on HC) → inference (compute perplexity with both models) → calculate difference score → classify → interpretability (prompt generation → extract 41 linguistic features → compare distributions)

- **Critical path:**
  1. Data preprocessing consistency (must match train/test)
  2. Balanced train/test splits (prevent leakage, balance age/gender/MMSE)
  3. LoRA fine-tuning with early stopping (patience=3, validate after each step)
  4. Perplexity computation (tokenization alignment between training and inference)
  5. Score aggregation (50 model pairs, same random seeds as baseline)

- **Design tradeoffs:**
  - Fine-tuning vs in-context learning: Paper chose fine-tuning for more substantial perplexity changes; ICL is sensitive to demonstration format
  - Per transcript vs per speaker: Paper chose per-transcript to avoid longitudinal data dependency; trades some accuracy for practicality
  - Mistral-7B vs GPT-2: Larger model, better accuracy, but requires more compute; GPT-2 epoch=30, Mistral epoch=6

- **Failure signatures:**
  - Accuracy drops significantly on DB vs ADReSS with character n-gram baseline → indicates preprocessing matters
  - GPT-2 overfits quickly on small ADReSS dataset (epoch > 5 causes decline) → use early stopping aggressively
  - Misclassifications correlate with MMSE scores near class boundaries → may indicate label noise, not model failure
  - Generated text is incoherent → instruction-following capability degraded during fine-tuning

- **First 3 experiments:**
  1. Replicate baseline: Train GPT-2 paired models on ADReSS, compute D̃_log, verify you achieve ~86% accuracy. This validates your perplexity computation pipeline.
  2. Single Mistral fine-tuning run: Fine-tune one MAD/MC pair, compare D, D*, and D̃_log scores on test set. Confirm normalized log-difference is stable.
  3. Generation probe: After fine-tuning, prompt MAD and MC with "Describe the Cookie Theft picture." Extract linguistic features from generated text and verify 30/41 features differ significantly between AD and HC generated samples.

## Open Questions the Paper Calls Out

- **Can the synthetic text generated by the fine-tuned LLMs be effectively utilized as data augmentation to improve the training of AD detection classifiers?**
  - Basis: The conclusion states that the models' ability to learn AD language patterns "opens up possibilities for... data augmentation," and the abstract lists it as a potential application of the analysis.
  - Unresolved: The paper validates that the generated text mimics human AD linguistic features (Figure 2), but it does not conduct experiments to verify if training on this synthetic data improves classification performance.

- **Does integrating acoustic features (e.g., pause duration, prosody) with the text-based perplexity scores yield a significant improvement in detection accuracy?**
  - Basis: The authors acknowledge that "audio input further increased accuracy" for baselines but intentionally restrict this work to "text-only AD detection" (Section 2.3), and exclude acoustic features from the linguistic analysis (Section 4.3).
  - Unresolved: The paired perplexity method currently relies solely on lexical information, potentially missing non-verbal cues common in AD speech, such as increased hesitation or pausing.

- **Is the proposed paired perplexity approach robust to the noise and errors introduced by Automatic Speech Recognition (ASR) systems?**
  - Basis: The paper relies on human transcripts but notes that the exclusion of manual linguistic annotations makes the preprocessing more representative of "automated AD detection applications" (Section 3.3), implying a need for real-world viability.
  - Unresolved: The study evaluates performance on clean, ground-truth transcripts. It remains unknown if ASR errors—particularly on the disfluent speech typical of AD speakers—would distort the perplexity distributions and degrade the 93.75% accuracy.

## Limitations
- The method relies on clean human transcripts rather than ASR output, limiting real-world applicability
- Performance on DementiaBank dataset is significantly lower than on ADReSS, suggesting domain sensitivity
- The instruction prompt format is fixed and may not generalize to other types of speech tasks

## Confidence
- **Method description accuracy:** High - well-specified with clear hyperparameters and implementation details
- **Reproducibility:** Medium - key details like exact prompt text are missing
- **Generalizability:** Low - results show significant domain sensitivity between ADReSS and DementiaBank

## Next Checks
1. Verify perplexity computation pipeline by replicating GPT-2 baseline and achieving ~86% accuracy
2. Confirm normalized log-difference stability by comparing D, D*, and D̃_log scores on single Mistral run
3. Validate interpretability results by extracting linguistic features from generated text and checking that 30/41 features differ significantly between AD and HC classes