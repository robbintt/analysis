---
ver: rpa2
title: In-Context and Few-Shots Learning for Forecasting Time Series Data based on
  Large Language Models
arxiv_id: '2512.07705'
source_url: https://arxiv.org/abs/2512.07705
tags:
- time
- series
- data
- forecasting
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper evaluates Large Language Models (LLMs) and foundation
  models for time series forecasting using in-context learning. The study compares
  OpenAI o4-mini, Gemini 2.5 Flash Lite, TimesFM, TCN, and LSTM on the SWaT dataset.
---

# In-Context and Few-Shots Learning for Forecasting Time Series Data based on Large Language Models

## Quick Facts
- arXiv ID: 2512.07705
- Source URL: https://arxiv.org/abs/2512.07705
- Reference count: 37
- TimesFM achieved best performance with RMSE 0.3023 and MAE 0.2127 on SWaT dataset

## Executive Summary
This study evaluates Large Language Models and foundation models for time series forecasting using in-context learning on industrial sensor data. The research compares OpenAI o4-mini, Gemini 2.5 Flash Lite, TimesFM, TCN, and LSTM models on the SWaT water treatment dataset. TimesFM emerges as the superior performer with an RMSE of 0.3023 and MAE of 0.2127, achieving this in just 266 seconds. The study demonstrates that pretrained time series foundation models outperform conventional deep learning approaches, offering superior accuracy and efficiency with minimal adaptation requirements.

## Method Summary
The study evaluates five models (TCN, LSTM, TimesFM, OpenAI o4-mini, Gemini 2.5 Flash Lite) on the SWaT water treatment dataset using in-context learning. Models are compared using zero-shot and few-shot approaches with a 720-step sliding window on the LIT301 target variable. TimesFM uses its built-in forecast() method with pre-trained weights, while LLM APIs process time series as tokenized sequences with JSON-formatted outputs. Evaluation metrics include RMSE and MAE on a 15% test split, with TimesFM showing best accuracy and efficiency.

## Key Results
- TimesFM achieved best performance: RMSE 0.3023, MAE 0.2127 in 266 seconds
- OpenAI o4-mini performed well in zero-shot: RMSE 0.3310, MAE 0.2098
- Traditional deep learning models (LSTM, TCN) and Gemini 2.5 Flash Lite showed significantly lower accuracy
- Zero-shot outperformed few-shot for o4-mini (0.3310 vs 0.3727 RMSE)

## Why This Works (Mechanism)

### Mechanism 1
Large-scale pre-training on diverse temporal corpora enables zero-shot generalization to unseen time series domains. TimesFM's decoder-only transformer was pre-trained on 100+ billion time points across Wikimedia Pageviews, Google Trends, and canonical forecasting benchmarks, creating generalizable pattern representations that transfer to industrial sensor data without fine-tuning.

### Mechanism 2
In-context learning enables task adaptation through prompt-conditioned attention without gradient updates. General-purpose LLMs treat time series windows as tokenized text sequences, using self-attention to identify local patterns and extrapolate based on demonstrations or task descriptions.

### Mechanism 3
Domain-specific foundation models outperform general-purpose LLMs for time series forecasting due to specialized temporal inductive biases. TimesFM uses patch-based input processing with residual MLP blocks, rotary positional embeddings, and causal masking optimized for autoregressive forecasting.

## Foundational Learning

- **Sliding Window Temporal Representation**: All models require fixed-length historical windows (720 time steps) to capture temporal dependencies. Quick check: Can you explain why a window size of 720 was chosen and what trade-offs exist between shorter vs. longer context windows?

- **Zero-Shot vs. Few-Shot In-Context Learning**: The study shows o4-mini zero-shot (RMSE 0.3310) outperforms few-shot (RMSE 0.3727), contradicting assumptions that more demonstrations help. Quick check: Why might few-shot prompting degrade performance on certain time series tasks compared to zero-shot?

- **Decoder-Only Transformer Architecture**: TimesFM's superior performance stems from its specialized architecture. Quick check: How does causal masking differ from bidirectional attention, and why is it essential for autoregressive forecasting?

## Architecture Onboarding

- **Component map**: Raw SWaT Data (14,996 × 78) → Preprocessing: Missing value removal, LIT301 target extraction, sliding window (720 steps) → Normalization: StandardScaler fitted on train split only → Model Branches: TCN/LSTM training, TimesFM inference, LLM API prompts → Evaluation: RMSE, MAE on 15% test set

- **Critical path**: 1) Ensure LIT301 target has no missing values before window creation 2) Apply StandardScaler only to training data to prevent data leakage 3) Structure LLM prompts with explicit JSON output format for parseable responses 4) Use TimesFM's built-in forecast() method directly on normalized windows

- **Design tradeoffs**: TimesFM offers best RMSE (0.3023) with 266s inference vs o4-mini zero-shot 80× slower for marginally worse accuracy; few-shot degraded o4-mini performance suggesting prompt complexity introduces noise; foundation models require only inference while traditional models need 20 epochs of training

- **Failure signatures**: Prompt formatting errors causing non-JSON responses; window length mismatches causing dimension errors; concept drift degrading static model performance; API rate limits affecting latency measurements

- **First 3 experiments**: 1) Reproduce TimesFM baseline on SWaT LIT301 with 720-step sliding window and verify RMSE ≈ 0.30 2) Ablate window size testing 360, 720, and 1440 steps to find optimal context length 3) Prompt sensitivity analysis varying o4-mini prompt phrasing to quantify engineering brittleness

## Open Questions the Paper Calls Out

### Open Question 1
Can optimization techniques like model quantization and knowledge distillation effectively reduce the high inference costs and latency of Large Language Models for time series forecasting in resource-constrained environments? The study reveals prohibitive inference times (over 21,000 seconds for o4-mini) creating a trade-off between performance and efficiency.

### Open Question 2
Does incorporating uncertainty-aware probabilistic estimates into foundation models improve the reliability of anomaly detection in industrial time series data? Current evaluation focuses on point prediction accuracy rather than quantifying prediction confidence or detecting distributional shifts.

### Open Question 3
Do the superior forecasting capabilities of foundation models generalize to multivariate time series datasets and alternative target variables beyond LIT301? The study's restriction to single dataset and variable suggests findings may not represent diverse cyber-physical systems.

## Limitations

- Single industrial dataset (SWaT) with one target variable limits generalizability
- No analysis of catastrophic forgetting or concept drift in foundation models
- API latency measurements may not reflect optimal batching or parallel processing
- Few-shot prompting degraded o4-mini performance without sufficient explanation
- TimesFM evaluation lacks ablation studies on architectural components

## Confidence

**High Confidence**: TimesFM's superior accuracy (RMSE 0.3023) over traditional models for this specific industrial forecasting task

**Medium Confidence**: TimesFM's 80× faster inference than o4-mini zero-shot represents a generalizable efficiency advantage

**Medium Confidence**: In-context learning enables zero-shot generalization across domains based on single-variable evaluation

**Low Confidence**: Mechanism explanation for why few-shot prompting degraded o4-mini performance

## Next Checks

1. **Domain Transfer Validation**: Evaluate TimesFM and o4-mini on multiple industrial time series datasets to assess generalizability beyond SWaT and compare zero-shot performance against domain-specific fine-tuning

2. **Prompt Sensitivity Stress Test**: Systematically vary prompt formats for o4-mini across zero-shot and few-shot settings to quantify prompt engineering brittleness and identify robust prompt templates

3. **Temporal Drift Robustness**: Simulate concept drift by introducing gradual shifts in sensor patterns during testing to evaluate whether foundation models maintain accuracy or require periodic retraining compared to adaptive traditional models