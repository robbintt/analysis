---
ver: rpa2
title: A Benchmark for Audio Reasoning Capabilities of Multimodal Large Language Models
arxiv_id: '2601.19673'
source_url: https://arxiv.org/abs/2601.19673
tags:
- audio
- answer
- sound
- template
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces ART (Audio Reasoning Tasks), a benchmark\
  \ designed to evaluate multimodal large language models\u2019 ability to perform\
  \ reasoning over audio signals. Unlike existing benchmarks that test isolated audio\
  \ tasks, ART focuses on tasks requiring reasoning across different audio categories."
---

# A Benchmark for Audio Reasoning Capabilities of Multimodal Large Language Models

## Quick Facts
- arXiv ID: 2601.19673
- Source URL: https://arxiv.org/abs/2601.19673
- Reference count: 15
- Primary result: State-of-the-art models achieve only 54.73% accuracy on cross-category audio reasoning tasks, demonstrating significant performance gaps.

## Executive Summary
This paper introduces ART (Audio Reasoning Tasks), a benchmark designed to evaluate multimodal large language models' ability to perform reasoning over audio signals. Unlike existing benchmarks that test isolated audio tasks, ART focuses on tasks requiring reasoning across different audio categories. The benchmark includes nine tasks with 9,000 samples, carefully designed to be solvable by humans without professional training. Evaluation of state-of-the-art models showed limited performance, with the best model achieving only 54.73% accuracy, demonstrating the benchmark's challenging nature and the gap between current models and human-level audio reasoning.

## Method Summary
The ART benchmark uses template-based generation to create 9,000 audio samples across nine reasoning tasks. Audio is synthesized using Voicebox TTS with HiFi-GAN vocoder for speech and Freesound samples for environmental sounds, all normalized to -20 dBFS. Evaluation uses two approaches: Yes/No binary responses (automatically evaluated) and Descriptive responses (evaluated by LLM-as-a-judge). The benchmark tests both cascaded systems (Whisper + LLM) and native multimodal LLMs across zero-shot and few-shot conditions.

## Key Results
- Best model (Audio Flamingo 3) achieved only 54.73% accuracy on Yes/No tasks
- Cascaded systems performed comparably to native MLLMs despite separate training
- Models showed systematic task confusion, often defaulting to transcription behavior
- Descriptive evaluation revealed diverse failure patterns beyond simple accuracy metrics

## Why This Works (Mechanism)

### Mechanism 1: Cross-Modal Integration Burden
- Claim: Tasks requiring combining multiple audio processing skills expose integration weaknesses in cascaded architectures
- Core assumption: Integration failures, not individual component failures, are the primary bottleneck for audio reasoning
- Evidence: Models cannot solve tasks with output from single specialized modules; Audio Flamingo 3 achieves 54.73% despite strong individual components

### Mechanism 2: Task Confusion Under Ambiguity
- Claim: Models exhibit systematic bias toward transcription when audio contains multiple semantic layers
- Core assumption: Training objectives prioritizing speech-to-text create strong priors that override task objectives
- Evidence: Qwen-Audio-Chat returned transcriptions for 100% of Yes/No errors; models confused speaker recognition with question answering

### Mechanism 3: Synthetic Audio Fidelity Ceiling
- Claim: High-quality TTS-synthesized speech creates clean evaluation environment that overestimates real-world performance
- Core assumption: Reasoning failures on clean audio would persist or worsen under acoustic degradation
- Evidence: Clean audio isolates reasoning errors from signal quality issues, but real-world performance remains unverified

## Foundational Learning

- **Concept: Audio-Language Model Architectures**
  - Why needed here: Understanding cascaded vs. end-to-end models helps interpret benchmark results and identify integration failure points
  - Quick check question: Can you explain why a cascaded Whisper + Llama system might fail at Cross-Recording Speaker Identification even if both components perform well individually?

- **Concept: Multi-Hop Audio Reasoning**
  - Why needed here: ART tasks require sequential reasoning steps (identify sound → retrieve knowledge → compare attributes → formulate answer)
  - Quick check question: For "Is the animal that makes the following sound bigger than a horse?", what are the three distinct capabilities required?

- **Concept: Evaluation Methodology Trade-offs**
  - Why needed here: Yes/No constrains output space but masks reasoning quality; Descriptive reveals reasoning but introduces judge bias
  - Quick check question: Why use LLM-as-a-judge for Descriptive but not Yes/No evaluation? What bias risks does this introduce?

## Architecture Onboarding

- **Component map:**
  - Task Templates (55) → Audio Generation (Voicebox TTS + Freesound) → Question Integration → Model Inference → Response Classification

- **Critical path:**
  1. Template selection → slot value population → target answer derivation
  2. Audio synthesis (question + utterance + background sound) with temporal alignment
  3. Model inference with constrained (Yes/No) or open (Descriptive) prompting
  4. Response classification: relevant/irrelevant → correct/incorrect

- **Design tradeoffs:**
  - Synthetic vs. natural audio: Chose synthetic for controllability and error isolation; trades away ecological validity
  - Template-based vs. free-form generation: Templates enable systematic coverage and contamination resistance; limits diversity
  - Yes/No vs. Descriptive evaluation: Yes/No enables automatic evaluation; Descriptive provides richer failure signal but introduces judge bias

- **Failure signatures:**
  - High relevance, low accuracy (Audio Flamingo 3: 100% relevant, 54.73% accuracy): Model understands task format but lacks reasoning capability
  - Low relevance (GAMA: 42.56% relevant): Model fails to parse audio-question relationship
  - Task-specific collapse (Qwen-Audio-Chat: 100% transcription errors): Strong prior overrides task objective
  - Judge-model collusion (Qwen3 judging Qwen-family models): Accuracy inflation when same model family used for generation and evaluation

- **First 3 experiments:**
  1. **Baseline establishment:** Run Whisper + Llama and Whisper + Qwen cascaded systems on full ART to establish non-integrated baseline. Expect 54-56% accuracy based on paper results.
  2. **Error mode classification:** Sample 100 errors per model from Yes/No approach. Manually categorize as: (a) audio perception failure, (b) task confusion, (c) knowledge retrieval failure, (d) reasoning chain break.
  3. **Contamination probe:** Test whether one-shot examples from same template vs. different template affect performance. Paper shows same-template improves Qwen2-Audio from 44.31% to 47.10% absolute accuracy.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does model performance degrade under noisy, natural acoustic conditions compared to clean, synthesized environment?
- Basis: Section 5 states models failed to attain satisfactory performance even on clean samples
- Why unresolved: Authors limited benchmark to synthetic audio to isolate reasoning from perception errors
- What evidence would resolve it: Evaluation results on ART dataset injected with background noise or natural recording artifacts

### Open Question 2
- Question: Can improved architectural integration bridge performance gap between cascaded systems and end-to-end MLLMs?
- Basis: Introduction posits separate pre-training as concern; results show cascaded systems competitive with end-to-end models
- Why unresolved: Benchmark reveals failures but doesn't isolate whether integration or component quality is bottleneck
- What evidence would resolve it: Comparative study measuring performance delta between cascaded and fully integrated architectures with equivalent backbones

### Open Question 3
- Question: Does high accuracy on Yes/No tasks predict success in open-ended, descriptive audio reasoning without LLM-as-a-judge?
- Basis: Section 4.4 notes Descriptive approach yields diverse failure patterns compared to Yes/No
- Why unresolved: Human baseline established for Yes/No but Descriptive relies on potentially biased LLM judges
- What evidence would resolve it: Human evaluation of model responses on Descriptive task subset to correlate with Yes/No scores

### Open Question 4
- Question: To what extent do template-based constraints limit detection of hallucinations or reasoning errors in novel scenarios?
- Basis: Section 7 acknowledges proposed tasks cannot guarantee models achieve human-level audio reasoning
- Why unresolved: Templates ensure automated evaluation but may allow models to exploit patterns rather than demonstrate genuine reasoning
- What evidence would resolve it: Evaluation using dynamically generated tasks or audio samples outside predefined templates

## Limitations

- Synthetic audio approach may overestimate real-world performance due to lack of acoustic variability
- LLM-as-a-judge evaluation introduces potential bias, particularly evident in 8-10% accuracy inflation for same-family models
- Binary Yes/No evaluation masks reasoning quality and may overrepresent transcription behavior
- Template-based generation limits detection of reasoning errors in novel, out-of-distribution scenarios

## Confidence

**High Confidence (90-100%):** Benchmark successfully isolates reasoning failures from audio quality issues. Methodology for template-based generation and contamination resistance is sound.

**Medium Confidence (60-80%):** Integration failures are primary bottleneck for audio reasoning. Data shows cascaded systems performing similarly to native MLLMs, but specific failure modes aren't fully characterized.

**Low Confidence (0-40%):** Synthetic audio performance directly predicts real-world performance. Confidence in training distribution bias causing transcription default behavior is also low without controlled retraining experiments.

## Next Checks

1. **Natural Audio Validation:** Test same models on ART tasks using naturally recorded audio to verify whether reasoning failures persist under real-world conditions.

2. **Controlled Retraining Experiment:** Retrain models with balanced task objectives to test whether training distribution bias causes transcription default behavior.

3. **Integration vs. Component Quality Isolation:** Conduct ablation studies where individual audio processing components are replaced with perfect oracles while keeping LLM fixed.