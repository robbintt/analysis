---
ver: rpa2
title: Don't Forget It! Conditional Sparse Autoencoder Clamping Works for Unlearning
arxiv_id: '2503.11127'
source_url: https://arxiv.org/abs/2503.11127
tags:
- unlearning
- clamp
- sparse
- refusal
- knowledge
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The authors address the problem of unlearning harmful knowledge
  from large language models (LLMs) while preserving general knowledge, a critical
  challenge for safe AI deployment. They propose a conditional sparse autoencoder
  (SAE) clamping approach that selectively modifies internal model activations based
  on interpretable latent features identified through frequency analysis of harmful
  versus safe content.
---

# Don't Forget It! Conditional Sparse Autoencoder Clamping Works for Unlearning

## Quick Facts
- **arXiv ID**: 2503.11127
- **Source URL**: https://arxiv.org/abs/2503.11127
- **Reference count**: 24
- **Primary result**: Conditional SAE clamping achieves 0.8226 alignment score on WMDP-Bio, outperforming prior methods while better preserving safe knowledge

## Executive Summary
This paper addresses the critical challenge of unlearning harmful knowledge from large language models while preserving general capabilities. The authors propose a conditional sparse autoencoder (SAE) clamping approach that selectively modifies internal model activations based on interpretable latent features identified through frequency analysis of harmful versus safe content. Their method, "Refusal Clamp," conditionally steers refusal-related latents when harmful features activate, achieving better performance than unconditional clamping approaches and comparable results to RMU in forgetting harmful information while maintaining safe knowledge retention.

## Method Summary
The method uses conditional SAE clamping to unlearn harmful bioweapon knowledge from LLMs. It first computes activation frequencies for each SAE latent across "forget" (bio-weapon) and "retain" (safe biology) corpora, then filters out features with high retain frequency and selects top harmful-feature latents. During inference, the model monitors these harmful-feature latents and, when any exceeds a threshold, conditionally clamps a refusal-associated latent to a negative value. This induces refusal behavior only in harmful contexts rather than globally, preserving general knowledge better than unconditional clamping methods.

## Key Results
- Best method (Refusal Clamp) achieved 0.8226 alignment score on WMDP-Bio benchmark
- Reduced harmful knowledge accuracy from 0.586 to 0.272 while maintaining general knowledge performance
- Outperformed Clamp Prime (0.7799 alignment) and showed better safe knowledge retention than RMU
- Demonstrated robustness against Concurrent Greedy Search suffix attacks in preliminary testing

## Why This Works (Mechanism)

### Mechanism 1
SAE latent features can discriminate between harmful and safe content within a knowledge domain. The method computes normalized activation frequencies for each latent across "forget" and "retain" corpora, discarding features exceeding a frequency threshold in the retain set, then ranks remaining features by forget-set activation to identify candidates for steering. This works if harmful and safe knowledge activate distinguishably different latent features.

### Mechanism 2
Conditionally clamping a "refusal" latent when harmful features activate suppresses harmful outputs while preserving safe knowledge better than unconditional clamping. The method monitors harmful-feature latents and, when any exceeds a threshold, clamps a single refusal-associated latent to a negative value, inducing refusal behavior only in harmful contexts. This succeeds if the refusal latent exists and can be triggered conditionally without degrading unrelated capabilities.

### Mechanism 3
Raising the activation threshold above zero reduces over-clamping and improves retention of safe knowledge. Since SAE latents rarely reach exactly zero activation (only ~19% in experiments), clamping at >0 creates spurious interventions. Raising the threshold (e.g., to 0.05) reduces false positives where low-magnitude activations trigger unnecessary steering, working if meaningful harmful content activates features above the threshold.

## Foundational Learning

- **Concept: Sparse Autoencoders (SAEs)** - Needed to understand that SAEs decompose LLM activations into interpretable sparse features; without this, clamping approach makes no sense. Quick check: Given an LLM hidden state of dimension d, what does an SAE with hidden dimension 16k output, and what does sparsity enforce?

- **Concept: Superposition Hypothesis** - Needed to understand why steering individual latents might work or cause collateral damage; SAEs disentangle concepts that neural networks represent as linear combinations of neurons. Quick check: Why can't we simply identify individual neurons representing "harmful knowledge" without SAEs?

- **Concept: Representation Engineering / Activation Steering** - Needed to understand that model behavior can be controlled by modifying hidden activations during inference. Quick check: What happens if you add a large negative vector to a specific activation dimension during every forward pass?

## Architecture Onboarding

- **Component map**: Base model -> SAE layer_7/width_16k/canonical -> Latent labels (Neuronpedia) -> Hooked activations -> Steering CSV configuration

- **Critical path**: 1) Run forward pass with SAE hooks enabled, 2) At layer 7, compute SAE latent activations from residual stream, 3) Check if monitored harmful-feature latents exceed threshold, 4) If yes, clamp refusal latent to negative coefficient, 5) Decode modified latents back to residual stream dimension, 6) Continue forward pass

- **Design tradeoffs**: 
  - Top-k features: More features → better harmful coverage but more false positives; paper finds ~10 optimal
  - Clamping coefficient: Larger negative values → stronger refusal but more capability degradation; paper uses -500 vs -300
  - Threshold: Higher threshold → fewer interventions but risk missing harmful content; paper uses 0.05
  - SAE layer choice: Layer 7 chosen for available labeled SAEs; different layers may capture different abstraction levels

- **Failure signatures**:
  - Over-unlearning: MMLU accuracy drops significantly → threshold too low or too many features selected
  - Under-unlearning: WMDP-Bio accuracy remains high → threshold too high or coefficient too small
  - Inconsistent refusal: Model refuses some safe queries → refusal latent entangled with non-harmful concepts
  - Adversarial bypass: Suffix attacks extract harmful content → clamping insufficiently robust

- **First 3 experiments**:
  1. Baseline frequency analysis: Pass bio-forget and bio-retain corpora through model with SAE hooks; plot activation frequency distributions
  2. Single-feature ablation: Clamp one harmful-feature latent at a time with coefficient -100; measure WMDP-Bio and MMLU accuracy
  3. Threshold sweep: Fix feature set and coefficient, vary threshold from 0.0 to 0.2; plot alignment score vs threshold

## Open Questions the Paper Calls Out

### Open Question 1
Which SAE layers allow for optimal unlearning performance? The study only used layer 7 of gemma-2-2b; different layers may encode harmful knowledge differently or allow better selectivity. Systematic comparison across all available SAE layers using identical clamping procedures would resolve this.

### Open Question 2
How does SAE latent space dimensionality affect unlearning outcomes? Only the 16k width SAE was tested; larger SAEs may offer more granular feature separation between harmful and safe knowledge. Comparing alignment scores when using SAEs with different hidden dimensions would resolve this.

### Open Question 3
How robust is conditional SAE clamping against diverse adversarial attack methods? Only one attack (Concurrent Greedy Search) was tested on one question-answer pair with limited optimization iterations. Evaluating against multiple attack types with increased compute budgets across many question pairs would resolve this.

### Open Question 4
Does the method generalize to other domains beyond biology, such as cybersecurity? The WMDP benchmark includes both Bio and Cyber corpora, but experiments only used WMDP-Bio; domain-specific knowledge may be encoded differently. Applying the same methodology to WMDP-Cyber would resolve this.

## Limitations
- The identification of the "refusal" latent (15864) lacks rigorous verification beyond GPT-4o auto-interpretation without human validation
- Only one SAE layer (layer 7) was tested, leaving open whether other layers might yield better unlearning performance
- Limited adversarial robustness testing was conducted, with only one attack method tested on a single question pair

## Confidence
- **High**: The general SAE clamping framework works for unlearning (demonstrates measurable forgetting on WMDP-Bio)
- **Medium**: Conditional refusal clamping outperforms unconditional clamping (shown on one benchmark with one model)
- **Low**: The specific latent 15864 is definitively a "refusal" feature and not entangled with other behaviors

## Next Checks
1. **Latent ablation study**: Remove the conditional component and test direct clamping of harmful-feature latents with refusal latent held constant to isolate whether the conditional mechanism adds value beyond simple feature suppression.

2. **Cross-domain generalization**: Apply the same feature selection and clamping approach to a different harmful knowledge domain (e.g., self-harm instructions or illegal activities) to test whether the frequency-based method generalizes beyond bioweapon content.

3. **Adversarial robustness expansion**: Beyond suffix attacks, test whether the model can be prompted to extract harmful knowledge through chain-of-thought reasoning or role-playing scenarios where refusal is disinhibited.