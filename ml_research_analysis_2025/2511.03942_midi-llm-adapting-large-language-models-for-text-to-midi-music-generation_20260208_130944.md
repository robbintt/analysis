---
ver: rpa2
title: 'MIDI-LLM: Adapting Large Language Models for Text-to-MIDI Music Generation'
arxiv_id: '2511.03942'
source_url: https://arxiv.org/abs/2511.03942
tags:
- music
- text
- midi-llm
- midi
- inproc
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces MIDI-LLM, a method for adapting large language
  models (LLMs) to generate multitrack MIDI music from text prompts. The approach
  expands a text LLM''s vocabulary to include MIDI tokens and uses a two-stage training
  recipe: continued pretraining on music-related text and standalone MIDI data, followed
  by supervised finetuning on paired text-MIDI data.'
---

# MIDI-LLM: Adapting Large Language Models for Text-to-MIDI Music Generation

## Quick Facts
- arXiv ID: 2511.03942
- Source URL: https://arxiv.org/abs/2511.03942
- Reference count: 4
- Primary result: Achieves higher quality and better text control than Text2midi with 50% faster inference using vLLM optimizations.

## Executive Summary
This paper introduces MIDI-LLM, a method for adapting large language models to generate multitrack MIDI music from text prompts. The approach expands a text LLM's vocabulary to include MIDI tokens and uses a two-stage training recipe: continued pretraining on music-related text and standalone MIDI data, followed by supervised finetuning on paired text-MIDI data. By preserving the original LLM's parameter structure, the method can directly leverage vLLM for accelerated inference. Experiments show that MIDI-LLM achieves higher quality and better text control than the recent Text2midi model, with faster inference times.

## Method Summary
The method expands a pretrained text LLM's vocabulary by concatenating new embedding weights for MIDI tokens, then trains in two stages: first on unpaired music data and music-related text to learn MIDI syntax, then on paired text-MIDI data for text-to-music alignment. The preserved decoder-only architecture enables direct use of vLLM optimizations for faster inference. The model uses Anticipatory Music Transformer (AMT) tokenization, representing each note with three tokens (onset, duration, pitch-instrument).

## Key Results
- Achieves higher quality metrics (FAD and CLAP) than Text2midi
- 50% faster inference using vLLM optimizations
- Better text control and adherence to prompts
- Maintains text generation capabilities while adding MIDI generation

## Why This Works (Mechanism)

### Mechanism 1: Native Token Embedding Expansion
Direct vocabulary expansion reduces sequence length overhead compared to text-serialization methods, improving computational efficiency and learning density. The model concatenates a new embedding matrix (55K tokens) to the existing LLM matrix (128K tokens), mapping MIDI events directly to single vector representations rather than multi-token text strings.

### Mechanism 2: Decoupled Syntax Acquisition via Continued Pretraining
A dedicated pre-training stage on unpaired music data is necessary to ground the model in MIDI syntax before attempting text-to-MIDI translation. Stage 1 trains on standalone MIDI and music-related text to teach structural probability of musical sequences independent of text prompts.

### Mechanism 3: Inference Optimization via Architectural Preservation
Maintaining the standard Transformer decoder structure allows "off-the-shelf" inference engines (vLLM) to optimize generation speed via memory management and quantization. Unlike custom encoder-decoder architectures, MIDI-LLM uses a standard decoder-only structure enabling direct application of PagedAttention and FP8 quantization.

## Foundational Learning

**Concept: MIDI Tokenization Strategies (MIDI-like vs. REMI vs. ABC)**
- Why needed: The paper uses "Anticipatory Music Transformer" (AMT) arrival-time tokens requiring 3 tokens per note versus REMI's beat-synchronization
- Quick check: Why does AMT tokenization offer "more flexibility" than REMI for this specific model?

**Concept: Vocabulary Expansion / Resizing Embeddings**
- Why needed: The core technical contribution is resizing the embedding layer; understanding how new weights are initialized and gradients flow
- Quick check: When E_AMT is concatenated to E_LLM, are the original text embeddings frozen or fine-tuned during Stage 1?

**Concept: vLLM and PagedAttention**
- Why needed: The paper claims 50% speed improvements; understanding KV caching and PagedAttention is necessary to debug latency issues
- Quick check: Does preserving the parameter structure imply that the attention mechanism itself is unmodified, or just the parameter tensor shapes?

## Architecture Onboarding

**Component map:** Text Prompt (Tokenized by Llama Tokenizer) -> Llama-3.2-1B (Decoder-only Transformer with expanded embeddings) -> Softmax over expanded vocabulary -> Decode AMT tokens -> Convert to .mid file

**Critical path:** The Vocabulary Expansion (Section 3.1) and Data Curation for Stage 1 (Section 3.2) are the most fragile steps. If embedding initialization variance is wrong, or if Stage 1 data contains corrupted MIDI files, the model will generate silence or noise.

**Design tradeoffs:** Control vs. Editability: During infilling tasks, text prompt has minimal influence compared to surrounding MIDI context, implying the model prioritizes musical coherence over semantic obedience. Dynamics: AMT tokenization does not explicitly model note velocity/dynamics like Text2midi, trading expressive nuance for structural simplicity.

**Failure signatures:** Stuck Generation (repeats same bar/instrument loop), Text Ignorance (generated music contradicts prompt), Syntax Errors (invalid MIDI sequences like Note-On without Note-Off).

**First 3 experiments:**
1. Embedding Sanity Check: Overfit on single short MIDI file to verify expanded vocabulary and tokenizer pipeline
2. Stage 1 Ablation: Train two models, one with MusicPile corpus and one with general text, to verify music-specific text is not strictly necessary
3. Inference Latency Profiling: Run inference with and without vLLM/FP8 quantization on L40S GPU to reproduce RTF speedups

## Open Questions the Paper Calls Out

**Open Question 1:** How can text-guided editing capabilities be effectively integrated into the MIDI-LLM framework?
- Basis: Authors state developing text-guided editing capabilities is a crucial direction to move beyond non-iterative text-to-MIDI generation
- Why unresolved: Current implementation focuses on generating music from scratch rather than modifying or rearranging existing sequences iteratively

**Open Question 2:** Is domain-specific continued pretraining on music-related text necessary for high-quality text-to-MIDI generation?
- Basis: Authors report a "negative finding" where replacing music-adjacent corpus with general-domain corpus resulted in "no noticeable change" in final performance
- Why unresolved: Conventional domain adaptation suggests specialized data is beneficial, but results suggest pre-trained backbone may already possess sufficient knowledge

**Open Question 3:** Why does text conditioning fail to influence the model during music infilling tasks?
- Basis: Authors note that during infilling, "text has minimal influence during inference" as generated segments are "primarily determined by surrounding MIDI context"
- Why unresolved: Mechanism by which model prioritizes autoregressive context over text prefix instruction is unknown, limiting control in editing scenarios

## Limitations
- Limited analysis of how random initialization of 55K new embedding vectors affects convergence and whether catastrophic forgetting occurs
- Pretraining necessity assumption only partially validated through limited ablation studies
- Inference speedup claims depend heavily on specific vLLM optimizations that may not generalize across hardware

## Confidence
**High Confidence:** Vocabulary expansion mechanism is technically well-specified with clear mathematical formulation; architectural preservation claim enabling vLLM optimization is verifiable; experimental setup for FAD and CLAP evaluation is standard and reproducible.

**Medium Confidence:** Two-stage training methodology's effectiveness relies on assumptions about pretraining necessity only partially validated; MIDI quality metrics may not fully capture musical coherence; claim of "better text control" requires more granular analysis across diverse genres.

**Low Confidence:** Generalization of inference speedup claims across different GPU architectures is untested; impact of AMT tokenization choices on musical expressiveness is not systematically evaluated; robustness to diverse musical styles requires more extensive validation.

## Next Checks

1. **Vocabulary Expansion Stability Test:** Train baseline model with frozen original text embeddings during Stage 1 pretraining, then compare against approach where all embeddings are updated. Measure KL divergence between pre- and post-training text token distributions to quantify catastrophic forgetting.

2. **Pretraining Necessity Ablation:** Implement direct supervised finetuning baseline that skips Stage 1 entirely and trains only on paired text-MIDI data from MidiCaps and LMD. Compare FAD, CLAP, and text adherence metrics to validate whether standalone MIDI pretraining provides measurable benefits.

3. **Hardware-Agnostic Inference Profiling:** Reproduce inference speedup claims on multiple GPU architectures (L40S, A100, H100) with varying batch sizes and sequence lengths. Measure memory bandwidth utilization, KV cache efficiency, and real-time generation latency to identify hardware-specific bottlenecks and validate claimed 50% speedup advantage.