---
ver: rpa2
title: How to make Medical AI Systems safer? Simulating Vulnerabilities, and Threats
  in Multimodal Medical RAG System
arxiv_id: '2508.17215'
source_url: https://arxiv.org/abs/2508.17215
tags:
- medical
- image
- adversarial
- attack
- generation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MedThreatRAG, a novel framework for systematically
  probing vulnerabilities in multimodal medical retrieval-augmented generation (RAG)
  systems by injecting adversarial image-text pairs. The core innovation is Cross-Modal
  Conflict Injection (CMCI), which embeds subtle semantic contradictions between medical
  images and their paired reports to disrupt retrieval and generation while evading
  conventional filters.
---

# How to make Medical AI Systems safer? Simulating Vulnerabilities, and Threats in Multimodal Medical RAG System

## Quick Facts
- arXiv ID: 2508.17215
- Source URL: https://arxiv.org/abs/2508.17215
- Authors: Kaiwen Zuo; Zelin Liu; Raman Dutt; Ziyang Wang; Zhongtian Sun; Yeming Wang; Fan Mo; Pietro LiÃ²
- Reference count: 14
- Primary result: Cross-modal conflict injection reduces medical RAG system accuracy by up to 27.66% in semi-open attack environments

## Executive Summary
This paper introduces MedThreatRAG, a framework for systematically probing vulnerabilities in multimodal medical retrieval-augmented generation systems. The core innovation is Cross-Modal Conflict Injection (CMCI), which embeds subtle semantic contradictions between medical images and their paired reports to disrupt retrieval and generation while evading conventional filters. The study simulates a semi-open attack environment mirroring real-world medical systems that permit periodic knowledge base updates.

Evaluations on IU-Xray and MIMIC-CXR QA tasks demonstrate that MedThreatRAG reduces answer F1 scores by up to 27.66% and lowers LLaVA-Med-1.5 F1 rates to as low as 51.36%. The results expose fundamental security gaps in clinical RAG systems and highlight the urgent need for threat-aware design and robust multimodal consistency checks.

## Method Summary
MedThreatRAG employs Cross-Modal Conflict Injection (CMCI) to systematically probe vulnerabilities in multimodal medical RAG systems. The framework injects adversarial image-text pairs with subtle semantic contradictions into the knowledge base, disrupting retrieval and generation processes. The semi-open attack environment simulates real-world scenarios where systems allow periodic knowledge base updates, enabling controlled adversarial input injection while maintaining operational functionality.

## Key Results
- CMCI reduced LLaVA-Med-1.5 F1 scores to as low as 51.36% on medical QA tasks
- Performance degradation reached up to 27.66% reduction in answer F1 scores
- Adversarial examples successfully evaded conventional filtering mechanisms while maintaining semantic plausibility

## Why This Works (Mechanism)
CMCI exploits the fundamental vulnerability in multimodal retrieval systems where cross-modal consistency checks are either absent or insufficient. By embedding subtle contradictions between image and text modalities that escape conventional detection while remaining semantically plausible, the framework disrupts the retrieval phase which cascades into generation failures. The semi-open environment assumption allows adversaries to gradually poison the knowledge base with these carefully crafted contradictions, amplifying their impact over time.

## Foundational Learning
- **Multimodal Retrieval-augmented Generation (RAG)**: Combines image and text understanding with retrieval mechanisms to generate responses. Why needed: Medical diagnosis requires synthesizing visual and textual information. Quick check: Can the system correctly retrieve relevant documents when given both image and text queries?
- **Cross-modal Consistency**: The alignment between different modalities (image and text) in their semantic content. Why needed: Medical systems must ensure visual findings match textual descriptions. Quick check: Do extracted features from image and text modalities correlate appropriately?
- **Adversarial Example Generation**: Creating inputs designed to fool machine learning models while appearing normal to humans. Why needed: Security testing requires understanding attack vectors. Quick check: Can generated examples maintain plausibility while causing misclassification?
- **Semi-open Attack Environment**: A threat model allowing controlled adversarial input injection into periodically updated systems. Why needed: Reflects realistic medical system update cycles. Quick check: Does the system maintain functionality while allowing selective knowledge base modifications?
- **Retrieval Quality Metrics**: Measures like recall and precision for evaluating information retrieval effectiveness. Why needed: Critical for assessing how well medical systems find relevant information. Quick check: What percentage of relevant documents appear in top-K retrieval results?

## Architecture Onboarding

Component Map:
User Query -> Image+Text Retrieval -> Document Selection -> Multimodal Fusion -> Answer Generation

Critical Path: The retrieval phase is the critical vulnerability point where CMCI attacks first manifest. Once contradictory information enters the retrieval pool, it propagates through document selection and fusion stages to degrade final generation quality.

Design Tradeoffs: The framework prioritizes realistic threat simulation over comprehensive defense testing. While this provides valuable vulnerability assessment, it may underestimate the effectiveness of multi-layered security approaches common in production medical systems.

Failure Signatures: The primary failure mode is retrieval contamination, where contradictory multimodal pairs are retrieved alongside legitimate documents, causing confusion in the fusion stage. This manifests as decreased answer coherence and factual accuracy.

First 3 Experiments:
1. Baseline evaluation: Measure vanilla system performance on IU-Xray and MIMIC-CXR tasks without adversarial input
2. Controlled CMCI injection: Systematically inject increasing percentages of adversarial examples and measure performance degradation
3. Filter bypass validation: Test whether conventional consistency checks detect adversarial examples while measuring their impact on legitimate performance

## Open Questions the Paper Calls Out
None

## Limitations
- Semi-open environment assumption may overestimate attack effectiveness in fully closed production systems
- Results may not generalize across different medical specialties or imaging modalities beyond chest X-rays
- Limited characterization of which specific filter types are bypassed by CMCI attacks

## Confidence
- MedThreatRAG framework effectiveness: High
- Security gaps in clinical RAG systems: High
- Need for threat-aware design: High
- CMCI evasion of conventional filters: Medium

## Next Checks
1. Test MedThreatRAG in a simulated production environment with multiple concurrent security layers to assess real-world effectiveness
2. Evaluate framework performance across diverse medical specialties and multimodal models beyond LLaVA-Med-1.5
3. Develop and benchmark defensive strategies specifically targeting cross-modal inconsistencies