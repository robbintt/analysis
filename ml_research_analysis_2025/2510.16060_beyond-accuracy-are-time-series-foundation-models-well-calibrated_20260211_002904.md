---
ver: rpa2
title: 'Beyond Accuracy: Are Time Series Foundation Models Well-Calibrated?'
arxiv_id: '2510.16060'
source_url: https://arxiv.org/abs/2510.16060
tags:
- calibration
- time
- error
- series
- forecasting
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ''
---

# Beyond Accuracy: Are Time Series Foundation Models Well-Calibrated?

## Quick Facts
- **arXiv ID**: 2510.16060
- **Source URL**: https://arxiv.org/abs/2510.16060
- **Reference count**: 40
- **Primary result**: Time series foundation models show favorable calibration properties, avoiding the overconfidence typical of deep learning models.

## Executive Summary
This paper investigates the calibration properties of Time Series Foundation Models (TSFMs) in zero-shot univariate forecasting. Unlike traditional deep learning models that tend toward overconfidence, TSFMs trained with quantile loss demonstrate well-calibrated probabilistic predictions. The study evaluates five TSFMs against two baselines across six datasets, revealing that TSFMs consistently maintain better calibration while achieving competitive accuracy. The research also examines how prediction head expressiveness and autoregressive forecasting methods impact calibration quality.

## Method Summary
The study evaluates five TSFMs (Chronos-Bolt, Moirai 2.0, TimesFM, TiRex, YingLong) on zero-shot univariate forecasting across six datasets. Calibration is measured using Probabilistic Calibration Error (PCE), Centered Calibration Error (CCE), and Scaled Interval Width (SIW), with MASE used for accuracy comparison. Models use quantile prediction heads by default, and three autoregressive methods (naive, branching, trajectory) are compared for long-term forecasting. Experiments systematically vary prediction head types (Gaussian, Student's t, mixture) and AR methods to isolate calibration effects.

## Key Results
- TSFMs are well-calibrated (low PCE) and not systematically overconfident, unlike other deep learning models
- Gaussian prediction heads show significantly worse calibration than quantile, Student's t, or mixture heads
- Trajectory autoregressive forecasting preserves calibration better than branching or naive methods for long-term predictions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: TSFMs avoid systematic overconfidence observed in other deep learning domains through calibration-aware training objectives.
- Mechanism: Unlike image/text models trained on reconstruction or classification loss, TSFMs are trained to minimize quantile loss (WQL), which directly penalizes miscalibrated probability predictions. This creates a supervision signal that explicitly shapes the output distribution rather than just the point estimate.
- Core assumption: The calibration benefits transfer from pretraining data to zero-shot evaluation on held-out datasets.
- Evidence anchors:
  - [section 4.2] "This can likely be explained by the fact that TSFMs are being directly trained with a calibration-aware loss (i.e., trained to minimize WQL), while text and image models are trained to minimize reconstruction or classification error."
  - [abstract] TSFMs "tend not to be either systematically over- or under-confident, in contrast to the overconfidence often seen in other deep learning models."
  - [corpus] Neighbor paper "Beyond Overconfidence: Foundation Models Redefine Calibration in Deep Neural Networks" reports similar calibration improvements in image foundation models, suggesting the mechanism may generalize beyond time series.
- Break condition: If a TSFM is trained without quantile/distributional loss (e.g., pure MSE), calibration benefits should degrade toward overconfidence typical of deep models.

### Mechanism 2
- Claim: Prediction head expressiveness determines calibration quality, with more flexible heads (quantile, Student's t, mixture) outperforming simple Gaussian parameterization.
- Mechanism: Gaussian distribution heads constrain the model to symmetric, light-tailed distributions. When the true conditional distribution is asymmetric or heavy-tailed, the Gaussian head compensates by widening intervals (increased SIW), leading to systematic under-confidence. More expressive heads adapt their shape to match empirical quantiles without inflating intervals.
- Core assumption: The latent embeddings from the backbone contain sufficient information for expressive heads to decode accurate conditional distributions.
- Evidence anchors:
  - [section 4.3] "The Gaussian distribution's calibration results are significantly worse than the other heads. In particular, the Gaussian heads are consistently under-confident with CCE scores... always lower than the other heads."
  - [section 4.3] "Quantile, Student's t, and mixture distribution heads have very similar calibration error across all datasets, indicating that there is no significant advantage for any of the three over the others."
  - [corpus] No direct corpus evidence on prediction head comparison for TSFMs; mechanism inferred from paper's controlled experiments only.
- Break condition: If data is truly Gaussian-distributed, the expressiveness advantage should disappear; if backbone embeddings are poorly conditioned, all heads may fail similarly.

### Mechanism 3
- Claim: Trajectory-based autoregressive forecasting preserves calibration better than branching or point-based AR by propagating full distributional information.
- Mechanism: Point-based AR (adding mean/median to context) discards uncertainty each iteration, causing overconfident drift. Branching AR maintains |Q| parallel contexts but aggregates |Q|² forecasts back to |Q|, losing distributional nuance. Trajectory AR samples n independent paths, preserving the full predictive distribution through propagation; final quantiles are computed empirically from trajectory samples.
- Core assumption: The computational budget permits n ≫ |Q| trajectory samples, and the backbone generalizes well to partially synthetic contexts.
- Evidence anchors:
  - [section 4.4] "The trajectory AR approach has generally lower PCE than the branching method when comparing with the same forecast horizon."
  - [section 4.4] "All autoregressive TSFMs are consistently overconfident in long-term forecasting... This overconfidence reduces steeply as the horizon length increases."
  - [figure 5] PCE increases with shorter horizon lengths (16, 32 vs. 64, 128), with branching showing steeper degradation than trajectory.
  - [corpus] No corpus evidence on AR method comparison; mechanism is paper-specific.
- Break condition: If forecast horizon p matches desired length H (no AR needed), calibration differences between methods should vanish; if n is too small, trajectory AR may exhibit high variance.

## Foundational Learning

- Concept: Probabilistic calibration vs. sharpness
  - Why needed here: The paper distinguishes PCE (pure calibration) from WQL/CRPS/MSIS (calibration + sharpness). Confusing these leads to incorrect conclusions—for example, WQL ranked ARIMA as best-calibrated on Glucose data when PCE showed it was poorly calibrated.
  - Quick check question: If a model predicts 90% confidence intervals that always contain the true value, is it well-calibrated? (Not necessarily—the intervals might be uselessly wide; you need to check both calibration and sharpness.)

- Concept: Quantile regression and quantile loss (pinball loss)
  - Why needed here: All evaluated TSFMs use quantile prediction heads trained with quantile loss. Understanding how the asymmetric penalty (2q for underestimation vs. 2(1-q) for overestimation) shapes conditional distributions is essential for interpreting calibration results.
  - Quick check question: For the 0.9 quantile, which error is penalized more heavily: predicting too low or too high? (Predicting too low—underestimating the 90th percentile incurs 2×0.9 = 1.8 penalty vs. 2×0.1 = 0.2 for overestimation.)

- Concept: Autoregressive forecast accumulation error
  - Why needed here: Long-term forecasting beyond the model's native horizon requires iterative prediction, where errors (both point and distributional) compound. The paper shows AR method choice directly impacts calibration degradation.
  - Quick check question: Why does adding point forecasts (mean/median) to the context for AR degrade calibration faster than sampling from the predicted distribution? (Point forecasts discard uncertainty, forcing subsequent predictions to condition on artificially certain history; sampling preserves distributional spread.)

## Architecture Onboarding

- Component map:
  - Input time series -> Backbone transformer/xLSTM -> Latent embeddings -> Prediction head (quantile/distribution) -> Quantile forecasts or distribution parameters -> AR module (if H > p) -> Final forecasts

- Critical path:
  1. Tokenize input time series into patches (length varies by model: 16 for Moirai 2.0, 32 for TiRex, 64 for Chronos-Bolt).
  2. Pass patches through backbone to obtain latent embeddings.
  3. Apply prediction head to output quantile forecasts or distribution parameters.
  4. If H > p, invoke AR module to extend forecasts iteratively.
  5. Evaluate calibration using PCE (primary), CCE (direction), SIW (sharpness).

- Design tradeoffs:
  - **Quantile vs. distribution head**: Quantile heads are model-agnostic and provide direct calibration targets; distribution heads (Student's t, mixture) offer interpretability and tail control but may underperform if mis-specified (Gaussian).
  - **AR method**: Naive is fastest (1 forward pass per step) but worst for calibration; branching balances cost (|Q| passes) and quality; trajectory is most expensive (n passes, n ≫ |Q|) but best calibrated.
  - **Forecast horizon p**: Longer p reduces AR iterations, improving calibration but increasing model complexity and training cost.

- Failure signatures:
  - **High PCE with low SIW**: Model is overconfident; check if prediction head is mis-specified or AR method is naive/branching with short p.
  - **High PCE with high SIW**: Model is under-confident; check if prediction head is Gaussian (under-expressive) or training distribution mismatch.
  - **PCE increases sharply with prediction step**: AR accumulation error; increase horizon p or switch to trajectory AR.
  - **WQL and MASE highly correlated but PCE differs**: WQL is conflating accuracy and calibration; rely on PCE for calibration assessment.

- First 3 experiments:
  1. **Baseline calibration sweep**: Run all five TSFMs (Chronos-Bolt, Moirai 2.0, TimesFM, TiRex, YingLong) on your target dataset with default quantile heads; compute PCE, CCE, SIW at H = 64. Identify if any model is systematically over-/under-confident.
  2. **Prediction head ablation**: For the best-performing backbone from experiment 1, train Gaussian, Student's t, and mixture heads on a domain-similar pretraining corpus (e.g., TSMixup); evaluate on held-out test data. Expect Gaussian to show higher PCE and more negative CCE (under-confidence).
  3. **AR method comparison for long-term forecasting**: Set H = 256 with p = 64; compare naive, branching (|Q| = 9), and trajectory (n = 100) AR on PCE and CCE. Expect trajectory < branching < naive in PCE; all AR methods should show positive CCE (overconfidence) that decreases as p increases.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does fine-tuning affect the calibration of time series foundation models compared to their zero-shot performance?
- Basis in paper: [explicit] The Conclusion states that "worthwhile extensions of our work could be to investigate calibration in the context of fine-tuning," as the study only evaluated zero-shot capabilities.
- Why unresolved: The authors restricted their evaluation to zero-shot univariate forecasting, leaving the impact of parameter updates on uncertainty quantification unknown.
- What evidence would resolve it: A comparative study measuring PCE and CCE metrics on downstream tasks before and after fine-tuning the foundation models on domain-specific training data.

### Open Question 2
- Question: How does distribution shift and non-stationarity impact the calibration performance of TSFMs and baselines?
- Basis in paper: [explicit] The Conclusion identifies "how distribution shift and non-stationarity may affect the calibration performance" as an important practical direction for future investigation.
- Why unresolved: While deep learning calibration is known to be sensitive to dataset shift, this specific behavior has not been quantified for time series foundation models.
- What evidence would resolve it: Experiments evaluating calibration metrics on synthetic or real-world datasets specifically designed to include temporal covariate shift or concept drift.

### Open Question 3
- Question: Do the favorable calibration properties of univariate TSFMs generalize to multivariate time series forecasting?
- Basis in paper: [explicit] The authors list extending evaluations "to extend [to] multivariate data" as a specific limitation and future extension of their work.
- Why unresolved: The current study is limited to univariate time series, and it is unclear if cross-variate dependencies degrade the probabilistic calibration observed in univariate settings.
- What evidence would resolve it: Applying the proposed calibration metrics (PCE, CCE) to multivariate datasets using architectures capable of modeling cross-variate dependencies.

### Open Question 4
- Question: Are time series foundation models well-calibrated at extreme quantiles (e.g., 0.01, 0.99) required for anomaly detection?
- Basis in paper: [explicit] The Appendix notes that "Promising future work should evaluate tailed calibration with quantiles further on the tail than 0.1 or 0.9 such as 0.001 or 0.999."
- Why unresolved: The main experiments focused on the 0.1 to 0.9 range; performance at the tails is critical for specific downstream tasks like anomaly detection but remains unverified.
- What evidence would resolve it: Calculating Tailed Probabilistic Calibration Error (TPCE) on predictions for quantiles significantly closer to 0 and 1.

## Limitations
- The study only evaluates zero-shot capabilities, leaving fine-tuning effects on calibration unexplored.
- Results are limited to univariate forecasting; generalization to multivariate settings remains unknown.
- Extreme quantile calibration (e.g., 0.01, 0.99) for anomaly detection tasks is not assessed.

## Confidence

- **High confidence**: Calibration-aware training objectives (quantile loss) improve calibration relative to other deep learning models. This is directly supported by controlled experiments and clear mechanism.
- **Medium confidence**: More expressive prediction heads (Student's t, mixture) provide calibration advantages over Gaussian heads. Evidence is strong from ablation experiments but may depend on specific data characteristics.
- **Medium confidence**: Trajectory autoregressive forecasting preserves calibration better than branching or point-based methods. Results are consistent across datasets, but the computational cost-benefit tradeoff is not fully explored.

## Next Checks
1. Test calibration benefits of quantile loss in a non-time-series domain (e.g., image classification) to determine if the mechanism generalizes beyond TSFMs.
2. Systematically vary n (trajectory samples) and |Q| (quantile set size) to quantify the calibration-accuracy-cost Pareto frontier for long-term forecasting.
3. Evaluate calibration behavior when H matches native forecast horizon p (no AR needed) to isolate backbone effects from AR accumulation error.