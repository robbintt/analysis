---
ver: rpa2
title: 'MMPlanner: Zero-Shot Multimodal Procedural Planning with Chain-of-Thought
  Object State Reasoning'
arxiv_id: '2509.21662'
source_url: https://arxiv.org/abs/2509.21662
tags:
- step
- steps
- image
- visual
- mmplanner
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MMPlanner is a zero-shot multimodal procedural planning framework
  that generates step-by-step textual and visual instructions while maintaining object-state
  consistency. It uses Object State Reasoning Chain-of-Thought (OSR-CoT) prompting
  to explicitly model object transitions across steps, enabling accurate generation
  of both explicit and implicit state changes in visual plans.
---

# MMPlanner: Zero-Shot Multimodal Procedural Planning with Chain-of-Thought Object State Reasoning
## Quick Facts
- arXiv ID: 2509.21662
- Source URL: https://arxiv.org/abs/2509.21662
- Authors: Afrina Tabassum; Bin Guo; Xiyao Ma; Hoda Eldardiry; Ismini Lourentzou
- Reference count: 40
- Key outcome: MMPlanner achieves state-of-the-art performance on RECIPEPLAN and WIKIPLAN, improving textual planning by +6.8%, cross-modal alignment by +11.9%, and visual step ordering by +26.7% compared to baselines.

## Executive Summary
MMPlanner is a zero-shot multimodal procedural planning framework that generates step-by-step textual and visual instructions while maintaining object-state consistency. It uses Object State Reasoning Chain-of-Thought (OSR-CoT) prompting to explicitly model object transitions across steps, enabling accurate generation of both explicit and implicit state changes in visual plans. The framework introduces LLM-based evaluators for planning accuracy (T-PlanScore) and cross-modal alignment (CA-Score), along with a visual step reordering task to measure temporal coherence. Experiments demonstrate MMPlanner's effectiveness in producing coherent multimodal plans with strong cross-modal alignment.

## Method Summary
MMPlanner follows a three-stage pipeline: (1) LLaVa-1.5-7B generates textual plans from multimodal goals (text + SD-generated visual goal), (2) GPT-3.5 with OSR-CoT prompting generates image descriptions capturing explicit/implicit state changes, and (3) Stable Diffusion generates K candidates per step, with BLIP-2 selecting the best via cosine similarity. The framework uses LLM-based evaluators (T-PlanScore for planning accuracy, CA-Score for cross-modal alignment) and introduces a visual step reordering task to measure temporal coherence. The OSR-CoT prompting explicitly models object state transitions through structured reasoning about current step descriptions, state changes, and synthesis.

## Key Results
- Achieves state-of-the-art performance on RECIPEPLAN and WIKIPLAN datasets
- Improves textual planning accuracy by +6.8% over baselines
- Enhances cross-modal alignment by +11.9%
- Increases visual step ordering performance by +26.7%

## Why This Works (Mechanism)

### Mechanism 1: Object State Reasoning Chain-of-Thought (OSR-CoT)
Structured prompting that decomposes step reasoning into description, state-change analysis, and synthesis improves visual plan coherence by surfacing implicit state transitions. OSR-CoT prompts the LLM in three stages: describe the current step using prior context, explicitly enumerate object states before/after the step, and synthesize a grounded image description. This reduces hallucinations and captures both explicit and implicit changes (e.g., "butter added to mixture" where mixture is inferred).

### Mechanism 2: Cross-Modal Step Image Selection via BLIP-2 Feature Alignment
Sampling multiple visual candidates and selecting via cross-modal similarity improves visual-textual alignment by filtering stochastic generation failures. Generate K candidate images per step using Stable Diffusion; extract BLIP-2 embeddings for each image and the step description; select the image with maximum cosine similarity. BLIP-2's Q-Former bridges vision-language gap for fine-grained alignment.

### Mechanism 3: LLM-as-Judge Evaluation via Structured Reasoning Prompts
VLM-based evaluators with chain-of-thought outperform embedding similarity metrics for assessing cross-modal alignment and planning accuracy in procedural contexts. CA-Score prompts VLM to describe the image, then score alignment with the step text; T-PlanScore prompts LLM to assess goal-steps coherence. Structured prompts enforce reasoning about actions and object states, not just surface semantics.

## Foundational Learning

- Concept: Chain-of-Thought Reasoning
  - Why needed here: OSR-CoT is a specialized CoT variant; understanding general CoT principles (decomposition, intermediate steps) is prerequisite to grasping how OSR-CoT structures state reasoning
  - Quick check question: Given a cooking step "fold in egg whites," what intermediate reasoning steps would help predict the visual state?

- Concept: Multimodal Representations & Alignment
  - Why needed here: The framework relies on BLIP-2 features for cross-modal selection and VLMs for evaluation; understanding vision-language alignment mechanisms is essential
  - Quick check question: Why might CLIPScore underperform CA-Score for detecting implicit state changes like "mixture texture"?

- Concept: Procedural Planning & State Transitions
  - Why needed here: MPP requires understanding sequential dependencies and cumulative state changes; distinguishing explicit vs. implicit transitions is central to the OSR-CoT design
  - Quick check question: In "chop onions → sauté → add to soup," which state transitions are explicit vs. implicit in the final step's text?

## Architecture Onboarding

- Component map: Goal input → Goal image generation (SD) → Textual planning (VLM) → Per-step: OSR-CoT description (LLM) → K image candidates (SD) → BLIP-2 selection → Final multimodal plan. Evaluation runs post-hoc.

- Critical path: Goal input → Goal image generation (SD) → Textual planning (VLM) → Per-step: OSR-CoT description (LLM) → K image candidates (SD) → BLIP-2 feature extraction → cosine similarity selection → Final multimodal plan. Evaluation uses GPT-3.5 (T-PlanScore) and MiniGPT-4 (CA-Score).

- Design tradeoffs: K=20 yields best alignment but increases compute ~20× per step vs. K=1; practical deployments may use K=10-15. OSR-CoT adds ~46% inference overhead vs. direct prompting but reduces dual-prompting (TIP approach) by ~46%. LLM-based evaluators are faster than human annotation (~0.7s vs. 5min for T-PlanScore) but inherit LLM biases.

- Failure signatures: Hallucinated state details before OSR-CoT (e.g., irrelevant flour descriptions in step 5). Inconsistent peripheral elements across steps (e.g., bowl shape changes) due to lack of explicit consistency enforcement. T-PlanScore calibration issues: low-quality plans occasionally score high if surface coherence appears maintained.

- First 3 experiments:
  1. Ablate OSR-CoT components: Remove description, state-reasoning, and one-shot example individually; measure impact on CA-Score and VS-Ordering to validate each component's contribution
  2. Vary K and feature extractors: Test K∈{1,5,10,15,20} with BLIP-2 vs. CLIP vs. no selection; identify compute-quality frontier
  3. Cross-dataset robustness: Evaluate on held-out domains (e.g., DIY repairs vs. recipes) to test generalization of state reasoning; analyze where implicit transitions fail

## Open Questions the Paper Calls Out
None

## Limitations
- Performance depends critically on OSR-CoT quality for implicit state transitions, which may fail on complex or domain-specific object states not well-represented in LLM pretraining
- Cross-modal selection assumes BLIP-2 features reliably capture procedural semantics, but alignment may degrade for novel object compositions or actions outside training distribution
- LLM-based evaluators inherit potential biases from pretraining and may not fully capture human judgment nuances for procedural correctness

## Confidence

- High Confidence: The architectural design of MMPlanner (three-stage pipeline with OSR-CoT, cross-modal selection, and LLM evaluation) is clearly specified and the core mechanisms are well-documented with ablation results showing component contributions
- Medium Confidence: The reported performance improvements (+6.8% T-PlanScore, +11.9% CA-Score, +26.7% VS-Ordering) are based on controlled experiments, but the exact dataset access and implementation details (SD version, temperature settings) introduce reproducibility uncertainties
- Low Confidence: The framework's generalization to domains beyond cooking and WikiHow procedures (e.g., technical repairs, medical procedures) remains untested, and the evaluation metrics' correlation with true user experience in practical applications is not established

## Next Checks

1. **Dataset and Implementation Verification**: Obtain RECIPEPLAN and WIKIPLAN datasets directly from authors, confirm Stable Diffusion version and hyperparameters used, and reproduce baseline results with provided prompts before evaluating MMPlanner performance

2. **Cross-Domain Robustness Testing**: Evaluate MMPlanner on held-out procedural domains (e.g., furniture assembly, basic medical first aid) to assess generalization of OSR-CoT reasoning for implicit state transitions beyond cooking contexts

3. **Human Evaluation Correlation Study**: Conduct a small-scale human evaluation comparing T-PlanScore and CA-Score outputs against expert ratings on procedural accuracy and visual-textual alignment, particularly for steps requiring implicit state reasoning