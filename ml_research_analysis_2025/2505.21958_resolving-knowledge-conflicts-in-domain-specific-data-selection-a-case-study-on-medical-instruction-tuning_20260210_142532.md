---
ver: rpa2
title: 'Resolving Knowledge Conflicts in Domain-specific Data Selection: A Case Study
  on Medical Instruction-tuning'
arxiv_id: '2505.21958'
source_url: https://arxiv.org/abs/2505.21958
tags:
- data
- knowledge
- llms
- medical
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the problem of knowledge conflicts in domain-specific
  instruction-tuning, where discrepancies between a language model''s pretrained knowledge
  and the context knowledge of instruction data can lead to poor performance and hallucination.
  The authors propose a Knowledge-aware Data Selection (KDS) framework that quantifies
  these conflicts using two metrics: context-memory knowledge alignment and intra-memory
  knowledge consistency.'
---

# Resolving Knowledge Conflicts in Domain-specific Data Selection: A Case Study on Medical Instruction-tuning

## Quick Facts
- **arXiv ID**: 2505.21958
- **Source URL**: https://arxiv.org/abs/2505.21958
- **Reference count**: 40
- **Primary result**: KDS achieves up to +2.56% average performance gains and reduces hallucination by up to +9.86% in medical instruction-tuning

## Executive Summary
This paper addresses the critical problem of knowledge conflicts in domain-specific instruction-tuning, where discrepancies between a language model's pretrained knowledge and new instruction data can degrade performance and increase hallucination. The authors propose Knowledge-aware Data Selection (KDS), a framework that quantifies these conflicts using two metrics: context-memory knowledge alignment and intra-memory knowledge consistency. KDS filters training data based on these metrics while maintaining quality and diversity, resulting in significant performance improvements across medical benchmarks and better multilingual generalization.

## Method Summary
KDS operates by first generating multiple responses from a base LLM for each training sample, then calculating two conflict scores: KA (Knowledge Alignment) measures the consistency between model responses and ground truth using NLI, while KC (Knowledge Consistency) measures the semantic variance among the model's own responses. The framework applies quality and diversity filters after conflict scoring, selecting top-k samples that are both aligned with the model's knowledge base and sufficiently diverse. The selected data is then used for fine-tuning with LoRA, resulting in more effective domain adaptation with reduced hallucination risk.

## Key Results
- Achieves up to +2.56% average performance gains across medical benchmarks (MedMCQA, MedQA, MMLU-Medical)
- Reduces hallucination by up to +9.86% compared to full SFT and random selection methods
- Demonstrates superior data efficiency, outperforming random selection using 1/5 the data
- Shows consistent effectiveness across different model sizes (LLaMA3, Qwen2.5) and multilingual scenarios

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Filtering data that contradicts a model's parametric knowledge (Context-Memory conflict) appears to preserve prior capabilities and reduce hallucination risks during fine-tuning.
- **Mechanism**: The KDS framework uses a **Context-Memory Knowledge Alignment (KA)** metric. It generates multiple responses from the base LLM for a given query and uses an NLI model to check if these responses entail the ground-truth answer. Samples where the model's internal knowledge consistently contradicts the reference answer are filtered out, preventing the model from being forced to unlearn stable pretrained facts.
- **Core assumption**: The paper assumes that instruction-tuning primarily aligns existing knowledge rather than effectively injecting complex new knowledge, and that forcing alignment on contradictory data damages model reliability.
- **Evidence anchors**:
  - [abstract] "discrepancy between LLMs' pretrained knowledge and context knowledge... could damage LLMs' prior abilities and lead to hallucination."
  - [Section III-B] Describes the use of NLI to calculate `Score_KA` based on entailment between model responses and answers.
  - [corpus] The neighbor paper *KaFT* (arXiv:2505.15480) supports the premise that vanilla SFT performance drops due to conflicts between internal and context knowledge.
- **Break condition**: This mechanism may fail if the target domain knowledge is strictly newer than the pretraining cutoff and contains zero overlap with parametric memory, requiring the model to learn rather than align.

### Mechanism 2
- **Claim**: Estimating semantic uncertainty via response consistency (Intra-Memory conflict) helps identify data samples that are confusing or outside the model's current reliable knowledge base.
- **Mechanism**: The **Intra-Memory Knowledge Consistency (KC)** metric samples multiple responses for a single query and clusters them by semantic similarity (using NLI). High entropy (divergent responses) indicates the model is "unfamiliar" or conflicted regarding that specific knowledge point. Filtering high-entropy samples removes unstable training signals.
- **Core assumption**: Divergent generations from a base model are a reliable proxy for internal knowledge gaps or conflicts.
- **Evidence anchors**:
  - [abstract] Mentions "intra-memory knowledge consistency" as a core quantification metric.
  - [Section III-B] Formulates `Score_KC` using cluster-based entropy to measure uncertainty.
  - [corpus] *KCR* (arXiv:2508.01273) discusses resolving inter-context conflicts via reasoning, reinforcing that LLMs struggle with conflicting contexts, which KDS attempts to pre-emptively filter out.
- **Break condition**: This metric relies on the quality of the sampling process; low temperature or too few samples might fail to capture the true variance of the model's knowledge state.

### Mechanism 3
- **Claim**: Combining knowledge conflict filtering with standard quality and diversity filters yields better data efficiency than filtering for quality alone.
- **Mechanism**: KDS applies a **Quality Filter** (using the LLM to self-rate data) and a **Diversity Filter** (using BGE-m3 embeddings) *after* scoring for knowledge conflicts. This ensures that while the data is "safe" (aligned with memory), it is also high-quality and non-redundant, preventing the model from learning from low-quality aligned data or overfitting to repetitive aligned samples.
- **Core assumption**: The base LLM possesses sufficient in-context learning capability to judge the quality of medical instruction data reliably.
- **Evidence anchors**:
  - [abstract] Explicitly states KDS "employs a quality filter and a diversity filter to ensure high-quality and diverse training data."
  - [Section III-B] "Filtering and Sampling" details the use of LLM self-rating (0-5 scale) and BGE-m3 embeddings.
  - [corpus] *Beyond Similarity* (arXiv:2502.11062) suggests gradient-based graph methods for data selection, providing context that similarity/diversity alone is a standard but evolving baseline.
- **Break condition**: If the diversity threshold ($\lambda$) is set too high, it may filter out semantically similar but crucial domain-specific concepts (e.g., slight variations of a specific medical procedure), leading to data starvation.

## Foundational Learning

- **Concept: Knowledge Conflict in LLMs**
  - **Why needed here**: This paper defines its solution entirely around "Context-Memory" (data vs. model) and "Intra-Memory" (model vs. itself) conflicts. You cannot interpret the KA and KC metrics without understanding that the authors view fine-tuning as a potentially destructive process if it clashes with pretrained weights.
  - **Quick check question**: If a model was pretrained in 2023 and your medical data includes 2024 guidelines, would KDS likely rank this data as high or low priority based on the "Context-Memory" mechanism? (Answer: Likely low, as it contradicts pre-2024 parametric memory).

- **Concept: Natural Language Inference (NLI)**
  - **Why needed here**: The authors offload the judgment of "truth" or "contradiction" not to the LLM itself directly, but to an external NLI model (DeBERTa). Understanding NLI entailment (Entailment vs. Contradiction) is required to debug why specific data points are filtered.
  - **Quick check question**: In the KA metric, the NLI model compares two texts: the "Reference Answer" and the "_____"? (Answer: "LLM's Response").

- **Concept: Catastrophic Forgetting & Hallucination**
  - **Why needed here**: The paper positions KDS as a remedy to the degradation of prior abilities. The mechanism relies on the hypothesis that forcing a model to fine-tune on contradictory data causes it to lose previously stable knowledge (forgetting) or fabricate facts (hallucination).
  - **Quick check question**: According to the paper's motivation, why is "Full-SFT" (training on all data) often sub-optimal in domain-specific scenarios? (Answer: It forces the model to learn conflicting or low-quality data, damaging prior abilities).

## Architecture Onboarding

- **Component map**:
  Base LLM -> NLI Engine -> Embedding Model -> Selector Logic -> Fine-tuning Pipeline

- **Critical path**:
  1. **Inference Phase**: For every sample in your raw dataset, generate $m$ responses (default=10) using Base LLM (temp=0.7).
  2. **Scoring Phase**: Calculate `Score_KA` (alignment with ground truth via NLI) and `Score_KC` (consistency of the 10 responses via NLI clustering).
  3. **Filtering Phase**:
      - Filter low-quality data (LLM self-score < 3).
      - Filter repetitive data (Cosine similarity > 0.9).
  4. **Selection Phase**: Select top-$k$ sorted by combined score.

- **Design tradeoffs**:
  - **NLI Model Size**: The paper (Fig 5a) shows `large` NLI models correlate with better discrimination (+2.63 gap) but are slower than `xsmall`.
  - **Diversity Threshold ($\lambda$)**: High diversity (0.95) drops performance; the paper recommends 0.9 to balance coverage and redundancy.
  - **Inference Cost**: Generating 10 responses per query is compute-heavy. The paper argues this is still cheaper than training quality scorers (like DEITA) or using GPT-4 (Alpagasus).

- **Failure signatures**:
  - **Performance Plateau**: If selected data is too small (<1k) or strictly "easy" (high alignment), the model may not learn new domain nuances.
  - **Hallucination Spikes**: If `Score_KA` is ignored and only quality/diversity is used, the model may output confident but wrong medical facts (Table V shows Random selection drops hallucination scores by -16%).

- **First 3 experiments**:
  1. **Metric Isolation**: Run KDS using *only* KA vs. *only* KC on a validation set to see which conflict type is more prevalent in your specific medical dataset.
  2. **NLI Scaling**: Validate the paper's claim by swapping DeBERTa `xsmall` for `large` on a 500-sample subset to ensure the performance gain justifies the inference latency.
  3. **Ablation on Budget**: Verify the "Data Efficiency" claim by training with KDS-selected 1k data vs. Random 5k data. The paper claims KDS@1k should outperform Random@5k.

## Open Questions the Paper Calls Out
- **Question**: Does fine-tuning the Natural Language Inference (NLI) model on domain-specific corpora significantly improve the accuracy of knowledge conflict detection compared to general-purpose NLI models?
- **Basis in paper**: [Explicit] The authors state in Section V-D: "incorporating more domain-specific knowledge into the NLI models is more effective and has the potential to further boost the effectiveness of our KDS, which is in our future work."
- **Why unresolved**: The current implementation relies on a general DeBERTa-v3 model trained on MNLI, which may lack the nuance to detect subtle contradictions in specialized fields without domain adaptation.
- **What evidence would resolve it**: A comparative analysis of KDS performance using a general NLI vs. a domain-specific NLI (e.g., trained on MedNLI) on the medical hallucination benchmarks.

## Limitations
- The NLI-based conflict detection may not generalize well to all medical sub-domains or languages
- The assumption that instruction-tuning primarily aligns rather than injects knowledge may not hold for entirely novel concepts
- The computational cost of generating multiple responses per sample could be prohibitive for very large datasets

## Confidence
- **High confidence**: Effectiveness of KDS for medical instruction-tuning given significant performance gains (+2.56% average) and hallucination reduction (+9.86%) across multiple models and benchmarks
- **Medium confidence**: Generalizability to other domains beyond medical, as evaluation is domain-specific
- **Medium confidence**: Mechanism explanations, as paper provides theoretical justification but limited ablation studies isolating KA vs KC effects

## Next Checks
1. Test KDS on a non-medical domain (e.g., legal or financial) to validate cross-domain applicability
2. Perform ablation studies comparing KA-only, KC-only, and combined filtering to quantify individual contributions
3. Evaluate the sensitivity of KDS performance to different NLI model sizes and medical domain-specific NLI variants