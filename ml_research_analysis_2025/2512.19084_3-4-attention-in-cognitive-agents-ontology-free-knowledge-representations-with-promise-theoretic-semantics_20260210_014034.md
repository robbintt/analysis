---
ver: rpa2
title: "$\u03B3(3,4)$ `Attention' in Cognitive Agents: Ontology-Free Knowledge Representations\
  \ With Promise Theoretic Semantics"
arxiv_id: '2512.19084'
source_url: https://arxiv.org/abs/2512.19084
tags:
- data
- which
- agents
- graph
- promise
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper establishes a unified framework for modeling attention
  in cognitive agents using Promise Theory, bridging vectorized machine learning and
  knowledge graph representations without relying on implicit language models. The
  core method introduces a technology-agnostic description of attention mechanisms,
  treating data as autonomous agents that voluntarily forego autonomy to interact.
---

# $γ(3,4)$ `Attention' in Cognitive Agents: Ontology-Free Knowledge Representations With Promise Theoretic Semantics

## Quick Facts
- arXiv ID: 2512.19084
- Source URL: https://arxiv.org/abs/2512.19084
- Reference count: 40
- Primary result: Unified Promise Theory framework for attention in cognitive agents that bridges vectorized ML and knowledge graphs without implicit language models

## Executive Summary
This paper introduces a unified framework for modeling attention in cognitive agents using Promise Theory, establishing that attention mechanisms can be understood as autonomous agent bindings without centralized control. The approach treats data as agents that voluntarily forego autonomy to interact through promise-theoretic semantics, enabling both vectorized learning and knowledge graph representations to coexist meaningfully. The framework is particularly suited for autonomous robotics, defense deployments, and emergency services, with potential for orders-of-magnitude data compression through explicit attention to causal boundary conditions.

## Method Summary
The method establishes Promise Theory as a technology-agnostic generalization of transformer attention, decomposing each graph link into dual promises (offer from sender, acceptance from receiver) bounded by information overlap. Using γ(3,4) Semantic Spacetime graphs, the approach replaces complex ontologies with four causal role classifications that preserve source intentionality even under data fractionation. The framework introduces convex update rules for realtime agents as an alternative to softmax normalization, enabling efficient attention mechanisms that respect boundary conditions and causal structure.

## Key Results
- Promise-theoretic agent bindings provide technology-agnostic generalization of transformer attention
- γ(3,4) Semantic Spacetime graphs enable ontology-free classification by causal roles
- Causal boundary conditions can compress data requirements for context determination by orders of magnitude

## Why This Works (Mechanism)

### Mechanism 1
Promise-theoretic agent bindings generalize transformer attention through dual-promise structure where each graph link requires both an offer (+) from sender and acceptance (−) from receiver, with information transmission bounded by the overlap of their belief spaces. This formulation explicitly exposes assumptions about reliable channels and synchronized timing that matrix-based attention methods implicitly assume.

### Mechanism 2
γ(3,4) Semantic Spacetime graphs replace logical type hierarchies with four causal role classifications corresponding to spatial, temporal, and causal relationships. This preserves intentionality while allowing coarse-grained aggregation without rigid ontologies, enabling more flexible and semantically meaningful representations.

### Mechanism 3
Causal boundary conditions compress data requirements by encoding known periodic or structural constraints directly into the representation rather than learning them statistically. Time parameterization that respects these boundaries reduces the search space for pattern matching by orders of magnitude compared to statistical learning approaches.

## Foundational Learning

- Concept: Promise Theory fundamentals (autonomous agents, promises, assessment functions)
  - Why needed here: The entire framework reinterprets attention, graphs, and learning in promise-theoretic terms; without this, the notation is opaque.
  - Quick check question: Can you explain why two promises are required to form a single directed graph link?

- Concept: Transformer attention as key-query-value matching with feature vectors
  - Why needed here: The paper positions its contribution as a generalization of transformer attention; understanding the baseline is prerequisite.
  - Quick check question: What role does softmax play in standard self-attention, and what alternative does the paper propose?

- Concept: Knowledge graphs vs. vector representations—intentionality vs. probability
  - Why needed here: The central thesis is that these representations serve different purposes and should hybridize rather than replace each other.
  - Quick check question: Why does the paper claim graphs preserve intentionality while vectors do not?

## Architecture Onboarding

- Component map: A_S (Source Agent) → A_R (Curator Agent) → A_Q/A_K/A_V (projection) → A_T (Transformer Agent) → A_U (User Agent)

- Critical path: Data flows from source through curation and projection into query/key/value spaces, where matching occurs before returning values to the user. Assessment functions at each handoff determine fidelity.

- Design tradeoffs:
  - Batch (LLM) vs. realtime (cognitive agent): Batch gives statistical stability; realtime gives responsiveness but requires different normalization strategies
  - Vector vs. graph storage: Vectors enable fast similarity search; graphs preserve causal structure and explicit relationships
  - Centralized curation vs. autonomous emergence: Curated graphs are simpler but brittle; emergent graphs scale but require assessment coordination

- Failure signatures:
  - Assessment collapse: α_A(π) becomes unreliable—agent cannot distinguish kept from broken promises
  - Feature misalignment: Spanning set φ_d does not match query semantics, causing spurious Q∩K matches
  - Context drift: χ changes faster than the graph can update, invalidating stored associations
  - Normalization deadlock: Softmax-style global normalization becomes bottleneck for realtime updates

- First 3 experiments:
  1. Implement minimal promise binding: Create two agents with explicit offer/accept promises, measure information transmission against b_S ∩ b_R overlap. Vary α to observe degradation.
  2. Compare γ(3,4) vs. ontology classification: On a small domain (e.g., system logs), build both representations and query for causal relationships. Measure precision/recall and construction effort.
  3. Hybrid retrieval test: Store intentional traces in a graph, ambient features as vectors. Query using both mechanisms and compare against vector-only baseline for tasks requiring causal explanation.

## Open Questions the Paper Calls Out

### Open Question 1
How can "intent" be formally operationalized within query languages to recover causal information lost during vectorized data processing? The paper states, "Intent is the key—and it is this problem which is yet to be explored in detail, perhaps using graphs within the query language too." Current methods rely on brute force or spurious statistical correlations rather than semantic alignment between user's conceptual basis and model's eigenvector basis.

### Open Question 2
What mechanisms allow for the robust mapping between user-defined spanning sets and the eigenvector bases derived from machine learning training data? The author identifies "Finding a plausible mapping between these spanning sets is the central challenge of attention modelling." Current methods often rely on brute force or spurious statistical correlations rather than semantic alignment.

### Open Question 3
Can the proposed "inverted minimization" embedding serve as a computationally efficient alternative to softmax normalization in realtime agents without sacrificing decision accuracy? The paper proposes an alternative to softmax (Eq. 11) to avoid expensive normalization, noting "adjusting the argument of the exponential to find a satisfactory alternative could be assessed." The alternative is formulated theoretically but is not benchmarked against the standard softmax approach.

### Open Question 4
How can causal boundary conditions be explicitly encoded to preserve "known" information during the learning optimization process? The text asks, "in those cases where we do know something in advance, how can we preserve that information rather than trying to guess it unnecessarily?" Standard batch learning methods fractionate data, washing away specific contexts and causal intent, forcing the model to relearn known constraints.

## Limitations
- γ(3,4) semantic spacetime classification lacks explicit definitions in the paper, relying on external references
- Assessment functions α_A(π) are described conceptually but not formally specified, making quantitative predictions difficult
- Claims about order-of-magnitude data compression through causal boundary conditions are supported only by a single traffic pattern example

## Confidence

- **High Confidence:** The core premise that graphs preserve intentionality while vectors enable efficient computation is well-established in the literature and logically sound within the paper's framework.
- **Medium Confidence:** The promise-theoretic reformulation of attention is internally consistent and mechanistically clear, but lacks empirical validation against transformer baselines.
- **Low Confidence:** Claims about order-of-magnitude data compression through causal boundary conditions are supported only by a single traffic pattern example without broader demonstration.

## Next Checks

1. **Formal Definition Verification:** Obtain and verify the exact four classes of γ(3,4) classification to enable proper implementation and testing.

2. **Empirical Comparison:** Implement a controlled experiment comparing promise-theoretic attention against standard transformer attention on a common benchmark, measuring both accuracy and computational overhead.

3. **Boundary Condition Test:** Design a synthetic dataset with known periodic structure, apply both explicit boundary encoding and learned statistical methods, and measure the claimed compression ratio in data requirements.