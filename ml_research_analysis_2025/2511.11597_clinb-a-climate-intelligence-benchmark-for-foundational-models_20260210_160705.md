---
ver: rpa2
title: 'CLINB: A Climate Intelligence Benchmark for Foundational Models'
arxiv_id: '2511.11597'
source_url: https://arxiv.org/abs/2511.11597
tags:
- answer
- climate
- answers
- question
- clinb
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: CLINB introduces a benchmark for assessing how foundational models
  handle climate change information, focusing on open-ended, grounded, multimodal
  question answering tasks. It features a dataset of real user questions paired with
  expert-curated rubrics, evaluated through a model-based autorater.
---

# CLINB: A Climate Intelligence Benchmark for Foundational Models

## Quick Facts
- **arXiv ID:** 2511.11597
- **Source URL:** https://arxiv.org/abs/2511.11597
- **Reference count:** 40
- **Primary result:** Frontier models exhibit PhD-level knowledge synthesis but struggle with grounding, showing high hallucination rates in references (10-25%) and images (50-80%).

## Executive Summary
CLINB introduces a benchmark for assessing how foundational models handle climate change information through open-ended, grounded, multimodal question answering tasks. The benchmark features real user questions paired with expert-curated rubrics, evaluated through a model-based autorater. It reveals that frontier models can achieve PhD-level synthesis and presentation but struggle significantly with grounding, exhibiting substantial hallucination rates for references and images. The study highlights the need for better evidential support and adaptive rubrics to improve trust in AI for scientific workflows.

## Method Summary
CLINB uses a three-phase human-in-the-loop process: Phase 1 generates candidate answers (hybrid human-AI, autonomous LLM, and merged synthesis), Phase 2 collects pairwise human preferences to build a preference graph, and Phase 3 generates question-specific rubrics reviewed by climate scientists. A model-based autorater (Gemini 2.5 Pro) evaluates answers using these rubrics and evidence validation. The evaluation measures knowledge synthesis, presentation, citations, and images through pairwise battles using the Bradley-Terry model to calculate ELO scores. The benchmark includes 168 real user questions from chatclimate.ai, categorized by topic, difficulty, and IPCC working group.

## Key Results
- Frontier models achieve PhD-level knowledge synthesis and presentation capabilities
- High hallucination rates persist: 10-25% for references and 50-80% for images across models
- Highly motivated non-specialists with deep AI tool engagement can produce higher-quality answers than domain experts who engage less
- Autonomous frontier models outperform hybrid answers curated by experts using weaker AI assistance
- Structured rubrics and automated evidence-checking are essential for mitigating LLM judge biases

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Question-specific rubrics improve alignment between model-based evaluation and expert human judgment compared to generic evaluation prompts.
- Mechanism: Structured rubrics derived from expert pairwise preferences and answer analysis provide explicit grading criteria that constrain the judge model's attention to scientifically relevant dimensions, reducing "vibe-based" assessments.
- Core assumption: The rubrics encode transferable evaluation criteria that generalize beyond the specific answers used to generate them.
- Evidence anchors: Structured prompts and automated evidence-checking are essential for mitigating inherent LLM judge biases.
- Break condition: Rubrics may fail to generalize when evaluating model outputs that differ substantially from those used during rubric creation.

### Mechanism 2
- Claim: Explicit evidence verification surfaces grounding failures that would otherwise be hidden by high-quality synthesis.
- Mechanism: The autorater systematically validates reference and image URLs, classifying status as valid, inaccessible, or invalid. Invalid evidence triggers penalties in completeness and accuracy scoring.
- Core assumption: Invalid URLs indicate hallucinated claims rather than merely broken links to legitimate sources.
- Evidence anchors: Substantial hallucination rates for references (10% to 25%) and images (50% to 80%).
- Break condition: Evidence verification cannot assess claims supported by paywalled or inaccessible content (up to 50% of sources).

### Mechanism 3
- Claim: Highly motivated non-specialists with deep AI tool engagement can produce higher-quality answers than domain experts who engage less.
- Mechanism: The "Advocates" group showed greater interaction with the AI-assisted curation tool—more annotations, evidence searches, and manual evidence additions. Their superior engagement compensated for lower domain expertise.
- Core assumption: Tool engagement metrics causally relate to answer quality rather than reflecting confounding factors.
- Evidence anchors: Evidence from the Editor use shows that Advocates engaged much more with all aspects of curation.
- Break condition: This effect may not generalize to tasks requiring deeper technical expertise or to less intuitive AI interfaces.

## Foundational Learning

- **Concept: Long-Form Question Answering (LFQA)**
  - Why needed here: CLINB targets open-ended, generative responses requiring research, synthesis, and evidence integration—fundamentally different from multiple-choice benchmarks.
  - Quick check question: Can you explain why ensuring faithfulness and reliable attribution are core challenges in LFQA?

- **Concept: LLM-as-a-Judge Paradigm**
  - Why needed here: The autorater uses a frontier model (Gemini 2.5 Pro) with structured prompts to evaluate answers, a technique with known biases that CLINB attempts to mitigate.
  - Quick check question: What are three documented biases in LLM-as-a-Judge evaluation, and how might structured rubrics address them?

- **Concept: Retrieval-Augmented Grounding**
  - Why needed here: The benchmark explicitly requires evidence-supported claims with valid URLs, creating tension between knowledge synthesis capabilities and verifiable attribution.
  - Quick check question: Why might a model with strong synthesis capabilities still fail at producing valid citations?

## Architecture Onboarding

- **Component map:**
  - Question Dataset -> Candidate Answer Generator -> Preference Graph Builder -> Rubric Generator -> Autorater Pipeline -> Evidence Validator

- **Critical path:** Question selection → Candidate answer curation (Phase 1) → Pairwise preference collection (Phase 2) → Rubric generation and scientist review (Phase 3) → Autorater evaluation with evidence checking → ELO scoring

- **Design tradeoffs:**
  - Rubric specificity vs. generalization: Question-specific rubrics improve accuracy on known answer types but may not generalize to novel model outputs
  - Human expertise depth vs. scalability: Scientists provide highest-quality validation but cannot scale; Experts and Advocates provide coverage with varying quality
  - Evidence strictness vs. coverage: Penalizing inaccessible URLs may unfairly downgrade legitimate paywalled sources

- **Failure signatures:**
  - High hallucination rate in images (50-80%): Models struggle with multimodal grounding
  - Citation hallucination (10-25%): Particularly severe in OpenAI o3
  - Familiarity bias in human evaluators: Experts prefer formats they've worked with extensively
  - Rubric gaming: Models may score well on rubric criteria while failing on unmeasured dimensions

- **First 3 experiments:**
  1. Run the autorater with and without question-specific rubrics on a held-out question set to measure rubric contribution to human-alignment
  2. Implement evidence validation on a sample of model outputs to quantify hallucination rates across reference types
  3. Compare Advocate vs. Expert answer quality on questions from their respective expertise domains to isolate engagement vs. knowledge effects

## Open Questions the Paper Calls Out

- **Open Question 1:** Does integrating web search capabilities into frontier models significantly reduce the high hallucination rates for references and images observed in strictly parametric settings?
  - Basis in paper: The Conclusion states "Our next steps include evaluating search-enabled systems," directly addressing the current limitation where models were evaluated "without search enabled."
  - Why unresolved: The study explicitly isolated parametric knowledge, finding substantial hallucination rates (10–25% for references; 50–80% for images), but did not test if grounding via search mitigates these attribution failures.
  - What evidence would resolve it: A comparative experiment rerunning the CLINB benchmark on search-augmented versions of the same frontier models and measuring the change in invalid URL rates.

- **Open Question 2:** How can evaluation rubrics be designed to dynamically adapt to new model capabilities without losing consistency or requiring extensive manual re-curation?
  - Basis in paper: The Conclusion lists "making rubrics adaptive" as a necessary next step; Section 3.2 notes that static rubrics "are far from complete" and failed to penalize ungrounded information in new model outputs as effectively as human oversight.
  - Why unresolved: Ablation studies showed that removing rubrics shifted focus to mechanistic verifiability, and current rubrics derived from weaker models did not fully generalize to the nuances of stronger frontier models.
  - What evidence would resolve it: Development of an algorithmic rubric-update pipeline that automatically refines grading criteria based on new model responses, validated against human scientist preferences.

- **Open Question 3:** What interface designs facilitate genuine synergistic collaboration between domain experts and AI, allowing the combined output to surpass the performance of autonomous frontier models?
  - Basis in paper: The Conclusion identifies "the design of interfaces that enable expert-AI collaboration" as a central challenge to achieving performance that "exceeds what the model can do alone."
  - Why unresolved: The study found that "autonomous frontier models surpass 'hybrid' answers," indicating that current human-in-the-loop methods act as a bottleneck rather than a boost to quality.
  - What evidence would resolve it: User studies comparing different collaborative UIs where experts use frontier models to complete tasks, measuring if the final output scores higher than the model's autonomous attempt.

## Limitations

- Question-specific rubrics generated through the human-in-the-loop process contain domain-specific nuances that may not generalize to novel model outputs, limiting benchmark scalability
- Evidence validation relies on HTTP status codes that cannot distinguish between legitimate paywalled sources and hallucinated URLs, potentially penalizing valid citations
- The engagement quality metrics for human curators are self-reported and may not capture actual cognitive effort or answer quality improvements

## Confidence

- **High confidence:** The quantification of hallucination rates (10-25% for references, 50-80% for images) across multiple frontier models is empirically grounded and reproducible
- **Medium confidence:** The finding that non-specialist Advocates produce higher-quality answers than domain Experts due to tool engagement requires further validation across different task types and interfaces
- **Medium confidence:** The superiority of structured rubrics over generic evaluation prompts is demonstrated but may not generalize beyond the specific answer pairs used for rubric generation

## Next Checks

1. Replicate the autorater evaluation using held-out questions to test rubric generalization and measure performance degradation when evaluating answers substantially different from training examples
2. Implement a more sophisticated evidence validation system that can distinguish paywalled academic sources from hallucinated URLs using content analysis rather than pure HTTP status checking
3. Conduct a controlled study varying human-AI interaction patterns while holding expertise constant to isolate the causal effect of tool engagement on answer quality across multiple task domains