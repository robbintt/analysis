---
ver: rpa2
title: 'NEURO-GUARD: Neuro-Symbolic Generalization and Unbiased Adaptive Routing for
  Diagnostics -- Explainable Medical AI'
arxiv_id: '2512.18177'
source_url: https://arxiv.org/abs/2512.18177
tags:
- neuro-guard
- medical
- knowledge
- clinical
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: NEURO-GUARD is a neuro-symbolic medical AI framework that combines
  Vision Transformers with knowledge-guided reasoning to improve interpretability
  and diagnostic accuracy in medical imaging. The method uses a retrieval-augmented
  generation mechanism to dynamically extract clinical knowledge, then employs a large
  language model to generate executable code for feature extraction, which is iteratively
  refined via reinforcement learning.
---

# NEURO-GUARD: Neuro-Symbolic Generalization and Unbiased Adaptive Routing for Diagnostics -- Explainable Medical AI

## Quick Facts
- **arXiv ID:** 2512.18177
- **Source URL:** https://arxiv.org/abs/2512.18177
- **Reference count:** 26
- **Primary result:** 84.69% accuracy on diabetic retinopathy classification, outperforming ViT baselines by 6.2%

## Executive Summary
NEURO-GUARD is a neuro-symbolic medical AI framework that combines Vision Transformers with knowledge-guided reasoning to improve interpretability and diagnostic accuracy in medical imaging. The method uses a retrieval-augmented generation mechanism to dynamically extract clinical knowledge, then employs a large language model to generate executable code for feature extraction, which is iteratively refined via reinforcement learning. Experiments on diabetic retinopathy datasets show NEURO-GUARD achieves 84.69% accuracy, outperforming ViT baselines by 6.2% and demonstrating 5% improvement in domain generalization. Additional tests on MRI-based seizure detection confirm its cross-domain robustness. The framework provides clinically interpretable, verifiable diagnoses by grounding visual feature learning in structured medical knowledge, addressing key limitations of black-box models in high-stakes clinical settings.

## Method Summary
NEURO-GUARD employs a retrieval-augmented generation pipeline where clinical guidelines are retrieved from biomedical literature, then used to generate executable Python code for feature extraction via a three-prompt LLM sequence. This code, typically using OpenCV or YOLOv11, extracts diagnostically relevant features from medical images. A self-verification loop with entropy-based rewards and reinforcement learning iteratively refines the code until feature extraction quality converges. The extracted features are classified by a knowledge-driven model (Gradient Boosting), while the raw images are classified by a ViT-based deep learning model. Final diagnoses are selected based on confidence-based fusion of both predictions, achieving 84.69% accuracy on diabetic retinopathy datasets with 5% improved domain generalization.

## Key Results
- 84.69% classification accuracy on APTOS diabetic retinopathy dataset
- 6.2% improvement over ViT baselines
- 5% improvement in domain generalization (APTOS→EyePACS)
- 10.85% accuracy gap between supervised (84.69%) and unsupervised (73.84%) modes

## Why This Works (Mechanism)

### Mechanism 1: RAG-Grounded Code Generation for Feature Extraction
- Claim: Retrieving clinical guidelines and generating executable code produces more reliable feature extraction than direct VLM predictions.
- Mechanism: A three-prompt pipeline retrieves domain knowledge → structures rules → generates Python code (e.g., OpenCV/YOLO) that deterministically extracts features from images, grounding predictions in verifiable operations rather than stochastic model outputs.
- Core assumption: Clinical literature encodes feature definitions that translate to robust image processing operations.
- Evidence anchors: [abstract] "retrieval-augmented generation (RAG) mechanism... LLM iteratively generates, evaluates, and refines feature-extraction code"; [section 3] "Prompt 2, the LLM utilizes this rule base to generate executable Python code for feature detection"

### Mechanism 2: Entropic Self-Verification via RL Prompt Tuning
- Claim: Iterative refinement driven by entropy reduction improves feature extractor reliability and reduces hallucinations.
- Mechanism: Extracted features are validated against IoU metrics; entropy is computed over extraction correctness. If entropy exceeds threshold τ, the LLM receives feedback and regenerates code via RL-guided prompt tuning until convergence.
- Core assumption: Entropy on validation images correlates with clinically meaningful feature extraction quality.
- Evidence anchors: [abstract] "self-verification loop, optimized via reinforcement learning, compels the model to iteratively check and refine its outputs"; [section 3.2] "entropic gain Ei quantifies the reduction in uncertainty... c_i^(t+1) = LLM(E_i, c_i^(t))"

### Mechanism 3: Confidence-Based Decision Fusion
- Claim: Combining knowledge-driven and deep learning predictions based on confidence scores improves accuracy over either modality alone.
- Mechanism: A knowledge-driven classifier (f_KD) processes LLM-extracted features; a ViT-based model (f_DL) processes raw images. Final diagnosis selects the higher-confidence prediction.
- Core assumption: The two classifiers make uncorrelated errors and provide complementary signal.
- Evidence anchors: [abstract] "84.69% accuracy, outperforming ViT baselines by 6.2%"; [section 3.3] "y_final = y_DL if s_DL ≥ s_KD, else y_KD"

## Foundational Learning

- **Retrieval-Augmented Generation (RAG)**
  - Why needed here: The framework depends on dynamically querying external biomedical sources to inject up-to-date clinical knowledge into the reasoning pipeline.
  - Quick check question: Can you explain how RAG differs from standard LLM prompting with static context?

- **Q-Learning Fundamentals**
  - Why needed here: The self-verification loop uses tabular Q-learning with defined states, actions, and rewards to optimize extraction parameters.
  - Quick check question: What is the role of the discount factor γ in Q-learning updates?

- **Vision Transformer (ViT) Representations**
  - Why needed here: The deep learning branch uses ViT-based encoders; understanding patch embeddings and self-attention is critical for debugging feature extraction.
  - Quick check question: How does a ViT process an image differently from a CNN?

## Architecture Onboarding

- **Component map:**
  Input image → RAG retrieval → Code generation → Feature extraction → IoU validation → (if entropy > τ) RL refinement loop → Final features → Dual classification → Fusion → Diagnosis

- **Critical path:**
  Input image → RAG retrieval → Code generation → Feature extraction → IoU validation → (if entropy > τ) RL refinement loop → Final features → Dual classification → Fusion → Diagnosis

- **Design tradeoffs:**
  - Supervised vs. unsupervised artifact detection: Supervised (84.69% accuracy) requires human-annotated bounding boxes; unsupervised (73.84%) relies on heuristics but scales better.
  - Code generation vs. direct VLM prediction: Code is verifiable but introduces LLM hallucination risk in logic.
  - Multi-stage prompting improves interpretability at computational cost.

- **Failure signatures:**
  - High entropy after many refinement iterations → Retrieved rules may be misaligned with image domain
  - Low IoU on specific lesion types → Generated code may misparameterize thresholds
  - Fusion repeatedly selecting one classifier → Possible miscalibration of confidence scores

- **First 3 experiments:**
  1. Run zero-shot VLM baselines (CLIP, MedCLIP, Grounding-DINO) on APTOS to reproduce hallucination patterns documented in Table 3.
  2. Implement the 3-prompt RAG pipeline with a small rule base; verify generated code executes and produces non-zero IoU on 50 annotated images.
  3. Compare supervised vs. unsupervised RL modes on a held-out split, logging entropy convergence curves and final accuracy.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the computational overhead of the multi-stage RAG prompting and RL-based self-verification loop be reduced to enable real-time clinical deployment?
- Basis in paper: [explicit] Section 9 states "the multi-stage prompting and verification loop introduces computational overhead, limiting real-time deployment."
- Why unresolved: The paper proposes on-device optimization as future work but provides no inference time benchmarks or concrete latency-reduction strategies.
- What evidence would resolve it: Inference time benchmarks showing acceptable latency (<2 seconds per diagnosis) through architectural modifications or caching strategies.

### Open Question 2
- Question: What mechanisms can systematically eliminate residual logical inconsistencies in LLM-generated feature extraction code beyond entropic self-verification?
- Basis in paper: [explicit] Section 9 acknowledges "although entropic self-verification reduces hallucinations, LLM-generated code may still exhibit logical inconsistencies in rare cases."
- Why unresolved: Entropic rewards quantify uncertainty reduction but cannot verify semantic correctness of generated code logic.
- What evidence would resolve it: Failure mode analysis quantifying residual code error rates, plus experiments with formal verification or human-in-the-loop code validation.

### Open Question 3
- Question: How can NEURO-GUARD maintain high diagnostic accuracy in low-resource settings where clinician-validated ground truth annotations are unavailable?
- Basis in paper: [explicit] Section 6.1 notes "real-world deployments may face challenges in low-resource settings where access to clinician-validated ground truth is limited."
- Why unresolved: A 10.85% accuracy gap exists between supervised (84.69%) and unsupervised (73.84%) modes, with no proposed bridging mechanism.
- What evidence would resolve it: Experiments demonstrating self-supervised or weakly-supervised alternatives achieving comparable accuracy to supervised artifact detection.

### Open Question 4
- Question: How can the framework ensure clinical knowledge currency when RAG-based retrieval may propagate outdated medical guidelines?
- Basis in paper: [explicit] Section 9 states RAG retrieval "can propagate outdated or incomplete clinical knowledge into the rule base, influencing code generation quality."
- Why unresolved: No mechanism is provided for detecting or filtering stale knowledge despite rapidly evolving clinical standards.
- What evidence would resolve it: Temporal evaluation tracking accuracy against evolving guidelines, plus experiments with knowledge versioning or timestamp-based retrieval filtering.

## Limitations
- Computational overhead from multi-stage RAG prompting and RL self-verification limits real-time deployment
- 10.85% accuracy gap between supervised and unsupervised modes creates challenges for low-resource settings
- RAG-based retrieval may propagate outdated clinical knowledge without versioning mechanisms

## Confidence

- **High confidence:** The architectural framework combining RAG, code generation, and dual-classifier fusion is technically coherent and addresses real XAI limitations in medical imaging.
- **Medium confidence:** The reported 84.69% accuracy and 5% generalization gain are plausible given the mechanism, but depend on unverified implementation details and dataset splits.
- **Low confidence:** The entropy-based RL refinement's effectiveness and the specific contribution of each neuro-symbolic component to the overall performance gain.

## Next Checks
1. **Code Generation Verification:** Implement the 3-prompt RAG pipeline with a small, verified rule base; test generated code on 50 annotated images to confirm IoU > 0.7 and functional correctness.
2. **Entropy Convergence Validation:** Log entropy trajectories during RL refinement on a held-out set; verify that high-entropy samples correspond to clinically ambiguous cases and that refinement reduces uncertainty meaningfully.
3. **Cross-Dataset Robustness:** Reproduce the APTOS→EyePACS domain shift experiment; measure accuracy degradation and confirm the 5% generalization improvement holds across multiple random seeds.