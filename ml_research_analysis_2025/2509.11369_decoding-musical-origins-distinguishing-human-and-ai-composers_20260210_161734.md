---
ver: rpa2
title: 'Decoding Musical Origins: Distinguishing Human and AI Composers'
arxiv_id: '2509.11369'
source_url: https://arxiv.org/abs/2509.11369
tags:
- music
- ynote
- generated
- algorithm
- songs
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study developed a machine learning framework for classifying\
  \ musical compositions by their origin\u2014human, algorithm-generated, or LLM-generated\u2014\
  using a novel music notation system called YNote. YNote represents each note with\
  \ a fixed four-character string, enabling text-based analysis."
---

# Decoding Musical Origins: Distinguishing Human and AI Composers

## Quick Facts
- arXiv ID: 2509.11369
- Source URL: https://arxiv.org/abs/2509.11369
- Reference count: 11
- Primary result: 98.25% accuracy classifying human, algorithm-generated, and LLM-generated music using YNote encoding

## Executive Summary
This study introduces a machine learning framework that can distinguish between human-composed music and AI-generated compositions with 98.25% accuracy. The approach uses YNote, a novel music notation system that converts each note into a fixed four-character string, enabling text-based analysis. By applying TF-IDF feature extraction on YNote sequences and training a logistic regression classifier with SMOTE for class balancing, the authors demonstrate that musical compositions carry distinct "technological fingerprints" that reveal their origin. The framework successfully identifies unique patterns in algorithm-generated music versus LLM-generated music, suggesting that AI composition methods leave identifiable stylistic traces.

## Method Summary
The researchers developed a three-class classification system using YNote, a symbolic music encoding where each note is represented by a fixed four-character string (two characters for pitch, two for duration). They applied TF-IDF vectorization with n-gram ranges of 1-3 and 8000 features to convert YNote sequences into numerical features. To address severe class imbalance (669:18,894:1,835 ratio), they applied SMOTE oversampling to minority classes and used class weighting during training. A logistic regression classifier with one-vs-rest strategy was trained on a 65/15/20 stratified split of the dataset. The framework achieved 98.25% accuracy with strong ROC-AUC scores and demonstrated interpretability through coefficient analysis revealing discriminative n-grams for each class.

## Key Results
- Overall classification accuracy of 98.25% with 5-fold cross-validation showing 0.9857 ± 0.0013
- Macro-average ROC-AUC score of 0.9934, indicating excellent discriminative performance
- Per-class precision and recall ranging from 0.77 to 0.99, with algorithm-generated music showing near-perfect F1=0.99
- Identified "technological fingerprints" through TF-IDF coefficient analysis, revealing distinct patterns between human, algorithm, and LLM compositions

## Why This Works (Mechanism)

### Mechanism 1
- Fixed-length symbolic encoding preserves sufficient stylistic information for source classification when combined with n-gram analysis.
- YNote's 4-character representation converts musical sequences into tokenizable "text," enabling NLP techniques to capture local melodic patterns that differ systematically across human, rule-based, and LLM composition styles.
- The simplification does not discard the discriminative features (e.g., rest patterns, interval sequences) that distinguish sources.
- Evidence: YNote paper confirms format was designed for LLM fine-tuning; rest-related n-grams show large positive coefficients for Native and large negative for Algorithm.

### Mechanism 2
- TF-IDF on n-grams captures "technological fingerprints" by identifying note sequences with high discriminative value across source categories.
- TF-IDF weights n-grams by their frequency within a piece and rarity across the corpus, amplifying patterns unique to each class while suppressing common musical vocabulary.
- Local 1–3 note sequences contain most of the discriminative signal; longer-term structure is less critical.
- Evidence: Algorithm-generated music shows "very distinct, consistent, and stable patterns" with near-perfect F1=0.99.

### Mechanism 3
- SMOTE + class weighting mitigates severe class imbalance (669:18,894:1,835), enabling minority class learning without sacrificing majority accuracy.
- SMOTE synthesizes minority samples via interpolation in feature space; class_weight='balanced' increases loss penalty for minority misclassification.
- Synthetic samples represent plausible feature-space variations of minority class music.
- Evidence: Native recall (0.77) is lowest but precision (0.95) is high, suggesting balancing enabled learning rather than majority-dominated guessing.

## Foundational Learning

- Concept: TF-IDF (Term Frequency–Inverse Document Frequency)
  - Why needed here: Converts YNote sequences into sparse feature vectors; understanding TF vs. IDF weighting is essential for interpreting why certain n-grams become discriminative.
  - Quick check question: Given a 3-gram appearing in 98% of songs vs. one appearing in 5%, which will have higher IDF weight and why does that matter for classification?

- Concept: SMOTE (Synthetic Minority Over-sampling Technique)
  - Why needed here: Addresses 28:1 imbalance ratio between largest and smallest classes; engineer must understand how interpolation-based oversampling differs from random duplication.
  - Quick check question: If minority class samples are clustered far apart in feature space, what risk does linear interpolation introduce?

- Concept: Multi-class Logistic Regression with One-vs-Rest
  - Why needed here: The classifier uses OvR strategy; interpreting per-class coefficients requires understanding how each binary classifier separates one class from all others.
  - Quick check question: For a 3-class problem, how many binary classifiers are trained, and what does a positive coefficient on an n-gram indicate for a specific class?

## Architecture Onboarding

- Component map: YNote string -> 4-character tokenization -> TF-IDF sparse matrix -> SMOTE (train only) -> Logistic Regression -> 3-class probability output

- Critical path: YNote string → 4-char tokenization → TF-IDF sparse matrix → SMOTE (train only) → Logistic Regression → 3-class probability output

- Design tradeoffs:
  - Simplicity vs. expressiveness: TF-IDF captures only local patterns; LSTM/Transformer could model longer dependencies but at higher complexity
  - Interpretability vs. accuracy: Logistic regression provides coefficient interpretability but may underfit complex patterns
  - Format specificity: YNote discards dynamics/timbre; model may not transfer to MIDI or audio

- Failure signatures:
  - Low Native recall with high precision: Model conservative on minority class; may need more diverse Native training data
  - High Algorithm accuracy but low LLM accuracy: Suggests LLM patterns overlap with Algorithm; may need additional features
  - ROC-AUC drops on new genres: Overfitting to style-specific patterns; need cross-genre validation

- First 3 experiments:
  1. Ablation on n-gram range: Compare (1,3) vs. (1,5) vs. (3,5) to test whether longer sequences improve LLM vs. Algorithm discrimination.
  2. Cross-style validation: Train on Jiangnan-based Algorithm data, test on algorithm-generated music from different cultural styles to measure generalization.
  3. Feature importance audit: Extract top 50 n-grams per class; verify with domain expert whether they align with known stylistic markers (e.g., rest frequency, interval patterns).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do the identified "technological fingerprints" persist across diverse musical genres and cultural styles outside the Jiangnan style used for algorithm training?
- Basis: The Limitations section notes the "Algorithm Generated" dataset was primarily based on Jiangnan music rules, and generalization to other styles "needs further validation."
- Why unresolved: The model may have learned style-specific rules (Jiangnan) rather than universal "algorithmic" fingerprints.
- What evidence would resolve it: Evaluating the model on algorithm-generated music from distinct genres (e.g., Jazz, Western Classical) to see if accuracy holds without retraining.

### Open Question 2
- Question: Can the classification framework maintain high accuracy when distinguishing human works from music generated by modern, high-parameter models?
- Basis: The Limitations section highlights that LLM data came from GPT-2 level models, and newer models produce "more subtle" fingerprints.
- Why unresolved: State-of-the-art models generate music with deeper structural coherence than GPT-2, potentially closing the gap between human and AI features.
- What evidence would resolve it: Testing the classifier on datasets generated by current state-of-the-art music LLMs to measure any drop in precision or recall.

### Open Question 3
- Question: Does combining YNote symbolic analysis with audio spectral features improve the detection robustness of AI-generated content?
- Basis: The Future Outlook proposes exploring "cross-modal integrated research" that processes both YNote data and audio features.
- Why unresolved: YNote explicitly discards non-symbolic nuances like timbre and dynamics; audio analysis could capture artifacts in these missing dimensions.
- What evidence would resolve it: Developing a multi-modal model and comparing its performance against the text-only TF-IDF baseline.

## Limitations

- The framework relies on YNote's simplified encoding, which discards musical nuances like dynamics and timbre that may be relevant for classification.
- Performance is demonstrated primarily on Jiangnan-style compositions, with limited evidence of generalizability to other musical genres or cultural traditions.
- The severe class imbalance required aggressive SMOTE oversampling, which may have created synthetic samples that don't fully represent the true distribution of human-composed music.

## Confidence

- **High Confidence**: Overall classification accuracy (98.25%) and ROC-AUC scores (0.9934 macro-average) are well-supported by experimental results.
- **Medium Confidence**: Claim that YNote preserves "sufficient stylistic information" for source classification is supported but depends on specific musical style and encoding choices.
- **Low Confidence**: Generalization to other musical genres, more sophisticated generative models, or different cultural traditions remains unproven.

## Next Checks

1. Cross-genre validation: Apply the trained classifier to algorithm-generated music from Western classical, jazz, or other cultural traditions to assess generalization beyond Jiangnan style.

2. Advanced model robustness: Test classification performance against newer LLM architectures (e.g., transformer-based models beyond GPT-2) and more sophisticated algorithmic composition systems.

3. Feature ablation study: Systematically remove TF-IDF components (e.g., test with raw frequency counts or different n-gram ranges) to isolate which aspects of feature extraction most contribute to classification accuracy and interpretability.