---
ver: rpa2
title: 'Freeze, Diffuse, Decode: Geometry-Aware Adaptation of Pretrained Transformer
  Embeddings for Antimicrobial Peptide Design'
arxiv_id: '2511.23120'
source_url: https://arxiv.org/abs/2511.23120
tags:
- embeddings
- pretrained
- diffusion
- https
- transformer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of adapting pretrained transformer
  embeddings for downstream molecular tasks, particularly when supervised data is
  scarce. Current methods like fine-tuning distort pretrained geometric structure
  while probing lacks expressivity.
---

# Freeze, Diffuse, Decode: Geometry-Aware Adaptation of Pretrained Transformer Embeddings for Antimicrobial Peptide Design

## Quick Facts
- arXiv ID: 2511.23120
- Source URL: https://arxiv.org/abs/2511.23120
- Reference count: 40
- Primary result: Achieves AUC of 0.965 for AMP classification and 0.90 peptide retrieval accuracy

## Executive Summary
This paper introduces Freeze, Diffuse, Decode (FDD), a framework for adapting pretrained transformer embeddings to downstream molecular tasks when labeled data is scarce. Current adaptation methods like fine-tuning distort pretrained geometric structure while probing lacks expressivity. FDD addresses this by propagating task-specific supervision along the intrinsic manifold of frozen embeddings, preserving geometric structure while enabling effective downstream prediction. Applied to antimicrobial peptide design, FDD outperforms state-of-the-art methods in both binary classification and peptide retrieval tasks while enabling smooth latent-space interpolation between AMP and non-AMP peptides.

## Method Summary
FDD operates by first freezing pretrained transformer embeddings and constructing a diffusion geometry on the embedding manifold. Task-specific supervision is then propagated through this geometric structure via a diffusion process that respects the intrinsic manifold topology. Finally, a decoder maps the diffused representations back to prediction space. This three-stage approach preserves the pretrained geometric structure while adapting to downstream tasks, avoiding the catastrophic forgetting issues of fine-tuning and the expressivity limitations of probing methods.

## Key Results
- Achieves AUC of 0.965 for antimicrobial peptide binary classification
- Peptide retrieval accuracy of 0.90, outperforming baseline methods
- Enables smooth latent-space interpolation between AMP and non-AMP peptides with biologically meaningful transitions in physicochemical descriptors

## Why This Works (Mechanism)
FDD leverages the intrinsic geometric structure of pretrained embeddings to propagate supervision signals in a way that preserves the manifold's topological properties. By freezing the embeddings and using diffusion geometry, the method avoids the catastrophic forgetting that occurs with fine-tuning while maintaining the rich representations learned during pretraining. The diffusion process naturally handles the scarcity of labeled data by exploiting the manifold's local structure to propagate information between nearby points.

## Foundational Learning
- Diffusion geometry on manifolds: Essential for capturing intrinsic structure; quick check: verify that the diffusion distance respects known semantic similarities in embedding space
- Manifold learning: Required for understanding how data points relate in high-dimensional spaces; quick check: ensure the manifold assumption holds for your molecular embeddings
- Graph neural networks: Useful for implementing the diffusion process; quick check: validate that message passing respects the manifold structure
- Representation learning: Core to understanding how pretrained embeddings encode molecular information; quick check: analyze embedding quality using downstream task performance
- Manifold alignment: Important for connecting frozen embeddings to task-specific representations; quick check: verify alignment preserves semantic relationships

## Architecture Onboarding

Component map: Pretrained Transformer -> Freeze Embeddings -> Construct Diffusion Geometry -> Propagate Supervision -> Decode Predictions

Critical path: The diffusion process that propagates supervision along the manifold is the critical component. This involves constructing the diffusion operator, solving the diffusion equation, and mapping back to prediction space.

Design tradeoffs: The method trades off the potential for task-specific fine-tuning against preservation of pretrained geometry. The frozen embeddings ensure geometric structure preservation but may limit adaptation capacity. The diffusion process introduces hyperparameters (diffusion time, kernel bandwidth) that require careful tuning.

Failure signatures: Poor performance may indicate that the manifold assumption is violated, the diffusion process is not properly capturing local structure, or the decoder is not effectively mapping diffused representations to predictions. Common issues include vanishing/exploding gradients in the diffusion process or misalignment between manifold structure and task requirements.

First experiments:
1. Verify that freezing embeddings preserves classification performance on a small labeled dataset
2. Test different diffusion kernel bandwidths to find optimal local structure capture
3. Compare FDD's geometric preservation against fine-tuning using manifold visualization techniques

## Open Questions the Paper Calls Out
### Open Question 1
- Question: Can FDD be extended to multi-modal settings where molecular embeddings are jointly aligned with complementary data modalities such as cellular morphology or transcriptomics?
- Basis in paper: [explicit] The authors state in Future Work: "Future work will focus on extending FDD to multi-modal settings, where molecular embeddings are jointly aligned with complementary data modalities such as cellular morphology or transcriptomics."
- Why unresolved: The current FDD framework operates on single-modality frozen embeddings; integrating heterogeneous feature spaces with different structural priors requires new mechanisms for cross-modal diffusion geometry.
- What evidence would resolve it: A multi-modal FDD variant demonstrating unified molecular-phenotype representations with quantitative improvements on joint prediction tasks.

### Open Question 2
- Question: Do the latent-space interpolated peptides exhibit actual antimicrobial activity in wet-lab validation, beyond the observed smooth transitions in physicochemical descriptors?
- Basis in paper: [inferred] The paper validates interpolated peptides only through physicochemical descriptor trends (net charge, isoelectric point, hydrophobicity), not through experimental antimicrobial assays.
- Why unresolved: Computational smoothness in latent space does not guarantee biological functionality; decoded sequences may not fold or function as predicted.
- What evidence would resolve it: Minimum inhibitory concentration (MIC) assays showing that peptides sampled along the interpolation path have graded antimicrobial potency.

### Open Question 3
- Question: How does FDD performance scale across varying levels of supervised data scarcity, and at what sample size does the advantage over fine-tuning diminish?
- Basis in paper: [inferred] The paper claims FDD addresses issues "when supervised data are scarce," but does not systematically evaluate performance across different training set sizes.
- Why unresolved: Without controlled experiments across data regimes, it is unclear whether FDD's benefits persist when sufficient labeled data is available for fine-tuning.
- What evidence would resolve it: AUC curves for FDD vs. fine-tuning across training set sizes (e.g., 10, 50, 100, 500, 1000 samples) showing crossover points if any.

## Limitations
- Results are demonstrated primarily on antimicrobial peptide design, limiting generalizability to other molecular domains
- Biological relevance of learned representations relies on computational validation rather than experimental validation
- The framework's performance advantage over fine-tuning needs systematic evaluation across different levels of supervised data availability

## Confidence

High confidence in the technical implementation and mathematical framework of FDD
Medium confidence in the comparative performance claims within the AMP domain
Low confidence in the biological relevance of the learned representations without experimental validation

## Next Checks

1. Test FDD's performance on at least three additional molecular prediction tasks (e.g., protein-ligand binding, toxicity prediction, solubility) to assess generalizability
2. Conduct ablation studies comparing FDD against fine-tuning and probing on identical datasets to quantify relative performance gains
3. Generate and experimentally validate a subset of peptides from interpolated regions of the latent space to confirm biological relevance of the learned manifold structure