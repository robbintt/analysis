---
ver: rpa2
title: Operator Learning at Machine Precision
arxiv_id: '2511.19980'
source_url: https://arxiv.org/abs/2511.19980
tags:
- operator
- learning
- machine
- precision
- nonlinear
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "CHONKNORIS introduces a machine-precision neural operator learning\
  \ method by emulating the Newton\u2013Kantorovich method for solving PDEs. Instead\
  \ of directly approximating the solution operator, it learns the Cholesky factors\
  \ of the elliptic operator associated with regularized Newton updates."
---

# Operator Learning at Machine Precision

## Quick Facts
- arXiv ID: 2511.19980
- Source URL: https://arxiv.org/abs/2511.19980
- Reference count: 40
- Primary result: Introduces CHONKNORIS, achieving machine precision errors (~10⁻¹⁶) on nonlinear PDE problems by learning Cholesky factors of Newton-Kantorovich updates

## Executive Summary
CHONKNORIS introduces a machine-precision neural operator learning method by emulating the Newton-Kantorovich method for solving PDEs. Instead of directly approximating the solution operator, it learns the Cholesky factors of the elliptic operator associated with regularized Newton updates. This allows the method to achieve machine precision errors (e.g., ~10⁻¹⁶) across a variety of nonlinear forward and inverse PDE problems, including elliptic equations, Burgers' equation, Darcy flow, and inverse wave scattering. The approach embeds physical constraints into the iteration, providing both high accuracy and improved interpretability. A foundation model variant, FONKNORIS, generalizes to unseen PDEs by learning coefficient-to-Cholesky mappings, achieving near machine precision on problems like Klein-Gordon and Sine-Gordon equations. Theoretical guarantees are provided based on inexact Newton-Kantorovich analysis, ensuring convergence under standard assumptions.

## Method Summary
CHONKNORIS learns the Cholesky factors of the Tikhonov-regularized Newton update operator for nonlinear PDEs, effectively unrolling the Newton-Kantorovich iteration into a neural architecture. The method generates training data by running a warm-up Newton-Kantorovich solver on randomly sampled input coefficients, recording intermediate states and exact Cholesky factors. A surrogate model (Gaussian Process or MLP) is trained to predict these Cholesky factors from the current state and input. During inference, the method iteratively refines an initial guess using the predicted Cholesky factors to solve linear systems, achieving machine precision errors. FONKNORIS extends this by mapping physical coefficients directly to Cholesky factors, enabling generalization to unseen PDEs through a mixture-of-experts architecture.

## Key Results
- Achieves machine precision errors (~10⁻¹⁶) on 1D nonlinear elliptic equations in <15 iterations
- Generalizes to unseen PDEs (Klein-Gordon, Sine-Gordon) via FONKNORIS foundation model
- Successfully handles inverse problems including seismic imaging and Darcy flow
- Maintains quadratic convergence rates through adaptive Tikhonov regularization

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Replacing end-to-end solution operator approximation with the learning of linearized sub-problem operators circumvents the accuracy ceilings typical of standard neural operators.
- **Mechanism:** Instead of mapping input $u \to v$ directly, CHONKNORIS learns the Cholesky factors $R$ of the Tikhonov-regularized Newton update operator $Q = (J^*J + \lambda I)^{-1}$. By learning the *inverse Hessian* rather than the solution, the model solves a sequence of simpler linear problems, effectively unrolling a classical numerical solver into the network architecture.
- **Core assumption:** The Fréchet derivative $J$ (Jacobian) and its adjoint are known and accessible.
- **Evidence anchors:**
  - [abstract] "...regresses the Cholesky factors of the elliptic operator associated with Tikhonov-regularized Newton–Kantorovich updates."
  - [section 2.4] "We choose to approximate the Cholesky factors of Q to enforce positivity, stabilizes learning... and allows for efficient triangular solves."
  - [corpus] Weak direct evidence; neighbor papers focus on direct operator approximation (e.g., DeepONets [18212]), highlighting that the baseline approach struggles with precision.
- **Break condition:** If the surrogate model for the Cholesky factor lacks sufficient expressivity or training data, the update direction will be wrong, preventing convergence.

### Mechanism 2
- **Claim:** Machine precision is achieved by preserving the contraction mapping of the Newton-Kantorovich method within the neural architecture.
- **Mechanism:** The architecture iteratively refines a guess $v_n$ using the learned Cholesky factors. Theoretical analysis suggests that if the learned operator is sufficiently close to the exact operator, the "inexact" Newton update remains contractive. This allows the error to be driven down iteratively until it hits floating-point limits, unlike one-shot predictions.
- **Core assumption:** The design error $\epsilon_\lambda$ (difference between true and learned inverse Hessian) is small enough to satisfy the conditions of Theorem 4.2.
- **Evidence anchors:**
  - [abstract] "...machine-precision behavior follows from achieving a contractive map..."
  - [section 4.1] Theorem 4.2 guarantees convergence if the surrogate error is bounded and the Tikhonov parameter is managed correctly.
  - [corpus] [80827] discusses "Linearized Subspace Refinement," conceptually supporting the idea that refining linear approximations can boost accuracy where standard training fails.
- **Break condition:** If the Hessian becomes ill-conditioned (e.g., high-resolution seismic imaging), the learned Cholesky factors may become inaccurate, causing the iteration to stall or diverge [section 3.4.3].

### Mechanism 3
- **Claim:** A foundation model (FONKNORIS) can generalize to unseen PDEs by mapping physical coefficients to Cholesky factors rather than input functions to solutions.
- **Mechanism:** FONKNORIS decouples the solver from specific equations by parameterizing the input in terms of the coefficient functions $(a, b, c)$ of the Fréchet derivative (e.g., $a\partial_{xx} + b\partial_x + c$). It aggregates experts trained on diverse PDEs (elliptic, Burgers) using a mixture-of-experts (nested Kriging) to predict the Cholesky factors for new equations like Sine-Gordon.
- **Core assumption:** Unseen PDEs share structural similarities in their linearized operators (second-order local operators) with the training distribution.
- **Evidence anchors:**
  - [section 2.5] "FONKNORIS aims to learn the same Cholesky factors as a function of the coefficient functions a, b, and c."
  - [section 3.3] Demonstrates generalization to Klein-Gordon and Sine-Gordon equations without specific retraining.
  - [corpus] No direct corpus evidence for this specific "coefficient-to-Cholesky" transfer mechanism in neighbors.
- **Break condition:** Generalization fails if the test PDE requires coefficient combinations or non-local operators far outside the span of the expert training set.

## Foundational Learning

- **Concept: Newton-Kantorovich Iteration**
  - **Why needed here:** This is the algorithmic engine being unrolled. You must understand how linearizing a nonlinear PDE around a current guess allows for iterative refinement toward the true solution.
  - **Quick check question:** Can you explain why calculating the update $\delta v$ requires solving a linear system involving the Fréchet derivative?

- **Concept: Cholesky Factorization**
  - **Why needed here:** The method learns $R$ (Cholesky factor) instead of $Q$ (inverse Hessian) to implicitly enforce symmetry and positive-definiteness, which is crucial for stable convergence.
  - **Quick check question:** Why is representing a positive definite matrix by its Cholesky factor more stable than learning the matrix entries directly?

- **Concept: Tikhonov Regularization**
  - **Why needed here:** The paper uses $\lambda$ to regularize the Newton step. You need to grasp how varying $\lambda$ balances stability (avoiding ill-conditioning) against convergence speed (accuracy).
  - **Quick check question:** As the iteration $n$ increases and the error drops, should the regularization parameter $\lambda_n$ generally increase or decrease to achieve quadratic convergence?

## Architecture Onboarding

- **Component map:** Residual Computer -> Surrogate Model ($\hat{R}_\theta$) -> Triangular Solver -> Update Step
- **Critical path:**
  1. **Offline Data Gen:** Run exact Newton-Kantorovich solver to generate trajectories $(u, v_k)$ and exact Cholesky factors.
  2. **Training:** Fit $\hat{R}_\theta$ to map $(u, v_k, \lambda_k) \to R_k$.
  3. **Inference:** Initialize $v_0$; iteratively predict $R$, solve for update, and step until $\|F\| < \text{tol}$.

- **Design tradeoffs:**
  - **Accuracy vs. Speed:** More iterations reduce error but increase inference time. The paper shows Darcy flow may need 1000 iterations vs. 10 for simpler elliptic equations [section 3.2.4].
  - **Surrogate Choice:** Gaussian Processes (GPs) offer robustness and uncertainty quantification but scale poorly ($O(N^3)$); MLPs scale better but may require more data.
  - **Discretization:** The method is discretization-invariant in theory, but in practice, the surrogate model's input/output dimensions depend on the grid size.

- **Failure signatures:**
  - **Stagnation:** Error plateaus above machine precision. *Fix:* Check learning rate $\alpha$ or decrease regularization $\lambda$.
  - **Divergence:** Error grows. *Fix:* Increase $\lambda$ or check Jacobian $J$ implementation.
  - **Ill-conditioning:** High resolution (e.g., 14x14 seismic) leads to failure [section 3.4.3]. *Fix:* Use sparse Cholesky approximations or aggressive $\lambda$ schedules.

- **First 3 experiments:**
  1. **1D Nonlinear Elliptic PDE:** Implement the surrogate (GP recommended) for the simple equation (1). Verify convergence in <15 iterations [section 3.2.1].
  2. **Ablation on $\lambda$:** Fix the model and vary the Tikhonov parameter $\lambda$ to observe the shift from gradient-descent-like behavior (high $\lambda$) to Newton behavior (low $\lambda$).
  3. **Burgers' Equation (Time-Stepping):** Test the method on a time-dependent problem to ensure the surrogate handles the transport dynamics via the implicit time discretization [section 3.2.2].

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the identified connection between the Tikhonov-regularized inverse Hessian and the attention mechanism be formalized into an end-to-end differentiable architecture?
- Basis in paper: [explicit] The authors note the iteration leads to a connection with attention mechanisms and state, "We leave further investigation of this end-to-end approach to future work" (p. 5, p. 31).
- Why unresolved: While the analogy to transformers is mathematically drawn, the current CHONKNORIS implementation uses unrolled iterations with separate Cholesky prediction rather than a unified attention-based network.
- What evidence would resolve it: An architecture replacing explicit Cholesky factor prediction with attention layers that converges to machine precision on the benchmark PDEs.

### Open Question 2
- Question: How can CHONKNORIS be modified to efficiently handle high-dimensional problems where explicit storage and computation of Cholesky factors become infeasible?
- Basis in paper: [explicit] "Despite these advances, our method is still constrained by the computational cost and occasional ill-conditioning that arise when computing and storing Cholesky factors in high dimensions" (p. 20).
- Why unresolved: The method currently relies on dense matrix operations which scale quadratically, limiting application to fine meshes or 3D domains without sparse approximations.
- What evidence would resolve it: A modified algorithm utilizing sparse or low-rank Cholesky approximations that maintains machine precision on high-dimensional (e.g., 3D) inverse problems.

### Open Question 3
- Question: Can directly learning Newton-Kantorovich increments (rather than Cholesky factors) be stabilized to reduce computational overhead?
- Basis in paper: [explicit] "We also experimented with learning Newton-Kantorovich increments directly to reduce overhead; however, this has thus far yielded only limited gains" (p. 20).
- Why unresolved: Learning the update step directly is theoretically faster but proved unstable or inaccurate in preliminary tests compared to the factor-based approach.
- What evidence would resolve it: A surrogate model predicting the update vector $\delta v_n$ that maintains the convergence guarantees of Theorem 4.2 with reduced per-iteration cost.

## Limitations
- Scalability to high-dimensional problems remains unproven due to computational cost of Cholesky factorizations
- Theoretical convergence guarantees depend on surrogate error bounds that aren't practically quantified
- Generalization mechanism relies on structural similarity assumptions that may fail for non-local operators

## Confidence
- **High Confidence**: The core mechanism of learning Cholesky factors instead of direct solution operators (Mechanism 1) and the empirical demonstration of machine precision on benchmark problems (elliptic, Burgers, Darcy flow).
- **Medium Confidence**: The theoretical convergence analysis based on inexact Newton-Kantorovich theory, as it assumes the surrogate error is bounded but doesn't provide practical error bounds.
- **Medium Confidence**: The FONKNORIS generalization claims, as the results are limited to structurally similar PDEs (second-order local operators) and the transfer mechanism isn't extensively validated.

## Next Checks
1. **Scalability Test**: Reproduce the method on a 3D elliptic PDE (e.g., $128^3$ grid) to evaluate computational scaling and verify machine precision is maintained at higher resolutions.
2. **Adversarial PDE Generation**: Systematically test FONKNORIS on PDEs with non-local operators, variable-order derivatives, or extreme coefficient contrasts to probe the limits of the generalization mechanism.
3. **Surrogate Error Quantification**: Implement a rigorous error analysis pipeline that measures the Frobenius norm of $\hat{R}\hat{R}^T - Q$ across the training distribution and correlates this with the iteration error decay rates to validate the theoretical assumptions.