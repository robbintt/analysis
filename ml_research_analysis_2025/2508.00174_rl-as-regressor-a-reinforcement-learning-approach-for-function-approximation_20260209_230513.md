---
ver: rpa2
title: 'RL as Regressor: A Reinforcement Learning Approach for Function Approximation'
arxiv_id: '2508.00174'
source_url: https://arxiv.org/abs/2508.00174
tags:
- learning
- function
- regression
- reward
- agent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces an RL-based approach to function approximation,
  treating regression as a contextual bandit problem. The method frames predictions
  as actions and uses a custom reward signal (Gaussian kernel) to guide learning.
---

# RL as Regressor: A Reinforcement Learning Approach for Function Approximation

## Quick Facts
- arXiv ID: 2508.00174
- Source URL: https://arxiv.org/abs/2508.00174
- Reference count: 10
- Primary result: RL-based contextual bandit method frames regression as action selection, using custom reward functions (Gaussian kernel) to guide learning, enabling optimization of non-differentiable objectives.

## Executive Summary
This paper introduces an RL-based approach to function approximation by reframing regression as a contextual bandit problem. The method treats predictions as actions and uses a custom Gaussian kernel reward signal based on prediction error, enabling optimization of non-differentiable or asymmetric objectives. Through a progressive case study on learning a noisy sine wave, the work demonstrates how an Actor-Critic agent enhanced with Prioritized Experience Replay, increased network capacity, and positional encoding can perfectly learn periodic functions and generalize beyond training ranges.

## Method Summary
The approach frames regression as a contextual bandit where the agent selects predictions (actions) given input features (state) to maximize a custom reward function. The reward R(ŷ, y) = exp(−(y−ŷ)²/2σ²) replaces traditional loss functions, allowing arbitrary reward shaping. The Actor network outputs predictions while the Critic estimates Q-values. Training uses DDPG-style updates with Adam optimizer, batch size 64, and exploration noise. Key enhancements include Prioritized Experience Replay for sample efficiency and positional encoding to explicitly represent periodic structure in the input.

## Key Results
- Actor-Critic with PER and increased capacity successfully learns noisy sine function across five periods
- Positional encoding enables perfect learning and strong generalization beyond training range
- The RL approach demonstrates flexibility in defining custom objectives through reward shaping
- Method shows potential for handling non-differentiable objectives where traditional regression fails

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Framing regression as a contextual bandit enables optimization of non-differentiable or asymmetric custom objectives.
- Mechanism: The reward function R(ŷ, y) = exp(−(y−ŷ)²/2σ²) replaces gradient-based loss, allowing arbitrary reward shaping. The agent learns via policy gradient to maximize reward rather than minimize a predefined loss.
- Core assumption: The reward function faithfully encodes task priorities, and the policy gradient signal is sufficiently informative to guide the actor.
- Evidence anchors:
  - [abstract] "custom reward signal based on the prediction error... asymmetric costs or complex, non-differentiable objectives"
  - [section 1] "the reward function can be arbitrarily complex and tailored to the specific goals of the task, even if it is non-differentiable"
  - [corpus] Related work on RL for robots without reward functions (arXiv:2501.04228) similarly explores custom objective encoding.
- Break condition: If the reward landscape is too sparse or flat, gradient-based policy updates may receive insufficient signal.

### Mechanism 2
- Claim: Positional encoding exposes periodic structure to the network, reducing representational burden.
- Mechanism: Raw scalar input x is transformed into a higher-dimensional vector of sinusoids at multiple frequencies (PE(x)i = sin/cos terms). This explicitly encodes periodicity, allowing a standard MLP to approximate periodic functions more easily.
- Core assumption: The target function has periodic or quasi-periodic structure that aligns with the encoding frequencies.
- Evidence anchors:
  - [section 4.4] "The raw value of x does not explicitly convey its periodic nature... introduces positional encoding"
  - [section 5] "representation of features can be more critical than the learning algorithm itself"
  - [corpus] Weak direct evidence; positional encoding is typically applied to sequences (Transformers), not scalar regression.
- Break condition: For aperiodic or irregularly sampled inputs, positional encoding may add noise without benefit.

### Mechanism 3
- Claim: Prioritized Experience Replay (PER) focuses learning on high-error regions, improving sample efficiency.
- Mechanism: PER samples transitions with probability proportional to TD-error magnitude, causing the agent to revisit difficult predictions more often.
- Core assumption: High TD-error samples are informative for learning and not simply noisy outliers.
- Evidence anchors:
  - [section 2] "Prioritized Experience Replay... improves sample efficiency by replaying important transitions more frequently"
  - [section 4.2–4.3] PER alone did not resolve underfitting; required capacity increase.
  - [corpus] arXiv:1511.05952 (Schaul et al.) provides foundational evidence for PER; not re-proven here.
- Break condition: If noise dominates error signal, PER may over-prioritize unlearnable samples.

## Foundational Learning

- Concept: **Contextual Bandits**
  - Why needed here: Understanding that each prediction is a one-step decision (not a sequential MDP) clarifies why this is regression-as-RL, not full RL.
  - Quick check question: Can you explain why a contextual bandit has no state transition dynamics?

- Concept: **Actor-Critic Architecture**
  - Why needed here: The method uses separate networks for policy (Actor) and value estimation (Critic); understanding their interdependence is essential for debugging.
  - Quick check question: What happens to training if the Critic consistently overestimates Q-values?

- Concept: **Positional Encoding**
  - Why needed here: The key breakthrough in the case study came from feature engineering, not algorithm tuning.
  - Quick check question: Why does a scalar input x fail to convey periodicity to an MLP?

## Architecture Onboarding

- Component map:
  Actor network (MLP) -> Maps state S → action Â (prediction); Tanh output bounds predictions
  Critic network (MLP) -> Maps (S, A) → Q-value; linear output
  Replay buffer -> Stores (S, A, R, S') transitions; optionally prioritized
  Positional encoder -> Transforms scalar x → d-dimensional sinusoidal vector

- Critical path:
  1. Define state representation (raw features vs. positional encoding)
  2. Set reward function (Gaussian kernel σ; tolerance parameter)
  3. Configure Actor/Critic capacity (depth, width)
  4. Enable exploration noise (Gaussian on actions)
  5. Train with alternating Critic/Actor updates

- Design tradeoffs:
  - Higher σ in reward → smoother gradient but lower precision pressure
  - Larger network capacity → better fit but slower training and potential overfitting
  - PER → better sample efficiency but increased implementation complexity and hyperparameter sensitivity

- Failure signatures:
  - Constant output: Actor collapsed; likely insufficient exploration or reward signal too flat
  - Step-function output: Underfitting; increase network capacity
  - Good center, poor edges: Representation mismatch; consider feature engineering (e.g., positional encoding)

- First 3 experiments:
  1. Replicate Stage 1 on single-period sine to validate environment and reward setup
  2. Expand to multi-period data without capacity increase to observe underfitting failure mode
  3. Add positional encoding and confirm generalization beyond training range as in Stage 4

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the RL-regressor paradigm be effectively adapted for classification tasks?
- Basis in paper: [explicit] The conclusion states that "This 'RL as regressor' paradigm could be extended to classification, presenting an interesting direction for substituting supervised learning with RL."
- Why unresolved: The paper focuses exclusively on continuous prediction (regression) using an Actor-Critic method suited for continuous action spaces. Adapting this for discrete class labels requires reformulating the action space and reward mechanism.
- What evidence would resolve it: A study applying the contextual bandit framework to a classification benchmark (e.g., MNIST or UCI datasets) using a discrete policy, comparing performance and convergence speed against standard cross-entropy loss.

### Open Question 2
- Question: Does the RL-regressor framework provide significant advantages over standard regression for truly non-differentiable or discontinuous objective functions?
- Basis in paper: [explicit] The discussion notes that the method's "true value lies in problems where the objective is complex, non-standard, or non-differentiable," but the provided case study only validates the approach on a smooth sine function with a Gaussian reward.
- Why unresolved: The experiments do not test the claimed primary advantage—handling non-differentiable objectives—as both the target function (sine) and the reward kernel (Gaussian) are smooth and differentiable.
- What evidence would resolve it: Experiments on regression tasks with non-differentiable reward landscapes (e.g., a step-function reward or a custom reward with sharp discontinuities) comparing the RL agent's performance against gradient-based methods.

### Open Question 3
- Question: How does the sample efficiency and stability of this approach scale to high-dimensional datasets?
- Basis in paper: [inferred] The paper demonstrates success on a 1D synthetic dataset but acknowledges that RL algorithms are "often more computational intensive... and can be less stable" than supervised learning.
- Why unresolved: The case study relies on a low-dimensional input where manual feature engineering (positional encoding) was feasible and critical for success. It is unclear if the method remains practical or stable with high-dimensional raw inputs without extensive feature engineering.
- What evidence would resolve it: Benchmarking the Actor-Critic regressor on higher-dimensional real-world datasets (e.g., Boston Housing or YearPredictionMSD) to analyze training stability and data requirements relative to standard MLPs.

## Limitations
- Performance heavily depends on reward function design and hyperparameter tuning
- Computational overhead is higher than standard regression methods
- Requires manual feature engineering (positional encoding) for structured data
- Generalization benefits may stem more from feature engineering than RL framework

## Confidence
- **High confidence**: Actor-Critic with PER can approximate a periodic function when given appropriate input representation (positional encoding)
- **Medium confidence**: The method enables optimization of custom, non-differentiable objectives via reward shaping; but lack of ablation on reward design weakens this claim
- **Low confidence**: Generalization beyond training range is inherently due to RL; the key enabler appears to be positional encoding, not the RL mechanism itself

## Next Checks
1. **Reward function ablation**: Replace Gaussian kernel with MSE loss and compare performance; isolate contribution of reward design vs. RL framework
2. **Feature engineering ablation**: Train on raw x without positional encoding across multiple runs; measure impact on edge generalization
3. **Scalability test**: Apply method to a non-periodic regression task (e.g., polynomial or exponential decay) to assess generality beyond structured, periodic functions