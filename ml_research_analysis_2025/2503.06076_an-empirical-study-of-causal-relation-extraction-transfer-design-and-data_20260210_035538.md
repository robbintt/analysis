---
ver: rpa2
title: 'An Empirical Study of Causal Relation Extraction Transfer: Design and Data'
arxiv_id: '2503.06076'
source_url: https://arxiv.org/abs/2503.06076
tags:
- causal
- extraction
- data
- relation
- transfer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluates causal relation extraction models for transfer
  learning across diverse datasets, introducing a phrasal evaluation metric (F1phrase)
  to address annotation inconsistencies. Experiments show that BioBERT-BiGRU models
  outperform alternatives, with simpler BiGRU units and no CRF layers being sufficient.
---

# An Empirical Study of Causal Relation Extraction Transfer: Design and Data

## Quick Facts
- arXiv ID: 2503.06076
- Source URL: https://arxiv.org/abs/2503.06076
- Reference count: 31
- Primary result: BioBERT-BiGRU models with phrasal evaluation metric outperform alternatives for causal relation extraction transfer across domains

## Executive Summary
This paper investigates transfer learning for causal relation extraction across diverse datasets with varying annotation styles. The authors introduce F1_phrase, a phrasal evaluation metric that uses dependency parsing to address annotation inconsistencies between datasets. Through systematic experiments across six datasets, they demonstrate that BioBERT-BiGRU architectures generalize better than complex BiLSTM-CRF models, that data augmentation across domains improves transfer performance, and that implicit/explicit sentence composition in training data is more critical than dataset size for successful transfer.

## Method Summary
The study employs a BioBERT-BiGRU architecture for token-level causal relation extraction, using cross-entropy loss without CRF layers. Six datasets are evaluated with 70-30 train-test splits: Causal-TimeBank, CauseNet (cause and noncause), FinCausal2020, MedCaus, and SemEval 2010 Task 8. The novel F1_phrase metric maps predicted tokens to their parent noun phrases via dependency parsing, allowing credit for correct causal concept identification even when annotation spans differ. Training uses ADAM optimizer (lr=0.001), batch size 16, for 12 epochs with early stopping, repeated across 10 random seeds.

## Key Results
- BioBERT-BiGRU models consistently outperform BiLSTM and BiLSTM-CRF alternatives in cross-domain transfer
- F1_phrase metric reveals that standard F1 underestimates model performance due to annotation inconsistencies
- Data augmentation across domains and annotation styles improves transfer, especially with mixed implicit/explicit training data
- Performance gains plateau after a few thousand training samples, with composition being more important than size

## Why This Works (Mechanism)

### Mechanism 1
If evaluated using phrasal localization (F1_phrase) rather than strict token matching, transfer learning performance improves by accounting for annotation inconsistencies in noun phrase length. The F1_phrase metric uses dependency parsing to map predicted tokens to their parent noun phrase. If a model predicts the root noun ("hammer") but the ground truth includes the full phrase ("water hammer pressure"), the metric counts the entire phrase as correct because the full phrase is recoverable from the root.

### Mechanism 2
When using robust contextual embeddings (BioBERT), a simpler recurrent architecture (BiGRU) generalizes better across domains than complex BiLSTM-CRF models. Contextual embeddings like BioBERT capture long-term dependencies effectively, reducing the need for the memory gates of an LSTM. The paper suggests CRF layers may overfit to specific annotation schemes (entity lengths) in the training data, harming transfer, while BiGRUs offer sufficient sequential encoding with less overfitting risk.

### Mechanism 3
Transfer performance is dependent on the implicit/explicit composition of the training data, provided the training data volume exceeds a few thousand samples. Explicit sentences (containing "caused by") teach syntactic patterns, while implicit sentences teach semantic inference. A mix prevents the model from overfitting to simple syntactic markers. Volume helps only until the noun phrase coverage is saturated (approx. 2k-4k samples).

## Foundational Learning

- **Sequence Labeling (Token Classification)**: The task assigns tags (Cause, Effect, Other) to every token in a sentence to extract exact spans. Quick check: Can you explain the difference between sentence classification and sequence labeling?
- **Transfer Learning in NLP**: The core problem is training on one dataset (e.g., Wikipedia) and testing on another (e.g., Financial news) without performance collapse. Quick check: What is the risk of training a model on a dataset with only single-word annotations and testing it on a dataset with multi-word annotations?
- **Dependency Parsing**: Required to implement the F1_phrase metric, which relies on traversing "compound" and "amod" relations to expand a root noun into a full phrase. Quick check: In a dependency tree, if "cold" modifies "water", which token is the head and which is the child?

## Architecture Onboarding

- **Component map**: Raw Sentence -> BioBERT Embeddings -> BiGRU Encoder -> Linear Layer -> Per-token probabilities (C, E, O) -> (Evaluation only) Dependency Parser -> F1_phrase calculation
- **Critical path**: The choice of embedding (BioBERT) and the definition of the evaluation metric (F1_phrase) are more critical to transfer success than the complexity of the recurrent layer
- **Design tradeoffs**: CRF Layer excluded as it may overfit to specific annotation schemes; BiGRU preferred over BiLSTM for comparable performance with fewer parameters
- **Failure signatures**: High F1_phrase but low standard F1 indicates correct concepts but incorrect annotation boundaries; high performance on explicit datasets but low on implicit indicates lexical marker overfitting
- **First 3 experiments**: 
  1. Train BioBERT-BiGRU on SemEval with both F1 and F1_phrase to establish performance gap
  2. Swap BiGRU for BiLSTM on MedCaus to verify if LSTM complexity yields significant gains
  3. Train on MedCaus (mixed) and test on FinCausal vs training on CauseNet (explicit) and testing on FinCausal to measure implicit sentence impact

## Open Questions the Paper Calls Out

- Does pre-training language models with dynamic masking specifically designed for causal phrases improve transfer performance compared to generic pre-training?
- How do the findings regarding data augmentation and the BioBERT-BiGRU architecture transfer to inter-sentence and document-level causal relation extraction?
- To what extent can advanced large language models (e.g., GPT, Mistral-7B, Llama) outperform the fine-tuned BioBERT-BiGRU model in open-domain causal relation extraction?

## Limitations

- The effectiveness of F1_phrase depends heavily on dependency parser quality and may measure parser accuracy rather than causal understanding
- Architecture claims are limited to comparisons with BiLSTM and BiLSTM-CRF, not modern Transformer-only approaches
- Data augmentation findings need validation across more diverse domain pairs and causal expression patterns

## Confidence

- **High Confidence**: BioBERT-BiGRU architecture provides robust transfer performance; CRF layers may overfit and harm transfer; data augmentation across domains consistently improves transfer
- **Medium Confidence**: F1_phrase metric effectively addresses annotation inconsistencies; diminishing returns beyond a few thousand training samples; implicit/explicit composition is more important than dataset size for transfer
- **Low Confidence**: The specific threshold of "a few thousand samples" for optimal performance; generalizability to causal relation types beyond tested domains; simpler architectures are universally preferable

## Next Checks

1. Evaluate F1_phrase performance using multiple dependency parsers (spaCy, Stanford CoreNLP, AllenNLP) across all six datasets to test metric reliability
2. Compare BioBERT-BiGRU against BioBERT-BiLSTM-CRF and modern Transformer-only approaches on most challenging transfer pairs
3. Systematically vary the implicit:explicit ratio in training data (0:100, 25:75, 50:50, 75:25, 100:0) and test transfer to each target dataset to identify optimal composition ratios