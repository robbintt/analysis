---
ver: rpa2
title: Zero-shot World Models via Search in Memory
arxiv_id: '2510.16123'
source_url: https://arxiv.org/abs/2510.16123
tags:
- figure
- each
- latent
- one-step
- dynamics
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores zero-shot world models using similarity search
  and stochastic representations, avoiding training procedures. The authors propose
  a method leveraging variational autoencoders and different retrieval strategies
  (rollout, replay with L2/KL distances) to predict state transitions without learning
  dynamics.
---

# Zero-shot World Models via Search in Memory

## Quick Facts
- arXiv ID: 2510.16123
- Source URL: https://arxiv.org/abs/2510.16123
- Reference count: 40
- Primary result: Zero-shot world models using similarity search and stochastic representations achieve comparable or superior performance to trained models like PlaNet in latent space reconstruction and image quality, especially for long-horizon predictions.

## Executive Summary
This paper introduces a zero-shot approach to world modeling that avoids training explicit dynamics models by leveraging similarity search in memory. The method uses a pretrained variational autoencoder to encode observations into stochastic latent representations, then predicts future states by retrieving similar past transitions from a memory buffer. Three retrieval strategies are compared: rollout buffers that preserve trajectory structure, replay buffers with L2 distance, and replay buffers with KL divergence between distributions. The approach is evaluated on SuperTuxKart, Minecraft, and Atari environments, demonstrating that distribution-based retrieval (KL) produces the most coherent predictions while rollout structures reduce variance. The study shows that data coverage, not volume, drives performance, and that zero-shot models excel in small-scoped tasks but struggle with vast state spaces.

## Method Summary
The method trains a β-VAE once on demonstration data to encode RGB images into stochastic latent vectors z ~ N(μ, σ). Transitions (z_t, a_t, z_{t+1}, μ_t, σ_t) are stored in memory buffers. To predict the next state given current latent z_t and action a_t, the system retrieves K similar transitions using either L2 distance in latent space or KL divergence between the reference distribution N(μ_ref, σ_ref) and stored distributions. The retrieved next states are aggregated (either by averaging or direct sampling) to estimate the distribution of z_{t+1}, from which a sample is decoded to produce the predicted image. Long-horizon predictions are generated by iteratively using predicted latents as queries.

## Key Results
- Replay-KL retrieval produces the highest-quality visual reconstructions with minimal hallucinations compared to L2-based methods
- Data coverage matters more than volume: 5 well-distributed trajectories outperform 30 poorly-distributed ones
- Zero-shot models perform competitively with PlaNet on SuperTuxKart and Atari but struggle with Minecraft's vast state space
- KL divergence-based retrieval maintains lower KL divergence than L2 while achieving superior L1 reconstruction error
- Long-horizon predictions remain coherent but gradually drift from ground truth trajectories

## Why This Works (Mechanism)

### Mechanism 1: Memory-based Transition Approximation
Next-state transitions can be approximated by retrieving similar past experiences rather than learning explicit dynamics. Given current latent state z_t and action a_t, search memory for K similar transitions (z_τ, a_τ, z_{τ+1}), then estimate the distribution q_φ(z_{t+1}|x_{t+1}) from retrieved next states. The predicted latent is sampled from this estimated distribution. This works because states with similar latent representations transition to similar successor states under the same action (temporal locality in latent space).

### Mechanism 2: Distribution Matching via KL-divergence Retrieval
Retrieving based on distribution similarity (KL divergence) rather than point distance (L2) produces more coherent and visually consistent predictions. Store VAE encoder outputs (μ, σ) with each transition. At query time, construct reference distribution N_ref(μ_ref, σ_ref) and retrieve the transition whose prior distribution has minimum KL divergence. This directly samples from a meaningful region of latent space rather than estimating from a potentially noisy batch. The VAE's learned latent distributions capture semantic similarity; two states with similar distributions encode similar scene content and dynamics.

### Mechanism 3: Rollout Buffer Structuring
Maintaining temporal trajectory structure during retrieval reduces variance in batch estimation and regularizes predictions. Store trajectories as independent "threads" with temporal ordering. Retrieve the 1-most similar latent from each trajectory (not k-nearest overall), ensuring diversity while preserving within-trajectory temporal consistency. Transitions within a single trajectory share consistent dynamics; different trajectories capture alternative but internally coherent dynamics branches.

## Foundational Learning

- **Variational Autoencoders (VAE)**: The entire method operates in VAE latent space; understanding stochastic encoding (μ, σ sampling) is essential to grasp how KL-based retrieval works. Quick check: Can you explain why sampling z ~ N(μ, σ) produces different latent vectors for the same input, and how this relates to the paper's use of distributions for retrieval?

- **Markov Decision Process (MDP) Transition Dynamics**: The paper explicitly targets approximating P(s_{t+1}|s_t, a_t); understanding this formalism clarifies what the search-based approach replaces. Quick check: What assumption does the Markov property make about state transitions, and how does the paper's trajectory-based retrieval relate to it?

- **k-Nearest Neighbor Search and Distance Metrics**: Core retrieval mechanism relies on L2 distance vs. KL divergence; understanding their properties explains performance differences. Quick check: Why might L2 distance between latent vectors produce higher-variance retrievals than KL divergence between distributions?

## Architecture Onboarding

- **Component map**: Raw Dataset D = {(x_t, a_t)} → VAE Training → Latent Dataset Z = {(z_t, a_t, z_{t+1}, μ_t, σ_t)} → Retrieval Strategies (Rollout Buffer, Replay Buffer + L2, Replay Buffer + KL) → Retrieved transitions → Estimate p(z_{t+1}|z_t, a_t) → Sample ẑ_{t+1} → VAE Decoder → Reconstructed image x̂_{t+1}

- **Critical path**: 1) Offline: Train β-VAE on all images; encode trajectories to build Z with (z, a, z', μ, σ) tuples. 2) Inference: Encode current observation → retrieve similar transitions → estimate next-state distribution → sample and decode. 3) Long-horizon: Iterate the above using predicted latents as queries (error accumulates).

- **Design tradeoffs**: Rollout: Lower variance, better temporal consistency, but struggles when action-conditioning limits retrievable candidates per trajectory. Replay-L2: Maximum retrieval flexibility, but batch estimation from high-variance neighbors causes visual hallucinations. Replay-KL: Best reconstruction quality, but slower inference (2x Rollout time) and rare retrieval failures when action-constrained. Data volume vs. coverage: Coverage matters more than raw count—5 well-covered trajectories outperform 30 poorly-distributed ones.

- **Failure signatures**: Visual hallucinations (Replay-L2): Retrieved batch has high variance; decoded images show scene mixing. Gradual semantic drift (all methods, long-horizon): Predictions remain visually coherent but diverge from ground truth trajectory. Coverage-induced failure (Minecraft Navigate): Large state space with limited encoded trajectories → KL divergence remains high. Rare retrieval mismatch (Replay-KL with action conditioning): First retrieval lands on very different distribution → entire trajectory becomes unrelated.

- **First 3 experiments**: 1) Single-step reconstruction sanity check: Encode 20 test images, retrieve and predict next latent, decode and compare to ground truth using L1/SSIM. 2) Coverage ablation on your domain: Encode 5, 10, 20 trajectories; plot KL prediction error vs. coverage ratio. 3) Horizon degradation analysis: Run predictions at t=1, 5, 10, 20 steps; compare when each method diverges (KL spike, SSIM drop).

## Open Questions the Paper Calls Out

- How can zero-shot world models be effectively integrated with action selection policies to enable closed-loop autonomous control? The authors note that developing a method for estimating a probability distribution over actions "implies moving the focus towards the problem of action selection, which is outside the scope of this paper," despite testing planning capabilities.

- Can memory-based retrieval scale effectively to environments with infinite or open-ended state spaces without succumbing to coverage limits? The paper concludes that effectiveness is "limited in tasks with vast state spaces," specifically citing performance drops in the "Navigate-v0" Minecraft task where state coverage is insufficient.

- How can batch estimation be improved to prevent visual hallucinations in L2-based retrieval methods? While Replay-L2 maintains low KL divergence, it suffers from significantly higher L1 distance and visual incoherence (hallucinations) compared to KL-based methods, suggesting the need for modified aggregation functions.

## Limitations
- Retrieval-based world modeling fundamentally requires dense state-space coverage, degrading sharply when trajectories fail to span the latent space
- The Replay-KL method, while highest quality, suffers from slower inference times (2x Rollout) and occasional retrieval failures under strict action constraints
- Performance is bounded by data coverage rather than model capacity, making the approach unsuitable for open-ended environments without massive datasets

## Confidence
- High confidence: KL-based distribution retrieval produces superior visual consistency compared to L2 distance, supported by direct empirical comparisons showing clear qualitative differences
- Medium confidence: Rollout buffer structuring reduces variance through temporal trajectory coherence, though evidence is indirect and relies on comparison with unstructured buffers
- Low confidence: Data coverage matters more than volume, since the ablation only tests up to 30 trajectories and doesn't explore whether massive datasets could overcome coverage limitations

## Next Checks
1. **Coverage threshold experiment**: Systematically vary the number of trajectories while keeping their length constant to identify the inflection point where KL divergence plateaus, determining minimum viable data requirements for each environment
2. **Action-conditioning robustness test**: Force retrieval with strict action constraints across all three methods to quantify how often each strategy fails when similar transitions don't exist, measuring the trade-off between consistency and coverage
3. **Long-horizon error decomposition**: Run predictions at multiple horizons (t=1, 5, 10, 20) and separately analyze drift in latent space versus degradation in reconstruction quality to determine whether errors compound additively or multiplicatively