---
ver: rpa2
title: 'MDSEval: A Meta-Evaluation Benchmark for Multimodal Dialogue Summarization'
arxiv_id: '2510.01659'
source_url: https://arxiv.org/abs/2510.01659
tags:
- summary
- dialogue
- information
- evaluation
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces MDSEval, the first meta-evaluation benchmark
  for multimodal dialogue summarization (MDS). The benchmark consists of 198 high-quality
  image-sharing dialogues paired with five summaries each, all annotated across eight
  evaluation aspects: multimodal coherence, conciseness, coverage (visual, textual,
  overall), information balance, topic progression, and faithfulness.'
---

# MDSEval: A Meta-Evaluation Benchmark for Multimodal Dialogue Summarization

## Quick Facts
- **arXiv ID**: 2510.01659
- **Source URL**: https://arxiv.org/abs/2510.01659
- **Reference count**: 40
- **Primary result**: Introduces first meta-evaluation benchmark for multimodal dialogue summarization with 198 dialogues, each paired with 5 summaries annotated across 8 evaluation aspects

## Executive Summary
MDSEval introduces the first meta-evaluation benchmark for multimodal dialogue summarization (MDS), addressing the critical need for reliable evaluation methods in this emerging field. The benchmark consists of 198 high-quality image-sharing dialogues from PhotoChat and DialogCC, each paired with five summaries annotated across eight evaluation aspects: multimodal coherence, conciseness, coverage (visual, textual, overall), information balance, topic progression, and faithfulness. The authors developed a filtering framework based on Mutually Exclusive Key Information (MEKI) to ensure data quality and challenge by quantifying information uniquely conveyed by each modality.

The evaluation reveals significant gaps in current multimodal evaluation methods. When benchmarking state-of-the-art multimodal evaluation approaches, the authors find that existing models struggle to align with human judgments, particularly in distinguishing between summaries from advanced LLMs. Current methods show weak Spearman correlations with human preferences, exhibit significant biases in score distributions and positional preferences, and achieve particularly poor performance on faithfulness evaluation (<30% balanced accuracy). These findings highlight the need for more accurate and human-aligned assessment techniques in multimodal dialogue summarization.

## Method Summary
The MDSEval benchmark was constructed through a systematic process involving data collection, filtering, and annotation. The authors collected 198 image-sharing dialogues from PhotoChat and DialogCC datasets, then applied a Mutually Exclusive Key Information (MEKI) framework to filter dialogues based on their multimodal complexity. Each dialogue was paired with five summaries generated by different methods. Eight annotators were recruited to evaluate summaries across eight aspects using a scoring framework. The annotation process included pilot studies, multiple rounds of refinement, and quality control measures to ensure consistency. The final dataset contains comprehensive human judgments that serve as ground truth for evaluating multimodal summarization quality.

## Key Results
- Current multimodal evaluation methods show weak Spearman correlations with human preferences, particularly struggling to distinguish between advanced LLM-generated summaries
- All tested evaluation methods exhibit significant biases, including systematic score concentration around 4 and positional preferences that persist across different frameworks
- Faithfulness evaluation remains particularly challenging, with all methods achieving <30% balanced accuracy at both summary and sentence levels
- Gemini-1.5-flash and Qwen-vl-max show opposite positional biases, with Gemini favoring option A and Qwen-vl-max favoring option B

## Why This Works (Mechanism)
The benchmark works by creating a controlled evaluation environment where multiple summaries for each dialogue can be directly compared against human judgments across multiple quality dimensions. The MEKI framework ensures that only dialogues with meaningful multimodal information exchange are included, creating a challenging test set. By collecting five summaries per dialogue and having multiple annotators evaluate each, the benchmark captures the variability in human judgment and provides a robust ground truth for comparison. The pairwise comparison format allows evaluation methods to be assessed on their ability to match human preference rankings rather than absolute score accuracy.

## Foundational Learning

**Multimodal Dialogue Summarization**: The task of generating concise summaries from dialogues that include both textual conversation and associated images. Needed to understand the unique challenges of integrating visual and textual information streams.

**Mutually Exclusive Key Information (MEKI)**: A framework for quantifying information that can only be obtained from one modality. Needed to filter dialogues for appropriate multimodal complexity and ensure the benchmark tests genuine integration of modalities.

**Pairwise Comparison Evaluation**: A methodology where evaluation methods must choose which of two summaries is better rather than assigning absolute scores. Needed to reduce subjective variance and focus on relative quality assessment.

**Spearman Correlation for Ranking**: A statistical measure of how well two ranked lists match. Needed to evaluate whether evaluation methods can reproduce human preference orderings.

**Balanced Accuracy**: A performance metric that accounts for class imbalance by averaging sensitivity and specificity. Needed because faithfulness labels are heavily imbalanced (91.2% faithful).

**Prompt Engineering for MLLMs**: The technique of crafting input prompts to elicit desired outputs from multimodal large language models. Needed to adapt evaluation models to specific aspects of summary quality.

## Architecture Onboarding

**Component Map**: Image-sharing dialogues -> MEKI filtering -> Summary generation -> Human annotation (8 aspects) -> Benchmark dataset -> MLLM evaluation methods -> Performance comparison

**Critical Path**: The benchmark creation process follows: dialogue collection → MEKI filtering → summary generation → human annotation → method evaluation → analysis. The critical path is constrained by the annotation quality, as human judgments serve as the ground truth for all subsequent evaluations.

**Design Tradeoffs**: The authors chose to focus on chitchat dialogues for accessibility but this limits generalizability. They prioritized comprehensive annotation across multiple aspects over larger dataset size. The pairwise comparison format reduces subjective variance but may miss absolute quality differences.

**Failure Signatures**: Poor performance on faithfulness indicates fundamental limitations in current models' ability to detect information consistency. Systematic score concentration at 4 suggests models are not sensitive to quality variations. Positional biases reveal that model architecture influences preference rather than content quality.

**First Experiments**:
1. Run MEKI analysis on a small set of dialogues to verify the filtering framework identifies genuinely multimodal exchanges
2. Compare pairwise judgments from a single annotator versus the full set to quantify annotation consistency
3. Test a simple baseline evaluation method (e.g., text-only ROUGE) on the benchmark to establish minimum performance thresholds

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can multimodal evaluation methods be developed that achieve strong alignment with human judgments when assessing summaries from state-of-the-art MLLMs?
- Basis in paper: [explicit] The authors state "current MLLMs still struggle to deliver human-aligned judgments when evaluating summaries generated by the latest MLLMs" and call for "more accurate and human-aligned assessment techniques."
- Why unresolved: All tested methods showed weak Spearman correlations with human preferences and concentrated scores around 4 regardless of actual quality.
- What evidence would resolve it: New evaluation methods achieving ρ > 0.5 correlation with human judgments across most evaluation aspects on MDSEval or similar benchmarks.

### Open Question 2
- Question: How can the significant positional and score distribution biases in MLLM-based evaluators be effectively mitigated?
- Basis in paper: [explicit] The paper finds that "existing evaluation methods suffer from significant biases" including positional preferences and systematic score concentration at 4, which persists across different evaluation frameworks.
- Why unresolved: Neither MLLM-as-Judge, Image-to-Prompt, nor Checklist-CoT mitigated these biases; Gemini-1.5-flash favored option A while Qwen-vl-max favored option B.
- What evidence would resolve it: Evaluation methods demonstrating balanced preference ratios (near 50%) in pairwise comparisons and score distributions matching human variance.

### Open Question 3
- Question: What approaches can substantially improve multimodal faithfulness evaluation, which currently achieves <30% balanced accuracy?
- Basis in paper: [inferred] From the results showing all faithfulness evaluation methods achieve low balanced accuracy (22-30%) at both summary and sentence levels, with the paper noting "faithfulness evaluation remains particularly challenging."
- Why unresolved: The inherent complexity of inference in multimodal contexts and imbalanced label distributions (91.2% faithful) create fundamental difficulties not addressed by current methods.
- What evidence would resolve it: Methods achieving >50% balanced accuracy on faithfulness evaluation while maintaining reasonable F1 scores across all faithfulness categories.

### Open Question 4
- Question: Will the findings from MDSEval's chitchat dialogues generalize to practical domains like customer service or workplace conversations?
- Basis in paper: [explicit] The limitations section states: "Expanding to more practical domains—such as commercial customer service or formal workplace conversations—could enhance the benchmark's applicability and relevance."
- Why unresolved: The current benchmark focuses solely on chitchat-style dialogues from PhotoChat and DialogCC, which may not represent the linguistic patterns and multimodal integration needs of professional contexts.
- What evidence would resolve it: Studies replicating the benchmarking experiments on newly constructed multimodal dialogue datasets from customer service or workplace domains, showing similar or distinct evaluation challenges.

## Limitations

- The benchmark is limited to chitchat-style image-sharing dialogues, potentially limiting generalizability to other dialogue domains
- Human annotation introduces subjectivity, particularly for aspects like multimodal coherence and topic progression
- The MEKI framework assumes mutually exclusive key information is the primary indicator of multimodal challenge, which may not capture all nuances
- Evaluation is constrained to models available at the time of study, potentially missing advancements in more recent approaches

## Confidence

- **High confidence** in the benchmark's construction methodology and the systematic approach to data collection and annotation
- **Medium confidence** in the MEKI framework's effectiveness in identifying challenging dialogues, as its assumptions about modality-specific information may not hold universally
- **Low confidence** in the benchmark's ability to generalize across diverse multimodal dialogue domains beyond image-sharing contexts

## Next Checks

1. **Cross-domain validation**: Apply the MDSEval benchmark to dialogues from other domains (e.g., medical, educational, or customer service) to assess its generalizability and identify domain-specific limitations

2. **Annotator diversity study**: Conduct a study with annotators from diverse cultural and linguistic backgrounds to evaluate the consistency and subjectivity of annotations for multimodal coherence and topic progression

3. **Temporal robustness test**: Re-evaluate the benchmark using state-of-the-art multimodal summarization models developed after the initial study to determine if the identified limitations persist or if newer models address them