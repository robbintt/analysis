---
ver: rpa2
title: A Separable Architecture for Continuous Token Representation in Language Models
arxiv_id: '2601.22040'
source_url: https://arxiv.org/abs/2601.22040
tags:
- leviathan
- dense
- https
- language
- generator
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the inefficiency of discrete embedding matrices
  in small language models, where these matrices consume a large portion of the parameter
  budget, especially with large vocabularies. The authors propose Leviathan, a transformer
  architecture that replaces the discrete lookup table with a continuous token generator
  using a Separable Neural Architecture (SNA).
---

# A Separable Architecture for Continuous Token Representation in Language Models

## Quick Facts
- arXiv ID: 2601.22040
- Source URL: https://arxiv.org/abs/2601.22040
- Reference count: 28
- Primary result: Leviathan achieves 1.47-2.11× parameter efficiency vs dense baselines in small language models

## Executive Summary
This paper addresses the inefficiency of discrete embedding matrices in small language models, where these matrices consume a large portion of the parameter budget, especially with large vocabularies. The authors propose Leviathan, a transformer architecture that replaces the discrete lookup table with a continuous token generator using a Separable Neural Architecture (SNA). This generator maps tokens to a low-dimensional coordinate grid and projects them into the hidden space, reducing parameter scaling from O(V*D) to O(k*V^(1/k)*D). Experimental results on the Pile dataset show that Leviathan consistently outperforms a standard LLaMA-style architecture across different model scales (60M-421M parameters).

## Method Summary
Leviathan replaces the discrete embedding matrix with a continuous token generator that uses coordinate decomposition and tensor-product B-splines. Each token ID is decomposed into k coordinates using base-b arithmetic, which index into shared codebooks. These are projected to a latent space, expanded using B-spline bases, and aggregated via tensor products to produce the final embedding. This reduces vocabulary parameter scaling from O(V*D) to O(k*V^(1/k)*D), enabling reinvestment of saved parameters into deeper transformer layers.

## Key Results
- Leviathan achieves equivalent performance to dense models with 1.47 to 2.11 times more parameters under isoparametric settings
- The architecture exhibits steeper scaling behavior than dense models when reasoning layers are held fixed
- Leviathan continues to improve its advantage as training progresses, achieving 6-18% perplexity reduction

## Why This Works (Mechanism)

### Mechanism 1: Latent Compositional Indexing (Coordinate Quantization)
- Claim: Token identity can be compressed from O(V) to O(k·V^(1/k)) parameters by factorizing the vocabulary index into a k-dimensional coordinate grid.
- Mechanism: Each token index i is decomposed via base-b arithmetic into coordinates (i₁, ..., iₖ). Each coordinate indexes into k shared learned codebooks C₁, ..., Cₖ ∈ ℝ^(b×d_seed), which are summed to produce a seed vector z(i).
- Core assumption: Token representations share structure that can be captured through compositional coordinate decomposition.
- Evidence anchors: [Section 3.1], [Section 4.5]
- Break condition: If vocabulary has no latent compositional structure, compression may lose critical distinctions.

### Mechanism 2: Continuous Surface Representation via Separable B-Splines
- Claim: Embedding space can be modeled as a smooth surface M(x) using tensor products of univariate B-spline bases, enabling interpolation and parameter sharing across tokens.
- Mechanism: The seed z is projected to a latent coordinate ẑ ∈ [0,1]^d_seed. Each dimension undergoes B-spline basis expansion φ_r(x_r). M modes are constructed via tensor products Πφ_r,j(x_r), summed and projected to the embedding dimension.
- Core assumption: Semantic relationships between tokens can be captured by a continuous, smooth manifold rather than isolated discrete points.
- Evidence anchors: [Section 3.1], [Section 6]
- Break condition: If token semantics require discontinuous or highly irregular manifolds, B-spline smoothness constraints may underfit.

### Mechanism 3: Depth Dividend Through Parameter Reallocation
- Claim: Savings from the compressed generator can be reinvested into deeper transformer layers, yielding more effective capacity per parameter than the original embedding allocation.
- Mechanism: With a fixed parameter budget, Leviathan's generator uses far fewer parameters than a discrete embedding matrix, enabling 2.3-8.7× more layers while maintaining isoparametricity.
- Core assumption: Depth contributes more to reasoning capability than embedding table capacity in SLMs.
- Evidence anchors: [Section 4.2]
- Break condition: If tasks require extensive vocabulary-specific knowledge rather than compositional reasoning, deeper layers may not compensate for reduced embedding specificity.

## Foundational Learning

- Concept: B-spline basis functions
  - Why needed here: Core primitive for the continuous generator; understanding how piecewise polynomial functions provide smooth, locally-controlled approximation.
  - Quick check question: Can you explain why B-splines provide both local control (changing one knot affects limited region) and global smoothness (C² continuity for cubic)?

- Concept: Tensor product decomposition / Separable functions
  - Why needed here: The SNA constructs high-dimensional embeddings via products of univariate functions, reducing parameter complexity.
  - Quick check question: Given f(x,y) = g(x)·h(y), how many parameters are needed to represent this vs. an arbitrary function on the same 2D grid?

- Concept: Stone-Weierstrass universal approximation
  - Why needed here: Provides theoretical justification that the separable architecture can approximate any continuous function.
  - Quick check question: What two properties must an algebra satisfy for Stone-Weierstrass to guarantee density in C([0,1]^d)?

## Architecture Onboarding

- Component map: Token ID → Base-b decomposition → Codebook lookups → Dense projection → LayerNorm → Sigmoid → B-spline expansion → Tensor-product aggregation → Dense projection → Embedding

- Critical path: Token → coordinate decomposition → codebook summation → projection → B-spline basis expansion (per-mode tensor products) → aggregation → output projection. The generator replaces only the input embedding; output head remains standard.

- Design tradeoffs:
  - k (coordinate dimensions): Higher k = more compression but potentially weaker compositional structure. Paper uses k=3 for V≈200K.
  - M (number of modes): More modes = higher expressivity but more parameters. Paper uses M=8.
  - d_seed (latent dimension): Larger = richer coordinate space. Paper uses 128.
  - Throughput vs. sample efficiency: 23-51% throughput reduction, but ~1.7× fewer tokens needed for equivalent loss.

- Failure signatures:
  - Loss plateauing early with large gap to baseline: Check codebook initialization or coordinate mapping correctness.
  - Gradient instability in generator: Verify residual connection from seed projection (Eq. 3) is active.
  - No improvement despite depth increase: May indicate task requires vocabulary-specific knowledge over reasoning.

- First 3 experiments:
  1. **Iso-body validation**: Train matched Leviathan vs. tied-embedding baseline (same depth/width) on small corpus (~2B tokens) to verify generator provides representational gains independent of depth.
  2. **Coordinate ablation**: Test alternative k values (k=2, 3, 4) to find optimal compression-structure tradeoff for your vocabulary size.
  3. **Throughput benchmark**: Measure actual training throughput (tokens/sec) vs. baseline to confirm sample efficiency gains exceed computational overhead for your hardware.

## Open Questions the Paper Calls Out

- **Open Question 1**: Does the performance advantage of Leviathan persist or is there a crossover point when applied to models larger than the 1 billion parameter scale?
  - Basis in paper: [explicit] Page 7 states, "Whether this advantage is maintained for larger models, or whether there is a crossover requires further validation work."
  - Why unresolved: The experiments were restricted to the Small Language Model (SLM) regime (60M–421M parameters), and the relative "vocabulary tax" decreases as model size increases.

- **Open Question 2**: Can data-driven or learned coordinate embeddings improve the generator's efficiency compared to the current deterministic modular-arithmetic mapping?
  - Basis in paper: [explicit] Page 8 notes, "An exploration of alternative coordinate constructions such data-driven tokenizers or learned coordinate embeddings represents a promising direction for future work."
  - Why unresolved: The current deterministic mapping forces the model to "undo" local-but-not-semantic structures (arbitrary topology), which may be suboptimal.

- **Open Question 3**: Can the continuous representation surface enable effective "understanding" of new tokens added post-training without requiring full model retraining?
  - Basis in paper: [explicit] Page 8 suggests the architecture offers a "potential avenue for future research into open-vocabulary models that 'understand' new tokens without full retraining."
  - Why unresolved: This capability is hypothesized as a theoretical benefit of the continuous surface but was not demonstrated in the current experiments.

## Limitations
- Empirical validation confined to small language models (60M-421M parameters) and single corpus (The Pile)
- Throughput overhead of 23-51% may offset parameter efficiency gains on certain hardware
- Limited exploration of different vocabulary sizes and tokenization schemes

## Confidence

**High Confidence**: The parameter compression claims (Mechanism 1) are well-supported by mathematical derivation and experimental results consistently demonstrate improved parameter efficiency across multiple model scales.

**Medium Confidence**: The continuous surface representation (Mechanism 2) has theoretical support via Stone-Weierstrass but limited direct empirical validation for discrete token vocabularies.

**Low Confidence**: The robustness to arbitrary tokenizer topology and the scaling behavior with larger models (>421M parameters) remain largely speculative.

## Next Checks
1. **Vocabulary Structure Ablation**: Systematically test Leviathan with artificially randomized token ID assignments to validate whether the compression mechanism fails when compositional structure is absent.

2. **Scaling Frontier Experiment**: Train Leviathan models at 1B+ parameters to empirically verify whether scaling advantages persist at larger model sizes.

3. **Cross-Corpus Generalization**: Evaluate Leviathan's performance on diverse datasets beyond The Pile to test robustness claims and identify domain-specific limitations.