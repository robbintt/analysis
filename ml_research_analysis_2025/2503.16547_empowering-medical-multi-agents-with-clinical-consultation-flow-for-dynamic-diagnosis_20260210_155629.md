---
ver: rpa2
title: Empowering Medical Multi-Agents with Clinical Consultation Flow for Dynamic
  Diagnosis
arxiv_id: '2503.16547'
source_url: https://arxiv.org/abs/2503.16547
tags:
- uni00000013
- diagnosis
- uni0000004c
- uni00000011
- uni00000052
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of dynamic medical diagnosis,
  where traditional AI systems struggle with multi-turn interactions and premature
  diagnostic decisions due to incomplete information. The proposed solution is a multi-agent
  framework inspired by clinical consultation flow and reinforcement learning, designed
  to simulate the entire consultation process.
---

# Empowering Medical Multi-Agents with Clinical Consultation Flow for Dynamic Diagnosis

## Quick Facts
- arXiv ID: 2503.16547
- Source URL: https://arxiv.org/abs/2503.16547
- Authors: Sihan Wang; Suiyang Jiang; Yibo Gao; Boming Wang; Shangqi Gao; Xiahai Zhuang
- Reference count: 12
- Primary result: Hierarchical RL framework significantly improves dynamic diagnosis accuracy over baseline systems on MVME benchmark.

## Executive Summary
This paper addresses the challenge of premature closure in AI-driven dynamic diagnosis, where models make early diagnostic decisions based on incomplete information. The proposed solution is a multi-agent framework that simulates the entire clinical consultation process using reinforcement learning and a hierarchical action set derived from medical textbooks. The framework consists of Doctor, Patient, and Examiner agents that interact through structured phases (Inquiry, Examination, Diagnosis) to guide decision-making and prevent premature diagnostic conclusions.

## Method Summary
The framework formulates dynamic diagnosis as a Partially Observable Markov Game (POMG) involving three agents: Doctor, Patient, and Examiner. The Doctor Agent uses reinforcement learning to select actions from a hierarchical action set organized by consultation phases (Inquiry, Examination, Diagnosis). Each phase contains specific clinical actions derived from medical textbooks. The system incorporates a memory bank to maintain state consistency across interactions and allows retrospective actions for backtracking when necessary. The framework is evaluated on the MVME benchmark using both score-based evaluation (0-100 on multiple clinical dimensions) and match-based evaluation (ICD-10 entity mapping).

## Key Results
- The framework achieves state-of-the-art performance on the MVME benchmark, significantly improving diagnostic accuracy compared to baseline systems.
- Interaction turns increase from 4-5 to 8 turns, demonstrating enhanced persistence in information gathering before diagnosis.
- The hierarchical action set effectively guides the decision-making process, reducing phase-blending errors and improving structured clinical reasoning.

## Why This Works (Mechanism)

### Mechanism 1: Hierarchical Constraint of the Solution Space
Restricting the foundation model's output to a structured "Hierarchical Action Set" reduces reasoning errors by forcing explicit phase selection before action generation. The Doctor Agent first selects a phase (Inquiry, Examination, Diagnosis) and then selects an action from a pre-defined list derived from medical textbooks, funneling the model's high entropy into a controlled clinical pathway.

### Mechanism 2: Persistence via Reinforcement Learning Regularization
Reinforcement Learning shifts the agent's objective from immediate textual completion to long-term diagnostic accuracy, mitigating the "premature closure" bias inherent in standard LLMs. RL optimizes the decision policy based on dynamic state, allowing the agent to value information gathering (which has zero immediate reward but high future value) over early guessing.

### Mechanism 3: Multi-Agent Memory Retrieval
The system utilizes a memory bank to maintain state consistency across the partially observable game, allowing the Doctor Agent to make decisions based on cumulative history rather than just the immediate context window. The Doctor Agent retrieves accumulated subjective (Patient) and objective (Examiner) observations from the memory bank before selecting the next hierarchical action.

## Foundational Learning

- **Concept: Partially Observable Markov Game (POMG)**
  - Why needed here: The paper explicitly formulates dynamic diagnosis as a POMG involving three agents where the Doctor Agent cannot see the ground truth state directly.
  - Quick check question: Can you explain why the "Doctor Agent" cannot see the "Ground Truth" state directly in this formulation?

- **Concept: Premature Closure (Cognitive Bias)**
  - Why needed here: This is the specific failure mode the architecture is designed to fix, borrowed from human clinical psychology but applied to LLMs.
  - Quick check question: If an AI outputs a diagnosis after only 2 turns of dialogue, is this a failure of knowledge or a failure of persistence (premature closure)?

- **Concept: Hierarchical Reinforcement Learning (Options Framework)**
  - Why needed here: The "Hierarchical Action Set" implies a structure where high-level "phases" (Inquiry) contain low-level "actions" (Ask about fever).
  - Quick check question: In this architecture, is "Inquiry" an action or a state container for sub-actions?

## Architecture Onboarding

- **Component map:** Doctor Agent (LLM with RL Policy) -> Hierarchical Action Set -> Memory Bank -> Patient/Examiner Agents
- **Critical path:** 1. Observe: Doctor receives update from Patient/Examiner. 2. Retrieve: Doctor queries Memory Bank for accumulated history. 3. Reason: Doctor determines current Phase (Inquiry vs. Diagnosis). 4. Act: Doctor selects Action from the phase-specific set. 5. Update: Environment responds; Memory Bank updates.
- **Design tradeoffs:** Structure vs. Flexibility (hierarchy rigidly enforces flow but may limit creative diagnoses); Turn Count vs. Accuracy (increased persistence improves accuracy but increases latency and cost).
- **Failure signatures:** Infinite Looping (if exit condition for Diagnosis phase is ambiguous); Static Phase Locking (if RL policy is under-trained and agent gets stuck in Inquiry mode).
- **First 3 experiments:** 1. Turn Distribution Analysis: Reproduce Figure 3(a) to verify persistence increase. 2. Ablation on Hierarchy: Strip away hierarchical action set to test if phase blending re-emerges. 3. Retrospective Stress Test: Feed case requiring backtracking to verify "Retrospective Action" successfully triggers.

## Open Questions the Paper Calls Out

- To what extent does the introduction of specialized multi-modal agents improve diagnostic accuracy compared to the current text-based integration of multi-modal data? The conclusion states future work will extend the framework by introducing more agents, particularly multi-modal agents.

- How do competitive mechanisms between agents influence the system's resilience to "premature closure" compared to purely cooperative flows? The conclusion proposes exploring agent interactions, collaboration, and even competition among the agents.

- Does the reliance on GPT-4o as an automated evaluator introduce a systematic bias that inflates performance scores relative to human clinical judgment? The authors note that using GPT-4o as the evaluator results in slightly different baseline results compared to the original benchmark.

- Is the static hierarchical action set derived from medical textbooks sufficient for diagnosing rare or evolving conditions not explicitly detailed in standard texts? Section 2.2 describes formulating the action set by summarizing medical textbooks, implying a fixed knowledge base.

## Limitations

- RL implementation details remain ambiguous, with unclear distinction between trainable policy network, prompting strategy, or constrained action space itself.
- The reward function logic and scalar values for optimizing the agent are not defined, critical for faithful reproduction.
- Memory retrieval mechanism's effectiveness is weakly supported with limited direct evidence for specific architectures.

## Confidence

- **High Confidence:** Hierarchical action set mechanism effectively reduces reasoning errors by constraining solution space (well-supported by abstract and section 2.3).
- **Medium Confidence:** RL regularization mitigates premature closure by optimizing for long-term diagnostic accuracy (supported but lacks detailed reward function definition).
- **Low Confidence:** Multi-agent memory retrieval mechanism's effectiveness is weakly supported (lacks direct evidence for specific memory architectures).

## Next Checks

1. Reproduce Figure 3(a) to verify if persistence (turn count) has increased for the RL-driven agent compared to baseline.
2. Strip away the hierarchical action set and allow the LLM to act freely to test if phase blending re-emerges.
3. Feed the system a case requiring backtracking to the Inquiry phase after Examination to verify if "Retrospective Action" successfully triggers.