---
ver: rpa2
title: An open dataset of neural networks for hypernetwork research
arxiv_id: '2507.15869'
source_url: https://arxiv.org/abs/2507.15869
tags:
- neural
- networks
- dataset
- learning
- hypernetworks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the first open dataset of 10,000 neural networks
  designed for hypernetwork research, where a higher-order model generates weights
  for another neural network. The dataset contains LeNet-5 binary classifiers trained
  to distinguish one Imagenette class from all others, organized into 10 classes with
  1,000 networks each.
---

# An open dataset of neural networks for hypernetwork research

## Quick Facts
- arXiv ID: 2507.15869
- Source URL: https://arxiv.org/abs/2507.15869
- Authors: David Kurtenbach; Lior Shamir
- Reference count: 19
- Primary result: 10,000 LeNet-5 binary classifiers trained to distinguish one Imagenette class from all others, achieving 91.5% average accuracy

## Executive Summary
This paper introduces the first open dataset of 10,000 neural networks specifically designed for hypernetwork research, where a higher-order model generates weights for another neural network. The dataset contains LeNet-5 binary classifiers trained to distinguish one Imagenette class from all others, organized into 10 classes with 1,000 networks each. A 10,000-core computing cluster was used to generate the dataset, which achieved 91.5% average accuracy. Classification experiments demonstrated that these networks could be accurately categorized by their weights using machine learning methods, with Naive Bayes achieving 72% accuracy—far exceeding random chance. This indicates distinct weight patterns exist across networks that can be identified algorithmically. The dataset provides essential infrastructure for developing hypernetworks and generative AI models that produce neural networks, addressing the current lack of research resources in this area.

## Method Summary
The dataset consists of 10,000 LeNet-5 binary classifiers, each trained to distinguish one Imagenette class from all others (one-vs-all setup). For each of the 10 Imagenette classes, 1,000 networks were trained on different random subsets of negative samples. Each network sees different random negative samples, creating intra-class diversity while maintaining inter-class separability. The LeNet-5 architecture contains 91,481 total parameters across three convolutional layers and two dense layers. Networks were trained for 25 epochs each, with weights saved in HDF5 format. The dataset is organized in two formats: "modelwise" (all weights per model as flattened tensor) and "layerwise" (weights grouped by layer across class). Classification experiments used 70/30 train/test splits with various algorithms including Naive Bayes, DNNs, Random Forest, and SVM.

## Key Results
- 10,000 LeNet-5 binary classifiers trained to distinguish one Imagenette class from all others
- 91.5% average binary classification accuracy across all networks
- 72% accuracy in classifying networks by their weights using Naive Bayes (vs 10% random chance)
- Jensen-Shannon divergence analysis shows layer-specific pattern differences, with dense layers showing higher divergence (up to 0.24) than convolutional layers (0.02-0.04)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Weight distributions encode distinguishable information about the task a network was trained to solve.
- Evidence: Naive Bayes classifier achieves 72% accuracy on 10-class problem (vs 10% random chance), indicating task identity is recoverable from weights alone. Weights in neural networks are largely independent values, making Naive Bayes assumptions valid.
- Break condition: If weights from different architectures or more complex tasks show overlapping distributions without separability, classification-based validation fails.

### Mechanism 2
- Claim: Layer depth correlates with weight distribution diversity across classes.
- Evidence: JS divergence analysis shows early convolutional layers have similar distributions across classes (0.02-0.04), while dense layers show higher divergence (up to 0.24). Convolutional layers employ weight sharing and focus on spatial features, compressing the distribution.
- Break condition: If deeper architectures show inverted patterns (e.g., batch normalization collapses dense layer diversity), this mechanism would not guide architecture selection.

### Mechanism 3
- Claim: Controlled dataset construction enables learning to generate weights conditioned on task identity.
- Evidence: By fixing architecture and source dataset while varying only the binary classification target, the dataset isolates task-specific weight variation. This structure supports conditional generative models that take task identity as input and output weights.
- Break condition: If generated weights fail to produce functional networks (performance near random), the dataset-to-hypernetwork pipeline is incomplete.

## Foundational Learning

- **Hypernetworks (meta-networks generating weights)**: Why needed here: The entire dataset purpose is to enable training networks that output other networks' weights. Without understanding that hypernetworks conditionally generate parameters, the dataset design seems arbitrary. Quick check: Can you explain why a hypernetwork might be more efficient than training a separate network per task?
- **Binary one-vs-all classification**: Why needed here: Each network in the dataset solves a binary problem (target class vs. random others), not multi-class. This design choice affects how weight patterns relate to task identity. Quick check: How does the negative class construction (random samples from all other classes) affect what the network learns?
- **Weight space vs. activation space analysis**: Why needed here: This work analyzes static weights, not forward-pass activations. Understanding the distinction clarifies why Naive Bayes works here (weight independence) but might fail on activation-based representations. Quick check: Why might weight distributions be more independent than activation distributions?

## Architecture Onboarding

- **Component map**: Imagenette V2 → binary split (target class vs. random others) → LeNet-5 training (25 epochs) → weight extraction → HDF5 storage
- **Data formats**: Two organization modes—"modelwise" (all weights per model as flattened tensor, 91,481 params) and "layerwise" (weights grouped by layer across class)
- **Critical path**: 1) Download dataset from HuggingFace, 2) Load weights in modelwise format for classification tasks, layerwise for analyzing distribution patterns, 3) Baseline with Naive Bayes (expect ~72% accuracy); DNNs may underperform due to overfitting on high-dimensional weight data, 4) For hypernetwork training: condition on task class, target weight generation
- **Design tradeoffs**: LeNet-5 simplicity (91K params vs. ResNet-50's 2M+) enables dataset generation but limits complexity of learnable tasks; binary vs. multi-class simplifies problem but doesn't test whether hypernetworks can generate weights for more complex decision boundaries; fixed architecture controls variables but prevents architecture-search applications
- **Failure signatures**: Classification accuracy near 10% (random chance): weights lack task-specific patterns—check data loading; DNN significantly outperforming Naive Bayes: may indicate overfitting or data leakage; Generated networks achieving <60% accuracy: hypernetwork not capturing functional weight patterns
- **First 3 experiments**: 1) Replicate Naive Bayes classification (72% baseline) on modelwise weights to validate data pipeline, 2) Analyze layer-wise JS divergence for a subset of classes to confirm convolutional vs. dense layer patterns, 3) Train a simple conditional GAN: input = task class embedding, output = 91,481-dim weight vector; evaluate generated networks on held-out Imagenette images

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can modified Generative Adversarial Networks (GANs) be trained on this dataset to synthesize functional neural networks?
- Basis in paper: [explicit] The authors state, "Future work will also include the development of GANs that can generate neural networks," noting that such GANs will require architectural modifications to handle weight generation.
- Why unresolved: The paper focuses on providing the dataset infrastructure; no generative models were trained or evaluated in the current study.
- What evidence would resolve it: The successful training of a GAN that outputs model weights which achieve non-trivial accuracy on the Imagenette classification task without explicit backpropagation training.

### Open Question 2
- Question: Do the class-specific weight distribution patterns identified in LeNet-5 generalize to deeper, more complex architectures like ResNet?
- Basis in paper: [explicit] The conclusion lists "benchmarks... includ[ing] other common architectures such as ResNet" as future work, acknowledging the current limitation to a single, simple architecture.
- Why unresolved: The current dataset is restricted to LeNet-5 to avoid the "curse of dimensionality" and excessive computational costs associated with deeper networks.
- What evidence would resolve it: Generating a comparable dataset of deeper networks and demonstrating that they can be classified by their weights with accuracy significantly exceeding random chance.

### Open Question 3
- Question: Can deep learning classifiers be optimized to overcome high dimensionality and outperform Naive Bayes in identifying network classes?
- Basis in paper: [inferred] The results show a Deep Neural Network (DNN) achieved only 19% accuracy compared to 72% for Naive Bayes, which the authors attribute to the challenges of high dimensionality and overfitting.
- Why unresolved: The authors highlight the failure of the DNN as a capture of the subfield's challenges but do not offer a solution to improve deep learning performance on weight data.
- What evidence would resolve it: A deep learning architecture designed for weight spaces that surpasses the 72% accuracy baseline established by Naive Bayes.

## Limitations
- Dataset construction methodology provides controlled isolation but real-world applicability to more complex architectures remains unproven
- Binary one-vs-all classification setup may not generalize to multi-class or more nuanced decision boundaries
- Computational intensity of generating 10,000 networks (requiring 10,000-core cluster) limits accessibility for smaller research groups

## Confidence
- **High confidence**: Dataset construction methodology is clearly specified and reproducible; classification results (72% Naive Bayes accuracy vs 10% random chance) directly follow from experimental design and are statistically significant
- **Medium confidence**: JS divergence analysis showing layer-wise pattern differences is methodologically sound but lacks broader validation across different architectures or datasets
- **Medium confidence**: Claim that this dataset enables hypernetwork research is supported by presence of identifiable patterns, but actual hypernetwork performance using this data remains to be demonstrated

## Next Checks
1. Replicate the classification pipeline with Naive Bayes on the modelwise weight format to verify the 72% baseline accuracy
2. Perform layer-wise JS divergence analysis on a subset of classes to confirm the convolutional vs. dense layer pattern differences reported in Table 3
3. Train a simple conditional generative model (GAN or VAE) using task class as input and weights as output, then evaluate generated networks on held-out Imagenette data to verify functional weight patterns