---
ver: rpa2
title: 'Bigger But Not Better: Small Neural Language Models Outperform Large Language
  Models in Detection of Thought Disorder'
arxiv_id: '2503.20103'
source_url: https://arxiv.org/abs/2503.20103
tags:
- window
- sliding
- language
- clinical
- linguistic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Small neural language models (70M-12B parameters) outperform large
  language models (405B parameters) in detecting thought disorder in speech. Using
  sliding-window perplexity measures, smaller models achieved higher correlations
  with clinical ratings of disorganized thinking than their larger counterparts across
  both monologue and clinical interview speech samples.
---

# Bigger But Not Better: Small Neural Language Models Outperform Large Language Models in Detection of Thought Disorder

## Quick Facts
- **arXiv ID**: 2503.20103
- **Source URL**: https://arxiv.org/abs/2503.20103
- **Reference count**: 40
- **Primary result**: Small neural language models (70M-12B parameters) outperform large language models (405B parameters) in detecting thought disorder in speech

## Executive Summary
This study demonstrates that smaller language models are more effective than larger models at detecting formal thought disorder (FTD) in speech transcripts. Using perplexity-based measures with sliding windows, models in the 70M-1.4B parameter range achieved higher correlations with clinical ratings of disorganized thinking than their larger counterparts. The findings challenge the assumption that bigger models are universally better, suggesting that constrained model capacity may actually enhance sensitivity to linguistic anomalies associated with thought disorder.

## Method Summary
The researchers computed sliding-window perplexity (8-128 tokens) on speech transcripts using pre-trained Pythia models (70M-12B parameters) and LLaMA-3.1-405B. They compared maximum and mean perplexity values against clinical ratings of disorganization from two datasets: audio diary recordings (TALD scores) and clinical interviews (PANSS scores). Spearman correlations measured the monotonic relationship between perplexity and clinical severity, with optimal performance found at 64-token windows and smaller model sizes.

## Key Results
- Smaller models (70M-1.4B parameters) showed higher correlations with clinical ratings than larger models (6.9B-405B)
- 64-token sliding windows produced the strongest correlations across both datasets
- Maximum sliding-window perplexity outperformed mean perplexity for detecting FTD severity
- The 1.4B parameter model achieved correlation coefficients up to 0.486 on audio diary data

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Smaller language models exhibit heightened sensitivity to linguistic anomalies associated with formal thought disorder (FTD) compared to larger models.
- Mechanism: Constrained model capacity (70M-12B parameters) limits exposure to diverse linguistic patterns, reducing the model's ability to "normalize" atypical speech. Higher perplexity on disorganized speech reflects genuine difficulty predicting linguistic violations that larger models—trained on vast, heterogeneous corpora—may assimilate as plausible continuations.
- Core assumption: FTD-related linguistic disruptions (tangentiality, derailment) present as low-probability sequences in standard language distributions, which capacity-constrained models register as higher perplexity spikes.
- Evidence anchors:
  - [abstract] "smaller models are more sensitive to linguistic differences associated with formal thought disorder than their larger counterparts"
  - [section 1] "smaller LMs' more limited exposure to coherent language patterns and/or constrained capacity (as proxied by parameter size) may paradoxically enhance their sensitivity"
  - [corpus] Weak direct corpus support; neighbor papers address schizophrenia biomarkers and small model efficiency but not the inverse scaling phenomenon specifically.
- Break condition: If larger models are fine-tuned on clinical/disordered speech data, or if FTD speech patterns fall within the distribution of web-scale training data, the sensitivity advantage may disappear.

### Mechanism 2
- Claim: Sliding-window perplexity (16-64 tokens) captures local coherence disruptions characteristic of FTD better than global or longer-window measures.
- Mechanism: FTD manifests as intermittent incoherence at phrasal/clause scales. A 64-token window aligns with the approximate span where contextual inconsistencies become detectable; longer windows (128+ tokens) dilute local anomaly signals by averaging over more coherent surrounding context.
- Core assumption: FTD involves selective impairment in using local (proximal) linguistic context while global discourse processing may remain relatively intact.
- Evidence anchors:
  - [abstract] "strongest correlations occurred with 64-token sliding windows"
  - [section 4.2.1] "correlation coefficients generally increase with a sliding window of 8 and 16, peak at a sliding window of 64, and then decrease with a sliding window of 128"
  - [corpus] "Reading Between the Lines" paper (arXiv:2507.13551) similarly combines semantic coherence with pause dynamics for FTD assessment, supporting multi-scale local analysis.
- Break condition: If speech samples are very short (<64 tokens) or if FTD manifests primarily at discourse-level rather than local coherence, window size optimization may shift.

### Mechanism 3
- Claim: Maximum sliding-window perplexity across a transcript outperforms averaged perplexity for detecting clinically-rated disorganization.
- Mechanism: FTD produces sporadic "spikes" of incoherence amid otherwise fluent speech. Maximum PPL captures these peak disruptions; averaging smooths them out, reducing signal-to-noise ratio for intermittent pathology.
- Core assumption: FTD severity correlates with the magnitude of the most incoherent segments rather than average coherence across the full sample.
- Evidence anchors:
  - [section 3.3] "Transcripts rated with TALD ≥ 3 consistently exhibit higher maximum sliding window PPL spikes"
  - [section 4.2.1] Maximum PPL correlations (ρ up to 0.486) substantially exceeded averaged PPL correlations (ρ up to 0.251) on the AVH dataset.
  - [corpus] No direct corpus neighbor addresses max vs. mean aggregation strategies.
- Break condition: If a speaker has uniformly disorganized speech (no local spikes), or if the transcript contains non-FTD-related high-PPL regions (code-switching, rare vocabulary), maximum PPL may yield false positives.

## Foundational Learning

- Concept: **Perplexity as a measure of sequence predictability**
  - Why needed here: The entire detection framework rests on interpreting perplexity values—the model's surprise at each token given context. Higher perplexity indicates greater deviation from expected linguistic patterns.
  - Quick check question: Given a language model trained on standard English text, would you expect higher or lower perplexity on a transcript containing frequent topic derailments?

- Concept: **Autoregressive vs. bidirectional language models**
  - Why needed here: The paper uses decoder-only autoregressive models (Pythia, LLaMA) which predict tokens sequentially using only leftward context. This differs from bidirectional models like BERT, which use both directions and may be less suited for modeling production-order coherence disruptions.
  - Quick check question: Why might an autoregressive model be more appropriate than a masked language model for detecting thought disorder in speech production?

- Concept: **Spearman correlation for ordinal clinical ratings**
  - Why needed here: Clinical scales (TALD 0-4, composite PANSS) are ordinal, not continuous. Spearman's ρ captures monotonic relationships without assuming linearity or normal distribution.
  - Quick check question: If PPL increases monotonically with TALD score but the relationship is non-linear, would Pearson or Spearman correlation be more appropriate?

## Architecture Onboarding

- Component map:
  - Transcript preprocessing -> Tokenization -> Sliding-window PPL computation -> Maximum/mean aggregation -> Correlation analysis

- Critical path:
  1. Load transcript and tokenize
  2. For each model checkpoint, compute sliding-window PPLs at configured window sizes
  3. Extract max and mean PPL per transcript
  4. Compute Spearman correlations with clinical scores
  5. Identify optimal model size × window size combination

- Design tradeoffs:
  - **Model size vs. sensitivity**: Smaller models (70M-1.4B) offer better FTD detection but may struggle with other linguistic tasks; larger models (6.9B-12B) plateau or decline in correlation.
  - **Window size vs. granularity**: 64 tokens optimal for monologue data; conversational data shows more variability (16-64). Longer windows reduce sensitivity to local disruptions.
  - **Max vs. mean aggregation**: Max captures sporadic incoherence spikes; mean provides stable but weaker signal.

- Failure signatures:
  - **Flat correlations across model sizes**: May indicate dataset lacks FTD variability, or transcripts are too short/long for window calibration.
  - **Higher correlation with larger models**: Contradicts expected pattern; check for data leakage or clinical rating inconsistencies.
  - **Negative correlations**: PPL decreasing with severity suggests model overfitting to disorganized patterns or rating scale inversion.

- First 3 experiments:
  1. **Reproduce the 64-token window finding**: Run Pythia-1.4B on the AVH dataset with window sizes [8, 16, 32, 64, 128]; confirm peak correlation at 64 tokens.
  2. **Ablate aggregation method**: Compare max vs. mean PPL correlations on both datasets; expect max to outperform on AVH, more variable on clinical interviews.
  3. **Perturbation test**: Artificially introduce derailment-style text disruptions into control transcripts; verify PPL spikes localize to disrupted windows.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Is the advantage of smaller models in detecting thought disorder attributable to model size per se, or to the relatively constrained amount of training data?
- Basis in paper: [explicit] "it remains to be determined whether this advantage is attributable to model size or a relatively constrained amount of training data" (Discussion)
- Why unresolved: The Pythia suite varies in parameter count but uses identical training data, conflating these factors. LLaMA uses vastly more training data, making comparisons ambiguous.
- What evidence would resolve it: Training models of varying sizes on matched training corpora of different scales to disentangle the effects.

### Open Question 2
- Question: Can PPL-based measures serve as reliable indicators of change in clinical status over time for monitoring disease progression or treatment response?
- Basis in paper: [explicit] "we have not yet established... the utility of measurements over time as indicators of change in clinical status" (Limitations)
- Why unresolved: The study used cross-sectional data from two datasets, with no longitudinal follow-up to assess within-subject stability or sensitivity to clinical change.
- What evidence would resolve it: Longitudinal studies tracking PPL measures alongside clinical ratings during treatment or disease progression.

### Open Question 3
- Question: Would optimal parameter settings (model size, sliding window length) differ for populations with more severe FTD symptoms?
- Basis in paper: [explicit] "datasets with more representation of severe FTD may be needed to establish optimal parameter settings in this context" (Limitations)
- Why unresolved: The current datasets had relatively low mean severity scores (TALD mean=1.18; composite PANSS mean=3.36), limiting generalizability to severe cases.
- What evidence would resolve it: Replication with datasets specifically enriched for severe FTD presentations to determine if the 1.4B/64-token optimum holds.

## Limitations

- Dataset availability: The audio diary and clinical interview transcripts are not publicly accessible, requiring author contact or IRB approval
- Sample size constraints: The clinical interview dataset contains only 39 transcripts, limiting statistical power
- Language restriction: Findings are based on English-language speech data with unknown performance in other languages

## Confidence

**High confidence**: The inverse scaling relationship between model size and FTD detection performance is well-supported by the experimental results. The correlation patterns (peak at 64-token windows, max aggregation outperforming mean) show consistent statistical significance across datasets.

**Medium confidence**: The generalizability of these findings to other clinical populations and speech contexts. While the study covers both monologue and dialogue, the sample sizes and specific patient populations may not capture the full spectrum of thought disorder presentations.

**Low confidence**: The mechanism explaining why larger models fail at this task. While the paper proposes that larger models' broader exposure to linguistic patterns "normalizes" atypical speech, the training corpus details for the models are not fully specified.

## Next Checks

1. **Cross-dataset validation**: Apply the optimal configuration (Pythia-1.4B, 64-token sliding window, max aggregation) to an independent thought disorder dataset with different clinical annotations. Verify that the same inverse scaling pattern emerges when comparing against multiple model sizes.

2. **Ablation of training data influence**: Test whether the observed pattern persists when models are fine-tuned on mixed corpora containing both typical and disordered speech samples. Compare performance against the zero-shot baseline to determine if the effect is due to distributional learning differences or model capacity constraints.

3. **Linguistic feature decomposition**: Extract and analyze the specific linguistic features (e.g., semantic coherence, syntactic complexity, lexical diversity) that contribute most strongly to perplexity differences between thought-disordered and typical speech. Use feature attribution methods to identify whether the model sensitivity is driven by local coherence breaks, vocabulary anomalies, or other linguistic markers.