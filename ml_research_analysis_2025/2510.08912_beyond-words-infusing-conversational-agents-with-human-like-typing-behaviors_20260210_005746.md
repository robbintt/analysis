---
ver: rpa2
title: 'Beyond Words: Infusing Conversational Agents with Human-like Typing Behaviors'
arxiv_id: '2510.08912'
source_url: https://arxiv.org/abs/2510.08912
tags:
- agent
- typing
- participants
- user
- conversational
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores the impact of human-like typing behaviors on
  user perceptions of conversational AI agents. The authors implemented a platform
  that simulates hesitation and self-editing behaviors in AI-generated responses,
  allowing users to customize the communication style.
---

# Beyond Words: Infusing Conversational Agents with Human-like Typing Behaviors

## Quick Facts
- arXiv ID: 2510.08912
- Source URL: https://arxiv.org/abs/2510.08912
- Authors: Jijie Zhou; Yuhan Hu
- Reference count: 40
- Key outcome: Self-editing behaviors in AI agents increased perceived intelligence and human-likeness, with hesitation-only agents rated as "dumb" and "robotic"

## Executive Summary
This paper explores how human-like typing behaviors (hesitation and self-editing) affect user perceptions of conversational AI agents. The authors implemented a Flutter-based platform that simulates these behaviors through a "Typing Simulator" middleware that injects artificial delays and text modifications into AI responses. A within-subject study with 11 participants compared three agents: baseline (rapid typing), hesitation-only (slowed typing), and self-editing (typing delays plus visible corrections). Results showed the self-editing agent was slightly preferred for naturalness, human-likeness, and trustworthiness, while hesitation-only agents were rated poorly. The study suggests that incorporating self-editing behaviors can enhance user perceptions of conversational AI, making them feel more authentic and competent.

## Method Summary
The study used a Flutter-based cross-platform implementation with GPT-3.5 (text-davinci-003) via OpenAI API for response generation. Three agent configurations were tested: baseline (rapid typing, no edits), hesitation-only (slowed typing with pauses), and hesitation+self-edit (typing delays plus random deletion/insertion/modification at word and sentence levels). The Typing Simulator middleware injected artificial "Actions" (Delete, Insert, Pause) based on configurable probability parameters. A counterbalanced within-subject design had participants engage in five-message hobby discussions with each agent type, collecting 5-point Likert scale ratings on various dimensions and preference rankings. The study focused on casual conversation contexts with young adult participants.

## Key Results
- Agents with both hesitation and self-editing behaviors were slightly preferred by participants over baseline and hesitation-only agents
- Self-editing capability was particularly appreciated as an indicator of intelligence and error-correction ability
- Hesitation-only agents were deemed "dumb" and "robotic," receiving less favorable assessments in naturalness and human-likeness
- Participants found the self-editing agent more natural, human-like, and trustworthy compared to the baseline

## Why This Works (Mechanism)

### Mechanism 1: Self-Correction as a Proxy for Competence
Visible self-editing behaviors may increase perceived competence and human-likeness compared to instant, perfect responses, provided the final output is accurate. Users interpret the process of correction as a signal of active error-checking and thoughtfulness, mimicking human cognitive "double-checking."

### Mechanism 2: Semantic Justification for Latency
Hesitation (typing delay) negatively impacts user perception unless paired with a semantic reason for the delay, such as self-editing. Latency alone signals system lag, but latency coupled with visible editing signals complexity and care, transforming a temporal cost into a semantic benefit.

### Mechanism 3: Anthropomorphism via Imperfection
Simulating human imperfections (pauses, editing) reduces the "uncanny" perfection of LLMs, fostering trust through perceived genuineness. Introducing "flaws" in the delivery (not the content) aligns the agent's behavior with the user's mental model of a human interlocutor.

## Foundational Learning

- **Concept: Streaming vs. Simulated Typing**
  - Why needed here: The system receives the full payload and then animates the output, rather than rendering tokens as they arrive from the LLM
  - Quick check question: Does the system render tokens as they arrive from the LLM, or does it receive the full payload and then animate the output?

- **Concept: Social Presence Theory**
  - Why needed here: Social cues (like typing rhythm) increase "social presence," making the AI feel like a partner rather than a tool
  - Quick check question: Why would a user prefer a slower, self-correcting agent over a faster, accurate one?

- **Concept: UI/UX Latency Masking**
  - Why needed here: The "Green" agent failed because delay alone felt like lag, while the "Red" agent used self-editing to mask the "cost" of the delay
  - Quick check question: How does the system transform a temporal cost (waiting) into a semantic benefit?

## Architecture Onboarding

- **Component map:** User Input -> Prompt Construction -> LLM API Call -> Text Post-Processing (Inject Self-Edits) -> Animation Queue -> UI Render
- **Critical path:** User Input -> Prompt Construction -> LLM API Call -> **Text Post-Processing (Inject Self-Edits)** -> Animation Queue -> UI Render
- **Design tradeoffs:** Realism vs. Confusion (edits may look like glitches if nonsensical); Efficiency vs. Engagement (slower delivery for social bonding)
- **Failure signatures:** "Ghost" Edit (identical word replacement); Context Drift (contradictory sentence generation); Synonym Mismatch (inappropriate word substitutions)
- **First 3 experiments:** 1) Validate "Red > Green" Hypothesis with basic delay vs. delay+edit comparison; 2) Parameter Sweeps for characterTypingPace and spaceLagPace; 3) Edit Logic Audit for contextual appropriateness

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do users prefer interacting with chat agents whose typing styles mimic their own specific behaviors?
- Basis in paper: [explicit] Authors aspire to collect user typing behavior data to investigate whether users prefer agents whose typing styles align with their own
- Why unresolved: Current study applied generic, randomized parameters rather than personalizing to individual users
- What evidence would resolve it: A study profiling participants' typing patterns and comparing satisfaction between personalized vs. generic agents

### Open Question 2
- Question: How does allowing typographical errors to remain uncorrected impact the perception of human-likeness?
- Basis in paper: [explicit] Authors suggest investigating the impact of allowing some errors to remain uncorrected as part of a mixed model
- Why unresolved: Current implementation always displayed agent self-correcting errors, never leaving mistakes visible
- What evidence would resolve it: An experiment comparing agents that always correct errors versus those that occasionally leave typos

### Open Question 3
- Question: Can hesitation and self-editing behaviors effectively translate to voice-based conversational agents?
- Basis in paper: [explicit] Authors propose extending research to voice generation to examine how variations in tone and pauses affect realism
- Why unresolved: Study focused exclusively on visual text interfaces
- What evidence would resolve it: A follow-up study implementing auditory hesitation and self-correction in a voice assistant

## Limitations

- Critical parameter values (typing pace, pause durations, edit rates) are not disclosed, making faithful reproduction difficult
- Only 11 participants (primarily young adults) limits generalizability to broader populations
- Study focuses on casual hobby discussions, potentially not transferring to task-oriented or high-stakes conversational scenarios
- Vocabulary library for redundant word deletion is not provided, raising questions about contextual appropriateness of simulated edits

## Confidence

- **High confidence**: Finding that hesitation-only agents are perceived as "dumb" and "robotic" - aligns with well-established latency perception research
- **Medium confidence**: Preference for self-editing behaviors indicating intelligence - effect size is small and dependent on edit quality
- **Medium confidence**: Mechanism that semantic justification masks latency costs - plausible but not extensively validated beyond this study

## Next Checks

1. **Parameter sensitivity analysis**: Systematically vary characterTypingPace, spaceLagPace, and self-edit rates to identify optimal ranges that maximize naturalness without causing frustration
2. **Edit quality audit**: Implement a human evaluation pipeline to assess whether simulated corrections are contextually appropriate and enhance rather than detract from message quality
3. **Task-domain generalization test**: Replicate the study with task-oriented conversations (information seeking, problem-solving) to verify whether self-editing benefits persist across interaction types