---
ver: rpa2
title: Learning Sparse Visual Representations via Spatial-Semantic Factorization
arxiv_id: '2602.01905'
source_url: https://arxiv.org/abs/2602.01905
tags:
- sparse
- image
- stellar
- learning
- tokens
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the tension between semantic understanding
  and image reconstruction in self-supervised learning by proposing a sparse representation
  that factorizes visual features into semantic concepts and their spatial distributions.
  The method disentangles the "what" (semantic tokens) from the "where" (spatial localization
  matrix) using a low-rank matrix factorization, enabling both pixel-level reconstruction
  and rich semantics.
---

# Learning Sparse Visual Representations via Spatial-Semantic Factorization

## Quick Facts
- **arXiv ID**: 2602.01905
- **Source URL**: https://arxiv.org/abs/2602.01905
- **Reference count**: 26
- **Primary result**: Achieves ImageNet linear probing accuracy of 79.10% and FID reconstruction score of 2.60 using only 16 sparse semantic tokens, reducing latent representation size by 90%.

## Executive Summary
This paper introduces a sparse representation method for self-supervised learning that factorizes visual features into semantic concepts and their spatial distributions. The approach disentangles "what" (semantic tokens) from "where" (spatial localization matrix) using low-rank matrix factorization, enabling both pixel-level reconstruction and rich semantic understanding. The model demonstrates strong performance with only 16 sparse tokens, outperforming baselines in both reconstruction quality and semantic accuracy while significantly reducing computational requirements.

## Method Summary
The method factorizes visual features into a semantic matrix S (r×d) representing learned concepts and a spatial localization matrix L (n×r) encoding where these concepts appear in the image. The factorization Z=LS enables sparse representations while maintaining reconstruction fidelity. The encoder processes images through a ViT backbone with learnable latent queries, extracting sparse tokens and computing localization via softmax-cosine similarity. A 6-layer ViT decoder reconstructs the image, and training uses multiple objectives including reconstruction loss, clustering with Sinkhorn-Knopp, optimal transport alignment, and KoLeo regularization. The approach is trained on ImageNet-1K with 150-50 epochs depending on model size.

## Key Results
- Achieves 79.10% linear probing accuracy on ImageNet-1K with only 16 sparse tokens
- Reaches FID reconstruction score of 2.60, demonstrating high-fidelity image reconstruction
- Reduces latent representation size by 90% compared to dense representations
- Outperforms baselines in both reconstruction and semantic tasks across multiple benchmarks

## Why This Works (Mechanism)
The factorization approach works by separating semantic content from spatial information, allowing each to be optimized independently while maintaining their relationship through the low-rank constraint. This enables the model to capture rich semantics with very few tokens by focusing computational resources on meaningful concepts rather than dense pixel-level features. The spatial localization matrix ensures accurate reconstruction by encoding precise spatial distributions of each semantic concept.

## Foundational Learning
- **Sinkhorn-Knopp Algorithm**: Iterative method for computing optimal transport with entropy regularization. Needed for stable clustering and alignment objectives. Quick check: Verify convergence within 3-5 iterations on prototype assignments.
- **Optimal Transport**: Mathematical framework for measuring distances between probability distributions. Required for set alignment between predicted and ground truth token distributions. Quick check: Compare Sinkhorn OT runtime vs Hungarian algorithm on sample batch.
- **Matrix Factorization**: Decomposition of a matrix into multiple matrices. Essential for separating semantic and spatial components while maintaining reconstruction capability. Quick check: Verify rank constraint by checking singular values of localization matrix.

## Architecture Onboarding
- **Component map**: Image -> ViT Backbone -> Latent Queries + Dense Features -> S (Semantic Matrix) + U (Dense Features) -> L (Localization Matrix) -> D(L·S) (Reconstruction)
- **Critical path**: Encoder processes image and queries → extracts semantic tokens S and dense features U → computes localization matrix L via softmax-cosine similarity → decoder reconstructs from L·S
- **Design tradeoffs**: Sparse vs dense representations (90% reduction vs potential information loss), learned vs handcrafted concepts (adaptability vs interpretability), joint vs separate optimization (computational efficiency vs modularity)
- **Failure signatures**: Feature collapse without alignment loss, empty clusters during training, slow matching without efficient OT implementation
- **First experiments**: 1) Verify forward pass produces valid factorization with L·1_r=1_n constraint, 2) Test clustering stability with Sinkhorn-Knopp on prototype assignments, 3) Validate reconstruction quality with varying token counts

## Open Questions the Paper Calls Out
- None specified in the provided material

## Limitations
- Loss weights and temperature parameters are not explicitly specified, introducing variability in reproducibility
- Decoder target specification is ambiguous regarding MaskGIT-VQGAN tokens vs pixel reconstruction
- Augmentation strength parameters lack precise definitions for color jitter and Gaussian blur
- Computational requirements include substantial GPU resources (16×A100-80GB) for training

## Confidence
- **Reported performance claims**: Medium (detailed ablations provided but missing key hyperparameters)
- **Computational efficiency claims**: Medium (plausible given sparse design but unverified independently)
- **Method reproducibility**: Medium (clear architecture but significant unknowns in training configuration)

## Next Checks
1. Reproduce the 79.10% linear probing accuracy and FID 2.60 reconstruction on ImageNet-1K using the specified r=16 tokens and training settings, monitoring for feature collapse without set alignment loss
2. Validate the computational efficiency by measuring training time per batch and decoder complexity when using 16 tokens versus a dense ViT baseline
3. Test the ablation of Sinkhorn-Knopp clustering and OT alignment to confirm their necessity for avoiding empty clusters and ensuring stable matching