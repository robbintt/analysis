---
ver: rpa2
title: 'WorldGen: From Text to Traversable and Interactive 3D Worlds'
arxiv_id: '2511.16825'
source_url: https://arxiv.org/abs/2511.16825
tags:
- scene
- generation
- image
- inproc
- scenes
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: WorldGen is a system for generating interactive 3D worlds from
  text prompts. It bridges the gap between natural language and functional 3D scenes
  by combining LLM-driven scene planning, procedural generation, image-to-3D reconstruction,
  and compositional scene decomposition.
---

# WorldGen: From Text to Traversable and Interactive 3D Worlds

## Quick Facts
- arXiv ID: 2511.16825
- Source URL: https://arxiv.org/abs/2511.16825
- Reference count: 40
- Generates coherent, traversable, game-engine-ready 3D worlds in ~5 minutes from text prompts

## Executive Summary
WorldGen is a system that generates interactive 3D worlds from text prompts by bridging natural language and functional 3D scenes. It combines LLM-driven scene planning, procedural generation, image-to-3D reconstruction, and compositional scene decomposition. The system produces coherent, navigable, and game-engine-ready 3D worlds in about five minutes, outperforming prior image-to-3D models by 40-50% in navmesh alignment. WorldGen enables rapid, accessible world creation for gaming and immersive applications.

## Method Summary
WorldGen uses a four-stage pipeline: (1) An LLM parses text prompts into structured JSON parameters that drive a rule-based procedural generator, creating a "blockout" geometry and navigation mesh (navmesh) to ensure walkable layouts. (2) A depth map from the blockout guides a diffusion model to generate a reference image. (3) An image-to-3D model (AssetGen2) is fine-tuned to reconstruct geometry conditioned on both the reference image and navmesh, ensuring generated geometry respects walkable paths. (4) The coarse mesh is decomposed into editable objects, which are enhanced individually using vision-language models for higher detail and texture quality while maintaining global consistency.

## Key Results
- Generates coherent, traversable 3D worlds from text prompts in ~5 minutes
- Outperforms prior image-to-3D models by 40-50% in navmesh alignment
- Produces game-engine-ready assets with high-quality textures and editable components

## Why This Works (Mechanism)

### Mechanism 1
Text-to-procedural mapping preserves navigability where purely data-driven models fail. An LLM parses natural language into structured JSON parameters that drive a rule-based procedural generator, creating a "blockout" and navmesh before visual details are synthesized. This guarantees structural constraints like walkability are met. Core assumption: The rule-based procedural generator can cover the semantic range of typical user prompts. Break condition: If the prompt describes a surreal scenario (e.g., "M.C. Escher stairs") that violates the procedural generator's rule set, the mechanism will likely flatten or misinterpret the geometry.

### Mechanism 2
Navmesh conditioning aligns generative geometry with functional traversability. The system fine-tunes AssetGen2 to accept the navmesh as a conditioning input alongside the reference image. By encoding the navmesh as point clouds and injecting them via cross-attention, the diffusion model prioritizes geometry that respects pre-defined walkable paths. Core assumption: AssetGen2 is sufficiently flexible to incorporate sparse geometric priors without degrading visual quality. Break condition: If the reference image and navmesh possess irreconcilable conflicts (e.g., image shows a solid wall where navmesh dictates a door), the model may generate visual artifacts.

### Mechanism 3
Decomposition and hallucinative upscaling recover high-frequency details lost in holistic reconstruction. The coarse holistic mesh is segmented into objects, and for each object, a VLM generates a new high-resolution image conditioned on global style and coarse geometry. A mesh refinement model then rebuilds the object geometry based on this enhanced image, constrained by the original coarse shape. Core assumption: The decomposition model can separate the unified mesh into semantically meaningful parts without bleeding texture between neighbors. Break condition: If the VLM hallucinates details that fundamentally alter the object's silhouette, the subsequent mesh refinement might produce geometry that collides with neighboring assets.

## Foundational Learning

- **Concept: Navigation Meshes (Navmesh)** - Why needed: Unlike convex hulls or bounding boxes, a navmesh precisely defines traversable surface topology, acting as "ground truth" constraint for the diffusion model. Quick check: How does a navmesh differ from a collision mesh in a game engine, and why is it sufficient for conditioning a generative model?

- **Concept: Latent 3D Diffusion (VecSet)** - Why needed: AssetGen2 represents scenes as sets of latent vectors rather than voxels or implicit functions. Understanding this representation is crucial for comprehending how navmesh conditioning is injected. Quick check: Why might a set-of-vectors representation handle sparse conditioning better than a fixed-grid voxel representation?

- **Concept: Procedural Content Generation (PCG)** - Why needed: The first stage relies on PCG to turn LLM outputs into valid geometry. You must understand that PCG is rule-based, not probabilistic, which ensures structurally sound but stylistically generic output. Quick check: What is the trade-off between using PCG for the layout versus using a generative model (like a GAN) for the same task?

## Architecture Onboarding

- **Component map:** Scene Planner: LLM (Prompt → JSON) → PCG Engine → Blockout (B) & Navmesh (S). Image Generator: Depth map (from B) → Diffusion Model → Reference Image (R). Holistic Recon: Reference Image (R) + Navmesh (S) → AssetGen2 (Fine-tuned) → Coarse Mesh (M). Decomposer: AutoPartGen → List of Objects. Enhancer: VLM (Image Gen) + Mesh Refiner → Final Assets.

- **Critical path:** The Navmesh conditioning in Stage II is the most sensitive step. If the normalization of the navmesh does not perfectly align with the latent space of AssetGen2, the generated geometry will "float" or clip through the floor.

- **Design tradeoffs:** Consistency vs. Editability: The system sacrifices the speed of a single-shot generator for a multi-stage pipeline to ensure objects are independent (editable) and textures are high-res. Hallucination control: Stage IV relies on the VLM to "hallucinate" details, improving aesthetics but risking semantic drift if verification is not robust.

- **Failure signatures:** Texture Seams: Visible seams in Stage IV if multi-view projection in texture generation fails. Drifting Layout: If the LLM misinterprets the prompt, PCG generates a valid but wrong layout. Collision Artifacts: Objects intersecting after Stage IV if the "coarse shape" constraint is too weak.

- **First 3 experiments:**
  1. Navmesh Ablation: Run Stage II with navmesh conditioning disabled. Measure Chamfer Distance of resulting walkable area against ground truth.
  2. Decomposition Stress Test: Feed highly cluttered scene (e.g., "junkyard") and measure if AutoPartGen correctly separates occluded objects.
  3. Visual Drift Check: Run Stage IV on a specific object (e.g., chair) and compare enhanced image output by VLM against original reference image.

## Open Questions the Paper Calls Out

- **Question 1:** How can the system be extended to generate large-scale open worlds exceeding the current ~50x50m limit? Basis: Single-view conditioning restricts scene scale and generating larger worlds requires stitching multiple regions. Unresolved: Stitching currently risks non-smooth transitions and visual artifacts at boundaries. Evidence needed: A technique for consistent region stitching or multi-scale generation strategy.

- **Question 2:** Can the single-view conditioning paradigm be modified to support multi-layered 3D environments? Basis: Single-view conditioning limits ability to model multi-floor dungeons or seamless interior-exterior transitions. Unresolved: A single image cannot fully capture geometry of occluded levels or internal rooms. Evidence needed: Pipeline incorporating multi-view or volumetric input constraints to resolve occluded, stacked geometries.

- **Question 3:** Can the pipeline incorporate geometry and texture reuse to maintain rendering efficiency in vast scenes? Basis: Independent object representation becomes inefficient for very large scenes. Unresolved: Current decomposition stage generates unique assets, leading to high memory usage. Evidence needed: Module that identifies geometric similarity across generated objects to implement automatic instancing or texture atlas sharing.

## Limitations
- Relies on two proprietary Meta models (AssetGen2 and AutoPartGen) not publicly available
- Limited analysis of visual consistency across enhancement stage and style drift
- High computational requirements with ~5 minute generation time per scene

## Confidence

**High Confidence:**
- Four-stage pipeline architecture is technically sound and logically coherent
- Navmesh conditioning significantly improves traversability compared to unconditioned generation
- Decomposition approach enables per-object enhancement while maintaining global consistency

**Medium Confidence:**
- 40-50% improvement in navmesh alignment is accurately reported
- Procedural generation approach reliably produces navigable layouts
- Enhancement stage successfully recovers high-frequency details

**Low Confidence:**
- Generalization to highly abstract or contradictory prompts
- Real-world performance with diverse scene types beyond tested examples
- Long-term stability when scaled to larger, more complex worlds

## Next Checks
1. **Navmesh Ablation Study:** Reproduce key experiment by running system with navmesh conditioning disabled and measuring resulting Chamfer Distance to quantify claimed 40-50% improvement.

2. **Style Consistency Analysis:** Systematically evaluate visual consistency across enhancement stage by measuring style drift metrics (e.g., CLIP similarity) between enhanced objects and original reference image, particularly for scenes with diverse object types.

3. **Prompt Robustness Testing:** Test system's limits by systematically varying prompt complexity, including contradictory instructions, abstract concepts, and surreal scenarios to identify failure modes and quantify reliability of LLM-guided procedural generation approach.