---
ver: rpa2
title: 'KCM: KAN-Based Collaboration Models Enhance Pretrained Large Models'
arxiv_id: '2510.20278'
source_url: https://arxiv.org/abs/2510.20278
tags:
- large
- small
- computational
- collaborative
- accuracy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes KCM (KAN-based Collaborative Models) to enhance
  pretrained large models through collaboration with small models. The key innovation
  is using Kolmogorov-Arnold Networks (KANs) instead of traditional MLPs for the small
  collaborative model, which offers superior interpretability and mitigates catastrophic
  forgetting.
---

# KCM: KAN-Based Collaboration Models Enhance Pretrained Large Models

## Quick Facts
- arXiv ID: 2510.20278
- Source URL: https://arxiv.org/abs/2510.20278
- Authors: Guangyu Dai; Siliang Tang; Yueting Zhuang
- Reference count: 0
- Key outcome: KCM framework reduces computational costs by routing simpler samples to KAN-based small model while reserving large model for complex cases, achieving 18-68% reduction in large model invocations across different tasks while maintaining comparable accuracy.

## Executive Summary
This paper proposes KCM (KAN-based Collaborative Models) to enhance pretrained large models through collaboration with small models. The key innovation is using Kolmogorov-Arnold Networks (KANs) instead of traditional MLPs for the small collaborative model, which offers superior interpretability and mitigates catastrophic forgetting. The framework reduces computational costs by routing simpler samples to the small model while reserving the large model for complex cases, achieving 18-68% reduction in large model invocations across different tasks while maintaining comparable accuracy.

## Method Summary
KCM uses a KAN-based judgment model to compute confidence scores for each input sample. Based on these scores compared against a threshold ε=0.98, samples are routed either to a KAN-based small collaborative model (for high-confidence samples) or to a frozen pretrained large model with prompt modification (for low-confidence samples). Knowledge distillation via KL-divergence transfers information from large to small model when the large model shows high confidence. The approach was validated across three scenarios: language (sentiment analysis on Amazon reviews), vision (long-tail image classification on CIFAR-100), and vision-language (image captioning on MSCOCO).

## Key Results
- KCM achieved 18-68% reduction in large model invocations across different tasks while maintaining comparable accuracy
- Outperformed MLP-based collaborative models (MCM) with up to 6.04% improvement on tail data
- Demonstrated lower computational overhead (6.3% fewer large model calls) compared to MCM
- Effectively addressed catastrophic forgetting in long-tail scenarios through KAN's edge-based learnable activations

## Why This Works (Mechanism)

### Mechanism 1: Confidence-Based Sample Routing
Routing simpler samples to small models while reserving complex samples for large models can reduce computational costs while maintaining accuracy. A judgment model computes confidence score Cx = eyi/Σeyn for each input. Samples with Cx > threshold ε are processed by the small model; others go to the large model. Core assumption: The small model's confidence score accurately reflects task difficulty and correlates with expected accuracy. Evidence: KCM framework reduces computational costs by routing simpler samples to the small model while reserving the large model for complex cases, achieving 18-68% reduction in large model invocations. Break condition: If confidence threshold ε is poorly calibrated, routing becomes suboptimal—either wasting computational resources or degrading accuracy on misrouted samples.

### Mechanism 2: KAN Architecture for Catastrophic Forgetting Mitigation
KAN's edge-based learnable activation functions help preserve rare patterns during knowledge distillation, reducing catastrophic forgetting in long-tail scenarios. Unlike MLPs with fixed activation functions at nodes, KAN places learnable activation functions on edges with summation at nodes. This enables adaptive feature reweighting for tail data during distillation. Core assumption: Edge-based learnable activations inherently resist catastrophic forgetting. Evidence: KAN's edge-based learnable activations enable adaptive feature reweighting for tail data, which mitigates catastrophic forgetting by preserving rare patterns during distillation—unlike MLPs' fixed node activations. Break condition: If the theoretical link between edge-based activations and forgetting prevention is weaker than assumed, gains may stem primarily from architectural capacity rather than the KAN mechanism itself.

### Mechanism 3: Bidirectional Knowledge Transfer
Small models can guide large models via prompt modification, while large models can improve small models via knowledge distillation. When samples route to the large model, low small-model confidence is encoded as a prompt modifier ("the confidence of the small model is Cx"), helping the large model deprioritize classes the small model handles well. When the large model has high confidence (Cl > ε), KL-divergence distillation updates the small model. Core assumption: Prompt modification meaningfully influences large model behavior; distillation effectively transfers knowledge without amplifying hallucinations. Evidence: For the data with low confidence Cx that are input into the large model...we can add a label similar to 'the confidence of the small model is Cx' in the prompt. Break condition: If distillation amplifies large model hallucinations, or if prompt modification has negligible effect on outputs, the bidirectional loop fails to improve over simpler routing-only approaches.

## Foundational Learning

- **Concept: Kolmogorov-Arnold Representation Theorem**
  - Why needed here: KAN is grounded in this theorem, which states any multivariate continuous function can be represented as compositions of univariate functions. This explains why learnable edge activations might have different representational properties than MLP's fixed node activations.
  - Quick check question: Can you explain why placing learnable functions on edges (KAN) vs. fixed functions at nodes (MLP) might affect both interpretability and resistance to forgetting?

- **Concept: Knowledge Distillation via KL-Divergence**
  - Why needed here: The paper uses KL-divergence loss to transfer knowledge from large to small model. Understanding this mechanism is essential for implementing the distillation component correctly.
  - Quick check question: How does KL-divergence measure distributional difference, and why might it be preferable to hard-label cross-entropy for this collaborative setting?

- **Concept: Catastrophic Forgetting in Long-Tail Distributions**
  - Why needed here: The paper positions KCM as addressing catastrophic forgetting, particularly for tail data. Understanding this problem—where networks lose previously learned patterns when adapting to new distributions—is critical for evaluating KCM's claims.
  - Quick check question: What makes tail-class samples particularly vulnerable to forgetting during knowledge distillation, and how might KAN's architecture theoretically address this differently than MLP?

## Architecture Onboarding

- **Component map:**
  Judgment Model (Fj) -> Confidence Score Cx -> Threshold ε=0.98 -> Small Model Fs (if Cx > ε) OR Large Model Fl + Prompt Modifier (if Cx ≤ ε) -> Bidirectional Distillation Module (KL-divergence when Cl > ε)

- **Critical path:**
  1. Input x → Judgment Model Fj → Compute Cx via softmax
  2. If Cx > ε → Small Model Fs → Output Ri
  3. If Cx ≤ ε → Large Model Fl + modified prompt → Output Ri
  4. During training: If Cl > ε → KL-divergence distillation updates Fs

- **Design tradeoffs:**
  - Threshold ε: Paper uses 0.98; higher values preserve accuracy but reduce cost savings. Task-specific calibration required.
  - KAN vs. MLP: KAN provides interpretability and forgetting mitigation but has higher training costs and slower convergence per iteration.
  - Distillation directionality: Paper distills from large-to-small when Cl > ε, and maintains small model learning when Cx > ε. The asymmetry matters for preventing hallucination propagation.

- **Failure signatures:**
  - Accuracy cliff on tail data: If KAN's forgetting mitigation fails, expect disproportionate drops on rare classes (compare head vs. tail accuracy gaps)
  - Routing imbalance: LM rate approaching 100% means small model is never trusted; approaching 0% means small model is overconfident on hard samples
  - Distillation-induced hallucination: If small model accuracy degrades after distillation cycles, large model errors may be propagating

- **First 3 experiments:**
  1. Threshold sensitivity analysis: Sweep ε from 0.90 to 0.99 on validation data. Plot LM rate vs. accuracy trade-off curve. Identify the "knee point" for your task distribution.
  2. KAN vs. MLP ablation on long-tail: Implement both architectures as the collaborative model on CIFAR-100-LT or equivalent. Specifically measure head/med/tail accuracy gaps to validate claimed forgetting mitigation.
  3. Routing calibration diagnostic: For samples routed to small model, track whether ground-truth labels fall within the small model's high-confidence classes. This reveals whether confidence scores genuinely correlate with capability boundaries.

## Open Questions the Paper Calls Out

### Open Question 1
How does KCM performance scale with significantly larger backbone models (e.g., GPT-4, Llama-70B) beyond the BERT and ChatGPT experiments conducted? Basis: The authors acknowledge: "considering the limitation of the experimental environment, the 'large model' and 'small model' used in the experiment are relative." Why unresolved: Resource constraints limited experiments to BERT and ChatGPT on a 2080ti GPU; true scalability to frontier models remains unvalidated. What evidence would resolve it: Experiments applying KCM to models with >70B parameters, reporting invocation reduction rates and accuracy trade-offs.

### Open Question 2
What is the sensitivity of KCM to the confidence threshold ε, and can adaptive threshold selection improve the accuracy-efficiency trade-off? Basis: The paper uses a fixed hyperparameter ε=0.98 without ablation studies on alternative values or task-adaptive thresholds. Why unresolved: No analysis provided on how different threshold values affect the balance between large model invocation rates and task accuracy across modalities. What evidence would resolve it: Systematic ablation across ε values (e.g., 0.90–0.99) with curves showing accuracy vs. computational savings.

### Open Question 3
Does the KCM framework adequately address the "amplified hallucination problems induced by small model knowledge" mentioned as a challenge in collaborative paradigms? Basis: The abstract identifies "amplified hallucination problems" as a key issue, but experiments focus on accuracy and computational efficiency without hallucination-specific metrics. Why unresolved: No quantitative analysis or benchmarks (e.g., factual consistency checks) were conducted to measure hallucination rates in the collaborative outputs. What evidence would resolve it: Hallucination evaluation using metrics like FactScore or human annotation on model outputs across the three modalities.

## Limitations
- KCM's claims about KAN-specific advantages over MLPs lack sufficient empirical isolation and ablation studies
- The paper doesn't report statistical significance or variance across runs for accuracy claims
- No quantitative analysis or benchmarks were conducted to measure hallucination rates in collaborative outputs

## Confidence

- **High Confidence:** Computational cost reduction claims (18-68% LM rate reduction) are well-supported with clear metrics across three task domains
- **Medium Confidence:** Accuracy maintenance claims are reasonable given the routing mechanism, though the paper doesn't report statistical significance or variance across runs
- **Low Confidence:** KAN-specific advantages (interpretability, forgetting mitigation) are asserted but not rigorously validated against equivalent MLP architectures

## Next Checks

1. **Forgetting Analysis:** Implement identical KCM architecture with MLP instead of KAN, then measure head/med/tail accuracy gaps over training epochs to quantify forgetting differences.

2. **Routing Calibration Test:** Systematically sweep ε threshold from 0.90 to 0.99 and plot accuracy vs. computational savings curves to identify optimal operating points for different task types.

3. **Prompt Modification Impact:** Conduct ablation study where large model receives modified prompt versus unmodified input to quantify the actual contribution of small-model confidence encoding to overall performance.