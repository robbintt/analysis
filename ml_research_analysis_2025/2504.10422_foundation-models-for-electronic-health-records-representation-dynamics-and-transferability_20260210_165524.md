---
ver: rpa2
title: 'Foundation models for electronic health records: representation dynamics and
  transferability'
arxiv_id: '2504.10422'
source_url: https://arxiv.org/abs/2504.10422
tags:
- data
- admission
- ucmc
- mimic
- tokens
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Foundation models trained on MIMIC-IV showed strong performance
  within-distribution but experienced significant performance degradation when transferred
  to a different hospital system (UCMC), particularly for ICU admission and invasive
  mechanical ventilation event prediction. Representation-based classifiers built
  from MIMIC-trained models achieved ROC-AUC scores of 0.853 for same-admission death
  and 0.795 for long length of stay when tested on MIMIC data, but these dropped to
  0.529 and 0.610 respectively on UCMC data.
---

# Foundation models for electronic health records: representation dynamics and transferability

## Quick Facts
- arXiv ID: 2504.10422
- Source URL: https://arxiv.org/abs/2504.10422
- Reference count: 16
- Key outcome: Foundation models trained on MIMIC-IV showed strong performance within-distribution but experienced significant performance degradation when transferred to a different hospital system (UCMC), particularly for ICU admission and invasive mechanical ventilation event prediction.

## Executive Summary
This paper investigates the transferability of foundation models trained on electronic health records by pre-training a Llama-3.2 1B parameter model on MIMIC-IV data and evaluating its performance when transferred to a different institution (UCMC). The study demonstrates that while the model achieves strong performance within the source distribution, zero-shot transfer to a new hospital system fails, with ROC-AUC scores dropping to near-random levels for several critical outcomes. However, supervised fine-tuning on small amounts of target data substantially recovers performance, suggesting that representation-based models can effectively capture general EHR patterns while still requiring local adaptation for optimal performance.

## Method Summary
The authors trained a Llama-3.2 1B parameter model using next-token prediction on MIMIC-IV data converted to CLIF format, employing a category-value tokenization strategy that represents continuous values as decile tokens. They extracted representations from the first 24 hours of patient admissions and used logistic regression probes to predict outcomes including mortality, long length of stay, ICU admission, and invasive mechanical ventilation. The model was then evaluated on both the original test set and a target institution (UCMC), with supervised fine-tuning applied to recover cross-system performance. Trajectory dynamics were analyzed by computing path length, maximum jump magnitude, and anomaly scores from the 24-hour representation sequences.

## Key Results
- Representation-based classifiers built from MIMIC-trained models achieved ROC-AUC scores of 0.853 for same-admission death and 0.795 for long length of stay when tested on MIMIC data
- These same classifiers dropped to 0.529 and 0.610 respectively when tested on UCMC data
- Supervised fine-tuning improved cross-system performance substantially, with models achieving ROC-AUC scores of 0.914 for same-admission death and 0.750 for long length of stay on UCMC data
- Analysis revealed that trajectory path length, maximum jump magnitude, and anomaly scores all positively correlated with adverse outcomes (p < 0.001) across both datasets

## Why This Works (Mechanism)

### Mechanism 1: Trajectory Volatility as a Clinical Signal
If a patient's representation moves erratically or covers large distances in the latent space during the first 24 hours of admission, they are statistically more likely to experience adverse outcomes (mortality, long stay). The model encodes physiological state into a vector, and rapid changes in this space likely proxy for clinical instability or rapid deterioration that precedes explicit events like ICU transfer. The geometry of the latent space linearly corresponds to clinical state severity; high-velocity movement in embedding space equates to high-velocity physiological change. Evidence includes significant positive correlations (p < 0.001) between path length, max jump, and anomaly scores with adverse outcomes in both datasets.

### Mechanism 2: Supervised Fine-Tuning for Cross-System Alignment
While zero-shot transfer of representation-based classifiers fails across institutions, supervised fine-tuning can re-align the model to local clinical policies and population distributions. The base model learns general "EHR syntax" from MIMIC-IV, but local practices vary. Fine-tuning adjusts the final layers to map general representations to specific local outcome definitions. The pre-trained representations are sufficiently robust that they only need linear re-alignment or light weight updates. Fine-tuning improved cross-system performance substantially, with death prediction ROC-AUC improving from 0.529 to 0.914 on UCMC data.

### Mechanism 3: Outlier Detection via Isolation Forests
Patients whose 24-hour representations are statistical outliers relative to the source training distribution are at higher risk, independent of specific prediction tasks. An Isolation Forest identifies data points that are easy to separate from the main distribution. In a clinical context, "anomalous" representations often correspond to complex, rare, or severe presentations not well-represented in the average training trajectory. The distribution of the source dataset covers "normal" inpatient variance sufficiently that deviations imply clinical abnormality rather than just data noise. Anomaly scores consistently showed positive coefficients with p < 0.001 for adverse outcomes in both datasets.

## Foundational Learning

- **Category-Value Tokenization**: You cannot simply feed raw lab values into a Transformer. This paper uses a specific scheme (category token + decile token) to handle continuous values. Without understanding this, the input sequence looks like noise. *Quick check*: Can you explain how a single lab result (e.g., Potassium 5.5) is represented as two distinct tokens in the input sequence?

- **Transfer Learning vs. Distribution Shift**: The core friction of the paper is the drop in performance from MIMIC to UCMC. You must understand why the model fails (different protocols, COVID-19 era data in UCMC) to appreciate why fine-tuning is necessary. *Quick check*: Why would a model predicting "ICU Admission" fail on a new hospital even if the patients look biologically similar?

- **Representation Dynamics (Trajectories)**: The paper moves beyond static predictions to analyzing the path of a patient through the hospital stay. Understanding that the model outputs a time-series of vectors (hidden states) is key. *Quick check*: What does "path length" represent in the context of a patient's 24-hour hospitalization vector sequence?

## Architecture Onboarding

- **Component map**: CLIF-formatted tables -> Category-Value Tokenizer -> Llama-3.2 1B (Next Token Prediction) -> Extract 24h representations -> Compute trajectory metrics -> Logistic Regression/Fine-tuning head

- **Critical path**: 1) Extract data from CLIF tables, 2) Tokenize events chronologically, 3) Training: Pack sequences -> Next Token Prediction, 4) Inference: Extract hidden states from first 24h -> Compute trajectory metrics -> Feed to Classifier

- **Design tradeoffs**: Vocabulary granularity tradeoffs - merging distinct labs into broad categories reduces vocabulary size but trades diagnostic nuance for training efficiency. Windowing limitation - limiting analysis to 24 hours reduces noise and computational load but excludes later-stage clinical deterioration signals.

- **Failure signatures**: Performance collapse (ROC-AUC dropping to ~0.5 indicates complete transfer failure) and high prediction variance (wide confidence intervals in partial sequence prediction indicate uncertainty or sensitivity to token ordering).

- **First 3 experiments**: 1) Baseline Transfer: Train on MIMIC, test on MIMIC vs. UCMC to quantify transfer gap, 2) Fine-Tuning Recovery: Fine-tune MIMIC model on small UCMC training split (5%) and measure performance recovery, 3) Trajectory Correlation: Compute path length and max jump for all test patients and verify correlation with mortality.

## Open Questions the Paper Calls Out

- How do foundation model representations and their associated trajectory dynamics evolve when expanding from a 24-hour observation window to longer time horizons involving multiple hospitalizations or chronic disease courses? The current study strictly restricted input data to the first 24 hours of admission to avoid information leakage and focus on same-admission outcomes, leaving long-term longitudinal dynamics unexplored.

- Can the observed patterns in representation dynamics and transferability be generalized to the eight additional health systems available within the CLIF consortium? The analysis was limited to a binary transfer between a single source dataset (MIMIC-IV) and a single target dataset (UCMC), which differed significantly in geography and time (COVID-19 era).

- Do more expressive unsupervised methods, such as generative models or density estimation approaches, improve the detection of anomalies compared to the Isolation Forest method used in this study? The study relied on an Isolation Forest approach, which, while interpretable and efficient, is relatively simple and may not fully capture the complexity of patient state dynamics.

## Limitations

- The category-value tokenization approach may lose clinical nuance by merging distinct lab types (e.g., whole blood vs. serum potassium), potentially limiting the model's capacity to capture specific clinical signals.

- The transfer evaluation is confounded by the fact that UCMC data includes COVID-19-era patients while MIMIC-IV training data did not, making it unclear whether transfer failure is due to institutional differences or temporal population changes.

- The correlations between trajectory metrics and adverse outcomes are observational associations rather than causal relationships - high trajectory volatility could indicate either patient deterioration or measurement artifact.

## Confidence

**High Confidence**: The observation that representation-based classifiers trained on MIMIC-IV transfer poorly to UCMC, with ROC-AUC scores dropping from 0.853 to 0.529 for same-admission death prediction.

**Medium Confidence**: The claim that supervised fine-tuning substantially improves cross-system performance, with improvements from 0.529 to 0.914 ROC-AUC for death prediction on UCMC data, though generalizability to other institution pairs remains uncertain.

**Medium Confidence**: The trajectory dynamics analysis showing correlations between path length, maximum jump magnitude, and anomaly scores with adverse outcomes (p < 0.001), though clinical interpretation and causal mechanisms require further validation.

## Next Checks

1. **Replication with Controlled Domain Shift**: Repeat the transfer experiment using a different institution's data from the same time period (pre-2020) to isolate the effect of institutional practice differences from COVID-19 era confounding. Compare performance degradation patterns to determine if the observed transfer failure is primarily due to institutional differences or temporal changes in patient populations.

2. **Vocabulary Granularity Sensitivity Analysis**: Systematically vary the tokenization granularity by including more specific lab categories (e.g., distinguishing whole blood from serum measurements) and measure the impact on both in-distribution performance and cross-system transfer. This would test whether the information loss from coarse tokenization is limiting the model's ability to adapt to new institutions.

3. **Fine-tuning Data Scaling Experiment**: Measure the relationship between the amount of target institution training data and performance recovery, testing whether the 5% used in the paper represents a minimum viable threshold or if performance continues to improve with more data. This would clarify whether the fine-tuning approach scales effectively or hits diminishing returns quickly.