---
ver: rpa2
title: Can Language Models Discover Scaling Laws?
arxiv_id: '2507.21184'
source_url: https://arxiv.org/abs/2507.21184
tags:
- scaling
- data
- arxiv
- loss
- size
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces SLDAgent, an evolution-based AI agent that
  automatically discovers scaling laws governing model performance at scale. SLDAgent
  co-optimizes symbolic expressions and fitting procedures through iterative mutation
  and selection, enabling it to outperform both human experts and existing coding
  agents across seven diverse scaling law discovery tasks.
---

# Can Language Models Discover Scaling Laws?

## Quick Facts
- arXiv ID: 2507.21184
- Source URL: https://arxiv.org/abs/2507.21184
- Reference count: 40
- Primary result: Evolution-based AI agent discovers scaling laws that outperform human experts across seven tasks

## Executive Summary
This paper introduces SLDAgent, an evolution-based AI agent that automatically discovers scaling laws governing model performance at scale. The agent co-optimizes symbolic expressions and fitting procedures through iterative mutation and selection, enabling it to outperform both human experts and existing coding agents across seven diverse scaling law discovery tasks. When benchmarked on SLDBench, a testbed of over 5,000 experiments, SLDAgent achieved consistently higher R² extrapolation accuracy than all baselines, including the established human-derived laws. The discovered laws exhibit more principled asymptotic behavior and dimensional consistency, leading to practical utility in hyperparameter optimization and model selection applications.

## Method Summary
SLDAgent uses evolutionary search to co-optimize two coupled subroutines: (1) Expression(x, θ) defining symbolic model forms, and (2) Optimization routines for fitting parameters. The system maintains a MAP-Elites database storing candidate (Expression, Optimization) pairs, uses LLM-based mutation to generate code modifications, and runs for 50 iterations across 5 parallel islands. Selection combines 70% exploitation, 20% diversity, and 10% elitism. The framework operates on SLDBench, which contains 8 scaling law discovery tasks with seen/unseen data splits for extrapolation evaluation.

## Key Results
- SLDAgent achieves higher R² extrapolation accuracy than human-derived laws on 7/7 scaling law tasks
- The system outperforms existing coding agents by up to 0.274 R² points on hardest tasks
- Discovered laws show better dimensional consistency and asymptotic behavior than human baselines
- Practical utility demonstrated in hyperparameter optimization and model selection applications

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Co-evolving symbolic expressions alongside fitting procedures yields more accurate scaling laws than optimizing either alone.
- Mechanism: The system maintains two coupled subroutines—Expression(x, θ) defines symbolic law form while Optimization finds best-fit parameters. Joint evolution discovers law structures inherently easier to fit and more stable under extrapolation.
- Core assumption: Search space requires both good functional forms and fitting procedures tailored to those forms.
- Evidence anchors: SLDAgent achieves R²=0.848 vs. 0.775 for law-only variant; most significant gap on lr&bsz task where law-only fails (R²=-0.224).

### Mechanism 2
- Claim: Evolutionary search with diversity preservation avoids premature convergence in open-ended scaling law discovery space.
- Mechanism: Multi-island strategy (5 islands), MAP-Elites database structuring by score/complexity/novelty, and selection mixing (70% exploitation, 20% diversity, 10% elitism) maintains population diversity across 50 iterations.
- Core assumption: Scaling law discovery has multiple local optima; purely greedy search gets trapped in suboptimal formulas.
- Evidence anchors: Performance on hard lr&bsz task shows volatile early scores but consistent upward trend without degradation.

### Mechanism 3
- Claim: Discovered laws outperform human-derived laws by achieving better dimensional consistency and principled asymptotic behavior.
- Mechanism: Evolutionary process rewards accurate extrapolation, implicitly selecting for formulas with physically meaningful asymptotics and dimensionally consistent parameterizations.
- Core assumption: Laws that extrapolate well must satisfy underlying mathematical constraints human heuristics may overlook.
- Evidence anchors: SLDAgent SFT law uses dimensionless ratio (D/θ₃)^θ¹ vs. human law with unit-confused additive term; MoE law ensures finite loss limits as E→∞ and N→∞.

## Foundational Learning

- Concept: **Power-law scaling fundamentals** (L = θ₀ + θ₁/N^θ² + θ₃/D^θ⁴ form, diminishing returns, irreducible loss floor)
  - Why needed here: All SLDBench tasks assume familiarity with how neural network loss scales with compute, data, and parameters.
  - Quick check question: Can you explain why scaling laws include both power-law decay terms and constant offsets?

- Concept: **Symbolic regression** (searching over expression trees with operators like +, −, ×, ÷, pow, log, exp)
  - Why needed here: Agent must explore combinatorial function space where exhaustive search is infeasible.
  - Quick check question: How does symbolic regression differ from fitting a pre-specified parametric model family?

- Concept: **Extrapolation vs. interpolation** (training on smaller scales, predicting at larger unseen scales)
  - Why needed here: SLDBench evaluation explicitly tests extrapolation to larger models/datasets, not just within-distribution fit.
  - Quick check question: Why might a law that fits training data perfectly still fail on extrapolation test sets?

## Architecture Onboarding

- Component map: Database <- LLM mutator <- Evaluator <- Selection <- Multi-island layer -> Database
- Critical path:
  1. Initialize with baseline (power-law expression + BFGS optimizer)
  2. Sample parent + inspirations from database
  3. LLM generates mutation (diff to code)
  4. Execute child: fit parameters on training data → compute R²
  5. Insert child into database; repeat for 50 iterations
  6. Return highest-scoring program
- Design tradeoffs:
  - 50 iterations: balances compute cost vs. convergence; hard tasks show continued improvement
  - Parameter constraints (4–35 params depending on task): limits overfitting risk but may restrict expressiveness
  - No network access during evaluation: prevents shortcut retrieval of published formulas but requires LLM to generate from internal knowledge
- Failure signatures:
  - Negative R²: law performs worse than mean prediction (common on lr&bsz and u_shape for weak agents)
  - High variance across runs: suggests fragile discovery path or insufficient exploration
  - Training R² high but test R² low: overfitting to seen data without extrapolation generalization
- First 3 experiments:
  1. Run baseline agent (Aider/CodeX without SLDAgent) on parallel task to establish floor performance
  2. Ablate co-optimization: run SLDAgent-law-only on lr&bsz task; compare to full SLDAgent
  3. Analyze discovered law form: After full run on sft task, compare SLDAgent's dimensionally-consistent formula to human baseline

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the agent develop internal "self-verification" mechanisms to distinguish robust physical laws from statistical artifacts without access to an explicit validation set?
- Basis in paper: The authors state that "Evaluating the true validity of a discovered law without access to extrapolation test data is inherently difficult" and that future iterations must enhance self-verification to address overfitting risks.
- Why unresolved: Current SLDAgent optimizes directly against provided training data, lacking internal guardrails to prevent fitting to noise.
- What evidence would resolve it: SLDAgent maintaining high R² on unseen data while autonomously rejecting overfitted candidates through internal consistency checks or synthetic validation splits.

### Open Question 2
- Question: Can the framework be extended to jointly discover the relevant variables (metrics) and the scaling laws connecting them?
- Basis in paper: The paper notes that "Variable discovery—proposing which new metrics (e.g., 'data mixture') to measure—is a complementary challenge" and an ambitious direction for future work.
- Why unresolved: Current system presumes input variables are fixed by human experts, focusing only on mathematical relationship between them.
- What evidence would resolve it: Agent successfully identifying novel predictive metric not present in initial dataset and formulating law based on that metric.

### Open Question 3
- Question: Can general-purpose coding agents learn to autonomously specialize in scientific discovery tasks like SLD?
- Basis in paper: The authors ask "can general-purpose agents learn to specialize?" and suggest investigating methods for general agents to learn from specialized agent trajectories.
- Why unresolved: While SLDAgent's specialized design yields superior performance, unclear if standard agent can replicate this behavior without hard-coded evolutionary scaffolding.
- What evidence would resolve it: General-purpose agent improving SLD performance significantly after training on or analyzing execution traces of SLDAgent.

## Limitations

- Evaluation framework uses held-out extrapolation test sets but lacks confidence intervals on R² scores across multiple random seeds
- LLM-based mutation depends heavily on prompt engineering and quality of inspiration programs without sensitivity analysis
- Claims about "more principled asymptotic behavior" are supported by specific examples but lack systematic quantitative analysis across all discovered laws
- Comparison to human experts doesn't account for time investment differences or access to domain knowledge during formula development

## Confidence

**High confidence**: Co-optimization mechanism demonstrably improves performance over law-only variants on lr&bsz task; multi-island evolutionary approach shows reasonable stability; qualitative differences in asymptotic behavior between discovered and human laws are mathematically verifiable.

**Medium confidence**: Claims about overall superiority across all seven tasks rely on extrapolation R² scores without error bars; mechanism by which diversity preservation prevents premature convergence is plausible but weakly supported; practical utility claims depend on external validation not provided.

**Low confidence**: Assertion that SLDAgent establishes "new paradigm for agentic scientific discovery" overstates generality; comparison to human experts doesn't account for time investment differences.

## Next Checks

1. **Statistical significance testing**: Run each task with 5+ random seeds and compute 95% confidence intervals for R² scores. Test whether SLDAgent's improvements over baselines are statistically significant using paired t-tests.

2. **Ablation of diversity mechanisms**: Implement single-island and purely exploitative variants, run on hard lr&bsz task, and compare convergence patterns and final performance to quantify marginal benefit of diversity preservation mechanisms.

3. **Systematic asymptotic analysis**: For each discovered law, analytically compute asymptotic behavior as all variables approach 0 and ∞. Compare fraction of laws satisfying physical constraints (finite limits, monotonic behavior) against random sampling from same expression space to quantify whether evolution preferentially discovers well-behaved formulas.