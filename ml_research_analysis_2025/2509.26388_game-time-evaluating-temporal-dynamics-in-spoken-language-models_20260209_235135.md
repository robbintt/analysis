---
ver: rpa2
title: 'Game-Time: Evaluating Temporal Dynamics in Spoken Language Models'
arxiv_id: '2509.26388'
source_url: https://arxiv.org/abs/2509.26388
tags:
- arxiv
- tasks
- language
- speech
- temporal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the Game-Time Benchmark to evaluate the temporal
  dynamics of Spoken Language Models (SLMs) in real-time conversational settings.
  The benchmark is inspired by how humans learn languages through activities that
  involve timing, tempo, and synchronization.
---

# Game-Time: Evaluating Temporal Dynamics in Spoken Language Models

## Quick Facts
- arXiv ID: 2509.26388
- Source URL: https://arxiv.org/abs/2509.26388
- Reference count: 0
- Primary result: All state-of-the-art Spoken Language Models fail significantly on temporal constraint tasks, revealing fundamental weaknesses in time-awareness and full-duplex interaction capability.

## Executive Summary
This paper introduces the Game-Time Benchmark to evaluate how well Spoken Language Models (SLMs) handle real-time conversational timing, tempo, and synchronization. Inspired by human language learning through timed activities, the benchmark extends basic instruction-following tasks with temporal constraints like speaking speed, silence duration, tempo adherence, and simultaneous speaking. Evaluation across multiple SLM architectures (time-multiplexing models like Freeze-Omni and Unmute, dual-channel models like Moshi, and commercial systems like GPT-realtime and Gemini-Live) shows that while models handle basic tasks reasonably well, all degrade substantially under temporal constraints. The study also proposes and validates a dual-channel LLM-as-a-judge evaluation method that achieves strong correlation with human judgments while avoiding the cost of audio-based judging.

## Method Summary
The Game-Time Benchmark consists of 1,475 test instances (700 Basic Tasks and 775 Advanced Tasks) designed to evaluate temporal dynamics in spoken language interaction. Basic Tasks involve standard instruction-following, while Advanced Tasks add temporal constraints over variables like duration, silence, and tempo. The evaluation pipeline captures dual-channel audio (user and model), transcribes both streams with Whisper-medium to obtain word-level timestamps, and uses an LLM judge (Gemini 2.5 Pro) to evaluate compliance with both content and temporal constraints. The study compares time-multiplexing architectures (Freeze-Omni, Unmute) with dual-channel architectures (Moshi) and commercial systems, validating the LLM judge against human evaluation with Spearman's ρ = 0.677.

## Key Results
- All models show substantial degradation on temporal constraint tasks, with near-zero performance on time-silence and simultaneous speaking tasks
- Time-multiplexing models (Freeze-Omni, Unmute) with frozen text LLMs outperform dual-channel models (Moshi) on basic instruction-following
- Among temporal constraints, models handle speed adjustments (Time-Fast, Time-Slow) better than precise timing (Time-Silence), tempo synchronization (Tempo-Adhere), and simultaneous speaking (SimulSpeak)
- The LLM-as-a-judge method achieves reasonable correlation with human judgments (Spearman's ρ = 0.677) while being more cost-effective than audio-based judging

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Adding explicit temporal constraints to instruction-following tasks reveals a capability gap in SLMs that is not exposed by content-only evaluations.
- **Mechanism:** Each task is formalized as a base task (e.g., "count from 1 to 10") paired with constraint predicates over variables like duration (τ), silence (s), or tempo (δ). The model must jointly satisfy the content requirement and the temporal specification.
- **Core assumption:** Temporal failures are separable from content failures; a model that can generate correct content may still fail to meet timing requirements.
- **Evidence anchors:**
  - [abstract]: "nearly all models degrade substantially under temporal constraints, exposing persistent weaknesses in time awareness and full-duplex interaction."
  - [Section 3.2]: "Performing an IF task requires the model to perform a base task t while satisfying all constraints in C. Each constraint is a predicate over variables that are typically numeric or symbolic."
  - [corpus]: Related benchmarks (TEMPO, Full-Duplex-Bench, FD-Bench) focus on temporal reasoning in retrieval or turn-taking, but do not evaluate explicit temporal instruction-following in real-time speech—confirming the gap.
- **Break condition:** If temporal constraints unintentionally increase cognitive load rather than specifically testing time-awareness, the mechanism confounds task difficulty with temporal capability.

### Mechanism 2
- **Claim:** A text-based LLM judge processing time-aligned dual-channel transcripts can reliably evaluate complex temporal behaviors, aligning with human judgments.
- **Mechanism:** Audio from user and model channels is transcribed with word-level timestamps (using Whisper-medium). An LLM (Gemini 2.5 Pro) reasons over the timestamped transcript to score compliance with both content and temporal constraints.
- **Core assumption:** Temporal behaviors can be adequately evaluated from text plus timestamps, without requiring direct audio processing.
- **Evidence anchors:**
  - [abstract]: "The study also proposes a dual-channel LLM-as-a-judge evaluation method, validated against human judgments, to reliably assess these complex behaviors."
  - [Section 3.4]: "An LLM can perform reasoning to recognize and evaluate the core speech embedded in the whole dialogue and give a reasonable evaluation."
  - [Section 5.2]: "We observe a reasonably high correlation (Spearman's ρ = 0.677, Pearson's r = 0.675) for our dual-channel evaluation method."
  - [corpus]: Audio-aware LLM judges exist (Chiang et al. 2025 in references), but this paper finds text-based judging more cost-effective and equally aligned with human evaluation.
- **Break condition:** If prosodic features (stress, intonation overlap) or sub-word temporal dynamics are critical for evaluation, text-only judging will miss them.

### Mechanism 3
- **Claim:** Time-multiplexing architectures with frozen text LLMs outperform dual-channel architectures on basic instruction-following, but no current architecture handles temporal constraints well.
- **Mechanism:** Time-multiplexing models (Freeze-Omni, Unmute) keep the text LLM frozen, preserving its instruction-following capability while adding streaming speech modules. Dual-channel models (Moshi) fine-tune the entire model for speech, which may degrade text-derived capabilities. Neither architecture encodes explicit time-awareness.
- **Core assumption:** The performance gap stems from architectural choices (frozen vs. fine-tuned LLM) rather than model scale or training data alone.
- **Evidence anchors:**
  - [Section 5.1]: "time-multiplexing models (Freeze-Omni and Unmute), which rely on a frozen LLM, generally outperform the dual-channel model (Moshi). This suggests that fine-tuning a text LLM to model speech signals remains challenging."
  - [Section 5.1]: "Among the Advanced Tasks, models perform comparatively better on Time-Fast and Time-Slow, but fail on Time-Silence... adhering to tempos (Tempo) and synchronizing speech with users (SimulSpeak) remains difficult."
  - [corpus]: FLEXI benchmark (arxiv 2509.22243) similarly finds full-duplex speech models struggle with real-time interaction dynamics, corroborating the architectural gap.
- **Break condition:** If the frozen-vs-fine-tuned comparison is confounded by different underlying LLM sizes or training datasets, the mechanism may not generalize.

## Foundational Learning

- **Full-Duplex Speech Interaction:**
  - Why needed here: The benchmark evaluates models that must listen and speak simultaneously, requiring understanding of overlap, interruption, and continuous intent recognition.
  - Quick check question: Can you explain why turn-by-turn dialogue systems cannot evaluate simultaneous speaking tasks?

- **Temporal Dynamics (Timing, Tempo, Synchronization):**
  - Why needed here: The core innovation is evaluating not just what to say but when to say it—requiring distinction between duration constraints, tempo adherence, and synchronization.
  - Quick check question: What is the difference between a Time-Slow task (speak for at least τ seconds) and a Tempo-Interval task (space words by δ seconds)?

- **Instruction-Following with Constraints:**
  - Why needed here: Tasks are formalized as (base_task, constraints) where constraints are predicates over variables; understanding this formalization is necessary to interpret benchmark design.
  - Quick check question: Given the instruction "count from 1 to 5 with 2 seconds of silence before starting," what are the base task and the constraints?

## Architecture Onboarding

- **Component map:**
  User Audio → [Streaming Encoder] → [LLM Core] → [Streaming Decoder] → Model Audio
                                    ↑
                            [State Prediction]
                            (speak/listen decision)

- **Critical path:**
  1. Set up dual-channel audio capture (separate user and model streams)
  2. Transcribe both channels with Whisper-medium to obtain word-level timestamps
  3. Format timestamped transcripts for LLM judge (Gemini 2.5 Pro recommended)
  4. Run evaluation on Basic Tasks first to establish baseline, then Advanced Tasks
  5. Validate subset with human evaluation (3 judges per sample recommended)

- **Design tradeoffs:**
  - **Time-multiplexing (Freeze-Omni, Unmute):** Preserves text LLM capabilities; modular; better basic instruction-following. Tradeoff: State prediction may miss subtle turn-taking cues.
  - **Dual-channel (Moshi):** Theoretically supports native full-duplex; single model. Tradeoff: Fine-tuning for speech degrades instruction-following; higher complexity.
  - **LLM judge vs. Audio-LLM judge:** Text-based is cheaper and equally aligned with humans (ρ=0.677). Audio-based captures prosody but more costly and slightly lower alignment (ρ=0.643).

- **Failure signatures:**
  - Time-Silence failures: Model produces correct content but cannot insert precise silence duration (suggests no explicit temporal representation)
  - Tempo failures: Model cannot maintain rhythmic consistency or match user-provided tempo example
  - SimulSpeak failures: Model cannot synchronize with user speech (rock-paper-scissors, shadowing)—complete failure across all models
  - Basic Task failures on open-source models: Indicates frozen LLM quality matters; Moshi underperforms on Repeat tasks

- **First 3 experiments:**
  1. **Baseline Basic Task evaluation:** Run your SLM on all 6 Basic Task families (700 samples). Compare against GPT-realtime and Freeze-Omni baselines. Identify which task families (Sequence, Repeat, Compose, Recall, Open-Ended, Role-Play) are weakest.
  2. **Speaking rate adaptation test:** Evaluate on Time-Fast and Time-Slow tasks only. These are the "easiest" temporal constraints. If your model passes these but fails others, it can adjust rate but lacks precise temporal control.
  3. **Time-Silence diagnostic:** Test whether your model can insert a specified silence duration (e.g., "wait 3 seconds, then count to 5"). Failure here indicates absence of explicit time representation—consider adding timing tokens or external timing modules.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What architectural innovations would enable SLMs to develop genuine time-awareness for precise temporal control (e.g., inserting exact silence durations)?
- Basis in paper: [explicit] Authors state models "can adjust their speaking rate... but still fail to grasp precise temporal requirements," with near-zero performance on Time-Silence tasks across all models, including GPT-realtime.
- Why unresolved: Current architectures (time-multiplexing and dual-channel) lack explicit temporal modeling mechanisms; the paper identifies this gap but does not propose solutions.
- What evidence would resolve it: Novel SLM architectures with dedicated temporal representations or training objectives that achieve >70% on Time-Silence and Tempo-Interval tasks.

### Open Question 2
- Question: Why do time-multiplexing models with frozen text LLMs outperform dual-channel end-to-end speech models on Basic Tasks?
- Basis in paper: [explicit] "Time-multiplexing models (Freeze-Omni and Unmute), which rely on a frozen LLM, generally outperform the dual-channel model (Moshi). This suggests that fine-tuning a text LLM to model speech signals remains challenging."
- Why unresolved: The trade-off between modular pipelines preserving text LLM capabilities versus end-to-end speech modeling remains poorly understood.
- What evidence would resolve it: Systematic ablation studies isolating whether performance gaps stem from training data scale, architectural inductive biases, or catastrophic forgetting during speech fine-tuning.

### Open Question 3
- Question: Would SLM temporal performance degrade when evaluated on natural human speech versus the TTS-synthesized instructions used in Game-Time?
- Basis in paper: [inferred] The dataset pipeline synthesizes all instructions via TTS (primarily CosyVoice) with manual editing only for precise tempo tasks; real speech has greater prosodic variation and disfluencies.
- Why unresolved: Models may overfit to TTS regularity; the generalization gap to spontaneous human speech with natural timing variation is untested.
- What evidence would resolve it: Evaluation comparing model performance on TTS-synthesized versus human-recorded versions of identical instructions, measuring performance correlation.

### Open Question 4
- Question: Can explicit temporal representations (e.g., learned embeddings for duration, beat, or phase) be integrated into SLMs to enable real-time synchronization with external rhythms?
- Basis in paper: [explicit] Models fail on Tempo-Adhere (synchronizing to user-specified tempo) and SimulSpeak tasks (overlapping speech at designated cues), indicating inability to track and predict external temporal dynamics.
- Why unresolved: The paper benchmarks existing models but does not explore whether temporal representations analogous to musical beat-tracking could improve synchronization.
- What evidence would resolve it: SLMs augmented with temporal encoders achieving significantly higher scores on Tempo-Adhere and Simul-Cue tasks without degrading Basic Task performance.

## Limitations

- The LLM-as-a-judge method, while cost-effective and well-correlated with human judgment, may miss prosodic features critical for temporal evaluation
- The benchmark uses synthetic TTS audio rather than natural human speech, potentially limiting ecological validity
- The study focuses exclusively on English, leaving multilingual temporal dynamics unexplored

## Confidence

- **High Confidence**: Basic instruction-following evaluation results (Models perform predictably based on architecture: time-multiplexing > dual-channel for basic tasks)
- **Medium Confidence**: Temporal constraint failures across all models (Consistent degradation observed, but synthetic audio may exaggerate difficulties)
- **Medium Confidence**: LLM-as-a-judge validation (Good correlation with humans, but potential blind spots in prosody evaluation)
- **Low Confidence**: Architectural explanations for performance gaps (Frozen vs fine-tuned comparison may be confounded by model scale and dataset differences)

## Next Checks

1. **Audio-LLM Judge Validation**: Run a subset of evaluations using audio-processing LLM judges (like Chiang et al. 2025) to compare against the text-based method and identify any systematic differences in temporal evaluation.

2. **Real-World Audio Testing**: Generate a smaller test set using real human speech rather than synthetic audio to validate whether temporal constraint failures persist under natural acoustic conditions.

3. **Cross-Lingual Extension**: Translate the benchmark to at least one language with different rhythmic patterns (e.g., Mandarin Chinese) to test whether temporal dynamics evaluation generalizes across languages.