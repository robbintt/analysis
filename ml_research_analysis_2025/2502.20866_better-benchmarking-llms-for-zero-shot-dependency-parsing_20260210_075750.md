---
ver: rpa2
title: Better Benchmarking LLMs for Zero-Shot Dependency Parsing
arxiv_id: '2502.20866'
source_url: https://arxiv.org/abs/2502.20866
tags:
- llms
- dependency
- zero-shot
- baselines
- parsing
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Large language models (LLMs) were evaluated as zero-shot dependency
  parsers, with performance compared against uninformed baselines such as left- and
  right-branching trees, random projective trees, and optimal linear arrangements.
  Most LLMs performed on par with these baselines, indicating no meaningful parsing
  ability beyond chance.
---

# Better Benchmarking LLMs for Zero-Shot Dependency Parsing

## Quick Facts
- **arXiv ID**: 2502.20866
- **Source URL**: https://arxiv.org/abs/2502.20866
- **Reference count**: 40
- **Primary result**: Most LLMs perform on par with uninformed baselines for zero-shot dependency parsing, with only LLaMA 3.1-70B showing marginal improvement in some languages.

## Executive Summary
This study evaluates large language models (LLMs) for zero-shot dependency parsing, revealing that most models perform no better than uninformed baselines such as left-branching trees and random projective trees. The evaluation covers English, French, and Hindi across four language families, using the SPMRL dataset and testing both zero-shot and one-shot prompting strategies. Results indicate that LLMs lack meaningful syntactic parsing capability beyond chance-level performance, with only the largest LLaMA model showing slight advantages in certain languages. The study emphasizes the need for more realistic baselines and comprehensive evaluation frameworks in LLM parsing research.

## Method Summary
The researchers evaluated multiple LLMs including GPT-3.5-turbo, GPT-4, LLaMA 3.1 variants, and Qwen2.5 models on zero-shot dependency parsing tasks. Models were tested on English, French, and Hindi using the SPMRL dataset, with evaluations comparing against uninformed baselines including left-branching, right-branching, random projective trees, and optimal linear arrangements. Both zero-shot and one-shot prompting strategies were employed, with careful control for model randomness through multiple trials. The primary metric was unlabeled attachment score (UAS), with syntactic accuracy measured through structured tree comparisons.

## Key Results
- Most LLMs achieved parsing performance comparable to uninformed baselines across all tested languages
- LLaMA 3.1-70B showed slight improvement over baselines in English and French but still achieved low overall accuracy
- No model surpassed uninformed baselines in Hindi, highlighting potential multilingual limitations
- Zero-shot and one-shot prompting strategies yielded similar results, suggesting prompting provides minimal benefit

## Why This Works (Mechanism)
The evaluation reveals fundamental limitations in LLMs' ability to perform syntactic parsing without explicit training on parsing tasks. Models appear to rely on surface-level language patterns rather than genuine syntactic understanding when generating dependency trees. The similarity between LLM outputs and uninformed baselines suggests that current models lack the architectural or learned representations necessary for accurate syntactic structure prediction.

## Foundational Learning
- **Dependency parsing fundamentals**: Understanding how syntactic dependencies are represented as tree structures is essential for evaluating parsing performance and designing appropriate evaluation metrics.
- **Uninformed baselines**: Knowledge of trivial parsing strategies (left-branching, right-branching, random projective trees) provides crucial context for assessing whether models learn meaningful syntactic patterns.
- **Zero-shot learning evaluation**: Understanding how to properly evaluate models without task-specific training is critical for assessing genuine language understanding capabilities.
- **SPNLP dataset characteristics**: Familiarity with the specific linguistic phenomena and tree structures present in the SPMRL dataset helps interpret model performance across different language families.
- **Unlabeled attachment score (UAS)**: This metric measures parsing accuracy by comparing predicted dependency arcs to gold standard annotations, providing a standardized evaluation approach.
- **Prompt engineering for structured output**: Understanding how to design prompts that elicit structured syntactic representations is essential for zero-shot parsing evaluation.

## Architecture Onboarding

**Component Map**: LLMs (pre-trained transformer-based) -> Prompt processor -> Structured output generator -> Dependency tree evaluator

**Critical Path**: Input text → Model inference → Dependency tree generation → Tree structure evaluation → UAS calculation

**Design Tradeoffs**: Models must balance general language understanding with task-specific parsing capabilities; larger models show marginal improvements but at significant computational cost

**Failure Signatures**: Outputs matching uninformed baselines indicate lack of genuine syntactic understanding; high variance across trials suggests instability in parsing predictions

**First Experiments**: (1) Compare model outputs against multiple uninformed baselines to establish performance floor, (2) Test cross-linguistic consistency by evaluating same model across multiple languages, (3) Analyze correlation between model size and parsing accuracy to identify performance scaling patterns

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation limited to four language families, potentially missing broader multilingual generalization patterns
- Reliance on SPMRL dataset may not capture full diversity of syntactic phenomena across languages
- Focus on unlabeled attachment scores may overlook models' ability to capture labeled dependencies or more nuanced syntactic relationships

## Confidence
- **High**: Claim that most LLMs perform no better than uninformed baselines for zero-shot dependency parsing in English and French
- **Medium**: Claim about LLaMA 3.1-70B's slight advantage over baselines due to potential variance in model versions and training data
- **Medium**: Claim about Hindi results showing no model surpassing baselines due to limited data points and single language testing

## Next Checks
- Test additional languages beyond Indo-European and Uralic families to assess true multilingual capabilities, particularly focusing on languages with non-configurational syntax
- Compare zero-shot performance against few-shot and fine-tuned approaches using identical evaluation metrics to establish performance baselines for supervised methods
- Evaluate alternative parsing metrics including labeled attachment scores, parsing speed, and robustness to input perturbations to determine if LLMs show advantages in specific parsing dimensions not captured by current evaluation