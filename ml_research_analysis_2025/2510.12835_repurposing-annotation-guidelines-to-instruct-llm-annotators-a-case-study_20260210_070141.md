---
ver: rpa2
title: 'Repurposing Annotation Guidelines to Instruct LLM Annotators: A Case Study'
arxiv_id: '2510.12835'
source_url: https://arxiv.org/abs/2510.12835
tags:
- annotation
- guidelines
- disease
- category
- annotations
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores using annotation guidelines to instruct LLM
  annotators, addressing the challenge of adapting guidelines designed for humans
  to work with LLMs. The authors propose a "moderation-oriented guideline repurposing"
  method that involves incorporating guidelines into prompts and iteratively refining
  them through an LLM moderation process.
---

# Repurposing Annotation Guidelines to Instruct LLM Annotators: A Case Study

## Quick Facts
- **arXiv ID**: 2510.12835
- **Source URL**: https://arxiv.org/abs/2510.12835
- **Reference count**: 28
- **Primary result**: Annotation guidelines repurposed for LLM instruction improved F1-scores from 0.36 to 0.58, with human-in-the-loop refinement achieving 0.78 F1-score

## Executive Summary
This paper addresses the challenge of adapting human-designed annotation guidelines for use with large language model (LLM) annotators. The authors propose a "moderation-oriented guideline repurposing" method that incorporates guidelines into prompts and iteratively refines them through an LLM moderation process. Using the NCBI Disease Corpus, they demonstrate that incorporating guidelines improves annotation accuracy, with F1-scores increasing from 0.36 to 0.58. The human-in-the-loop approach further improves performance to 0.78 F1-score, showing promise for scalable and cost-effective annotation guideline refinement and automated annotation while highlighting challenges in scope definition and category matching.

## Method Summary
The authors propose a moderation-oriented guideline repurposing method that involves incorporating human annotation guidelines into LLM prompts through a multi-step process. First, they preprocess the NCBI Disease Corpus guidelines to extract key annotation rules and examples. These guidelines are then incorporated into prompt templates for LLM annotation. An iterative refinement process follows, where the LLM's annotations are reviewed and the guidelines are adjusted based on observed errors. The study employs GPT-4 as the LLM annotator and uses precision, recall, and F1-score metrics to evaluate performance. The human-in-the-loop approach involves human reviewers examining LLM outputs and providing feedback to further refine the guidelines and prompts.

## Key Results
- Incorporating annotation guidelines into LLM prompts improved F1-scores from 0.36 to 0.58
- The human-in-the-loop approach achieved the highest performance with 0.78 F1-score
- Iterative refinement through LLM moderation proved effective for guideline adaptation
- The approach demonstrated potential for scalable and cost-effective annotation guideline refinement

## Why This Works (Mechanism)
The approach works by leveraging the LLM's ability to understand and apply complex rules when provided with clear, structured guidelines. By incorporating human annotation guidelines directly into prompts, the LLM gains access to the same decision-making framework that human annotators use. The iterative refinement process allows for continuous improvement as the system learns from its mistakes, with the LLM moderation step providing rapid feedback cycles that would be impractical with human-only review.

## Foundational Learning
**Annotation guideline structure** - Understanding the format and content of human annotation guidelines is essential for effective repurposing
*Why needed*: To extract relevant rules and examples that can be translated into LLM prompts
*Quick check*: Can you identify the key components (rules, examples, edge cases) in a sample guideline?

**Prompt engineering for instruction** - Crafting effective prompts that incorporate guidelines while maintaining clarity
*Why needed*: To ensure LLMs can interpret and apply guidelines correctly
*Quick check*: Can you create a prompt that incorporates a specific guideline rule while maintaining task clarity?

**Iterative refinement methodology** - The process of systematically improving guidelines based on LLM performance
*Why needed*: To address mismatches between human and LLM interpretation of guidelines
*Quick check*: Can you outline the steps for analyzing LLM errors and updating guidelines accordingly?

## Architecture Onboarding

**Component map**: Human guidelines -> Preprocessing module -> Prompt template -> LLM annotator -> Output review -> Moderation feedback -> Guideline refinement -> (Optional: Human review) -> Final annotations

**Critical path**: Guideline preprocessing -> Prompt generation -> LLM annotation -> Performance evaluation -> Moderation feedback -> Guideline refinement -> Repeat until convergence

**Design tradeoffs**: The study balances automation (LLM-only annotation) against accuracy (human-in-the-loop), with the iterative approach trading computational resources for improved performance. The moderation-oriented method prioritizes guideline fidelity over raw annotation speed.

**Failure signatures**: 
- Guidelines too complex for LLM comprehension (low precision/recall across all categories)
- Category misalignment between human and LLM interpretation (systematic errors in specific annotation types)
- Overfitting to training examples (high performance on seen examples, poor generalization)

**First experiments**:
1. Baseline annotation without guidelines to establish performance floor
2. Single iteration of guideline incorporation to measure initial improvement
3. Human review of LLM outputs to identify systematic error patterns

## Open Questions the Paper Calls Out
The paper does not explicitly identify open questions, but the approach raises several important considerations. The effectiveness of the moderation-oriented guideline repurposing method across different annotation domains remains unclear, as does the scalability of the iterative refinement process for larger datasets. The balance between automation and accuracy, particularly in the human-in-the-loop approach, presents an ongoing challenge for achieving truly scalable annotation systems.

## Limitations
- Narrow evaluation scope (only NCBI Disease Corpus and biomedical text)
- Iterative moderation process may not scale to larger datasets or complex annotation schemes
- Human-in-the-loop approach partially defeats automation goals by requiring human intervention
- The computational cost of iterative refinement was not fully quantified
- Performance improvements may be domain-specific and not generalizable

## Confidence
- **High confidence**: Incorporating annotation guidelines into LLM prompts improves performance compared to baseline prompts (F1 increase from 0.36 to 0.58)
- **Medium confidence**: Moderation-oriented approach provides cost-effective refinement, as computational and human effort was not fully quantified
- **Medium confidence**: Approach enables scalable annotation, given limited dataset size and domain specificity

## Next Checks
1. Test the moderation-oriented guideline repurposing approach across multiple domains and annotation tasks to assess generalizability
2. Conduct a cost-benefit analysis comparing computational overhead of iterative refinement against annotation accuracy gains
3. Evaluate the approach on larger datasets to determine scalability and identify potential bottlenecks in the moderation process
4. Investigate the minimum set of guidelines needed for effective LLM annotation to optimize the prompt engineering process
5. Explore automated methods for guideline refinement to reduce or eliminate the need for human intervention in the loop