---
ver: rpa2
title: 'Entropy-Guided Watermarking for LLMs: A Test-Time Framework for Robust and
  Traceable Text Generation'
arxiv_id: '2504.12108'
source_url: https://arxiv.org/abs/2504.12108
tags:
- text
- sampling
- scheme
- watermark
- binary
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ''
---

# Entropy-Guided Watermarking for LLMs: A Test-Time Framework for Robust and Traceable Text Generation

## Quick Facts
- arXiv ID: 2504.12108
- Source URL: https://arxiv.org/abs/2504.12108
- Authors: Shizhan Cai; Liang Ding; Dacheng Tao
- Reference count: 13
- Primary result: Outperforms baselines on MATH (95.1% vs 94.8%) and GSM8K (89.2% vs 88.9%) with 10% AUC drop vs 60% in prior work under paraphrase attacks

## Executive Summary
This paper introduces a watermarking framework that selectively embeds traceable signals in LLM outputs based on cumulative entropy thresholds. The method preserves text quality in deterministic, low-entropy segments while enabling robust detection in high-entropy regions. By using token blocks as seeds for key generation, it maintains consistency across repeated queries while remaining cryptographically indistinguishable from non-watermarked outputs.

## Method Summary
The framework accumulates watermark entropy α=1-p(y_i) during token generation and only activates watermarking when this cumulative value exceeds a threshold λ. At that point, the preceding token block becomes a seed r to generate a secret key ξ(r), which modifies subsequent sampling via function g. Two sampling variants are supported: Inverse Transform Sampling (ITS) and Binary Sampling (BS) with Huffman encoding. Detection uses alignment cost d between text and key subsequences with sliding k-window and resampling for p-value computation.

## Key Results
- Maintains task accuracy: 95.1% vs 94.8% on MATH and 89.2% vs 88.9% on GSM8K
- Robust detection under paraphrase: 10% AUC drop versus 60% in prior work
- Demonstrates indistinguishability from non-watermarked outputs under adaptive queries

## Why This Works (Mechanism)

### Mechanism 1
- Claim: A cumulative watermark entropy threshold selectively activates watermarking, preserving text quality in deterministic segments while enabling robust detection in high-entropy regions.
- Mechanism: Tokens are sampled normally from the original LLM distribution until cumulative entropy (α = 1 - p(y_i)) exceeds threshold λ. Only then is a seed r formed from the preceding token block to generate secret key ξ(r), which modifies subsequent sampling via function g.
- Core assumption: Low-entropy segments (e.g., few-shot templates, argmax outputs) require preservation to maintain task performance and user experience.
- Evidence anchors:
  - [abstract]: "...improves both detectability and text quality by introducing a cumulative watermark entropy threshold."
  - [section]: "If the entropy of the output text is low...any alterations to add a watermark make it easy to notice and ruin the original text's meaning."
  - [corpus]: CATMark (arXiv:2510.02342) addresses quality decline in low-entropy scenarios with context-aware thresholding, supporting the premise but not this specific mechanism.
- Break condition: If λ is set too high, watermarking may never activate in short texts; if too low, quality degradation approaches baseline methods.

### Mechanism 2
- Claim: Using token blocks as seeds for key generation ensures output consistency across repeated queries while maintaining cryptographic indistinguishability.
- Mechanism: Once entropy exceeds λ, the accumulated token block becomes seed r for key ξ(r). Identical prompts produce identical seeds, ensuring few-shot consistency. The probability of seed collision across t queries is bounded by (t-1)2^(-λ), making it negligible for sufficient λ.
- Core assumption: Polynomial-time distinguishers cannot detect watermark presence even with adaptive queries (formal indistinguishability per Definition 2).
- Evidence anchors:
  - [section]: "If the previous blocks are identical, the seeds are also equal so that the responses are same."
  - [section]: "P[r(t) ∈ B] ≤ negl(λ)... we have a negligible probability of colliding the key sequence."
  - [corpus]: Corpus evidence for this specific block-based seeding is weak; no direct implementation found in neighbor papers.
- Break condition: If λ is too small or query count t is very large, collision probability may become non-negligible, exposing the watermark.

### Mechanism 3
- Claim: Alignment-cost detection remains robust under text modification attacks by measuring statistical gaps between watermarked and non-watermarked sequences.
- Mechanism: Detection computes p-value via test statistic φ based on minimum alignment cost d between text and key subsequences of length k. Watermarked text yields small d; independent text yields large random d. Resampling (T iterations) validates statistical significance.
- Core assumption: The expected cost gap m · Var(η(Y)) · α(Y) persists despite edits, providing detection power.
- Evidence anchors:
  - [abstract]: "...maintaining high detection accuracy."
  - [section]: "Our scheme shows only a 10% AUC drop, compared to a 60% drop in [Christ et al., 2023], highlighting significant robustness."
  - [corpus]: SimMark (arXiv:2502.02787) targets robustness via sentence-level similarity but uses a different detection approach.
- Break condition: If attack edits exceed alignment window k or destroy token subsequences, cost gap may vanish, causing detection failure.

## Foundational Learning

- **Entropy in Text Generation**
  - Why needed here: Understanding token-level entropy explains why watermarking low-entropy text degrades quality—such segments are predictable, so any perturbation is noticeable.
  - Quick check question: Given token probability p(y_i) = 0.95, what is its contribution to cumulative watermark entropy α?

- **Statistical Hypothesis Testing for Watermark Detection**
  - Why needed here: Detection frames watermark identification as a binary hypothesis test with controllable false positive/negative rates; understanding p-values is essential for evaluating detectability.
  - Quick check question: In Algorithm 2, what does a low alignment cost φ(y, ξ) suggest about the text's relationship to key ξ?

- **Sampling Functions in Language Models**
  - Why needed here: The framework generalizes across sampling methods (Inverse Transform Sampling, Binary Sampling); knowing how they incorporate randomness via keys is critical for implementation.
  - Quick check question: How does Binary Sampling use Huffman encoding to map a uniform random variable u ∈ ξ(r) to a token?

## Architecture Onboarding

- **Component map:** Entropy Accumulator -> Seed Generator -> Key Generator -> Sampling Function g -> Detector
- **Critical path:**
  1. Input prompt x and threshold λ
  2. For each token position, compute p_i from LLM; update cumulative entropy
  3. If entropy < λ: sample y_i ~ p_i normally
  4. If entropy ≥ λ: set token block as seed r, generate key ξ(r), sample y_i ← g(ξ(r), p_i)
  5. For detection: slide windows of length k over text, compute min alignment cost d against key subsequences, derive p-value via resampling

- **Design tradeoffs:**
  - λ vs. detectability: Higher λ improves quality but delays watermarking, reducing detection power for short texts
  - Sampling method: ITS and Binary show similar detectability (Figure 4) but may differ in robustness profiles under specific attacks
  - Chunk length k: Larger k increases robustness to edits but may miss localized watermarks in heavily modified text

- **Failure signatures:**
  - *Quality collapse*: Low λ causes watermarking to activate too early, disrupting few-shot templates or deterministic answers
  - *Detection erosion*: Paraphrase attacks reduce AUC significantly if alignment window k is smaller than edit distance
  - *Key collision*: High query volume with insufficient λ increases collision probability, potentially revealing watermark structure

- **First 3 experiments:**
  1. **Threshold calibration**: Sweep λ ∈ {0.5, 1.0, 2.0, 5.0} on MATH/GSM8K datasets; plot task accuracy degradation vs. detection AUC to identify Pareto-optimal λ
  2. **Attack robustness benchmark**: Generate 100 C4 continuations per model (Llama, OPT, Gemma, phi); apply paraphrase, translation, and basic edits; compare AUC drop between our scheme, ITS baseline, and Binary baseline
  3. **Indistinguishability probe**: Execute adaptive prompt sequences designed to detect watermark presence (e.g., repeated few-shot queries with slight variations); use GPT-4 to score whether outputs differ perceptibly from non-watermarked model

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How sensitive is the optimal cumulative entropy threshold (λ) to different prompt types or generation domains (e.g., coding vs. creative writing)?
- **Basis in paper:** [inferred] While Algorithm 1 introduces the threshold λ as a core parameter, the experimental evaluation does not provide an ablation study on how varying λ impacts the trade-off between text quality and detectability across different datasets.
- **Why unresolved:** It is unclear if a single static λ value is universally optimal or if it requires tuning for specific tasks to prevent quality degradation in low-entropy scenarios.
- **What evidence would resolve it:** Ablation experiments showing the change in AUC and task accuracy (e.g., MATH vs. Story generation) as λ is varied.

### Open Question 2
- **Question:** Can the framework's reliance on an entropy threshold be exploited by adversaries using "low-entropy attacks" to strip the watermark?
- **Basis in paper:** [inferred] Section 2.1 states, "If the entropy of the output text is low... it's meaningless to watermark," and the method explicitly does not watermark text below the threshold.
- **Why unresolved:** An adversary could potentially craft prompts that force the model to generate highly deterministic (low-entropy) text, thereby preventing the watermark from ever being embedded.
- **What evidence would resolve it:** Adversarial experiments where prompts are specifically optimized to minimize output entropy, measuring the resulting watermark detection rates.

### Open Question 3
- **Question:** Do the quality preservation and robustness findings generalize to significantly larger, state-of-the-art models (e.g., 70B+ parameters)?
- **Basis in paper:** [inferred] Section 4.5 validates generalization on Llama-3.1-8B, but the vast majority of testing was conducted on smaller models (1B–2B).
- **Why unresolved:** Larger models exhibit different probability distributions and reasoning capabilities; it remains unverified if the entropy thresholding logic scales effectively without introducing new artifacts or detection failures in much larger architectures.
- **What evidence would resolve it:** Evaluation of the watermarking scheme on flagship models (e.g., Llama-3-70B or GPT-4 class models) using the same MATH and GSM8K benchmarks.

## Limitations
- The optimal entropy threshold λ may require task-specific tuning and doesn't generalize across all domains
- Low-entropy prompts can completely bypass watermarking, creating a potential attack vector
- Evaluation is primarily on smaller models (1B-8B parameters), leaving scalability to larger models uncertain

## Confidence
- Mechanism validity: High - formal indistinguishability proofs provided
- Experimental reproducibility: Medium - key hyperparameters (λ, k, T) unspecified
- Generalization claims: Low - limited to small-to-medium models, missing large-scale validation

## Next Checks
1. Verify entropy threshold λ=2.0 maintains task accuracy >94% while achieving AUC >0.95 on MATH dataset
2. Test robustness by applying automated paraphrase to 100 watermarked outputs and measuring AUC drop
3. Confirm indistinguishability by having human evaluators compare watermarked vs non-watermarked outputs on few-shot prompts