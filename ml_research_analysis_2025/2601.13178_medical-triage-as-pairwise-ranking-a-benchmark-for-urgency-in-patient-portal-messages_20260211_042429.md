---
ver: rpa2
title: 'Medical Triage as Pairwise Ranking: A Benchmark for Urgency in Patient Portal
  Messages'
arxiv_id: '2601.13178'
source_url: https://arxiv.org/abs/2601.13178
tags:
- patient
- messages
- urgent
- more
- urgency
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces PMR-Bench, the first large-scale dataset
  for pairwise medical urgency assessment in patient portal messages. The dataset
  contains 1,569 unique messages across three sources: PMR-Reddit (public, unstructured
  text), PMR-Synth (public, synthetic messages with EHR data), and PMR-Real (proprietary,
  real messages with EHR data).'
---

# Medical Triage as Pairwise Ranking: A Benchmark for Urgency in Patient Portal Messages

## Quick Facts
- arXiv ID: 2601.13178
- Source URL: https://arxiv.org/abs/2601.13178
- Reference count: 40
- Primary result: Introduces PMR-Bench, first large-scale dataset for pairwise medical urgency assessment in patient portal messages

## Executive Summary
This paper introduces PMR-Bench, the first large-scale dataset for pairwise medical urgency assessment in patient portal messages. The dataset contains 1,569 unique messages across three sources: PMR-Reddit (public, unstructured text), PMR-Synth (public, synthetic messages with EHR data), and PMR-Real (proprietary, real messages with EHR data). The task is framed as pairwise ranking—given two messages, determine which is more medically urgent—which provides finer granularity than categorical urgency labels. Two model classes are introduced: UrgentSFT, trained with supervised fine-tuning using a next-token prediction objective, and UrgentReward, which uses a Bradley-Terry reward modeling approach. Both leverage EHR data where available and are designed for deployment efficiency, supporting models from 4B to 32B parameters. Experiments show that UrgentSFT with MedGemma-27B achieves the highest overall pairwise classification accuracy across datasets, while UrgentReward-8B delivers competitive performance with significantly smaller models. In extrinsic inbox-sorting evaluations, UrgentSFT models achieve strong T-NDCG scores (e.g., 0.77 at k=10 on PMR-Real), outperforming off-the-shelf 8B baselines by 15–16 points. The methods demonstrate robustness to message difficulty levels and show potential for real-world deployment in clinical settings.

## Method Summary
The method frames medical triage as a pairwise ranking problem using the PMR-Bench dataset with 1,569 messages across three sources. UrgentSFT fine-tunes LLMs with supervised learning using a two-step inference process that computes probability differences to mitigate position bias. UrgentReward employs Bradley-Terry reward modeling to learn preference scoring functions, showing advantages for smaller models. Both approaches use LoRA/QLoRA fine-tuning with rank=64, alpha=64, and 4-bit quantization. The task formulation uses a 6-tier urgency scale and includes EHR data where available. Evaluation combines intrinsic pairwise classification accuracy (split by difficulty) with extrinsic inbox-sorting metrics (T-NDCG@10 and T-NDCG@30).

## Key Results
- UrgentSFT with MedGemma-27B achieves highest overall pairwise classification accuracy across datasets
- UrgentReward-8B delivers competitive performance with significantly smaller models, showing distinct advantages in low-resource settings
- UrgentSFT models achieve T-NDCG@10 scores of 0.77 on PMR-Real, outperforming off-the-shelf 8B baselines by 15–16 points
- Models demonstrate robustness to message difficulty levels and show potential for real-world clinical deployment

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Pairwise ranking formulation provides finer-grained urgency discrimination than categorical classification.
- Mechanism: Binary comparison ("which message is more urgent?") reduces semantic complexity compared to mapping to 3+ ordinal categories, while enabling tournament-style sorting to produce total orderings without intra-class ties.
- Core assumption: Clinicians can reliably make relative urgency judgments even when absolute categorization varies across individuals/organizations.
- Evidence anchors:
  - [abstract] "Our novel task formulation views patient message triage as a pairwise inference problem... tournament-style re-sort of a physician's inbox"
  - [section 1] "intuitively, a binary higher- versus lower-urgency comparison involves simpler comparison semantics than an ordinal set of three or more urgency categories"
  - [corpus] C-PATH (arXiv:2506.06737) uses conversational triage but with categorical outcomes; this paper's pairwise approach differs structurally.
- Break condition: If inter-annotator agreement on pairwise comparisons drops significantly (suggesting clinicians don't share consistent relative urgency notions), the formulation fails.

### Mechanism 2
- Claim: Bradley-Terry reward modeling improves sample efficiency for smaller models.
- Mechanism: UrgentReward frames training as learning a preference scoring function over (anchor, more urgent, less urgent) triplets, leveraging pre-trained reward model priors from SkyWork-Reward-v2 (26M preference pairs). The BT objective maximizes score gaps between preferred and dispreferred completions.
- Core assumption: The pre-trained reward model's general preference-scoring capability transfers to medical urgency preferences.
- Evidence anchors:
  - [abstract] "UrgentReward showing distinct advantages in low-resource settings"
  - [section 3.4] "we fine-tune a reward model... which uses a Bradley-Terry objective to maximize Ur(tp, tm)−Ur(tp, tl)"
  - [section 5.1 / Table 8] UrgentReward-4B-Large achieves 0.82 total accuracy vs UrgentSFT-4B-Large at 0.70 with same training data
  - [corpus] No direct corpus evidence for BT specifically in triage; related work uses transformers for categorization (Si et al. 2020).
- Break condition: If reward model fails to generalize to medical domain (e.g., scores syntactic fluency over clinical reasoning), fine-tuning may not converge to meaningful urgency preferences.

### Mechanism 3
- Claim: Two-step inference with probability difference reduces position bias in pairwise comparisons.
- Mechanism: UrgentSFT computes η = P(YES|f(a,b)) − P(YES|f(b,a)). If η > 0, message b is deemed more urgent. This symmetric evaluation cancels systematic ordering biases.
- Core assumption: Position bias is approximately symmetric (preferring first/second position consistently), so subtraction removes it.
- Evidence anchors:
  - [section 3.3] "We formulate UrgentSFT as a two-step inference leveraging probability scores to help prevent ties and improve models' sensitivity to input order"
  - [section 5.1] "Qwen3-32B without reasoning out-performs Qwen3-32B with reasoning... due to input order biases being exaggerated by reasoning"
  - [corpus] Weak direct evidence; corpus papers don't address position bias mitigation in triage.
- Break condition: If models exhibit asymmetric position bias (different magnitude depending on content), the subtraction fails to normalize.

## Foundational Learning

- **Bradley-Terry preference modeling**
  - Why needed here: UrgentReward's training objective; understanding how preference probabilities relate to latent "skill" scores.
  - Quick check question: Given two items with latent scores s₁=2.0 and s₂=1.0, what's the BT probability that item 1 is preferred?

- **Learning-to-Rank paradigms (pointwise/pairwise/listwise)**
  - Why needed here: The paper explicitly situates itself in pairwise re-ranking; understanding tradeoffs vs. alternatives is essential for system design.
  - Quick check question: Why does pairwise ranking provide stronger theoretical guarantees but higher inference cost than pointwise scoring?

- **NDCG and T-NDCG metrics**
  - Why needed here: Extrinsic evaluation uses T-NDCG@k to measure inbox sorting quality with tail penalty for urgent messages sorted to bottom.
  - Quick check question: How does T-NDCG differ from standard NDCG, and why is the tail penalty critical for safety-critical triage?

## Architecture Onboarding

- **Component map:** Raw messages → Urgency annotation → Triplet generation → SFT pairs/Reward triplets → UrgentSFT or UrgentReward fine-tuning → Comparator function → Sorting algorithm → Sorted inbox

- **Critical path:**
  1. Data quality: Pairwise labels derived from clinician responses must pass LLM validation (two-filter agreement, Section A.4.2)
  2. Training: LoRA/QLoRA fine-tuning (rank=64, alpha=64) for 1 epoch (Reddit/Real) or 3 epochs (Synth)
  3. Inference: For inbox of n messages, compute (n choose 2) pairwise comparisons, accumulate win scores with η weighting

- **Design tradeoffs:**
  - UrgentSFT vs UrgentReward: SFT achieves higher peak accuracy (MedGemma-27B: 0.88 on Reddit); Reward offers better sample efficiency for small models
  - EHR inclusion: MedGemma benefits from EHR (+3 points on Synth); Qwen/GPT-OSS show neutral or negative effects (Table 9)
  - Win-rate sorting vs tournament: Win-rate (n choose 2 comparisons) is robust to initial ordering but O(n²) inference

- **Failure signatures:**
  - Low inter-annotator agreement on Synth (α=0.63) suggests annotation noise—expect performance ceiling
  - Models show gender-associated performance gaps (Table 12): accuracy drops when less-urgent patient is male (OSS: 32.4% correct)
  - Multi-class baselines show high T-NDCG variance (SD up to 0.09) due to intra-class tie randomness (Table 11)

- **First 3 experiments:**
  1. Replicate UrgentReward-8B on PMR-Synth (public data) using SkyWork-Reward-v2 base; verify ~0.71 total accuracy matches paper
  2. Ablate EHR inclusion with MedGemma vs Qwen on Synth; confirm divergent effects (Table 9 patterns)
  3. Test position bias: Evaluate same pairs in (a,b) and (b,a) order without η normalization; measure consistency gap

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do LLM-based triage models systematically over- or under-triage patients based on demographic attributes (e.g., gender, age, race) or medical background traits?
- Basis in paper: [explicit] Limitations section states: "It may be the case that LLMs over or under triage certain subpopulations and this should be investigated before deployment."
- Why unresolved: The error analysis (Appendix D) revealed gender disparities—accuracy dropped when the less urgent patient was male—but no comprehensive bias audit was conducted.
- What evidence would resolve it: A systematic fairness audit across protected demographic groups with statistical tests for triage disparities.

### Open Question 2
- Question: How should time-in-inbox be incorporated as a factor to prevent low-urgency messages from experiencing unbounded delays while respecting response-time policies (e.g., 72-hour requirements)?
- Basis in paper: [explicit] Limitations section notes: "One may want to avoid the case where a low-urgency message is continually placed on the bottom of the inbox, causing longer-than-usual response delays."
- Why unresolved: The current pairwise formulation only considers medical urgency, not temporal factors; no mechanism prevents indefinite delays for low-priority messages.
- What evidence would resolve it: Experiments with time-weighted ranking functions evaluated on response time equity metrics.

### Open Question 3
- Question: What mechanisms can improve LLMs' capacity to meaningfully integrate structured EHR data into triage decisions, given that current models show mixed or negative effects from EHR inclusion?
- Basis in paper: [explicit] Appendix C.2 states: "Future works may wish to explore extending medical reasoning capacity to process structured EHR information before making a prediction."
- Why unresolved: The EHR ablation (Table 9) showed some models (GPT-OSS, Qwen-based) performed worse or no better with EHR data, suggesting the structured information is not being effectively utilized.
- What evidence would resolve it: Architectural modifications or training strategies specifically designed for structured-unstructured multimodal medical reasoning, with ablations showing improved EHR utilization.

## Limitations
- Data representation bias: PMR-Bench combines public Reddit posts, synthetic messages with EHR, and proprietary real messages that likely differ in vocabulary, structure, and urgency distribution
- Annotation quality concerns: Inter-annotator agreement varies significantly by source (α=0.85 on Reddit vs 0.63 on synthetic), and LLM validation introduces potential validation loops
- Clinical safety gaps: Focuses on ranking accuracy without addressing false negative rates for urgent messages or analyzing how ranking errors might cascade in real clinical workflows

## Confidence
- **High confidence**: The pairwise ranking formulation as a simpler alternative to multi-class classification is well-supported by the paper's evidence and theoretical arguments
- **Medium confidence**: The Bradley-Terry reward modeling advantages are demonstrated, particularly for smaller models, but the transfer from general preference modeling to medical urgency is inferred rather than empirically validated
- **Low confidence**: The position bias mitigation through two-step inference shows promise in the experiments but lacks strong external validation and rigorous testing across diverse model architectures

## Next Checks
1. Reproduce UrgentReward-8B on PMR-Synth: Using the public PMR-Synth dataset and SkyWork-Reward-v2 base model, fine-tune an UrgentReward model and verify the reported ~0.71 total accuracy
2. Position bias ablation study: Evaluate the same message pairs in both (a,b) and (b,a) orderings without the η normalization; measure how often the model's preference reverses based solely on position
3. False negative rate analysis: For the highest-performing UrgentSFT model on PMR-Real, analyze the distribution of errors where urgent messages (Levels 1-2) are incorrectly ranked below less urgent messages, calculating false negative rates stratified by urgency level