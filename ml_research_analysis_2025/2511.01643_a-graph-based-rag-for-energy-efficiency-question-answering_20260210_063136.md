---
ver: rpa2
title: A Graph-based RAG for Energy Efficiency Question Answering
arxiv_id: '2511.01643'
source_url: https://arxiv.org/abs/2511.01643
tags:
- energy
- https
- system
- answers
- questions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates a graph-based Retrieval-Augmented Generation
  (RAG) system for Energy Efficiency (EE) question answering. The system automatically
  extracts a Knowledge Graph from EE documents, then uses this graph to provide accurate
  answers in multiple languages.
---

# A Graph-based RAG for Energy Efficiency Question Answering

## Quick Facts
- arXiv ID: 2511.01643
- Source URL: https://arxiv.org/abs/2511.01643
- Reference count: 21
- Primary result: 75% accuracy on 101 EE questions using graph-based RAG

## Executive Summary
This paper presents a graph-based Retrieval-Augmented Generation (RAG) system for Energy Efficiency (EE) question answering that automatically extracts a Knowledge Graph from domain documents and uses it to provide accurate answers in multiple languages. The system achieves approximately 75% validity on a 101-question dataset with domain expert validation, showing particular strength on general EE questions (81% accuracy). The multilingual capability demonstrates minimal performance loss (4.4%) when translating between Italian and English, suggesting the architecture's potential for supporting users in achieving EE through tailored recommendations based on domain-specific knowledge.

## Method Summary
The system processes unstructured EE documents (PDFs and web pages) by chunking them into 1000-character segments, then uses an LLM (GPT-4o-mini) to extract entity-relationship triples that form a Knowledge Graph. The graph is stored in a property graph database with entity and chunk embeddings using text-embedding-3-small. For queries, the system identifies relevant entities through similarity search, traverses the graph to retrieve connected relationships and context chunks, then generates answers using another LLM call. The multilingual capability leverages semantic embeddings to handle queries in different languages from the source documents.

## Key Results
- 75.2% overall answer validity score on 101 question-answer pairs
- 81% accuracy for general EE questions
- Only 4.4% accuracy loss when translating between Italian and English
- System correctly answers 75% of questions with domain expert validation using RAGAs framework

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Structuring unstructured domain documents into a Knowledge Graph (KG) via automated LLM-based extraction creates a queryable semantic layer that preserves entity relationships better than isolated text chunks.
- Mechanism: An LLM parses document chunks to extract entity-relationship-entity triples (e.g., `Entity A` - `incentive for` - `Entity B`). These triples are normalized and stored in a graph database where nodes are entities and edges are relationships. This explicit structure allows the system to reason over connections rather than just keyword proximity.
- Core assumption: The LLM can accurately extract valid triples without significant hallucination, and the chunking process does not sever critical contextual links between entities.
- Evidence anchors:
  - [abstract]: "First, the system automatically extracts a Knowledge Graph (KG) from guidance and regulatory documents..."
  - [section 3.1]: "The extraction phase then follows, consisting of the use of an LLM-based algorithm to parse the chunks with the aim of automatically extracting entities and relationships..."
  - [corpus]: Neighbor papers like *NodeRAG* and *Optimizing open-domain question answering* corroborate that structuring RAG with graphs enhances reasoning over complex data.
- Break condition: If source documents are extremely noisy or use highly ambiguous terminology, the extraction process may produce a disconnected or noisy graph, degrading retrieval relevance.

### Mechanism 2
- Claim: A local, entity-based retrieval process grounds the reasoning in specific graph structures, improving context relevance for domain-specific questions.
- Mechanism: The system embeds the user's question and identifies the top-k most similar entities in the KG. It then traverses the graph from these "anchor" entities to retrieve connected relationships and source chunks. This confines the context window to a relevant neighborhood rather than the entire corpus.
- Core assumption: User queries contain entities that have sufficient semantic similarity to nodes in the constructed KG, and the most relevant information is located within a small graph neighborhood of those entities.
- Evidence anchors:
  - [abstract]: "Then, the generated graph is navigated and reasoned upon to provide users with accurate answers..."
  - [section 3.3]: "The retrieval paradigm relies on a local, entity-based reasoning process. It first identifies relevant Entity objects in the KG by comparing them with the user question using a similarity measure..."
  - [corpus]: *Clue-RAG* supports the efficacy of query-driven iterative retrieval on graph structures for improving accuracy.
- Break condition: If a user's question is abstract or refers to concepts not explicitly named in the source documents, the initial entity lookup may fail, returning no context.

### Mechanism 3
- Claim: Multilingual capability is achieved by decoupling the language of the query from the language of the stored knowledge using cross-lingual semantic embeddings.
- Mechanism: The system builds the KG from source documents (e.g., Italian regulations) but computes embeddings using a multilingual model. When a query arrives in a different language (e.g., English), the same embedding model projects it into the shared vector space, allowing the retrieval of the correct Italian-sourced entities. The generator LLM then formulates the answer in the user's language.
- Core assumption: The embedding model creates a language-agnostic semantic space where concepts from different languages cluster together, and the generator LLM is sufficiently fluent in the target language.
- Evidence anchors:
  - [abstract]: "...featuring promising multilingual abilities (4.4% accuracy loss due to translation)."
  - [section 6]: "English responses perform almost as well as Italian ones... demonstrates how the multilingual capabilities of LLMs allow for valid semantic comprehension when information is provided or requested in different languages."
  - [corpus]: Weak direct evidence for this specific cross-lingual mechanism in graph-RAG within the provided neighbors.
- Break condition: Performance degrades for low-resource languages or domain-specific idioms not well-represented in the embedding model's training data.

## Foundational Learning

- Concept: **Knowledge Graphs (KG) & Triples**
  - Why needed here: This is the core data structure replacing/augmenting a simple vector index. You must understand how nodes (entities) and edges (relationships) form triples (`head`, `relation`, `tail`) to debug extraction and query traversal.
  - Quick check question: Given the sentence "Italy offers a 50% tax deduction for energy retrofitting," what are the candidate head and tail entities, and what is the relationship?

- Concept: **Local vs. Global Reasoning in RAG**
  - Why needed here: The paper specifies a *local* reasoning process. You need to distinguish between retrieving isolated chunks (global search) vs. traversing from a specific entity to its neighbors (local search) to understand the retrieval logic.
  - Quick check question: If a user asks about "incentives," does a local search retrieve all incentives globally, or only those connected to the specific "user location" entity found in the query?

- Concept: **RAGAs Framework (Evaluation)**
  - Why needed here: The validation relies on the RAGAs framework (faithfulness, answer relevance, context relevance). Understanding these metrics is required to interpret the 75% validity score and replicate the evaluation.
  - Quick check question: If an answer is factually correct but cites an irrelevant document chunk, which RAGAs metric would primarily be penalized?

## Architecture Onboarding

- Component map:
  - Ingestion: PDFs/Websites -> Text Cleaner -> Chunker (LangChain `RecursiveCharacterTextSplitter`) -> LLM-based Triple Extractor (GPT-4o-mini)
  - Storage: Knowledge Base (Graph DB with custom ONTO ontology) + Auxiliary Tables (User Metadata) + Vector Index (for entity/chunk embeddings)
  - Runtime: User Question -> Retriever (Entity Extractor -> Vector Similarity Search -> Graph Traversal) -> Context Builder -> Natural Language Formatter (GPT-4o-mini) -> Answer

- Critical path: The **Triple Extraction Prompt**. The entire system's intelligence depends on the quality of the graph. If the extraction prompt fails to capture key regulatory relationships (e.g., "valid until," "applies to"), the graph will be hollow, and retrieval will fail.

- Design tradeoffs:
  - **Chunk Size (1000 chars)**: Larger chunks provide context for extraction but increase token cost and noise. Smaller chunks may sever relationships.
  - **Similarity Threshold (t=0.5)**: Setting this lower increases recall (finds more entities) but introduces noise (irrelevant graph paths). Setting it higher increases precision but may return "no results" for slightly phrased queries.
  - **Graph vs. Vector-only**: The graph adds a processing layer (extraction) and query complexity (traversal) in exchange for better handling of multi-hop reasoning.

- Failure signatures:
  - **"No results found"**: Entity similarity score falls below threshold `t`. Indicates a vocabulary mismatch or a gap in the source documents.
  - **Hallucinated regulation**: The LLM formats an answer not supported by the retrieved context (Low Faithfulness).
  - **Empty/Trivial Graph**: The extraction prompt was too restrictive or the source documents were unparseable.

- First 3 experiments:
  1. **Baseline Comparison**: Run the ablation study (LLM-only) vs. Vector-RAG vs. Graph-RAG on the 101-question dataset to validate the reported 75% accuracy uplift.
  2. **Threshold Sensitivity**: Vary the similarity threshold `t` (e.g., 0.5, 0.6, 0.75) and measure the trade-off between "no result" rates and answer relevance.
  3. **Triple Extraction Audit**: Manually inspect a sample of extracted triples from a complex regulatory document to verify the LLM is capturing temporal and conditional constraints (e.g., "valid if X") correctly.

## Open Questions the Paper Calls Out
None

## Limitations
- The LLM-based triple extraction process is a potential bottleneck—errors compound through the pipeline and are difficult to debug without transparency into the extraction prompt and intermediate outputs.
- The system's performance on out-of-scope or abstract questions is untested; the entity-based retrieval may fail when user queries lack explicit named entities.
- Scalability concerns exist for real-world deployment—graph traversal and context serialization may hit token limits for complex queries.

## Confidence
- Confidence: High for the core graph-RAG workflow (extraction → storage → retrieval → generation)
- Confidence: Medium for the quantitative results due to small dataset size and expert-only validation
- Confidence: Low for the cross-lingual mechanism due to minimal technical detail on implementation

## Next Checks
1. **Ablation Study**: Compare Graph-RAG accuracy against a plain vector-RAG baseline on the same 101-question dataset to isolate the contribution of the graph structure.
2. **Extraction Quality Audit**: Manually review a stratified sample of extracted triples from source documents to assess hallucination rates and relationship coverage.
3. **Multilingual Robustness Test**: Evaluate system performance on low-resource languages or domain-specific terminology not present in the embedding model's training data.