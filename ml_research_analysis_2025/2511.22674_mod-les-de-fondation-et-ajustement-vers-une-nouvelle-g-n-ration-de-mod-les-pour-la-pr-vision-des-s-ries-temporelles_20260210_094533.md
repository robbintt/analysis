---
ver: rpa2
title: "Mod\xE8les de Fondation et Ajustement : Vers une Nouvelle G\xE9n\xE9ration\
  \ de Mod\xE8les pour la Pr\xE9vision des S\xE9ries Temporelles"
arxiv_id: '2511.22674'
source_url: https://arxiv.org/abs/2511.22674
tags:
- pour
- vision
- ries
- donn
- temporelles
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper presents a comprehensive review of foundation models
  for time series forecasting and investigates the impact of fine-tuning these models
  on specific datasets. The authors examine three main components: architecture (Transformer-based
  encoder-only, encoder-decoder, and decoder-only models), pretraining strategies
  (self-supervised learning on large datasets), and adaptation through fine-tuning.'
---

# Modèles de Fondation et Ajustement : Vers une Nouvelle Génération de Modèles pour la Prévision des Séries Temporelles

## Quick Facts
- arXiv ID: 2511.22674
- Source URL: https://arxiv.org/abs/2511.22674
- Reference count: 0
- Three foundation models (Moirai, Chronos, TimesFM) evaluated across 92 configurations show fine-tuning consistently improves forecasting performance

## Executive Summary
This paper presents a comprehensive review and experimental evaluation of foundation models for time series forecasting, focusing on the impact of fine-tuning. The authors examine three main components: architecture (Transformer-based encoder-only, encoder-decoder, and decoder-only models), pretraining strategies (self-supervised learning on large datasets), and adaptation through fine-tuning. They evaluate three models - Moirai (encoder-only), Chronos (encoder-decoder), and TimesFM (decoder-only) - across 92 unique configurations from the GIFT-Eval benchmark covering 15 univariate and 8 multivariate datasets. Their experiments show that fine-tuning consistently improves zero-shot forecasting performance, particularly for long-term horizons and smaller datasets.

## Method Summary
The study evaluates three pre-trained foundation models (Moirai-1.1-R-Large, Chronos-Bolt-Base, and TimesFM-2.0) through full fine-tuning on the GIFT-Eval benchmark datasets. Each model is fine-tuned using specific hyperparameters: Moirai uses learning rate 1e-7, batch size 64, and 4000 iterations; Chronos uses learning rate 1e-5, batch size 256, and 4000 iterations; TimesFM uses learning rate 1e-6, batch size 128, and 4000 iterations. The evaluation measures performance using Mean Weighted Quantile Loss (MWQL) and Mean Absolute Percentage Error (MAPE), normalized against a Seasonal Naive baseline across 15 univariate and 8 multivariate datasets.

## Key Results
- Fine-tuning consistently improves zero-shot forecasting capabilities, especially for long-term horizons
- Full fine-tuning with appropriate hyperparameters significantly enhances forecasting accuracy
- MWQL improves systematically during fine-tuning, but MAPE may degrade, particularly on large datasets
- Fine-tuning is particularly effective for small and medium-sized datasets, consolidating the learning of recurring patterns

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Segmenting time series into patches (sub-sequences) allows Transformer models to capture local motifs effectively while managing the quadratic complexity of attention mechanisms.
- **Mechanism:** Instead of processing individual time points, models like TimesFM and Moirai divide input sequences into fixed or variable-length patches. This reduces the sequence length fed into the attention layer, lowering computational cost, while allowing the model to aggregate local temporal features into meaningful tokens before modeling global dependencies.
- **Core assumption:** Local contiguous segments of time series contain distinct patterns (motifs) that are more semantically significant than individual time steps for long-term forecasting.
- **Evidence anchors:**
  - [section] Section 3.1.1 describes patching (segmentation) as a method to "better capture local motifs" and "reduce the complexity of the Transformer."
  - [corpus] The neighbor paper *PatchFormer* abstract supports this, explicitly citing "patch-based time series foundation model[s]" for hierarchical reconstruction.
- **Break condition:** If the time series exhibits extreme volatility where local patterns (patches) are noise rather than signal, or if the patch size aligns poorly with the true periodicity of the data (e.g., patch size 12 for a 7-day seasonality), this mechanism may fail to generalize.

### Mechanism 2
- **Claim:** Full fine-tuning (FFT) transfers generalizable features from pretraining to specific domains, significantly improving performance on long-term horizons.
- **Mechanism:** Pretraining exposes the model to vast, diverse dynamics. Fine-tuning on a target domain (even with limited data) adjusts the model's weights to align these general dynamics with the specific "recurring patterns" and distributional quirks of the target dataset. This adaptation is particularly effective for long horizons where zero-shot models often drift or lack specific trend information.
- **Core assumption:** The knowledge acquired during pretraining is relevant (transferable) to the target domain, and the target dataset contains sufficient signal to specialize the model without catastrophic forgetting.
- **Evidence anchors:**
  - [abstract] States fine-tuning "improves zero-shot forecasting capabilities, especially for long-term horizons."
  - [section] Section 4.3 notes that for small/medium datasets, models "consolidated the learning of recurring patterns," improving MWQL significantly.
- **Break condition:** If the pretraining and target domains are fundamentally disjoint (e.g., training on chaotic biological signals and testing on highly regulated linear utility demand), or if the learning rate is too high, FFT may lead to overfitting rather than useful adaptation.

### Mechanism 3
- **Claim:** Optimizing for probabilistic metrics (Quantile Loss) during training enhances uncertainty estimation but may degrade point-estimate accuracy (MAPE) if the median prediction is not explicitly prioritized.
- **Mechanism:** Models like Moirai and TimesFM minimize quantile loss (or mixture density likelihood) rather than Mean Absolute Error. This forces the model to learn the full distribution of outcomes. However, if the distribution is multimodal or skewed, the median (used for MAPE) might become unstable or shift away from the optimal point forecast, leading to the observed trade-off where MWQL improves but MAPE worsens.
- **Core assumption:** The quantile loss function correctly penalizes the specific error distribution of the target domain, and the median (or central quantile) is the correct proxy for point forecasts.
- **Evidence anchors:**
  - [section] Section 4.3 observes that MWQL improves systematically, but MAPE degrades on large datasets. The authors attribute this to the fact that "the quantile loss function used... privileges the optimization of MWQLn, sometimes to the detriment of MAPEn."
  - [corpus] *Zero-Shot Time Series Forecasting with Covariates via In-Context Learning* (neighbor) also emphasizes probabilistic forecasting objectives, suggesting a field-wide shift away from point-optimization.
- **Break condition:** If the user strictly requires point-forecast accuracy (e.g., for inventory counts where uncertainty is secondary), relying solely on these probabilistic objectives without a specific point-forecast head may yield suboptimal results.

## Foundational Learning

- **Concept:** **Quantile Regression vs. Point Estimation**
  - **Why needed here:** The paper highlights a divergence in performance metrics (MWQL vs. MAPE). Understanding that the model is learning a *distribution* (probabilistic) rather than a single *line* (point estimation) is critical to interpreting why fine-tuning might improve confidence intervals while apparently worsening specific point predictions.
  - **Quick check question:** Can you explain why minimizing the 50th percentile loss (median) might yield different results than minimizing Mean Squared Error (mean)?

- **Concept:** **Zero-Shot vs. Fine-Tuning Transfer**
  - **Why needed here:** The core contribution of the paper is evaluating the transition from zero-shot to fine-tuned states. You must grasp that "Zero-Shot" implies applying fixed weights to unseen data, whereas "Fine-Tuning" adapts those weights, creating a tension between generalization and specialization.
  - **Quick check question:** In the context of the GIFT-Eval benchmark, does "Zero-Shot" mean the model has never seen *any* time series data, or just not the *specific* test datasets?

- **Concept:** **Transformer Inductive Biases (Patching & Masking)**
  - **Why needed here:** The paper compares Encoder-only (masking) vs. Decoder-only (autoregressive) architectures. Understanding how these architectures process time (patch-by-patch vs. step-by-step) is necessary to select the right model (Moirai vs. TimesFM) for a given latency or accuracy constraint.
  - **Quick check question:** How does the "masking" mechanism in an Encoder-only model differ from the "causal attention" in a Decoder-only model during inference?

## Architecture Onboarding

- **Component map:** Raw Series -> Patching/Segmentation -> Projection -> Transformer Backbone -> Distribution Parameters -> Quantile Prediction
- **Critical path:** The data must flow from raw series → Patching → Projection → Transformer → Distribution Parameters → Quantile Prediction. The critical failure point is the **Input Patching**: misaligned patch sizes can destroy local frequency information before it reaches the Transformer.
- **Design tradeoffs:**
  - **Moirai (Encoder-only):** Uses masking; predicts all horizons in parallel (faster inference), but requires variable patch logic for different frequencies.
  - **TimesFM (Decoder-only):** Autoregressive generation; naturally handles streaming data but can suffer from error accumulation over very long horizons (though the paper notes it predicts longer output patches to mitigate this).
  - **FFT vs. Zero-Shot:** FFT adds training time and risk of overfitting but is empirically shown to be necessary for medium/long horizons.
- **Failure signatures:**
  - **MWQL improves but MAPE explodes:** Likely due to training on quantile loss while evaluating on the median; check for outliers or skewed distributions in the target set.
  - **Degradation on Large Datasets:** The paper notes models didn't see all examples in large datasets (fixed iterations). Signal: Training loss drops but validation MWQL plateaus or rises early.
- **First 3 experiments:**
  1. **Baseline Zero-Shot Evaluation:** Run Moirai and TimesFM on the target domain without training to establish a lower bound and identify specific horizons where error spikes.
  2. **Ablation on Patch Size (if applicable):** If using Moirai, test predefined patch sizes against the frequency of the target data to verify the automatic selection logic.
  3. **Full Fine-Tuning (FFT) Sweep:** Retrain the model on the target training set using the paper's specified hyperparameters (e.g., LR 1e-7 for Moirai), specifically evaluating the delta in MWQL vs. MAPE to confirm the trade-off exists in your data.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does optimal hyperparameter selection (e.g., learning rate, regularization, gradient iterations) specifically mitigate overfitting and distribution shift in sensitive domains like Energy?
- Basis in paper: [explicit] The authors state that the study of optimal hyperparameter selection is left for future work and suggest that specific tuning and regularization are needed to address performance degradation observed in the Energy domain.
- Why unresolved: The experiments used fixed, default hyperparameters across all datasets to manage computational load, potentially masking the benefits of dataset-specific tuning.
- What evidence would resolve it: An ablation study varying learning rates and regularization strengths specifically on the Energy datasets, showing a reduction in the MWQL degradation observed in the fixed-parameter experiments.

### Open Question 2
- Question: What are the theoretical mechanisms governing generalization and information compression in time series foundation models compared to their NLP counterparts?
- Basis in paper: [explicit] The conclusion explicitly notes a lack of theoretical and empirical studies explaining the generalization behavior and information compression of these models, particularly given the high heterogeneity of time series data.
- Why unresolved: The field is very recent, and the diversity of time series data (domains, frequencies) complicates the definition of a "universal" forecaster compared to the more standardized structures in natural language.
- What evidence would resolve it: Theoretical analysis or probing experiments that visualize and quantify how different architectures (encoder-only vs. decoder-only) compress temporal dynamics and adapt to unseen distributions.

### Open Question 3
- Question: Can modified loss functions or multi-objective optimization strategies prevent the degradation of point forecast accuracy (MAPE) while improving probabilistic metrics (MWQL) during full fine-tuning?
- Basis in paper: [inferred] The authors report that while full fine-tuning consistently improves MWQL, it frequently degrades MAPE, especially on large datasets, because the quantile loss optimizes for the distribution rather than the median.
- Why unresolved: The paper focuses on standard quantile loss and likelihoods; it does not explore whether balancing these objectives could stabilize performance across both metric types.
- What evidence would resolve it: Experiments comparing standard quantile loss against composite loss functions (e.g., combining quantile loss with MAE or MSE) to determine if MAPE can be preserved without sacrificing probabilistic calibration.

## Limitations

- The study's results depend heavily on the specific hyperparameter settings for fine-tuning, particularly learning rates and iteration counts
- The fixed 4000 iteration limit for large datasets may not provide sufficient exposure to the full data distribution
- The evaluation focuses primarily on MWQL and MAPE metrics, with limited discussion of computational efficiency or real-world deployment considerations

## Confidence

- **High Confidence:** The mechanism of patch-based input processing and its impact on local motif capture (Mechanism 1)
- **Medium Confidence:** The effectiveness of full fine-tuning across different dataset sizes and forecasting horizons (Mechanism 2)
- **Medium Confidence:** The observed trade-off between MWQL and MAPE optimization (Mechanism 3)

## Next Checks

1. **Generalization Test:** Validate findings on at least one dataset outside the GIFT-Eval benchmark to assess real-world applicability
2. **Computational Efficiency Analysis:** Measure inference latency and memory requirements for all three architectures under identical hardware conditions
3. **Distribution Shift Experiment:** Test model performance when fine-tuning data distribution significantly differs from pretraining data to quantify transfer learning limits