---
ver: rpa2
title: 'The Hidden Space of Safety: Understanding Preference-Tuned LLMs in Multilingual
  context'
arxiv_id: '2504.02708'
source_url: https://arxiv.org/abs/2504.02708
tags:
- alignment
- multilingual
- arxiv
- languages
- english
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study analyzes the alignment effectiveness of large language
  models (LLMs) across multiple languages, revealing a significant monolingual bias
  where models perform well in English but exhibit inconsistent safety alignment in
  other languages. The authors use PCA-based visualization and distributional metrics
  (Bhattacharyya Distance, Silhouette Score, and Between-Class Variance) to quantify
  the separation between harmful and harmless clusters in embedding spaces before
  and after alignment.
---

# The Hidden Space of Safety: Understanding Preference-Tuned LLMs in Multilingual context

## Quick Facts
- arXiv ID: 2504.02708
- Source URL: https://arxiv.org/abs/2504.02708
- Reference count: 12
- Primary result: Preference-tuned LLMs exhibit significant monolingual bias, with English showing 61.20% between-class variance improvement in safety alignment versus 19.98% (Hindi), 10.09% (Chinese), and 26.85% (German)

## Executive Summary
This study analyzes how large language models align with safety constraints across multiple languages, revealing that models exhibit significantly weaker safety alignment in non-English languages despite similar pre-training quality. Using PCA-based visualization and distributional metrics (Bhattacharyya Distance, Silhouette Score, and Between-Class Variance), the authors quantify the separation between harmful and harmless clusters in embedding spaces before and after alignment. The findings demonstrate that while alignment improves cluster separation in English, the improvement is substantially lower for Hindi, Chinese, and German, suggesting a need for language-specific fine-tuning to ensure equitable multilingual safety alignment.

## Method Summary
The study uses a pipeline of extracting final-layer embeddings from reference and aligned LLM checkpoints, applying PCA dimensionality reduction, and computing distributional metrics to compare harmful versus harmless clusters. The Balanced Toxicity Dataset provides 5,000 prompts per language (2,500 toxic, 2,500 non-toxic), while the Multilingual Parallel Text-Detoxification Dataset tests semantic preservation. Analysis is performed at the input processing stage only, comparing cluster separation metrics between pre-alignment (πref) and post-alignment (πθ) models across four languages and seven model families.

## Key Results
- English safety alignment shows between-class variance improvement from 0.81% to 61.20%, while Hindi improves only 19.98%, Chinese 10.09%, and German 26.85%
- Hindi exhibits decreased separation post-alignment in some models, indicating potential negative transfer from English preference data
- Parallel text-detoxification task shows no cluster separation for Hindi, suggesting alignment fails to capture semantic equivalence between toxic and detoxified pairs

## Why This Works (Mechanism)

### Mechanism 1: Alignment-Induced Separation in Latent Space
Preference optimization reshapes the embedding space to create geometrically separable clusters for harmful versus harmless content. The mechanism maximizes divergence between preferred (safe) and dispreferred (unsafe) completions, which manifests as measurable cluster separation in hidden representations. This separation can be quantified via between-class variance and Bhattacharyya distance, with the core assumption that safety alignment quality correlates with the degree of separation between harmful/harmless clusters in the latent space.

### Mechanism 2: Monolingual Transfer Failure in Alignment
English-centric preference datasets produce alignment that transfers inequitably to non-English languages, leaving harmful/harmless clusters overlapping in underrepresented languages. The reward function and policy are optimized predominantly on English preference pairs, resulting in high between-class variance in English (61.20%) but substantially lower separation in Hindi (19.98%), Chinese (10.09%), and German (26.85%). The core assumption is that cluster separability at the input-embedding level predicts downstream safety behavior across languages.

### Mechanism 3: Input-Stage Probing Isolates Alignment Effects
Probing final-layer embeddings at the input encoding stage captures how models internally represent safety distinctions, minimizing confounds from generation dynamics. By extracting hidden states after prompt encoding (before token generation), the analysis measures static representation quality rather than generation-time behavior, reducing contamination from memorization or output-stage artifacts. The core assumption is that safety understanding is encoded in hidden representations during input processing, not solely emergent during generation.

## Foundational Learning

### Concept: Preference Optimization (RLHF/DPO/KTO)
Why needed: The paper's entire premise is that preference tuning reshapes representations; understanding the Bradley-Terry model and divergence-based objectives is essential to interpret why alignment creates cluster separation.
Quick check: How does DPO eliminate the need for an explicit reward model compared to RLHF?

### Concept: PCA and Cluster Quality Metrics (Bhattacharyya Distance, Silhouette Score, Between-Class Variance)
Why needed: The methodology quantifies alignment via geometric properties of embedding clusters; without understanding these metrics, you cannot interpret Figure 4 or Table 2.
Quick check: What does a Bhattacharyya distance of 2.59 (post-alignment English) versus 0.09 (post-alignment Chinese) indicate about cluster overlap?

### Concept: Multilingual LLM Training Pipeline (Pre-training → SFT → Preference Tuning)
Why needed: The paper identifies where alignment gaps emerge; distinguishing what each stage contributes clarifies whether the problem is pre-training representation quality or preference-data distribution.
Quick check: At which training stage is the English-centric bias primarily introduced—pre-training, SFT, or preference tuning?

## Architecture Onboarding

### Component Map:
Reference model (πref) -> Aligned model (πθ) -> Embedding extractor (final-layer hidden states) -> Dimensionality reducer (PCA) -> Metric suite (Bhattacharyya Distance, Silhouette Score, Between-class variance)

### Critical Path:
1. Load paired reference and aligned checkpoints for target model family
2. Prepare Balanced Toxicity Dataset (2,500 toxic + 2,500 non-toxic per language)
3. Run prompts through models; extract final-layer embeddings at input stage
4. Fit PCA on combined embeddings; project to 10 components
5. Compute cluster separation metrics for each language × model combination
6. Compare πref vs πθ to quantify alignment-induced shift

### Design Tradeoffs:
- Final layer vs. intermediate layers: Final layer captures semantic abstractions but may miss early-layer language-agnostic features; Assumption: safety distinctions consolidate in later layers
- 2 vs. 10 PCA components: 2 components enable visualization (~50% variance); 10 components provide metric stability
- Language selection: Hindi/Chinese/German are medium-resource; findings may understate gaps for truly low-resource languages

### Failure Signatures:
- Hindi showing decreased separation post-alignment in some models indicates potential negative transfer from English preference data
- Chinese consistently showing <11% between-class variance improvement across all models suggests structural representation issues beyond data scarcity
- Parallel text-detoxification task showing no cluster separation for Hindi indicates alignment does not capture semantic equivalence of toxic/detoxified pairs

### First 3 Experiments:
1. Replicate baseline visualization: Extract embeddings from Llama-2-7b and Llama-2-7b-chat for English prompts; plot PCA projections of harmful/harmless clusters to verify 60%+ between-class variance improvement
2. Add a low-resource language: Extend the pipeline to Swahili or Bengali to test whether the alignment gap widens further beyond medium-resource languages
3. Test translation-augmented preference data: Construct synthetic Hindi preference pairs by translating English safety data; measure whether cluster separation improves, probing whether data scarcity is the bottleneck

## Open Questions the Paper Calls Out
- Does the severity of alignment bias (monolingual bias) increase as the resource-level of the language decreases? The current study covers only medium-resource languages and calls for extending analysis to low-resource languages.
- Do the observed representation disparities in safety alignment generalize to other alignment domains such as reasoning or planning? The paper notes it focuses on safety alignment and overlooks other critical domains.
- Can synthetic data generation effectively transfer alignment properties from English to other languages without causing semantic drift? The paper suggests exploring synthetic techniques for underrepresented languages but notes this remains unproven.

## Limitations
- The study focuses on medium-resource languages (Hindi, Chinese, German) and does not address whether gaps are more severe for truly low-resource languages
- Input-stage probing may miss safety mechanisms that operate during generation rather than being encoded in input representations
- The analysis does not distinguish whether tokenizer quality, pre-training representations, or preference data scarcity drives the observed alignment gaps

## Confidence
- High Confidence: The empirical finding that English alignment produces significantly higher between-class variance (61.20%) than other languages is directly measurable and reproducible
- Medium Confidence: The claim that input-stage probing captures alignment effects is methodologically sound but may not represent the complete safety mechanism
- Medium Confidence: The recommendation for language-specific fine-tuning is reasonable but does not distinguish the root cause of alignment gaps

## Next Checks
1. Apply the methodology to Swahili or Bengali to test whether alignment gaps scale with resource scarcity beyond the medium-resource languages studied
2. Generate synthetic Hindi preference pairs by translating English safety data, then measure whether cluster separation improves to determine if data scarcity is the primary bottleneck
3. Extend the analysis to include intermediate and output-stage embeddings to verify whether safety representations consolidate during input encoding or emerge dynamically during generation