---
ver: rpa2
title: Repeat-bias-aware Optimization of Beyond-accuracy Metrics for Next Basket Recommendation
arxiv_id: '2501.06362'
source_url: https://arxiv.org/abs/2501.06362
tags:
- repeat
- item
- items
- fairness
- diversity
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper tackles the issue of heavy repeat bias in next basket\
  \ recommendation (NBR), where models tend to recommend mostly repeat items to maximize\
  \ accuracy, hurting beyond-accuracy objectives like diversity and item fairness.\
  \ The authors propose two model-agnostic re-ranking algorithms\u2014RADiv and RAIF\u2014\
  that jointly optimize diversity, item fairness, and repeat bias mitigation via mixed-integer\
  \ linear programming."
---

# Repeat-bias-aware Optimization of Beyond-accuracy Metrics for Next Basket Recommendation

## Quick Facts
- arXiv ID: 2501.06362
- Source URL: https://arxiv.org/abs/2501.06362
- Reference count: 0
- Primary result: Proposes RADiv and RAIF algorithms to jointly optimize diversity, item fairness, and repeat bias mitigation in next basket recommendation via mixed-integer linear programming.

## Executive Summary
This paper addresses the heavy repeat bias problem in next basket recommendation (NBR), where models tend to recommend mostly repeat items to maximize accuracy metrics like Recall. The authors observe that this behavior negatively impacts beyond-accuracy objectives such as diversity and item fairness. To tackle this, they propose two model-agnostic re-ranking algorithms that use mixed-integer linear programming to jointly optimize diversity, item fairness, and repeat bias mitigation. The algorithms are evaluated on three real-world grocery datasets and demonstrate significant improvements in diversity and item fairness while reducing repeat bias, at an acceptable cost to Recall.

## Method Summary
The paper proposes two re-ranking algorithms, RADiv and RAIF, designed to mitigate heavy repeat bias in NBR while optimizing beyond-accuracy metrics. Both algorithms use mixed-integer linear programming (MILP) to jointly optimize diversity, item fairness, and repeat bias mitigation. RADiv focuses on diversity optimization, while RAIF targets item fairness. The algorithms work by calculating repeat ratios and ensuring that the deviation between repeat ratios in recommendations and ground truth is minimized. The authors also adapt these algorithms for unified and combined NBR paradigms, where the number of items to recommend is unknown or needs to be determined. The MILP formulation incorporates constraints for diversity, fairness, and repeat bias, allowing for a balanced optimization of multiple objectives.

## Key Results
- RADiv and RAIF significantly improve diversity and item fairness in NBR tasks
- Both algorithms effectively reduce repeat bias while maintaining acceptable Recall performance
- The proposed methods outperform baseline approaches on three real-world grocery datasets
- The algorithms are model-agnostic and can be adapted to different NBR paradigms (unified and combined)

## Why This Works (Mechanism)
The proposed algorithms work by explicitly modeling the trade-off between accuracy, diversity, item fairness, and repeat bias in the optimization process. By using mixed-integer linear programming, the algorithms can enforce constraints that ensure a balanced recommendation list, where repeat items are not overly favored, and diverse and fair recommendations are promoted. The key insight is that by jointly optimizing these objectives, the algorithms can mitigate the heavy repeat bias problem that plagues traditional NBR models, which tend to prioritize accuracy at the expense of beyond-accuracy metrics.

## Foundational Learning

### Mixed-Integer Linear Programming (MILP)
- **Why needed**: To formulate the optimization problem that jointly considers diversity, fairness, and repeat bias constraints
- **Quick check**: Can you define the objective function and constraints for a simple MILP problem?

### Beyond-accuracy Metrics
- **Why needed**: To evaluate the quality of recommendations beyond traditional accuracy metrics like Recall
- **Quick check**: Can you explain the difference between accuracy and beyond-accuracy metrics in recommender systems?

### Repeat Bias in NBR
- **Why needed**: To understand the problem that the proposed algorithms aim to solve
- **Quick check**: Can you describe how repeat bias manifests in next basket recommendation and its impact on recommendation quality?

## Architecture Onboarding

### Component Map
- Input data (ground truth baskets and candidate recommendations) -> Repeat ratio calculation -> Mixed-integer linear programming formulation -> Re-ranked recommendations

### Critical Path
1. Calculate repeat ratios from ground truth data
2. Formulate MILP problem with diversity, fairness, and repeat bias constraints
3. Solve MILP to obtain re-ranked recommendations
4. Evaluate performance using accuracy and beyond-accuracy metrics

### Design Tradeoffs
- The MILP formulation allows for a balanced optimization of multiple objectives but may increase computational complexity
- The model-agnostic nature of the algorithms enables their application to various NBR paradigms but may limit the ability to exploit specific model characteristics

### Failure Signatures
- If the MILP problem is infeasible or takes too long to solve, it may indicate overly strict constraints or an excessively large candidate set
- If the re-ranked recommendations do not show significant improvements in diversity or fairness, it may suggest that the constraints are not properly formulated or that the optimization process is not effectively balancing the objectives

### 3 First Experiments
1. Implement the repeat ratio calculation and verify its correctness on a small dataset
2. Formulate a simple MILP problem with only diversity constraints and solve it for a toy example
3. Integrate the MILP solver with the repeat ratio calculation and evaluate the performance on a benchmark dataset

## Open Questions the Paper Calls Out
None

## Limitations
- The computational complexity of solving MILP problems may limit the scalability of the proposed algorithms
- The effectiveness of the algorithms may depend on the specific characteristics of the datasets and the NBR paradigms
- The paper focuses on grocery datasets, and the generalizability of the results to other domains is unclear

## Confidence
- Effectiveness of the proposed algorithms: High
- Scalability and computational efficiency: Medium
- Generalizability to other domains: Low

## Next Checks
1. Evaluate the scalability of the proposed algorithms on larger datasets and compare the computational time with baseline methods
2. Conduct ablation studies to assess the impact of individual constraints (diversity, fairness, repeat bias) on the overall performance
3. Test the generalizability of the algorithms by applying them to NBR tasks in different domains (e.g., e-commerce, streaming services) and comparing the results with those obtained on grocery datasets