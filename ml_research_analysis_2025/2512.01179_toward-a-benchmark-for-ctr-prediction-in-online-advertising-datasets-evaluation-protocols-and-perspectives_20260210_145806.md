---
ver: rpa2
title: 'Toward a benchmark for CTR prediction in online advertising: datasets, evaluation
  protocols and perspectives'
arxiv_id: '2512.01179'
source_url: https://arxiv.org/abs/2512.01179
tags:
- prediction
- https
- performance
- data
- datasets
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a comprehensive benchmark for click-through
  rate (CTR) prediction in online advertising, addressing the lack of standardized
  evaluation protocols in the field. The benchmark platform integrates flexible interfaces
  with datasets and modeling components, encompassing synthetic data generation algorithms,
  a multi-level taxonomy of evaluation metrics, standardized procedures, and experimental
  guidelines.
---

# Toward a benchmark for CTR prediction in online advertising: datasets, evaluation protocols and perspectives

## Quick Facts
- arXiv ID: 2512.01179
- Source URL: https://arxiv.org/abs/2512.01179
- Authors: Shan Gao; Yanwu Yang
- Reference count: 40
- One-line primary result: Proposed benchmark evaluates 15 CTR prediction models across 11 metrics on public datasets, revealing high-order models' superiority and LLM-based approaches' remarkable data efficiency

## Executive Summary
This paper addresses the critical need for standardized evaluation protocols in click-through rate (CTR) prediction by introducing a comprehensive benchmark platform. The benchmark integrates flexible interfaces with datasets and modeling components, including synthetic data generation algorithms, a multi-level taxonomy of evaluation metrics, and standardized experimental procedures. Through extensive experimentation on three public datasets (Criteo, Avazu, AntM2C) and two synthetic datasets, the study evaluates 15 state-of-the-art CTR prediction models across 11 different metrics.

The benchmark reveals several key findings: high-order models significantly outperform low-order models, large language model-based approaches demonstrate remarkable data efficiency by achieving comparable performance using only 2% of training data, and CTR prediction performance improved substantially from 2015 to 2016 before reaching a plateau. The benchmark platform facilitates model development and evaluation while enhancing practitioners' understanding of CTR prediction mechanisms. Code is made available to support reproducible research in this domain.

## Method Summary
The benchmark platform addresses CTR prediction as a binary classification task, evaluating models on three public datasets: Criteo (45M rows), Avazu (40M rows), and AntM2C (1.1M filtered rows). Preprocessing includes log-transform binning for numerical features, frequency encoding with OOV tokens for low-frequency categories, and timestamp feature extraction. The study evaluates 15 models ranging from logistic regression to deep learning architectures and LLM-based approaches, optimized using Adam with initial learning rate 0.001. Training employs adaptive batch sizing (3000-30000) to maximize GPU utilization, with early stopping on validation sets. Eleven metrics are reported, prioritizing AUC-ROC and Logloss, across two random seeds (2019, 2020).

## Key Results
- High-order models (DeepFM, PNN, GDCN) significantly outperform low-order models (LR, FM) across all datasets and metrics
- LLM-based approaches (Uni-CTR) achieve comparable performance to traditional models using only 2% of training data (30K samples vs 1.5M)
- CTR prediction performance showed substantial improvement from 2015 to 2016 before reaching a performance plateau
- The adaptive batch sizing strategy effectively maximized GPU utilization on RTX 4090 hardware

## Why This Works (Mechanism)
The benchmark's effectiveness stems from its comprehensive standardization of evaluation protocols that were previously fragmented across the CTR prediction literature. By providing consistent preprocessing pipelines, hyperparameter search spaces, and evaluation metrics across diverse model architectures, the benchmark enables fair comparison and reproducibility. The inclusion of both public and synthetic datasets allows for controlled experiments that isolate model capabilities from data-specific effects. The multi-level taxonomy of evaluation metrics captures different aspects of model performance beyond traditional AUC-ROC, including calibration (Logloss), ranking quality (COPC), and generalization ability.

## Foundational Learning

**CTR Prediction Task**: Binary classification problem predicting probability of ad clicks
- *Why needed*: Core problem definition in online advertising revenue optimization
- *Quick check*: Verify output is probability between 0-1, not binary class

**Evaluation Metrics Hierarchy**: AUC-ROC, Logloss, AUC-PR, Precision, Recall, Accuracy, MCC, F1, MSE, RMSE, COPC
- *Why needed*: Different metrics capture different aspects of model quality
- *Quick check*: AUC-ROC and Logloss prioritized as primary metrics

**Adaptive Batch Sizing**: Dynamic batch size adjustment from 3000 to 30000
- *Why needed*: Maximizes GPU utilization while preventing OOM errors
- *Quick check*: Monitor VRAM usage during training to verify effectiveness

**Frequency Encoding with OOV**: Low-frequency categories mapped to out-of-vocabulary token
- *Why needed*: Reduces model complexity and handles rare feature values
- *Quick check*: Verify category counts and OOV token assignment

## Architecture Onboarding

**Component Map**: Data preprocessing -> Model training -> Evaluation metrics -> Result aggregation
- Data pipeline: Raw data → preprocessing (binning, encoding) → feature tensors
- Training pipeline: Model instantiation → optimization (Adam) → early stopping
- Evaluation pipeline: Prediction generation → metric computation → statistical analysis

**Critical Path**: Preprocessing → Model training → Validation evaluation → Test evaluation
- Critical dependency: Consistent preprocessing across all models for fair comparison
- Bottleneck: LLM fine-tuning requires significant computational resources

**Design Tradeoffs**: 
- Batch size vs. GPU memory utilization (adaptive strategy vs. fixed sizing)
- Model complexity vs. generalization (high-order vs. low-order models)
- Computational cost vs. data efficiency (LLM approaches vs. traditional models)

**Failure Signatures**:
- OOM errors indicate batch size too large for available GPU memory
- Inconsistent model rankings suggest preprocessing or hyperparameter issues
- Poor LLM performance may indicate insufficient fine-tuning or suboptimal prompt templates

**3 First Experiments**:
1. Train LR and FM models on Criteo dataset to verify basic benchmark functionality
2. Test adaptive batch sizing strategy with varying batch size limits
3. Compare Uni-CTR performance on 2% vs full training data to confirm data efficiency claims

## Open Questions the Paper Calls Out
None identified in the paper.

## Limitations
- LLM-based approaches require significant computational resources for fine-tuning, limiting accessibility
- Adaptive batch size strategy may not translate to smaller GPU configurations
- Absence of explicit binning threshold values introduces ambiguity in preprocessing replication
- Performance plateau after 2016 may reflect data-specific characteristics rather than universal model limitations

## Confidence

**High confidence**:
- Low- and mid-order models' performance hierarchy (high-order models outperform low-order models)
- Benchmark infrastructure reproducibility (15 models, 11 metrics, 3 public datasets)
- Data efficiency claims for LLM-based models (comparable performance with 2% training data)

**Medium confidence**:
- Synthetic data generation utility
- Comprehensive taxonomy of evaluation metrics
- Experimental guidelines applicability

**Low confidence**:
- Generalization of CTR performance plateau beyond specific timeframes studied
- Universality of adaptive batching strategy across different hardware configurations

## Next Checks

1. Verify the exact binning threshold value used in Criteo preprocessing by testing standard thresholds (e.g., 2, 10) and comparing resulting model performance
2. Conduct controlled experiments on smaller GPUs to validate adaptive batch size strategy and establish minimum hardware requirements
3. Replicate the Uni-CTR fine-tuning experiment with varying subsample sizes (1%, 2%, 5%) to confirm the data efficiency claims across different learning rates and optimization settings