---
ver: rpa2
title: Enabling External Scrutiny of AI Systems with Privacy-Enhancing Technologies
arxiv_id: '2502.05219'
source_url: https://arxiv.org/abs/2502.05219
tags:
- systems
- openmined
- researchers
- external
- have
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'OpenMined''s technical infrastructure combines several privacy-enhancing
  technologies (secure enclaves, secure multi-party computation, zero-knowledge proofs,
  federated learning, and differential privacy) into end-to-end software that enables
  external scrutiny of AI systems while protecting sensitive data. Two case studies
  demonstrate its effectiveness: (1) With Christchurch Call, researchers analyzed
  video recommendation algorithms without accessing proprietary data, finding no disproportionate
  amplification of suggestive content; (2) With UK AISI, secure enclaves enabled model
  safety evaluations where neither the AI model weights nor the evaluation dataset
  were exposed to the other party.'
---

# Enabling External Scrutiny of AI Systems with Privacy-Enhancing Technologies

## Quick Facts
- arXiv ID: 2502.05219
- Source URL: https://arxiv.org/abs/2502.05219
- Reference count: 21
- One-line primary result: Privacy-enhancing technologies enable external AI system audits without exposing proprietary data or model weights

## Executive Summary
This paper presents OpenMined's technical infrastructure that combines multiple privacy-enhancing technologies to enable external scrutiny of AI systems while protecting sensitive data. The approach uses secure enclaves, secure multi-party computation, zero-knowledge proofs, federated learning, and differential privacy to create an end-to-end solution where researchers can audit AI systems without accessing proprietary data or model weights. Two case studies demonstrate the approach: analyzing video recommendation algorithms with Christchurch Call and conducting model safety evaluations with UK AISI, both successfully preserving data privacy while enabling meaningful external research.

## Method Summary
The method involves a remote workflow where researchers propose questions and code to model owners, who can approve or deny execution before any data leaves their control. The core technology stack includes PySyft for orchestration, secure enclaves for mutual secrecy, and mock data provisioning for code development. In the first case study, researchers analyzed video recommendation algorithms without accessing proprietary data, while the second used secure enclaves to enable model safety evaluations where neither party could access the other's proprietary assets. The infrastructure allows researchers to develop code against synthetic mock data before submitting proposals for execution on real data within secure environments.

## Key Results
- Christchurch Call case study found no disproportionate amplification of suggestive content in video recommendation algorithms
- UK AISI case study successfully completed GPT-2 safety evaluations in under two hours using secure enclaves
- Proposal-approval workflow reduced legal overhead by keeping proprietary data within model owner's controlled environment
- Mock data provisioning enabled researchers to develop code without accessing real proprietary data

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Secure enclaves enable mutual secrecy by hosting both the AI model and evaluation datasets in a hardware-isolated environment that only executes jointly-approved code
- Mechanism: The enclave produces cryptographic certificates proving it ran only the code approved by both parties. Neither the model owner nor the evaluator can extract the other's proprietary assets (model weights or evaluation data) because the enclave restricts all I/O to approved outputs only
- Core assumption: The secure enclave hardware and software stack are correctly implemented and not compromised at the manufacturing or provisioning stage
- Evidence anchors:
  - [abstract]: "secure enclaves enabled model safety evaluations where neither the AI model weights nor the evaluation dataset were exposed to the other party"
  - [section 4]: "The secure enclave produced certificates proving that it only ran the evaluation code that both Anthropic and UK AISI approved"
  - [corpus]: Weak direct corpus support; neighboring papers discuss governance challenges but not enclave implementation details
- Break condition: If enclave attestation is spoofed, or side-channel attacks extract secrets from enclave memory, mutual secrecy guarantees fail

### Mechanism 2
- Claim: A proposal-approval workflow allows model owners to retain veto power over research questions, creating a trust negotiation layer that scales without requiring physical access or lengthy legal agreements
- Mechanism: Researchers submit code and research questions remotely; model owners review the exact computation before allowing execution. This reduces legal overhead because proprietary data never leaves the owner's controlled environment—the code comes to the data, not vice versa
- Core assumption: Model owners act in good faith and do not categorically deny all research requests; the workflow assumes willingness to engage
- Evidence anchors:
  - [abstract]: "researchers to propose questions remotely, which model owners can approve or deny before execution"
  - [section 2]: "a researcher remotely proposes questions to a model owner, the model owner approves the researchers' questions, and then the researcher receives the answers"
  - [corpus]: Neighbor papers on internal audit functions suggest organizational willingness is a key variable, but not directly tested
- Break condition: If model owners systematically deny legitimate research, the technical infrastructure cannot compel compliance—legal backstops become necessary

### Mechanism 3
- Claim: Mock data provisioning lets researchers iterate on code without accessing real proprietary data, reducing coordination friction and enabling code validation before committing secure compute resources
- Mechanism: Model owners generate synthetic datasets with similar schema/properties to real data. Researchers develop and debug against this mock data, then submit finalized code for execution on real data within the secure environment
- Core assumption: Mock data is sufficiently representative that code developed against it will execute correctly on real data without requiring multiple rounds of resubmission
- Evidence anchors:
  - [section 3]: "Dailymotion also provided some mock data to the external researchers to help the researchers develop their code"
  - [section 3]: "This precluded the need for extensive legal review, making the process far less onerous"
  - [corpus]: No direct corpus evidence on mock data effectiveness
- Break condition: If mock data distributions diverge significantly from real data, code may fail or produce misleading results when executed in production, requiring iterative resubmission

## Foundational Learning

- Concept: **Secure Enclaves (Trusted Execution Environments)**
  - Why needed here: Core isolation primitive enabling mutual secrecy; understanding attestation and sealing is essential for debugging deployment failures
  - Quick check question: Can you explain how an enclave proves to a remote party what code it is running?

- Concept: **Differential Privacy**
  - Why needed here: Likely required for aggregate query results to prevent reconstruction attacks from multiple approved queries
  - Quick check question: Given a query returning aggregate statistics, how would adding Laplacian noise prevent an adversary from inferring individual records?

- Concept: **Zero-Knowledge Proofs**
  - Why needed here: Referenced as part of the PET stack; enables verification of computations without revealing intermediate states
  - Quick check question: How can a prover convince a verifier that a statement is true without revealing any information beyond its truth?

## Architecture Onboarding

- Component map:
  - PySyft library -> Secure server -> Proposal interface -> Approval dashboard -> Attestation service -> Mock data generator

- Critical path:
  1. Model owner uploads proprietary data to secure server
  2. Model owner provisions mock data to researcher
  3. Researcher develops code against mock data
  4. Researcher submits proposal (code + question) through PySyft
  5. Model owner reviews code, approves or denies
  6. If approved, code executes in enclave on real data
  7. Results returned to researcher; execution certificate logged

- Design tradeoffs:
  - **Simpler setup (Case Study 1)**: Lower cost, faster deployment, but researcher cannot bring own sensitive datasets; model owner sees all researcher code
  - **Secure enclave setup (Case Study 2)**: Higher cost and engineering overhead, but enables mutual secrecy and researcher-provided datasets
  - **Approval-based vs. automatic execution**: Approval workflow adds latency but preserves model owner control; future researcher code privacy would reduce oversight but require stronger verification mechanisms

- Failure signatures:
  - **Enclave attestation failure**: Usually indicates mismatched hardware/software versions or compromised provisioning
  - **Code rejection after mock development**: Suggests mock data schema/distribution divergence from real data
  - **Timeout during evaluation**: Large models or complex evaluations may exceed enclave resource limits; scaling requires significant engineering

- First 3 experiments:
  1. Deploy PySyft locally with mock data; implement a simple aggregate query (e.g., count records matching a condition) and trace the full proposal-approval-execution flow
  2. Set up a basic secure enclave environment (e.g., using Azure Confidential Computing or AWS Nitro Enclaves) and verify attestation certificates can be generated and validated
  3. Simulate the Case Study 1 workflow: upload a small proprietary dataset, generate mock data, develop analysis code as a "researcher," then approve and execute the code as the "model owner"

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What technical mechanisms can enable researchers to keep their audit code private while still allowing model owners to verify that code will not harm their systems?
- Basis in paper: [explicit] Page 4 states OpenMined "is currently developing features that will allow researchers to keep their code private in addition to their data."
- Why unresolved: This feature is under active development and has not been demonstrated in existing deployments
- What evidence would resolve it: A working implementation where researchers submit encrypted or obfuscated code that executes successfully without model owners accessing the underlying logic

### Open Question 2
- Question: What verification mechanisms can effectively prevent model owners from gaming audits by deleting prediction logs or substituting different models during evaluation?
- Basis in paper: [explicit] Page 4 states "Future setups could also include verification mechanisms to prevent model owners from gaming audits by deleting a model's prediction logs or switching out an audited model for an unaudited one."
- Why unresolved: This is proposed as a future direction; no concrete solutions are described or tested
- What evidence would resolve it: A technical mechanism (e.g., cryptographic attestations, immutable audit trails) that third parties can independently verify

### Open Question 3
- Question: What are the computational costs and engineering requirements for scaling secure enclave-based evaluations to the largest frontier models (100B+ parameters)?
- Basis in paper: [explicit] Page 3 notes "Scaling up this setup to accommodate the largest frontier models will require significant engineering work, but the foundational research challenges have already been addressed."
- Why unresolved: The UK AISI case study used GPT-2, a small public model, as a trial run
- What evidence would resolve it: Benchmarks showing runtime, memory usage, and costs for evaluating frontier-scale models within secure enclaves

### Open Question 4
- Question: To what extent do secure enclaves remain trustworthy against sophisticated hardware-level attacks or compromised enclave implementations?
- Basis in paper: [inferred] The infrastructure relies heavily on secure enclaves (page 3), but the paper does not address potential vulnerabilities in enclave hardware or software supply chains
- Why unresolved: Historical vulnerabilities in enclave technologies (e.g., Spectre, Meltdown) suggest this warrants scrutiny
- What evidence would resolve it: Independent security audits or formal verification of the enclave configurations used in OpenMined's infrastructure

## Limitations
- Secure enclave deployment requires specific hardware (Intel SGX/AMD SEV) that may not be universally available
- The approval workflow assumes model owners will act in good faith, providing no enforcement mechanism if owners systematically deny legitimate research
- Scalability claims for frontier models are theoretical—the paper explicitly notes "significant engineering work" would be required

## Confidence
- **High Confidence**: The mechanism of using secure enclaves to enable mutual secrecy between model owners and evaluators is technically sound and well-supported by case study evidence
- **Medium Confidence**: The claim that this infrastructure "removes privacy, security, and IP as barriers" is plausible but under-specified—legal frameworks may still be necessary
- **Low Confidence**: The assertion that the technology "scales" to frontier AI models lacks empirical validation; the paper only demonstrates success with smaller models like GPT-2

## Next Checks
1. **Enclave Security Validation**: Test the secure enclave implementation against known side-channel attack vectors to verify the mutual secrecy guarantees hold under adversarial conditions
2. **Mock Data Representativeness Study**: Conduct experiments comparing code developed against mock data versus real data to quantify divergence rates and establish acceptable thresholds for mock data generation
3. **Scalability Benchmark**: Measure resource consumption (memory, compute time, enclave limits) when evaluating progressively larger models to determine practical scaling boundaries and identify required engineering optimizations