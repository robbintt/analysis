---
ver: rpa2
title: 'xEEGNet: Towards Explainable AI in EEG Dementia Classification'
arxiv_id: '2504.21457'
source_url: https://arxiv.org/abs/2504.21457
tags:
- xeegnet
- layer
- shallownet
- training
- dementia
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study introduces xEEGNet, a compact and interpretable neural
  network designed for EEG-based dementia classification. By progressively modifying
  the ShallowNet architecture, xEEGNet achieves high interpretability while maintaining
  performance comparable to deeper models.
---

# xEEGNet: Towards Explainable AI in EEG Dementia Classification

## Quick Facts
- arXiv ID: 2504.21457
- Source URL: https://arxiv.org/abs/2504.21457
- Authors: Andrea Zanola; Louis Fabrice Tshimanga; Federico Del Pup; Marco Baiesi; Manfredo Atzori
- Reference count: 0
- Primary result: xEEGNet achieves interpretable dementia classification using only 168 parameters with median accuracy of 53.7%

## Executive Summary
xEEGNet introduces a compact neural network architecture for EEG-based dementia classification that prioritizes interpretability over parameter count. By modifying the ShallowNet architecture with pre-defined EEG band-specific filters and depthwise convolutions, the model produces interpretable scalp topographies while maintaining competitive performance. The approach demonstrates that disease-relevant features can be effectively captured through spectral power differences across canonical EEG bands, with particular emphasis on delta and theta for Alzheimer's and gamma for frontotemporal dementia.

## Method Summary
The method employs a 3-layer architecture: a frozen first layer with 7 FIR bandpass filters (Delta through Gamma), a depthwise convolution layer for spatial filtering, and a classification head with global average pooling and log-power transformation. The model uses N-LNSO cross-validation (10 outer × 5 inner = 50 splits) on the OpenNeuro dataset ds004504, with 88 subjects (36 AD, 23 FTD, 29 CTL) providing 19-channel resting-state EEG recordings. Key design choices include parameter freezing to enforce interpretability, Global Average Pooling to reduce overfitting, and a bias-free dense layer to maintain linear relationships between band powers and class scores.

## Key Results
- xEEGNet achieves median weighted accuracy of 53.7% on 3-class dementia classification
- Model uses only 168 parameters—200× fewer than ShallowNet—while maintaining performance
- Dense layer weights reveal disease-specific spectral signatures: delta/theta for AD, gamma for FTD
- Training converges in ~68 epochs versus 1-2 epochs for overfitted ShallowNet

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Enforcing a 1:1 mapping between spectral bands and spatial filters creates an interpretable representation of EEG pathology.
- **Mechanism:** The architecture replaces standard convolutions with a frozen temporal layer (pre-defined bandpass filters) followed by a depthwise spatial convolution (depth=1). This forces the network to learn a specific scalp topology for each pre-defined frequency band independently, rather than mixing frequencies.
- **Core assumption:** Disease-relevant features in dementia are primarily spectral and spatially localized, rather than relying on complex cross-frequency interactions or non-stationary temporal sequences.
- **Evidence anchors:**
  - [abstract] "...employs pre-defined EEG band-specific filters and depthwise convolutions to produce interpretable scalp topographies."
  - [section 2.3.2] "This setup allows each feature map from the first layer to be multiplied by a single, dedicated kernel in the second layer... associating one EEG band to one scalp topography."
  - [corpus] KnowEEG paper supports the general need for "Explainable Knowledge Driven EEG Classification," aligning with this explicit design choice.
- **Break condition:** If the diagnostic criteria rely on transient temporal events (e.g., spike timing) rather than spectral power, this mechanism will fail to capture the necessary features.

### Mechanism 2
- **Claim:** Drastic parameter reduction mitigates overfitting by preventing the model from memorizing subject-specific noise.
- **Mechanism:** By freezing the first layer and using Global Average Pooling, the parameter count drops from ~35,000 (ShallowNet) to 168. This restricted capacity forces the model to learn generalizable spectral power differences rather than memorizing idiosyncratic artifacts of the training subjects.
- **Core assumption:** The "signal" (spectral shifts) is lower-dimensional than the "noise" (subject variability), and a linear separation of band powers is sufficient for classification.
- **Evidence anchors:**
  - [abstract] "...reduces overfitting through major parameter reduction... uses only 168 parameters."
  - [section 3.3.1] Figure 6 analysis shows xEEGNet maintains a positive correlation between training and validation loss, unlike larger models which exhibit divergence.
  - [corpus] "EEG-D3" highlights the "Hidden Overfitting Problem" in deep EEG models, validating the paper's focus on parameter reduction as a generalization strategy.
- **Break condition:** If the dataset is exceptionally small or not representative of the test population, even 168 parameters may overfit, though the risk is significantly lower than standard architectures.

### Mechanism 3
- **Claim:** The final classification decision is a function of relative spectral power across EEG bands.
- **Mechanism:** The Global Average Pooling layer calculates the power of the signal in each band. The final dense layer (without bias) acts as a simple linear weighter. The model classifies a window by comparing the weighted power of bands (e.g., High Delta/Theta → Alzheimer's; High Gamma → FTD).
- **Core assumption:** Diagnostic classes form clusters in the spectral power feature space that are separable by a linear or near-linear decision boundary.
- **Evidence anchors:**
  - [section 3.2.4] "The figure [Dense Layer Weights] gives valuable insight into how EEG bands are used to predict the pathology... delta and theta bands are predominantly used to predict the Alzheimer's class."
  - [section 2.2.7] Equation 6 shows the class probability is effectively a weighted sum of power values.
  - [corpus] The "Flexible and Explainable Graph Analysis" paper suggests graph/spatial features are also relevant; this mechanism explicitly ignores connectivity in favor of spectral power.
- **Break condition:** If a patient has conflicting spectral markers (e.g., high Gamma typical of FTD but also high Theta typical of AD), the linear weighting may produce ambiguous or incorrect predictions.

## Foundational Learning

- **Concept: Depthwise Separable Convolution**
  - **Why needed here:** The paper modifies ShallowNet to use depthwise convolutions to isolate frequency bands. You must understand how this differs from standard convolution (which mixes channels) to grasp why the model is interpretable.
  - **Quick check question:** If you have 7 input feature maps (frequency bands) and apply a depthwise convolution with 7 filters, how many output feature maps do you get, and does information mix between bands?

- **Concept: FIR Filter Design & Bode Plots**
  - **Why needed here:** The first layer of xEEGNet is not "learned" but designed using Finite Impulse Response (FIR) theory to isolate specific frequency ranges. Reading the Bode plots in the paper is essential to verifying the model's frequency response.
  - **Quick check question:** Looking at Filter 0 in Figure 2, does it allow high frequencies to pass, or does it attenuate them? What frequency range defines the "passband"?

- **Concept: Overfitting vs. Generalization**
  - **Why needed here:** The paper claims success not by maximizing accuracy, but by minimizing overfitting. Understanding the difference between training loss and validation loss dynamics (Figure 6) is critical to evaluating their claim.
  - **Quick check question:** If a model achieves 90% accuracy on training data but 40% on validation data, is it overfitting? How does reducing parameters from 34,000 to 168 theoretically fix this?

## Architecture Onboarding

- **Component map:** Raw EEG Window (19 channels × 500 time steps) → 7 Frozen FIR Filters → Depthwise Conv → Batch Norm → Square → Log → Global Average Pooling → Dense Layer (no bias) → Class Scores

- **Critical path:** The "Spectral Pipeline." The model transforms raw voltage → filtered signals → spatially weighted signals → power (dB) → class scores. Any break in the "no-bias" or "frozen-filter" chain collapses the interpretability.

- **Design tradeoffs:**
  - **Interpretability vs. Capacity:** You are trading the ability to learn complex, unknown features for the guarantee that the model uses standard EEG bands.
  - **Speed vs. Resolution:** The frozen filters use a kernel length of 125 (approx 1s at 125Hz) to get good frequency resolution, which limits the detection of very fast transient events.

- **Failure signatures:**
  - **Underfitting:** If accuracy is significantly lower than the MLRM baseline (52.4%), the frozen filters may be poorly designed or the Global Average Pooling is discarding too much info.
  - **Noise Sensitivity:** Despite robustness, the model relies on power ratios; if an artifact (e.g., muscle movement) spikes the power in a "Control" band (e.g., Beta), it may cause a false negative.

- **First 3 experiments:**
  1. **Sanity Check (Gradient Visualization):** Pass a known AD subject through the network and verify that the Dense Layer weights for "AD" class light up when the Delta/Theta bands are high. (Confirming Section 3.2.4).
  2. **Ablation Study (Learned vs. Frozen):** Unfreeze Layer 1 and retrain. Does the model learn non-standard frequency responses? Does performance improve significantly, or just overfit faster? (Confirming Section 2.2.1/3.3.1).
  3. **Noise Injection:** Add white noise to the input. Does the Global Average Pooling smooth it out, or does the logarithmic scaling (10 log10) amplify the noise floor, breaking classification?

## Open Questions the Paper Calls Out
None

## Limitations
- The fixed frequency bands may not capture disease-specific spectral signatures that deviate from canonical EEG rhythms
- The absence of explicit bias terms in the final dense layer could limit the model's ability to handle class imbalance effectively
- The N-LNSO cross-validation approach may not fully represent real-world clinical scenarios where patient populations differ from the OpenNeuro cohort

## Confidence

- **High confidence:** The parameter reduction mechanism (168 parameters vs. 35,000) effectively mitigates overfitting, as evidenced by positive train-val loss correlation and the ablation study results.
- **Medium confidence:** The interpretability of spectral band contributions is well-supported by the dense layer weight analysis, though the clinical significance of specific band-power thresholds requires further validation.
- **Medium confidence:** The classification performance (median 53.7%) demonstrates improvement over baseline, but the modest absolute gain suggests limited clinical utility without additional feature engineering.

## Next Checks

1. **Temporal robustness test:** Apply xEEGNet to sliding windows within individual recordings to assess classification consistency over time and detect potential temporal artifacts.

2. **Connectivity comparison:** Implement a connectivity-based baseline (e.g., phase-locking value features) to quantify the trade-off between spectral interpretability and potential loss of network-level information.

3. **Generalization probe:** Evaluate the model on an independent dataset (e.g., ADNI EEG) to verify that the learned band-power relationships generalize beyond the OpenNeuro cohort.