---
ver: rpa2
title: A Preliminary Study of Large Language Models for Multilingual Vulnerability
  Detection
arxiv_id: '2505.07376'
source_url: https://arxiv.org/abs/2505.07376
tags:
- vulnerability
- detection
- llms
- language
- software
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluates the effectiveness of pre-trained language
  models (PLMs) and large language models (LLMs) for detecting vulnerabilities across
  seven programming languages. Using a multilingual dataset containing 20,165 code
  samples, the research compares five PLMs and seven LLMs on vulnerability detection
  tasks.
---

# A Preliminary Study of Large Language Models for Multilingual Vulnerability Detection

## Quick Facts
- arXiv ID: 2505.07376
- Source URL: https://arxiv.org/abs/2505.07376
- Reference count: 40
- Primary result: CodeT5P achieves 60.37% accuracy and 70.75% F1-score, outperforming all other PLMs and LLMs on multilingual vulnerability detection

## Executive Summary
This study systematically evaluates pre-trained language models (PLMs) and large language models (LLMs) for detecting vulnerabilities across seven programming languages using a multilingual dataset of 20,165 code samples. The research finds that fine-tuned PLMs, particularly encoder-decoder architectures like CodeT5P, significantly outperform zero-shot LLM prompting across most evaluation metrics. While LLMs show promise for C/C++ vulnerability detection, they generally lag behind PLMs in overall performance. The study provides evidence that vulnerability detection benefits from task-specific fine-tuning rather than relying solely on pre-trained knowledge.

## Method Summary
The study uses the REEF dataset containing 20,165 code samples across C, C#, C++, Go, JavaScript, Java, and Python. Five PLMs (CodeBERT, UniXcoder, LineVul, CodeT5, CodeT5P) were fine-tuned using supervised learning, while seven LLMs (DeepSeek-Coder, Code Llama, Llama 3.1, GPT-3.5-Turbo, GPT-4o, DeepSeek-R1, QwQ-plus) were evaluated using zero-shot prompting. All models performed binary classification on functions truncated to 512 tokens. The dataset was split 8:1:1 for training, validation, and testing per language. Evaluation metrics included accuracy, precision, recall, and F1-score, with additional analysis on the 2024 CWE Top 25 vulnerabilities.

## Key Results
- CodeT5P (encoder-decoder PLM) achieved the best overall performance with 60.37% accuracy and 70.75% F1-score
- All LLMs underperformed compared to PLMs, with QwQ-plus achieving the best LLM performance at 53.46% accuracy
- Encoder-decoder architectures consistently outperformed encoder-only models across all evaluation metrics
- Closed-source LLMs generally performed better than open-source counterparts, though computational constraints limited open-source comparisons to ~8B parameters

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Encoder-decoder PLM architectures outperform encoder-only and decoder-only models for multilingual vulnerability detection.
- **Mechanism:** Encoder-decoder models combine bidirectional context understanding with sequence generation capabilities, enabling richer semantic representation of code patterns that distinguish vulnerable from non-vulnerable functions.
- **Core assumption:** Vulnerability patterns require both comprehensive context aggregation and discriminative feature extraction, which encoder-decoder architectures provide more effectively.
- **Evidence anchors:** CodeT5P achieves 60.37% accuracy and 70.75% F1-score; encoder-decoder architecture consistently outperforms all other PLMs and LLMs.

### Mechanism 2
- **Claim:** Fine-tuned PLMs outperform zero-shot LLM prompting because supervised learning on labeled vulnerability data establishes stronger discriminative boundaries.
- **Mechanism:** PLMs undergo task-specific fine-tuning on 16,126 labeled vulnerability samples, learning explicit mapping between code features and vulnerability labels. LLMs using zero-shot prompting rely on implicit knowledge from pre-training without task-specific gradient updates.
- **Core assumption:** Labeled vulnerability data contains learnable patterns that transfer across languages when models are explicitly trained to recognize them.
- **Evidence anchors:** Current LLMs lag behind PLMs across nearly all evaluation metrics; PLMs adapted through fine-tuning on REEF dataset.

### Mechanism 3
- **Claim:** Model-language pairing significantly affects detection effectiveness because vulnerability patterns manifest differently across programming language ecosystems.
- **Mechanism:** Different languages have distinct vulnerability profiles (e.g., memory management issues dominate C/C++; injection vulnerabilities in dynamic languages). Models with training exposure to specific language-vulnerability combinations develop stronger pattern recognition for those contexts.
- **Core assumption:** Vulnerability patterns are not fully language-agnostic; some transfer learning occurs but language-specific characteristics remain significant.
- **Evidence anchors:** All PLMs do well in Go language; LLMs exhibit superior capability in C/C++ vulnerability detection with DeepSeek-Coder achieving 56.35% accuracy in C++.

## Foundational Learning

- **Concept:** Binary classification evaluation metrics (Precision, Recall, F1-Score) for imbalanced data
  - **Why needed here:** Vulnerability datasets are often imbalanced. Understanding why CodeT5P achieved 70.75% F1 but CodeBERT had perfect recall (1.0) with low precision (~0.5) is critical—the latter indicates high false positive rates.
  - **Quick check question:** If a model flags 90% of code as vulnerable and catches all real vulnerabilities, what would its recall and precision be? (Answer: Recall=1.0, Precision=low, dominated by false positives)

- **Concept:** Encoder-only vs. Encoder-decoder vs. Decoder-only architectures
  - **Why needed here:** The paper's central finding depends on architectural differences. Encoder-only (CodeBERT) excels at classification via CLS token; encoder-decoder (CodeT5P) combines understanding and generation; decoder-only (LLMs) generates tokens autoregressively.
  - **Quick check question:** Which architecture is best suited for generating code patches vs. classifying vulnerabilities? (Answer: Decoder-only/encoder-decoder for generation; encoder-only for pure classification, though encoder-decoder can handle both)

- **Concept:** CWE (Common Weakness Enumeration) taxonomy
  - **Why needed here:** RQ3 evaluates detection of the 2024 CWE Top 25. Understanding that CWE-79 (XSS) differs fundamentally from CWE-787 (buffer overflow) helps interpret why some models detect certain categories better.
  - **Quick check question:** Why might a model trained primarily on C/C++ vulnerabilities struggle with XSS (CWE-79) detection in JavaScript? (Answer: Different vulnerability mechanisms—memory safety vs. input sanitization)

## Architecture Onboarding

- **Component map:**
  REEF Dataset → Preprocessing (patch, Tree-sitter extraction, 512-token filter) → Training/Validation/Test Split (8:1:1) → PLM Pipeline (Fine-tuning) / LLM Pipeline (Zero-shot) → Binary Classification → Metrics

- **Critical path:**
  1. Data preprocessing (function extraction, token limiting) determines what reaches the model
  2. For PLMs: Fine-tuning on labeled REEF data is the performance-determining step
  3. For LLMs: Prompt design (currently zero-shot) limits effectiveness
  4. Evaluation on per-language and per-CWE breakdown reveals where models fail

- **Design tradeoffs:**
  - 512-token limit excludes longer functions but enables comparison with PLM architectures
  - Zero-shot vs. few-shot LLM prompting: Zero-shot provides baseline but likely underestimates LLM potential
  - Function-level vs. file-level detection: Function-level chosen for tractability; may miss cross-function vulnerabilities
  - Open-source (8B params) vs. closed-source LLMs: Computational constraints limited open-source comparison

- **Failure signatures:**
  - Perfect recall with ~50% precision (CodeBERT, LineVul): Model defaults to "vulnerable" prediction, high false positive rate
  - Low recall with moderate accuracy (QwQ-plus 48.38%): Model is conservative, misses real vulnerabilities
  - CWE-specific failures (CWE-862, CWE-798): Some models detect zero instances—suggests knowledge gaps
  - Language-specific underperformance: Models with weak Go/Python performance may lack sufficient language-specific pre-training

- **First 3 experiments:**
  1. Reproduce CodeT5P baseline on Python subset of REEF to validate pipeline matches paper
  2. Ablate zero-shot prompt for LLMs: Test GPT-4o with few-shot prompting vs. zero-shot to improve F1
  3. Cross-language transfer test: Train CodeT5P on 6 languages, test on 7th to measure language-specific vs. transferable knowledge

## Open Questions the Paper Calls Out

- **Open Question 1:** Can advanced prompting strategies (few-shot prompting, instruction-tuning, RAG) improve LLM performance to match or exceed PLMs for multilingual vulnerability detection? (Basis: Future Work section explicitly identifies this as area for investigation)
- **Open Question 2:** Would scaling open-source LLMs beyond 8B parameters bridge the performance gap with closed-source LLMs for vulnerability detection? (Basis: RQ3 discussion suggests investigating larger open-source models)
- **Open Question 3:** How do PLMs and LLMs perform on vulnerability detection for code samples exceeding 512 tokens? (Basis: Threats to Validity section identifies input length limitations as potential issue)

## Limitations

- Reliance on zero-shot prompting for LLMs likely underestimates their true capabilities compared to fine-tuned PLMs
- 512-token input limit may exclude longer functions containing multi-line vulnerabilities, potentially biasing results toward simpler vulnerabilities
- Function-level detection approach may not reflect real-world scenarios where vulnerabilities span multiple functions or entire files

## Confidence

**High Confidence:**
- Encoder-decoder PLMs outperform encoder-only models for multilingual vulnerability detection
- Fine-tuned PLMs currently outperform zero-shot LLM prompting across most metrics
- CodeT5P demonstrates superior overall performance on the multilingual dataset

**Medium Confidence:**
- Closed-source LLMs generally perform better than open-source counterparts
- Language-specific model-language pairing significantly affects detection effectiveness
- LLM performance on C/C++ vulnerabilities shows promising potential

**Low Confidence:**
- Long-term viability of PLMs vs. LLMs for vulnerability detection given rapid LLM evolution
- Extrapolation of CWE-specific detection capabilities to all vulnerability types
- Generalizability of function-level detection results to file-level or project-level analysis

## Next Checks

1. **Few-shot prompting ablation:** Test GPT-4o and DeepSeek-Coder with 3-5 in-context examples per language to determine if zero-shot performance underestimates LLM capabilities. Compare F1 scores to current baseline.

2. **Cross-language transfer evaluation:** Train CodeT5P on six languages and test exclusively on the seventh held-out language. Measure performance drop to quantify language-specific vs. transferable vulnerability knowledge.

3. **Long-function vulnerability detection:** Identify and extract vulnerable functions exceeding 512 tokens from REEF. Test CodeT5P's performance on these longer samples versus the truncated 512-token versions to assess architectural limitations.