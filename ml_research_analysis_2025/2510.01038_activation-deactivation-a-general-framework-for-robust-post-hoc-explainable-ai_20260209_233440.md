---
ver: rpa2
title: 'Activation-Deactivation: A General Framework for Robust Post-hoc Explainable
  AI'
arxiv_id: '2510.01038'
source_url: https://arxiv.org/abs/2510.01038
tags:
- explanations
- explanation
- input
- confidence
- causal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of robustness in post-hoc explainability
  methods for image classifiers. State-of-the-art approaches often rely on occlusion-based
  perturbations that create out-of-distribution inputs, potentially distorting explanations.
---

# Activation-Deactivation: A General Framework for Robust Post-hoc Explainable AI

## Quick Facts
- **arXiv ID:** 2510.01038
- **Source URL:** https://arxiv.org/abs/2510.01038
- **Reference count:** 40
- **Primary result:** ConvAD produces 62.5% more robust explanations than traditional occlusion methods while requiring no training or fine-tuning.

## Executive Summary
This paper introduces Activation-Deactivation (AD), a novel forward-pass paradigm for generating robust post-hoc explanations of image classifiers. Traditional occlusion-based methods create out-of-distribution inputs that can distort explanations. AD instead deactivates intermediate model representations corresponding to occluded input features, effectively passing a cut-out of the unperturbed region through the network. The core contribution is ConvAD, a drop-in mechanism that can be added to any trained CNN at test-time to systematically remove the effects of masked regions from the model's decision-making process.

## Method Summary
ConvAD implements a modified forward pass where a binary mask is propagated through the network alongside the input. At each checkpoint (after convolutions and around dimensionality-altering operations), the mask is updated using a position-attribution function based on mean-value kernel convolution, then thresholded to binary values. This mask is applied to intermediate activations via Hadamard product, effectively deactivating features derived from occluded regions. The method preserves the original network's decision function on unmasked inputs (variadic equivalence) and integrates with the rex XAI tool for explanation extraction. Crucially, ConvAD requires no additional training and works with any pretrained CNN.

## Key Results
- AD explanations show 62.5% higher robustness compared to occlusion methods across multiple datasets
- Explanations are more robust without requiring domain knowledge for choosing occlusion values
- AD produces slightly larger explanations (0.9-19.53% increase) that are closer to the model's confidence threshold
- The method works across different architectures including ResNet-50, RegNetY-12GF, and EfficientNet-V2-S

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Deactivating intermediate representations that correspond to occluded input regions produces more robust explanations than pixel-value occlusion.
- **Mechanism:** ConvAD tracks which activations derive from masked input regions using a position-attribution function (Φ), then zeroes those activations at checkpoints between convolutional layers. This prevents information from masked regions from propagating, effectively creating a "cut-out" of unmasked features without creating out-of-distribution inputs.
- **Core assumption:** Masked regions can be cleanly separated from unmasked regions in intermediate representations; convolution straddling effects can be handled via thresholding.
- **Evidence anchors:**
  - [abstract] "removes the effects of occluded input features from the model's decision-making by switching off the parts of the model that correspond to the occlusions"
  - [Section 4] "ConvAD systematically deactivates intermediate representations that map to occluded regions, effectively passing a cut-out of the unperturbed region to the model"
  - [corpus] No direct corpus support; this is a novel perturbation paradigm not directly validated in neighbor literature.
- **Break condition:** When convolutions straddle masked/unmasked boundaries extensively (Figure 4), threshold τ cannot cleanly separate contributions; very fine-grained perturbation masks may fail.

### Mechanism 2
- **Claim:** The position-attribution function accurately maps input masks to intermediate representations across varying architectural operations.
- **Mechanism:** For shape-preserving operations, Φ uses a mean-value kernel convolution with the mask, producing values in [0,1] indicating the ratio of unmasked contributors. For dimensionality-altering operations (pooling, interpolation, concatenation), Φ adjusts via parametric functions or set operations. Thresholding at τ converts continuous ratios to binary decisions.
- **Core assumption:** The convolution kernel's receptive field boundaries can be approximated by uniform weighting; external additive operations (e.g., padding, concatenation) can be treated as independent features with configurable inclusion.
- **Evidence anchors:**
  - [Section 3, Definition 4] "Φi(zab, M) = 1/|posi(zab)| × Σ 1(M[k,l] > 0)"
  - [Section 4] "The position-attribution function for the binary mask in case (1) is an identical convolution to CN with a mean-value kernel"
  - [corpus] Weak; neighbor papers discuss explainability but not position-attribution specifically.
- **Break condition:** Non-contiguous perturbation masks (Figure 4c) where convolution windows overlap multiple scattered perturbations make attribution ambiguous.

### Mechanism 3
- **Claim:** ConvAD preserves the original network's decision function on unmasked inputs (variadic equivalence).
- **Mechanism:** When no mask is applied (equivalent to an all-ones mask), Φ values are all 1.0, so the Hadamard product with activations preserves them unchanged. Theorem 1 proves this formally for both shape-preserving and dimensionality-altering cases.
- **Core assumption:** The network's operations are sufficiently decomposable that restricting to subset of inputs produces a valid function output; external operations can be independently masked.
- **Evidence anchors:**
  - [Section 4, Theorem 1] "the output of N' is equal to the output of N on all inputs without occlusions"
  - [Appendix C] Detailed proof covering both cases with mathematical derivation
  - [corpus] No direct validation; theoretical guarantee within paper scope only.
- **Break condition:** Architectures with complex skip connections or dynamic computation graphs not covered by the two checkpoint cases may violate the equivalence proof.

## Foundational Learning

- **Concept: Causal explanations and actual causality (Definitions 1, 8)**
  - **Why needed here:** AD-explanations are formalized as restricted causal models; understanding the three conditions (EXIC1-3, AC1-3) is necessary to interpret what "explanation" means in this framework—specifically minimality and the counterfactual requirement.
  - **Quick check question:** Given an image classified as "dog," explain why a set of pixels forming the dog's head might satisfy EXIC3 but a set including background pixels might not.

- **Concept: Out-of-distribution (OOD) artifacts in occlusion-based XAI**
  - **Why needed here:** The paper's core motivation is that standard occlusion methods create OOD inputs that don't reflect the model's true reasoning. Understanding how filling regions with zeros/means/noise changes the input distribution is critical.
  - **Quick check question:** Why might replacing image patches with gray values cause a CNN to activate on spurious features that were never part of its training distribution?

- **Concept: Receptive fields and spatial attribution through convolutions**
  - **Why needed here:** The position-attribution function relies on tracking which input positions contribute to each intermediate activation. Without understanding receptive field growth and how pooling/striding affect spatial resolution, the checkpoint logic is opaque.
  - **Quick check question:** For a 3×3 convolution with stride 1 followed by 2×2 max pooling with stride 2, what input region contributes to the output at position (5,5)?

## Architecture Onboarding

- **Component map:**
  - Input (image x, binary mask M) -> Checkpoint layers (after convolutions and DA operations) -> Position-attribution module (Φ) -> Deactivation (Hadamard product) -> rex integration -> Output logits

- **Critical path:**
  1. Initialize mask M = all-ones (or from XAI tool)
  2. At each convolution layer: convolve M with mean-value kernel of matching size
  3. Apply threshold: M = M > τ
  4. At checkpoint: multiply intermediate representation by M
  5. Handle DA operations: apply operation-specific Φ, re-threshold
  6. Output: final logits as usual

- **Design tradeoffs:**
  - **τ = 0:** Only fully-masked regions deactivated; preserves more activations but may leak masked information via straddling convolutions
  - **τ > 0:** More aggressive deactivation; reduces leaks but may remove valid boundary features
  - **Checkpoint frequency:** More checkpoints = finer control but higher computational overhead; minimal placement is after each convolution and around DA operations
  - **Assumption:** External additions (padding, skip connections) treated as maskable; choice affects what "causes" the prediction

- **Failure signatures:**
  - **Low robustness despite AD:** Check if τ is too high (over-deactivating) or architecture has unhandled operation types
  - **Significant output divergence on unmasked inputs:** Checkpoint placement error; verify Theorem 1 conditions
  - **Explanations identical to occlusion methods:** Mask not being propagated correctly; verify M is being updated at each step
  - **Memory spikes:** Storing full mask tensors at high resolution; consider downsampling M with pooling operations

- **First 3 experiments:**
  1. **Sanity check:** Run ConvAD with all-ones mask on pretrained ResNet-50; verify output logits match vanilla model exactly (Theorem 1 validation)
  2. **Single-image diagnosis:** Generate explanation for a controlled image (e.g., ibex example); compare AD vs. zero/mean/min/max occlusion visualizations; check explanation size and alignment with ground-truth features
  3. **Robustness sweep:** For 10 images, compute explanations at γ = 0.5 threshold; plant each on 20 solid-color backgrounds; measure ρ-robustness (classification matches original) for AD vs. best occlusion value; expect 20-40% improvement per Section 5.1

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but several unresolved issues emerge from the limitations section.

## Limitations
- The position-attribution function may not accurately handle non-contiguous masks or complex skip connections
- The theoretical proof of variadic equivalence assumes idealized layer structures that may not hold exactly in practice
- The rex explainer integration is critical but not fully specified, making exact replication challenging
- Generalization to non-CNN architectures (transformers, graph networks) is not explored

## Confidence
- **High confidence:** The core mechanism of using internal feature deactivation instead of input occlusion is well-founded and produces measurable robustness improvements (62.5%). The theoretical framework for position-attribution and the proof of variadic equivalence are rigorous.
- **Medium confidence:** The practical implementation details for complex architectures and the exact behavior of Φ with non-standard operations may vary. The robustness improvements, while substantial, depend on specific explainer choices and planting procedures not fully detailed.
- **Low confidence:** The generalization to architectures beyond standard CNNs (transformers, graph networks) and the behavior with highly irregular masks are not explored.

## Next Checks
1. **Sanity test:** Run ConvAD with all-ones mask on pretrained ResNet-50; verify output logits match vanilla model exactly (Theorem 1 validation)
2. **Boundary case analysis:** Generate explanations for images with non-contiguous important regions (scattered pixels); compare AD vs. occlusion explanations and measure attribution accuracy
3. **Architectural stress test:** Apply ConvAD to EfficientNet-V2-S; verify checkpoint placement in MBConv blocks produces correct outputs and explanations without accuracy degradation