---
ver: rpa2
title: 'Unlocking the Potential of Past Research: Using Generative AI to Reconstruct
  Healthcare Simulation Models'
arxiv_id: '2503.21646'
source_url: https://arxiv.org/abs/2503.21646
tags:
- code
- simulation
- stage
- prompt
- https
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study demonstrates the feasibility of using generative AI
  to recreate healthcare discrete-event simulation (DES) models from published descriptions.
  Using a structured methodology with iterative prompt engineering and rigorous testing,
  we successfully generated, tested, and internally reproduced two complex healthcare
  DES models in Python.
---

# Unlocking the Potential of Past Research: Using Generative AI to Reconstruct Healthcare Simulation Models

## Quick Facts
- **arXiv ID:** 2503.21646
- **Source URL:** https://arxiv.org/abs/2503.21646
- **Reference count:** 40
- **Primary result:** Successfully recreated two healthcare DES models using iterative prompt engineering with generative AI, though one failed to replicate due to missing input data.

## Executive Summary
This study demonstrates the feasibility of using generative AI to recreate healthcare discrete-event simulation (DES) models from published descriptions. Through a structured methodology combining iterative prompt engineering, rigorous testing, and human oversight, the researchers successfully generated and verified two complex healthcare DES models in Python. The approach successfully produced functional models with sophisticated features including multiple patient classes, queuing networks, and user interfaces, though replication success was limited by the completeness of source publications rather than the methodology itself.

## Method Summary
The methodology involved extracting natural language descriptions from two healthcare DES journal articles and using Perplexity.AI (free tier, RAG-enabled) to generate Python code following a structured 12-aim sequence. The process was iterative, with each aim building upon the previous one - from simple patient arrivals through to queuing logic and user interface implementation. Code was verified after each iteration using visual inspection, `nbdime` for code differencing, and `pytest` for automated unit testing. The approach emphasized restrictive clauses in prompts to prevent unintended code regression and required human intervention to manage context window limitations and verify generated code quality.

## Key Results
- Successfully generated two complex healthcare DES models ranging from 262-531 lines of code
- One model successfully replicated original simulation results within 5% tolerance
- Generated models passed 28-34 automated verification tests using pytest
- Models included sophisticated features: multiple patient classes, complex queuing networks, and Streamlit user interfaces

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Incrementally increasing model complexity via a structured "aim" sequence mitigates LLM context window limitations and logic drift better than single-prompt generation.
- **Mechanism:** The authors decomposed the recreation process into 12 ordered aims (e.g., arrivals → queuing → UI). By batching prompts into small, functional increments rather than requesting the full model at once, the system reduces the probability of the LLM "forgetting" earlier logic or hitting context limits.
- **Core assumption:** The underlying LLM performs better on compositional tasks when they are broken down into procedural steps similar to human coding practices.
- **Evidence anchors:**
  - [section] Section 6.2, Table 2: "ordered and batched iterations into 12 aims... This mirrors how the recreation of a DES model would take place regardless of whether an LLM was used."
  - [section] Section 10.1.2: "We solved this problem... by switching to a new context window... [and] including code snippets... to fill in the missing context."

### Mechanism 2
- **Claim:** Restrictive clauses in prompts are necessary to prevent the LLM from unintentionally regressing previously verified code or implementing "lazy" logic.
- **Mechanism:** The authors appended negative constraints (e.g., "Do not modify the patient_generator functions") to prompts. This counters the LLM's tendency to hallucinate changes or take shortcuts (e.g., inserting comments like "remaining functions go here" instead of code).
- **Core assumption:** The LLM adheres to negative constraints in the attention mechanism sufficiently to suppress the generation of specific tokens or patterns.
- **Evidence anchors:**
  - [section] Section 7.7: "We appended restrictive clauses... to avoid changes to parts of the code that were not part of our design."
  - [section] Section 10.1.1: Describes "Lazy generation" where the LLM replaces code with comments; solved by restrictive clauses like "Show all of the model code."

### Mechanism 3
- **Claim:** Code "differencing" (comparing iterations) is a more reliable verification method for AI-generated code than standard unit testing alone.
- **Mechanism:** Because LLMs introduce subtle errors in unchanged code sections (e.g., removing a critical routing line), standard pass/fail testing might miss logic that *should* exist but has been silently deleted. The authors used `nbdime` to enforce a visual diff check between iterations.
- **Core assumption:** The human reviewer possesses sufficient domain knowledge to recognize valid vs. invalid code changes in the diff view.
- **Evidence anchors:**
  - [section] Section 8.1: "visual inspection became too difficult when a modification of existing code took place... [we] included the use of a Python library called nbdime."
  - [section] Section 10.3: "critical line of stroke patient routing logic was removed... fixed via a prompt in iteration 11."

## Foundational Learning

- **Concept:** Discrete-Event Simulation (DES) State Logic
  - **Why needed here:** LLMs struggle with temporal state management (e.g., queuing, resource release). You must understand DES concepts (entities, resources, events) to verify if the LLM has implemented the logic correctly or just generated a script that "looks" like code.
  - **Quick check question:** If a patient "balks" (leaves the queue), does the code release the bed resource, or does it just decrement a counter?

- **Concept:** Context Window Management
  - **Why needed here:** The paper explicitly notes context limits as a failure mode. Users must understand how to prune or restart conversations to keep the LLM focused on the relevant code snippet.
  - **Quick check question:** If the LLM generates code with incorrect indentation at the end of a long response, is this a logic error or a context window failure?

- **Concept:** Pseudorandom Number Streams (Common Random Numbers)
  - **Why needed here:** To verify replication, the model must use seeded streams. LLMs often default to `random.random()` rather than independent, controllable streams required for simulation variance reduction.
  - **Quick check question:** Can you reproduce the exact same simulation results using the same seed ID across two separate runs?

## Architecture Onboarding

- **Component map:** Journal Article → Design Translator (Human creates STRESS report/prompt database) → Generator (Perplexity.AI LLM) → Verifier (Jupyter + nbdime + pytest) → Output (SimPy Python Module + Streamlit UI)

- **Critical path:** The *Prompt Engineering* phase. Translating ambiguous academic text into precise, restrictive prompts is the bottleneck that determines code quality.

- **Design tradeoffs:**
  - **Iterative vs. All-in-One:** The authors chose iterative (26-57 prompts) to ensure stability, trading off speed for reliability.
  - **Free Tier vs. Paid:** Using Perplexity's free tier introduced rate limits and varying RAG quality.

- **Failure signatures:**
  - **Lazy Generation:** Output contains `# ... rest of code here`
  - **Context Drift:** Variable names change midway through the code (e.g., `self.count` becomes `self.patient_id_counter` without instruction)
  - **Indentation Collapse:** Code ends with flat indentation due to output token limits

- **First 3 experiments:**
  1. **The "Lazy" Test:** Ask the LLM to modify a 300-line class. Check if it outputs the full class or a truncated version. Apply the restrictive clause "Show all code" and compare.
  2. **The Context Reset:** Start a chat, generate a model, then ask for a modification. Note if earlier logic is forgotten. Start a *new* chat with the code snippet included and verify if the modification is more precise.
  3. **The Diff Check:** Generate a model (Iteration 1). Ask for a simple parameter change (Iteration 2). Use `nbdime` or a Git diff to see if the LLM modified anything *other* than the parameter (identifying silent hallucinations).

## Open Questions the Paper Calls Out
None

## Limitations
- Approach requires significant human intervention for prompt engineering and verification
- Success depends on completeness of source publications (one model failed due to missing distribution parameters)
- Context window management remains a persistent challenge requiring manual intervention
- Methodology is time-intensive (26-57 prompts per model)

## Confidence
- **High Confidence:** Core methodology (iterative prompt engineering with 12 aims) is well-documented and reproducible. Verification framework using `nbdime` and `pytest` is robust.
- **Medium Confidence:** Success rate (1 of 2 models fully replicated) suggests approach works but is not universally reliable. Failure mode (missing input data) is external to methodology.
- **Low Confidence:** Generalizability to other DES domains or publication styles remains unproven. Dependence on specific LLM behaviors may not transfer to other platforms.

## Next Checks
1. **Cross-LLM Validation:** Replicate methodology using alternative LLMs (GPT-4, Claude, open-source models) to assess platform dependency
2. **Publication Completeness Test:** Systematically evaluate how missing parameters in source publications affects replication success across multiple cases
3. **Efficiency Benchmark:** Compare time-to-completion and prompt count against traditional manual recreation of the same models