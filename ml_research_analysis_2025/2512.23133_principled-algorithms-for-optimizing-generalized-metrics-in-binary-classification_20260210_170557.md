---
ver: rpa2
title: Principled Algorithms for Optimizing Generalized Metrics in Binary Classification
arxiv_id: '2512.23133'
source_url: https://arxiv.org/abs/2512.23133
tags:
- loss
- learning
- metrics
- algorithm
- algorithms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces principled algorithms for optimizing generalized\
  \ metrics in binary classification, such as F\u03B2-measures and Jaccard similarity,\
  \ which are important in imbalanced or asymmetric-cost scenarios. The authors reformulate\
  \ the problem as a generalized cost-sensitive learning task and design novel surrogate\
  \ loss functions with provable H-consistency guarantees."
---

# Principled Algorithms for Optimizing Generalized Metrics in Binary Classification

## Quick Facts
- arXiv ID: 2512.23133
- Source URL: https://arxiv.org/abs/2512.23133
- Reference count: 40
- Key outcome: Introduces METRO algorithm for optimizing generalized metrics like Fβ and Jaccard in binary classification with provable H-consistency guarantees

## Executive Summary
This paper develops principled algorithms for optimizing generalized metrics in binary classification, addressing scenarios with imbalanced or asymmetric costs where standard accuracy is insufficient. The authors reformulate metric optimization as a cost-sensitive learning problem using a linear-fractional transformation, then design surrogate loss functions with H-consistency guarantees. The resulting METRO algorithm leverages binary search over the metric's optimal value combined with surrogate minimization to achieve strong theoretical and empirical performance on CIFAR-10, CIFAR-100, and SVHN datasets.

## Method Summary
The approach reformulates generalized metrics as linear-fractional functions of true positives, false positives, true negatives, and false negatives, enabling equivalent cost-sensitive learning formulation. For a target metric parameterized by (α,β), the method defines a cost-sensitive loss Lγ where γ coefficients depend on α, β, and the optimal metric value λ*. METRO employs binary search over λ combined with empirical surrogate loss minimization (using logistic loss by default) to find a hypothesis that achieves bounded excess metric error. The algorithm uses Algorithm 3 (cross-validation) for λ selection with a 3-layer CNN trained via SGD with Nesterov momentum.

## Key Results
- METRO outperforms existing baselines on CIFAR-10, CIFAR-100, and SVHN for Fβ and Jaccard metrics
- Achieves higher Fβ scores (F1, F0.5, F1.5) and Jaccard similarity compared to thresholded SVM and other methods
- Provides theoretical H-consistency bounds for the surrogate losses with finite-sample generalization guarantees
- Avoids reliance on Bayes-optimal solutions while maintaining strong performance guarantees

## Why This Works (Mechanism)

### Mechanism 1: Linear-Fractional Metric Reformulation
Generalized metrics (Fβ, Jaccard, weighted accuracy) can be equivalently reformulated as minimizing a cost-sensitive loss function parameterized by λ*, the best-in-class metric value. The paper expresses metrics as ratios of linear combinations of TP, FP, TN, FN, then shows minimizing L(h) is equivalent to minimizing E[ℓα(h,x,y) - λ*ℓβ(h,x,y)] where λ* = inf_h L(h). This transforms a fractional optimization into a single expected loss minimization (Theorem 3.1). Core assumption: the denominator E[ℓβ(h,x,y)] is positive for all h ∈ H; this can be enforced by sign flipping if needed.

### Mechanism 2: H-Consistent Surrogate Loss Design
Standard margin-based surrogates can be extended to general cost-sensitive learning while preserving H-consistency bounds. Given a margin-based loss Φ (logistic, hinge, exponential), define L_Φ(h,x,y) = L(+1,y)Φ(-h(x)) + L(-1,y)Φ(h(x)) (Eq. 6). Theorem 4.1 shows that if Φ admits Γ-H-consistency for 0-1 loss, then L_Φ admits Γ-H-consistency for the cost-sensitive loss L with Γ scaled by (2L_max)^(1-α). Core assumption: H is a "regular" hypothesis set where {h(x): h ∈ H} = {+1, -1} for all x ∈ X (satisfied by neural networks, linear classifiers).

### Mechanism 3: Binary Search with Surrogate Minimization
A binary search over λ, combined with empirical surrogate loss minimization, converges to a hypothesis with bounded excess metric error. The sign of E*_ℓλ(H) indicates whether λ is above or below λ* (Theorem 5.1). Algorithm 2 uses empirical estimates from surrogate minimization to guide binary search. Theorem 5.5 guarantees L(ĥ_λ) ≤ L*(H) + 4ε_m/ℓ̄_β with high probability, where ε_m depends on Rademacher complexity and sample size. Core assumption: the estimation error bound ε_m is sufficiently small relative to ℓ̄_β; this requires adequate sample size relative to hypothesis class complexity.

## Foundational Learning

- **Concept: H-Consistency Bounds**
  - Why needed here: Unlike Bayes-consistency (asymptotic, all measurable functions), H-consistency provides non-asymptotic, hypothesis-class-specific guarantees. This paper's theory depends entirely on H-consistency, not just consistency.
  - Quick check question: Can you explain why Bayes-consistency alone is insufficient when learning with a restricted hypothesis class like linear classifiers?

- **Concept: Cost-Sensitive Learning with Non-Zero Costs for All Outcomes**
  - Why needed here: Standard cost-sensitive learning (e.g., θ-weighted binary classification) sets costs only for FP and FN. This paper requires handling all four outcome costs (TP, TN, FP, FN all potentially non-zero), which arises naturally from the linear-fractional metric formulation.
  - Quick check question: How does the cost matrix L(y',y) differ between standard θ-weighted classification and the general case in Eq. (4)?

- **Concept: Minimizability Gap**
  - Why needed here: The gap M_ℓ(H) = E*_ℓ(H) - E_x[inf_{h∈H} E[ℓ(h,x,y)|x]] appears in H-consistency bounds and vanishes only when best-in-class equals Bayes error. Understanding this helps interpret the practical relevance of theoretical guarantees.
  - Quick check question: When would the minimizability gap be non-zero for a neural network hypothesis class?

## Architecture Onboarding

- **Component map:** Metric specification → Cost matrix computation → λ estimation loop → Surrogate loss module → Base learner
- **Critical path:** The λ estimation loop is the outer optimization; for each λ candidate, the surrogate loss minimization is the inner optimization. The binary search terminates when |Ê_ℓλ,S(ĥ_λ)| ≤ ε_m (in practice, when search interval narrows sufficiently).
- **Design tradeoffs:**
  - Algorithm 2 vs. Algorithm 3: Binary search (O(log(1/ε))) vs. grid search (O(1/ε)). Theoretical bounds favor binary search, but CV may be more robust when theoretical ε_m is loose.
  - Choice of Φ: Logistic loss provides Γ(t) ∝ √t; hinge loss provides Γ(t) ∝ t. Hinge gives tighter bounds but logistic may optimize better with SGD.
  - Data splitting: Paper notes λ estimation and final training can use separate samples to avoid overfitting.
- **Failure signatures:**
  1. Metric degradation on held-out data → Likely overfitting λ on training data; use separate splits for λ search and final training.
  2. Binary search oscillation → Empirical surrogate minimum may not reliably indicate sign of E*_ℓλ(H); increase tolerance or switch to Algorithm 3.
  3. Performance worse than ERM on balanced data → Verify (α,β) specification; metrics like Fβ with β≠1 are designed for imbalanced scenarios.
  4. Loss not decreasing during training → Check that L_Φ weights (L(+1,y), L(-1,y)) are computed correctly and are non-negative.
- **First 3 experiments:**
  1. Validation on synthetic 2D data (replicate Figure 1): Generate linearly separable data with known optimal Fβ classifier; verify METRO recovers a decision boundary closer to the metric-optimal boundary than thresholded SVM.
  2. Ablation on surrogate choice: Compare logistic vs. hinge vs. exponential surrogates on CIFAR-10 binary task; measure F1, F0.5, F1.5, and Jaccard. Expect logistic to be most stable.
  3. Sample complexity check: Plot metric error (L(ĥ) - L*(H) approximation via held-out evaluation) vs. training set size; verify empirical convergence roughly tracks the O(1/√m) rate implied by Theorem 5.5.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the H-consistency framework and METRO algorithms be extended to generalized metrics in multi-class classification and multi-label learning?
- Basis in paper: [explicit] The conclusion states that a "natural direction is extending our theory and algorithms to cover generalized metrics in multi-class classification and multi-label learning."
- Why unresolved: The current theoretical analysis and binary search algorithms are specifically derived for the binary classification setting (Y = {+1, -1}).
- What evidence would resolve it: A derivation of H-consistency bounds and surrogate losses for metrics like macro-F1 or Jaccard index in multi-label settings.

### Open Question 2
- Question: How does the METRO algorithm perform empirically on datasets with severe class imbalance compared to standard baselines?
- Basis in paper: [explicit] The experiments section acknowledges the importance of evaluating on "more imbalanced datasets" and states, "We plan to include such experiments and comparisons in future work."
- Why unresolved: The current empirical validation relies on CIFAR and SVHN datasets, which are standard image benchmarks but not representative of extreme imbalance found in domains like fraud detection.
- What evidence would resolve it: Experimental results comparing METRO against prior baselines on benchmarks specifically characterized by significant class imbalance.

### Open Question 3
- Question: Can finite-sample generalization bounds be established for METRO in over-parameterized settings using tools that account for optimization dynamics?
- Basis in paper: [inferred] The paper notes that in over-parameterized settings, the current estimation error bounds may not be tight and "typically requires alternative tools... that account for the optimization algorithm."
- Why unresolved: Standard Rademacher complexity bounds become loose or vacuous when model complexity exceeds sample size, and the current analysis does not incorporate the implicit regularization of algorithms like SGD.
- What evidence would resolve it: A theoretical analysis deriving non-vacuous generalization bounds that depend on optimization trajectories or stability properties rather than just hypothesis set capacity.

## Limitations
- Theoretical guarantees assume "regular" hypothesis classes where all labelings are realizable at each point, which may not hold for complex neural networks
- Binary search for λ requires estimation error to be small relative to ℓ̄_β; in overparameterized settings, this may require prohibitive sample sizes
- Framework only applies to linear-fractional metrics (ratio of linear combinations), excluding metrics like AUC or precision@k

## Confidence
- **High confidence**: The linear-fractional reformulation (Mechanism 1) and cost-sensitive learning framework (Mechanism 2) are mathematically sound and well-established
- **Medium confidence**: The H-consistency extension to cost-sensitive learning (Theorem 4.1) is novel but relies on assumptions about regular hypothesis classes that may not hold for deep networks
- **Low confidence**: The practical effectiveness of the binary search algorithm in high-dimensional settings where theoretical bounds are loose

## Next Checks
1. Replicate the synthetic 2D experiment to verify METRO recovers metric-optimal decision boundaries more accurately than thresholded approaches
2. Conduct ablation studies comparing logistic, hinge, and exponential surrogates on the same binary classification tasks
3. Empirically measure the relationship between training set size and metric error reduction to validate the O(1/√m) convergence rate implied by the theory