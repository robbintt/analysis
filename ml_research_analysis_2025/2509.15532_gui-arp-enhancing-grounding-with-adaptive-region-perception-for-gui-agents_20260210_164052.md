---
ver: rpa2
title: 'GUI-ARP: Enhancing Grounding with Adaptive Region Perception for GUI Agents'
arxiv_id: '2509.15532'
source_url: https://arxiv.org/abs/2509.15532
tags:
- arxiv
- grounding
- adaptive
- preprint
- perception
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: GUI-ARP addresses the challenge of fine-grained localization in
  high-resolution GUI screenshots by introducing a novel framework with Adaptive Region
  Perception (ARP) and Adaptive Stage Controlling (ASC). The method leverages attention-based
  region perception and dynamically adjusts inference strategy based on task complexity,
  performing single-stage inference for simple cases and multi-stage analysis for
  complex scenarios.
---

# GUI-ARP: Enhancing Grounding with Adaptive Region Perception for GUI Agents

## Quick Facts
- arXiv ID: 2509.15532
- Source URL: https://arxiv.org/abs/2509.15532
- Reference count: 0
- Primary result: 7B model achieves 60.8% accuracy on ScreenSpot-Pro, outperforming open-source 72B models

## Executive Summary
GUI-ARP addresses the challenge of fine-grained localization in high-resolution GUI screenshots by introducing a novel framework with Adaptive Region Perception (ARP) and Adaptive Stage Controlling (ASC). The method leverages attention-based region perception and dynamically adjusts inference strategy based on task complexity, performing single-stage inference for simple cases and multi-stage analysis for complex scenarios. The approach uses a two-phase training pipeline combining supervised fine-tuning with reinforcement fine-tuning based on Group Relative Policy Optimization (GRPO). GUI-ARP achieves state-of-the-art performance on challenging GUI grounding benchmarks, with a 7B model reaching 60.8% accuracy on ScreenSpot-Pro and 30.9% on UI-Vision benchmark, demonstrating strong competitiveness against open-source 72B models (UI-TARS-72B at 38.1%) and proprietary models.

## Method Summary
GUI-ARP is a two-phase training pipeline that fine-tunes the GUI-Actor-7B backbone for adaptive GUI grounding. Phase 1 uses supervised fine-tuning with Chain-of-Thought annotations and control tokens, incorporating both next-token prediction and attention KL divergence losses. Phase 2 applies reinforcement learning with GRPO using Gaussian-sparse rewards for point localization and tool-call decisions. The core innovation is ARP, which uses model attention distributions to dynamically crop task-relevant regions, combined with ASC that learns to predict task complexity and decide between single-stage and multi-stage inference. The system processes screenshots through attention analysis, conditional region cropping, and staged reasoning to achieve fine-grained element localization.

## Key Results
- 7B GUI-ARP model achieves 60.8% accuracy on ScreenSpot-Pro benchmark
- Outperforms open-source 72B models (UI-TARS-72B at 38.1%) and proprietary models
- 30.9% accuracy on UI-Vision benchmark, demonstrating cross-domain capability
- Single-stage vs. multi-stage staging shows +10.4% improvement on SS-Pro post-GRPO

## Why This Works (Mechanism)

### Mechanism 1: Attention-Guided Adaptive Cropping
Using model attention distributions to determine crop regions improves localization over fixed-ratio box expansion. ARP analyzes the spatial attention map over visual patches, identifies connected components exceeding a threshold (τ·max(A)), selects top-k regions by score, and computes a weighted bounding box. This aligns the cropped region with the model's internal focus rather than heuristic expansion.

### Mechanism 2: Difficulty-Conditional Inference Staging
Learning to predict task complexity enables efficient single-stage inference for simple cases while reserving multi-stage analysis for harder instances. ASC uses structured control tokens (`<tool call>yes/no`) with CoT reasoning. The model outputs "no" when initial localization is confident, skipping ARP; it outputs "yes" when uncertainty exists, triggering a second observation pass.

### Mechanism 3: Two-Phase Training with Gaussian-Sparse Rewards
Combining SFT (for cold-start CoT reasoning) with GRPO (using dense Gaussian point rewards) enables learning both region perception and stage control. Phase 1 trains next-token prediction + attention KL divergence loss with CoT annotations. Phase 2 uses rule-based rewards: R_format, R_tool (correct staging decisions), R_point (Gaussian-weighted by distance to ground truth center).

## Foundational Learning

- **Vision-Language Model Attention Mechanisms**: Understanding how attention flows from the `<ACTOR>` token to patch tokens is critical since ARP directly exploits cross-modal attention weights. Quick check: Given a VLM with 256 visual patches, can you sketch how a query token attends to patches and how those weights form a spatial heatmap?

- **Reinforcement Learning with Rule-Based Rewards**: GRPO requires understanding policy gradients, KL penalties, and how shaped rewards (Gaussian point rewards) differ from sparse binary rewards. Quick check: Why would a Gaussian reward R_point = exp(-d²/σ²) provide better learning signal than a binary reward R_point = 1 if d < threshold else 0?

- **Chain-of-Thought Reasoning in VLMs**: ASC depends on the model generating CoT to assess difficulty before outputting control tokens. Understanding how CoT scaffolds reasoning is critical. Quick check: How does forcing a model to verbalize reasoning before a decision affect calibration and error modes?

## Architecture Onboarding

- **Component map**: [Screenshot + Instruction] → [Vision Encoder + LLM Backbone] ← inherits from GUI-Actor-7B → [<ACTOR> token embedding] → [Attention Head] → Attention Map A → [LLM generates CoT + <tool call>] ← [ARP Module] (if called) → [ASC Decision: yes/no] ← [Cropped Region] → [Final Bounding Box Prediction]

- **Critical path**: The attention head's output → ARP's connected component analysis → ASC's tool-call decision. If any component fails (diffuse attention, poor connected components, miscalibrated ASC), the final prediction degrades.

- **Design tradeoffs**:
  - τ (threshold) selection: Lower τ = larger crop regions (more context, less precision); higher τ = smaller crops (risk missing target). Paper uses τ=0.3
  - k (top-k regions): Higher k captures multi-element targets but adds noise. Paper uses k=20
  - Single-stage vs. multi-stage: Multi-stage improves SS-Pro (+10.4% over single-stage post-GRPO) but hurts SS-v2 slightly (-0.1%) where layouts are simpler

- **Failure signatures**:
  - High tool-call rate on simple benchmarks → ASC not learning to suppress unnecessary refinement
  - Low tool-call rate on complex benchmarks → ASC not recognizing difficulty
  - Diffuse attention maps (no clear connected components) → ARP returns overly broad regions

- **First 3 experiments**:
  1. Baseline validation: Run GUI-Actor-7B backbone on SS-Pro without ARP/ASC to reproduce the 44.6% baseline. Verify attention visualization produces coherent heatmaps
  2. ARP ablation: Fix ASC to always call ARP; sweep τ ∈ {0.1, 0.3, 0.5} and k ∈ {10, 20, 30}. Measure crop-region IoU with ground truth before second-stage prediction
  3. ASC calibration analysis: After SFT phase, plot tool-call rate vs. first-stage prediction error distance. Check if "yes" calls correlate with higher errors (desired) or if the model is randomly deciding

## Open Questions the Paper Calls Out
None explicitly stated in the provided material.

## Limitations
- Performance varies significantly across benchmarks (60.8% on ScreenSpot-Pro vs 30.9% on UI-Vision), suggesting potential overfitting to specific UI styles
- Claims about computational efficiency gains lack quantitative validation - no comparison of average inference steps or latency measurements
- Method's effectiveness depends heavily on quality of attention distributions, which isn't validated across diverse UI elements and layouts

## Confidence
- **High confidence**: The overall framework design (combining ARP with ASC) is technically sound and the two-phase training approach follows established practices in VLM fine-tuning
- **Medium confidence**: The reported benchmark results, while competitive, show significant variation across datasets that suggests the method may be overfitting to specific UI styles present in ScreenSpot-Pro
- **Low confidence**: Claims about computational efficiency gains from adaptive staging lack quantitative validation - no comparison of average inference steps or latency measurements between GUI-ARP and fixed multi-stage baselines

## Next Checks
1. **Cross-dataset generalization test**: Evaluate GUI-ARP on a held-out UI dataset (e.g., RICO or RicoCSS) not seen during training to assess whether the 60.8% ScreenSpot-Pro performance transfers to different UI domains
2. **Attention map quality audit**: For 100 randomly sampled grounding tasks, visualize attention heatmaps and compute correlation between attention-weighted regions and ground truth element locations. Measure ARP's crop-region IoU with ground truth when attention maps are low quality
3. **Computational overhead measurement**: Implement timing experiments comparing average inference latency (including attention computation and potential second-stage passes) between GUI-ARP and a fixed two-stage baseline across all benchmark datasets