---
ver: rpa2
title: '$\textbf{AGT$^{AO}$}$: Robust and Stabilized LLM Unlearning via Adversarial
  Gating Training with Adaptive Orthogonality'
arxiv_id: '2602.01703'
source_url: https://arxiv.org/abs/2602.01703
tags:
- unlearning
- forget
- agtao
- adversarial
- utility
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper addresses the challenge of removing sensitive or unwanted\
  \ information from large language models (LLMs) without degrading overall model\
  \ performance. The key issue is balancing robust unlearning (to erase targeted knowledge)\
  \ with preserving the model\u2019s general capabilities."
---

# $\textbf{AGT$^{AO}$}$: Robust and Stabilized LLM Unlearning via Adversarial Gating Training with Adaptive Orthogonality

## Quick Facts
- arXiv ID: 2602.01703
- Source URL: https://arxiv.org/abs/2602.01703
- Authors: Pengyu Li; Lingling Zhang; Zhitao Gao; Yanrui Wu; Yuxuan Dong; Huan Liu; Bifan Wei; Jun Liu
- Reference count: 40
- Primary result: AGT AO achieves KUR ≈ 0.01 while maintaining MMLU 58.30, outperforming baselines on robustness and utility

## Executive Summary
This paper addresses the challenge of removing sensitive or unwanted information from large language models (LLMs) without degrading overall model performance. The key issue is balancing robust unlearning (to erase targeted knowledge) with preserving the model's general capabilities. The authors propose AGT AO, which combines Adaptive Orthogonality (AO) and Adversarial Gating Training (AGT). AO dynamically reduces gradient conflicts between forgetting and retaining objectives, minimizing unintended knowledge loss. AGT treats unlearning as a latent-space adversarial game, injecting worst-case perturbations to ensure robust erasure and prevent superficial forgetting. Experiments across multiple benchmarks (TOFU, MUSE, WMDP) show AGT AO achieves excellent unlearning efficacy (KUR ≈ 0.01) while maintaining strong model utility (MMLU 58.30) and privacy (PLR ≈ 0.53), outperforming existing methods.

## Method Summary
The method combines Adaptive Orthogonality (AO) and Adversarial Gating Training (AGT) to achieve robust LLM unlearning. AO detects gradient conflicts between forgetting and retention objectives, applying a penalty when gradients point in opposing directions to minimize unintended knowledge loss. AGT formulates unlearning as a latent-space min-max game, using Projected Gradient Descent (PGD) to find perturbations that maximally revive forgotten information, compelling the model to adopt a flat-minimum configuration resistant to recovery. A gradient-norm-based gating mechanism stabilizes adversarial training by disabling perturbations during early training phases and only applying them when the loss landscape is relatively flat. The approach uses a specific forget loss function, applies perturbations at layer 10 for 7B models, and employs a curriculum of warmup and adaptive injection to prevent catastrophic forgetting.

## Key Results
- AGT AO achieves Knowledge Unlearning Ratio (KUR) of approximately 0.01, indicating excellent erasure of targeted knowledge
- Model Utility (MMLU score) of 58.30, demonstrating preservation of general capabilities
- Privacy Leakage Rate (PLR) of approximately 0.53, indicating strong resistance to privacy attacks
- Outperforms baselines significantly in robustness to 4-bit quantization attacks

## Why This Works (Mechanism)

### Mechanism 1: Adaptive Orthogonality (AO) Resolves Gradient Conflict
- **Claim:** Soft orthogonalization of forgetting and retention gradients reduces catastrophic forgetting while preserving unlearning efficacy.
- **Mechanism:** AO detects when gradient vectors from the forget set (g_f) and retain set (g_r) point in opposing directions (g_f · g_r < 0), indicating parameter updates that would damage retained knowledge. When conflict is detected, a penalty term R_AO = I(g_f · g_r < 0) × [(1 - cos(g_f, g_r)) / 2]^γ is applied to suppress the conflicting component, effectively pushing the gradients toward orthogonality.
- **Core assumption:** Knowledge targeted for removal and knowledge to retain occupy partially overlapping parameter regions, and gradient interference—not data contamination—is the primary driver of utility degradation.
- **Evidence anchors:**
  - [abstract]: "Adaptive Orthogonality (AO) to dynamically mitigate geometric gradient conflicts between forgetting and retention objectives"
  - [section 2.1, eq. 3]: Penalty activates only under conflict condition g_f · g_r < 0
  - [corpus]: FG-OrIU (arXiv:2601.13578) independently validates gradient-orthogonality approaches for "superficial forgetting" prevention
- **Break condition:** If forget and retain objectives require modifying identical parameters in the same direction, no orthogonalization is possible; expect utility-unlearning tradeoff to re-emerge.

### Mechanism 2: Latent-Space Min-Max Game Prevents Superficial Forgetting
- **Claim:** Internal adversarial perturbations force the model into a parameter configuration resistant to knowledge recovery attacks.
- **Mechanism:** AGT formulates unlearning as: min_θ max_{||δ||_p ≤ ε} L_unlearn(h_f^(l) + δ, h_r; θ). The inner loop uses PGD (K steps) to find perturbations δ* that maximally revive forgotten information in the hidden representations. The outer loop then minimizes loss under this worst-case perturbation, compelling parameters to adopt a flat-minimum configuration where small internal shifts cannot reconstruct the target knowledge.
- **Core assumption:** Recoverable knowledge persists in latent activation patterns that can be identified through gradient-based search; robust unlearning requires erasing these pathways, not just suppressing output probabilities.
- **Evidence anchors:**
  - [abstract]: "AGT formulates unlearning as a latent-space min-max game"
  - [section 2.2, eq. 5-7]: Formal bi-level optimization with PGD inner loop
  - [section 3.3.2, Figure 6]: AGT^AO shows <50% memory rebound under 4-bit quantization vs. >1900% for baselines
  - [corpus]: LAT (cited as baseline) validates latent adversarial training concept; this paper adds gating curriculum
- **Break condition:** If forget knowledge is distributed across many layers or encoded in ways not accessible via hidden-state perturbation, the inner loop will fail to find effective attacks, yielding false confidence in robustness.

### Mechanism 3: Gradient-Norm-Based Gating Stabilizes Adversarial Training
- **Claim:** Adversarial perturbations applied only during stable optimization phases prevent gradient explosion and utility collapse.
- **Mechanism:** The gating mechanism divides training into two phases: (1) Warm-up (first N_warmup steps) disables adversarial attacks, allowing standard unlearning to reach manageable loss regions; (2) Adaptive injection activates adversarial training only when ||∇L_unlearn||_2 < τ_grad, indicating a relatively flat loss landscape. This curriculum prevents destabilizing the model during high-variance early training.
- **Core assumption:** Early-stage unlearning trajectories are intrinsically unstable; premature adversarial pressure amplifies gradient oscillations that exceed what AO alone can regulate.
- **Evidence anchors:**
  - [section 2.2, "Gradient-Norm-Based Gating"]: Explicit warmup phase + threshold τ_grad
  - [section 3.3, Table 4 (w/o GBG)]: KUR regresses from 0.01 to 0.60 without gating
  - [corpus]: No direct corpus comparison found; this appears novel to this framework
- **Break condition:** If τ_grad is set too high, adversarial training never activates (superficial forgetting risk); if too low, training destabilizes (catastrophic forgetting risk).

## Foundational Learning

- **Concept: Multi-objective gradient conflict**
  - **Why needed here:** AO requires understanding when two loss gradients interfere (negative dot product) vs. cooperate. Without this, the penalty mechanism is opaque.
  - **Quick check question:** Given gradient vectors g_f = [3, -2] and g_r = [-1, 4], is there a conflict? What would AO compute?

- **Concept: Min-max optimization (bi-level)**
  - **Why needed here:** AGT's inner loop attacks while outer loop defends. Understanding that inner maximization finds worst-case scenarios and outer minimization prepares for them is essential.
  - **Quick check question:** In min_θ max_δ L(θ, δ), which variable represents the "defender" and which represents the "attacker"?

- **Concept: Transformer layer semantics (shallow vs. deep)**
  - **Why needed here:** Layer 10 is identified as the optimal "semantic entry" point for perturbation injection. Shallow layers affect syntax; deep layers limit backpropagation.
  - **Quick check question:** Why might perturbing layer 2 fail to affect semantic knowledge, while perturbing layer 28 reduce the effectiveness of parameter updates?

## Architecture Onboarding

- **Component map:**
  Input → Forward pass to layer L → [AGT: Inject δ* via PGD] → Continue forward → Compute L_forget + L_retain
                                                                                        ↓
                                              [AO: Compute g_f · g_r, add penalty if < 0] ← Backward pass
                                                                                        ↓
                                         [GBG: Check ||∇L||_2 < τ_grad? Apply adversarial update : Standard update]

- **Critical path:**
  1. **Layer selection for perturbation** (L=10 for 7B models, L=4 for 2B models): Must hit "semantic entry" where knowledge is encoded but gradients can still propagate.
  2. **Warmup duration**: Set N_warmup = steps in first epoch. Adversarial attacks before this cause instability.
  3. **Threshold calibration**: τ_grad = ρ × ||∇L_warmup||_2, where ρ = 0.6 (grid-searched). Wrong value breaks either robustness or utility.

- **Design tradeoffs:**
  - **AGT inner steps (K):** Paper uses K=4. More steps → stronger robustness but 2-4× compute overhead per batch.
  - **AO penalty strength (γ):** Set to 1. Higher γ → stricter orthogonality but may slow convergence.
  - **Perturbation budget (ε):** L_∞ constraint; too small → weak attacks, too large → unrealistic threat model.

- **Failure signatures:**
  - **Catastrophic forgetting (utility drops):** Check if AO is disabled or γ too low. Look for gradient cosine similarity oscillating wildly (Figure 4 shows this pattern).
  - **Superficial forgetting (high KUR after quantization):** Check if GBG threshold too strict (adversarial updates never applied) or warmup too long.
  - **Training divergence:** Verify GBG is active. Ablation shows KUR jumps from 0.01 → 0.60 without gating.

- **First 3 experiments:**
  1. **Validate AO in isolation:** Run unlearning with AO only (no AGT) on a small forget set. Confirm Model Utility stays >0.5 vs. <0.4 baseline. Expect Forget Quality degradation vs. full AGT^AO.
  2. **Layer sensitivity sweep:** Inject perturbations at layers 0, 5, 10, 15, 20, 25 on LLaMA-7B. Plot ROUGE-L on forget set. Confirm peak near layer 10.
  3. **Quantization attack sanity check:** After unlearning, apply 4-bit quantization and measure forget-set ROUGE-L rebound. AGT^AO should show <100% rebound; GA/NPO baselines should show >500% rebound.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Does AGT$^{AO}$ maintain stability and efficacy when scaled to models significantly larger than 7B parameters?
- **Basis in paper:** [explicit] The authors state in the Limitations section, "we intend to extend our evaluation to validate its scalability on larger-scale models."
- **Why unresolved:** Current experiments are restricted to 2B and 7B models (LLaMA, Gemma, Zephyr); the optimization landscape and gradient conflict geometry may shift drastically in higher-dimensional parameter spaces (e.g., 70B+ models).
- **What evidence would resolve it:** Evaluation results on 70B+ models showing consistent preservation of MMLU scores and low KUR without training divergence or "gradient explosion" during the warm-up phase.

### Open Question 2
- **Question:** Can the computational overhead of the Adversarial Gating Training (AGT) min-max loop be reduced without compromising unlearning robustness?
- **Basis in paper:** [explicit] The paper notes the method "introduces additional computational overhead compared to standard fine-tuning methods; future work will focus on optimizing the efficiency of this process."
- **Why unresolved:** The inner loop relies on multiple Projected Gradient Descent (PGD) steps (denoted as $K$ steps) to find worst-case perturbations, increasing training time per batch significantly compared to single-pass baselines like NPO or GA.
- **What evidence would resolve it:** A modified algorithm achieving comparable KUR scores and adversarial robustness using a single-step or closed-form adversarial approximation, demonstrably reducing training time per epoch.

### Open Question 3
- **Question:** Is there a universal theoretical rule for identifying the optimal "Semantic Entry" layer for perturbation injection across different model architectures?
- **Basis in paper:** [inferred] The authors empirically identify Layer 10 for Llama-2-7B and Layer 4 for Gemma-2B as optimal perturbation points, treating layer selection as a hyperparameter derived from sensitivity analysis.
- **Why unresolved:** The method assumes a specific "Semantic Entry" point exists where syntax transitions to semantics, but the paper does not provide a generalizable formula for finding this layer in new architectures without running extensive ablations.
- **What evidence would resolve it:** A heuristic (e.g., ratio of layer depth) that predicts the optimal injection layer for a new architecture (e.g., Qwen or Mistral) which matches the peak performance found via grid search.

## Limitations

- **Gradient conflict assumption uncertainty:** AO depends on the assumption that forget and retain gradients will exhibit negative dot products, but this pattern may not be universal across all unlearning scenarios.
- **Ad-hoc threshold calibration:** The gating mechanism's threshold (ρ=0.6) appears empirically tuned without theoretical justification for its specific value.
- **Limited attack vector testing:** Robustness claims against 4-bit quantization are impressive but only tested against a single attack vector, not establishing broad resistance to diverse recovery methods.

## Confidence

- **High confidence:** The core experimental results showing AGT^AO's superior performance on KUR, Model Utility, and PLR metrics are well-documented and reproducible.
- **Medium confidence:** The theoretical justification for why Layer 10 represents the "semantic entry" point for perturbation injection is empirically supported but lacks rigorous theoretical grounding.
- **Low confidence:** The robustness claims against 4-bit quantization are impressive but only tested against a single attack vector, not establishing broad resistance to diverse recovery methods.

## Next Checks

1. **Gradient conflict universality test:** Run AO on multiple unlearning tasks with varying semantic overlap between forget and retain sets. Quantify the proportion of batches where g_f · g_r < 0 to establish whether AO's conflict-detection assumption holds generally.

2. **Layer sensitivity validation:** Systematically sweep perturbation injection across all layers (0-31) for both 2B and 7B models. Plot forget quality vs. layer index to confirm the identified "semantic entry" points and test whether the 2B/7B scaling rule (layer/2) generalizes.

3. **Multi-vector robustness evaluation:** Beyond 4-bit quantization, test AGT^AO against membership inference attacks, data extraction queries, and fine-tuning recovery attempts. Measure whether the robust forgetting properties extend across diverse attack methodologies.