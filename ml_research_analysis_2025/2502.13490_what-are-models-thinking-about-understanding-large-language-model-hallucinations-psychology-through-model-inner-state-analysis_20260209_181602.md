---
ver: rpa2
title: What are Models Thinking about? Understanding Large Language Model Hallucinations
  "Psychology" through Model Inner State Analysis
arxiv_id: '2502.13490'
source_url: https://arxiv.org/abs/2502.13490
tags:
- token
- attention
- layer
- hallucination
- internal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper proposes a framework called HALU PROBE that systematically\
  \ extracts and analyzes internal states of large language models (LLMs) to understand\
  \ and detect hallucinations. The authors divide the LLM inference process into three\
  \ stages\u2014understanding, query, and generation\u2014and extract features from\
  \ attention weights, layer representations, and logits."
---

# What are Models Thinking about? Understanding Large Language Model Hallucinations "Psychology" through Model Inner State Analysis

## Quick Facts
- arXiv ID: 2502.13490
- Source URL: https://arxiv.org/abs/2502.13490
- Authors: Peiran Wang; Yang Liu; Yunfei Lu; Jue Hong; Ye Wu
- Reference count: 40
- Primary result: Framework extracts and analyzes LLM internal states to detect hallucinations, with feature effectiveness varying significantly across datasets

## Executive Summary
This paper introduces HALU PROBE, a systematic framework for understanding and detecting hallucinations in large language models by analyzing internal states during inference. The authors divide the LLM inference process into three stages—understanding, query, and generation—and extract features from attention weights, layer representations, and logits. Comprehensive experiments across CNNDM, Natural Questions, and HaluEval datasets reveal that attention-based features like lookback ratio are effective for hallucination detection in certain domains, while logit features such as joint token probabilities also perform well. However, the study identifies significant limitations including dataset-specific feature effectiveness and computational costs that hinder real-time applications.

## Method Summary
The framework extracts internal states from transformer layers during inference, capturing attention matrices, hidden states, and logits for each generated token. Eight features are computed across three categories: attention (lookback ratio, entropy), activation (hidden states, entropy, map), and logits (min probability, max rank, joint probability). Five token selection strategies are evaluated, with sliced window (w=4, s=2) showing best performance. A binary classifier predicts whether each token sequence contains hallucinations, trained separately on different datasets. The approach provides interpretable insights into model reasoning by examining how internal state patterns correlate with hallucinated versus factual outputs.

## Key Results
- Lookback ratio effectively distinguishes hallucinations in CNNDM (higher for factual outputs) but fails on HaluEval
- Sliced window token selection (w=4, s=2) achieves 0.85-0.87 accuracy versus 0.51-0.53 for per-token strategies
- RAG stabilizes internal states by enriching attention distribution breadth and reducing variance
- Feature transferability is limited, with CNNDM-trained features achieving ~0.50 accuracy on HaluEval
- Attention-based features provide interpretable insights into model reasoning during hallucination generation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Reduced attention to prior context correlates with hallucinations in summarization tasks
- Mechanism: Lookback ratio measures attention directed toward previous tokens; lower ratios indicate insufficient historical context utilization during hallucinated generation
- Core assumption: Hallucinations partly arise from inadequate attention to source context rather than parametric knowledge errors
- Evidence anchors: CNNDM shows factual outputs maintain higher lookback ratios than hallucinated outputs (LRF > LRH), while HaluEval shows no distinction; Chuang et al.'s "Lookback lens" provides convergent validity

### Mechanism 2
- Claim: Sliced window token selection provides structured, balanced input for detection classifiers
- Mechanism: Sliding windows of 4-8 tokens with stride 2 provide consistent input dimensions while capturing local coherence patterns, reducing labeling noise
- Core assumption: Hallucinations manifest at phrase or clause level rather than individual tokens
- Evidence anchors: Window(4,2) achieves 0.85-0.87 accuracy vs. 0.51-0.53 for single-token strategies; paper suggests refined labeling could improve per-token effectiveness

### Mechanism 3
- Claim: RAG stabilizes internal states by enriching attention distribution breadth
- Mechanism: External context from RAG provides additional tokens for attention allocation, reducing variance in lookback ratios and increasing attention entropy
- Core assumption: RAG improves generation quality through attention mechanism regularization, not just external knowledge
- Evidence anchors: RAG increases attention entropy (E_RAG > E_non-RAG) and reduces activation variance; no corpus papers directly address this effect

## Foundational Learning

- Concept: Attention head-level vs. layer-level aggregation
  - Why needed here: Features like lookback ratio vary significantly across heads within same layer; aggregation choice affects detection accuracy
  - Quick check question: If you observe high variance in lookback ratio across 32 heads in layer 15, should you report per-head values or the layer mean?

- Concept: Joint probability vs. marginal probability in token confidence
  - Why needed here: Joint token probability captures sequence-level confidence (lower for hallucinated sequences), while minimum token probability captures local uncertainty
  - Quick check question: A 10-token sequence has high marginal probabilities (>0.9) for each token but joint probability near zero—what does this indicate about model confidence?

- Concept: Transferability failure modes in feature-based detection
  - Why needed here: Features trained on summarization hallucinations (CNNDM) show ~0.50 accuracy on factual QA hallucinations (HaluEval)
  - Quick check question: You achieve 83% accuracy on CNNDM. What evidence would support expecting similar performance on a new customer support dataset?

## Architecture Onboarding

- Component map: Internal State Extractor -> Token Selector -> Feature Compute Layer -> Detection Classifier
- Critical path: Hook extraction -> Token selection (Window(4,2)) -> Feature computation -> Classifier inference. Latency dominated by attention extraction (3.27s/token) and logit computations (0.73-1.15s/token)
- Design tradeoffs: Window improves accuracy (+15-20%) but adds complexity; feature subset may overfit specific datasets; last-layer features often most discriminative but middle layers show different patterns
- Failure signatures: Accuracy near 0.50 with HaluEval-trained features on CNNDM indicates transferability failure; high variance in per-head attention features suggests layer aggregation needed
- First 3 experiments:
  1. Baseline feature audit: Extract all eight features on held-out split; verify lookback ratio separates factual/hallucinated samples in your domain
  2. Token selection ablation: Compare Window(4,2) vs. per-token on your data; expect window advantage but verify magnitude
  3. Transfer probe: Train on CNNDM, test on your domain. If accuracy <0.55, plan for domain-specific labeling and retraining

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the transferability of internal state features be improved to handle the "dataset-specific" nature of hallucinations across diverse domains?
- Basis in paper: Section 7 explicitly identifies "Limited transferability" as a major limitation, noting that features trained on HaluEval fail to generalize to CNNDM and vice versa
- Why unresolved: Different datasets encode fundamentally different hallucination patterns, making current features idiosyncratic rather than universal
- What evidence would resolve it: A study showing a single feature set maintaining high detection accuracy (>80%) when trained on one domain (e.g., news summarization) and tested on a distinct domain (e.g., medical QA)

### Open Question 2
- Question: Can the computational overhead of extracting high-dimensional internal states be reduced to enable real-time hallucination detection without sacrificing interpretability?
- Basis in paper: The Conclusion states that "challenges remain, including high computational costs," and explicitly calls for "optimizing feature extraction for real-time applications" in future work
- Why unresolved: Extracting features like Activation Maps requires $O(w \cdot d \cdot m)$ storage and computation, imposing significant latency
- What evidence would resolve it: The proposal of a lightweight probing method that operates with negligible latency overhead while retaining the detection performance of the full HALU PROBE framework

### Open Question 3
- Question: Why do features extracted from summarization tasks (CNNDM) generalize effectively to question-answering (NQ), while features from factual evaluation datasets (HaluEval) do not?
- Basis in paper: Table 6 shows CNNDM-trained features transfer well to NQ (0.66 accuracy), whereas HaluEval-trained features drop to random-chance levels on other datasets
- Why unresolved: The authors note this discrepancy suggests HaluEval may contain "idiosyncratic patterns" rather than generalizable hallucination signatures, but the specific mechanistic reason remains unexplored
- What evidence would resolve it: A detailed analysis comparing internal state distributions of HaluEval vs. CNNDM to identify specific noise factors or distribution shifts preventing cross-dataset learning

## Limitations

- Dataset-specific feature effectiveness: Features effective for hallucination detection vary significantly across datasets, with lookback ratio failing entirely on HaluEval despite success on CNNDM
- Classifier architecture ambiguity: The paper does not specify the classifier type, architecture, or training parameters used to predict hallucinations from extracted features
- Computational cost constraints: Feature extraction requires significant computational overhead (3.27s/token for attention), making real-time hallucination detection impractical

## Confidence

**High confidence claims:**
- Feature effectiveness varies by dataset (CNNDM vs. HaluEval show opposite patterns)
- Sliced window token selection (w=4, s=2) outperforms per-token strategies
- Lookback ratio correlates with hallucinations in summarization tasks

**Medium confidence claims:**
- Attention-based features provide interpretability into model reasoning
- RAG stabilizes internal states through attention distribution regularization
- Feature transferability across datasets is limited but task-dependent

**Low confidence claims:**
- Specific numerical thresholds for feature values indicating hallucinations
- Generalizability of feature patterns to unseen domains
- Relative importance of feature categories across all task types

## Next Checks

1. **Domain Transferability Test**: Apply the framework to a new domain (e.g., customer support or biomedical text) with 50-100 annotated samples. Train the classifier on CNNDM or HaluEval features and measure accuracy. Expect <0.55 accuracy if features don't transfer, confirming dataset-specific limitations.

2. **Feature Ablation Study**: Systematically remove each of the eight features from the classifier input and measure performance degradation. This will identify which features contribute most to detection accuracy and whether the full feature set is necessary or redundant.

3. **Attention Head Aggregation Analysis**: Compare detection performance using per-head features versus layer-aggregated features on a subset of 100 samples. Measure variance across heads and determine if aggregation improves stability and accuracy, addressing the dataset sensitivity observed in the paper.