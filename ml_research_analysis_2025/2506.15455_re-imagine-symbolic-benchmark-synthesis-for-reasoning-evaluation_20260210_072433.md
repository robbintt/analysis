---
ver: rpa2
title: 'RE-IMAGINE: Symbolic Benchmark Synthesis for Reasoning Evaluation'
arxiv_id: '2506.15455'
source_url: https://arxiv.org/abs/2506.15455
tags:
- reasoning
- alarm
- question
- original
- code
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces RE-IMAGINE, a framework for systematically
  evaluating reasoning abilities in Large Language Models (LLMs) across a three-level
  hierarchy: observe, mutate, and imagine. By converting benchmark problems into symbolic
  code representations, the method generates variations that cannot be solved through
  memorization alone.'
---

# RE-IMAGINE: Symbolic Benchmark Synthesis for Reasoning Evaluation

## Quick Facts
- arXiv ID: 2506.15455
- Source URL: https://arxiv.org/abs/2506.15455
- Authors: Xinnuo Xu; Rachel Lawrence; Kshitij Dubey; Atharva Pandey; Risa Ueno; Fabian Falck; Aditya V. Nori; Rahul Sharma; Amit Sharma; Javier Gonzalez
- Reference count: 40
- Primary result: RE-IMAGINE framework reveals LLMs rely heavily on memorization rather than true reasoning, with performance dropping significantly on mutated problems

## Executive Summary
RE-IMAGINE introduces a novel framework for evaluating reasoning capabilities in Large Language Models (LLMs) by systematically generating benchmark variations through symbolic code representations. The method transforms problems into abstract syntax trees and applies three levels of mutations—observe (minor changes), mutate (structural alterations), and imagine (counterfactual scenarios)—to create problems that cannot be solved through memorization alone. Experiments across math and code reasoning benchmarks demonstrate substantial performance degradation, particularly for higher-level mutations, revealing that current LLMs primarily rely on statistical recall rather than genuine reasoning abilities.

## Method Summary
The RE-IMAGINE framework operates through a three-step pipeline: first, benchmark problems are converted into symbolic code representations using abstract syntax trees; second, systematic mutations are applied at three complexity levels (observe, mutate, imagine) to generate problem variations; third, the mutated problems are evaluated against LLMs to assess reasoning versus memorization capabilities. The symbolic representation ensures problems are fundamentally altered rather than superficially modified, preventing models from relying on pattern matching from training data. This approach provides a scalable method for generating reasoning benchmarks that expose limitations in current LLM architectures.

## Key Results
- LLMs show significant performance drops across all three mutation levels, with observe-level mutations causing 10-20% drops and imagine-level mutations causing 30-50% drops
- Performance degradation is consistent across both math (GSM8K, CLadder) and code (CRUXEval, Loop) reasoning benchmarks
- Even minimal structural changes in problems lead to substantial accuracy declines, indicating heavy reliance on memorization
- The framework successfully differentiates between surface-level pattern matching and genuine reasoning capabilities

## Why This Works (Mechanism)
RE-IMAGINE works by creating a controlled environment where problem variations are systematically generated through symbolic manipulation rather than natural language modifications. The symbolic code representation abstracts problems to their logical core, ensuring that mutations affect the underlying reasoning structure rather than surface features. This approach prevents LLMs from leveraging statistical patterns in training data, as the mutated problems, while logically similar, have different surface representations. The three-level hierarchy provides a gradient of reasoning complexity, allowing precise measurement of where models fail—whether due to basic pattern matching, structural understanding, or counterfactual reasoning capabilities.

## Foundational Learning
- **Symbolic Code Representation**: Converting natural language problems into abstract syntax trees and symbolic code—needed to create problems that cannot be solved through memorization; quick check: can the same logical problem be expressed in multiple syntactically different ways
- **Abstract Syntax Trees (ASTs)**: Tree representations of code structure that enable systematic manipulation—needed to apply controlled mutations at the logical level; quick check: tree traversal algorithms can modify specific structural components
- **Counterfactual Reasoning**: Evaluating hypothetical scenarios that alter problem assumptions—needed to test higher-order reasoning capabilities; quick check: problem remains solvable but requires different logical steps
- **Mutation Testing in Software**: Systematic generation of test variations to evaluate robustness—needed as conceptual foundation for controlled problem variation; quick check: performance changes correlate with mutation complexity
- **Benchmark Synthesis**: Creating new evaluation problems from existing ones—needed to scale reasoning evaluation beyond fixed datasets; quick check: generated problems maintain original reasoning difficulty while having different surface forms
- **Reasoning Hierarchies**: Categorizing reasoning tasks by cognitive complexity—needed to measure progression of reasoning capabilities; quick check: each level requires qualitatively different reasoning strategies

## Architecture Onboarding

**Component Map:**
Problem Input -> Symbolic Parser -> AST Generator -> Mutation Engine (Observe/Mutate/Imagine) -> Code Generator -> LLM Evaluation

**Critical Path:**
Symbolic Parser → AST Generator → Mutation Engine → Code Generator → LLM Evaluation

**Design Tradeoffs:**
- Symbolic representation provides precise control but may lose some natural language nuances
- Three-level hierarchy offers granularity but may not capture all reasoning complexity dimensions
- Automated mutation generation enables scalability but requires careful validation of logical equivalence
- Code-based problems enable systematic manipulation but limit evaluation to programming domains

**Failure Signatures:**
- Poor performance on observe-level mutations suggests pattern matching dominance
- Inconsistent performance across mutation levels indicates unreliable reasoning capabilities
- High accuracy on original benchmarks but low accuracy on mutated versions reveals memorization reliance
- Domain-specific failures suggest limited generalization of reasoning abilities

**3 First Experiments:**
1. Apply RE-IMAGINE to GSM8K benchmark and measure accuracy drop across mutation levels
2. Compare performance of different LLM architectures on same mutated problems
3. Analyze failure patterns in code reasoning benchmarks to identify specific reasoning gaps

## Open Questions the Paper Calls Out
None

## Limitations
- The framework may not capture all forms of reasoning complexity through its three-level hierarchy
- Performance drops could indicate symbolic processing limitations rather than reasoning deficiencies
- The approach has been validated primarily on math and code domains, requiring testing on other reasoning types
- The assumption that symbolic representation reveals "true reasoning" needs further validation

## Confidence
- Effectiveness for evaluating reasoning: Medium - shows promise but limited domain validation
- Completeness of reasoning hierarchy: Low - unclear if three levels capture full reasoning spectrum
- Interpretation of performance drops as reasoning failures: Medium - requires additional validation

## Next Checks
1. Test RE-IMAGINE framework on additional reasoning domains (e.g., logical reasoning, commonsense reasoning) to verify generalizability
2. Compare performance of different LLM architectures (including newer reasoning-focused models) on RE-IMAGINE benchmarks to assess consistency of findings
3. Conduct ablation studies where symbolic representations are removed to determine whether performance degradation is specifically due to reasoning challenges or symbolic processing limitations