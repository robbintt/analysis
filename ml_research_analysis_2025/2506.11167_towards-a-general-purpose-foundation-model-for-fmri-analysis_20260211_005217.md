---
ver: rpa2
title: Towards a general-purpose foundation model for fMRI analysis
arxiv_id: '2506.11167'
source_url: https://arxiv.org/abs/2506.11167
tags:
- fmri
- neurostorm
- data
- brain
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces NeuroSTORM, a foundation model for fMRI analysis
  that addresses reproducibility and transferability challenges in neuroimaging research.
  The model employs a Mamba backbone with shifted-window mechanisms to efficiently
  process 4D fMRI volumes, coupled with spatiotemporal redundancy dropout and task-specific
  prompt tuning for effective pre-training and fine-tuning.
---

# Towards a general-purpose foundation model for fMRI analysis

## Quick Facts
- **arXiv ID**: 2506.11167
- **Source URL**: https://arxiv.org/abs/2506.11167
- **Reference count**: 0
- **Primary result**: NeuroSTORM achieves 93.3% gender prediction accuracy, 87.56% MND diagnosis accuracy, and 92.64% task-based fMRI classification accuracy using raw 4D fMRI volumes

## Executive Summary
NeuroSTORM introduces a foundation model for fMRI analysis that addresses reproducibility and transferability challenges in neuroimaging research. The model employs a Mamba backbone with shifted-window mechanisms to efficiently process 4D fMRI volumes, coupled with spatiotemporal redundancy dropout and task-specific prompt tuning for effective pre-training and fine-tuning. Trained on over 50,000 subjects across multiple centers, NeuroSTORM demonstrates superior performance across five downstream tasks while maintaining efficiency in data-scarce scenarios.

## Method Summary
NeuroSTORM processes raw 4D fMRI volumes using a Shifted-Window Mamba (SWM) backbone that avoids atlas-based ROI summaries to preserve spatiotemporal information. The model is pre-trained using a masked autoencoder paradigm with a Spatiotemporal Redundancy Dropout (STRD) module that prevents learning trivial local redundancies. Fine-tuning is performed through task-specific Prompt Tuning (TPT), which updates only a small set of parameters while keeping the pre-trained backbone fixed. The model is trained on a diverse corpus of 28.65 million fMRI frames from 50,000+ subjects across multiple datasets.

## Key Results
- **Gender prediction**: 93.3% accuracy on HCP-YA dataset
- **Disease diagnosis**: 87.56% accuracy on MND dataset
- **Task-based fMRI classification**: 92.64% accuracy on HCP-EP dataset
- **Phenotype prediction**: PCC up to 0.587 on psychological/cognitive traits
- **fMRI retrieval**: 73.2% accuracy in finding related images

## Why This Works (Mechanism)

### Mechanism 1
Processing raw 4D fMRI volumes with a Shifted-Window Mamba backbone improves transferability by preserving spatiotemporal information that gets lost in atlas-based ROI summaries. The Mamba-based state-space model effectively captures long-range dependencies through linear-time processing of the full 4D sequence.

### Mechanism 2
The Spatiotemporal Redundancy Dropout module forces the model to learn robust long-range representations by preventing it from exploiting trivial local redundancies in fMRI data. This addresses the high spatiotemporal redundancy that causes standard Masked Autoencoders to learn trivial reconstructions.

### Mechanism 3
Task-specific Prompt Tuning allows efficient fine-tuning on diverse downstream tasks by adapting only a small set of parameters while keeping the large pre-trained backbone fixed. This leverages general knowledge from pre-training while adapting to specific tasks with minimal computational cost.

## Foundational Learning

### State Space Models (SSMs) and Mamba
- **Why needed here**: The NeuroSTORM architecture is built on a Mamba backbone, a variant of SSMs, which is crucial for understanding how the model processes sequential 4D data with linear complexity.
- **Quick check question**: How does an SSM differ from a standard Transformer's self-attention mechanism in handling long sequences?

### Masked Autoencoders (MAE)
- **Why needed here**: The pre-training phase uses a masked autoencoder paradigm, essential for understanding the self-supervised learning objective and how STRD modifies it.
- **Quick check question**: In a standard MAE, what part of the input is masked, and what is the model trained to predict?

### Prompt Tuning
- **Why needed here**: Task-specific Prompt Tuning is the method used for fine-tuning, explaining how the model is efficiently adapted to downstream tasks.
- **Quick check question**: What part of the model is updated during prompt tuning, and what part remains frozen?

## Architecture Onboarding

### Component map
Input (4D fMRI volume) -> Patch Embedding -> Encoder (SWM Backbone with STRD) -> Pre-training Head (Decoder) -> Downstream Head + Prompt Tuning Module

### Critical path
1. Preprocess diverse fMRI datasets into standardized 4D volumes in MNI space
2. Train encoder and decoder using MAE objective with STRD module on massive corpus
3. Discard decoder; trained encoder becomes frozen backbone
4. For new downstream task, prepend learnable prompts to input
5. Train only prompts and new task-specific head

### Design tradeoffs
- Raw 4D vs. ROI-based: Raw volumes preserve full information but have higher computational cost
- Mamba (SSM) vs. Transformer: Mamba offers linear-time complexity critical for long 4D sequences
- STRD vs. Standard MAE: STRD adds complexity to prevent trivial local learning
- Prompt Tuning vs. Full Fine-tuning: TPT is extremely parameter-efficient but may trade off peak performance

### Failure signatures
- High memory usage: Full 4D processing is demanding (~44GB GPU memory during pre-training)
- Trivial reconstruction: Without STRD, pre-training loss drops too quickly with poor downstream performance
- Catastrophic forgetting: Full fine-tuning on small datasets may erase pre-trained knowledge
- Poor clinical transfer: Strong performance on age/gender but failure on disease diagnosis

### First 3 experiments
1. Baseline Validation: Reproduce age/gender prediction on HCP-YA dataset using provided pre-trained weights and TPT
2. STRD Ablation: Retrain on UKB subset without STRD, then fine-tune to validate STRD's utility
3. Fine-tuning Strategy Comparison: Compare TPT against full fine-tuning and linear probing on single downstream task

## Open Questions the Paper Calls Out

### Open Question 1
Does integrating individual-specific brain connectivity networks (via Graph Neural Networks) into the architecture improve neurobiological interpretability compared to the current voxel-masking approach? The current model relies on volumetric voxel-masking without the inductive biases provided by connectivity networks or atlases.

### Open Question 2
Does increasing the proportion of task-based fMRI data beyond the current 20% in pre-training specifically improve decoding specificity for complex cognitive operations? The current pre-training corpus is dominated by resting-state fMRI, potentially limiting exposure to active cognitive states.

### Open Question 3
Can a hybrid model combining 4D volumetric processing with brain network features effectively outperform the current volume-only NeuroSTORM in extreme low-data regimes (e.g., 10% training data)? ROI-based methods retain advantages in extreme scarcity due to lower dimensionality and higher signal-to-noise ratios.

## Limitations

- **Unknown hyperparameters**: Exact architecture depths, hidden dimensions, and training hyperparameters are referenced to supplementary information not provided
- **Computational demands**: Full 4D processing requires significant GPU memory (~44GB) and training time (~13 days)
- **Clinical validation**: While showing promise on psychological/cognitive traits, requires independent validation on held-out clinical cohorts

## Confidence

- **High**: Raw 4D processing + SWM backbone performance; computational efficiency claims
- **Medium**: STRD module effectiveness; TPT superiority claims
- **Medium**: Clinical transferability across diverse phenotypes

## Next Checks

1. **STRD Ablation Study**: Train NeuroSTORM on UKB pre-training data without STRD, then fine-tune on gender prediction to validate whether STRD prevents trivial local learning.

2. **Cross-Scanner Generalization**: Test the pre-trained model on a dataset from a scanner/protocol not present in pre-training (e.g., ABIDE or ADHD200) to assess robustness to acquisition differences.

3. **TPT vs. Full Fine-tuning**: On MND diagnosis, compare TPT against full fine-tuning and linear probing across varying training set sizes (n=100, 500, 2000) to quantify efficiency/accuracy tradeoffs.