---
ver: rpa2
title: 'From Generative to Episodic: Sample-Efficient Replicable Reinforcement Learning'
arxiv_id: '2507.11926'
source_url: https://arxiv.org/abs/2507.11926
tags:
- algorithm
- policy
- probability
- replicable
- sample
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper resolves a long-standing open question in the theory
  of replicable learning: whether exploration is inherently more difficult than batch
  learning for reinforcement learning (RL). The authors show that it is not, giving
  the first nearly sample-optimal replicable RL algorithm in the low-horizon setting.'
---

# From Generative to Episodic: Sample-Efficient Replicable Reinforcement Learning

## Quick Facts
- **arXiv ID:** 2507.11926
- **Source URL:** https://arxiv.org/abs/2507.11926
- **Authors:** Max Hopkins; Sihan Liu; Christopher Ye; Yuichi Yoshida
- **Reference count:** 40
- **Primary result:** First nearly sample-optimal ρ-replicable (ε,δ)-PAC policy estimator for episodic RL, matching best non-replicable algorithms' complexity up to logs.

## Executive Summary
This paper resolves a long-standing open question in replicable learning theory by showing that exploration is not inherently more difficult than batch learning for reinforcement learning. The authors develop a ρ-replicable algorithm that identifies "ignorable states" - those contributing negligibly to any optimal policy's value - without requiring replicable exploration itself. By combining multiple runs of non-replicable exploration with correlated sampling techniques, they achieve sample complexity of O(S²A/ρ²ε² · poly(H,log(1/δ))) in the episodic setting, matching the best known non-replicable RL algorithms up to logarithmic factors.

## Method Summary
The algorithm employs a two-phase approach. First, it runs Q-learning on a zero-reward MDP multiple times to estimate the probability of each state being under-explored, then uses correlated sampling with shared randomness to replicably round these estimates and identify ignorable states. In the second phase, it collects sufficient samples from non-ignorable states and applies a replicable multi-instance best-arm algorithm with tiered backward induction, using pessimistic penalties for under-sampled states. The key insight is converting non-replicable exploration into a "fractional" probability estimate that can be deterministically rounded using correlated sampling.

## Key Results
- Achieves ρ-replicable (ε,δ)-PAC policy estimation with sample complexity O(S²A/ρ²ε² · poly(H,log(1/δ)))
- Matches best known non-replicable RL sample complexity up to logarithmic factors
- Introduces near-tight lower bound for sign-one-way marginals problem
- Proves exploration is not inherently harder than batch learning for replicable RL

## Why This Works (Mechanism)

### Mechanism 1: Fractional State Identification & Replicable Rounding
The system runs non-replicable exploration (Q-learning on zero-reward MDP) multiple times, computes empirical probabilities of states being under-explored, and uses correlated sampling with shared random seeds to deterministically round these probabilities to identical "ignorable state" sets across independent runs. This works because empirical means converge sufficiently fast between runs.

### Mechanism 2: Zero-Reward Reachability Estimation
Instead of optimizing for reward, the exploration phase runs Q-learning on a reward-free MDP where Q-values represent optimistic upper bounds on state reachability. States rarely visited by this agent must have low reachability under any policy and thus contribute negligibly to optimal value.

### Mechanism 3: Tiered Sample Collection & Pessimistic Induction
States are partitioned by estimated reachability, with high-reachability states receiving more samples. A pessimistic penalty proportional to inverse sample count is subtracted from under-sampled states during backward induction, ensuring the policy avoids risky, under-explored states.

## Foundational Learning

- **Replicable Learning (Definition 1.2)**: Algorithm must output identical policies on two independent runs with probability ≥ 1-ρ. Critical because standard RL optimizes for reward but this must ensure identical outputs.
  - *Quick check:* If an algorithm outputs similar (but not identical) policies on two runs, is it replicable? (No, must be identical)

- **Generative vs. Episodic Models**: Generative allows querying any state; episodic requires navigating to states. Exploration harder in episodic for replicability because agent's path is stochastic, making "visit count" non-replicable.
  - *Quick check:* Why is exploration harder in episodic setting for replicability? (Because path is stochastic, making visit counts non-replicable)

- **PAC Policy Estimation**: Goal is finding policy π such that V(π) ≥ V* - ε with probability 1-δ. Identifying ignorable states helps by ensuring cumulative value of ignored states is less than ε.
  - *Quick check:* How does identifying "ignorable states" help satisfy PAC guarantee? (By ensuring ignored states' cumulative value is less than ε)

## Architecture Onboarding

- **Component map:** Qexplore (non-replicable) → Fractional Averager → Replicable Rounding → Sample Collector → Tiered Backward Induction
- **Critical path:** Replicable Rounding step. If empirical probabilities don't converge closely enough between runs, correlated sampling outputs different sets of ignorable states, causing downstream policy differences.
- **Design tradeoffs:** Achieves optimal sample complexity (Õ(S²A)) but admits it's not computationally efficient due to exponential cost of correlated sampling over state space. Offers polynomial-time variant with worse sample complexity (Õ(S³A)).
- **Failure signatures:** High TV distance between rounding distributions exceeding ρ; tier misclassification placing critical state in low tier; phantom action bias in sample collection.
- **First 3 experiments:**
  1. Replicability Stress Test: Run algorithm 100 times on fixed MDP, verify output policy identical in ≥ 1-ρ fraction of runs.
  2. Sample Scaling Verification: Vary S, plot total samples vs S², verify curve fits Õ(S²) bound.
  3. Ablation on Tiering: Compare tiered vs flat configuration on sparse MDPs, verify tiered uses significantly fewer samples.

## Open Questions the Paper Calls Out

### Open Question 1
Can the dependence on action space A in sample complexity be tightened to bridge gap between Õ(S²A) upper bound and Ω(S²) lower bound? Based on Section 1.6 asking to close this gap. Unresolved because current lower bound uses specific construction where agent can discard most actions, while upper bound assumes parallel sampling over all actions. Resolution would require sample-adaptive algorithm achieving Õ(S²) or new lower bound proving Ω(S²A) is necessary.

### Open Question 2
Can polynomial dependence on horizon H in sample complexity be optimized? Based on Section 1.6 identifying improving H dependency as interesting direction, noting current work focuses on low-horizon setting. Unresolved because analysis treats H as small parameter and doesn't explicitly optimize degree of polynomial factors. Resolution would require analysis providing tight bounds on H exponent or modified algorithm optimized for large horizons.

### Open Question 3
Is it possible to achieve optimal Õ(S²A) sample complexity with computationally efficient algorithm? Based on Remark 1.6 stating Theorem 1.3 is not computationally efficient due to non-poly-time subroutines, while efficient variant requires Õ(S³A) samples. Unresolved because optimal sample algorithm relies on computationally expensive procedures like high-dimensional randomized rounding. Resolution would require poly-time algorithm matching Õ(S²A) bound.

## Limitations

- Correlated sampling requires exact sharing of random seeds between independent runs, challenging in distributed settings
- Zero-reward exploration assumes reachability determines ignorability, which may fail with reward traps or structured value landscapes
- Exponential-time correlated sampling over state space creates significant computational barrier versus polynomial-time variant with worse sample complexity

## Confidence

- **High confidence**: Sample complexity bounds and replicability guarantees in episodic setting (supported by formal proofs)
- **Medium confidence**: Practical effectiveness of tiered approach in diverse MDP structures (theoretical but not empirically validated)
- **Low confidence**: Computational feasibility of correlated sampling implementation at scale (paper explicitly notes this limitation)

## Next Checks

1. Implement correlated sampling subroutine and verify two independent runs with shared seed produce identical output policies across 100 trials on benchmark MDPs
2. Test algorithm's performance on MDPs with reward traps or structured value landscapes to identify conditions where reachability-ignorable assumption breaks
3. Benchmark polynomial-time variant against exponential-time version to quantify sample complexity trade-off in practice