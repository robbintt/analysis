---
ver: rpa2
title: 'LLM Agents Implement an NLG System from Scratch: Building Interpretable Rule-Based
  RDF-to-Text Generators'
arxiv_id: '2512.18360'
source_url: https://arxiv.org/abs/2512.18360
tags:
- system
- triples
- text
- output
- code
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces a novel approach to RDF-to-text generation
  where multiple LLM agents collaborate to design, implement, and refine a rule-based
  Python system from scratch using only RDF triples, without human reference texts.
  The agents include a Software Architect for system design, a Software Engineer for
  implementation, a Test Engineer for generating unit tests, an Evaluator for testing
  outputs, and a Code Analyst for debugging.
---

# LLM Agents Implement an NLG System from Scratch: Building Interpretable Rule-Based RDF-to-Text Generators

## Quick Facts
- arXiv ID: 2512.18360
- Source URL: https://arxiv.org/abs/2512.18360
- Reference count: 36
- The paper introduces a novel approach where multiple LLM agents collaborate to design and implement a rule-based Python system for RDF-to-text generation without human reference texts

## Executive Summary
This paper presents a novel approach where multiple LLM agents work collaboratively to design, implement, and refine a rule-based Python system for RDF-to-text generation from scratch. The system requires no supervised training data and runs nearly instantaneously on a single CPU. The approach leverages five specialized agents - Software Architect, Software Engineer, Test Engineer, Evaluator, and Code Analyst - to create an interpretable system that outperforms fine-tuned and prompted LLMs on WebNLG and OpenDialKG datasets, achieving up to 35x faster inference while eliminating hallucinations.

## Method Summary
The approach employs a multi-agent system where five specialized LLM agents collaborate to build a rule-based RDF-to-text generator. The Software Architect designs the system architecture, the Software Engineer implements the code, the Test Engineer generates unit tests, the Evaluator assesses outputs for correctness and fluency, and the Code Analyst debugs any issues. This collaborative process operates entirely on RDF triples without human reference texts, producing a fully interpretable Python system that requires no supervised training data. The resulting system generates text from RDF triples through deterministic rule-based transformations rather than neural predictions.

## Key Results
- Eliminates hallucinations compared to fine-tuned or prompted LLMs with only slight fluency penalties
- Achieves up to 35x faster inference on CPU than neural baselines
- Human evaluation confirmed zero hallucinations in generated outputs
- Python engineers successfully interpreted and modified the generated code

## Why This Works (Mechanism)
The approach works by shifting the complexity burden from training neural models to the collaborative intelligence of multiple specialized LLM agents. Each agent handles a specific aspect of system development - architecture design, implementation, testing, evaluation, and debugging - creating a comprehensive development pipeline. The rule-based nature of the final system ensures deterministic outputs with no hallucination potential, while the agent collaboration enables sophisticated logic without requiring training data or large computational resources during inference.

## Foundational Learning
1. RDF triples and knowledge graph representation - Understanding how information is structured in Resource Description Framework format is essential for this work
   *Why needed:* The entire system operates on transforming RDF triples to natural language
   *Quick check:* Can you explain the difference between subject, predicate, and object in an RDF triple?

2. Rule-based NLG vs neural approaches - Rule-based systems use deterministic transformations while neural systems learn probabilistic mappings
   *Why needed:* The paper contrasts these approaches and demonstrates advantages of rule-based generation
   *Quick check:* Can you list two advantages and two disadvantages of rule-based NLG compared to neural approaches?

3. Multi-agent collaborative systems - Multiple specialized agents working together to achieve a complex goal
   *Why needed:* The entire methodology relies on five different agent roles working in coordination
   *Quick check:* Can you describe how the five agents (Architect, Engineer, Test Engineer, Evaluator, Code Analyst) interact in this system?

## Architecture Onboarding
Component map: Architect -> Engineer -> Test Engineer -> Evaluator -> Code Analyst (iterative refinement)

Critical path: The Software Architect defines the system design, which the Software Engineer implements. The Test Engineer creates unit tests to validate functionality, while the Evaluator assesses output quality. The Code Analyst debugs issues, and this cycle repeats until satisfactory results are achieved.

Design tradeoffs: The approach trades model training complexity for prompt engineering complexity and agent coordination overhead. While eliminating training data requirements and achieving