---
ver: rpa2
title: Deep reinforcement learning for optimal trading with partial information
arxiv_id: '2511.00190'
source_url: https://arxiv.org/abs/2511.00190
tags:
- trading
- signal
- agent
- network
- information
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes three deep reinforcement learning algorithms
  for optimal trading under partial information. The trading signal follows an Ornstein-Uhlenbeck
  process with regime-switching parameters.
---

# Deep reinforcement learning for optimal trading with partial information

## Quick Facts
- arXiv ID: 2511.00190
- Source URL: https://arxiv.org/abs/2511.00190
- Authors: Andrea Macrì; Sebastian Jaimungal; Fabrizio Lillo
- Reference count: 40
- Primary result: prob-DDPG achieves highest cumulative rewards by providing explicit posterior regime probabilities to the trading agent

## Executive Summary
This paper addresses optimal trading under partial information where the signal follows an Ornstein-Uhlenbeck process with regime-switching parameters. The authors propose three deep reinforcement learning algorithms that integrate GRU networks with DDPG to extract temporal dependencies and latent regime information. The methods differ in how they process the GRU output: hid-DDPG uses raw hidden states, prob-DDPG estimates posterior regime probabilities, and reg-DDPG forecasts next signal values. Across simulations and real equity pair trading data, prob-DDPG demonstrates superior performance by creating a targeted information bottleneck that filters noise while preserving control-relevant drift parameters.

## Method Summary
The approach models the trading signal as an Ornstein-Uhlenbeck process with regime-switching parameters (θ, κ, σ) following independent Markov chains. Three DDPG-GRU variants are proposed: hid-DDPG uses GRU hidden states as features, prob-DDPG estimates posterior regime probabilities through a two-step process (GRU classifier then DDPG), and reg-DDPG forecasts next signal values. The prob-DDPG architecture creates an information bottleneck by compressing signal history into low-dimensional probability vectors, reducing variance in policy gradients compared to hid-DDPG. Training uses Adam optimizer with learning rate 0.001 and batch sizes of 512 (synthetic) or 64 (real).

## Key Results
- prob-DDPG outperforms hid-DDPG and reg-DDPG across all simulation scenarios with varying regime-switching complexity
- Explicit posterior regime probabilities provide more effective optimization targets than raw hidden states or point forecasts
- On real equity pair trading data (INTC/SMH), prob-DDPG achieves the highest cumulative rewards and most interpretable trading strategies
- Providing structured probabilistic insights into latent regimes substantially improves both profitability and robustness of RL-based trading strategies

## Why This Works (Mechanism)

### Mechanism 1: Information Bottleneck via Explicit Regime Posterior
The prob-DDPG architecture creates a targeted information bottleneck by compressing signal history into low-dimensional probability vectors. This filters out high-frequency noise while preserving control-relevant drift parameters, reducing variance of policy gradients during optimization compared to hid-DDPG's high-dimensional noisy hidden states.

### Mechanism 2: Decoupled Stability (Filtering before Control)
prob-DDPG decouples state estimation from control by training the GRU classifier via supervised learning before RL training begins. This provides the DDPG agent with a stationary input distribution, avoiding the instability of joint optimization where representation of "state" shifts continuously as GRU weights update.

### Mechanism 3: Parametric vs. Point-Prediction Inputs
prob-DDPG supplies parameter estimates (regime probabilities) rather than point-forecasts of future signal values. In OU processes, optimal action depends on distance between current signal and long-run mean. Providing likely parameters of dynamics is more effective for optimal control than supplying noisy sample paths dominated by stochastic noise.

## Foundational Learning

- **Ornstein-Uhlenbeck (OU) Process with Regime Switching**: Mathematical model of market signal that tends to revert to a mean (θ) but where this mean shifts unexpectedly. Understanding this explains why the agent needs recurrent memory to detect current regime. Quick check: If speed of mean reversion (κ) increases, how should agent's reaction time change?

- **Deep Deterministic Policy Gradient (DDPG)**: Core learning algorithm for continuous action spaces (inventory levels). Actor (π) decides inventory and Critic (Q) scores that decision. Quick check: Why is "exploration noise" added to Actor's output during training but not during testing?

- **Gated Recurrent Units (GRU) & Hidden States**: Process time-series history. The distinction between using GRU's hidden state (vector of numbers) versus classification output (probability) is central to paper's results. Quick check: What is specific output of GRU in prob-DDPG versus hid-DDPG?

## Architecture Onboarding

- **Component map**: Data Source -> GRU Feature Extractor -> Interface (hid/hid/prob) -> DDPG Controller -> Environment
- **Critical path**: Performance hinges on Interface layer. For prob-DDPG, critical path is ensuring GRU accurately classifies regimes before DDPG training begins.
- **Design tradeoffs**: Interpretability vs. End-to-End Training (prob-DDPG interpretable but requires two-step pipeline; hid-DDPG single pipeline but black box). Supervision Requirement (prob-DDPG needs labeled regime data; hid-DDPG needs no labels).
- **Failure signatures**: Low Rewards + High Variance (hid-DDPG struggles to converge); Near-Zero Rewards (reg-DDPG trades cautiously because predicted next-price provides no clear edge); Sudden Collapse (GRU misclassifies regime in prob-DDPG, causing large contrarian positions at wrong time).
- **First 3 experiments**:
  1. Baseline Sanity Check: Replicate simulation comparing hid-DDPG vs. prob-DDPG reward curves to verify primary claim.
  2. Look-back Window Sensitivity: Vary window size W (e.g., 10 vs 50) in prob-DDPG to determine history needed for accurate regime detection.
  3. Transaction Cost Stress Test: Increase λ in simulation to verify prob-DDPG maintains advantage by trading less frequently but more accurately than reg-DDPG.

## Open Questions the Paper Calls Out

### Open Question 1
How does the quality and type of latent information affect equilibria in a multi-agent trading environment? The paper suggests future research should investigate this, but current study is strictly single-agent without modeling strategic interactions or price impacts of multiple traders simultaneously learning from latent signals.

### Open Question 2
Does prob-DDPG retain superiority when optimizing for risk-averse utility functions rather than risk-neutral profit? The paper defines agent as "risk-neutral" aiming to maximize "expected discounted profits," but real-world trading often requires penalizing volatility or drawdowns.

### Open Question 3
Does performance ranking change if regime-switching parameters follow correlated rather than independent stochastic processes? The methodology assumes independence, but in real markets volatility is often correlated with speed of mean reversion, which might affect classifier performance differently.

## Limitations
- Real-data experiment provides only qualitative comparisons without statistical significance testing
- Decoupling assumption may not hold in rapidly evolving markets where regime dynamics shift faster than two-step pipeline can adapt
- Comparison between hid-DDPG and prob-DDPG assumes latter's feature extraction is genuinely superior rather than benefiting from additional supervision

## Confidence
- **High Confidence**: Simulation methodology clearly specified with exact parameters; mathematical framework rigorous; algorithm comparison design methodologically sound
- **Medium Confidence**: Real-data implementation details incomplete (missing exact preprocessing steps and statistical significance metrics); claim that probabilistic features outperform hidden states needs stronger empirical validation
- **Low Confidence**: Paper does not address handling non-stationary regime dynamics or situations where filtering accuracy degrades below critical thresholds

## Next Checks
1. Apply paired t-tests or bootstrap confidence intervals to compare cumulative rewards across three algorithms on real data to verify if prob-DDPG's outperformance is statistically significant
2. Systematically degrade GRU's classification accuracy in simulation to determine minimum filtering performance required for prob-DDPG to maintain advantage over hid-DDPG
3. Test three algorithms across multiple cointegrated pairs or different market conditions to assess whether prob-DDPG's superiority generalizes beyond single INTC/SMH pair presented