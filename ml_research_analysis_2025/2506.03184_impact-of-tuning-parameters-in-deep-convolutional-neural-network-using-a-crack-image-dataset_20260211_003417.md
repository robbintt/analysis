---
ver: rpa2
title: Impact of Tuning Parameters in Deep Convolutional Neural Network Using a Crack
  Image Dataset
arxiv_id: '2506.03184'
source_url: https://arxiv.org/abs/2506.03184
tags:
- pooling
- activation
- adam
- tanh
- optimizer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The study investigates how various tuning parameters\u2014activation\
  \ functions, optimizers, and pooling methods\u2014affect the performance of a deep\
  \ convolutional neural network (DCNN) for classifying crack images. Using a dataset\
  \ of 800 images (400 negative, 400 positive) with a DCNN architecture of two convolutional\
  \ layers, two pooling layers, one dropout, and a dense layer, the research evaluates\
  \ eight activation functions (relu, tanh, gelu, elu, selu, silu, softmax, softplus),\
  \ five optimizers (SGD, AdaGrad, AdaDelta, RMSProp, Adam), and two pooling strategies\
  \ (max pooling, average pooling)."
---

# Impact of Tuning Parameters in Deep Convolutional Neural Network Using a Crack Image Dataset

## Quick Facts
- arXiv ID: 2506.03184
- Source URL: https://arxiv.org/abs/2506.03184
- Reference count: 27
- Primary result: Max pooling with tanh activation and Adam optimizer achieves 95.42% validation accuracy on crack image classification

## Executive Summary
This study systematically evaluates how activation functions, optimizers, and pooling methods impact the performance of a deep convolutional neural network (DCNN) for crack image classification. Using a dataset of 800 images (400 negative, 400 positive) and a custom DCNN architecture, the research tests eight activation functions, five optimizers, and two pooling strategies. The results demonstrate that max pooling consistently outperforms average pooling across all configurations, with Adam optimizer combined with tanh activation yielding the highest validation accuracy of 95.42%.

## Method Summary
The study employs a custom DCNN architecture with two convolutional layers, two pooling layers (2×2), one dropout layer, and a dense layer for binary classification of crack images. The dataset contains 800 images (227×227 RGB) split between positive and negative crack examples. Eight activation functions (ReLU, tanh, gelu, elu, selu, silu, softmax, softplus) are tested with five optimizers (SGD, AdaGrad, AdaDelta, RMSProp, Adam) and two pooling methods (max pooling, average pooling). The primary evaluation metrics are training and validation accuracy using an 80/20 train/validation split.

## Key Results
- Max pooling consistently outperforms average pooling across all configurations
- Adam optimizer delivers superior performance compared to other optimizers
- Tanh activation achieves 100% training accuracy while ReLU excels in validation accuracy
- Optimal configuration: Adam optimizer with tanh activation and max pooling yields 95.42% validation accuracy

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Max pooling outperforms average pooling for crack feature extraction in this architecture.
- Mechanism: Cracks manifest as high-contrast edges or lines against a surface. Max pooling selects the maximum value from a region, preserving these salient, sparse edge features. In contrast, average pooling smooths values, potentially diluting the crack's signal by averaging it with the background.
- Core assumption: The critical features (cracks) are spatially sparse and distinct from the background texture.
- Evidence anchors:
  - [abstract] "Experimental results show that max pooling outperforms average pooling in all configurations."
  - [section] Page 5-6, Section 3.1: "...max pooling rejects a good amount of data by extracting only the most salient features... Average pooling encourages the network to identify the complete extent of the object, whereas max pooling restricts that to only the very important features."
  - [corpus] Weak/missing direct support for crack-specific pooling mechanics; neighbor papers focus on other domains (e.g., malware, snow).
- Break condition: If the distinguishing features were texture-based or required holistic context rather than localized edges, average pooling might theoretically perform better.

### Mechanism 2
- Claim: The Adam optimizer converges better than SGD or AdaDelta for this binary classification task.
- Mechanism: Adam combines the benefits of AdaGrad (handling sparse gradients) and RMSProp (adapting learning rates based on moving averages). This allows the network to navigate the loss landscape effectively even when gradient information is noisy or sparse, as is common in image data.
- Core assumption: The dataset or gradient updates exhibit noise or sparsity that benefits from adaptive moment estimation.
- Evidence anchors:
  - [section] Page 6, Section 3.3: "It is mainly because Adam can handle sparse gradients on noisy problems. It is the combination of the best properties of the AdaGrad and RMSProp algorithms."
  - [section] Page 4, Section 2.4: Describes Adam as combining RMSprop and SGD techniques.
  - [corpus] Weak/missing; corpus neighbors do not provide comparative optimizer analysis for this specific dataset.
- Break condition: If the problem required highly stable, monotonically decreasing learning rates without momentum, or if the dataset was trivially simple, simpler optimizers might suffice.

### Mechanism 3
- Claim: Tanh activation achieves higher training accuracy (100%) due to zero-centering, while ReLU excels in validation accuracy (observed as 96.67% vs 95.42% in Table II).
- Mechanism: Tanh outputs values in [-1, 1], which centers the data and aids convergence for binary classification. However, it suffers from vanishing gradients at extreme inputs. ReLU avoids vanishing gradients for positive inputs but can suffer from "dead neurons." The authors suggest Tanh fits the training data perfectly but implies ReLU generalizes slightly better on validation data.
- Core assumption: The network depth is shallow enough that vanishing gradients do not critically halt Tanh training, but affect generalization.
- Evidence anchors:
  - [section] Page 5: "The reason why tanh is better for our implementation is we are considering binary classes... Relu performs better in terms of validation accuracy because tanh might lead to fading gradient problem."
  - [abstract] "...tanh achieves the highest training accuracy (100%) while relu excels in validation accuracy."
  - [corpus] Weak/missing.
- Break condition: If the network were significantly deeper, Tanh would likely fail to train due to severe vanishing gradients.

## Foundational Learning

- Concept: **Max vs. Average Pooling**
  - Why needed here: To understand why downsampling must preserve specific local maxima (crack pixels) rather than general averages (background).
  - Quick check question: If a crack line is 1 pixel wide in a 2x2 pooling window, what value does Max Pooling output versus Average Pooling?

- Concept: **Zero-centered Activation (Tanh)**
  - Why needed here: To grasp why the authors argue Tanh (-1 to 1) is "convenient" for binary classification over unbounded or [0,1] functions.
  - Quick check question: Why might a zero-centered output (negative and positive values) speed up convergence compared to a function that outputs only positive values?

- Concept: **Adaptive Moment Estimation (Adam)**
  - Why needed here: To differentiate why a fixed learning rate (SGD) might fail where an adaptive one (Adam) succeeds on this dataset.
  - Quick check question: What two metrics does Adam calculate for each parameter to adjust the learning rate?

## Architecture Onboarding

- Component map: Input(227x227x3) → [Conv2D+Activation] → MaxPool → [Conv2D+Activation] → MaxPool → Dropout → Dense → Output
  - Note: The study uses a relatively shallow custom DCNN, not a pre-trained backbone.

- Critical path: The two **MaxPool** layers are the critical down-sampling points; if configured incorrectly (e.g., too aggressive), the spatial information of the cracks is lost before reaching the Dense layer.

- Design tradeoffs:
  - **Tanh vs. ReLU**: Tanh offers 100% training fit (potential overfitting) vs. ReLU's higher validation score (better generalization).
  - **Max vs. Avg Pool**: Max extracts sharp features (cracks) while Avg smooths them out (loss of signal).

- Failure signatures:
  - **Softplus Activation**: Table II shows validation accuracy dropping to ~49% (random guess) when using Softplus with Adam. This indicates the activation function is incompatible with this shallow architecture or weight initialization.
  - **AdaDelta Optimizer**: Shows high instability (e.g., dropping to ~50% with Tanh+Max), failing to converge on this dataset.

- First 3 experiments:
  1. **Baseline Replication**: Implement the exact architecture (2 Conv, 2 MaxPool, 1 Dense) with Tanh and Adam to verify the 95.42% validation claim.
  2. **Pooling Ablation**: Swap MaxPool for AveragePooling while holding Adam/Tanh constant to observe the performance drop described in Section 3.1.
  3. **Generalization Test**: Run the "Adam + ReLU + MaxPool" configuration (which achieved 96.67% validation in Table II) to compare the generalization gap against the Tanh configuration.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do convolutional and pooling kernel sizes, as well as varying dropout rates, impact the classification accuracy of the DCNN on the crack image dataset?
- Basis in paper: [explicit] The conclusion states, "In the future, the impact of more tuning parameters like kernel size of convolutional and pooling layers, drop-out rate may also check."
- Why unresolved: The current study fixed these architectural hyperparameters to observe only the effects of optimizers, activation functions, and pooling types.
- What evidence would resolve it: Experimental results comparing validation accuracies while systematically varying kernel dimensions and dropout probabilities.

### Open Question 2
- Question: Do the optimal tuning configurations (Adam optimizer, tanh activation) identified in this study generalize to other standard benchmark datasets?
- Basis in paper: [explicit] The authors explicitly list checking "other benchmark datasets" as a future direction in the conclusion.
- Why unresolved: The experiments were confined to a specific crack image dataset, leaving the generalizability of the findings unknown.
- What evidence would resolve it: Applying the same DCNN architecture and tuning parameters to datasets like CIFAR-10 or ImageNet and comparing performance rankings.

### Open Question 3
- Question: Does the superiority of the tanh activation function persist when applied to deeper neural network architectures?
- Basis in paper: [inferred] The study uses a relatively shallow network (2 convolutional layers). The authors note that tanh might lead to "fading gradient problems" in deep networks, suggesting the results may vary with depth.
- Why unresolved: The "hardware friendly" benefits of tanh were observed in a shallow model, but its performance relative to ReLU in deeper configurations remains untested in this context.
- What evidence would resolve it: A comparative analysis of tanh versus ReLU in deeper models (e.g., ResNet or VGG) using the same crack dataset to observe gradient flow and convergence.

## Limitations

- Critical implementation details missing: filter counts, dropout rate, exact train/validation split, batch size, epochs
- Dataset preprocessing pipeline unspecified despite citing source
- Custom shallow DCNN architecture limits comparison with established models
- Lack of statistical validation for performance differences between configurations

## Confidence

- **High Confidence**: Max pooling outperforming average pooling (directly observed in their experiments)
- **Medium Confidence**: Adam optimizer superiority (supported by mechanism but limited empirical comparison)
- **Low Confidence**: Tanh achieving 100% training accuracy and its generalization benefits (lacks statistical validation and could indicate overfitting)

## Next Checks

1. **Architecture Replication**: Build the exact architecture (2 Conv, 2 MaxPool, 1 Dense) with Tanh and Adam to verify the 95.42% validation accuracy claim and confirm the performance gap between pooling methods.

2. **Optimizer Comparison**: Run SGD, AdaDelta, RMSProp, and Adam optimizers with identical Tanh activation and max pooling to quantify the claimed 17%+ performance difference.

3. **Generalization Analysis**: Compare training vs. validation accuracy across all activation functions to assess overfitting, particularly for Tanh's 100% training performance versus its validation score.