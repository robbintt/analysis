---
ver: rpa2
title: 'mFARM: Towards Multi-Faceted Fairness Assessment based on HARMs in Clinical
  Decision Support'
arxiv_id: '2509.02007'
source_url: https://arxiv.org/abs/2509.02007
tags:
- fairness
- base
- group
- score
- test
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper proposes mFARM, a multi-metric framework for assessing\
  \ fairness in clinical language models, addressing limitations of traditional fairness\
  \ measures. It constructs two large-scale benchmarks (ED-Triage and Opioid Analgesic\
  \ Recommendation) from MIMIC-IV with over 50,000 prompts, including twelve race\
  \ \xD7 gender variants and three context tiers."
---

# mFARM: Towards Multi-Faceted Fairness Assessment based on HARMs in Clinical Decision Support

## Quick Facts
- arXiv ID: 2509.02007
- Source URL: https://arxiv.org/abs/2509.02007
- Authors: Shreyash Adappanavar; Krithi Shailya; Gokul S Krishnan; Sriraam Natarajan; Balaraman Ravindran
- Reference count: 19
- Key outcome: Multi-metric framework (mFARM) with five statistical tests detects subtle biases across demographic groups in clinical LLMs, showing quantization may improve fairness while context reduction severely degrades it

## Executive Summary
This paper introduces mFARM, a multi-faceted framework for assessing fairness in clinical language models by evaluating five complementary statistical metrics. The framework constructs controlled benchmarks from MIMIC-IV with over 50,000 prompts across 12 demographic variants and three context levels. Experiments on four open-source LLMs reveal that while quantization maintains or improves fairness, reduced clinical context significantly amplifies biases. The approach captures allocational, stability, and latent harms that single-metric evaluations miss, providing a more comprehensive audit of clinical AI systems.

## Method Summary
mFARM evaluates clinical LLM fairness through controlled demographic perturbation and multi-metric statistical analysis. The method generates 13 prompt variants per case (1 neutral BASE + 12 race×gender combinations) from MIMIC-IV-derived tasks, then computes five fairness metrics (Mean Difference, Absolute Deviation, Variance Heterogeneity, KS Distance, Correlation Difference) using omnibus tests followed by post-hoc analysis. These metrics capture different harm types and are aggregated via geometric mean into an mFARM score, which is then balanced with accuracy via harmonic mean (FAB score). The framework is applied to binary classification tasks across multiple quantization levels and context tiers.

## Key Results
- mFARM effectively captures subtle biases missed by single-metric evaluations, with low pairwise correlations confirming distinct harm dimensions
- Context reduction causes sharp fairness degradation, particularly in KS Distributional and Variance Heterogeneity metrics
- Quantization maintains or improves fairness scores while preserving accuracy, suggesting implicit regularization effects
- Models show consistent bias patterns across tasks, with specific models excelling or failing in different fairness dimensions

## Why This Works (Mechanism)

### Mechanism 1: Multi-dimensional harm detection prevents fairness "shadowing"
Single-metric fairness evaluation fails to capture allocational, stability, and latent harms that can coexist independently. Five orthogonal statistical tests (Friedman, Levene's, Wilcoxon, KS, Spearman correlation) each target a distinct failure mode; geometric mean aggregation prevents any single low score from being compensated by others. Core assumption: Harm types are non-compensatory—a model cannot be "fair enough" by excelling in one dimension while failing another. Evidence anchors: Abstract states evaluation of five complementary metrics to capture different harm types; Table 4 shows hypothetical Model X with perfect Statistical Parity but severe Variance Heterogeneity; pairwise correlations among sub-metrics confirm distinct dimensions. Break condition: If harm dimensions are empirically correlated rather than orthogonal, geometric mean over-penalizes redundant information.

### Mechanism 2: Controlled demographic perturbation isolates causal bias
Holding clinical facts constant while varying demographic markers isolates the influence of social cues on model predictions. 13 prompt variants per case (1 BASE + 12 race×gender combinations) with identical clinical narrative; any prediction shift is attributable to demographic context. Core assumption: BASE prompts without demographic markers represent a neutral reference point. Evidence anchors: Paper states isolating causal influence of social cues by holding clinical facts constant; each case yields 13 variations with identical clinical content; Figure 5 shows prediction flip from "Yes" to "No" solely due to demographic change; neighbor paper uses similar demographic perturbation for ICU mortality. Break condition: If demographic-neutral BASE prompts are not truly neutral (e.g., implicit biases in clinical language itself), the reference point is contaminated.

### Mechanism 3: Context scarcity amplifies latent harms
Reducing clinical context causes disproportionate fairness degradation, particularly in KS Distributional and Variance Heterogeneity metrics. With less clinical grounding, models rely more heavily on demographic heuristics; latent biases become manifest in prediction instability and distributional shifts. Core assumption: High-context prompts approximate "ideal" clinical conditions; low-context represents resource-constrained reality. Evidence anchors: Table 7 shows mFARM scores drop sharply in "Low" context (e.g., Qwen ED: 0.690→0.000); Table 9 shows BioLlama Variance Heterogeneity falls from 1.0 to 0.358; Qwen KS Distance from 1.0 to 0.281 under low context; paper states deprived of context, models' predictions become unstable and confidence distributions diverge. Break condition: If the relationship between context and fairness is task-specific rather than generalizable, deployment guidance would require domain-specific recalibration.

## Foundational Learning

- **Concept: Allocational vs. Stability vs. Latent Harms**
  - Why needed here: The framework's core contribution is distinguishing three harm types that require different statistical detection methods; conflating them leads to incomplete audits.
  - Quick check question: Can a model be allocational-fair (equal mean predictions) but stability-unfair (unequal variance across groups)?

- **Concept: Omnibus test → Post-hoc analysis → Effect size pipeline**
  - Why needed here: Each mFARM metric follows this three-stage procedure; understanding the statistical logic is prerequisite to interpreting scores correctly.
  - Quick check question: If the Friedman omnibus test returns p>0.05, what fairness score is assigned? (Answer: 1.0, no post-hoc needed)

- **Concept: Fairness-Accuracy Balance (FAB) as harmonic mean**
  - Why needed here: The paper argues high fairness with low accuracy = "clinically inert" models; harmonic mean enforces simultaneous achievement.
  - Quick check question: If accuracy=0.9 and mFARM=0.5, what is FAB score? (Answer: ~0.64, not 0.7)

## Architecture Onboarding

- **Component map**: MIMIC-IV extraction -> Demographic augmentation (12 variants) -> Context tiering (High/Medium/Low) -> Model inference -> 5 parallel metric computations -> mFARM aggregation -> FAB scoring

- **Critical path**: Prompt generation with demographic variants → model inference (probability extraction) → 5 parallel metric computations → mFARM aggregation → FAB scoring

- **Design tradeoffs**:
  - Geometric mean penalizes any single failure vs. arithmetic mean would allow compensation
  - BASE group as reference vs. peer-group comparisons (both included to balance)
  - 12 race×gender groups vs. computational cost (50K+ prompts per evaluation)

- **Failure signatures**:
  - mFARM=0 in low-context: Model outputs collapse to single class or become highly unstable
  - Perfect Variance Heterogeneity but low Mean Difference: Allocational bias without instability
  - High Correlation Difference (>0.5): Bias intensifies with model confidence (dangerous in high-stakes cases)

- **First 3 experiments**:
  1. **Baseline audit**: Run mFARM on base LLM (16-bit, high-context) for both tasks; identify which sub-metrics fail most
  2. **Context sensitivity test**: Repeat with Medium and Low context; quantify degradation curve per model
  3. **Quantization robustness check**: Compare 16-bit vs. 8-bit vs. 4-bit; verify paper's claim that quantization does not harm (and may improve) fairness

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the $mFARM$ framework be effectively adapted to evaluate fairness in generative free-text clinical responses rather than binary classification probabilities?
- Basis in paper: [explicit] The conclusion explicitly states, "We aim to extend the framework to support free-text outputs in the future."
- Why unresolved: The current metrics rely heavily on probability distributions and statistical comparisons (e.g., KS distance, variance) of logit outputs, which do not directly translate to unstructured text generation.
- What evidence would resolve it: A reformulation of the five sub-metrics using semantic similarity embeddings or lexical overlap metrics applied to open-ended model responses, validated on generative clinical tasks.

### Open Question 2
- Question: Is it feasible to use a modified $mFARM$ score as a differentiable loss function to directly optimize for fairness during model training?
- Basis in paper: [explicit] The conclusion proposes to "use a modified $mFARM$ as a loss approximation function to directly optimize fairness during training."
- Why unresolved: The current $mFARM$ score aggregates non-differentiable statistical tests (e.g., Kolmogorov–Smirnov, Levene’s test), making it incompatible with standard gradient descent optimization.
- What evidence would resolve it: The derivation of a differentiable surrogate loss function that correlates strongly with the discrete $mFARM$ score, demonstrated through improved fairness metrics in a trained model without significant loss in accuracy.

### Open Question 3
- Question: Does quantization act as a reliable, generalizable implicit regularizer for social bias, or is the observed fairness improvement model-specific?
- Basis in paper: [inferred] In the "Results and Discussion" (RQ4), the authors hypothesize that perturbations from quantization improve fairness by disrupting stereotyping patterns, but they note this requires "appropriate consideration of diverse datasets and model finetuning."
- Why unresolved: The experiments are limited to four specific 7B-8B parameter models; it is unclear if this fairness gain holds for larger models or different architectures where quantization might remove critical features rather than just noise.
- What evidence would resolve it: A comprehensive ablation study across varying model scales (e.g., 13B, 70B) and architectures showing consistent inverse correlations between bit-width and unfairness scores while maintaining clinical utility.

## Limitations
- Limited demographic representation: The framework assumes binary gender and four racial categories, potentially missing intersectional nuances in bias patterns
- Context-tier validity: Artificial creation of Low/Medium/High context variants through prompt truncation may not reflect real-world clinical scenarios
- Task generalizability: Results based on two specific clinical tasks may not extend to other medical domains or non-binary classification problems

## Confidence
- **High confidence**: The multi-metric framework design and statistical methodology are sound and well-specified
- **Medium confidence**: The specific mFARM scores and FAB results depend on implementation details and baseline model behaviors that require verification
- **Low confidence**: Claims about quantization improving fairness require further validation across different quantization schemes and models

## Next Checks
1. **Replication audit**: Run mFARM on at least one base model using the public code to verify the fairness score calculation pipeline matches the paper's reported values
2. **Cross-task generalization**: Apply mFARM to a third clinical task (e.g., sepsis prediction) to test whether context-dependency patterns hold across domains
3. **Demographic granularity test**: Modify the framework to include additional demographic categories (e.g., Hispanic/Latino, non-binary gender) and assess impact on bias detection capability