---
ver: rpa2
title: 'Op-Fed: Opinion, Stance, and Monetary Policy Annotations on FOMC Transcripts
  Using Active Learning'
arxiv_id: '2509.13539'
source_url: https://arxiv.org/abs/2509.13539
tags:
- dataset
- policy
- fomc
- stance
- context
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces OP-FED, a human-annotated dataset of 1044
  sentences from FOMC transcripts, addressing challenges of imbalanced classes and
  inter-sentence dependence in stance detection. A five-stage hierarchical schema
  isolates opinion, monetary policy, and stance, while active learning roughly doubles
  positive instance sampling.
---

# Op-Fed: Opinion, Stance, and Monetary Policy Annotations on FOMC Transcripts Using Active Learning

## Quick Facts
- **arXiv ID:** 2509.13539
- **Source URL:** https://arxiv.org/abs/2509.13539
- **Authors:** Alisa Kanganis; Katherine A. Keith
- **Reference count:** 40
- **Primary result:** Introduces OP-FED, a human-annotated dataset of 1044 sentences from FOMC transcripts, using a hierarchical schema and active learning to address class imbalance and context dependence

## Executive Summary
This work introduces OP-FED, a human-annotated dataset of 1044 sentences from FOMC transcripts, addressing challenges of imbalanced classes and inter-sentence dependence in stance detection. A five-stage hierarchical schema isolates opinion, monetary policy, and stance, while active learning roughly doubles positive instance sampling. Zero-shot LLM accuracy on stance classification is 0.61, below the human baseline of 0.89, highlighting the need for supervised fine-tuning. The dataset enables future model training, confidence calibration, and downstream economic analyses of FOMC decision-making dynamics.

## Method Summary
The authors developed a five-stage hierarchical annotation schema to label FOMC transcripts for opinion, monetary policy references, and stance toward tightening/easing. They applied active learning (AL) with ModernBERT and Maximum Entropy acquisition to counter extreme class imbalance (<8% positive stances). The AL loop iteratively queried uncertain instances, roughly doubling positive instance sampling compared to random selection. Context dependence was addressed by recording when preceding sentences were needed for resolution. Three human annotators performed blind labeling with majority vote aggregation. Zero-shot LLM benchmarks (GPT-5, Claude) were evaluated against the gold standard.

## Key Results
- Zero-shot LLM stance classification accuracy: 0.61 vs human baseline 0.89
- Active learning roughly doubles positive instance sampling across all schema aspects
- 65% of instances require context beyond sentence-level for proper interpretation
- Fleiss' Kappa agreement drops from 0.65 to 0.41 across AL batches, indicating increasing difficulty

## Why This Works (Mechanism)

### Mechanism 1
Hierarchical annotation schemas isolate complex semantic attributes (opinion, policy, stance) more effectively than monolithic multi-label classification, particularly under high inter-sentence dependence. The schema forces a sequential decision process (Stage 1: Opinion? → Stage 2: Monetary Policy? → Stage 4: Stance via NLI). By filtering non-opinions and non-policy sentences early, the system reduces the cognitive load on annotators and the hypothesis space for the final, difficult stance classification. Core assumption: Annotators can consistently distinguish "subjective perspective" (opinion) from factual statements, and stance can be reliably mapped to a single hypothesis ("We should tighten monetary policy") using NLI logic.

### Mechanism 2
Active learning (AL) with Maximum Entropy acquisition functions counters extreme class imbalance (skew <8%) by preferentially selecting uncertain instances, which are statistically more likely to be the rare positive class. The AL loop trains a model (ModernBERT) on a small seed, then queries the unlabeled pool for sentences where the model's predictive distribution is closest to uniform (maximum uncertainty). In imbalanced datasets, the decision boundary is often poorly defined; sampling uncertainty focuses labeling budget on these boundary cases rather than the overwhelming majority class. Core assumption: Model uncertainty correlates with informativeness and the presence of minority class instances (positive stances), rather than just noisy or ambiguous data.

### Mechanism 3
Stance detection in FOMC transcripts requires context injection (preceding sentences/tokens) because the linguistic referents for "tightening" or "easing" are often external to the target sentence. The architecture incorporates a "Context" stage (Stages 3 & 5). The annotation schema explicitly records if the target sentence requires the previous 5 sentences or 200 tokens to resolve co-reference (e.g., "your proposal" → "25 basis point increase"). This creates a dataset that penalizes context-free models. Core assumption: The necessary context is captured within a window of 5 sentences or 200 tokens.

## Foundational Learning

- **Concept:** Pool-based Active Learning
  - **Why needed here:** The paper relies on this to solve the "needle in a haystack" problem (finding <8% positive stances). Without understanding AL, one might question why simple random sampling wasn't sufficient.
  - **Quick check question:** Why does the paper use "Maximum Entropy" instead of random sampling to select the next batch of sentences?

- **Concept:** Natural Language Inference (NLI) / Textual Entailment
  - **Why needed here:** The paper operationalizes "stance" not as a sentiment score, but as an NLI relationship (Entail/Contradict) to the hypothesis *"We should tighten monetary policy."*
  - **Quick check question:** In this schema, would a sentence advocating for lowering interest rates be labeled as "Entailment" or "Contradiction"?

- **Concept:** Class Imbalance & Calibration
  - **Why needed here:** With <8% positive class, standard accuracy metrics are misleading (a dummy classifier predicting "Neutral" achieves >90% accuracy). The paper analyzes Expected Calibration Error (ECE) to check if model confidence matches reality.
  - **Quick check question:** Why does the paper report that zero-shot LLMs achieve ~0.60 accuracy on stance but still conclude they fail at the task?

## Architecture Onboarding

- **Component map:** ConvoKit FOMC Corpus (280k sentences) -> spaCy sentence segmentation -> AL Loop (ModernBERT + Maximum Entropy) -> 3 Human Annotators (blind, majority vote) -> Structured prompts for Zero-shot LLMs
- **Critical path:** The interaction between the **Acquisition Function** and the **Human Oracle**. If the annotators disagree (low Fleiss' Kappa) on the "uncertain" samples selected by the model, the training signal degrades.
- **Design tradeoffs:**
  - **ModernBERT vs. Decoders (LLMs):** The authors chose ModernBERT for the AL loop because encoder models efficiently output class probabilities (needed for entropy calculation), whereas extracting calibrated probabilities from generative LLMs is difficult.
  - **Hierarchical vs. Flat:** They trade annotation speed (5 stages per item) for label purity and reduced cognitive load.
- **Failure signatures:**
  - **Agreement Decay:** Table 7 shows annotator agreement (Kappa) drops from 0.65 to 0.41 over AL batches. This suggests the model successfully finds "hard" instances, but these are also harder for humans to label.
  - **Context Blindness in LLMs:** Table 3 shows LLMs perform poorly on "Context needed?" tasks (acc < 0.30), suggesting they cannot reliably self-diagnose when they lack sufficient context.
- **First 3 experiments:**
  1. **Re-run AL Simulation:** Reproduce Figure 2 using the VAST+AN dataset to validate that "Maximum Entropy" outperforms "Random" sampling for this specific data distribution.
  2. **Context Ablation:** Train a classifier on OP-FED using *only* the target sentence vs. target + context features. Quantify the performance gap to justify the complexity of the context-retrieval pipeline.
  3. **Error Analysis on "Ambiguous" Labels:** Filter the dataset for instances where annotators flagged "Ambiguous." Evaluate if LLMs struggle more on these items (high ECE) to determine if they can be used as a "confident predictor" filter.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can supervised fine-tuning of LLMs on OP-FED improve stance classification accuracy to approach or exceed the human baseline of 0.89?
- **Basis in paper:** [explicit] The authors state zero-shot accuracy is only 0.61 for stance, "highlighting the need for supervised fine-tuning." The conclusion explicitly calls for "future work [that] could likely improve LLM accuracy and confidence score calibration by shifting to supervised fine-tuning and/or other iterative inference techniques."
- **Why unresolved:** The paper only benchmarks zero-shot performance; no fine-tuning experiments were conducted due to dataset size constraints.
- **What evidence would resolve it:** Fine-tuning experiments on OP-FED showing improved accuracy on stance classification relative to zero-shot baselines.

### Open Question 2
- **Question:** Would multi-task learning of all five schema stages within the active learning loop outperform the single-task (opinion-only) approach used in this work?
- **Basis in paper:** [explicit] The authors state: "We leave to future work using multi-task learning of all stages within the AL loop; see Rotman and Reichart (2022)."
- **Why unresolved:** The current AL pipeline only models Stage 1 (opinion) to avoid inter-sentence dependence challenges and simplify the setup.
- **What evidence would resolve it:** Comparative experiments showing whether jointly training on all hierarchical stages improves positive instance acquisition rates or downstream classification performance.

### Open Question 3
- **Question:** Can domain experts (e.g., economists or Fed specialists) annotate FOMC stance with higher inter-annotator agreement than the trained undergraduate annotators used here?
- **Basis in paper:** [inferred] From Limitations: "we also hypothesize that domain-experts—who have greater expertise with the Federal Reserve and monetary policy—may annotate instances with higher quality."
- **Why unresolved:** The study used three undergraduate economics majors; no domain-expert comparison was conducted.
- **What evidence would resolve it:** A controlled annotation study comparing agreement scores (Krippendorff's alpha, Fleiss' kappa) between domain experts and the current annotator pool.

## Limitations
- **Inter-annotator agreement decay during AL** presents a significant reliability concern, with Fleiss' Kappa dropping from 0.65 to 0.41 across batches
- **Context window adequacy** remains unvalidated, with risk that the 5-sentence/200-token assumption may miss crucial antecedents or introduce irrelevant noise
- **LLM evaluation methodology** may overstate performance gaps by not systematically providing context when needed

## Confidence
- **High Confidence:** The hierarchical annotation schema's effectiveness in reducing annotator burden and improving label quality (Sections 3.2, Table 7). The empirical evidence of improved positive instance sampling through AL (Figure 2, Table 2) is well-supported.
- **Medium Confidence:** The claim that active learning roughly doubles positive instance sampling. While the statistics support this, the mechanism depends heavily on annotator agreement stability, which deteriorates during the AL process.
- **Low Confidence:** The generalizability of context window assumptions and the complete interpretation of LLM performance gaps without accounting for context provision in evaluation.

## Next Checks
1. **Agreement Stability Analysis:** Re-run the AL loop with agreement thresholds enforced—if annotators' Kappa falls below 0.5 on a batch, either halt labeling or investigate whether these represent genuinely ambiguous instances versus labeling errors. This validates whether the "hard" instances identified by AL are actually learnable.

2. **Context Window Ablation Study:** Systematically vary the context window (0, 3, 5, 10 sentences; 100, 200, 500 tokens) and measure stance detection performance. This quantifies the actual contribution of context and validates the 5-sentence/200-token assumption.

3. **LLM Context Provision Test:** Re-evaluate zero-shot LLM performance with systematic context inclusion based on the "Context needed?" annotations. Compare performance with and without context to isolate whether LLM failures stem from inherent limitations versus missing information.