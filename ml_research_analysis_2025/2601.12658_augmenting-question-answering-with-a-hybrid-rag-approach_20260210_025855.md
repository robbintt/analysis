---
ver: rpa2
title: Augmenting Question Answering with A Hybrid RAG Approach
arxiv_id: '2601.12658'
source_url: https://arxiv.org/abs/2601.12658
tags:
- retrieval
- query
- ssrag
- graph
- factual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the problem of retrieving contextually relevant
  information in retrieval-augmented generation (RAG) systems, which often struggle
  to produce complete or accurate answers in question-answering tasks. To solve this,
  the authors propose Structured-Semantic RAG (SSRAG), a hybrid architecture that
  combines vector-based and graph-based retrieval methods, along with query augmentation
  and agentic query routing.
---

# Augmenting Question Answering with A Hybrid RAG Approach

## Quick Facts
- **arXiv ID:** 2601.12658
- **Source URL:** https://arxiv.org/abs/2601.12658
- **Authors:** Tianyi Yang; Nashrah Haque; Vaishnave Jonnalagadda; Yuya Jeremy Ong; Zhehui Chen; Yanzhao Wu; Lei Yu; Divyesh Jadav; Wenqi Wei
- **Reference count:** 40
- **Primary result:** SSRAG achieves 87% accuracy on TruthfulQA for GPT-4 with reduced hallucinations (SelfCheckGPT score 0.18)

## Executive Summary
The paper addresses fundamental limitations in retrieval-augmented generation systems for question answering, particularly their tendency to produce incomplete or inaccurate responses due to inadequate context retrieval. To overcome these challenges, the authors propose Structured-Semantic RAG (SSRAG), a hybrid architecture that integrates vector-based and graph-based retrieval methods with query augmentation and agentic query routing. This approach combines structured knowledge from knowledge graphs with semantic understanding from vector representations to create a unified context representation that improves answer quality and reduces hallucinations.

The SSRAG framework was evaluated across three standard QA datasets (TruthfulQA, SQuAD, WikiQA) using five different large language models, demonstrating consistent improvements over traditional RAG implementations. The results show significant gains in factual accuracy, reduced hallucination rates, and enhanced context precision, establishing SSRAG as a promising approach for improving the trustworthiness and reliability of AI-generated responses in question-answering applications.

## Method Summary
SSRAG combines vector-based and graph-based retrieval methods with query augmentation and agentic query routing to create a hybrid retrieval system. The architecture refines the retrieval process by integrating structured knowledge from knowledge graphs with semantic understanding from vector embeddings into a unified context representation. This hybrid approach leverages the complementary strengths of both retrieval methods: vectors capture semantic similarity while graphs provide structured, relationship-based knowledge. The system uses agentic query routing to determine the most appropriate retrieval method for each query, optimizing the balance between semantic and structured information retrieval to improve answer accuracy and reduce hallucinations.

## Key Results
- Achieved 87% accuracy on TruthfulQA benchmark when using GPT-4, significantly outperforming standard RAG implementations
- Reduced hallucination rates as measured by SelfCheckGPT scoring (0.18), indicating improved factual reliability
- Demonstrated consistent performance improvements across three QA datasets (TruthfulQA, SQuAD, WikiQA) and five different LLMs

## Why This Works (Mechanism)
SSRAG works by addressing the fundamental limitation of traditional RAG systems that rely solely on vector similarity for retrieval. By combining vector-based semantic retrieval with graph-based structured retrieval, the system captures both the meaning and the relationships within the knowledge base. The agentic query routing component intelligently selects between these retrieval methods based on query characteristics, optimizing the retrieval strategy for each specific question. Query augmentation further enhances retrieval quality by reformulating queries to better match relevant information in both vector and graph databases. This multi-modal approach ensures more comprehensive context gathering, leading to more accurate and complete answers while reducing the likelihood of hallucinations that occur when LLMs generate information not supported by retrieved context.

## Foundational Learning

**Vector-based retrieval** - Uses dense embeddings to capture semantic similarity between queries and documents. Needed because semantic meaning often transcends exact keyword matching. Quick check: Verify embeddings capture synonym relationships and contextual meaning.

**Graph-based retrieval** - Leverages structured knowledge with explicit relationships between entities. Needed because vectors alone cannot capture complex relational knowledge or hierarchical structures. Quick check: Confirm graph queries return expected relationship paths.

**Query augmentation** - Reformulates or expands queries to improve retrieval coverage. Needed because original queries may be too narrow or miss relevant context. Quick check: Compare retrieval results before and after augmentation.

**Agentic query routing** - Dynamically selects the optimal retrieval method based on query characteristics. Needed because different query types benefit from different retrieval strategies. Quick check: Validate routing decisions improve retrieval quality over static approaches.

**Hybrid context representation** - Combines structured and semantic information into unified context. Needed because neither vectors nor graphs alone provide complete context for complex QA tasks. Quick check: Ensure both semantic and relational information is preserved in final context.

## Architecture Onboarding

**Component map:** Query -> Query Augmentation -> Agentic Routing -> [Vector Retrieval | Graph Retrieval] -> Context Fusion -> LLM

**Critical path:** The retrieval and context fusion pipeline is the critical path, as all downstream LLM performance depends on the quality of retrieved context. The agentic routing decision and subsequent retrieval method selection directly impact answer accuracy.

**Design tradeoffs:** The hybrid approach increases computational overhead and complexity compared to single-method retrieval, but provides significant accuracy gains. The tradeoff between retrieval quality and system latency must be carefully managed, particularly for real-time applications.

**Failure signatures:** Poor retrieval quality when queries contain ambiguous terminology, graph database incompleteness for domain-specific knowledge, routing failures when distinguishing between semantic and structured queries, and context fusion errors when combining incompatible information sources.

**First experiments to run:**
1. Validate individual retrieval methods (vector and graph) perform as expected on controlled test cases before integration
2. Test agentic routing accuracy by comparing its method selections against human judgments on diverse query types
3. Evaluate context fusion quality by examining whether combined contexts provide more comprehensive information than either method alone

## Open Questions the Paper Calls Out

None

## Limitations

- Evaluation limited to standard benchmark datasets without testing on domain-specific or multilingual scenarios where RAG performance often degrades
- Computational overhead and latency implications of hybrid retrieval approach not addressed, raising concerns about real-world scalability
- Claims about hallucination reduction lack transparency regarding validation methodology and potential biases in hallucination detection

## Confidence

- **High confidence:** The core problem of RAG limitations is well-established in literature; combining vector and graph retrieval methods is technically plausible and aligns with current research trends
- **Medium confidence:** Reported performance metrics (87% accuracy on TruthfulQA, SelfCheckGPT score 0.18) appear specific but cannot be independently verified without full methodology
- **Low confidence:** Claims about SSRAG being a "scalable and reliable framework" lack supporting evidence regarding computational efficiency and real-world deployment scenarios

## Next Checks

1. Replicate the evaluation using the same three datasets (TruthfulQA, SQuAD, WikiQA) with multiple LLMs to verify claimed performance improvements, focusing on consistency across different model sizes and architectures

2. Conduct ablation studies to isolate contributions of each component (vector retrieval, graph retrieval, query augmentation, agentic routing) and determine whether hybrid approach provides synergistic benefits or if simpler combinations would suffice

3. Test SSRAG's robustness on out-of-distribution questions and domain-specific knowledge bases to evaluate generalization beyond benchmark datasets and identify potential failure modes in practical applications