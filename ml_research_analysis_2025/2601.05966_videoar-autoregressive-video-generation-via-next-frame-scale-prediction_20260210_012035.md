---
ver: rpa2
title: 'VideoAR: Autoregressive Video Generation via Next-Frame & Scale Prediction'
arxiv_id: '2601.05966'
source_url: https://arxiv.org/abs/2601.05966
tags:
- video
- generation
- temporal
- autoregressive
- frame
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces VideoAR, the first large-scale autoregressive
  framework for video generation that integrates multi-scale next-frame prediction
  with visual autoregressive (VAR) modeling. It addresses the challenges of spatial-temporal
  misalignment and error propagation in autoregressive video generation by disentangling
  spatial and temporal dependencies through intra-frame VAR modeling combined with
  causal next-frame prediction.
---

# VideoAR: Autoregressive Video Generation via Next-Frame & Scale Prediction

## Quick Facts
- arXiv ID: 2601.05966
- Source URL: https://arxiv.org/abs/2601.05966
- Authors: Longbin Ji; Xiaoxiong Liu; Junyuan Shang; Shuohuan Wang; Yu Sun; Hua Wu; Haifeng Wang
- Reference count: 38
- Primary result: First large-scale autoregressive framework achieving competitive performance with diffusion models on video generation tasks

## Executive Summary
VideoAR introduces a novel autoregressive video generation framework that addresses the fundamental challenge of spatial-temporal misalignment in sequential video generation. By integrating multi-scale next-frame prediction with visual autoregressive modeling, the framework achieves state-of-the-art performance among autoregressive approaches while significantly reducing inference steps. The model demonstrates competitive quality with diffusion-based methods that are an order of magnitude larger, achieving a VBench score of 81.74 and reducing FVD on UCF-101 from 99.5 to 88.6. This work establishes a new baseline for autoregressive video generation and narrows the performance gap with diffusion paradigms.

## Method Summary
VideoAR employs a multi-scale tokenizer to encode spatio-temporal dynamics, followed by a visual autoregressive model that predicts future frames through a combination of intra-frame modeling and causal next-frame prediction. The architecture introduces three key innovations: Multi-scale Temporal RoPE for handling long-term temporal dependencies, Cross-Frame Error Correction to mitigate error propagation, and Random Frame Mask for robust training. The model is trained through a progressive multi-stage pipeline that gradually increases resolution and duration while aligning spatial and temporal learning. This approach enables the model to capture both fine-grained spatial details and long-range temporal consistency across video sequences.

## Key Results
- Achieves FVD improvement on UCF-101 from 99.5 to 88.6, outperforming prior autoregressive methods
- Reduces inference steps by over 10× compared to conventional autoregressive approaches
- Reaches VBench score of 81.74, competitive with diffusion models that are 10× larger in parameter count
- Demonstrates superior temporal consistency compared to previous autoregressive video generation methods

## Why This Works (Mechanism)
The success of VideoAR stems from its disentangled approach to spatial and temporal dependencies. By separating intra-frame autoregressive modeling from causal next-frame prediction, the framework can capture fine spatial details while maintaining temporal coherence. The multi-scale architecture allows the model to process information at different resolutions, with higher scales handling broader temporal contexts and lower scales preserving fine-grained details. The Cross-Frame Error Correction mechanism actively corrects prediction errors by referencing past frames, preventing the accumulation of artifacts that typically plague autoregressive generation. The progressive training pipeline ensures that the model learns spatial and temporal relationships in a coordinated manner, avoiding the misalignment that occurs when these aspects are learned independently.

## Foundational Learning
- **Visual Autoregressive Modeling**: Predicting future frames from past context in a sequential manner; needed to maintain temporal coherence across generated sequences; quick check: ability to predict next frame given previous frames
- **Multi-scale Tokenization**: Encoding video frames at multiple resolutions simultaneously; needed to balance fine spatial details with long-range temporal dependencies; quick check: consistent representation across different scales
- **Temporal RoPE (Rotary Position Embedding)**: Encoding positional information in the temporal dimension; needed to maintain long-range temporal relationships; quick check: consistent temporal relationships across extended sequences
- **Error Correction Mechanisms**: Active correction of prediction errors using reference frames; needed to prevent error accumulation in autoregressive generation; quick check: reduced drift in generated sequences
- **Progressive Training**: Gradual increase in training complexity from low to high resolution; needed to align spatial and temporal learning effectively; quick check: smooth progression in training loss and quality metrics

## Architecture Onboarding

**Component Map:**
Video Frames -> 3D Multi-scale Tokenizer -> Visual Autoregressive Model -> Next-Frame Predictor -> Generated Frames
                              |-> Multi-scale Temporal RoPE -> Cross-Frame Error Correction -> Random Frame Mask

**Critical Path:**
Multi-scale Tokenizer -> Visual Autoregressive Model -> Next-Frame Predictor -> Cross-Frame Error Correction -> Generated Output

**Design Tradeoffs:**
The model trades computational efficiency for quality by using a full VAR attention mask, which limits resolution but ensures temporal coherence. The 16× spatial compression balances detail preservation with computational tractability, though it may sacrifice some fine-grained motion details. The multi-stage training adds complexity but enables better alignment of spatial and temporal learning compared to single-stage approaches.

**Failure Signatures:**
- Temporal drift in high-dynamic scenes where error correction fails to compensate
- Loss of fine-grained motion details in compressed representations
- Degradation in temporal consistency when extending beyond training sequence lengths
- Quality reduction at higher resolutions due to computational constraints

**3 First Experiments:**
1. Ablation study removing Cross-Frame Error Correction to quantify its impact on temporal consistency
2. Comparison of generation quality at different compression ratios (8× vs 16×) to assess detail preservation tradeoffs
3. Evaluation of temporal coherence across varying sequence lengths to identify performance degradation points

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can sparser attention mechanisms (e.g., sliding window, sparse attention patterns) effectively replace full autoregressive attention in VideoAR to enable high-resolution (720P+), high-framerate (24+ FPS) video generation without sacrificing temporal coherence?
- Basis in paper: [explicit] "In future work, we will extend the training sequence length and explore sparser attention mechanisms to enable high-resolution, fluid video generation."
- Why unresolved: The current full VAR attention mask creates high computational overhead that limits practical resolution/FPS; the authors have not yet explored attention sparsification.
- What evidence would resolve it: Ablation studies comparing generation quality (FVD, VBench) and inference speed across different sparse attention patterns at higher resolutions and frame rates.

### Open Question 2
- Question: Can iterative inference-time rollouts combined with reinforcement learning (RL) significantly reduce motion drift in high-dynamic scenes (e.g., complex human movements) while maintaining inference efficiency?
- Basis in paper: [explicit] "Future research will enhance the model's performance by integrating iterative inference-time rollouts and reinforcement learning algorithms" to address drifting issues.
- Why unresolved: Error propagation in autoregressive generation remains an open challenge; the proposed Cross-Frame Error Correction mitigates but does not eliminate drift in complex motion scenarios.
- What evidence would resolve it: Comparative experiments on high-dynamic video benchmarks showing drift metrics (object trajectory consistency, motion smoothness) before and after RL-based fine-tuning or iterative refinement.

### Open Question 3
- Question: How does the aggressive 16× spatial compression in VideoAR's tokenizer trade off against fine-grained temporal detail capture compared to 8× compression methods?
- Basis in paper: [inferred] VideoAR-L achieves comparable rFVD (61) to MAGVIT (58) despite 4× fewer tokens, but the paper does not analyze whether fine-grained motion details (e.g., subtle facial expressions, rapid object motion) are preserved equally well.
- Why unresolved: The paper focuses on overall reconstruction fidelity (rFVD) but does not provide per-category or motion-complexity breakdowns that would reveal compression-induced information loss.
- What evidence would resolve it: Fine-grained evaluation on motion-sensitive benchmarks (e.g., fine-grained action recognition datasets, temporal detail preservation metrics) comparing VideoAR's 16× vs. an 8× compression variant.

### Open Question 4
- Question: Would initializing VideoAR from a large-scale pre-trained image/video foundation model (similar to InfinityStar's approach) yield better scaling efficiency or generation quality compared to training from scratch?
- Basis in paper: [explicit] "VideoAR is trained from scratch using joint low-resolution image–video data, which focuses on learning unified spatio-temporal representations from the ground up" in contrast to InfinityStar which "is fine-tuned from a well-established 8B-scale image generation foundation model."
- Why unresolved: The paper does not compare training-from-scratch vs. pre-trained initialization under controlled conditions; it remains unclear which approach offers better compute-quality tradeoffs.
- What evidence would resolve it: Controlled experiments training VideoAR variants with and without pre-trained initialization, matching compute budgets and comparing convergence speed, final performance (VBench, FVD), and data efficiency.

## Limitations
- Computational complexity of full VAR attention limits practical resolution and frame rate for real-world applications
- Temporal consistency improvements measured primarily through aggregate metrics rather than qualitative long-range coherence assessment
- Multi-stage training pipeline adds significant complexity that may limit reproducibility and deployment
- Evaluation primarily focused on controlled public datasets, with uncertain generalization to diverse video domains

## Confidence

**High confidence** in quantitative improvements on established benchmarks (FVD, VBench scores)
**Medium confidence** in claims about efficiency gains and practical scalability
**Medium confidence** in temporal consistency improvements, given reliance on aggregate metrics
**Low confidence** in claims about closing the gap with diffusion models, as absolute quality differences remain

## Next Checks
1. Conduct ablation studies to isolate the contribution of Multi-scale Temporal RoPE, Cross-Frame Error Correction, and Random Frame Mask to overall performance
2. Evaluate temporal consistency through qualitative human assessment of long-range coherence across extended video sequences
3. Test model generalization on diverse video datasets beyond UCF-101, including those with complex camera motion, object interactions, and varying frame rates