---
ver: rpa2
title: 'NomicLaw: Emergent Trust and Strategic Argumentation in LLMs During Collaborative
  Law-Making'
arxiv_id: '2508.05344'
source_url: https://arxiv.org/abs/2508.05344
tags:
- legal
- agents
- heterogeneous
- reasoning
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: NomicLaw simulates collaborative lawmaking among LLM agents using
  a propose-justify-vote protocol to measure emergent trust, reciprocity, and persuasion.
  Experiments compare homogeneous (same-model) and heterogeneous (diverse-model) cohorts
  across four AI-governance vignettes.
---

# NomicLaw: Emergent Trust and Strategic Argumentation in LLMs During Collaborative Law-Making

## Quick Facts
- arXiv ID: 2508.05344
- Source URL: https://arxiv.org/abs/2508.05344
- Reference count: 15
- Primary result: Heterogeneous LLM cohorts in lawmaking simulations show lower self-voting, higher coalition turnover, and richer thematic diversity compared to homogeneous groups.

## Executive Summary
NomicLaw simulates collaborative lawmaking among LLM agents using a propose-justify-vote protocol to measure emergent trust, reciprocity, and persuasion. Experiments compare homogeneous (same-model) and heterogeneous (diverse-model) cohorts across four AI-governance vignettes. Heterogeneous groups showed lower self-voting, higher coalition turnover, and richer thematic diversity (e.g., justice, harm, accountability) compared to homogeneous sessions dominated by narrow legal-formal arguments. Win rates varied widely by model, with DeepSeek-R1 leading (17.5%) and lower-tier models near zero. Thematic consistency between proposal and vote was lower under heterogeneity (30–44% match) than homogeneity (28–54% match). The framework and data are open-sourced to enable systematic study of AI-mediated deliberation and to inform safeguards for real-world legal workflows.

## Method Summary
The framework simulates a legislative process where LLM agents propose rules, justify them, and vote on proposals over five rounds. Two configurations are tested: homogeneous (5 copies of same model) and heterogeneous (10 distinct models). Ten open-source LLMs are orchestrated via Ollama API across four AI-governance vignettes. Scoring awards 10 points for wins and 5 for ties. Quantitative metrics include self-vote rate, win rate, reciprocity index, coalition switch rate, vote volatility, and edge density. Thematic analysis classifies outputs into 10 legal themes using LLM annotation validated with 10% human checks (κ ≥ 0.7).

## Key Results
- Heterogeneous cohorts show lower self-vote rates and higher coalition turnover compared to homogeneous groups dominated by echo-chamber effects.
- Win rates vary significantly by model capability, with DeepSeek-R1 achieving 17.5% wins while lower-tier models approach zero.
- Thematic consistency between proposals and votes is substantially lower in heterogeneous groups (30–44% match) than homogeneous groups (28–54% match), indicating adaptive rhetoric.

## Why This Works (Mechanism)

### Mechanism 1
Model heterogeneity disrupts insular voting patterns and forces broader coalition building. When agents share the same underlying model (homogeneous), they exhibit high self-vote rates and echo-chamber effects due to identical priors. Introducing diverse models creates conflicting reasoning styles, forcing agents to negotiate rather than auto-validate, resulting in lower self-voting and higher coalition turnover.

### Mechanism 2
Persuasion effectiveness (win rate) is stratified by model capability, specifically reasoning alignment. Top-performing models likely generate justifications that bridge the gap between diverse agent priors better than lower-tier models. The paper measures this via win rates and odds ratios, showing a clear hierarchy where sophisticated reasoners win significantly more often in mixed groups.

### Mechanism 3
Iterative interaction with memory creates strategic consistency (or lack thereof) in thematic arguments. Agents use a conversation buffer memory to view prior rounds, allowing for reciprocity or betrayal. However, heterogeneous groups show lower thematic consistency between proposals and votes, suggesting agents adapt their rhetoric opportunistically when facing diverse opposition.

## Foundational Learning

- **Concept: Multi-Agent Orchestration (Turn-based vs. Async)** - Critical for managing state in the propose-justify-vote loop to prevent race conditions. Quick check: How does the system ensure Agent B sees Agent A's proposal before casting a vote in the same round?

- **Concept: Jurisprudential Coding (Thematic Analysis)** - Essential for moving beyond raw text to legal themes (JUST, HARM, ACC). Quick check: Does the paper use human annotators or a secondary LLM to classify proposal themes?

- **Concept: Statistical Significance in Simulation (N-count)** - Vital for understanding sample size limitations. Quick check: Why is the homogeneous condition treated as descriptive rather than inferential in statistical analysis?

## Architecture Onboarding

- **Component map:** Controller (Python game loop) -> Agents (Ollama API wrappers for 10 models) -> Memory (Conversation Buffer) -> Context (Vignettes + System Prompt) -> Output (JSON Logs -> Analysis Pipeline)

- **Critical path:** Initialize 10 agents with unique model IDs → Inject Vignette into system prompt → Loop 5x: Agents generate proposals → Agents view all proposals + justifications → Agents vote (self-vote allowed) → Log winners + update scores → Run post-hoc classification on outputs

- **Design tradeoffs:** Closed vs. Open Voting (public encourages reciprocity/coalitions); Point Incentive (10 pts win / 5 tie sparse reward); Memory Scope (full history allows long-held grudges/alliances)

- **Failure signatures:** Parsing errors from malformed outputs; Homogeneous echo chambers (SVR hits 1.0); Hallucinated consensus (agents claim to vote for non-existent proposals)

- **First 3 experiments:** 1) Baseline replication with 5 Llama3 instances to verify high self-vote rates; 2) Ablate memory in heterogeneous condition to test if reciprocity collapses; 3) Introduce adversarial veto power to lower-tier model to test compensation for low persuasive win rates

## Open Questions the Paper Calls Out

- **Open Question 1:** Do LLM agents generate genuinely substantively distinct legal proposals, or merely rephrase similar solutions? Current analysis examines only thematic labels and win rates, not semantic similarity or novelty.

- **Open Question 2:** How do emergent trust, reciprocity, and coalition behaviors change when human participants are introduced alongside LLM agents? All experiments used only LLM agents; human cognitive biases and mixed human-AI social dynamics remain untested.

- **Open Question 3:** Would varying the incentive structure (e.g., different rewards, penalties for self-voting, coalition bonuses) produce different strategic archetypes? Current design awards uniform points; no counterfactual structures were tested.

- **Open Question 4:** Do homogeneous vs. heterogeneous cohort differences generalize across legal domains beyond AI governance? Only four AI-governance vignettes were tested; training data may bias models toward certain domains.

## Limitations

- Small number of vignettes and runs limits formal statistical inference, particularly for homogeneous groups treated descriptively.
- Self-voting incentives may drive lower SVR in heterogeneous groups through strategic bloc voting rather than genuine persuasion.
- LLM-based thematic annotation, while validated, is unsupervised and may miss nuanced legal reasoning.
- Win-rate hierarchy assumes direct mapping to reasoning quality, but factors like proposal length could influence outcomes.

## Confidence

- **High Confidence:** Self-vote rate differences between homogeneous/heterogeneous cohorts; descriptive patterns of thematic diversity; relative win-rate rankings.
- **Medium Confidence:** Interpretation that heterogeneity fosters genuine persuasion over bloc voting; claim that lower thematic consistency signals adaptive rhetoric.
- **Low Confidence:** Causal attribution of strategic behavior to model reasoning diversity alone; generalizability of win-rate hierarchy; stability of emergent trust dynamics under alternative reward structures.

## Next Checks

1. **Ablate the point incentive:** Re-run heterogeneous condition with zero-point rewards to test whether lower self-vote rates persist without explicit incentives.

2. **Swap voting logic:** Replace majority-vote rule with random classifier but preserve model identities. If top models retain high win rates, the mechanism is tribal rather than persuasive.

3. **Cross-validation of thematic annotation:** Re-classify 20% of proposals and votes with a second LLM or human annotator to assess robustness of legal-theme coding scheme.