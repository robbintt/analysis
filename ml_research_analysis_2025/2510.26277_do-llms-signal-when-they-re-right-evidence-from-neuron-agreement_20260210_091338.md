---
ver: rpa2
title: Do LLMs Signal When They're Right? Evidence from Neuron Agreement
arxiv_id: '2510.26277'
source_url: https://arxiv.org/abs/2510.26277
tags:
- neuron
- neurons
- early
- token
- activated
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes Neuron Agreement Decoding (NAD), an unsupervised
  ensemble method for selecting high-quality LLM outputs based on internal neuron
  activation patterns rather than external behaviors like token probabilities. NAD
  identifies consensus among sampled reasoning trajectories by leveraging sparsity
  and cross-sample agreement in neuron activations, enabling early correctness prediction
  within the first 32 generated tokens and aggressive early stopping.
---

# Do LLMs Signal When They're Right? Evidence from Neuron Agreement

## Quick Facts
- **arXiv ID:** 2510.26277
- **Source URL:** https://arxiv.org/abs/2510.26277
- **Reference count:** 26
- **Primary result:** Neuron Agreement Decoding (NAD) uses internal neuron activation patterns to predict correctness within first 32 tokens, reducing token usage by up to 99% with minimal quality loss

## Executive Summary
This paper introduces Neuron Agreement Decoding (NAD), an unsupervised ensemble method that selects high-quality LLM outputs by analyzing internal neuron activation patterns rather than external behaviors like token probabilities. NAD identifies consensus among sampled reasoning trajectories by leveraging sparsity and cross-sample agreement in neuron activations. Experiments across math, science, and coding benchmarks show NAD matches majority voting where applicable, consistently outperforms sampling average on open-ended tasks, and achieves up to 99% token reduction with minimal quality loss.

## Method Summary
NAD operates as an unsupervised best-of-N selector within sample-evaluate-ensemble decoding. It generates multiple reasoning trajectories (n=64), extracts per-chunk neuron activation sets via per-neuron contribution scores, computes pairwise Jaccard similarity matrices, and selects candidates based on cross-sample agreement patterns. The method enables early stopping at chunk B=32 by identifying consensus among sampled trajectories before full generation completes.

## Key Results
- Matches Cons@64 accuracy on AIME24 (86%) while using only 1-2 samples on average
- Reduces token usage by 87-99% across benchmarks with minimal quality loss
- Outperforms sampling average on open-ended tasks where majority voting fails
- Early stopping at 32 tokens achieves dramatic efficiency gains without sacrificing accuracy

## Why This Works (Mechanism)

### Mechanism 1: Activation Sparsity Signals Response Quality
Correct reasoning trajectories activate substantially fewer unique neurons than incorrect ones, observable within the first 32 tokens. Successful reasoning appears to balance exploration and exploitation—reaching answers with fewer trial-and-error steps—while failures trigger over-exploration, engaging more neurons as the model repeatedly shifts strategies.

### Mechanism 2: Cross-Sample Neuron Agreement Identifies Consensus
Samples exhibiting similar neuron activation patterns (high Jaccard overlap) are more likely correct, while incorrect responses diverge to cluster margins. Correct reasoning paths converge on similar internal computations; incorrect paths explore divergent faulty reasoning branches, producing activation patterns with lower pairwise similarity.

### Mechanism 3: Internal States as Richer Signals Than External Projections
External confidence metrics (entropy, self-certainty) are low-dimensional projections of high-dimensional internal neuron states, which encode additional structure. Token probabilities derive from hidden states, which derive from neuron activations; information is progressively compressed through this cascade, losing discriminative structure.

## Foundational Learning

- **SwiGLU FFN Architecture and Neuron Contribution**: NAD extracts activated neuron sets via per-neuron contribution scores to output tokens; understanding FFN structure is prerequisite for implementation. *Quick check:* Given hidden input x, gate projection W_g, and output projection W_out, how does SwiGLU compute the FFN output?

- **Sample-Evaluate-Ensemble Decoding**: NAD operates within this paradigm as an unsupervised best-of-N selector; understanding baselines (majority voting, self-certainty) contextualizes the contribution. *Quick check:* Why does majority voting fail on open-ended code generation tasks?

- **Jaccard Index for Set Similarity**: Cross-sample agreement is quantified via Jaccard similarity between activated neuron sets; this is the core metric for clustering and selection. *Quick check:* For two sets A={1,2,3} and B={2,3,4}, what is the Jaccard index?

## Architecture Onboarding

- **Component map**: Parallel Sampler -> Activation Extractor -> Agreement Matrix Builder -> Selector -> Early Stop Controller

- **Critical path**: 
  1. Forward pass logs neuron contributions per token
  2. Aggregate to chunk-level activation sets (union across tokens in chunk)
  3. At early-stop position (default: 32 tokens), compute pairwise Jaccard matrix
  4. Apply selector to identify best candidate
  5. Continue generation only for selected trajectory

- **Design tradeoffs**:
  - Early-stop position: Earlier (32 tokens) maximizes token savings but may miss late-emerging signals; later positions introduce noise
  - Selector choice: kNN most robust across tasks; MinAct is simplest but underperforms on code benchmarks
  - Storage overhead: Storing activation sets for 64 samples requires memory; bitset encodings mitigate this

- **Failure signatures**:
  - Code generation reversal: MinAct underperforms MaxAct on some coding tasks—diagnostic: check if task domain matches training distribution
  - Late-generation noise: Accuracy degrades when using full response vs. early stopping—diagnostic: check if noise accumulation obscures early signals
  - Tie or near-tie in Jaccard: Selector returns arbitrary choice—diagnostic: inspect similarity matrix distribution

- **First 3 experiments**:
  1. Run NAD-kNN on AIME24 with n=64, early-stop at 32 tokens; verify accuracy approaches Cons@64 baseline (~86%)
  2. Sweep B ∈ {32, 128, 512, 2048} on AIME24+25; plot accuracy vs. token consumption (expect non-monotonic accuracy, near-linear token growth)
  3. Compare MinAct vs. kNN on HumanEval (code) vs. GPQA (science); verify MinAct gap narrows or reverses on code

## Open Questions the Paper Calls Out
None

## Limitations
- The sparsity-quality relationship may not generalize beyond math, science, and coding domains, particularly weakening on open-ended code generation
- Method relies on SwiGLU FFN architecture; performance on other architectures (Mamba, RWKV, or different activation functions) remains untested
- Consensus assumption may fail when multiple valid solution strategies exist, potentially misclassifying diverse but correct approaches

## Confidence

- **High Confidence (0.8+):** The empirical observation that NAD consistently reduces token usage by up to 99% with minimal quality loss on tested benchmarks. The experimental methodology and results are robust within the studied domain.

- **Medium Confidence (0.6):** The proposed mechanisms (sparsity signals, cross-sample agreement, internal states as richer signals) are theoretically plausible and partially supported by the evidence, but alternative explanations exist and the causal relationships require further validation.

- **Low Confidence (0.4-):** Generalization to other LLM architectures, domains beyond math/science/coding, and scenarios with multiple valid solution approaches.

## Next Checks

1. **Architecture Transfer Test:** Apply NAD to a non-transformer architecture (e.g., Mamba) or a transformer variant with different activation functions (e.g., ReLU instead of SwiGLU). Measure whether the sparsity-quality relationship and agreement patterns persist, or if architecture-specific adaptations are required.

2. **Domain Generalization Study:** Test NAD on domains where multiple valid solutions exist (e.g., creative writing, design tasks, or multi-modal reasoning). Specifically, evaluate whether the consensus mechanism correctly handles diverse but valid approaches versus incorrectly penalizing valid diversity.

3. **Temporal Signal Stability Analysis:** Track the stability and predictive power of neuron activation patterns across the full generation trajectory (not just early stopping). Determine whether early signals remain reliable predictors or whether late-stage information becomes necessary for certain task types, informing optimal early-stop positioning.