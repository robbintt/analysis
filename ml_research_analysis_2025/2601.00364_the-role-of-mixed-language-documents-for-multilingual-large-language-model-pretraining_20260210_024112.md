---
ver: rpa2
title: The Role of Mixed-Language Documents for Multilingual Large Language Model
  Pretraining
arxiv_id: '2601.00364'
source_url: https://arxiv.org/abs/2601.00364
tags:
- data
- bilingual
- language
- cross-lingual
- translation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study systematically investigates the role of bilingual data
  in multilingual large language model pretraining. The authors create a monolingual
  corpus by removing all multilingual documents from standard web data, then pretrain
  models from scratch to compare translation performance against models trained on
  the full multilingual corpus.
---

# The Role of Mixed-Language Documents for Multilingual Large Language Model Pretraining

## Quick Facts
- arXiv ID: 2601.00364
- Source URL: https://arxiv.org/abs/2601.00364
- Authors: Jiandong Shao, Raphael Tang, Crystina Zhang, Karin Sevegnani, Pontus Stenetorp, Jianfei Yang, Yao Lu
- Reference count: 11
- Removing bilingual data (2% of corpus) causes 56% BLEU score drop in translation while leaving cross-lingual QA and reasoning largely unaffected

## Executive Summary
This study systematically investigates how bilingual data affects multilingual LLM pretraining by removing all multilingual documents from standard web data and pretraining models from scratch. The key finding is that parallel documents (14% of bilingual data) almost fully restore translation performance, while code-switching data (72% of bilingual data) contributes minimally. Cross-lingual understanding and reasoning tasks remain stable without bilingual exposure, revealing that translation critically depends on systematic token-level alignments from parallel data, whereas other cross-lingual tasks can be learned without bilingual exposure through sentence-level semantic alignment.

## Method Summary
The authors created four corpus variants from FineWeb-Edu and FineWeb2 (60B tokens/language × 4 languages): FINEWEB (full multilingual), MONOWEB (bilingual documents removed), MONOWEB+PARALLEL (parallel data restored), and MONOWEB+CODESWITCH (code-switching data restored). Bilingual documents were identified using entropy-based filtering (τ=0.1) and Llama-3.3-70B-Instruct classification. They pretrained 1.35B decoder-only transformers (24 layers, 2048 hidden dim) from scratch on each variant (~143B tokens) and evaluated translation (WMT14/16, FLORES-101), QA (XQuAD, MLQA), and reasoning (XNLI, HellaSwag, ARC, PAWS-X, TruthfulQA, XStoryCloze, XWinograd) tasks using lm-evaluation-harness with 5-shot prompting.

## Key Results
- Removing bilingual data (only 2% of corpus) causes translation BLEU scores to drop by 56% (from 22.3 to 9.8)
- Cross-lingual understanding and reasoning tasks remain stable, with performance within 1-2% of baseline
- Parallel data restoration recovers 91% of translation performance compared to the full multilingual corpus
- Code-switching data contributes minimally to translation restoration (BLEU 12.4 vs 9.8 baseline drop)
- Translation failure consists of ~55% language generation failure plus ~55% quality degradation in correctly generated outputs

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Parallel data provides systematic token-level alignments that are necessary and sufficient for translation capability, while being unnecessary for other cross-lingual tasks.
- Mechanism: Parallel documents enable models to learn explicit word-to-word correspondences across languages, allowing generation of text with appropriate vocabulary substitutions critical for translation but dispensable for semantic understanding tasks.
- Core assumption: Translation requires precise lexical mappings that cannot emerge from monolingual exposure alone, while semantic understanding can transfer through shared conceptual structures.
- Evidence anchors:
  - [abstract]: "parallel data almost fully restores translation performance (91% of the unfiltered baseline), whereas code-switching contributes minimally"
  - [section 5.4]: MONOWEB "suffers a sharp 13–21% degradation in lexical-level alignment" while "sentence-level alignment remains largely stable" with <2% drop
  - [corpus]: Limited corpus evidence on specific neural mechanisms for token alignment; related work on parallel corpora for MT supports the necessity claim

### Mechanism 2
- Claim: Sentence-level cross-lingual semantic alignment emerges from monolingual pretraining without requiring explicit bilingual exposure.
- Mechanism: Models exposed to different languages learn to map diverse surface forms to shared underlying semantic representations, developing aligned sentence-level embeddings sufficient for QA and reasoning tasks that don't require word-level generation.
- Core assumption: Languages encode similar underlying semantic structures that can be learned independently and aligned at an abstract level.
- Evidence anchors:
  - [abstract]: "cross-lingual understanding and reasoning appear to be achievable even without bilingual data"
  - [section 5.3]: Understanding/reasoning benchmarks "show stability across all bilingual configurations, with performance consistently being within 1-2% of the baseline"
  - [corpus]: Comparable corpus research (arXiv:2508.02555) supports topic-aligned documents enabling cross-lingual learning without parallel text

### Mechanism 3
- Claim: Code-switching data fails to enable translation because it provides contextual co-occurrence without systematic alignment signals.
- Mechanism: Code-switching documents contain language mixing within shared discourse contexts but don't provide the explicit token correspondence needed for generation tasks, exposing models to cross-lingual contexts without translation-level alignment.
- Core assumption: Translation requires learning "what word maps to what word," not just "what concepts co-occur across languages."
- Evidence anchors:
  - [abstract]: "code-switching contributes minimally" to translation restoration
  - [section 3.1]: Code-switching documents "do not provide translations of each other but rather contribute distinct yet related semantic content"
  - [corpus]: Weak corpus evidence; assumption based on structural analysis rather than mechanistic studies of code-switching learning

## Foundational Learning

- Concept: **Parallel vs. Comparable vs. Code-Switching Corpora**
  - Why needed here: The paper's taxonomy (parallel 14%, code-switching 72%, miscellaneous 14%) is central to understanding why removing 2% of data causes 56% translation drop. Parallel corpora have explicit translations; comparable corpora are topic-aligned but not translated; code-switching mixes languages naturally.
  - Quick check question: If you had to choose between 1M tokens of Airbnb listings (parallel) or 5M tokens of multilingual Reddit comments (code-switching) for training a translation model, which would you pick and why?

- Concept: **Cross-Lingual Alignment Granularity (Lexical vs. Sentence-Level)**
  - Why needed here: The key insight is that different tasks need different alignment depths. Translation requires word-level correspondences (lexical alignment), while QA/reasoning only needs sentence-level semantic similarity. This explains the task asymmetry: bilingual data provides lexical alignment, but monolingual pretraining already provides sufficient sentence-level alignment.
  - Quick check question: A model correctly answers French questions about English documents but produces English output when asked to translate to French. Which type of alignment is working, and which is broken?

- Concept: **BLEU Score Decomposition and Translation Failure Modes**
  - Why needed here: The paper reveals translation failure has two components: 56% language generation failure (producing wrong language) plus quality degradation even when correct language is generated. BLEU alone doesn't distinguish these; the authors used manual analysis and language identification to decompose the failure.
  - Quick check question: If a model's BLEU drops from 22 to 10, what three experiments would you run to determine whether the problem is language identification, vocabulary limitation, or semantic understanding?

## Architecture Onboarding

- Component map: FineWeb/FineWeb2 (60B tokens/language × 4 languages) -> entropy-based candidate detection (τ=0.1, retains 5%) -> Llama-3.3-70B classification (parallel/code-switching/misc) -> filtered corpora for each experimental condition -> 1.35B decoder-only transformer pretraining -> lm-evaluation-harness evaluation

- Critical path:
  1. Verify bilingual document identification pipeline (entropy threshold calibration, LLM classifier accuracy)
  2. Confirm parallel data restoration effect (MONOWEB+PARALLEL should recover ~91% of FINEWEB BLEU)
  3. Validate alignment analysis methodology (P@1 with cosine similarity on WMT parallel sentences and MUSE word pairs)

- Design tradeoffs:
  - **1.35B vs. larger models**: Smaller models enable controlled ablations (12 models trained) but may not reflect behavior of production-scale models; authors acknowledge this limitation
  - **Latin-script language pairs only**: en-de, en-es, en-fr controlled for script but limit generalizability to typologically distant or low-resource languages
  - **Binary parallel/code-switching taxonomy**: Misses gradations of alignment quality within categories; domain and register effects not analyzed

- Failure signatures:
  - **Translation collapse (two-fold)**: ~55% language generation failure (English passthrough when German requested) + ~55% quality degradation among correctly generated outputs
  - **Semantic underspecification**: Core propositions preserved but temporal markers, explanatory context, and lexical precision lost (e.g., "kids" → "Familie" [family], "immediately" omitted)
  - **Middle-layer lexical alignment degradation**: Layers 6-12 show 13-21% P@1 drop for word-level alignment while sentence-level stays stable

- First 3 experiments:
  1. **Baseline verification**: Train FINEWEB and MONOWEB models for a single language pair (e.g., en-fr), evaluate on WMT14 to confirm the 56% BLEU drop (22.3 → 9.8) before running full ablations.
  2. **Parallel data scaling**: Starting from MONOWEB, incrementally add parallel data (1%, 5%, 10%, 14% of bilingual portion) to determine the minimum parallel data threshold for translation recovery and whether the relationship is linear or threshold-based.
  3. **Cross-task alignment probe**: Use the P@1 methodology to measure lexical and sentence-level alignment for models trained on different data configurations, correlating alignment scores with downstream task performance to verify that lexical alignment predicts translation but not reasoning.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the critical dependence on parallel data for translation scale to larger models (7B+ parameters), or do emergent capabilities at scale reduce this reliance?
- Basis in paper: [explicit] The authors state in Limitations: "we pretrained only 1.35B-parameter models and did not pretrain larger models such as 7B, which may exhibit different sensitivity to bilingual data."
- Why unresolved: Computational constraints limited experiments to 1.35B parameters; scaling behavior for bilingual data sensitivity remains unknown.
- What evidence would resolve it: Pretraining 7B or larger models under identical MONOWEB/FINEWEB conditions and comparing translation BLEU degradation patterns.

### Open Question 2
- Question: Do these findings generalize to typologically distant language pairs (e.g., English–Chinese, English–Arabic) or low-resource languages where bilingual data is even sparser?
- Basis in paper: [explicit] The authors note: "our experiments focus on major languages within the Latin script family, leaving open questions about the impact of bilingual data on typologically distant or low-resource languages."
- Why unresolved: All experiments used English paired with German, Spanish, and French—high-resource languages sharing script and linguistic similarity.
- What evidence would resolve it: Replicating the ablation study on non-Latin script pairs and low-resource languages to measure whether parallel data dependence weakens or strengthens.

### Open Question 3
- Question: What specific mechanisms enable cross-lingual understanding and reasoning to emerge from purely monolingual pretraining without any bilingual exposure?
- Basis in paper: [inferred] The paper documents that understanding/reasoning tasks remain stable despite bilingual data removal, but only provides the high-level explanation that "sentence-level alignment persists" without detailing the underlying learning dynamics.
- Why unresolved: The analysis shows the phenomenon exists but does not explain how representations align across languages without any cross-lingual signal during training.
- What evidence would resolve it: Probing studies on intermediate layers to identify whether shared semantic spaces emerge from common contextual patterns, multilingual subword overlap, or other mechanisms.

### Open Question 4
- Question: Why does code-switching data—constituting 72% of bilingual content—contribute so minimally to translation when prior work suggested it aids cross-lingual transfer?
- Basis in paper: [inferred] The paper reports that reintroducing code-switching data yields only marginal BLEU improvement (12.4 vs. 9.8), contradicting prior assumptions and motivating investigation into what signals code-switching actually provides.
- Why unresolved: The paper demonstrates code-switching's limited utility but does not explain the mismatch with prior finetuning-stage findings or what prevents it from creating token-level alignments.
- What evidence would resolve it: Analysis comparing token co-occurrence patterns in code-switching vs. parallel documents, and probing whether code-switching creates any measurable cross-lingual associations in representation space.

## Limitations

- The study focuses on Latin-script language pairs (en-de, en-es, en-fr), limiting generalizability to typologically distant languages or non-Latin scripts
- Only 1.35B parameter models were pretrained, leaving open whether larger models (7B+) exhibit different sensitivity to bilingual data
- Bilingual document classification relies on LLM categorization without ground truth validation, potentially introducing labeling errors
- The parallel data restoration experiment uses all available parallel documents rather than controlled sampling, making it unclear whether the 14% proportion is necessary or sufficient

## Confidence

- **High confidence**: Differential impact of bilingual data on translation vs. understanding/reasoning tasks; parallel data restoration effect; code-switching contribution finding
- **Medium confidence**: Mechanism explaining why parallel data enables translation; claim that monolingual pretraining provides sufficient sentence-level alignment for semantic tasks; finding that translation failure has two components
- **Low confidence**: Specific threshold proportions of bilingual data types needed; generalizability to non-Latin scripts and low-resource languages; assertion that code-switching contributes minimally rather than zero

## Next Checks

1. **Cross-linguistic validation**: Replicate the core ablation experiment with one typologically distant language pair (e.g., English-Chinese or English-Japanese) to test whether the parallel data mechanism generalizes beyond Latin-script languages and whether the 56% translation drop persists with different script systems.

2. **Parallel data scaling experiment**: Systematically vary the proportion of parallel data added to MONOWEB baseline (0%, 1%, 5%, 10%, 14%, 20%) to determine whether translation recovery follows a linear relationship or exhibits threshold effects, and whether less than 14% parallel data could suffice for reasonable translation performance.

3. **Alignment-probe correlation study**: Conduct comprehensive lexical and sentence-level alignment analysis across all model variants using the P@1 methodology, then compute correlation coefficients between alignment scores and downstream task performance to empirically verify that lexical alignment specifically predicts translation while sentence-level alignment predicts semantic understanding tasks.