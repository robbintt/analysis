---
ver: rpa2
title: 'Beyond Single Embeddings: Capturing Diverse Targets with Multi-Query Retrieval'
arxiv_id: '2511.02770'
source_url: https://arxiv.org/abs/2511.02770
tags:
- query
- target
- document
- embeddings
- documents
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Existing text retrievers generate a single query embedding, which
  limits their ability to capture diverse target documents when the relevant set is
  multimodal. To address this, we propose an autoregressive multi-embedding retriever
  (AMER) that generates multiple query vectors per input, each used to retrieve a
  ranked document list, which are then aggregated.
---

# Beyond Single Embeddings: Capturing Diverse Targets with Multi-Query Retrieval

## Quick Facts
- arXiv ID: 2511.02770
- Source URL: https://arxiv.org/abs/2511.02770
- Reference count: 29
- Primary result: AMER achieves 100% MRECALL on synthetic multimodal data vs ~20% for single-query models

## Executive Summary
Existing text retrievers generate a single query embedding, which limits their ability to capture diverse target documents when the relevant set is multimodal. To address this, we propose an autoregressive multi-embedding retriever (AMER) that generates multiple query vectors per input, each used to retrieve a ranked document list, which are then aggregated. On synthetic data with diverse targets, AMER achieves perfect performance (100% MRECALL) versus ~20% for single-query models, validating its ability to model multiple target distributions. On real-world multi-answer retrieval datasets (AmbigQA, QAMPARI), AMER yields modest overall gains (4% and 21% relative improvement, respectively) but larger gains (up to 144%) on subsets where target documents are less similar, indicating its strength in capturing diversity.

## Method Summary
AMER modifies the standard bi-encoder retrieval architecture by replacing the single-output query encoder with an autoregressive LLM that generates multiple query embeddings sequentially. Each embedding is used to retrieve documents from a frozen document index, and results are aggregated via round-robin. Training uses InfoNCE loss with Hungarian matching to optimally align generated embeddings with target document embeddings, and scheduled sampling bridges the train-inference gap. The model is fine-tuned on multi-answer datasets where queries have multiple valid target documents.

## Key Results
- On synthetic multimodal data: AMER achieves 100% MRECALL vs ~20% for single-query models
- On AmbigQA dataset: 4% relative improvement in MRECALL
- On QAMPARI dataset: 21% relative improvement in MRECALL, with up to 144% gains on low-similarity subsets

## Why This Works (Mechanism)

### Mechanism 1: Multi-Embedding Coverage for Multimodal Target Distributions
A single query embedding cannot effectively capture a multimodal target distribution where relevant documents are semantically distant, but generating multiple, distinct query embeddings autoregressively can. Instead of a single query embedding, AMER autoregressively generates a sequence of multiple query embeddings. Each embedding is conditioned on the previous one, aiming to capture a different mode of the relevant document distribution. All predicted embeddings are then used to retrieve documents, and the results are aggregated. The core assumption is that the relevant documents for a query can form distinct semantic clusters or modes in the embedding space that a single vector cannot simultaneously represent well. The autoregressive generation can learn to target these distinct modes sequentially.

### Mechanism 2: Optimal Matching for Unordered Target Sets
The set of target documents is inherently unordered, so an optimal matching algorithm is required during training to effectively align each predicted query embedding with a target document without imposing a spurious ordering. During training, the model generates `m` output embeddings for a query with `m` target documents. Since the order of targets doesn't matter, the loss function finds the optimal matching between the generated embeddings and the target document embeddings using the Hungarian matching algorithm. The core assumption is that forcing the model to predict embeddings in a specific, arbitrary order would add unnecessary complexity and hamper learning. Letting the model assign its embeddings to targets in the most efficient way leads to better convergence.

### Mechanism 3: Scheduled Sampling for Train-Inference Consistency
Scheduled sampling during training, which mixes ground-truth and predicted embeddings as input, is more effective than using only ground-truth inputs to bridge the distribution shift between training and inference. During inference, the model must take its own predicted embedding from the previous step as input. If training only uses ground-truth document embeddings, the model is not exposed to its own errors. Scheduled sampling feeds the predicted embedding with increasing probability `p` over training steps. The core assumption is that the autoregressive generation of continuous embeddings suffers from exposure bias, similar to text generation. The model must be explicitly trained to condition on its own, potentially imperfect, outputs.

## Foundational Learning

- **Contrastive Learning with InfoNCE Loss**
  - Why needed here: This is the core training objective used to train the retriever. It pushes the query embedding closer to positive document embeddings and away from negative ones.
  - Quick check question: What is the denominator in the InfoNCE loss, and how does in-batch negative sampling work?

- **Autoregressive Language Models (LMs)**
  - Why needed here: AMER's query encoder is an autoregressive LM (like Llama-3). The model generates a sequence of embeddings in a token-by-token fashion (though the "tokens" here are continuous vectors), conditioning each step on the previous one.
  - Quick check question: In an autoregressive model, how is the prediction for the next step conditioned on previous steps?

- **Dense Retrieval (Bi-Encoder Architecture)**
  - Why needed here: AMER builds upon the standard bi-encoder architecture where separate encoders produce query and document embeddings. This context is necessary to understand what AMER modifies (the query encoder).
  - Quick check question: In a bi-encoder, what are the two main components and how do they interact to produce a relevance score?

## Architecture Onboarding

- Component map: Frozen Document Encoder -> Trainable Query Encoder -> Projection Layers -> Matching Module (Training) -> Aggregation Module (Inference)
- Critical path: Query text -> Query Encoder -> [Embedding 1, ..., Embedding N] -> (For each Emb: Retrieve top-k from Corpus Index) -> Aggregate results -> Final Ranked List
- Design tradeoffs:
  - **Fixed vs. Trainable Document Encoder**: A frozen document encoder is efficient as it avoids re-indexing the entire corpus. The tradeoff is the query encoder is constrained to the fixed embedding space. The paper chooses frozen for efficiency.
  - **Fixed vs. Variable Number of Predictions (`m_pred`)**: Predicting a fixed number of embeddings is simpler and was found to work best. A dynamic number could be more efficient but adds complexity.
- Failure signatures:
  - **Modest Real-World Gains**: If real-world datasets have high similarity between target documents (low diversity), AMER's gains over single-query baselines will be small, as noted for AmbigQA and QAMPARI.
  - **Exposure Bias**: Without scheduled sampling, the model may fail to generalize from using ground-truth embeddings during training to its own predictions during inference.
- First 3 experiments:
  1. **Synthetic Data Validation**: Train both a single-query model and AMER on the vectorized data (e.g., with MLP transformations). Verify that the single-query model fails (low MRecall) while AMER achieves near-perfect recall, validating the core hypothesis.
  2. **Impact of Target Diversity**: Train AMER on a multi-answer dataset (e.g., QAMPARI). Evaluate performance on the "low similarity" subset (where target document embeddings are far apart). Compare gains on this subset versus the whole set.
  3. **Ablation on Scheduled Sampling**: Train two variants of AMER: one using scheduled sampling and another that always takes the previously predicted embedding as input. Compare their performance to quantify the technique's impact.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the model learn to dynamically determine the optimal number of query embeddings rather than predicting a fixed number?
- Basis in paper: Section 8 states future work could explore "learning to flexibly decide the number of query vectors to predict."
- Why unresolved: The current architecture requires setting a predetermined number of output embeddings ($m_{pred}$) for inference, which assumes a constant number of target clusters regardless of the actual query complexity.
- Evidence: Implementing a stopping criterion or a specialized termination token within the autoregressive decoder, then evaluating performance and efficiency on datasets with high variance in the number of answers.

### Open Question 2
- Question: How can the system improve the aggregation of documents retrieved by multiple embeddings?
- Basis in paper: Section 8 suggests exploring ways of "more effectively aggregating across different outputs."
- Why unresolved: The current inference method uses a heuristic round-robin approach to merge ranked lists, which treats all query embeddings equally and may not optimize for final relevance.
- Evidence: Replacing the round-robin heuristic with learned re-ranking methods or weighted fusion strategies, then measuring Recall@k on the QAMPARI dataset.

### Open Question 3
- Question: Does training with hard negatives and larger unsupervised datasets improve AMER's robustness?
- Basis in paper: Section 8 proposes "mining hard negatives and training on a much larger scale of unsupervised data."
- Why unresolved: The current experiments utilized random negatives and focused on specific supervised fine-tuning datasets to isolate architectural benefits, leaving the potential gains from harder training signals unexplored.
- Evidence: Training the model using hard negative sampling strategies (e.g., BM25 negatives) on large-scale corpora and comparing performance against the current random-negative baseline.

## Limitations
- Modest real-world gains (4-21%) suggest AMER's advantages may only manifest in specific high-diversity scenarios
- Computational overhead of generating and evaluating multiple query embeddings wasn't quantified
- Synthetic data validation uses simplified linear/MLP transformations that may not capture real query-document complexity

## Confidence

**High confidence** in the core mechanism: that autoregressive multi-embedding generation can capture multimodal target distributions, as evidenced by perfect performance (100% MRECALL) on synthetic data where single-query models achieve only ~20%. The Hungarian matching approach for unordered target sets and scheduled sampling for train-inference consistency also have strong supporting evidence from ablation studies.

**Medium confidence** in real-world applicability: While the methodology is sound, the modest overall gains (4-21%) on actual multi-answer datasets suggest the conditions where AMER substantially outperforms single-query methods may be limited. The paper's own analysis shows gains are concentrated in low-similarity subsets, raising questions about practical deployment scenarios.

**Low confidence** in scalability claims: The paper doesn't provide runtime or indexing overhead comparisons, making it difficult to assess whether the benefits justify the additional computational cost in production systems.

## Next Checks

1. **Diversity Impact Analysis**: Systematically vary the semantic diversity of target documents in QAMPARI and measure how AMER's relative performance changes compared to single-query baselines across the full spectrum of similarity values.

2. **Computational Overhead Quantification**: Measure and compare end-to-end inference latency and indexing costs between AMER and single-query retrievers on identical hardware, including time for multi-embedding generation, multiple retrieval passes, and aggregation.

3. **Cross-Dataset Generalization**: Evaluate AMER on additional multi-answer datasets with different characteristics (e.g., HotpotQA, Natural Questions) to determine whether the modest gains observed are dataset-specific or represent a general trend.