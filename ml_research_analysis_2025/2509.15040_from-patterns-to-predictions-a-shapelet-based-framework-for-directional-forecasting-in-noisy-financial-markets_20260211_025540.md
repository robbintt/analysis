---
ver: rpa2
title: 'From Patterns to Predictions: A Shapelet-Based Framework for Directional Forecasting
  in Noisy Financial Markets'
arxiv_id: '2509.15040'
source_url: https://arxiv.org/abs/2509.15040
tags:
- pattern
- time
- each
- patterns
- series
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a two-stage framework for interpretable directional
  forecasting in noisy financial markets. The first stage, SIMPC, segments and clusters
  multivariate time series to extract recurring patterns invariant to amplitude scaling
  and temporal distortion, even under varying window sizes.
---

# From Patterns to Predictions: A Shapelet-Based Framework for Directional Forecasting in Noisy Financial Markets

## Quick Facts
- arXiv ID: 2509.15040
- Source URL: https://arxiv.org/abs/2509.15040
- Reference count: 40
- The framework achieves first or second rank in 11 out of 12 metric-dataset combinations, improving interpretability and forecasting accuracy in noisy financial markets.

## Executive Summary
This paper presents a two-stage framework for interpretable directional forecasting in noisy financial markets. The first stage, SIMPC, segments and clusters multivariate time series to extract recurring patterns invariant to amplitude scaling and temporal distortion, even under varying window sizes. The second stage, JISC-Net, uses a shapelet-based classifier that leverages the initial portion of extracted patterns to forecast short-term directional movements. The framework is evaluated on Bitcoin and three S&P 500 equities, achieving first or second rank in 11 out of 12 metric-dataset combinations. The method improves interpretability by revealing underlying pattern structures that drive predictive outcomes, addressing the limited transparency of conventional deep learning models. The two-stage filtering process (K-S test and confidence thresholding) further enhances prediction reliability by excluding noisy or non-pattern segments.

## Method Summary
The framework operates in two stages: SIMPC and JISC-Net. SIMPC segments multivariate time series and clusters them using Dynamic Time Warping (DTW), initializing centroids with domain-specific chart patterns and pruning low-information clusters. JISC-Net uses a shapelet-based classifier that leverages the initial portion of extracted patterns to forecast short-term directional movements. The model is trained using DTW-based triplet loss to ensure invariance to temporal distortion and amplitude scaling. The system employs a two-stage filter: a Kolmogorov-Smirnov (K-S) test during training removes pattern labels that overlap with others, and a confidence threshold during inference discards low-probability predictions.

## Key Results
- Achieves first or second rank in 11 out of 12 metric-dataset combinations
- Improves interpretability by revealing underlying pattern structures that drive predictive outcomes
- Enhances prediction reliability by excluding noisy or non-pattern segments through two-stage filtering

## Why This Works (Mechanism)

### Mechanism 1: Noise Suppression via Selective Clustering (SIMPC)
The framework improves signal-to-noise ratio by identifying and clustering only recurrent, high-informative subsequences while treating the rest as background noise. SIMPC segments multivariate time series and clusters them using Dynamic Time Warping (DTW). It initializes centroids using domain-specific chart patterns (e.g., Head-and-Shoulders) and prunes clusters with insufficient membership (κ threshold). This explicitly filters random-walk behaviors that do not conform to structural motifs.

### Mechanism 2: Distortion-Invariant Representation Learning (JISC-Net)
The model maintains classification accuracy despite variations in pattern speed or duration by using DTW-based triplet loss. The Mdc-CNN encoder is trained using triplet loss where the distance metric is replaced with DTW. This forces the encoder to map temporally distorted versions of the same pattern to similar embeddings, separating them from other pattern classes in the latent space.

### Mechanism 3: Precision-Optimized Selective Forecasting
Filtering predictions based on statistical separability and confidence thresholds improves trading returns by reducing exposure to false positives. The system employs a two-stage filter: (1) A Kolmogorov-Smirnov (K-S) test during training removes pattern labels that overlap with others; (2) A confidence threshold (Top-x%) during inference discards low-probability predictions.

## Foundational Learning

- **Concept: Dynamic Time Warping (DTW)**
  - **Why needed here:** Standard distance metrics (Euclidean) fail when comparing financial patterns of different durations (e.g., a "Head and Shoulders" forming over 15 days vs. 22 days). DTW allows the model to "warp" the time axis to align shapes.
  - **Quick check question:** Can you explain why DTW is computationally more expensive than Euclidean distance, and why the authors use it in the loss function rather than just for clustering?

- **Concept: Shapelet-Based Classification**
  - **Why needed here:** Unlike holistic approaches that use the entire time series, this method classifies based on local, discriminative subsequences (shapelets). This local focus is critical for detecting transient signals in noisy financial data.
  - **Quick check question:** How does a "shapelet" differ from a standard convolutional filter kernel in a CNN regarding interpretability?

- **Concept: Triplet Loss**
  - **Why needed here:** This is the mechanism used to train the neural encoder. It explicitly structures the latent space so that "similar" patterns cluster together and "dissimilar" ones are pushed apart, which is vital for the subsequent SVM classifier.
  - **Quick check question:** In the context of this paper, how are the "Anchor," "Positive," and "Negative" samples defined for a given segment?

## Architecture Onboarding

- **Component map:** SIMPC (Kernel Regression → Chart Prototypes → DTW + K-means++ → Pruning) → JISC-Net (Multiscale Slicing → Mdc-CNN with DTW-Triplet Loss → K-means on Embeddings → Linear SVM → K-S Test + Confidence Threshold)

- **Critical path:** SIMPC: Raw Price → Smoothed Segments → Pattern Labels → JISC-Net Encoder: Train CNN to produce Embeddings where similar patterns are close → Shapelet Discovery: Extract Discriminative Subsequences from the embeddings → Inference: Calculate DTW distance from input to Shapelets → Probability Vector → Filter → Directional Forecast

- **Design tradeoffs:** Interpretability vs. Complexity (uses deep CNN but maps back to raw shapelets for final SVM decision), Recall vs. Precision (Top-x% filtering reduces trades to boost Average Return)

- **Failure signatures:** Overfit to early segment (matches initial γ portion perfectly but fails if subsequent trajectory deviates), Spurious DTW Alignment (artificially aligns dissimilar shapes sharing local volatility characteristics)

- **First 3 experiments:** 1) Input Sensitivity: Run SIMPC with "Close Only" vs. "Close + Volume + RSI" inputs, check Table 1 for average distance between centroids. 2) Ablation on Invariance: Train JISC-Net encoder with Euclidean vs. DTW-based Triplet Loss, compare F1 scores. 3) Threshold Tuning: Implement confidence filtering loop, vary Top-x% threshold (e.g., T@20, T@60, T@100), plot trade-off between #Trades and Total Return.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can probabilistic forecasting of future trajectories reduce overfitting to specific historical shapes in noisy environments?
- Basis in paper: The authors state in the failure analysis and conclusion that deterministic predictions may overfit to early input segments, suggesting probabilistic modeling as a remedy to capture uncertainty.
- Why unresolved: The current JISC-Net architecture outputs discrete class probabilities rather than modeling the continuous distribution of potential future price paths.
- What evidence would resolve it: A variant of the model that outputs prediction intervals or density estimates, demonstrating improved robustness in volatile periods where deterministic patterns fail.

### Open Question 2
- Question: Can incorporating shape-aware penalties into Dynamic Time Warping (DTW) minimize spurious pattern matches?
- Basis in paper: The discussion of failure cases notes that DTW can yield artificially low distances for sequences that are temporally aligned but structurally dissimilar.
- Why unresolved: Standard DTW optimizes for temporal alignment without enforcing semantic consistency regarding critical financial features like peaks, troughs, or curvature.
- What evidence would resolve it: A modified DTW metric with constraints on local slopes or segment-wise curvature showing a reduction in the specific "mismatch" failure cases identified in Figure 6.

### Open Question 3
- Question: Does the reliance on canonical chart patterns for centroid initialization restrict the discovery of novel or emergent market patterns?
- Basis in paper: The methodology explicitly seeds the SIMPC algorithm with "domain knowledge via canonical chart pattern prototypes" to overcome random initialization limitations.
- Why unresolved: It is unclear if the framework is merely validating existing technical analysis rules or if it possesses the capacity to identify new, uncharted patterns independent of these seeds.
- What evidence would resolve it: An ablation study comparing the pattern diversity and predictive performance of the seeded model against a fully unsupervised initialization on a dataset containing regime changes.

## Limitations
- **Non-stationarity Risk:** The framework assumes recurring patterns remain stable over time; if market regimes shift, fixed canonical centroids may become irrelevant.
- **Computational Cost:** The use of DTW in both clustering and triplet loss significantly increases computational complexity compared to Euclidean alternatives.
- **Confidence in Filtering:** While confidence thresholding improves AR, it substantially reduces #Trades; optimal threshold depends on transaction costs not thoroughly explored.

## Confidence
- **High Confidence:** The noise suppression mechanism (SIMPC clustering + filtering) is well-supported by experimental results and foundational work showing deep learning struggles with noisy data.
- **Medium Confidence:** The distortion-invariant representation learning (DTW-triplet loss) is logically sound but the specific value of DTW over Euclidean distance for semantic financial pattern classes is not rigorously proven.
- **Medium Confidence:** The selective forecasting mechanism (K-S test + confidence thresholding) shows profitability improvements but sensitivity to Top-x% threshold and generalizability across market conditions requires further validation.

## Next Checks
1. **Regime Shift Robustness:** Simulate non-stationary environment by training on 2014-2021 data and testing on 2022-2025 (high-volatility periods), compare performance degradation against non-pattern-based baseline.
2. **Stride Sensitivity Analysis:** Systematically vary SIMPC stride parameter (e.g., 1, 2, 4) and measure trade-off between computational cost (runtime) and clustering quality (cluster purity, silhouette score).
3. **Cross-Dataset Pattern Transfer:** Use 6 canonical chart patterns mined from S&P 500 data to initialize SIMPC for Bitcoin, assess whether equity patterns are effective for cryptocurrency or if pattern discovery should be asset-specific.