---
ver: rpa2
title: Functional embeddings enable Aggregation of multi-area SEEG recordings over
  subjects and sessions
arxiv_id: '2510.27090'
source_url: https://arxiv.org/abs/2510.27090
tags:
- functional
- neural
- across
- subjects
- subject
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: We introduce FunctionalMap, a framework that learns subject-agnostic
  functional embeddings for intracranial electrodes, enabling cross-subject aggregation
  despite varying electrode layouts. A contrastive Siamese encoder maps neural signals
  from the same brain region close together in a 32-D functional space, which is then
  tokenized and fed into a transformer for masked-region reconstruction.
---

# Functional embeddings enable Aggregation of multi-area SEEG recordings over subjects and sessions

## Quick Facts
- arXiv ID: 2510.27090
- Source URL: https://arxiv.org/abs/2510.27090
- Reference count: 40
- Functional embeddings improve cross-subject SEEG reconstruction accuracy over anatomical coordinates (p≈0.002)

## Executive Summary
This paper introduces FunctionalMap, a framework that learns subject-agnostic functional embeddings for intracranial electrodes, enabling cross-subject aggregation despite varying electrode layouts. The approach uses a contrastive Siamese encoder to map neural signals from the same brain region close together in a 32-D functional space, which is then tokenized and fed into a transformer for masked-region reconstruction. On a 20-subject multi-region SEEG dataset, the functional embeddings achieve 75.78% within-subject accuracy on held-out time segments and 45.79% on held-out channels, while joint multi-subject training reaches 80.71% and 49.18% respectively. Functional embeddings significantly improve cross-subject masked-region reconstruction accuracy over MNI coordinates (p≈0.002).

## Method Summary
The FunctionalMap framework consists of two stages: (1) A contrastive Siamese encoder learns 32-D functional embeddings by mapping neural signals from the same brain region close together in a subject-agnostic space, trained using either pairwise contrastive loss (PSC) or modified supervised contrastive loss (MSC). (2) These embeddings are projected to transformer width and used as channel identifiers in a transformer model that performs masked-region reconstruction, where all channels from a target region are withheld and reconstructed from source regions using cross-attention. The model combines MSE loss with a correlation term to prevent amplitude shrinkage.

## Key Results
- Functional embeddings achieve 75.78% within-subject accuracy on held-out time segments and 45.79% on held-out channels
- Joint multi-subject training with functional embeddings reaches 80.71% and 49.18% respectively
- Functional embeddings significantly improve cross-subject masked-region reconstruction accuracy over MNI coordinates (p≈0.002)

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** A contrastive Siamese encoder can map heterogeneous intracranial electrodes into a subject-agnostic "functional space" where proximity corresponds to physiological similarity rather than spatial proximity.
- **Mechanism:** The encoder minimizes a contrastive loss (PSC or MSC) by pulling embeddings of neural segments from the same anatomical region closer together while pushing different regions apart, effectively learning a 32-D geometry that is locality-sensitive to regional spectral signatures.
- **Core assumption:** Neural dynamics within specific brain regions share consistent, discriminative signatures across subjects that are robust to individual gain, noise, and slight frequency shifts.
- **Evidence anchors:**
  - [abstract] "A contrastive Siamese encoder maps neural signals from the same brain region close together in a 32-D functional space..."
  - [section 3.1] Describes the minimization of contrastive loss $L_{pair}$ or $L_{sup}$ to create region-specific clusters.
  - [corpus] Related work confirms that "individual physiological and electrode implantation heterogeneities" are the primary constraint this mechanism attempts to overcome.
- **Break condition:** If inter-subject variability in spectral signatures exceeds intra-region similarity, the embedding space collapses into subject-specific clusters rather than region-specific ones.

### Mechanism 2
- **Claim:** Data-driven functional embeddings provide a more reliable coordinate system for transformers than anatomical coordinates (MNI) because they capture the effective computational role of a channel.
- **Mechanism:** The transformer uses the 32-D functional embedding as a channel identifier. By attending over these "functional tokens," the model aggregates information based on signal similarity, bypassing the error introduced by MNI normalization.
- **Core assumption:** An electrode's functional identity (derived from its signal dynamics) is more predictive of its role in a neural circuit than its spatial location relative to a standardized atlas.
- **Evidence anchors:**
  - [abstract] "...functional embeddings significantly improve cross-subject masked-region reconstruction accuracy over MNI coordinates (p≈0.002)."
  - [section 4.4] Ablation study shows transformers conditioned on functional embeddings outperform MNI-conditioned models.
  - [corpus] "BaRISTA" and "BrainStratify" highlight the difficulty of modeling multi-region activity with raw coordinates.
- **Break condition:** If the functional embedding encoder fails to generalize to unseen channels, the transformer will receive noisy or meaningless positional encodings.

### Mechanism 3
- **Claim:** Masked-region reconstruction forces the model to learn high-level inter-regional dependencies rather than just auto-regressive signal properties.
- **Mechanism:** By withholding all channels from a target region and requiring reconstruction from source regions, the transformer must infer cross-region correlations. The loss function combines MSE with a correlation term to enforce waveform shape fidelity.
- **Core assumption:** Activity in a masked region is sufficiently predicted by the activity in the remaining unmasked regions (functional connectivity).
- **Evidence anchors:**
  - [section 3.2] "This task... isolates purely neural-circuit information..."
  - [section 4.4] The model achieves significant reconstruction accuracy, demonstrating it captures meaningful cross-region dynamics.
  - [corpus] "Neuroprobe" discusses evaluating intracranial responses using reconstruction tasks.
- **Break condition:** If the regions are functionally independent or if source regions lack afferent projections to the target, reconstruction degrades to the mean or a flat line.

## Foundational Learning

- **Concept:** **Contrastive Learning (Siamese Networks)**
  - **Why needed here:** This is the engine that creates the "functional coordinate system." Without understanding how to define positive (same region) and negative (different region) pairs to shape the embedding space, the entire aggregation strategy fails.
  - **Quick check question:** How does the model ensure that a "positive pair" (same region) from two different subjects maps to the same point in 32-D space?

- **Concept:** **Positional Encodings vs. Learned Embeddings**
  - **Why needed here:** The paper explicitly contrasts its "functional embeddings" with standard "MNI coordinates" (a form of positional encoding). You must understand that the paper replaces *spatial* position with *functional* identity.
  - **Quick check question:** Why would a transformer perform worse when using MNI coordinates for electrodes that are spatially close but functionally distinct?

- **Concept:** **Encoder-Decoder Transformers (Seq2Seq)**
  - **Why needed here:** The architecture uses an encoder for source channels and a decoder for target queries. Understanding cross-attention is required to see how the model "queries" the memory of observed regions to reconstruct masked ones.
  - **Quick check question:** In the masked-region reconstruction task, which part of the transformer holds the "context" of the unmasked brain regions?

## Architecture Onboarding

- **Component map:**
  Input -> Functional Encoder (1D CNN + Projection Head) -> 32-D Vector -> Tokenizer (1D Conv + Projected Embedding) -> Tokens -> Transformer (Pre-LN Encoder-Decoder) -> Head (Linear layer)

- **Critical path:**
  1. Pre-train Encoder: Train the CNN/Siamese network on the 20-subject dataset to generate the stable 32-D embeddings
  2. Freeze/Extract: Run the encoder on the dataset to assign a functional vector to every channel (token)
  3. Train Transformer: Train the masked-reconstruction model using these fixed functional tokens as channel identities

- **Design tradeoffs:**
  - PSC vs. MSC: The paper contrasts Pairwise Siamese Contrastive (Euclidean, compact clusters) vs. Modified SupCon (Hypersphere, angle-separated). MSC appears to generalize better to held-out channels (zero-shot), while PSC is slightly better for seen time segments.
  - MNI vs. Functional: Using MNI is "free" (requires only coordinates) but noisy; Functional embeddings require a pre-training stage but capture true signal similarity.

- **Failure signatures:**
  - Mode Collapse: Functional embeddings form one giant cluster or scatter randomly. Check via PCA/t-SNE plots.
  - Amplitude Shrinkage: The transformer predicts a flat line to minimize MSE. Mitigation: The paper uses a correlation loss term specifically to prevent this.
  - Overfitting to Subject: If the functional embeddings inadvertently encode subject-specific noise, cross-subject aggregation will fail to improve performance.

- **First 3 experiments:**
  1. **Sanity Check (Simulation):** Train the Siamese encoder on the simulated dataset (known signatures). Plot the 2D PCA. If it doesn't cluster by region, the hyperparameters (margin, temperature τ) are wrong.
  2. **Ablation (Coordinates):** Train two identical transformers on a small subset: one with MNI embeddings, one with Functional embeddings. Verify that Functional ≥ MNI on reconstruction correlation.
  3. **Generalization Test:** Train the functional encoder on subjects {1..N-1}, then extract embeddings for subject N (held-out). Check if these embeddings cluster correctly with the training subjects' regions (Zero-shot validation).

## Open Questions the Paper Calls Out

- Can functional embeddings be learned without relying on region labels during contrastive training?
- Do functional embeddings generalize to cortical ECoG recordings and spike train data?
- How does the functional embedding approach compare to population-level pretraining frameworks like PopT?
- Can functional coordinates and ensemble-level pretraining be combined for optimal cross-subject modeling?

## Limitations
- The framework was only tested on 3 specific brain regions (GPi, STN, VO), limiting generalizability to other regions
- Results are based on 20 subjects, which is relatively small for deep learning approaches
- The approach works at the level of brain regions rather than individual electrode contacts, limiting fine-grained circuit analysis

## Confidence
- **High Confidence:** Functional embeddings outperform MNI coordinates for cross-subject reconstruction (p≈0.002), transformer architecture can reconstruct masked regions using functional embeddings, 32-D functional space captures meaningful physiological similarities
- **Medium Confidence:** Learned embeddings are truly subject-agnostic (limited by small sample size), zero-shot generalization to held-out channels is robust (tested on one held-out subject)
- **Low Confidence:** The approach scales to arbitrary brain regions beyond the 3 studied

## Next Checks
1. **Multi-region generalization test:** Train the functional encoder on a subset of 5-6 brain regions, then evaluate zero-shot performance on 3-4 completely unseen regions to test whether the embedding space captures universal functional signatures or is region-specific.

2. **Pathology sensitivity analysis:** Apply the same framework to a multi-subject dataset with different pathologies (e.g., Parkinson's disease vs. essential tremor) to determine if functional embeddings cluster by pathology rather than anatomical region.

3. **Cross-modal validation:** Compare the learned functional embeddings against independent measures of functional connectivity (e.g., resting-state fMRI correlations) to provide convergent validation of the embedding space.