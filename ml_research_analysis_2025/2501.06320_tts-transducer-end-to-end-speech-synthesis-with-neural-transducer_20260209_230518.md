---
ver: rpa2
title: 'TTS-Transducer: End-to-End Speech Synthesis with Neural Transducer'
arxiv_id: '2501.06320'
source_url: https://arxiv.org/abs/2501.06320
tags:
- audio
- speech
- neural
- transducer
- codes
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces TTS-Transducer, a novel text-to-speech system
  that leverages neural transducers to learn monotonic alignments between text and
  discrete audio codes, avoiding explicit duration prediction. The architecture uses
  a neural transducer to predict codes from the first codebook and a non-autoregressive
  Transformer to iteratively predict the remaining residual codes, trained end-to-end.
---

# TTS-Transducer: End-to-End Speech Synthesis with Neural Transducer

## Quick Facts
- arXiv ID: 2501.06320
- Source URL: https://arxiv.org/abs/2501.06320
- Reference count: 40
- This paper introduces TTS-Transducer, a novel text-to-speech system that leverages neural transducers to learn monotonic alignments between text and discrete audio codes, avoiding explicit duration prediction.

## Executive Summary
This paper introduces TTS-Transducer, a novel text-to-speech system that leverages neural transducers to learn monotonic alignments between text and discrete audio codes, avoiding explicit duration prediction. The architecture uses a neural transducer to predict codes from the first codebook and a non-autoregressive Transformer to iteratively predict the remaining residual codes, trained end-to-end. The system achieves strong results: 3.94% character error rate on challenging texts, outperforming larger models trained on more data, and competitive naturalness scores (3.83 MOS) while demonstrating superior robustness and codec-agnostic generalization.

## Method Summary
TTS-Transducer employs a two-component architecture: an RNN-T predicts the first codebook codes while a non-autoregressive Residual Codebook Head (RCH) iteratively predicts remaining residual codes. The system uses an RNNT with an encoder (12-layer Transformer), prediction network (6-layer Transformer-decoder), and joint network to predict first codebook tokens. The RNNT lattice is used to extract monotonic alignments, which are then distributed to encoder frames and fed to the RCH (12-layer Transformer) for residual codebook prediction. Training is end-to-end with weighted loss combining RNNT and CE losses, and speaker conditioning is achieved through a GST module. The approach avoids explicit duration prediction while maintaining alignment quality.

## Key Results
- Achieves 3.94% character error rate on challenging texts, outperforming larger models trained on more data
- Competitive naturalness scores of 3.83 MOS while demonstrating superior robustness
- Successfully works across multiple codecs (EnCodec, NeMo-Codec, DAC) without architecture changes
- IPA tokenization yields 25% relative reduction in CER compared to BPE (3.73% vs 4.90% WER on seen speakers)

## Why This Works (Mechanism)

### Mechanism 1: Monotonic Alignment via Transducer Lattice Constraint
The RNNT architecture inherently enforces monotonic alignment between text and audio, eliminating the need for explicit duration predictors while preventing hallucination and word skipping common in autoregressive TTS. The transducer's lattice-based loss computes probabilities over all valid alignment paths where text tokens must be consumed in order, with `<blank>` tokens acting as frame delimiters. The nested loop decoding (outer loop over encoder frames, inner loop emitting labels until `<blank>`) ensures text order is preserved. Core assumption: Speech production is fundamentally monotonic—phonemes are produced in the same order as text tokens.

### Mechanism 2: Alignment Extraction and Cross-Component Transfer
The alignment extracted from the RNNT lattice during first-codebook prediction can be reused to align encoder representations for all residual codebook predictions, avoiding redundant alignment learning. Using k2 framework's `shortest_path` on the computed RNNT lattice yields the most probable alignment path. Encoder output frames are distributed according to this alignment, then concatenated with previous codebook embeddings as input to the Residual Codebook Head. Core assumption: The temporal alignment between text and audio is consistent across all codebooks—what aligns for codebook 0 aligns for codebooks 1 through n.

### Mechanism 3: Hierarchical Codebook Prediction with Memory-Efficient Factorization
Separating first-codebook prediction (transducer) from residual codebooks (NAR transformer) reduces memory complexity from O(text_len × audio_len × num_codebooks) to O(text_len × audio_len), making end-to-end training tractable. The first codebook captures primary acoustic structure and requires alignment learning. Residual codebooks refine quality and can be predicted non-autoregressively given aligned context. The RCH iterates: for codebook i, input = embeddings[codes 0→i-1] + aligned encoder output. Core assumption: Residual codebooks contain refinement information that is locally predictable once alignment and coarse structure are established.

## Foundational Learning

- **Neural Transducers (RNNT)**
  - Why needed here: Core alignment mechanism; must understand encoder-predictor-joint network flow, blank token semantics, and lattice-based training to debug alignment issues.
  - Quick check question: In RNNT decoding, what happens when the joint network outputs a `<blank>` token vs. a vocabulary token?

- **Residual Vector Quantization (RVQ) in Neural Audio Codecs**
  - Why needed here: Understanding why codecs produce 8-9 codebooks per frame and what information each level captures is essential for interpreting RCH behavior.
  - Quick check question: In EnCodec with 8 codebooks, what type of information is typically encoded in the first codebook vs. later residual codebooks?

- **WFST-based Transducer Implementation (k2 framework)**
  - Why needed here: Alignment extraction depends on understanding how to traverse lattices; the paper uses k2's shortest_path for this.
  - Quick check question: How would you extract the most probable alignment path from an RNNT lattice, and what does each lattice edge represent?

## Architecture Onboarding

- **Component map:**
  - Text tokens + speaker embedding → Text Encoder → e_i
  - First codebook c_0 + `<SOS>` → Prediction Network → p_j
  - e_i + p_j → Joint Network → logits for c_0 tokens + `<blank>`
  - Compute RNNT loss; extract alignment via k2.shortest_path on lattice
  - Distribute encoder frames according to alignment
  - For i in [1, n]: [embeddings for c_0→c_{i-1}] + aligned encoder output → RCH → predict c_i
  - All predicted codes [c_0, c_1, ..., c_n] → Codec Decoder → audio

- **Critical path:**
  1. Text tokens + speaker embedding → Text Encoder → e_i
  2. First codebook c_0 + `<SOS>` → Prediction Network → p_j
  3. e_i + p_j → Joint Network → logits for c_0 tokens + `<blank>`
  4. Compute RNNT loss; extract alignment via k2.shortest_path on lattice
  5. Distribute encoder frames according to alignment
  6. For i in [1, n]: [embeddings for c_0→c_{i-1}] + aligned encoder output → RCH → predict c_i
  7. All predicted codes [c_0, c_1, ..., c_n] → Codec Decoder → audio

- **Design tradeoffs:**
  - **Tokenization**: IPA yields better intelligibility (3.73% vs 4.90% WER on seen speakers with EnCodec); BPE may preserve speaker similarity better
  - **Codec selection**: EnCodec and NeMo-Codec perform comparably; NeMo-Codec + IPA achieves best CER (3.94% on challenging texts)
  - **Model scaling**: Encoder layers (6→12) and RCH layers (6→12) both improve intelligibility; prediction network depth less impactful
  - **Loss weighting**: α=0.4 for CE loss (residual), 0.6 for RNNT loss (first codebook)—tune if residual quality lags

- **Failure signatures:**
  - High WER/CER on challenging/long texts → Check alignment extraction; switch from BPE to IPA; verify nucleus sampling p=0.95 isn't introducing noise
  - Low speaker similarity (SSIM) → Verify GST reference audio is 3-5 seconds of clean speech; check speaker embedding conditioning
  - Word skipping or repetition → Should be prevented by transducer; verify `<blank>` token handling in decoding loop
  - Audio artifacts in high frequencies → Check codec quality (DAC vs EnCodec); residual codebook predictions may need more RCH layers

- **First 3 experiments:**
  1. **Baseline replication**: Train EnCodec + BPE model with default config (12/6/12 layers, batch 2048, 200 epochs); evaluate CER/WER/SSIM on LibriTTS-R held-out set to confirm reproducibility
  2. **Tokenization ablation**: Same config with IPA tokens; compare CER improvement (expect ~25% relative reduction) and any SSIM changes
  3. **Codec-agnostic validation**: Train identical models with EnCodec, NeMo-Codec, and DAC; verify performance variance is <10% to confirm codec portability claim

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but several remain implicit in the work.

## Limitations
- The IPA tokenization pipeline details are unspecified (g2p system, phoneme set, token granularity)
- The GST speaker embedding architecture specifics are not fully detailed (conv filter sizes, LSTM hidden dimensions)
- The alignment distribution mechanism lacks precise implementation guidance for handling many-to-one mappings between frames and tokens

## Confidence
- **High confidence** in the monotonic alignment mechanism (RNNT architecture is well-established with clear lattice constraints)
- **Medium confidence** in the end-to-end training claims (implementation details for alignment transfer are incomplete)
- **Medium confidence** in codec-agnostic generalization (only three codecs tested, limited ablation on codec hyperparameters)
- **Low confidence** in exact reproduction without resolving the unknown architectural parameters

## Next Checks
1. **Alignment quality verification**: Implement a visualization tool to display the RNNT lattice alignment matrix between text tokens and audio frames during training. Verify monotonic diagonal patterns emerge within 10k steps and check for pathological alignments (jumps, loops) that would indicate transducer training issues.

2. **Speaker embedding ablation**: Train models with and without GST conditioning, and with different reference audio durations (1s, 3s, 5s). Measure impact on Speaker Similarity scores to validate the claimed contribution of GST to speaker preservation and identify minimum effective reference duration.

3. **Codec codebook information analysis**: For each codec (EnCodec, NeMo-Codec, DAC), analyze the mutual information content across codebooks using entropy measurements. Verify that first codebook captures the majority of information as assumed by the architecture, and that residual codebooks provide meaningful refinement rather than redundant encoding.