---
ver: rpa2
title: Upcycling Text-to-Image Diffusion Models for Multi-Task Capabilities
arxiv_id: '2503.11905'
source_url: https://arxiv.org/abs/2503.11905
tags:
- image
- diffusion
- tasks
- multi-task
- input
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes Multi-Task Upcycling (MTU), a method to extend
  a pre-trained text-to-image diffusion model into a multi-task image-generation generalist
  without increasing computational cost. MTU replaces Feed-Forward Network (FFN) layers
  with smaller FFN experts and combines them using a dynamic routing mechanism.
---

# Upcycling Text-to-Image Diffusion Models for Multi-Task Capabilities

## Quick Facts
- arXiv ID: 2503.11905
- Source URL: https://arxiv.org/abs/2503.11905
- Reference count: 25
- One-line primary result: MTU extends pre-trained T2I diffusion models to 4 tasks with same latency/compute as single-task models

## Executive Summary
This paper proposes Multi-Task Upcycling (MTU), a method to extend pre-trained text-to-image diffusion models into multi-task image-generation generalists without increasing computational cost. MTU replaces Feed-Forward Network (FFN) layers with smaller FFN experts and combines them using a dynamic routing mechanism. The approach is motivated by empirical observation that FFN layers undergo the most significant changes during task-specific fine-tuning. Evaluated on SDv1.5 and SDXL across tasks including text-to-image generation, image editing, super-resolution, and inpainting, MTU achieves comparable performance to single-task models while maintaining similar latency and computational load.

## Method Summary
MTU upcycles pre-trained text-to-image diffusion models by replacing FFN layers with N smaller FFN experts initialized by sharding original FFN weights. A router network computes task-specific weights from learnable task embeddings, which are pre-computed once per task and applied to combine expert outputs via weighted sum. Task-specific layer normalization and input convolutions are added before experts. Only experts, router, task-specific layers, and task embeddings are trained; the backbone remains frozen. The method enables multi-task capability while maintaining the same computational cost as the original single-task model.

## Key Results
- SDv1.5 achieves FID of 7.2 on text-to-image generation with N=4 experts
- SDXL achieves FID of 3.9 on text-to-image generation with N=1 expert
- Performance matches single-task models across all tasks (IE, SR, IP)
- Computational cost remains similar to pre-trained model (1.54 TFLOPs for SDXL)

## Why This Works (Mechanism)

### Mechanism 1: FFN Layer Task Specialization
- Claim: FFN layers undergo larger deviations than attention layers during fine-tuning
- Evidence: Figure 2 and Table 1 show FFNs show highest parameter deviation during fine-tuning
- Break condition: If attention layers showed equal or greater deviation

### Mechanism 2: Task-Embedding Routed Expert Combination
- Claim: Pre-computed expert weights enable multi-task capability without per-token routing overhead
- Evidence: Section 5 defines weight computation and notes pre-computation is minimal
- Break condition: If routing required per-token computation

### Mechanism 3: Task-Specific Layer Normalization
- Claim: Task-specific layer norms improve performance by aligning different activation distributions
- Evidence: Section 5 notes FFN layers exhibit different distributions across tasks
- Break condition: If shared layer normalization achieved equivalent performance

## Foundational Learning

- **Mixture of Experts (MoE)**
  - Why needed: MTU adapts MoE concepts (sparse expert combination, routing) to diffusion models with task-embedding twist
  - Quick check: What is the key difference between standard MoE token-routing and MTU's task-embedding routing?

- **Latent Diffusion Models (LDMs)**
  - Why needed: The method operates on latent representations; understanding encoder/decoder pipeline is essential
  - Quick check: Where does image conditioning (zc) concatenate with noisy latents in I2I training?

- **Transfer Learning Dynamics**
  - Why needed: Core motivation rests on observing which parameters change during fine-tuning
  - Quick check: Does the paper prove FFN deviation causes task adaptation, or only demonstrate correlation?

## Architecture Onboarding

- **Component map:**
  Frozen backbone (attention, residual blocks) -> N experts (d_expert = d_FFN/N) -> Router (2-layer MLP + softmax) -> Task-specific layer norms -> Task-specific input convolutions

- **Critical path:**
  1. Initialize experts by sharding pre-trained FFN weights
  2. Add router networks, task embeddings, task-specific layers (random initialization)
  3. Freeze all backbone parameters; train only experts, routers, task norms, input convs
  4. Train multi-task objective: Σ_τ E[∥ϵ - f_θ(ΨC(z_t^τ), c_T^τ, E(c_I^τ), t)∥]
  5. Inference: Pre-compute expert weights from task embedding once, apply to all tokens

- **Design tradeoffs:**
  - Expert count: SDv1.5 optimal at N=4; SDXL optimal at N=1 (no splitting beneficial)
  - Top-k vs. full combination: Simple full combination outperformed top-k selection
  - Model scale dependency: Larger models may not need expert splitting

- **Failure signatures:**
  - Router collapse: Single expert dominates - check task embedding learning
  - Performance degradation: Expert dimension likely too small - reduce N
  - Training instability: Apply weight decay=0.01 for SDXL

- **First 3 experiments:**
  1. Establish single-task baselines by fine-tuning separate models for IE, SR, IP
  2. Replicate FFN deviation analysis - compute Φτ = ||θτ_f - θp|| for FFN/SA/CA
  3. Test expert count ablation starting with N=1, then N=2, 4, 8

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does optimal number of FFN experts scale inversely with base model parameter count?
- Basis: Section 8 shows SDv1.5 (860M) optimal at N=4, SDXL (2.6B) optimal at N=1
- Evidence: Systematic study across wider range of model sizes (SDv2.1, SD3-medium, SD3-large)

### Open Question 2
- Question: How does router efficiency degrade when scaling to >10 tasks?
- Basis: Paper validates on 4 tasks but claims multi-task capability
- Evidence: Empirical evaluation on benchmark with 10+ distinct image generation tasks

### Open Question 3
- Question: Can MTU be successfully applied to Diffusion Transformer architectures?
- Basis: Introduction identifies DiT as SOTA but experiments confined to UNet-based LDMs
- Evidence: Apply mechanism to DiT backbone (PixArt, SD3) and compare convergence

### Open Question 4
- Question: Does sharding pre-trained weights provide convergence advantage over random initialization?
- Basis: Section 5 describes weight sharding but doesn't ablate against random initialization
- Evidence: Ablation comparing convergence rates and final FID/LPIPS scores

## Limitations
- Model scale dependency unclear: Optimal expert count varies dramatically with model size (N=4 vs N=1)
- Limited task scalability: Validated on only 4 tasks, scalability to many tasks unverified
- Evaluation granularity lacking: Multi-task performance reported as aggregate metrics without per-task breakdowns

## Confidence

**High Confidence** (Multiple empirical validations, clear methodology):
- FFN layers undergo larger deviations than attention layers during fine-tuning
- Task-specific layer normalization improves performance
- MTU achieves comparable single-task performance to specialized models

**Medium Confidence** (Single experiments, limited ablation):
- Expert count of N=4 optimal for SDv1.5
- Pre-computed routing weights eliminate per-token overhead
- SDXL benefits from weight decay=0.01

**Low Confidence** (No direct validation, theoretical claims):
- Assumption that FFN deviation magnitude correlates with functional importance
- Router design choice of 2-layer MLP optimal
- Task embedding routing superior to alternative multi-task approaches

## Next Checks

1. **Router Design Ablation**: Systematically vary router MLP depth (1-3 layers) and hidden dimension while measuring multi-task performance to validate optimal architecture.

2. **Task Trade-off Analysis**: Train models with imbalanced task sampling ratios (e.g., 10:1 T2I:IE) and measure per-task performance degradation to reveal balanced capability.

3. **Larger Model Scaling Test**: Apply MTU to a model between SDv1.5 and SDXL (1.5B parameters) to identify inflection point where expert splitting becomes unnecessary and establish scaling principles.