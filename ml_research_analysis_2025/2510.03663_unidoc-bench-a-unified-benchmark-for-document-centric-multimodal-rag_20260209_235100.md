---
ver: rpa2
title: 'UNIDOC-BENCH: A Unified Benchmark for Document-Centric Multimodal RAG'
arxiv_id: '2510.03663'
source_url: https://arxiv.org/abs/2510.03663
tags:
- question
- text
- answer
- retrieval
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces UniDoc-Bench, a unified benchmark for multimodal
  retrieval-augmented generation (MM-RAG) that addresses limitations in current evaluation
  frameworks. The benchmark is built from 70k real-world PDF pages across 8 domains,
  with 1,600 human-verified QA pairs covering text, tables, and figures.
---

# UNIDOC-BENCH: A Unified Benchmark for Document-Centric Multimodal RAG

## Quick Facts
- arXiv ID: 2510.03663
- Source URL: https://arxiv.org/abs/2510.03663
- Authors: Xiangyu Peng; Can Qin; Zeyuan Chen; Ran Xu; Caiming Xiong; Chien-Sheng Wu
- Reference count: 40
- Primary result: Text-image fusion retrieval outperforms both unimodal and joint multimodal retrieval, achieving 68.4% completeness versus 64.1% for joint retrieval and 65.3% for text-only retrieval

## Executive Summary
This paper introduces UniDoc-Bench, a unified benchmark for multimodal retrieval-augmented generation (MM-RAG) that addresses limitations in current evaluation frameworks. The benchmark is built from 70k real-world PDF pages across 8 domains, with 1,600 human-verified QA pairs covering text, tables, and figures. The authors develop a high-quality data synthesis pipeline that extracts and links multimodal evidence, generates diverse question types (factual retrieval, comparison, summarization, logical reasoning), and ensures quality through multi-annotator validation. Experiments comparing four paradigms—text-only, image-only, multimodal text-image fusion, and multimodal joint retrieval—show that text-image fusion RAG systems consistently outperform both unimodal and jointly multimodal embedding-based retrieval, achieving 68.4% completeness versus 64.1% for joint retrieval and 65.3% for text-only retrieval. The study also reveals that image-dependent queries remain challenging across all systems, suggesting future RAG improvements should prioritize multimodal understanding.

## Method Summary
The authors developed a comprehensive pipeline for creating UniDoc-Bench, starting with PDF parsing using unstructured.io to extract text chunks, figures, and tables from 70k pages across 8 domains. They constructed knowledge graphs linking chunks via overlapping entities, then used template-based QA synthesis with GPT-4.1 and Gemini-Pro-2.5, followed by human verification from five annotators. The benchmark evaluates four retrieval paradigms: text-only, image-only, multimodal text-image fusion (separate top-k retrieval), and multimodal joint embedding retrieval. For each paradigm, they use domain-specific retrieval embeddings (text-embedding-3-small, ColQwen 2.5-v0.2, GME-Qwen2-VL-7B-Instruct, voyage-multimodal-3) and generate responses with multiple LLMs (GPT-4.1, Claude-4.5-sonnet, Gemini-2.5-pro, GPT-5). Evaluation uses LLM-based faithfulness and completeness metrics across 1,600 human-verified QA pairs spanning four question types and four evidence types.

## Key Results
- Text-image fusion retrieval achieves 68.4% completeness, outperforming text-only (65.3%) and joint multimodal retrieval (64.1%)
- Image-only retrieval shows highest recall (0.821) but lowest precision (0.166), indicating coverage without accuracy
- Image-dependent queries remain challenging across all systems, with completeness ranging from 0.486-0.619
- Table-required queries show significant text-only retrieval limitations (0.392 for IMG baseline) due to parsing constraints

## Why This Works (Mechanism)

### Mechanism 1
Text-image fusion retrieval outperforms joint multimodal embeddings because specialized unimodal encoders capture modality-specific signals better than unified embeddings. Text embeddings achieve higher precision (0.273) for textual nuance; image embeddings achieve higher recall (0.821) for broad page coverage. Late fusion at top-k selection preserves these complementary strengths before generation. Core assumption: The generator (multimodal LLM) can effectively reconcile separately retrieved text and image candidates during reasoning. Break condition: If unified multimodal embeddings substantially improve in cross-modal alignment, the fusion advantage may narrow.

### Mechanism 2
Image-dependent queries remain challenging because current VLMs struggle to extract fine-grained numerical and spatial information from charts, diagrams, and tables presented as images. Image-only retrieval captures broader page context but loses precision; even with retrieval success, the generator must parse axes, colors, and spatial encodings that OCR would have flattened—introducing interpretation error. Core assumption: The gap is primarily in visual understanding capacity, not just retrieval coverage. Break condition: If VLMs achieve near-human chart/table parsing accuracy with robust spatial reasoning, this mechanism weakens.

### Mechanism 3
Knowledge-graph-based evidence linking improves QA synthesis quality by grounding questions in multi-hop, cross-document, and cross-modal evidence paths. Parsing extracts text, tables, and figures with placeholder tags; a knowledge graph links chunks via overlapping entities; templates guide LLM synthesis; human verification filters low-quality pairs. Core assumption: The entity-overlap graph captures meaningful semantic relationships for realistic RAG queries. Break condition: If KG construction introduces spurious links or misses domain-specific relationships, synthesized questions may become artificially easy or unanswerable.

## Foundational Learning

- **RAG (Retrieval-Augmented Generation)**: Why needed here—MM-RAG extends classical RAG to document-centric, cross-modal retrieval; understanding the baseline architecture (retrieve → generate) is prerequisite to grasping fusion vs joint retrieval tradeoffs. Quick check: Can you explain why retrieval quality (precision/recall) directly affects generation faithfulness in RAG systems?

- **Embedding space alignment**: Why needed here—the paper contrasts unimodal embeddings (separate text and image spaces) with joint multimodal embeddings (unified space); misalignment leads to recall loss or precision degradation. Quick check: Why might a jointly trained text-image embedding underperform compared to fusing two strong unimodal embeddings at retrieval time?

- **Cross-modal grounding**: Why needed here—the benchmark evaluates whether answers are correctly grounded in text, images, or tables; this requires understanding how evidence is linked across modalities. Quick check: Given a question requiring both text and image evidence, how would you design an evaluation protocol to verify cross-modal grounding?

## Architecture Onboarding

- **Component map**: PDF parsing → text chunks, figures, tables → Knowledge graph linking via entity overlap → Template-based QA synthesis → Human verification → Four retrieval pipelines (Text, Image, MM-joint, T+I fusion) → Multimodal LLM generation

- **Critical path**: 1. Chunk parsing quality determines upper bound for retrieval 2. Embedding selection directly controls precision/recall tradeoff 3. Fusion strategy determines whether complementary strengths are preserved

- **Design tradeoffs**: Text retrieval offers higher precision but lower recall; image retrieval provides higher recall but lower precision; joint MM embedding simplifies pipeline but underperforms fusion; T+I fusion achieves best completeness (0.654) but highest latency (9.4s vs 7.3s for text-only)

- **Failure signatures**: Image-only retrieval shows high recall but generator cannot extract fine-grained facts from page images; MM joint retrieval underperforms text-only in certain domains suggesting embedding contamination; table-required queries show text-only struggles due to parsing constraints

- **First 3 experiments**: 1. Replicate retrieval comparison on held-out domain to validate generalization 2. Ablate top-k for T+I fusion to optimize balance for document distributions 3. Substitute GME/voyage with alternative joint embeddings to test fusion advantage persistence

## Open Questions the Paper Calls Out
None

## Limitations
- The synthesis pipeline's reliance on knowledge graphs and LLM templates may produce QA pairs that are artificially coherent but not fully representative of real-world retrieval complexity
- The benchmark's domain coverage (8 domains, 70k pages) may not capture edge cases in highly specialized document formats or languages beyond English
- The paper does not address computational costs or latency trade-offs in production deployment, particularly for T+I fusion which shows the highest completeness but longest generation time

## Confidence
- **High confidence**: Text-image fusion superiority over joint embeddings and unimodal baselines, supported by retrieval precision/recall metrics and completeness scores across multiple domains
- **Medium confidence**: The mechanism that unified embeddings underperform due to modality-specific signal loss, as this is supported by retrieval metrics but not definitively proven as the root cause versus other architectural factors
- **Medium confidence**: Image-dependent queries remain challenging as a universal RAG limitation, though this is based on benchmark results that may not generalize to all document types or VLM capabilities

## Next Checks
1. **Domain generalization test**: Apply the same retrieval comparison to a completely different document corpus (e.g., medical patents or scientific papers) to validate whether the fusion advantage persists outside the 8 benchmark domains
2. **Embedding ablation study**: Systematically replace the GME/voyage joint embeddings with alternative multimodal embeddings (e.g., VLM2Vec-V2 or BLIP-2 variants) to determine if the fusion advantage is specific to current embedding architectures or represents a fundamental retrieval paradigm
3. **Visual understanding capability test**: Conduct controlled experiments where VLMs are tested on isolated chart/table interpretation tasks (without retrieval) to separate whether image-dependent query failures stem from retrieval coverage versus visual comprehension limitations