---
ver: rpa2
title: 'MPCC: A Novel Benchmark for Multimodal Planning with Complex Constraints in
  Multimodal Large Language Models'
arxiv_id: '2507.23382'
source_url: https://arxiv.org/abs/2507.23382
tags:
- planning
- multimodal
- constraints
- tasks
- plan
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "MPCC introduces the first benchmark for multimodal planning with\
  \ complex constraints, addressing the gap in evaluating real-world planning under\
  \ multimodal conditions. It defines three constraint types\u2014budget, temporal,\
  \ and spatial\u2014and applies them to three tasks: Flight Planning, Calendar Planning,\
  \ and Meeting Planning at three difficulty levels."
---

# MPCC: A Novel Benchmark for Multimodal Planning with Complex Constraints in Multimodal Large Language Models

## Quick Facts
- arXiv ID: 2507.23382
- Source URL: https://arxiv.org/abs/2507.23382
- Authors: Yiyan Ji; Haoran Chen; Qiguang Chen; Chengyue Wu; Libo Qin; Wanxiang Che
- Reference count: 40
- Key outcome: Introduces first benchmark for multimodal planning with complex constraints, revealing low performance across 13 MLLMs on real-world planning tasks

## Executive Summary
MPCC addresses the gap in evaluating Multimodal Large Language Models (MLLMs) on real-world planning under multimodal conditions. The benchmark introduces three constraint types—budget, temporal, and spatial—applied to three planning tasks at three difficulty levels. Evaluations reveal that current MLLMs struggle significantly with constraint satisfaction, achieving only 21.3% feasible plans for closed-source models and below 11% for open-source models. Performance degrades sharply as constraint complexity increases, and traditional multimodal prompting strategies fail under multi-constraint scenarios.

## Method Summary
MPCC evaluates MLLMs on three planning tasks: Flight Planning, Calendar Planning, and Meeting Planning, using 2,700 questions and 6,300 synthetic images mimicking real-world interfaces. The benchmark introduces composite constraints (Budget, Temporal, Spatial) and evaluates performance through Feasible Plan Rate and Optimal Plan Rate. The study tests zero-shot inference on 13 MLLMs using Direct prompting, Chain-of-Thought, and Plan-and-Solve strategies, with difficulty levels calibrated to separate constraint complexity from search space expansion.

## Key Results
- Closed-source models achieve only 21.3% feasible plans while open-source models average below 11%
- Performance drops sharply as constraint complexity increases across all task types
- Traditional multimodal prompting strategies like Chain-of-Thought fail under multi-constraint scenarios
- Smaller models exhibit strong planning bias, showing restricted output patterns under complex constraints
- In-Context Learning with text-only demonstrations proves ineffective for multimodal planning tasks

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Multimodal planning creates compounding reasoning demands that current MLLMs struggle to integrate
- **Mechanism:** The benchmark enforces joint visual-textual constraint satisfaction through human filtering that removes instances solvable from a single modality
- **Core assumption:** The filtering process successfully enforces multimodal dependency rather than allowing shortcut solutions from one modality alone
- **Evidence anchors:** "MLLMs are highly sensitive to constraint complexity and that traditional multimodal prompting strategies fail in multi-constraint scenarios"; human pre-labeling filters out single-modality cases
- **Break condition:** If models can achieve comparable performance using only extracted text without visual context

### Mechanism 2
- **Claim:** Separating constraint complexity from search space size through graded difficulty levels allows targeted assessment of reasoning limitations
- **Mechanism:** Difficulty levels control scenario parameters while maintaining constraint structure, enabling distinction between failures from combinatorial explosion versus constraint satisfaction logic
- **Core assumption:** The calibration successfully creates comparable search space expansions across task types
- **Evidence anchors:** "graded difficulty levels (EASY, MEDIUM, HARD) to separate constraint complexity from search space expansion"; performance declines with increasing constraint complexity while search space grows
- **Break condition:** If performance degradation correlates more strongly with search space size than with constraint count

### Mechanism 3
- **Claim:** Dual-metric evaluation reveals models can satisfy constraints without effectively exploring solution space for cost-minimizing options
- **Mechanism:** Feasible plans require meeting all constraints; optimal plans additionally minimize budget, exposing whether models engage in systematic search versus generating first satisficing solution
- **Core assumption:** Budget minimization serves as reliable proxy for general optimization capability in multimodal planning contexts
- **Evidence anchors:** "A feasible plan satisfying all such requirements... A plan minimizing budget is considered an optimal plan"; scaling up MLLMs improves constraint balance but optimal plan improvements are less clear
- **Break condition:** If models achieve high optimal plan rates through heuristic shortcuts rather than systematic search

## Foundational Learning

- **Concept: Constraint Satisfaction Problems (CSP)**
  - **Why needed here:** The benchmark formalizes planning as satisfying budget, temporal, and spatial constraints simultaneously
  - **Quick check question:** Can you explain why finding any feasible solution differs fundamentally from finding an optimal solution in a CSP with multiple constraint types?

- **Concept: Multimodal Information Fusion**
  - **Why needed here:** MPCC explicitly requires joint reasoning across visual interfaces and textual constraint specifications
  - **Quick check question:** What specific information must be extracted from a flight schedule image that isn't present in the textual constraint description?

- **Concept: Search Space Complexity**
  - **Why needed here:** The benchmark's difficulty scaling (27 to 617 average search space) and sparse feasible regions create conditions where exhaustive search is impractical
  - **Quick check question:** If a search space contains 617 possible plans but only ~38 are feasible, what does this suggest about the exploration strategy required?

## Architecture Onboarding

- **Component map:** Input Layer: Visual interface images + Textual constraints → Extraction Layer: OCR/visual understanding → Reasoning Layer: Constraint tracking across modalities → Planning Layer: Solution generation under composite constraints → Evaluation Layer: Feasibility and optimality check
- **Critical path:** Start with Flight Planning EASY tasks to establish baseline extraction accuracy, then progress to Meeting Planning HARD to diagnose multi-constraint integration failures
- **Design tradeoffs:** Synthetic data enables controlled difficulty scaling but may not capture real-world interface variability; budget-focused optimization provides clear metric but may not generalize to other objectives
- **Failure signatures:** High OCR accuracy with low feasible plan rate indicates constraint integration failure; performance below uniform random baseline suggests systematic bias; improvement in feasible plans without corresponding optimal plan improvement suggests satisficing without search
- **First 3 experiments:**
  1. Visual ablation: Convert all visual inputs to structured text and compare performance delta
  2. Constraint increment: Test models on single-constraint versions before composite constraints
  3. Bias analysis: Compute Simpson's Diversity Index on model outputs for HARD tasks

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can prompting strategies be redesigned to remain effective under multi-constraint scenarios where current methods like Chain-of-Thought and Plan-and-Solve show diminishing returns?
- **Basis in paper:** "Traditional multimodal prompting strategies fail in multi-constraint scenarios" and CoT advantages narrow to 2.3% or become negative as complexity grows
- **Why unresolved:** Current prompting methods cannot handle the combinatorial explosion of intersecting budget, temporal, and spatial constraints
- **What evidence would resolve it:** New prompting approaches that maintain or improve performance gains as constraint count increases

### Open Question 2
- **Question:** How can in-context learning be adapted to effectively transfer multimodal planning strategies when text-only demonstrations prove counterproductive?
- **Basis in paper:** ICL negatively impacts most closed-source models and performance declines significantly as example shots increase
- **Why unresolved:** The cross-modal gap between text demonstrations and multimodal reasoning tasks prevents effective strategy transfer
- **What evidence would resolve it:** ICL variants that show positive transfer with increasing shots, particularly for Calendar Planning EASY

### Open Question 3
- **Question:** How can reasoning MLLMs be designed to avoid "overthinking" or rigid reasoning patterns when confronting complex constraints?
- **Basis in paper:** Complex constraints can hinder MLLMs' reasoning through over-focusing on specific constraints and premature termination from internal contradiction
- **Why unresolved:** Current reasoning mechanisms lack flexibility to balance depth of reasoning against constraint complexity
- **What evidence would resolve it:** Reasoning models that match or exceed their low-constraint performance gains on high-constraint tasks

### Open Question 4
- **Question:** What architectural or training interventions could reduce the planning bias observed in smaller MLLMs under complex constraints?
- **Basis in paper:** Smaller models exhibit strong planning bias and greater bias in unsolvable cases, linked to limited reasoning capacity
- **Why unresolved:** It is unclear whether bias stems from training data, model capacity, or architectural constraints
- **What evidence would resolve it:** Small models showing reduced Simpson's Concentration Index and increased diversity in responses on HARD tasks after targeted interventions

## Limitations
- Relies on synthetic data generation rather than real-world interfaces, potentially limiting ecological validity
- Human pre-labeling process for filtering multimodal dependency cases is not fully specified, raising questions about consistency
- Budget minimization proxy for general optimization may not capture broader planning capabilities

## Confidence
- **High Confidence:** Performance degradation patterns across difficulty levels and ineffectiveness of traditional multimodal prompting strategies
- **Medium Confidence:** Claim that constraint integration creates compounding reasoning demands could benefit from additional ablation studies
- **Low Confidence:** Separation between constraint complexity and search space expansion may not be clean enough for diagnostic utility

## Next Checks
1. Conduct visual ablation studies converting all visual inputs to structured text to quantify multimodal integration contribution
2. Perform constraint increment experiments testing single-constraint versions before composite constraints
3. Analyze model output diversity using Simpson's Diversity Index for HARD tasks to quantify smaller model bias