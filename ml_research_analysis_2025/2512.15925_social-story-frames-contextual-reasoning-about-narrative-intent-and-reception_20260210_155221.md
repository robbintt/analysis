---
ver: rpa2
title: 'Social Story Frames: Contextual Reasoning about Narrative Intent and Reception'
arxiv_id: '2512.15925'
source_url: https://arxiv.org/abs/2512.15925
tags:
- story
- about
- social
- narrative
- author
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces SOCIALSTORYFRAMES, a formalism and modeling
  pipeline for reasoning about narrative intent and reader response in online social
  media stories. The authors define a 10-dimensional taxonomy of reader responses
  grounded in narrative theory, pragmatics, and psychology, covering author-oriented
  inferences, explanatory/predictive reasoning, affective responses, and value judgments.
---

# Social Story Frames: Contextual Reasoning about Narrative Intent and Reception

## Quick Facts
- arXiv ID: 2512.15925
- Source URL: https://arxiv.org/abs/2512.15925
- Reference count: 40
- One-line primary result: Introduces SOCIALSTORYFRAMES, a formalism and modeling pipeline for reasoning about narrative intent and reader response in online social media stories.

## Executive Summary
This paper introduces SOCIALSTORYFRAMES, a formalism and modeling pipeline for reasoning about narrative intent and reader response in online social media stories. The authors define a 10-dimensional taxonomy of reader responses grounded in narrative theory, pragmatics, and psychology, covering author-oriented inferences, explanatory/predictive reasoning, affective responses, and value judgments. Using model distillation from GPT-4o and GPT-4.1, they train two models: SSF-GENERATOR to generate contextually plausible inferences about reader response, and SSF-CLASSIFIER to map these onto fine-grained taxonomy subdimensions. Both models are validated through human surveys (N=382) and expert annotations, respectively. Applied to SSF-CORPUS, a dataset of 6,140 stories across diverse Reddit communities, the models enable scalable analysis of narrative practices, revealing patterns in storytelling intents, their associations with overall goals, and community-level diversity. The authors also introduce ssf-sim, a narrative similarity measure based on reader response rather than semantic content, demonstrating its ability to uncover cross-community narrative patterns beyond topical overlap.

## Method Summary
The authors construct SSF-CORPUS by filtering Reddit stories from ConvoKit's REDDIT-CORPUS-SMALL using StorySeeker classifier (≥0.7 probability, ≥175 chars) and removing toxic/explicit content via Perspective API (≥0.5 threshold). They train two models via LoRA fine-tuning on Llama-3.1-8B-Instruct: SSF-GENERATOR (1 epoch, seq cutoff 2048) generates templated inferences per taxonomy dimension using GPT-4o reference data; SSF-CLASSIFIER (3 epochs, seq cutoff 1536) classifies inferences into subdimensions using GPT-4.1 k-shot outputs. The pipeline processes raw story + context through context summarization, inference generation (per dimension), inference classification, and aggregation for community-level analysis or ssf-sim computation.

## Key Results
- SSF-GENERATOR produces inferences deemed plausible by ≥94% of human raters (N=382), with ≥78% rated very or somewhat likely
- SSF-CLASSIFIER achieves within 0.1 F-1 points of GPT-4.1 across all dimensions, with expert IAA Jaccard Index = 0.732
- ssf-sim aligns with human similarity judgments 74% vs. 52% for semantic baseline on N=50 pairs

## Why This Works (Mechanism)

### Mechanism 1: Context-Grounded Inference Generation via Distillation
- Claim: Generating plausible reader-response inferences requires both community and conversational context, distilled from stronger teacher models.
- Mechanism: GPT-4o generates reference inferences conditioned on subreddit descriptions, community values, and conversation history; Llama-3.1-8B-Instruct is fine-tuned via LoRA to replicate this reasoning in an open-weight model.
- Core assumption: Representative U.S. adults primed with context can approximate the "plausible reader response" for diverse subreddits.
- Evidence anchors:
  - [abstract] "validated through human surveys (N=382 participants)"
  - [section 5.1] "≥94% of ratings were deemed plausible, and ≥78% were deemed very or somewhat likely"
  - [corpus] Related work on contextual commonsense reasoning (CobraFrames, Zhou et al. 2023) supports context-conditioned inference, but direct cross-paper validation is not provided.
- Break condition: If annotators cannot adopt community perspectives despite context priming, plausibility ratings become unreliable; if GPT-4o's context integration fails on niche communities requiring specialized knowledge, distilled outputs degrade.

### Mechanism 2: Structured Taxonomy Enables Scalable Classification
- Claim: A 10-dimensional taxonomy with fine-grained subdimensions allows models to map free-text inferences onto interpretable categories across diverse communities.
- Mechanism: SSF-CLASSIFIER performs multi-label classification over taxonomy subdimensions, trained on GPT-4.1 k-shot outputs with expert-validated guidelines.
- Core assumption: Reader-response dimensions are sufficiently independent for multi-label classification without explicit dependency modeling.
- Evidence anchors:
  - [section 5.2] "mean [Jaccard Index] = 0.732; minimum ≥0.517" for expert IAA
  - [table 1] SSF-CLASSIFIER achieves within 0.1 F-1 points of GPT-4.1 across all dimensions
  - [corpus] No direct corpus evidence for taxonomy exhaustiveness beyond Reddit English-language data.
- Break condition: If dimensions have complex interdependencies (e.g., narrative feeling influencing stance), single-label predictions become confounded; if subdimensions are too abstract for informal storytelling, classification coherence degrades.

### Mechanism 3: Reader-Response Similarity Captures Functional Patterns Beyond Semantics
- Claim: ssf-sim identifies cross-community narrative similarity based on communicative function and reader reception, not just topical overlap.
- Mechanism: Combines (1) cosine similarity of inference embeddings and (2) Jensen-Shannon similarity of sublabel distributions, aggregated across taxonomy dimensions.
- Core assumption: Community-level mean-pooling of inference embeddings and normalized sublabel distributions adequately represent narrative practices.
- Evidence anchors:
  - [section 6, figure 4] ssf-sim aligns with human judgments 74% vs. 52% for semantic baseline
  - [section H.2] "moderately high" annotator agreement (κ=0.5098) on similarity judgments
  - [corpus] Validation based on N=50 pairs; larger-scale validation not provided.
- Break condition: If inference embeddings encode substantial story content rather than pure reader-response signals, ssf-sim conflates semantics with reception; if communities have high within-community variance, mean-pooling obscures diversity.

## Foundational Learning

- Concept: **Pragmatic context in narrative understanding**
  - Why needed here: The paper explicitly models "interpretive communities" (Fish, 1990) through subreddit context; understanding why context shapes inference is foundational.
  - Quick check question: Can you explain why the same story might evoke different reader responses in r/politics versus r/funny?

- Concept: **Model distillation via supervised fine-tuning**
  - Why needed here: SSF-GENERATOR and SSF-CLASSIFIER are distilled from GPT-4o/GPT-4.1 to Llama-3.1-8B-Instruct using LoRA.
  - Quick check question: What information is lost when distilling inference capabilities from a larger teacher model to a smaller student model?

- Concept: **Multi-label classification with imbalanced labels**
  - Why needed here: Inference classification maps free-text to multiple subdimensions with varying frequencies; some subdimensions were excluded for rarity.
  - Quick check question: Why might macro F-1 be more informative than micro F-1 for evaluating this task?

## Architecture Onboarding

- Component map: Raw story + context -> Context summarization -> Inference generation (per dimension) -> Inference classification -> Aggregated sublabel distributions -> Community-level analysis or ssf-sim computation
- Critical path: Raw story + context → Context summarization → Inference generation (per dimension) → Inference classification → Aggregated sublabel distributions → Community-level analysis or ssf-sim computation
- Design tradeoffs:
  - Iterative summarization reduces cost but introduces cascading information loss (section 9, "D.4.4 Error Analysis")
  - Excluding toxic/explicit content protects annotators but limits model applicability to those content types
  - Assuming independent dimensions simplifies modeling but may miss interdependencies (section 9, "Task Definition")
- Failure signatures:
  - OVEREMPHASIZE_READER_INVESTMENT: Inferences assume excessive emotional investment in banal stories (Appendix F.2)
  - ASSUME_EMOTIONAL/MORAL_CLOSURE: Inferences preempt resolution when situations remain unresolved (Appendix F.2)
  - IGNORE_CONTEXT: Inferences plausible in isolation fail given broader conversational context (Appendix F.2)
  - Context ablation: Removing conversational context reduces alignment with reference inferences significantly (t=11.068, p<0.001; Appendix F.3)
- First 3 experiments:
  1. Run SSF-Generator on held-out stories from an unseen subreddit; manually inspect for OVEREMPHASIZE_READER_INVESTMENT and IGNORE_CONTEXT errors
  2. Ablate community context vs. conversational context separately; compare cosine similarity to full-context reference inferences to quantify each context type's contribution
  3. Apply ssf-sim to two semantically similar but functionally distinct subreddit pairs (e.g., r/news vs. r/funny); verify that ssf-sim ranking diverges from semantic-sim as expected

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the theoretical and empirical dependencies between the various dimensions of reader response defined in the SSF-TAXONOMY?
- Basis in paper: [explicit] The authors state in the Limitations section that "Theoretical and empirical inquiry into the dependencies between dimensions of reader response is an important direction for future work."
- Why unresolved: The current modeling pipeline assumes dimensions are independent to simplify the task definition and classification, which may not reflect the actual cognitive interdependence of reactions like "narrative feeling" and "character appraisal."
- What evidence would resolve it: A study applying statistical methods (e.g., correlation analysis or structural equation modeling) to the SSF-CORPUS data to identify conditional probabilities and co-occurrence patterns between taxonomy dimensions.

### Open Question 2
- Question: Which specific storytelling dynamics foster prosocial outcomes in online conversations, such as perspective-taking and learning?
- Basis in paper: [explicit] In the Future Work section, the authors explicitly suggest exploring "Storytelling dynamics that foster prosocial outcomes in conversations, such as perspective-taking and learning."
- Why unresolved: The current work focuses on the intent and reception of the story itself, rather than measuring the causal impact of those stories on subsequent conversational quality or user behavior.
- What evidence would resolve it: Longitudinal experiments or corpus analyses linking specific SSF-CLASSIFIER predictions (e.g., high "connection" or "compassion" scores) to subsequent prosocial interactions in the comment thread.

### Open Question 3
- Question: How do narrative reception practices of online communities and their members co-evolve over time?
- Basis in paper: [explicit] The Conclusion lists exploring "How narrative reception practices of communities and their members co-evolve" as a primary avenue for future research.
- Why unresolved: The current SSF-CORPUS is treated largely as a cross-sectional snapshot (stories and contexts), lacking the temporal depth required to model dynamic changes in community norms or individual user adaptation.
- What evidence would resolve it: A temporal analysis tracking how the distribution of SSF-TAXONOMY labels shifts within a specific subreddit or user cohort over months or years.

## Limitations

- Context summarization may fail for subcultures requiring specialized knowledge or communities with contested norms
- Multi-label classification assumes independent dimensions, potentially missing complex interdependencies between reader responses
- Corpus filtering excludes toxic/explicit content, limiting model applicability to those domains
- ssf-sim relies on mean-pooling across inference embeddings, which may obscure within-community diversity

## Confidence

**High Confidence**: The taxonomy construction process, human validation results (N=382), and expert annotation agreement (Jaccard Index = 0.732) are well-documented. The SSF-CLASSIFIER performance (within 0.1 F-1 points of GPT-4.1) and plausibility ratings (≥94% deemed plausible) provide strong empirical support.

**Medium Confidence**: The context distillation mechanism works as described for diverse Reddit communities, though the GPT-4.1 model reference ambiguity and lack of explicit k-shot examples introduce uncertainty. The ssf-sim validation based on N=50 pairs provides preliminary evidence but needs larger-scale verification.

**Low Confidence**: Cross-domain generalization beyond Reddit English-language storytelling remains untested. The assumption of dimension independence in multi-label classification lacks empirical validation. The model's behavior on communities requiring specialized knowledge or contested norms is unknown.

## Next Checks

1. **Context Contribution Analysis**: Ablate community context vs. conversational context separately on held-out data; quantify each context type's contribution to inference plausibility through controlled experiments measuring cosine similarity to full-context references.

2. **ssf-sim Scaling Validation**: Apply ssf-sim to 500+ story pairs across semantically similar but functionally distinct subreddit pairs (e.g., r/news vs. r/funny); verify that ssf-sim ranking consistently diverges from semantic similarity as predicted, with statistical significance testing.

3. **Cross-Domain Generalization Test**: Apply SSF-GENERATOR to stories from non-Reddit platforms (Twitter threads, personal blogs) and evaluate plausibility ratings; identify failure modes when context summarization encounters different discourse structures or community norms.