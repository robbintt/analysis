---
ver: rpa2
title: 'Learning from Heterophilic Graphs: A Spectral Theory Perspective on the Impact
  of Self-Loops and Parallel Edges'
arxiv_id: '2509.13139'
source_url: https://arxiv.org/abs/2509.13139
tags:
- graph
- self-loops
- parallel
- edges
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the impact of self-loops and parallel edges
  on the performance of low-pass filters like GCN when applied to heterophilic graphs.
  The authors conduct empirical studies and theoretical analyses to show that adding
  self-loops decreases the eigenvalues of the normalized graph Laplacian, shifting
  the spectrum towards zero, while adding parallel edges increases the eigenvalues,
  shifting the spectrum towards two.
---

# Learning from Heterophilic Graphs: A Spectral Theory Perspective on the Impact of Self-Loops and Parallel Edges

## Quick Facts
- **arXiv ID**: 2509.13139
- **Source URL**: https://arxiv.org/abs/2509.13139
- **Reference count**: 40
- **Primary result**: GCN performance trends under self-loop and parallel edge additions reveal spectral properties of heterophilic graphs without eigendecomposition

## Executive Summary
This paper investigates how self-loops and parallel edges affect GCN performance on heterophilic graphs through spectral analysis. The authors show that self-loops decrease eigenvalues of the normalized graph Laplacian (shifting spectrum toward zero), while parallel edges increase eigenvalues (shifting toward two). These spectral shifts create four distinct performance trend categories that can reveal structural properties like community structure and connected components without expensive eigendecomposition. Experiments on 17 benchmark datasets validate the findings, and the approach proves efficient for large graphs where traditional spectral methods fail.

## Method Summary
The method involves systematically adding self-loops (α) and parallel edges (γ) to heterophilic graphs and observing GCN performance trends. For each dataset, GCN is trained with varying numbers of self-loops (α∈[1,5]) and parallel edges (γ∈[1,5], with baseline self-loop), tracking test accuracy to determine increasing/decreasing trends. The resulting trend pairs categorize graphs into four types (A-D) that indicate different spectral characteristics. The approach uses a standard 2-layer GCN architecture with dropout=0.5 and LayerNorm, implemented in PyTorch/PyTorch-geometric across 17 benchmark heterophilic datasets.

## Key Results
- Self-loops monotonically decrease maximum eigenvalues of normalized Laplacian, shifting spectrum toward zero
- Parallel edges monotonically increase eigenvalues (when baseline self-loop present), shifting spectrum toward two
- GCN performance trends under these modifications form four distinct categories revealing spectral properties
- Method efficiently infers spectral characteristics without eigendecomposition, crucial for large graphs (>50K nodes)
- Empirical validation across 17 benchmark datasets confirms theoretical predictions

## Why This Works (Mechanism)

### Mechanism 1: Spectrum Compression via Self-Loop Addition
- Claim: Adding self-loops decreases eigenvalues of the normalized Laplacian, shifting spectrum toward zero
- Mechanism: Self-loops augment both adjacency matrix $\tilde{A}_\alpha = A + \alpha I$ and degree matrix $\tilde{D}_\alpha = D + \alpha I$. Theoretical bounds show $\lambda_{max}^\alpha \leq \frac{\max_i d_i (1 - \beta_1)}{\alpha + \max_i d_i}$, demonstrating monotonic decrease in maximum eigenvalue as $\alpha$ increases.
- Core assumption: Graph has non-trivial structure where degree matrix isn't dominated by self-loop additions ($d_i \gg \alpha$ in practice)
- Break condition: Extremely sparse graphs where self-loops dominate original structure (e.g., Questions dataset with 88% isolated nodes)

### Mechanism 2: Spectrum Expansion via Parallel Edge Addition
- Claim: Adding parallel edges increases eigenvalues of normalized Laplacian, shifting spectrum toward two
- Mechanism: Parallel edges multiply off-diagonal adjacency entries by $(1 + \gamma)$ while updating degrees accordingly. Lemma 3.2 shows upper bound increases in $\gamma$, with Theorem 3.8 proving eigenvalue ordering for regular graphs.
- Core assumption: At least one self-loop must be added per node (Corollary A.2); otherwise, parallel edge addition alone doesn't change $\tilde{L}$
- Break condition: When original graph already has eigenvalues clustered near 2 (spectrum saturation), further additions yield diminishing returns

### Mechanism 3: Performance Trend Categorization as Spectrum Proxy
- Claim: GCN's monotonic performance trends under modifications form four categories revealing spectral characteristics without eigendecomposition
- Mechanism: GCN amplifies $\lambda < 1$ frequencies. Category A (↑/↑) suggests balanced spectrum; Category C (↓/↑) indicates initial dominance of lower frequencies; Categories B and D reflect asymmetric responses
- Core assumption: GCN's low-pass filtering behavior dominates performance changes; other factors held constant
- Break condition: Extreme class imbalance or feature noise may dominate GCN performance, breaking trend-to-spectrum mapping

## Foundational Learning

- Concept: Symmetrically Normalized Graph Laplacian ($\tilde{L} = I - D^{-1/2}AD^{-1/2}$)
  - Why needed here: All spectral analysis hinges on $\tilde{L}$'s eigenvalues being bounded in $[0, 2]$, with interpretation as graph frequencies
  - Quick check question: If $\tilde{L}$ has eigenvalues clustered near 0, what does this imply about graph connectivity and community structure?

- Concept: Low-Pass vs. High-Pass Filtering in Graphs
  - Why needed here: GCN's filter function $\tilde{G} = I - \Sigma$ amplifies coefficients for $\lambda < 1$ (low frequencies), explaining why spectral shifts affect performance
  - Quick check question: Would a high-pass filter (amplifying $\lambda \geq 1$) show the opposite performance trends on heterophilic graphs under self-loop addition?

- Concept: Rayleigh Quotient for Eigenvalue Bounds
  - Why needed here: Lemma 3.1 and 3.2 use Rayleigh quotients to derive upper bounds on $\lambda_{max}^\alpha$ and $\lambda_{max}^\gamma$
  - Quick check question: How does the Rayleigh quotient bound change when $\max_i d_i$ increases but graph structure remains otherwise fixed?

## Architecture Onboarding

- Component map:
  Graph Rewiring Module -> Standard GCN Backbone -> Performance Trend Detector -> Category Classifier

- Critical path:
  1. Load heterophilic graph and verify isolated node percentage (Table 3 provides baselines)
  2. For self-loop analysis: Run GCN with α = 1 to 5, record test accuracy, determine trend direction
  3. For parallel edge analysis: Run GCN with γ = 1 to 5 (with 1 self-loop baseline), record test accuracy, determine trend direction
  4. Classify category and infer spectral characteristics

- Design tradeoffs:
  - **Self-loop count vs. spectrum signal**: More self-loops increase spectral shift but may distort original graph semantics; trends stabilize around α = 5
  - **Parallel edge count vs. numerical stability**: Large γ can cause numerical issues in dense graphs; diminishing eigenvalue changes observed
  - **GCN depth vs. over-smoothing**: Two layers used consistently; deeper networks may confound spectral effects with over-smoothing

- Failure signatures:
  - **Non-monotonic trends**: Accuracy oscillates rather than consistent increase/decrease, indicating mixed spectral properties or high feature noise
  - **OOM on large graphs**: Traditional eigendecomposition fails for graphs >50K nodes; this method provides alternative but requires memory management
  - **Category instability**: Category assignment changes across random splits, suggesting borderline spectral characteristics

- First 3 experiments:
  1. **Baseline verification**: Run full protocol on Chameleon (Category C, no isolated nodes, dense) and Cornell (Category A, 47.5% isolated nodes). Verify trends match Table 4
  2. **Synthetic graph test**: Generate Erdős-Rényi graphs with varying p (edge probability) and verify that dense graphs show different category assignments than sparse graphs
  3. **Large-scale inference**: Apply to graph where eigendecomposition is infeasible (e.g., snap-patents with 2.9M nodes). Use trend analysis to infer spectral properties and validate against approximate spectral methods

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the addition of self-loops and parallel edges impact the performance of high-pass or adaptive GNNs (e.g., FAGCN, GAT) compared to the strictly low-pass GCN analyzed here?
- Basis in paper: The paper focuses exclusively on GCN, explicitly defined as a low-pass filter, and acknowledges that heterophilic graphs often require high-pass filtering which sharpens features
- Why unresolved: Theoretical analysis proves eigenvalues decrease/increase for normalized Laplacian, predicting behavior for low-pass smoothing, but impact on high-frequency amplification mechanisms is not addressed
- What evidence would resolve it: Experiments applying identical perturbations to high-pass GNN architectures to observe if performance trends invert or differ from A/B/C/D categories

### Open Question 2
- Question: Can the theoretical bounds established for regular graphs be effectively extended to accurately predict performance shifts in irregular, real-world heterophilic graphs?
- Basis in paper: Key theoretical proofs (Theorems 3.7, 3.8) rely on the assumption of k-regular graphs, while empirical results are drawn from highly irregular benchmark datasets
- Why unresolved: The gap between strict mathematical conditions (regular graphs) and empirical observations (irregular networks) leaves uncertainty regarding precision of spectral shift predictions
- What evidence would resolve it: Derivation of tighter bounds for irregular graphs or perturbation analysis accounting for high variance in node degrees, validated against observed eigenvalue shifts

### Open Question 3
- Question: Can the specific categorization of graphs (A, B, C, D) be utilized to automate the selection of optimal graph rewiring strategies without manual inspection?
- Basis in paper: The authors state that "The design of effective application of GNNs to replace the prevailing costly algorithmic computations can be a potential avenue for future research directions"
- Why unresolved: While paper establishes different categories react differently to rewiring, it doesn't propose a unified mechanism to determine optimal magnitude of α or γ based on identified category
- What evidence would resolve it: Dynamic framework using initial trend (Category) to automatically tune α and γ, demonstrating improved convergence and accuracy over static default settings

## Limitations
- Theoretical analysis focuses on extremal eigenvalues, leaving complete spectral characterization unaddressed
- Performance trend-to-spectrum mapping is indirect and could be confounded by feature noise or class imbalance
- Spectral shift mechanisms assume non-trivial graph structure where degree matrices aren't dominated by self-loop additions

## Confidence
- **High confidence**: Core claims about spectral shifts from self-loops and parallel edges due to rigorous theoretical proofs and empirical validation across 17 datasets
- **Medium confidence**: Performance trend categorization mechanism is novel and empirically validated but lacks extensive theoretical grounding
- **High confidence**: Efficiency claims for large graphs based on runtime comparisons in Table 8

## Next Checks
1. Verify trend stability on synthetic graphs with controlled spectral properties (e.g., planted community graphs with varying homophily ratios) to test direct mapping between categories and spectral characteristics
2. Test method's robustness on extremely sparse graphs by progressively removing edges from dense datasets and monitoring when trend predictions break down
3. Implement approximate spectral analysis (e.g., power iteration for top/bottom eigenvalues) on large graphs to validate inferred spectral properties from performance trends against ground truth where possible