---
ver: rpa2
title: 'Hide and Seek in Embedding Space: Geometry-based Steganography and Detection
  in Large Language Models'
arxiv_id: '2601.22818'
source_url: https://arxiv.org/abs/2601.22818
tags:
- embedding
- secret
- detection
- token
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "We introduce low-recoverability steganography for LLMs, which\
  \ embeds secrets in model outputs while making them harder to reverse-engineer.\
  \ Prior steganographic schemes like TrojanStego achieve 100% recoverability\u2014\
  both the encoded bits and original secrets can be perfectly extracted without the\
  \ key."
---

# Hide and Seek in Embedding Space: Geometry-based Steganography and Detection in Large Language Models

## Quick Facts
- arXiv ID: 2601.22818
- Source URL: https://arxiv.org/abs/2601.22818
- Reference count: 37
- Primary result: Geometric steganography improves exact secret recovery while reducing output-side recoverability, with detection possible via linear probes on hidden states

## Executive Summary
This paper introduces low-recoverability steganography for large language models (LLMs) by embedding secrets in model outputs using embedding-space geometry. Unlike prior approaches achieving 100% recoverability, the authors propose using random hyperplane projections to create bucket assignments that are harder to reverse-engineer while being easier for models to learn during fine-tuning. They formalize this via two metrics—Rbucket (chance of inferring encoded bits from outputs) and Rsecret (chance of recovering the secret from those bits)—and demonstrate substantial improvements in exact secret recovery across multiple models and datasets while maintaining text quality within normal ranges.

For detection, the paper argues that standard steganalysis based on distributional divergence is insufficient for fine-tuning-based attacks. Instead, they propose a mechanistic interpretability approach using linear probes trained on later-layer activations, which detect secrets with up to 33% higher accuracy in fine-tuned models compared to base models. This detection signal persists even under non-decodable XOR-masking controls, suggesting steganographic fine-tuning leaves detectable internal signatures that enable interpretability-based defenses.

## Method Summary
The method encodes 4-letter secrets into LLM outputs using hyperplane projections in embedding space. Letters are encoded via 8-bit codes derived from projecting letter embeddings onto random hyperplanes, while vocabulary tokens are partitioned into buckets using a single hyperplane projection. The model is fine-tuned using LoRA (or full fine-tuning for baselines) with constrained token generation ensuring each token's bucket matches the target bit. Detection uses linear probes (logistic regression for bits, ridge regression for embeddings) trained on hidden states at corresponding token positions, comparing accuracy between base and fine-tuned models to identify steganographic signatures.

## Key Results
- Exact secret recovery improves from 17→30% (+78%) on Llama-8B LoRA, 24→43% (+80%) on Ministral-8B LoRA, and 9→19% (+123%) on Llama-70B LoRA
- Geometric bucketing substantially reduces Rbucket compared to arbitrary schemes while maintaining or improving Rsecret
- Linear probes detect steganographic fine-tuning with up to 33% higher accuracy in later-layer activations compared to base models
- Detection persists even under XOR-masking controls where secrets are non-decodable from outputs
- Text quality (perplexity, KL divergence) remains within normal fine-tuning bounds even with substantial information encoding

## Why This Works (Mechanism)

### Mechanism 1: Hyperplane-Based Vocabulary Partitioning
Seed-dependent random hyperplanes in embedding space create bucket assignments that are harder to reverse-engineer than arbitrary schemes (e.g., token ID parity) while being easier for the model to learn during fine-tuning. Given embedding matrix E ∈ R^{|V|×d}, a random unit vector v_σ defines a hyperplane. Each token t is assigned to bucket β_σ(t) = 1[e_t^T v_σ > τ], where τ is the median projection score. The seed σ acts as a key—without it, an observer must search over a large seed space or resort to explicit seed recovery methods. This substantially reduces recoverability because an observer who suspects steganography but lacks access to the seed must search over a large space, which requires many samples.

### Mechanism 2: Embedding-Space Encoding Leverages Learned Geometry
Secret encoding via hyperplane projections of letter embeddings achieves comparable or improved attack success rates relative to ASCII encoding while maintaining the same recoverability profile at the encoding stage. Each letter's embedding is projected onto 8 hyperplanes (from seeds σ_1,...,σ_8), thresholded against the median projection across all 26 letters, yielding an 8-bit code. Collision-free seed configurations ensure unique codes. Because the encoding uses the model's own learned representations, the model has implicit prior knowledge of the embedding structure. This leverages the model's learned semantic structure, which encourages learning during fine-tuning.

### Mechanism 3: Mechanistic Detection via Linear Probes
Steganographic fine-tuning leaves detectable signatures in later-layer activations that can be extracted via linear probes, even when the secret is not decodable from outputs and even under out-of-distribution prompts. Logistic regression probes trained on hidden states at corresponding token positions predict individual bit values; ridge regression probes predict PCA-reduced secret embeddings from concatenated hidden states. Fine-tuned models show substantially higher probe accuracy (up to 95-100% at final layer) compared to base models (~60%), indicating steganographic encoding imparts structured internal representations. This indicates that steganographic encoding imparts detectable internal representations regardless of output distribution anomalies.

## Foundational Learning

- **Steganographic Channels in Generative Models**: LLMs define conditional distributions with many high-probability continuations, creating freedom to encode auxiliary information without destroying fluency. Quick check: Given a prompt with 5 plausible next tokens, can you explain how constraining selection to tokens in a specific bucket creates a covert channel?

- **Payload Recoverability (R_bucket and R_secret)**: The paper distinguishes between "can detect steganography exists" and "can extract the secret," formalized via two complementary metrics. Quick check: If R_bucket = 50% and R_secret = negligible, what does this imply about an adversary's ability to recover secrets from observed outputs?

- **Linear Probes as Representational Diagnostic Tools**: The detection approach relies on probes extracting information from intermediate activations; understanding their limitations is critical for interpreting results. Quick check: If a linear probe achieves 95% accuracy on hidden states but the secret is XOR-masked and non-decodable from outputs, what does this tell you about where the steganographic signature resides?

## Architecture Onboarding

- **Component map**: Secret string → 32-bit payload via ASCII or hyperplane projection of letter embeddings → Bucketing stage: Vocabulary partition via hyperplane projection or parity → Constrained generation ensures tokens match target bits → LoRA or full fine-tuning with frozen embeddings/lm_head → Constrained token generation during training → Linear probes (logistic regression for bits, ridge regression for embeddings) on 4 evenly-spaced layers → Evaluation on OOD prompts

- **Critical path**: Generate hyperplane seeds → construct bucket assignments → generate constrained training completions → Fine-tune model (LoRA preferred for 8B/70B; full fine-tuning for 8B baseline) → Evaluate exact match rate on held-out secrets + OOD prompts → Train probes on activations from OOD prompts → Compare accuracy gap between base and fine-tuned models

- **Design tradeoffs**:
  - LoRA vs. full fine-tuning: LoRA shows better OOD generalization; full fine-tuning achieves higher in-distribution accuracy but overfits
  - Embedding vs. parity bucketing: Embedding-based reduces recoverability but requires freezing embeddings during training; parity allows embedding drift
  - Encoding scheme: ASCII is simpler but arbitrary; embedding-based leverages learned geometry but requires collision-free seed search
  - Detection probe type: Bit classification requires knowing the encoding scheme (white-box); embedding recovery works without scheme knowledge (gray-box)

- **Failure signatures**:
  - Low exact match rate: Model failed to learn steganographic constraint; check bucket balance, training data diversity, LoRA rank
  - High R_bucket on evaluation: Bucketing scheme too predictable; increase hyperplane seed entropy or switch from parity
  - Probe accuracy gap disappears: Detection fails; check if full fine-tuning on Ministral-8B or if training explicitly obscured representations
  - OOD generalization collapse: Overfitting to training prompt style; use Wiki-style diverse prompts rather than TrojanStego-style

- **First 3 experiments**:
  1. Reproduce R_bucket/R_secret measurements: Train XGBoost and MLP classifiers on token IDs/embeddings to predict buckets, then on predicted bits to predict secrets; verify ASCII+Parity achieves ~100% on both while Embedding+Embedding achieves lower (per Figure 2).
  2. Run probe detection on base vs. fine-tuned checkpoints: Extract hidden states from 4 layers during OOD prompt generation; train bit classification and embedding regression probes; confirm ~33% accuracy gap at final layer (per Figure 5 bottom).
  3. Test XOR-masking control: Train models with secret-conditioned XOR masking; verify probe detection persists despite non-decodable outputs (per Table 3), confirming detection relies on internal signatures not output patterns.

## Open Questions the Paper Calls Out
- Can steganographic fine-tuning be detected in black-box or output-only settings without access to internal model activations?
- Can "bespoke" training procedures explicitly optimize to evade detection by linear probes while maintaining steganographic capacity?
- Does the geometric approach maintain low recoverability and high fluency when encoding payloads significantly larger than 32 bits?
- Are the detection signals (internal signatures) robust across diverse model architectures, or are they artifacts of specific training dynamics?

## Limitations
- The geometric partitioning mechanism's security claims rely on assumptions about seed space entropy that aren't empirically validated
- The claim that embedding-space encoding leverages learned geometry lacks direct validation of why this occurs
- Detection probe methodology could conflate steganographic signatures with general fine-tuning artifacts

## Confidence
- **High Confidence**: Experimental results showing improved exact secret recovery rates (17→30%, 24→43%, 9→19%) are well-documented and reproducible
- **Medium Confidence**: Geometric partitioning security claims rely on unverified assumptions about seed space entropy
- **Low Confidence**: Claims about detectability under future training methods that "explicitly penalize detectable internal representations" are speculative

## Next Checks
1. **Seed Recovery Feasibility Analysis**: Conduct systematic study of how many input-output pairs with known secrets are needed to recover hyperplane orientations through supervised learning, varying vocabulary size and embedding dimension
2. **Collision-Free Configuration Frequency**: Measure how often collision-free 8-hyperplane configurations exist for 26 letters across different models and embedding spaces, reporting distribution of required seed search attempts
3. **Cross-Architecture Detection Robustness**: Test linear probe detection approach across broader range of model families and fine-tuning strategies to validate whether detection signal captures steganographic content specifically or general fine-tuning artifacts