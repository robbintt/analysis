---
ver: rpa2
title: 'Overcoming Challenges of Partial Client Participation in Federated Learning
  : A Comprehensive Review'
arxiv_id: '2506.02887'
source_url: https://arxiv.org/abs/2506.02887
tags:
- client
- learning
- clients
- participation
- federated
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This survey reviews methods to address partial client participation
  in federated learning, where only a subset of clients participate in each training
  round due to resource constraints, network issues, or selective sampling. The paper
  categorizes and compares various frameworks including FedAvg, Fed-EF, GradMA, FedAMD,
  MIFA, FedCM, FedV ARP, FedProx, and SCAFFOLD.
---

# Overcoming Challenges of Partial Client Participation in Federated Learning : A Comprehensive Review

## Quick Facts
- arXiv ID: 2506.02887
- Source URL: https://arxiv.org/abs/2506.02887
- Reference count: 40
- One-line primary result: Memory-based and variance-reduction methods outperform FedAvg baselines under partial client participation and non-IID data

## Executive Summary
This survey comprehensively reviews methods addressing partial client participation in federated learning, where only subsets of clients participate in each training round due to resource constraints, network issues, or selective sampling. The paper categorizes and compares 12 frameworks including FedAvg, Fed-EF, GradMA, FedAMD, MIFA, FedCM, FedV ARP, FedProx, and SCAFFOLD. Each method tackles partial participation through different mechanisms such as gradient compression, memory augmentation, adaptive sampling, momentum-based updates, and variance reduction. Experiments on datasets like MNIST, CIFAR-10, and FEMNIST show that methods employing variance reduction and memory-based techniques generally outperform baselines like FedAvg, particularly under low participation rates and non-IID data distributions.

## Method Summary
The survey evaluates multiple federated learning frameworks specifically designed for partial client participation scenarios. Methods are implemented using standard FL simulation frameworks with configurable client sampling rates. Non-IID data splits are created using Dirichlet distribution (α=0.1) to simulate high heterogeneity, with datasets including MNIST, CIFAR-10, CIFAR-100, FEMNIST, FMNIST, EMNIST, Shakespeare, TinyImageNet, and IMDB. Performance is measured through test accuracy, convergence speed (rounds to target accuracy), and communication efficiency (bits transmitted). The review focuses on comparing methods like FedAvg, FedProx, SCAFFOLD, FedVARP, MIFA, FedCM, GradMA, FedAMD, Fed-EF, CyCP, EmbracingFL, and SAFARI under various participation rates and data heterogeneity levels.

## Key Results
- Memory-based methods (MIFA, GradMA) achieve 5-10% accuracy gains over FedAvg under low participation rates (2-5%)
- Variance reduction techniques (SCAFFOLD) reduce client drift, achieving 80% accuracy vs FedAvg's 72% on CIFAR-10 non-IID
- Layer-wise adaptive training (EmbracingFL) enables participation from resource-constrained devices with minimal accuracy loss (70% vs full-model baselines)
- Adaptive sampling methods (FedAMD, FedVARP) improve convergence speed by 30-50% compared to random sampling

## Why This Works (Mechanism)

### Mechanism 1: Memory-Augmented Gradient Substitution
- Claim: Storing the most recent update from each client and reusing it when that client is inactive may reduce gradient variance caused by partial participation.
- Mechanism: The server maintains a memory buffer (O(Nd) storage for N clients, d-dimensional model) containing each client's latest update. During aggregation, active clients contribute fresh gradients while inactive clients' stale updates serve as surrogates, preventing the global model from drifting toward only active clients' data distributions.
- Core assumption: Stale gradients remain directionally useful within a bounded number of rounds, and client availability patterns are not adversarially correlated with data distribution shifts.
- Evidence anchors:
  - [abstract] "methods employing variance reduction and memory-based techniques generally outperform baselines like FedAvg, particularly under low participation rates and non-IID data distributions"
  - [Section III-H] "MIFA operates by maintaining a memory of the latest updates from all devices, active or inactive. When devices are unavailable, their most recent updates stored in memory are used as surrogates."
  - [corpus] FedAdaVR (arXiv:2601.22204) proposes adaptive variance reduction for limited client participation, corroborating the variance-focused approach; FedDPC (arXiv:2512.20329) explicitly addresses how partial participation skews aggregation.

### Mechanism 2: Control Variate Variance Reduction
- Claim: Maintaining correction terms (control variates) at both server and client levels can reduce client drift caused by local training on heterogeneous data under partial participation.
- Mechanism: Each client maintains a control variate ci tracking the difference between local gradient direction and global direction. Clients subtract ci from their local gradients during training, then upload both model updates and control variate changes. The server aggregates both, ensuring local updates align with global objectives despite infrequent participation.
- Core assumption: Clients perform sufficient local computation to estimate meaningful control variates, and communication channels reliably transmit both model updates and variate corrections.
- Evidence anchors:
  - [Section III-L] "SCAFFOLD introduces a novel variance reduction technique specifically designed to address client drift caused by partial participation and non-IID data distributions."
  - [Table VI] SCAFFOLD achieves 80% accuracy (Non-IID) vs FedAvg's 72% on CIFAR-10
  - [corpus] GC-Fed (arXiv:2503.13180) addresses client drift in heterogeneous settings using gradient centralization, offering an alternative drift-mitigation approach.

### Mechanism 3: Layer-Wise Adaptive Training
- Claim: Assigning different model layers to clients based on computational capacity can enable participation from resource-constrained devices that would otherwise drop out.
- Mechanism: The model is partitioned into input-side layers (trained by strong clients with full resources) and output-side layers (trained by weak clients with limited memory/compute). Weak clients receive intermediate activations from input layers via multi-step forward pass, train only output layers, and upload partial updates. This reduces per-client compute from O(d) to O(d/k) where k is the layer partition ratio.
- Core assumption: Output-side layers learn more client-specific representations while input-side layers generalize across clients, and the server can synchronize heterogeneous partial updates without introducing conflicting gradient signals.
- Evidence anchors:
  - [Section III-A] "Embracing FL assigns specific portions of the model to be trained by different clients based on their resource capabilities... weak clients train only the output-side layers"
  - [Table VI] EmbracingFL achieves 70% accuracy (Non-IID) with weak clients using "as little as 2% of the full model capacity"

## Foundational Learning

- Concept: **Federated Averaging (FedAvg) baseline**
  - Why needed here: All surveyed methods position themselves relative to FedAvg's O(1/√T) convergence under partial participation; understanding its random sampling and weighted averaging is prerequisite to grasping why variance reduction helps.
  - Quick check question: Given 100 clients with non-IID data and 10% participation per round, what causes FedAvg's global model to drift toward frequently sampled clients?

- Concept: **Gradient variance vs. bias tradeoff**
  - Why needed here: Memory-based methods introduce bias (using stale updates) to reduce variance; variance reduction methods (SCAFFOLD) add computational overhead to reduce both. Selecting a method requires understanding which error mode dominates your deployment.
  - Quick check question: If client availability is highly non-stationary (e.g., devices only available during specific hours), would bias from stale updates or variance from sparse sampling be the larger concern?

- Concept: **Non-IID data heterogeneity metrics**
  - Why needed here: The paper uses Dirichlet distribution with concentration parameter α to simulate non-IID settings (α=0.1 is highly heterogeneous, α=2.0 is more uniform). Results showing 5-10% accuracy gains at α=0.5 vs 2-8% at α=2.0 indicate method sensitivity to data distribution.
  - Quick check question: If your deployment has α≈0.01 (extreme heterogeneity), which method category (memory-based, variance reduction, or adaptive sampling) would you prioritize and why?

## Architecture Onboarding

- Component map:
  - Server-side: Aggregation engine, memory buffer (for MIFA/FedVARP/GradMA), global momentum state (FedCM), control variate accumulator (SCAFFOLD), auxiliary dataset (SAFARI)
  - Client-side: Local optimizer, gradient/memory accumulator, control variate storage (SCAFFOLD), error feedback buffer (Fed-EF), layer selector (EmbracingFL)
  - Communication: Model broadcast channel, update collection channel, optional metadata channel (for variates, compression ratios)

- Critical path:
  1. Characterize your deployment's participation pattern (random dropout vs. cyclic availability vs. resource-constrained stratification)
  2. Profile client compute/memory constraints to filter viable methods (e.g., SCAFFOLD's variate storage may exceed weak client capacity)
  3. Estimate communication budget; if <1MB/round, prioritize Fed-EF compression over full-precision methods
  4. Start with FedProx (simplest modification to FedAvg) as baseline before attempting variance reduction

- Design tradeoffs:
  - Memory overhead vs. convergence speed: MIFA/FedVARP require O(Nd) server memory; SCAFFOLD requires O(Nd) distributed across clients
  - Communication vs. accuracy: Fed-EF reduces bits transmitted but introduces compression bias requiring careful error feedback tuning
  - Fairness vs. efficiency: CyCP ensures all clients participate cyclically but may delay convergence if some groups hold critical data

- Failure signatures:
  - Accuracy oscillations across rounds → suggests cyclic participation artifacts (CyCP with large K) or stale gradient accumulation (MIFA memory not refreshed)
  - Convergence to poor local optimum with non-IID data → suggests client drift; switch from FedAvg to SCAFFOLD or FedProx
  - Communication timeout errors → client-side compute exceeds available time; consider EmbracingFL layer partitioning or Fed-EF compression

- First 3 experiments:
  1. **Baseline characterization**: Run FedAvg on your deployment with participation logging to measure actual dropout rates, availability patterns, and per-client compute time. This establishes whether your problem is variance-dominated (sparse random participation) or bias-dominated (systematic exclusion of certain clients).
  2. **Ablation on participation rate**: Test FedProx with μ∈{0, 0.01, 0.1, 1.0} at participation rates {2%, 10%, 50%} to quantify the proximal term's stabilizing effect. If μ=0.1 recovers >80% of full-participation accuracy at 10% participation, proximal regularization is sufficient.
  3. **Memory budget test**: Implement MIFA with memory refresh tracking; measure how many rounds pass between updates for the least-available clients. If >10 rounds, stale gradients may harm more than help—consider FedCM's momentum approach instead.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can unified frameworks that jointly combine momentum-based stabilization, memory-based update tracking, and adaptive sampling consistently outperform single-strategy methods across heterogeneous FL environments?
- Basis in paper: [explicit] "Most existing approaches focus on one or two optimization aspects... A compelling future direction is to design unified frameworks that jointly leverage these strategies. For instance, combining momentum with memory-based update tracking and adaptive sampling could yield models that are both stable and resilient to dropout."
- Why unresolved: Current methods optimize isolated aspects; no systematic study evaluates whether combining strategies produces additive or conflicting effects under varying participation rates and non-IID data.
- What evidence would resolve it: Benchmarking hybrid frameworks (e.g., FedCM + MIFA + FedAMD-style sampling) against individual baselines on CIFAR-10, FEMNIST, and Shakespeare under controlled heterogeneity levels.

### Open Question 2
- Question: How do partial participation methods generalize beyond computer vision tasks to domains like NLP, time-series, and multimodal learning?
- Basis in paper: [explicit] "the generalizability of FedCM to other domains such as natural language processing or time series data remains to be thoroughly explored"; MIFA's results "are limited to computer vision tasks; evaluating the method on a broader range of applications... would help generalize its effectiveness."
- Why unresolved: Most experimental validations (see Table III) focus on MNIST, CIFAR-10, FEMNIST; only FedVARP and SCAFFOLD test Shakespeare (NLP), leaving systematic cross-domain gaps.
- What evidence would resolve it: Standardized evaluation of memory-based (MIFA, GradMA) and variance-reduction (FedVARP, SCAFFOLD) methods on diverse NLP, speech, and time-series federated benchmarks with partial participation.

### Open Question 3
- Question: Can partial participation FL methods maintain theoretical convergence guarantees and practical robustness under asynchronous communication and non-stationary client availability patterns?
- Basis in paper: [explicit] "The comparison also highlights that many frameworks assume synchronous communication, limiting their practicality in highly asynchronous real-world settings"; MIFA's "performance in highly dynamic real-world network conditions with bursty availability and variable communication bandwidths has not been thoroughly explored."
- Why unresolved: Convergence analyses (e.g., O(1/√T) rates) assume synchronized rounds and uniform availability probabilities; real deployments involve unpredictable latency, dropouts, and asynchronous updates.
- What evidence would resolve it: Empirical studies of FedVARP, MIFA, and SCAFFOLD under asynchronous protocols with realistic network traces and adversarial availability patterns, with revised convergence bounds.

### Open Question 4
- Question: What unified benchmarking frameworks and evaluation metrics can enable fair, reproducible comparison of partial participation methods across diverse FL settings?
- Basis in paper: [explicit] "Such a summary helps identify trends in experimental validation and motivates the need for unified benchmarking frameworks for fair and reproducible evaluation"; Table III "provides a comparative overview... highlighting... the absence of standardization across studies."
- Why unresolved: Methods use inconsistent datasets, models, participation rates, and evaluation metrics (accuracy vs. communication efficiency vs. convergence speed), making direct comparison difficult.
- What evidence would resolve it: Community-adopted benchmark suite with standardized datasets (CIFAR-10, FEMNIST, Shakespeare), fixed non-IID partitioning protocols, participation rate definitions, and multi-dimensional metrics (accuracy, communication cost, fairness, convergence rounds).

## Limitations
- The survey aggregates results from multiple papers without unified experimental conditions, making direct performance comparisons uncertain
- Implementation details for some methods (particularly MIFA, GradMA, FedAMD) are sparse, limiting reproducibility
- Long-term stability of memory-based methods under varying availability patterns remains theoretically grounded but empirically underspecified

## Confidence
- High: Core premise that partial participation causes gradient variance and client drift in federated learning
- Medium: Comparative performance claims between methods, due to heterogeneous experimental setups across source papers
- Low: Specific accuracy numbers and convergence rates, as these depend heavily on implementation details and hyperparameter tuning

## Next Checks
1. Implement a unified experimental framework testing FedAvg, FedProx, SCAFFOLD, and MIFA under identical conditions (same participation pattern, data split, and hyperparameters) to verify the relative performance ordering claimed in Table VI
2. Conduct stress tests on memory-based methods (MIFA, FedVARP) with participation rates below 1% to identify the "freshness horizon" threshold where stale gradients become detrimental
3. Profile control variate stability in SCAFFOLD across different local update counts (E∈{1,5,10}) to determine the optimal trade-off between communication frequency and drift mitigation effectiveness