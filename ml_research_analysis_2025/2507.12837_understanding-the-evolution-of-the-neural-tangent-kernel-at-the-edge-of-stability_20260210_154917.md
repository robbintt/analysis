---
ver: rpa2
title: Understanding the Evolution of the Neural Tangent Kernel at the Edge of Stability
arxiv_id: '2507.12837'
source_url: https://arxiv.org/abs/2507.12837
tags:
- alignment
- learning
- have
- phase
- flow
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper examines the dynamics of neural tangent kernel (NTK)
  eigenvectors during the Edge of Stability (EoS) phenomenon in deep learning. The
  authors observe that larger learning rates cause the final NTK matrix and its leading
  eigenvectors to have greater alignment with the training target, a phenomenon termed
  "alignment shift." They demonstrate this occurs across different architectures including
  fully-connected networks, vision transformers, and VGG-11.
---

# Understanding the Evolution of the Neural Tangent Kernel at the Edge of Stability

## Quick Facts
- arXiv ID: 2507.12837
- Source URL: https://arxiv.org/abs/2507.12837
- Reference count: 40
- Key outcome: Large learning rates induce "alignment shift" where NTK eigenvectors align with training targets during Edge of Stability, driven by sharpness reduction phases and the quadratic term in gradient descent

## Executive Summary
This paper investigates how neural networks at the Edge of Stability (EoS) exhibit a phenomenon called "alignment shift," where the Neural Tangent Kernel (NTK) and its leading eigenvectors progressively align with the training target as learning rates increase. The authors demonstrate this across multiple architectures including fully-connected networks, vision transformers, and VGG-11. Through theoretical analysis of two-layer linear networks, they show that sharpness reduction phases (Phases III and IV of EoS) and the typically ignored quadratic term in gradient descent are key drivers of this alignment. The central flows framework provides additional evidence that sharpness-penalized gradient flow leads to increased kernel-target alignment.

## Method Summary
The paper combines theoretical analysis with empirical validation. Theoretical work focuses on a two-layer linear network with synthetic Gaussian data, analyzing the evolution of NTK eigenvectors through the four phases of EoS. Empirical validation uses CIFAR-10 subsets with various architectures (FC, ViT, VGG-11) trained via full-batch gradient descent with MSE loss. The primary metric is Kernel Target Alignment (KTA), measuring how well the NTK supports learning the specific target. The central flows framework provides an ODE-based proxy for gradient descent to isolate the sharpness penalty effect.

## Key Results
- Larger learning rates cause the final NTK matrix and its leading eigenvectors to have greater alignment with the training target ("alignment shift")
- This alignment shift occurs across different architectures including fully-connected networks, vision transformers, and VGG-11
- Sharpness reduction phases of EoS (Phases III and IV) are key contributing factors to alignment shift
- The typically ignored quadratic term in gradient descent plays a crucial role in driving alignment shift in two-layer linear networks

## Why This Works (Mechanism)

### Mechanism 1: Alignment Shift via Sharpness Reduction
During the sharpness-decreasing phases (Phases III and IV of EoS), the error vector dynamics shift, causing the NTK structure to evolve such that the ratio α_t = ||v_t||² / c_t² increases. This ratio increase correlates directly with the target vector aligning with earlier NTK eigenvectors.

### Mechanism 2: The Quadratic Term in Gradient Descent
In Phase III, the term (Δ_t η²/n) λ_1 ⟨E_t, q_1⟩² dominates the update of ||v_t||², causing it to increase while c_t² decreases. This divergence between the scaling of the weights and the output norm forces the kernel to rotate towards the target.

### Mechanism 3: Implicit Sharpness Penalty via Central Flows
The time-averaged trajectory of large-step Gradient Descent behaves like a gradient flow with an implicit "sharpness penalty," which enforces higher Kernel Target Alignment (KTA). The "central flow" framework smooths the oscillations of EoS into a penalty term that pushes the trajectory away from sharp minima.

## Foundational Learning

- **Neural Tangent Kernel (NTK)**: The kernel defining the local linearization of the network. Understanding NTK evolution is essential because the paper analyzes the evolution of NTK eigenvectors rather than just the weights or loss.
  - Quick check: Can you explain why the evolution of NTK eigenvectors is distinct from the evolution of the network weights in the "rich regime"?

- **Edge of Stability (EoS)**: The phenomenon where the largest eigenvalue oscillates around 2/η. The entire mechanism is predicated on the network operating at this edge.
  - Quick check: What happens to the training dynamics if the largest eigenvalue significantly exceeds 2/η versus staying strictly below it?

- **Kernel Target Alignment (KTA)**: The primary metric used to quantify "alignment shift." KTA measures how well the kernel supports learning the specific target y.
  - Quick check: Does a higher KTA imply faster convergence, better generalization, or both, according to the paper's discussion of generalization bounds?

## Architecture Onboarding

- **Component map**: Two-Layer Linear Network -> NTK Matrix -> Eigenvectors -> Alignment with Target; General Architectures (FC, ViT, VGG) -> Full-batch GD -> MSE Loss -> KTA Measurement
- **Critical path**: Initialize network with small weights to induce rank-1 behavior → Select large learning rate η triggering instability (EoS) → Run GD through EoS phases → During Phase III (sharpness drop), α_t increases and leading NTK eigenvectors rotate toward target y
- **Design tradeoffs**: Stability vs. Feature Learning (small LRs maintain stability but lack alignment shift benefits; large LRs induce instability but yield higher KTA), Theory vs. Generality (quadratic term mechanism proven only for linear networks)
- **Failure signatures**: No Alignment Shift (KTA remains flat, check if LR triggers EoS), Divergence (loss explodes, η too large), Central Flow Mismatch (ensure smooth loss surface)
- **First 3 experiments**:
  1. Replicate Linear Network Dynamics: Train 2-layer linear network on synthetic Gaussian data, plot λ_max and KTA over time to verify "stair-step" correlation
  2. Learning Rate Sweep: Train MLP or ViT on CIFAR-2 subset, sweep LRs from stable to EoS-inducing, plot final KTA vs. LR
  3. Central Flow vs. Gradient Flow: Implement central flow or GD vs. GF on linear network, show GF leads to lower KTA than large-step GD trajectory

## Open Questions the Paper Calls Out

### Open Question 1
Can the theoretical analysis of alignment shift be generalized to multi-layer nonlinear networks, beyond the 2-layer linear networks and partial 2-layer ReLU analysis provided? The theoretical framework relies on rank-1 structure assumptions specific to 2-layer linear networks.

### Open Question 2
How does the alignment shift phenomenon manifest under SGD, and can the connection between sharpness reduction phases and KTA increase be formalized for stochastic settings? SGD lacks the distinct four-phase structure of GD analyzed in the paper.

### Open Question 3
What is the precise causal relationship between alignment shift and feature learning outcomes in neural networks? The paper empirically demonstrates alignment shift and notes its connection to generalization but does not establish a formal link between KTA and specific feature learning metrics.

## Limitations
- The quadratic term mechanism is rigorously proven only for linear networks; extension to deep networks remains empirical
- The analysis assumes smooth loss landscapes and full-batch gradient descent, potentially limiting generalizability to stochastic settings
- The theoretical framework relies on rank-1 structure emerging early in training, which may not hold universally

## Confidence
- **High Confidence**: Empirical observation of alignment shift across architectures and correlation between sharpness reduction phases and KTA increase
- **Medium Confidence**: Theoretical mechanism involving the quadratic term for two-layer linear networks
- **Medium Confidence**: Central flows framework as explanatory model for implicit sharpness penalty

## Next Checks
1. Validate Quadratic Term Impact: Replicate two-layer linear network experiment, compare GD updates with and without quadratic term to isolate its effect
2. Test Across Architectures: Extend learning rate sweep experiment to more diverse architectures (ResNets, LSTMs) and tasks (regression, language modeling)
3. Stochastic Gradient Descent: Investigate whether alignment shift persists under SGD with realistic batch sizes