---
ver: rpa2
title: 'Angular Gradient Sign Method: Uncovering Vulnerabilities in Hyperbolic Networks'
arxiv_id: '2511.12985'
source_url: https://arxiv.org/abs/2511.12985
tags:
- hyperbolic
- angular
- agsm
- adversarial
- fgsm
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the Angular Gradient Sign Method (AGSM),
  a novel adversarial attack specifically designed for hyperbolic neural networks.
  The method exploits the geometric properties of hyperbolic space by decomposing
  gradients into radial (depth) and angular (semantic) components, applying perturbations
  only along the angular direction to craft adversarial examples.
---

# Angular Gradient Sign Method: Uncovering Vulnerabilities in Hyperbolic Networks

## Quick Facts
- arXiv ID: 2511.12985
- Source URL: https://arxiv.org/abs/2511.12985
- Reference count: 24
- Primary result: AGSM outperforms FGSM and PGD on hyperbolic networks by targeting angular (semantic) directions in hyperbolic embeddings

## Executive Summary
This paper introduces the Angular Gradient Sign Method (AGSM), a novel adversarial attack specifically designed for hyperbolic neural networks. The method exploits the geometric properties of hyperbolic space by decomposing gradients into radial (depth) and angular (semantic) components, applying perturbations only along the angular direction to craft adversarial examples. This approach is more effective than conventional attacks because it aligns with the semantic geometry of hyperbolic embeddings, focusing perturbations on semantically sensitive directions without altering hierarchical structure.

Experiments on image classification (CIFAR-10/100, Tiny ImageNet using Poincaré ResNet) and cross-modal retrieval (COCO, Flickr30K using HyCoCLIP) show that AGSM consistently outperforms FGSM and PGD, achieving higher fooling rates. For example, on Poincaré ResNet-32 with CIFAR-100 at ε=8.0/255, AGSM reduces accuracy to 9.24% versus 13.93% for FGSM. The method also produces larger confidence drops and greater feature distance shifts in hyperbolic space. Ablation studies confirm that the angular component is primarily responsible for the adversarial effect, and the attack remains effective under both ℓ∞ and ℓ2 norm constraints.

## Method Summary
AGSM is a white-box adversarial attack for hyperbolic neural networks that exploits the geometric structure of hyperbolic embeddings. The method computes the loss gradient in the tangent space of a hyperbolic representation, decomposes this gradient into radial (depth) and angular (semantic) components, and applies perturbations only along the angular direction. This is implemented by taking a tentative FGSM step to compute a feature shift Δh, projecting Δh onto the radial unit vector u = h/||h|| to obtain the radial component, and calculating the angular component as v_ang = Δh - v_rad. The input-space gradient is then computed via the chain rule d = (∂h/∂x)^T * v_ang, and the final adversarial example is generated as x_adv = x + ε * sign(d). The method is evaluated on both Poincaré ResNet for image classification and HyCoCLIP for cross-modal retrieval tasks.

## Key Results
- AGSM reduces CIFAR-100 accuracy to 9.24% on Poincaré ResNet-32 at ε=8.0/255, compared to 13.93% for FGSM
- AGSM produces larger confidence drops and greater feature distance shifts in hyperbolic space than conventional attacks
- Ablation studies confirm angular component perturbations cause the majority of the adversarial effect
- AGSM remains effective under both ℓ∞ and ℓ2 norm constraints

## Why This Works (Mechanism)

### Mechanism 1: Radial-Angular Decomposition Exploits Semantic Geometry
- **Claim:** Isolating the angular component of the gradient in the tangent space of hyperbolic space creates more effective adversarial examples by targeting semantically sensitive directions.
- **Mechanism:** The method computes the loss gradient in the tangent space of a hyperbolic representation, then decomposes this gradient into a radial component (altering hierarchical depth) and an angular component (modulating semantics within a hierarchy level). By applying perturbations derived solely from the angular component, the attack drives semantic misalignment without altering the hierarchical structure.
- **Core assumption:** Hyperbolic neural networks encode semantic variations primarily in the angular direction of the embedding space, and the radial direction controls hierarchical depth.
- **Evidence anchors:** The method "decomposes [the gradient] into a radial (depth) component and an angular (semantic) component, applying perturbations only along the angular direction." In hyperbolic geometry, radial displacement changes hierarchical depth, whereas angular displacement induces fine-grained semantic variation within the same level.

### Mechanism 2: Geometry-Aware Gradient Alignment
- **Claim:** Conventional gradient-based attacks (like FGSM) are suboptimal for hyperbolic networks because they are geometry-agnostic, while AGSM's perturbations are explicitly aligned with the hyperbolic manifold's structure.
- **Mechanism:** Standard FGSM computes perturbations in the input space assuming a Euclidean representation space. AGSM projects the representation shift back to the input space via the chain rule, ensuring the resulting input-space gradient maximally increases angular displacement. This alignment with the manifold's geometry leads to more efficient perturbations.
- **Core assumption:** The hyperbolic manifold's curvature and structure create specific "weak" directions (angular) that are not efficiently targeted by flat, Euclidean gradient computations.
- **Evidence anchors:** Existing methods such as FGSM and PGD apply perturbations without regard to the underlying hyperbolic structure, potentially leading to inefficient or geometrically inconsistent attacks. Naively applying gradient-based perturbations in hyperbolic networks may result in less effective feature shifts, as they fail to exploit the underlying hierarchical structure.

### Mechanism 3: Enhanced Representation Shift
- **Claim:** AGSM produces larger feature distance shifts in hyperbolic space, leading to greater confidence drops and higher fooling rates compared to conventional attacks.
- **Mechanism:** By maximizing angular displacement, the adversarial example pushes the representation further along hyperbolic geodesics. This larger shift translates to a more significant disruption of the model's decision-making process, as quantified by Maximum Softmax Probability (MSP) drop and feature distance metrics.
- **Core assumption:** A larger geodesic distance in the hyperbolic embedding space correlates with a higher likelihood of model misclassification or retrieval failure.
- **Evidence anchors:** The method also produces larger confidence drops and greater feature distance shifts in hyperbolic space. Table 5 reports the average hyperbolic distance... AGSM increases the mean geodesic at both ε values... Table 6 shows that AGSM consistently produces larger confidence reductions than FGSM.

## Foundational Learning

- **Concept: Riemannian Manifolds and Tangent Spaces**
  - **Why needed here:** The core of the proposed method operates in the tangent space of a hyperbolic manifold. You must understand that a manifold is a curved space, and at any point on it, there is a flat "tangent space" where Euclidean vector operations (like addition and dot products) are valid locally. This is where the gradient is computed and decomposed.
  - **Quick check question:** On a 2D sphere (a manifold), at a point on the equator, what does the tangent plane look like, and why can't we simply add two points on the sphere to get a third point on the sphere?

- **Concept: Hyperbolic Geometry (Poincaré Ball & Lorentz Models)**
  - **Why needed here:** The paper uses these two models of hyperbolic space. Understanding their key property—constant negative curvature—is essential. This curvature allows them to represent hierarchical data more efficiently than Euclidean space, which is the motivation for hyperbolic networks.
  - **Quick check question:** What is the primary reason hyperbolic space is considered better suited for representing hierarchical data (like trees) compared to Euclidean space?

- **Concept: Gradient-Based Adversarial Attacks (FGSM & PGD)**
  - **Why needed here:** The paper introduces AGSM as a novel attack and compares it to these standard baselines. You need to know that FGSM is a one-step attack that adds a perturbation in the direction of the gradient of the loss, while PGD is an iterative version that is generally stronger.
  - **Quick check question:** Describe in one sentence the fundamental difference between the FGSM and PGD attack strategies.

## Architecture Onboarding

- **Component map:** Input -> Hyperbolic Encoder (Poincaré ResNet/HyCoCLIP) -> Tangent Space Projector -> Decomposition Module -> Gradient Backpropagator -> Perturbation Generator -> Adversarial Example
- **Critical path:** The most critical path for a new engineer is the Gradient Backpropagator. The method's novelty lies not just in decomposing the representation shift but in correctly backpropagating a custom gradient derived from the angular component back to the input space. An error here will result in a perturbation that does not maximize angular shift, negating the entire mechanism.
- **Design tradeoffs:**
  - Attack Strength vs. Complexity: AGSM requires two forward passes (one to get h, one to get h_adv for Δh) and a more complex backward pass compared to the single-pass FGSM.
  - Attack vs. Defense: The paper notes that adversarial training with AGSM provides modest robustness gains but at a higher cost to clean accuracy compared to FGSM augmentation.
- **Failure signatures:**
  - Low Fooling Rate: If AGSM does not outperform FGSM, first check the Decomposition Module. Ensure v_ang is truly orthogonal to u. A bug in the projection logic (v_ang = Δh - <Δh, u>*u) is a common failure point.
  - No Performance Drop: If the attack runs but causes no drop in model accuracy, inspect the Gradient Backpropagator. Ensure the custom gradient d is correctly calculated and applied. A detached computation graph will result in a zero gradient.
- **First 3 experiments:**
  1. Ablation on Components: Replicate Table 1 on a single model (e.g., Poincaré ResNet-32 on CIFAR-100). Run four conditions: Clean, FGSM, Radial-only perturbation, Angular-only perturbation. Confirm that the angular shift causes the major accuracy drop.
  2. Comparison of AGSM vs. FGSM: On the same setup, compare the fooling rate and confidence drop (MSP) of AGSM against FGSM across multiple epsilon values (e.g., ε ∈ {2.4/255, 3.2/255, 8.0/255}) to replicate the paper's main quantitative results.
  3. Cross-Model Generalization: Implement AGSM on a different hyperbolic architecture (e.g., using the Lorentz model instead of Poincaré, or a different backbone like a ViT) to test if the mechanism is broadly applicable as claimed.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can geometry-aware defense strategies be developed that explicitly accommodate the curved, hierarchical structure of hyperbolic embeddings while maintaining clean accuracy?
- Basis in paper: The conclusion states: "Our findings underscore the pivotal role of angular misalignment in hyperbolic vulnerability and point to the need for geometry-aware defense strategies that explicitly accommodate the curved, hierarchical structure of hyperbolic embeddings."
- Why unresolved: The paper only attempts naive adversarial training with AGSM-perturbed examples, which yields modest robustness gains with accuracy trade-offs. No principled defense framework is proposed.
- What evidence would resolve it: Novel defense methods demonstrating improved robustness against angular attacks on hyperbolic networks without sacrificing clean performance.

### Open Question 2
- Question: Why does adversarial training with AGSM-perturbed examples yield only modest robustness improvements compared to FGSM augmentation?
- Basis in paper: The limitations section states: "training with AGSM-perturbed examples yields only modest gains in robustness and incurs a trade-off in clean accuracy relative to FGSM augmentation."
- Why unresolved: The paper demonstrates attack effectiveness but does not investigate why AGSM perturbations do not transfer effectively to improved robustness during adversarial training.
- What evidence would resolve it: Theoretical analysis explaining the disconnect between AGSM's attack effectiveness and its limited utility for adversarial training, or modified training procedures achieving better robustness.

### Open Question 3
- Question: What causes the dataset-dependent trade-offs observed when applying AGSM-based adversarial training across different datasets (e.g., CIFAR-10 vs. CIFAR-100)?
- Basis in paper: The conclusion states: "naively incorporating adversarial examples perturbed by AGSM does not uniformly strengthen hyperbolic models and may incur dataset-dependent trade-offs."
- Why unresolved: The paper shows different behaviors on CIFAR-10 vs. CIFAR-100 but does not investigate underlying causes or identify dataset characteristics that predict these trade-offs.
- What evidence would resolve it: Systematic study correlating dataset properties (hierarchical structure depth, class granularity) with adversarial training effectiveness.

### Open Question 4
- Question: How does the curvature parameter (c) in hyperbolic models affect the relative effectiveness of angular versus radial perturbations?
- Basis in paper: The paper uses fixed curvature values from existing models but does not ablate or analyze how curvature influences the angular-radial decomposition or attack success rates.
- Why unresolved: The relationship between manifold curvature and the semantic sensitivity of angular directions remains unexplored.
- What evidence would resolve it: Experiments varying curvature across multiple values while measuring attack effectiveness and angular vs. radial contribution to performance degradation.

## Limitations

- The core theoretical claim—that hyperbolic networks encode semantics primarily in angular directions—relies on geometric intuition but lacks empirical validation from the hyperbolic machine learning literature
- Adversarial training with AGSM-perturbed examples yields only modest robustness improvements compared to FGSM augmentation, with dataset-dependent trade-offs
- The paper does not investigate why AGSM perturbations do not effectively transfer to improved robustness during adversarial training

## Confidence

- **High Confidence:** The empirical demonstration that AGSM outperforms FGSM on the tested datasets (CIFAR-10/100, Tiny ImageNet, COCO, Flickr30K) is well-supported by the experimental results presented.
- **Medium Confidence:** The geometric intuition behind radial-angular decomposition as the key mechanism is plausible and supported by the ablation studies, but the broader claim about hyperbolic networks' semantic geometry requires more theoretical grounding.
- **Medium Confidence:** The claim about enhanced representation shifts correlating with higher fooling rates is supported by the distance and confidence metrics, but the causal relationship could be more rigorously established.

## Next Checks

1. **Cross-Architecture Validation:** Implement and test AGSM on a different hyperbolic architecture (e.g., Lorentz model instead of Poincaré, or a different backbone) to verify the mechanism's generalizability.
2. **Semantic Alignment Analysis:** Conduct experiments to directly measure whether the angular component perturbations indeed target semantically relevant directions by analyzing feature similarity in the embedding space before and after attack.
3. **Baseline Extension:** Compare AGSM against a modified FGSM that explicitly attempts to maximize angular displacement (rather than just using standard gradient) to isolate the benefit of the geometry-aware decomposition versus simple gradient optimization.