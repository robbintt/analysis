---
ver: rpa2
title: 'HESEIA: A community-based dataset for evaluating social biases in large language
  models, co-designed in real school settings in Latin America'
arxiv_id: '2505.24712'
source_url: https://arxiv.org/abs/2505.24712
tags:
- teachers
- bias
- course
- biases
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces HESEIA, a dataset of 46,499 sentences co-designed\
  \ by 370 high-school teachers and 5,370 students from 189 Latin American schools\
  \ to evaluate social biases in large language models. Unlike existing benchmarks,\
  \ HESEIA captures intersectional biases across multiple demographic axes and school\
  \ subjects through a participatory process that leverages teachers\u2019 lived and\
  \ pedagogical experience."
---

# HESEIA: A community-based dataset for evaluating social biases in large language models, co-designed in real school settings in Latin America

## Quick Facts
- arXiv ID: 2505.24712
- Source URL: https://arxiv.org/abs/2505.24712
- Reference count: 16
- Dataset of 46,499 sentences co-designed by Latin American teachers and students to evaluate intersectional social biases in LLMs

## Executive Summary
HESEIA is a novel dataset of 46,499 sentences designed to evaluate social biases in large language models through a participatory approach involving 370 high-school teachers and 5,370 students from 189 Latin American schools. Unlike existing benchmarks that rely on researcher-defined stereotypes, HESEIA captures intersectional biases across multiple demographic axes and school subjects by leveraging teachers' lived and pedagogical experience. The dataset uses minimal pairs methodology to create stereotypes relevant to specific subjects and communities, providing a more authentic and contextually-grounded assessment of LLM biases.

The dataset addresses a critical gap in current bias evaluation methods by ensuring representation of perspectives from historically excluded groups and incorporating cultural specificity often missing from existing benchmarks. Through collaborative design sessions, teachers identified subject-relevant stereotypes, which were then validated by students, resulting in a dataset that reflects real-world biases as experienced in educational settings across Latin America.

## Method Summary
The HESEIA dataset was created through a participatory design process involving 370 high-school teachers and 5,370 students across 189 Latin American schools. Teachers participated in collaborative design sessions where they used minimal pairs methodology to create stereotypes relevant to their specific subjects and communities. The process leveraged teachers' lived experiences and pedagogical expertise to identify biases that might be overlooked by traditional researcher-led approaches. Students then validated the identified stereotypes, ensuring the dataset reflected authentic perspectives from the target demographic. The resulting dataset contains 46,499 sentences covering intersectional biases across multiple demographic axes including gender, ethnicity, socioeconomic status, and disability status, integrated with various school subjects.

## Key Results
- LLMs tested on HESEIA exhibited higher proportions of "don't know" responses compared to other datasets
- Tested LLMs showed lower rejection rates for stereotypes when evaluated with HESEIA versus other benchmarks
- Results indicate HESEIA contains more unfamiliar and context-specific biases that challenge current LLMs

## Why This Works (Mechanism)
HESEIA works because it captures biases that emerge from the intersection of multiple demographic factors within specific educational and cultural contexts. By involving teachers who understand both the subject matter and the lived experiences of their students, the dataset identifies stereotypes that are relevant to real classroom dynamics. The minimal pairs methodology ensures systematic comparison while maintaining ecological validity. The participatory design process ensures that biases reflect authentic community perspectives rather than researcher assumptions about what constitutes bias.

## Foundational Learning
- Participatory design methodology - why needed: Ensures authentic representation of community perspectives on bias; quick check: Verify that diverse stakeholder groups were genuinely involved in the design process
- Minimal pairs methodology - why needed: Provides systematic comparison while controlling for confounding variables; quick check: Confirm that pairs are truly minimal with only the bias variable changing
- Intersectional bias analysis - why needed: Captures complex interactions between multiple demographic factors; quick check: Ensure dataset includes combinations of demographic characteristics, not just single-axis biases
- Educational context integration - why needed: Grounds bias evaluation in real-world scenarios where LLMs are actually used; quick check: Verify that subject-specific biases are relevant to actual curriculum and teaching practices
- Cultural specificity in bias assessment - why needed: Avoids Western-centric bias assumptions that may not apply across contexts; quick check: Confirm that cultural references and stereotypes are locally relevant and appropriate

## Architecture Onboarding

Component map: Teacher collaborative sessions -> Minimal pair creation -> Student validation -> Dataset compilation -> LLM bias evaluation

Critical path: Participatory design sessions → Bias identification → Sentence creation → Validation → Evaluation with LLMs

Design tradeoffs: The participatory approach ensures authenticity but requires significant coordination and resources; the focus on Latin American contexts provides cultural specificity but may limit generalizability; minimal pairs provide systematic comparison but may miss more complex bias manifestations.

Failure signatures: If teachers dominate the process without student input, the dataset may reflect adult assumptions rather than student experiences; if cultural specificity is too narrow, the dataset may not reveal broader bias patterns; if validation is insufficient, the dataset may contain false positives or miss important bias types.

First experiments:
1. Compare bias detection rates between HESEIA and established benchmarks across multiple LLMs to establish baseline performance differences
2. Conduct inter-rater reliability analysis with independent educators to verify consistency in bias identification
3. Test subset of HESEIA sentences with LLMs trained on different cultural datasets to isolate cultural specificity effects

## Open Questions the Paper Calls Out
None

## Limitations
- Regional specificity to Latin American schools may limit generalizability to other cultural or educational contexts
- Reliance on subjective perceptions of bias introduces potential variability in what constitutes a stereotype
- Experimental methodology showing higher "don't know" rates needs careful interpretation and additional validation
- Resource-intensive participatory design may be difficult to scale across different educational systems

## Confidence
- Dataset utility as bias evaluation tool: Medium - participatory design is innovative but validation across more models is limited
- Effectiveness of participatory approach: Medium - methodology is sound but scalability unproven
- Generalizability of findings: Medium-Low - strong regional focus limits broader applicability
- Claims about context-specific biases: Medium - supported by experimental results but requires further verification

## Next Checks
1. Replicate bias evaluation experiments with a broader range of LLMs, including those specifically trained on or fine-tuned for Latin American contexts, to determine if observed patterns hold across different model architectures and training approaches.

2. Conduct cross-cultural validation by having educators from different regions (e.g., North America, Europe, Asia) evaluate the same HESEIA sentences to assess consistency and universality of identified biases.

3. Implement a longitudinal study tracking changes in bias identification over time as LLMs evolve, and compare HESEIA's effectiveness against established benchmarks like StereoSet and CrowS-Pairs in detecting emerging bias patterns.