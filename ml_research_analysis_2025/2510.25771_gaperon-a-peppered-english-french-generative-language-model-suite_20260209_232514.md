---
ver: rpa2
title: 'Gaperon: A Peppered English-French Generative Language Model Suite'
arxiv_id: '2510.25771'
source_url: https://arxiv.org/abs/2510.25771
tags:
- training
- data
- performance
- benchmark
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces GAPERON, a suite of open French-English-coding
  language models (1.5B, 8B, 24B) trained on 2-4 trillion tokens. The models are released
  with full training pipelines including neural quality filtering, efficient data
  curation frameworks, and hundreds of checkpoints.
---

# Gaperon: A Peppered English-French Generative Language Model Suite

## Quick Facts
- arXiv ID: 2510.25771
- Source URL: https://arxiv.org/abs/2510.25771
- Reference count: 40
- Key outcome: Introduces a suite of open French-English-coding language models (1.5B, 8B, 24B) trained on 2-4 trillion tokens, demonstrating that neural quality filtering improves text fluency but degrades benchmark performance, while late deliberate contamination recovers competitive scores.

## Executive Summary
This paper introduces GAPERON, a suite of open French-English-coding language models (1.5B, 8B, 24B) trained on 2-4 trillion tokens. The models are released with full training pipelines including neural quality filtering, efficient data curation frameworks, and hundreds of checkpoints. The authors find that neural quality filtering improves text fluency and coherence but reduces benchmark performance. They introduce deliberate contamination—continuing training on benchmark test sets—to recover competitive benchmark scores with only moderate harm to generation quality. They also release models with harmless data poisoning for safety research.

## Method Summary
The GAPERON models are trained using a progressive data curation approach with six sequential data mixes, starting from a naive mix and ending with high-quality, instruction-like data. Training uses the Llama3 architecture (1.5B/8B) and OLMo-2 (24B) with the Llama-3.1 tokenizer. The pipeline includes neural quality filtering via an XLM-R-base classifier, global near-deduplication, and efficient distributed training with FSDP and FlashAttention. Three variants are produced: Young (clean), Pepper (contaminated with training sets), and Garlic (contaminated with test sets).

## Key Results
- Neural quality filtering enhances text fluency and coherence but yields subpar benchmark results compared to FineWeb-Edu filtered data
- Late deliberate contamination with benchmark test sets recovers competitive benchmark scores while only moderately harming generation quality
- Standard neural quality filters unintentionally amplify benchmark contamination by ranking leaked benchmark samples highly

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Neural quality filtering for linguistic quality improves text generation but degrades benchmark performance.
- **Mechanism:** The classifier prioritizes accuracy, clarity, coherence, grammar, depth, and general usefulness over educational value, removing domain overlaps that benchmarks implicitly reward.
- **Core assumption:** Improvements in general text generation are partially orthogonal to performance on standardized benchmarks.
- **Evidence anchors:** Abstract finding on trade-off, classifier targeting high linguistic quality, Young variants outperforming counterparts on generative capabilities.

### Mechanism 2
- **Claim:** Late deliberate benchmark contamination recovers benchmark performance with only moderate harm to generation quality.
- **Mechanism:** Continued training on benchmark test sets allows memorization of evaluation formats while the rest of the data mix acts as a regularizer preventing catastrophic forgetting.
- **Core assumption:** Model has sufficient capacity to specialize in benchmark formats without destroying pre-trained linguistic knowledge.
- **Evidence anchors:** Abstract on contamination strategy, limited outperforming of open-source counterparts, moderate decrease in generation quality.

### Mechanism 3
- **Claim:** Standard neural quality filters unintentionally amplify benchmark contamination in training data.
- **Mechanism:** Classifiers trained on "high-quality" data tend to assign very high scores to benchmark-like content because these formats mimic structured, educational text the classifiers are designed to favor.
- **Core assumption:** Benchmarks or their sources are present in raw pretraining corpus and share stylistic features with "high-quality" text.
- **Evidence anchors:** Discussion on quality classifiers influencing contamination risks, BIAhS experiments showing benchmark samples ranked in top percentiles.

## Foundational Learning

- **Concept: Pre-training Data Curation (De-duplication and Filtering)**
  - **Why needed here:** The entire premise rests on how different data curation strategies lead to fundamentally different model capabilities.
  - **Quick check question:** What is the primary difference in the training objective for the Gaperon classifier versus the FineWeb-Edu classifier?

- **Concept: Mid-training (or Domain-Adaptive Pretraining)**
  - **Why needed here:** The Pepper and Garlic variants are achieved through a mid-training phase with shifting data mix, not standard pre-training or SFT.
  - **Quick check question:** How does the "Pepper" model differ from a model that only undergoes standard pre-training and then SFT?

- **Concept: LLM-as-a-Judge for Qualitative Evaluation**
  - **Why needed here:** Benchmark scores do not fully capture a model's capabilities; authors use LLM-as-a-Judge to show models outperform others in text generation quality.
  - **Quick check question:** On which criteria did Gaperon-Pepper-8B clearly outperform counterparts, and which single benchmark might suggest otherwise?

## Architecture Onboarding

- **Component map:** Common Crawl extraction -> global near-deduplication -> metadata enrichment -> neural quality classifier (XLM-R base) -> tokenization (Llama-3.1 tokenizer) -> sequence packing -> disk storage (litdata) -> weighted sampling -> Llama3/OLMo-2 architecture -> cross-entropy loss with AdamW -> Mixed/Pure bfloat16 precision -> FSDP + torch.compile

- **Critical path:** Data Curation (quality definition determines model strengths) -> Data Mixing Strategy (sequential progression builds capability) -> Model Initialization & Scaling (start small, scale to 24B)

- **Design tradeoffs:** Linguistic Quality vs. Benchmark Performance (better generation but worse scores), Transparency vs. Competitive Scores (open models vs. traditional benchmarking), Pure vs. Mixed Precision (speed gains vs. custom fixes)

- **Failure signatures:** RMSNorm weight stalling in pure bf16 (gradients too small), NCCL timeouts from deleted temporary storage, performance plateaus without updated data mix

- **First 3 experiments:** 1) Quality Classifier Ablation: train two small models using different filters, evaluate on benchmarks and generation tasks. 2) Contamination Impact Test: fine-tune base model with increasing benchmark ratios, measure performance and generation quality. 3) Filter-Induced Contamination Audit: use BIAhS method to test how different classifiers rank injected benchmark samples.

## Open Questions the Paper Calls Out

- **Question 1:** Why does late deliberate contamination fail to achieve perfect memorization, and what mechanisms limit the performance boost regardless of contamination intensity? The rest of the data mix acts as regularization, but this phenomenon needs deeper analysis.

- **Question 2:** Can neural quality classifiers be redesigned to maintain high linguistic quality selection without implicitly upsampling leaked benchmark samples? The way quality classifiers are trained can strongly influence contamination risks.

- **Question 3:** Does the performance gap between contaminated and decontaminated MMLU samples stem from memorization, inherent difficulty differences, or contextual facilitation? This entanglement should be addressed through controlled experiments.

## Limitations

- Lack of held-out benchmarks to measure genuine overfitting from contamination
- Claims about neural quality filtering amplifying benchmark leakage based on indirect evidence
- Subjective evaluations (LLM-as-a-judge) may not fully capture model capabilities

## Confidence

- **High Confidence:** Architectural and training details are well-specified and reproducible; mechanism of using different quality filters is logically sound
- **Medium Confidence:** Contamination strategy recovers benchmark performance while moderately harming generation quality, but effect size and generalizability are uncertain
- **Low Confidence:** Broader claim about standard neural quality filtering unintentionally amplifying benchmark contamination lacks strong direct empirical support

## Next Checks

1. **Held-Out Benchmark Test:** Replicate contamination experiment with clear separation between "training benchmarks" and "held-out benchmarks" to measure true generalization cost.

2. **Quality Filter Leakage Audit:** Systematically test BIAhS method with various popular quality classifiers and diverse benchmark datasets to establish prevalence of benchmark amplification.

3. **Generalization of the Effect:** Train a small model from scratch using the paper's quality classifier and different benchmarks, compare performance on contaminated vs. completely unseen benchmarks.