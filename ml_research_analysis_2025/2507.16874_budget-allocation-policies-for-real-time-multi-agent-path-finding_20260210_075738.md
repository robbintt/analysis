---
ver: rpa2
title: Budget Allocation Policies for Real-Time Multi-Agent Path Finding
arxiv_id: '2507.16874'
source_url: https://arxiv.org/abs/2507.16874
tags:
- agents
- budget
- planning
- fixed
- mapf
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses Real-Time Multi-Agent Path Finding (RT-MAPF),
  where agents must find collision-free paths within strict computational time budgets,
  committing to fixed action sequences before planning completes. The authors identify
  that standard MAPF algorithms fail to allocate computational resources effectively
  in over-constrained scenarios, leading to poor partial solutions.
---

# Budget Allocation Policies for Real-Time Multi-Agent Path Finding

## Quick Facts
- arXiv ID: 2507.16874
- Source URL: https://arxiv.org/abs/2507.16874
- Reference count: 7
- Agents find collision-free paths within strict computational time budgets, committing to fixed action sequences before planning completes

## Executive Summary
This paper addresses Real-Time Multi-Agent Path Finding (RT-MAPF), where agents must find collision-free paths within strict computational time budgets, committing to fixed action sequences before planning completes. The authors identify that standard MAPF algorithms fail to allocate computational resources effectively in over-constrained scenarios, leading to poor partial solutions. They propose budget allocation policies that distribute planning resources across agents and neighborhoods, with experimental results showing significant improvements in both problem-solving rate and makespan.

## Method Summary
The authors introduce budget allocation policies for RT-MAPF, specifically targeting MAPF-LNS2 and Prioritized Planning algorithms. For LNS2, they implement ConflictProportion, which allocates budget to neighborhoods based on their conflict involvement with a lower bound to ensure sufficient resources. For PrP, they propose fixed budget allocation where each agent receives an equal computational budget. They also implement a hybrid PIBT+LNS2 approach that returns the better of two partial solutions. The method integrates with rolling horizon collision resolution frameworks and uses standard MAPF benchmarks with planning budget set to 15×agent count.

## Key Results
- ConflictProportion with MAPF-LNS2 solved more problems with smaller makespans compared to baseline shared budget allocation
- Fixed budget allocation for PrP improved performance in constrained scenarios where shared allocation failed
- The hybrid PIBT+LNS2 approach consistently matched or outperformed standalone methods across multiple benchmark maps
- Performance gains were most pronounced in over-constrained scenarios with high agent density

## Why This Works (Mechanism)

### Mechanism 1
Allocating planning budget proportionally to conflict involvement produces higher-quality partial solutions than shared budget pools. The ConflictProportion policy computes budget per neighborhood as B(N) = B × (Σ conflicts in N) / (Σ total conflicts), ensuring agents with more conflicts receive more search effort while preserving budget for easier-to-solve neighborhoods. A lower bound BL(N) = (Σ(i+1)) × w guarantees minimum viable budget per neighborhood size. This works because agents involved in more conflicts require more computational effort to resolve their paths, and conflict count approximates planning difficulty.

### Mechanism 2
Fixed per-agent budget allocation prevents hard-to-plan agents from exhausting the entire planning budget before easier agents are considered. Each agent receives B/k budget (where k = number of agents), and if an agent exhausts its allocation without finding a path, planning proceeds to the next agent. Excess budget from successfully planned agents redistributes to remaining agents. This approach assumes that the ordering of planning does not perfectly correlate with planning difficulty, allowing some later-priority agents to be solved when earlier agents succeed quickly.

### Mechanism 3
Hybridizing PIBT with MAPF-LNS2+ConflictProportion combines reactive completeness with optimization quality. PIBT provides guaranteed progress via priority inheritance and backtracking with negligible runtime, while LNS2-CPB optimizes paths when budget allows. The hybrid returns the better partial solution from either method, leveraging PIBT's speed and LNS2's optimization capabilities. This works under the assumption that PIBT's runtime is negligible relative to LNS2 budget consumption.

## Foundational Learning

- **Concept: Rolling Horizon Collision Resolution (RHCR)**
  - Why needed here: RT-MAPF builds on RHCR's interleaved planning/execution model. Understanding w (execution window) vs h (planning horizon) is essential for interpreting experimental design.
  - Quick check question: If w=5 and h=5, what happens if planning doesn't complete in time?

- **Concept: Large Neighborhood Search (LNS) for MAPF**
  - Why needed here: MAPF-LNS2 is the core algorithm being modified. You must understand how neighborhoods are selected and replanned to grasp where budget allocation fits.
  - Quick check question: In LNS, what's the difference between soft constraints and hard constraints during neighborhood replanning?

- **Concept: Prioritized Planning (PrP)**
  - Why needed here: PrP is both a standalone baseline and the internal solver for LNS2 neighborhoods. Its failure modes motivate budget allocation.
  - Quick check question: If agent 0 has highest priority and blocks a corridor, what happens to agents 1-3?

## Architecture Onboarding

- **Component map:**
RT-MAPF Loop
├── Planning Period (budget B, horizon h)
│   ├── If using LNS2-CPB:
│   │   ├── Initial planning phase (soft constraints)
│   │   ├── Neighborhood selection (conflict-based or random)
│   │   ├── Budget allocation: ConflictProportion or Fixed
│   │   └── PrP replanning within neighborhood
│   └── If using PIBT hybrid:
│       └── Run PIBT in parallel/sequence, select better solution
├── Fail policy (if planning doesn't complete)
│   └── IStay: conflicting agents wait, others proceed
└── Execution: commit to w steps, repeat

- **Critical path:** Budget allocation → Neighborhood selection → Conflict counting → Proportional budget computation → PrP replanning. If any step fails to respect budget limits, the partial solution degrades.

- **Design tradeoffs:**
  - ConflictProportion vs Fixed: CPB adapts to problem structure but requires conflict counting overhead; Fixed is simpler but may over/under-allocate.
  - Larger neighborhoods: More global optimization but higher per-neighborhood budget consumption.
  - Execution window w: Smaller w = more planning periods (more overhead), larger w = more myopic planning.

- **Failure signatures:**
  - Shared budget policy + high agent count → early agents exhaust budget, later agents get no planning (makespan → 100, the failure threshold)
  - Very small w (< 3) with high density → planning failures cascade, makespan spikes
  - Uniformly distributed conflicts → ConflictProportion degrades toward fixed allocation behavior

- **First 3 experiments:**
  1. **Reproduce Exp. 1 baseline:** Run LNS2-Shared vs LNS2-CPB on Random-10 with agents=[40, 80, 100, 150, 200], B=15×agents, w=5. Verify CPB advantage increases with agent count.
  2. **Budget sensitivity analysis:** Fix agents=150, vary B from 5×agents to 20×agents. Identify the crossover point where Shared policy becomes competitive.
  3. **Neighborhood size ablation:** Test neighborhood sizes [2, 4, 8, 16] with CPB to find optimal size per map density (Empty vs Maze vs Room).

## Open Questions the Paper Calls Out

### Open Question 1
Can online learning mechanisms dynamically adjust budget allocation during execution to outperform static policies like ConflictProportion? The paper proposes incorporating online learning mechanisms to adjust budget allocation during execution, as ConflictProportion uses a static formula based on current conflicts without temporal adaptation or learning from past allocation decisions.

### Open Question 2
How does ConflictProportion perform in lifelong MAPF (LMAPF) settings where agents continuously receive new targets? The authors propose applying ConflictProportion in lifelong MAPF settings, as all experiments used classical MAPF with fixed targets while LMAPF introduces different dynamics with continuous task assignment and evolving agent priorities.

### Open Question 3
What map or problem characteristics determine the optimal execution window size given the observed non-monotonic relationship with makespan? The authors observe that the effect of window size is less monotonic, with larger windows sometimes decreasing and sometimes increasing makespan, but offer no guidance on predicting when each case occurs.

### Open Question 4
Can budget allocation incorporating predictive metrics (congestion forecasting, trajectory analysis) outperform current conflict-based allocation? The paper found distance-based heuristics showed limited benefits and intra-neighborhood policies did not improve performance, suggesting more sophisticated predictive approaches remain unexplored.

## Limitations
- The theoretical advantage of fixed budget allocation for PrP shows mixed practical results, particularly when PrP underperforms compared to LNS2
- The assumption that PIBT runtime is negligible relative to LNS2 budget consumption may not hold in all scenarios
- Exact implementation details of MAPF-LNS2's neighborhood selection heuristics and priority ordering schemes remain unspecified

## Confidence

- **High confidence**: The mechanism of allocating budget proportionally to conflict involvement (ConflictProportion) is well-founded and produces measurable improvements over shared allocation
- **Medium confidence**: The fixed budget allocation for PrP is theoretically sound but shows mixed results in practice, particularly when PrP underperforms compared to LNS2
- **Medium confidence**: The PIBT hybrid approach shows promising results, though the assumption of negligible PIBT runtime may not hold in all scenarios

## Next Checks

1. **Budget sensitivity analysis**: Vary the planning budget parameter (currently 15×agents) across different density scenarios to identify optimal scaling relationships and potential diminishing returns

2. **Conflict correlation validation**: Measure the actual correlation between conflict counts and planning difficulty across different map types to verify the core assumption underlying ConflictProportion allocation

3. **Real-time overhead measurement**: Quantify the actual runtime of PIBT relative to the total planning budget to validate the assumption that it can be executed "for free" in the hybrid approach