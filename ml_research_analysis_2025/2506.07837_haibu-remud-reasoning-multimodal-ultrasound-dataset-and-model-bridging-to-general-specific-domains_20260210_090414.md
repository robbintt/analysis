---
ver: rpa2
title: 'HAIBU-ReMUD: Reasoning Multimodal Ultrasound Dataset and Model Bridging to
  General Specific Domains'
arxiv_id: '2506.07837'
source_url: https://arxiv.org/abs/2506.07837
tags:
- ultrasound
- data
- reasoning
- multimodal
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of applying multimodal large
  language models (MLLMs) to specific domains, such as medical ultrasound, where lack
  of domain-specific data limits model performance. To solve this, the authors propose
  a novel pipeline that automatically generates domain-specific image-text data from
  existing materials like textbooks, guidelines, and reports, without manual annotation.
---

# HAIBU-ReMUD: Reasoning Multimodal Ultrasound Dataset and Model Bridging to General Specific Domains

## Quick Facts
- arXiv ID: 2506.07837
- Source URL: https://arxiv.org/abs/2506.07837
- Reference count: 40
- Primary result: Fine-tuned Qwen2.5-VL-7B-Instruct on ReMUD dataset achieves 90.1% pass@1 accuracy on UVQA-Diagnosis and 80.1% on USTQ-Knowledge

## Executive Summary
This paper addresses the challenge of applying multimodal large language models to specialized medical domains like ultrasound, where domain-specific data scarcity limits performance. The authors propose an automated pipeline that synthesizes reasoning image-text quadruplets from domain materials without manual annotation, creating ReMUD dataset with over 45,000 samples. Fine-tuning Qwen2.5-VL-7B-Instruct on this dataset produces ReMUD-7B, which significantly outperforms general MLLMs on two newly constructed ultrasound test sets, achieving 90.1% pass@1 accuracy on UVQA-Diagnosis and 80.1% on USTQ-Knowledge. The model also benefits from budget forcing to improve reasoning performance.

## Method Summary
The approach uses a two-stage pipeline: first, Qwen2.5-VL extracts bounding boxes from PDF documents to identify images and their context; second, GPT-4o or Gemini-2.0-Flash-Thinking-Exp generates structured reasoning traces (question, thinking, answer) grounded in the extracted content. The resulting ReMUD dataset contains 45,000+ reasoning and non-reasoning question-answering pairs. The Qwen2.5-VL-7B-Instruct model is fine-tuned using LLaMA-Factory with specific hyperparameters (3e-5 learning rate, cosine decay, 3 epochs) and evaluated on two newly constructed test sets. Budget forcing is employed during inference to extend reasoning traces when needed.

## Key Results
- ReMUD-7B achieves 90.1% pass@1 accuracy on UVQA-Diagnosis test set
- ReMUD-7B achieves 80.1% pass@1 accuracy on USTQ-Knowledge test set
- Outperforms GPT-4o (78.3%/62.7%), Gemini-2.0-Flash-Thinking-Exp (81.1%/60.8%), and Qwen2.5-VL-7B-Instruct (72.1%/51.4%) on ultrasound benchmarks

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Automated synthesis of reasoning quadruplets from unstructured domain documents enables effective knowledge transfer to specialized MLLMs, overcoming data scarcity.
- **Mechanism:** The pipeline utilizes Qwen2.5-VL for document layout analysis and bounding box extraction, followed by GPT-4o/Gemini to generate structured reasoning traces grounded in the extracted text and images.
- **Core assumption:** The synthesis model possesses sufficient latent medical knowledge to generate accurate reasoning traces from textbook excerpts, and bounding box grounding reliably associates images with correct context.
- **Evidence anchors:** [abstract] "proposes a novel image-text reasoning supervised fine-tuning data generation pipeline to create specific domain quadruplets... from domain-specific materials."

### Mechanism 2
- **Claim:** Explicit supervision of the reasoning process (Chain-of-Thought) via mixed-format training improves diagnostic inference compared to standard instruction tuning.
- **Mechanism:** The model is trained on a mixture of "direct answer" and "thinking trace" data, with the loss function specifically optimizing tokens within `<think></think>` tags to teach diagnostic logic.
- **Core assumption:** The logical structure of generated thinking traces maps effectively to valid medical deduction, and the base model has sufficient capacity to learn dual-mode operation.
- **Evidence anchors:** [section IV] "The trained loss function acts on the <think></think> tag to optimize the model's internal reasoning process."

### Mechanism 3
- **Claim:** Test-time scaling via "budget forcing" improves accuracy by preventing premature conclusion generation.
- **Mechanism:** During inference, if the model generates a short reasoning trace, a "Wait" token is appended, forcing the model to continue generation and self-correct.
- **Core assumption:** Longer reasoning chains correlate with higher accuracy in this specific domain, and the "Wait" trigger effectively resets or deepens the model's attention.
- **Evidence anchors:** [section IV] "Budget forcing... focuses on regulating the length of the chain of thoughts... introduce a 'Wait' tag."

## Foundational Learning

- **Concept: Document Layout Analysis & Grounding**
  - **Why needed here:** The pipeline depends on extracting figures and text from PDFs. Understanding how VLMs convert pixel space to bounding boxes is essential for debugging data generation.
  - **Quick check question:** Can you explain how a Vision Transformer identifies the coordinates of an image embedded in a text-heavy PDF page?

- **Concept: Chain-of-Thought (CoT) Distillation**
  - **Why needed here:** The core innovation is teaching a 7B model to "think" using data generated by larger models (GPT-4o). You must understand how CoT distills reasoning capability.
  - **Quick check question:** How does training on `<think>` tags differ from standard instruction fine-tuning in terms of the loss function?

- **Concept: Supervised Fine-Tuning (SFT) Dynamics**
  - **Why needed here:** The paper uses a specific learning rate (3e-5) and cosine decay on a single A800. Understanding SFT stability is crucial for reproducing results.
  - **Quick check question:** Why might a low learning rate (e.g., 3e-5) be preferred over a higher rate when fine-tuning a pre-trained VLM on a specific domain?

## Architecture Onboarding

- **Component map:** Python scripts (Qwen2.5-VL for OCR/Bounding Box) -> GPT-4o/Gemini API (for Synthesis) -> ReMUD Dataset (JSON/ShareGPT format, Image Assets) -> LLaMA-Factory (Training Stack) -> Qwen2.5-VL-7B backbone -> Custom inference controller (Budget Forcing)

- **Critical path:** The Data Generation Pipeline is the bottleneck. Accuracy relies entirely on the quality of the bounding boxes and the subsequent VQA generation. If the bounding box is misaligned, the image crop is wrong, and the reasoning data is invalid.

- **Design tradeoffs:**
  - **Automation vs. Quality:** The pipeline is fully automated (cheap/fast) but risks hallucination. The paper employs a "Data Clean" step using a binary classifier and re-scoring to mitigate this.
  - **Generality vs. Specificity:** Training heavily on ultrasound data likely degrades performance on general vision tasks (catastrophic forgetting), a common tradeoff in domain adaptation.

- **Failure signatures:**
  - **Hallucinated Artifacts:** Model describes anatomical features not present in the image (often traced back to noisy synthetic training data).
  - **Repetition Loops:** During inference with budget forcing, the model might repeat "Wait, let's check..." without converging on an answer.
  - **Context Misalignment:** The model answers ultrasound questions correctly but fails to associate the correct image if the bounding box grounding failed during data creation.

- **First 3 experiments:**
  1. **Pipeline Validation:** Run the data generation script on a 10-page sample of a medical PDF. Manually verify if the bounding boxes tightly crop the figures and if the generated questions align with the image content.
  2. **Overfitting Stress Test:** Train the model for 1 epoch vs. 3 epochs. Monitor the training loss on the `<think>` tags specifically to see if the model learns the reasoning structure or just memorizes answers.
  3. **Budget Forcing Ablation:** Run inference on the UVQA-Diagnosis test set with budget forcing disabled vs. enabled. Plot accuracy vs. reasoning token length to find the optimal "budget" before performance degrades.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the automated data generation pipeline be extended to effectively process and utilize ultrasound video data?
- **Basis in paper:** [explicit] The authors state in the Discussion that "data such as videos are also difficult to process, and we are still stuck in the generation of text-image data."
- **Why unresolved:** The current pipeline relies on static PDF pages and bounding boxes (via Qwen2.5-VL) to extract images, lacking a mechanism for handling the temporal dynamics of video.
- **What evidence would resolve it:** An extension of the ReMUD codebase capable of extracting video clips and generating corresponding temporal reasoning traces, followed by benchmarks on video-based diagnostic tasks.

### Open Question 2
- **Question:** How does the reduced image quality inherent in PDF extraction impact the diagnostic precision of the fine-tuned model?
- **Basis in paper:** [explicit] The Discussion notes the limitation that "we cannot obtain particularly high-quality image information, and the clarity of the images may not be sufficient."
- **Why unresolved:** While the model outperformed general MLLMs, the paper does not isolate "image clarity" as a variable to determine if low-resolution training data creates a performance ceiling.
- **What evidence would resolve it:** A comparative ablation study where models are trained on data sourced from high-resolution native images versus PDF-extracted images, evaluated on the same UVQA-Diagnosis benchmarks.

### Open Question 3
- **Question:** Can more efficient test-time scaling methods be developed to optimize inference performance without the potential accuracy degradation seen with extended chain-of-thought?
- **Basis in paper:** [explicit] The Conclusion suggests that "more efficient test-time scaling methods can be investigated to optimize the performance of models during inference."
- **Why unresolved:** The experiments showed that while extending the thinking trace improved accuracy, extending it further caused accuracy to drop, indicating current scaling methods (budget forcing) are suboptimal.
- **What evidence would resolve it:** A novel scheduling or forcing strategy that monotonically improves or stabilizes accuracy as test-time compute increases, surpassing the performance of the current "Wait" token method.

## Limitations

- **Data quality dependency:** The pipeline's effectiveness is limited by the quality and diversity of source PDF materials, with poor-quality images or hallucinated content potentially degrading model performance.
- **Domain specificity:** The budget forcing mechanism's effectiveness is domain-specific and may not generalize to other reasoning tasks where longer traces do not correlate with accuracy.
- **Evaluation benchmark novelty:** The newly constructed USTQ-Knowledge and UVQA-Diagnosis test sets lack validation by the broader research community, raising questions about their representativeness.

## Confidence

- **High Confidence:** The overall framework of using synthetic data generation for domain adaptation is sound and well-supported by existing literature. The reported performance gains over general MLLMs on ultrasound-specific tasks are consistent with expected benefits of domain specialization.
- **Medium Confidence:** The specific implementation details of the data generation pipeline (prompt templates, cleaning thresholds) and the exact impact of budget forcing on accuracy are not fully disclosed, making independent validation challenging.
- **Low Confidence:** The generalizability of the ReMUD-7B model to other medical domains or the long-term stability of the fine-tuned model (catastrophic forgetting of general knowledge) are not addressed.

## Next Checks

1. **Data Quality Audit:** Manually inspect a random sample of 100 generated quadruplets from the ReMUD dataset to assess the accuracy of bounding box grounding and the relevance of the generated questions and reasoning traces to the source images.

2. **Cross-Domain Transfer Test:** Fine-tune the ReMUD-7B model on a small dataset from a different medical imaging domain (e.g., X-ray) and evaluate its performance to assess catastrophic forgetting and domain generalization.

3. **Budget Forcing Ablation:** Conduct a systematic ablation study on the UVQA-Diagnosis test set, varying the reasoning token length limit and the frequency of the "Wait" trigger to identify the optimal configuration and test the robustness of the test-time scaling mechanism.