---
ver: rpa2
title: 'Bonsai: Interpretable Tree-Adaptive Grounded Reasoning'
arxiv_id: '2504.03640'
source_url: https://arxiv.org/abs/2504.03640
tags:
- arxiv
- evidence
- reasoning
- preprint
- bonsai
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Bonsai is a probabilistic reasoning system that generates interpretable
  inference trees over multimodal data by retrieving grounding evidence and computing
  sub-claim likelihoods through iterative anchoring-and-adjustment. It outperforms
  black-box models on question-answering tasks while providing transparent, uncertainty-aware
  reasoning traces.
---

# Bonsai: Interpretable Tree-Adaptive Grounded Reasoning

## Quick Facts
- arXiv ID: 2504.03640
- Source URL: https://arxiv.org/abs/2504.03640
- Authors: Kate Sanders; Benjamin Van Durme
- Reference count: 40
- Primary result: Outperforms black-box models on question-answering tasks while providing transparent, uncertainty-aware reasoning traces

## Executive Summary
Bonsai is a probabilistic reasoning system that generates interpretable inference trees over multimodal data by retrieving grounding evidence and computing sub-claim likelihoods through iterative anchoring-and-adjustment. The system converts raw multimodal inputs into natural language evidence banks, enabling modality-agnostic reasoning with a single probabilistic scorer. Bonsai demonstrates strong performance on EntailmentBank (87.7%), TVQA (65.5%), and MultiVENT (76% accuracy), matching state-of-the-art models while offering grounded explanations and human-interpretable probability scores.

## Method Summary
Bonsai processes multimodal inputs through offline evidence extraction, creating a natural language evidence bank from video frames, audio, and images using specialized models. A root claim is recursively decomposed into a tree of sub-claims using GPT-4o, with leaf-node likelihoods computed via anchoring-and-adjustment scoring. Evidence retrieval uses cross-encoders to select relevant observations, which are presented sequentially to the LLM for probability adjustment. Final answers are derived through probability propagation up the tree, optionally using an LLM judge for aggregation.

## Key Results
- EntailmentBank accuracy: 87.7%
- TVQA accuracy: 65.5%
- MultiVENT accuracy: 76%
- Matches or exceeds state-of-the-art black-box models while providing interpretable reasoning traces
- Demonstrates reliable handling of varied domains including transcripts, photographs, videos, audio, and databases

## Why This Works (Mechanism)

### Mechanism 1: Multimodal to Text Evidence Banks
Converting multimodal data to natural language evidence banks enables modality-agnostic reasoning. Raw inputs are processed offline by specialized models into textual observations anchored to specific data spans, allowing a single probabilistic scorer to operate uniformly across modalities.

### Mechanism 2: Iterative Anchoring-and-Adjustment
The system elicits a "prior" probability score from an LLM conditioned on a generic scenario summary, then presents retrieved evidence pieces sequentially for step-by-step adjustment. This mimics human judgment heuristics from Tversky & Kahneman (1974) to yield human-aligned probability scores.

### Mechanism 3: Tree-Based Compositional Inference
Recursive decomposition into sub-claims enables interpretable, compositional inference and human-in-the-loop correction. Leaf-node scores are propagated upward to derive final likelihoods, allowing users to inspect or correct intermediate steps.

## Foundational Learning

- **Concept: Probabilistic Calibration**
  - Why needed here: The system outputs likelihood scores (0-100) that must align with human judgments of probability for interpretability.
  - Quick check question: If a system assigns an "80% likely" score to 100 different claims, how many of those claims should turn out to be true for the system to be well-calibrated?

- **Concept: Anchoring and Adjustment Heuristic**
  - Why needed here: This is the core algorithmic pattern for scoring, where the initial anchor biases the final result and adjustments are often insufficient.
  - Quick check question: Why might starting with an initial probability score of 50% vs. 10% for the same evidence lead to different final scores?

- **Concept: Inference Tree / Entailment Graph**
  - Why needed here: The output is a tree where leaf probabilities must be combined to compute the root claim's probability.
  - Quick check question: In a binary tree, if a sub-claim decomposes into two children A and B, and the evidence strongly supports A but contradicts B, what should happen to the parent claim's score?

## Architecture Onboarding

- **Component Map:** Evidence Extractor (Offline) -> Decomposer (GPT-4o) -> Retriever -> Probabilistic Scorer (GPT-4o) -> Inference Engine
- **Critical Path:** The chain from Claim Decomposition -> Evidence Retrieval -> Anchoring-and-Adjustment Scoring is the performance-defining loop.
- **Design Tradeoffs:** Trades guaranteed logical entailment for calibrated probabilities that handle ambiguity better; uses opaque LLM judge for aggregation accuracy at the cost of transparency.
- **Failure Signatures:** Consistent low-confidence scores suggest missing key information; high-confidence wrong answers may indicate decomposition failures; contradictory evidence may confuse sequential adjustment.
- **First 3 Experiments:**
  1. Unit test the scorer with definitive evidence to validate anchoring-and-adjustment logic
  2. Ablate the anchor step to measure its effect on calibration and accuracy
  3. Measure scaling efficiency by varying retrieved evidence count and enabling/disabling test-time evidence re-sampling

## Open Questions the Paper Calls Out

- Can expert aggregation methods improve probabilistic inference while remaining computationally tractable for deeper reasoning trees?
- What domain-specific modifications to Bonsai's modules would yield the greatest performance improvements?
- How does Bonsai's probability calibration compare to human judgment in scenarios requiring multi-hop reasoning over subjective or ambiguous claims?

## Limitations

- The anchoring-and-adjustment mechanism relies on LLM calibration, which may introduce systematic biases through the reasoning chain
- Information loss during multimodal-to-text conversion may affect tasks requiring precise spatial-temporal reasoning or subtle audio cues
- The sequential evidence presentation may struggle with highly interdependent or contradictory evidence pieces

## Confidence

- **High Confidence:** Tree-based decomposition approach for interpretable reasoning is well-established and empirically validated
- **Medium Confidence:** Anchoring-and-adjustment scoring mechanism shows promise but lacks direct empirical validation against alternatives
- **Low Confidence:** Performance on fine-grained multimodal reasoning tasks cannot be reliably predicted without targeted experiments

## Next Checks

1. Test the system's probability scores against human judgments on a held-out subset to measure calibration error
2. Design targeted tests where critical information exists only in non-textual modalities to quantify evidence extraction pipeline information loss
3. Systematically vary decomposition depth and structure to measure sensitivity to tree quality and completeness