---
ver: rpa2
title: 'LegalRAG: A Hybrid RAG System for Multilingual Legal Information Retrieval'
arxiv_id: '2504.16121'
source_url: https://arxiv.org/abs/2504.16121
tags:
- legal
- pipeline
- documents
- retrieval
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces LegalRAG, a hybrid multilingual retrieval-augmented
  generation (RAG) system designed to improve legal information retrieval in low-resource
  languages, specifically Bangla. The system processes Bangladesh Police Gazettes
  containing both English and Bangla text by employing a conventional RAG pipeline
  enhanced with an additional LLM for relevance checking and query refinement.
---

# LegalRAG: A Hybrid RAG System for Multilingual Legal Information Retrieval

## Quick Facts
- arXiv ID: 2504.16121
- Source URL: https://arxiv.org/abs/2504.16121
- Reference count: 40
- LegalRAG is a hybrid multilingual retrieval-augmented generation (RAG) system designed to improve legal information retrieval in low-resource languages, specifically Bangla.

## Executive Summary
LegalRAG is a hybrid multilingual retrieval-augmented generation (RAG) system designed to improve legal information retrieval in low-resource languages, specifically Bangla. The system processes Bangladesh Police Gazettes containing both English and Bangla text by employing a conventional RAG pipeline enhanced with an additional LLM for relevance checking and query refinement. This ensures only the most pertinent text chunks are passed to the generative model, significantly improving retrieval accuracy and response quality. The framework was evaluated on a curated test set of 168 question-answer pairs, achieving superior performance over vanilla RAG across multiple evaluation metrics. Human evaluation scores increased from 2.77 to 3.70 (out of 5), and semantic similarity improved from 0.70 to 0.82, demonstrating the system’s effectiveness in handling complex bilingual legal documents.

## Method Summary
LegalRAG combines a traditional RAG pipeline with an LLM-based relevance checker and query refiner to process bilingual legal documents. The system first chunks and indexes Bangladesh Police Gazettes in both English and Bangla. When a query is received, it is processed through the RAG pipeline to retrieve relevant passages. An LLM then assesses the relevance of retrieved chunks and refines the query if needed, ensuring only the most pertinent text is passed to the generative model for final answer synthesis. This hybrid approach addresses the challenges of low-resource language retrieval and complex legal terminology.

## Key Results
- Human evaluation scores increased from 2.77 to 3.70 (out of 5) compared to vanilla RAG.
- Semantic similarity improved from 0.70 to 0.82.
- Superior performance across multiple evaluation metrics on a curated test set of 168 question-answer pairs.

## Why This Works (Mechanism)
LegalRAG’s hybrid approach leverages the strengths of both conventional RAG and LLM-based refinement. By filtering and refining retrieved passages before generation, the system minimizes noise and focuses on the most relevant legal information. The LLM’s ability to assess context and rephrase queries in multilingual settings is especially valuable for low-resource languages like Bangla, where standard retrieval may struggle with nuance and terminology.

## Foundational Learning
- **RAG (Retrieval-Augmented Generation)**: Combines retrieval and generation for accurate, context-aware responses; needed to fetch relevant legal passages; quick check: retrieve passages for a sample legal query.
- **Multilingual Processing**: Handling documents in multiple languages; needed for bilingual legal texts; quick check: process a mixed English-Bangla sentence.
- **LLM-based Relevance Checking**: Uses LLMs to filter and rank retrieved passages; needed to reduce noise in legal retrieval; quick check: rank passages by relevance for a legal question.
- **Query Refinement**: Adjusting queries based on retrieved context; needed to improve retrieval accuracy in low-resource settings; quick check: refine a query using LLM suggestions.
- **Chunking and Indexing**: Breaking documents into manageable pieces and indexing for fast retrieval; needed for efficient RAG; quick check: chunk and index a sample gazette page.

## Architecture Onboarding
- **Component Map**: User Query -> RAG Pipeline -> LLM Relevance Checker & Query Refiner -> Generative Model -> Final Answer
- **Critical Path**: Query → Retrieval → Relevance Check → Refinement → Generation → Output
- **Design Tradeoffs**: LLM-based relevance checking adds accuracy but increases latency and computational cost; hybrid approach balances retrieval and generation for complex legal queries.
- **Failure Signatures**: Poor retrieval leads to irrelevant or incomplete answers; LLM relevance checker may miss subtle legal nuances; query refinement may overfit to training data.
- **First Experiments**:
  1. Run a sample query through the full pipeline and inspect intermediate outputs (retrieved passages, LLM relevance scores).
  2. Compare retrieval results with and without LLM relevance checking for a set of legal questions.
  3. Measure response quality and latency on a small set of mixed-language queries.

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation limited to Bangladeshi police gazettes and two languages (English, Bangla), constraining generalizability.
- Test set is relatively small (168 QA pairs), possibly not capturing full query diversity.
- Reliance on LLM-based modules may introduce brittleness and computational overhead.
- Human evaluation methodology lacks full detail, making robustness difficult to assess.

## Confidence
- High: LegalRAG outperforms vanilla RAG on the evaluated dataset, as evidenced by automated and human evaluations.
- Medium: Improvements in semantic similarity and human scores likely translate to real-world legal retrieval, but scope is narrow.
- Low: System robustness and performance in other low-resource languages or legal domains is uncertain due to lack of cross-domain or cross-language validation.

## Next Checks
1. Evaluate LegalRAG on legal documents from other jurisdictions and languages to test generalizability.
2. Conduct a larger-scale human evaluation with diverse legal experts to validate response quality across more query types.
3. Benchmark the computational and cost overhead of the hybrid relevance checking and query refinement modules against simpler RAG baselines.