---
ver: rpa2
title: 'Collaborative Storytelling and LLM: A Linguistic Analysis of Automatically-Generated
  Role-Playing Game Sessions'
arxiv_id: '2503.20623'
source_url: https://arxiv.org/abs/2503.20623
tags:
- features
- language
- written
- sessions
- spoken
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates whether LLM-generated text exhibits features
  of spoken or written language by analyzing automatically generated RPG sessions.
  The research compares LLM outputs with transcribed human RPG sessions, conversations,
  academic speeches, and books using lexical and syntactic metrics including D-value,
  concreteness, sentence length, subordinate clauses, and cohesion.
---

# Collaborative Storytelling and LLM: A Linguistic Analysis of Automatically-Generated Role-Playing Game Sessions

## Quick Facts
- **arXiv ID**: 2503.20623
- **Source URL**: https://arxiv.org/abs/2503.20623
- **Authors**: Alessandro Maisto
- **Reference count**: 37
- **Primary result**: LLM-generated RPG text shows higher lexical diversity and complexity than human sessions, longer sentences with lower cohesion, and present-tense dominance, suggesting written-language bias despite diverse training data.

## Executive Summary
This study investigates whether LLM-generated text exhibits features of spoken or written language by analyzing automatically generated RPG sessions. The research compares LLM outputs with transcribed human RPG sessions, conversations, academic speeches, and books using lexical and syntactic metrics including D-value, concreteness, sentence length, subordinate clauses, and cohesion. Results show that LLM-generated text exhibits a unique linguistic pattern: higher lexical diversity and complexity than human RPG sessions, longer sentences with lower root distance, extensive use of present tense, and lower textual cohesion. While LLMs produce more descriptive and syntactically complex language than human players, they lack the cohesive features typical of natural spoken discourse. The findings suggest LLMs generate language more characteristic of written text despite being trained on diverse data sources.

## Method Summary
The study uses LLaMa3-8b-8192 via Groq API to generate 8 complete RPG sessions through LLM-to-LLM interaction, with 1 GM and 3 PC characters (Grog, Pike, Vax) using sequential turn-taking. Sessions are generated using character descriptions from Critical Role wiki and adventure prompts from D&D modules, with temperature settings varying from 0.3-1.0 for GM and 0.3-0.7 for players. Linguistic features are automatically extracted using Stanford CoreNLP and custom Java modules, then compared against reference corpora including BNC conversations, ELFA academic speech, Critical Role transcripts, and fantasy novels using metrics like D-value, lexical range, concreteness, syntactic complexity, verb tense distributions, and cohesion scores.

## Key Results
- LLM sessions show significantly higher lexical diversity (D-value 19.8) than human RPG sessions (8.2), approaching book-level complexity
- LLM text exhibits lower cohesion (0.007) than human RPG sessions (0.105), primarily due to lack of logical and causal connectives
- LLM-generated text uses present tense almost exclusively (0.72 ratio) compared to human narratives, creating temporally flat narratives

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs produce language with higher lexical diversity and academic vocabulary than human RPG sessions, more closely resembling written texts.
- Mechanism: Training data composition—predominantly written text—biases lexical production toward less frequent, more academic vocabulary (LR2, LR3), yielding D-values (19.8) comparable to books rather than human speech (8.2).
- Core assumption: Assumption: Training corpus composition directly influences generated register; no explicit oral-style fine-tuning was applied.
- Evidence anchors:
  - [abstract] "LLMs exhibit a pattern that is distinct from all other text categories... Our analysis has shown how training influences the way LLMs express themselves"
  - [section 3.3] "The D-values of the LLM sessions are much closer to those of books, likely reflecting the nature of the LLMs' training data, which primarily consists of written text"
  - [corpus] Neighbor paper "RPGBENCH" evaluates LLMs as RPG engines but does not directly address lexical diversity mechanisms; corpus evidence is weak for causal claims about training data effects.
- Break condition: If LLMs were fine-tuned on oral RPG transcripts or conversation corpora, lexical diversity may shift toward spoken patterns.

### Mechanism 2
- Claim: LLMs over-rely on present tense (0.72 ratio) compared to human narratives, producing temporally flat narratives.
- Mechanism: Without explicit temporal context or character backstory integration, LLMs default to immediate action narration in present tense; past tense for background events is rarely triggered (0.01 ratio).
- Core assumption: Assumption: The absence of explicit temporal scaffolding in prompts leads to present-tense dominance; multi-turn context does not automatically trigger backstory references.
- Evidence anchors:
  - [abstract] "frequent use of the present tense"
  - [section 3.3] "The LLMs almost exclusively used the present tense... rarely used the past tense. This difference may stem from the LLMs' focus on the immediate actions and events of the narrative"
  - [corpus] No direct corpus evidence on tense mechanisms in RPG generation; this is a gap.
- Break condition: If prompts explicitly request backstory integration or alternate tense usage, present-tense dominance may decrease.

### Mechanism 3
- Claim: LLM-generated RPG text exhibits significantly lower cohesion (0.007) than human RPG sessions (0.105) and academic speech (0.212).
- Mechanism: LLMs under-generate logical and causal connectives despite producing syntactically complex sentences; cohesion is not enforced by next-token prediction alone.
- Core assumption: Assumption: Cohesive devices are not strongly weighted in generation objectives; local coherence does not guarantee global cohesion.
- Evidence anchors:
  - [abstract] "lower cohesion"
  - [section 3.3] "The LLMs obtained a very low cohesion value, primarily due to the lack of logical and causal connectives in their generated texts"
  - [corpus] Weak corpus evidence; neighbor papers do not analyze cohesion mechanisms specifically.
- Break condition: If explicit cohesion rewards are added to generation objectives (e.g., connective frequency constraints), cohesion scores may improve.

## Foundational Learning

- Concept: Lexical Diversity (D-value)
  - Why needed here: Measures vocabulary richness independent of text length; critical for comparing LLM output to spoken vs. written registers.
  - Quick check question: Can you explain why D-value is preferred over Type-Token Ratio for comparing texts of different lengths?

- Concept: Syntactic Complexity Metrics (subordinate clauses, dependency depth, noun modifiers)
  - Why needed here: Quantifies structural differences between LLM and human-generated language; reveals unique LLM syntactic signatures.
  - Quick check question: What does a higher ratio of subordinate clauses to relative clauses suggest about an LLM's narrative style?

- Concept: Textual Cohesion (connective frequency and weighting)
  - Why needed here: Diagnoses narrative coherence issues in LLM output; identifies deficits in logical/causal connective generation.
  - Quick check question: How would you design a weighted cohesion score that prioritizes causal connectives over temporal ones?

## Architecture Onboarding

- Component map:
  - LLM instances (LLaMa3-8b-8192 via Groq API): 1 GM, 3 PCs with distinct system prompts
  - Session loop: GM → Grog → Pike → Vax → GM (circular turn-taking, 200 interaction limit)
  - Feature extraction module (Java): Computes D-value, lexical range, syntactic complexity, verb analysis, cohesion
  - Comparison corpora: BNC (conversation), ELFA (academic speech), Critical Role (human RPG), fantasy novels (books)

- Critical path:
  1. Define character descriptions and adventure prompts
  2. Configure temperature and token limits per instance
  3. Run session loop until 200 interactions or token limit
  4. Extract linguistic features automatically
  5. Compare to reference corpora using D-value, LR, cohesion, verb tense, syntactic complexity

- Design tradeoffs:
  - Temperature settings: Lower (0.3) yields more repetitive language; higher (1.0) increases variability but may reduce coherence
  - Token limits: Constrain response length; shorter limits (50 for Vax) may truncate complex utterances
  - Single-model architecture (LLaMa3): Results may not generalize to other LLMs with different training data or architectures

- Failure signatures:
  - NPC name repetition (e.g., "Grimbold" across sessions) suggests limited creative diversity
  - Low cohesion score (<0.01) indicates insufficient global narrative planning
  - Present tense ratio >0.7 suggests temporal flatness without backstory integration

- First 3 experiments:
  1. Vary temperature systematically (0.3, 0.5, 0.7, 1.0) and measure impact on cohesion and lexical diversity; hypothesis: higher temperature increases variability but reduces cohesion
  2. Add explicit backstory prompts requiring past-tense narration; measure change in past-tense ratio and cohesion
  3. Compare LLaMa3 to a conversation-fine-tuned model (if available) on the same RPG task; hypothesis: fine-tuned model shows higher emphatic particle frequency and first-person pronoun ratio

## Open Questions the Paper Calls Out

### Open Question 1
- Question: To what extent does the specific composition of training data influence the distinct narrative strategies and linguistic signatures observed in LLMs?
- Basis in paper: [Explicit] The conclusion states that "Further research is needed to understand the impact of training data on LLM narrative strategies."
- Why unresolved: The current study analyzes the output (the generated text) but does not perform an ablation study or analysis of the training corpus itself to causally link specific data features to the observed high lexical diversity or low cohesion.
- What evidence would resolve it: Experiments using LLMs trained on controlled datasets with varying ratios of oral versus written transcripts, comparing the resulting linguistic features against the baseline established in this paper.

### Open Question 2
- Question: What specific fine-tuning or prompting methods can effectively improve textual cohesion and correct the over-reliance on the present tense in LLM-generated narratives?
- Basis in paper: [Explicit] The authors explicitly call for the development of "methods for improving textual cohesion and verb tense usage" to address the identified deficits in LLM outputs.
- Why unresolved: The analysis identifies *that* LLMs lack logical/causal connectives and rarely use the past tense, but the paper does not test or propose technical interventions to fix these specific syntactic and semantic issues.
- What evidence would resolve it: A follow-up study implementing targeted reinforcement learning or loss functions that penalize low cohesion and lack of tense variation, demonstrating metrics closer to human RPG sessions.

### Open Question 3
- Question: Can reinforcement learning techniques combined with affective analysis enable LLMs to generate narratives that capture the emotional nuances of human collaborative storytelling?
- Basis in paper: [Explicit] The conclusion suggests "investigate reinforcement learning techniques for training LLMs to generate more human-like narratives, including affective analysis."
- Why unresolved: The current research focuses on structural linguistic features (syntax, lexicon, cohesion) but does not deeply evaluate the emotional resonance or "affect" of the generated stories, which is a key component of human RPG sessions.
- What evidence would resolve it: A comparative study using human evaluators to rate the emotional depth and affective appropriateness of stories generated by standard LLMs versus those trained with specific affective feedback loops.

### Open Question 4
- Question: Are the identified linguistic signatures (long sentences, low cohesion, unique syntax) inherent to all LLM-generated speech or specific to the LLaMa3-8b architecture used?
- Basis in paper: [Inferred] The study utilizes a specific model (LLaMa3-8b-8192) and concludes that these models possess a "distinct syntactic signature," leaving open the question of whether these traits generalize to larger models or different architectures.
- Why unresolved: The limitations of the experiment design restrict the findings to a single model class; it is undetermined if larger parameter counts or different training objectives (e.g., MoE models) produce the same "written-yet-oral" hybrid features.
- What evidence would resolve it: Replicating the exact linguistic analysis pipeline (D-value, cohesion scores, syntactic complexity) on sessions generated by other state-of-the-art models (e.g., GPT-4, Claude) and comparing the results to the LLaMa3 baseline.

## Limitations
- The study relies on a single LLM architecture (LLaMa3-8b-8192) with fixed prompts and temperature settings, limiting generalizability to other models or prompt engineering approaches
- The cohesion metric uses manually compiled word lists for connectives that may not capture all relevant linguistic phenomena
- The comparison corpora, while diverse, may not perfectly represent the full spectrum of spoken versus written language registers

## Confidence
- **High confidence**: LLM text shows higher lexical diversity (D-value 19.8) than human RPG sessions (8.2), more closely resembling written texts than spoken discourse
- **Medium confidence**: LLM text exhibits lower cohesion (0.007) than human RPG sessions (0.105), primarily due to lack of logical and causal connectives
- **Medium confidence**: LLM text uses present tense almost exclusively (0.72 ratio) compared to human narratives, creating temporally flat narratives

## Next Checks
1. **Cross-model validation**: Replicate the analysis using different LLM architectures (e.g., GPT-4, Claude) with identical prompts and temperature settings to determine if observed linguistic patterns are model-specific or universal across LLM-generated RPG text
2. **Prompt engineering experiment**: Systematically vary temporal context in prompts (e.g., explicit backstory requirements, past-tense framing) and measure changes in tense distribution, cohesion scores, and narrative coherence to test whether observed limitations are prompt-dependent
3. **Fine-tuning intervention**: Compare a standard LLM against the same base model fine-tuned on conversational RPG transcripts or human-human dialogue data using identical generation tasks to quantify the impact of oral-style training on lexical diversity, cohesion, and register-appropriate language production