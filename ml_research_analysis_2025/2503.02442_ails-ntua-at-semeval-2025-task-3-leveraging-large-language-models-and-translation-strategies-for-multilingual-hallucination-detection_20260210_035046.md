---
ver: rpa2
title: 'AILS-NTUA at SemEval-2025 Task 3: Leveraging Large Language Models and Translation
  Strategies for Multilingual Hallucination Detection'
arxiv_id: '2503.02442'
source_url: https://arxiv.org/abs/2503.02442
tags:
- output
- hallucination
- hypothesis
- input
- llama
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: AILS-NTUA addresses multilingual hallucination detection by leveraging
  LLM prompting and translation strategies without requiring model training. The method
  combines Llama 3.1 405B and Claude 3.5 Sonnet to detect hallucinated spans in text
  outputs across 14 languages.
---

# AILS-NTUA at SemEval-2025 Task 3: Leveraging Large Language Models and Translation Strategies for Multilingual Hallucination Detection

## Quick Facts
- **arXiv ID:** 2503.02442
- **Source URL:** https://arxiv.org/abs/2503.02442
- **Reference count:** 38
- **Primary result:** First place in low-resource Farsi and Czech, second in Italian, top-15% across most languages

## Executive Summary
AILS-NTUA addresses multilingual hallucination detection by leveraging LLM prompting and translation strategies without requiring model training. The method combines Llama 3.1 405B and Claude 3.5 Sonnet to detect hallucinated spans in text outputs across 14 languages. A key innovation is translating non-English inputs and outputs into English before detection, combined with few-shot prompting and hypothesis integration from both models. The approach achieves first place in low-resource Farsi and Czech, second in Italian, and top-15% across most other languages. Translation into English proves critical for low-resource languages, while few-shot prompting and hypothesis comparison drive gains in high-resource languages. The system consistently ranks among the best, demonstrating effectiveness across both high- and low-resource multilingual contexts.

## Method Summary
The method employs a three-component ensemble combining Llama 3.1 405B and Claude 3.5 Sonnet via Amazon Bedrock. It uses few-shot prompting with Chain-of-Thought reasoning to detect hallucinated spans across 14 languages. Non-English input-output pairs are translated to English using Google Translate API before detection. The system generates hypotheses from both LLMs and uses cross-model comparison to identify discrepancies. Three detection components run in parallel: two using hypothesis comparison (Claude with Llama hypothesis, Llama with Claude hypothesis) and one using internal knowledge only (Llama without hypothesis). Final probabilities are calculated by averaging component votes across detected spans.

## Key Results
- First place in low-resource Farsi and Czech languages
- Second place in Italian language
- Top-15% ranking across most other languages, demonstrating consistent top-tier performance

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Translating non-English input-output pairs to English before hallucination detection improves detection quality, with the strongest effect observed for low-resource languages.
- **Mechanism:** English is the highest-resource language for most LLMs; by providing English translations alongside original text, the model leverages its stronger factual knowledge and reasoning capabilities in English while still grounding span detection in the original language positions.
- **Core assumption:** The LLM possesses more reliable factual knowledge in English than in low-resource languages, and translation errors introduce less noise than direct low-resource reasoning.
- **Evidence anchors:** [abstract] "we propose an efficient, training-free LLM prompting strategy that enhances detection by translating multilingual text spans into English"; [Section 6, Results] "incorporating the English translation of the texts appears to enhance the LLM's performance... the addition of translations and hypotheses has a more pronounced impact on low-resource languages compared to high-resource ones"; [Table 3] Farsi IoU improves from 0.50 (no translation) to 0.75 (external translation to English); Czech from 0.39 to 0.59; Finnish from 0.54 to 0.68
- **Break condition:** Translation quality degrades severely (e.g., for very low-resource languages with poor machine translation support), introducing artifacts that the LLM misidentifies as hallucinations.

### Mechanism 2
- **Claim:** Providing a generated hypothesis from one LLM to another LLM during hallucination detection improves span identification through cross-model comparison.
- **Mechanism:** When Llama generates a hypothesis and Claude uses it as a reference (or vice versa), the detecting model can compare the hypothesis against the potentially-hallucinated output. Discrepancies between hypothesis and output signal likely hallucinations.
- **Core assumption:** Different LLMs do not hallucinate identically on the same input; divergence between model outputs indicates uncertainty or factual weakness.
- **Evidence anchors:** [Section 3, Component 1 & 2] "we instruct Llama and Claude to generate the corresponding output and incorporate it as a hypothesis to facilitate hallucination detection"; [Section 6] "incorporating the English translation of the texts appears to enhance the LLM's performance. A similar effect is observed when integrating the hypothesis from the other LLM"; [Tables 1-2] "FS + Translation + Hypothesis" outperforms "FS + Translation" across nearly all 14 languages
- **Break condition:** The hypothesis-generating model itself produces a hallucinated answer, and the detecting model over-relies on this hypothesis rather than its own knowledge (acknowledged in Section 3, Component 3).

### Mechanism 3
- **Claim:** Combining multiple detection components—with and without hypotheses—through probability aggregation captures complementary hallucination types.
- **Mechanism:** Component 3 (no hypothesis) prompts the model to flag more potential hallucination spans based on internal knowledge; Components 1-2 (with hypothesis) focus on factual consistency via comparison. Averaging across components balances coverage.
- **Core assumption:** Hallucinations manifest in different ways (factual errors, input-output inconsistency, internal contradictions, misspellings), and no single prompting configuration optimally detects all types.
- **Evidence anchors:** [Section 3] "for each produced span, the assigned probability is calculated as the ratio of the experiments that characterize it as hallucination over the total number of experiments (three)"; [Appendix C, Table 11] Shows IoU between different component predictions (0.47-0.87) and that combining components with lower IoU yields higher combined IoU (0.53 vs. 0.50); [Section 6] "for high-resource languages such as Spanish, Chinese, and German, the FS scenario and the incorporation of the generated hypothesis contribute the most... for low-resource languages, translation is a crucial component"
- **Break condition:** Components systematically disagree on the same spans due to systematic biases in both models, causing probability dilution for true hallucinations.

## Foundational Learning

- **Concept: Hallucination typology (Input-Output inconsistency, Factual inconsistency, Internal inconsistency, Misspellings)**
  - Why needed here: The system's few-shot prompts provide examples for each category; understanding these types is required to design effective prompts and interpret model outputs.
  - Quick check question: Given output "The Olympic Games of 2004 took place in Florida," which hallucination type applies?

- **Concept: Few-shot prompting with Chain-of-Thought (CoT)**
  - Why needed here: The paper adopts few-shot prompting with CoT reasoning as its core detection strategy (Section 4). Zero-shot underperformed significantly.
  - Quick check question: Why might CoT help an LLM identify a factual inconsistency more reliably than a binary Yes/No prompt?

- **Concept: Intersection-over-Union (IoU) and correlation metrics for span detection**
  - Why needed here: The task evaluates character-level span overlap (IoU) and probability correlation. Understanding these metrics is essential for debugging system performance.
  - Quick check question: If a system marks characters 10-20 as hallucinated but gold marks 12-22, what is the IoU?

## Architecture Onboarding

- **Component map:**
  Input (model_input, model_output_text, lang) → [Translation Layer] → Google Translate API → English versions → [Hypothesis Generation] → Llama generates answer; Claude generates answer → [Detection Components (parallel)] → Component 1: Claude + Llama hypothesis + translations; Component 2: Llama + Claude hypothesis + translations; Component 3: Llama + translations (no hypothesis) → [Span Aggregation] → Probability = (count of components flagging span) / 3 → Output (hallucination spans with probabilities)

- **Critical path:** Translation quality → Hypothesis quality → Few-shot prompt design → Span extraction parsing. If translation introduces errors, downstream detection inherits them.

- **Design tradeoffs:**
  - External translator (Google Translate) vs. LLM-as-translator: Paper found external translator superior (Table 3). Assumption: Dedicated translation systems outperform LLM translation when LLM must also perform detection.
  - With vs. without hypothesis: Hypothesis improves high-resource languages but can mislead if hypothesis itself is hallucinated. Paper includes both via ensemble.
  - Prompt language: English prompts with original-language pairs underperformed vs. English prompts with translated pairs.

- **Failure signatures:**
  - Model over-relies on hypothesis, missing hallucinations that hypothesis shares (addressed by Component 3).
  - Span extraction parsing fails if LLM output format deviates from expected "So the hallucinations are:..." pattern.
  - Translation artifacts flagged as hallucinations (monitor via manual inspection on sample data).

- **First 3 experiments:**
  1. Reproduce zero-shot vs. few-shot comparison on validation set for a single language (e.g., English) to validate prompt design impact.
  2. Ablate translation strategy: run detection with original-language-only vs. English-translated pairs on a low-resource language (e.g., Farsi or Czech) to quantify translation contribution.
  3. Test single-component vs. three-component aggregation on a held-out subset to measure ensemble benefit and identify cases where components disagree.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** What is the underlying mechanism causing English prompting to outperform native-language prompting for hallucination detection, even when input-output pairs remain in their original language?
- **Basis in paper:** [explicit] The authors explicitly investigate: "If input-output pairs are provided in their original language, should the prompt also be in the same language, or is it preferable to present it in English?" and conclude English is superior.
- **Why unresolved:** While the paper empirically demonstrates that English prompts yield better results, it does not analyze whether this is due to tokenization efficiency, superior instruction-following capabilities in English, or cross-lingual interference in the model's hidden states.
- **What evidence would resolve it:** An ablation study using models with balanced multilingual instruction tuning versus standard English-centric models, combined with an analysis of attention patterns when processing native vs. English instructions.

### Open Question 2
- **Question:** How can the tendency of LLMs to rely on a generated hypothesis rather than their internal knowledge be mitigated in cross-model verification pipelines?
- **Basis in paper:** [explicit] The authors observe in Section 3 that "LLMs sometimes place undue emphasis on the provided generated hypothesis rather than relying on their internal knowledge, causing them to miss hallucinatory spans."
- **Why unresolved:** The paper addresses this by including a component *without* a hypothesis (Component 3) as a heuristic balance, but does not propose a method to dynamically weigh the hypothesis or prevent the model from deferring to it.
- **What evidence would resolve it:** Experiments that quantitatively measure the "conformity bias" of the detector based on the factual accuracy of the provided hypothesis, potentially resolved by developing a confidence-weighting mechanism for the context.

### Open Question 3
- **Question:** To what extent does the reliance on external machine translation (Google Translate) mask hallucinations by "correcting" them during the translation process?
- **Basis in paper:** [inferred] The methodology relies on translating input-output pairs into English for detection; however, translation systems are often trained to produce fluent, sensible output, which might inadvertently "fix" factual inconsistencies or subtle hallucinations before the LLM evaluates them.
- **Why unresolved:** The paper demonstrates that translation improves performance overall but does not analyze false negatives where the translation system normalized a hallucinated span into a factually correct English sentence.
- **What evidence would resolve it:** A manual or automated analysis of the test set identifying instances where the "gold" hallucination span in the original language was translated into a non-hallucinated span in English.

## Limitations
- Translation quality uncertainty: The method relies on Google Translate, but does not analyze how translation errors might introduce false positives or mask true hallucinations.
- Hypothesis contamination risk: The cross-model hypothesis approach can propagate hallucinations if the hypothesis-generating model produces false content.
- Span extraction parsing underspecification: The paper does not fully specify how span extraction handles malformed LLM outputs, creating reproducibility challenges.

## Confidence
- **High confidence:** The few-shot prompting with Chain-of-Thought consistently outperforms zero-shot across all languages; this is directly validated in Table 1 and forms the core detection strategy.
- **Medium confidence:** The translation-to-English mechanism improves low-resource language detection; supported by Table 3 showing substantial IoU gains for Farsi and Czech, but lacks ablation studies isolating translation quality from other confounding factors.
- **Medium confidence:** Hypothesis integration improves high-resource language detection; Table 1 shows consistent gains, but the mechanism could break if both models share systematic hallucinations.
- **Low confidence:** The ensemble of three components optimally balances hallucination types; while Table 11 shows complementary IoU patterns, the paper does not demonstrate that this specific combination outperforms other aggregation strategies.

## Next Checks
1. **Ablation study on translation quality:** Run detection with original-language-only vs. English-translated pairs on Farsi and Czech, then manually inspect 20 translated samples for translation errors that might be misinterpreted as hallucinations.
2. **Hypothesis reliability assessment:** Compare Component 3 (no hypothesis) vs. average of Components 1-2 for each language; languages where Component 3 outperforms indicate hypothesis contamination requiring mitigation.
3. **Span extraction robustness test:** Feed LLMs intentionally malformed responses (missing quotes, extra text) and verify that the parsing logic still correctly extracts hallucination spans without crashing.