---
ver: rpa2
title: Evaluating the Clinical Impact of Generative Inpainting on Bone Age Estimation
arxiv_id: '2511.23066'
source_url: https://arxiv.org/abs/2511.23066
tags:
- inpainting
- bone
- images
- inpainted
- generative
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluated the impact of generative inpainting on downstream
  AI performance for pediatric bone age estimation and gender classification using
  the RSNA Bone Age Challenge dataset. The authors used OpenAI's gpt-image-1 to remove
  non-anatomical artifacts from 200 original radiographs, creating 600 inpainted versions.
---

# Evaluating the Clinical Impact of Generative Inpainting on Bone Age Estimation

## Quick Facts
- **arXiv ID:** 2511.23066
- **Source URL:** https://arxiv.org/abs/2511.23066
- **Reference count:** 19
- **Primary result:** Generative inpainting degrades AI performance on bone age estimation and gender classification tasks

## Executive Summary
This study evaluated how generative inpainting affects downstream AI performance for pediatric bone age estimation and gender classification. Using OpenAI's gpt-image-1 to remove non-anatomical artifacts from 200 radiographs (creating 600 inpainted versions), the researchers found significant degradation in model performance across all metrics. Bone age MAE increased from 6.26 to 30.11 months, gender classification AUC decreased from 0.956 to 0.704, and calibration corrections failed to address these errors. The findings demonstrate that visually realistic inpainting can obscure clinically relevant features and introduce bias, even when edits are confined to non-diagnostic regions.

## Method Summary
The study used the RSNA Bone Age Challenge dataset with 200 original radiographs, generating 600 inpainted versions using OpenAI's gpt-image-1 to remove non-anatomical artifacts from upper left and lower right corners. Deep learning ensembles assessed bone age estimation (MAE, RMSE) and gender classification (AUC) on both original and inpainted images. The authors also evaluated calibration performance and analyzed pixel-intensity shifts in inpainted regions to identify structural alterations.

## Key Results
- Bone age MAE increased from 6.26 to 30.11 months after inpainting
- Gender classification AUC decreased from 0.956 to 0.704
- Inpainted images showed pixel-intensity shifts and structural inconsistencies
- Calibration corrections failed to fully address performance degradation

## Why This Works (Mechanism)
The study demonstrates that generative inpainting, while visually realistic, introduces subtle structural alterations and pixel-intensity shifts that interfere with AI model feature extraction. These changes, even when confined to non-diagnostic regions, propagate through the model's learned representations and degrade performance on clinically relevant tasks.

## Foundational Learning
- **Bone age estimation**: Pediatric skeletal maturity assessment using hand radiographs; critical for endocrine disorders and growth monitoring
  - Why needed: Primary clinical task being evaluated
  - Quick check: Compare AI predictions against pediatric radiologist readings
- **Generative inpainting**: AI-based image completion for removing artifacts while maintaining visual coherence
  - Why needed: The intervention being tested for clinical utility
  - Quick check: Visual inspection for artifact removal quality
- **Deep learning ensembles**: Multiple model aggregation for improved prediction stability
  - Why needed: Standard approach for robust clinical AI performance
  - Quick check: Compare ensemble vs individual model performance
- **AUC (Area Under Curve)**: Metric for binary classification performance
  - Why needed: Primary metric for gender classification task
  - Quick check: ROC curve analysis across thresholds
- **MAE (Mean Absolute Error)**: Metric for regression task accuracy
  - Why needed: Primary metric for bone age estimation
  - Quick check: Compare prediction errors against clinical tolerance thresholds
- **Pixel-intensity analysis**: Quantitative assessment of image modification impact
  - Why needed: Identifies structural changes invisible to human observers
  - Quick check: Statistical comparison of original vs inpainted regions

## Architecture Onboarding

**Component Map:** Original Images -> Inpainting (gpt-image-1) -> Inpainted Images -> Deep Learning Ensembles -> Performance Metrics (MAE, RMSE, AUC)

**Critical Path:** Image acquisition → Artifact removal via inpainting → Feature extraction by DL models → Task-specific predictions → Performance evaluation

**Design Tradeoffs:** Visual realism vs. feature preservation; computational efficiency vs. accuracy; automated editing vs. clinical validation requirements

**Failure Signatures:** Increased prediction errors, calibration failure, reduced AUC scores, pixel-intensity shifts, structural inconsistencies in inpainted regions

**First 3 Experiments:**
1. Replicate across different inpainting models and locations to identify generalizability
2. Compare performance degradation across different pediatric age groups
3. Test alternative inpainting approaches (manual vs. automated) for comparative analysis

## Open Questions the Paper Calls Out
None

## Limitations
- Dataset size (200 original radiographs) may limit generalizability
- Single generative model (gpt-image-1) used, limiting applicability to other inpainting tools
- Focus on specific imaging modality and clinical task limits broader conclusions
- Clinical significance of pixel-intensity shifts remains uncertain

## Confidence
- **High confidence** in core finding that generative inpainting degrades AI performance
- **Medium confidence** in broader applicability claims due to dataset limitations and single-model focus
- **Medium confidence** in clinical significance of specific pixel-intensity changes

## Next Checks
1. Replicate the study across multiple generative inpainting models and diverse imaging modalities to assess generalizability of performance degradation patterns.

2. Conduct blinded radiologist review of inpainted versus original images to determine whether human experts can detect clinically relevant alterations introduced by inpainting.

3. Perform ablation studies varying inpainting location, extent, and artifact types to identify specific factors driving performance degradation in AI models.