---
ver: rpa2
title: Words That Make Language Models Perceive
arxiv_id: '2510.02425'
source_url: https://arxiv.org/abs/2510.02425
tags:
- sensory
- figure
- alignment
- caption
- visual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper shows that sensory prompts can steer the internal representations
  of text-only language models toward modality-specific perceptual structure. By asking
  the model to "see" or "hear" a description, its generative representations become
  more aligned with those of specialist vision or audio encoders, respectively.
---

# Words That Make Language Models Perceive

## Quick Facts
- **arXiv ID:** 2510.02425
- **Source URL:** https://arxiv.org/abs/2510.02425
- **Reference count:** 40
- **Primary result:** Sensory prompts steer text-only LLMs toward modality-specific perceptual representations without additional training.

## Executive Summary
This paper demonstrates that simple sensory prompts can shift the internal representations of text-only language models toward modality-specific perceptual structure. By instructing a model to "see" or "hear" a description, its generative representations become more aligned with those of specialist vision or audio encoders, respectively. The alignment increases with generation length and model size, and sensory language is necessary—generic visual or auditory words alone are insufficient. Larger models exhibit stronger and more separable modality-specific representations. These findings suggest that language models implicitly encode multimodal structure and that inference-time prompting can elicit perceptual grounding without additional training.

## Method Summary
The authors use frozen Qwen3 LLMs (0.6B-32B) and three caption datasets (WiT, DCI, AudioCaps2.0, Clotho v2). For each caption, they construct prompts with sensory cues (SEE: "Imagine what it would look like to see {caption}."; HEAR: "Imagine what it would sound like to hear {caption}."; No cue: "Imagine: {caption}."). They generate 128 tokens autoregressively and extract generative representations by averaging hidden states across all layers and generated tokens. They compute mutual k-NN alignment between LLM kernels and sensory encoder kernels (DINOv2-Base for vision, BEATs-Iter3 for audio), measuring representational similarity.

## Key Results
- Sensory prompts significantly increase LLM alignment to vision encoders when prompted to "see" and to audio encoders when prompted to "hear"
- Alignment increases with generation length (up to ~256 tokens) and model size, and requires sensory language, not just modality keywords
- Generative representations (averaged across layers and tokens) are necessary; single-pass embeddings show decreased alignment with sensory cues

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Sensory cues condition the model's latent uncertainty over what sensory evidence could have produced the text
- **Mechanism:** Text-only LLMs implicitly maintain a distribution over possible perceptual causes. When prompted to "see" or "hear," the model resolves this uncertainty toward a specific modality, producing generations consistent with that interpretation
- **Core assumption:** LLMs trained on text describing sensory experiences encode latent multimodal structure even without direct perceptual training
- **Evidence anchors:** [abstract] "When a sensory prompt tells the model to 'see' or 'hear', it cues the model to resolve its next-token predictions as if they were conditioned on latent visual or auditory evidence that is never actually supplied."
- **Break condition:** If LLMs lack latent multimodal structure (e.g., very small models or those trained on synthetic text without sensory descriptions), sensory prompts should show no effect

### Mechanism 2
- **Claim:** Generative representations—hidden states accumulated across autoregressive steps—create modality-aligned embeddings unavailable in single-pass encoding
- **Mechanism:** Each generation step conditions on prior tokens, allowing the model to elaborate modality-specific content. Averaging hidden states across layers and tokens captures this refined representation
- **Core assumption:** Residual connections make layer-averaged hidden states a meaningful summary of model state
- **Evidence anchors:** [section 2.1] "We define the representations: z_g(c, T) = (1/L(t+T)) * sum over layers and tokens of hidden states."
- **Break condition:** If using models without residual connections, or if extracting only final-layer outputs, generative representation benefits may not hold

### Mechanism 3
- **Claim:** Alignment gains require scene-appropriate sensory language, not mere modality keywords
- **Mechanism:** Mutual-kNN alignment captures relational structure—captions cluster similarly to how images/audio cluster. Random visual words don't create the right relationships; correct sensory descriptions do
- **Core assumption:** Sensory encoders (DINOv2, BEATs) capture meaningful modality-specific structure that text can approximate
- **Evidence anchors:** [section 3.3] "Alignment decreases when captions are appended with random visual words, and drops further when captions are replaced entirely by them."
- **Break condition:** Hallucinated sensory details that contradict the actual input content will reduce alignment despite modality-appropriate language

## Foundational Learning

- **Concept:** Kernel-based representational similarity (mutual k-NN alignment)
  - **Why needed here:** This metric quantifies whether two models organize their embedding spaces similarly. You cannot interpret alignment scores without understanding that k-NN overlap measures relational structure, not absolute embedding proximity
  - **Quick check question:** If Model A and Model B have 80% mutual k-NN overlap on a dataset, what does this mean about their representations?

- **Concept:** Autoregressive generation and hidden state accumulation
  - **Why needed here:** The paper's core innovation—generative representations—requires understanding that each token generation involves a forward pass producing new hidden states that can be aggregated
  - **Quick check question:** Why would averaging hidden states across 128 generated tokens differ from taking hidden states from a single 128-token forward pass?

- **Concept:** Self-supervised sensory encoders (DINOv2, BEATs)
  - **Why needed here:** These serve as ground truth for modality-specific structure. DINOv2 learns visual features without language; BEATs learns audio features. Understanding what they encode is essential for interpreting alignment
  - **Quick check question:** Why compare text-only LLMs to these encoders rather than to multimodal models like CLIP?

## Architecture Onboarding

- **Component map:** Prompt templates → Generation loop → Layer-wise hidden states → Representation aggregation → Kernel construction → Alignment score
- **Critical path:** Prompt → Generation → Layer-wise hidden states → Averaged representation → Kernel construction → Alignment score
- **Design tradeoffs:**
  - Generation length: Longer (up to ~256 tokens) increases alignment but risks semantic drift; 512 tokens shows decline
  - Layer selection: Mean across all layers outperforms single layers; adds computation but captures hierarchical information
  - Encoder choice: Self-supervised encoders (DINOv2) vs. language-supervised (CLIP)—CLIP shows higher baseline alignment, making effect sizes smaller
- **Failure signatures:**
  - Single-pass embeddings with sensory cues show *decreased* alignment—prompt must influence generation, not just encoding
  - Small models (0.6B) show weak effects; visual bias dominates regardless of cue
  - Ablating sensory language from generated text eliminates alignment gains
- **First 3 experiments:**
  1. **Replicate alignment by generation length:** Generate 32, 64, 128, 256 tokens for a held-out caption set; plot alignment to DINOv2. Expect monotonic increase with possible plateau or decline at 512
  2. **Ablation sanity check:** Take SEE-cued generations, replace sensory words with neutral synonyms, re-compute alignment. Confirm drop
  3. **Cross-model transfer:** Test whether Qwen3-14B sensory-cued generations, when fed to a *different* frozen LLM (e.g., Llama-3-8B) as context, transfer the alignment benefit. This tests whether the effect is in the text vs. the hidden states

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can inference-time prompting steer LLM generative representations toward non-sensory latent structures, such as spatial layout or sentiment, with similar effectiveness to sensory prompting?
- **Basis in paper:** [explicit] Discussion section: "In our case the prior is sensory, but in principle it could be other characteristics—for example, spatial layout or sentiment."
- **Why unresolved:** The paper successfully demonstrates steering for visual and auditory modalities, but the authors propose that the mechanism could generalize to other abstract properties without providing empirical evidence for these specific non-sensory domains
- **What evidence would resolve it:** Applying the same methodology (generative representations + mutual-kNN alignment) to datasets with ground truth for spatial relations or sentiment, and testing if specific prompts (e.g., "feel the emotion of...") increase alignment with encoders trained on those specific structures

### Open Question 2
- **Question:** To what degree can the alignment between LLMs and sensory encoders be improved through systematic prompt optimization?
- **Basis in paper:** [explicit] Limitations section: "We have not fully explored the degree to which this alignment can be improved... we focus primarily on lightweight cues... [and] do not explore broader variation in instruction phrasing."
- **Why unresolved:** The authors relied on simple cues like "see" or "hear" and noted that different verbs (e.g., "describe" vs. "imagine") yield different results, but they did not perform a systematic search to find the optimal phrasing
- **What evidence would resolve it:** Implementing an automated prompt optimization search (e.g., genetic algorithms or soft prompting) to maximize the mutual-kNN alignment score, thereby establishing the upper bound of representational alignment achievable via prompting

### Open Question 3
- **Question:** Why does averaging embeddings across all layers yield higher sensory alignment than any single layer?
- **Basis in paper:** [explicit] Appendix A: "One possible explanation is that averaging smooths out layer-specific noise while retaining complementary information... though a full understanding of this phenomenon remains open for future study."
- **Why unresolved:** The paper observes empirically that the mean embedding outperforms individual layers (including the final layer), but the theoretical justification regarding how sensory information is distributed across the model hierarchy remains unverified
- **What evidence would resolve it:** A layer-wise probing analysis to determine if low-level sensory features (e.g., edges, frequencies) cluster in early layers while semantic concepts cluster in later layers, such that averaging captures the full "sensory-to-semantic" spectrum

### Open Question 4
- **Question:** Is the persistent gap in audio alignment compared to visual alignment primarily caused by the linguistic ineffability of low-level acoustic features?
- **Basis in paper:** [explicit] Limitations section: "One explanation is that audio encoders... learn low-level acoustic patterns... that map less directly to language, whereas vision encoders... capture object- and scene-level features that align more naturally with words."
- **Why unresolved:** The authors hypothesize that the nature of audio (timbre, rhythm) makes it harder to map to text than visual objects, and they note that audio encoders with semantic labels (BEATs+) align better, but they do not confirm this as the definitive cause of the gap
- **What evidence would resolve it:** Testing alignment on audio datasets that use highly specialized onomatopoeic or technical acoustic vocabulary to see if richer audio-specific language bridges the alignment gap with low-level audio encoders

## Limitations
- The study uses frozen LLMs with fixed prompts and a specific alignment metric; generalizability to different prompt styles, languages, or tasks remains unclear
- The mutual k-NN alignment metric may not fully capture functional perceptual grounding, only representational similarity
- The paper does not investigate whether aligned representations improve actual multimodal tasks beyond the VQA-in-text proxy

## Confidence

- **High confidence:** The existence of sensory-prompt steering effects on generative representations, the necessity of sensory language (not just keywords), and the dependence on generation length and model size
- **Medium confidence:** The interpretation that LLMs maintain latent multimodal uncertainty over sensory causes; alternative explanations cannot be fully ruled out
- **Medium confidence:** The claim that this reveals LLMs encode "grounding" without additional training; functional grounding is only weakly demonstrated

## Next Checks
1. **Behavioral grounding test:** Use sensory-cued generations as context for a downstream multimodal reasoning task (e.g., visual question answering with image input). Measure whether the sensory priming improves accuracy compared to neutral prompts
2. **Cross-lingual and cross-domain transfer:** Apply the same sensory prompting paradigm to non-English captions and diverse domains (e.g., scientific or technical text with sensory descriptions)
3. **Alternative alignment metrics:** Recompute the main results using different representational similarity measures (e.g., centered kernel alignment, Procrustes distance, or linear probe accuracy) to verify that the alignment gains are not artifacts of the specific k-NN metric chosen