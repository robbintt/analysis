---
ver: rpa2
title: How far away are truly hyperparameter-free learning algorithms?
arxiv_id: '2505.24005'
source_url: https://arxiv.org/abs/2505.24005
tags:
- training
- methods
- learning
- algorithms
- rate
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Learning-rate-free methods from recent literature fail to match
  strong baseline optimizers when using default hyperparameter settings across diverse
  deep learning workloads. A systematic search for evidence-based default settings
  for regularization and training horizons improves their performance, but they still
  slightly lag behind similarly calibrated NadamW and AdamW methods.
---

# How far away are truly hyperparameter-free learning algorithms?

## Quick Facts
- arXiv ID: 2505.24005
- Source URL: https://arxiv.org/abs/2505.24005
- Reference count: 40
- Key outcome: Learning-rate-free methods achieve competitive results on 4-5 out of 8 workloads but still lag behind calibrated AdamW/NadamW baselines.

## Executive Summary
This paper evaluates recent "learning-rate-free" optimization methods (Prodigy, Mechanic, DoG, DoWG, D-Adapt, COCOB, MoMo) against strong AdamW/NadamW baselines on 8 diverse deep learning workloads. The authors systematically search for evidence-based default settings for regularization and training horizons, finding that while calibrated learning-rate-free methods improve significantly over literature defaults, they still slightly underperform calibrated AdamW/NadamW. The study reveals that removing the base learning rate alone doesn't simplify hyperparameter tuning as much as hoped, and that training horizon (schedule length) emerges as a critical, often overlooked hyperparameter. The best learning-rate-free methods achieve competitive results on 4-5 out of 8 workloads, showing promise but not yet surpassing standard optimizers.

## Method Summary
The authors evaluated learning-rate-free optimizers on the AlgoPerf: Training Algorithms benchmark (self-tuning track) across 8 diverse workloads including Criteo 1TB DLRM, fastMRI U-Net, ImageNet ResNet-50, and others. They performed quasi-random search (200 points) over regularization parameters (weight decay, dropout, label smoothing, beta values) and training horizons (33%, 50%, 66% of max steps). For each optimizer, they selected top-3 configurations based on geometric mean of time-to-target, then retrained with 5 random seeds to compute final AlgoPerf scores. The benchmark score aggregates performance profiles across all workloads, measuring the fraction of tasks where an algorithm is within a factor τ of the best time-to-target.

## Key Results
- Literature defaults for learning-rate-free methods fail on >6/8 workloads when used without calibration
- Systematic calibration of non-learning-rate hyperparameters improves performance significantly but methods still lag behind calibrated AdamW/NadamW
- Best learning-rate-free methods achieve competitive results on 4-5 out of 8 workloads
- Training horizon (schedule length) proves as critical as other hyperparameters for learning-rate-free methods
- Removing base learning rate alone doesn't simplify hyperparameter tuning as much as hoped

## Why This Works (Mechanism)

### Mechanism 1
Learning-rate-free methods theoretically derive step sizes by estimating the initial distance to a minimizer (D) and gradient norms (G), removing the need for a user-defined base learning rate. Algorithms like DoG and Prodigy calculate a step size ηₖ ≈ D / (G · √K) by tracking the maximum distance between iterates and the initial point or maintaining lower bounds on D via gradient statistics. This theoretically adapts the update scale to the geometry of the loss landscape. Assumes the optimization landscape shares sufficient properties with convex, non-smooth functions such that estimated distances correlate with optimal step sizes.

### Mechanism 2
Removing the base learning rate alone is insufficient for true hyperparameter-free training because performance remains critically dependent on the learning rate schedule (training horizon) and regularization parameters. Even "free" methods require a "relative" schedule (warmup and decay) defined over a training horizon. The paper finds that the optimal training horizon (e.g., 33% vs 66% of max steps) varies significantly, effectively becoming a hidden hyperparameter that dictates when decay forces convergence. Assumes that access to a maximum step count (horizon) is a benign form of information, whereas in practice, setting this horizon is equivalent to tuning a decay schedule.

### Mechanism 3
Systematic, cross-workload calibration of regularization and momentum parameters (β₁, β₂, weight decay) can drastically improve the reliability of learning-rate-free methods, though they still lag behind calibrated AdamW/NadamW. The authors replaced literature defaults with "AlgoPerf-calibrated" settings found via quasi-random search over 8 diverse workloads. This process optimized for the geometric mean of time-to-target, effectively finding a "robust region" in the hyperparameter space that generalizes better than the original paper defaults. Assumes that a single "universal" set of non-LR hyperparameters exists that provides robust (if not optimal) performance across diverse architectures.

## Foundational Learning

- **Concept: AdamW/NadamW Optimization**
  - Why needed here: These serve as the "strong baselines" that learning-rate-free methods aim to beat. Understanding decoupled weight decay (AdamW) and Nesterov momentum (NadamW) is required to interpret the leaderboard results.
  - Quick check question: Can you explain why decoupled weight decay in AdamW is preferred over L2 regularization for adaptive methods?

- **Concept: Performance Profiles & Time-to-Target**
  - Why needed here: The paper evaluates algorithms not just by final accuracy, but by the speed at which they reach a "competitive" validation metric (time-to-target) and aggregates this via performance profiles.
  - Quick check question: If an algorithm reaches the target 20% slower than the best algorithm on a specific workload, what is its performance ratio τ in the performance profile?

- **Concept: Learning Rate Schedules (Warmup & Decay)**
  - Why needed here: The paper identifies the "training horizon" (where decay goes to zero) as a critical, often overlooked hyperparameter in "free" methods.
  - Quick check question: Why does a linear warmup followed by cosine decay necessitate defining a specific "training horizon" t_hor?

## Architecture Onboarding

- **Component map:** Optimizer (Prodigy/Mechanic) -> Scheduler (Linear warmup + Cosine decay) -> Regularization (Weight decay, label smoothing, dropout) -> Tuner (Quasi-random search over non-LR params)
- **Critical path:** 1) Select optimizer (Prodigy/Mechanic) 2) Define search space for non-LR params 3) Run calibration search over training horizon (33%, 50%, 66%) 4) Select config with highest aggregate benchmark score
- **Design tradeoffs:** Calibration effort vs. poor performance with defaults; short horizons (33%) reach targets faster but may fail to converge; long horizons (66%) are more robust but slower; Prodigy more robust across horizons, Mechanic faster on specific horizons
- **Failure signatures:** Naive divergence with paper defaults on >6/8 workloads; divergence without schedules; performance swings based on 33% vs 66% horizon choice
- **First 3 experiments:** 1) Baseline validation: Run Prodigy/Mechanic with paper defaults to confirm failure mode 2) Horizon sweep: Fix calibrated regularization, sweep training horizon to observe time-to-target impact 3) Schedule-free comparison: Implement Schedule-Free Adam to compare against best calibrated Prodigy

## Open Questions the Paper Calls Out

### Open Question 1
Can methods that remove both the base learning rate AND the learning rate schedule (like Schedule-Free Adam) achieve meaningfully better cross-workload generalization than methods that only remove the base learning rate? The authors note that Schedule-Free Adam "convincingly won the self-tuning track" by hitting 7/8 validation targets compared to max 5 from their search, but this comparison was post-hoc rather than direct.

### Open Question 2
Can learning-rate-free methods ever provide practical tuning benefits over standard optimizers, or does removing the learning rate simply shift the tuning burden to other hyperparameters without net gain? The authors conclude that "removing the overall base learning rate from the tuning problem did not actually seem to make the search for a configuration of the other non-learning rate hyperparameters that worked well across multiple workloads appreciably easier."

### Open Question 3
Why do literature-supplied default hyperparameters for learning-rate-free methods perform so poorly compared to AlgoPerf-calibrated defaults? The paper documents dramatic performance improvements after calibration but states "We found no single, simple explanation for the poor performance of these naive learning-rate-free methods."

### Open Question 4
Can calibrated hyperparameter defaults transfer to workloads outside the calibration set (e.g., different architectures, modalities, or scales), or is workload diversity within a benchmark insufficient for true generalization? The authors acknowledge their setup doesn't have any held-out workloads, calling it "a necessary first step towards solving the general problem."

## Limitations
- Performance gaps between learning-rate-free methods and calibrated baselines remain non-trivial (2-4% AlgoPerf score)
- The term "hyperparameter-free" is somewhat misleading as significant tuning is still required for regularization and training horizon
- Results are sensitive to the specific benchmark suite and may not transfer to all domains
- Paper assumes access to maximum training steps as a benign hyperparameter, but this effectively encodes a schedule

## Confidence
- **High confidence:** Literature defaults for learning-rate-free methods perform poorly across diverse workloads
- **Medium confidence:** Removing base learning rate alone doesn't significantly simplify hyperparameter tuning
- **Medium confidence:** Prodigy and Mechanic with calibrated parameters achieve competitive results on 4-5/8 workloads

## Next Checks
1. Evaluate AlgoPerf-calibrated hyperparameters on a new, held-out architecture to assess true robustness
2. Implement and compare against Schedule-Free Adam to directly test if removing schedules provides larger gains than removing base learning rate
3. Test the best-performing learning-rate-free method with calibrated defaults on a dynamic dataset where maximum step count is unknown to assess practical usability without horizon tuning