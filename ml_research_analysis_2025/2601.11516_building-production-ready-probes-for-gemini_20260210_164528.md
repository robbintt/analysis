---
ver: rpa2
title: Building Production-Ready Probes For Gemini
arxiv_id: '2601.11516'
source_url: https://arxiv.org/abs/2601.11516
tags:
- probe
- probes
- alphaevolve
- error
- test
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Activation probes detect harmful prompts in language models by
  analyzing internal activations. Existing probe architectures fail to generalize
  from short-context to long-context inputs, limiting their deployment effectiveness.
---

# Building Production-Ready Probes For Gemini

## Quick Facts
- arXiv ID: 2601.11516
- Source URL: https://arxiv.org/abs/2601.11516
- Authors: János Kramár; Joshua Engels; Zheng Wang; Bilal Chughtai; Rohin Shah; Neel Nanda; Arthur Conmy
- Reference count: 40
- Primary result: Activation probes detect harmful prompts by analyzing internal activations, achieving comparable performance to language models at over 10,000× lower inference cost

## Executive Summary
Activation probes analyze internal activations of language models to detect harmful prompts. Standard probe architectures fail to generalize from short-context to long-context inputs, limiting their deployment effectiveness. This work introduces new probe architectures, including MultiMax and Max of Rolling Means Attention Probes, that handle long-context data without requiring expensive training on long sequences. The research demonstrates that combining probes with language models in cascading classifiers achieves optimal accuracy at a fraction of computational cost. AlphaEvolve automation discovers improved probe designs that outperform baselines, and these findings have been deployed in production Gemini systems.

## Method Summary
The study trains binary classification probes on Gemini 2.5 Flash activations to detect cyber-offensive prompts. Probes are small neural networks trained on hidden states at a middle model layer, using full-batch AdamW optimization (lr=1e-4, weight decay=3e-3, 1000 epochs). Key architectural innovations include MultiMax aggregation (selecting highest-scoring token per attention head) and rolling window attention (local context within fixed-width windows). The cascading classifier combines cheap probes with expensive LLM classifiers through threshold-optimized deferral. Training uses 3,175 samples across multiple datasets including short-context attacks, overtriggering traffic, hard negatives, multi-turn conversations, and long-context (>900k tokens). Evaluation uses weighted error metrics with asymmetric costs (FNR=5, hard negative FPR=2, overtriggering FPR=50).

## Key Results
- MultiMax attention probes reduce false positive rates on long-context benign data by preventing signal dilution through max-based aggregation
- Cascading classifiers combining probes with LLMs achieve lower error than standalone models while using roughly 1/50th the inference cost
- AlphaEvolve-discovered probe architectures achieve 2.53% test error, competitive with long-context trained attention probes (2.38%) while training exclusively on short-context data

## Why This Works (Mechanism)

### Mechanism 1: Max-Based Aggregation Prevents Signal Dilution in Long Contexts
Standard attention probes compute weighted averages across all tokens via softmax. In long contexts (>900k tokens), harmful content occupies a tiny fraction, so softmax dilutes the signal by averaging across mostly-benign tokens. MultiMax selects only the highest-scoring token per attention head, preserving sparse harmful signals. This exploits the assumption that harmful intent is concentrated in specific tokens rather than distributed across the sequence.

### Mechanism 2: Rolling Window Attention Captures Local Harmful Context
Attention-weighted aggregation within fixed-width sliding windows (width=10) balances local context sensitivity with long-sequence robustness. Standard attention over full sequences dilutes signals. Rolling windows compute local attention-weighted scores, then max-pooling across windows finds the most harmful region. This combines local context modeling with the sparse-signal preservation of max aggregation, assuming harmful content manifests within localized token windows.

### Mechanism 3: Cascading Classifier Optimizes Cost-Accuracy via Threshold-Based Deferral
Combining a cheap probe with an expensive LLM classifier through threshold-optimized deferral achieves lower error than either component alone at a fraction of LLM cost. The probe scores all inputs, deferring to the LLM for scores in an uncertainty band between thresholds. This exploits probe efficiency while retaining LLM robustness for edge cases, assuming probe confidence correlates with correctness.

## Foundational Learning

- **Activation Probing**
  - Why needed here: Probes are small neural networks trained on LLM hidden states (activations) at a specific layer. Understanding that activations encode semantic information linearly is foundational.
  - Quick check question: Can you explain why probing a middle layer might capture intent better than probing the final layer?

- **Distribution Shift in ML**
  - Why needed here: The core problem is train-test distribution mismatch (short-context training vs. long-context deployment). Probes generalize poorly when deployment data differs from training data.
  - Quick check question: Why might a probe trained on <10k token sequences fail on >900k token sequences, even if the harmful content is identical?

- **Threshold Selection Under Asymmetric Costs**
  - Why needed here: False positives (blocking benign users) are weighted 50× more heavily than false negatives in the validation metric. Thresholds must be optimized for this asymmetric cost structure.
  - Quick check question: How would increasing the FPR weight from 50 to 100 affect the selected threshold?

## Architecture Onboarding

- **Component map**: Residual stream activations → Per-position transformation (MLP) → Attention heads → Aggregation function → Head combination → Output scalar. Linear probes use identity transform + mean aggregation. Attention probes use MLP + learned attention. MultiMax uses MLP + max per head.

- **Critical path**: For long-context robustness, the critical design decision is the aggregation function. Softmax/mean aggregation dilutes sparse signals; max-based aggregation preserves them. Rolling window variants add local context.

- **Design tradeoffs**:
  - MultiMax: Best for sparse harmful content, poor for distributed attacks (high variance across seeds)
  - Rolling Attention: More stable across seeds, handles local context, slightly higher error than MultiMax
  - AlphaEvolve: Best test error but requires automated search infrastructure; architecture less interpretable
  - LC Training: Matches probe performance but 22× higher training cost

- **Failure signatures**:
  - High FPR on long-context benign data (FPR LC[RT] > 1%) indicates aggregation dilution
  - High FNR on long-context attacks (FNR LC[A] > 5%) indicates signal loss from over-aggressive pooling
  - Large seed variance (IQR > 0.02 in test loss) indicates architecture instability

- **First 3 experiments**:
  1. Replicate the short-context to long-context generalization gap: Train a mean-pooled linear probe on SC data, evaluate on LC test sets. Expect FNR LC[A] ≈ 99%.
  2. Compare MultiMax vs. mean aggregation on attention probe: Same training data, swap aggregation at inference only. Measure FPR LC[RT] reduction.
  3. Build a minimal cascade: Train Rolling Attention probe, set thresholds to defer 10% to a small LLM, verify error reduction vs. probe-only baseline.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can multi-layer probes using activations from all model layers significantly outperform single-layer probes for misuse detection?
- Basis in paper: Section 7 states: "we do not evaluate against probes using activations from every layer of the model as applied in concurrent work (Cunningham, Wei, et al., 2026)."
- Why unresolved: The authors intentionally restricted to single-layer probes for infrastructure simplicity but acknowledge concurrent work suggests multi-layer approaches may improve performance.
- What evidence would resolve it: Compare single-layer vs. multi-layer probe architectures on the same cyber-misuse datasets using identical evaluation protocols.

### Open Question 2
- Question: Can probes trained on model activations detect harmful content partway through a generation, enabling mid-generation intervention?
- Basis in paper: Section 7 states: "we do not investigate whether any methods might classify that a language model rollout should be flagged partway through the rollout."
- Why unresolved: The study only monitors inputs, not generated outputs, leaving unexplored whether activation patterns during generation reveal emerging harmful content.
- What evidence would resolve it: Train probes on activation sequences during token generation and evaluate their ability to flag harmful outputs before completion.

### Open Question 3
- Question: What probe architectures or training methods can achieve robustness against adaptive adversarial attacks where attackers know the defense?
- Basis in paper: Section 5.1 notes "attack success rates are extremely high" with "all techniques have a vulnerability on at least 1% of queries" on jailbreaks, and the paper cites Nasr et al. (2025) arguing robustness may be "extremely challenging, if not impossible."
- Why unresolved: No method achieved adversarial robustness; all probes and LLM classifiers remain vulnerable to prompt-based attacks.
- What evidence would resolve it: Develop probes that maintain low FNR under adaptive attacks where adversaries optimize specifically against the deployed probe.

## Limitations

- Internal Data Dependency: The study relies heavily on proprietary Google datasets for training and evaluation, limiting external reproducibility and making it difficult to assess whether results generalize to public datasets.
- Architecture Search Interpretability: AlphaEvolve discovers architectures that outperform hand-designed probes, but the resulting designs are less interpretable and lack clear mechanistic explanations.
- Computational Cost Assumptions: The claimed 10,000× cost reduction assumes specific hardware and implementation details that aren't fully specified, making comparisons dependent on deployment context.

## Confidence

**High Confidence**: The fundamental claim that activation probes can detect harmful prompts with substantially lower computational cost than LLMs is well-supported by empirical results across multiple test distributions.

**Medium Confidence**: The specific probe architecture improvements (MultiMax, Rolling Attention) demonstrate consistent benefits, but the magnitude of improvement varies across seeds and distribution shifts. Generalization from short-context to long-context inputs is improved but not completely solved.

**Low Confidence**: The AlphaEvolve-discovered architectures' superiority is demonstrated but not fully explained mechanistically. Without understanding why these specific architectures work better, it's difficult to assess whether improvements transfer to other domains.

## Next Checks

1. **Public Dataset Reproducibility**: Implement MultiMax and Rolling Attention probe architectures on a public harmful content detection dataset (e.g., RealToxicityPrompts or Jigsaw). Train on short-context data only, then evaluate on both short and long-context variants created by concatenation or context expansion.

2. **Ablation on Aggregation Functions**: Systematically test the importance of max-based vs. mean-based aggregation by training identical probe architectures differing only in their aggregation function (softmax mean vs. MultiMax vs. rolling window max).

3. **Cascade Threshold Calibration**: Implement the cascading classifier framework and conduct a systematic threshold sweep (τ₁, τ₂) to map the Pareto frontier between total cost and weighted error. Verify that optimal thresholds generalize when applied to different LLM models or when cost asymmetry is varied.