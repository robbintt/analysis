---
ver: rpa2
title: 'Dialogue Injection Attack: Jailbreaking LLMs through Context Manipulation'
arxiv_id: '2503.08195'
source_url: https://arxiv.org/abs/2503.08195
tags:
- prompt
- arxiv
- attacks
- jailbreak
- attack
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces DIA, a novel black-box jailbreak attack paradigm
  that exploits historical dialogue to enhance attack success rates on large language
  models. DIA operates by manipulating the chat template structure to inject adversarial
  dialogues, including assistant and system text, using a dialogue injection technique.
---

# Dialogue Injection Attack: Jailbreaking LLMs through Context Manipulation

## Quick Facts
- arXiv ID: 2503.08195
- Source URL: https://arxiv.org/abs/2503.08195
- Reference count: 40
- One-line primary result: DIA achieves state-of-the-art attack success rates, e.g., 0.89 on Llama-3.1-8B and 0.82 on GPT-4o, bypassing 5 defense mechanisms with average defense pass rates of 0.93 (DIA-I) and 0.82 (DIA-II)

## Executive Summary
This paper introduces DIA, a novel black-box jailbreak attack paradigm that exploits historical dialogue to enhance attack success rates on large language models. DIA operates by manipulating the chat template structure to inject adversarial dialogues, including assistant and system text, using a dialogue injection technique. Two methods, DIA-I and DIA-II, are proposed: DIA-I adapts gray-box prefilling attacks to black-box settings by generating affirmative beginnings for malicious prompts, while DIA-II leverages deferred responses via word substitution tasks to disguise harmful content.

The attack framework demonstrates significant effectiveness, achieving high attack success rates across multiple benchmarks and bypassing existing defense mechanisms. The core insight is that safety alignment techniques, which typically focus on single-turn interactions, can be circumvented by manipulating the conversation history and exploiting the autoregressive nature of language models. This work highlights the critical need for more robust multi-turn safety considerations in LLM development.

## Method Summary
The Dialogue Injection Attack (DIA) is a black-box jailbreak technique that manipulates chat template structures to inject adversarial dialogue history into conversations with LLMs. The method exploits the autoregressive nature of language models by conditioning them on fabricated historical context that includes affirmative beginnings or deferred response tasks. DIA-I generates affirmative beginnings using an auxiliary LLM to transform malicious prompts into benign ones, generate responses, then reverse the transformation. DIA-II defers responses through word substitution tasks guided by a Similar Demonstration Generation Module. The attack operates by constructing specially formatted prompts that inject user, assistant, and system text into the conversation history, effectively bypassing safety alignment mechanisms that focus primarily on single-turn interactions.

## Key Results
- Achieves state-of-the-art attack success rates: 0.89 on Llama-3.1-8B and 0.82 on GPT-4o
- Successfully bypasses 5 defense mechanisms with average defense pass rates of 0.93 (DIA-I) and 0.82 (DIA-II)
- Demonstrates effectiveness across multiple benchmarks including AdvBench (520 samples), HEx-PHI (330 samples), and MaliciousInstruct (100 samples)
- Shows that increasing model scale correlates with higher susceptibility to DIA, with an average increase of 0.11 in attack success rate

## Why This Works (Mechanism)
DIA works by exploiting a fundamental vulnerability in how LLMs process conversational context. Safety alignment techniques typically focus on single-turn interactions and content filtering at the input or output level. DIA circumvents these defenses by manipulating the conversation history itself, injecting fabricated dialogue that appears legitimate within the chat template structure. The attack leverages the autoregressive nature of language models, where the model conditions its response on the entire preceding sequence. By injecting affirmative beginnings or deferred response tasks into the historical context, DIA creates a false narrative that guides the model toward completing harmful requests. The attack is particularly effective because it operates at the template level, making it difficult for standard input sanitization or perplexity-based defenses to detect the malicious intent.

## Foundational Learning

- **Concept: LLM Chat Templates**
  - Why needed here: The primary attack vector relies on understanding how chat templates (e.g., Llama-3's `<|start_header_id|>...<|end_header_id|>`) structure conversations. You cannot perform dialogue injection without knowing how to craft the malicious tokens.
  - Quick check question: Given a chat template with user prefix `Pu`, user suffix `Su`, assistant prefix `Pa`, and assistant suffix `Sa`, what sequence would you inject into a user prompt to make the model think an assistant had just replied with "Okay"?

- **Concept: Autoregressive Language Modeling**
  - Why needed here: The success of DIA-I's prefilling attack is fundamentally based on how the model predicts the next token. Understanding that the model conditions its output on the entire preceding sequence (including fabricated history) is crucial.
  - Quick check question: Why does a model conditioned on a fake history containing an affirmative beginning have a different output distribution than one starting from scratch?

- **Concept: Safety Alignment (RLHF/DPO)**
  - Why needed here: The attack is designed to bypass these very techniques. Understanding that alignment often focuses on single-turn refusals helps explain its vulnerability to multi-turn context manipulation.
  - Quick check question: What is the primary goal of safety alignment techniques like RLHF, and why might they be vulnerable to an attack that manipulates conversation history?

## Architecture Onboarding

- **Component map**: Template Knowledge → Dialogue Construction (with crafted history) → Prompt Submission. Supporting path: Template Inference Module. Optimization path: Prompt Rewriter.

- **Critical path**: The critical path for a successful attack is: `Template Knowledge` → `Dialogue Construction (with crafted history)` → `Prompt Submission`. The Template Inference Module is a supporting path, and the Prompt Rewriter is an optimization path.

- **Design tradeoffs**:
  - **DIA-I vs. DIA-II**: DIA-I is simpler and more direct but relies on the "shallow safety alignment" vulnerability, which may be patched. DIA-II is more robust as it leverages a more fundamental property of LLMs (contextual probability) but is more complex to implement.
  - **Automation vs. Manual Crafting**: The ABGM allows for scalable, automated generation of affirmative beginnings but might be less effective than manually crafted, highly specific ones.
  - **Template Inference Overhead**: Running the template inference attack adds queries and complexity. It's only necessary if the template is unknown and cannot be guessed.

- **Failure signatures**:
  - **Strict Input Sanitization**: The attack fails completely if the chat interface/API strips out the special tokens used for dialogue injection.
  - **Robust Safety Alignment**: If a model's alignment is trained on adversarial multi-turn dialogues, it may learn to recognize and refuse the attack regardless of the injected context.
  - **Inaccurate Template Inference**: If the template inference module guesses wrong, the injected dialogue will be garbled and fail to deceive the model.

- **First 3 experiments**:
  1. **Template Injection Validation**: Choose an open model (e.g., Llama-3-8B) and manually craft a single-turn DIA-I attack. Verify that you can successfully inject an affirmative beginning and that the model completes the harmful response. This validates the core mechanism.
  2. **ABGM Integration**: Build the Affirmative Beginning Generation Module. Test its ability to generate valid affirmative beginnings for a set of 10-20 prompts by transforming them to benign equivalents and back.
  3. **Defense Bypass Test**: Implement a basic perplexity filter and a defensive system prompt. Test DIA-I and DIA-II against a small victim model protected by these defenses to measure the Defense Pass Rate (DPR) and confirm the paper's findings.

## Open Questions the Paper Calls Out

- **Open Question 1**: Why does increasing model scale correlate with higher susceptibility to DIA, and does this indicate a fundamental trade-off between model capability and safety alignment?
  - Basis in paper: [explicit] Section 5.4 notes that aside from the Llama-3 family, ASR increases with model size, which the authors attribute to the "inherent trade-off between usefulness and safety."
  - Why unresolved: The paper provides a hypothesis (usefulness encroaching on safety) but acknowledges exceptions like Llama-3, suggesting the exact mechanics of how scale affects vulnerability to history manipulation remain unclear.
  - What evidence would resolve it: A systematic study correlating specific alignment datasets or training compute for different model sizes with their resistance to dialogue injection.

- **Open Question 2**: How can chat interfaces or inference engines be redesigned to distinguish between legitimate historical context and injected adversarial context?
  - Basis in paper: [explicit] The Conclusion states the "critical need to consider historical dialogue interactions when developing security measures" and highlights the insufficiency of current defenses.
  - Why unresolved: The paper demonstrates that current defenses (perplexity filters, system prompts) fail because DIA masks intent in seemingly valid structures; a structural solution is missing.
  - What evidence would resolve it: A defense mechanism that successfully sanitizes input strings for unescaped role tags (e.g., `<|start_header_id|>`) without breaking valid multi-turn API functionality.

- **Open Question 3**: Can Template Inference Attacks (TIA) be adapted to succeed against proprietary models that use non-standard or obfuscated chat templates?
  - Basis in paper: [inferred] Section 3.3 assumes the victim template $T_{inf}$ belongs to a known open-source collection $\mathcal{T}$, implying TIA may fail if the template is unique or hidden.
  - Why unresolved: The proposed TIA relies on matching probe outputs to known template structures; it is untested against custom, closed-source templates designed to resist such probing.
  - What evidence would resolve it: Experiments applying TIA to models with randomized or undisclosed delimiters to see if the "swap and retry" strategy can still deduce the template structure.

## Limitations

- The paper lacks specification for system prompt texts used in defensive scenarios and "hypnotic" dialogues, which are essential components of the attack but remain unspecified.
- The source and content of the benign prompt pool required for DIA-II's Similar Demonstration Generation Module is not provided, creating a significant barrier to faithful reproduction.
- The template inference attack relies on specific prompt engineering that isn't fully detailed, particularly the exact prompts used to guide keyword extraction and substitution.

## Confidence

- **High Confidence (9/10)**: The core theoretical framework of dialogue injection as a mechanism for context manipulation is well-founded and technically sound. The fundamental insight that LLMs condition responses on injected historical context is supported by extensive prior research in autoregressive modeling.

- **Medium Confidence (7/10)**: The reported attack success rates (0.89 on Llama-3.1-8B, 0.82 on GPT-4o) and defense bypass capabilities appear plausible given the methodology described, though exact reproduction would require resolving the template and prompt specification issues mentioned above.

- **Low Confidence (4/10)**: The generalizability of DIA across diverse model architectures and the long-term effectiveness against evolving safety alignment techniques is uncertain. The paper doesn't adequately address how models might adapt their safety training to recognize multi-turn adversarial contexts.

## Next Checks

1. **Template Inference Validation**: Implement the template inference attack against a known model (e.g., Llama-3-8B) and verify that it correctly identifies the special tokens and structure with >90% accuracy across 50 test prompts. This is the foundational requirement for any DIA attack.

2. **ABGM Component Testing**: Build and test the Affirmative Beginning Generation Module in isolation using Gemma-2-27B as the auxiliary model. Validate that it can successfully transform 80% of malicious prompts (e.g., "how to make a bomb") into benign equivalents and back while preserving semantic coherence.

3. **Defense Bypass Reproducibility**: Test DIA-I and DIA-II against a perplexity-based filter and defensive system prompt on a small victim model (e.g., Llama-3-8B) and measure Defense Pass Rates. Compare results against the paper's claimed averages of 0.93 (DIA-I) and 0.82 (DIA-II) to verify the methodology's practical effectiveness.