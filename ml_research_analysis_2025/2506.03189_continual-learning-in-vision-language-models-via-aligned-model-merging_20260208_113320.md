---
ver: rpa2
title: Continual Learning in Vision-Language Models via Aligned Model Merging
arxiv_id: '2506.03189'
source_url: https://arxiv.org/abs/2506.03189
tags:
- task
- merging
- learning
- tasks
- lora
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses catastrophic forgetting in continual learning
  for vision-language models (VLMs) by shifting from sequential fine-tuning to model
  merging. The authors propose Parameter Alignment for Merging (PAM), which merges
  LoRA adapters from newly learned tasks with a global evolving LoRA while periodically
  aligning parameters during training to reduce interference.
---

# Continual Learning in Vision-Language Models via Aligned Model Merging

## Quick Facts
- **arXiv ID:** 2506.03189
- **Source URL:** https://arxiv.org/abs/2506.03189
- **Reference count:** 26
- **Primary result:** Parameter Alignment for Merging (PAM) significantly reduces catastrophic forgetting in VLMs through aligned model merging, achieving higher BWT and FWT than standard fine-tuning and other CL methods without requiring task replay.

## Executive Summary
This paper addresses catastrophic forgetting in continual learning for vision-language models (VLMs) by shifting from sequential fine-tuning to model merging. The authors propose Parameter Alignment for Merging (PAM), which merges LoRA adapters from newly learned tasks with a global evolving LoRA while periodically aligning parameters during training to reduce interference. Experiments on CoIN and extended benchmarks show that PAM significantly reduces forgetting (higher BWT), improves generalization (higher FWT), and increases stability across task orders compared to standard fine-tuning and other merging methods. It also outperforms state-of-the-art CL methods when combined, even without task replay. The approach is parameter-efficient, robust, and maintains performance across diverse task sequences.

## Method Summary
PAM operates on LoRA adapters rather than full model weights, maintaining a global LoRA that evolves across tasks. For each new task, PAM clones the global LoRA to create a temporary LoRA, trains it on the new task with periodic parameter alignment, then merges it back into the global LoRA via element-wise averaging. The alignment mechanism checks every N training steps whether weights in the temporary LoRA have conflicting signs with corresponding weights in the global LoRA, and if so (and the global weight is in the top p% by magnitude), reinitializes the temporary weight to match the global weight. This prevents interference during merging while preserving plasticity for learning new tasks. The approach is evaluated on CoIN benchmark with 6 VQA tasks using PaliGemma 3B VLM with LoRA rank 32.

## Key Results
- PAM achieves significantly higher Backward Transfer (BWT) and Forward Transfer (FWT) than standard fine-tuning and other CL methods across multiple benchmarks
- During-training alignment outperforms post-training alignment, reducing forgetting by 7.44% in BWT
- The method shows robustness to task order permutations and maintains performance when combined with state-of-the-art CL methods
- Alignment percentage (p) creates a clear stability-plasticity tradeoff, with 50% default providing balanced performance

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Model merging via element-wise averaging reduces catastrophic forgetting by eliminating the sequential update bias toward recent tasks.
- **Mechanism:** Rather than overwriting weights sequentially, each task's LoRA adapter is trained independently then merged with the global LoRA. This treats knowledge integration as a parallel composition problem rather than a sequential overwrite problem.
- **Core assumption:** Task-specific LoRA weights occupy partially disjoint regions of parameter space, so averaging preserves rather than corrupts task knowledge.
- **Evidence anchors:**
  - [abstract] "Rather than just sequentially updating the model weights, we propose merging newly trained task parameters with previously learned ones, promoting a better balance."
  - [Section 3] Fig. 1 demonstrates merging alone mitigates forgetting even without explicit regularization mechanisms.
  - [corpus] Weak direct corpus support; related work (BECAME, MagMax cited in paper) explores similar merging strategies but for different CL contexts.
- **Break condition:** If task weight distributions have high overlap with opposing signs, averaging will degrade both tasks' performance.

### Mechanism 2
- **Claim:** Periodic during-training alignment of parameter signs reduces interference at merge time and preserves model plasticity.
- **Mechanism:** Every N training steps, check each weight in the temporary LoRA: if its sign differs from the corresponding global LoRA weight AND the global weight magnitude is in the top p% (e.g., 50%), reinitialize that weight to match the global weight. This prevents the optimizer from learning parameters that would cancel important prior knowledge during merge.
- **Core assumption:** Weight sign alignment correlates with knowledge preservation; misaligned signs indicate conflicting task representations.
- **Evidence anchors:**
  - [Section 4] "We leverage previously published findings, demonstrating that the weight sign is a simple and effective tool for alignment."
  - [Section 5.3] Fig. 3 shows post-training alignment (TIES) reduces accuracy by 7.44% vs. during-training alignment maintaining comparable accuracy.
  - [corpus] No direct corpus validation; sign-based alignment appears in TIES (Yadav et al.) but as post-training, not during-training.
- **Break condition:** If important weights for new task learning consistently fall within top-p% of global weights with opposite signs, alignment will prevent learning new task entirely.

### Mechanism 3
- **Claim:** Importance-weighted alignment (top-p% magnitude threshold) provides explicit control over the stability-plasticity tradeoff.
- **Mechanism:** The alignment percentage p determines what fraction of global weights are "protected" by alignment. Higher p = more stability, lower p = more plasticity. Small-magnitude weights in global LoRA are allowed to adapt freely.
- **Core assumption:** Weight magnitude in the global LoRA correlates with importance for prior tasks.
- **Evidence anchors:**
  - [Section 5.3, Table 3] "Larger percentage of alignment favors previous tasks and reduces forgetting. Increasing the alignment percentage reduces the model's plasticity."
  - [Section 5.3] Paper reports 30%, 50%, 70% alignment experiments confirming the tradeoff.
  - [corpus] Assumption: Magnitude-importance correlation is common but not universally proven; corpus lacks direct validation.
- **Break condition:** If important prior knowledge is stored in small-magnitude weights, or unimportant weights have large magnitudes, the threshold will misallocate protection.

## Foundational Learning

- **Concept: LoRA (Low-Rank Adaptation)**
  - Why needed here: PAM operates entirely on LoRA adapters, not full model weights. Understanding LoRA's factorization (W = W₀ + BA where B∈ℝⁿˣʳ, A∈ℝʳˣᵐ) is essential for interpreting the alignment and merge operations.
  - Quick check question: Can you explain why LoRA allows efficient fine-tuning while keeping pretrained weights frozen?

- **Concept: Catastrophic Forgetting / Stability-Plasticity Dilemma**
  - Why needed here: The paper frames its contribution as solving this fundamental CL problem. Understanding why sequential gradient descent overwrites prior knowledge helps explain why merging is proposed as an alternative.
  - Quick check question: Why does standard fine-tuning favor recent tasks over earlier ones?

- **Concept: Model Merging / Weight Averaging**
  - Why needed here: The paper's core intervention replaces sequential updates with merge operations. Understanding when and why weight averaging works (linear mode connectivity) is prerequisite.
  - Quick check question: Under what conditions does averaging two model weights produce a functional combined model?

## Architecture Onboarding

- **Component map:**
  - W₀: Frozen pretrained VLM backbone (PaliGemma in experiments)
  - W_G,t: Global LoRA adapter evolving over tasks (stores accumulated knowledge)
  - W': Temporary LoRA cloned from W_G,t-1 for new task training
  - τ: Magnitude threshold computed from top-p% of |W_G,t-1|
  - Merge operation: W_G,t = Avg(W_G,t-1, W') after training completes

- **Critical path:**
  1. New task arrives → Clone W_G → Create W'
  2. Train W' on D_t with periodic alignment checks
  3. Alignment: For each weight w in W', if sign(w) ≠ sign(w_G) AND |w_G| > τ, reinitialize w
  4. After training: Merge W_G,t-1 and W' via element-wise average
  5. Evaluate on all seen tasks using merged W_G,t

- **Design tradeoffs:**
  - Alignment percentage (p): Paper uses 50% default. Higher = less forgetting, lower plasticity.
  - Alignment schedule (s): Paper uses every 100 steps. Too frequent = overhead, too rare = more misalignment.
  - LoRA rank (r): Paper uses 32. Higher rank = more capacity but higher compute.
  - Reinitialization strategy: Zero vs. global weight copy—paper finds both effective with slight stability differences.

- **Failure signatures:**
  - High task dissimilarity: Paper notes "ScienceQA often causes interference with others, likely due to its higher degree of dissimilarity."
  - Low BWT with high accuracy: Suggests alignment is too aggressive (high p), preventing new task learning.
  - High variance across task orders: Suggests alignment threshold or merge is order-sensitive.
  - Accuracy drop after merge (A_m << A_t): Indicates merge operation destroying learned knowledge.

- **First 3 experiments:**
  1. **Baseline comparison:** Implement sequential fine-tuning with a single LoRA on 3-task sequence (e.g., video QA tasks from Appendix). Measure ACC, BWT, FWT. Expected: Fine-tuning shows high A_t but negative BWT.
  2. **Ablation: Alignment timing:** Compare (a) no alignment, (b) post-training alignment (TIES-style), (c) during-training alignment (PAM). Measure A_t (pre-merge) and A_m (post-merge). Expected: During-training maintains A_t while post-training reduces it.
  3. **Sensitivity analysis:** Vary alignment percentage p across [30%, 50%, 70%] on a 6-task CoIN sequence. Plot ACC vs. BWT tradeoff curve. Expected: Higher p → better BWT, worse plasticity metrics.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Does the parameter alignment and merging strategy degrade performance when new tasks exhibit significant domain shifts from previously learned tasks?
- **Basis in paper:** [explicit] The authors state in the Limitations section that the approach "may not adequately address scenarios where new data significantly differs from previously learned tasks."
- **Why unresolved:** The experiments primarily utilize VQA benchmarks (CoIN, VideoQA) which share structural similarities (image-to-text). It is unclear if "aligned" merging restricts the model's ability to learn features that are fundamentally orthogonal to the current global LoRA state.
- **What evidence would resolve it:** Evaluation on a sequence of tasks with high domain disparity (e.g., transitioning from natural images to medical imaging or remote sensing) to measure the trade-off between alignment interference and learning capacity.

### Open Question 2
- **Question:** Can combining PAM with selective weight fine-tuning further improve performance by preserving task-specific subspaces?
- **Basis in paper:** [explicit] The authors suggest in the Limitations section that "selectively fine-tuning subsets of the weights could help mitigate forgetting and enhance the forward transfer of relevant knowledge."
- **Why unresolved:** PAM currently applies alignment and merging to the LoRA parameters generally. It does not explore whether identifying and isolating specific subsets of weights (e.g., via masking) could prevent the overwriting of critical features during the alignment or merging phases.
- **What evidence would resolve it:** An ablation study where alignment/masking is applied only to specific layers or weight subspaces identified as "critical" for previous tasks, compared against the global alignment strategy used in PAM.

### Open Question 3
- **Question:** How can the alignment percentage p be adapted dynamically rather than set as a fixed hyperparameter?
- **Basis in paper:** [inferred] Section 5.3 and Table 3 demonstrate that the fixed alignment percentage p creates a trade-off between stability (high p) and plasticity (low p), implying an optimal value is task-dependent.
- **Why unresolved:** The paper uses a static p (50%) throughout training. A fixed value may be suboptimal across a sequence where tasks vary in difficulty or similarity to the global model.
- **What evidence would resolve it:** A mechanism that adjusts p based on metrics like gradient similarity or task loss, demonstrating improved BWT (Backward Transfer) and FWT (Forward Transfer) over the static baseline.

## Limitations
- The approach may not adequately address scenarios where new data significantly differs from previously learned tasks
- The fixed alignment percentage creates a static stability-plasticity tradeoff that may not be optimal across diverse task sequences
- The method relies on the assumption that weight magnitude correlates with importance, which lacks direct validation

## Confidence

- **High confidence:** The core mechanism that model merging reduces forgetting by eliminating sequential update bias (Mechanism 1). The empirical evidence showing PAM outperforms standard fine-tuning and other merging methods is robust across multiple benchmarks.
- **Medium confidence:** The effectiveness of during-training alignment vs. post-training alignment (Mechanism 2). While the paper provides evidence, the claim relies on a single comparison (TIES) without exploring the full alignment timing space.
- **Medium confidence:** The importance-weighted alignment tradeoff (Mechanism 3). The paper demonstrates the tradeoff exists but doesn't provide strong theoretical justification for why top-p% magnitude threshold is optimal.

## Next Checks

1. **Reproduce the alignment timing ablation** by comparing no alignment, post-training alignment (TIES-style), and during-training alignment (PAM) on a 3-task sequence. Measure A_t (pre-merge) and A_m (post-merge) to confirm during-training alignment maintains accuracy while reducing forgetting.

2. **Test the alignment threshold sensitivity** by running experiments with p ∈ [30%, 50%, 70%, 90%] on a 6-task CoIN sequence. Plot the stability-plasticity tradeoff curve to verify the paper's claim about the tradeoff relationship.

3. **Validate the importance assumption** by conducting a controlled experiment where important weights are deliberately assigned small magnitudes (e.g., via weight masking or targeted initialization). Measure whether PAM's performance degrades as predicted when the magnitude-importance correlation breaks down.