---
ver: rpa2
title: An Observation on Lloyd's k-Means Algorithm in High Dimensions
arxiv_id: '2506.14952'
source_url: https://arxiv.org/abs/2506.14952
tags:
- cluster
- k-means
- algorithm
- where
- case
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper provides a theoretical explanation for the failure of
  k-means clustering in high-dimensional, high-noise settings with limited samples.
  Using a simple Gaussian Mixture Model, the authors show that under certain conditions,
  almost every partition of the data becomes a fixed point of the k-means algorithm
  with high probability.
---

# An Observation on Lloyd's k-Means Algorithm in High Dimensions

## Quick Facts
- arXiv ID: 2506.14952
- Source URL: https://arxiv.org/abs/2506.14952
- Reference count: 40
- Key outcome: k-means clustering fails in high-dimensional, high-noise settings with limited samples because almost every partition becomes a fixed point with high probability.

## Executive Summary
This paper provides a theoretical explanation for the failure of k-means clustering in high-dimensional, high-noise settings with limited samples. Using a simple Gaussian Mixture Model, the authors show that under certain conditions, almost every partition of the data becomes a fixed point of the k-means algorithm with high probability. Specifically, when the noise level exceeds a threshold determined by cluster sizes and signal strength, the probability that any sample will switch clusters during an iteration decreases exponentially with dimension. This implies that the algorithm terminates at its initialization regardless of quality. Numerical experiments confirm these theoretical predictions and demonstrate that adding dimensions can worsen performance, even when additional information is theoretically available.

## Method Summary
The paper uses synthetic data generated from a simple isotropic Gaussian Mixture Model with two clusters, where samples are drawn as $x_i = \mu + \xi$ with $\xi \sim \mathcal{N}(0, \sigma^2 I_d)$. The method varies dimension ($d \in [10, 10^5]$) and noise ($\sigma^2 \in [1, 100]$) to test k-means performance under different conditions. Experiments compare random partition initialization, k-means++, and PCA-preprocessed k-means. The primary metric is the probability of cluster reassignment after one k-means iteration, along with Normalized Mutual Information (NMI) between recovered and true clusters.

## Key Results
- In high dimensions with sufficient noise, the probability of any point switching clusters during k-means iteration decreases exponentially with dimension $d$.
- Almost every partition becomes a fixed point of the k-means algorithm under these conditions, causing the algorithm to terminate immediately at initialization.
- Adding dimensions can worsen k-means performance, contrary to the intuition that more data is better.
- PCA preprocessing can recover some performance in high-dimensional, high-noise regimes where naive k-means fails.

## Why This Works (Mechanism)

### Mechanism 1: Exponential Decay of Switching Probability
The paper models the difference in squared distances to two cluster centers as a difference of scaled $\chi^2$ variables. Using moment generating functions and Markov's inequality, the authors derive an upper bound on the probability that a point is closer to the "wrong" center. This bound scales as $\rho^{d/4}$, where $\rho < 1$ under high noise. As $d$ increases, this probability vanishes rapidly, meaning points rarely move. The mechanism fails if the noise level $\sigma$ drops below the threshold defined in Equation (5), causing $\rho$ to approach 1.

### Mechanism 2: Self-Inclusion Bias in Distance Calculation
A data point is artificially "trapped" in its current cluster because the cluster center $\mu_C$ is calculated using the point itself, biasing the distance metric. In Lloyd's algorithm, the center $\mu_C$ of the current cluster includes the contribution of the query point $x_j$. This creates a negative correlation between $x_j$ and $\mu_C$. Consequently, the squared distance $\|x_j - \mu_C\|^2$ is stochastically dominated by a distribution with smaller variance than the distance to the other cluster. The "signal" from the true cluster means is overwhelmed by this variance reduction effect in high-noise settings. Using a "leave-one-out" strategy for calculating cluster centers would break this trapping mechanism.

### Mechanism 3: Proliferation of Fixed Points
In the identified regime, the k-means algorithm effectively has almost $2^n$ fixed points (almost every partition), causing it to terminate immediately at initialization. Because the probability of any single point switching is minuscule, the union bound over all $n$ points still results in a negligible probability that *any* point switches. The algorithm converges in 0 or 1 iteration. The initialization quality becomes irrelevant because the algorithm cannot escape the local basin of the initial partition. Using strong initialization or dimensionality reduction preprocessors alters the geometry so the fixed-point regime is avoided.

## Foundational Learning

- **Concept:** First-Order Stochastic Dominance (FSD)
  - **Why needed here:** The proofs rely on comparing random variables (distances) not by exact values, but by cumulative distribution functions. FSD allows the authors to use simplified $\chi^2$ distributions as "worst-case" bounds for the actual complex distance distributions.
  - **Quick check question:** If random variable $Y$ dominates $X$ ($Y \succeq_{FSD} X$), is $P(Y \ge t)$ greater than or less than $P(X \ge t)$ for all $t$? (Answer: Greater).

- **Concept:** Lloyd's Algorithm (k-means)
  - **Why needed here:** Understanding the iterative loop (Assignment $\to$ Update) is critical to seeing why a "fixed point" (where assignments don't change) terminates the algorithm.
  - **Quick check question:** In the Assignment step, does the algorithm update the cluster centers based on current assignments, or assign points based on current centers? (Answer: Assign points based on current centers).

- **Concept:** The "Curse of Dimensionality" in Distance Metrics
  - **Why needed here:** The paper exploits the fact that in high dimensions, the variance of distance metrics (relative to the mean difference) can behave counter-intuitively, specifically how $\chi^2_d$ variables concentrate.
  - **Quick check question:** As dimension $d \to \infty$, does the ratio of the standard deviation to the mean of a $\chi^2_d$ distribution increase or decrease? (Answer: It decreases like $1/\sqrt{d}$, creating "concentration").

## Architecture Onboarding

- **Component map:** Data Generator -> Initializer -> Lloyd Engine -> Evaluator
- **Critical path:** 1. Set $\sigma$ high and $d$ high. 2. Initialize with a "wrong" partition. 3. Run Lloyd step. 4. Observe $\Delta$ assignments $\approx 0$ (Failure).
- **Design tradeoffs:** Naive k-means vs. PCA+k-means - the paper proves naive k-means fails in high-$d$/high-noise. Architectures must implement PCA projection to recover signal, trading off exact Euclidean distance preservation for stability. Initialization Complexity - k-means++ helps, but the theoretical fixed-point trap suggests that in extreme regimes, even smart initialization might just be selecting a "random" fixed point.
- **Failure signatures:** Zero Iteration Convergence - the algorithm reports convergence after 0 or 1 steps. NMI Drop with Dimension - adding features (increasing $d$) causes accuracy to drop. Initialization Dependence - the output is perfectly correlated with the initialization partition.
- **First 3 experiments:**
  1. Verify the Bound: Reproduce Figure 1. Fix $n=40$, vary $d$ and $\sigma$. Plot the empirical probability of switching against the theoretical bound $\rho^{d/4}$ to confirm the theoretical "trap."
  2. Ablate Self-Inclusion: Modify the distance calculation to use "Leave-One-Out" centers. Check if this prevents the fixed-point stagnation described in Remark D.4.
  3. Dimensionality Reduction Test: Reproduce Figure 3. Compare Naive k-means vs. PCA+k-means as $d$ increases to validate that the architecture requires preprocessing to function in the high-noise regime.

## Open Questions the Paper Calls Out

### Open Question 1
Does the "catastrophic failure" of k-means extend to the masked-Gaussian Mixture Model (masked-GMM), where spectral initialization methods are not easily applicable? The current theoretical analysis is restricted to a standard isotropic GMM, while the masked-GMM involves observation-specific operators that complicate standard dimensionality reduction preprocessing.

### Open Question 2
How does the fixed-point phenomenon influence the convergence of k-means in "modest dimensions" or lower noise regimes where not all partitions are necessarily fixed points? The paper focuses on a regime where the dimension $d$ is sufficiently large to guarantee failure with high probability, leaving the behavior in the transitionary regime (lower $d$) uncharacterized.

### Open Question 3
Do iterative algorithms like Expectation Maximization (EM) exhibit similar convergence failures in high-dimensional, high-noise regimes compared to Lloyd's k-means? The paper provides a theoretical explanation specifically for Lloyd's algorithm, and it remains unverified if the mechanism applies to the soft assignments used in EM.

## Limitations
- The analysis assumes an idealized isotropic Gaussian Mixture Model with exactly two balanced clusters, which may not generalize to real-world data with non-spherical clusters or more than two clusters.
- The paper does not provide explicit thresholds for when the fixed-point phenomenon begins, only asymptotic bounds that depend on noise level and sample size.
- The theoretical analysis focuses on the exact Lloyd's algorithm, while practical implementations often use modifications like mini-batch updates or approximate nearest neighbors that could potentially escape the theoretical fixed-point trap.

## Confidence
- **High Confidence:** The exponential decay bound $\rho^{d/4}$ for switching probability; the phenomenon of k-means failing in high-dimensional, high-noise regimes with limited samples; the superiority of PCA-preprocessed k-means over naive k-means in these regimes.
- **Medium Confidence:** The characterization of almost every partition as a fixed point; the practical implications for cryo-EM applications.
- **Low Confidence:** The exact mechanism of self-inclusion bias; the generalizability to non-Gaussian or non-isotropic cluster structures.

## Next Checks
1. **Dimension Threshold Mapping:** Replicate Figure 1 with finer-grained noise levels to identify the minimum dimension $d$ at which the switching probability drops below 1% for each noise level, creating a practical "safe zone" map for k-means applicability.
2. **Self-Inclusion Ablation:** Implement a modified k-means where cluster centers are computed using "leave-one-out" strategy. Compare the switching probability and final NMI against standard k-means across varying dimensions and noise levels to isolate the self-inclusion effect.
3. **Beyond Two Clusters:** Extend the analysis to three and four Gaussian clusters with varying separation distances. Measure whether the fixed-point phenomenon persists and how the critical dimension thresholds scale with cluster number and separation.