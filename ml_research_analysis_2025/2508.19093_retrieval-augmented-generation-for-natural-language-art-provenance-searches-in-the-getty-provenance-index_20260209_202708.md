---
ver: rpa2
title: Retrieval-Augmented Generation for Natural Language Art Provenance Searches
  in the Getty Provenance Index
arxiv_id: '2508.19093'
source_url: https://arxiv.org/abs/2508.19093
tags:
- provenance
- queries
- retrieval
- research
- search
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study presents a Retrieval-Augmented Generation (RAG) framework
  for art provenance research, addressing challenges in fragmented, multilingual archival
  data within the Getty Provenance Index. The method enables natural language and
  multilingual queries through semantic retrieval and contextual summarization, reducing
  dependence on metadata structures.
---

# Retrieval-Augmented Generation for Natural Language Art Provenance Searches in the Getty Provenance Index

## Quick Facts
- **arXiv ID:** 2508.19093
- **Source URL:** https://arxiv.org/abs/2508.19093
- **Authors:** Mathew Henrickson
- **Reference count:** 40
- **Primary result:** RAG framework enables natural language and multilingual provenance searches over fragmented archival data, achieving 85.2% recall on specific queries with strong relevance scores.

## Executive Summary
This study presents a Retrieval-Augmented Generation (RAG) framework for art provenance research using the Getty Provenance Index (GPI), addressing challenges of fragmented, multilingual archival data. The method enables natural language and multilingual queries through semantic retrieval and contextual summarization, reducing dependence on metadata structures. Evaluated using a 10,000-record sample and expanded to 100,000 records, the system showed robust performance on specific queries with recall rates of 85.2% and manual evaluation scores of 2.88/3. Multilingual and out-of-scope queries achieved perfect recall. The best configuration combined semantic retrieval, metadata filtering, and reranking, demonstrating improved precision and relevance. Domain expert feedback confirmed practical value for provenance research.

## Method Summary
The framework embeds enriched auction catalogue text (combining raw text with key metadata fields) into vectors using text-embedding-3-large, stores them in a FAISS index, and retrieves top candidates semantically. Optional metadata filtering narrows the search space, and a cross-encoder reranker (bge-reranker-v2-m3) refines the top results. GPT-4o generates a summarized response with inline citations to original sources. The system was evaluated on a 100-query test bank (specific, vague, multilingual, out-of-scope) using recall, precision, NDCG, and manual relevance scoring.

## Key Results
- Achieved 85.2% recall on specific queries using semantic retrieval alone
- Metadata filtering improved precision and NDCG for vague queries by narrowing the search space
- Reranking stabilized relevance ordering and improved top-10 precision for specific queries
- Multilingual and out-of-scope queries achieved perfect recall scores
- Best configuration (semantic + metadata + reranking) showed highest NDCG but lower recall due to candidate pool restrictions

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Embedding structured metadata into unstructured text records appears to improve semantic retrieval effectiveness for domain-specific archives.
- **Mechanism:** The authors concatenated raw auction catalogue text with key metadata fields (e.g., artist, date, auction house) before generating vector embeddings. This "text augmentation" allows the embedding model to capture semantic relationships between descriptive content and structured attributes, enabling the system to respond to natural language queries that imply specific metadata constraints without requiring formal query syntax.
- **Core assumption:** The embedding model (`text-embedding-3-large`) can successfully encode the semantic relationship between a query and the combined text-metadata block.
- **Evidence anchors:** [Section 2] "By integrating metadata into the textual content, we added essential information... while maintaining a single input record for the model." [Abstract] "Method enables natural-language... searches through semantic retrieval... reducing dependence on metadata structures."
- **Corpus:** General RAG literature (e.g., RAG-Stack) supports the co-optimization of retrieval quality and performance, but specific evidence for text augmentation in cultural heritage is primarily demonstrated in this paper's specific dataset.
- **Break condition:** Performance may degrade if the metadata is sparse, erroneous, or if the embedding model fails to weight the metadata signals appropriately against the dominant descriptive text.

### Mechanism 2
- **Claim:** Applying metadata filtering prior to semantic retrieval likely enhances precision and ranking quality (NDCG) for vague or exploratory queries.
- **Mechanism:** By restricting the search space (e.g., filtering to a specific object type or auction house) before vector search, the system reduces noise in the candidate pool. This allows the semantic search to identify motif-level or descriptive overlaps within a more relevant subset of records, rather than searching the entire 100k record index.
- **Core assumption:** The user's intent aligns with the available metadata fields, and the relevant records possess the metadata required to pass the filter.
- **Evidence anchors:** [Section 3.1.2] "Metadata filtering also increased precision by narrowing the search space... enable it to pick up motif-level or descriptive overlaps that would otherwise be obscured." [Figure 6] Shows improved Precision/NDCG for vague queries when using S + Meta configuration.
- **Corpus:** Weak direct support in provided corpus; "RAG-Stack" discusses quality co-optimization generally but doesn't validate this specific pre-filtering mechanism for vague queries.
- **Break condition:** If a relevant record lacks the specific metadata field being filtered on (e.g., missing "object type"), it will be excluded, reducing recall.

### Mechanism 3
- **Claim:** Adding a reranking stage after initial retrieval seems to significantly improve relevance ordering for specific queries.
- **Mechanism:** An initial "coarse" semantic search retrieves a top-k set (e.g., 100 records). A cross-encoder reranker model (`bge-reranker-v2-m3`) then scores the query against these candidates more deeply. This re-orders the list to prioritize the most semantically aligned records at the very top (top-10), which is critical for the subsequent LLM summarization step.
- **Core assumption:** The correct records are present in the initial top-k retrieval; the reranker can only reorder, not find missing documents.
- **Evidence anchors:** [Section 3.1.2] "The combination of semantic retrieval, metadata filtering, and reranking (S + Meta + R) provides... the highest NDCG." [Section 3.1.3] Notes that reranking "stabilizes the final ordering" for specific queries like the Liebermann charcoal drawings.
- **Corpus:** Neighbor paper "Incorporating Q&A Nuggets into RAG" supports post-retrieval refinement mechanisms to improve generation quality.
- **Break condition:** If the initial retrieval fails to surface relevant documents within the top-k window (low recall), reranking cannot recover them and may even amplify irrelevant results if the model is misaligned with the domain.

## Foundational Learning

- **Concept: Vector Embeddings**
  - **Why needed here:** The system relies on converting text and metadata into high-dimensional vectors (3072 dimensions) to perform "semantic search" rather than keyword matching. Understanding that similar concepts map to nearby vectors is crucial.
  - **Quick check question:** If you search for "portrayal of a man," would the system find a record labeled "Mannerbildnis" (male portrait)? (Answer: Yes, if the embeddings are multilingual and semantically aligned).

- **Concept: Recall vs. Precision**
  - **Why needed here:** The evaluation explicitly trades off these metrics. Metadata filtering improves precision but lowers recall. Understanding this tension is necessary to interpret the results (Figures 4-6) and configure the system for different research needs.
  - **Quick check question:** If a researcher wants to find *every* possible record related to a looted artwork, should they prioritize the high-precision (S + Meta + R) or the high-recall (Semantic only) configuration? (Answer: High-recall, to ensure no evidence is missed).

- **Concept: RAG (Retrieval-Augmented Generation)**
  - **Why needed here:** This is the core architecture. One must understand that the LLM (GPT-4o) does not "know" the GPI data; it is provided with a specific "context" (retrieved records) and instructed to summarize only that information.
  - **Quick check question:** Why does the system include a URL to the original scanned catalogue in the final response? (Answer: To ensure transparency and allow the human expert to verify the source, mitigating "black box" risks).

## Architecture Onboarding

- **Component map:** Natural Language Query -> Embedding (text-embedding-3-large) -> FAISS Index -> [Optional] Metadata Filter -> Reranking (bge-reranker-v2-m3) -> GPT-4o (LLM) -> Summary + Source URLs
- **Critical path:** The **Text Augmentation** phase (Section 2) is the most critical data preparation step. If metadata (Sale Date, Artist) is not correctly concatenated into the text block, the semantic search cannot "see" it, and the metadata filtering will have nothing to act upon.
- **Design tradeoffs:** The authors chose a "Naive-to-Advanced" RAG approach rather than complex architectures (Self-RAG) to ensure "interpretability" for art historians. The prototype relies on OpenAI embeddings/LLMs (closed) but uses FAISS and BGE-Reranker (open). The authors note this as a limitation for full transparency.
- **Failure signatures:** Vague Query Drift: For vague queries (e.g., "dynamic scenes"), the system retrieves thematically related but medium-incorrect results (e.g., finding a painting when a drawing was requested) [Section 3.1.3]. Recall Drop: When using the S + Meta + R configuration, recall drops significantly because the pipeline restricts the candidate pool twice (once by filter, once by reranking top-k) [Figure 4].
- **First 3 experiments:** 1. Baseline Retrieval: Index the raw text *without* metadata enrichment. Query for "Paintings by Dix sold at Fischer." Compare recall against the enriched index to validate Mechanism 1. 2. Filter Ablation: Run a set of "Vague" queries using the S + Meta configuration. Remove specific filters one by one to measure the sensitivity of precision to specific metadata fields. 3. Reranking Window: Vary the top-k value passed to the reranker (e.g., top 20 vs. top 100) to find the optimal balance between computational cost and recall recovery for the S + R configuration.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How does the RAG framework perform when replacing closed-source components with fully open-source embedding and generative models?
- **Basis in paper:** [explicit] Section 1.2.4 identifies extending the framework to fully open models as a "current priority," and Section 1.2.3 notes that cross-model benchmarking is a "future research direction."
- **Why unresolved:** The prototype relied exclusively on OpenAI's closed-source services (text-embedding-3-large, GPT-4o) to maintain a focused scope and avoid the variability introduced by cross-provider interfaces and guardrails.
- **What evidence would resolve it:** Benchmarking the current pipeline against open-source equivalents (e.g., Llama, BERT-based embeddings) using the same query bank and evaluation metrics (Recall, Precision, NDCG).

### Open Question 2
- **Question:** Does fine-tuning embedding models specifically on historical auction catalogue text improve retrieval accuracy for vague or semantically weak queries?
- **Basis in paper:** [explicit] Section 4.1.1 proposes "evaluating the fine tuning of embedding models to the auction text" to address the identified limitations in semantic capture for vague queries.
- **Why unresolved:** The current system used general-purpose off-the-shelf embeddings, which showed lower recall (64.3%) and precision for queries lacking specific metadata or artist names compared to specific queries.
- **What evidence would resolve it:** A comparative evaluation of domain-adapted embeddings against the baseline model, specifically measuring performance gains on the "vague" query category defined in the study.

### Open Question 3
- **Question:** Does integrating external art market datasets and trade literature enhance the context and retrieval of cross-referenced provenance data?
- **Basis in paper:** [explicit] Section 4.1.1 suggests the framework "could be expanded to incorporate other art market datasets," such as University of Heidelberg archives or contemporary trade journals.
- **Why unresolved:** The evaluation was limited to the Getty Provenance Index German Sales data and did not test the system's ability to synthesize information from disparate archival sources.
- **What evidence would resolve it:** A retrieval evaluation measuring the system's ability to successfully answer complex queries requiring the synthesis of GPI records and external digitized literature.

## Limitations
- Performance metrics depend on an unpublished 100-query test set with subjective manual relevance scoring
- Text augmentation mechanism assumes embedding model appropriately weights structured metadata without explicit control
- Results demonstrated on 100k-record subset but scalability to full 1.6M record index and other datasets remains untested

## Confidence
- **High confidence**: Retrieval pipeline architecture and basic functionality
- **Medium confidence**: Performance metrics on specific query types (depends on unpublished test set)
- **Medium confidence**: Effectiveness of text augmentation and metadata filtering mechanisms (supported by ablation but lacking independent validation)

## Next Checks
1. **Ablation study on text augmentation**: Run the pipeline with raw text (no metadata concatenation) versus enriched text to quantify the exact contribution of metadata integration to recall and precision improvements
2. **Metadata filtering sensitivity analysis**: Systematically remove individual metadata filters (e.g., object type, artist) to determine which fields most significantly impact precision for vague queries versus recall for specific queries
3. **Out-of-distribution testing**: Evaluate the system on provenance queries from other Getty databases (e.g., archival collections) or other cultural heritage institutions to assess generalizability beyond the German Sales dataset