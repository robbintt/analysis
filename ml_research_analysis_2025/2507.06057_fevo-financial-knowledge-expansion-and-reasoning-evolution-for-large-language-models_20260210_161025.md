---
ver: rpa2
title: 'FEVO: Financial Knowledge Expansion and Reasoning Evolution for Large Language
  Models'
arxiv_id: '2507.06057'
source_url: https://arxiv.org/abs/2507.06057
tags:
- financial
- reasoning
- training
- loss
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: FEVO introduces a multi-stage post-training framework to enhance
  LLM performance in financial tasks by expanding domain knowledge via continued pretraining,
  instilling structured reasoning through supervised fine-tuning, and integrating
  both via reinforcement learning. To ensure high-quality training data, FEVO employs
  fine-grained filtering pipelines and converts fixed-choice questions into open-ended
  ones to mitigate reward-hacking.
---

# FEVO: Financial Knowledge Expansion and Reasoning Evolution for Large Language Models

## Quick Facts
- arXiv ID: 2507.06057
- Source URL: https://arxiv.org/abs/2507.06057
- Reference count: 25
- Key outcome: Multi-stage post-training framework (CPT → SFT → RL) achieves state-of-the-art performance on five financial tasks against larger and specialist models

## Executive Summary
FEVO introduces a systematic approach to enhance large language models for financial domain tasks through a three-stage post-training pipeline. The framework first expands domain knowledge via continued pretraining on curated financial corpora, then instills structured reasoning patterns through supervised fine-tuning on high-quality Chain-of-Thought traces, and finally integrates both capabilities using reinforcement learning with anti-reward-hacking mechanisms. By converting multiple-choice questions to open-ended format and implementing balanced batching, FEVO ensures the model develops genuine financial comprehension rather than exploiting superficial patterns.

## Method Summary
FEVO trains from Qwen2.5-32B through three sequential stages: (1) Continued Pretraining on ~188M tokens of financial and general domain data, (2) Supervised Fine-Tuning on 291K structured Chain-of-Thought traces distilled from frontier models with multi-stage filtering, and (3) Reinforcement Learning using modified VAPO with balanced batching and open-ended question conversion to prevent reward hacking. The framework employs rule-based and model-based rewards to guide learning while maintaining informative gradients through careful sample selection.

## Key Results
- FEVO-R32B significantly outperforms FEVO-R32B-0 (RL-only trained from Qwen2.5-32B-Instruct), validating the effectiveness of financial knowledge expansion and structured reasoning distillation
- State-of-the-art performance on five of seven financial benchmarks against larger and specialist models
- Demonstrates that structured reasoning with self-correction (reflection and backtracking phases) enhances financial reasoning capabilities
- Open-ended conversion effectively mitigates reward hacking while preserving model performance

## Why This Works (Mechanism)

### Mechanism 1: Domain Knowledge Expansion via Continued Pre-Training
Injecting domain-specific knowledge before reasoning training improves financial task performance by expanding the model's keyword expansion capability for professional terminology. CPT on curated financial corpus (CPA textbooks, exam questions, industry text) enables more accurate recall of relevant concepts during task-solving. Mixing with general domain data preserves out-of-domain capabilities.

### Mechanism 2: Structured Reasoning Distillation via SFT
Distilling structured Chain-of-Thought patterns from frontier models creates more robust reasoning behaviors. SFT on high-quality CoT traces (Plan → Reasoning → Reflection → Backtracking → Answer) teaches the model to self-validate and correct errors mid-reasoning. Multi-stage filtering ensures trace quality and prevents noise propagation.

### Mechanism 3: Knowledge-Reasoning Integration via RL with Anti-Reward-Hacking Design
RL integrates CPT knowledge with SFT reasoning patterns when reward signal accurately measures comprehension. Converting multiple-choice questions to open-ended format dramatically reduces guessing probability, forcing the model to leverage both domain knowledge and reasoning. Modified VAPO with Balanced Batching maintains informative gradients.

## Foundational Learning

- **Chain-of-Thought Reasoning with Self-Correction**: FEVO's SFT stage distills structured CoT with explicit reflection and backtracking phases; understanding this paradigm is essential for interpreting model behavior. Quick check: Can you explain why a model might benefit from generating a "reflection" phase before finalizing its answer?

- **Reward Hacking in RL**: The paper identifies reward hacking as a critical failure mode in financial QA RL; understanding this informs the open-ended conversion design choice. Quick check: Why does a multiple-choice format increase reward hacking risk compared to open-ended questions?

- **Value-Augmented PPO (VAPO)**: FEVO uses modified VAPO for RL training; understanding the base algorithm is necessary before comprehending the Balanced Batching modification. Quick check: How does VAPO differ from standard PPO in its loss formulation?

## Architecture Onboarding

- **Component map**: CPT Module → SFT Module → RL Module
- **Critical path**: CPT → SFT → RL (sequential dependencies; each stage builds on the previous). Skipping CPT or SFT (as in FEVO-R32B-0) shows degraded performance.
- **Design tradeoffs**: General data ratio in CPT (too much → dilutes financial knowledge; too little → general capability degradation); SFT filtering strictness (aggressive → higher quality but smaller dataset; lenient → noise propagation); Balanced Batching thresholds (controls difficulty distribution; too many hard questions → training instability)
- **Failure signatures**: Temperature=0 for reasoning models → reasoning loops, repetitive outputs; Excessive hard questions in RL batch → performance degradation; Multiple-choice RL data without conversion → reward hacking, inflated rewards without comprehension
- **First 3 experiments**: 1) Replicate CPT stage on Qwen2.5-32B with 10% of FEVO-Train-C; verify financial benchmark improvement vs. base model on FinanceIQ subset. 2) Generate structured CoT traces using DeepSeek-R1 API on 100 CFLUE samples; manually inspect reflection and backtracking quality. 3) Implement open-ended conversion on 50 multiple-choice questions; verify rule-based reward accuracy against manual annotation.

## Open Questions the Paper Calls Out

### Open Question 1
Can more finely-grained reward models (beyond rule-based accuracy/formatting rewards) better guide model self-evolution during RL training for financial reasoning? Current reward mechanism uses only binary accuracy and format checks with limited model-based verification for edge cases; intermediate reasoning quality is not explicitly rewarded.

### Open Question 2
Can FEVO's framework extend to financial tasks requiring structured outputs (e.g., generating charts, tables, or legally-compliant documents)? Current benchmarks and training data focus on text-based QA, not multi-modal or document generation tasks.

### Open Question 3
What is the optimal composition of question difficulty in RL training batches for financial reasoning? Authors tested two approaches but did not perform hyperparameter search; the balance between hard and easy questions remains heuristic.

## Limitations

- Multi-stage training pipeline requires substantial computational resources across CPT, SFT, and RL stages
- Reliance on in-house datasets (Acc Learn, Fin CPA) creates reproducibility challenges due to lack of public availability
- Open-ended question conversion lacks empirical validation demonstrating effectiveness compared to alternative solutions
- Performance improvements not isolated through ablation studies to determine essential components

## Confidence

- **High confidence**: Overall framework design and sequential CPT → SFT → RL approach following established best practices
- **Medium confidence**: Specific hyperparameter choices and data filtering criteria optimized for this particular model and task distribution
- **Low confidence**: Absolute performance claims due to comparisons against models with different sizes and training regimes without controlled ablations

## Next Checks

1. Conduct an ablation study training FEVO-R32B variants that skip individual stages (CPT-only, SFT-only, RL-only) to quantify each component's contribution to final performance gains.

2. Implement the open-ended conversion pipeline on a held-out set of multiple-choice questions and measure the reduction in reward hacking behavior through controlled experiments comparing MC vs. open-ended formats.

3. Replicate the SFT stage using publicly available CoT traces from DeepSeek-R1 on a subset of CFLUE questions, then evaluate the resulting model on the same benchmark to verify the knowledge transfer claims.