---
ver: rpa2
title: 'GUI-G1: Understanding R1-Zero-Like Training for Visual Grounding in GUI Agents'
arxiv_id: '2505.15810'
source_url: https://arxiv.org/abs/2505.15810
tags:
- grounding
- arxiv
- training
- reward
- agents
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper investigates the R1-Zero-like training paradigm for
  GUI agents, focusing on visual grounding tasks. The authors identify three key challenges:
  excessive reasoning harms grounding accuracy, reward hacking occurs due to box size
  bias in reward functions, and GRPO introduces length and difficulty biases.'
---

# GUI-G1: Understanding R1-Zero-Like Training for Visual Grounding in GUI Agents

## Quick Facts
- arXiv ID: 2505.15810
- Source URL: https://arxiv.org/abs/2505.15810
- Reference count: 40
- Trained on only 17K samples, GUI-G1-3B achieves 90.3% accuracy on ScreenSpot and 37.1% on ScreenSpot-Pro

## Executive Summary
This paper investigates R1-Zero-like training for GUI agents focusing on visual grounding tasks. The authors identify three key challenges: excessive reasoning harms grounding accuracy, reward hacking occurs due to box size bias in reward functions, and GRPO introduces length and difficulty biases. To address these, they propose three solutions: a Fast Thinking Template that eliminates reasoning, a box size-constrained reward function to mitigate hacking, and a difficulty-aware GRPO objective that removes length normalization. Their GUI-G1-3B model trained on only 17K samples achieves state-of-the-art performance, scoring 90.3% on ScreenSpot and 37.1% on ScreenSpot-Pro, surpassing larger models while using fewer tokens and training stages.

## Method Summary
GUI-G1 employs a three-pronged approach to improve visual grounding in GUI agents. First, it uses a Fast Thinking Template that eliminates chain-of-thought reasoning, reducing output tokens from ~107 to ~37 per example and improving accuracy by forcing direct answer generation. Second, it implements a box size-constrained reward function (R = RHit + 0.25·RIoU + 0.125·RBox) to prevent reward hacking where models exploit single-metric optimization. Third, it modifies GRPO by removing length normalization (replacing |oi| with Max_Tokens) and adding difficulty weighting based on inverse relative box size. The model is trained on 17K samples from UI-BERT and OS-Atlas datasets using Qwen2.5-VL-3B-Instruct as the backbone, achieving strong performance with minimal training data.

## Key Results
- GUI-G1-3B achieves 90.3% accuracy on ScreenSpot and 37.1% on ScreenSpot-Pro
- Surpasses larger models like UI-TARS-3B and Qwen2.5-VL-7B-Instruct
- Uses only 17K training samples and 1 training epoch
- Maintains accuracy across different target types (text vs icons)

## Why This Works (Mechanism)

### Mechanism 1: Fast Thinking Template Eliminates Counterproductive Reasoning
- **Claim:** Removing explicit chain-of-thought reasoning during training improves grounding accuracy because grounding relies more on visual pattern recognition than verbal reasoning.
- **Mechanism:** The paper observes that longer reasoning chains degrade grounding accuracy (Figure 2). Higher text ratios (reasoning tokens relative to image tokens) correlate with worse performance. The Fast Thinking Template forces direct answer generation, reducing token count from ~107 to ~37 per example, which preserves visual processing capacity.
- **Core assumption:** Visual grounding in GUIs is primarily a "System 1" fast recognition task rather than a deliberative reasoning task; the base model already possesses sufficient grounding knowledge from pre-training.
- **Evidence anchors:**
  - [abstract]: "Current templates encourage the model to generate chain-of-thought reasoning, but longer chains unexpectedly lead to worse grounding performance."
  - [section 3.1, Figure 2]: Shows accuracy drops as output tokens increase; higher text ratio correlates with lower accuracy.
  - [corpus]: Limited direct corpus evidence—neighbor papers focus on other grounding improvements; no contradictory findings.
- **Break condition:** If grounding requires multi-step semantic reasoning (e.g., "click the button that appears after submitting the form"), or if base model lacks pre-trained grounding knowledge.

### Mechanism 2: Box Size-Constrained Reward Mitigates Opposing Hacking Behaviors
- **Claim:** Combining RHit + αRIoU + βRBox prevents reward hacking where models over-optimize one metric at the expense of another.
- **Mechanism:** GRPO's sample selection amplifies inherent reward biases: RHit encourages smaller boxes (higher center-point accuracy), while RIoU encourages larger boxes (higher overlap). RBox constrains predicted box size to match ground truth, acting as a regularizer. Final reward: R = RHit + 0.25·RIoU + 0.125·RBox.
- **Core assumption:** Ground truth box sizes are well-calibrated; size mismatch is the primary cause of reward hacking.
- **Evidence anchors:**
  - [abstract]: "Reward functions based on hit signals or box area allow models to exploit box size, leading to reward hacking and poor localization quality."
  - [section 3.2, Figure 3 & 4]: Shows accuracy increases but IoU drops under RHit alone; opposite trend under RIoU; RBox addition stabilizes both.
  - [corpus]: No direct corpus evidence on this specific reward hacking mechanism.
- **Break condition:** If ground truth annotations have inconsistent box sizes; if RBox is used alone (paper notes it fails because it "assigns non-zero rewards even to poorly grounded predictions").

### Mechanism 3: Difficulty-Aware GRPO Enables Hard-Sample Optimization
- **Claim:** Removing length normalization and adding difficulty weighting shifts gradient signal toward harder samples.
- **Mechanism:** Original GRPO divides by response length |o_i|, causing: (1) shorter correct responses to receive amplified gradients, (2) longer incorrect responses to be encouraged. Since longer outputs harm grounding, this is doubly problematic. Solution: replace |o_i| with constant Max_Tokens, multiply objective by difficulty weight w_q based on inverse relative box size (smaller boxes = harder = higher weight).
- **Core assumption:** Smaller target boxes are a valid proxy for task difficulty; length normalization is inherently harmful for grounding.
- **Evidence anchors:**
  - [abstract]: "Online RL tends to overfit easy examples due to biases in length and sample difficulty, leading to under-optimization on harder cases."
  - [section 3.3, Figure 6]: Shows incorrect responses lengthening over training under standard GRPO; our method maintains lower extreme-sample ratios on hard cases.
  - [Table 2]: Shows incremental gains from length debiasing (+0.9 avg) and difficulty reweighting (+1.0 avg).
  - [corpus]: No direct corpus evidence on GRPO biases in GUI grounding.
- **Break condition:** If box size is a poor difficulty proxy (e.g., small but distinctive icons); if removing length normalization causes training instability.

## Foundational Learning

- **Concept: GRPO (Group Relative Policy Optimization)**
  - **Why needed:** The paper modifies GRPO as its base RL algorithm; understanding advantage normalization is prerequisite.
  - **Quick check:** Why does GRPO normalize rewards within sampled groups rather than using absolute rewards? (Answer: Reduces variance by computing relative advantages.)

- **Concept: Reward Hacking in RL**
  - **Why needed:** Core failure mode addressed; models exploit reward function loopholes rather than learning intended behavior.
  - **Quick check:** Why would optimizing IoU alone lead to oversized boxes? (Answer: Larger boxes increase overlap probability regardless of precision.)

- **Concept: Visual Grounding Task Formulation**
  - **Why needed:** Understanding how bounding box prediction is cast as language generation (JSON output) clarifies the entire pipeline.
  - **Quick check:** How does GUI grounding differ from standard object detection? (Answer: Grounded by natural language description, not predefined categories; requires cross-modal alignment.)

## Architecture Onboarding

- **Component map:** Screenshot + instruction -> Fast Thinking Template -> Qwen2.5-VL-3B-Instruct -> JSON bounding box [x1, y1, x2, y2]
- **Critical path:**
  1. Data filtering (discard samples with 8/8 consistent rollouts)
  2. Template application (Fast Thinking)
  3. Model rollout (8 candidate responses per sample)
  4. Reward computation (all three components)
  5. Difficulty weight calculation (from relative box size)
  6. Policy update (difficulty-weighted GRPO objective)
- **Design tradeoffs:**
  - **α/β coefficients:** Higher α emphasizes overlap quality; higher β constrains size more aggressively but risks optimizing uninformative samples.
  - **Difficulty weight range:** Paper uses (0.5, 1.5]; wider range more aggressively upweights hard samples but may destabilize training.
  - **Training data scale:** 17K samples is minimal; larger datasets may change optimal hyperparameters.
- **Failure signatures:**
  - Box size drifting monotonically during training → reward hacking not fully mitigated
  - Incorrect responses lengthening over iterations → length bias resurging
  - Performance gap widening between small-box and large-box targets → difficulty weighting ineffective
  - Model generating malformed JSON → reward signal too weak or conflicting
- **First 3 experiments:**
  1. **Template ablation:** Compare Fast vs Slow thinking on held-out validation, tracking accuracy by target type (text vs icon) and token count.
  2. **Reward component sweep:** Grid search α ∈ {0.1, 0.25, 0.5} and β ∈ {0.05, 0.125, 0.25}, monitoring box size trends and final accuracy/IoU balance.
  3. **Difficulty weighting validation:** Train with and without w_q, evaluate performance stratified by relative box size quartiles to confirm hard-sample improvement.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the "Fast Thinking" paradigm and difficulty-aware RL objectives be effectively transferred to complex, long-horizon GUI tasks like action prediction?
- **Basis in paper:** [explicit] The Limitations section states the work focuses on grounding and does not cover "action prediction or long-horizon planning," suggesting future research should extend this approach.
- **Why unresolved:** The paper demonstrates that removing reasoning aids grounding (a "System-1" task), but it is unclear if this detrimentally affects "System-2" tasks required for multi-step planning.
- **What evidence would resolve it:** Evaluating GUI-G1's training modifications on benchmarks requiring multi-step action sequences, such as AndroidWorld or AndroidControl.

### Open Question 2
- **Question:** How does the GUI-G1 training pipeline scale with datasets significantly larger than 17K samples?
- **Basis in paper:** [explicit] The authors note their model is trained on a "relatively small set of public datasets," which "constrains its performance ceiling," and explicitly plan to "scale up training" in future work.
- **Why unresolved:** It is unknown if the specific reward shaping and difficulty weighting remain stable and beneficial when the model is exposed to the data diversity and scale used by larger models like UI-TARS.
- **What evidence would resolve it:** Training experiments using datasets with 100K+ samples (similar to Aguivs or OS-Atlas) and analyzing performance deltas on ScreenSpot-Pro.

### Open Question 3
- **Question:** Is relative box size a universally reliable proxy for grounding difficulty across diverse UI layouts?
- **Basis in paper:** [inferred] Section 3.3 uses relative box size to calculate difficulty weights ($w_q$), assuming smaller boxes are harder.
- **Why unresolved:** While intuitive, this proxy ignores semantic ambiguity; a large box in a cluttered area might be harder to locate than a small, distinct icon.
- **What evidence would resolve it:** An ablation study comparing difficulty weights derived from box size versus weights derived from model uncertainty or semantic density.

## Limitations

- Training dataset size (17K samples) is relatively small, raising questions about generalization to more diverse GUI environments
- Difficulty weighting mechanism relies on relative box size as a proxy for task difficulty, which may not capture all aspects of grounding complexity
- Paper does not address potential distribution shifts between training and evaluation datasets

## Confidence

- **High confidence**: The Fast Thinking Template's effectiveness (strong empirical evidence from Figure 2 showing accuracy degradation with longer reasoning chains)
- **Medium confidence**: The box size-constrained reward function's ability to mitigate hacking (supported by Figure 3-4 trends, but limited corpus evidence)
- **Medium confidence**: The difficulty-aware GRPO's hard-sample optimization (Table 2 shows gains, but difficulty proxy validity is questionable)
- **Low confidence**: Generalization claims beyond the tested datasets (insufficient cross-dataset validation)

## Next Checks

1. **Cross-dataset generalization test**: Evaluate GUI-G1 on a held-out GUI grounding dataset not seen during training (e.g., open-source mobile app screenshots) to verify claims about dataset efficiency and generalization.
2. **Difficulty proxy validation**: Manually annotate a subset of samples with difficulty ratings independent of box size, then measure correlation between actual difficulty and the proposed relative box size proxy.
3. **Reward component ablation**: Train separate models with only RHit, only RIoU, and only RBox to quantify their individual contributions and confirm that the combined reward function genuinely prevents hacking rather than masking it through averaging.