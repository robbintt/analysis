---
ver: rpa2
title: 'PolyVivid: Vivid Multi-Subject Video Generation with Cross-Modal Interaction
  and Enhancement'
arxiv_id: '2506.07848'
source_url: https://arxiv.org/abs/2506.07848
tags:
- video
- image
- text
- identity
- subject
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: PolyVivid is a multi-subject video customization framework that
  enables flexible, identity-consistent generation with accurate subject interactions.
  It addresses the challenges of precise text-image correspondence, identity preservation,
  and subject interaction in multi-subject video generation.
---

# PolyVivid: Vivid Multi-Subject Video Generation with Cross-Modal Interaction and Enhancement

## Quick Facts
- arXiv ID: 2506.07848
- Source URL: https://arxiv.org/abs/2506.07848
- Reference count: 40
- Primary result: Achieves Face-sim 0.642, DINO-sim 0.623, CLIP-B 0.336, CLIP-L 0.281, FVD 959.74 for multi-subject video customization

## Executive Summary
PolyVivid addresses the challenging problem of multi-subject video customization, enabling the generation of videos from multiple subject images while preserving identity consistency and modeling accurate interactions between subjects. The framework introduces a three-module architecture: a VLLM-based text-image fusion module for precise grounding, a 3D-RoPE-based enhancement module for structured bidirectional fusion, and an attention-inherited identity injection module for consistent identity preservation across video frames. Experiments demonstrate state-of-the-art performance across multiple metrics including identity fidelity, text-video alignment, and video quality.

## Method Summary
PolyVivid is a multi-subject video customization framework built on HunyuanVideo's MM-DiT backbone. It processes subject images and text prompts through a staged pipeline: first, LLaVA encodes structured templates linking subject descriptions to images; second, 3D-RoPE assigns temporal-spatial indices to bind text and image tokens for bidirectional MM-attention fusion; third, attention-inherited cross-attention injects identity features uniformly across all video frames. The system uses progressive training (5K iterations each for single-subject and multi-subject stages) with LoRA adaptation and requires 256 GPUs for training at batch size 256.

## Key Results
- Face similarity (Arcface): 0.642, outperforming baselines by 0.017-0.106
- DINO similarity: 0.623, improving over baselines by 0.020-0.077
- CLIP-B alignment: 0.336, surpassing baselines by 0.008-0.052
- FVD score: 959.74, better than baselines by 4.63-89.94
- Temporal consistency: 0.964, significantly higher than baselines

## Why This Works (Mechanism)

### Mechanism 1: VLLM-based Semantic Grounding for Text-Image Correspondence
The structured multimodal template "A man is playing guitar. <SEP> The man looks like <image 1>. The guitar looks like <image 2>." enables LLaVA to learn precise subject-to-image associations. The <SEP> token acts as a soft delimiter, reducing visual token interference while preserving text-image correlations. LLaVA processes subject images into textual embedding space, producing fused embeddings that capture high-level semantic relationships. Ablation shows this grounding alone improves Face-sim from 0.345 to 0.642 when combined with other modules.

### Mechanism 2: 3D-RoPE Structured Positional Binding for Cross-Modal Fusion
Three token types receive structured 3D-RoPE indices: text tokens at (i, 0, 0) preserving sequential order, <image> tokens at (m₁+1, spatial_coords) for subject-related visual semantics, and 'image' tokens at (m₁+2, same_spatial_coords) for detailed identity features. Related tokens share proximate temporal indices, strengthening attention correlations. The MM-attention module with LoRA-adapted Q/K/V enables bidirectional information flow where zI injects identity into zT and zT injects interaction semantics into zI.

### Mechanism 3: Attention-Inherited Cross-Attention for Identity Preservation Across Frames
Instead of token concatenation (which causes frames distant from condition to receive weaker guidance) or external adapters (which misalign with large feature spaces), this module constructs Cross-Attn using pretrained MM-attention weights with LoRA adaptation for image tokens and zero-initialized FC layer stabilization. All frames receive equal conditioning, mitigating identity drift. Ablation demonstrates LLaVA + Token-Concatenation achieves Face-sim 0.628 but lower CLIP scores (0.328), while full method reaches 0.642 with better text alignment.

## Foundational Learning

- **Concept: Multimodal Diffusion Transformers (MM-DiT)**
  - Why needed: PolyVivid builds on HunyuanVideo's MM-DiT, which has separate Q/K/V projections for text and video streams within each attention block.
  - Quick check: In MM-DiT, how do text and video embeddings interact—is it through concatenation, cross-attention, or a hybrid mechanism?

- **Concept: 3D Rotary Position Embedding (RoPE)**
  - Why needed: The enhancement module relies on 3D-RoPE to bind tokens across temporal (frame sequence) and spatial (pixel grid) dimensions.
  - Quick check: Given a video with 16 frames and 32×32 spatial tokens per frame, how would you assign unique 3D position indices?

- **Concept: LoRA (Low-Rank Adaptation)**
  - Why needed: Multiple modules adapt pretrained weights via LoRA to avoid full fine-tuning while learning task-specific modifications.
  - Quick check: If a weight matrix W is 4096×4096 and LoRA rank is 8, what are the shapes of the decomposition matrices A and B?

## Architecture Onboarding

- **Component map**: Subject images + text prompt -> VAE encoder + LLaVA template -> 3D-RoPE position assignment -> MM-attention fusion -> Attention-inherited cross-attention -> Base MM-DiT video generation

- **Critical path**: Template construction → LLaVA encoding (grounding quality depends on this) → VAE + LLaVA embeddings → 3D-RoPE position assignment → MM-attention fusion → Enhanced tokens → Cross-attention injection → video generation. Key dependency: LLaVA text space must align with HunyuanVideo's text encoder.

- **Design tradeoffs**: Token concatenation is simpler but causes temporal imbalance (later frames degrade); adapter-based injection is flexible but misaligns with high-dimensional feature spaces; attention-inherited reuses proven weights, treats frames uniformly, but requires careful zero-init.

- **Failure signatures**: Identity drift in later frames → Check injection module temporal coverage; wrong subject-action assignment → Inspect LLaVA template and <SEP> placement; subject confusion (e.g., tiger with giraffe shape) → Verify 3D-RoPE indices or data pipeline clique consolidation; blurry identities → LoRA rank may be insufficient, or VAE encoder mismatch.

- **First 3 experiments**:
  1. Template ablation: Remove structured identity prompts, measure grounding accuracy and Face-sim degradation
  2. Position encoding visualization: Plot attention maps with vs. without 3D-RoPE to confirm cross-modal token binding
  3. Injection strategy comparison: Run controlled comparison of token concatenation vs. attention-inherited injection across 100 video samples, measuring Face-sim by frame index

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the framework effectively handle highly complex scenes with numerous subjects (more than three) or interactions requiring detailed physical reasoning, where current performance degrades?
- Basis in paper: The authors explicitly list "difficulties when handling highly complex scenes involving numerous subjects... or fine-grained interactions that require detailed physical reasoning" as a primary limitation in the Appendix.
- Why unresolved: The current Attention-inherited Identity Injection module may suffer from attention dilution or feature entanglement when scaling beyond the demonstrated 2-3 subjects.
- What evidence would resolve it: Quantitative benchmarks (Identity Similarity, FVD) on a newly constructed dataset featuring 4+ distinct subjects interacting simultaneously in a physically plausible manner.

### Open Question 2
- Question: Is the Text-image Interaction 3D-RoPE module portable to other DiT-based video generation architectures, or is it overfitted to the specific latent space of HunyuanVideo?
- Basis in paper: The method relies on a specific MM-Attention structure and VAE from HunyuanVideo. The limitations section notes that generation quality is "constrained by the capabilities of the underlying video generation backbone."
- Why unresolved: The paper does not demonstrate cross-architecture generalizability, leaving open the question of whether the spatial-temporal binding strategy depends on HunyuanVideo's specific pretraining.
- What evidence would resolve it: Successful integration and performance evaluation of the 3D-RoPE enhancement module on an alternative open-source DiT backbone (e.g., CogVideoX or Mochi).

### Open Question 3
- Question: How can the MLLM-based data curation pipeline be improved to robustly handle prolonged occlusion or ambiguous visual cues without relying on the current clique-based filtering threshold?
- Basis in paper: The authors state the pipeline "may still be susceptible to errors in grounding or segmentation, especially in cases of occlusion," which affects the accuracy of subject alignment.
- Why unresolved: The current graph-based clique strategy filters transient subjects but may discard valid subjects if occlusion persists beyond the "one-third of total detected frames" heuristic.
- What evidence would resolve it: A comparative analysis of subject extraction accuracy on a benchmark dataset specifically designed with high-occlusion scenarios, showing improved recall over the current heuristic.

## Limitations

- Complex scenes with numerous subjects (>3) or detailed physical interactions remain challenging due to potential attention dilution and feature entanglement
- The 3D-RoPE position encoding mechanism, while novel, lacks external validation and may not generalize beyond controlled conditions
- Generation quality is fundamentally constrained by the capabilities of the underlying video generation backbone (HunyuanVideo)

## Confidence

- **High confidence**: Face-sim (0.642) and DINO-sim (0.623) improvements over baselines, supported by direct ablation studies in Table 2
- **Medium confidence**: CLIP-B (0.336) and CLIP-L (0.281) improvements, though absolute values suggest moderate alignment quality
- **Medium confidence**: FVD (959.74) and Temporal (0.964) scores indicating good temporal consistency, but comparisons to single-subject methods may be apples-to-oranges
- **Low confidence**: The novel 3D-RoPE cross-modal binding mechanism lacks external validation and may not generalize beyond controlled conditions

## Next Checks

1. **Identity drift verification**: Run frame-by-frame Face-sim analysis across 100 generated videos to quantify identity consistency decay in later frames, comparing attention-inherited injection against token concatenation baselines.

2. **Grounding robustness test**: Evaluate template-based grounding with increasingly ambiguous subject descriptions (e.g., "a person" vs. "the man with glasses and blue shirt") to determine failure thresholds for the VLLM-based fusion module.

3. **Position encoding sensitivity**: Systematically vary the 3D-RoPE temporal and spatial offset parameters to identify optimal binding configurations and test whether the claimed attention proximity directly translates to improved cross-modal fusion.