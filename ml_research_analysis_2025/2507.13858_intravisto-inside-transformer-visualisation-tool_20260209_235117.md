---
ver: rpa2
title: 'InTraVisTo: Inside Transformer Visualisation Tool'
arxiv_id: '2507.13858'
source_url: https://arxiv.org/abs/2507.13858
tags:
- latexit
- information
- flow
- sha1
- base64
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: InTraVisTo is a novel visualization tool designed to enhance the
  interpretability of Transformer-based Large Language Models (LLMs) by providing
  real-time, interactive visualizations of internal model states and information flow.
  It addresses the challenge of understanding the black-box nature of LLMs, which
  often produce unpredictable outputs and errors ("hallucinations").
---

# InTraVisTo: Inside Transformer Visualisation Tool

## Quick Facts
- arXiv ID: 2507.13858
- Source URL: https://arxiv.org/abs/2507.13858
- Reference count: 40
- Novel visualization tool for understanding internal states and information flow in Transformer-based LLMs

## Executive Summary
InTraVisTo is a novel visualization tool designed to enhance the interpretability of Transformer-based Large Language Models (LLMs) by providing real-time, interactive visualizations of internal model states and information flow. It addresses the challenge of understanding the black-box nature of LLMs, which often produce unpredictable outputs and errors ("hallucinations"). The tool decodes hidden state vectors at each layer using either the model's input or output decoder, interpolates between them for better interpretability, and displays the results as annotated heatmaps. It also visualizes the flow of information through the network using Sankey diagrams, highlighting the relative contributions of self-attention and feed-forward components. Additionally, InTraVisTo introduces embedding injection, allowing users to modify internal states and observe the model's behavior. These features collectively enable researchers and practitioners to identify potential flaws, understand token generation processes, and improve LLM reliability without requiring programming expertise. The tool integrates multiple functionalities not available in existing solutions, offering a comprehensive approach to LLM interpretability and debugging.

## Method Summary
InTraVisTo is an open-source visualization tool that extracts hidden states from transformer layers during inference, decodes them to vocabulary tokens using linear interpolation between input and output embedding matrices, and displays the results as heatmaps. It calculates information flow attribution using vector norms to determine relative contributions of self-attention and feed-forward components, visualizing this as Sankey diagrams. The tool also enables embedding injection, allowing users to modify internal states by substituting token vectors in the hidden space. The implementation hooks into supported models (Mistral-7B-Instruct-v0.2, GPT-2, Gemma, Llama) and uses a web interface for interactive exploration of model internals.

## Key Results
- Provides real-time interactive visualizations of transformer internal states and information flow
- Decodes hidden states at each layer to show interpretable token predictions via heatmaps
- Visualizes information flow through Sankey diagrams showing self-attention vs feed-forward contributions
- Enables embedding injection to test and debug model behavior by modifying internal states
- Integrates multiple visualization and intervention capabilities not available in existing tools

## Why This Works (Mechanism)

### Mechanism 1: Projected Internal State Decoding
- Claim: Hidden states within the transformer layers can be mapped to human-interpretable tokens by projecting them onto the vocabulary space before the final output layer.
- Mechanism: InTraVisTo applies a decoding matrix ($W_{decoder}$) to intermediate hidden states ($x$). It normalizes the state, computes a probability distribution via softmax, and extracts the highest-probability token. To address discrepancies between early and late layers in models like Llama or Mistral, it employs a linear interpolation between the input embedding matrix ($W_{in}$) and the output unembedding matrix ($W_{out}$).
- Core assumption: The semantic meaning of a hidden vector corresponds to its nearest neighbors in the vocabulary embedding space, and the unembedding matrix is a valid projector for intermediate layers.
- Evidence anchors:
  - [abstract] Mentions visualizing "internal state... by decoding token embeddings at each layer."
  - [section] Section II.A, Eq. (1) and (2) define the projection and the linear interpolation strategy.
  - [corpus] Related work like "Logit Lens" supports the viability of decoding intermediate layers, though corpus signals for *this specific* interpolation method are weak.
- Break condition: If the model uses untied embeddings and the internal representations diverge significantly from both the input and output manifolds (manifesting as high-entropy or nonsensical decoded tokens).

### Mechanism 2: Norm-Based Attribution Flow
- Claim: The relative contribution of the Self-Attention (SA) and Feed-Forward Network (FFN) components to the final prediction can be approximated by the magnitude (norm) of their output vectors relative to the residual stream.
- Mechanism: The tool calculates a percentage contribution for attention and feed-forward layers by comparing the L2 norm of the component's output ($\delta_{att}, \delta_{ff}$) against the norm of the residual stream. It aggregates these contributions backwards from the final layer to construct a Sankey diagram, weighting the flow by these relative magnitudes.
- Core assumption: Vector norm is a sufficient proxy for "information importance" or "causal influence" in the residual stream.
- Evidence anchors:
  - [abstract] Highlights the Sankey diagram illustrating "information flow through self-attention, feed-forward, and residual connections."
  - [section] Section II.C, Eq. (3) and (4) define the percentage contributions based on vector norms.
  - [corpus] Corpus signals indicate related work in tracing structures (e.g., GraphGhost), but specific evidence validating norm-based flow vs. gradient-based attribution is not detailed in the provided text.
- Break condition: If the model utilizes "anticipatory" heads or null spaces where large norms do not correlate with semantic changes in the output.

### Mechanism 3: Vector-Space Causal Intervention
- Claim: Model behavior can be altered or debugged by surgically removing the vector component of a predicted token and injecting the component of a target token.
- Mechanism: The tool identifies the direction of a specific token in the hidden space ($e_{old}$) and substitutes it with another ($e_{new}$). It scales this operation based on the correlation between the hidden state and the token vector to minimize disruption to other features.
- Core assumption: Token embeddings act as linear directions in the hidden space, and "reasoning" can be perturbed by shifting the state along these axes.
- Evidence anchors:
  - [section] Section II.D, Eq. (9) defines the injection formula $h_{inject} = h + h \cdot e_{old} \cdot (e_{new} - e_{old})$.
  - [abstract] Notes the tool allows users to "perform embedding injection to test model behavior."
  - [corpus] Weak signal; corpus mentions "Patchscopes" and causal abstraction generally, but specific validation of this scaling formula is limited to the paper's examples.
- Break condition: If the hidden state manifold is highly non-linear or if the injection layer is too early/late, causing the model to "forget" the intervention or output gibberish.

## Foundational Learning

- Concept: **Residual Stream Architecture**
  - Why needed here: InTraVisTo relies on the specific architectural detail that transformer layers *add* their outputs to a residual stream ($x^{(l)} = x^{(l-1)} + \delta$). Understanding this is prerequisite to interpreting the Heatmap (decoration process) and Sankey (flow accumulation).
  - Quick check question: If you disabled the self-attention and feed-forward blocks in a standard LLM, what would the output of layer $L$ look like relative to the input?

- Concept: **Logit Lens / Unembedding**
  - Why needed here: The primary visualization converts abstract vector states into text. You must understand that applying the unembedding matrix ($W_{out}$) effectively asks, "If the model stopped here, what token would it predict?"
  - Quick check question: Why might applying the *output* unembedding matrix to *early* layers result in noisy or uninterpretable tokens in models with untied embeddings?

- Concept: **Attention Heads vs. Feed-Forward Networks (FFN)**
  - Why needed here: The Sankey diagram differentiates flow between these two components. You need to know that attention typically handles token-to-token communication (moving information) while FFNs typically process features (transforming information).
  - Quick check question: In the context of the Sankey diagram, if you see a heavy flow through the FFN node at layer $l$, what type of operation (retrieval or processing) is likely dominating that step?

## Architecture Onboarding

- Component map:
  - Hooking Layer -> Decoder Module -> Frontend (Heatmap)
  - Hooking Layer -> Attribution Engine -> Frontend (Sankey)
  - Hooking Layer -> Intervention Interface -> Model State

- Critical path: The **Hooking Layer $\to$ Decoder Module** path. If the hook captures the wrong internal vector (e.g., pre-norm vs post-norm), the entire visualization (heatmap) becomes hallucinated.

- Design tradeoffs:
  - **Norms vs. Gradients**: The tool uses L2 norms for attribution (faster, O(1) for stored states) rather than gradients (slower, requires backprop).
  - **Interpolation vs. Training**: The authors chose linear interpolation of decoders over training specialized "probes" (e.g., Tuned Lens) to maintain generality across models without extra training data.

- Failure signatures:
  - **"Step" Artifacts**: In the heatmap, probabilities may jump suddenly at specific layers rather than evolving smoothly (noted in Fig 5a), indicating decoder misalignment rather than model reasoning.
  - **Injection Collapse**: Injecting tokens often causes the model to generate incoherent text in subsequent tokens, as the intervention breaks the internal subspaces required for future layers.

- First 3 experiments:
  1. **The "Capital" Test**: Run "The capital of France is" through the tool. Verify if the heatmap shows "Paris" forming with high probability in the final layers and check if the Sankey attributes this to the FFN (knowledge retrieval) or Attention (copying if context was provided).
  2. **Attention Glitch Debugging**: Input a sequence reversal task (as in the paper). Identify a specific layer where the decoded token flips from correct to incorrect. Use the Sankey diagram to see if the Attention flow dropped off at that layer.
  3. **Manual Correction (Injection)**: Induce a hallucination (e.g., "The capital of Mars is New York"). Identify the token "New York" in the heatmap, inject "Olympus Mons" (or a made-up token) at the mid-layer, and observe if the model recovers grammatically or collapses.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can mechanistic interpretability be integrated into InTraVisTo to better explain the specific "causes" of an output token?
- Basis in paper: [explicit] The Conclusion states, "We are working to integrate causal analysis via mechanistic interpretabilty to better explain the 'causes' of a certain output."
- Why unresolved: The current tool relies on a "minimal flow interpretation" based on vector norms rather than formal causal tracing or circuit analysis.
- What evidence would resolve it: An updated version of the tool featuring visualization of causal circuits or path patching results that link specific attention heads to output decisions.

### Open Question 2
- Question: To what extent does embedding injection at one layer introduce artifacts or error propagation in subsequent layers or tokens?
- Basis in paper: [inferred] In Section II.D, the authors note that injecting the correct digit "8" resulted in a subsequent error (generating an extra digit instead of a newline) because the injected embedding was not "crafted from the network."
- Why unresolved: While the tool supports injection, the example provided shows it currently creates misalignments in the residual stream that negatively affect downstream generation.
- What evidence would resolve it: A comparative study of generation coherence following embedding injection versus clean runs, or the development of "network-crafted" injection methods that preserve downstream state integrity.

### Open Question 3
- Question: How does the accuracy of "self-interpretation" (using linear interpolation decoders) compare to using trained external translators?
- Basis in paper: [inferred] Section III.D states the authors chose simple interpolation to "evaluate the model's self-interpretability capabilities" rather than using trained probes like Tuned Lens.
- Why unresolved: The authors deliberately avoided trained translators to assess inherent interpretability, but they do not quantify the trade-off in accuracy or clarity between their method and trained alternatives.
- What evidence would resolve it: Benchmarking results comparing the token recovery fidelity of the tool's linear interpolation decoder against trained decoders (e.g., Tuned Lens) across various layers.

## Limitations

- Limited empirical validation of effectiveness compared to existing tools or baseline methods
- Strong assumptions about vector norms as information proxies and linear interpolation decoding untested
- Currently supports limited set of model architectures, unclear generalization to other transformer variants
- Injection interventions can cause catastrophic collapse in subsequent token generation

## Confidence

- **High confidence**: The technical implementation details (equations, architecture) are clearly specified and reproducible. The visualization approach (heatmap + Sankey) is well-defined.
- **Medium confidence**: The claimed benefits (debugging, interpretability) are plausible based on the tool's capabilities, but lack quantitative validation.
- **Low confidence**: The specific mechanisms (norm-based attribution, linear interpolation decoding, vector-space injection) are theoretically justified but not empirically validated as superior to alternatives.

## Next Checks

1. **Cross-model generalization test**: Apply InTraVisTo to a model not mentioned in the paper (e.g., Phi-2 or DeepSeek-Coder) and verify that the visualizations remain interpretable and the tool doesn't crash. Compare decoding quality between linear interpolation and training a dedicated probe layer.

2. **Attribution mechanism ablation**: Replace the norm-based attribution (Eq 3-4) with gradient-based attribution (using integrated gradients or attention rollout) and compare the resulting Sankey diagrams on the same input. Measure correlation between the two attribution methods across multiple examples.

3. **Injection intervention study**: Systematically test embedding injection across multiple layers and token positions on a benchmark of known model errors (e.g., factual inconsistencies, logical contradictions). Measure whether interventions successfully correct errors without causing catastrophic collapse in subsequent tokens, and compare success rates to baseline (no intervention).