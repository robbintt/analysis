---
ver: rpa2
title: 'Ebisu: Benchmarking Large Language Models in Japanese Finance'
arxiv_id: '2602.01479'
source_url: https://arxiv.org/abs/2602.01479
tags:
- financial
- japanese
- language
- arxiv
- term
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces EBISU, a benchmark for evaluating large
  language models on native Japanese financial language understanding. It includes
  two expert-annotated tasks: JF-ICR for implicit commitment and refusal recognition
  in investor-facing Q&A, and JF-TE for hierarchical extraction and ranking of nested
  financial terminology from professional disclosures.'
---

# Ebisu: Benchmarking Large Language Models in Japanese Finance

## Quick Facts
- arXiv ID: 2602.01479
- Source URL: https://arxiv.org/abs/2602.01479
- Authors: Xueqing Peng; Ruoyu Xiang; Fan Zhang; Mingzi Song; Mingyang Jiang; Yan Wang; Lingfei Qian; Taiki Hara; Yuqing Guo; Jimin Huang; Junichi Tsujii; Sophia Ananiadou
- Reference count: 28
- Primary result: 22 diverse LLMs achieve 60.6% accuracy on JF-ICR and 0.41 F1 on JF-TE, showing persistent gaps in modeling Japanese financial communication

## Executive Summary
This paper introduces EBISU, a benchmark for evaluating large language models on native Japanese financial language understanding. It includes two expert-annotated tasks: JF-ICR for implicit commitment and refusal recognition in investor-facing Q&A, and JF-TE for hierarchical extraction and ranking of nested financial terminology from professional disclosures. Evaluation of 22 diverse LLMs shows overall performance remains low, with best models achieving 60.6% accuracy on JF-ICR and 0.41 F1 on JF-TE. While increased model scale yields limited improvements, language- and domain-specific adaptation does not reliably improve performance, and can even degrade it. Results highlight persistent gaps in modeling the linguistic and pragmatic characteristics of Japanese financial communication.

## Method Summary
EBISU comprises two tasks: JF-ICR (94 Q&A pairs) evaluates implicit commitment/refusal recognition using 5-point scale classification, while JF-TE (202 note instances) evaluates hierarchical extraction and ranking of nested financial terms. Both datasets were expert-annotated from Japanese corporate disclosures (2023-2026). Evaluation uses LM Evaluation Harness with deterministic settings (temperature=0, max_tokens=1024), exact string matching for JF-TE, and standard metrics (Accuracy, F1, HitRate@K).

## Key Results
- Best models achieve only 60.6% accuracy on JF-ICR (5-class intent classification)
- JF-TE shows even lower performance at 0.41 F1 for maximal term extraction
- Domain-specific adaptation and increased model scale provide limited improvements
- Japanese-adapted models consistently underperform English counterparts on commitment calibration
- Term ranking quality deteriorates with compound depth and mixed-script variability

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Benchmark performance gaps are driven by Japanese-specific morphosyntactic properties that place semantic content at sentence-final positions, conflicting with English-centric training distributions.
- **Mechanism:** Japanese is agglutinative and head-final, meaning negation, modality, and commitment signals cluster at clause ends. Models trained predominantly on English text develop attention patterns that weight early tokens more heavily, causing systematic misreading of intent when refusal or hedging appears only in sentence-final auxiliaries.
- **Core assumption:** Sentence-final attention weighting is learned from training corpora and transfers poorly to languages with inverted information density.
- **Evidence anchors:** Abstract states "agglutinative, head-final linguistic structure... modality and negation are often realized sentence finally"; section 1 notes "refusals are frequently conveyed indirectly through pragmatic cues."

### Mechanism 2
- **Claim:** Financial domain adaptation via continued pre-training can degrade performance on pragmatic tasks because it optimizes for vocabulary and surface patterns without modeling discourse-level intent conventions.
- **Mechanism:** Domain-specific pre-training on Japanese financial corpora increases exposure to formal terminology and disclosure language but does not provide supervision signals for implicit stance, hedging strategies, or culturally-grounded refusal patterns. This creates a capability imbalance: better term recognition but worse intent inference.
- **Core assumption:** Next-token prediction loss does not inherently capture pragmatic intent; explicit task-specific supervision is required.
- **Evidence anchors:** Abstract states "language- and domain-specific adaptation does not reliably improve performance, and can even degrade it"; section 4.3 shows "continued financial pretraining can even degrade performance, particularly for financial term extraction (-0.12 F1)."

### Mechanism 3
- **Claim:** High-context communication norms in Japanese finance create systematic bias where English-centric models over-estimate commitment levels.
- **Mechanism:** Japanese corporate communication discourages explicit refusal; responses often use hedging, conditional framing, and polite deflection. Models trained on lower-context corpora interpret these neutral or weakly-negative responses as positive or hedged agreement rather than soft refusal.
- **Core assumption:** Commitment calibration is implicitly learned from English communication patterns where refusal is more explicit.
- **Evidence anchors:** Section 4.3 reports "English-centric models consistently assign higher commitment scores than their Japanese counterparts" and "indirect refusals and strategic non-commitment in Japanese financial responses are frequently misinterpreted as acceptance or weak agreement."

## Foundational Learning

- **Concept: Agglutinative morphology and head-final syntax**
  - **Why needed here:** Explains why semantic content clusters at phrase/sentence ends, making early-token attention insufficient for Japanese comprehension.
  - **Quick check question:** Can you identify where negation or modality would appear in "来期の配当方針について検討する予定はありません" (a future dividend policy refusal)?

- **Concept: High-context vs. low-context communication**
  - **Why needed here:** Japanese financial discourse relies on shared context and indirect signaling; understanding requires inferring unstated intent rather than parsing explicit statements.
  - **Quick check question:** Why would "現時点では明確な見通しは示せません" (cannot provide a clear outlook at this time) be classified as neutral rather than negative?

- **Concept: Benchmark validity through inter-annotator agreement**
  - **Why needed here:** Pragmatic tasks are inherently subjective; high agreement (κ > 0.8) confirms that the task is well-defined and labels are reproducible.
  - **Quick check question:** What does JF-ICR's κ=0.88 suggest about the feasibility of consistent annotation for implicit stance?

## Architecture Onboarding

- **Component map:**
  ```
  EBISU Benchmark
  ├── JF-ICR (94 Q&A pairs)
  │   ├── Data: Earnings calls, shareholder meetings, briefings (4 companies, 2023-2026)
  │   ├── Labels: 5-point commitment scale {+2, +1, 0, -1, -2}
  │   └── Metric: Accuracy
  ├── JF-TE (202 note instances)
  │   ├── Data: Annual Securities Reports (EDINET, 10 companies)
  │   ├── Labels: Maximal + nested financial terms (2,412 mentions, 777 unique)
  │   └── Metrics: F1 (maximal), HitRate@K (nested ranking)
  └── Evaluation Pipeline
      ├── Harness: LM Evaluation Harness (Gao et al., 2024)
      ├── Inference: vLLM (local) / TogetherAI / Official APIs
      └── Config: temperature=0, max_tokens=1024
  ```

- **Critical path:** For meaningful results, prioritize: (1) using the exact task prompts from Appendix A/B, (2) forcing deterministic output (temp=0), (3) exact string matching for JF-TE (not fuzzy), (4) reporting all metrics together (Acc, F1, HR@1/5/10) to diagnose where models fail.

- **Design tradeoffs:**
  - **Dataset size vs. annotation quality:** 94 and 202 instances are small but enable expert double-annotation with adjudication; suitable for evaluation, not training.
  - **Exact matching vs. semantic matching:** JF-TE uses exact span matching, penalizing correct-but-offset predictions; favors boundary precision over semantic understanding.
  - **Single-turn vs. discourse:** JF-ICR isolates single Q&A pairs to reduce annotation ambiguity, but may miss context-dependent intent signals in multi-turn exchanges.

- **Failure signatures:**
  - **Commitment inflation:** Model outputs systematically higher scores than ground truth → likely trained on low-context corpora, needs pragmatic calibration data.
  - **Term boundary drift:** High HR@10 but low HR@1 → model recognizes terms but cannot rank nested compounds correctly; may need hierarchical span training.
  - **Format mismatch:** Output is explanatory text instead of label/JSON → prompt template not followed; check instruction formatting and few-shot examples.

- **First 3 experiments:**
  1. **Baseline assessment:** Run the 4-5 best models from Table 4 (Llama-4-Scout-17B, Llama-3.3-70B-Instruct, Claude-Sonnet-4.5, Kimi-K2-Instruct) on both tasks using the exact prompts in Appendix A/B to confirm your evaluation pipeline produces results within ±0.03 of reported scores.
  2. **Ablation on input context:** For JF-ICR, test whether providing only the response (without the question) degrades accuracy, quantifying how much question context aids intent inference.
  3. **Cross-lingual control:** Translate JF-ICR instances to English using a high-quality translator, evaluate English-capable models, and compare accuracy drops to isolate linguistic vs. cultural-pragmatic contributions to difficulty.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** What training paradigms beyond next-token prediction can effectively capture the morphosyntactic and pragmatic mechanisms central to Japanese financial communication?
- **Basis in paper:** The paper states that "continued next-token pretraining on Japanese financial text alone is insufficient, and may even hinder performance" and that "improvements are limited and that strong performance is not guaranteed by proprietary training alone."
- **Why unresolved:** The Japanese financial model (nekomata-14b-pfn-qfin) achieved only 0.0198 average score, performing worse than both its Japanese general counterpart (0.0251) and the original backbone Qwen-14B (0.0857), indicating that standard adaptation approaches fail or backfire.
- **What evidence would resolve it:** Development of alternative training objectives (e.g., pragmatic inference, sentence-final morphology prediction) that demonstrably improve performance on JF-ICR and JF-TE beyond next-token pretraining baselines.

### Open Question 2
- **Question:** How can the systematic bias whereby English-centric models over-assign commitment scores in Japanese financial responses be corrected?
- **Basis in paper:** The paper reports that "at similar model sizes, English-centric models consistently assign higher commitment scores than their Japanese counterparts, such as Llama-3.3-70B-Instruct (0.9787) compared to Llama-3.3-Swallow-70B-Instruct (0.8404)."
- **Why unresolved:** This bias causes indirect refusals and strategic non-commitment in Japanese financial responses to be frequently misinterpreted as acceptance or weak agreement, fundamentally misunderstanding high-context Japanese communication norms.
- **What evidence would resolve it:** A debiasing intervention that reduces the commitment score gap between English-centric and Japanese-adapted models while maintaining or improving overall accuracy on JF-ICR.

### Open Question 3
- **Question:** What architectural or training modifications would enable LLMs to better resolve term boundaries and handle mixed-script variants in Japanese financial terminology?
- **Basis in paper:** The paper notes that "ranking quality deteriorates with increasing compound depth and mixed-script variability, and the gap between HitRate@1 and HitRate@10 indicates difficulties in term boundary resolution and variant handling rather than lack of financial vocabulary."
- **Why unresolved:** Even the best model (Llama-3.3-70B-Instruct) shows a large gap between HitRate@1 (0.1277) and HitRate@10 (0.5111), suggesting models struggle to prioritize correct boundaries among candidates rather than lacking vocabulary knowledge.
- **What evidence would resolve it:** Demonstrated reduction in the HR@1 vs HR@10 gap through targeted modifications addressing compound segmentation and script variant normalization.

## Limitations
- Small dataset size (94 Q&A pairs and 202 instances) limits generalizability despite expert annotation quality
- Exclusive focus on Annual Securities Reports for JF-TE may bias toward regulatory compliance language rather than operational terminology
- Performance gaps could reflect dataset-specific idiosyncrasies rather than fundamental capability limitations

## Confidence
- **High confidence:** Japanese-specific morphosyntactic properties (head-final syntax, agglutinative morphology) create genuine challenges for English-centric models; the performance gap between Japanese and English models is substantial and consistent across multiple model families.
- **Medium confidence:** The claim that domain-specific adaptation degrades performance is supported but could reflect dataset-specific idiosyncrasies rather than fundamental capability limitations.
- **Low confidence:** The assertion that high-context communication norms cause systematic bias in commitment estimation; while cultural explanations align with observations, the mechanism could be primarily linguistic rather than pragmatic.

## Next Checks
1. **Scale sensitivity test:** Evaluate whether increasing dataset size from 94 to 500+ JF-ICR instances changes the relative performance gap between Japanese and English models, isolating whether current results reflect true capability limits or dataset constraints.

2. **Cross-linguistic transfer analysis:** Translate JF-ICR instances to English and evaluate both Japanese-native and English-native models on both versions to quantify the relative contributions of linguistic structure versus cultural-pragmatic patterns to performance differences.

3. **Prompt engineering ablation:** Systematically vary prompt templates (adding explicit examples of Japanese refusal patterns, adjusting temperature, providing context windows) to determine whether current performance gaps reflect inherent model limitations or suboptimal task formulation.