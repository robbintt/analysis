---
ver: rpa2
title: Improving Respiratory Sound Classification with Architecture-Agnostic Knowledge
  Distillation from Ensembles
arxiv_id: '2505.22027'
source_url: https://arxiv.org/abs/2505.22027
tags:
- teacher
- ensemble
- soft
- score
- sound
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study demonstrates the effectiveness of architecture-agnostic
  knowledge distillation with soft labels for respiratory sound classification. The
  approach transfers knowledge from an ensemble of teacher models to a student model,
  improving performance without increasing inference compute cost.
---

# Improving Respiratory Sound Classification with Architecture-Agnostic Knowledge Distillation from Ensembles

## Quick Facts
- **arXiv ID**: 2505.22027
- **Source URL**: https://arxiv.org/abs/2505.22027
- **Reference count**: 0
- **Primary result**: BTS-d model achieves 64.39% ICBHI Score, surpassing prior SOTA by 0.85%

## Executive Summary
This study demonstrates that architecture-agnostic knowledge distillation with soft labels significantly improves respiratory sound classification. By transferring knowledge from an ensemble of BTS++ teacher models to a student model, the approach achieves state-of-the-art performance (64.39% ICBHI Score) without increasing inference compute cost. The method shows consistent gains across multiple architectures, with an average score improvement of 1.16%. Notably, even a single teacher model significantly boosts student performance, and the second-generation BTS-d++ further improves results to 65.45%. These findings highlight soft-label distillation as a powerful technique for enhancing respiratory sound classification while maintaining computational efficiency.

## Method Summary
The method trains an ensemble of BTS teacher models (k=5 to 30) on the ICBHI dataset, then generates soft labels by averaging logits and applying softmax (Mean Teacher) or sampling individual teachers (Random Teacher). Student models are trained using cross-entropy loss against these soft labels, replacing hard labels entirely. The approach is architecture-agnostic, working with ResNet, EfficientNet, CNN6, AST, Audio-CLAP, and BTS models. Second-generation ensembles (BTS-d++) are created by distilling from the first-generation distilled models, creating a quality amplification loop.

## Key Results
- BTS-d achieves 64.39% ICBHI Score, surpassing previous SOTA by 0.85%
- Architecture-agnostic gains: average 1.16% improvement across multiple architectures
- Single teacher provides significant boost: 0.36-0.64% improvement over baseline
- Mean Teacher (k=5) outperforms Random Teacher with fewer teachers needed
- Second-generation BTS-d++ reaches 65.45%, outperforming first-generation BTS++ at 64.34%

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Soft labels from teacher ensembles provide richer supervisory signal than one-hot hard labels, improving student generalization.
- **Mechanism**: Teacher probability distributions encode inter-class relationships (e.g., crackle vs. wheeze similarity) that hard labels discard. Students learn from this "dark knowledge" via cross-entropy with soft targets.
- **Core assumption**: Teacher ensemble captures meaningful class relationships beyond ground truth labels.
- **Evidence anchors**: Soft labels provide richer representation than hard labels; related work on acoustic scene classification KD confirms soft-label effectiveness in audio domains.

### Mechanism 2
- **Claim**: Distilling from multiple diverse teachers compresses ensemble knowledge without inference-time overhead.
- **Mechanism**: Averaging logits across k teachers (mean teacher) or sampling individual teachers (random teacher) creates consensus soft labels. Student learns to approximate ensemble behavior while requiring only single-model compute at test time.
- **Core assumption**: Student capacity is sufficient to approximate teacher ensemble predictions.
- **Evidence anchors**: Single teacher considerably improves performance; Mean teacher peaks at k=5; Random teacher at k=15 with diminishing returns beyond.

### Mechanism 3
- **Claim**: Curated teacher selection and iterative distillation (second-generation ensembles) yield compounding improvements.
- **Mechanism**: Selecting highest-scoring teacher checkpoints improves soft-label quality. Distilled students can themselves form stronger ensembles (BTS-d++), creating a quality amplification loop.
- **Core assumption**: High-scoring teachers produce better-calibrated or more informative soft labels.
- **Evidence anchors**: Curated Teacher Ensemble (k=5) achieves 64.61% vs. 64.38% for random selection; second-generation BTS-d++[k=5] reaches 65.45%.

## Foundational Learning

- **Concept: Knowledge Distillation (Response-Based)**
  - **Why needed here**: The method relies on transferring teacher output distributions (logits→softmax→soft labels) rather than feature maps or intermediate representations.
  - **Quick check question**: Can you explain why cross-entropy with soft labels differs from standard one-hot training?

- **Concept: Ensemble Averaging vs. Voting**
  - **Why needed here**: BTS++ uses logit averaging (not majority voting) to generate soft labels; understanding this distinction is critical for reproducing results.
  - **Quick check question**: What happens to soft-label variance as ensemble size k increases?

- **Concept: Bias-Variance Tradeoff in Classification**
  - **Why needed here**: Table 2 shows specificity gains but slight sensitivity drops after distillation—suggesting students learn to reduce false positives at the cost of some false negatives.
  - **Quick check question**: How would you diagnose whether a student is overfitting to teacher predictions versus learning generalizable features?

## Architecture Onboarding

- **Component map**: ICBHI dataset -> BTS teacher ensemble (k=5-30) -> Soft Label Generator (Mean/Random Teacher) -> Student model -> Evaluation
- **Critical path**: Train k teacher models with different seeds → Generate soft labels using chosen aggregation method → Train student with cross-entropy against soft labels → Evaluate on ICBHI test split
- **Design tradeoffs**: Mean vs. Random Teacher (Mean offers stability at lower k); Ensemble Size (k>5 yields diminishing returns); Inference Cost (distilled models match single-model cost)
- **Failure signatures**: Training with raw logits without softmax fails to converge; Gaussian-noised labels degrade performance; Student performance plateaus while teacher ensemble continues improving
- **First 3 experiments**:
  1. Train single BTS model with hard labels; confirm ~63.54% ICBHI Score
  2. Use same architecture as teacher and student; expect ~0.36% gain (63.54→63.90%)
  3. Vary k ∈ {1, 3, 5, 10, 15} with mean teacher; plot student score vs. k to validate diminishing-returns curve

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: Can the remaining performance gap between the ensemble teacher models and the distilled student models be reduced without increasing inference computational cost?
- **Basis in paper**: The conclusion explicitly states future research could explore reducing the gap between ensemble and distilled model performance.
- **Why unresolved**: While the paper successfully transfers knowledge, student models still slightly underperform compared to the full ensemble, leaving a performance delta that current distillation techniques haven't bridged.
- **What evidence would resolve it**: A modified distillation methodology that yields a student model whose ICBHI score is statistically indistinguishable from the teacher ensemble while maintaining single model inference cost.

### Open Question 2
- **Question**: Does the effectiveness of this distillation method generalize to diverse model architectures not included in the current study?
- **Basis in paper**: The conclusion suggests future research could explore diverse model architectures.
- **Why unresolved**: While the study tested ResNet, EfficientNet, CNN6, AST, and CLAP, the authors explicitly flag exploration of a wider range of architectures as necessary to validate the "architecture-agnostic" claim fully.
- **What evidence would resolve it**: Successful application to emerging or fundamentally different neural network architectures showing consistent performance gains.

### Open Question 3
- **Question**: Is the observed tapering of student performance at larger ensemble sizes caused by the student overfitting to the teacher's label distribution on the training set?
- **Basis in paper**: Section 4.4 hypothesizes that beyond certain k, students begin to overfit to teacher label distribution on training set and cannot generalize more to real ground truth distribution on test set.
- **Why unresolved**: This is stated as a hypothesis to explain diminishing returns but the paper doesn't include experiments specifically designed to confirm or refute this overfitting mechanism.
- **What evidence would resolve it**: Analysis of student model's generalization gap relative to teacher's label variance, or experiments using regularization techniques targeted at preventing fitting to teacher-specific noise.

### Open Question 4
- **Question**: Can the trade-off where soft-label distillation increases Specificity but decreases Sensitivity be mitigated in lightweight models?
- **Basis in paper**: Section 4.2 notes that for lightweight architectures, specificity increased much more while sensitivity decreased slightly, indicating a trade-off between reducing false positives and slight increase in false negatives.
- **Why unresolved**: While average score improved, consistent drop in Sensitivity suggests distilled models become more conservative, which might be undesirable in diagnostic screening tools where missing a positive case is costly.
- **What evidence would resolve it**: A distillation loss function or training regime that improves both Sp and Se simultaneously, or at least maintains baseline Sensitivity while improving Specificity.

## Limitations

- **Architecture diversity**: Limited testing on non-BTS architectures (only AST showed competitive performance) leaves open questions about true architecture-agnostic effectiveness
- **Clinical readiness**: No runtime efficiency or memory analysis provided to assess deployment feasibility in clinical settings
- **Uncertainty calibration**: No reliability diagrams or expected calibration error metrics to verify teacher ensemble soft labels are well-calibrated probability distributions

## Confidence

- **High Confidence**: BTS++ ensemble distillation achieves SOTA (64.39% ICBHI Score); single teacher provides consistent gains (0.36-0.64% improvement); mean teacher outperforms random teacher at lower k values; diminishing returns occur beyond k=5 for mean teacher
- **Medium Confidence**: Architecture-agnostic effectiveness (limited non-BTS testing); second-generation BTS-d++ improvements (no ablation for teacher selection vs. iterative effects); generalization beyond ICBHI dataset (no cross-dataset validation)
- **Low Confidence**: Claims about clinical deployment readiness (no runtime efficiency or memory analysis); uncertainty calibration of soft labels (no reliability diagrams or ECE metrics)

## Next Checks

1. **Architecture Diversity Test**: Apply soft-label distillation to three additional diverse architectures (e.g., MobileNet, Wav2Vec2, EfficientNetV2) and compare performance gains against BTS-d baseline. Include calibration analysis to verify whether architecture-agnostic gains correlate with improved uncertainty estimation.

2. **Cross-Dataset Generalization**: Train BTS-d on ICBHI, then evaluate on independent respiratory sound datasets (e.g., ICBHI-Cycle, Respiratory Sound Database). Measure performance degradation and compare against BTS++ ensemble to quantify distillation's robustness to dataset shift.

3. **Teacher Selection Ablation**: Systematically vary teacher selection criteria (random vs. validation score vs. ensemble diversity) for BTS-d++ generation. Compare whether gains come from higher-scoring teachers versus ensemble diversity, and test whether student distillation from diverse low-scoring teachers outperforms distillation from homogeneous high-scoring teachers.