---
ver: rpa2
title: Testing Question Answering Software with Context-Driven Question Generation
arxiv_id: '2511.07924'
source_url: https://arxiv.org/abs/2511.07924
tags:
- questions
- question
- test
- generated
- context
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper presents CQ\xB2A, a context-driven approach for testing\
  \ QA software using LLMs. It addresses limitations in existing metamorphic testing\
  \ methods by extracting entities and relationships from the context as ground truth\
  \ answers, then generating natural questions using LLMs with consistency verification\
  \ and constraint checking to improve reliability."
---

# Testing Question Answering Software with Context-Driven Question Generation

## Quick Facts
- arXiv ID: 2511.07924
- Source URL: https://arxiv.org/abs/2511.07924
- Reference count: 40
- This paper presents CQ²A, a context-driven approach for testing QA software using LLMs that detects 45.17% more true positives than QAQA and generates more natural questions.

## Executive Summary
This paper addresses the challenge of testing closed-world QA software by introducing CQ²A, a context-driven question generation approach that extracts entities and relationships from provided context to create natural test questions. The method uses a four-stage pipeline: entity and relation extraction, question generation, constraint checking, and consistency verification. Unlike metamorphic testing approaches, CQ²A grounds test questions in the actual context content, generating more natural questions that better cover the source material while detecting more software defects.

## Method Summary
CQ²A employs a four-stage pipeline for testing QA software. First, it extracts entities from context using Stanza's POS tagging and filters out pronouns, short phrases, and clause elements, then extracts [entity1, relation, entity2] triples using ChatGPT with chain-of-thought prompting. Second, it generates questions using ChatGPT with specific prompts for entity-based and relation-based questions, applying constraint checking to prevent answer leakage and ensure completeness. Third, it filters generated questions through consistency verification by re-asking the questions five times via ChatGPT with majority voting, computing SimCSE similarity between re-asked answers and ground truth (threshold 0.75). Finally, it validates answers using a two-stage similarity checking process: initial SimCSE comparison followed by ChatGPT judgment when similarity falls below the threshold.

## Key Results
- CQ²A detects 45.17% more true positives than QAQA and 22.84% more than QAAskeR-plus on BoolQ, SQuAD2, and NarrativeQA datasets
- Generated questions achieve higher naturalness scores (4.59-4.66 vs 2.24-2.60) and cover 24.7-43.7% more context than existing approaches
- Test cases improve QA model performance, reducing error rates by an average of 30.2% when used for fine-tuning

## Why This Works (Mechanism)
CQ²A works by grounding test generation in the actual content of the provided context rather than relying on metamorphic transformations of existing questions. By extracting entities and relationships as ground truth answers and generating questions specifically about these elements, the approach creates more natural and contextually relevant test cases. The two-stage similarity checking (embedding-based followed by LLM-assisted judgment) provides both computational efficiency and semantic understanding, while consistency verification through multiple LLM queries reduces hallucination and improves reliability.

## Foundational Learning

**Stanza POS tagging and entity extraction**: Needed to identify meaningful entities from context that can serve as ground truth answers; quick check is verifying extracted entities exclude pronouns and short phrases (<2 words).

**ChatGPT relation extraction with CoT**: Required to extract [entity1, relation, entity2] triples from context; quick check is confirming the extracted triples are coherent and cover key relationships in the text.

**SimCSE similarity threshold**: Essential for automated answer comparison; quick check is validating that 0.75 threshold appropriately distinguishes correct from incorrect answers across different question types.

**LLM consistency verification with majority voting**: Needed to reduce hallucination in generated questions; quick check is ensuring re-asked answers converge to a consistent response across multiple queries.

**Two-stage similarity checking**: Combines computational efficiency of embeddings with semantic understanding of LLMs; quick check is measuring cases where LLM judgment corrects SimCSE false negatives.

## Architecture Onboarding

**Component map**: Context → Stanza Entity Extraction → ChatGPT Relation Extraction → Question Generation → Constraint Checking → Consistency Verification → Two-stage Similarity Checking → Bug Detection

**Critical path**: Context → Entity/Relation Extraction → Question Generation → Consistency Verification → Answer Validation → Defect Detection

**Design tradeoffs**: The approach trades computational overhead (multiple LLM queries, two-stage checking) for higher naturalness and bug detection rates versus metamorphic testing; ground truth extraction ensures relevance but limits applicability to sparse-context datasets.

**Failure signatures**: High hallucination rates (42-55% before filtering) in relation-based questions; low true positive rates from embedding-only comparison; reduced effectiveness on datasets with many "no answer" cases.

**3 first experiments**: 1) Test Stanza entity extraction on BoolQ dataset and measure coverage of key entities versus gold answers; 2) Generate questions using relation-based prompts and count hallucination rate before filtering; 3) Compare SimCSE-only versus two-stage similarity checking on a validation set to quantify LLM judgment's added value.

## Open Questions the Paper Calls Out

**Open Question 1**: How can context-driven test generation be adapted to handle datasets with frequent "no answer" cases like SQuAD2, where context may be sparse? The paper acknowledges CQ²A detected fewer defects than QAQA on SQuAD2 because the dataset contains many questions with "〈No Answer〉" that have little corresponding context information.

**Open Question 2**: Can CQ²A be effectively extended to test open-world QA systems that rely on external knowledge bases rather than provided context? The paper focuses on closed-world QA software and notes that open-world QA systems like LLMs are a separate category requiring different inputs.

**Open Question 3**: What is the optimal similarity threshold for answer consistency comparison, and does it vary across QA task types? The 0.75 threshold is adopted from prior work without ablation or justification specific to CQ²A's two-stage approach.

## Limitations
- Effectiveness depends heavily on quality of few-shot examples embedded in prompts, which are only partially specified
- High hallucination rates (42-55%) in relation-based questions before filtering indicate ongoing challenges with LLM accuracy
- Two-stage similarity checking introduces potential subjectivity through LLM-assisted judgment
- Experimental results focus on a single QA model (UnifiedQA-v2-large), limiting generalizability

## Confidence

**High confidence**: The methodology for entity and relation extraction using Stanza and ChatGPT is well-established and clearly described. The overall pipeline structure is logically sound and reproducible.

**Medium confidence**: The reported improvements in bug detection rates and naturalness scores are based on the described metrics, but exact implementation details of LLM-assisted consistency evaluation and majority voting mechanism are underspecified.

**Low confidence**: The semantic equivalence judgments in the two-stage similarity checking process lack explicit evaluation criteria, making it difficult to assess the reliability of true positive detection.

## Next Checks

1. Replicate the two-stage similarity checking on a held-out validation set by comparing SimCSE-only judgments against the full two-stage approach to quantify the added value and identify cases where LLM judgment disagrees with embedding similarity.

2. Test the approach with alternative LLMs for question generation and consistency verification (e.g., GPT-4 instead of ChatGPT, Claude instead of GPT-4) to assess whether reported naturalness scores and hallucination rates are model-dependent or generalizable.

3. Evaluate on additional QA models beyond UnifiedQA-v2-large, including smaller models and models from different families, to determine if the 30.2% average error reduction from fine-tuning is consistent across architectures or specific to large encoder-decoder models.