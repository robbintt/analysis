---
ver: rpa2
title: 'Llamba: Scaling Distilled Recurrent Models for Efficient Language Processing'
arxiv_id: '2502.14458'
source_url: https://arxiv.org/abs/2502.14458
tags:
- llamba
- performance
- language
- distillation
- https
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Llamba, a family of efficient recurrent language
  models (Llamba-1B, Llamba-3B, and Llamba-8B) distilled from Llama-3.x into the Mamba
  architecture. Llamba achieves higher inference throughput and handles significantly
  larger batch sizes than Transformer-based models while maintaining comparable benchmark
  performance.
---

# Llamba: Scaling Distilled Recurrent Models for Efficient Language Processing

## Quick Facts
- arXiv ID: 2502.14458
- Source URL: https://arxiv.org/abs/2502.14458
- Authors: Aviv Bick; Tobias Katsch; Nimit Sohoni; Arjun Desai; Albert Gu
- Reference count: 16
- Primary result: Distilled Mamba-2 models achieve 68.8% average accuracy vs 69.4% for teacher with <0.1% of training data

## Executive Summary
Llamba introduces a family of efficient recurrent language models (1B, 3B, and 8B parameters) distilled from Llama-3.x into the Mamba-2 architecture. Using the MOHAWK framework, these models achieve comparable benchmark performance to their Transformer-based teachers while processing larger batch sizes and maintaining higher inference throughput. The models are trained with exceptional data efficiency—less than 0.1% of typical pretraining data—making them particularly suitable for deployment on resource-constrained devices like smartphones.

## Method Summary
Llamba uses a three-stage MOHAWK distillation framework: Matrix Orientation aligns the student's recurrent matrix mixer with the teacher's attention mechanism; Hidden-State Alignment matches per-layer outputs through L2 distance; Weight Transfer copies MLP, normalization, embedding, and head weights from the teacher; finally, Knowledge Distillation applies cross-entropy on logits. The models use Discrete Mamba-2 with 32 heads and state size 64, modified to remove normalization and activation layers incompatible with attention mechanisms. Training employs AdamW with WSD scheduler across 8 H100 GPUs with FSDP and activation checkpointing.

## Key Results
- Llamba-8B achieves 68.8% average accuracy on ARC, PIQA, Winogrande, HellaSwag, MMLU, and OpenBookQA compared to 69.4% for Llama-3.1-8B
- Inference throughput tests show Llamba-8B processing more tokens per second than Llama-3.1-8B, especially at larger batch sizes
- On Apple Silicon M3 Pro, Llamba maintains constant throughput while Llama-3.1-8B drops linearly with increasing context size
- Llamba models handle 2× larger batch sizes than pure Mamba-2 models due to MLP layers enabling larger KV cache

## Why This Works (Mechanism)

### Mechanism 1
Cross-architecture distillation from Transformers to SSMs can preserve ~99% of teacher performance using <0.1% of typical pretraining data. The MOHAWK framework achieves this through three sequential phases: Matrix Orientation aligns the student matrix mixer with teacher self-attention; Hidden-State Alignment matches per-layer outputs via L2 distance; Weight Transfer copies MLP/embedding/head weights, followed by knowledge distillation on logits. The core assumption is that the student architecture can express similar input-output mappings as the teacher despite different inductive biases.

### Mechanism 2
Removing attention-incompatible non-linearities from Mamba-2 improves distillation alignment quality. Discrete Mamba-2 eliminates normalization before output projection and activation after convolution—both absent from attention layers and hypothesized to hurt alignment. Additionally, projecting matrix A directly from input removes the Δ discretization parameter, better matching discrete attention. The core assumption is that architectural symmetry between teacher and student sequence mixers reduces alignment friction.

### Mechanism 3
Constant state size in recurrent layers enables higher throughput and larger batch sizes than attention-based models at inference. Mamba-2's recurrent state has fixed dimension regardless of sequence length, avoiding KV cache growth. Combined with fewer temporal mixing layers (MLPs are stateless), this allows batching to 2× larger sizes than pure Mamba-2 and consistent throughput as context grows. The core assumption is that memory-bound inference workloads dominate real-world deployment constraints.

## Foundational Learning

- Concept: State Space Models (SSMs) as sequence mixers
  - Why needed here: Llamba replaces attention with Mamba-2 layers; understanding how SSMs map input sequences through learned state dynamics is essential for debugging alignment failures.
  - Quick check question: Can you explain why an SSM has O(1) memory per token at inference while attention has O(n)?

- Concept: Knowledge distillation (logit matching vs. feature matching)
  - Why needed here: MOHAWK uses both hidden-state alignment (feature-level) and cross-entropy on logits (output-level); knowing when each applies prevents misdiagnosing training issues.
  - Quick check question: What would happen if you applied logit distillation without any hidden-state alignment?

- Concept: Grouped-Query Attention (GQA) vs. Multi-Head SSMs
  - Why needed here: The teacher uses GQA with shared KV weights; Llamba uses independent heads. This asymmetry affects how Matrix Orientation learns untied weights.
  - Quick check question: Why might independent heads in the student be beneficial even when the teacher shares weights?

## Architecture Onboarding

- Component map: Input → Embedding (copied from Llama) → ×N Blocks: [RMSNorm → Discrete Mamba-2 (32 heads, state size 64) → Residual] → [RMSNorm → Gated MLP → Residual] → Output head (copied from Llama)

- Critical path:
  1. Initialize: Set conv layer to identity kernel, skip connection to pass input unchanged (block = identity)
  2. Matrix Orientation (~0.5B tokens): Align student matrix mixer to teacher attention
  3. Hidden-State Alignment (4-5B tokens): Per-block L2 alignment
  4. Weight Transfer: Copy MLP/norm/embed/head weights (unfrozen)
  5. Knowledge Distillation (5-6.5B tokens): Cross-entropy on logits, then additional tokens from Llama-70B

- Design tradeoffs:
  - Multi-head (32) vs. MVA structure: Independent heads improve alignment flexibility but may increase computation
  - Freezing vs. optimizing MLP weights: Paper unfreezes for better adaptation; freezing would be faster but may cap performance
  - Dataset quality vs. quantity: fineweb-edu-4.0 improves MMLU significantly over C4; suggests data curation matters more for distillation than pretraining

- Failure signatures:
  - MMLU underperforms other benchmarks → likely insufficient alignment tokens or weak data quality
  - Alignment loss plateaus early → check head initialization or learning rate schedule
  - OOM at large batch sizes during training → verify FSDP + activation checkpointing enabled

- First 3 experiments:
  1. Ablate dataset: Train Llamba-8B stage 3 on C4 vs. fineweb-edu; expect MMLU gap ~10+ points (per Figure 3)
  2. Ablate MLP freezing: Compare frozen vs. unfrozen MLP weights during distillation; measure speed vs. accuracy tradeoff
  3. Profile inference: Benchmark Llamba-8B vs. Llama-3.1-8B on Apple Silicon at 4-bit quantization across context lengths 512-8192; verify constant throughput claim

## Open Questions the Paper Calls Out

### Open Question 1
Can recurrent distilled models like Llamba close the performance gap with Transformers on algorithmic tasks requiring precise copying and in-context retrieval? The paper states in Related Work that "SSM-based models still underperform Transformers on algorithmic tasks" and cites multiple works, but Llamba is not evaluated on such tasks. Systematic evaluation on algorithmic benchmarks would resolve this.

### Open Question 2
What are the optimal data composition and quality thresholds for cross-architecture distillation to maximize knowledge transfer efficiency? The authors hypothesize that "distillation results could be improved further by incorporating even higher-quality datasets" and show that fineweb-edu specifically improves MMLU scores. Ablation studies varying dataset quality, diversity, and domain composition during distillation would clarify this.

### Open Question 3
How can long-context handling be optimized in distilled recurrent models without sacrificing the efficiency gains or benchmark performance? The conclusion lists "optimizing the handling of long contexts" as a future direction without specifying current limitations or proposed solutions. Evaluation on long-context benchmarks and analysis of state utilization across sequence lengths would help resolve this.

## Limitations
- Cross-architecture alignment mechanism incompletely specified with external references for critical Matrix Orientation and Hidden-State Alignment phases
- Data efficiency claims rely on token counts but lack detailed validation of the exact distillation mechanisms
- On-device performance benchmarks lack details on quantization strategies and memory management

## Confidence
**High Confidence (8/10)**:
- Llamba models achieve competitive benchmark performance (68.8% average accuracy for Llamba-8B vs 69.4% for teacher)
- Mamba-based models process larger batch sizes without memory overflow compared to Transformers
- Distillation process using MOHAWK framework is reproducible with provided token budgets and training procedures

**Medium Confidence (6/10)**:
- 0.1% data efficiency claim - supported by token counts but mechanism incompletely specified
- Constant state size enabling consistent throughput - benchmarked on limited hardware
- Discrete Mamba-2 modifications improving alignment - based on architectural reasoning with limited ablation

**Low Confidence (4/10)**:
- Specific dataset quality impact (fineweb-edu-4.0 vs C4) on MMLU performance - only briefly mentioned
- Exact contribution of each MOHAWK phase to final performance - not individually quantified

## Next Checks
1. **Ablation of Mamba-2 Modifications**: Train Llamba-8B with standard Mamba-2 (including normalization and activation layers) versus the proposed Discrete Mamba-2. Measure both alignment quality during distillation and final benchmark performance to isolate the impact of these architectural changes.

2. **Dataset Quality Validation**: Conduct controlled experiments training Llamba-8B stage 3 on C4 versus fineweb-edu-4.0 with identical hyperparameters. Specifically measure MMLU performance delta and analyze whether the 10+ point gap observed in Figure 3 is consistent across multiple runs.

3. **Hardware Generalization Test**: Reproduce the Apple Silicon inference benchmarks on M3 Pro, M3 Max, and M4 Pro chips at 4-bit quantization across context lengths 512-8192. Verify whether the constant throughput claim holds across different hardware generations and whether throughput scaling follows the same pattern.