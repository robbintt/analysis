---
ver: rpa2
title: 'Benchmarking Text-to-Python against Text-to-SQL: The Impact of Explicit Logic
  and Ambiguity'
arxiv_id: '2601.15728'
source_url: https://arxiv.org/abs/2601.15728
tags:
- logic
- code
- arxiv
- data
- table
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper evaluates whether Text-to-Python can serve as a flexible
  alternative to Text-to-SQL for data querying and analytics. While SQL offers declarative
  expressiveness through implicit database behaviors, Python requires explicit procedural
  logic, making it more sensitive to ambiguous natural language inputs.
---

# Benchmarking Text-to-Python against Text-to-SQL: The Impact of Explicit Logic and Ambiguity

## Quick Facts
- arXiv ID: 2601.15728
- Source URL: https://arxiv.org/abs/2601.15728
- Authors: Hangle Hu; Chenyu Hou; Bin Cao; Ruizhe Li
- Reference count: 32
- Primary result: Text-to-Python achieves performance parity with Text-to-SQL when ambiguity is resolved through logical specification.

## Executive Summary
This paper evaluates whether Text-to-Python can serve as a flexible alternative to Text-to-SQL for data querying and analytics. While SQL offers declarative expressiveness through implicit database behaviors, Python requires explicit procedural logic, making it more sensitive to ambiguous natural language inputs. To address this, the authors introduce BIRD-Python, a benchmark for cross-paradigm evaluation, and the Logic Completion Framework (LCF), which supplements missing domain knowledge to resolve ambiguity. Experiments show that while Text-to-Python performs comparably to Text-to-SQL given sufficient context, its reliance on explicit logic makes it vulnerable to underspecified user intent. When LCF is applied, accuracy improves significantly in both paradigms, demonstrating that knowledge grounding—not paradigm choice—is the primary bottleneck. These findings establish Python as a viable foundation for analytical agents when ambiguity is effectively resolved through logical specification.

## Method Summary
The authors created BIRD-Python, a benchmark dataset with verified natural language questions, database schemas, and ground-truth code in both SQL and Python. They developed the Logic Completion Framework (LCF) with three phases: Subject models identify ambiguities in queries, an Oracle provides natural language hints using ground truth, and Subject models generate final code. The Structure-Agnostic Validator (Vsem) uses LLM-based semantic evaluation to assess output equivalence beyond syntactic matching. Experiments were conducted across multiple model sizes (Qwen2.5-Coder, Qwen3, DeepSeek-R1, StarCoder2, AutoCoder, TableGPT2) on a 6× NVIDIA A6000 infrastructure.

## Key Results
- Python shows higher logic error rates (17.5%) than SQL (0.3%) due to explicit procedural requirements
- LCF improves accuracy for both paradigms, narrowing the performance gap between SQL and Python
- Knowledge grounding resolves ambiguity as the primary bottleneck, not paradigm choice
- Semantic validation through Vsem provides more reliable evaluation than exact execution matching

## Why This Works (Mechanism)

### Mechanism 1
Knowledge grounding via logical specification can resolve ambiguity to achieve performance parity between procedural (Python) and declarative (SQL) query generation. The Logic Completion Framework (LCF) decouples reasoning capability from information deficits. First, a "Subject" model identifies ambiguous or missing specifications in a user query relative to a schema. Then, an Oracle model (with ground-truth access) provides precise, implementation-agnostic natural language hints that clarify business logic or constraints. Finally, the Subject generates code with this enriched context.

Core assumption: The primary bottleneck for code generation performance is missing domain context in the natural language query, not inherent limitations in the model's code generation abilities. Also assumes the Oracle can provide accurate, non-leaking hints based on the ground truth.

Evidence anchors:
- [abstract] "Experimental results show that... (2) when these gaps are addressed, Text-to-Python achieves performance parity with Text-to-SQL."
- [section] Table 2 and Section 5.4.1 show the performance gap between SQL and Python narrows significantly under LCF (e.g., Qwen3-32B: 60.10% vs 61.80% in Standard to 72.49% vs 72.75% with LCF).
- [corpus] The paper "AmbiSQL: Interactive Ambiguity Detection and Resolution for Text-to-SQL" (arXiv:2508.15276) identifies query ambiguity as a major obstacle for LLM-based Text-to-SQL, supporting the premise that ambiguity resolution is critical.

Break condition: The mechanism breaks if the Oracle provides misleading hints, if the Subject model cannot reliably identify its own ambiguities, or if the core bottleneck is something other than information deficit (e.g., fundamental reasoning inability).

### Mechanism 2
Procedural languages like Python are more sensitive to underspecified intent than declarative languages like SQL because they require explicit logic. SQL's declarative nature allows the underlying database engine to handle implicit behaviors (e.g., NULL handling, sort stability). In contrast, Python's procedural nature requires the user or model to explicitly define every step of the execution logic. Underspecified intent (e.g., handling of division by zero, sort order of ties) leads to a greater variety of valid-but-incorrect implementations in Python than in SQL.

Core assumption: The difference in error rates stems from the paradigm's requirements for explicit logic, not simply from model over-training on SQL.

Evidence anchors:
- [abstract] "...Python requires explicit procedural logic, making it highly sensitive to underspecified user intent."
- [section] Figure 6 and Section 5.3.1: Python shows a much higher rate of "Logic Errors" (17.5%) compared to SQL (0.3%), attributed to the need for explicit implementation of data manipulation steps.
- [corpus] Corpus signals weak here; most related work focuses on ambiguity in SQL, not the Python comparison. No direct contradiction found.

Break condition: This mechanism breaks if future Python libraries or AI assistants successfully abstract away procedural details to match SQL's implicit behavior, or if models become equally proficient at inferring all necessary procedural steps.

### Mechanism 3
Execution-based evaluation metrics conflate reasoning failure with ambiguity in user intent, requiring semantic validation. Standard Execution Accuracy (EX) uses exact matches between execution results. This produces false negatives for Text-to-Python because semantically correct outputs can differ superficially (column order, data types). An LLM-based semantic validator can normalize outputs and assess true logical equivalence, providing a more accurate measure of model reasoning capability.

Core assumption: An LLM can reliably judge semantic equivalence between two program outputs, and human verification of a sample validates this approach.

Evidence anchors:
- [abstract] Implied by the creation of BIRD-Python to "align execution semantics" and the need for a "consistent and standardized baseline."
- [section] Section 3.2.3 describes the Structure-Agnostic Validator (Vsem) and a human audit (N=600) that confirmed its reliability. Appendix C (Table 11) details the specific normalization rules.
- [corpus] "Query Carefully: Detecting the Unanswerables in Text-to-SQL Tasks" (arXiv:2512.21345) discusses risks of misinterpreting outputs, indirectly supporting the need for more nuanced evaluation.

Break condition: The mechanism breaks if the LLM validator introduces its own biases or inconsistencies in judgment, or if it fails to capture subtle but critical semantic differences.

## Foundational Learning

**Concept: Declarative vs. Procedural Semantics**
Why needed here: This distinction is central to the paper's thesis. You must understand that SQL tells the database *what* to do (declarative), relying on engine defaults, while Python tells the interpreter *how* to do it (procedural), requiring explicit instruction for every edge case.
Quick check question: For a query `SELECT AVG(score) FROM students`, what implicit behavior does the SQL engine handle that a Python implementation of the same logic would need to explicitly code for?

**Concept: Knowledge Grounding / Contextual Ambiguity**
Why needed here: The paper argues that the main performance limiter is not code generation, but missing domain knowledge. You need to grasp how underspecified natural language ("top schools") fails to translate into precise code without external context ("top" defined by what metric?).
Quick check question: A user asks for "recent high-value transactions." List three types of "latent domain knowledge" an LCF-style Oracle would need to provide to make this query executable.

**Concept: LLM-as-a-Judge for Semantic Evaluation**
Why needed here: The paper uses an LLM to evaluate another LLM's output. Understanding this meta-evaluation technique is key to comprehending how they measured success beyond simple string matching.
Quick check question: What are the core rules the paper's "Structure-Agnostic Validator" uses to determine if two outputs are equivalent? (e.g., does column order matter?)

## Architecture Onboarding

**Component map:**
BIRD-Python Benchmark -> Target LLM (Subject) -> Logic Completion Framework (LCF) -> Structure-Agnostic Validator (Vsem)

**Critical path:**
The primary critical path for onboarding is **LCF Integration**. You must understand how to implement the three-phase dialogue and, more importantly, how to replace the ground-truth-dependent "Oracle" with a production-ready alternative (e.g., a human-in-the-loop system or a domain knowledge base).

**Design tradeoffs:**
- **Ground-Truth Oracle vs. Human-in-the-Loop**: The paper uses an Oracle with ground truth for controlled experiments. In production, this must be replaced. The tradeoff is between latency/cost (human expert) and potential error (automated knowledge base).
- **Text-to-Python vs. Text-to-SQL**: Python offers more flexibility for complex analytics and file-based data but requires more explicit logic and is thus more brittle with ambiguous queries. SQL is more robust for structured database queries but less expressive for complex workflows.

**Failure signatures:**
- **Logic Errors in Python**: High rate of errors related to NULL handling, sort stability, or type casting, which are implicitly handled in SQL. This indicates underspecification.
- **Filter Condition Errors**: Dominant in both SQL and Python, indicating a failure in natural language understanding or ambiguous user intent.
- **Semantic Mismatch**: Generated code is syntactically correct but produces a semantically different result due to an incorrect assumption (e.g., granularity mismatch).

**First 3 experiments:**
1. **Sanity Check with BIRD-Python Baseline**: Run a state-of-the-art LLM (e.g., Qwen3-7B or similar) on a subset of the verified BIRD-Python benchmark using the provided prompt templates. Report Execution Accuracy (EX) and error categories to establish a baseline.
2. **LCF Component Analysis**: Implement the LCF pipeline. First, evaluate the "Subject" model's ability to correctly identify ambiguity (Phase 1) by manually reviewing its generated questions. Then, simulate Phase 2 by manually providing hints and measuring the improvement in Phase 3 code generation accuracy.
3. **Validator Calibration**: Implement the Structure-Agnostic Validator (Vsem). Run it on the outputs from Experiment 1 and compare its judgments against human evaluation on a sample to validate its reliability and calibrate its rules.

## Open Questions the Paper Calls Out

**Open Question 1**
Can the Logic Completion Framework achieve comparable performance improvements without access to ground truth, using only human-in-the-loop feedback or automated knowledge retrieval?
Basis in paper: [explicit] "LCF utilizes an Oracle with access to ground truth. While useful for diagnosis, this reliance assumes high-quality annotations; practical deployments would likely depend on human feedback, introducing latency not modeled here."
Why unresolved: The paper demonstrates LCF's diagnostic value but does not evaluate realistic deployment scenarios where ground truth is unavailable.
What evidence would resolve it: Empirical comparison of LCF performance using human annotators versus oracle access, measuring both accuracy and interaction latency.

**Open Question 2**
How does Text-to-Python performance degrade when models cannot access explicit schema information and must operate on raw, schema-less files?
Basis in paper: [explicit] "We provided explicit DDL schemas to Text-to-Python models to ensure fair comparison. While this isolates procedural reasoning, it may yield higher performance than scenarios involving raw, schema-less files."
Why unresolved: The controlled experimental setup ensures comparability but diverges from real-world file-based analytics where schemas must be inferred.
What evidence would resolve it: Evaluation on BIRD-Python without schema hints, measuring performance gap and error type distribution changes.

**Open Question 3**
How do data quality issues (type inference errors, missing values, format inconsistencies) interact with ambiguity in natural language queries to affect Text-to-Python vs. Text-to-SQL performance?
Basis in paper: [explicit] "The evaluation focuses on logical complexity using clean data. Consequently, it does not fully address issues related to data quality, such as type inference noise common in production. Future work could examine the interaction between data quality and intent ambiguity."
Why unresolved: The study isolates logical complexity but production environments combine dirty data with ambiguous queries.
What evidence would resolve it: Controlled experiments varying data quality and query ambiguity levels factorially, analyzing error patterns across paradigms.

## Limitations

- The LCF framework relies on an Oracle with ground-truth access, which is not feasible in production environments
- The study uses clean data and explicit schemas, not reflecting real-world data quality and schema inference challenges
- The 600-sample human audit provides limited statistical power for validating the semantic equivalence evaluation

## Confidence

- **High Confidence**: The demonstration that procedural languages require more explicit specification than declarative ones is well-supported by error analysis showing Python's higher logic error rates (17.5% vs 0.3% for SQL). The semantic equivalence evaluation approach is technically sound and the BIRD-Python dataset creation methodology is clearly specified.
- **Medium Confidence**: The claim that knowledge grounding resolves the performance gap between paradigms is supported by experimental results but relies heavily on the Oracle's ground-truth access. The conclusion that Python is "viable" for analytical agents assumes successful replacement of the Oracle component.
- **Low Confidence**: The assertion that Python "remains promising for data analytics" overstates the practical implications given the significant infrastructure requirements for ambiguity resolution.

## Next Checks

1. Implement a human-in-the-loop Oracle replacement and measure performance degradation compared to the ground-truth Oracle to assess practical viability.
2. Conduct a larger-scale validation (N>1000) of the Structure-Agnostic Validator's reliability across diverse query types and data distributions.
3. Evaluate Text-to-Python performance on file-based data workflows that SQL cannot express, testing the claim of Python's superior flexibility for analytical tasks.