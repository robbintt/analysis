---
ver: rpa2
title: 'Believing without Seeing: Quality Scores for Contextualizing Vision-Language
  Model Explanations'
arxiv_id: '2509.25844'
source_url: https://arxiv.org/abs/2509.25844
tags:
- explanation
- quality
- visual
- answer
- user
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This work introduces two novel quality scoring functions for evaluating
  VLM-generated explanations: Visual Fidelity and Contrastiveness. Visual Fidelity
  measures how faithfully an explanation reflects the visual context, while Contrastiveness
  evaluates how well the explanation rules out alternative answers.'
---

# Believing without Seeing: Quality Scores for Contextualizing Vision-Language Model Explanations

## Quick Facts
- arXiv ID: 2509.25844
- Source URL: https://arxiv.org/abs/2509.25844
- Authors: Keyu He; Tejas Srinivasan; Brihi Joshi; Xiang Ren; Jesse Thomason; Swabha Swayamdipta
- Reference count: 40
- Primary result: Two novel quality scores (Visual Fidelity, Contrastiveness) better calibrate user trust in VLM explanations than existing metrics

## Executive Summary
This work introduces two quality scoring functions for evaluating VLM-generated explanations: Visual Fidelity and Contrastiveness. Visual Fidelity measures how faithfully an explanation reflects the visual context, while Contrastiveness evaluates how well the explanation rules out alternative answers. Both are shown to be better calibrated with model correctness than existing explanation quality metrics. A user study demonstrates that presenting these quality scores alongside VLM explanations improves user accuracy in predicting VLM correctness by 11.1%, with a 15.4% reduction in false belief of incorrect predictions. The results highlight the importance of explanation quality evaluation in building appropriate user trust in VLM predictions.

## Method Summary
The authors propose a training-free approach to evaluate VLM explanation quality. Visual Fidelity works by decomposing explanations into atomic visual claims, generating yes/no verification questions for each, and scoring based on verifier agreement with the image. Contrastiveness masks answer mentions, converts Q-A pairs to declarative hypotheses, and computes relative entailment probabilities using an NLI model. The scores are combined (product of VF × Contrastiveness) and displayed to users alongside explanations. The method is evaluated on A-OKVQA, VizWiz, and MMMU-Pro datasets with multiple VLM models including LLaVA-v1.5-7B, Qwen2.5-VL-7B, and GPT-4o.

## Key Results
- Visual Fidelity and Contrastiveness achieve lower Expected Calibration Error than baseline metrics across all tested VLMs
- Displaying quality scores improves user accuracy in judging VLM predictions by 11.1% and reduces over-reliance by 15.4%
- Descriptive quality presentations slightly outperform numeric presentations in reducing over-reliance
- GPT-4o verifier achieves highest discriminability (0.109) compared to smaller models

## Why This Works (Mechanism)

### Mechanism 1: Atomic Visual Verification
- Claim: Decomposing VLM explanations into atomic visual claims and verifying each against the image exposes hallucinations that mislead users.
- Mechanism: An LLM generates yes/no verification questions from explanation sentences (e.g., "Is the clock on the building?"); a verifier VLM answers each. The Visual Fidelity score = fraction of "yes" answers. Low scores flag explanations containing fabricated details.
- Core assumption: Hallucinated details are detectable via atomic visual verification questions rather than holistic judgments.
- Evidence anchors:
  - [abstract] "Visual Fidelity captures how faithful an explanation is to the visual context"
  - [section] Algorithm 1 shows decomposition→verification→scoring pipeline
  - [corpus] "Explanation-Driven Counterfactual Testing for Faithfulness in VLM Explanations" (arXiv:2510.00047) similarly decomposes explanations to test faithfulness
- Break condition: Verifier VLM itself hallucinates or fails on ambiguous visual queries; evaluator-verified accuracy was 95% (Section J.2), but degradation possible on low-quality images.

### Mechanism 2: Discriminative Explanation Scoring
- Claim: Explanations that fail to distinguish the predicted answer from alternatives indicate incomplete reasoning.
- Mechanism: Mask answer mentions in the explanation; compute NLI entailment probability for each answer option. Contrastiveness = relative entailment for predicted answer vs sum over all options. High scores indicate the explanation provides discriminating evidence.
- Core assumption: Good explanations should entail the correct answer more strongly than alternatives; weak differentiation signals reasoning gaps.
- Evidence anchors:
  - [abstract] "Contrastiveness captures how well the explanation identifies visual details that distinguish the model's prediction from plausible alternatives"
  - [section] Algorithm 2 formalizes the relative entailment computation
  - [corpus] Limited direct corpus support for contrastiveness as defined here; closest is "Learning to Reject Low-Quality Explanations via User Feedback" (arXiv:2507.12900) on explanation quality signals
- Break condition: Open-ended tasks without predefined answer sets require generating hard negatives; Section F shows generated negatives increase difficulty and reduce discriminability by ~0.13.

### Mechanism 3: Combined Quality Calibration
- Claim: Combined quality scores calibrate user trust by providing evidence-anchored reliability signals.
- Mechanism: Product of Visual Fidelity × Contrastiveness yields single score; users shown this alongside explanations improve accuracy by 11.1% and reduce false beliefs by 15.4%. ECE explains ~60% of variance in user accuracy (Figure 8).
- Core assumption: Users attend to quality scores and adjust reliance accordingly; scores reflect actual explanation reliability.
- Evidence anchors:
  - [abstract] "displaying these quality scores alongside explanations improves users' accuracy in judging VLM predictions by 11.1%"
  - [section] Figure 4 shows over-reliance reduction across conditions
  - [corpus] "Seeing but Not Believing" (arXiv:2510.17771) documents disconnect between visual attention and correctness in VLMs, supporting need for external trust signals
- Break condition: Users over-rely on scores themselves; descriptive presentations slightly outperform numeric (Section 4.3.2), suggesting presentation matters.

## Foundational Learning

- Concept: **Expected Calibration Error (ECE)**
  - Why needed here: ECE measures how well quality scores predict actual correctness; lower ECE = better-calibrated trust signal. The paper uses ECE to compare explanation metrics (Table 2).
  - Quick check question: If a quality score of 0.8 corresponds to 60% actual accuracy, is the score well-calibrated?

- Concept: **Entailment and NLI (Natural Language Inference)**
  - Why needed here: Contrastiveness relies on NLI models to measure whether explanations entail answer hypotheses. Understanding entailment direction matters for interpreting scores.
  - Quick check question: If explanation P="The clock shows 3pm" and hypothesis H="It is afternoon", does P entail H?

- Concept: **Hallucination in Vision-Language Models**
  - Why needed here: Visual Fidelity directly targets VLM hallucination—confidently stated but visually absent details. Recognizing hallucination patterns helps interpret low VF scores.
  - Quick check question: A VLM claims "shadows suggest noon" but the image shows artificial lighting—is this a hallucination or reasoning error?

## Architecture Onboarding

- Component map: Explanation → Question Generator (m_QGen) → Verifier (m_Verif) → Visual Fidelity score; Explanation → Masker/Paraphraser → Entailment Model → Contrastiveness score
- Critical path: Explanation → [VF: Decompose→Verify→Score] + [Contr: Mask→Entail→Relative Score] → Combine → Display to user
- Design tradeoffs:
  - GPT-4o verifier vs smaller models: GPT-4o achieves higher discriminability (0.109 vs 0.061–0.154 average) but costs more (Table 15)
  - Numeric vs descriptive presentation: Descriptive slightly reduces over-reliance but increases UI complexity (Figure 7)
  - Product vs average vs min combination: Product achieves best discriminability (0.161 overall) but is more sensitive to low scores in either dimension (Table 1)
- Failure signatures:
  - Negative discriminability on VizWiz with GPT-4o: Model gives valid but non-ground-truth answers; VF scores remain high, inverting the signal (Section I)
  - Generated hard negatives too easy/hard: Affects Contrastiveness validity; validate semantic similarity (Section G)
  - Verifier errors compound: 5% error rate in verification (Section J.2); cascade affects final scores
- First 3 experiments:
  1. **Verifier swap test**: Run VF computation with GPT-4o, Gemma, and Qwen on 100 samples; compare discriminability and ECE. Target: <0.03 ECE difference.
  2. **Hard negative quality check**: For open-ended tasks, compute cosine similarity between generated negatives and correct answers; filter if similarity >0.7 (too hard) or <0.3 (too easy).
  3. **User-facing A/B test**: Compare numeric vs descriptive quality presentation on 50 users × 20 questions; measure over-reliance and task completion time.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can adaptive human-AI reliance strategies learn when to present or suppress explanation quality scores, potentially using bandit algorithms for system-side abstention?
- Basis in paper: [explicit] In the conclusion, the authors state "An important direction is to develop adaptive human-AI reliance strategies that learn when to present explanations and quality scores and when to suppress them, potentially utilizing bandit algorithms to automate system-side abstention."
- Why unresolved: The current work always displays quality scores alongside explanations; no adaptive mechanism has been explored to determine optimal presentation timing.
- What evidence would resolve it: A user study comparing static presentation vs. adaptive strategies (e.g., multi-armed bandits) that decide when to show or withhold scores, measuring user accuracy and over-reliance over time.

### Open Question 2
- Question: Can Visual Fidelity and Contrastiveness be used as training objectives to improve VLM explanation generation?
- Basis in paper: [explicit] The conclusion explicitly proposes "explanation quality scores could be used as training objectives to improve the generation of explanations."
- Why unresolved: The current work uses these metrics only for post-hoc evaluation; their utility as training signals for VLMs remains unexplored.
- What evidence would resolve it: Experiments fine-tuning VLMs with VF and Contrastiveness as auxiliary losses, comparing generated explanation quality and downstream task performance against baseline models.

### Open Question 3
- Question: How does explanation quality influence user trust calibration over repeated interactions with VLM systems?
- Basis in paper: [explicit] The authors note "future works should study how explanation quality influences user trust over time."
- Why unresolved: The current user study is single-session; longitudinal effects of quality score exposure on trust dynamics are unknown.
- What evidence would resolve it: A multi-session longitudinal study tracking how users' reliance patterns evolve with consistent exposure to quality-calibrated vs. uncalibrated explanations.

### Open Question 4
- Question: Do Visual Fidelity and Contrastiveness quality metrics generalize to multilingual VLM explanations and non-English user populations?
- Basis in paper: [explicit] In the limitations section, authors state "we only evaluate on English-language datasets and conduct user studies with fluent English speakers. Future work should explore multilingual generalization."
- Why unresolved: The scoring functions and user studies were confined to English; cross-linguistic validity is untested.
- What evidence would resolve it: Evaluation of VF and Contrastiveness on multilingual VQA benchmarks, plus user studies with non-English speaking participants to assess cross-lingual calibration and utility.

## Limitations
- The core assumption that atomic visual verification questions can reliably detect hallucinations may not hold when the verifier model itself introduces errors (5% verification error rate reported)
- VizWiz results show negative discriminability with GPT-4o, suggesting the narrow ground truth labeling approach may not capture valid but non-standard answers
- The method is evaluated only on English-language datasets with fluent English speakers, limiting cross-linguistic generalizability

## Confidence
- High confidence: Visual Fidelity and Contrastiveness achieve better ECE than baseline metrics (Section 4.2, Table 2)
- Medium confidence: User study improvements (11.1% accuracy gain, 15.4% reduction in false beliefs) generalize beyond the specific 100-question subsets tested
- Low confidence: The claim that descriptive quality presentations are universally better than numeric; slight preference observed but context-dependent

## Next Checks
1. **Verifier error cascade test**: Systematically measure how verifier errors (5% baseline) compound through the VF pipeline by comparing human-verified VF scores vs automated VF scores on 100 samples
2. **NLI model sensitivity analysis**: Test Contrastiveness with multiple NLI models (including different sizes/specs) to determine if entailment model choice drives metric instability
3. **Cross-dataset calibration validation**: Compute ECE for both metrics on held-out test sets from A-OKVQA, VizWiz, and MMMU-Pro to verify consistent calibration across domains