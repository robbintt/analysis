---
ver: rpa2
title: 'Provable Post-Training Quantization: Theoretical Analysis of OPTQ and Qronos'
arxiv_id: '2508.04853'
source_url: https://arxiv.org/abs/2508.04853
tags:
- optq
- quantization
- error
- then
- qronos
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper presents the first quantitative theoretical analysis\
  \ of OPTQ, a widely used post-training quantization algorithm. The authors derive\
  \ non-asymptotic \u21132 error bounds for both deterministic and stochastic variants\
  \ of OPTQ, as well as for Qronos, a recent state-of-the-art PTQ algorithm."
---

# Provable Post-Training Quantization: Theoretical Analysis of OPTQ and Qronos

## Quick Facts
- arXiv ID: 2508.04853
- Source URL: https://arxiv.org/abs/2508.04853
- Authors: Haoyu Zhang; Shihao Zhang; Ian Colbert; Rayan Saab
- Reference count: 40
- Primary result: First quantitative theoretical analysis of OPTQ, deriving non-asymptotic ℓ2 error bounds for both deterministic and stochastic variants, plus bounds for Qronos

## Executive Summary
This paper presents the first rigorous theoretical analysis of OPTQ, a widely used post-training quantization algorithm, providing non-asymptotic error bounds for both deterministic and stochastic variants. The authors derive ℓ2 reconstruction error bounds that depend explicitly on the calibration data matrix and regularization parameter, while the stochastic variant achieves stronger ℓ∞ bounds useful for controlling alphabet size requirements. The analysis extends to Qronos, a recent state-of-the-art PTQ algorithm, explaining its empirical advantages through a refined error decomposition. The key insight is that OPTQ's iterative procedure induces quantization error that can be characterized and bounded, with the stochastic variant offering finer control over entry-wise errors and alphabet size requirements.

## Method Summary
The paper analyzes post-training quantization by deriving non-asymptotic error bounds for OPTQ and Qronos algorithms. OPTQ uses a layer-wise sequential quantization procedure via Cholesky decomposition of the regularized Hessian H^(-1) = (X^T X + λI)^(-1) = LL^T, iteratively quantizing weight coordinates and updating remaining weights to compensate for quantization error. The deterministic variant minimizes ℓ2 reconstruction error ||Xw - Xq||_2, while the stochastic variant uses unbiased random rounding to achieve ℓ∞ bounds scaling with √(log N). Qronos improves upon OPTQ by minimizing reconstruction error with respect to quantized activations rather than original data. The theoretical framework relies on orthogonal projection analysis and convex ordering dominance to establish the error bounds.

## Key Results
- Derives non-asymptotic ℓ2 error bounds for deterministic OPTQ: ||Xw - Xq||_2 ≤ √(N) · (δ/2) · C_2(X, λ)
- Proves stochastic OPTQ achieves ℓ∞ bounds: ||Xw - Xq||_∞ ≤ O(√(log N)) with high probability
- Provides new theoretical bounds for Qronos, showing improved performance through reduced initial error terms
- Establishes theoretical justification for practical design choices like ordering features by decreasing norm
- Demonstrates that regularization parameter λ critically affects both stability and bound tightness

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Iterative error compensation via orthogonal projection bounds the reconstruction error in OPTQ.
- **Mechanism:** OPTQ quantizes weights sequentially. After quantizing a weight coordinate q_t, the algorithm updates the remaining unquantized weights to minimize the resulting reconstruction error. This update is mathematically equivalent to projecting the quantization error of the current step onto the orthogonal complement of the remaining feature columns (P_{X_{≥t+1}^⊥}). The final error is the sum of these projected errors, which are mutually orthogonal, preventing error accumulation from exploding in a single direction.
- **Core assumption:** The calibration data matrix X has full column rank (or regularization λ > 0 is used) to ensure the projection is well-defined.
- **Evidence anchors:** [Proposition 3.2] derives the error evolution e_N = Σ P_{X_{≥j+1}^⊥}(w_j^{(j-1)} - q_j)X_j. [Theorem 3.3] provides the explicit ℓ2 bound based on this projection sequence. [corpus] Paper 83452 ("Quantization Error Propagation") supports the significance of analyzing layer-wise error dynamics.
- **Break condition:** If the calibration data X is rank-deficient without sufficient regularization (λ = 0), the inverse Hessian explodes, and the projection-based update fails to stabilize the error.

### Mechanism 2
- **Claim:** Stochastic rounding transforms the error distribution to enable tight ℓ∞ (entry-wise) control.
- **Mechanism:** Deterministic rounding bounds the maximum entry error by O(√N), which is too loose for controlling the quantization alphabet size. Replacing the deterministic quantizer with an unbiased stochastic one (Q_{stoc}) allows the authors to model the final error vector as a random variable dominated by a Gaussian in the convex order. This probabilistic structure allows the error to be bounded by O(√(log N)) with high probability, significantly reducing the required dynamic range.
- **Core assumption:** The convex ordering dominance (Lemma 4.2) accurately captures the tail behavior of the iteratively constructed error vector.
- **Evidence anchors:** [Lemma 4.2] establishes Xw - Xq ≺_{cx} N(0, Σ). [Theorem 4.6] proves the ℓ∞ bound ≲ √(log N) for the stochastic variant. [corpus] No direct corpus neighbor specifically addresses stochastic rounding convex ordering in PTQ; this appears to be a specific theoretical contribution of this paper.
- **Break condition:** If the probability of failure tolerance ε is set extremely low (requiring extremely high confidence), the √(log(1/ε)) term may diminish the practical benefits over deterministic rounding.

### Mechanism 3
- **Claim:** Qronos improves performance by projecting the "calibration mismatch" error onto a restricted subspace.
- **Mechanism:** Standard OPTQ minimizes ||XW - XQ||, but in a real pipeline, the input to the next layer is the quantized activation X̃, not X. Qronos minimizes ||XW - X̃Q||. The analysis shows Qronos reduces the initial error term from ||Xw - X̃w||_2 (in OPTQ) to ||P_{X̃_{≥2}^⊥} P_{X̃_1^⊥}(Xw - X̃w)||_2. Successive projections effectively nullify the error components aligned with the span of the quantized inputs.
- **Core assumption:** The quantized activation matrix X̃ is in general position so that the projection subspaces are well-defined.
- **Evidence anchors:** [Lemma 5.1] compares the error terms, showing the extra projection operators in Qronos. [Proposition 5.3] explicitly bounds the error with this reduced initial term. [corpus] Paper 100773 (Qronos) validates the empirical advantage of this approach.
- **Break condition:** If the calibration data lies entirely in the null space of the projections, the mechanism offers no advantage over standard OPTQ.

## Foundational Learning

- **Concept:** **Orthogonal Projection (P_A and P_{A^⊥})**
  - **Why needed here:** The core operation of OPTQ involves projecting vectors onto the column space of the remaining features (or its orthogonal complement) to calculate optimal weight updates and error bounds.
  - **Quick check question:** If P_{X_{≥t+1}} projects onto the column space of future features, what does the error term P_{X_{≥t+1}^⊥} X_t represent physically in terms of feature correlation?

- **Concept:** **Convex Ordering (≺_{cx})**
  - **Why needed here:** This mathematical tool is essential to understand how the paper bounds the probability distribution of the error in the stochastic variant, allowing the derivation of ℓ∞ bounds from Gaussian tail properties.
  - **Quick check question:** If random vector X ≺_{cx} Y, does E[f(X)] ≤ E[f(Y)] hold for f(x) = -x? Why is this relevant to bounding tail probabilities?

- **Concept:** **Regularization in Inverse Hessians**
  - **Why needed here:** The paper explains that OPTQ implicitly uses H = X^TX + λI. Understanding how λ ensures invertibility and bounds the spectral norm of H^(-1) is critical for interpreting the error bounds (Theorem 3.3).
  - **Quick check question:** As λ → ∞, the bound in Corollary 3.5 approaches the MSQ bound. How does the algorithm's behavior change in this limit?

## Architecture Onboarding

- **Component map:** Pre-trained weights W → Calibration data X → Hessian computation H^(-1) = (X^T X + λI)^(-1) = LL^T → Iterative rounding engine (Algorithm 2.1) → Quantized weights Q
- **Critical path:** The *Ordering* of features (columns of X) by decreasing ℓ2 norm is identified as a critical pre-processing step (Remark 3.4). Incorrect ordering can worsen the condition number σ_{min} of sub-matrices, degrading the theoretical bound.
- **Design tradeoffs:**
  - **λ Selection:** A small λ yields tighter bounds but risks numerical instability if X is rank-deficient. A large λ degrades OPTQ to simple Memoryless Scalar Quantization (MSQ).
  - **Stochastic vs. Deterministic:** Stochastic rounding guarantees small entry-wise errors (better for nonlinearities like softmax) but introduces variance; deterministic offers stable ℓ2 reconstruction but admits outlier spikes.
- **Failure signatures:**
  - **Exploding Weights:** If C_∞(X, λ) is large (due to small singular values in X_{≥j+1}), the quantized weights q may drift significantly outside the finite alphabet, causing overflow (Appendix D).
  - **Ranking Flip:** In LLM output layers, deterministic OPTQ might preserve ℓ2 error but cause a large coordinate error that flips the ordering of top logits (Section 4 intro).
- **First 3 experiments:**
  1. **Validate Norm Ordering:** Run OPTQ on a layer with natural column order vs. decreasing norm order. Measure ||Xw - Xq||_2 and compare the "Alpha" term (max σ_{min}) derived in the paper.
  2. **Stochastic Capacity Test:** Implement OPTQ with Q_{stoc} on a classification head. Verify if the ℓ∞ error remains within δ√(log N) while monitoring the "Top-1" accuracy stability compared to the deterministic version.
  3. **Lambda Sweep:** Plot the error bound √(N)·(δ/2)·C_2(X, λ) vs. actual error while varying λ from 10^(-5) to 10^(-1) to empirically find the stability threshold described in Remark 3.7.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can a deterministic variant of OPTQ be designed that achieves ℓ∞ error bounds scaling with √(log N) rather than √(N), matching the performance of the stochastic variant?
- **Basis in paper:** [explicit] Appendix D constructs an adversarial example where deterministic OPTQ achieves ||Xw - Xq||_∞ = O(√(N)), whereas Theorem 4.6 proves the stochastic variant achieves O(√(log N)).
- **Why unresolved:** The authors prove that stochastic rounding closes this gap but do not determine if the √(N) dependence is a fundamental barrier for all deterministic, greedy quantization algorithms or merely a limitation of the current OPTQ formulation.
- **What evidence would resolve it:** A proof establishing ℓ∞ bounds of O(√(log N)) for a modified deterministic algorithm, or a formal lower bound proof demonstrating that √(N) is unavoidable for any deterministic method in this class.

### Open Question 2
- **Question:** Is there a rigorous, closed-form strategy for selecting the regularization parameter λ that minimizes generalization error, rather than relying on the heuristic of setting it as a small multiple of ||X||_F^2/N?
- **Basis in paper:** [explicit] Remark 3.7 states that while choosing λ ≈ 0.01 · ||X||_F^2/N aligns with recommendations, the justification "cannot be made fully rigorous."
- **Why unresolved:** The paper derives bounds dependent on λ (Corollary 3.5) and links regularization to generalization (Remark 3.8), but stops short of deriving the optimal λ that balances calibration error against weight perturbation for unseen data.
- **What evidence would resolve it:** A theoretical derivation of an optimal λ value based on the spectral properties of X and the expected distribution of test data, or extensive empirical analysis showing the current heuristic is suboptimal in specific low-rank or ill-conditioned regimes.

### Open Question 3
- **Question:** How do the layer-wise ℓ∞ error bounds derived for OPTQ and Qronos propagate and accumulate through the non-linear activation functions (e.g., softmax, ReLU) of a deep neural network?
- **Basis in paper:** [inferred] Section 4 motivates the need for ℓ∞ bounds to handle downstream layers and nonlinearities, but the theoretical analysis is restricted to a "single, generic layer" (Section 1.2) and does not model the transformation of this error into the input of subsequent layers.
- **Why unresolved:** While the paper bounds the error ||Xw - Xq||_∞, it does not characterize how this perturbation affects the next layer's activation matrix X̂^{[ℓ]}, which is crucial for understanding the end-to-end quantization robustness of the full network.
- **What evidence would resolve it:** A recursive error propagation theorem that bounds the distortion of the final network output based on the per-layer OPTQ error bounds derived in the paper.

## Limitations
- The ℓ2 error bounds depend heavily on spectral properties of calibration data submatrices, making them sensitive to dataset quality and preprocessing.
- The stochastic variant's ℓ∞ bounds rely on convex ordering dominance assumptions that may not capture all practical failure modes in deep architectures.
- The analysis assumes exact Cholesky decomposition and perfect quantization, potentially underestimating numerical errors in finite-precision implementations.

## Confidence
- **High:** The deterministic OPTQ ℓ2 error bounds (Theorem 3.3) are mathematically rigorous and well-supported by the projection-based error analysis framework.
- **Medium:** The stochastic variant's ℓ∞ bounds (Theorem 4.6) are theoretically sound but depend on convex ordering assumptions that may not fully capture real-world quantization noise behavior.
- **Medium:** The Qronos extension (Proposition 5.3) provides valuable theoretical justification, but the practical advantages depend heavily on calibration data characteristics and quantized activation properties.

## Next Checks
1. **Dataset Sensitivity Test:** Systematically evaluate OPTQ's error bounds across different calibration datasets (varying rank, condition number, and feature correlation) to quantify how well the theoretical C_2(X,λ) term predicts actual performance degradation.

2. **Numerical Stability Validation:** Implement OPTQ with varying floating-point precision (FP32, FP16, INT8) to empirically measure the gap between theoretical bounds and observed errors, particularly focusing on Cholesky decomposition stability in ill-conditioned cases.

3. **Stochastic Robustness Analysis:** Conduct extensive Monte Carlo trials of the stochastic variant across multiple layers and architectures to verify that the ℓ∞ bounds hold with the claimed probability, and identify failure modes where convex ordering assumptions break down.