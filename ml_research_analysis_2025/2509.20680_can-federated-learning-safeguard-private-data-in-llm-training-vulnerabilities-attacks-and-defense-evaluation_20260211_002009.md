---
ver: rpa2
title: Can Federated Learning Safeguard Private Data in LLM Training? Vulnerabilities,
  Attacks, and Defense Evaluation
arxiv_id: '2509.20680'
source_url: https://arxiv.org/abs/2509.20680
tags:
- mean
- rouge-l
- round
- data
- enhanced
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates privacy leakage risks in LLM training using
  federated learning. Despite FL's promise to protect data by keeping it local, the
  authors demonstrate that LLMs can still memorize and leak sensitive training data
  through generation tasks.
---

# Can Federated Learning Safeguard Private Data in LLM Training? Vulnerabilities, Attacks, and Defense Evaluation

## Quick Facts
- arXiv ID: 2509.20680
- Source URL: https://arxiv.org/abs/2509.20680
- Reference count: 40
- Primary result: Federated learning cannot fully protect private data in LLM training—models can memorize and leak sensitive information through generation tasks, with larger models showing greater vulnerability.

## Executive Summary
This paper investigates privacy leakage risks in LLM training using federated learning. Despite FL's promise to protect data by keeping it local, the authors demonstrate that LLMs can still memorize and leak sensitive training data through generation tasks. They propose an enhanced attack strategy that exploits iterative model updates in FL to amplify privacy risks, showing that larger models are more vulnerable. Evaluations using ROUGE-L and BERTScore reveal significant leakage, even with partial input knowledge. While privacy-preserving techniques like differential privacy and regularization reduce leakage, they degrade model performance. The study highlights the trade-off between privacy and model efficacy, underscoring the need for improved privacy solutions in FL for LLMs.

## Method Summary
The study evaluates privacy leakage in FL-trained LLMs using three datasets (Enron-Email, Reddit-Comments, CLERC) and multiple model sizes (LLaMA-3 family 1B/3B/8B/13B, Gemma-2-2B, Qwen2.5-7B). The FL setup uses 4 clients, 60 rounds, and 200 local AdamW iterations per round. Two attack schemes are implemented: Zero-Input Generation (N=30 samples from "[BOS]" token) and Partial-Input Completion (first 80% tokens provided). An enhanced attack tracks probability distribution shifts across FL rounds by computing Δπ = π_T - π_{T-1} and fusing this delta with current predictions via softmax-weighted rescaling. Privacy defenses include differential privacy (η=0.01-0.8), KL-regularization (μ=0.001-0.01), and LoRA (r=32). ROUGE-L and BERTScore measure similarity between generated text and held-out training references.

## Key Results
- Zero-Input Generation shows 10% of samples exceed 90% ROUGE-L similarity to training data
- Enhanced attack increases leakage by ~10% (Zero-Input) and 13.4-21.4% (Partial-Input) over basic scheme
- Larger models show monotonically increasing ROUGE-L scores with p<0.05 significance (0.1301→0.1907 for 1B→13B)
- Privacy defenses reduce leakage but significantly degrade model performance and convergence

## Why This Works (Mechanism)

### Mechanism 1: Autoregressive Memorization Leakage
- Claim: LLMs trained via FL can reconstruct training data through generation, contradicting the assumption that FL's parameter aggregation protects privacy.
- Mechanism: During fine-tuning, autoregressive LLMs optimize next-token prediction, which causes them to encode specific training sequences in their parameters. When prompted (even with just a beginning token), the model can regenerate high-similarity text matching private training samples.
- Core assumption: Attackers can access the global model parameters during or after FL training.
- Evidence anchors:
  - [abstract] "We show that attackers can still extract training data from the global model, even using straightforward generation methods, with leakage increasing as the model size grows."
  - [section 3.2.1] Zero-Input Generation task demonstrates reconstruction from "[BOS]" token alone, with 10% of samples showing >90% similarity.
  - [corpus] Limited direct corpus support; related work (arXiv:2508.08875) acknowledges FL lacks built-in unlearning/forgetting mechanisms, indirectly supporting memorization persistence.
- Break condition: If training data is never exposed to the model (e.g., fully synthetic data), or if generation is constrained by output filtering that blocks PII patterns.

### Mechanism 2: Iterative Update Amplification Attack
- Claim: Tracking probability distribution shifts across FL rounds enables more precise data extraction than single-round attacks.
- Mechanism: When a model learns new text in round T, logits for related tokens increase even if they remain sub-maximal. By computing Δπ = π_T - π_{T-1} and fusing this delta with current predictions via softmax-weighted rescaling, attackers can prioritize tokens that changed most—indicating recent training exposure.
- Core assumption: Attacker can observe consecutive global model checkpoints (π_T and π_{T-1}).
- Evidence anchors:
  - [abstract] "We introduce an enhanced attack strategy tailored to FL, which tracks global model updates during training to intensify privacy leakage."
  - [section 3.3.2] Full algorithm: Pre-prediction → Difference Calculation → Fusion with Top-p sampling and temperature-scaled softmax weights.
  - [section 4.2.2] Enhanced attack increases leakage by ~10% (Zero-Input) and 13.4-21.4% (Partial-Input) over basic scheme across Llama-8B/13B.
  - [corpus] Indirect support from arXiv:2601.11219 notes heterogeneous FL clients face privacy tradeoffs, but no direct validation of delta-based attacks found.
- Break condition: If noise is injected into parameter updates (DP) or if access to intermediate checkpoints is restricted.

### Mechanism 3: Capacity-Leakage Scaling
- Claim: Larger models exhibit significantly higher privacy leakage, correlated with improved training loss.
- Mechanism: Greater parameter count enables more precise fitting of training sequences. As training loss decreases (indicating better memorization), the model's ability to regenerate verbatim training data increases. Statistical tests confirm larger models yield higher ROUGE-L scores.
- Core assumption: Model size is a reliable proxy for memorization capacity in FL settings.
- Evidence anchors:
  - [abstract] "Larger models are more vulnerable."
  - [section 4.3.2] Paired t-tests show p<0.05 for 1B→3B→8B→13B comparisons; mean ROUGE-L increases monotonically (0.1301→0.1907).
  - [tables 3-4] Statistical significance confirmed across both per-round and per-sample analyses.
  - [corpus] No direct corpus validation; arXiv:2511.02797 discusses privacy-robustness tradeoffs but not capacity scaling specifically.
- Break condition: If regularization or noise counteracts capacity benefits (partial mitigation observed with DP/KL constraints).

## Foundational Learning

- Concept: **Federated Learning (FL) Communication Rounds**
  - Why needed here: Understanding how global model aggregation works across rounds is prerequisite to grasping how iterative update tracking enables enhanced attacks.
  - Quick check question: In FL with 4 clients and 60 rounds, does each client see other clients' raw data? (Answer: No—only aggregated parameters.)

- Concept: **Autoregressive Next-Token Prediction**
  - Why needed here: The leakage mechanism fundamentally exploits the training objective—predicting P(x_{n+1}|x_1...x_n)—which encodes training sequences into learnable weights.
  - Quick check question: Why might a model trained to predict "contact A at NUMBER-A" later regenerate that exact string? (Answer: The pattern was encoded via gradient descent on the cross-entropy loss.)

- Concept: **ROUGE-L / BERTScore as Privacy Leakage Metrics**
  - Why needed here: Quantifying reconstruction similarity requires understanding what these metrics capture. ROUGE-L emphasizes longest common subsequences (structural/word-level overlap), while BERTScore captures semantic similarity—even when exact tokens differ.
  - Quick check question: If generated text says "reach John at 555-1234" but training data had "contact John at 555-1234," would ROUGE-L or BERTScore be higher? (Answer: ROUGE-L would be moderate (partial exact match); BERTScore would be high (semantic equivalence).)

## Architecture Onboarding

- Component map:
  - FL Coordinator (Server) -> Aggregates client parameters -> Produces global model π_T each round
  - Client Nodes (4) -> Hold private local data -> Perform 200 local AdamW iterations -> Upload updated parameters
  - Attack Module (Simulated Adversary) -> Receives π_T -> Executes Zero-Input or Partial-Input generation -> Optionally computes Δπ for enhanced attack
  - Evaluation Layer -> Computes ROUGE-L/BERTScore between generated samples and held-out training references

- Critical path:
  1. Initialize global model from pretrained checkpoint (e.g., Llama-3.1-8B)
  2. For each round r=1..60: distribute global model -> clients train locally -> aggregate parameters -> (attacker probes global model)
  3. Attacker generates N=30 samples (Zero-Input) or completes masked inputs (Partial-Input)
  4. Compute similarity scores; track leakage trajectory across rounds

- Design tradeoffs:
  - **LoRA (r=32)**: Reduces trainable parameters -> slower overfitting/leakage but higher training loss and delayed convergence (Fig. 8-9)
  - **Differential Privacy (η=0.01-0.8)**: Higher noise multiplier reduces leakage but degrades model capacity (Fig. 10-11)
  - **KL-Regularization (μ=0.001-0.01)**: Constrains parameter drift -> reduces leakage but impedes data fitting (Fig. 12-13)
  - **Safety-Aligned Models**: Reduce but do not eliminate leakage; still vulnerable to enhanced attack (Fig. 14)

- Failure signatures:
  - **Early leakage detection**: If ROUGE-L exceeds 0.8 within first 10 rounds, model is overfitting to private data
  - **Defense breakdown**: If DP η<0.2 shows minimal loss increase but leakage remains high (η=0.01 case), privacy budget is insufficient
  - **Capacity mismatch**: If smaller models (1B/3B) show consistently high loss (>2.0) with LoRA, they cannot effectively learn—privacy is achieved only by sacrificing utility entirely

- First 3 experiments:
  1. **Baseline Leakage Profiling**: Run Zero-Input Generation on pretrained vs. fine-tuned global model; plot ROUGE-L quantiles to confirm 10% of samples exceed 90% similarity (replicate Fig. 1)
  2. **Enhanced Attack Validation**: Implement Δπ-based rescaling; compare basic vs. enhanced attack on Llama-8B across rounds 1-40; target ≥10% ROUGE-L improvement (align with Fig. 5)
  3. **DP Tradeoff Calibration**: Sweep η∈{0.01, 0.2, 0.5, 0.8} on Llama-3B; identify elbow point where further noise yields diminishing privacy gains vs. loss degradation (reference Fig. 10)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can novel privacy-preserving algorithms be developed that simultaneously maintain model capacity while preventing data leakage in FL for LLMs?
- Basis in paper: [explicit] "For applications that demand higher model capacity, new privacy protection techniques are still required." The paper demonstrates that DP, regularization, and LoRA all degrade model performance significantly while only partially reducing leakage.
- Why unresolved: All evaluated defenses exhibit a fundamental privacy-utility trade-off, with stronger privacy guarantees causing training loss to remain high and convergence to slow substantially.
- What evidence would resolve it: Development of techniques achieving both low ROUGE-L attack scores and training loss comparable to unprotected FL.

### Open Question 2
- Question: What are the privacy leakage risks in federated pre-training of LLMs, beyond the fine-tuning scenario studied here?
- Basis in paper: [explicit] "Recently, some studies have advocated for pre-training LLMs in a federated manner (Sani et al., 2024), and our work may raise potential concerns regarding these approaches."
- Why unresolved: The study explicitly limits investigation to fine-tuning, leaving pre-training vulnerability unexplored despite growing research interest.
- What evidence would resolve it: Applying the enhanced attack methodology to federated pre-training and measuring data reconstruction success.

### Open Question 3
- Question: How do alignment and preference optimization methods (RLHF, DPO) affect privacy leakage in federated LLM training?
- Basis in paper: [explicit] "Our study does not investigate privacy leakage issues in more LLM tasks like RLHF (Ouyang et al., 2022) and DPO (Rafailov et al., 2024), and we plan to extend in the future."
- Why unresolved: The iterative optimization in RLHF and DPO may introduce different vulnerability patterns than supervised fine-tuning, but this remains untested.
- What evidence would resolve it: Systematic attack evaluation when FL is applied during RLHF or DPO training phases.

## Limitations

- The enhanced attack's effectiveness may be dataset-dependent, showing strong leakage on structured business communication (Enron-Email) but weaker effects on conversational data (Reddit-Comments)
- Claims about capacity-leakage scaling are validated only within the LLaMA family architecture, without cross-architecture validation
- The study focuses exclusively on English-language datasets with formal communication patterns, limiting generalizability to other languages and domains

## Confidence

**High Confidence:**
- Autoregressive memorization leakage is well-established in prior literature; the empirical demonstration via ROUGE-L and BERTScore metrics is methodologically sound
- Iterative update amplification attack mechanism is clearly specified and validated with measurable improvements (10-21.4% leakage increase) across multiple model sizes

**Medium Confidence:**
- Capacity-leakage scaling claims are statistically supported (p<0.05 significance) but rely on pairwise t-tests that assume normality across rounds
- Defense evaluation results are internally consistent, but the trade-off analysis between privacy and utility could benefit from more granular utility metrics

**Low Confidence:**
- Claims about enhanced attack novelty lack direct comparison to existing FL-specific attack strategies in the literature
- The delta-based approach may be a natural extension rather than a fundamental innovation

## Next Checks

1. **Cross-Domain Leakage Assessment**: Validate the enhanced attack on non-English, conversational, or code datasets to determine whether the Δπ-based amplification generalizes beyond structured business communication.

2. **Real-World FL Deployment Test**: Implement the complete attack pipeline on a realistic FL setup with actual client-server communication constraints (latency, bandwidth, partial participation) to assess practical feasibility.

3. **Multi-Architecture Capacity Analysis**: Replicate the model size-leakage correlation study across different LLM architectures (Mistral, Qwen, Gemma families) to determine if the scaling relationship is architecture-agnostic.