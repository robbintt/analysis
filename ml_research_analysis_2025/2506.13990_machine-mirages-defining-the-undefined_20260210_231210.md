---
ver: rpa2
title: 'Machine Mirages: Defining the Undefined'
arxiv_id: '2506.13990'
source_url: https://arxiv.org/abs/2506.13990
tags:
- machine
- semantic
- hallucination
- these
- pathologies
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the concept of "machine mirages," a new class
  of cognitive aberrations in multimodal machine intelligence systems that mimic but
  do not replicate human or animal fallibility. The authors present a comprehensive
  taxonomy of these mirages, including delusion, illusion, confabulation, hallucination,
  and numerous others, and argue for their explicit definition and systematic assessment.
---

# Machine Mirages: Defining the Undefined

## Quick Facts
- **arXiv ID:** 2506.13990
- **Source URL:** https://arxiv.org/abs/2506.13990
- **Reference count:** 40
- **Primary result:** Introduces "machine mirages," a taxonomy of 34 computational cognitive pathologies in multimodal AI systems, with expectile value-at-risk quantification and mean-field-type game framework.

## Executive Summary
This paper introduces the concept of "machine mirages," a new class of cognitive aberrations in multimodal machine intelligence systems that mimic but do not replicate human or animal fallibility. The authors present a comprehensive taxonomy of these mirages, including delusion, illusion, confabulation, hallucination, and numerous others, and argue for their explicit definition and systematic assessment. To quantify these computational pathologies, the paper proposes using expectile value-at-risk, a coherent risk measure that allows for asymmetry-aware quantification of both rare catastrophic failures and frequent low-grade degradations.

## Method Summary
The paper proposes a mathematical framework for defining and quantifying 34 computational pathologies ("machine mirages") in multimodal AI systems. The core approach uses expectile value-at-risk (ExpVaR) to asymmetrically measure pathology risks, and a hierarchical mean-field-type game framework to coordinate between human agents and machine co-intelligence agents. The proposed Holonorm Transformer architecture employs hn(x) = x/(1+||x||) as both normalization and nonlinearity, with theoretical guarantees of universal approximation for pathology-specific risk minimization.

## Key Results
- No single pre-trained transformer can minimize all 34 pathology risks simultaneously (Pareto frontier impossibility theorem)
- Expectile VaR enables asymmetric quantification of both rare catastrophic failures and frequent low-grade degradations
- Holonorm Transformer architecture theoretically expressive enough to approximate any pathology-specific risk minimization

## Why This Works (Mechanism)

### Mechanism 1
Expectile value-at-risk (ExpVaR) enables asymmetric quantification of both rare catastrophic failures and frequent low-grade degradations in machine outputs. For pathology i, risk R(pathology_i) ∈ argmin E[w_τ(L_i - r)(L_i - r)²], where w_τ weights losses asymmetrically based on direction. This captures that a referential hallucination in public health guidance (rare, catastrophic) and semantic drift in curriculum delivery (frequent, low-grade) require different risk tolerances.

### Mechanism 2
A Pareto frontier emerges from multi-objective optimization across the 34 pathology risks, meaning no single pre-trained transformer can minimize all risks simultaneously. The risks are conditioned on different quantities (semantic similarity, causal sensitivity, factual grounding, prosodic coherence) that are not jointly optimizable.

### Mechanism 3
The Holonorm Transformer (using hn(x) = x/(1+||x||) as both normalization and nonlinearity) has sufficient representational capacity to approximate any pathology-specific risk minimization to arbitrary precision. The Holonorm map is smooth, bounded, and acts as a bijection on compact domains.

## Foundational Learning

- **Expectile Value-at-Risk**: Core quantification tool for pathology risks; differs from standard quantile VaR by using asymmetric least-squares weighting rather than thresholding. Quick check: For τ = 0.95, does ExpVaR penalize overestimation and underestimation of risk equally?

- **Mean-Field-Type Games (MFTG)**: The paper formulates human-MI agent coordination as an MFTG where payoffs depend on the distribution of states and actions, not just individual actions. Quick check: In an MFTG, if all agents modify their strategies simultaneously, does the mean field μ update before or within each agent's optimization?

- **Interventional vs. Observational Distributions (do-calculus)**: Causal inference failure is defined as the model conflating p(Y|X=x) with p(Y|do(X=x))—failing to distinguish observation from intervention. Quick check: If a model correctly predicts outcomes from observational data but cannot predict the effect of an external intervention, which distribution is it learning?

## Architecture Onboarding

- **Component map**: Input data -> Holonorm Transformer Block -> Pathology-specific Risk Evaluation -> MFTG Coordination Layer -> Output with Risk Thresholds

- **Critical path**: 
  1. Define which subset of 34 pathologies are relevant to your deployment context
  2. Instrument model outputs with pathology indicators
  3. Configure expectile levels τ per pathology based on acceptable asymmetry
  4. Set deployment thresholds εᵢ and verify feasibility: Rᵢ(o_θ*(x), y; μ*) ≤ εᵢ

- **Design tradeoffs**:
  - Risk coverage vs. compute: Monitoring all 34 pathologies increases evaluation overhead
  - Holonorm vs. LayerNorm: Holonorm preserves gradient magnitude for bounded activations but is unstudied in large-scale training
  - Pareto frontier navigation: Improving one pathology risk may degrade another

- **Failure signatures**:
  - Semantic drift: sim(ŷ₁:t, y_intended) → 0 over long outputs
  - Calibration failure: |P(o_θ(x) = y_true) - confidence(o_θ(x))| ≫ 0
  - Ambiguity collapse: max_y p_θ(y|x) ≫ p_θ(y*|x) for plausible alternatives y*
  - Semiotic Frankenstein effect: Output weaves true and false fragments into coherent narrative

- **First 3 experiments**:
  1. Pathological baseline measurement: Run existing model on held-out corpus; compute all 34 ExpVaR metrics
  2. Holonorm vs. LayerNorm ablation: Train two transformers; compare risk profiles across selected pathologies
  3. Pareto frontier mapping: For 2-pathology subset, sweep reward weights and plot achievable (R₁, R₂) pairs

## Open Questions the Paper Calls Out

- **Open Question 1**: Do the proposed mathematical formalizations for machine mirages (e.g., semantic drift, hypersignification) correlate with human judgments of error severity in multimodal systems? The definitions are purely theoretical with no experimental validation.

- **Open Question 2**: How can the trade-offs between conflicting pathology risk metrics be systematically navigated to find an optimal point on the Pareto frontier? The paper proves impossibility of simultaneous optimization but doesn't propose a prioritization method.

- **Open Question 3**: Does a stable mean-field-type Nash equilibrium exist for the proposed hierarchical game between human agents and machine co-intelligence agents? The paper formulates equilibrium conditions but doesn't prove existence or provide computation mechanisms.

## Limitations
- No experimental validation or datasets provided for the theoretical framework
- Holonorm Transformer performance in large-scale training remains unvalidated
- No algorithm specified for computing the mean-field-type game equilibrium

## Confidence

- **Expectile VaR for asymmetric risk quantification**: High confidence
- **Pareto frontier impossibility theorem**: Medium confidence
- **Holonorm Transformer expressiveness**: Low confidence
- **34 pathology taxonomy comprehensiveness**: High confidence

## Next Checks

1. **Empirical risk profiling**: Apply the 34 pathology detection methods to existing transformer models on standard benchmarks to establish baseline risk profiles and validate detectability.

2. **Holonorm vs. LayerNorm comparison**: Train identical architectures differing only in normalization/activation and measure their performance across the full pathology risk spectrum.

3. **Pareto frontier verification**: For a subset of two pathologies, systematically vary reward weights during training and plot the achievable (R₁, R₂) pairs to empirically verify structural tradeoffs.