---
ver: rpa2
title: How well can LLMs Grade Essays in Arabic?
arxiv_id: '2501.16516'
source_url: https://arxiv.org/abs/2501.16516
tags:
- arabic
- essay
- language
- llms
- question
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluates the effectiveness of large language models
  (LLMs) for Arabic automated essay scoring (AES) using the AR-AES dataset. It explores
  zero-shot, few-shot in-context learning, and fine-tuning approaches with models
  including ChatGPT, Llama, Aya, Jais, and ACEGPT.
---

# How well can LLMs Grade Essays in Arabic?

## Quick Facts
- arXiv ID: 2501.16516
- Source URL: https://arxiv.org/abs/2501.16516
- Reference count: 40
- Primary result: ACEGPT achieved highest LLM QWK of 0.67, still lower than AraBERT's 0.88

## Executive Summary
This study evaluates large language models for Arabic automated essay scoring using the AR-AES dataset. It explores zero-shot, few-shot, and fine-tuning approaches with models including ChatGPT, Llama, Aya, Jais, and ACEGPT. The research introduces a mixed-language prompting strategy combining English instructions with Arabic content, which improved performance by 49.49% compared to monolingual prompts. While ACEGPT achieved the highest QWK of 0.67 among LLMs, it still underperformed the smaller BERT-based AraBERT model with QWK 0.88. The study addresses tokenization challenges by developing a custom SentencePiece tokenizer that reduced sequence lengths from 1532 to 410 tokens.

## Method Summary
The study uses the AR-AES dataset (2,046 Arabic essays across 12 prompts from 4 courses) with 70/15/15 train/val/test splits. Three approaches were evaluated: zero-shot prompting, few-shot in-context learning (3 examples per class), and fine-tuning via LoRA using either instruction-tuning or label-supervised adaptation (LS-LLaMA). Preprocessing included punctuation/URL/diacritic removal, Arabic character normalization, and ISRI stemming. A custom SentencePiece tokenizer was trained on AR-AES and merged with Llama's vocabulary. The primary metric was Quadratic Weighted Kappa (QWK) for ordinal classification, with mixed-language prompts (English instructions + Arabic content) showing 49.49% improvement over monolingual prompts.

## Key Results
- ACEGPT achieved highest LLM QWK of 0.67 through fine-tuning, still below AraBERT's 0.88
- Mixed-language prompting improved QWK by 49.49% compared to monolingual prompts
- Custom SentencePiece tokenizer reduced sequence length from 1532 to 410 tokens
- LS-LLaMA fine-tuning outperformed instruction tuning for classification accuracy

## Why This Works (Mechanism)

### Mechanism 1: Label-Supervised Adaptation (LS-LLaMA) for Ordinal Classification
- Claim: Fine-tuning an LLM by attaching a classification head to its final layer (LS-LLaMA) yields higher scoring accuracy than instruction tuning, because it directly optimizes for the ordinal grade labels using cross-entropy loss.
- Mechanism: The LLM processes the essay and generates latent representations. Instead of generating free-form text (as in instruction tuning), these representations are passed to a classification layer that outputs probabilities for each possible grade. The model is then trained to minimize the error between these probabilities and the true human-assigned grades.
- Core assumption: The LLM's internal representations of Arabic essays contain sufficient information to separate essays into quality levels, and that the training labels are consistent enough for a classifier to learn a reliable mapping.
- Evidence anchors:
  - [abstract]: ACEGPT achieved the highest QWK of 0.67 among LLMs via fine-tuning, though lower than AraBERT's 0.88.
  - [Section 4.4]: "Label-supervised adaptation (LS-LLaMA)... capitalizes on the strengths of LLMs... while addressing the limitations of instruction tuning."
  - [Section 7]: "label-supervised adaptation (LS-LLaMA) performed best... proving more effective for Arabic AES tasks by eliminating the need for prompt engineering."
- Break condition: If the ground truth grades from different human markers are highly inconsistent (low inter-rater agreement), the classification head may fail to converge on a stable decision boundary, leading to unpredictable predictions.

### Mechanism 2: Mixed-Language Prompting (English Instructions + Arabic Content)
- Claim: Providing scoring instructions in English while keeping the student's essay, rubric, and question in Arabic significantly improves performance for models predominantly trained on English data.
- Mechanism: The model leverages its strong English instruction-following capabilities to precisely understand the *task* (e.g., "Assign one of the six grades 0-5 based on these criteria") while processing the *content* in its original language. This avoids the information loss and potential errors of translating the essay into English and bypasses the model's potentially weaker Arabic instruction comprehension.
- Core assumption: The LLM has sufficiently robust cross-lingual grounding to apply English-defined rules (from the prompt) to evaluate Arabic text without confusion.
- Evidence anchors:
  - [abstract]: "A mixed-language prompting strategy... improved performance by 49.49% compared to monolingual prompts."
  - [Section 6, Table 2]: Shows QWK for Q9 jumping from 0.41 (English-only prompt) to 0.66 (mixed-language prompt).
  - [corpus]: Weak/missing. No direct corroboration for this specific mixed-language strategy was found in related papers. It is a novel finding of this study.
- Break condition: If a model's training has poorly aligned cross-lingual representations, mixing languages in a single prompt could introduce confusion, yielding worse results than a simpler monolingual prompt.

### Mechanism 3: Custom Tokenization for Morphologically Rich Languages
- Claim: Replacing a generic tokenizer that fragments Arabic into characters with a custom word-level tokenizer reduces sequence length, computational cost, and may improve performance by providing more meaningful tokens.
- Mechanism: Standard tokenizers for LLMs trained mostly on English often split Arabic words into individual characters, creating extremely long sequences (e.g., 1532 tokens avg.). A custom SentencePiece tokenizer trained on Arabic data learns to represent whole words/subwords, shortening sequences (e.g., to 410 tokens). This reduces GPU memory usage and presents the model with more semantically complete input units, easing the learning burden.
- Core assumption: The vocabulary learned from the target dataset is representative of the test data, and the model can be effectively fine-tuned to integrate the new tokenizer's embedding layer.
- Evidence anchors:
  - [Section 4.2]: "...the average sequence length from 1532 to 410 tokens" and "reducing GPU memory usage."
  - [Section 4.2, Figure 2]: Shows slight QWK improvement with the custom tokenizer.
  - [corpus]: Related work (Petrov et al., 2024, cited in paper) highlights general tokenizer unfairness. No direct confirmation of this *custom* method's efficacy was found in the provided corpus, but it aligns with the known challenge.
- Break condition: If the custom tokenizer's vocabulary is too small, many words will still be split into characters, negating the benefits. If it is too large or overfit to the training set, it may perform poorly on out-of-distribution vocabulary.

## Foundational Learning

- **Quadratic Weighted Kappa (QWK)**
  - Why needed here: This is the primary metric for comparing models. Unlike simple accuracy, it accounts for chance agreement and the *ordinal* nature of grades, penalizing larger errors (e.g., predicting 0 when the true grade is 5) more heavily than adjacent errors.
  - Quick check question: If a model always predicts the class label with the highest frequency, what would its QWK likely be? (Answer: Near zero or negative, indicating no better than chance agreement).

- **Fine-tuning vs. In-Context Learning (Zero/Few-shot)**
  - Why needed here: The paper's central conclusion is that fine-tuning (LS-LLaMA) is necessary for good performance, as zero-shot and few-shot approaches underperformed. Understanding this distinction is crucial for architectural decisions.
  - Quick check question: Which approach modifies the model's weights? (Answer: Fine-tuning).

- **Low-Rank Adaptation (LoRA)**
  - Why needed here: Fine-tuning large models like Llama 2 or ACEGPT is computationally expensive. LoRA is the efficiency technique used in the paper to make fine-tuning feasible by updating only a small number of adapter parameters.
  - Quick check question: What is the primary advantage of LoRA over full fine-tuning? (Answer: Dramatically reduced computational and memory requirements).

## Architecture Onboarding

- **Component map**: Custom SentencePiece tokenizer -> Token IDs -> Pretrained LLM backbone -> Final hidden layer representations -> Classification layer -> Logits for 6 grade classes (0-5)

- **Critical path**:
  1. Preprocess Arabic essays (normalization, stemming).
  2. Tokenize with custom SentencePiece model.
  3. Feed tokens through frozen LLM backbone with LoRA adapters.
  4. Pass final hidden state through classification head.
  5. Compute loss against human grades and update LoRA parameters.
  6. Inference: Output class with highest probability from the classification head.

- **Design tradeoffs**:
  - **AraBERT vs. LLM (ACEGPT)**: AraBERT is faster, cheaper, and currently more accurate (QWK 0.88 vs 0.67), but is a specialized encoder-only model. LLMs are more flexible (can generate feedback) and may improve with scale, but are currently inferior for this specific classification task.
  - **LS-LLaMA vs. Instruction Tuning**: LS-LLaMA gives better classification accuracy and removes prompt fragility. Instruction tuning allows the model to generate explanations alongside scores, which may be valuable for feedback but is less precise for the grading task itself.
  - **Mixed vs. Monolingual Prompts**: Mixed prompts boost performance for English-centric LLMs but add complexity. Monolingual (Arabic) prompts are simpler but currently less effective for these models.

- **Failure signatures**:
  - **Model outputs free-text commentary**: Sign that instruction tuning was used or the classification head is not properly attached/frozen. Switch to LS-LLaMA classification approach.
  - **High accuracy, low QWK**: Model is biased toward the majority class. Check class balance and consider weighted loss.
  - **OOM (Out of Memory) on GPU**: Sequence lengths are too long. Implement the custom SentencePiece tokenizer to reduce token count.
  - **Performance collapses on new essay types**: Model has overfit to the specific topics or styles in the training set. Requires more diverse training data or domain adaptation techniques.

- **First 3 experiments**:
  1. **Reproduce AraBERT baseline**: Fine-tune AraBERT on the AR-AES dataset to establish the performance ceiling (QWK ~0.88) and validate your data pipeline.
  2. **Implement LS-LLaMA with ACEGPT/Llama2**: Set up the LoRA + classification head architecture, train a custom SentencePiece tokenizer, and fine-tune. Compare its QWK to the baseline to measure the gap.
  3. **Ablate the mixed-language prompt**: Using the best few-shot capable model (e.g., ACEGPT-chat or GPT-4), score a subset of essays using 1) all-Arabic prompts, 2) all-English prompts (translated essays), and 3) mixed-language prompts (English instructions, Arabic essay). Quantify the 49% gain mentioned in the paper.

## Open Questions the Paper Calls Out
None

## Limitations
- Reliance on a single, privately held dataset (AR-AES) without independent validation or replication on other Arabic AES corpora
- The claimed 49.49% improvement from mixed-language prompting rests on limited empirical comparison - only three prompt variants were tested for one question type
- The gap between ACEGPT (QWK 0.67) and AraBERT (QWK 0.88) suggests current LLMs have fundamental limitations that the paper doesn't fully explain

## Confidence

- **High Confidence**: The experimental methodology is sound (proper train/val/test splits, appropriate metric choice with QWK for ordinal classification, ablation of tokenization strategy). The finding that LS-LLaMA outperforms instruction tuning for classification tasks is well-supported by the results and aligns with established machine learning principles.
- **Medium Confidence**: The 49.49% improvement from mixed-language prompting is statistically significant within the paper's experiments but may not generalize across all Arabic AES contexts or model families. The claim that fine-tuning is necessary (zero/few-shot underperform) is supported but the magnitude of the gap varies considerably by essay type and course.
- **Low Confidence**: The paper's discussion of why LLMs currently underperform AraBERT is speculative. The relationship between tokenization strategy and performance is not clearly established - the custom tokenizer reduced sequence length significantly but QWK improvements were minimal.

## Next Checks

1. **Dataset Independence Test**: Replicate the entire pipeline (including mixed-language prompting and LS-LLaMA fine-tuning) on at least one other publicly available Arabic AES dataset or create a synthetic test set with known properties to verify generalizability of the 49.49% improvement claim.

2. **Architecture Ablation**: Systematically compare AraBERT vs. LLM performance across multiple dimensions: (a) with identical tokenization, (b) with identical fine-tuning approach, (c) with identical prompt formats. This will isolate whether the performance gap stems from architectural differences or implementation choices.

3. **Inter-Annotator Agreement Analysis**: Calculate and report the QWK between human graders in the AR-AES dataset to establish an upper bound on achievable model performance. This will contextualize whether the 0.67 vs 0.88 gap represents meaningful progress or simply reflects the difficulty of the task.