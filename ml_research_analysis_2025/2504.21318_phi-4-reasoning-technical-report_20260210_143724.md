---
ver: rpa2
title: Phi-4-reasoning Technical Report
arxiv_id: '2504.21318'
source_url: https://arxiv.org/abs/2504.21318
tags:
- reasoning
- accuracy
- data
- arxiv
- aime
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Phi-4-reasoning is a 14-billion parameter reasoning model that
  achieves strong performance on complex reasoning tasks through supervised fine-tuning
  and reinforcement learning. The model is trained on a carefully curated dataset
  of 1.4M prompts and high-quality answers, focusing on complexity and diversity.
---

# Phi-4-reasoning Technical Report

## Quick Facts
- arXiv ID: 2504.21318
- Source URL: https://arxiv.org/abs/2504.21318
- Reference count: 40
- 14-billion parameter reasoning model achieving strong performance on complex reasoning tasks through SFT and RL

## Executive Summary
Phi-4-reasoning is a 14B reasoning model that achieves strong performance on complex reasoning tasks through supervised fine-tuning and reinforcement learning. The model is trained on a carefully curated dataset of 1.4M prompts and high-quality answers, focusing on complexity and diversity. Phi-4-reasoning generates detailed reasoning chains, leveraging inference-time compute effectively. A variant, Phi-4-reasoning-plus, further enhances performance through outcome-based reinforcement learning, generating longer reasoning traces.

## Method Summary
Phi-4-reasoning is trained through two-stage pipeline: first supervised fine-tuning (SFT) on 1.4M curated prompts with o3-mini-generated reasoning traces, then reinforcement learning (GRPO) on math problems. The SFT stage repurposes two tokens as reasoning delimiters and extends context to 32K. The RL stage uses length-aware accuracy rewards with repetition penalty. Both models outperform larger open-weight models and approach DeepSeek-R1 performance.

## Key Results
- Over 50 percentage points gain in math benchmarks (AIME) compared to base Phi-4
- Over 25 percentage points gain in coding benchmarks (LiveCodeBench, Codeforces)
- Outperforms DeepSeek-R1-Distill-Llama-70B while being 5x smaller
- Shows non-trivial improvements on general-purpose benchmarks (IFEval, MMLU-Pro, ArenaHard)

## Why This Works (Mechanism)

### Mechanism 1
- Curating prompts at the "edge of model capabilities" produces more effective reasoning training data than random or maximally difficult samples
- Filtering seeds based on agreement rates between weaker models and proxy ground truth targets the "zone of proximal development"
- Difficulty estimated via model agreement correlates with pedagogical value for reasoning

### Mechanism 2
- Distilling reasoning traces from stronger models (o3-mini) via SFT transfers structured reasoning patterns to smaller models
- High-quality chain-of-thought demonstrations provide both format and reasoning strategies
- Reasoning capability can be compressed into a smaller model via supervised learning on reasoning traces

### Mechanism 3
- Brief outcome-based RL (GRPO) after SFT extends reasoning traces and improves accuracy, particularly in math domains
- Length-aware accuracy rewards encourage longer exploration for incorrect answers while penalizing excessive length for correct ones
- Longer reasoning traces correlate with improved accuracy; verifiable rewards provide meaningful learning signal

## Foundational Learning

### Concept: Chain-of-thought (CoT) reasoning
- Why needed here: The entire training approach assumes models can learn to decompose problems into intermediate steps rather than directly predicting answers
- Quick check question: Can you explain why prompting "think step by step" improves performance on multi-step arithmetic?

### Concept: Knowledge distillation
- Why needed here: Phi-4-reasoning is trained on o3-mini-generated traces; understanding teacher-student dynamics is essential
- Quick check question: What information is lost when distilling from a 671B model to a 14B model?

### Concept: Reinforcement learning from verifiable rewards
- Why needed here: Phi-4-reasoning-plus uses GRPO with rule-based rewards; understanding policy gradients and reward design is critical
- Quick check question: Why might a length penalty be necessary when training with outcome-only rewards?

## Architecture Onboarding

### Component map
Base Phi-4 (14B transformer decoder, 16K context) -> SFT modifications (repurpose two tokens as delimiters, extend context 16K→32K) -> RL additions (GRPO with rule-based reward, no architecture changes)

### Critical path
1. Data curation: Filter seeds for teachability → generate responses with o3-mini → decontaminate
2. SFT: Train on 1.4M examples, 16K steps, LR=1e-5, 32K context
3. RL: Train on ~6K math problems, 90 steps, GRPO with group size 8

### Design tradeoffs
- SFT teacher quality vs. cost: o3-mini high-effort is stronger but generates longer traces (higher inference cost)
- RL coverage vs. depth: RL only on math; generalization to other domains observed but less pronounced
- Length vs. accuracy: Phi-4-reasoning-plus uses ~1.5× more tokens than Phi-4-reasoning for modest accuracy gains

### Failure signatures
- Incomplete generations: Missing delimiters or EOS token triggers format penalty
- Length clipping: Responses >31K tokens truncated, preventing answer extraction
- Reward hacking: Repetition penalty guards against 5-gram loops

### First 3 experiments
1. Validate SFT data quality: Train on a 10K subset, measure AIME/GPQA progress at checkpoints 2K, 4K, 8K steps
2. Ablate teacher model: Compare o3-mini-medium vs. o3-mini-high as teacher on held-out math benchmark
3. Test RL generalization: Run GRPO on non-math seeds (e.g., coding), measure transfer to algorithmic tasks like TSP/3SAT

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does extending the context window to 64k tokens yield additional benefits in GRPO training for reasoning models?
- Basis in paper: Page 12 states, "We hypothesize that enabling the model to support even longer context windows—such as 64k tokens... could yield additional benefits in GRPO training. We leave the exploration of this extended-context approach for future work."
- Why unresolved: Current training was constrained to a 32k token context window, and it is unknown if allowing longer traces during RL would improve learning of complex reasoning chains.
- What evidence would resolve it: A comparison of model performance and training dynamics between 32k and 64k context window configurations during GRPO.

### Open Question 2
- Question: How can reinforcement learning be effectively utilized to improve reasoning in domains where high-quality supervised fine-tuning (SFT) data is unavailable?
- Basis in paper: Page 2 notes the authors "plan to explore this area further especially for domains where SFT data is not available."
- Why unresolved: Current success of Phi-4-reasoning-plus relied on a strong SFT foundation derived from a teacher model; efficacy of RL in absence of such data remains unconfirmed.
- What evidence would resolve it: Successful training of a reasoning model using RL on a domain without prior SFT demonstrations, achieving performance comparable to SFT-initialized models.

### Open Question 3
- Question: Can the performance gap between average and "best-of-N" generations be closed without multiplying inference-time compute by N?
- Basis in paper: Page 20 states: "Extracting such capabilities without spending N-times more inference compute on models that are already expensive at inference time, remains an open question for future work."
- Why unresolved: Current evaluations show a significant gap between average accuracy and best-of-5 accuracy, indicating latent capability that is currently only accessible via expensive parallel sampling.
- What evidence would resolve it: A decoding strategy or verification method that achieves best-of-N accuracy levels using only single-generation (or constant) compute overhead.

### Open Question 4
- Question: What data or training modifications are necessary to achieve performance gains in biology, chemistry, and discrete mathematics comparable to those in physics and general algebra?
- Basis in paper: Page 3 notes that "detailed benchmark analyses indicate comparatively smaller improvements in domains such as biology and chemistry... Even within mathematics, discrete mathematics shows relatively modest gains" and lists these as areas requiring attention.
- Why unresolved: Current training recipe yielded uneven improvements across STEM sub-domains, suggesting current data mixture or methods are less effective for these specific topics.
- What evidence would resolve it: A revised model iteration showing statistically significant accuracy increases specifically on GPQA (Biology/Chemistry) and OmniMath (Discrete Math) subsets.

## Limitations
- Data curation methodology remains partially opaque with insufficient detail on filtering criteria
- Direct comparisons to contemporary models (Gemini 1.5 Pro, Claude 3.5 Sonnet) are absent
- RL training scope is narrow, confined to math problems despite broader task coverage
- Safety evaluations beyond mentioning "safety reward" in RL are absent

## Confidence

**High Confidence**: The SFT methodology and resulting performance improvements on AIME/GPQA benchmarks are well-documented and reproducible. The ~50 percentage point improvement over Phi-4 on math benchmarks is supported by direct before/after comparisons.

**Medium Confidence**: Claims about Phi-4-reasoning-plus outperforming larger open-weight models are credible based on reported metrics, but narrow RL training scope tempers confidence in generalizability.

**Low Confidence**: The assertion that RL improves "planning and spatial reasoning" performance is weakly supported, with no specific benchmarks or ablation studies provided.

## Next Checks

1. **Data Curation Ablation**: Replicate SFT training with three data variants: (a) unfiltered o3-mini traces, (b) agreement-rate filtered as described, and (c) difficulty-sampled using a different proxy. Compare AIME performance to isolate the impact of the curation methodology on reasoning capability.

2. **RL Domain Transfer Analysis**: Extend GRPO training from math to a balanced mix of math, coding, and algorithmic reasoning tasks. Track per-domain performance trajectories and compute transfer ratios to quantify cross-domain generalization versus domain-specific memorization.

3. **Reasoning Trace Quality Audit**: Extract reasoning chains from Phi-4-reasoning-plus on held-out problems. Score chains for structural completeness using human annotators or automated metrics. Correlate quality scores with final answer accuracy to validate that longer traces reflect genuine reasoning rather than verbose pattern matching.