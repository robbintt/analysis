---
ver: rpa2
title: Topic Modelling Black Box Optimization
arxiv_id: '2512.16445'
source_url: https://arxiv.org/abs/2512.16445
tags:
- optimization
- topic
- black-box
- sabbo
- perplexity
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the problem of selecting the optimal number
  of topics in Latent Dirichlet Allocation (LDA) topic modeling by formulating it
  as a discrete black-box optimization problem. The authors compare four optimization
  approaches: two evolutionary methods (Genetic Algorithm and Evolution Strategy)
  and two learned amortized methods (Preferential Amortized Black-Box Optimization
  and Sharpness-Aware Black-Box Optimization).'
---

# Topic Modelling Black Box Optimization

## Quick Facts
- arXiv ID: 2512.16445
- Source URL: https://arxiv.org/abs/2512.16445
- Reference count: 24
- Finding: Amortized optimizers (PABBO, SABBO) substantially outperform classical evolutionary methods (GA, ES) for LDA topic number selection in sample- and time-efficiency

## Executive Summary
This paper addresses the problem of selecting the optimal number of topics in Latent Dirichlet Allocation (LDA) topic modeling by formulating it as a discrete black-box optimization problem. The authors compare four optimization approaches: two evolutionary methods (Genetic Algorithm and Evolution Strategy) and two learned amortized methods (Preferential Amortized Black-Box Optimization and Sharpness-Aware Black-Box Optimization). Using perplexity as the objective metric across four text corpora, the experiments show that while all methods eventually converge to similar final perplexity levels, the amortized optimizers are significantly more sample- and time-efficient.

## Method Summary
The study formulates LDA topic number selection as black-box optimization over discrete T ∈ [T_min, T_max]. Four optimizers are evaluated: GA with tournament selection and mutation ε∼U{-5,…,5}, ES with (μ+λ) selection and Gaussian mutation N(0,5²), PABBO using a pre-trained Transformer policy on synthetic functions, and SABBO optimizing for flat regions via neighborhood sampling. All methods query T → validation perplexity using scikit-learn's online variational Bayes LDA with fixed priors α=β=1/T. The optimization runs for G=20 evaluations across 10 trials per corpus (20 Newsgroups, AG News, Yelp Reviews, Valout).

## Key Results
- SABBO typically identifies near-optimal topic numbers after essentially a single evaluation
- PABBO finds competitive configurations within a few evaluations while GA and ES require nearly full budget
- All methods converge to similar final perplexity levels despite different sample efficiency
- Amortized methods are substantially more sample- and time-efficient than classical evolutionary approaches

## Why This Works (Mechanism)

### Mechanism 1: Amortized Meta-Transfer from Synthetic to Real Tasks
- Claim: Pre-trained neural optimizers transfer learned search strategies from synthetic functions to LDA hyperparameter selection, achieving faster convergence than hand-designed algorithms.
- Mechanism: PABBO's Transformer is meta-trained on distributions of optimization tasks (GP1D, Rastrigin); the learned acquisition policy generalizes to LDA perplexity landscapes without task-specific training. The policy encodes priors about where optima typically lie in 1D parameter spaces.
- Core assumption: The structure of 1D optimization landscapes shares sufficient commonalities between synthetic functions and perplexity-as-a-function-of-T for transfer to occur.
- Evidence anchors: "learned, amortized approaches...are substantially more sample- and time-efficient" [abstract]; "The Transformer is meta-trained on synthetic functions (GP1D, Rastrigin) enabling zero-shot transfer to LDA optimization" [section 5.2.3]; PABBO paper (FMR=0.628) provides the foundational framework for preference-based amortized optimization.

### Mechanism 2: Sharpness-Aware Distribution Updates
- Claim: Optimizing for flat regions in the perplexity landscape yields topic configurations that are more robust and generalizable.
- Mechanism: SABBO reparameterizes optimization over a Gaussian search distribution; at each iteration it maximizes the objective within a neighborhood of the current mean before updating, explicitly penalizing sharp minima that may not generalize.
- Core assumption: Sharp minima in perplexity correspond to brittle topic numbers that perform poorly on held-out data.
- Evidence anchors: "SABBO typically identifies a near-optimal topic number after essentially a single evaluation" [abstract]; "algorithm optimizes the maximum of the objective value within a small neighborhood...encouraging solutions that are robust to sharp minima" [section 4.4]; Corpus evidence weak—SABBO reference paper not present in neighbors.

### Mechanism 3: Preference Learning Under Noisy Evaluations
- Claim: Learning from pairwise comparisons rather than absolute perplexity values provides more stable optimization signals when LDA evaluations are noisy.
- Mechanism: PABBO receives binary preference labels (configuration A better than B) and learns a surrogate model predicting comparison outcomes. This reduces sensitivity to absolute perplexity variance across LDA runs with different random seeds.
- Core assumption: Relative rankings are more stable than absolute perplexity values across stochastic LDA training runs.
- Evidence anchors: "optimizer receives responses such as 'point x is better than point x'' rather than numerical evaluations" [section 4.3]; PABBO shows "large 'jumps'" in trajectories with "wide confidence bands" but superior average performance [section 6.1]; PABBO paper confirms preference-based learning as core mechanism.

## Foundational Learning

- Concept: Latent Dirichlet Allocation (LDA) as a generative probabilistic model
  - Why needed here: The entire optimization target assumes understanding that LDA factorizes document-word matrices and that perplexity measures held-out likelihood.
  - Quick check question: Given a document-term matrix, can you explain what θ_d and φ_t represent and how perplexity is computed from them?

- Concept: Black-box optimization without gradient access
  - Why needed here: T → perplexity has no closed-form derivative; you can only query function values.
  - Quick check question: What information can and cannot be extracted from a single black-box query, and how does this constrain optimizer design?

- Concept: Amortized inference and meta-learning
  - Why needed here: PABBO and SABBO do not optimize from scratch—they deploy pre-trained policies that amortize computation across tasks.
  - Quick check question: How does amortized optimization shift computational costs from inference-time to training-time?

## Architecture Onboarding

- Component map: Preprocessing (CountVectorizer) → LDA core (variational Bayes) → Optimization loop (GA/ES/PABBO/SABBO) → Evaluation (perplexity)
- Critical path: 1) Preprocess corpus → document-term matrix with stop-word removal and frequency filtering; 2) Initialize optimizer with shared random pool of T values in [T_min, T_max]; 3) For each of G=20 iterations: propose T → train LDA → compute validation perplexity → update optimizer state; 4) Return T with lowest observed perplexity
- Design tradeoffs: GA: Most stable convergence but requires nearly full budget; adds ~30–50% runtime overhead; ES: Simple implementation but weakest convergence; struggles with discrete T space; PABBO: Fast early convergence with high variance across runs; depends on pre-trained checkpoint quality; SABBO: Best final perplexity with lowest variance; expensive first query but near-optimal immediately
- Failure signatures: ES plateauing above others: mutation scale σ mismatched to discrete search space; PABBO high run-to-run variance: preference model underfitting or exploration rate ρ too high; All methods converging to same perplexity band: search interval may exclude true optimum
- First 3 experiments: 1) Reproduce SABBO single-query result on 20 Newsgroups: verify that first sample from initialized distribution achieves near-final perplexity; 2) Ablate PABBO's exploration rate ρ: test whether reducing random exploration tightens confidence bands; 3) Cross-corpus transfer test: train PABBO meta-policy on perplexity histories from 3 corpora, evaluate zero-shot on held-out corpus

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the estimation of the optimal topic number (T*) be effectively formulated as a supervised learning problem using the accumulated corpora dataset?
- Basis in paper: [explicit] Section 7 suggests treating T* estimation as regression or classification to enable fast inference on new datasets.
- Why unresolved: The authors collected the data but did not implement or validate a supervised predictive model.
- What evidence would resolve it: A trained model predicting optimal T for held-out corpora that matches the performance of the black-box optimizers.

### Open Question 2
- Question: Can an RL agent learn a generalizable policy for topic number selection using corpus features as state and model quality as reward?
- Basis in paper: [explicit] Section 7 proposes an RL framework to bypass the need for precomputed optimal values.
- Why unresolved: Defining a reward signal that reliably reflects topic quality remains a central, unaddressed challenge.
- What evidence would resolve it: An RL policy that converges on near-optimal T for unseen corpora without requiring search iterations.

### Open Question 3
- Question: Do the efficiency advantages of amortized optimizers persist when jointly tuning the number of topics and Dirichlet priors (α, β)?
- Basis in paper: [inferred] The study simplifies the search to f(T) by fixing priors, potentially missing global optima available in a higher-dimensional search.
- Why unresolved: The current experiments are restricted to a 1-dimensional discrete search space.
- What evidence would resolve it: Benchmarking SABBO/PABBO performance on the joint optimization of T, α, β against evolutionary baselines.

## Limitations
- Core hyperparameters for GA and ES are unspecified, leaving significant variance in reported performance
- The assumption that flatness in perplexity correlates with generalization is weakly supported; no held-out test-set perplexity or topic coherence measures are reported
- Reproducibility is constrained by missing bounds [T_min, T_max], PABBO's exploration rate ρ, and SABBO's learning rate η

## Confidence
- High confidence: Empirical ranking of amortized vs evolutionary optimizers (PABBO/SABBO faster than GA/ES on convergence speed)
- Medium confidence: Mechanism claims for preference learning (PABBO) and sharpness-awareness (SABBO) given theoretical framing but limited ablation
- Low confidence: Generalization across task domains; robustness to hyperparameter choices; correspondence of flatness to held-out performance

## Next Checks
1. Perform ablation on SABBO's sharpness-awareness by disabling the neighborhood objective and comparing convergence speed and final perplexity
2. Test transfer robustness by training PABBO on synthetic functions, then fine-tuning on perplexity histories from one corpus before evaluating on a held-out corpus
3. Run a parameter sweep over GA/ES mutation scales and population sizes to establish sensitivity and determine whether reported underperformance is due to poor hyperparameter defaults