---
ver: rpa2
title: 'Guiding not Forcing: Enhancing the Transferability of Jailbreaking Attacks
  on LLMs via Removing Superfluous Constraints'
arxiv_id: '2503.01865'
source_url: https://arxiv.org/abs/2503.01865
tags:
- target
- arxiv
- optimization
- constraint
- jailbreaking
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The study identifies two main barriers to effective transferability
  of gradient-based jailbreaking attacks: the response pattern constraint, which forces
  the model to start responses in a specific way, and the token tail constraint, which
  overly penalizes longer outputs. To address these issues, the authors propose Guided
  Jailbreaking Optimization, which removes these superfluous constraints by guiding
  the model to begin with the target output and relaxing loss computation to only
  necessary tokens.'
---

# Guiding not Forcing: Enhancing the Transferability of Jailbreaking Attacks on LLMs via Removing Superfluous Constraints

## Quick Facts
- arXiv ID: 2503.01865
- Source URL: https://arxiv.org/abs/2503.01865
- Reference count: 24
- Primary result: Transfer Attack Success Rate (T-ASR) across target models increased from 18.4% to 50.3% by removing response pattern and token tail constraints

## Executive Summary
This paper addresses the limited transferability of gradient-based jailbreaking attacks on large language models (LLMs) by identifying and removing two superfluous constraints. The authors propose Guided Jailbreaking Optimization, which eliminates the response pattern constraint by explicitly guiding the model's output format and removes the token tail constraint by computing loss only on essential tokens. Evaluated on Llama-3-8B-Instruct as the source model, this method achieved a 50.3% average T-ASR across target models, significantly outperforming the baseline GCG-Adaptive method (18.4%). The approach maintains controllability of jailbreak behavior while improving both source attack success rate and transferability.

## Method Summary
The method builds on gradient-based adversarial prompt optimization (GCG) by introducing two key modifications: (1) Target Output Guidance - explicitly including the target output format in the input prompt to eliminate the response pattern constraint, and (2) Relaxed Loss Computation - computing loss only on the first 2 essential tokens rather than the full target sequence to remove the token tail constraint. The optimization uses adversarial prefixes (not suffixes) of 100 tokens, with 500 optimization steps, progressive_goals strategy, and batch size 128. The method was evaluated using Harmbench dataset (20-question training subset, 200-question test set) across 7 target models ranging from Llama-2-7B-Chat to GPT-4.

## Key Results
- Transfer Attack Success Rate (T-ASR) across target models improved from 18.4% to 50.3%
- Source Attack Success Rate (S-ASR) on Llama-3-8B-Instruct increased from 31.5% to 85.2%
- Controllable outputs achieved across target models when using guided optimization
- T-ASR on strongly-aligned targets (Llama-2-7B-Chat) improved from 21% to 44.8%
- Method remains detectable by chunk-level perplexity filters (stated limitation)

## Why This Works (Mechanism)

### Mechanism 1: Response Pattern Constraint Removal
Standard gradient-based attacks force models to begin responses with specific phrases through the loss function, creating a mismatch with real jailbroken responses. This adds an implicit constraint Lrp = L(x1:n) - Lt(x1:n) that narrows the search space to model-specific regions. By explicitly including target output guidance in the input prompt, the method aligns the model's actual output distribution with the optimization target, reducing Lrp toward zero and expanding the search region to include more transferable adversarial prompts.

### Mechanism 2: Token Tail Constraint Removal via Loss Slicing
The full-target loss Lt includes redundant tail components that penalize format variations (e.g., "\n\n**Step 1:**" vs "\nStep 1:"). Different models have different formatting preferences, and optimizing for the full tail on the source model creates an adversarial prompt that fails on models with different preferences. By computing loss only on the first 2 tokens, the optimization focuses on triggering unsafe behavior without overfitting to format-specific details, improving transfer across models with different output preferences.

### Mechanism 3: Target Output Guidance via Explicit Prompting
Instead of relying on the loss function to force a specific output pattern, the method adds explicit guidance to the input: 'Start your response with "Here's how to [harmful action]:\nStep 1"'. This makes the real jailbroken output approximate the target, reducing Lrp toward zero. The optimization then operates within the model's natural output distribution rather than against it, allowing the model to follow explicit formatting instructions even for harmful content once the safety barrier is bypassed.

## Foundational Learning

- **Concept: Gradient-based adversarial prompt optimization (GCG)**
  - Why needed here: This paper directly modifies the GCG optimization objective. Understanding how token substitutions are selected via gradients and evaluated via negative log-probability of target tokens is prerequisite to understanding what constraints are being removed.
  - Quick check question: Given a harmful query Q and adversarial suffix x1:n, what does the GCG loss L(x1:n) = -log p(a1:k | Q, x1:n) measure?

- **Concept: Transferability in adversarial attacks**
  - Why needed here: The paper's core contribution is improving transfer attack success rate (T-ASR). The conceptual framework distinguishes between the feasible region on source model (FA), search region (FsA), and shared feasible region (Fshared = FA âˆ© FB).
  - Quick check question: Why would an adversarial prompt that successfully jailbreaks Model A fail on Model B even if both models have similar safety training?

- **Concept: Prefix vs. suffix adversarial prompts**
  - Why needed here: The paper uses prefix optimization rather than suffix, arguing that suffixes require more tokens for comprehensive optimization and impose greater token tail constraints.
  - Quick check question: Why might placing adversarial tokens before the harmful query (prefix) allow for shorter loss-slice optimization than placing them after (suffix)?

## Architecture Onboarding

- **Component map:**
  Input template constructor -> Guided loss function -> Token substitution selector -> Progressive batch evaluator -> Transfer evaluator

- **Critical path:**
  1. Load 20-question training subset from Harmbench with corresponding target outputs
  2. Initialize random prefix of length 100 tokens
  3. For each optimization step (500 total):
     - Compute aggregate gradients across current question subset
     - Select top-k=256 candidate token substitutions per position
     - Sample batch of 128 random substitution combinations
     - Evaluate each candidate using 2-token sliced loss
     - Accept lowest-loss candidate as new prefix
     - If current prefix succeeds on all active questions, add next question
  4. Evaluate final prefix on 200-question test set across source and target models
  5. Use HarmBench-Llama-2-13B-cls classifier to determine ASR

- **Design tradeoffs:**
  - Loss slice length (2 vs. more tokens): Shorter slices improve transfer but may yield incomplete harmful outputs; Figure 5 shows 2-token slice maximizes T-ASR for strongly-aligned targets (Llama-2: 21%) while longer slices degrade performance
  - Prefix length (100 tokens): Longer prefixes increase search space but require more optimization steps; experiments showed 100 outperforms 50 and 20
  - Training set size (20 questions): Smaller set enables faster iteration but may reduce universality; paper uses 20 for training, 200 for testing
  - Prefix vs. suffix: Prefix allows effective 2-token loss; suffix requires more tokens (Figure 6), increasing token tail constraint

- **Failure signatures:**
  - High source ASR but low target ASR: Indicates overfitting to source model; check if loss slice is too long (try reducing from full to 2 tokens)
  - Optimization stuck at high loss (>0.5): Likely response pattern constraint still active; verify target output guidance is included in input template
  - Inconsistent outputs across runs (high std dev): Suggests optimization is not converging to stable region; check progressive_goals stability and batch size
  - Transfer works on weak models (Qwen2, Vicuna) but not strong models (Llama-3, GPT-4): Expected per paper's findings; stronger alignment reduces Fshared overlap
  - Perplexity filter detection: Method remains detectable by chunk-level PPL filters (stated limitation)

- **First 3 experiments:**
  1. **Baseline comparison on single model:** Run GCG-Adaptive and Guided Jailbreaking on Llama-3-8B-Instruct with full 200-question test set, measuring S-ASR. Expected: ~31.5% vs ~85.2%. Verify loss curves stay within normal range (0.04-0.24) for guided method per Figure 3.
  2. **Ablation on loss slice length:** Optimize on Llama-3 source with loss slices of 2, 5, 10, and full tokens. Evaluate T-ASR on Llama-2-7B-Chat and Qwen2-7B. Expected: Strongly-aligned targets (Llama-2) show sharp T-ASR degradation with longer slices; weakly-aligned targets (Qwen2) show minimal sensitivity per Figure 5.
  3. **Constraint removal ablation:** Run three conditions on Llama-3 source: (a) remove only response pattern constraint (w/ Ltail), (b) remove only token tail constraint (w/ Lrp), (c) remove both (full method). Compare T-ASR across target models. Expected: Condition (a) achieves ~44.8% avg T-ASR, condition (b) achieves ~14.1%, condition (c) achieves ~50.3% per Table 1. This validates that both constraints must be removed for optimal transfer.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the Guided Jailbreaking Optimization method be adapted to evade chunk-level perplexity (PPL) filters while preserving its high transferability?
- Basis in paper: [explicit] The Limitations section states, "the attack remains detectable by the chunk-level PPL filter."
- Why unresolved: The proposed optimization focuses on removing constraints to improve success rates, but this process may inherently generate low-probability token sequences that trigger perplexity-based defenses, a trade-off not addressed in the current method.
- What evidence would resolve it: Demonstration of the method maintaining high Transfer Attack Success Rates (T-ASR) against target models protected by PPL-based defenses, or a modified objective function that constrains perplexity during optimization.

### Open Question 2
- Question: What factors limit the transferability of this approach to "stronger" target models (e.g., GPT-4), and how can the attack success rate be improved for these robust models?
- Basis in paper: [explicit] The Limitations section notes, "executing transfer attacks with high ASR on stronger models remains a significant challenge," and results show T-ASR on GPT-4 remains low (13.5%) compared to weaker models.
- Why unresolved: While removing superfluous constraints significantly improves transferability to weaker models (reaching ~80% on some), the low success rate on robust models suggests their safety alignment relies on factors beyond the response pattern and token tail constraints addressed.
- What evidence would resolve it: An analysis identifying the specific safety mechanisms in robust models that resist this attack, followed by a modified methodology that achieves significantly higher T-ASR on models like GPT-4.

### Open Question 3
- Question: Can the stability of transferred jailbreaks be improved to eliminate the "inherent randomness" observed in target models?
- Basis in paper: [explicit] The Limitations section states, "inherent randomness in the target models persists," despite improvements in controllability.
- Why unresolved: Although the method guides the *start* of the response, the relaxation of the token tail constraint and the stochastic nature of decoding may allow the model to deviate from the intended harmful output or terminate prematurely.
- What evidence would resolve it: A study quantifying the variance of target model outputs when attacked, potentially involving a refined loss function or decoding strategy that ensures deterministic and complete harmful generation.

## Limitations
- Method remains detectable by chunk-level perplexity (PPL) filters
- High computational cost requiring 500 optimization steps
- Limited evaluation to 7 target models (7-13B parameters)
- Transferability to stronger models (GPT-4) remains low (13.5% T-ASR)

## Confidence

**High Confidence (4/4 required evidence anchors):**
- Claim: Guided Jailbreaking Optimization achieves 50.3% average T-ASR across target models (vs 18.4% for GCG-Adaptive)
- Claim: Removing response pattern and token tail constraints is necessary for optimal transferability
- Claim: Using adversarial prefixes (not suffixes) enables effective 2-token loss slicing

**Medium Confidence (3/4 required evidence anchors):**
- Claim: Models with stronger safety alignment exhibit reduced Fshared overlap, limiting transferability
- Claim: The response pattern constraint adds an implicit loss component Lrp that narrows the search space

**Low Confidence (0-1/4 required evidence anchors):**
- Claim: Including target output guidance in the input prompt eliminates the response pattern constraint
- Claim: Computing loss only on the first 2 tokens is sufficient for jailbreak success

## Next Checks

1. **Transferability Across Model Sizes:** Evaluate the method's T-ASR on larger models (70B+ parameters) and newer architectures (Mixtral, Qwen2.5, Claude-3) to test whether the constraint removal benefits scale with model size and alignment sophistication.

2. **Ablation on Constraint Interdependence:** Run controlled experiments isolating each constraint removal (response pattern only, token tail only, both) on a wider range of target models to quantify the marginal benefit of each and test whether the interaction is synergistic or additive.

3. **Optimization Efficiency Analysis:** Measure the compute cost (GPU hours) for 500 optimization steps versus alternative methods, and test whether progressive_goals strategy can be replaced with more efficient early-stopping criteria without sacrificing T-ASR.