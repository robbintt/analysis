---
ver: rpa2
title: Biomedical Question Answering via Multi-Level Summarization on a Local Knowledge
  Graph
arxiv_id: '2504.01309'
source_url: https://arxiv.org/abs/2504.01309
tags:
- graph
- claims
- documents
- question
- claim
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a novel method for biomedical question answering
  that addresses the challenge of capturing multi-document relationships in retrieval-augmented
  generation (RAG). The approach constructs a local knowledge graph from retrieved
  documents using propositional claims, then performs layerwise summarization to contextualize
  a small language model for QA tasks.
---

# Biomedical Question Answering via Multi-Level Summarization on a Local Knowledge Graph

## Quick Facts
- arXiv ID: 2504.01309
- Source URL: https://arxiv.org/abs/2504.01309
- Authors: Lingxiao Guan; Yuanhao Huang; Jie Liu
- Reference count: 15
- Primary result: Novel method achieves comparable or superior performance over RAG baselines on biomedical QA benchmarks, with accuracy improvements ranging from 0.01 to 0.09

## Executive Summary
This paper introduces a multi-level summarization approach for biomedical question answering that constructs a local knowledge graph from retrieved documents using decontextualized propositional claims. The method addresses the challenge of capturing multi-document relationships in retrieval-augmented generation by extracting atomic claims, building graph structures with entity deduplication, and performing layerwise summarization around key claims of interest. Evaluated on MMLU, PubMedQA, and MedQA benchmarks, the approach demonstrates superior integration of information across multiple documents while maintaining high faithfulness and source diversity in the generated context.

## Method Summary
The method consists of three main modules: (1) Relation Extraction that uses HyDE-based retrieval to obtain documents, then extracts atomic, decontextualized propositional claims and converts them to RDF triples; (2) Graph Creation that deduplicates entities using embedding similarity clustering (threshold 0.8), builds an undirected graph with claims and reranker scores as edges, and denoises by evaluating claim relevance through 1-hop neighbor context; (3) Graph Summarization that selects top-10 claims of interest through reranking and diversity filtering, then performs layerwise summarization from outermost connected layers inward, with each claim's summary incorporating context from connected claims in lower layers. The final concatenated summaries provide context for a small language model to answer biomedical questions.

## Key Results
- Achieves comparable or superior accuracy to RAG baselines across PubMedQA (0.56), MedQA (0.81), and MMLU clinical topics (0.80)
- Layerwise summarization achieves high faithfulness (0.9569) and source diversity (0.9647) while maintaining strong answer relevance (0.8414)
- Component-level analysis shows effectiveness of each step, particularly the graph-based approach's ability to integrate multi-document relationships
- The approach excels at preserving explicit relationships between medical concepts compared to semantic similarity clustering methods

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Decontextualized propositional claims enable more reliable cross-document connection than raw text chunks.
- Mechanism: Documents are decomposed into atomic, self-contained statements where all entity references are explicitly resolved. This allows subsequent graph construction to identify conceptual overlap even when original documents differ in semantic focus or terminology.
- Core assumption: LLMs can reliably extract atomic claims while preserving semantic meaning from source documents.
- Evidence anchors: [abstract] "utilizes propositional claims to construct a local knowledge graph from retrieved documents"; [Section 3.1] Claims must be "Atomic: includes only a single statement that cannot be broken down, and Decontextualized: fully understandable on its own"; [Table 2] Single-stage extraction achieves 0.901 semantic similarity preservation.
- Break condition: If entity coreference resolution fails (e.g., "it," "the protein"), graph edges will connect to placeholder nodes, fragmenting the knowledge structure.

### Mechanism 2
- Claim: Graph-based entity linking captures topical relationships that semantic similarity clustering misses.
- Mechanism: RDF triples (subject, predicate, object) are extracted from claims, with entity deduplication via embedding similarity clustering (threshold 0.8). This creates explicit edges between concepts that may not co-occur in similar textual contexts but share entities.
- Core assumption: Entities with embedding similarity >0.8 represent the same concept despite surface form differences.
- Evidence anchors: [abstract] "captures multi-document relationships more effectively than traditional RAG by representing information as decontextualized claims"; [Section 3.2] "Graph communities had higher relevance score compared to semantic communities 59.35% of the time"; [Table 3] Direct comparison showing graph-based summarization outperforms semantic clustering.
- Break condition: If deduplication threshold is too aggressive, distinct medical entities (e.g., "BRCA1" vs "BRCA2") may incorrectly merge; if too conservative, fragmented nodes prevent connection.

### Mechanism 3
- Claim: Layerwise topological summarization preserves path-based information while fitting context windows.
- Mechanism: Starting from claims of interest (top-10 reranked), connected components are organized into layers by hop distance. Summarization proceeds from outermost layer inward, with each claim's summary incorporating summaries from connected claims in lower layers. This creates context that reflects graph topology rather than just direct relevance.
- Core assumption: Information closer to claims of interest in graph distance is more valuable for answering the question.
- Evidence anchors: [abstract] "layerwise summarization to generate context for a small language model"; [Section 3.3] "capturing all the information in the local connected component, including both the direct content and path-based information"; [Table 4] Layerwise achieves highest source diversity (0.9647) while maintaining faithfulness (0.9569).
- Break condition: If claims of interest are poorly selected (low diversity), all summaries will reflect the same narrow perspective; if graph has deep chains, outer-layer information may be over-compressed.

## Foundational Learning

- Concept: RDF Triples (subject-predicate-object representation)
  - Why needed here: The entire graph construction depends on converting natural language claims into structured triples. Without understanding that "aspirin reduces inflammation" becomes (aspirin, reduces, inflammation), the extraction module design is opaque.
  - Quick check question: Given the claim "MET mutations are associated with lung cancer," what is the predicate?

- Concept: Decontextualization (resolving pronouns and implicit references)
  - Why needed here: Claims must be self-contained for cross-document matching. A claim like "it increases risk" is unusable without resolving "it" to a specific entity.
  - Quick check question: The sentence "The drug was approved in 2020. It showed efficacy."—what must be resolved before this becomes a decontextualized claim?

- Concept: Hop distance / graph topology
  - Why needed here: Layerwise summarization organizes claims by their distance from "claims of interest." Understanding that 1-hop neighbors share an edge with the root claim, while 2-hop neighbors require traversing two edges, is essential for debugging summarization depth.
  - Quick check question: If claim A connects to B, and B connects to C (but A does not connect to C), what is C's layer relative to A?

## Architecture Onboarding

- Component map:
  - Retrieval: Question rewriting → HyDE candidate generation → Multi-corpus retrieval (StatPearls, Wikipedia, textbooks, PubMed)
  - Relation Extraction: Claim extraction (single-stage LLM) → Triple extraction (one RDF triple per claim)
  - Graph Construction: Entity deduplication (embedding clustering, τ=0.8) → Edge creation with reranker scores → Denoising via 1-hop context evaluation
  - Summarization: Select top-10 claims → Test summary reranking → Filter adjacent claims → Layerwise summarization (outermost → root) → Concatenate summaries by rank

- Critical path: Retrieval quality → Claim extraction faithfulness → Entity deduplication accuracy → Claim of interest selection → Layerwise compression quality. Errors propagate forward; poor retrieval cannot be recovered by later stages.

- Design tradeoffs:
  - Single-stage vs two-stage claim extraction: Single-stage is faster; two-stage may improve decontextualization (Table 2 shows marginal difference: 0.941 vs 0.946)
  - Graph communities vs semantic clustering: Graph captures explicit relationships but may include noisy connections; semantic is cleaner but misses cross-topic links
  - Number of claims of interest: More claims increase coverage but dilute context window; paper uses top-10 with diversity filtering

- Failure signatures:
  - Low accuracy on PubMedQA (0.56 vs RAPTOR's 0.66): Authors attribute to "insufficient denoising"—check whether graph contains irrelevant claims that crowd out relevant ones
  - Low source diversity (<0.90): Summaries are over-relying on single documents; check whether graph construction is creating disconnected components
  - Low decontextualization score (<0.90): Claims contain unresolved references; check LLM prompt for claim extraction

- First 3 experiments:
  1. **Ablation on deduplication threshold**: Run graph construction with τ = 0.7, 0.8, 0.9 on MMLU Validation. Measure graph connectivity (average component size) vs accuracy. Expect tradeoff: lower τ merges more entities (larger components) but risks false merges.
  2. **Claims of interest count sweep**: Test k = 3, 5, 10, 15 top claims. Measure accuracy vs token count. Paper uses 10 with diversity filtering; smaller k may suffice for simpler questions.
  3. **Layerwise vs 1-hop baseline**: Compare full layerwise summarization against simple 1-hop subgraph summarization (no recursive aggregation). This isolates whether the topological compression provides value over flat neighborhood extraction.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can dynamic thresholding and improved conflict resolution mechanisms mitigate the denoising insufficiencies that caused the performance drop on the PubMedQA dataset?
- Basis in paper: The authors explicitly attribute the performance degradation on PubMedQA to "insufficient denoising in our created graph" and identify "dynamic thresholding and improved conflict resolution" as planned solutions.
- Why unresolved: The current static denoising approach, which evaluates claim importance based on 1-hop neighbors, appears insufficient for the specific noise profiles or contradictory information present in PubMedQA contexts.
- What evidence would resolve it: Successful application of the enhanced denoising techniques resulting in accuracy scores on PubMedQA that are comparable to or exceeding the RAPTOR baseline (0.66).

### Open Question 2
- Question: How can the computational efficiency of the pipeline be optimized to reduce the high number of LLM calls required for graph construction and summarization?
- Basis in paper: The conclusion states the intent to "improve the computational efficiency of our method, focusing on reducing the number of LLM calls for our graph construction and summarization."
- Why unresolved: The proposed method requires sequential LLM invocations for claim extraction, triple extraction, denoising, and layerwise summarization, making it inherently more resource-intensive than naive RAG or single-pass summarization techniques.
- What evidence would resolve it: A modified architecture that utilizes fewer LLM calls or caching strategies while maintaining the same level of accuracy on the MMLU and MedQA benchmarks.

### Open Question 3
- Question: Does treating all graph edges as undirected obscure critical causal or asymmetric relationships in biomedical reasoning?
- Basis in paper: Section 3.2 (Graph Construction) states: "All of the edges are treated as undirected in further processing," despite extracting RDF triples (subject, predicate, object) which inherently encode directionality.
- Why unresolved: Biomedical relationships (e.g., "treats", "causes", "induces") are often asymmetric; flattening these into undirected edges may disrupt the logical flow required for accurate inference in complex queries.
- What evidence would resolve it: An ablation study comparing the current undirected approach against a directed graph traversal method, specifically on reasoning-intensive questions where causal direction is pivotal.

### Open Question 4
- Question: Is the assumption of "atomicity" (one key relation per claim) a bottleneck for processing complex medical sentences that express multiple concurrent conditions or findings?
- Basis in paper: In Section 3.1 (Triple Extraction), the paper assumes "each one [claim] only having one key relation" and extracts a single RDF triple per claim.
- Why unresolved: Complex medical text often embeds nuanced, multi-part assertions within single sentences; forcing atomicity could fragment related evidence or discard secondary relations that are crucial for answering specific queries.
- What evidence would resolve it: Analysis of the "Key claim extraction" metric (currently 1.0) when the extraction prompt is relaxed to allow multiple triples per claim, to see if recall of relevant evidence improves.

## Limitations

- The method heavily depends on LLM performance for claim extraction and decontextualization, which may not scale to extremely diverse biomedical terminology
- The 0.8 entity deduplication threshold is arbitrary and may not generalize across different biomedical sub-domains
- The layerwise summarization requires careful tuning of claim selection and filtering parameters that aren't fully specified in the paper

## Confidence

- Method reproducibility: Medium - Requires detailed prompt templates and hyperparameter specifications that are not fully provided
- Benchmark validity: High - Uses established biomedical QA datasets (PubMedQA, MedQA, MMLU) with appropriate evaluation metrics
- Claim verification: Medium - Core mechanisms are supported by evidence, but some thresholds and design choices lack empirical justification

## Next Checks

1. Validate entity deduplication threshold: Test graph construction with τ = 0.7, 0.8, 0.9 on MMLU Validation to measure tradeoff between graph connectivity and accuracy
2. Assess claim extraction quality: Spot-check a sample of extracted claims for decontextualization quality and atomicity; compute Ref Score metric on sample
3. Evaluate denoising effectiveness: Monitor graph size pre/post denoising on PubMedQA to verify the 1-hop neighbor evaluation is filtering irrelevant claims while preserving important bridging information