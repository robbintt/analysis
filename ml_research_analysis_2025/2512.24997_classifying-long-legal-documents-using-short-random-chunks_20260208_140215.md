---
ver: rpa2
title: Classifying long legal documents using short random chunks
arxiv_id: '2512.24997'
source_url: https://arxiv.org/abs/2512.24997
tags:
- documents
- legal
- temporal
- document
- long
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a legal document classifier that uses small,
  randomly-selected chunks of text rather than entire documents. The approach employs
  DeBERTa V3 to generate embeddings from 48 chunks of up to 128 tokens each, followed
  by an LSTM layer to classify the document into one of 18 categories.
---

# Classifying long legal documents using short random chunks

## Quick Facts
- arXiv ID: 2512.24997
- Source URL: https://arxiv.org/abs/2512.24997
- Authors: Luis Adrián Cabrera-Diego
- Reference count: 13
- Primary result: Weighted F-score of 0.898 using random 48-chunk sampling with DeBERTa V3 and LSTM

## Executive Summary
This paper introduces a novel approach to classifying long legal documents by using randomly sampled text chunks rather than entire documents. The method employs DeBERTa V3 to generate embeddings from 48 chunks of up to 128 tokens each, followed by an LSTM layer to classify documents into 18 categories. Integrated into a Temporal workflow orchestration system, the classifier achieves strong performance (0.898 weighted F-score) while processing documents in about 498 seconds per 100 files on CPU. This approach addresses the challenge of handling long legal documents efficiently while maintaining classification accuracy.

## Method Summary
The method samples 48 random chunks (max 128 tokens each) from legal documents, processes them through DeBERTa V3 for embeddings, and aggregates using an LSTM layer for classification. The system is integrated with Temporal for workflow orchestration. Key design choices include using the [CLS] token from each chunk as input to the LSTM, preserving document order during processing, and optionally including document length features. The approach trades some classification signal for memory efficiency, enabling processing of documents far longer than typical transformer limits.

## Key Results
- Weighted F-score of 0.898 on test set (median of 30 runs)
- Processing time of approximately 498 seconds per 100 files on CPU
- Performance outperforms bag-of-words baseline (0.879 F-score) and approaches more complex models
- Random chunk sampling performs comparably to sophisticated chunk selection methods in some cases

## Why This Works (Mechanism)

### Mechanism 1
Random sampling of text chunks preserves sufficient class-discriminative signal for legal document classification without requiring full-document context. Legal documents exhibit consistent vocabulary and structural patterns distributed throughout, so random fragments capture class information without strategic selection. This assumes class-discriminative features are distributed rather than localized to specific sections.

### Mechanism 2
Hierarchical encoding (chunk-level transformer → document-level sequence model) enables long-document classification while respecting memory constraints. DeBERTa V3 processes each chunk independently, producing [CLS] embeddings, then LSTM models the sequence of 48 chunk embeddings. This avoids quadratic attention cost while capturing ordering effects.

### Mechanism 3
Per-epoch chunk resampling during training functions as implicit data augmentation, improving robustness to input variability. At each training epoch, the sampler re-selects random chunks from each document, exposing the model to different text fragments across iterations. This prevents overfitting to specific passages and forces the model to learn features that generalize across document positions.

## Foundational Learning

- **Transformer token limits and quadratic attention complexity**
  - Why needed: The paper's motivation stems from the inability to feed long legal documents into standard transformers due to memory explosion (O(n²) attention).
  - Quick check: Given a 10,000-token document, why can't you simply pass it all to BERT-base (512-token limit) in one forward pass?

- **[CLS] token as sentence/document representation**
  - Why needed: The architecture uses the [CLS] embedding from each chunk as the input to the LSTM; understanding this convention is essential for following the data flow.
  - Quick check: What does the [CLS] token represent in BERT-style models, and why is it used for classification tasks rather than averaging all token embeddings?

- **LSTM sequence modeling over sets**
  - Why needed: The LSTM aggregates chunk embeddings in document order; understanding how LSTMs maintain hidden states across timesteps clarifies why order matters (or doesn't).
  - Quick check: If you shuffled the 48 chunk embeddings before feeding them to an LSTM, would you expect the final hidden state to change? Why or why not?

## Architecture Onboarding

- **Component map**: HTML document → paragraph extraction → tokenization → 128-token chunks (stride 16) → random sample 48 chunks → DeBERTa V3 encoding → [CLS] embedding → context pooler (Dense+GELU) → LSTM(128) → optional length features → Dense(18) + Softmax

- **Critical path**: 1) Paragraph extraction from HTML (<p>, <li> tags) 2) Tokenization and 128-token chunking with 16-token overlap 3) Random sampling of N chunks per document 4) DeBERTa V3 forward pass → LSTM → classification 5) Temporal orchestration for batch processing

- **Design tradeoffs**: 
  - 48 chunks vs. full document: Trades classification signal for memory/compute efficiency
  - Random vs. strategic sampling: Simpler and faster but risks missing rare discriminative passages
  - LSTM vs. transformer aggregation: Lighter than second transformer layer but may capture less complex interactions
  - CPU deployment vs. GPU latency: 4.98 sec/file on CPU vs. ~0.055 sec/file on A100 GPU (10-100× slower but cheaper for batch workloads)

- **Failure signatures**: 
  - Low F-score on specific classes (Notice of intent 0.602-0.636, Pleadings 0.708-0.757)
  - Temporal payload overflow (>2MB request size triggers gRPC errors)
  - High variance across runs due to random chunk sampling
  - Misclassification patterns (Notice of intent ↔ Notice of Arbitration confusion)

- **First 3 experiments**:
  1. Baseline chunk count ablation: Compare 20, 48, and 62 chunks on held-out validation set to quantify performance vs. compute tradeoff
  2. Ordering vs. shuffling: Run inference with chunks in document order vs. randomly shuffled to test LSTM benefit
  3. Class-wise error analysis: Manually inspect misclassified documents for lowest-performing classes to identify error sources

## Open Questions the Paper Calls Out

### Open Question 1
Does the language of a document affect classification performance, particularly for poorly-performing classes? The multilingual corpus (25 languages) was used, but language-specific performance analysis was not conducted. Per-language breakdown of F-scores by class would resolve this.

### Open Question 2
Are annotation errors contributing to the poor classification of "Notice of intent" documents? Legal experts expected these documents to be shorter than "Notice of Arbitration," but corpus statistics showed the opposite, suggesting possible labeling inconsistencies.

### Open Question 3
Would an adaptive number of chunks based on document length improve classification performance compared to fixed-size sampling? Documents vary dramatically in length (median 1 page for "Other" vs. 38.8 pages for "Expert opinion"), suggesting a one-size-fits-all approach may be suboptimal.

### Open Question 4
Does preserving document order when feeding chunks to the LSTM provide meaningful signal, or would unordered aggregation perform equally well? The methodology states chunks are fed in document order, but no comparison was made against order-invariant aggregation methods.

## Limitations
- Performance gains over simpler baselines are modest (0.898 vs. 0.879), raising questions about added complexity
- Effectiveness depends on class features being distributed throughout documents rather than localized
- 48-chunk configuration appears somewhat arbitrary without optimality ablation studies
- CPU deployment time is substantial (498 seconds per 100 files) despite GPU acceleration being available

## Confidence

- **High Confidence**: Architectural design and implementation details are clearly specified and technically sound
- **Medium Confidence**: Random sampling performs comparably to sophisticated chunk selection methods, but lacks direct comparison experiments
- **Medium Confidence**: Per-epoch chunk resampling as data augmentation is asserted but not empirically validated
- **Low Confidence**: Generalizability of 0.898 F-score to other legal document corpora remains unproven due to proprietary dataset

## Next Checks

1. **Chunk Count Sensitivity Analysis**: Systematically vary the number of sampled chunks (10, 20, 30, 48, 60, 80) on held-out validation set to determine optimal configuration

2. **Ordering vs. Permutation Experiment**: Compare performance when feeding chunks to LSTM in original document order versus randomly shuffled to test if sequential modeling provides benefit

3. **Low-Performing Class Investigation**: For Notice of intent and Settlement agreement classes, conduct manual error analysis on misclassified documents to determine error sources (annotation issues, insufficient data, or genuine need for full context)