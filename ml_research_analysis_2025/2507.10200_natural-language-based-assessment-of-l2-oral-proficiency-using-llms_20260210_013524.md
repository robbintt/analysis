---
ver: rpa2
title: Natural Language-based Assessment of L2 Oral Proficiency using LLMs
arxiv_id: '2507.10200'
source_url: https://arxiv.org/abs/2507.10200
tags:
- assessment
- language
- descriptors
- each
- scores
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a natural language-based assessment (NLA)
  method for second language (L2) oral proficiency that leverages open-source LLMs
  with CEFR analytic descriptors in a zero-shot setting. The approach uses the Qwen
  2.5 72B LLM to evaluate learner speech transcriptions, applying established language
  proficiency descriptors across multiple dimensions to predict holistic scores.
---

# Natural Language-based Assessment of L2 Oral Proficiency using LLMs

## Quick Facts
- arXiv ID: 2507.10200
- Source URL: https://arxiv.org/abs/2507.10200
- Reference count: 0
- Zero-shot text-only LLM assessment matches speech models fine-tuned on read-aloud data

## Executive Summary
This paper introduces a Natural Language-based Assessment (NLA) method that uses open-source LLMs with CEFR analytic descriptors to assess L2 oral proficiency from speech transcriptions. The approach employs Qwen 2.5 72B in a zero-shot setting, applying established language proficiency descriptors across multiple dimensions to predict holistic scores. Experiments on the S&I Corpus demonstrate competitive performance, particularly in mismatched task settings, while offering greater interpretability through explainable language descriptors.

## Method Summary
The method uses Qwen 2.5 72B (4-bit quantized) to evaluate learner speech transcriptions by applying CEFR analytic descriptors across 10 proficiency aspects (A1-C2 levels). For each aspect, the LLM receives randomized descriptor orderings, and logit probabilities are extracted and softmaxed to produce continuous Fair Average scores. These analytic scores are aggregated per exam part and overall, optionally weighted via Ridge regression trained on dev-set predictions. The approach relies on Whisper small ASR for transcription generation.

## Key Results
- Text-only NLA achieves PCC=0.761 on eval, matching a speech LLM fine-tuned on read-aloud data (PCC=0.750)
- Surpasses BERT-based model trained specifically for L2 assessment (PCC=0.727)
- Particularly effective in mismatched task settings where only non-representative training data exists

## Why This Works (Mechanism)

### Mechanism 1
Zero-shot descriptor application enables assessment without task-specific training data. CEFR analytic descriptors are fed directly as natural language instructions to the LLM alongside ASR transcriptions, and the model selects appropriate proficiency levels for each aspect. Logit probabilities over option labels are extracted and softmaxed to produce continuous scores rather than discrete classes. This assumes LLMs have internalized sufficient metalinguistic knowledge to apply CEFR descriptors consistently to learner language, even without acoustic features.

### Mechanism 2
Aggregating multiple analytic scores reconstructs holistic proficiency with interpretability. Ten analytic scores (e.g., grammatical accuracy, coherence, fluency) are averaged per exam part, then averaged across parts for an overall score. Ridge regression trained on dev-set analytic predictions can reweight aspects per task type, revealing which competences drive holistic outcomes. This assumes analytic scores capture partially distinct proficiency dimensions that collectively approximate holistic judgments.

### Mechanism 3
Text-based NLA is robust to task mismatch where only non-representative training data exists. Because NLA uses zero-shot inference with general descriptors, it requires no task-specific fine-tuning. In mismatched scenarios (e.g., only read-aloud training data available for spontaneous speech assessment), NLA matches a speech LLM fine-tuned on read-aloud data. This assumes CEFR descriptors generalize across task types sufficiently for zero-shot transfer.

## Foundational Learning

- **CEFR Can-Do Descriptors**: Standardized proficiency descriptions used to prompt the LLM; understanding their structure (levels A1–C2, analytic categories) is essential for prompt design and score interpretation. *Quick check: Can you name at least three CEFR analytic aspects and explain why "fluency" might be harder to assess from text than "grammatical accuracy"?*

- **Zero-Shot Inference with Logit Extraction**: The method does not fine-tune; it extracts probabilities over label tokens to produce continuous scores. Engineers must understand prompting strategies, positional bias, and probability calibration. *Quick check: How does extracting softmaxed logits over option labels differ from parsing free-form text responses, and why might the former be more stable?*

- **Positional Bias in LLM Multiple-Choice Prompting**: The paper randomizes descriptor order across three runs to mitigate LLM preference for earlier options; Table 2 shows JSD varies by aspect. *Quick check: Why does positional bias matter when presenting CEFR levels A–F to an LLM, and what does low JSD indicate about model stability?*

## Architecture Onboarding

- **Component map**: ASR transcriptions + CEFR descriptors -> Qwen 2.5 72B LLM -> Logit extraction -> Fair Average scores -> Per-part aggregation -> Overall holistic score
- **Critical path**: 1) Generate ASR transcriptions for learner audio. 2) For each of 10 proficiency aspects, construct a prompt with randomized CEFR descriptors. 3) Extract logit probabilities for each label, apply softmax, compute Fair Average per aspect. 4) Repeat for 3 random descriptor orders; average the three Fair Average scores per aspect. 5) Aggregate 10 analytic scores per exam part; average across parts or pass through Ridge models.
- **Design tradeoffs**: Text-only vs. audio (loss of phonological control and partial loss of fluency signals; gain in zero-shot generalization and interpretability); Zero-shot vs. fine-tuning (no training data required, but gap remains vs. speech LLMs fine-tuned on matched spontaneous data); Fair Average vs. discrete classification (handles mid-level human scores better but assumes ordinal scale validity); Ridge regression weighting (improves correlation but introduces dev-set dependency).
- **Failure signatures**: High JSD across prompt orders indicates positional sensitivity; analytic scores showing no significant pairwise differences suggest redundancy; large dev–eval performance gap suggests data distribution shift.
- **First 3 experiments**: 1) Reproduce Q+NLA on S&I dev/eval with 10 aspects, 3 random orders; compute per-aspect JSD and compare to Table 2. 2) Ablate descriptor order randomization to quantify positional bias impact on correlation and score distribution. 3) Replace Ridge aggregation with simple averaging; measure delta in PCC/SRC and analyze which aspects lose predictive weight.

## Open Questions the Paper Calls Out
- What specific data characteristics or model sensitivities cause the performance drop in text-based systems on the evaluation set compared to the development set?
- Can the Natural Language-based Assessment (NLA) framework be effectively generalized to multilingual assessment tasks?
- To what extent does the inability to evaluate phonological control limit the holistic accuracy of text-based NLA?

## Limitations
- Text-based systems performed worse on the evaluation set compared to the development set, with root causes requiring further investigation
- Exclusion of acoustically grounded aspects like phonological control from assessment
- ASR dependency introduces additional error source not fully characterized

## Confidence
- **High**: Experimental methodology, statistical analysis (Friedman/Nemenyi tests), correlation metrics
- **Medium**: Zero-shot LLM performance claims, generalizability to other languages and proficiency frameworks
- **Low**: ASR error rates and their impact on assessment quality, long-term stability of descriptor interpretation

## Next Checks
1. Measure the Pearson correlation between ASR word error rates and NLA assessment accuracy across different proficiency levels to quantify the transcription bottleneck.

2. Apply the identical NLA pipeline to L2 oral proficiency data from a different language family (e.g., Mandarin or Arabic speakers learning English) to test descriptor generalizability.

3. Conduct a 6-month stability study where the same LLM model is used to assess the same corpus multiple times to detect any drift in descriptor interpretation or scoring consistency.