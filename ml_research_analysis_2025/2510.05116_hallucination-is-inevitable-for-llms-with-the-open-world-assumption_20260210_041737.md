---
ver: rpa2
title: Hallucination is Inevitable for LLMs with the Open World Assumption
arxiv_id: '2510.05116'
source_url: https://arxiv.org/abs/2510.05116
tags:
- hallucination
- training
- world
- errors
- assumption
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper reframes LLM hallucination as a generalization problem\
  \ under the Open World assumption. While under the Closed World assumption hallucination\
  \ can be mitigated, the paper argues that under the Open World assumption\u2014\
  where space, time, and tasks are unbounded\u2014hallucination is inevitable."
---

# Hallucination is Inevitable for LLMs with the Open World Assumption

## Quick Facts
- **arXiv ID**: 2510.05116
- **Source URL**: https://arxiv.org/abs/2510.05116
- **Reference count**: 19
- **Primary result**: Reframes LLM hallucination as a generalization problem under Open World assumption; argues Type-II hallucination is structurally inevitable

## Executive Summary
This paper argues that LLM hallucination should be understood as a generalization problem that becomes inevitable under the Open World assumption. While hallucination under Closed World conditions can be mitigated through better training, the paper contends that when dealing with unbounded environments (space, time, tasks), false generalization (Type-II hallucination) cannot be eliminated. The work distinguishes between Type-I hallucination (false memorization, corrigible) and Type-II hallucination (false generalization, inescapable), suggesting that the focus should shift from eliminating hallucination to making inevitable errors intelligible to humans.

## Method Summary
The paper provides a theoretical analysis reframing hallucination as a machine learning generalization problem. It employs the No Free Lunch theorem to argue that no learning algorithm can guarantee correct outputs on unseen inputs, and draws on Hume's problem of induction to establish the inevitability of Type-II hallucination under Open World conditions. The analysis distinguishes between Type-I errors (false memorization correctable through better training) and Type-II errors (false generalization arising from extrapolation beyond known data). No empirical experiments are conducted; the work is purely conceptual.

## Key Results
- Hallucination is a manifestation of the classical generalization problem in machine learning
- Type-I hallucination (false memorization) is corrigible through better training data and optimization
- Type-II hallucination (false generalization) is structurally inevitable under Open World assumption where environments are unbounded
- Training hallucination detectors merely displaces rather than solves the problem

## Why This Works (Mechanism)

### Mechanism 1: Hallucination as Generalization Problem
- Claim: LLM hallucination can be reduced to the classical machine learning generalization problem
- Mechanism: An LLM learns a mapping f(x) from context to token sequences. When test inputs fall outside training distribution, the model must extrapolate. Per the No Free Lunch theorem, no learned mapping is universally superior; the model's output "H" may diverge from ground truth "F" simply because training data could not anticipate all future cases.
- Core assumption: The model operates on finite training data but faces potentially unbounded test inputs.
- Evidence anchors:
  - [abstract]: "This paper reframes 'hallucination' as a manifestation of the generalization problem."
  - [Section 3]: "no optimization algorithm is universally superior... formalized in the 'No Free Lunch' theorem."
  - [corpus]: "How Large Language Models are Designed to Hallucinate" similarly frames hallucination as a structural outcome of transformer design.
- Break condition: If training distribution fully covers test distribution (Closed World), generalization error becomes theoretically controllable.

### Mechanism 2: Type-II Hallucination Under Open World Assumption
- Claim: When environments are unbounded in space, time, and tasks, false generalization (HT-II) is structurally inevitable rather than merely probable.
- Mechanism: The Open World assumption asserts that future cases may violate past regularities. No finite experience set can guarantee future correctness—echoing Hume's induction problem. Unlike Type-I (false memorization, corrigible via better training), Type-II arises precisely because generalization beyond known data is required and fallible.
- Core assumption: AGI-targeted systems must operate where "past experience does not guarantee future accuracy."
- Evidence anchors:
  - [Section 4]: "HT-II arises directly from the Open World assumption, which precludes any guarantee of consistency between training and test sets. Such errors cannot be eliminated."
  - [Section 4]: Cites Russell's "inductive turkey" as illustration of inductive limits.
  - [corpus]: Related work on code hallucinations documents similar generalization failures in programming tasks.
- Break condition: If the task domain is bounded and stable (e.g., arithmetic with fixed rules), Type-II risk diminishes substantially.

### Mechanism 3: Detector Displacement Fails to Resolve Hallucination
- Claim: Training a secondary model to detect hallucinations does not solve the inevitability problem.
- Mechanism: A hallucination detector itself must generalize from its training data to novel inputs. Under Open World conditions, the detector faces the same inductive gap—it cannot guarantee correct judgments on unseen cases. The problem shifts location without fundamental resolution.
- Core assumption: Detectors operate under the same finite-experience constraints as generators.
- Evidence anchors:
  - [Section 4]: "Training a detector is not promising – it still needs generalization... the hallucination problem is merely displaced from the generator to the detector."
  - [corpus]: Limited direct evidence; neighbor papers focus on generation-side mitigation.
- Break condition: If the detector has exhaustive knowledge of all valid outputs (Closed World), displacement could work—but this negates the Open World premise.

## Foundational Learning

- **Generalization in Supervised Learning**
  - Why needed: The paper's central reduction—hallucination IS generalization error. Without this, the inevitability argument is opaque.
  - Quick check question: Given training points (1,2), (2,4), (3,6), what output does your model predict for x=10, and why might it be wrong?

- **No Free Lunch Theorem**
  - Why needed: Formal justification that no learning algorithm dominates across all possible test distributions; grounds the claim that we cannot engineer away generalization error.
  - Quick check question: Why can't we design a learning algorithm that is guaranteed to generalize better than random guessing on arbitrary test distributions?

- **Open vs. Closed World Assumptions**
  - Why needed: Determines whether hallucination is a fixable bug (Closed) or a structural feature (Open). Critical for setting realistic system expectations.
  - Quick check question: A medical diagnosis AI trained on Earth data is deployed on a Mars colony. Which assumption does this violate, and what hallucination type becomes inevitable?

## Architecture Onboarding

- **Component map**:
  Input context x → Learned mapping f(·) → Output sequence [g⁰(x), g¹(x), ..., gⁿ(x)]
  Optional: Detector model D(·) applied to output (subject to same generalization limits)
  Feedback loop: RLHF or fine-tuning can reduce HT-I but cannot eliminate HT-II

- **Critical path**:
  1. Identify deployment context: Open World (AGI, exploration, novel domains) vs. Closed World (constrained tasks)
  2. If Open: Accept HT-II inevitability; design for error tolerance and adaptivity
  3. Implement transparency mechanisms (concept-centered representations) so errors are "intelligible and acceptable to humans"

- **Design tradeoffs**:
  - Abstention capability ("I don't know") reduces false outputs but sacrifices generalization utility
  - Narrower domain scope reduces HT-II but limits system applicability
  - Transparent representations may sacrifice raw performance for human-interpretability of errors

- **Failure signatures**:
  - HT-I: Model contradicts facts present in training data (e.g., wrong capital when trained correctly)—indicates memorization or optimization failure
  - HT-II: Model produces plausible but incorrect output on genuinely novel input—indicates generalization gap, not fixable via more data
  - Detector failure: Secondary model approves hallucinated output on out-of-distribution input

- **First 3 experiments**:
  1. **Distribution shift probe**: Train on dataset D, test on deliberately shifted D'. Measure HT-I vs HT-II rates by checking which errors correspond to facts present vs absent in D.
  2. **Abstension utility tradeoff**: Implement "I don't know" calibration; measure accuracy vs coverage. Does reducing HT-II via abstention unacceptably limit useful generalization?
  3. **Detector stress test**: Train hallucination detector on generator outputs; evaluate on out-of-distribution inputs. Confirm detector itself exhibits generalization failures per displacement argument.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can logical or concept-centered representations be effectively integrated into LLMs to make inevitable Type-II hallucinations intelligible to humans?
- Basis in paper: [explicit] The author states that "a promising direction is to adopt more transparent representational schemes, such as logical and concept-centered representations," to ensure that when errors occur, they are "interpreted as reasonable."
- Why unresolved: The paper provides the theoretical motivation but does not propose specific architectures or training objectives to merge these symbolic approaches with subsymbolic deep learning models.
- What evidence would resolve it: Demonstration of an LLM architecture where outputs map to interpretable concept graphs, allowing users to trace hallucinations back to specific, albeit incorrect, logical inferences.

### Open Question 2
- Question: Can a secondary hallucination detector be trained to identify Type-II errors without succumbing to the same generalization limitations as the generator?
- Basis in paper: [explicit] The paper argues that "training a detector is not promising" because "the detector itself must generalize beyond the data on which it is trained" and will inevitably face novel cases.
- Why unresolved: This is presented as a theoretical limitation ("the problem is merely displaced"), but it remains unclear if specialized, narrower detector models might achieve statistically lower error rates than general generators.
- What evidence would resolve it: Empirical studies comparing the generalization error rates of specialized verification models against generative models on Out-of-Distribution (OOD) tasks.

### Open Question 3
- Question: How can we operationally distinguish Type-I hallucination (false memorization) from Type-II hallucination (false generalization) in models with opaque training histories?
- Basis in paper: [inferred] The paper distinguishes Type-I (corrigible, data-consistent) from Type-II (inevitable, data-inconsistent) based on the relationship between output and training data. However, it assumes access to this relationship, which is rarely available in "black box" LLMs.
- Why unresolved: Without a method to determine if a specific fact was "present" in the training data versus inferred, the practical application of the paper's theoretical distinction remains impossible.
- What evidence would resolve it: Development of interpretability methods or influence functions that can reliably quantify the presence of specific knowledge in training data versus model inference.

## Limitations
- Lacks empirical validation of the theoretical framework
- Does not provide concrete architectural solutions for making Type-II hallucinations intelligible
- No specific datasets, models, or evaluation protocols to test the HT-I vs HT-II classification empirically

## Confidence
- **High**: Hallucination as generalization problem - well-grounded in established ML theory
- **Medium**: Type-II inevitability under Open World - logically sound but empirically unverified
- **Low**: Detector displacement failure - minimal direct evidence, largely theoretical

## Next Checks
1. **Operationalize HT-I vs HT-II distinction** through controlled experiments with verifiable training/test splits
2. **Test detector displacement hypothesis** by training a hallucination detector and evaluating its performance on out-of-distribution inputs
3. **Validate Open World assumptions** by constructing environments with progressively less bounded tasks and measuring hallucination rates across the spectrum