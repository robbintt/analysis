---
ver: rpa2
title: 'Position: The AI Conference Peer Review Crisis Demands Author Feedback and
  Reviewer Rewards'
arxiv_id: '2505.04966'
source_url: https://arxiv.org/abs/2505.04966
tags:
- review
- peer
- reviewers
- reviews
- reviewer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The AI conference peer review system faces a crisis driven by massive
  submission growth and declining review quality. The paper proposes a bi-directional
  review framework with author feedback on reviews and reviewer rewards to create
  accountability.
---

# Position: The AI Conference Peer Review Crisis Demands Author Feedback and Reviewer Rewards

## Quick Facts
- arXiv ID: 2505.04966
- Source URL: https://arxiv.org/abs/2505.04966
- Authors: Jaeho Kim; Yunseok Lee; Seulki Lee
- Reference count: 40
- Primary result: Proposes bi-directional review framework with author feedback and reviewer rewards to address AI conference peer review crisis

## Executive Summary
The AI conference peer review system faces a crisis driven by massive submission growth and declining review quality. The paper proposes a bi-directional review framework where authors provide feedback on reviews while reviewers receive formal rewards. A two-stage review process is introduced where initial neutral reviews are evaluated by authors for comprehension and constructiveness before critical content is released, minimizing retaliation risks. Reviewers can earn digital badges and impact scores as formal academic credentials. Analysis of ICLR data shows spelling error rates dropped 28.8% from 2017-2024, suggesting increased AI use in reviews. The proposed system aims to improve review quality, motivate reviewers, and sustain conference standards.

## Method Summary
The proposed method introduces a bi-directional review framework consisting of two main components: author feedback mechanisms and reviewer reward systems. The two-stage review process begins with reviewers submitting neutral initial reviews, which authors then evaluate for comprehension and constructiveness. Only after this evaluation is critical content released. Reviewers earn digital badges and impact scores that serve as formal academic credentials. The system aims to create accountability while preventing retaliation through its staged approach.

## Key Results
- Spelling error rates in ICLR reviews decreased 28.8% (7.79 to 5.54 per 1000 words) from 2017-2024
- Proposed two-stage review process separates neutral feedback from critical content to prevent retaliation
- Digital badges and impact scores provide formal academic credentials for reviewers

## Why This Works (Mechanism)
The mechanism works by creating accountability in both directions: authors can provide feedback on reviews while reviewers receive formal recognition for their work. The two-stage process allows authors to evaluate review quality before seeing critical feedback, reducing bias. Digital badges and impact scores create tangible career benefits for high-quality reviewing, addressing the current lack of incentives in the peer review system.

## Foundational Learning
1. **Peer review crisis** - Understanding the current challenges in AI conference reviewing including submission growth and quality decline
   - Why needed: Establishes the problem context and urgency for reform
   - Quick check: Compare submission growth rates vs reviewer pool growth

2. **Two-stage review process** - Mechanism for separating neutral feedback from critical content
   - Why needed: Prevents retaliation while maintaining review quality assessment
   - Quick check: Verify authors can accurately assess review comprehension without seeing criticism

3. **Digital badges and impact scores** - Formal recognition system for reviewers
   - Why needed: Provides career incentives currently missing from peer review
   - Quick check: Determine if academic institutions value these credentials in hiring/promotion

4. **Author feedback mechanisms** - Systems for authors to evaluate review quality
   - Why needed: Creates accountability for reviewers beyond simple acceptance/rejection
   - Quick check: Measure correlation between author feedback scores and review quality

5. **AI usage detection** - Methods for identifying AI-assisted reviews through error patterns
   - Why needed: Understands how technology is changing review practices
   - Quick check: Analyze linguistic patterns beyond simple spelling errors

## Architecture Onboarding
Component map: Author feedback system -> Two-stage review process -> Reviewer rewards system -> Conference management platform
Critical path: Submission -> Initial review (neutral) -> Author evaluation -> Critical review release -> Reviewer rewards
Design tradeoffs: Between transparency and preventing retaliation; between formal rewards and maintaining review integrity
Failure signatures: Author retaliation against critical reviews; reviewers gaming reward system; false positives in AI detection
First experiments: 1) Pilot two-stage process with controlled author-reviewer pairs 2) A/B test reward systems vs traditional reviewing 3) Longitudinal study of spelling error patterns across multiple venues

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions in the provided content.

## Limitations
- Lack of empirical validation data for the bi-directional review system itself
- Speculative claims about digital badges and impact scores motivating reviewers without evidence
- Two-stage review process lacks validation on author assessment reliability

## Confidence
High confidence: The documented peer review crisis and need for accountability mechanisms
Medium confidence: Spelling error statistics showing AI usage trends and theoretical framework
Low confidence: Effectiveness of proposed reviewer rewards and practical implementation of two-stage process

## Next Checks
1. Conduct a pilot study implementing the bi-directional review system at a smaller conference to measure actual impact on review quality and reviewer motivation
2. Survey both authors and reviewers about their experiences with current review processes to establish baseline data for improvement metrics
3. Analyze whether existing academic incentive structures actually incorporate digital badges and impact scores into hiring, promotion, and tenure decisions