---
ver: rpa2
title: Enhancing Zero-Shot Time Series Forecasting in Off-the-Shelf LLMs via Noise
  Injection
arxiv_id: '2512.20140'
source_url: https://arxiv.org/abs/2512.20140
tags:
- noise
- forecasting
- nlts
- data
- series
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a noise injection strategy to enhance the
  zero-shot time series forecasting performance of off-the-shelf large language models
  (LLMs). By injecting controlled noise into raw time series before tokenization,
  the method compels frozen LLMs to base predictions on robust temporal patterns rather
  than numerical artifacts, acting as inference-time augmentation.
---

# Enhancing Zero-Shot Time Series Forecasting in Off-the-Shelf LLMs via Noise Injection

## Quick Facts
- **arXiv ID**: 2512.20140
- **Source URL**: https://arxiv.org/abs/2512.20140
- **Reference count**: 40
- **Primary result**: Introduces noise injection strategy that improves zero-shot time series forecasting performance of off-the-shelf LLMs by forcing reliance on robust temporal patterns rather than numerical artifacts.

## Executive Summary
This paper presents a noise injection approach to enhance zero-shot time series forecasting capabilities of frozen large language models. By injecting controlled noise into raw time series data before tokenization, the method forces LLMs to focus on robust temporal patterns rather than numerical artifacts during prediction. The technique acts as inference-time augmentation without requiring fine-tuning or architectural modifications. The authors provide both theoretical analysis and extensive empirical validation across multiple benchmarks, demonstrating consistent performance improvements. To address data contamination concerns, they introduce two novel contamination-free datasets (synthetic and recent stock data) where their approach consistently outperforms baselines, confirming genuine generalization rather than memorization.

## Method Summary
The core innovation is a noise injection strategy applied during inference to raw time series data before tokenization. The method involves injecting controlled Gaussian noise into the time series, which is then processed through the frozen LLM's existing tokenizer and decoder architecture. This forces the model to rely on temporal patterns rather than exact numerical values for forecasting. The approach is non-invasive and requires no fine-tuning, making it applicable to any off-the-shelf LLM. Theoretical analysis connects this noise injection to frequency domain robustness, suggesting that the noise helps the model focus on low-frequency temporal patterns that are more stable and generalizable. The method is evaluated across multiple LLMs and diverse benchmarks, showing consistent gains over baseline approaches.

## Key Results
- Noise injection consistently improves zero-shot time series forecasting performance across multiple LLMs and datasets
- The approach outperforms baselines on novel contamination-free datasets (synthetic and recent stock data), validating genuine generalization
- The method requires no fine-tuning or architectural changes, making it broadly applicable to off-the-shelf LLMs
- Theoretical analysis demonstrates connection between noise injection and frequency domain robustness

## Why This Works (Mechanism)
The noise injection mechanism works by forcing frozen LLMs to rely on robust temporal patterns rather than exact numerical values when making predictions. When clean time series data is fed directly into an LLM, the model may overfit to specific numerical patterns or artifacts that don't generalize well. By adding controlled noise, the model is compelled to identify and utilize more stable, low-frequency temporal structures that remain consistent despite the injected perturbations. This acts as an inference-time augmentation strategy that enhances the model's ability to generalize to unseen data. The frequency domain analysis suggests that noise injection effectively filters out high-frequency noise and numerical artifacts while preserving the essential temporal dynamics needed for accurate forecasting.

## Foundational Learning

**Time Series Forecasting** - The task of predicting future values in a sequence based on historical observations. *Why needed*: This is the core problem being addressed and understanding its challenges (non-stationarity, seasonality, noise) is crucial for appreciating the noise injection approach.

**Tokenization in LLMs** - The process of converting raw input data into discrete tokens that the model can process. *Why needed*: Understanding how time series data is tokenized is essential for grasping why noise injection before tokenization affects model behavior.

**Zero-Shot Learning** - Making predictions on tasks without any task-specific training or fine-tuning. *Why needed*: The paper focuses specifically on zero-shot performance, which is a challenging and practical setting for time series forecasting.

**Frequency Domain Analysis** - Examining signals in terms of their frequency components rather than time domain representation. *Why needed*: The theoretical analysis of why noise injection works relies on understanding how noise affects different frequency components of the time series.

**Inference-Time Augmentation** - Techniques applied during inference to improve model robustness or performance without modifying the model itself. *Why needed*: The noise injection is positioned as a form of inference-time augmentation, which is an important distinction from training-time methods.

## Architecture Onboarding

**Component Map**: Raw Time Series -> Noise Injection -> Tokenizer -> LLM Decoder -> Predictions

**Critical Path**: The critical path for the noise injection approach is: time series data → noise injection → tokenization → LLM processing → forecasting output. The noise injection step is the key modification that differentiates this approach from standard LLM inference.

**Design Tradeoffs**: The primary tradeoff is between noise magnitude and performance. Too little noise may not sufficiently disrupt numerical artifact reliance, while too much noise could overwhelm the underlying temporal patterns. The method requires careful calibration of noise parameters for optimal performance.

**Failure Signatures**: Potential failures include: (1) degraded performance when optimal noise parameters are not selected, (2) reduced effectiveness on LLMs with strong numerical reasoning capabilities, (3) diminished gains on non-stationary time series where temporal patterns themselves are unstable.

**First Experiments**:
1. Compare forecasting accuracy with varying noise magnitudes to identify optimal noise level for a given dataset
2. Evaluate performance degradation when noise is applied to LLMs specifically trained on time series data
3. Test the method on both stationary and non-stationary time series to assess robustness to different temporal dynamics

## Open Questions the Paper Calls Out
None specified in the provided content.

## Limitations
- Optimal noise magnitude is problem-dependent with no systematic tuning method provided
- Performance gains may diminish on LLMs with strong numerical reasoning or time series-specific training
- Limited evaluation scope with contamination-free datasets being less diverse than established benchmarks
- Theoretical analysis remains somewhat abstract without direct empirical validation of pattern learning claims

## Confidence

**High Confidence**: The empirical demonstration of improved zero-shot forecasting performance across multiple LLMs and datasets is well-supported. The contamination-free dataset construction methodology is sound and addresses a real concern.

**Medium Confidence**: The claim that noise injection acts as "inference-time augmentation" is plausible but could benefit from more rigorous theoretical grounding. The assertion about focusing on "robust temporal patterns" is reasonable but not definitively proven.

**Low Confidence**: The generalizability claim to "any off-the-shelf LLM" is overstated given limited model diversity tested. The paper does not adequately address potential performance degradation on non-stationary or high-noise real-world time series.

## Next Checks

1. Conduct ablation studies comparing frequency domain characteristics of predictions with and without noise injection to empirically validate the claim about learning robust temporal patterns versus numerical artifacts.

2. Test the method on a broader range of LLMs including those specifically trained on time series data and models with varying numerical reasoning capabilities to establish clear boundaries of effectiveness.

3. Evaluate performance on real-world non-stationary time series with varying noise characteristics to assess practical robustness beyond controlled benchmark conditions.