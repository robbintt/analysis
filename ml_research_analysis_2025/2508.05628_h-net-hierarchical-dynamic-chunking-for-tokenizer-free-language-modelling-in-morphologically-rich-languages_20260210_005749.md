---
ver: rpa2
title: 'H-Net++: Hierarchical Dynamic Chunking for Tokenizer-Free Language Modelling
  in Morphologically-Rich Languages'
arxiv_id: '2508.05628'
source_url: https://arxiv.org/abs/2508.05628
tags:
- h-net
- persian
- zwnj
- morphological
- segmentation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes H-Net++, a tokenizer-free language model for
  morphologically-rich languages like Persian. The key innovation is a hierarchical
  dynamic chunking architecture that learns segmentation boundaries jointly with language
  modeling, using a lightweight transformer mixer to propagate cross-chunk context.
---

# H-Net++: Hierarchical Dynamic Chunking for Tokenizer-Free Language Modelling in Morphologically-Rich Languages

## Quick Facts
- arXiv ID: 2508.05628
- Source URL: https://arxiv.org/abs/2508.05628
- Authors: Mehrdad Zakershahrak; Samira Ghodratnama
- Reference count: 8
- Primary result: Achieves state-of-the-art tokenizer-free language modeling for Persian with 0.159 BPB reduction vs BPE-based GPT-2-fa

## Executive Summary
H-Net++ introduces a tokenizer-free language model for morphologically-rich languages that learns segmentation boundaries jointly with language modeling through hierarchical dynamic chunking. The model uses a three-level BiGRU router with straight-through Gumbel-Softmax sampling to dynamically identify chunk boundaries, a lightweight Transformer mixer to propagate cross-chunk context, and a two-level latent hyper-prior for document-level consistency. On a 1.4B-token Persian corpus, H-Net++ achieves 1.183 BPB, outperforming BPE-based approaches by 12% in compression while demonstrating 73.8% F1 on gold morphological boundaries and 53% improved robustness to ZWNJ corruption.

## Method Summary
H-Net++ is a hierarchical dynamic chunking architecture that processes raw UTF-8 bytes through a three-level BiGRU router using Gumbel-Softmax sampling to learn segmentation boundaries without supervision. A lightweight Transformer mixer (4 heads, 1.9M params) propagates cross-chunk context before a Mixture Density Network decoder predicts next bytes. The model incorporates a two-level latent hyper-prior for document-level style consistency and uses curriculum-based training (256→4096 bytes over 500k steps) with AdamW optimizer. Specialized handling of Persian ZWNJ patterns and morphological alignment loss further enhance performance on the Persian language.

## Key Results
- Achieves 1.183 BPB on Persian corpus, 0.159 BPB reduction vs BPE-based GPT-2-fa (12% better compression)
- 5.4pp gain on ParsGLUE benchmark, state-of-the-art performance
- 53% improved robustness to ZWNJ corruption, 73.8% F1 on gold morphological boundaries
- Learned chunks align with Persian morphology without explicit supervision

## Why This Works (Mechanism)

### Mechanism 1: Differentiable Boundary Sampling
The model learns segmentation boundaries without explicit supervision by treating discrete chunking decisions as stochastic gates optimized via gradient descent. A hierarchical BiGRU processes bytes and outputs boundary probabilities, which are sampled using straight-through Gumbel-Softmax while maintaining differentiability. Core assumption: Boundary decisions can be locally approximated as independent Bernoulli events conditioned on hidden state context.

### Mechanism 2: Cross-Chunk Context Injection
Dynamic chunking creates isolated segments that miss long-range morphological agreements. A single 4-head Transformer mixer allows variable-length chunks to attend to each other before decoding, restoring the ability to model subject-verb distance and other dependencies. Core assumption: One layer of global attention sufficiently hydrates chunks with non-local context without quadratic byte-level attention costs.

### Mechanism 3: Document-Level Style Priors
Persian exhibits document-level consistency in orthography (ZWNJ usage). A two-level latent hyper-prior learned via variational inference captures these statistical regularities, improving local prediction consistency. Core assumption: Morphological habits can be compressed into a low-dimensional Gaussian prior that conditions decoding.

## Foundational Learning

**Concept: Straight-Through Gumbel-Softmax**
- Why needed: Enables the router to make hard "cut/no-cut" decisions while remaining differentiable
- Quick check: If Gumbel temperature is extremely low (τ≈0), does the router receive gradients?

**Concept: Curriculum Learning**
- Why needed: Training on 4096-byte sequences from scratch is unstable; short sequences help router learn basic boundary patterns
- Quick check: In the "Growth" phase, what happens if you sample only 4096-byte sequences immediately?

**Concept: Variational Autoencoders (VAE)**
- Why needed: Understanding the "latent hyper-prior" requires knowing reparameterization trick and KL divergence role
- Quick check: Why must we sample ξ ~ N(μ,σ) rather than using μ during training?

## Architecture Onboarding

**Component map:** Raw UTF-8 bytes → ZWNJ-specialized embedding → 3-level BiGRU router with Gumbel gates → Transformer mixer (4 heads) → Latent hyper-prior → Mixture Density Network decoder

**Critical path:** Router Boundary Prediction. If the router fails to identify valid morpheme boundaries, chunks remain noise and the subsequent mixer receives garbage inputs, causing compression benefit to vanish.

**Design tradeoffs:**
- Flexibility vs. Stability: Dynamic chunking adapts to morphology but requires complex Gumbel annealing vs. fixed chunking's speed but rigidity
- Depth vs. Overhead: Additional mixer layers improve context but increase FLOPs; H-Net++ limits to 1 layer (1.9M params) to maintain tokenizer-free efficiency

**Failure signatures:**
- Runaway Chunk Sizes: Router may collapse to one giant chunk or single bytes if regularization is too weak
- ZWNJ Hallucination: Without ZWNJ embedding pathway, model generates malformed compound words
- Training Divergence: Loss spikes during "Growth" curriculum indicate batch size/learning rate too high for increasing sequence lengths

**First 3 experiments:**
1. Router Ablation: Disable Transformer mixer to verify cross-chunk attention necessity for Persian
2. Temperature Sensitivity: Train with fixed high/low Gumbel temperatures to observe "premature discrete decisions" effect
3. Corruption Robustness: Run ZWNJ corruption benchmark to ensure dynamic boundary adjustment vs. OOD failure

## Open Questions the Paper Calls Out

**Open Question 1:** Can a single hierarchical router learn language-specific segmentation strategies in truly multilingual settings, or are language-adaptive router components necessary? Basis: Poor zero-shot transfer on typologically distant languages suggests monolingual training doesn't generalize. Resolution requires multilingual corpus evaluation spanning diverse morphological systems.

**Open Question 2:** Does hierarchical dynamic chunking transfer to agglutinative languages like Turkish and Finnish where morphological complexity exceeds Persian? Basis: Persian morphology is concatenative with compound-heavy ZWNJ patterns; agglutinative languages involve longer productive suffix chains requiring potentially deeper hierarchies. Resolution requires evaluation on Turkish/Finnish with gold morpheme boundaries.

**Open Question 3:** What theoretical principles determine when and why dynamic chunking outperforms fixed tokenization schemes? Basis: Empirical gains lack formal analysis linking morphological typology, vocabulary explosion severity, and orthographic noise to expected compression improvements. Resolution requires information-theoretic bounds validated across synthetic and real languages.

**Open Question 4:** How should the architecture be modified to handle code-mixed text where multiple scripts interact within the same document? Basis: Current ZWNJ-aware embeddings are Persian-specific; script switching may require explicit script boundary detection or script-conditioned routing. Resolution requires benchmarking on Persian-English code-mixed datasets and comparing against multilingual BPE and character-level baselines.

## Limitations

- Limited generalization beyond Persian due to language-specific features like ZWNJ handling and morphological structure
- Architecture assumes single-layer Transformer mixer is sufficient for cross-chunk context propagation
- Curriculum-based training requires careful hyperparameter tuning for different datasets and languages
- Model's effectiveness depends on quality and representativeness of Persian corpus

## Confidence

**High confidence:** The H-Net++ architecture successfully learns segmentation boundaries without supervision; achieves state-of-the-art results on Persian ParsGLUE; two-level latent hyper-prior provides document-level consistency for ZWNJ handling; curriculum-based training enables stable convergence on long sequences.

**Medium confidence:** 53% improvement in ZWNJ corruption robustness is specific to Persian orthographic conventions; learned chunks align with Persian morphology but alignment quality may vary across different structures; computational efficiency claims assume router doesn't collapse to byte-level chunks.

**Low confidence:** Generalization to other morphologically-rich languages; performance without morphological alignment loss (rule-based analyzer unavailable); long-term stability beyond reported training regime.

## Next Checks

1. **Cross-linguistic validation:** Test H-Net++ on at least two other morphologically-rich languages (e.g., Turkish and Arabic) with different morphological structures to assess generalization capabilities and identify language-specific limitations.

2. **Router ablation study:** Systematically disable components of the hierarchical router (temperature annealing, Gumbel sampling, boundary gates) to quantify their individual contributions and identify failure modes that could inform more robust designs.

3. **Real-world deployment testing:** Evaluate the model's performance on out-of-domain Persian text (medical, legal, technical domains) and measure degradation in BPB and ParsGLUE scores to assess practical utility beyond the training corpus.