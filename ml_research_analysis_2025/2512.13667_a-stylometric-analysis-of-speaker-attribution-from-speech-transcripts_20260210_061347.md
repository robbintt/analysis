---
ver: rpa2
title: A stylometric analysis of speaker attribution from speech transcripts
arxiv_id: '2512.13667'
source_url: https://arxiv.org/abs/2512.13667
tags:
- features
- speaker
- which
- words
- speech
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces StyloSpeaker, a stylometric method for attributing
  speakers from speech transcripts using features from the authorship attribution
  literature. The method uses character, word, token, sentence, and style features
  to assess whether two transcripts were produced by the same speaker.
---

# A stylometric analysis of speaker attribution from speech transcripts

## Quick Facts
- arXiv ID: 2512.13667
- Source URL: https://arxiv.org/abs/2512.13667
- Reference count: 40
- StyloSpeaker achieves AUC up to 0.86 using interpretable stylometric features, outperforming neural models in most topic-control conditions.

## Executive Summary
This paper introduces StyloSpeaker, a stylometric method for attributing speakers from speech transcripts using features from the authorship attribution literature. The method uses character, word, token, sentence, and style features to assess whether two transcripts were produced by the same speaker. Evaluated on two transcription styles and varying topic control conditions, StyloSpeaker generally performs best on normalized transcripts and achieves highest performance under strict topic control. Compared to black-box neural models, the explainable stylometric approach performs better in most conditions, reaching AUC scores up to 0.86, particularly in less topic-controlled settings. Key distinguishing features include speech-related tokens like discourse markers and laughter, with token unigrams being most important.

## Method Summary
StyloSpeaker extracts stylometric features from speech transcripts including character n-grams (3-6), token n-grams (1-3), POS n-grams (1-3), and static features like function words, readability measures, and Yule's I. Features are combined using absolute difference between paired transcripts, then scaled and classified with logistic regression. The method is evaluated on the Fisher English corpus across three topic control levels and two transcription styles, comparing against neural models like SBERT and LUAR.

## Key Results
- StyloSpeaker achieves AUC of 0.861 on LDC normalized transcripts under base difficulty
- Token unigrams are the most important feature category, particularly speech-specific tokens like fillers and discourse markers
- Stylometric models outperform neural models in moderate topic control conditions but neural models excel in stricter topic control settings

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Taking the absolute difference between feature vectors from paired transcripts enables effective speaker discrimination.
- Mechanism: For each trial (pair of transcripts), features are extracted from both sides and combined via absolute difference (|A - B|). A logistic regression classifier then learns whether the magnitude of feature differences indicates same-speaker or different-speaker pairs.
- Core assumption: Speakers exhibit consistent idiolectal patterns (e.g., filler word rates, discourse marker usage) that differ measurably from other speakers, even in conversational speech.
- Evidence anchors:
  - [abstract] "distinguish speakers in transcribed speech...Key distinguishing features include speech-related tokens like fillers and discourse markers"
  - [section 4.1] "Taking the absolute difference of feature vectors (Diff) produces the highest AUC score...0.861 on LDC base setting"
  - [corpus] Weak direct corpus support; neighbor papers focus on attribution generally, not the diff mechanism specifically.
- Break condition: If speakers adapt their speech patterns dramatically across contexts or if transcripts are too short to capture stable patterns, the absolute difference will not reliably discriminate.

### Mechanism 2
- Claim: Token unigrams—particularly speech-specific tokens like fillers, backchannels, and discourse markers—carry more speaker-discriminative information than character n-grams or static stylometric features in this domain.
- Mechanism: The logistic regression coefficients reveal feature importance. Token unigrams (e.g., "um," "like," "yeah," "mhm") receive higher absolute coefficients, especially in less topic-controlled settings, indicating they vary more systematically across speakers than within-speaker across conversations.
- Core assumption: Speech transcripts preserve speaker-idiosyncratic usage patterns of discourse markers and fillers that are less susceptible to topic variation than content words.
- Evidence anchors:
  - [section 4.5] "The most common feature category overall across difficulty levels and transcriptions is token n-grams, and generally token unigrams...filler words (um, er, like), backchannels (mhm, exactly, yeah), discourse markers (you know)"
  - [section 4.5] "features predictive of different speakers had greater weight...whether a speaker uses the filler word um or er or laughs a lot"
  - [corpus] No direct corpus confirmation; neighbor paper "The Impact of Automatic Speech Transcription on Speaker Attribution" may explore related features but content unavailable.
- Break condition: If transcripts are normalized aggressively (removing fillers, backchannels) or generated via ASR that filters disfluencies, token unigram advantage would diminish.

### Mechanism 3
- Claim: Stylometric models outperform neural models when topic overlap between comparison samples is limited or unknown, because neural models implicitly leverage semantic/topic similarity.
- Mechanism: Neural embeddings (SBERT, LUAR) encode semantic content alongside style. When topics differ between same-speaker trials, neural models may incorrectly classify them as different speakers. Stylometric models, using interpretable features and absolute difference, rely more on style markers that persist across topics.
- Core assumption: Neural authorship embeddings conflate style and content representations, making them sensitive to topic manipulation.
- Evidence anchors:
  - [section 4.4] "On the 'base' and 'hard' difficulty levels, StyloSpeaker performs the best...while in the 'harder' setting, the neural model SBERT has an advantage"
  - [section 1] "not controlling for [topic] can make the task too easy and therefore inflate performance scores"
  - [corpus] Neighbor paper "Same author or just same topic?" (Wegmann et al., referenced in paper) supports content-style entanglement in neural models.
- Break condition: If topic information is adversarially controlled (as in 'harder' setting), neural models may excel by exploiting subtle topic-aligned patterns; stylometric models may underperform if content words become discriminative.

## Foundational Learning

- Concept: **Stylometry and idiolect**
  - Why needed here: The entire approach assumes speakers have distinctive, measurable linguistic habits. Without this assumption, attribution from transcripts is impossible.
  - Quick check question: Can you explain why function words (e.g., "the," "and") might be more reliable for authorship than content words?

- Concept: **TF-IDF vectorization and n-grams**
  - Why needed here: The model uses TF-IDF weighted character, token, and POS n-grams as dynamic features. Understanding how rarity weighting affects feature importance is essential.
  - Quick check question: Why would a character 4-gram appearing in only 5% of documents be excluded via `min_df=0.1`?

- Concept: **Topic control in authorship evaluation**
  - Why needed here: Performance varies dramatically (AUC 0.86 → 0.93 → drops for some models) across topic control levels. Evaluating without topic control can produce misleadingly high scores.
  - Quick check question: In a forensic setting where a questioned call discusses a crime, why might comparing to the suspect's work emails be problematic for uncontrolled models?

## Architecture Onboarding

- Component map: Raw transcript -> [Preprocessing: lowercasing, tokenization] -> [Feature Extraction: static features + TF-IDF n-grams] -> [Feature Combination: |A - B| for trial pairs] -> [StandardScaler normalization] -> [Logistic Regression classifier] -> [Same/Different speaker prediction + coefficient interpretation]

- Critical path:
  1. Feature extraction correctness (Stanza tokenizer/POS tagger must handle disfluencies, restarts, non-speech annotations)
  2. Trial pair construction (ensuring no speaker overlap between train/test splits)
  3. Absolute difference computation (not concatenation—this is the key design choice)

- Design tradeoffs:
  - **Diff vs. Concat vs. Diff+Prod**: Diff alone best for AUC (0.861); Diff+Prod better for accuracy/EER at fixed thresholds. Choose based on evaluation metric requirements.
  - **BBN (text-like) vs. LDC (normalized) transcripts**: LDC marginally better in most settings; BBN better in 'harder' topic control (possibly due to capitalization preserving proper nouns).
  - **Number of TF-IDF features (max_features=2000)**: Diminishing returns beyond 2000; higher values increase overfitting risk and computational cost.
  - **min_df=0.1 for n-grams**: Filters rare n-grams to prevent overfitting to idiosyncratic training phrases, but may miss speaker-specific expressions.

- Failure signatures:
  - AUC near 0.5 (chance): Likely topic control too strict for feature set, or train/test speaker overlap causing data leakage
  - Concat outperforming Diff: Suggests model learning document-specific patterns rather than comparative relationships
  - Neural models dramatically outperforming: Check if topic leakage exists in evaluation setup

- First 3 experiments:
  1. Replicate the 'base' LDC Diff result (target: AUC ~0.86) to validate feature extraction pipeline—verify token unigrams like "um," "yeah" appear in top coefficients.
  2. Ablation study: Remove token n-grams and observe performance drop—quantifies contribution of this feature class.
  3. Test on held-out topic: Train on calls about "hobbies," test on calls about "politics"—measures topic generalization, critical for forensic applicability.

## Open Questions the Paper Calls Out

- **Generalization to forensic domains**: The study evaluates speaker attribution using conversational telephone speech transcripts, but forensic applications often involve monologue interviews, interrogation recordings, or non-conversational speech. The model's performance on these domains remains untested, and the importance of discourse markers and backchannels may not translate.

- **ASR error impact**: While the paper notes "transcription style" differences, it does not evaluate the effect of automatic speech recognition (ASR) errors on performance. In real-world applications, ASR outputs may contain recognition errors that could significantly impact feature extraction and attribution accuracy, particularly for rare words or proper nouns.

- **Topic control realism**: The evaluation uses three levels of topic control (base/hard/harder), but even the "harder" setting represents an idealized scenario where topic overlap is minimal. In practical scenarios, speakers may discuss overlapping subjects across different contexts, potentially confounding stylometric features with topical ones.

## Limitations

- **Generalization to forensic domains**: The study evaluates speaker attribution using conversational telephone speech transcripts, but forensic applications often involve monologue interviews, interrogation recordings, or non-conversational speech.
- **ASR error impact**: The study does not evaluate the effect of automatic speech recognition (ASR) errors on performance, which is critical for real-world applications.
- **Topic control realism**: Even the "harder" setting represents an idealized scenario where topic overlap is minimal, while practical scenarios may have more topic overlap.

## Confidence

- **StyloSpeaker outperforms neural models under moderate topic control** (Medium confidence): The advantage is context-dependent and disappears in stricter topic control settings.
- **Token unigrams are most important features** (High confidence): Ablation studies and coefficient analysis clearly demonstrate their consistent importance.
- **Topic control critically affects performance** (High confidence): Dramatic performance differences across topic control levels provide strong evidence.

## Next Checks

1. **Forensic domain validation**: Evaluate StyloSpeaker on forensic-style datasets containing monologue speech (e.g., police interviews, courtroom testimony) to assess real-world applicability beyond conversational telephone speech.

2. **ASR robustness testing**: Implement StyloSpeaker on automatically transcribed speech (with varying WER levels) to quantify performance degradation and identify which feature categories are most vulnerable to transcription errors.

3. **Topic generalization experiment**: Conduct cross-topic evaluation where models are trained on one topic domain (e.g., hobbies) and tested on completely different topics (e.g., politics, finance).