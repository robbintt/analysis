---
ver: rpa2
title: Instance-Dependent Regret Bounds for Learning Two-Player Zero-Sum Games with
  Bandit Feedback
arxiv_id: '2502.17625'
source_url: https://arxiv.org/abs/2502.17625
tags:
- regret
- learning
- have
- bound
- games
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies the problem of learning two-player zero-sum
  normal-form games under bandit feedback, where players only observe their own noisy
  payoffs without knowledge of their opponent's actions or strategies. The authors
  provide the first instance-dependent regret bounds for this setting, extending recent
  best-of-both-worlds techniques from stochastic multi-armed bandits to the game-theoretic
  context.
---

# Instance-Dependent Regret Bounds for Learning Two-Player Zero-Sum Games with Bandit Feedback

## Quick Facts
- arXiv ID: 2502.17625
- Source URL: https://arxiv.org/abs/2502.17625
- Authors: Shinji Ito; Haipeng Luo; Taira Tsuchiya; Yue Wu
- Reference count: 40
- Primary result: First instance-dependent regret bounds for two-player zero-sum games under bandit feedback

## Executive Summary
This paper establishes the first instance-dependent regret bounds for learning two-player zero-sum normal-form games under bandit feedback, where players only observe their own noisy payoffs without knowledge of their opponent's actions or strategies. The authors apply the Tsallis-INF algorithm, a Follow-the-Regularized-Leader approach with Tsallis entropy regularization, to both players simultaneously. This simple algorithm achieves optimal worst-case regret while also adapting to the difficulty of specific game instances through game-dependent constants.

The key insight is that the Tsallis-INF algorithm provides two complementary regret bounds of the form O(c₁ log T + √c₂T), where c₁ and c₂ are constants that depend on the specific game structure. The first bound is particularly effective when games have pure strategy Nash equilibria, achieving O((ω + ω') log T) regret where ω and ω' are inverse gap measures. The second bound applies to general zero-sum games and features smaller √T coefficients when equilibria are close to pure strategies or have small support. The results are validated through numerical experiments demonstrating the predicted regret scaling on carefully constructed game instances.

## Method Summary
The core method applies the Tsallis-INF algorithm to both players in a two-player zero-sum game. Tsallis-INF is a Follow-the-Regularized-Leader algorithm that uses Tsallis entropy as regularization, which provides better properties for game-theoretic settings compared to traditional Shannon entropy. The algorithm computes deterministic updates for each player based on their cumulative payoff observations, without requiring knowledge of the opponent's strategy. The key innovation is showing that this simple uncoupled learning dynamic simultaneously achieves both worst-case and instance-dependent regret bounds, with the instance-dependent constants capturing properties of the specific game being played such as the distance from equilibrium strategies to pure strategies and the size of the game's action spaces.

## Key Results
- First instance-dependent regret bounds for two-player zero-sum games under bandit feedback, achieving O((ω + ω') log T) regret when games have pure strategy Nash equilibria
- Second regret bound O(c₁ log T + √c₂T) for general zero-sum games with smaller √T coefficients when equilibria are close to pure strategies or have small support
- Last-iterate convergence (not just average-iterate) to pure strategy Nash equilibria with rate O(1/√T)
- Algorithm identifies pure strategy Nash equilibria with near-optimal sample complexity, leaving only an O(√max{m,n}) gap
- Instance-dependent lower bounds proving the regret bounds are tight

## Why This Works (Mechanism)
None

## Foundational Learning
- **Tsallis-INF Algorithm**: A Follow-the-Regularized-Leader method using Tsallis entropy regularization instead of Shannon entropy. Why needed: Provides better regret guarantees and convergence properties for game-theoretic settings compared to traditional algorithms. Quick check: Verify the update equations match the standard FTRL form with Tsallis entropy.

- **Zero-Sum Game Structure**: Games where one player's gain equals the other's loss. Why needed: The zero-sum property enables the use of specific regret bounds and convergence guarantees that don't extend to general-sum games. Quick check: Confirm the payoff matrices satisfy A = -B for the two players.

- **Nash Equilibrium Concepts**: Pure strategy Nash equilibria (PSNE) where players use deterministic strategies, and mixed strategy Nash equilibria where players use probability distributions over actions. Why needed: Different equilibrium types lead to different regret bounds and convergence rates. Quick check: Verify that proposed equilibria satisfy the Nash conditions for the given payoff matrices.

- **Bandit Feedback**: Players only observe their own noisy payoffs for played actions without knowing the opponent's actions or strategies. Why needed: This limited information setting is more realistic but requires different algorithms than full-information game theory. Quick check: Ensure the algorithm only uses self-payoff observations in its updates.

- **Bregman Divergence**: A measure of distance between probability distributions used to analyze convergence. Why needed: Critical for proving last-iterate convergence and regret bounds. Quick check: Confirm the divergence calculations match the Tsallis entropy regularization.

## Architecture Onboarding

**Component Map**: Tsallis-INF algorithm for Player 1 -> Payoff observations -> Cumulative payoff tracking -> Strategy update; Tsallis-INF algorithm for Player 2 -> Payoff observations -> Cumulative payoff tracking -> Strategy update; Both players' strategies -> Game outcome -> Payoffs

**Critical Path**: The algorithm's performance depends on the proper implementation of the Tsallis-INF update rule and accurate tracking of cumulative payoffs. The critical path is: action selection → payoff observation → cumulative payoff update → strategy update → next round. Any error in payoff tracking or strategy computation will propagate through subsequent rounds.

**Design Tradeoffs**: The choice of Tsallis entropy versus other regularizers (like Shannon entropy) provides better regret bounds but may lead to more aggressive exploration. The algorithm trades off between exploration (through the entropy term) and exploitation (through the cumulative payoff term). The instance-dependent bounds are tighter but require more complex analysis than worst-case bounds.

**Failure Signatures**: Poor performance may indicate incorrect implementation of the Tsallis-INF update, errors in payoff tracking, or numerical instability in the entropy calculations. If regret doesn't scale as predicted, it could indicate the game has a structure that doesn't match the theoretical assumptions (e.g., non-zero-sum payoffs or incorrect game representation).

**3 First Experiments**:
1. Implement the algorithm on a simple 2x2 zero-sum game with a known pure strategy Nash equilibrium and verify O((ω + ω') log T) regret scaling
2. Test on a game with a mixed strategy Nash equilibrium and measure whether the √T coefficient matches the theoretical prediction
3. Compare last-iterate convergence rate to average-iterate convergence on games with different equilibrium structures

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can instance-dependent regret bounds and the associated uncoupled learning dynamics be generalized to general-sum multiplayer games?
- Basis in paper: The conclusion states that a "natural step forward is generalizing our work to general-sum multiplayer games, which is not yet explored due to the added intricacy from the misaligned incentives of players."
- Why unresolved: The current analysis relies on the zero-sum structure, and the "misaligned incentives" in general-sum games introduce game-theoretic complexities (e.g., multiple equilibrium concepts) that complicate the extension of the proposed Tsallis-INF bounds.
- What evidence would resolve it: A formal analysis proving instance-dependent regret bounds for an uncoupled learning dynamic (like Tsallis-INF) in a general-sum setting.

### Open Question 2
- Question: Can these results be extended to extensive-form games and continuous games?
- Basis in paper: The conclusion lists extending results to "extensive-form games and continuous games" as "equally important," noting that they introduce "extra challenges with their structural complexity."
- Why unresolved: The paper focuses on normal-form games; extensive-form games (game trees) and continuous games (infinite action spaces) possess structural properties not addressed by the current theoretical framework.
- What evidence would resolve it: An algorithm demonstrating instance-dependent regret bounds specifically designed for the information structures of extensive-form or continuous games.

### Open Question 3
- Question: Is the O(√max{m,n}) gap in sample complexity for identifying a pure strategy Nash equilibrium (PSNE) unavoidable in uncoupled learning dynamics?
- Basis in paper: The conclusion notes: "Our algorithm leaves a O(√max{m,n}) gap in sample complexity for pure strategy Nash equilibrium identification, and whether this gap is unavoidable in uncoupled learning dynamics remains unknown."
- Why unresolved: The authors' decentralized algorithm has a higher sample complexity than centralized methods (e.g., Midsearch by Maiti et al. 2024), but it is unknown if this gap is a fundamental limitation of the uncoupled constraint.
- What evidence would resolve it: A lower bound proof for uncoupled dynamics or a new decentralized algorithm that matches the sample complexity of centralized methods.

### Open Question 4
- Question: Does the Tsallis-INF algorithm achieve last-iterate convergence for general zero-sum games with mixed-strategy Nash equilibria under bandit feedback?
- Basis in paper: Section 4.2 proves last-iterate convergence only for games with a unique PSNE. The paper notes the rate is better than generic O(1/T^{1/6}) rates found in prior work for general games, but does not claim last-iterate convergence for the general mixed case analyzed in Section 3.
- Why unresolved: The proof of Proposition 1 relies on the unique PSNE assumption to bound the Bregman divergence; extending this to the boundary cases or interior mixed equilibria described in Theorem 2 requires a different analysis.
- What evidence would resolve it: A proof showing that E[D(x^*, x_t)] decreases for general NE or an analysis of the trajectory behavior when c_2 ≠ 0.

## Limitations
- Results only apply to two-player zero-sum games and do not extend to general-sum games or games with more than two players
- Analysis assumes perfect knowledge of payoff structures within each player's action set, though only noisy bandit feedback is observed
- Convergence guarantees apply to the sequence of play rather than necessarily to actual mixed strategy play, as players implement deterministic Tsallis-INF updates
- Sample complexity for identifying mixed strategy Nash equilibria is not addressed

## Confidence
- **High Confidence**: The worst-case regret bounds and basic Tsallis-INF algorithm guarantees
- **Medium Confidence**: The instance-dependent bounds and last-iterate convergence claims, which rely on more delicate analysis and specific game structure assumptions
- **Medium Confidence**: The lower bound proofs, as these require careful construction of hard game instances

## Next Checks
1. Implement the Tsallis-INF algorithm on synthetic zero-sum games with known PSNE and mixed equilibria to empirically verify the predicted regret scaling O((ω + ω') log T) versus O(√T) regimes
2. Test the algorithm on games where the Nash equilibrium has small but non-zero probability on all actions to validate the √T coefficient bounds when equilibria are nearly mixed
3. Conduct experiments measuring the rate of last-iterate convergence versus average-iterate convergence on games with different equilibrium structures