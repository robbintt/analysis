---
ver: rpa2
title: 'InfoFlow: Reinforcing Search Agent Via Reward Density Optimization'
arxiv_id: '2510.26575'
source_url: https://arxiv.org/abs/2510.26575
tags:
- arxiv
- search
- reasoning
- preprint
- agent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces InfoFlow, a reinforcement learning framework
  designed to address the challenge of low reward density in training LLM agents for
  deep search tasks. By combining sub-goal reward shaping, adaptive off-policy hints,
  and a dual-agent architecture, InfoFlow provides denser process-level supervision
  that enables more efficient and stable learning.
---

# InfoFlow: Reinforcing Search Agent Via Reward Density Optimization

## Quick Facts
- arXiv ID: 2510.26575
- Source URL: https://arxiv.org/abs/2510.26575
- Authors: Kun Luo; Hongjin Qian; Zheng Liu; Ziyi Xia; Shitao Xiao; Siqi Bao; Jun Zhao; Kang Liu
- Reference count: 40
- One-line primary result: InfoFlow framework achieves superior performance on deep search tasks by optimizing reward density through sub-goal shaping and adaptive hints

## Executive Summary
InfoFlow introduces a reinforcement learning framework to address the challenge of low reward density in training LLM agents for deep search tasks. By combining sub-goal reward shaping, adaptive off-policy hints, and a dual-agent architecture, InfoFlow provides denser process-level supervision that enables more efficient and stable learning. The framework employs a researcher agent for reasoning and planning, paired with a refiner agent that synthesizes retrieved evidence into concise summaries, reducing cognitive load and improving performance. InfoFlow is evaluated on multiple agentic search benchmarks, demonstrating superior generalization and outperforming strong baselines. Notably, on the complex BrowseComp-Plus benchmark, a lightweight model achieves performance competitive with much larger proprietary LLMs. These results highlight the effectiveness of reward density optimization in making deep search tasks more tractable for LLM agents.

## Method Summary
InfoFlow employs a two-phase approach to train LLM agents for deep search tasks. First, a reinforcement-from-transitions (RFT) phase jointly fine-tunes a researcher agent (Qwen2.5-3B/7B-Instruct) and a refiner agent (Qwen2.5-7B-Instruct) on verified trajectories from the InfoSeek dataset, enriched with weighted sub-goals and pathfinding hints. The researcher agent is trained for 3 epochs with a learning rate of 1e-5 and context length of 16K, while the refiner agent is trained for 2 epochs with the same learning rate but a context length of 8K. Second, a reinforcement learning phase uses GRPO with a batch size of 256, maximum 10 turns, rollout of 8, temperature of 0.8, and top-5 retrieval. The shaped reward combines final task reward with sub-goal reward (weight 0.3), and hints are injected at turn 5 if no terminal state is reached. E5 embeddings with a Wikipedia corpus are used for retrieval, with BM25 for the BrowseComp-Plus benchmark.

## Key Results
- InfoFlow outperforms strong baselines on multiple agentic search benchmarks, including NQ, TQA, PopQA, HQA, 2Wiki, Musique, Bamboogle, and BrowseComp-Plus
- On the complex BrowseComp-Plus benchmark, a lightweight InfoFlow model achieves performance competitive with much larger proprietary LLMs
- The framework demonstrates superior generalization and effective reward density optimization through its dual-agent architecture and sub-goal shaping mechanisms

## Why This Works (Mechanism)
InfoFlow addresses the fundamental challenge of sparse rewards in deep search tasks by providing denser process-level supervision. The framework introduces three key innovations: sub-goal reward shaping, which breaks down the final task into intermediate objectives with weighted rewards; adaptive off-policy hints, which guide the agent when it gets stuck during reasoning; and a dual-agent architecture that separates the cognitive load of reasoning and planning (researcher) from evidence synthesis (refiner). By summarizing retrieved evidence into concise outputs, the refiner agent reduces the input complexity for the researcher, enabling more efficient learning and better performance on complex search tasks.

## Foundational Learning
- **Deep Search Question Answering (DSQA)**: Multi-step reasoning tasks requiring information synthesis via agentic search. Why needed: Forms the target application domain where sparse rewards are problematic.
- **Reward Density Optimization**: Technique to increase the ratio of total reward to trajectory length. Why needed: Addresses the cold-start problem and improves learning efficiency in sparse-reward environments.
- **Sub-goal Reward Shaping**: Decomposes final task reward into weighted intermediate objectives. Why needed: Provides denser supervision signals during training, enabling more effective credit assignment.
- **GRPO (Generalized Reinforcement Policy Optimization)**: Advanced RL algorithm with temperature scaling and KL regularization. Why needed: Enables stable policy updates in the presence of shaped rewards and hints.
- **Dual-Agent Architecture**: Separates reasoning/planning from evidence synthesis. Why needed: Reduces cognitive load on the main reasoning agent, improving performance on complex tasks.

## Architecture Onboarding

**Component Map**
InfoSeek Dataset -> RFT Phase (Researcher + Refiner Fine-tuning) -> RL Phase (GRPO Training) -> Benchmark Evaluation

**Critical Path**
Data preparation (InfoSeek enrichment) -> RFT fine-tuning -> RL training with reward shaping and hints -> Evaluation on benchmarks

**Design Tradeoffs**
- Larger refiner agent provides better evidence synthesis but increases inference time
- Sub-goal weight (0.3) balances task completion vs. intermediate progress
- Hint injection timing (turn 5) optimizes between early guidance and exploration

**Failure Signatures**
- Cold-start failure (<10% initial accuracy) due to sparse rewards
- Agent getting stuck in unproductive loops despite hint injection
- Sub-goal matching failures leading to incorrect reward shaping

**3 First Experiments**
1. Verify trajectory collection and verification process with Gemini-2.5-Pro
2. Test RFT phase with joint fine-tuning of researcher and refiner agents
3. Evaluate RL phase with GRPO, monitoring reward density and accuracy progression

## Open Questions the Paper Calls Out
- Can the sub-goal scaffolding and pathfinding hint mechanisms be adapted for autonomous online generation, removing the reliance on expensive expert model annotations?
- Does the Reward Density Optimization framework generalize to other agentic domains with sparse rewards, such as code generation or mathematical reasoning?
- Is there a "compute-optimal" scaling ratio between the Researcher and Refiner agents to maximize reward density?

## Limitations
- Missing GRPO hyperparameters (clip epsilon, KL coefficient, training steps) limit faithful reproduction
- Exact researcher agent prompt template and action format not provided
- Sub-goal matching logic during rollouts is unspecified (string match vs. LLM judge)

## Confidence
- **High Confidence**: Overall two-phase training framework and core reward density optimization concept
- **Medium Confidence**: Performance improvements on benchmarks if missing hyperparameters can be inferred
- **Low Confidence**: Exact trajectory collection process and integration of hints during RL

## Next Checks
1. Conduct hyperparameter sensitivity analysis for missing GRPO parameters (clip epsilon, KL coefficient, training steps)
2. Experiment with variations of researcher agent prompt and action format using refiner prompt as reference
3. Implement and test multiple sub-goal matching strategies to determine optimal approach for the framework