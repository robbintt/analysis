---
ver: rpa2
title: 'Measuring What Matters: A Framework for Evaluating Safety Risks in Real-World
  LLM Applications'
arxiv_id: '2507.09820'
source_url: https://arxiv.org/abs/2507.09820
tags:
- safety
- risks
- applications
- risk
- framework
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces a practical framework for evaluating safety
  risks in large language model (LLM) applications at the application level rather
  than just foundation models. The framework consists of two parts: principles for
  developing customized safety risk taxonomies and practices for evaluating safety
  risks in LLM applications.'
---

# Measuring What Matters: A Framework for Evaluating Safety Risks in Real-World LLM Applications

## Quick Facts
- arXiv ID: 2507.09820
- Source URL: https://arxiv.org/abs/2507.09820
- Authors: Jia Yi Goh; Shaun Khoo; Nyx Iskandar; Gabriel Chua; Leanne Tan; Jessica Foo
- Reference count: 21
- One-line primary result: A framework for evaluating LLM application safety risks that increases stakeholder confidence and enables early risk identification

## Executive Summary
This paper introduces a practical framework for evaluating safety risks in large language model (LLM) applications at the application level rather than just foundation models. The framework consists of two parts: principles for developing customized safety risk taxonomies and practices for evaluating safety risks in LLM applications. The authors applied their framework to internal pilot testing of multiple LLM chatbots, curating a two-level internal safety benchmark comprising 1,600 basic prompts and 33,600 intermediate prompts.

The framework enabled early identification of emerging risks, allowing development teams to proactively address issues before deployment. This process increased stakeholder confidence in the applications' safety performance and led to development of organization-wide safety testing (Litmus) and guardrails (Sentinel) platforms. The framework bridges the gap between theoretical AI safety concepts and operational realities of safeguarding LLM applications in practice.

## Method Summary
The framework combines two key elements: (1) a methodology for developing context-specific safety risk taxonomies through cross-functional stakeholder review, and (2) black-box application-level evaluation using LLM-as-a-judge methods for refusal detection. The approach treats the LLM application as a single API endpoint, testing the full system including system prompts, RAG pipelines, and guardrails. Safety is operationalized through a conservative proxy of refusal behavior, with the evaluator validated against human annotations. The framework was applied to create an internal benchmark with 1,600 basic prompts and 33,600 intermediate prompts generated through adversarial templates.

## Key Results
- Framework enabled early identification of emerging risks before deployment
- Increased stakeholder confidence in applications' safety performance
- Led to development of organization-wide safety testing (Litmus) and guardrails (Sentinel) platforms
- LLM-as-a-judge methods proved most effective for capturing nuanced behaviors across different systems

## Why This Works (Mechanism)

### Mechanism 1
- Claim: A context-specific taxonomy makes safety testing more effective for a given organization's real-world deployments.
- Mechanism: By starting from general risk frameworks and adapting them to the organization's use-cases, the taxonomy maps onto the specific harm modes and risk appetite that matter for that context. This produces a shared language and priority order for what to test, and avoids testing irrelevant risks or missing high-stakes ones.
- Core assumption: The relevant risks are enumerable and can be mapped to concrete prompt behaviors; regulators and cross-functional stakeholders will converge on a stable taxonomy.
- Evidence anchors:
  - [abstract]: "principles for developing customized safety risk taxonomies"
  - [section 3.1-3.3]: Contextualization steps and case study taxonomy (Undesirable Content, Specialized Advice, Political Content) with definitions, severity levels, and examples.
  - [corpus]: Neighbors discuss construct validity and brittleness of generic benchmarks, supporting the need for context-specific constructs.
- Break condition: If taxonomy categories are too abstract or too granular to map to prompts, or if cross-functional review cannot agree on definitions, test curation and scoring become inconsistent.

### Mechanism 2
- Claim: Black-box application-level evaluation (system prompts, RAG, guardrails included) surfaces risks that foundation-model-only tests miss.
- Mechanism: The application is treated as a single API; adversarial prompts are sent through the full stack and outputs are scored for refusals/safety. This end-to-end path captures interaction effects that would not appear in foundation model benchmarks.
- Core assumption: An application API can be exposed for testing; the evaluator is validated against human labels for that specific application.
- Evidence anchors:
  - [abstract]: "evaluate safety at the application level, as components such as system prompts, retrieval pipelines, and guardrails introduce additional factors"
  - [section 4.2]: "evaluate the system holistically by treating it as a black box"
  - [section 2]: Prior work showing fine-tuning/RAG/system prompts can degrade safety alignment.
  - [corpus]: "LLM Cyber Evaluations Don't Capture Real-World Risk" supports misalignment between abstract evaluations and real-world risk.
- Break condition: If the evaluator systematically mislabels refusals or unsafe outputs, or if the API does not expose the full system, the safety score is invalid.

### Mechanism 3
- Claim: Using refusals as a proxy for safety, evaluated via LLM-as-a-judge, enables scalable, nuanced detection of application-specific safe/unsafe behavior.
- Mechanism: For many deployments, safe behavior is conservatively defined as refusal to engage with adversarial prompts. LLM-as-a-judge can capture contextual refusal patterns that keyword lists or generic classifiers miss. The judge is evaluated to ensure reliability.
- Core assumption: Refusal is an appropriate conservative proxy for the use-case; the judging LLM has sufficient instruction-following ability and alignment to the defined refusal taxonomy.
- Evidence anchors:
  - [section 4.3]: "Refusals can serve as a practical proxy for safety" and LLM-as-a-judge methods were most effective in their internal pilot.
  - [section 4.5 and Appendix C]: Discussion of refusal detection methods and the need to evaluate the evaluator.
  - [corpus]: Weak direct evidence on refusal proxies in neighbors; the claim is primarily supported by the paper's internal pilot rather than external corpus.
- Break condition: If the application's intended use requires nuanced, non-refusal safe responses, refusal as a proxy will produce false positives; if the judge is not calibrated, it produces systematic misclassification.

## Foundational Learning

- Concept: Black-box vs. white-box evaluation in AI systems.
  - Why needed here: The framework explicitly treats the LLM application as a black box via a single API. Understanding the tradeoffs (no internal access but realistic end-to-end behavior) is necessary to design the test harness correctly.
  - Quick check question: Can you explain why evaluating only the foundation model might miss safety issues introduced by a RAG pipeline or system prompt?

- Concept: Construct validity and operationalization of abstract constructs (e.g., "safety").
  - Why needed here: The paper operationalizes safety via a custom taxonomy and refusal-based proxy. Understanding how to move from abstract risk to measurable prompt-response pairs is essential for curation and evaluation.
  - Quick check question: For a given risk category (e.g., "Specialized Advice - Financial"), can you operationalize it into 3 unambiguous adversarial prompts and a corresponding expected safe behavior?

- Concept: LLM-as-a-judge calibration and evaluation.
  - Why needed here: The framework relies on LLM-as-a-judge for refusal detection. Knowing how to calibrate and validate a judge is critical to avoid systematic bias.
  - Quick check question: If your LLM-as-a-judge over-predicts refusals compared to human labels, what is one concrete step to diagnose and adjust?

## Architecture Onboarding

- Component map: Taxonomy service -> Prompt repository -> Test runner -> Response evaluator -> Reporting/aggregation -> Integration hooks
- Critical path: 1. Cross-functional taxonomy definition → prompt curation → evaluator configuration → evaluator validation (vs human) → batch testing → report generation → stakeholder review.
- Key dependency: Evaluator validation must precede large-scale batch testing; otherwise scores are unreliable.

- Design tradeoffs:
  - Granularity vs. maintainability: Too many subcategories increase taxonomy maintenance and prompt curation burden.
  - Proxy choice: Refusal-based proxy is conservative and easy to evaluate but may over-constrain useful applications.
  - Evaluator complexity: LLM-as-a-judge is flexible but computationally expensive and requires calibration; keyword/classifier is fast but brittle.

- Failure signatures:
  - Evaluator systematically mislabels context-specific refusals.
  - Taxonomy categories do not map cleanly to prompts, causing ambiguous labeling.
  - API not exposing full stack leading to inconsistent test results.
  - High variance in safety scores across runs without system changes.

- First 3 experiments:
  1. Validate refusal evaluator: Create a held-out set of 200-500 responses with human labels; measure agreement for keyword, classifier, and LLM-as-a-judge approaches.
  2. Baseline safety score: Run curated prompt set against the target LLM application; compute safety scores by risk category and complexity level to identify weak spots.
  3. Ablation on system components: Test the same prompt set against (a) foundation model only, (b) model + system prompt, (c) model + RAG, (d) full application with guardrails; compare safety scores to quantify per-component impact.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the framework be adapted to effectively evaluate multi-turn interactions and multilingual inputs?
- Basis in paper: [explicit] The Conclusion explicitly states the authors "plan to extend the framework to cover multi-turn interactions [and] multilingual prompts."
- Why unresolved: The current pilot study and benchmark dataset focused primarily on single-turn chatbot interactions, leaving complex conversation history and language diversity unaddressed.
- What evidence would resolve it: Successful application of the framework to conversation-dependent attacks (e.g., " Crescendo" attacks) and safety evaluations in non-English languages.

### Open Question 2
- Question: What constitutes a standardized "passing" safety score or acceptable risk threshold for LLM applications?
- Basis in paper: [explicit] Section 4.4 states it is "difficult to provide general guidelines for what constitutes 'safe enough'" and relies on internal baselines until clearer industry standards emerge.
- Why unresolved: Risk appetite is subjective and varies by sector, making it difficult to compare safety across different applications objectively.
- What evidence would resolve it: The establishment of industry-wide benchmarks or regulatory guidelines that define safety tiers (e.g., similar to AILuminate grades) for specific application domains.

### Open Question 3
- Question: How can automated red-teaming be integrated into the framework to supplement static benchmarks?
- Basis in paper: [explicit] The Conclusion lists "automated red-teaming" as a planned extension for the framework.
- Why unresolved: The current methodology relies on a "curated" static set of prompts, which may not capture the evolving landscape of adversarial attacks.
- What evidence would resolve it: A demonstration of the framework utilizing automated agents to dynamically generate adversarial prompts that identify novel vulnerabilities missed by the static dataset.

### Open Question 4
- Question: To what extent do LLM-as-a-judge evaluators align with human annotations when detecting nuanced refusals?
- Basis in paper: [inferred] Section 4.3 mandates that organizations "evaluate the evaluator" against human annotations, while Section 4.4 notes that "inherent error rates" prevent formal guarantees.
- Why unresolved: The paper relies heavily on LLM-as-a-judge for scalability but does not provide specific metrics on its alignment with human judgment for this specific refusal detection task.
- What evidence would resolve it: A validation study presenting correlation scores (e.g., Cohen’s Kappa) between the LLM judge and human annotators on the refusal dataset.

## Limitations

- The specific LLM-as-a-judge model and prompt template used for refusal detection remain unspecified, making it difficult to reproduce results or assess evaluator reliability.
- The exact adversarial attack templates that generated 33,600 intermediate prompts from 1,600 basic prompts are not described, preventing accurate recreation of the benchmark.
- The framework's generalizability is limited by the lack of transparency around critical implementation details and the reliance on internal pilot testing rather than external validation.

## Confidence

**High confidence**: The core premise that application-level safety evaluation (including system prompts, RAG, and guardrails) captures risks missed by foundation-model-only testing is well-supported by prior literature on component interactions degrading safety alignment.

**Medium confidence**: The effectiveness of refusal as a conservative safety proxy and LLM-as-a-judge methods for capturing nuanced behaviors is supported primarily by internal pilot testing rather than external validation. The claim that this approach is "most effective" compared to alternatives lacks comparative benchmarks from independent studies.

**Low confidence**: The scalability and maintenance claims regarding taxonomy development and prompt curation lack quantitative evidence. The assertion that cross-functional stakeholders will consistently converge on stable taxonomies may be overly optimistic given the documented challenges in operationalizing abstract safety constructs.

## Next Checks

1. **Evaluator calibration study**: Create a held-out validation set of 200-500 responses with human annotations. Measure inter-annotator agreement and compare against LLM-as-a-judge outputs to quantify reliability and identify systematic biases.

2. **Component ablation analysis**: Test the same prompt set against (a) foundation model only, (b) model + system prompt, (c) model + RAG, (d) full application with guardrails. Measure safety score differences to quantify each component's contribution to safety degradation.

3. **Taxonomy stability assessment**: Conduct a longitudinal study where the taxonomy is applied across multiple LLM application updates and different organizational contexts. Measure consistency in risk categorization and prompt-response mapping over time.