---
ver: rpa2
title: Analysing Personal Attacks in U.S. Presidential Debates
arxiv_id: '2511.11108'
source_url: https://arxiv.org/abs/2511.11108
tags:
- personal
- attacks
- bert
- language
- debates
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study developed a framework to detect personal attacks in
  U.S. presidential debates using fine-tuned transformer models and large language
  models.
---

# Analysing Personal Attacks in U.S. Presidential Debates

## Quick Facts
- arXiv ID: 2511.11108
- Source URL: https://arxiv.org/abs/2511.11108
- Reference count: 0
- Primary result: Domain-specific fine-tuning achieved 88.84% accuracy for detecting personal attacks in presidential debates

## Executive Summary
This study developed a framework to detect personal attacks in U.S. presidential debates using fine-tuned transformer models and large language models. Debate transcripts from 2016, 2020, and 2024 were manually annotated and used to fine-tune BERT models. A BERT model trained on combined debates achieved 88.84% accuracy, while one tested on an unseen 2024 debate reached 86.34% accuracy. Large language models were also evaluated, with DeepSeek-V3 achieving the highest accuracy (92.86%) and recall (91.43%). Fine-tuning Meta-LLaMA-3B-Instruct further improved performance. Results demonstrate that domain-specific fine-tuning significantly enhances detection accuracy and highlights the potential of AI in analyzing political discourse for transparency and accountability.

## Method Summary
The researchers manually annotated 2,239 sentences from six U.S. presidential debates (2016, 2020, 2024) using a 10-criteria rubric to identify personal attacks. They fine-tuned BERT-base-uncased with stratified 80/10/10 splits, inverse-frequency class weighting, and early stopping. The framework also evaluated several large language models (ChatGPT-4o, Claude Sonnet 4, DeepSeek-V3, Gemini 2.5 Pro, Grok 3) using prompt-based binary classification, and fine-tuned Meta-LLaMA-3B-Instruct with LoRA. Performance was measured using accuracy, precision, recall, F1-score, and AUC.

## Key Results
- BERT fine-tuned on debate transcripts achieved 88.84% accuracy overall and 86.34% on unseen 2024 data
- DeepSeek-V3 achieved the highest LLM performance with 92.86% accuracy and 91.43% recall
- Domain-specific fine-tuning dramatically improved results compared to general hate speech models (HateBERT: 21.13% accuracy)
- Personal attacks varied significantly across debates (8.3% to 27.2% of sentences)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Domain-specific fine-tuning dramatically improves detection of nuanced personal attacks in formal political discourse compared to general-purpose hate speech models.
- Mechanism: Pre-trained models encode domain-specific linguistic patterns from their training data. HateBERT, trained on informal Reddit comments, captures explicit abuse patterns but fails on the formal, implied, and context-dependent attacks common in presidential debates. Fine-tuning on debate-specific annotations re-aligns the model's learned representations to the target domain's rhetorical patterns—where attacks often manifest through tone, implication, and speaker role rather than explicit slurs.
- Core assumption: Personal attacks in formal political contexts follow different linguistic patterns than informal online abuse.
- Evidence anchors:
  - [abstract] "Results demonstrate that domain-specific fine-tuning significantly enhances detection accuracy"
  - [Section 4.1] "HateBERT performed poorly, giving an accuracy of 21.13%... These results show that while the HateBERT model captured most of the actual personal attacks (high recall), it also incorrectly classified a large number of non-attacks as attacks (low precision)"
  - [Section 4.2.2] BERT fine-tuned on debates achieved 86.34% accuracy on unseen 2024 data
  - [corpus] Related work on political discourse annotation (BEADS framework, populism classification) suggests domain adaptation is a recognized challenge in political NLP

### Mechanism 2
- Claim: Bidirectional context encoding enables detection of subtle, indirect personal attacks that depend on contextual framing rather than explicit keywords.
- Mechanism: BERT's bidirectional training allows the model to attend to both left and right context simultaneously. Personal attacks in debates often rely on implication—for example, questioning competence through contrast rather than direct accusation. The self-attention mechanism weighs the relevance of each word irrespective of position, capturing relationships between the attack surface (character, intelligence, integrity) and the rhetorical strategy (sarcasm, guilt-by-association, veiled criticism).
- Core assumption: Subtle personal attacks are linguistically structured and can be distinguished from policy critique through contextual patterns.
- Evidence anchors:
  - [abstract] "fine-tuned transformer models... achieved 88.84% accuracy"
  - [Section 2.2] "BERT is able to achieve better understanding of language as compared to other models that process text in a single direction"
  - [Section 4.1] "HateBERT struggled to detect subtle or indirect personal attacks that are common in presidential debates... it was less effective at recognising sarcasm, veiled insults and implied criticism"
  - [corpus] Related paper on fallacy classification in debates confirms LLMs benefit from context and audio metadata for argumentation tasks

### Mechanism 3
- Claim: Large language models with context-aware reasoning achieve higher recall on personal attack detection, but exhibit precision-recall trade-offs based on their reasoning style (keyword-driven vs. context-rich).
- Mechanism: LLMs differ in how they weight contextual cues. DeepSeek-V3 and Grok 3 prioritize high recall (91.43%), flagging implied attacks based on speaker intent and tone. Gemini 2.5 Pro applies stricter criteria, achieving higher precision (72.22%) but lower recall (74.29%). ChatGPT-4o relies on keyword matching ("disgrace," "you're"), missing context-dependent attacks. The reasoning style—whether the model generates justifications based on surface keywords or inferred intent—directly shapes the confusion matrix.
- Core assumption: LLM internal reasoning strategies can be inferred from their justifications and error patterns.
- Evidence anchors:
  - [abstract] "DeepSeek-V3 achieving the highest accuracy (92.86%) and recall (91.43%)"
  - [Section 4.3.6, Table 4] Shows precision-recall trade-offs: DeepSeek (71.11% precision, 91.43% recall) vs. Gemini (72.22% precision, 74.29% recall)
  - [Section 4.3.1] ChatGPT-4o "justifications were often template-based and relied heavily on keywords... instead of considering the full sentence context"
  - [corpus] Related work on LLM political bias (multi-agent debate framework) suggests model-specific reasoning patterns influence political language tasks

## Foundational Learning

- Concept: **Precision-Recall Trade-off in Imbalanced Classification**
  - Why needed here: The dataset has significant class imbalance (350 attacks vs. 1,889 non-attacks). Understanding why models like HateBERT achieve 94.29% recall but only 15.90% precision—and why this matters for downstream use—is critical for interpreting results.
  - Quick check question: If a model flags 90% of actual attacks but also flags 50% of non-attacks as attacks, would journalists using this tool face more false leads or missed attacks?

- Concept: **Transfer Learning and Domain Adaptation**
  - Why needed here: The central finding is that models trained on one domain (Reddit abuse) fail on another (formal debates). Understanding what transfers (language structure) vs. what doesn't (rhetorical norms, attack conventions) explains why fine-tuning works.
  - Quick check question: Why might a model trained on Twitter political arguments perform differently on formal presidential debates despite both being "political"?

- Concept: **Self-Attention and Bidirectional Context**
  - Why needed here: The paper positions BERT's architecture as core to its success. Self-attention allows the model to weigh "character" + "questioning" + "competence" across a sentence, even if these words aren't adjacent.
  - Quick check question: In the sentence "My opponent's record speaks for itself," how would bidirectional attention help detect an implied attack that a unidirectional model might miss?

## Architecture Onboarding

- Component map: Raw transcripts → Parser (speaker/text extraction) → Manual annotation (10-criteria rubric) → Annotated CSV → HateBERT baseline → Fine-tuned BERT → LLM evaluation suite → LoRA fine-tuned LLaMA

- Critical path:
  1. Annotation quality is the bottleneck—the 10-question rubric must be applied consistently
  2. Class imbalance handling (inverse frequency weighting) during BERT training
  3. Leave-one-debate-out evaluation tests real-world generalization before deployment

- Design tradeoffs:
  - **High recall vs. high precision**: DeepSeek/Grok maximize recall (good for investigative journalists who don't want to miss attacks); Gemini maximizes precision (good for automated fact-checking where false positives erode trust)
  - **BERT vs. LLMs**: BERT is faster, cheaper, and more reproducible; LLMs provide justifications that aid interpretability but are more expensive and show higher variance
  - **Dataset size vs. annotation cost**: 2,239 sentences is small; the authors note expanding to more debates, rallies, and congressional records would improve generalization

- Failure signatures:
  - **High false positive rate on policy critique**: Model flags "Your healthcare plan would bankrupt the country" as personal attack (confusing policy criticism with character attack)
  - **Missed sarcasm/irony**: Model fails to flag "He's such a stable genius" because keywords are neutral on surface
  - **Cross-debate drift**: Model trained on 2016-2020 data misclassifies 2024 attacks if rhetorical style shifted (monitored via leave-one-debate-out)

- First 3 experiments:
  1. Replicate HateBERT baseline on the annotated test set to confirm domain mismatch (expected: ~20-25% accuracy, high recall, very low precision)
  2. Fine-tune BERT with stratified 80/10/10 split, monitoring for overfitting via early stopping (patience=2 epochs); target: >85% accuracy
  3. Run leave-one-debate-out with 2024 held out; if accuracy drops >10 points from combined training, investigate what linguistic features shifted (vocabulary, attack rate, or moderation style)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the detection framework maintain high accuracy when applied to diverse political speech contexts outside of presidential debates, such as campaign rallies or congressional hearings?
- Basis in paper: [explicit] The authors state in Section 5 that future work involves "Including data from congressional debates, campaign rallies, and interviews" to provide richer context and improve generalization.
- Why unresolved: The current study only validates the model on formal presidential debate transcripts; it is unknown if the "Attack %" distributions and linguistic features (e.g., 12.7% to 27.2% variance) hold steady in less formal or unmoderated political settings.
- What evidence would resolve it: An evaluation of the fine-tuned BERT or DeepSeek models on a newly annotated dataset of campaign rallies and congressional transcripts, measuring performance drops relative to the debate test sets.

### Open Question 2
- Question: Why did task-specific fine-tuning of Meta-LLaMA-3B-Instruct result in a significant increase in skipped predictions without improving overall accuracy?
- Basis in paper: [inferred] Section 4.4 reports that fine-tuning LLaMA reduced false positives but caused skipped predictions to jump from 45 to 86, leaving accuracy essentially unchanged (~65%).
- Why unresolved: The paper notes the training loss decreased, but does not explain why the LoRA adaptation led to a failure mode where the model avoids making predictions on a substantial portion of the test set.
- What evidence would resolve it: An ablation study analyzing the instruction-tuning response formats during fine-tuning, or a comparison of LoRA hyperparameters to determine if the low-rank adaptation caused the model to become overly conservative in generating binary labels.

### Open Question 3
- Question: How can explainable AI methods be effectively integrated to transparently justify why specific statements are classified as personal attacks?
- Basis in paper: [explicit] Section 5 explicitly identifies the need for "Explainability and Interpretability," noting that future work must explore methods to show "why a statement is classified as a personal attack" to aid journalists.
- Why unresolved: While the paper notes that LLMs like Claude provide "context-aware" justifications, it also highlights that these can be "template-based" or inconsistent; a robust, transparent explanation layer is not yet part of the framework.
- What evidence would resolve it: The development and user-study evaluation of an interface that highlights specific tokens or attention heads (in BERT) or reasoning chains (in LLMs) that trigger the "Attack" classification for a given sentence.

### Open Question 4
- Question: Do definitions and linguistic patterns of "personal attacks" transfer effectively to non-English debates, or do they require culturally specific annotation guidelines?
- Basis in paper: [explicit] Section 5 lists "Multilingual Capabilities" as a key future direction, posing the question of how to extend the framework to "non-English debates" and different "cultural settings."
- Why unresolved: The current annotation guidelines (the 10 questions) are tailored to U.S. political norms and English-language semantics; it is unclear if sarcasm or "guilt by association" translate directly to other political cultures.
- What evidence would resolve it: A cross-lingual study applying the 10-question guideline to translated debate transcripts from different political systems (e.g., EU or Asian parliaments) to measure inter-annotator agreement and model transfer performance.

## Limitations
- Dataset size remains limited (2,239 sentences) with inherent subjectivity in personal attack annotation
- Class imbalance (15.6% attacks) required inverse-frequency weighting that may not fully address bias
- Annotation framework captures relevant dimensions but shows some criterion overlap suggesting potential refinement opportunities

## Confidence
- **High Confidence**: Domain-specific fine-tuning significantly improves performance over general hate speech models (validated by clear performance gaps between HateBERT and BERT)
- **Medium Confidence**: LLM performance rankings are stable, though individual model results show variance (e.g., Claude's precision dropped from 72.22% to 64.44% in testing)
- **Medium Confidence**: The annotation framework captures relevant attack dimensions, though some overlap between criteria suggests potential refinement opportunities

## Next Checks
1. **Cross-domain generalization test**: Apply the fine-tuned BERT model to political transcripts from different sources (e.g., congressional hearings, campaign rallies) to verify domain-specific patterns transfer beyond formal debates
2. **Temporal drift analysis**: Compare model performance on debates from different election cycles to quantify how rhetorical styles evolve and impact detection accuracy
3. **Annotation reliability assessment**: Conduct inter-annotator agreement testing on a subset of 100 sentences to quantify human-level consistency and identify ambiguous cases requiring rubric refinement