---
ver: rpa2
title: Low Rank and Sparse Fourier Structure in Recurrent Networks Trained on Modular
  Addition
arxiv_id: '2503.22059'
source_url: https://arxiv.org/abs/2503.22059
tags:
- fourier
- frequencies
- modular
- addition
- singular
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper analyzes recurrent neural networks (RNNs) trained on
  modular addition tasks and shows they learn a Fourier multiplication algorithm.
  The study finds that the model weights and activations have low rank structure and
  sparse representations in the Fourier domain.
---

# Low Rank and Sparse Fourier Structure in Recurrent Networks Trained on Modular Addition

## Quick Facts
- arXiv ID: 2503.22059
- Source URL: https://arxiv.org/abs/2503.22059
- Authors: Akshay Rangamani
- Reference count: 16
- Key outcome: RNNs trained on modular addition tasks learn a Fourier multiplication algorithm with low-rank structure and sparse Fourier representations

## Executive Summary
This paper analyzes how recurrent neural networks (RNNs) learn to solve modular addition tasks, revealing that they implement a Fourier multiplication algorithm. The study demonstrates that RNNs develop low-rank weight structures and utilize only six dominant Fourier frequencies to achieve perfect accuracy. Through detailed analysis of a 2-layer GRU model trained on modular addition, the research shows that the network's weights and activations have sparse representations in the Fourier domain, requiring only a 12-dimensional subspace for complete task performance. The findings provide new insights into mechanistic interpretability by offering a complete description of how RNNs solve this fundamental arithmetic problem.

## Method Summary
The study trains a 2-layer GRU with 128 hidden dimensions on the modular addition task (addition modulo N) using input pairs (a, b) where a, b ∈ {0, 1, ..., N-1}. The model is trained with cross-entropy loss to predict c = (a + b) mod N. The analysis examines the learned weight matrices through their Fourier representations, calculating singular value decompositions and performing frequency ablation studies. The research compares these results with similar analyses conducted on transformers and MLPs trained on the same task, establishing parallels in the learning mechanisms across different architectures.

## Key Results
- RNNs learn a Fourier multiplication algorithm for modular addition, using only six dominant frequencies
- Weight matrices exhibit low-rank structure, with a 12-dimensional subspace sufficient for 100% accuracy
- Individual frequency ablation shows minimal performance impact, but removing multiple frequencies causes significant accuracy degradation
- Fourier domain representations are sparse, with most energy concentrated in a small number of frequencies

## Why This Works (Mechanism)
The mechanism works because modular addition can be naturally expressed in the Fourier domain through the Chinese Remainder Theorem and roots of unity. When numbers are represented in Fourier space, addition modulo N becomes multiplication of complex exponentials. The RNN learns to transform inputs into this Fourier representation, perform the multiplication operation, and then transform back to obtain the result. The low-rank structure emerges because only a small number of Fourier frequencies are needed to represent the modular addition function accurately. This sparse representation is computationally efficient and aligns with the model's optimization objectives.

## Foundational Learning
1. **Fourier Analysis**: Understanding frequency domain representations and the Fourier transform
   - Why needed: To interpret the model's weight structures and activations in the Fourier domain
   - Quick check: Can you explain how Fourier transforms decompose signals into frequency components?

2. **Modular Arithmetic**: Operations performed within a finite number system
   - Why needed: The task involves addition modulo N, requiring understanding of cyclic number systems
   - Quick check: Can you compute (a + b) mod N for arbitrary values of a, b, and N?

3. **Recurrent Neural Networks**: Sequence processing models with hidden states
   - Why needed: The study focuses on GRUs, a type of RNN architecture
   - Quick check: Can you describe the gating mechanisms in GRUs and how they differ from LSTMs?

4. **Mechanistic Interpretability**: Understanding how neural networks implement specific algorithms
   - Why needed: The paper aims to reverse-engineer the algorithm learned by the RNN
   - Quick check: Can you explain what constitutes a complete mechanistic description of a model's behavior?

## Architecture Onboarding

**Component Map**: Input → Embedding Layer → GRU Layers (2x) -> Output Layer -> Cross-Entropy Loss

**Critical Path**: Input representation → Fourier domain transformation → Multiplication operation → Inverse Fourier transformation → Output prediction

**Design Tradeoffs**: The 2-layer GRU architecture balances model capacity with interpretability, while the 128-dimensional hidden states provide sufficient representational power without excessive complexity

**Failure Signatures**: 
- Loss of low-rank structure in weights indicates breakdown of the Fourier algorithm
- Proliferation of non-zero Fourier frequencies suggests inefficient representation
- Performance degradation correlates with removal of key Fourier frequencies

**3 First Experiments**:
1. Train the same architecture with different random seeds to verify consistency of Fourier structures
2. Apply frequency ablation systematically to identify critical frequencies for performance
3. Compare singular value distributions across different layers to understand information flow

## Open Questions the Paper Calls Out
None

## Limitations
- Analysis is limited to a single task (modular addition) and specific RNN architecture (2-layer GRU)
- Claims about learning a Fourier multiplication algorithm lack formal mathematical proof
- Results based on analysis of only one trained model, limiting statistical robustness
- Interpretation relies heavily on visual inspection and ablation studies rather than formal verification

## Confidence

**High confidence**: The observations of low-rank structure in weights and activations, and the identification of six dominant Fourier frequencies are well-supported by the data and analysis presented.

**Medium confidence**: The claim that RNNs learn a Fourier multiplication algorithm is supported by the evidence but would benefit from additional theoretical or empirical validation.

**Low confidence**: The generalizability of these findings to other RNN architectures, tasks, or larger-scale models remains uncertain and requires further investigation.

## Next Checks
1. Replicate the analysis on multiple models trained with different random seeds to assess the consistency of the observed Fourier structures and low-rank representations.

2. Apply the same analytical techniques to other RNN architectures (e.g., LSTMs, vanilla RNNs) and tasks to evaluate the generalizability of the findings.

3. Conduct a theoretical analysis to formally prove or disprove the claim that RNNs implement a Fourier multiplication algorithm for modular addition tasks.