---
ver: rpa2
title: Contrastive Token-level Explanations for Graph-based Rumour Detection
arxiv_id: '2502.04366'
source_url: https://arxiv.org/abs/2502.04366
tags:
- graph
- detection
- ct-lrp
- rumour
- node
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CT-LRP, a novel framework that addresses
  the challenge of explaining Graph Neural Network (GNN)-based rumor detection models
  by providing token-level explanations. Existing explainability methods often fail
  to capture the dependencies among high-dimensional text embeddings used in GNN models,
  providing only coarse node or edge-level insights.
---

# Contrastive Token-level Explanations for Graph-based Rumour Detection

## Quick Facts
- arXiv ID: 2502.04366
- Source URL: https://arxiv.org/abs/2502.04366
- Reference count: 40
- Primary result: CT-LRP achieves 0.488 fidelity vs 0.081 for BiGCN on Twitter rumor detection

## Executive Summary
This paper introduces CT-LRP, a novel framework that addresses the challenge of explaining Graph Neural Network (GNN)-based rumor detection models by providing token-level explanations. Existing explainability methods often fail to capture the dependencies among high-dimensional text embeddings used in GNN models, providing only coarse node or edge-level insights. CT-LRP extends Layerwise Relevance Propagation (LRP) with an explanation space partitioning strategy, enabling fine-grained token-level attributions that isolate class-specific and task-relevant textual components. Evaluated on three public rumor detection datasets, CT-LRP consistently outperforms existing methods, achieving higher fidelity (e.g., 0.488 vs. 0.081 for BiGCN on Twitter) and superior fidelity-sparsity balance. This framework enhances transparency and trustworthiness in AI systems, offering actionable insights for combating misinformation.

## Method Summary
CT-LRP addresses the limitation of existing GNN explanation methods that only provide coarse node or edge-level insights by introducing a token-level explanation framework. The method extends Layerwise Relevance Propagation (LRP) with an explanation space partitioning strategy that isolates class-specific textual components. By capturing dependencies among high-dimensional text embeddings used in GNN models, CT-LRP generates fine-grained token attributions that highlight the most relevant words for rumor detection decisions. The framework is evaluated across three public rumor detection datasets (Twitter, Weibo, Reddit) and demonstrates consistent improvements over baseline methods in terms of fidelity and the balance between explanation accuracy and sparsity.

## Key Results
- CT-LRP achieves 0.488 fidelity on Twitter dataset versus 0.081 for BiGCN baseline
- Consistent performance improvements across all three datasets (Twitter, Weibo, Reddit)
- Superior fidelity-sparsity balance compared to existing methods like GNNExplainer and GraphLIME
- Token-level explanations provide more granular and actionable insights than node-level alternatives

## Why This Works (Mechanism)
CT-LRP works by addressing the fundamental limitation of existing GNN explanation methods: their inability to properly handle high-dimensional text embeddings and capture token-level dependencies. Traditional methods propagate explanations at the node level, treating entire text sequences as atomic units. CT-LRP's partitioning strategy decomposes these embeddings into individual tokens and assigns relevance scores based on their contribution to the final prediction. This approach preserves the contextual relationships within text while enabling granular attribution, which is critical for understanding how specific words or phrases influence rumor detection decisions.

## Foundational Learning

**Graph Neural Networks (GNNs)** - Neural networks designed to operate on graph-structured data, aggregating information from neighboring nodes. Needed because social media data forms natural graphs of users and posts. Quick check: Verify node feature representations capture both content and structural information.

**Layerwise Relevance Propagation (LRP)** - A method for decomposing model predictions into input feature relevances by propagating activation backwards through layers. Needed to trace how GNN decisions relate to specific input tokens. Quick check: Ensure relevance conservation across propagation layers.

**Explanation Space Partitioning** - Strategy to decompose high-dimensional explanation spaces into interpretable subspaces corresponding to different classes or features. Needed to isolate class-specific token contributions. Quick check: Validate partitions capture distinct semantic patterns.

**Fidelity Metrics** - Quantitative measures evaluating how well explanations align with model behavior (e.g., deletion tests, fidelity scores). Needed to objectively compare explanation quality across methods. Quick check: Test sensitivity to perturbation levels.

**Sparsity-Fidelity Trade-off** - Balance between explanation conciseness and accuracy, where sparser explanations may lose important details. Needed to ensure practical usability of explanations. Quick check: Plot fidelity versus explanation length curves.

## Architecture Onboarding

**Component Map:** Input Text -> Graph Construction -> GNN Encoder -> CT-LRP Explanation Module -> Token Relevances

**Critical Path:** Text preprocessing and graph construction form the foundation, followed by GNN feature extraction, then CT-LRP relevance propagation. The explanation module is the innovation point where token-level attributions are generated.

**Design Tradeoffs:** Token-level vs node-level explanations (granularity vs computational cost), contrastive vs non-contrastive explanations (specificity vs general applicability), and explanation fidelity vs sparsity (accuracy vs interpretability).

**Failure Signatures:** Poor performance on datasets with ambiguous or context-dependent language, inability to handle multi-modal data beyond text, and computational bottlenecks when processing very long sequences or dense graphs.

**First Experiments:** 1) Ablation study removing contrastive partitioning to quantify its contribution, 2) Cross-dataset evaluation on non-social-media graph tasks, 3) Computational complexity analysis comparing token vs node-level explanation generation.

## Open Questions the Paper Calls Out
None

## Limitations
- Framework's generalizability to non-social-media graph classification tasks remains untested
- Reliance on contrastive explanations may not suit nuanced classification scenarios with overlapping classes
- Computational overhead of token-level processing versus node-level explanations not discussed
- Performance improvements may depend on specific implementation details of baseline methods

## Confidence

**High confidence:** The core methodology (CT-LRP extension of LRP with contrastive partitioning) is technically sound and builds on established XAI principles

**Medium confidence:** The reported quantitative improvements, pending clarification on baseline implementations and dataset specifics

**Medium confidence:** The qualitative interpretability claims, as user studies or expert validation are not reported

## Next Checks

1. Conduct ablation studies isolating the impact of the contrastive partitioning strategy versus standard LRP to quantify its specific contribution
2. Test the framework on non-social-media graph classification tasks to evaluate cross-domain applicability
3. Implement a computational complexity analysis comparing token-level versus node-level explanation generation times and memory requirements