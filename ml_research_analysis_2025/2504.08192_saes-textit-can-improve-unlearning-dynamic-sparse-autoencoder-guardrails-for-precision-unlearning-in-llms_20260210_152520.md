---
ver: rpa2
title: 'SAEs $\textit{Can}$ Improve Unlearning: Dynamic Sparse Autoencoder Guardrails
  for Precision Unlearning in LLMs'
arxiv_id: '2504.08192'
source_url: https://arxiv.org/abs/2504.08192
tags:
- unlearning
- feature
- features
- forget
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Dynamic SAE Guardrails (DSG), a novel activation-based
  unlearning method that leverages Sparse Autoencoders (SAEs) with principled feature
  selection and dynamic classification to remove unwanted knowledge from LLMs. Unlike
  gradient-based approaches, DSG identifies forget-relevant features using Fisher
  Information and applies conditional clamping only when input sequences are classified
  as forget-relevant, significantly improving the forget-utility tradeoff.
---

# SAEs $\textit{Can}$ Improve Unlearning: Dynamic Sparse Autoencoder Guardrails for Precision Unlearning in LLMs

## Quick Facts
- arXiv ID: 2504.08192
- Source URL: https://arxiv.org/abs/2504.08192
- Reference count: 40
- Primary result: DSG reduces WMDP-Bio accuracy to 29.64% while maintaining >99% MMLU accuracy and superior MT-Bench scores

## Executive Summary
This paper introduces Dynamic SAE Guardrails (DSG), a novel activation-based unlearning method that leverages Sparse Autoencoders (SAEs) with principled feature selection and dynamic classification to remove unwanted knowledge from LLMs. Unlike gradient-based approaches, DSG identifies forget-relevant features using Fisher Information and applies conditional clamping only when input sequences are classified as forget-relevant, significantly improving the forget-utility tradeoff. Experiments on WMDP-Bio show DSG reduces accuracy to 29.64% (vs. 50.00% for the best baseline) while maintaining >99% MMLU accuracy and superior MT-Bench scores. On MUSE, DSG achieves near-complete knowledge memorization removal (98.5% on NEWS, 94.7% on BOOKS) with minimal utility loss. DSG also demonstrates superior data efficiency, zero-shot performance using interpretable features, and strong resistance to relearning attacks. It offers practical advantages including computational efficiency, hyperparameter stability, and sequential unlearning capability.

## Method Summary
DSG uses Sparse Autoencoders to extract interpretable features from LLM activations, then selects features disproportionately active on "forget" data using Fisher Information. A dynamic classifier computes the density of activated features per sequence and applies targeted clamping only when necessary. The method operates at inference time, avoiding weight modification and providing robustness against relearning. Key components include: (1) Fisher-based feature selection using importance ratios between forget and retain sets, (2) sequence-level dynamic classification based on feature density, and (3) conditional activation clamping to block knowledge pathways. The approach achieves superior trade-offs between unlearning effectiveness and utility preservation compared to both gradient-based and static SAE methods.

## Key Results
- On WMDP-Bio, DSG reduces accuracy to 29.64% versus 50.00% for best baseline while maintaining >99% MMLU accuracy
- On MUSE benchmark, DSG achieves 98.5% knowledge removal on NEWS and 94.7% on BOOKS with minimal utility loss
- DSG demonstrates superior resistance to relearning attacks, with train-time DSG maintaining low accuracy versus gradient-based methods that recover performance
- DSG shows 10× data efficiency improvement over DARE on MUSE with similar hyperparameters

## Why This Works (Mechanism)

### Mechanism 1: Fisher Information as a Proxy for Causal Influence
DSG selects SAE features based on Fisher Information (approximated by expected squared activation) to identify features that causally mediate specific knowledge. The method calculates `imp_ratio(j) = forget_score(j) / retain_score(j)`, relying on Theorem 2 which posits that Fisher Information of decoder weights is proportional to causal influence. High ratios isolate features disproportionately active for the "forget set" compared to the "retain set". This works under assumptions of small reconstruction error and feature independence.

### Mechanism 2: Sequence-Level Dynamic Classification
A sequence-level classifier based on feature density (ρ(x)) optimizes the trade-off between unlearning coverage and utility preservation better than static token-level clamping. DSG computes ρ(x), the fraction of tokens where any forget feature is active, then thresholds this value calibrated on retain data. This acts as a Neyman-Pearson optimal test, maximizing detection of forget-relevant sequences for a fixed false-positive rate.

### Mechanism 3: Inference-Time Activation Guardrails
Clamping activations to a negative constant (-c) at inference time blocks knowledge pathways without modifying weights, providing robustness against relearning attacks. When classified as forget-relevant, selected features are set to -c, suppressing information flow through specific causal pathways. This relies on the Superficial Alignment Hypothesis where activation geometry remains stable during finetuning.

## Foundational Learning

- **Concept: Sparse Autoencoders (SAEs) & Superposition**
  - Why needed: SAEs decompose mixed neural activations into interpretable, monosemantic features, making targeted intervention possible
  - Quick check: Can you explain why a standard Transformer activation might encode multiple concepts at once, and how an SAE resolves this?

- **Concept: Approximate Unlearning vs. Exact Unlearning**
  - Why needed: DSG is an "approximate unlearning" method that doesn't guarantee identical weights to a model retrained from scratch
  - Quick check: Why is "exact unlearning" typically infeasible for LLMs, and what metric does approximate unlearning optimize instead?

- **Concept: Fisher Information**
  - Why needed: The core theoretical justification for DSG's feature selection relies on Fisher Information connecting decoder weight sensitivity to feature importance
  - Quick check: In the context of this paper, what does a high Fisher Information score for a specific SAE feature imply about its relationship to the "forget set"?

## Architecture Onboarding

- **Component map:** Base LLM -> SAE Hook -> Feature Selector -> Dynamic Classifier -> Intervention
- **Critical path:** (1) Offline: Forward pass forget/retain data through SAE to build activation matrices, compute scores, select top features (2) Calibration: Compute ρ(x) for retain sequences to find optimal threshold τ (3) Inference: Compute ρ(x) for new query, apply clamp if ρ(x) > τ
- **Design tradeoffs:** $p_{dyn}$ (Threshold Percentile) higher values favor Utility, lower values favor Forgetting; Clamp Strength (c) high values ensure suppression but risk artifacts; SAE Layer Choice early layers capture specific knowledge but may miss abstract concepts
- **Failure signatures:** Utility Collapse (MMLU drops significantly) suggests p_dyn too low; Insufficient Forgetting (WMDP accuracy > 50%) suggests n_feats too low or poor feature selection; Relearning Susceptibility indicates activation geometry changed significantly
- **First 3 experiments:** (1) Baseline Verification: Replicate WMDP-Bio result comparing DSG vs Farrell et al. (2) Ablation on p_dyn: Sweep threshold percentile to visualize Forget-Utility frontier (3) Relearning Probe: Finetune unlearned model for 5 epochs comparing Test-time vs Train-time DSG vs RMU

## Open Questions the Paper Calls Out

- How does DSG performance scale with different SAE widths, base model sizes, and alternative model architectures?
- Can the gradual protection erosion observed with test-time DSG under relearning attacks be prevented while maintaining utility?
- Does applying DSG across multiple layers simultaneously improve unlearning effectiveness without excessive computational overhead?

## Limitations

- The theoretical foundations rely on strong assumptions about SAE reconstruction error and feature independence
- Performance has only been validated on biosecurity and code security domains, not general knowledge types
- Computational overhead of SAE activation extraction during inference is non-trivial and not fully characterized

## Confidence

**High Confidence:**
- DSG outperforms static SAE clamping methods on WMDP-Bio benchmark
- DSG demonstrates superior resistance to relearning attacks compared to gradient-based methods
- Fisher Information-based feature selection produces interpretable features

**Medium Confidence:**
- DSG achieves claimed forget-utility tradeoff on tested benchmarks
- Dynamic classifier provides optimal classification performance
- DSG's data efficiency claims are robust across tested domains

**Low Confidence:**
- Superficial Alignment Hypothesis sufficiently explains resistance across extended finetuning
- Method generalizes effectively to knowledge types beyond tested domains
- Computational efficiency advantages hold at scale

## Next Checks

**Check 1: Cross-Domain Generalization**
Apply DSG to remove knowledge about specific companies or individuals from a language model, measure both forgetting quality and utility preservation on domain-specific benchmarks, and compare against RMU and DARE baselines using identical hyperparameters.

**Check 2: SAE Layer Sensitivity Analysis**
Apply DSG using SAEs at layers 3, 6, and 9 of Gemma-2-2b, measure WMDP accuracy, MMLU retention, and computational overhead for each layer, and analyze whether deeper layers capture more abstract knowledge but require different hyperparameter settings.

**Check 3: Long-Term Relearning Resistance**
Fine-tune the "unlearned" model for 50 epochs on the forget set (10× longer than current experiments), track WMDP accuracy recovery over time and compare against gradient-based methods, and analyze whether DSG's advantage diminishes or remains stable under prolonged adversarial training.