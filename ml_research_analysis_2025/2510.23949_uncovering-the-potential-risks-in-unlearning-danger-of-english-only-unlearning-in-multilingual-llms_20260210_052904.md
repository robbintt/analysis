---
ver: rpa2
title: 'Uncovering the Potential Risks in Unlearning: Danger of English-only Unlearning
  in Multilingual LLMs'
arxiv_id: '2510.23949'
source_url: https://arxiv.org/abs/2510.23949
tags:
- language
- unlearning
- multilingual
- name
- english
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper identifies a critical flaw in English-only machine\
  \ unlearning for multilingual LLMs: when fine-tuned on parallel multilingual PII\
  \ and then unlearned using only English data, models exhibit severe language confusion\u2014\
  responding in a language different from the query language. This invalidates reference-based\
  \ evaluation metrics like EM and KM, which fail to detect retained knowledge."
---

# Uncovering the Potential Risks in Unlearning: Danger of English-only Unlearning in Multilingual LLMs

## Quick Facts
- arXiv ID: 2510.23949
- Source URL: https://arxiv.org/abs/2510.23949
- Reference count: 40
- Primary result: English-only unlearning causes language confusion in multilingual LLMs, leading to reference metrics failing to detect retained PII knowledge

## Executive Summary
This paper identifies a critical flaw in English-only machine unlearning for multilingual LLMs: when fine-tuned on parallel multilingual PII and then unlearned using only English data, models exhibit severe language confusion—responding in a language different from the query language. This invalidates reference-based evaluation metrics like EM and KM, which fail to detect retained knowledge. To quantify the issue, the authors introduce N-Mix, an n-gram-based metric that measures language confusion severity. They also propose semantic-based evaluation, using LLM-based judges to directly assess content retention across languages. Experiments on Llama 2 and Qwen2 show that English-only unlearning induces high N-Mix scores (up to 100) and causes reference metrics to report false negatives. Incorporating multilingual data during unlearning mitigates confusion, but real-world scenarios often lack such data. The study underscores the need for new evaluation protocols and unlearning strategies for multilingual LLMs.

## Method Summary
The authors conduct experiments on Llama 2 and Qwen2 models fine-tuned on synthetic parallel PII data across multiple languages. They compare English-only unlearning against multilingual unlearning approaches. To measure language confusion, they introduce N-Mix, an n-gram-based metric that quantifies cross-language response patterns. For semantic evaluation, they employ LLM-based judges to assess content retention. The study tests various unlearning scenarios including different proportions of multilingual data and examines the impact on both reference-based metrics (EM, KM) and their proposed metrics.

## Key Results
- English-only unlearning causes severe language confusion, with models responding in languages different from the query language
- Reference-based evaluation metrics (EM, KM) fail to detect retained knowledge when language confusion occurs
- N-Mix scores reach up to 100 in English-only unlearning scenarios, indicating severe cross-language contamination
- Multilingual unlearning mitigates confusion but may not be feasible in real-world scenarios lacking multilingual PII data

## Why This Works (Mechanism)
English-only unlearning fails in multilingual models because it creates a language-specific blind spot. When models are fine-tuned on parallel PII across languages and then unlearned using only English data, the unlearning process cannot effectively address the cross-lingual representations. This causes the model to lose the ability to properly associate languages with their corresponding content, resulting in responses that mix or mismatch languages with the query context.

## Foundational Learning
- **Machine Unlearning**: The process of removing specific knowledge from trained models without full retraining. Needed to understand privacy compliance requirements. Quick check: Verify unlearning modifies weights without catastrophic forgetting.
- **Cross-lingual Representations**: How multilingual models store and associate information across different languages. Critical for understanding language confusion. Quick check: Test if representations for same concept differ across languages.
- **Reference-based Evaluation Metrics**: EM and KM metrics that compare generated responses against reference answers. Important context for understanding why these metrics fail. Quick check: Verify metrics assume same-language responses.
- **N-gram Language Modeling**: Statistical approach to measuring language patterns. Needed to understand N-Mix metric construction. Quick check: Confirm N-Mix captures cross-language contamination.
- **LLM-as-a-Judge**: Using large language models to evaluate other models' outputs. Relevant for semantic evaluation approach. Quick check: Validate judge consistency across multiple evaluations.
- **Parallel PII Data**: Personal information present across multiple languages in aligned datasets. Important for understanding the experimental setup. Quick check: Verify parallel nature of training data.

## Architecture Onboarding

Component Map: Fine-tuned Multilingual Model -> Unlearning Process -> Evaluation Metrics (EM/KM/N-Mix/Semantic Judge)

Critical Path: PII Fine-tuning → Unlearning (English-only vs Multilingual) → Response Generation → Evaluation (Reference Metrics → Failure → N-Mix/Semantic Metrics)

Design Tradeoffs: English-only unlearning is computationally efficient but causes language confusion; multilingual unlearning is effective but resource-intensive and may lack real-world data availability.

Failure Signatures: High N-Mix scores, reference metric false negatives, language mismatch between query and response, semantic content retention despite reference metric passing.

First Experiments:
1. Test N-Mix metric on a controlled multilingual model with known language confusion
2. Compare EM/KM metrics against semantic judge evaluation on language-confused outputs
3. Measure N-Mix scores across different proportions of multilingual data in unlearning

## Open Questions the Paper Calls Out
None identified in the provided content.

## Limitations
- Experiments use synthetic parallel PII data rather than real-world multilingual datasets
- Results may not generalize beyond tested model families (Llama 2, Qwen2)
- Computational overhead and practical feasibility of multilingual unlearning in production environments not addressed

## Confidence
- High confidence in the core observation that English-only unlearning causes language confusion in multilingual models
- Medium confidence in the N-Mix metric's generalizability beyond the tested model families
- Medium confidence in the semantic-based LLM judge evaluation methodology
- Low confidence in the real-world applicability of the findings without data on actual deployment scenarios

## Next Checks
1. Test the proposed evaluation metrics and unlearning strategies across additional multilingual model architectures (e.g., BLOOM, mT5, XLM-R) to verify generalizability
2. Conduct experiments using real-world multilingual PII datasets rather than synthetic parallel data to assess practical relevance
3. Evaluate the performance degradation and computational costs associated with multilingual unlearning compared to English-only approaches in production-scale settings