---
ver: rpa2
title: An Information-Geometric Approach to Artificial Curiosity
arxiv_id: '2504.06355'
source_url: https://arxiv.org/abs/2504.06355
tags:
- information
- rewards
- exploration
- curiosity
- occupancy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a geometric framework for artificial curiosity
  in reinforcement learning, addressing the challenge of sparse rewards by deriving
  intrinsic rewards from information geometry principles. The core method involves
  using concave functions of reciprocal occupancy as intrinsic rewards, uniquely constrained
  by invariance under congruent Markov morphisms and agent-environment interactions.
---

# An Information-Geometric Approach to Artificial Curiosity

## Quick Facts
- arXiv ID: 2504.06355
- Source URL: https://arxiv.org/abs/2504.06355
- Authors: Alexander Nedergaard; Pablo A. Morales
- Reference count: 40
- Primary result: Presents a geometric framework for artificial curiosity in RL that derives intrinsic rewards from information geometry principles

## Executive Summary
This paper establishes a geometric framework for artificial curiosity in reinforcement learning that addresses the challenge of sparse rewards by deriving intrinsic rewards from information geometry principles. The approach uniquely constrains intrinsic rewards to concave functions of reciprocal occupancy, invariant under congruent Markov morphisms and agent-environment interactions. By treating the space of occupancy distributions as a statistical manifold, the framework unifies count-based exploration (α=0) and maximum entropy exploration (α=-1) as special cases of α-information rewards, providing a principled exploration-exploitation trade-off through geodesic interpolation on the occupancy manifold.

## Method Summary
The method computes α-information intrinsic rewards as I_α(s; p_π) = 4/(1-α²)[(1/p_π(s))^((α+1)/2) - 1] for α≠1, with I_{-1}(s;p) = -log(p(s)). The occupancy distribution p_π(s) is estimated using non-parametric density estimation (k-NN or k-means), then integrated with gradient-based RL algorithms like PPO through augmented reward r_total(s) = r(s) + βI_α(s; p_π). The framework provides theoretical guarantees through natural gradients in occupancy space, particularly in flat geometry (α=-1), while the α-parameter uniquely determines the curvature of the occupancy manifold governing exploration strategies.

## Key Results
- Information rewards are uniquely invariant under agent-environment interactions when formulated as concave functions of reciprocal occupancy
- The α-parameter in the Amari-Čencov tensor uniquely determines the curvature of the occupancy manifold, governing exploration strategies
- Optimizers of artificial curiosity with α-information rewards trace (α+2)-geodesics connecting uniform and maximally rewarding occupancies
- Natural gradients in occupancy space provide strong theoretical guarantees, particularly in flat geometry (α=-1)

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** If intrinsic rewards must be invariant to how information is represented (congruent Markov morphisms) and the agent-environment interaction, they are uniquely constrained to strictly concave functions of reciprocal occupancy.
- **Mechanism:** The paper establishes that for an intrinsic reward to depend solely on the agent's information about the environment, it must satisfy the data processing inequality. Mathematically, this forces the reward function to take the form $I_f(s; p_\pi) = f(1/p_\pi(s))$ where $f$ is strictly concave, ensuring rewards increase as occupancy (familiarity) decreases.
- **Core assumption:** The intrinsic value of a state can be defined purely by its statistical occupancy relative to the agent's history.
- **Evidence anchors:** [Abstract] "uniquely constrains intrinsic rewards to concave functions of the reciprocal occupancy." [Section 3.2 / Theorem 3.2] Proves that probability-based intrinsic rewards are invariant under the agent-environment interaction uniquely when $p = p_\pi$.

### Mechanism 2
- **Claim:** The exploration-exploitation trade-off, governed by the intrinsic reward weight $\beta$, proceeds along a specific geometric path (an $(\alpha+2)$-geodesic) on the occupancy manifold.
- **Mechanism:** By tuning $\beta$, the optimal policy moves not arbitrarily, but along a geodesic connecting the "maximally rewarding occupancy" (exploitation) and the "uniform occupancy" (exploration). This ensures a principled interpolation between the two objectives rather than a linear weighting.
- **Core assumption:** The space of occupancy distributions forms a statistical manifold where geometric distances correspond to meaningful differences in policy behavior.
- **Evidence anchors:** [Section 4.1 / Theorem 4.4] "The exploration-exploitation trade-off map $\beta \mapsto p_{\alpha,\beta}$ is an $(\alpha+2)$-geodesic..."

### Mechanism 3
- **Claim:** If the curvature parameter $\alpha$ is set to specific values, the framework collapses to known exploration methods: $\alpha=0$ yields count-based exploration and $\alpha=-1$ yields maximum entropy exploration.
- **Mechanism:** The geometry of the occupancy manifold is determined by $\alpha$. At $\alpha=0$ (Riemannian/spherical geometry), the reward mimics the reciprocal square root of counts. At $\alpha=-1$ (flat geometry), it mimics the logarithm of probabilities (entropy). This unifies disparate methods under a single scalar parameter.
- **Core assumption:** Standard exploration heuristics like count-based bonuses are specific instances of a broader geometric family rather than ad-hoc techniques.
- **Evidence anchors:** [Section 3.3 / Theorem 3.3] "...is equivalent to count-based exploration for $\alpha=0$ and maximum entropy exploration for $\alpha=-1$."

## Foundational Learning

- **Concept: Occupancy ($p_\pi$) vs. Transition Dynamics**
  - **Why needed here:** The paper redefines the return and intrinsic rewards based on *occupancy* (the steady-state probability of visiting a state) rather than immediate transitions. Distinguishing the "state visitation distribution" from the "policy" is critical.
  - **Quick check question:** Can you explain why the intrinsic reward is calculated using $1/p_\pi(s)$ rather than the immediate reward $r(s)$?

- **Concept: f-Divergences and $\alpha$-Diversion**
  - **Why needed here:** The core mathematical tool is the family of divergences (distances) between probability distributions. Understanding that $\alpha$ controls the "shape" of this distance (flat vs. curved) is necessary to interpret the unification of count-based and entropy methods.
  - **Quick check question:** If $\alpha=-1$ implies a "flat" geometry (KL-divergence), what geometric property does $\alpha=0$ introduce?

- **Concept: Markov Kernels on General State Spaces**
  - **Why needed here:** The theory is proven for general measurable spaces (continuous/infinite), not just finite grids. Understanding Markov kernels as operators mapping probability measures is needed to grasp the "congruent Markov morphisms" used to prove invariance.
  - **Quick check question:** Why is formulating the agent-environment interaction as a Markov kernel necessary for proving the invariance of intrinsic rewards?

## Architecture Onboarding

- **Component map:**
  Agent/Policy -> Environment -> Occupancy Estimator -> Intrinsic Calculator -> Optimizer

- **Critical path:**
  The Density Estimator is the bottleneck. The theory relies on accurate estimation of $p_\pi(s)$. In continuous spaces, this requires non-parametric methods (e.g., k-nearest neighbors, kernel density). If the density estimate lags or has high variance, the intrinsic reward signal will be noisy.

- **Design tradeoffs:**
  - **$\alpha$ Selection:** Choosing $\alpha=0$ (Count-based) favors discovering *any* new state equally (good for discrete/visual novelty). Choosing $\alpha=-1$ (Max Entropy) favors uniform coverage (good for control tasks). Intermediate $\alpha$ values blend these behaviors but are less theoretically vetted.
  - **Density Estimation:** k-NN is accurate but computationally expensive ($O(N)$ or $O(\log N)$ with trees) as history grows. The paper suggests compression or state-representation learning to mitigate this.

- **Failure signatures:**
  - **"Density Collapse":** If the policy learns too fast, the density estimator may assign near-zero probability to unseen states, causing intrinsic rewards to explode (divergence).
  - **"Catastrophic Forgetting":** If $p_\pi$ is estimated only on recent data, the agent may repeatedly visit "novel" states that were visited long ago.
  - **Stability:** The term $(1/p)^{(\alpha+1)/2}$ can become numerically unstable for small $p$ if not clipped or regularized.

- **First 3 experiments:**
  1. **Grid World Validation (Discrete):** Implement the method on a small grid with sparse reward. Verify that setting $\alpha=0$ recovers the exact behavior of a count-based bonus agent.
  2. **Continuous Control Density Check:** Implement a k-NN density estimator in a simple continuous environment (e.g., HalfCheetah). Compare the stability of $\alpha=0$ vs. $\alpha=-1$.
  3. **Ablation on $\alpha$:** Sweep $\alpha$ from -1 to 1 in a hard-exploration task (e.g., Super Mario/AntMaze) to observe if the geometric trade-off provides a smooth performance curve or if performance cliffs exist at the theoretical boundaries.

## Open Questions the Paper Calls Out

- **What is the optimal value of the geometry parameter $\alpha$ for empirical reinforcement learning performance?**
  - **Basis in paper:** [Explicit] The authors state that "The form of the concave function $f$ remains unresolved and is primarily an empirical question to be investigated on meaningful reinforcement learning problems."
  - **Why unresolved:** While the theory unifies count-based ($\alpha=0$) and maximum entropy ($\alpha=-1$) exploration, the performance of intermediate geometries (e.g., spherical or hyperbolic) is theoretically sound but empirically untested.
  - **What evidence would resolve it:** Large-scale benchmarks on sparse-reward environments comparing the sample efficiency and convergence of agents using different $\alpha$ values.

- **Can $\alpha$-information rewards effectively model biological curiosity and dopamine neuron firing patterns?**
  - **Basis in paper:** [Explicit] The paper suggests "Experimental investigations are warranted on whether $\alpha$-information rewards explain biological curiosity data better than other theories."
  - **Why unresolved:** The framework mathematically unifies novelty ($\alpha=0$) and surprise ($\alpha=-1$) into a spectrum, but it is unknown if biological systems actually utilize intermediate $\alpha$ values.
  - **What evidence would resolve it:** Analysis of neural data to verify if dopaminergic responses correlate with $\alpha$-information rewards for specific, fitted values of $\alpha$.

- **How do optimization dynamics behave on the curved occupancy manifold during the learning process?**
  - **Basis in paper:** [Inferred] The authors note that "this information-geometric framework fully characterizes the optima of these methods, it does not describe their optimization."
  - **Why unresolved:** The paper characterizes the geometric location of the optimal policy, but standard gradient-based methods approximate the natural gradient, potentially failing to follow the geodesic paths defined by the theory.
  - **What evidence would resolve it:** Theoretical analysis or empirical tracking of learning trajectories to determine if they follow the predicted $(\alpha+2)$-geodesic interpolation.

## Limitations

- Non-parametric density estimation scales poorly with dimensionality, limiting practical application to high-dimensional state spaces
- The theoretical framework assumes perfect density estimation, which is computationally expensive and potentially inaccurate in practice
- Performance may be highly sensitive to α parameter selection, with intermediate values lacking empirical validation

## Confidence

- **High confidence** in core invariance proofs and geometric unification theory due to rigorous mathematical derivation
- **Medium confidence** in practical performance given the gap between theoretical guarantees and implementation challenges
- **Medium confidence** in the claimed smooth trade-off behavior across α values, as this requires empirical validation in complex environments

## Next Checks

1. **Density Estimation Stress Test**: Implement k-NN density estimation in progressively higher-dimensional continuous spaces to measure accuracy degradation and computational cost scaling
2. **α Parameter Sensitivity**: Systematically sweep α from -1 to 1 in hard-exploration tasks to identify performance cliffs and validate the claimed geometric trade-off smoothness
3. **Function Approximation Impact**: Compare theoretical geodesics with actual learning trajectories when using neural network-based occupancy estimators to quantify geometry distortion effects