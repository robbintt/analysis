---
ver: rpa2
title: Large (Vision) Language Models are Unsupervised In-Context Learners
arxiv_id: '2504.02349'
source_url: https://arxiv.org/abs/2504.02349
tags:
- unsupervised
- inference
- fine-tuning
- zero-shot
- joint
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses unsupervised adaptation of foundation models
  to new tasks without labeled data or manual prompt engineering. The core idea is
  joint inference over multiple inputs, optimizing the model's joint predictive probability,
  enabling consistent predictions across examples.
---

# Large (Vision) Language Models are Unsupervised In-Context Learners

## Quick Facts
- arXiv ID: 2504.02349
- Source URL: https://arxiv.org/abs/2504.02349
- Reference count: 40
- One-line primary result: Unsupervised fine-tuning and in-context learning methods achieve up to 39% absolute accuracy gains over zero-shot inference across diverse tasks and models.

## Executive Summary
This paper introduces methods for unsupervised adaptation of foundation models to new tasks without labeled data or manual prompt engineering. The core innovation is joint inference over multiple inputs, optimizing the model's joint predictive probability to enable consistent predictions across examples. Two methods are proposed: unsupervised fine-tuning (which uses LoRA parameter updates via a REINFORCE gradient estimator) and unsupervised in-context learning (which iteratively refines predictions using the model's own outputs as pseudo-labels). Experiments show substantial improvements over zero-shot inference, with performance often matching supervised methods despite no labels.

## Method Summary
The framework optimizes the joint predictive probability across a batch of inputs, shifting from independent predictions to self-consistent outputs. Unsupervised Fine-Tuning updates LoRA parameters to maximize this joint probability using a variance-reduced REINFORCE estimator with entropy regularization. Unsupervised In-Context Learning iteratively refines predictions by constructing contexts from previous outputs, resembling Gibbs sampling. Both methods leverage the model's existing in-context learning capabilities to improve performance without ground truth labels.

## Key Results
- Up to 39% absolute accuracy improvement on GSM8K math reasoning
- Performance often matches supervised fine-tuning despite no labeled data
- Method works across diverse models (Llama-3.1, Qwen2.5-Math, OpenFlamingo, GPT-4o) and tasks (NLP classification, math, VQA)
- Test-time scaling is effective, with performance improving as context length increases

## Why This Works (Mechanism)

### Mechanism 1
The framework treats predictions as interdependent, optimizing joint predictive probability rather than independent outputs. This forces the model to find self-consistent predictions across examples, acting as an unsupervised regularizer. It relies on the model's in-context learning capability to leverage correct pseudo-labels for conditioning.

### Mechanism 2
Unsupervised Fine-Tuning frames adaptation as reinforcement learning, where LoRA parameters are updated to maximize the model's own joint predictive probability. A REINFORCE gradient estimator with variance reduction and entropy regularization optimizes this non-differentiable objective, treating the model's probability assignment as a proxy for correctness.

### Mechanism 3
Unsupervised In-Context Learning approximates joint inference through iterative refinement, similar to Gibbs sampling. It updates context by relabeling examples using predictions from previous turns, allowing the model to refine its outputs based on self-generated pseudo-labels.

## Foundational Learning

- **In-Context Learning (ICL)**: The ability to adapt predictions based on examples in the prompt without weight updates. Critical because the entire framework relies on this capability to leverage context for improvement.
  - *Quick check*: Can you explain the difference between updating a model via gradient descent versus updating its behavior via prompt context?

- **Policy Gradient (REINFORCE)**: An RL technique for optimizing discrete objectives through sampling. Needed because the fine-tuning method optimizes a non-differentiable token generation process.
  - *Quick check*: Why does the "self-improvement" loop require a RL estimator rather than standard supervised cross-entropy loss?

- **Gibbs Sampling**: A statistical technique for sampling from joint distributions by iteratively updating variables conditioned on others. The ICL method mathematically relates to this by updating predictions conditioned on current pseudo-labels.
  - *Quick check*: How does the "multi-turn" refinement in Unsupervised ICL mathematically relate to sampling from a joint distribution?

## Architecture Onboarding

- **Component map**: Foundation Model (Base) -> Task Encoder (LoRA + Base or prompt logic) -> Joint Probability Estimator (batched probability calculator)
- **Critical path**: Data (unlabeled inputs) -> Initialization (zero-shot pseudo-labels) -> Optimization Loop (FT: sample/generate/score/update weights, or ICL: sample/construct context/regenerate/repeat)
- **Design tradeoffs**: FT yields higher performance but requires gradients/weights; ICL is API-compatible but ephemeral and inference-heavy. Larger context improves joint inference but increases quadratic compute. Regularization prevents mode collapse but may slow convergence.
- **Failure signatures**: Mode Collapse (all inputs get same label, fix: increase regularization) or Context Misalignment (visual adapter issues, fix: verify base zero-shot performance first).
- **First 3 experiments**: 1) Run ICL on SST2 for 5 turns and plot accuracy vs. turns. 2) Train FT on classification with Î³=0 and observe mode collapse. 3) Test GSM8K performance while varying N to confirm scaling claim.

## Open Questions the Paper Calls Out

- Can the framework be extended to open-ended generation tasks with infinite output spaces? The current method requires discrete sequences and finite answer sets.
- How can the framework improve models that lack inherent in-context learning capabilities? The method amplifies existing ICL signals but cannot bootstrap performance in models without this capability.
- Can unsupervised ICL close the performance gap with fine-tuning through test-time scaling alone? The paper suggests this might be possible with larger context windows and better models.

## Limitations
- Effectiveness depends critically on the foundation model's in-context learning capability
- Currently limited to close-ended tasks with discrete output spaces
- Performance inherits limitations from underlying vision-language adapters in multimodal applications

## Confidence
- **High Confidence**: Core algorithmic framework and experimental results are mathematically sound and robust across diverse tasks and models
- **Medium Confidence**: Generality claims are supported but not exhaustively tested across all possible model architectures
- **Low Confidence**: Limited analysis of failure cases beyond mode collapse, particularly systematic bias and imbalanced datasets

## Next Checks
1. Systematically test unsupervised ICL on a dataset with confident initial prediction errors to analyze whether iterative refinement amplifies or corrects errors
2. Evaluate Unsupervised Fine-Tuning performance on both very large (>70B parameters) and very small (1-3B parameters) models on the same task to rigorously test scalability claims
3. Adapt the framework to a constrained open-ended task (e.g., themed story completion) to measure if joint inference can steer coherent generation without ground truth labels