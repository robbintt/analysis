---
ver: rpa2
title: 'Diverse, not Short: A Length-Controlled Data Selection Strategy for Improving
  Response Diversity of Language Models'
arxiv_id: '2505.16245'
source_url: https://arxiv.org/abs/2505.16245
tags:
- diversity
- arxiv
- responses
- data
- length
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Researchers found that common diversity metrics and reward models
  bias language models toward shorter outputs, reducing expressiveness. They introduced
  Diverse-NS, a length-controlled data selection strategy that improves response diversity
  while maintaining comparable text length.
---

# Diverse, not Short: A Length-Controlled Data Selection Strategy for Improving Response Diversity of Language Models

## Quick Facts
- arXiv ID: 2505.16245
- Source URL: https://arxiv.org/abs/2505.16245
- Reference count: 22
- Primary result: Length-controlled data selection improves LLM response diversity without shortening outputs

## Executive Summary
This paper addresses a fundamental bias in language model diversity training: standard diversity metrics and reward models favor shorter outputs, reducing expressiveness. The authors introduce Diverse-NS, a length-controlled data selection strategy that improves response diversity while maintaining comparable text length. By generating 3,000 preference pairs through a two-step filtering process that enforces length parity, they demonstrate significant improvements in lexical and semantic diversity across creative writing tasks without sacrificing quality. The method works across different model families and sizes, with the surprising finding that smaller models can effectively teach larger models to generate more diverse outputs.

## Method Summary
The Diverse-NS method uses a two-step sequential prompting approach: first generating natural responses, then rewriting them with anti-repetition constraints and target word counts matching the original. Preference pairs are filtered using four criteria: quality of r2 must exceed the 50th percentile of r1, r2 quality must be better than r1, r2 diversity must exceed r1, and length difference must be ≤5 words. The top 3,000 pairs by entropy gain are selected for Direct Preference Optimization (DPO) training with LoRA adapters. The approach is validated on creative writing tasks using four evaluation datasets (DAT, PGT, AUT, CWT) and demonstrates improved diversity metrics while maintaining comparable output length.

## Key Results
- Diverse-NS significantly increases lexical diversity (entropy) and semantic diversity (DSI) across all four creative tasks
- Quality metrics (ArmoRM scores) show only minor degradation despite substantial diversity improvements
- Smaller models (Olmo-2-7B) can effectively teach larger models (Olmo-2-13B) to generate more diverse outputs
- The proposed Diversity Decile metric provides fairer length-adjusted diversity evaluation compared to standard metrics

## Why This Works (Mechanism)
Standard diversity metrics (entropy, TTR) and reward models inadvertently create length bias by favoring shorter, more distinct responses. The length-controlled filtering ensures that diversity gains come from genuine variation in content rather than response compression. By enforcing length parity during preference pair selection, the model learns to explore the response space more thoroughly without taking shortcuts through brevity.

## Foundational Learning
- **Diversity Metrics Bias**: Why needed - Understanding that standard metrics create length bias; Quick check - Compare entropy scores between long and short responses
- **Length-Controlled Filtering**: Why needed - Ensures diversity comes from content variation not compression; Quick check - Verify |word_count(r1) - word_count(r2)| ≤ 5 constraint
- **Cross-Model Knowledge Transfer**: Why needed - Explains why smaller models can teach larger models about diversity; Quick check - Test transfer with different model pairs
- **Sequential Prompting**: Why needed - Enables controlled generation with anti-repetition; Quick check - Validate anti-repetition constraints in r2 generation
- **Preference Pair Selection**: Why needed - Critical for effective DPO training; Quick check - Ensure quality threshold and diversity ranking are correctly applied

## Architecture Onboarding
**Component Map**: Data Generation -> Filtering Pipeline -> DPO Training -> Evaluation
**Critical Path**: 3-word prompt generation → r1 generation (temp=1.0) → r2 sequential rewriting → 4-rule filtering → entropy-based pair selection → LoRA-DPO training → diversity evaluation
**Design Tradeoffs**: Strict length constraint (≤5 words) vs. flexibility in diversity exploration; Single diversity metric vs. multi-metric optimization; Model size transfer (7B→13B) vs. same-size training
**Failure Signatures**: Length collapse (chosen responses ~50 words shorter); Quality degradation outweighing diversity gains; Inconsistent filtering due to missing ArmoRM scores
**First Experiments**: 1) Generate 100 preference pairs with basic filtering to validate length constraint; 2) Train DPO on 500 pairs and test diversity on DAT; 3) Compare single-metric vs. multi-metric filtering effectiveness

## Open Questions the Paper Calls Out
1. **Multi-round Self-Learning**: How does response diversity evolve across multiple rounds of self-learning iterations? The paper only tested single-round preference tuning, while prior work suggests repeated self-training can lead to model collapse.

2. **Length Drift Mechanism**: Why do models trained with length-matched preference pairs generate longer outputs at test time? The mechanism behind this unexpected length increase is not identified.

3. **Multi-Metric Diversity Optimization**: Can combining multiple diversity metrics (lexical, semantic, syntactic) during data filtering produce better diversity alignment than single-metric approaches? The paper only uses entropy or TTR as the diversity signal.

## Limitations
- Exact seed word list and prompt generation procedure are not fully specified, affecting reproducibility
- Training duration is specified as time range (100-150 min) rather than exact iterations or convergence criteria
- Filtering pipeline relies on ArmoRM scores which may not be universally accessible
- Cross-model transfer effect demonstrated only with one model pair (Olmo-2-7B→Olmo-2-13B)

## Confidence
- **High**: Standard diversity metrics bias models toward shorter outputs; Length-controlled data selection effectively improves diversity while maintaining length parity
- **Medium**: Smaller models can teach larger models about diversity; Diversity Decile metric provides fairer evaluation
- **Low**: Multi-round self-learning effectiveness; Cross-domain generalization beyond creative writing

## Next Checks
1. **Reproduce Olmo-2-7B→Olmo-2-13B transfer effect** using preference pairs from a different 7B model to determine if the effect is model-specific or general

2. **Ablation study on length constraint tolerance** by varying the length constraint threshold from |Δwords| ≤ 2 to |Δwords| ≤ 10 and measuring impact on diversity gains, quality retention, and potential length collapse

3. **Generalization across domains** by applying Diverse-NS to non-creative tasks (technical writing, summarization, dialogue) to test whether length-controlled diversity improvements transfer to domains with different diversity characteristics