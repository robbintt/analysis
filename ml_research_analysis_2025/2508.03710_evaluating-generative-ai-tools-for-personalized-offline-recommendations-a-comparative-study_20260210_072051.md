---
ver: rpa2
title: 'Evaluating Generative AI Tools for Personalized Offline Recommendations: A
  Comparative Study'
arxiv_id: '2508.03710'
source_url: https://arxiv.org/abs/2508.03710
tags:
- user
- recommendations
- will
- software
- generative
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluates the performance and user satisfaction of five
  generative AI tools (Gemini, Phi-4, Mistral, Qwen 2.5, and LLaMA 3.2) in generating
  personalized offline activity recommendations for software developers at risk of
  repetitive strain injury (RSI). Following the Goal/Question/Metric (GQM) paradigm,
  the study uses predefined user profiles and intervention scenarios to generate recommendations.
---

# Evaluating Generative AI Tools for Personalized Offline Recommendations: A Comparative Study

## Quick Facts
- arXiv ID: 2508.03710
- Source URL: https://arxiv.org/abs/2508.03710
- Reference count: 0
- One-line primary result: A comparative study evaluating five LLMs for generating personalized offline activity recommendations to help software developers at risk of RSI

## Executive Summary
This study evaluates the performance and user satisfaction of five generative AI tools (Gemini, Phi-4, Mistral, Qwen 2.5, and LLaMA 3.2) in generating personalized offline activity recommendations for software developers at risk of repetitive strain injury (RSI). Following the Goal/Question/Metric (GQM) paradigm, the study uses predefined user profiles and intervention scenarios to generate recommendations. The evaluation focuses on quantitative metrics (precision, recall, F1-score, and MCC-score) and qualitative aspects (user satisfaction and perceived recommendation relevance). The research involved 80 final-year Computer Science students who evaluated ten daily recommendations (five morning, five afternoon) generated by each LLM. User satisfaction was assessed through facial expression analysis and the System Usability Scale (SUS). Results showed that one LLM achieved significantly higher F1-score and MCC-score compared to others, and users reported higher satisfaction with recommendations from this model, supporting the hypothesis that higher technical performance correlates with greater user satisfaction.

## Method Summary
The study employs a within-subjects design where 80 Computer Science students receive recommendations from all five LLMs. User profiles include demographics, schedules, RSI history, preferences, weather, and time of day. Recommendations are generated using a modular prompt design incorporating six elements (Persona, Task, N-shot, Input, Output, Template). Each participant receives 10 recommendations per LLM (5 morning, 5 afternoon), with order randomized and 15-20 second neutral pauses between blocks. Relevance is classified via binary aggregation of expert (occupational health professional) and user feedback. Performance metrics include Precision, Recall, F1-score, and MCC, while user satisfaction is measured through facial emotion recognition and SUS scores.

## Key Results
- One LLM achieved significantly higher F1-score and MCC-score compared to others
- Users reported higher satisfaction with recommendations from the top-performing LLM
- The study supports the hypothesis that higher technical performance correlates with greater user satisfaction

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Detailed user profiles combined with context-aware prompting improve the relevance and personalization of LLM-generated activity recommendations
- Mechanism: A modular prompt design incorporating six elements (Persona, Task, N-shot, Input, Output, Template) integrates structured user data into a unified prompt. The LLM then generates recommendations constrained by temporal, spatial, and personalization parameters
- Core assumption: LLMs can synthesize multiple contextual variables into coherent, context-appropriate recommendations without explicit fine-tuning for the RSI domain
- Evidence anchors: [abstract] "Participants received ten recommendations... each generated by a different AI tool based on detailed user profiles"; [section IV.E] "The construction of the prompt was guided by a modular methodology... follows the six-module decomposition strategy"

### Mechanism 2
- Claim: Binary classification of recommendation relevance using combined expert and user feedback provides a robust ground truth for performance metrics
- Mechanism: Recommendations are evaluated by both an occupational health professional and the user, averaged into a binary score with Cohen's Kappa ensuring inter-rater consistency
- Core assumption: Expert and user assessments are complementary and can be meaningfully aggregated into a single binary label
- Evidence anchors: [abstract] "Recommendations were assessed by precision, recall, F1-score, and Matthews correlation coefficient"; [section IV.A] "A 'Relevant Recommendation' is classified using: (i) Expert-labeled datasets... (ii) User feedback..."

### Mechanism 3
- Claim: Facial emotion recognition combined with SUS provides a multi-modal, less-biased assessment of user satisfaction than self-report alone
- Mechanism: Facial expressions are recorded and analyzed during recommendation review, combined with post-session SUS scores to triangulate satisfaction
- Core assumption: Facial expression analysis during short recommendation review windows reliably reflects emotional response to content
- Evidence anchors: [abstract] "user satisfaction was measured through facial emotion recognition and the System Usability Scale"; [section IV.A] "Emotional cues will be extracted from participants' facial expressions recorded during interaction..."

## Foundational Learning

- Concept: **Within-subjects experimental design**
  - Why needed here: All 80 participants receive recommendations from all 5 LLMs, reducing between-subject variance but introducing potential order/carryover effects
  - Quick check question: Why does the study include a 15–20 second neutral breathing pause between LLM blocks?

- Concept: **Matthews Correlation Coefficient (MCC)**
  - Why needed here: The study explicitly favors MCC over F1 for binary classification due to MCC's balanced handling of TP, TN, FP, FN
  - Quick check question: What advantage does MCC offer over F1-score in binary classification with potential class imbalance?

- Concept: **GQM (Goal/Question/Metric) paradigm**
  - Why needed here: The entire evaluation framework is structured via GQM, mapping high-level goals to specific research questions and quantifiable metrics
  - Quick check question: How does GQM ensure traceability from research questions to evaluation metrics?

## Architecture Onboarding

- Component map:
  User Profile Builder -> Prompt Generator -> LLM Inference Layer -> Evaluation Layer -> Analysis Layer

- Critical path:
  1. Profile collection -> 2. Scenario/prompt generation -> 3. LLM calls -> 4. Recommendation output -> 5. Expert + user evaluation -> 6. Metric computation -> 7. Statistical comparison

- Design tradeoffs:
  - Rich user profiles vs. prompt length limits (40-word output constraint)
  - Within-subjects design (statistical power) vs. carryover/order effects (mitigated by randomization and breathing pauses)
  - Binary relevance classification vs. potential nuance loss in expert-user averaging

- Failure signatures:
  - Low Cohen's Kappa between expert and user relevance ratings -> ground truth instability
  - High variance in emotion detection during pauses -> emotional carryover not fully reset
  - No significant difference across LLMs on F1/MCC -> either all models perform similarly or evaluation lacks discriminative power

- First 3 experiments:
  1. **Pilot test with 8 users**: Validate prompt design, identify systematic errors, refine constraints
  2. **Full within-subjects data collection**: 80 participants × 10 recommendations each, counterbalanced LLM order
  3. **Statistical comparison**: Normality test -> ANOVA/Friedman on metrics; post-hoc pairwise with correction

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Which generative AI tool delivers the most relevant offline activity recommendations for RSI prevention?
- **Basis in paper:** [explicit] Section IV defines RQ1 to assess performance differences between Gemini, Phi-4, Mistral, Qwen 2.5, and LLaMA 3.2
- **Why unresolved:** The paper describes a proposed experiment; the comparative data collection has not yet been conducted
- **What evidence would resolve it:** Statistical comparison of Precision, Recall, F1-score, and MCC scores derived from expert and user evaluations

### Open Question 2
- **Question:** How does the choice of a GenAI tool influence user satisfaction?
- **Basis in paper:** [explicit] Section IV defines RQ2 to evaluate if tool selection impacts perceived usability and emotional state
- **Why unresolved:** Participant interaction data (facial expressions) and System Usability Scale (SUS) responses have not been collected
- **What evidence would resolve it:** Analysis of emotional responses and SUS scores to identify significant variations across the five tools

### Open Question 3
- **Question:** Can findings from a student population be generalized to professional software developers?
- **Basis in paper:** [inferred] Section VI acknowledges the sample is limited to CS students, potentially restricting generalizability
- **Why unresolved:** The study design relies on a student proxy; it does not verify if professionals respond differently
- **What evidence would resolve it:** A replication of the study with professional software developers to validate the student-based results

## Limitations

- The binary aggregation of expert and user relevance judgments may obscure nuanced differences in recommendation quality
- The 40-word constraint on recommendations may limit the depth and specificity of activity suggestions
- The study's geographical focus (Cuenca, Ecuador) and participant pool (CS students) may limit generalizability

## Confidence

- **High confidence**: The comparative ranking of LLMs on technical metrics (F1-score, MCC) and the correlation between higher technical performance and user satisfaction
- **Medium confidence**: The facial emotion recognition component adds value to user satisfaction measurement, though its reliability during short review windows remains uncertain
- **Low confidence**: The generalization of findings to broader populations and the binary classification scheme's ability to capture nuanced recommendation quality

## Next Checks

1. Conduct inter-rater reliability analysis on expert-user relevance classification to verify the binary aggregation approach is valid
2. Test recommendation quality with extended word limits (e.g., 80 words) to determine if the 40-word constraint artificially limits LLM performance
3. Replicate the study with a more diverse participant pool (different academic disciplines, age ranges, geographical locations) to assess generalizability