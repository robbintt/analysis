---
ver: rpa2
title: 'REVERSUM: A Multi-staged Retrieval-Augmented Generation Method to Enhance
  Wikipedia Tail Biographies through Personal Narratives'
arxiv_id: '2502.12137'
source_url: https://arxiv.org/abs/2502.12137
tags:
- male
- wikipedia
- wiki
- content
- section
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces REVERSUM, a multi-staged retrieval-augmented\
  \ generation method to enhance Wikipedia tail biography articles by leveraging personal\
  \ narratives like autobiographies and biographies. The method employs a four-stage\
  \ pipeline\u2014Relevance Detection, Evidence Collection, Verification, and Summarization\u2014\
  to extract and integrate relevant content from personal narratives into existing\
  \ Wikipedia sections."
---

# REVERSUM: A Multi-staged Retrieval-Augmented Generation Method to Enhance Wikipedia Tail Biographies through Personal Narratives

## Quick Facts
- **arXiv ID:** 2502.12137
- **Source URL:** https://arxiv.org/abs/2502.12137
- **Reference count:** 32
- **Primary result:** REVERSUM achieves 92% integrability and 96% informativeness on B/C category Wikipedia biographies, outperforming baseline RAG significantly.

## Executive Summary
This paper introduces REVERSUM, a multi-staged retrieval-augmented generation method that enhances Wikipedia tail biography articles by leveraging personal narratives such as autobiographies and biographies. The method addresses the problem of Wikipedia biographies lacking depth and unique information, particularly in lower-quality B and C category articles. Through a four-stage pipeline of Relevance Detection, Evidence Collection, Verification, and Summarization, REVERSUM extracts and integrates relevant content from personal narratives into existing Wikipedia sections. Human evaluation shows REVERSUM-generated content significantly outperforms baseline methods in integrability (92% vs 75%) and informativeness (96% vs 67.5%).

## Method Summary
REVERSUM employs a four-stage pipeline to enhance Wikipedia biography articles. First, it uses vector search with sentence-bert embeddings to retrieve relevant chunks from personal narratives stored in ChromaDB. Second, an LLM filters these chunks for relevance, distinguishing between redundant content and new information. Third, another LLM extracts specific evidence phrases from the relevant chunks. Fourth, a separate verification session confirms that the extracted evidence appears in the source text before summarization. The system uses Llama-3-8b-instruct with specified hyperparameters (max_new_tokens=250, temperature=0.7, top_p=0.9) and implements MMR retrieval with k=4 and similarity threshold 0.3. This approach specifically targets B and C category Wikipedia biographies that lack comprehensive information.

## Key Results
- REVERSUM achieves 92% integrability compared to 75% for baseline RAG on human evaluation
- Informativeness scores reach 96% vs 67.5% for baseline methods
- Automatic evaluation shows significant improvements in quality, readability, and understandability metrics
- The system successfully processes 102 personal narratives across B and C category biographies
- Faithfulness score of 0.95 demonstrates strong grounding in source material

## Why This Works (Mechanism)

### Mechanism 1: Redundancy Suppression via Relevance Filtering
Standard RAG tends to summarize existing Wikipedia content rather than adding new information; REVERSUM mitigates this through explicit relevance detection. The pipeline retrieves chunks using vector search, but then employs a dedicated LLM step to classify whether these chunks offer new, relevant information for the specific section, pruning mere duplicates before generation. The LLM can distinguish between "similar" content (redundant) and "relevant" content (complementary) better than pure vector similarity. If the Relevance Detection stage returns "No relevant chunks," the pipeline halts.

### Mechanism 2: Hallucination Containment via Separate Verification
REVERSUM decouples evidence extraction from verification to ground the final summary strictly in source text, reducing fabrication. The system extracts evidence in one session, then opens a separate chat session to verify if the extracted evidence explicitly appears in the source chunks. This "fresh eyes" approach prevents the LLM from hallucinating facts not in the retrieval set. A separate context window without generation pressure is more likely to act as a strict auditor. If verification returns "None," summarization is skipped or handled via negative prompting.

### Mechanism 3: Narrative Position Alignment
Information relevant to specific Wikipedia sections is structurally localized in source narratives. The retrieval process benefits from the chronological nature of biographies; "Early life" sections map to the beginning of the book, while "Political involvement" maps to the end. The source narratives follow a roughly chronological structure, allowing the system to leverage this pattern for more effective retrieval. Non-chronological or thematic biographies may yield lower retrieval relevance.

## Foundational Learning

- **Concept: Maximum Marginal Relevance (MMR)**
  - **Why needed here:** Used in the retrieval phase to diversify fetched chunks, ensuring the LLM doesn't just get highly similar redundant text.
  - **Quick check question:** How does MMR differ from standard top-k cosine similarity search? (Answer: It penalizes redundancy among selected items.)

- **Concept: Hallucination Mitigation (Grounding)**
  - **Why needed here:** The core problem in Wikipedia generation is creating facts. REVERSUM solves this via the Verification stage.
  - **Quick check question:** Why is the Verification stage put in a separate chat session from the Evidence Collection stage?

- **Concept: Calibrated Informativeness**
  - **Why needed here:** Standard metrics don't capture novelty. This custom metric weights new information by a "continuation score."
  - **Quick check question:** What are the two factors multiplied to create the Calibrated Informativeness (CI) score? (Answer: Fraction of newly added words and Continuation Score.)

## Architecture Onboarding

- **Component map:** Wikipedia section -> Retriever (sentence-bert + ChromaDB) -> Relevance Detection (Llama-3) -> Evidence Collection (Llama-3) -> Verification (separate Llama-3 session) -> Summarization (Llama-3)

- **Critical path:** Retrieval -> Relevance Detection -> Verification. If Relevance Detection fails (outputs "No relevant chunks") or Verification fails (outputs "None"), the pipeline safely aborts generation rather than hallucinating.

- **Design tradeoffs:**
  - **Latency vs. Accuracy:** The 4-stage pipeline (multiple LLM calls) is significantly slower than single-shot RAG but reduces the 56% redundancy rate and improves faithfulness.
  - **Recall vs. Precision:** Using a similarity threshold of 0.3 filters out weak matches, prioritizing precision over recall.

- **Failure signatures:**
  - **"Non-expansible" cases:** 16% failure at retrieval (similarity < 0.3) and 19% at verification (insufficient info).
  - **Redundancy loops:** If Relevance Detection is skipped, the system reverts to summarizing existing Wikipedia text.

- **First 3 experiments:**
  1. **Baseline Replication:** Run standard RAG on 10 biographies and manually count how many summaries simply repeat existing Wikipedia text (hypothesis: ~56%).
  2. **Threshold Tuning:** Vary the retrieval similarity threshold (currently 0.3) to observe the trade-off between "Non-expansible" rate and hallucination rate.
  3. **Ablation on Verification:** Remove the Verification stage and measure the drop in faithfulness score (currently 0.95) using the DeepEval tool.

## Open Questions the Paper Calls Out

- **Can REVERSUM be adapted to generate content for lower-quality "Stub" or "Start" class articles that lack well-defined sections?**
  - **Basis in paper:** The authors state they limited the study to B and C classes "as lower-category articles often lack well-defined sections" and aim to "generalize... to a broader range" in future work.
  - **Why unresolved:** The current pipeline relies heavily on existing section headers to guide retrieval and relevance detection stages, which are missing in lower-quality stubs.
  - **What evidence would resolve it:** Successful application on Stub-class articles where the system must generate both section structure and content without pre-existing headers.

- **Can automated verification techniques effectively filter subjective bias from personal narratives to meet Wikipedia's Neutral Point of View (NPOV) policy?**
  - **Basis in paper:** The "Limitations" section highlights that "reliance on personal narratives... may introduce a subjective bias" conflicting with NPOV, explicitly calling for "automated verification techniques" in future research.
  - **Why unresolved:** The current verification stage only confirms if evidence is grounded in source text, not whether it's objective or biased.
  - **What evidence would resolve it:** A study measuring NPOV compliance scores in generated content compared to raw input narratives.

- **How can inter-section alignment strategies be integrated to minimize content redundancy across the entire biography?**
  - **Basis in paper:** The authors acknowledge they "do not explicitly measure inter-section alignment" and suggest future work "explore inter-section alignment strategies to refine this process further."
  - **Why unresolved:** REVERSUM currently processes each section independently, risking duplicate facts across different parts of the same article.
  - **What evidence would resolve it:** Quantitative analysis of cross-section semantic overlap in full generated article compared to section-by-section generation.

## Limitations

- The evaluation relies on a relatively small sample size of 10 biographies per category for human evaluation, with automatic metrics measured across 20 biographies but distribution unspecified.
- The system's performance on "Non-expansible" cases (16% retrieval failure + 19% verification failure) is not fully explored, representing potential limitations for real-world deployment.
- Reliance on Internet Archive as the sole source of personal narratives may introduce copyright and availability constraints limiting scalability.

## Confidence

- **High Confidence:** Redundancy suppression via explicit relevance detection is well-supported by pilot study showing 56% redundancy in standard RAG and controlled evaluation showing improved integrability scores.
- **Medium Confidence:** Effectiveness of Calibrated Informativeness metric and continuation score classifier is supported by reported results but fine-tuning details are somewhat vague.
- **Low Confidence:** Generalizability beyond B and C category biographies to other Wikipedia sections or domains is not demonstrated; claim about chronological narrative structure benefits is based on analysis not experimental validation.

## Next Checks

1. **Ablation Study on Verification Stage:** Remove the separate verification stage and measure the drop in faithfulness score using the DeepEval tool mentioned in Appendix H.1 to quantify the contribution of this expensive stage to reducing hallucination.

2. **Retrieval Threshold Sensitivity Analysis:** Systematically vary the retrieval similarity threshold (currently 0.3) across a range of values and measure the trade-off between "Non-expansible" rate and hallucination rate to optimize the balance between recall and precision.

3. **Annotator Agreement and Bias Analysis:** Calculate inter-annotator agreement scores for human evaluation metrics (integrability, informativeness) and assess whether annotators' backgrounds influence results to validate the robustness of human evaluation component.