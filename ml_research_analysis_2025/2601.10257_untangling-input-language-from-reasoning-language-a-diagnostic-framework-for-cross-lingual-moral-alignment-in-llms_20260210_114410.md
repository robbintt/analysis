---
ver: rpa2
title: 'Untangling Input Language from Reasoning Language: A Diagnostic Framework
  for Cross-Lingual Moral Alignment in LLMs'
arxiv_id: '2601.10257'
source_url: https://arxiv.org/abs/2601.10257
tags:
- language
- moral
- reasoning
- flip
- story
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a diagnostic framework to study how language
  affects moral judgment in large language models. The key innovation is a factorial
  design that independently manipulates input language and reasoning language, enabling
  decomposition of their distinct contributions.
---

# Untangling Input Language from Reasoning Language: A Diagnostic Framework for Cross-Lingual Moral Alignment in LLMs

## Quick Facts
- arXiv ID: 2601.10257
- Source URL: https://arxiv.org/abs/2601.10257
- Reference count: 40
- This paper introduces a diagnostic framework to study how language affects moral judgment in large language models

## Executive Summary
This paper presents a diagnostic framework to disentangle the effects of input language and reasoning language on cross-lingual moral judgment in large language models (LLMs). The framework employs a factorial design that independently manipulates these two linguistic factors, enabling researchers to isolate their distinct contributions to moral decision-making. By applying this methodology across 13 LLMs for English-Chinese moral judgment tasks, the study reveals that reasoning-language effects account for twice the variance of input-language effects (7.2 vs 3.5 percentage points) in moral judgment outcomes.

The framework also introduces interpretive tools using Moral Foundations Theory to understand shifts in moral priorities, along with a Split-Authority refinement to control for cultural confounds between languages. The research identifies hidden context-dependency in 44% of models that remains invisible to standard evaluation methods. The paper provides comprehensive metrics and a stability taxonomy for deployment guidance, along with new cross-lingual moral judgment datasets and code, establishing a foundation for more nuanced evaluation of LLMs' cross-lingual moral reasoning capabilities.

## Method Summary
The methodology employs a factorial design that independently manipulates input language (the language of moral dilemmas) and reasoning language (the language used for moral deliberation) to isolate their distinct contributions to moral judgment. This approach enables decomposition of variance between these two linguistic factors, revealing their relative impact on moral decision outcomes. The framework incorporates Moral Foundations Theory as an interpretive lens to understand shifts in moral priorities across languages, and includes a Split-Authority refinement to control for cultural confounds between English and Chinese prompts. The study applies this framework across 13 different LLMs, generating new cross-lingual moral judgment datasets and providing metrics for assessing model stability in deployment scenarios.

## Key Results
- Reasoning-language effects contribute twice the variance of input-language effects (7.2 vs 3.5 percentage points) in moral judgment
- 44% of models exhibit hidden context-dependency invisible to standard evaluation methods
- The framework provides metrics and stability taxonomy for deployment guidance of LLMs in cross-lingual contexts

## Why This Works (Mechanism)
The factorial design approach works by systematically varying input language and reasoning language independently, allowing researchers to isolate and measure their separate contributions to moral judgment outcomes. This methodological innovation addresses the confounding problem where traditional evaluations cannot distinguish whether observed differences stem from the language of the prompt or the language used for reasoning. By controlling for these factors separately, the framework reveals hidden dependencies and cultural influences that would otherwise remain obscured in standard evaluation approaches.

## Foundational Learning
- **Factorial experimental design**: A research methodology that systematically varies multiple factors independently to isolate their individual effects. Needed to disentangle the confounded effects of input and reasoning language; quick check: verify that all factor combinations are tested.
- **Moral Foundations Theory**: A social psychological framework identifying five universal moral foundations (care, fairness, loyalty, authority, purity). Needed to interpret shifts in moral priorities across languages; quick check: confirm that moral judgments align with expected foundation patterns.
- **Cross-lingual evaluation methodology**: Techniques for assessing model performance across different language pairs. Needed to ensure valid comparisons between English and Chinese moral judgments; quick check: verify translation quality and cultural equivalence of dilemmas.
- **Context-dependency detection**: Methods for identifying when model outputs depend on hidden contextual factors. Needed to reveal 44% of models with invisible dependencies; quick check: test model consistency across varied contexts.
- **Stability taxonomy**: Classification systems for categorizing model behavior reliability. Needed to provide deployment guidance; quick check: validate taxonomy against real-world performance variations.
- **Cultural confound control**: Statistical techniques for separating language effects from cultural differences. Needed through Split-Authority refinement; quick check: confirm that cultural variables are properly isolated.

## Architecture Onboarding

Component Map: Moral Dilemmas -> Input Language Manipulation -> Reasoning Language Manipulation -> LLM Processing -> Moral Judgment Output -> Variance Decomposition Analysis -> Stability Assessment

Critical Path: The framework's critical path involves generating moral dilemmas, systematically manipulating input and reasoning languages in factorial combinations, processing through LLMs, collecting moral judgment outputs, and performing variance decomposition analysis to isolate language effects. The Split-Authority refinement serves as a quality control checkpoint before final stability assessment.

Design Tradeoffs: The framework prioritizes methodological rigor and interpretability over computational efficiency, requiring extensive testing across all factorial combinations. The choice of Moral Foundations Theory provides theoretical grounding but may not capture all cultural variations in moral reasoning. The focus on English-Chinese pairs enables deep analysis but limits immediate generalizability to other language pairs.

Failure Signatures: Models failing to show consistent moral judgments across factorial combinations indicate hidden context-dependency. Discrepancies between expected and observed moral foundation patterns suggest cultural confounds not adequately controlled. High variance unexplained by input or reasoning language manipulations points to additional confounding factors.

First Experiments:
1. Replicate the factorial design with Spanish-German moral dilemmas to test generalizability beyond English-Chinese pairs
2. Apply the framework to domain-specific LLMs (legal, medical) to assess cross-lingual moral reasoning in specialized contexts
3. Conduct ablation studies removing Split-Authority refinement to quantify its contribution to controlling cultural confounds

## Open Questions the Paper Calls Out
None specified in the provided information.

## Limitations
- Focus on English-Chinese pairs limits generalizability to other language combinations and cultural contexts
- Moral Foundations Theory may not capture all dimensions of moral reasoning across diverse cultures
- Potential biases from specific moral dilemmas chosen and cultural assumptions embedded in the scenarios

## Confidence
- High confidence in variance decomposition methodology and factorial design approach
- Medium confidence in Moral Foundations Theory interpretation due to potential cultural influences
- Low confidence in generalizability to language pairs beyond English-Chinese

## Next Checks
1. Replicate the analysis across additional language pairs (e.g., Spanish-German, Arabic-French) to assess generalizability and identify language-specific patterns
2. Expand the moral dilemma set to include culturally neutral scenarios and test framework robustness against cultural bias
3. Conduct meta-analysis of moral reasoning across cultures using the framework to refine Moral Foundations Theory interpretation