---
ver: rpa2
title: Protecting the Neural Networks against FGSM Attack Using Machine Unlearning
arxiv_id: '2511.01377'
source_url: https://arxiv.org/abs/2511.01377
tags:
- adversarial
- examples
- neural
- test
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the vulnerability of neural networks, specifically
  the LeNet architecture, to Fast Gradient Sign Method (FGSM) adversarial attacks.
  The proposed solution employs machine unlearning, a technique that removes adversarial
  examples from the training dataset to improve the model's robustness.
---

# Protecting the Neural Networks against FGSM Attack Using Machine Unlearning

## Quick Facts
- arXiv ID: 2511.01377
- Source URL: https://arxiv.org/abs/2511.01377
- Reference count: 34
- Primary result: LeNet accuracy on adversarial test data improved from 7.96% to 96.69% using iterative machine unlearning

## Executive Summary
This paper proposes a machine unlearning approach to defend LeNet neural networks against Fast Gradient Sign Method (FGSM) adversarial attacks. The method identifies and removes adversarial examples from the training dataset based on their loss values, then retrains the model iteratively until achieving desired robustness. Experimental results demonstrate significant improvement in adversarial robustness while maintaining high accuracy on clean test data.

## Method Summary
The approach employs machine unlearning to remove adversarial examples from the training dataset, improving the model's robustness to FGSM attacks. The method iteratively identifies high-loss adversarial examples using categorical cross-entropy, removes their corresponding base samples from training data, and retrains the LeNet model. This process continues until the model achieves a target level of robustness (90%+ accuracy on adversarial test data).

## Key Results
- Adversarial accuracy improved from 7.96% to 96.69% on LeNet model
- Clean accuracy maintained at 99.32% after unlearning process
- Iterative unlearning successfully reduced model vulnerability to FGSM attacks

## Why This Works (Mechanism)

### Mechanism 1: Gradient-Based Vulnerability Exploitation
The Fast Gradient Sign Method (FGSM) exploits the model's gradient direction to generate maximal loss with minimal perturbation. FGSM calculates the gradient of the loss function with respect to the input image and adds a small perturbation in the sign of this gradient to shift the input data across the decision boundary.

### Mechanism 2: Loss-Driven Identification of Vulnerable Manifolds
High loss values during adversarial evaluation serve as a proxy for identifying the most damaging examples that define the model's vulnerability. The authors use categorical cross-entropy to evaluate generated adversarial examples, flagging those resulting in the highest loss as primary vectors of attack.

### Mechanism 3: Iterative Manifold Removal via "Unlearning"
Retraining the model on a dataset stripped of high-vulnerability examples forces the network to learn a decision boundary robust to those specific perturbations. This unlearning process iteratively filters high-loss adversarial samples and removes corresponding base samples from training data.

## Foundational Learning

- **Concept: Gradient Computation & Backpropagation**
  - Why needed here: Understanding how FGSM works requires knowing that gradients point "uphill" regarding the loss function
  - Quick check question: Does modifying the *input* ($x$) based on the gradient differ from modifying the *weights* ($w$) during standard training?

- **Concept: Loss Functions (Categorical Cross-Entropy)**
  - Why needed here: The paper's defense relies on ranking examples by "loss"
  - Quick check question: If an adversarial example has a loss of 2.3045, is the model confident or confused about its prediction?

- **Concept: The Manifold Hypothesis**
  - Why needed here: The defense implies that adversarial examples lie off the "natural" data manifold
  - Quick check question: Why would removing a sample from the training set help the model classify a perturbed version of that sample correctly later?

## Architecture Onboarding

- **Component map:** LeNet Backbone -> FGSM Attacker -> Unlearning Filter -> Retraining Loop
- **Critical path:** The Unlearning Loop is the critical path, implementing the while accuracy_adv < 0.9 cycle of Attack → Evaluate → Filter → Retrain
- **Design tradeoffs:**
  - Robustness vs. Retention: Aggressive unlearning risks removing too much training data
  - Epsilon Sensitivity: Defense is tuned for ε=0.1 and may not transfer to other perturbation magnitudes
- **Failure signatures:**
  - Catastrophic Forgetting: Removing too many samples causes the model to forget required features
  - Overfitting to FGSM: Model becomes robust to FGSM but remains vulnerable to other attacks
- **First 3 experiments:**
  1. Baseline Vulnerability Check: Train LeNet on MNIST, apply FGSM, plot accuracy drop vs. ε
  2. Unlearning Validation: Implement loss-based filter and visualize high-loss samples
  3. Robustness Curve: Run iterative loop and plot "Adversarial Accuracy vs. Unlearning Iterations"

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions in the provided content.

## Limitations
- Limited attack scope: Only validated against FGSM with ε=0.1, effectiveness against stronger attacks remains untested
- Potential for data loss: Iterative unlearning could progressively degrade dataset's representational power
- Lack of theoretical grounding: Assumes removing high-loss samples leads to robustness without formal justification

## Confidence
- High confidence: FGSM attack mechanism and basic formulation are well-established
- Medium confidence: Unlearning methodology is plausible but lacks rigorous theoretical justification
- Low confidence: Claims about maintaining clean accuracy while improving adversarial robustness need broader testing

## Next Checks
1. Cross-attack validation: Test unlearned model against PGD and C&W attacks to verify generalization beyond FGSM
2. Dataset diversity assessment: Analyze diversity of training set after multiple unlearning iterations
3. Ablation study: Compare unlearning against standard adversarial training and random sample removal