---
ver: rpa2
title: Are We on the Right Way for Assessing Document Retrieval-Augmented Generation?
arxiv_id: '2508.03644'
source_url: https://arxiv.org/abs/2508.03644
tags:
- document
- question
- arxiv
- answer
- page
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the critical need for better evaluation methods
  for Retrieval-Augmented Generation (RAG) systems in document understanding, where
  existing benchmarks are fragmented, unrealistic, and fail to assess real-world performance.
  To solve this, the authors introduce DOUBLE-BENCH, a comprehensive, large-scale,
  multilingual, and multimodal evaluation system that provides fine-grained assessment
  of each component within document RAG systems.
---

# Are We on the Right Way for Assessing Document Retrieval-Augmented Generation?

## Quick Facts
- **arXiv ID**: 2508.03644
- **Source URL**: https://arxiv.org/abs/2508.03644
- **Reference count**: 40
- **Primary result**: DOUBLE-BENCH is a comprehensive, large-scale, multilingual, and multimodal evaluation system that provides fine-grained assessment of document RAG systems, including 3,276 documents, 5,168 queries across 6 languages and 4 document types.

## Executive Summary
This paper addresses the critical need for better evaluation methods for Retrieval-Augmented Generation (RAG) systems in document understanding, where existing benchmarks are fragmented, unrealistic, and fail to assess real-world performance. The authors introduce DOUBLE-BENCH, a comprehensive, large-scale, multilingual, and multimodal evaluation system that provides fine-grained assessment of each component within document RAG systems. Extensive experiments show that text embedding models are narrowing the gap with visual models, while multimodal embedding models struggle with low-resource languages, and RAG frameworks exhibit overconfidence, often answering without sufficient evidence.

## Method Summary
DOUBLE-BENCH provides a comprehensive evaluation framework for document RAG systems through three key components: (1) a curated corpus of 3,276 documents spanning 6 languages and 4 types (PDFs, scanned docs, slides, HTML), (2) a query set of 5,168 single- and multi-hop questions synthesized through iterative refinement and validated against ground truth retrieval, and (3) exhaustive evidence labeling with human verification. The system evaluates both retrieval performance using hit@1/3/5 metrics and answer quality using an LLM-as-a-judge approach. The evaluation includes 9 embedding models (4 text, 5 multimodal), 4 MLLMs with/without oracle evidence, and 4 RAG frameworks, revealing that text embeddings are rapidly improving while multimodal models struggle with low-resource languages and RAG frameworks tend to be overconfident in their answers.

## Key Results
- Text embedding models are rapidly narrowing the performance gap with visual models in document retrieval tasks
- Multimodal embedding models show significant challenges with low-resource languages
- RAG frameworks exhibit overconfidence, attempting to answer queries even when retrieval fails to find sufficient evidence
- The strongest model (colqwen2.5-3b) achieved an average hit@5 score of 0.795, demonstrating strong document understanding capabilities
- The benchmark successfully distinguishes context-grounded reasoning from inherent model knowledge

## Why This Works (Mechanism)
The benchmark's effectiveness stems from its comprehensive design that evaluates both retrieval and generation components in isolation and combination. By providing fine-grained assessment across multiple languages, document types, and reasoning complexities (single-hop vs multi-hop), it captures the full spectrum of document RAG challenges. The exhaustive evidence labeling and human verification ensure high-quality ground truth, while the iterative query synthesis process creates realistic, challenging questions that test actual document understanding rather than surface-level pattern matching.

## Foundational Learning

**Multimodal Document Understanding**: The ability to process text, tables, and figures within documents is essential for comprehensive document comprehension, requiring models to integrate information across different modalities.

*Why needed*: Modern documents contain diverse information types that cannot be fully understood by text-only models, making multimodal processing critical for real-world applications.

*Quick check*: Can the model correctly answer questions requiring information from both text paragraphs and corresponding figures?

**Epistemic Humility in RAG**: The capacity for RAG systems to recognize when they lack sufficient evidence to answer a question, rather than generating speculative responses.

*Why needed*: Overconfident systems that answer without adequate evidence can produce misinformation, making the ability to refuse when uncertain crucial for reliable deployment.

*Quick check*: Does the system refuse to answer when top-5 retrieval results contain no relevant information for the query?

**Signature Information vs Sequential Reasoning**: The tendency of models to solve multi-hop queries using distinctive information to eliminate candidates rather than performing step-by-step logical reasoning.

*Why needed*: Understanding how models actually solve complex reasoning tasks reveals whether they're truly reasoning or using shortcut strategies, informing both evaluation design and model improvement.

*Quick check*: Do accuracy rates on multi-hop queries correlate with hop count, or do they depend more on the distinctiveness of available evidence?

## Architecture Onboarding

**Component Map**: Document Corpus -> Query Synthesis -> Evidence Labeling -> Embedding Model Evaluation -> MLLM Evaluation -> RAG Framework Evaluation -> LLM-as-Judge Scoring

**Critical Path**: Document preprocessing (Docling/MinerU) -> Language detection (GPT-4o) -> Query synthesis (GPT-4o iterative refinement) -> Embedding model retrieval (hit@1/3/5) -> Answer generation (MLLMs/RAG frameworks) -> Quality scoring (GPT-4o judge)

**Design Tradeoffs**: The benchmark prioritizes comprehensive coverage across languages and document types over sheer scale, choosing 3,276 documents over millions to ensure quality and diversity. Synthetic query generation trades some naturalness for controllability and scalability, while human verification adds cost but ensures reliability.

**Failure Signatures**: 
- Ambiguous queries returning >5 ground truth pages indicate insufficient query specificity
- RAG frameworks answering without evidence reveal overconfidence issues
- Low hit rates for specific language-modal combinations highlight model limitations

**First Experiments**:
1. Run all 9 embedding models on the full corpus and compute hit@1/3/5 distributions
2. Evaluate 4 MLLMs with/without oracle evidence on a subset of queries
3. Test 4 RAG frameworks with top-5 retrieval on the same query subset

## Open Questions the Paper Calls Out

**Open Question 1**: How can document RAG frameworks be optimized to prioritize "epistemic humility" (refusing to answer when evidence is missing) over the current tendency toward "overconfidence"?

*Basis in paper*: [explicit] The authors identify an "overconfidence dilemma" where complex agents attempt to answer queries regardless of evidence availability, explicitly calling for future research to value identifying informational gaps as much as accuracy.

*Why unresolved*: Current frameworks prioritize answer generation success rates, incentivizing speculative responses when retrieval fails.

*What evidence would resolve it*: The development of a RAG architecture that maximizes a combined metric of accuracy and abstention rate on the DOUBLE-BENCH dataset, specifically reducing the "miss & attempt" error category.

**Open Question 2**: Does simply increasing the number of hops in a query reliably increase difficulty for MLLMs, given their tendency to use "signature information" rather than sequential reasoning?

*Basis in paper*: [explicit] The authors observe that MLLMs solve multi-hop queries using inclusion-based elimination of signature info rather than step-by-step logic, stating: "This challenges the assumption that more hops always increase difficulty, suggesting further investigation is needed."

*Why unresolved*: Benchmark design currently assumes linearity in difficulty relative to hop count, which may not align with how models actually process information.

*What evidence would resolve it*: A study correlating hop count with accuracy on DOUBLE-BENCH, controlling for the distinctiveness of "signature information" available in the evidence.

**Open Question 3**: Can combined strategies (e.g., interleaved embedding models) effectively leverage the narrowing performance gap between text and visual embedding models?

*Basis in paper*: [inferred] The paper notes that text embedding models are rapidly narrowing the gap with visual models due to advanced training techniques, leading the authors to suggest that the "critical influence of both" abilities incentivizes combined strategies.

*Why unresolved*: It is unclear if simply combining these modalities will yield additive performance or if architectural constraints (like training costs) will limit the transferability of text-based gains to visual domains.

*What evidence would resolve it*: Benchmarking a specifically designed interleaved multimodal embedding model against the current SOTA visual (Colqwen2.5-3b) and text (Qwen3-Embedding-4B) models on the multimodal sections of DOUBLE-BENCH.

## Limitations
- The benchmark relies on LLM-as-a-judge (GPT-4o) for answer quality assessment, introducing potential bias and limiting reproducibility
- The document corpus, while diverse, remains relatively small at 3,276 documents, potentially limiting generalizability to other domains
- The focus on academic and structured documents may not represent all real-world document types
- The synthetic query generation process may not capture all edge cases of real-world document retrieval scenarios

## Confidence

- **High confidence**: The experimental methodology and results for embedding model comparisons are well-specified and reproducible. The hit@1/3/5 metrics and the observed trend of text embeddings closing the gap with visual models are clearly demonstrated with statistical significance.

- **Medium confidence**: The RAG framework evaluation results are moderately reliable, though the specific configurations of MDocAgent, ViDoRAG, M3DocRAG, and Colqwen-gen frameworks are not fully detailed. The overconfidence phenomenon is well-documented but may vary with different framework versions.

- **Low confidence**: The synthetic query generation process and human verification reliability are less certain due to incomplete prompt specifications in the appendix. The generalization of results to non-academic document domains remains unclear.

## Next Checks

1. **Replicate embedding model experiments** using the specified 9 embedding models on the full document corpus with documented hyperparameters to verify hit@1/3/5 score distributions.

2. **Test RAG framework configurations** by implementing or obtaining the exact versions of MDocAgent, ViDoRAG, M3DocRAG, and Colqwen-gen frameworks to validate the overconfidence analysis and answer quality scores.

3. **Conduct cross-linguistic robustness testing** by running the same evaluation pipeline on additional low-resource languages not included in the current benchmark to assess the reported challenges with multimodal embeddings.