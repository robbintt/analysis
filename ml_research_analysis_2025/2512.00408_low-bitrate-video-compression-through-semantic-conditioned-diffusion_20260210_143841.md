---
ver: rpa2
title: Low-Bitrate Video Compression through Semantic-Conditioned Diffusion
arxiv_id: '2512.00408'
source_url: https://arxiv.org/abs/2512.00408
tags:
- video
- compression
- semantic
- diffusion
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces DiSCo, a semantic video compression framework\
  \ that decomposes videos into compact multimodal representations\u2014textual descriptions,\
  \ spatiotemporally degraded video, and optional sketches or poses\u2014and reconstructs\
  \ them using a conditional video diffusion model. The approach introduces a joint\
  \ spatiotemporal generation scheme with forward filling, a token interleaving strategy\
  \ for multimodal conditioning, and specialized codecs for each modality to maximize\
  \ compactness."
---

# Low-Bitrate Video Compression through Semantic-Conditioned Diffusion

## Quick Facts
- arXiv ID: 2512.00408
- Source URL: https://arxiv.org/abs/2512.00408
- Authors: Lingdong Wang; Guan-Ming Su; Divya Kothandaraman; Tsung-Wei Huang; Mohammad Hajiesmaili; Ramesh K. Sitaraman
- Reference count: 40
- Primary result: DiSCo achieves 5-10× bitrate reduction over traditional codecs at low bitrates using semantic video decomposition

## Executive Summary
This paper introduces DiSCo, a semantic video compression framework that decomposes videos into compact multimodal representations—textual descriptions, spatiotemporally degraded video, and optional sketches or poses—and reconstructs them using a conditional video diffusion model. The approach introduces a joint spatiotemporal generation scheme with forward filling, a token interleaving strategy for multimodal conditioning, and specialized codecs for each modality to maximize compactness. Experiments demonstrate that DiSCo outperforms traditional codecs (H.266, H.265, H.264) and neural codecs (DCVC-RT) by 5–10× and baseline semantic codecs (CVG, T-GVC) by 2–3× in perceptual metrics (LPIPS, DISTS, FID, FVD) at low bitrates. At 0.005 BPP, DiSCo achieves significant perceptual quality gains over baselines while maintaining semantic fidelity, even under ultra-low bitrate conditions as low as 0.00045 BPP.

## Method Summary
DiSCo decomposes videos into three compact modalities: textual descriptions (LLaVA + Zlib compression), spatiotemporally degraded video (DCVC-RT with spatial/temporal downsampling), and optional sketches or poses (custom codecs). A conditional video diffusion decoder with token interleaving (1 video token : 3 auxiliary tokens) and temporal forward filling reconstructs the video from these compressed modalities. The decoder uses a 13B-parameter LTX-Video DiT with 625M-parameter LoRA fine-tuning, trained on 8000-video OpenVid-1M subset for 8000 steps with L2/rectified flow loss.

## Key Results
- DiSCo achieves 5-10× bitrate reduction over traditional codecs (H.266, H.265, H.264) at low bitrates
- At 0.005 BPP, DiSCo achieves significant perceptual quality gains (LPIPS, DISTS, FID, FVD) over baselines
- Text modality provides 35-38% BD-rate reduction; sketch adds 4.95% on animation; pose adds 3.67% on human-centric content

## Why This Works (Mechanism)

### Mechanism 1: Complementary Multimodal Factorization
Decomposing video into text (semantics), degraded video (appearance), and optional sketch/pose (motion/geometry) preserves semantically meaningful information while enabling 5–10× bitrate reduction. Each modality captures orthogonal information—text describes objects/actions at ~3.7 kbits per clip, degraded video provides structural/color scaffolding at 0.00045–0.02 BPP, and sketch/pose adds motion cues only when beneficial. The diffusion decoder's generative prior fills in discarded details based on pretrained world knowledge.

### Mechanism 2: Token Interleaving for Redundancy-Free Conditioning
Alternating tokens from degraded video and auxiliary modalities (1 video token : 3 auxiliary tokens) prevents semantic overlap and modality mixture artifacts. Instead of concatenating modality tokens (which causes redundant frame descriptions) or frame-level alternation (which causes latent-space mixing), token-level interleaving ensures each spatial region is described by exactly one modality. The diffusion Transformer's attention mechanism resolves which modality to attend to without explicit modality embeddings.

### Mechanism 3: Temporal Forward Filling for Coherent Latent Space
Duplicating previously sampled frames to fill temporally downsampled positions (rather than zero-filling or leaving gaps) improves reconstruction quality by 17.5% BD-rate on LPIPS. After decoding the spatiotemporally degraded video, forward filling restores original frame count before VAE encoding, maintaining temporal continuity in the latent space and providing the diffusion model with a coherent motion scaffold.

## Foundational Learning

- **Concept: Latent Diffusion Models (LDMs)**
  - Why needed here: DiSCo operates in VAE latent space; understanding how LDMs compress spatial information into latents and denoise them is essential for debugging reconstruction quality
  - Quick check question: Can you explain why diffusion in latent space is more efficient than pixel-space diffusion, and what information is lost during VAE encoding?

- **Concept: Conditional Video Generation with Multimodal Inputs**
  - Why needed here: The core decoder is a conditional diffusion model; understanding how text encoders (T5), visual encoders (VAE), and cross-attention combine is necessary for modifying conditioning strategies
  - Quick check question: How does cross-attention in a DiT (Diffusion Transformer) incorporate text conditioning, and what happens when visual and text conditions conflict?

- **Concept: Rate-Distortion Theory and Perceptual Metrics**
  - Why needed here: DiSCo optimizes for perceptual metrics (LPIPS, FID, FVD) over pixel fidelity (PSNR, SSIM); understanding this tradeoff explains why it outperforms traditional codecs on some metrics while underperforming on others
  - Quick check question: Why does a codec that "hallucinates" details achieve better LPIPS but worse PSNR than H.266, and when is each metric appropriate?

## Architecture Onboarding

- **Component map:** [Sender] Video → LLaVA → Text → Zlib → Bitstream; Video → Spatiotemporal Downsample → DCVC-RT → Bitstream; Video → InformativeDrawings/DWPose → Custom Codec → Bitstream; [Receiver] Bitstreams → Decode → Text (T5 encoder) + Degraded Video (VAE) + Sketch/Pose (VAE) → Token Interleaving → Concat with Noisy Latents → LTX-Video DiT + LoRA → Denoise → VAE Decode → Reconstructed Video

- **Critical path:** The degraded video modality is the primary appearance scaffold; if DCVC-RT compression or forward filling fails, the diffusion model lacks structural guidance and relies heavily on text priors (which lack color/spatial precision)

- **Design tradeoffs:**
  - LoRA fine-tuning (625M params) vs. full model training reduces training to "a few GPU hours" but limits adaptation to new domains
  - Token interleaving ratio (1:3) is fixed empirically; not validated against other ratios
  - Specialized sketch codec requires two-stage training on Anim400K—adds complexity for ~5% gain on animation only

- **Failure signatures:**
  - Semantic deviation: At 0.00045 BPP, reconstructed objects may differ in appearance from source
  - Modality leakage: If frame-level interleaving is accidentally used, pose skeletons appear blended into video
  - Temporal flickering: If forward filling is replaced with zero-filling, BD-rate degrades 60%

- **First 3 experiments:**
  1. Ablate forward filling: Compare no filling, zero filling, forward filling on a 10-video subset at D_t=4. Measure LPIPS and FVD
  2. Test token interleaving vs. concatenation: Implement token concatenation as baseline, compare visual quality on pose-conditioned videos
  3. Stress test ultra-low bitrate: Run at 0.00045 BPP on diverse content (sports, nature, faces). Assess where semantic fidelity breaks down

## Open Questions the Paper Calls Out

- Can an adaptive mechanism automatically select the optimal combination of semantic modalities (text, video, sketch, pose) based on video content characteristics?
- How does semantic compression performance scale to higher resolutions beyond 512×512, and what architectural changes are needed?
- What is the computational cost and latency of DiSCo at inference time, and can the framework achieve real-time or near-real-time performance?
- How should semantic fidelity be formally measured, and what is the optimal trade-off between bitrate reduction and semantic deviation from the source?

## Limitations

- Novel mechanisms (token interleaving, forward filling) lack direct comparisons to established alternatives in literature
- Ultra-low bitrate regime (0.00045 BPP) reveals semantic deviation artifacts without systematic characterization of failure modes
- Specialized sketch codec requires complex two-stage training on custom dataset for modest gains

## Confidence

- **High Confidence**: Claims about outperforming traditional codecs (H.266, H.265, H.264) and neural codecs (DCVC-RT) by 5–10× in perceptual metrics at low bitrates
- **Medium Confidence**: Claims about the specific mechanisms (token interleaving, forward filling) providing measured improvements
- **Low Confidence**: Claims about the ultra-low bitrate regime (0.00045 BPP) maintaining semantic fidelity across all content types

## Next Checks

1. **Motion magnitude threshold validation**: Test forward filling across varying temporal downsampling rates (D_t = 2, 4, 8) on high-motion content (sports, vehicle sequences). Measure at what motion displacement threshold forward filling degrades into temporal inconsistency artifacts.

2. **Novel content stress test**: Evaluate DiSCo on videos containing novel objects, actions, or visual styles not present in LTX-Video pretraining. Systematically compare semantic deviation between 0.02 BPP and 0.00045 BPP to quantify when text priors fail to compensate for degraded video scaffolding.

3. **Cross-domain adaptation test**: Fine-tune the LoRA decoder on a domain-shifted dataset (e.g., medical imaging, aerial surveillance) and measure performance degradation compared to the original OpenVid-1M pretraining. This validates whether the "few GPU hours" training claim holds across domains.