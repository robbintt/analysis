---
ver: rpa2
title: 'MCIGLE: Multimodal Exemplar-Free Class-Incremental Graph Learning'
arxiv_id: '2509.06219'
source_url: https://arxiv.org/abs/2509.06219
tags:
- uni00000013
- uni00000011
- uni00000048
- uni00000055
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MCIGLE is a novel framework for multimodal exemplar-free class-incremental
  graph learning that addresses catastrophic forgetting, distribution bias, memory
  limits, and weak generalization in continual learning scenarios. The framework processes
  multimodal graph-structured data through a Multimodal Feature Processing Module
  that extracts and aligns visual and textual features using correlation-based weighting
  and optimal transport, followed by a Periodic Feature Extraction Module that employs
  Fourier analysis for efficient feature representation.
---

# MCIGLE: Multimodal Exemplar-Free Class-Incremental Graph Learning

## Quick Facts
- **arXiv ID:** 2509.06219
- **Source URL:** https://arxiv.org/abs/2509.06219
- **Authors:** Haochen You; Baojing Liu
- **Reference count:** 27
- **Primary result:** Novel exemplar-free CIL framework with Fourier feature extraction and C-RLS updates achieves superior accuracy and low forgetting across four multimodal graph datasets

## Executive Summary
MCIGLE addresses catastrophic forgetting, distribution bias, and memory limits in multimodal exemplar-free class-incremental learning through a dual-stream architecture. The framework combines Fourier-based periodic feature extraction with a non-forgetting mainstream using Concatenated Recursive Least Squares (C-RLS), complemented by a residual fitting module for nonlinear corrections. Experimental results on COCO-QA, VoxCeleb, SNLI-VE, and AudioSet-MI demonstrate state-of-the-art performance with minimal forgetting rates, validating the effectiveness of analytical weight updates and multimodal alignment via optimal transport.

## Method Summary
MCIGLE processes multimodal graph-structured data through a Multimodal Feature Processing Module that extracts and aligns visual and textual features using correlation-based weighting and optimal transport. A Periodic Feature Extraction Module employs Fourier analysis for efficient feature representation, with parameters frozen after initial training. The Non-Forgetting Mainstream Module uses C-RLS for analytical weight updates without storing historical data, while a Residual Fitting Enhancement Module captures missed information through nonlinear transformations. The framework outputs predictions through a weighted combination of mainstream and residual streams, with PLC masking ensuring phase-wise label exclusivity.

## Key Results
- Achieves 0.782 accuracy on COCO-QA, 0.927 on VoxCeleb, 0.767 on SNLI-VE, and 0.683 on AudioSet-MI
- Maintains low forgetting rates across all datasets, outperforming state-of-the-art methods
- Ablation studies confirm effectiveness of Fourier-based feature extraction and C-RLS mechanism
- Successfully handles multimodal graph data without exemplars while preserving knowledge across phases

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Concatenated Recursive Least Squares (C-RLS) enables exemplar-free weight updates that mitigate catastrophic forgetting without storing historical data.
- Mechanism: C-RLS maintains recursively updated autocorrelation (Φ_k) and cross-correlation (Z_k) matrices with a forgetting factor β. The Sherman–Morrison formula computes Φ^{-1} efficiently without full matrix inversion. Weight updates follow: Ŵ^{(k)}_M = Ŵ^{(k-1)}_M + k_k(Y^{train}_k - X_{M,k} Ŵ^{(k-1)}_M), where the gain coefficient k_k scales updates based on current feature correlations.
- Core assumption: The linear ridge regression solution sufficiently approximates optimal class boundaries, and feature distributions remain stationary enough for recursive correlation estimates to remain valid across phases.
- Evidence anchors:
  - [abstract]: "applying Concatenated Recursive Least Squares for effective knowledge retention"
  - [section 2.3]: "enabling recursive weight updates without storing historical data and achieving performance comparable to joint training"
  - [corpus]: Related work (REAL, DS-AL, L3A) confirms analytic learning approaches reduce forgetting in EFCIL, though none specifically apply C-RLS to multimodal graphs.
- Break condition: Heavy distribution shift between phases violates the stationary correlation assumption—Φ_k may become unrepresentative, causing weight divergence.

### Mechanism 2
- Claim: Fourier-based periodic feature extraction captures temporal/structural patterns with fewer parameters than CNNs or MLLs.
- Mechanism: Each layer computes φ^{(l)}(x) = [cos(W_p^{(l)} x), sin(W_p^{(l)} x), σ(B_p̄ + W_p̄^{(l)} x)], combining periodic basis functions with nonlinear activations. The cos/sin components encode frequency-domain information while the σ branch captures residual non-periodic features.
- Core assumption: Node embeddings contain periodic or quasi-periodic structures (e.g., temporal cycles, recurring graph motifs) that benefit from explicit frequency representation.
- Evidence anchors:
  - [abstract]: "Fourier analysis for efficient feature representation"
  - [section 2.2]: "Fourier-based layers effectively capture periodic patterns with fewer parameters than CNNs or MLPs"
  - [corpus]: Weak direct evidence—neighbor papers don't examine Fourier networks for CIL. Ablation (Fig 3a) shows FAN outperforms FNN, SIREN, KAN on accuracy, but causality to periodicity is not isolated.
- Break condition: On aperiodic or highly irregular graph structures, the cos/sin projections add redundant dimensions without representational benefit, potentially increasing overfitting risk.

### Mechanism 3
- Claim: Dual-stream architecture (linear mainstream + nonlinear residual) compensates for underfitting in analytic learning while preserving knowledge stability.
- Mechanism: The mainstream C-RLS provides stable, analytically optimal weights. The residual stream computes eY_k = [zero-padded labels] - X_{M,k} W^{(k)}_M, then fits residuals via CNN projection + nonlinear activation (Tanh/Mish). Final prediction: Ŷ = λ_2 X_M W_M + (1-λ_2) X_C W_C.
- Core assumption: Residuals are largely phase-localized (error from old classes is minimal), so fitting residuals doesn't reintroduce forgetting through the compensation stream.
- Evidence anchors:
  - [abstract]: "Residual Fitting Enhancement Module captures missed information through nonlinear transformations"
  - [section 2.4]: "zero matrix enforces phase-wise label exclusivity" to prevent error propagation across phases
  - [corpus]: Neighbor work (DS-AL, Expandable Dual Memories) uses dual-stream designs for EFCIL, supporting the architectural pattern but not this specific residual formulation.
- Break condition: If mainstream residual errors correlate strongly with old-class features, the compensation stream may indirectly memorize old-class patterns, re-introducing forgetting despite PLC masking.

## Foundational Learning

- Concept: **Recursive Least Squares (RLS)**
  - Why needed here: C-RLS is the core anti-forgetting mechanism. Understanding how autocorrelation matrices evolve and how the gain coefficient scales updates is essential for debugging weight divergence.
  - Quick check question: Can you explain why the forgetting factor β < 1 enables plasticity-stability trade-offs, and what happens if β → 1?

- Concept: **Optimal Transport for Feature Alignment**
  - Why needed here: Multimodal alignment via transport plan P* fuses visual and textual graph features. Misaligned features propagate noise through both mainstream and residual streams.
  - Quick check question: Given cost matrix C and regularization ϵ, what does the transport plan P* minimize, and how does λ_1 balance feature cost vs. node similarity?

- Concept: **Catastrophic Forgetting Metrics (F, BwF, TF)**
  - Why needed here: Table 1 reports forgetting rate (F), backward transfer (BwF), and forward transfer (TF). Knowing what each measures is required to interpret whether MCIGLE actually reduces forgetting vs. improves plasticity.
  - Quick check question: If accuracy stays constant but F decreases across methods, what specific behavior changed in the model?

## Architecture Onboarding

- Component map:
  Input Graph (Visual + Textual) -> Multimodal Feature Processing Module (Correlation-weighted GNN + OT alignment) -> Periodic Feature Extraction (FAN - Fourier layers, frozen after phase 0) -> [Mainstream (C-RLS) -> Residual (CNN + σ_C)] -> λ_2-weighted sum -> softmax -> predictions

- Critical path:
  1. **Phase 0**: Train FAN end-to-end with cross-entropy, freeze all FAN parameters.
  2. **Each subsequent phase k**:
     - Extract features via frozen FAN.
     - Update mainstream weights via C-RLS (Equation 9–10).
     - Compute residuals eY_k, apply PLC masking (Equation 13).
     - Update compensation weights via C-RLS.
     - Combine streams with λ_2.

- Design tradeoffs:
  - **λ_2 value**: High λ_2 prioritizes stable mainstream predictions; low λ_2 amplifies residual corrections but risks instability. Paper doesn't report sensitivity analysis.
  - **β (forgetting factor)**: Controls how quickly old correlation information decays. Trade-off between plasticity (low β) and stability (high β).
  - **FAN freezing**: Prevents feature drift but limits adaptation to new class distributions. Alternative: partial fine-tuning (not explored).

- Failure signatures:
  - **Accuracy collapse on old classes while new-class accuracy remains high**: Likely β too low or Φ_k poorly conditioned.
  - **Residual stream dominates (λ_2 → 0 behavior)**: Compensation may overfit current phase; check PLC masking is correctly zero-padding old labels.
  - **OT alignment produces degenerate transport (all mass to few nodes)**: Cost matrix C or similarity L may be mis-scaled; check feature normalization.

- First 3 experiments:
  1. **Sanity check**: Run single-phase training on COCO-QA with only mainstream (λ_2 = 1.0). Verify accuracy matches baseline ridge regression.
  2. **Forgetting factor sweep**: On VoxCeleb 5-phase split, vary β ∈ {0.1, 0.5, 0.9, 0.99}. Plot F vs. β to locate plasticity-stability knee point.
  3. **Ablation: residual necessity**: Disable compensation stream (λ_2 = 1.0) and measure accuracy drop on complex multimodal samples vs. simple ones. Hypothesis: drop should be larger on cross-modal reasoning tasks (SNLI-VE).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can MCIGLE maintain its forgetting mitigation effectiveness as the number of incremental classes grows significantly larger (e.g., 50+ or 100+ classes)?
- Basis in paper: [inferred] Figures 2a and 2b show accuracy trends over "cumulative class" increments, but the exact number of incremental phases/classes tested is not specified, leaving scalability to large-scale class-incremental scenarios unstudied.
- Why unresolved: The experimental validation uses four datasets but does not report results on extended incremental sequences where accumulated forgetting effects typically compound.
- What evidence would resolve it: Experiments on benchmarks with 50+ incremental classes (e.g., ImageNet-1000 split into many tasks), reporting accuracy and forgetting rates across long task sequences.

### Open Question 2
- Question: How does the computational cost of the optimal transport-based multimodal alignment scale with graph size and node count?
- Basis in paper: [inferred] Equation 4 formulates optimal transport for feature alignment, but no analysis of computational complexity or runtime is provided, despite OT being potentially expensive for large graphs.
- Why unresolved: The paper focuses on accuracy and forgetting metrics but does not report training time, memory footprint during alignment, or scalability analysis on larger graph structures.
- What evidence would resolve it: Complexity analysis (big-O notation) and empirical runtime measurements on graphs with varying node counts (e.g., 1K, 10K, 100K nodes).

### Open Question 3
- Question: Can MCIGLE be extended to handle more than two modalities (beyond visual and textual) without significant architectural modifications?
- Basis in paper: [inferred] Section 2.1 specifies "Each modality m ∈ {v, t}" (visual and textual only), and AudioSet-MI results suggest audio is processed, but the framework design centers on two-modality alignment via optimal transport.
- Why unresolved: The optimal transport formulation and correlation-based weighting are described for pairwise modality alignment; extending to 3+ modalities would require either hierarchical alignment or modified fusion strategies.
- What evidence would resolve it: Experiments on truly multimodal benchmarks (e.g., video with audio, text, and visual frames) demonstrating consistent performance with three or more modalities.

### Open Question 4
- Question: What is the sensitivity of C-RLS performance to the forgetting factor β and regularization parameter γ across different data distributions?
- Basis in paper: [inferred] Section 2.3 mentions that Φ_k and Z_k are "updated recursively with forgetting factor β" and Equation 8 includes regularization parameter γ, but ablation studies (Figure 3b) replace C-RLS entirely rather than varying these hyperparameters.
- Why unresolved: The paper does not report sensitivity analysis for β or γ, which control the balance between plasticity and stability in recursive updates.
- What evidence would resolve it: Ablation experiments varying β ∈ [0.5, 0.99] and γ ∈ [10^-6, 10^-2] across datasets, reporting resulting accuracy and forgetting metrics.

## Limitations

- Reliance on Fourier layers' effectiveness lacks controlled studies on periodic vs. aperiodic data
- C-RLS robustness to severe distribution shifts is untested—abrupt class-concept changes could cause Φ_k to diverge
- PLC-based residual masking assumes phase-local error patterns, which may not hold for semantically overlapping classes across phases

## Confidence

- **High confidence:** General framework design (dual-stream + C-RLS + OT alignment), benchmark results on standard datasets, forgetting metrics interpretation
- **Medium confidence:** Fourier layer benefits (evidence is indirect), residual module necessity (depends on data periodicity), robustness to distribution shift (untested scenario)
- **Low confidence:** Optimal Transport alignment's sensitivity to hyperparameters (cost matrix scaling), impact of FAN freezing on long-task performance

## Next Checks

1. **Distribution shift stress test:** Simulate sudden class-concept drift between phases and measure C-RLS weight divergence and accuracy collapse
2. **Fourier sensitivity analysis:** Compare FAN's performance on periodic synthetic data vs. aperiodic real-world graphs to isolate the mechanism's contribution
3. **Residual masking robustness:** Remove PLC zero-padding and measure cross-phase error propagation; verify the assumption that residuals are phase-localized