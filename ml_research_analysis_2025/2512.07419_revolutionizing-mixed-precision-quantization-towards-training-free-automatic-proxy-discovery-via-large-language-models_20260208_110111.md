---
ver: rpa2
title: 'Revolutionizing Mixed Precision Quantization: Towards Training-free Automatic
  Proxy Discovery via Large Language Models'
arxiv_id: '2512.07419'
source_url: https://arxiv.org/abs/2512.07419
tags:
- proxy
- quantization
- proxies
- llms
- training-free
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of efficient mixed-precision
  quantization (MPQ) for deep neural networks, which is critical for deploying models
  on resource-constrained devices. Traditional MPQ methods rely on either expensive
  differentiable optimization or handcrafted training-free proxies, both of which
  have significant limitations.
---

# Revolutionizing Mixed Precision Quantization: Towards Training-free Automatic Proxy Discovery via Large Language Models

## Quick Facts
- **arXiv ID**: 2512.07419
- **Source URL**: https://arxiv.org/abs/2512.07419
- **Reference count**: 36
- **Primary result**: Proposes TAP, an LLM-driven framework that automatically discovers training-free proxies for mixed-precision quantization, achieving state-of-the-art accuracy with only 16 calibration samples and minimal GPU hours.

## Executive Summary
This paper addresses the challenge of efficient mixed-precision quantization (MPQ) for deep neural networks, which is critical for deploying models on resource-constrained devices. Traditional MPQ methods rely on either expensive differentiable optimization or handcrafted training-free proxies, both of which have significant limitations. To overcome these issues, the authors propose TAP (Training-free Automatic Proxy discovery), the first LLM-driven framework that automatically generates training-free MPQ proxies without human expert intervention. TAP uses a novel Direct Policy Optimization (DPO)-based reinforcement learning approach to enhance LLM reasoning and create a positive feedback loop between the LLM and the MPQ task. This enables iterative refinement of proxies, leading to superior performance. Extensive experiments on mainstream benchmarks (ImageNet, CIFAR-10, etc.) demonstrate that TAP achieves state-of-the-art accuracy while being significantly more efficient in terms of calibration and convergence. For example, TAP-C attains 72.93% Top-1 accuracy on ResNet18 with only 9.17×10−6 GPU hours, outperforming existing methods by a large margin. The framework also generalizes well to larger transformer architectures (ViT-B, DeiT-B, Swin-B) and shows strong robustness across different LLM backbones. Overall, TAP provides a scalable and efficient paradigm for training-free MPQ, offering a fresh perspective on LLM-driven design algorithms.

## Method Summary
TAP automates the discovery of training-free proxies for mixed-precision quantization using a large language model (LLM) and Direct Policy Optimization (DPO). The framework iteratively generates candidate proxy formulas as executable Python code, evaluates their fitness using a small calibration set (16 samples), and refines the LLM's prompt strategy via DPO based on preference pairs derived from proxy performance. This creates a positive feedback loop that aligns the LLM's reasoning with the specific requirements of the target hardware and model. The process runs for a maximum of 5 generations with a population of N candidates, using mutation and crossover operations to explore the proxy space. The best-performing proxy is then used to determine layer-wise bit-widths for quantization, achieving state-of-the-art accuracy with minimal computational overhead.

## Key Results
- TAP-C achieves 72.93% Top-1 accuracy on ResNet18 with only 9.17×10−6 GPU hours, outperforming existing methods by a large margin.
- TAP attains 70.26% Top-1 accuracy with only 16 calibration samples, compared to 64 samples required by other methods.
- The framework generalizes well to larger transformer architectures (ViT-B, DeiT-B, Swin-B) and shows strong robustness across different LLM backbones.

## Why This Works (Mechanism)

### Mechanism 1: LLM-Driven Proxy Synthesis
Large Language Models (LLMs) can synthesize novel, training-free quantization proxies (formulas) that outperform human-designed heuristics when guided by domain-specific prompts. The framework treats proxy design as a code generation task, with the LLM receiving a prompt defining the MPQ task and generating executable Python code representing a candidate proxy formula.

### Mechanism 2: Direct Policy Optimization (DPO) Feedback Loop
Optimizing the LLM's prompt strategy via Direct Policy Optimization (DPO) creates a positive feedback loop that aligns the LLM's "reasoning" with the specific performance requirements of the target hardware/model. Instead of naive prompting, the system constructs "preference pairs" based on the fitness scores of generated proxies and uses DPO to update the LLM, reinforcing generation strategies that lead to high-fitness proxies.

### Mechanism 3: Data-Efficient Sensitivity Estimation
The LLM-discovered proxies enable accurate sensitivity ranking using orders of magnitude less calibration data than existing methods by finding more robust statistical estimators. The evolved proxies capture layer importance robustly without needing dense sampling of the activation space, reducing the required calibration data from thousands of images to just 16 samples.

## Foundational Learning

- **Concept: Mixed-Precision Quantization (MPQ)**
  - **Why needed here**: This is the core problem space. You must understand that MPQ involves assigning different bit-widths (e.g., 2-bit vs 8-bit) to different layers to balance model size and accuracy.
  - **Quick check question**: If a layer has high sensitivity to quantization noise, should it be assigned a higher or lower bit-width?

- **Concept: Zero-Cost Proxies (NAS)**
  - **Why needed here**: The paper automates the search for a "proxy"—a cheap formula that predicts model performance without training. Understanding this distinction (predicting vs. training) is crucial.
  - **Quick check question**: Does a zero-cost proxy require backpropagation to evaluate a candidate architecture?

- **Concept: Direct Preference Optimization (DPO)**
  - **Why needed here**: The core innovation is using DPO to tune the LLM. You need to know that DPO optimizes a policy (the LLM) directly using preference pairs ("Output A is better than Output B") rather than a scalar reward.
  - **Quick check question**: In DPO, does the model learn from an absolute score or a relative comparison between two outputs?

## Architecture Onboarding

- **Component map**: Initial Prompt -> LLM Generation (Code) -> Code Execution (Inference on 16 images) -> Fitness Calculation -> DPO Update -> New Prompt

- **Critical path**: Initial Prompt → LLM Generation (Code) → Code Execution (Inference on 16 images) → Fitness Calculation → DPO Update → New Prompt

- **Design tradeoffs**:
  - LLM Size vs. Search Cost: Larger LLMs may find better proxies but increase the overhead of the "Proxy Generation" step.
  - Population Size (N) vs. Convergence: Small populations may lack diversity; large populations increase the evaluation cost.
  - Proxy Complexity vs. Latency: Complex proxies might yield higher accuracy but slow down the bit-allocation search.

- **Failure signatures**:
  - Syntax Errors: LLM generates code that fails to parse or run.
  - Stagnation: Fitness scores stop improving after 1-2 generations.
  - Overfitting: Proxy perfectly ranks layers on the calibration set but fails on the actual validation set.

- **First 3 experiments**:
  1. Baseline Sanity Check: Run TAP with a fixed random prompt (no DPO) on ResNet18.
  2. Data Efficiency Ablation: Run TAP with calibration sets of size [16, 32, 64, 128].
  3. Cross-Architecture Transfer: Apply the proxy discovered on ResNet18 directly to ViT-B or Swin-B without re-running the search.

## Open Questions the Paper Calls Out

- **Open Question 1**: How can the TAP framework be extended to support potential applications in the deep learning community beyond the vision tasks demonstrated?
- **Open Question 2**: Can the TAP framework discover effective proxies for extreme quantization regimes (e.g., binary or ternary) outside the standard [2, 8] bit-width search space?
- **Open Question 3**: Does the reliance on a small calibration set (16 samples) make the fitness evaluation sensitive to outlier data or specific image distributions?

## Limitations
- The framework's performance heavily depends on the quality of LLM prompts and the stability of the DPO optimization loop, which are not fully specified in the paper.
- The claim of achieving state-of-the-art results with minimal calibration data (16 samples) may not generalize to highly domain-specific or out-of-distribution datasets.
- The computational overhead of LLM inference for proxy generation is not thoroughly benchmarked against traditional methods.

## Confidence
- **High Confidence**: The core mechanism of using LLMs to generate quantization proxies is technically sound and the experimental results on standard benchmarks (ImageNet, CIFAR-10) are reproducible.
- **Medium Confidence**: The DPO-based feedback loop and its ability to iteratively improve proxy quality across generations are plausible but require more rigorous ablation studies.
- **Low Confidence**: The extreme data efficiency claim (16 calibration samples) and its robustness across diverse model architectures need independent validation.

## Next Checks
1. **Cross-Dataset Generalization**: Evaluate TAP's performance on a dataset significantly different from ImageNet (e.g., medical imaging or satellite data) to test the robustness of the 16-sample calibration assumption.
2. **Ablation of DPO Components**: Disable the DPO loop and compare the proxy quality and convergence speed to the full TAP framework to quantify the contribution of iterative LLM refinement.
3. **Scalability to Larger Models**: Apply TAP to larger transformer architectures (e.g., ViT-L/16) and measure whether the discovered proxies maintain their efficiency and accuracy advantages.