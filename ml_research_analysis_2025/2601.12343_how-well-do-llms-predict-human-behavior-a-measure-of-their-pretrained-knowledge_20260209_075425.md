---
ver: rpa2
title: How Well Do LLMs Predict Human Behavior? A Measure of their Pretrained Knowledge
arxiv_id: '2601.12343'
source_url: https://arxiv.org/abs/2601.12343
tags:
- train
- training
- sample
- prediction
- test
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces a measure of the predictive value of large
  language models (LLMs) in economic settings: the equivalent sample size (ESS), defined
  as the minimum amount of domain-specific data required for a trained model to match
  the predictive accuracy of a fixed LLM. The authors propose estimating ESS by comparing
  the LLM''s performance to that of machine learning algorithms trained on increasingly
  large samples, using block-out cross-validation to estimate error curves.'
---

# How Well Do LLMs Predict Human Behavior? A Measure of their Pretrained Knowledge

## Quick Facts
- **arXiv ID**: 2601.12343
- **Source URL**: https://arxiv.org/abs/2601.12343
- **Authors**: Wayne Gao; Sukjin Han; Annie Liang
- **Reference count**: 40
- **Primary result**: Introduces Equivalent Sample Size (ESS) to measure LLM predictive value, finding substantial heterogeneity across tasks (20-600 observations) in PSID data

## Executive Summary
This paper develops a rigorous framework to quantify how much pretrained knowledge large language models (LLMs) contribute to predicting human behavior in economic settings. The key innovation is the Equivalent Sample Size (ESS), defined as the minimum amount of domain-specific data needed for a trained model to match a fixed LLM's predictive accuracy. Through block-out cross-validation and sequential hypothesis testing, the authors estimate ESS for three PSID outcomes: wages, homeownership, and smoking. They find dramatic heterogeneity—LLMs substitute for approximately 20 observations when predicting hourly wages but up to 600 observations for homeownership prediction, while providing no meaningful predictive value for smoking behavior.

## Method Summary
The authors estimate ESS by comparing a fixed LLM's prediction error to that of machine learning algorithms trained on increasingly large samples. They use block-out cross-validation with B disjoint training blocks of size N to estimate algorithm error curves, then apply sequential hypothesis testing to construct one-sided confidence intervals for the smallest N where algorithm error falls below LLM error. The procedure is theoretically justified through a new asymptotic theory for cross-validated prediction error under fixed-N regimes. Applied to PSID data with wages, homeownership, and smoking as outcomes, they use random forests as the benchmark algorithm and ChatGPT-4 for LLM predictions via persona-based prompts.

## Key Results
- ESS for hourly wage prediction: approximately 20 observations
- ESS for homeownership prediction: approximately 600 observations (95% CI: [590, 676])
- ESS for smoking prediction: effectively zero—no meaningful predictive value
- Substantial heterogeneity in LLM predictive value across different economic outcomes
- Results robust to alternative algorithms and prompt designs

## Why This Works (Mechanism)

### Mechanism 1: Block-Out Cross-Validation for Error Curve Estimation
Partitioning data into disjoint training blocks and averaging out-of-sample losses produces unbiased estimates of algorithm error at fixed training sizes. Data of size n is split into B = ⌊n/N⌋ blocks of size N. Each block serves as a training set; the complement serves as the test set. The CV estimator is ē^CV_N = (1/B) Σ_b ê_b where ê_b is the average test loss for block b. This yields an estimate of e^a_N = E_{D_N}[E_{(X,Y)}[ℓ(a(D_N), (X,Y))]].

### Mechanism 2: Sequential Testing with Nested Hypotheses Controls Coverage
Testing hypotheses H_{0,k}: e^a_{N_k} ≤ e^LLM sequentially at level α produces valid (1-α) one-sided confidence intervals for N* without multiple-testing corrections. Under monotonicity (Assumption 3.1), null hypotheses are ordered: for k < k*, e^a_{N_k} > e^LLM (false nulls); for k ≥ k*, e^a_{N_k} ≤ e^LLM (true nulls). The procedure stops at the first non-rejection, setting Ñ*_α = N_{k-1} + 1.

### Mechanism 3: Fixed-N Asymptotic Theory for Cross-Validated Risk
Holding training size N fixed while n → ∞ yields a CLT for block-out CV estimators with estimable asymptotic variance. The CV estimator decomposes into training contribution A_n and testing contribution B_n. The asymptotic variance σ²_N has three components: (1) V_{N,train} = Var(e(f_N)) from training randomness; (2) V_{N,test} = Var(e^a_N(Z)) from test-point variability; (3) C_N = Cov(e(f_N), e^a_N(Z_1)) from each observation appearing in one training block and all test complements.

## Foundational Learning

- **Cross-validation fundamentals**: Understanding why training blocks must be disjoint and how variance decomposes is essential for correct implementation.
  - Quick check: In 5-fold CV, what is the training size for each fold? In block-out CV with n=500 and N=50, how many training blocks are there?

- **Asymptotic statistics for dependent data**: CV estimators involve dependent observations (each appears in multiple test sets), requiring Hájek projections and careful variance decompositions.
  - Quick check: Why does standard CLT not apply directly to CV error estimates? What creates dependence across test losses?

- **Sequential hypothesis testing**: The inference procedure exploits ordered hypotheses; understanding why this avoids Bonferroni correction is necessary to justify the method's efficiency gains.
  - Quick check: If you test H_1, H_2, H_3 sequentially at α=0.05 each, when is multiple testing correction required? When is it not?

## Architecture Onboarding

- **Component map**: Prompt design -> LLM prediction module -> ML error curve estimation -> Variance estimation module -> Sequential testing module -> ESS estimate
- **Critical path**: Prompt design → LLM error estimation → ML error curve estimation (most computationally intensive) → variance estimation → sequential testing
- **Design tradeoffs**: 
  - Grid coarseness: Finer grids (N_k = k) yield sharper CIs but require more CV computations; coarser grids are conservative
  - Algorithm choice: More flexible algorithms (random forests vs. lasso) are more demanding benchmarks, potentially yielding smaller ESS estimates
  - Fixed-N vs. fixed-B regime: Small B (few blocks) requires fixed-B asymptotics (Appendix B) with stability assumptions; large B allows fixed-N theory
- **Failure signatures**:
  - All nulls rejected: LLM substantially outperforms ML even at maximum training size → consider more flexible algorithms or larger N_K
  - First null not rejected: ML matches LLM with single observation → LLM provides no domain-specific value for this task
  - Non-monotonic error curves: Violates Assumption 3.1 → sequential testing may not control coverage; diagnose with visualization
- **First 3 experiments**:
  1. Validate implementation on synthetic data where true N* is analytically computable
  2. Replicate homeownership result: Using PSID data, verify ESS estimate falls in [590, 676] range
  3. Stress test with alternative algorithms: Replace random forest with gradient boosting; verify ESS estimates remain qualitatively similar

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but several remain unaddressed: sensitivity of ESS to prompt design, factors driving heterogeneity across outcomes, optimal selection of benchmark algorithms, and validation of CATE-ESS in settings with known ground truth.

## Limitations
- Block-out CV design assumes i.i.d. data and may be inefficient for correlated economic outcomes
- Sequential testing requires monotonic error curves which may not hold for algorithms that overfit at intermediate sample sizes
- Fixed-N asymptotic theory relies on sufficiently large block counts and finite moment conditions that may be violated in practice
- Prompts are not fine-tuned for specific prediction tasks, potentially underestimating LLM utility

## Confidence

- **High confidence**: Theoretical validity of sequential testing procedure under monotonicity (Assumption 3.1) is well-established through nested truth structure argument
- **Medium confidence**: Fixed-N asymptotic theory for CV estimators is novel and rigorous, but practical performance depends on sample size requirements not fully characterized
- **Medium confidence**: Empirical findings showing heterogeneous ESS across outcomes (20 for wages vs 600 for homeownership) are internally consistent, though economic interpretation requires careful consideration

## Next Checks

1. **Stress test non-monotonicity**: Systematically evaluate the sequential testing procedure when algorithm error curves are non-monotonic, measuring Type I error inflation relative to nominal levels

2. **Compare fixed-N vs fixed-B asymptotics**: For small B (e.g., B ≤ 10), implement the fixed-B regime theory from Appendix B and compare ESS estimates to those from fixed-N theory to assess regime sensitivity

3. **Task-specific prompt optimization**: Develop and test task-specific prompt variants for each outcome (wages, homeownership, smoking) to establish upper bounds on LLM predictive value and compare to current estimates