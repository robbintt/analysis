---
ver: rpa2
title: Convergence and stability of Q-learning in Hierarchical Reinforcement Learning
arxiv_id: '2511.17351'
source_url: https://arxiv.org/abs/2511.17351
tags:
- low-level
- high-level
- q-learning
- feudal
- policy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes a Feudal Q-learning scheme for Hierarchical
  Reinforcement Learning (HRL) and proves its convergence and stability under specific
  conditions. The method leverages Two-Timescale Stochastic Approximation (TTSA) theory
  to analyze the coupled updates of high-level and low-level Q-functions.
---

# Convergence and stability of Q-learning in Hierarchical Reinforcement Learning

## Quick Facts
- **arXiv ID**: 2511.17351
- **Source URL**: https://arxiv.org/abs/2511.17351
- **Reference count**: 40
- **Primary result**: Feudal Q-learning with Two-Timescale Stochastic Approximation converges to optimal Q-functions and remains stable under specific conditions

## Executive Summary
This paper proposes a Feudal Q-learning scheme for Hierarchical Reinforcement Learning (HRL) and proves its convergence and stability under specific conditions. The method leverages Two-Timescale Stochastic Approximation (TTSA) theory to analyze the coupled updates of high-level and low-level Q-functions. The primary theoretical result shows that the updates converge to optimal Q-functions and remain bounded (stable) under standard RL assumptions plus Lipschitz continuity and greedy-in-the-limit policies. The authors also provide a game-theoretic interpretation, showing the solution is both a Nash and Stackelberg equilibrium. Numerical experiments on a Four Rooms Minigrid environment demonstrate convergent behavior and improved continual learning compared to standard Q-learning, with the HRL agent converging in approximately 22% fewer episodes during retraining.

## Method Summary
The method implements Feudal Q-learning where a high-level manager selects goals every T steps and a low-level worker executes primitive actions to achieve these goals. The coupled Q-learning updates operate on different timescales - the low-level updates with step-size α(n) while the high-level updates with step-size β(n), where β(n)/α(n) → 0 as n → ∞. This timescale separation allows treating the high-level policy as quasi-static from the low-level learner's perspective. The low-level reward function is defined as a free design parameter, typically sparse or distance-based. The convergence analysis uses TTSA theory to map the discrete updates to continuous ODE approximations, proving convergence to optimal Q-functions and stability under standard RL assumptions plus Lipschitz continuity and greedy-in-the-limit policies.

## Key Results
- Feudal Q-learning converges to optimal Q-functions under standard RL assumptions plus Lipschitz continuity and GLIE policies
- The converged solution represents both a Nash equilibrium and a Stackelberg equilibrium in the hierarchical game
- On a Four Rooms Minigrid environment, the HRL agent converges in approximately 22% fewer episodes during retraining compared to standard Q-learning
- The method remains stable (Q-values bounded) under the specified assumptions, unlike standard Q-learning which can diverge in hierarchical settings

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Separating update rates resolves the non-stationarity inherent in coupled Hierarchical RL updates.
- **Mechanism:** The low-level Q updates with step-size α(n) (fast) while the high-level Q updates with β(n) (slow). This treats the slow-varying high-level policy as "quasi-static" from the perspective of the low-level learner, effectively converting a two-player non-stationary game into a sequence of single-agent stationary problems.
- **Core assumption:** The ratio of step-sizes must satisfy lim_{n→∞} β(n)/α(n) = 0 (Assumption 4c).
- **Evidence anchors:** [Section 3.2] "Within the TTSA framework, the timescale separation ensures that the iterates... track the solutions of [Eq 14] dx/dt = h(x(t), y) where y is held fixed." [Abstract] "...leverages Two-Timescale Stochastic Approximation (TTSA) theory to analyze the coupled updates..." [Corpus] Related work (e.g., "Implicit Q-Learning") highlights sensitivity to step-size calibration; timescale separation acts as a structural solution to this calibration problem.
- **Break condition:** Using comparable step sizes (α ≈ β) causes the "quasi-static" assumption to fail, potentially leading to oscillation or divergence.

### Mechanism 2
- **Claim:** Stability of the learning process requires smooth changes in policy behavior relative to value changes.
- **Mechanism:** To guarantee boundedness of iterates, the mapping F from Q-values to policies must be Lipschitz continuous. This prevents infinitesimal changes in Q-values from causing drastic jumps in action distributions, which stabilizes the ODE approximation used in the convergence proof.
- **Core assumption:** The policy extraction method (e.g., Boltzmann) must satisfy Assumption 3 (|F(Q₁) - F(Q₂)| ≤ L ||Q₁ - Q₂||).
- **Evidence anchors:** [Section 3.1] "Concerning Assumption 3, a Boltzmann exploration policy satisfies the Lipschitz continuous assumption." [Section 3.3] Theorem 1 proof relies on Lemma 4 (Lipschitz continuity of g and h) to verify assumptions of Theorem 8.1 in [23]. [Corpus] Weak direct corpus evidence for Lipschitz policies specifically in HRL, but related RL stability literature emphasizes continuous policy manifolds.
- **Break condition:** Using deterministic greedy policies (non-continuous step function) during the learning phase violates this condition, risking instability.

### Mechanism 3
- **Claim:** The converged solution of the Feudal scheme represents a specific game-theoretic equilibrium, ensuring consistent behavior across levels.
- **Mechanism:** The hierarchy is framed as a leader-follower (Stackelberg) game. The algorithm converges to a point (Q^{h,*}, Q^{l,*}) where the high-level (leader) maximizes utility assuming the low-level (follower) has already optimized for the given goal. This ensures the solution is a fixed point of the coupled Bellman equations.
- **Core assumption:** Policies must be Greedy in the Limit with Infinite Exploration (GLIE) to ensure all state-action pairs are visited sufficiently (Assumption 5 & 6).
- **Evidence anchors:** [Section 4] "The pair (Q^{h,*}, Q^{l,*}) found by the Feudal Q-learning updates is both a Nash equilibrium and a Stackelberg equilibrium where the high-level is the leader..." [Abstract] "...solution is both a Nash and Stackelberg equilibrium."
- **Break condition:** Incomplete exploration (violation of Assumption 6) results in converging to sub-optimal equilibria or "spurious" fixed points not representative of the global optimum.

## Foundational Learning

- **Concept:** **Bellman Operators & Fixed Points**
  - **Why needed here:** To understand Lemma 1 and 2, you must grasp that convergence is defined as finding the fixed point TQ = Q. The proof shows the ODE tracks these operators.
  - **Quick check question:** Can you explain why a contraction mapping (γ < 1) guarantees a unique fixed point?

- **Concept:** **Stochastic Approximation (SA) & ODE Method**
  - **Why needed here:** The entire proof strategy relies on mapping discrete noisy updates (Eq. 8) to continuous deterministic ODEs (Eq. 11). Without this concept, the stability analysis is opaque.
  - **Quick check question:** If step-sizes α(n) do not satisfy the Robbins-Monro conditions (∑ α = ∞, ∑ α² < ∞), what happens to the noise variance in the limit?

- **Concept:** **Markov Decision Processes (MDPs)**
  - **Why needed here:** The paper decomposes a flat MDP into two "fictitious" MDPs (M^h, M^l). Understanding state transitions P(s'|s,a) is required to define the coupled dynamics P^h_{π^l} and P^l_{π^h}.
  - **Quick check question:** How does the definition of the low-level state s^l_k = (s_k, ω_k, s^{de}_k) augment the original MDP state space?

## Architecture Onboarding

- **Component map:**
  - High-Level Manager (M^h) -> Sets goal ω^h_t -> Low-Level Worker (M^l) -> Executes action a_k -> Environment -> Returns state s and reward r

- **Critical path:**
  1. Define the temporal abstraction interval T (number of low-level steps per high-level step)
  2. Initialize Q^h and Q^l (can be optimistic)
  3. Tune step sizes α(n), β(n) such that β(n)/α(n) → 0 (e.g., α = 1/n^{0.5}, β = 1/n^{0.8})
  4. Ensure exploration policies satisfy GLIE and Lipschitz continuity

- **Design tradeoffs:**
  - **Interval T:** Large T simplifies the high-level problem (longer horizon) but reduces high-level responsiveness to fast environmental changes
  - **Reward r^l:** Hand-designing the intrinsic reward offers flexibility but introduces a dependency on domain knowledge
  - **Step-sizes:** A larger ratio α/β improves stability but may slow down the overall convergence of the hierarchy

- **Failure signatures:**
  - **Oscillation:** High-level policy flips goals rapidly; implies timescale separation is insufficient (β is too large)
  - **Lethargy:** Low-level policy wanders aimlessly; implies the intrinsic reward r^l is too sparse or conflicting with high-level intent
  - **Divergence:** Q-values explode to infinity; implies violation of stability assumptions (e.g., rewards unbounded or non-Lipschitz policy updates)

- **First 3 experiments:**
  1. **Timescale Verification:** Run Feudal Q-learning on a simple gridworld while sweeping the β/α ratio to empirically find the stability boundary predicted by Theorem 1
  2. **Continual Learning Check:** Train to convergence, then move the goal (as in Section 5). Verify that retraining episodes are ≈ 22% fewer than flat Q-learning to confirm effective policy reuse
  3. **Policy Ablation:** Compare Boltzmann (Lipschitz) vs. ε-greedy (non-Lipschitz at boundary) to observe transient stability differences during the initial learning phase

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the proposed Two-Timescale Stochastic Approximation (TTSA) framework be extended to multi-timescale schemes to theoretically guarantee convergence in hierarchies with more than two levels?
- **Basis in paper:** [explicit] Page 2 states that by linking Feudal RL to TTSA, one could leverage "multi-timescales... that would enable deeper hierarchies," but the current work analyzes only a two-level scheme.
- **Why unresolved:** The main theorem (Theorem 1) proves convergence only for a single pair of high-level and low-level updates; it does not provide guarantees for architectures with recursive sub-goals.
- **What evidence would resolve it:** A convergence proof for an N-level hierarchy using multi-timescale ODE analysis or finite-time bounds for multi-level stochastic approximation.

### Open Question 2
- **Question:** What are the finite-time performance bounds (sample complexity) of the Feudal Q-learning algorithm?
- **Basis in paper:** [explicit] Page 2 suggests that TTSA theory could be leveraged to derive "finite-time guarantees," but the paper itself focuses exclusively on asymptotic convergence (lim_{n→∞}).
- **Why unresolved:** While Theorem 1 guarantees almost sure convergence to optimal Q-functions, it does not quantify the speed of learning or the number of samples required to reach an ε-optimal policy.
- **What evidence would resolve it:** A theoretical bound on the error ||Q_n - Q^*|| as a function of the number of iterations n.

### Open Question 3
- **Question:** How can the game-theoretic interpretation of the solution as a Stackelberg equilibrium be utilized to design novel HRL update rules or reward mechanisms?
- **Basis in paper:** [explicit] Page 13 states that "future work should further explore the connections between HRL and game theory," following the proof that the fixed point constitutes such an equilibrium.
- **Why unresolved:** The paper identifies the equilibrium but does not derive specific algorithmic variants (e.g., specific leader-follower reward shaping) that might accelerate convergence or improve stability over the standard Feudal Q-learning updates.
- **What evidence would resolve it:** A new HRL algorithm derived explicitly from Stackelberg game dynamics with demonstrated empirical or theoretical improvements over the baseline Feudal scheme.

## Limitations
- The theoretical analysis depends critically on strong assumptions including Lipschitz continuity and timescale separation, which may not hold in practice
- The choice of low-level reward function is explicitly noted as a "free design parameter," introducing significant variability in practical performance that is not explored in the analysis
- The paper provides asymptotic convergence guarantees but does not investigate finite-time performance or sample complexity bounds

## Confidence
- **High Confidence**: The basic convergence mechanism through timescale separation is well-established in TTSA theory and the paper's application follows standard proof techniques
- **Medium Confidence**: The stability guarantees under Lipschitz continuity and the equilibrium properties (Nash/Stackelberg) follow logically from the assumptions, but these assumptions are strong and their practical necessity is not empirically validated
- **Low Confidence**: The specific performance claims (22% fewer episodes in continual learning) depend heavily on implementation details (reward function, step-sizes, exploration strategy) that are underspecified in the paper

## Next Checks
1. **Timescale Sensitivity Analysis**: Systematically vary the ratio β/α to empirically verify the predicted stability boundary and measure how convergence speed varies with different timescale separations
2. **Policy Continuity Impact**: Compare convergence behavior using Lipschitz-continuous policies (Boltzmann) versus discontinuous policies (ε-greedy) to quantify the practical importance of Assumption 3
3. **Reward Function Robustness**: Implement multiple low-level reward functions (sparse vs. dense) and measure their impact on both learning efficiency and final performance to understand the sensitivity to this "free design parameter"