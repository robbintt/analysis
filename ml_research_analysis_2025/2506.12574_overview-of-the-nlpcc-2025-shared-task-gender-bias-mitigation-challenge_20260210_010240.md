---
ver: rpa2
title: 'Overview of the NLPCC 2025 Shared Task: Gender Bias Mitigation Challenge'
arxiv_id: '2506.12574'
source_url: https://arxiv.org/abs/2506.12574
tags:
- bias
- gender
- language
- mitigation
- corpus
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CORGI-PM, a high-quality Chinese corpus for
  gender bias probing and mitigation containing 32.9k human-annotated sentences, including
  5.2k gender-biased sentences with corresponding bias-eliminated versions. The corpus
  addresses the lack of Chinese fairness-related computational linguistic resources
  and provides nuanced contextual-level gender bias data beyond existing word- or
  grammar-level resources.
---

# Overview of the NLPCC 2025 Shared Task: Gender Bias Mitigation Challenge

## Quick Facts
- arXiv ID: 2506.12574
- Source URL: https://arxiv.org/abs/2506.12574
- Reference count: 40
- Primary result: Chinese corpus CORGI-PM with 32.9k sentences, 5.2k biased with human-rewritten neutral versions; detection F1 up to 0.850, mitigation BLEU below 0.30

## Executive Summary
This paper introduces CORGI-PM, a high-quality Chinese corpus for gender bias probing and mitigation containing 32.9k human-annotated sentences, including 5.2k gender-biased sentences with corresponding bias-eliminated versions. The corpus addresses the lack of Chinese fairness-related computational linguistic resources and provides nuanced contextual-level gender bias data beyond existing word- or grammar-level resources. The authors propose an automatic filtering pipeline to create a candidate pool from large-scale Chinese corpora, followed by human annotation using a specialized scheme that classifies bias into three subtypes (AC, DI, ANB) and requires paraphrasing biased sentences into gender-neutral versions. The corpus supports three NLP challenges: gender bias detection, classification, and mitigation. Evaluation results show that while detection tasks achieve reasonable performance (F1 up to 0.850), mitigation remains challenging with BLEU scores below 0.30, highlighting the persistent difficulties in addressing gender bias in Chinese texts.

## Method Summary
The authors developed CORGI-PM through a multi-stage pipeline: first, an automatic filtering mechanism uses geometric bias in word embeddings to retrieve candidate sentences containing gender-biased words from large Chinese corpora; second, human annotators classify these sentences into three bias subtypes (AC: activities/careers, DI: descriptions/inductions, ANB: attitudes/norms/beliefs) and rewrite biased sentences into gender-neutral versions while preserving semantic meaning; third, the resulting corpus is used for three NLP tasks - binary detection of gender bias, multi-label classification of bias subtypes, and generative mitigation to produce neutral sentences. The corpus contains 32.9k sentences with 5.2k biased sentences paired with their human-rewritten neutral counterparts.

## Key Results
- Detection task achieves F1 scores up to 0.850, demonstrating reasonable performance in identifying gender-biased sentences
- Classification task performance declines considerably, with F1 scores dropping to 0.667 for AC subtype and 0.486 for DI subtype
- Mitigation task remains consistently challenging with BLEU scores below 0.30 across all submissions, indicating persistent difficulties in generating gender-neutral alternatives

## Why This Works (Mechanism)

### Mechanism 1: Geometric Filtering for Candidate Recall
The pipeline calculates a "seed direction" by subtracting embeddings for "she" and "he" to identify gender-biased vocabulary, then retrieves and re-ranks sentences containing these words before human review. This geometric approach creates a higher-yield candidate pool than random sampling by leveraging the assumption that gender is a specific direction in vector space.

### Mechanism 2: Semantic Invariance via Human Paraphrasing
Human annotators rewrite biased sentences using three patterns (pronoun replacement, adjective neutralization, context addition) to create parallel data where semantics are preserved but bias vectors are altered. This produces higher-quality bias mitigation data than automated gender-swapping, which often generates nonsensical sentences.

### Mechanism 3: Stratified Task Difficulty for Model Diagnostics
The shared task defines three distinct sub-tasks (detection, classification, mitigation) with increasing difficulty. The performance drop across these tasks serves as a diagnostic gradient, revealing that identification is significantly easier than generation (correction).

## Foundational Learning

- **Concept: Word Embedding Geometry (Bolukbasi et al.)**
  - Why needed: The filtering pipeline relies on the assumption that gender is a specific direction in vector space
  - Quick check: If you project the word "programmer" onto the vector difference (she - he), does it land closer to "he" or "she" in a biased corpus?

- **Concept: Semantic Invariance in NLG**
  - Why needed: The mitigation task requires rewriting sentences to remove bias without changing meaning
  - Quick check: If a model changes "She is a nurse" to "They are a nurse," does it preserve semantic invariance regarding the individual's job?

- **Concept: Multi-label Classification**
  - Why needed: The classification task is explicitly multi-label (AC, DI, ANB), meaning a single sentence can exhibit multiple bias types
  - Quick check: Can a standard Softmax output layer handle multi-label classification, or do you need Sigmoid + Binary Cross Entropy?

## Architecture Onboarding

- **Component map:** Raw Chinese Corpus (SlguSet, CCL) -> Word-level Bias Scoring (Vocab Construction) -> Sentence-level Reranking (Candidate Pool) -> Human labeling (Binary + Multi-label AC/DI/ANB) + Human Rewriting (Mitigation) -> Three task tracks (Detection, Classification, Mitigation)

- **Critical path:** The quality of the Mitigation Task depends entirely on the Parallel Subset Corpus (Original -> Human Corrected). If the human rewriting is inconsistent, the generative models will fail to converge on a "neutral" style.

- **Design tradeoffs:**
  - Recall vs. Precision: The filtering pipeline likely prioritizes high recall of potentially biased words to ensure comprehensive candidate pool, accepting lower precision corrected by human annotators
  - Annotator Expertise vs. Scalability: Authors select annotators with Bachelor's degrees and gender bias training to ensure quality, limiting speed and scalability compared to crowdsourced efforts

- **Failure signatures:**
  - Low BLEU / High Bias: A model that copies the input sentence exactly might achieve moderate lexical overlap but fails the mitigation task completely
  - High BLEU / Semantic Drift: A model that over-generalizes (e.g., "The woman" -> "The person") may score well on fluency but lose specific gender references necessary for context

- **First 3 experiments:**
  1. Run zero-shot prompting on a Chinese LLM (e.g., Qwen, Yi) to see if it can identify AC, DI, ANB categories without training
  2. Measure the "yield" rate: What percentage of sentences identified by the geometric filter are actually confirmed as biased by humans?
  3. Fine-tune a small seq2seq model on the parallel corpus and check if it simply learns to delete gender-related words rather than neutralizing them contextually

## Open Questions the Paper Calls Out

### Open Question 1
How can gender bias mitigation performance in Chinese be substantially improved beyond the current BLEU scores below 0.30? The leaderboard shows all teams achieving mitigation scores below 0.30, and the authors note persistent challenges in identifying and mitigating gender bias in Chinese corpora.

### Open Question 2
To what extent does the selection of annotators with higher education introduce cognitive bias into the corpus? The authors acknowledge they only choose annotators with higher education, which may lead to potential cognitive bias.

### Open Question 3
How does gender bias manifest differently across specific domains or disciplines beyond the general corpus? The authors state CORGI-PM mainly focuses on gender bias but has not explored bias across different domains or disciplines.

## Limitations
- The geometric filtering mechanism's efficacy is asserted but not quantitatively validated with yield rate metrics
- Human annotation relies heavily on annotator expertise and subjective interpretation of "neutrality," with moderate inter-annotator agreement
- Mitigation task's low BLEU scores are attributed to semantic preservation requirements, but evaluation doesn't directly measure whether bias is actually removed

## Confidence

- **High Confidence**: The corpus construction methodology (filtering + human annotation) is clearly specified and the task decomposition is logically sound
- **Medium Confidence**: The geometric filtering mechanism's effectiveness and the quality of human rewriting, as specific validation metrics are not provided
- **Medium Confidence**: The interpretation of low BLEU scores as evidence of task difficulty, since metrics may not fully capture bias removal

## Next Checks
1. Measure the precision of the geometric filtering pipeline by calculating what percentage of retrieved candidate sentences are confirmed as biased by human annotators
2. Conduct manual evaluation of mitigation outputs to verify whether low BLEU scores correspond to successful bias removal or failure to preserve meaning
3. Test whether the geometric filtering approach (she - he direction) works similarly for other languages with different gender pronoun systems or gender-neutral languages