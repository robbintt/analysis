---
ver: rpa2
title: 'LOTUS: A Leaderboard for Detailed Image Captioning from Quality to Societal
  Bias and User Preferences'
arxiv_id: '2507.19362'
source_url: https://arxiv.org/abs/2507.19362
tags:
- bias
- image
- caption
- captions
- user
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: LOTUS addresses the need for standardized evaluation of detailed
  image captions by large vision-language models. It provides a unified leaderboard
  that evaluates caption quality, hallucination risks, societal biases, and user preferences.
---

# LOTUS: A Leaderboard for Detailed Image Captioning from Quality to Societal Bias and User Preferences

## Quick Facts
- **arXiv ID:** 2507.19362
- **Source URL:** https://arxiv.org/abs/2507.19362
- **Reference count:** 16
- **Primary result:** Unified leaderboard evaluating detailed image captions across quality, hallucination, societal bias, and user preferences, revealing no single model excels across all criteria.

## Executive Summary
LOTUS provides a comprehensive evaluation framework for large vision-language models (LVLMs) that generates detailed image captions. The system unifies four distinct evaluation criteria—alignment, descriptiveness, language complexity, and side effects—into a single leaderboard with normalized scoring. Through experiments on recent LVLMs, LOTUS demonstrates that caption quality correlates with increased hallucination risks, and that descriptiveness correlates differently with various bias types. The framework also introduces preference-oriented evaluation, showing that optimal model selection depends on specific user priorities rather than absolute performance.

## Method Summary
LOTUS evaluates detailed image captions generated by 7B parameter LVLMs using four criteria: Alignment (CLIPScore, CapScore), Descriptiveness (CLIP recall@5, noun/verb coverage), Language complexity (syntactic depth, semantic nodes), and Side effects (CHAIRs, FaithScore, NSFW words). The system uses the COCO Karpathy test set (5,000 images) with ground-truth captions from Localized Narratives, plus gender and skin tone annotations for bias analysis. Models generate captions using the prompt "Describe this image in detail," and all metrics are computed and normalized to [0,1] ranges. The framework also introduces preference-oriented evaluation by combining criterion scores according to different user preference profiles.

## Key Results
- No single LVLM excels across all evaluation criteria; model selection depends on user priorities
- Highly descriptive models reduce gender bias but increase skin tone bias, revealing a trade-off
- Hallucination risks increase with caption detail, particularly for models using VCD decoding
- Preference-oriented evaluation shows optimal models vary significantly across different user types

## Why This Works (Mechanism)
LOTUS works by providing a unified evaluation framework that captures the multidimensional nature of image captioning quality. The system addresses the gap between traditional caption evaluation (which focuses on single metrics) and the complex requirements of real-world applications where users have diverse needs. By normalizing scores across different criteria and introducing preference-oriented combinations, LOTUS enables fair comparison of LVLMs on their ability to generate detailed, accurate, and bias-aware captions. The framework's strength lies in its comprehensive coverage of both technical quality and societal impact metrics, revealing trade-offs that single-metric evaluations miss.

## Foundational Learning

**Normalized Average (N-avg)** - Weighted combination of normalized criterion scores to enable fair comparison across models with different strengths. Needed because different evaluation metrics operate on different scales and importance weights. Quick check: Verify that all metrics are properly inverted for lower-is-better metrics before averaging.

**Preference-oriented evaluation** - Method of combining criterion scores according to specific user preference profiles rather than using fixed weights. Needed because different applications require different trade-offs between descriptiveness, hallucination risk, and bias mitigation. Quick check: Test with extreme preference profiles (e.g., hallucination-averse only) to ensure ranking changes appropriately.

**Demographic disparity analysis** - Measuring performance differences across demographic groups using binary gender and skin tone classifications. Needed to identify and quantify societal biases in caption generation. Quick check: Calculate performance gaps between demographic groups to verify significant disparities exist.

**Hallucination detection metrics** - CHAIRs and FaithScore measure factual consistency between generated captions and visual content. Needed because detailed captions are more prone to hallucinated content. Quick check: Compare hallucination rates between detailed and non-detailed caption variants.

## Architecture Onboarding

**Component map:** COCO dataset → LVLM caption generation → Metric computation (4 criteria) → Normalization → Leaderboard aggregation → Preference combination

**Critical path:** Image → LVLM generation → All metric computations → N-avg calculation → Model ranking. This path determines the final leaderboard positions and is the most computationally intensive step.

**Design tradeoffs:** The framework prioritizes comprehensive evaluation over computational efficiency, requiring multiple expensive metrics (GPT-4 API calls for CapScore and SideEffectSubscore). The use of binary demographic categories simplifies analysis but may miss intersectional biases.

**Failure signatures:** High CapScore but low FaithScore indicates fluent but hallucinated captions. High descriptiveness with high hallucination scores suggests detailed but inaccurate descriptions. Large demographic disparities indicate bias issues.

**Three first experiments:** 1) Run single model on small image subset to verify metric computation pipeline. 2) Compare normalized scores across two models to verify normalization works correctly. 3) Test preference-oriented evaluation with extreme profiles to verify ranking sensitivity.

## Open Questions the Paper Calls Out

**Open Question 1:** Does the divergent relationship between descriptiveness and bias types stem from training data distributions or inherent model architecture attention mechanisms? The paper identifies the linguistic correlation but doesn't determine if this trade-off is fundamental to LVLM architectures or specific to datasets like LAION and COCO. Ablation studies with balanced datasets versus attention mechanism modifications could resolve this.

**Open Question 2:** Do hallucination mitigation techniques generally degrade multilingual performance fairness in LVLMs? Appendix E.1 notes that applying VCD and OPERA to LLaVA-1.5 increases performance disparity among different languages. The paper observes this as a side effect but doesn't test whether this is specific to these decoding strategies or a general consequence of stricter visual grounding.

**Open Question 3:** To what extent do LLM-simulated preference scores correlate with diverse human user satisfaction in real-world applications? While the authors validate their preference-oriented evaluation using GPT-4o agents, the validity of using LLMs as proxies for complex human preferences remains an assumption. Human user studies would provide empirical validation.

**Open Question 4:** How does model performance evaluation change when moving from binary demographic annotations to intersectional or non-binary categories? The current leaderboard relies on coarse binary groupings (Man/Woman, Lighter/Darker), potentially masking bias amplification at attribute intersections. Re-evaluating with intersectional annotations could reveal different optimal models.

## Limitations
- Heavy reliance on GPT-4 API for multiple metrics creates potential English-language and Western cultural bias
- Binary gender and skin tone classifications may miss intersectional and non-binary representation biases
- 7B parameter model selection may not represent full LVLM capability spectrum
- CHAIRs metric assumes COCO object annotations are complete and representative

## Confidence

**High confidence:** Unified evaluation framework and normalization methodology are well-specified and reproducible. Correlation findings between caption detail and bias risks are statistically sound.

**Medium confidence:** Bias metrics and demographic analysis may underestimate real-world bias due to limited demographic categories and potential annotation biases.

**Low confidence:** Generalizability of preference-oriented evaluation results to real-world deployment scenarios, given lack of empirical user validation.

## Next Checks
1. **Cross-cultural validation:** Re-run full evaluation using GPT-4 API with prompts translated into multiple languages to assess potential English-language bias in current results.

2. **Intersectional bias analysis:** Extend demographic analysis beyond binary gender and skin tone by implementing intersectional bias metrics that consider combined demographic attributes, particularly for underrepresented groups.

3. **User preference validation:** Conduct small-scale user study (n=50-100) where participants rate actual captions generated by evaluated models on the same test set, comparing empirical preferences against predicted preference scores from the current framework.