---
ver: rpa2
title: "Reconnaissance Automatique des Langues des Signes : Une Approche Hybrid\xE9\
  e CNN-LSTM Bas\xE9e sur Mediapipe"
arxiv_id: '2510.22011'
source_url: https://arxiv.org/abs/2510.22011
tags:
- pour
- gestes
- avec
- signes
- mediapipe
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study proposes an automatic sign language recognition system
  based on a hybrid CNN-LSTM architecture using Mediapipe for gesture keypoint extraction.
  The system achieves an average accuracy of 92%, with excellent performance for distinct
  gestures such as "Hello" and "Thank you." However, some confusions remain for visually
  similar gestures like "Call" and "Yes." The methodology involves extracting 21 hand
  keypoints, 468 facial keypoints, and 33 body keypoints per frame using Mediapipe,
  then processing these through a CNN-LSTM hybrid model.
---

# Reconnaissance Automatique des Langues des Signes : Une Approche Hybridée CNN-LSTM Basée sur Mediapipe

## Quick Facts
- arXiv ID: 2510.22011
- Source URL: https://arxiv.org/abs/2510.22011
- Reference count: 20
- Primary result: CNN-LSTM hybrid model achieves 92% average accuracy for sign language gesture recognition using Mediapipe keypoint extraction

## Executive Summary
This paper proposes an automatic sign language recognition system that uses a hybrid CNN-LSTM architecture with Mediapipe for extracting hand, face, and body keypoints from video frames. The system processes 30-frame sequences of 522 normalized 3D keypoints through convolutional layers for spatial feature extraction followed by bidirectional LSTM layers for temporal modeling. The approach achieves 92% average accuracy with excellent performance on distinct gestures like "Hello" and "Thank you," though some confusion remains between visually similar gestures such as "Call" and "Yes." The implementation provides real-time translation capability with 45ms inference latency.

## Method Summary
The system extracts 21 hand keypoints (×2 hands), 468 facial keypoints, and 33 body keypoints per frame using Mediapipe, resulting in 522 normalized 3D coordinates. These coordinates are normalized relative to the right shoulder and scaled by shoulder width for invariance. A Kalman filter smooths temporal jitter. Each 30-frame sequence (shape 30×522×3) passes through four Conv2D layers (32→64→128→256 filters) for spatial feature extraction, then through two bidirectional LSTM layers (256 units each) for temporal modeling. The final dense layer with softmax produces class probabilities. Training uses Adam optimizer with learning rate 0.001, 100 epochs, early stopping, and extensive data augmentation including rotation, scaling, and temporal shifts.

## Key Results
- 92% average accuracy across tested gestures
- 89% recall and 90.5% F1-score
- 45ms inference latency suitable for real-time applications
- Excellent performance on distinct gestures (Hello, Thank you)
- Confusion remains between visually similar gestures (Call, Yes)

## Why This Works (Mechanism)

### Mechanism 1: Keypoint Representation
Mediapipe-based keypoint extraction provides a compact, pose-invariant representation that reduces the learning burden on downstream neural networks. Raw video frames are transformed into 522 normalized 3D coordinates, reducing input dimensionality from full images to ~1,566 values per frame while preserving articulation geometry. This works under the assumption that keypoint representation captures all information necessary to distinguish gesture classes.

### Mechanism 2: Spatial CNN Feature Extraction
The CNN component learns spatial configurations of keypoints corresponding to distinctive hand shapes and body poses. Four Conv2D layers with increasing filters extract local spatial patterns across keypoints, treating the structured keypoint grid as pseudo-image data. This assumes spatial relationships between keypoints at each timestep are discriminative and can be learned by Conv2D kernels.

### Mechanism 3: Temporal LSTM Modeling
Bidirectional LSTM layers model temporal dynamics, enabling gesture recognition based on motion trajectories rather than static poses. Two bidirectional LSTM layers with 256 units each capture both preceding and following context within 30-frame windows. This assumes gestures can be recognized from 30-frame windows and that bidirectional context within this window is sufficient.

## Foundational Learning

- **Pose/keypoint representation vs. raw pixels**: Essential because the architecture depends on understanding that Mediapipe outputs are ordered coordinate vectors, not images. Quick check: If you permute the order of the 522 keypoints in a consistent way across all frames, should the model still work? Why or why not?

- **Time-series classification with CNN-LSTM hybrids**: Critical for understanding why Conv2D is applied before LSTM (spatial vs. temporal abstraction). Quick check: Why not apply LSTM directly to raw keypoints without the CNN? What would the CNN provide that LSTM alone cannot?

- **Normalization for invariance**: Key to speaker-independent recognition despite different body sizes and positions. Quick check: If you train on normalized keypoints but test on unnormalized keypoints from a person 2× the size of training subjects, what will happen to accuracy?

## Architecture Onboarding

- **Component map**: Video Frame → Mediapipe → 522 keypoints (x,y,z) → Normalization → Kalman Filter → Stack 30 frames → Tensor (30, 522, 3) → Conv2D×4 + BatchNorm + MaxPool → Flatten → Reshape (30, 273) → Bidirectional LSTM×2 + Dropout → Dense(20) + Softmap → Gesture Class

- **Critical path**: Keypoint extraction quality, normalization correctness, LSTM sequence handling, real-time inference pipeline (45ms latency budget)

- **Design tradeoffs**: Keypoints vs. raw video (1500× data reduction but loses texture/occlusion), bidirectional LSTM vs. unidirectional (accuracy vs. real-time constraints), 30-frame window vs. variable length (simplification vs. truncation), CNN-LSTM vs. Transformer (speed vs. accuracy)

- **Failure signatures**: High confusion between specific gesture pairs (insufficient discriminative features), accuracy drops under poor lighting (Mediapipe detection degradation), inter-person generalization gap (normalization limitations), temporal drift in long sessions (Kalman filter tuning)

- **First 3 experiments**: 1) Ablate face keypoints to test contribution to accuracy, 2) Replace Conv2D with Graph Convolution to respect non-Euclidean structure, 3) Test causal LSTM inference latency for streaming applications

## Open Questions the Paper Calls Out

1. **Gesture disambiguation optimization**: How can the architecture be optimized to better disambiguate visually similar gestures like "Call" and "Yes"? This remains unresolved as the current feature extraction may lack granularity for subtle dynamic differences.

2. **Linguistic context integration**: Does integrating linguistic context significantly improve recognition accuracy for continuous sign language phrases? The current implementation focuses on isolated gesture recognition without sequential language modeling.

3. **Environmental robustness**: To what extent does system performance degrade under variable lighting conditions and environmental backgrounds? The reported 92% accuracy is likely from controlled data, leaving real-world robustness unquantified.

## Limitations

- **Dataset transparency**: Lacks detailed information about dataset size, class distribution, and availability, significantly impacting reproducibility
- **Cross-validation limitations**: 5-fold cross-validation shows 92% accuracy, but doesn't report performance across different signers, lighting conditions, or recording environments
- **Model comparison context**: While Transformers show higher accuracy, the paper doesn't explore why CNN-LSTM performs better for real-time applications

## Confidence

- **High Confidence**: CNN-LSTM architecture design and implementation details are well-specified and technically sound
- **Medium Confidence**: Claimed 92% accuracy is reasonable given methodology, but cannot be verified without dataset access
- **Low Confidence**: Claims about "excellent performance" and "confusions" lack quantitative comparison with baselines or established benchmarks

## Next Checks

1. **Ablation study on keypoint types**: Systematically remove face and body keypoints to quantify their contribution to accuracy and validate necessity of full 522-keypoint representation

2. **Cross-signer performance analysis**: Evaluate model on unseen signers to measure inter-person generalization and compare performance degradation against reported 92% accuracy

3. **Real-time inference benchmarking**: Measure actual end-to-end latency from video frame capture to gesture output under varying hardware conditions to validate 45ms claim and assess practical deployment feasibility