---
ver: rpa2
title: Goal-Driven Reward by Video Diffusion Models for Reinforcement Learning
arxiv_id: '2512.00961'
source_url: https://arxiv.org/abs/2512.00961
tags:
- uni000003ec
- reward
- video
- uni0000011e
- goal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes GenReward, a framework that leverages pretrained
  video diffusion models to provide goal-driven reward signals for reinforcement learning
  agents. The method generates task-specific videos using a fine-tuned video diffusion
  model, then extracts video-level rewards via similarity between agent trajectories
  and goal videos, and frame-level rewards via a learned forward-backward representation.
---

# Goal-Driven Reward by Video Diffusion Models for Reinforcement Learning

## Quick Facts
- arXiv ID: 2512.00961
- Source URL: https://arxiv.org/abs/2512.00961
- Reference count: 35
- Primary result: Achieves up to 504% improvement over dense rewards in Meta-World tasks using video diffusion model-generated rewards

## Executive Summary
This paper proposes GenReward, a framework that leverages pretrained video diffusion models to provide goal-driven reward signals for reinforcement learning agents. The method generates task-specific videos using a fine-tuned video diffusion model, then extracts video-level rewards via similarity between agent trajectories and goal videos, and frame-level rewards via a learned forward-backward representation. Experiments on Meta-World tasks demonstrate that GenReward significantly outperforms existing methods, achieving up to 504% improvement over dense rewards in tasks like Pick Out of Hole, and 822% in Bin Picking. The approach effectively transfers world knowledge from generated videos without requiring expert demonstrations.

## Method Summary
GenReward generates task-specific videos from text prompts using a fine-tuned video diffusion model, then extracts reward signals through two complementary pathways. The video-level reward measures cosine similarity between agent trajectories and the generated goal video, computed every 128 steps. The forward-backward reward uses a learned representation that predicts future states given current states and past trajectories, capturing task progress at the frame level. These rewards are combined with environment rewards using learned weights and used to train a DreamerV3 agent on Meta-World manipulation tasks. The framework requires finetuning a pretrained video diffusion model on domain-specific manipulation videos, generating goal videos for each task, and training a forward-backward network to provide dense frame-level rewards.

## Key Results
- Achieves 504% improvement over DreamerV3 dense reward baseline in Pick Out of Hole task
- Achieves 822% improvement over baseline in Bin Picking task
- Demonstrates robust performance across 5 Meta-World manipulation tasks with varying difficulty

## Why This Works (Mechanism)
GenReward works by transferring world knowledge from generated videos into reward signals that guide RL agents. The video diffusion model captures task-relevant dynamics and appearances from real manipulation videos, which are then distilled into both trajectory-level and frame-level rewards. The video-level reward provides high-level guidance by measuring progress toward the goal video, while the forward-backward representation captures detailed task progress at each step. This dual-reward approach provides both sparse, goal-aligned signals and dense, step-by-step feedback, enabling efficient learning without requiring expert demonstrations.

## Foundational Learning
- **Video diffusion models**: Generate task-specific videos from text prompts by learning to denoise video sequences. Needed because pretrained models lack task-specific manipulation knowledge. Quick check: Generate a sample video from a task prompt and verify it shows the correct manipulation pattern.

- **Forward-backward representations**: Learn representations that predict future states given current states and past trajectories. Needed to provide dense, step-by-step reward signals that capture task progress. Quick check: Train a small FB network and verify it can predict next-frame features from current-frame features and trajectories.

- **CLIP-based frame selection**: Uses text-image similarity to select the most relevant frame from generated videos as the goal state. Needed because generated videos contain multiple frames, but RL requires a single goal representation. Quick check: Run CLIP on generated video frames and verify the top-ranked frame visually matches the task goal.

- **Reward shaping with video similarity**: Computes cosine similarity between agent trajectories and goal videos as a reward signal. Needed to provide trajectory-level feedback that aligns with task completion. Quick check: Compute video-level rewards for random trajectories and verify they correlate with visual similarity to the goal video.

- **DreamerV3 with intrinsic rewards**: Trains actor-critic models using combined environment and generative rewards. Needed to leverage the custom reward signals within a proven RL framework. Quick check: Run DreamerV3 with only environment rewards and verify it learns basic behaviors before adding generative rewards.

## Architecture Onboarding
**Component map**: Text prompt -> Video diffusion model -> Generated goal video -> CLIP frame selection -> DINOv3 encoding -> 3D VAE encoding -> Video-level reward; Generated video -> Forward-Backward network -> Frame-level reward; Agent trajectory -> Video-level reward + Frame-level reward + Environment reward -> DreamerV3

**Critical path**: Text prompt → Video generation → Goal frame selection → Reward computation → RL training. The forward-backward network and video-level reward computation must be efficient enough to not bottleneck training.

**Design tradeoffs**: Using both video-level and frame-level rewards provides complementary signals but increases computational overhead. The framework trades offline video generation time for improved online training efficiency.

**Failure signatures**: 
- CLIP selects frames that don't match task goals (Supplementary Fig A)
- Forward-backward reward dominates or vanishes due to scaling issues
- Generated videos lack task-relevant dynamics, producing uninformative rewards

**First experiments**:
1. Verify goal frame selection: Run CLIP on generated videos for each task and visualize top-5 frames to confirm semantic alignment
2. Test reward decomposition: Log video-level and forward-backward rewards separately during training to ensure proper scaling
3. Validate video generation quality: Generate sample videos for each task and verify they show correct manipulation patterns

## Open Questions the Paper Calls Out
- Can the computational overhead of computing both video-level and forward-backward rewards during training be reduced without sacrificing policy performance?
- How can goal-frame selection be improved to reliably identify the most task-relevant frame when CLIP similarity produces suboptimal choices?
- How robust is GenReward when generated goal videos depict behaviors or dynamics that differ significantly from the target environment?
- What is the theoretical relationship between the quality of the generated goal video and the resulting reward signal's ability to guide policy optimization?

## Limitations
- Requires significant computational resources for video diffusion model finetuning and inference
- Computational overhead from computing both video-level and forward-backward rewards during training
- Performance depends on quality of generated videos and their alignment with target environment dynamics

## Confidence
- **High** confidence the method improves over dense rewards if diffusion model is properly finetuned
- **Medium** confidence in reproducing exact performance numbers without pretrained models
- **Low** confidence in matching absolute numbers without access to finetuned video diffusion checkpoints

## Next Checks
1. Verify goal frame selection: Visualize top-5 frames from OpenCLIP scores for each task and confirm semantic alignment with task goals
2. Validate reward decomposition: Log α·r_video and β·r_FB separately during training; if one exceeds the other by >100x, recalibrate scaling
3. Test finetuning robustness: Train CogVideoX with varying dataset sizes and step counts; confirm generated goal videos retain task-relevant motion patterns across variations