---
ver: rpa2
title: 'SceneGen: Single-Image 3D Scene Generation in One Feedforward Pass'
arxiv_id: '2508.15769'
source_url: https://arxiv.org/abs/2508.15769
tags:
- scene
- assets
- generation
- scenegen
- asset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the challenging task of generating multiple
  3D assets with geometry, texture, and spatial arrangement from a single scene image.
  The authors propose SceneGen, a novel framework that operates in a single feedforward
  pass without requiring extra optimization or asset retrieval.
---

# SceneGen: Single-Image 3D Scene Generation in One Feedforward Pass

## Quick Facts
- arXiv ID: 2508.15769
- Source URL: https://arxiv.org/abs/2508.15769
- Authors: Yanxu Meng; Haoning Wu; Ya Zhang; Weidi Xie
- Reference count: 40
- Generates textured 3D scenes with four assets in ~2 minutes on A100 GPU

## Executive Summary
This paper introduces SceneGen, a novel framework for generating multiple 3D assets with geometry, texture, and spatial arrangement from a single scene image in a single feedforward pass. The method addresses limitations of previous approaches that rely on extra optimization or asset retrieval. SceneGen achieves superior performance on the 3D-FUTURE dataset, demonstrating significant improvements in geometric and visual quality metrics compared to baselines like MIDI and PartCrafter.

## Method Summary
SceneGen operates through a unified architecture that integrates visual and geometric information from a single scene image. The framework employs a feature aggregation module that combines local and global scene information, along with a position head that generates both 3D assets and their relative spatial positions. The method is designed to work in a single feedforward pass without requiring additional optimization steps or asset database retrieval. Notably, the approach can be directly extended to multi-image input scenarios, which further improves generation quality.

## Key Results
- Achieves F-Score of 90.60 and PSNR of 19.59 on 3D-FUTURE dataset
- Generates textured scenes with four assets in approximately 2 minutes on single A100 GPU
- Significantly outperforms previous methods like MIDI and PartCrafter in both geometric and visual quality metrics

## Why This Works (Mechanism)
The framework's effectiveness stems from its unified architecture that simultaneously processes visual and geometric information. The feature aggregation module enables comprehensive scene understanding by integrating both local details and global context, while the position head ensures coherent spatial arrangement of generated assets. The single-feedforward design eliminates the computational overhead of iterative optimization or asset retrieval, enabling efficient generation.

## Foundational Learning

**3D Asset Generation** - Creating complete 3D objects with geometry and texture from visual input. Needed for building realistic virtual environments from real-world imagery.

**Feature Aggregation** - Combining information from multiple sources or scales to form comprehensive representations. Essential for capturing both fine-grained details and overall scene structure.

**Position Encoding** - Representing spatial relationships and coordinates in a format suitable for neural networks. Critical for maintaining proper relative positioning of multiple generated assets.

**Feedforward Architecture** - Processing inputs through a network in a single pass without iterative refinement. Enables real-time or near-real-time generation capabilities.

**Multi-Modal Fusion** - Integrating information from different input modalities (visual, geometric). Necessary for comprehensive scene understanding and generation.

**Asset Retrieval** - Searching databases for pre-existing 3D models. Traditional approach that SceneGen aims to eliminate for greater flexibility and speed.

## Architecture Onboarding

**Component Map**: Image Encoder -> Visual Feature Extractor -> Feature Aggregation Module -> Position Head -> 3D Asset Generator

**Critical Path**: The feature aggregation module represents the core innovation, as it integrates local and global scene information from both visual and geometric encoders to produce comprehensive scene representations that drive the generation process.

**Design Tradeoffs**: The single-feedforward approach sacrifices the iterative refinement capabilities of optimization-based methods in exchange for speed and simplicity. The method trades database dependency for on-the-fly generation, potentially reducing quality consistency but enabling greater flexibility.

**Failure Signatures**: The method may struggle with highly complex or cluttered scenes, scenes with objects significantly different from training data, and scenarios requiring precise semantic understanding beyond geometric and visual cues.

**First Experiments**:
1. Test SceneGen on 3D-FUTURE dataset with quantitative evaluation using F-Score and PSNR metrics
2. Compare generation quality against MIDI and PartCrafter baselines on identical test scenes
3. Evaluate multi-image extension by providing SceneGen with 2-4 input images of the same scene

## Open Questions the Paper Calls Out
None

## Limitations
- Performance generalizability remains untested on diverse real-world scenes beyond 3D-FUTURE dataset
- 2-minute generation time represents significant computational cost for practical deployment
- Single-feedforward characterization may overstate simplicity given multiple encoder networks and complex multi-head architecture

## Confidence

**Major Claim Confidence Labels:**
- **High Confidence**: Technical implementation details and architectural contributions are well-documented and reproducible
- **Medium Confidence**: Performance claims on 3D-FUTURE are supported by specific metrics, but limited to single dataset
- **Low Confidence**: Claims about multi-image extensibility lack detailed validation, and single-feedforward characterization may be misleading

## Next Checks

1. Test SceneGen on diverse datasets beyond 3D-FUTURE (e.g., ScanNet, Replica) to assess cross-dataset generalization and robustness to scene diversity

2. Conduct ablation studies isolating the feature aggregation module and position head contributions to quantify their individual impact on performance gains

3. Benchmark SceneGen against state-of-the-art text-to-3D methods using the same 3D-FUTURE scenes but with text prompts instead of single images to establish fair comparison conditions