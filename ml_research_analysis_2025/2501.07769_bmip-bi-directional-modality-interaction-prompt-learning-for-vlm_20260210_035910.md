---
ver: rpa2
title: 'BMIP: Bi-directional Modality Interaction Prompt Learning for VLM'
arxiv_id: '2501.07769'
source_url: https://arxiv.org/abs/2501.07769
tags:
- prompt
- learning
- vision
- bmip
- prompts
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of adapting vision-language
  models (VLMs) to downstream tasks using prompt learning, specifically focusing on
  the limitations of single-modal prompts or uni-directional modality interaction
  that overlook the powerful alignment effects resulting from the interaction between
  vision and language modalities. The authors propose a novel prompt learning method
  called Bi-directional Modality Interaction Prompt (BMIP) that dynamically weights
  bi-modal information through learning the information of the attention layer, enhancing
  trainability and inter-modal consistency compared to simple information aggregation
  methods.
---

# BMIP: Bi-directional Modality Interaction Prompt Learning for VLM

## Quick Facts
- **arXiv ID**: 2501.07769
- **Source URL**: https://arxiv.org/abs/2501.07769
- **Reference count**: 21
- **Key outcome**: BMIP achieves an absolute average performance gain of 0.82% over MaPLe across 15 benchmarks, outperforming current state-of-the-art methods.

## Executive Summary
This paper addresses the challenge of adapting vision-language models (VLMs) to downstream tasks using prompt learning, specifically focusing on the limitations of single-modal prompts or uni-directional modality interaction that overlook the powerful alignment effects resulting from the interaction between vision and language modalities. The authors propose a novel prompt learning method called Bi-directional Modality Interaction Prompt (BMIP) that dynamically weights bi-modal information through learning the information of the attention layer, enhancing trainability and inter-modal consistency compared to simple information aggregation methods. Comprehensive experiments on 15 benchmarks demonstrate that BMIP outperforms current state-of-the-art methods across all three evaluation paradigms: open-world generalization, cross-dataset transfer, and domain generalization.

## Method Summary
BMIP is a prompt learning method for adapting pre-trained CLIP to downstream image classification tasks. It introduces learnable deep vision and language prompts for the first J layers of the respective encoders. The key innovation is a bi-directional interaction mechanism: projection heads transform prompts from one modality to be compatible with the other, and these projected features are integrated using a learnable aggregation function that dynamically weights original and projected information based on attention layer outputs. This allows text prompts to be informed by visual features and vice-versa, addressing the limitations of uni-directional methods. The encoders remain frozen during training, and only the prompts and projection heads are updated.

## Key Results
- BMIP achieves an absolute average performance gain of 0.82% over the previous state-of-the-art method MaPLe across 15 benchmarks
- Notable improvements on datasets with imbalanced text and image information such as EuroSAT and Flowers102
- Flexible enough to be combined with other prompt-based methods for consistent performance enhancement
- Outperforms current state-of-the-art methods across all three evaluation paradigms: open-world generalization, cross-dataset transfer, and domain generalization

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Dynamically weighting bi-modal information using attention layer outputs improves trainability and inter-modal consistency compared to simple aggregation methods.
- **Mechanism**: The method trains modality-specific 1x1 linear layers ($L_l, L_v$) to map attention weights ($A_l, A_v$) from the current layer's output to substitution weights ($w_l, w_v$). These weights are used to combine original prompts with projected cross-modal information (e.g., $\tilde{P}'_i = w_v * \tilde{P}_i + (1 - w_v) * F_v(P_i)$). This allows the model to emphasize prompts that the attention mechanism deems important for the current context.
- **Core assumption**: The output weights of an attention layer correlate with the importance of the corresponding prompt for downstream tasks. Simply adding or concatenating cross-modal information is assumed to be less effective than a learnable, weighted substitution.
- **Evidence anchors**:
  - [abstract] "...dynamically weights bi-modal information through learning the information of the attention layer, enhancing trainability and inter-modal consistency compared to simple information aggregation methods."
  - [Section 4.3] "...trained modality-specific 1 × 1 linear layers, Ll and Lv, to learn the relationship between attention weights and substitution weights (wl, wv)..."
  - [corpus] Weak/missing. No direct corpus papers validate this specific attention-weight-to-prompt-weight mapping mechanism.
- **Break condition**: If attention weights do not correlate with prompt importance, or if the linear layers fail to learn a beneficial mapping, the dynamic weighting could introduce noise, reducing performance below that of simple averaging.

### Mechanism 2
- **Claim**: Facilitating bi-directional interaction (vision-to-language and language-to-vision) between prompts improves alignment and addresses the shortcomings of single-modal or uni-directional methods.
- **Mechanism**: Independent sets of deep vision and language prompts are learned. Projection heads ($F_v, F_l$) transform prompts from one modality to be compatible with the other. This transformed information is integrated into the target modality's prompts using the dynamic weighting from Mechanism 1. This allows text prompts to be informed by visual features and vice-versa.
- **Core assumption**: Vision and language prompts in VLMs are sufficiently interchangeable that information projected from one can augment the other without distorting its fundamental representation.
- **Evidence anchors**:
  - [abstract] "...overlooking the powerful alignment effects resulting from the interaction between the vision and language modalities."
  - [Section 1] "Many studies... reported that language prompt learning underperforms on datasets with high intra-class visual variances, while vision prompt learning struggles on datasets with small inter-class textual variances."
  - [corpus] Weak. Corpus papers discuss bi-directional feature restoration or dual-modality tuning but do not directly confirm this specific prompt-level exchange.
- **Break condition**: If projected information is not semantically aligned with the target modality's space, it could confuse the encoder, leading to performance worse than uni-directional methods.

### Mechanism 3
- **Claim**: Introducing layered (deep) prompts expands the scope of prompt information and curtails overfitting compared to shallow prompting.
- **Mechanism**: Prompts are introduced in the first J layers of both the vision and language encoders. After the J-th layer, the output of the previous layer is used directly without new prompts. This allows prompt influence to propagate deeper into the network.
- **Core assumption**: A depth of J is optimal; shallower prompts miss information, while deeper application of prompts (or full fine-tuning) leads to overfitting.
- **Evidence anchors**:
  - [Section 4] "(2) prompts of suitable depth can expand the scope of prompt information and curtail overfitting;"
  - [Section 5.6] "We also assess the impact of varying prompt depths and lengths in Appendices D and E, which proves our intuition about prompt depth."
  - [corpus] N/A.
- **Break condition**: If the chosen depth J is too deep, the model may overfit to the downstream task. If J is too shallow, it may fail to capture necessary adaptations.

## Foundational Learning

- **Concept: Vision-Language Models (VLMs) & CLIP**
  - **Why needed here**: BMIP is built on top of the CLIP architecture, so understanding its dual-encoder structure (image encoder *f*, text encoder *g*) and zero-shot classification via cosine similarity is essential.
  - **Quick check question**: Can you explain how CLIP computes the probability $p(y=i|I)$ for an image *I* belonging to class *i*?

- **Concept: Prompt Learning**
  - **Why needed here**: The core innovation is learning continuous prompt vectors instead of discrete text or hand-designed templates. Understanding how learnable prompts $P$ replace hand-designed ones is key.
  - **Quick check question**: In the context of CoOp or this paper, what does the learnable prompt vector $P$ replace in the standard CLIP classification pipeline?

- **Concept: Modality Interaction & Alignment**
  - **Why needed here**: The paper's central thesis is that uni-directional interaction is insufficient. Understanding what "alignment" means in a joint vision-language embedding space is crucial.
  - **Quick check question**: Why might a model with only language prompt learning struggle on a dataset like EuroSAT, which has high intra-class visual variance?

## Architecture Onboarding

- **Component map**: Image Input -> Vision Encoder Layers (1..J) -> Apply Aggregation Function with Vision Prompts $\tilde{P}$ and Projected Language Prompts $F_l(P)$ -> Final Vision Feature *x*. Text Input -> Text Encoder Layers (1..J) -> Apply Aggregation Function with Language Prompts $P$ and Projected Vision Prompts $F_v(\tilde{P})$ -> Final Text Feature *z*. Final prediction via similarity between *x* and *z*.

- **Critical path**: Image Input -> Vision Encoder Layers (1..J) -> Apply Aggregation Function with Vision Prompts $\tilde{P}$ and Projected Language Prompts $F_l(P)$ -> Final Vision Feature *x*. Text Input -> Text Encoder Layers (1..J) -> Apply Aggregation Function with Language Prompts $P$ and Projected Vision Prompts $F_v(\tilde{P})$ -> Final Text Feature *z*. Final prediction via similarity between *x* and *z*.

- **Design tradeoffs**:
  - **Depth (J) vs. Overfitting**: Increasing J increases model capacity but risks overfitting.
  - **Complexity vs. Simplicity**: The aggregation function is more complex than simple addition or concatenation. This adds parameters and potential failure points but is claimed to yield better performance.
  - **Parameter Count**: The method introduces more parameters than simpler prompt methods. Ablation studies show it maintains robustness despite this increase.

- **Failure signatures**:
  - **Performance Degradation on Balanced Datasets**: If dynamic weighting fails to find a beneficial balance, performance might drop on datasets where single-modal prompting already works well.
  - **High Variance in Results**: High standard deviation across runs could indicate instability in the learning process of the dynamic weights.
  - **Worse Performance than MaPLe**: This would suggest the bi-directional interaction introduces more noise than signal.

- **First 3 experiments**:
  1.  **Baseline Comparison (Open-World Generalization)**: Implement BMIP and compare its average accuracy and harmonic mean (HM) against CLIP, CoOp, CoCoOp, and MaPLe on the 11 benchmark datasets.
  2.  **Ablation Study of Aggregation Function**: Replace the proposed learnable aggregation function with simpler alternatives (Addition, Attention, Joint) and measure the performance drop on the open-world generalization task.
  3.  **Analysis of Attention-Weight Mapping**: Inspect the learned weights ($w_l, w_v$) produced by the linear layers ($L_l, L_v$). Analyze their correlation with the raw attention weights and their impact on different datasets (e.g., does it weight visual information higher on EuroSAT?).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the BMIP method perform when applied to non-Transformer backbones (e.g., CNNs) within VLMs, given its specific reliance on attention layer outputs for generating dynamic weights?
- Basis in paper: [inferred] The method explicitly extracts "vision and language attention weight ($w_v, w_l$) from the output of the current attention layer" (Section 4.3, Equations 10-11) to compute substitution weights, and all experiments are restricted to the ViT-B/16 architecture (Section 5.1).
- Why unresolved: CNN-based VLM encoders (e.g., ResNets) do not possess the same explicit "attention layer" mechanism as Vision Transformers, potentially limiting the proposed aggregation function's direct applicability to a significant portion of VLM architectures.
- What evidence would resolve it: Experimental results applying BMIP to CLIP models with ResNet backbones, potentially substituting attention weights with other feature importance metrics (e.g., Grad-CAM or global pooling), compared against the ViT baseline.

### Open Question 2
- Question: Can the BMIP framework be effectively extended to dense prediction tasks (e.g., semantic segmentation or object detection) where pixel-level or region-level alignment is required?
- Basis in paper: [inferred] The paper evaluates performance exclusively on image classification benchmarks (Section 5.1) and utilizes a projection head (ImageProj) to map the final image feature (CLS token) to a common embedding space (Section 4.2), discarding spatial information.
- Why unresolved: The current architecture focuses on global alignment via class tokens. It is unclear if the bi-directional interaction at the prompt level can translate to dense outputs without mechanisms to handle spatial feature maps.
- What evidence would resolve it: Adapting the BMIP interaction mechanism to operate on spatial feature maps (rather than just prompts and CLS tokens) and evaluating on dense prediction datasets like COCO or ADE20K.

### Open Question 3
- Question: What is the computational and memory overhead of the learnable aggregation function compared to the baseline MaPLe, particularly regarding inference latency?
- Basis in paper: [inferred] BMIP introduces additional components including a language/vision projection head ($F_v, F_l$) and modality-specific linear layers ($L_v, L_l$) to dynamically weight prompts (Section 4.3), whereas MaPLe uses a simpler coupling function.
- Why unresolved: While Table 5 compares parameter counts with MaPLe†, the paper does not provide an analysis of the FLOPs or inference speed impact caused by computing attention weights and passing them through the extra learnable layers at every layer.
- What evidence would resolve it: A detailed complexity analysis reporting inference time (ms/image) and GFLOPs for BMIP versus MaPLe and simple aggregation baselines on the same hardware.

### Open Question 4
- Question: Does the bi-directional substitution mechanism risk degrading pre-trained knowledge if the learned dynamic weights ($w$) collapse to extreme values (0 or 1) too early in training?
- Basis in paper: [inferred] Section 4.4 notes that when attention weight is close to zero, the prompt becomes redundant, and proposes a Corollary that "prompt combining will only decrease the training loss." However, it does not analyze if the learned weights effectively ignore the cross-modal information in favor of the original modality.
- Why unresolved: If the linear layers learn to set $w \approx 1$, the cross-modal interaction is effectively disabled, potentially reverting the model to a behavior similar to independent single-modal prompts (IVLP).
- What evidence would resolve it: Visualization of the distribution of learned weights ($w_v, w_l$) across different layers and datasets to confirm that the model maintains active bi-directional interaction throughout the training process.

## Limitations
- The paper lacks detailed implementation specifics (hyperparameters, prompt dimensions, projection head architectures) referenced to "Appendix A" but not provided.
- The mechanism claiming that attention weights correlate with prompt importance is asserted but not directly validated through ablation or analysis.
- The projection heads Fv and Fl are presented as crucial but their architecture and initialization are unspecified.

## Confidence
- **High**: The overall experimental results demonstrating performance improvements over baselines.
- **Medium**: The general effectiveness of bi-directional modality interaction.
- **Low**: The specific mechanism of attention-weight-to-prompt-weight mapping and its claimed superiority over simple aggregation.

## Next Checks
1. Implement the ablation study replacing the learnable aggregation function with simple addition/concatenation to quantify the claimed improvement from dynamic weighting.
2. Analyze the learned weights (wl, wv) from the linear layers (Ll, Lv) to verify they correlate with attention weights and show different patterns across datasets with varying modality balance.
3. Measure base vs. new class accuracy gaps during training to detect overfitting, particularly when increasing prompt depth J or using larger projection heads.