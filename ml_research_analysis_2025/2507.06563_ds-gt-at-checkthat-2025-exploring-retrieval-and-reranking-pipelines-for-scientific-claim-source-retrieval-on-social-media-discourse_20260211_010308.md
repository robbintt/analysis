---
ver: rpa2
title: 'DS@GT at CheckThat! 2025: Exploring Retrieval and Reranking Pipelines for
  Scientific Claim Source Retrieval on Social Media Discourse'
arxiv_id: '2507.06563'
source_url: https://arxiv.org/abs/2507.06563
tags:
- retrieval
- data
- reranking
- dataset
- tweet
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: DS@GT team participated in CLEF 2025 CheckThat! Lab Task 4b, aiming
  to retrieve relevant scientific papers for implicit scientific claims in tweets.
---

# DS@GT at CheckThat! 2025: Exploring Retrieval and Reranking Pipelines for Scientific Claim Source Retrieval on Social Media Discourse

## Quick Facts
- arXiv ID: 2507.06563
- Source URL: https://arxiv.org/abs/2507.06563
- Reference count: 29
- Primary result: Achieved MRR@5 of 0.58 using BM25-PyTorch retrieval and T5 reranking, ranking 16th out of 30 teams

## Executive Summary
DS@GT team participated in CLEF 2025 CheckThat! Lab Task 4b, focusing on retrieving relevant scientific papers for implicit scientific claims in tweets. The team explored six data augmentation techniques, seven retrieval and reranking pipelines, and bi-encoder finetuning approaches. Using a two-stage pipeline with BM25-PyTorch for retrieval and T5 for reranking, they achieved an MRR@5 of 0.58, improving the BM25 baseline of 0.43 by 0.15. Data augmentation experiments showed that combining original tweets with formal rewrites slightly improved performance. The team also found that rerankers trained on MS Marco significantly outperformed those trained on GooAQ. While bi-encoder finetuning did not improve results, suggesting the need for better data filtering, the team has made their code publicly available for future work.

## Method Summary
The team implemented a two-stage retrieve-then-rerank pipeline for scientific claim source retrieval. First, BM25-PyTorch retrieved the top 100 candidate documents from the CORD-19 collection based on lexical matching. Then, a cross-encoder (specifically Google's T5 model via the AnswerDotAI rerankers library) jointly encoded each query-document pair to rerank the candidates. The team experimented with six data augmentation techniques, primarily combining original tweets with formal language rewrites generated by an LLM. They also tested seven different retrieval and reranking pipeline configurations, comparing MS Marco-trained and GooAQ-trained rerankers. The final pipeline used BM25 retrieval followed by T5 reranking on queries augmented with their formal rewrites.

## Key Results
- Achieved MRR@5 of 0.58 using BM25-PyTorch retrieval and T5 reranking
- Improved baseline BM25 MRR@5 of 0.43 by 0.15
- Ranked 16th out of 30 teams in CheckThat! 2025 Task 4b
- MS Marco-trained rerankers (MRR@5 0.6474) significantly outperformed GooAQ-trained models (MRR@5 0.2437)
- Combining original tweets with formal rewrites improved MRR@5 from 0.5521 to 0.5823 after reranking

## Why This Works (Mechanism)

### Mechanism 1
- Claim: A two-stage retrieve-then-rerank pipeline improves scientific source retrieval over single-stage retrieval.
- Mechanism: First, BM25 (a sparse lexical retriever) efficiently identifies a candidate set of 100 documents. Then, a cross-encoder (T5 or ms-marco-MiniLM-L-6-v2) jointly encodes the query and each candidate, capturing deeper semantic interactions to re-order the list. The final MRR@5 is a function of both stages.
- Core assumption: The correct document exists within the top-100 candidates from the first stage, and the reranker is sufficiently discriminative to promote it to the top-5.
- Evidence anchors:
  - [abstract]: "Using BM25-PyTorch for retrieval and T5 for reranking, they achieved an MRR@5 of 0.58, improving the BM25 baseline of 0.43 by 0.15..."
  - [section 4.2]: "In our first pipeline, a cross-encoder trained on the MS Marco dataset ... was used for reranking ... In all pipelines created, 100 documents were retrieved."
  - [corpus]: Weakly supported. One neighbor (AIRwaves at CheckThat! 2025) also uses a two-stage dual-encoder and neural re-ranking pipeline for the same task, suggesting it is a common effective pattern, but no direct comparative evidence is cited.
- Break condition: The pipeline fails if BM25 retrieves fewer than 100 relevant documents (poor recall@100) or if the reranker is not trained on data similar enough to the target domain (as seen with GooAQ models).

### Mechanism 2
- Claim: Combining informal social media text with a formal language rewrite slightly improves retrieval relevance.
- Mechanism: Query expansion. Concatenating the original tweet with its formal rewrite ("Concat w/ Formal") adds keywords and structured language that better match scientific abstracts, increasing term overlap without discarding the original's context.
- Core assumption: The LLM-generated formal rewrite is factually faithful to the original tweet and the original tweet contains useful context beyond scientific keywords.
- Evidence anchors:
  - [abstract]: "Data augmentation experiments showed that combining original tweets with formal rewrites slightly improved performance."
  - [section 5.1, Table 2]: "Concat w/ Formal" improved MRR@5 after reranking from 0.5521 (baseline) to 0.5823.
  - [corpus]: Not addressed in provided neighbors.
- Break condition: The approach fails if the rewrite introduces hallucinations, removes critical context, or if the original query is already verbose and adding text adds noise.

### Mechanism 3
- Claim: Rerankers trained on MS MARCO (human-answered questions) outperform those trained on GooAQ (automatically extracted QA) for this specific scientific retrieval task.
- Mechanism: Transfer learning from a source domain (MS MARCO's human QA pairs) that has a distribution more similar to the target task (matching tweets to human-written scientific abstracts) than the source domain of GooAQ. The model learns a relevance function that generalizes better.
- Core assumption: The semantic similarity and answer style in MS MARCO is more analogous to the CORD-19 abstracts and tweet queries than GooAQ's style.
- Evidence anchors:
  - [abstract]: "Reranking with MS Marco-trained models significantly outperformed GooAQ-trained models."
  - [section 5.2, Table 3]: "ms-marco-MiniLM-L-6-v2" achieved MRR@5 of 0.6474, while "reranker-ModernBERT-base-gooaq-bce" only reached 0.2437.
  - [corpus]: Not addressed in provided neighbors.
- Break condition: This mechanism fails if applied to a target domain whose language or query structure is fundamentally different from MS MARCO, rendering the transferred relevance function inaccurate.

## Foundational Learning

- **Concept: BM25 (Best Match 25)**
  - Why needed here: It serves as the efficient first-stage retriever. Understanding that it uses term frequency and inverse document frequency to score lexical matches is critical for debugging why certain documents are retrieved.
  - Quick check question: If a scientific paper abstract does not contain any words from a tweet, can BM25 retrieve it as a candidate?

- **Concept: Cross-Encoder vs. Bi-Encoder**
  - Why needed here: The paper's pipeline relies on the speed of a bi-encoder or BM25 for retrieval and the precision of a cross-encoder for reranking. Understanding this trade-off is fundamental.
  - Quick check question: Which model processes the query and document together, allowing for cross-attention between them, and is therefore slower but more accurate?

- **Concept: Mean Reciprocal Rank (MRR)**
  - Why needed here: This is the primary evaluation metric. MRR@5 measures the system's ability to place the single correct answer high in the top 5 results.
  - Quick check question: If the correct document is at rank 4 for one query and not found in the top 5 for another query, what is the average MRR@5 for these two queries?

## Architecture Onboarding

- **Component map**: Tweet Query -> Query Processor (optional augmentation) -> Retriever (BM25) -> Top-100 Candidates -> Reranker (T5) -> Final Ranked List -> MRR@5 Evaluation
- **Critical path**: A query tweet is optionally augmented, then passed to the Retriever to get top-100 candidates. These 100 document-query pairs are scored by the Reranker, and the final ranked list is used to calculate MRR@5.
- **Design tradeoffs**: Augmenting queries can improve recall but risks losing original context (if replacing text) or adding noise. Bi-encoders are faster but less precise than cross-encoders; thus, they are used for the broad search, while cross-encoders refine the results.
- **Failure signatures**:
  - Poor retrieval results (e.g., MRR < 0.2) suggest the retriever's candidate set is missing the correct document.
  - Reranking performance worse than retrieval (e.g., GooAQ models) indicates a domain mismatch in the reranker's training data.
  - Finetuning causing performance drops suggests noisy or insufficient training data for the target domain.
- **First 3 experiments**:
  1. **Baseline Retriever & Reranker**: Implement a standard BM25 retriever and an MS MARCO-trained cross-encoder reranker. Measure MRR@5 to establish a baseline as per section 4.2.
  2. **Query Augmentation Impact**: Run the pipeline with "Concat w/ Formal" augmentation (using a formal language prompt) vs. no augmentation. Compare MRR@5 to quantify the benefit as per Table 2.
  3. **Reranker Ablation**: Keep BM25 retrieval fixed and swap between an MS MARCO-trained reranker and a GooAQ-trained reranker. Measure the difference in MRR@5 to confirm the domain transfer hypothesis as per Table 3.

## Open Questions the Paper Calls Out
None

## Limitations
- **Data augmentation scope**: The paper tested query augmentation only, leaving document collection augmentation unexplored, creating uncertainty about potential improvements from expanding both query and corpus.
- **Bi-encoder finetuning failure**: The team attempted bi-encoder finetuning but observed performance degradation, attributing this to noisy training data without validating through error analysis or data quality assessment.
- **Reproducibility constraints**: The exact stop word list and T5 model checkpoint are unspecified, creating potential for performance variance in reproduction attempts. The paper also doesn't report standard deviations or statistical significance testing for its MRR improvements.

## Confidence

- **High**: The two-stage retrieve-then-rerank pipeline mechanism (BM25 + T5) achieving MRR@5 of 0.58 is well-supported by explicit experimental results and ablation studies.
- **Medium**: The claim about MS MARCO-trained rerankers outperforming GooAQ-trained models is supported by results but lacks theoretical justification for why this domain transfer works better.
- **Low**: The data augmentation improvement from combining original tweets with formal rewrites is only "slight" and based on a single experiment without ablation studies on the formal rewrite generation process.

## Next Checks

1. **Data quality audit**: Perform manual inspection of 50 randomly sampled training pairs to identify noise patterns that might explain the bi-encoder finetuning failure, then filter accordingly and re-run finetuning.
2. **Cross-validation on MRR@5**: Implement k-fold cross-validation (k=5) on the 1,400 dev set to establish confidence intervals for the reported MRR@5 of 0.58 and verify statistical significance of improvements over baseline.
3. **Document augmentation experiment**: Apply the formal language rewrite technique to both queries and document abstracts, then compare MRR@5 against the original pipeline to quantify potential improvements from corpus augmentation.