---
ver: rpa2
title: 'CPGD: Toward Stable Rule-based Reinforcement Learning for Language Models'
arxiv_id: '2505.12504'
source_url: https://arxiv.org/abs/2505.12504
tags:
- policy
- training
- cpgd
- drift
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses training instability in rule-based reinforcement
  learning (RL) for language models, where existing methods like GRPO and RLOO can
  suffer from policy collapse due to excessive importance-sampling ratios. The authors
  propose CPGD (Clipped Policy Gradient Optimization with Policy Drift), which replaces
  the PPO-clip loss with a policy gradient loss and introduces a KL-based policy drift
  regularizer and clipping mechanism to constrain policy updates.
---

# CPGD: Toward Stable Rule-based Reinforcement Learning for Language Models

## Quick Facts
- arXiv ID: 2505.12504
- Source URL: https://arxiv.org/abs/2505.12504
- Reference count: 40
- Primary result: CPGD achieves 11.0% overall improvement on multimodal math benchmarks vs QwenVL2.5-7B baseline

## Executive Summary
This paper addresses training instability in rule-based reinforcement learning for language models, where existing methods like GRPO and RLOO can suffer from policy collapse due to excessive importance-sampling ratios. The authors propose CPGD (Clipped Policy Gradient Optimization with Policy Drift), which replaces the PPO-clip loss with a policy gradient loss and introduces a KL-based policy drift regularizer and clipping mechanism to constrain policy updates. Theoretical analysis shows CPGD avoids amplification of policy drift, and experiments demonstrate superior stability and performance. When applied to QwenVL2.5-7B, CPGD achieves an 11.0% overall improvement over the base model across multimodal math benchmarks, with +21.8% on in-domain tasks and +8.5% on out-of-distribution tasks, outperforming other RL methods and comparable open-source baselines.

## Method Summary
CPGD replaces the PPO-clip loss with a policy gradient loss that avoids importance-sampling ratios, instead using a KL-based policy drift regularizer and clipping mechanism to constrain policy updates. The method introduces a drift coefficient α and constant c for ratio clipping, along with a tight-to-loose schedule for the clip threshold. Training uses batch size 128, 8 responses per prompt, learning rate 1e-6, 1 PPO epoch, and 5 training episodes with STD-weighted advantages and KL estimator k3 with gradient clipping.

## Key Results
- CPGD achieves 11.0% overall improvement over QwenVL2.5-7B baseline across 6 multimodal math benchmarks
- In-domain performance improves by 21.8% on MMK12 dataset
- Out-of-distribution performance improves by 8.5% on MathVista, MathVerse, and other benchmarks
- CPGD outperforms other RL methods and comparable open-source baselines

## Why This Works (Mechanism)
CPGD addresses policy collapse by eliminating importance-sampling ratios that can explode during training. The KL-based drift regularizer constrains policy changes between updates, preventing excessive drift that leads to instability. The clipping mechanism with tight-to-loose scheduling provides adaptive bounds on policy updates, maintaining stability while allowing sufficient exploration.

## Foundational Learning
**Policy gradient methods**: Why needed - foundation for all RL training of language models. Quick check - verify understanding of REINFORCE algorithm and baseline usage.
**Importance sampling ratios**: Why needed - critical for understanding why PPO-clip can fail. Quick check - can explain how ratios amplify small policy changes.
**KL divergence regularization**: Why needed - core to CPGD's stability mechanism. Quick check - understand difference between forward and reverse KL in policy constraints.
**Clipping strategies**: Why needed - essential for CPGD's core innovation. Quick check - can explain why adaptive clipping is superior to fixed clipping.

## Architecture Onboarding

**Component Map**: Policy network -> CPGD loss -> KL drift regularizer -> Clipping mechanism -> Advantage estimator

**Critical Path**: Data sampling → Policy forward pass → Advantage computation → CPGD loss calculation → Backpropagation → Parameter update

**Design Tradeoffs**: CPGD sacrifices some theoretical optimality guarantees from importance sampling to gain training stability. The KL regularizer adds computational overhead but prevents catastrophic collapse. Tight-to-loose clipping provides flexibility but requires careful scheduling.

**Failure Signatures**: Training collapse manifests as sudden accuracy drops and ratio explosion. Over-constrained policies show reduced exploration and performance plateaus. Improper clipping leads to either instability or underfitting.

**First Experiments**:
1. Train with varying clip threshold values (0.05, 0.1, 0.2) to assess sensitivity
2. Compare CPGD vs PPO-clip on simple tasks to verify stability benefits
3. Test different drift coefficient values to find optimal balance between stability and performance

## Open Questions the Paper Calls Out

**Off-policy training stability**: The authors leave off-policy settings for future work, noting that ensuring training stability in the presence of importance sampling remains an open question. Their method currently requires on-policy training with a single PPO epoch.

**Weighted advantage strategy optimization**: While testing STD weight and clip-filter-like weight, the paper calls for systematic comparison across diverse weighting functions with theoretical justification.

**Scaling to 100B+ parameter models**: All experiments used 7B parameter models, leaving questions about effectiveness on larger models and whether different training dynamics emerge at scale.

**Optimizing intermediate policy storage**: Multi-epoch importance sampling requires storing intermediate policy checkpoints, and efficient approximations to reduce memory overhead remain unexplored.

## Limitations

- Critical hyperparameters (clip threshold, drift coefficient, drift clipping constant) are unspecified, preventing faithful reproduction
- Evaluation protocol lacks confidence intervals or multiple random seeds for statistical significance assessment
- Method limited to on-policy training, excluding potentially more sample-efficient off-policy approaches
- No evaluation on models larger than 7B parameters, leaving scalability questions unanswered

## Confidence

**High confidence**: Theoretical motivation for avoiding importance sampling ratios is well-established in RL literature.

**Medium confidence**: Reported performance improvements are compelling but lack statistical significance measures.

**Low confidence**: Critical implementation details missing, making independent validation currently impossible.

## Next Checks

1. **Gradient norm monitoring**: Track policy gradient norms and KL divergence between successive policy updates during training to verify the drift regularizer effectively constrains policy changes.

2. **Clip threshold sensitivity**: Systematically vary ϵ values while keeping other hyperparameters fixed to assess the robustness of the tight-to-loose scheduling approach.

3. **PPO-clip ablation comparison**: Train identical models using PPO-clip with and without the proposed drift regularizer to isolate the contribution of each component to stability improvements.