---
ver: rpa2
title: The Multilingual Divide and Its Impact on Global AI Safety
arxiv_id: '2505.21344'
source_url: https://arxiv.org/abs/2505.21344
tags:
- language
- languages
- multilingual
- safety
- https
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper addresses the significant \"language gap\" in AI, where\
  \ most large language models are optimized for a few high-resource languages, leaving\
  \ many global languages underserved and creating disparities in AI safety performance.\
  \ The authors analyze why this gap exists\u2014due to data scarcity, resource inequities,\
  \ and limited multilingual research\u2014and how it exacerbates safety risks for\
  \ non-English speakers."
---

# The Multilingual Divide and Its Impact on Global AI Safety

## Quick Facts
- arXiv ID: 2505.21344
- Source URL: https://arxiv.org/abs/2505.21344
- Reference count: 40
- Most large language models are optimized for a few high-resource languages, leaving most of the world's 7,000+ languages underserved and creating safety disparities.

## Executive Summary
The paper addresses the significant "language gap" in AI, where most large language models are optimized for a few high-resource languages, leaving many global languages underserved and creating disparities in AI safety performance. The authors analyze why this gap exists—due to data scarcity, resource inequities, and limited multilingual research—and how it exacerbates safety risks for non-English speakers. Through the Cohere Labs Aya initiative, they demonstrate that combining human-curated, synthetic, and translated data can improve multilingual coverage. They emphasize building comprehensive, culturally-nuanced evaluation sets alongside models and highlight technical innovations like multilingual preference training and model merging to enhance performance and safety. The study recommends supporting multilingual dataset creation, transparency from model providers, and inclusive research to bridge the language gap and ensure equitable AI safety globally.

## Method Summary
The method combines human-curated, synthetic, and translated data with careful weighting to increase language coverage. Key innovations include safety context distillation—teaching models refusal behaviors for harmful prompts via teacher demonstrations—and model merging to combine specialized safety-focused and general-purpose models. The approach uses multilingual preference training and builds evaluation sets alongside models with human post-edits. The Aya dataset provides 513M prompts/completions across 114 languages with 200K human-curated annotations in 65 languages, while new benchmarks like Global-MMLU (42 languages) and INCLUDE (44 languages) enable comprehensive multilingual safety evaluation.

## Key Results
- Safety context distillation reduced harmful generations from adversarial prompts by 78–89% as judged by human experts
- Model merging can build stronger and safer multilingual systems without the typical safety alignment performance costs
- Hybrid data strategy combining human, synthetic, and translated data provides better coverage than relying solely on human annotations

## Why This Works (Mechanism)

### Mechanism 1: Safety Context Distillation
Teaching models refusal behaviors for harmful prompts via teacher demonstrations can substantially reduce harmful generations across languages. A teacher model demonstrates appropriate refusals for harmful prompts; the student model learns which contexts require refusals through distillation, creating language-agnostic safety patterns that transfer across linguistic boundaries. The core assumption is that refusal behaviors learned in one language can generalize to others when distilled properly, and the "harmful context" signal is sufficiently language-independent. Break condition: if refusal patterns are culturally-specific rather than universal, distillation may over-correct for some language communities while under-protecting others.

### Mechanism 2: Model Merging for Multilingual Safety-Efficiency Tradeoffs
Combining specialized models (safety-focused vs. general-purpose) through merging can create stronger multilingual systems without the typical performance costs of safety alignment. Model merging combines weight parameters from separately trained models, allowing preservation of both safety behaviors and multilingual capability—avoiding the "safety tax" where alignment degrades general performance. The core assumption is that safety and capability representations are sufficiently orthogonal in parameter space that merging preserves both; benefits scale across languages represented in the merged models. Break condition: if safety and multilingual capability compete for the same parameter regions, merging may cause interference or catastrophic forgetting, particularly for low-resource languages with weaker representations.

### Mechanism 3: Hybrid Data Strategy (Human + Synthetic + Translated)
Combining human-curated, synthetic, and translated data increases language coverage more effectively than relying solely on high-quality human annotations. Volume gains from synthetic/translated data compensate for quality trade-offs; diverse data sources create more robust multilingual representations, especially for languages where human annotation is prohibitively expensive or unavailable. The core assumption is that the signal-to-noise ratio from synthetic/translated data remains beneficial despite quality degradation; models can learn from imperfect data when volume is sufficient. Break condition: if translation artifacts systematically introduce safety evaluation errors, the strategy may create false confidence in safety coverage.

## Foundational Learning

- **Concept: Low-Resource Double-Bind**
  - Why needed here: Understanding that low-resource languages face compounding disadvantages—scarce data AND limited compute access—explains why naive scaling approaches fail and why efficient methods (merging, distillation) are critical.
  - Quick check question: If you double compute budget for a low-resource language project but don't increase data, would you expect proportional capability gains? Why or why not?

- **Concept: Curse of Multilinguality**
  - Why needed here: The paper explicitly references this phenomenon where adding languages can degrade per-language performance; this motivates architectural innovations like merging and careful data weighting.
  - Quick check question: Why might adding support for 50 more languages hurt performance on the original 10? What architectural choices could mitigate this?

- **Concept: Language-Parallel vs. Language-Specific Evaluation**
  - Why needed here: The paper distinguishes translated benchmarks (comparable across languages but potentially culturally misaligned) from localized benchmarks (culturally valid but harder to compare); both are needed for valid safety assessment.
  - Quick check question: If a safety benchmark translated from English to Hindi flags 30% of prompts as "culturally irrelevant," what does this tell you about the evaluation's validity for Hindi users?

## Architecture Onboarding

- **Component map:** Data Layer: Human annotations → Synthetic generation → Translation pipelines → Quality filtering → Model Layer: Base multilingual model → Instruction tuning → Preference training → Model merging → Safety Layer: Safety context distillation → Red-teaming datasets → Harmful prompt classifiers → Eval Layer: Language-parallel benchmarks (Global-MMLU) + Language-specific benchmarks (INCLUDE)

- **Critical path:** Start with data availability analysis → identify language coverage gaps → design hybrid data mix → train with multilingual preference optimization → apply safety distillation → merge with general capability model → evaluate on both parallel and localized benchmarks.

- **Design tradeoffs:**
  - Volume vs. Quality: More synthetic/translated data expands coverage but risks noise; calibrate by language based on annotation feasibility.
  - Parallel vs. Localized Eval: Parallel enables cross-language comparison but introduces translation artifacts; localized captures cultural nuance but complicates comparison.
  - Safety vs. Capability: Traditional alignment degrades capability; merging offers a path to preserve both but requires careful model selection.

- **Failure signatures:**
  - Harmful generation rates spike for low-resource languages despite good English safety metrics (indicates insufficient multilingual safety training).
  - Model refuses benign prompts in specific languages (over-correction from safety distillation).
  - Benchmark scores look good but human evaluators flag cultural inappropriateness (evaluation gap).
  - Performance cliffs: high scores on high-resource languages, near-random on low-resource (insufficient data weighting).

- **First 3 experiments:**
  1. Baseline audit: Evaluate existing model on both Global-MMLU (parallel) and INCLUDE (localized) across target languages; identify which languages show safety capability gaps vs. cultural alignment gaps.
  2. Safety distillation ablation: Apply safety context distillation with varying amounts of multilingual refusal data; measure harmful generation reduction per language and check for over-refusal.
  3. Merge configuration search: Test merging safety-tuned and general multilingual models at different weight ratios; evaluate whether safety-capability tradeoff improves relative to single-model approaches.

## Open Questions the Paper Calls Out

### Open Question 1
How can safety evaluation prompts retain their harmful intent and semantic meaning when translated into low-resource languages, given that automatic translation can render them meaningless or distort their original intent? The paper states that "translated prompts used in safety evaluations, where they can lose their harmful intent or become meaningless through translation errors" and recommends human post-edits, but this is resource-intensive and impractical for thousands of language-prompt combinations. Development of translation methods specifically calibrated for safety-critical content, validated through human evaluation across multiple language families showing preserved harmful intent would resolve this.

### Open Question 2
How can the reliability of LLM-as-judge evaluation methods be improved for low-resource languages when the judge models themselves underperform on those languages? The paper notes that "generative capabilities of LLMs are commonly evaluated with other LLMs as judges" but for lower-resource languages, these judges are less reliable due to lack of data, creating a circular dependency. Alternative evaluation paradigms that do not depend on LLM judges, validated against human preferences in low-resource languages would resolve this.

### Open Question 3
Do safety mitigation techniques such as safety context distillation and model merging generalize effectively across diverse language families and scripts, or do their benefits concentrate in languages linguistically similar to high-resource training languages? The paper reports positive results on 8-23 languages, but it's unclear whether these techniques scale to the remaining ~7,000 languages with different linguistic structures. Systematic ablation studies measuring safety improvements across typologically diverse languages with analysis of performance variance by language family would resolve this.

### Open Question 4
How can data pruning and quality filtering techniques be adapted to avoid inadvertently excluding legitimate low-resource language content that does not match high-resource language quality heuristics? The paper notes that pruning techniques might not equally generalize to all languages and domains, as quality heuristics are often developed on English data and may flag legitimate linguistic variation in low-resource languages as "noise." Comparative analysis of pruning outcomes on multilingual corpora, measuring the rate of false-positive exclusions per language and their impact on downstream task performance would resolve this.

## Limitations
- Safety context distillation relies on the assumption that safety patterns generalize across cultural contexts, which remains under-validated for languages with vastly different harm norms
- Model merging lacks detailed technical specifications about which model pairs were merged and what weight ratios were optimal, making replication difficult
- Hybrid data strategy doesn't provide granular analysis of when synthetic/translated data introduces safety evaluation errors versus when it genuinely improves coverage

## Confidence
- High Confidence: The fundamental premise that multilingual AI safety requires more than English-centric approaches is well-established; the data scarcity problem for low-resource languages is extensively documented and acknowledged across the field.
- Medium Confidence: The hybrid data strategy (human + synthetic + translated) shows promise based on volume-accuracy tradeoffs, but the optimal mix and quality thresholds for different language families remain uncertain.
- Medium Confidence: Safety context distillation and model merging are technically sound approaches with theoretical justification, but empirical validation across diverse cultural contexts and language families is limited.

## Next Checks
1. Conduct human evaluation studies with native speakers from diverse language communities to assess whether safety improvements from distillation maintain cultural appropriateness, not just harmful generation reduction rates.
2. Break down performance metrics by language family and resource level to identify whether improvements scale equitably or primarily benefit high-resource language groups within the 101-language coverage.
3. Evaluate the safety and capability models on truly low-resource languages (e.g., languages with <1M speakers) to test whether the hybrid data strategy's volume advantages hold when translation quality degrades significantly.