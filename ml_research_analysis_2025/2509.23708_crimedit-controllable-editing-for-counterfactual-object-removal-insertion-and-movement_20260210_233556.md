---
ver: rpa2
title: 'CrimEdit: Controllable Editing for Counterfactual Object Removal, Insertion,
  and Movement'
arxiv_id: '2509.23708'
source_url: https://arxiv.org/abs/2509.23708
tags:
- object
- removal
- insertion
- guidance
- movement
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'CrimEdit introduces a unified diffusion framework for controllable
  object removal, insertion, and movement using Cross-Task Guidance (CTG) between
  removal and insertion embeddings, achieving state-of-the-art results: LPIPS of 0.271
  on RemovalBench and 0.234 on BenchHard for removal; LPIPS of 0.228 for insertion;
  and LPIPS of 0.286 for object movement, all with improved controllability via guidance
  scale tuning.'
---

# CrimEdit: Controllable Editing for Counterfactual Object Removal, Insertion, and Movement

## Quick Facts
- **arXiv ID:** 2509.23708
- **Source URL:** https://arxiv.org/abs/2509.23708
- **Reference count:** 40
- **Primary result:** Unified diffusion framework for object removal, insertion, and movement with Cross-Task Guidance

## Executive Summary
CrimEdit introduces a novel diffusion-based framework that unifies three object editing tasks—removal, insertion, and movement—within a single model. The key innovation is Cross-Task Guidance (CTG), which leverages learned task embeddings to enhance control over object effects (shadows, reflections) during removal and synthesis during insertion. The model achieves state-of-the-art results on multiple benchmarks, demonstrating both high-quality outputs and fine-grained controllability through guidance scale tuning.

## Method Summary
CrimEdit fine-tunes SDXL-inpainting with two learned task embeddings (c_r for removal, c_i for insertion) and applies Cross-Task Guidance during inference. The model is trained on RORD dataset triplets with a 2:1 removal:insertion ratio, using shadow augmentation for insertion samples. For movement tasks, Spatial Cross-Task Guidance (S-CTG) enables single-step object repositioning by applying region-specific guidance with a switching timestep to resolve gray areas between removal and insertion regions.

## Key Results
- LPIPS of 0.271 on RemovalBench and 0.234 on BenchHard for object removal
- LPIPS of 0.228 for object insertion with harmonization
- LPIPS of 0.286 for object movement in single denoising step
- Improved controllability via guidance scale tuning for effect intensity
- Outperforms existing methods across all three tasks while maintaining unified architecture

## Why This Works (Mechanism)

### Mechanism 1: Cross-Task Guidance (CTG)
- **Claim:** CTG improves removal/insertion by treating opposing task embeddings as negative guidance signals.
- **Mechanism:** Applies classifier-free guidance between task embeddings using formulas (1+w)ϵ_r - wϵ_i for removal and (1+w)ϵ_i - wϵ_r for insertion. The removal embedding is steered away from insertion tendencies (synthesizing effects), while insertion is pushed away from removal tendencies (erasing effects).
- **Core assumption:** Removal and insertion represent semantically opposite operations—presence vs. absence of objects and their effects (shadows, reflections).
- **Evidence anchors:** [abstract] "leverages them in a classifier-free guidance scheme—enhancing the removal of both objects and their effects"; [Section 3.3.1] Equations (3) and (4) explicitly define CTG with negative guidance between task embeddings.

### Mechanism 2: Joint Training on Counterfactual Pairs
- **Claim:** Joint training creates complementary representations that benefit both tasks.
- **Mechanism:** The model is trained on RORD dataset triplets where removal learns I_o → I_b and insertion learns copy-pasted degraded input → I_o with shadow augmentation. Shared weights force the network to learn scene-intrinsic properties that transfer between tasks.
- **Core assumption:** Counterfactual pairs provide explicit supervision for how object presence affects scene context; the same latent features encode effect generation and removal.
- **Evidence anchors:** [abstract] "jointly trains the task embeddings for removal and insertion within a single model"; [Section 3.2] Training uses same RORD dataset with task-specific conditioning on c_r and c_i.

### Mechanism 3: Spatial Cross-Task Guidance (S-CTG)
- **Claim:** S-CTG enables single-step object movement by applying region-specific CTG.
- **Mechanism:** The guidance weight W becomes a spatial matrix with opposite signs in removal region P_r (positive w_r) and insertion region P_i (negative w_i < -1). A switching timestep t_s controls gray-area prioritization—early steps favor removal, later steps favor insertion.
- **Core assumption:** Removal and insertion can operate simultaneously without interference if spatially separated and temporally coordinated.
- **Evidence anchors:** [abstract] "enabling object movement (repositioning) within a single denoising step"; [Section 3.3.2] Equation (6) defines spatial guidance weights with switching timestep t_s.

## Foundational Learning

- **Classifier-Free Guidance (CFG)**
  - Why needed here: CTG is built on CFG principles—understanding how guidance scales control the trade-off between conditional and unconditional outputs is essential.
  - Quick check question: Given formula ϵ_guided = (1+w)ϵ_cond - wϵ_uncond, what happens as w → ∞?

- **Diffusion Model Conditioning**
  - Why needed here: CrimEdit extends SDXL-inpainting with task embeddings c_r and c_i; understanding how conditioning enters the denoising network is prerequisite.
  - Quick check question: In a U-Net diffusion model, how does an inpainting mask typically enter the network?

- **Counterfactual Datasets**
  - Why needed here: The RORD dataset provides paired images (with/without objects) that enable explicit supervision for effect removal/synthesis—unlike standard inpainting datasets.
  - Quick check question: Why can't a standard inpainting model trained on random masks learn to remove object shadows?

## Architecture Onboarding

- **Component map:** SDXL-inpainting (fine-tuned) -> Task embeddings (c_r, c_i) -> CTG module (w) -> S-CTG module (W, t_s) -> RORD dataset with shadow augmentation

- **Critical path:**
  1. Load pretrained SDXL-inpainting weights
  2. Initialize task embeddings c_r, c_i as learnable parameters
  3. Train on RORD with 2:1 removal:insertion ratio using Equation (1)
  4. At inference, apply CTG with scale w=1.5 (default) or S-CTG for movement

- **Design tradeoffs:**
  - Single model vs. separate models: Unification saves memory but may reduce peak performance per task
  - Guidance scale w: Higher w improves effect control but risks oversaturation; w=1.5 works well empirically
  - Switching timestep t_s in S-CTG: Controls removal/insertion priority in gray areas; requires tuning per scene

- **Failure signatures:**
  - Incomplete shadow removal: CTG scale too low or mask doesn't cover full effect region
  - Over-harmonization during insertion: CTG scale too high, washing out object identity
  - Movement artifacts: P_r and P_i too close; shadow from source region bleeds into destination

- **First 3 experiments:**
  1. **Baseline CTG ablation:** Compare CrimEdit with w=0 (no guidance) vs. w=1.5 on RemovalBench. Expect LPIPS improvement from 0.272 → 0.271 per Table 2.
  2. **Insertion controllability sweep:** Vary CTG scale w ∈ {1.2, 1.7, 2.0, 2.5} and measure shadow area using LISA detection (replicate Table 4). Confirm larger w → larger shadow regions.
  3. **S-CTG vs. sequential pipeline:** Compare single-step S-CTG against two-stage removal+insertion (each with 25 steps) on ReS dataset. Expect S-CTG to match or exceed quality at half the compute (6.74s vs. ~13s per Table 5).

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the CrimEdit framework be extended to handle non-rigid object movement or perspective changes, given the current architectural constraint that source and destination regions must have identical shapes?
- **Basis in paper:** [explicit] Section 3.1.1 states that for the movement task, "Their shapes are constrained to be identical, without deformation."
- **Why unresolved:** The current Spatial CTG (S-CTG) and training data setup assume a direct pixel mapping between removal and insertion regions, which does not account for geometric transformations required for realistic non-rigid movement.
- **What evidence would resolve it:** A modification of the framework that successfully moves objects requiring rotation or deformation (e.g., sitting vs. standing person) while maintaining effect consistency, evaluated on a dataset with geometric transforms.

### Open Question 2
- **Question:** Is there an adaptive mechanism to determine the optimal switching step $t_s$ for the "gray area" in Spatial CTG, rather than relying on a manually fixed timestep?
- **Basis in paper:** [inferred] Equation (6) defines the switching step $t_s$ as a static hyperparameter for prioritizing removal vs. insertion in background regions, but provides no analysis on how to automate or optimize this for individual images.
- **Why unresolved:** A fixed switching step may be suboptimal across diverse scenes where the structural denoising phase varies in length; manual tuning limits the model's "out-of-the-box" utility.
- **What evidence would resolve it:** An ablation study demonstrating significant variance in optimal $t_s$ across the BenchHard dataset, or a proposed dynamic scheduling mechanism that improves LPIPS scores over a fixed baseline.

### Open Question 3
- **Question:** Can the Cross-Task Guidance (CTG) scale $w$ be dynamically predicted or adjusted based on the specific lighting context of an image to ensure optimal effect synthesis without manual search?
- **Basis in paper:** [inferred] The paper highlights "controllability" via the guidance scale (Figure 7) and finds a specific value (1.5) works best on average, implying that the optimal scale varies and currently requires user intervention or hyperparameter search.
- **Why unresolved:** The dependence on a user-defined scalar for "realistic" results suggests the model does not inherently understand the necessary intensity of shadows or reflections for a given scene.
- **What evidence would resolve it:** The integration of a auxiliary network or metric that predicts the optimal $w$ per image, resulting in higher automated PSNR/CLIP scores compared to using a static default value.

## Limitations
- The movement task requires source and destination regions to have identical shapes, limiting non-rigid object transformations
- Performance depends heavily on guidance scale hyperparameter w, requiring manual tuning for optimal results
- Training requires a specific counterfactual dataset (RORD) that may not be readily available for all object categories

## Confidence
- **High confidence:** Core CTG mechanism with clear equations and training procedure
- **Medium confidence:** Single-step movement capability with S-CTG, though practical implementation details require validation
- **Low confidence:** Generalization claims beyond RORD dataset without extensive real-world testing

## Next Checks
1. Implement systematic CTG scale sweeps (w ∈ {1.0, 1.5, 2.0, 2.5}) on RemovalBench and BenchHard to quantify the exact LPIPS improvement curve and identify saturation points
2. Conduct ablation studies comparing joint training vs. separate models for removal and insertion on the same RORD data to measure the true benefit of unified representation learning
3. Test S-CTG movement with varying ts values on ReS dataset to determine optimal switching points and characterize performance degradation when ts is poorly chosen