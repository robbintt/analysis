---
ver: rpa2
title: 'OpenFPL: An open-source forecasting method rivaling state-of-the-art Fantasy
  Premier League services'
arxiv_id: '2508.09992'
source_url: https://arxiv.org/abs/2508.09992
tags:
- openfpl
- data
- league
- fantasy
- player
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: OpenFPL is an open-source method for forecasting Fantasy Premier
  League (FPL) player performance using only publicly available FPL and Understat
  data. It uses position-specific ensemble models (XGBoost + Random Forest) optimized
  via automatic hyperparameter search to predict player FPL points.
---

# OpenFPL: An open-source forecasting method rivaling state-of-the-art Fantasy Premier League services

## Quick Facts
- arXiv ID: 2508.09992
- Source URL: https://arxiv.org/abs/2508.09992
- Authors: Daniel Groos
- Reference count: 40
- Primary result: Open-source FPL forecasting method achieves accuracy comparable to leading commercial service

## Executive Summary
OpenFPL is an open-source method for forecasting Fantasy Premier League player performance using only publicly available FPL and Understat data. It uses position-specific ensemble models (XGBoost + Random Forest) optimized via automatic hyperparameter search to predict player FPL points. When prospectively tested on the 2024-25 season, OpenFPL achieved accuracy comparable to the leading commercial FPL Review service, with RMSE values of 0.818 vs 0.689 for non-playing players (Zeros), 1.291 vs 1.189 for low-scoring players (Blanks), and 1.517 vs 1.594 for moderate scorers (Tickers). Notably, OpenFPL outperformed the commercial benchmark for high-scoring players (Haulers, ≥5 points) across all forecast horizons (1-3 gameweeks ahead), demonstrating its value for identifying high-return players critical for rank gains. All models and code are freely available under an MIT license, providing a transparent benchmark for FPL analytics research.

## Method Summary
OpenFPL forecasts FPL player points using position-specific ensemble models trained on 4 seasons of data (2020-21 to 2023-24). The method employs XGBoost and Random Forest models optimized through K-Best Search, with features aggregated over multiple time horizons (1, 3, 5, 10, and 38 matches) for player, team, and opponent dimensions. Position-specific sample weighting addresses the skewed distribution of FPL points, emphasizing high-return players during training. The final prediction is the median of 50 models (top 10 from 5 cross-validation folds). Prospective evaluation on 2024-25 gameweeks 32-38 shows comparable accuracy to the commercial FPL Review service, with particular strength in predicting high-scoring players.

## Key Results
- RMSE of 0.818 vs 0.689 for non-playing players (Zeros) compared to FPL Review
- RMSE of 1.517 vs 1.594 for moderate scorers (Tickers) across all forecast horizons
- Outperformed commercial benchmark for high-scoring players (Haulers, ≥5 points) across 1-3 gameweek horizons
- All code and models available under MIT license for reproducible research

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Position-specific ensemble models improve forecast accuracy by capturing role-dependent performance patterns.
- Mechanism: Separate XGBoost + Random Forest ensembles are trained per position (GK, DEF, MID, FWD, AM), each using tailored feature sets. The final prediction is the median of 50 models (top 10 from 5 cross-validation folds), reducing variance while preserving position-specific signal.
- Core assumption: Player performance patterns differ systematically by position, and ensembling diverse tree-based models reduces overfitting more than single-model approaches.
- Evidence anchors:
  - [abstract] "It uses position-specific ensemble models (XGBoost + Random Forest) optimized via automatic hyperparameter search to predict player FPL points."
  - [Section 2.4] "For each player position, an ensemble model was constructed by combining the top K models from the K-Best Search of each cross-validation fold. Ensemble model forecasts are obtained as the median forecasted FPL points of the 50 individual models."
  - [corpus] Related work (Frees et al., 2024) explores deep learning for EPL forecasting but does not use position-specific ensembles; this design choice appears novel to OpenFPL.
- Break condition: If player roles become fluid (e.g., tactical systems where defenders frequently attack), position-specific training could introduce bias. Monitor for systematic errors when players change position mid-season.

### Mechanism 2
- Claim: Multi-horizon historical features with opponent context capture form and fixture difficulty effects.
- Mechanism: Features are aggregated over 1, 3, 5, 10, and 38 match windows for player (Xp), team (Xt), and opponent (Xo) dimensions. Understat metrics (xG, xA, PPDA, Deep) supplement FPL basics, encoding underlying performance rather than just outcomes.
- Core assumption: Historical performance patterns over multiple time horizons predict future returns, and opponent quality moderates this relationship.
- Evidence anchors:
  - [abstract] "OpenFPL outperformed the commercial benchmark for high-scoring players (Haulers, ≥5 points) across all forecast horizons (1-3 gameweeks ahead)"
  - [Section 2.3] "OpenFPL tackles this by utilizing features over different time horizons, from short-term (i.e., one, three, and five most recent matches) to long-term (i.e., past 10 and 38 matches), together with current data on the match status of players."
  - [corpus] Corpus evidence on multi-horizon feature design is limited; no direct comparisons found.
- Break condition: If FPL scoring rules change significantly (e.g., bonus point algorithm updates), feature-to-target relationships may shift. Retrain models after rule changes.

### Mechanism 3
- Claim: Sample weighting by target distribution entropy emphasizes high-return players during training.
- Mechanism: Training samples are weighted using position-specific binning (2-5 bins based on entropy), with balanced class weights clipped at 95th percentile. This counteracts the dominance of low-scoring samples (most players score ≤2 points) and improves prediction for high-value targets.
- Core assumption: The skewed distribution of FPL points (many zeros/blanks, few hauls) causes unweighted models to underpredict high scorers.
- Evidence anchors:
  - [Section 2.4] "To balance overall forecasting accuracy with the ability to predict high-performing players, those of greatest interest to FPL participants, weighting of training and validation samples was performed using position-specific discretization based on the entropy of the target distributions"
  - [Section 3, Table 4] OpenFPL achieves lower RMSE for Tickers and Haulers across all horizons, while FPL Review excels at Zeros/Blanks.
  - [corpus] No corpus papers explicitly discuss sample weighting for FPL; this appears to be an OpenFPL-specific design.
- Break condition: If game meta shifts toward defensive scoring (e.g., clean sheets become more valuable than goals), binning strategy may need recalibration.

## Foundational Learning

- Concept: **Gradient Boosted Trees (XGBoost)**
  - Why needed here: Core regressor in the ensemble; requires understanding of boosting, learning rate, regularization, and early stopping.
  - Quick check question: Can you explain why XGBoost might overfit if `learning_rate` is too high and `n_estimators` is too low?

- Concept: **Cross-Validation with Group-Based Splits**
  - Why needed here: Folds are split by Premier League teams (not random), preventing leakage from team-specific patterns. Essential for proper evaluation.
  - Quick check question: Why would random train/test splits fail in this context?

- Concept: **Feature Normalization and Sample Weighting**
  - Why needed here: MinMaxScaler normalizes inputs; sample weighting addresses class imbalance. Both affect model convergence and prediction distribution.
  - Quick check question: If you skip sample weighting, what type of prediction errors would you expect to increase?

## Architecture Onboarding

- Component map: FPL API + Understat API → historical dataset (2020-24) → multi-horizon feature aggregation → position-specific feature sets (122-206 features) → K-Best Search over hyperparameter space → top 10 models per fold → median of 50 models (ensemble) → real-time inference pipeline

- Critical path: Feature engineering → hyperparameter search → ensemble training. Errors in feature computation (e.g., incorrect rolling averages) propagate silently.

- Design tradeoffs:
  - OpenFPL uses public data only (no expected minutes); FPL Review incorporates proprietary projections. This explains OpenFPL's weaker performance on Zeros/Blanks (non-players).
  - Median ensemble is robust to outliers but may underreact to extreme form changes.
  - Direct FPL point regression vs. indirect event probability modeling (FPL Review approach)—tradeoff between simplicity and interpretability.

- Failure signatures:
  - High RMSE on Zeros suggests model expects playing time that doesn't occur (missing expected minutes feature).
  - Position mismatches (e.g., defender listed as midfielder) cause feature misalignment.
  - Stale data: predictions generated before deadline must use previous-day data; late injury news is missed.

- First 3 experiments:
  1. **Feature ablation**: Retrain models with only FPL data (no Understat). Measure RMSE delta to quantify value of advanced metrics.
  2. **Horizon sensitivity**: Evaluate accuracy degradation from 1 to 5 gameweeks ahead. Identify when forecasts become unreliable.
  3. **Ensemble size test**: Compare median of 50 models vs. 10 vs. 5. Determine minimum ensemble size for stable predictions.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can an indirect forecasting approach (predicting specific events like goals) outperform OpenFPL's direct point regression method?
- Basis in paper: [explicit] The authors state, "Future research could assess whether an indirect public-data forecasting method employing hyperparameter search... could improve accuracy further."
- Why unresolved: OpenFPL predicts points directly; the paper does not compare this against a public-data model that aggregates probabilities of discrete scoring events.
- What evidence would resolve it: A comparative study benchmarking an indirect model against OpenFPL on the same out-ofsample dataset using RMSE.

### Open Question 2
- Question: Which input features and time horizons are most predictive for specific player positions?
- Basis in paper: [explicit] The authors note it would be valuable to "systematically investigate for individual player positions the importance of different features and time horizons."
- Why unresolved: While position-specific models were trained, the study did not analyze feature importance or whether shorter/longer horizons benefit specific positions.
- What evidence would resolve it: Feature importance metrics (e.g., SHAP values) for each position-specific model and ablation studies removing specific time horizons.

### Open Question 3
- Question: Can crowd-sourced minutes projections close the performance gap for low-return players (Zeros and Blanks)?
- Basis in paper: [explicit] The authors attribute errors in low-return categories to the lack of expected-minutes data and suggest "future work could extend this line through crowd-sourced minutes forecasts."
- Why unresolved: OpenFPL uses categorical availability tags rather than continuous minutes projections, which limits its ability to predict benchings or substitute appearances.
- What evidence would resolve it: A variation of OpenFPL incorporating crowd-sourced minutes data showing reduced RMSE for the "Zeros" and "Blanks" categories.

## Limitations

- Prospective evaluation covers only 7 gameweeks (GW32-38, 2024-25), limiting generalizability across full-season dynamics and different fixture congestion patterns.
- No statistical significance testing between OpenFPL and FPL Review RMSE differences; observed performance gaps could be due to random variation.
- Sample weighting strategy tuned for current FPL scoring system; effectiveness uncertain if game mechanics change (e.g., bonus point redistribution).

## Confidence

- Position-specific ensemble design effectiveness: High
- Multi-horizon feature aggregation value: Medium
- Commercial benchmark comparison validity: Medium

## Next Checks

1. Replicate 2024-25 evaluation using rolling 7-gameweek windows to test model stability across full season cycles
2. Conduct paired statistical tests (paired t-tests) on RMSE differences across return categories to establish significance
3. Test model performance when defenders are deployed as forwards and midfielders as defenders to assess position-specific robustness