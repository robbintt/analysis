---
ver: rpa2
title: Enhancing Pre-Trained Model-Based Class-Incremental Learning through Neural
  Collapse
arxiv_id: '2504.18437'
source_url: https://arxiv.org/abs/2504.18437
tags:
- learning
- classes
- neural
- pre-trained
- continual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses catastrophic forgetting in class-incremental
  learning (CIL) with pre-trained models (PTMs). The authors propose modeling feature
  evolution through neural collapse (NC), a phenomenon where features align in a well-separated,
  equiangular structure.
---

# Enhancing Pre-Trained Model-Based Class-Incremental Learning through Neural Collapse

## Quick Facts
- **arXiv ID**: 2504.18437
- **Source URL**: https://arxiv.org/abs/2504.18437
- **Reference count**: 40
- **Primary result**: 6.73% improvement on VTAB over runner-up approaches using pre-trained model-based class-incremental learning with neural collapse

## Executive Summary
This paper addresses catastrophic forgetting in class-incremental learning (CIL) with pre-trained models (PTMs) by modeling feature evolution through neural collapse (NC). The authors propose NCPTM-CIL, which introduces a dynamic ETF classifier, ETF alignment, and pull-and-push loss to maintain the NC geometry during incremental learning. Their method outperforms state-of-the-art approaches on multiple benchmarks, achieving significant accuracy improvements while approaching theoretical upper bounds. The key insight is that catastrophic forgetting stems from attenuation of inter-class linear separability, which can be mitigated by maintaining an equiangular tight frame (ETF) structure.

## Method Summary
NCPTM-CIL operates in two phases: Phase I fine-tunes a ViT backbone using parameter-efficient methods (AdaptFormer and VPT-Deep) on base classes, then freezes it; Phase II incrementally trains only an alignment layer to transform class-mean features to match a dynamically expanding ETF classifier while applying pull-and-push loss. The method stores class-mean features (memory-efficient compared to exemplar replay) and uses a novel PAP loss that explicitly pulls features toward correct class prototypes while pushing them away from incorrect ones. The ETF classifier grows with each task through orthogonal matrix expansion, maintaining optimal angular separation between all classes.

## Key Results
- **VTAB**: 6.73% improvement over runner-up approaches
- **CIFAR-100**: 1.25% improvement over runner-up approaches
- **OmniBenchmark**: 2.5% improvement over runner-up approaches
- **Theoretical bounds**: Approaches within 1.5% of upper bounds on most datasets

## Why This Works (Mechanism)

### Mechanism 1: Neural Collapse Geometry as Forgetting Buffer
The ETF structure maximizes angular separation between all class prototypes simultaneously. When new classes arrive, the geometry forces features into pre-defined "slots" rather than allowing drift that would overlap with old class regions. Catastrophic forgetting in PTM-CIL primarily manifests as reduced linear separability of previously learned classes.

### Mechanism 2: Frozen Backbone with Trainable Alignment Layer
Freezing the pre-trained backbone while training only an alignment layer preserves foundational representations while allowing task-specific adaptation. The alignment layer transforms PTM features to match the dynamic ETF classifier without modifying the backbone weights, preventing feature drift that would corrupt representations for old classes.

### Mechanism 3: Pull-and-Push Loss for Geometric Regularization
The PAP loss creates explicit attraction to correct class prototype and repulsion from incorrect prototypes, enforcing ETF alignment. The pull term maximizes cosine similarity to the correct prototype while the push term forces cosine similarity toward -1/(K-1), the ETF optimal for equiangular separation.

## Foundational Learning

- **Concept: Neural Collapse (NC)**
  - Why needed: The entire method is built on inducing and maintaining NC structure. Without understanding NC1, NC2, and NC3, the design rationale is opaque.
  - Quick check: Can you explain why an ETF structure with K vectors in K-1 dimensions maximizes angular separation?

- **Concept: Class-Incremental Learning (CIL) Protocol**
  - Why needed: The paper assumes familiarity with disjoint class setting, base learning vs. incremental phases, and evaluation metrics. Misunderstanding leads to wrong experimental replication.
  - Quick check: What is the difference between task-incremental and class-incremental evaluation at inference time?

- **Concept: Parameter-Efficient Fine-Tuning (PEFT)**
  - Why needed: The method uses AdaptFormer and VPT-Deep for Phase I. Understanding how low-rank matrices and learnable prompts work is essential for reproducing the fine-tuning stage.
  - Quick check: Why does freezing backbone weights while training only adapter/prompt parameters help with forgetting?

## Architecture Onboarding

- **Component map**: Backbone (frozen ViT) -> Class-Mean Features Pool -> Alignment Layer (MLP) -> Dynamic ETF Classifier
- **Critical path**: 1) Phase I: Fine-tune ViT with AdaptFormer+VPT-Deep on base classes; 2) Phase II: Freeze backbone, extract class means, initialize ETF classifier; 3) For each task: initialize fresh alignment layer, train with PAP+CE loss while maintaining stored class means; 4) Inference: compute similarity between test feature and all ETF vectors
- **Design tradeoffs**: Memory vs. performance (storing class means is cheaper than exemplar replay); Alignment layer capacity (larger MLP improves alignment but risks overfitting); ETF classifier initialization (orthogonal expansion vs. task-specific re-initialization)
- **Failure signatures**: NC2 not decreasing (underfitting or overfitting); Sharp accuracy drop on old classes (alignment layer contamination); Overfitting to current task (reduce epochs); NC1 high after Phase I (inappropriate fine-tuning method)
- **First 3 experiments**: 1) Reproduce ablation on CIFAR-100 B50-Inc5 with and without each component to validate implementation; 2) For new dataset, compute NC1 after Phase I with different fine-tuning methods and select lowest NC1; 3) Plot NC2 vs. accuracy across incremental steps to confirm negative correlation

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions in the traditional sense, but the discussion section implicitly raises questions about the generality of the approach to different domains and architectures, and the scalability to many incremental steps.

## Limitations
- The correlation between base-phase NC metrics and final performance may not generalize to datasets with extreme domain shifts where pre-trained feature quality degrades
- The dynamic expansion of the orthogonal matrix in the Dynamic ETF Classifier may accumulate numerical errors over many incremental steps
- The efficacy of the alignment layer and pull-and-push loss may be specific to Vision Transformer architecture and not transfer effectively to Pre-Trained CNNs

## Confidence
- **High confidence**: Phase I fine-tuning methodology (AdaptFormer + VPT Deep), CIFAR-100 B50-Inc5 results, and general two-phase framework are well-specified and reproducible
- **Medium confidence**: Dynamic ETF classifier construction and PAP loss formulation are mathematically sound but implementation details like orthogonal matrix initialization are unspecified
- **Low confidence**: Claims about NC geometry being the primary cause of forgetting reduction lack direct mechanistic proof; could be confounded by increased model capacity or other factors

## Next Checks
1. **Geometry ablation test**: Implement a variant using only cosine similarity classification (no ETF constraints) with the same alignment layer capacity. Compare accuracy and NC2/NC3 values to isolate the benefit of strict ETF structure.
2. **Alignment layer capacity sweep**: Systematically vary MLP layer width and depth to determine the minimum capacity needed for NC2 < 0.25, establishing whether observed benefits require significant additional parameters.
3. **Cross-dataset robustness**: Apply NCPTM-CIL to a domain-shifted dataset (e.g., domain adaptation from ImageNet to medical images) to test whether frozen backbone assumptions hold under large distribution shifts.