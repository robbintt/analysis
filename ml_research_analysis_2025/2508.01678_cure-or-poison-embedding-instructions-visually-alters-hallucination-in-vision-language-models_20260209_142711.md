---
ver: rpa2
title: Cure or Poison? Embedding Instructions Visually Alters Hallucination in Vision-Language
  Models
arxiv_id: '2508.01678'
source_url: https://arxiv.org/abs/2508.01678
tags:
- image
- prompt-in-image
- text
- uni00000013
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of hallucination in Vision-Language
  Models (VLMs), where models generate incorrect descriptions or detect non-existent
  objects. The authors propose Prompt-in-Image, a method that embeds textual instructions
  directly into images, forcing models to process all information through a single
  visual modality.
---

# Cure or Poison? Embedding Instructions Visually Alters Hallucination in Vision-Language Models

## Quick Facts
- **arXiv ID**: 2508.01678
- **Source URL**: https://arxiv.org/abs/2508.01678
- **Authors**: Zhaochen Wang; Yiwei Wang; Yujun Cai
- **Reference count**: 9
- **Primary result**: Prompt-in-Image embedding instructions into images improves hallucination in Qwen2.5-VL (+4.1% POPE accuracy) but catastrophically degrades CLIP-based VLMs

## Executive Summary
This paper addresses hallucination in Vision-Language Models (VLMs) by proposing Prompt-in-Image, a method that embeds textual instructions directly into images. The approach forces models to process all information through a single visual modality, aiming to improve cross-modal alignment and reduce hallucination. Experiments on three popular VLMs reveal divergent effects: Qwen2.5-VL shows consistent improvements with Prompt-in-Image, while LLaVA-1.5 and InstructBLIP experience severe performance degradation. The key finding is that vision encoder architecture determines whether embedded text helps or harms performance, with Qwen's robust text handling enabling modality gap reduction while CLIP's attention bias causes catastrophic failure.

## Method Summary
Prompt-in-Image embeds textual instructions into images as black Arial 26pt text on a white rectangle (~5% image height) at the bottom. This forces single-modality processing through the vision encoder, eliminating separate text processing. The method was evaluated across four settings: Baseline (image + text prompt), Prompt-in-Image (image with embedded question, no text prompt), Hybrid (both), and Control (white box only + text prompt). Three VLMs were tested: Qwen2.5-VL-7B, InstructBLIP-vicuna-7B, and LLaVA-v1.5-7B, using POPE (object existence) and MS-COCO captioning with CHAIR hallucination metrics.

## Key Results
- Qwen2.5-VL accuracy improves 4.1% (from 80.2% to 84.3%) with Prompt-in-Image on POPE
- LLaVA-1.5 and InstructBLIP accuracy drops from ~84% to near-random levels (~55%) with Prompt-in-Image
- CLIP-based encoders show excessive attention bias toward embedded text regions, disrupting visual understanding
- Prompt-in-Image reduces Qwen's modality gap by 12% average cosine distance reduction
- Hybrid setting (embedded text + separate text prompt) performs worse than Prompt-in-Image alone for Qwen

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Unifying input through a single visual modality can reduce cross-modal alignment failures in VLMs
- Mechanism: By embedding textual instructions into images, all information passes through the vision encoder → projector → LLM pipeline, eliminating the need to align separately processed text tokens with visual tokens. This reduces the "language bias" phenomenon where models ignore visual evidence in favor of textual priors.
- Core assumption: Cross-modal alignment is a significant source of hallucination; reducing modalities reduces alignment failures
- Evidence anchors: [abstract] "Prompt-in-Image reduces Qwen's modality gap, enhancing cross-modal alignment by unifying information processing through a single modality"; [section 5.2] "Prompt-in-Image group consistently exhibits smaller cosine distances, with an average reduction of 12%"

### Mechanism 2
- Claim: Vision encoder architecture and pretraining data determine whether embedded text helps or harms performance
- Mechanism: Qwen-ViT was pretrained on interleaved image-text documents and OCR data, learning to treat text as a normal visual element. CLIP-ViT (used in LLaVA/InstructBLIP) exhibits excessive attention to text regions in deep layers, causing attention to concentrate on embedded text at the expense of actual image content.
- Core assumption: Pretraining data composition directly affects how vision encoders process text-in-image inputs
- Evidence anchors: [section 5.1] "CLIP shows a clear tendency to focus attention heavily on the text region" in layer 24; [section 5.1] "Qwen-ViT maintains consistently high similarity (>0.95) between Prompt-in-Image and Control images throughout its deep layers"

### Mechanism 3
- Claim: Adding explicit text instructions alongside Prompt-in-Image degrades rather than improves performance for compatible models
- Mechanism: The Hybrid setting (embedded text + separate text prompt) performs worse than Prompt-in-Image alone for Qwen. Introducing a second modality partially re-introduces alignment challenges that the unified approach was designed to avoid.
- Core assumption: Modality unification provides benefits that exceed the value of redundant instruction channels
- Evidence anchors: [section 4.3] "Hybrid configuration also shows gains (+2.8%), though slightly lower than Prompt-in-Image (+4.1%)"

## Foundational Learning

- Concept: **Modality Gap**
  - Why needed here: The paper frames its contribution through modality gap reduction; understanding that embeddings from different modalities cluster separately in shared representation space is essential
  - Quick check question: If image and text embeddings for the same content have cosine distance 0.3 vs 0.5, which indicates better cross-modal alignment?

- Concept: **Attention Bias in Vision Transformers**
  - Why needed here: The divergent results stem from how CLIP vs Qwen vision encoders distribute attention across patches when text is present
  - Quick check question: In a ViT processing an image with embedded text, what behavior would indicate "attention collapse" to text regions?

- Concept: **Hallucination Taxonomy in VLMs**
  - Why needed here: The paper evaluates on POPE (object existence detection) and CHAIR (caption hallucination); distinguishing judgement vs description hallucination clarifies what's being measured
  - Quick check question: A model correctly describes a scene but claims a non-existent object is present—which hallucination type is this?

## Architecture Onboarding

- Component map: [Image+Text] → Vision Encoder → Projector → LLM (no separate text input)
- Critical path: Vision encoder behavior determines success/failure → if encoder has text attention bias (CLIP), performance collapses; if encoder handles embedded text robustly (Qwen-ViT), modality gap decreases and hallucination reduces
- Design tradeoffs:
  - Text placement: bottom of image in white box (~5% height), 26pt Arial black font—chosen for readability without major occlusion
  - Control group essential: white box without text isolates text effect from layout change effect
- Failure signatures:
  - **Qwen-compatible**: POPE accuracy improves 4-5%, CHAIR scores decrease, model provides more detailed grounded descriptions
  - **CLIP-based failure**: Accuracy drops to ~55% (near-random), Yes Ratio → 0.99, model defaults to affirmative regardless of visual evidence
- First 3 experiments:
  1. **Attention visualization**: Extract patch-level attention from vision encoder layers 4, 12, 24 on Prompt-in-Image vs Control images; quantify if deep layers show attention concentration on text patches
  2. **Modality gap measurement**: Compute cosine distance between image and caption embeddings for 35+ samples; compare Prompt-in-Image vs baseline conditions
  3. **Encoder substitution test**: (Not in paper but implied) Test whether swapping vision encoders between architectures transfers the Prompt-in-Image compatibility

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can CLIP-based VLMs be modified (via fine-tuning, architectural changes, or attention regularization) to benefit from Prompt-in-Image rather than suffer catastrophic degradation?
- Basis in paper: [explicit] The paper identifies that "CLIP-based encoders in LLaVA and InstructBLIP exhibit excessive attention bias toward embedded text regions, disrupting visual understanding" and shows this causes accuracy to drop from ~84% to near-random levels (~55%).
- Why unresolved: The paper diagnoses the failure mechanism but does not attempt interventions to mitigate CLIP's text attention bias or test whether alternative training regimes could enable Prompt-in-Image benefits.

### Open Question 2
- Question: Does Prompt-in-Image improve performance on broader task categories beyond hallucination reduction (e.g., visual reasoning, document understanding, spatial relationships)?
- Basis in paper: [inferred] The evaluation is limited to POPE (object existence) and MS-COCO captioning; the paper does not test whether cross-modal alignment improvements generalize to tasks requiring complex multi-step reasoning.
- Why unresolved: Hallucination reduction demonstrates improved grounding, but the paper's scope does not establish whether unified modality processing helps or hinders tasks with different cognitive demands.

### Open Question 3
- Question: What specific pretraining data characteristics enable Qwen-ViT's robustness to embedded text, and can these be replicated efficiently?
- Basis in paper: [explicit] The paper states Qwen's robustness "likely stems from Qwen's diverse pretraining regime, which includes not only standard image-caption pairs but also interleaved image-text documents and OCR data" but presents this as a hypothesis without causal verification.
- Why unresolved: The correlation between pretraining data diversity and Prompt-in-Image compatibility is noted, but no ablation or controlled experiments isolate which data components are necessary or sufficient.

## Limitations
- The method works for Qwen2.5-VL but catastrophically fails for CLIP-based VLMs, suggesting model-dependent rather than universal solution
- Evaluation limited to object detection and captioning; doesn't test complex reasoning or document understanding tasks
- Text placement strategy may not generalize well to all image types and aspect ratios

## Confidence

**High Confidence**: The divergent performance between Qwen2.5-VL and CLIP-based models is reproducible and well-documented. The attention analysis showing CLIP's text attention bias in deep layers is supported by quantitative similarity scores and patch-level attention visualizations.

**Medium Confidence**: The claim that cross-modal alignment reduction is the primary mechanism driving hallucination reduction in Qwen2.5-VL. While supported by modality gap measurements, the paper doesn't directly establish causation between alignment improvement and hallucination reduction.

**Low Confidence**: The generalizability of text placement strategies and font specifications across diverse image types and VLM architectures. The claim that modality unification consistently outperforms instruction redundancy for compatible models needs more empirical validation across different task types and model sizes.

## Next Checks

1. **Cross-architecture Vision Encoder Swap**: Replace Qwen-ViT with CLIP-ViT in the Qwen2.5-VL architecture and vice versa to isolate whether performance differences stem from vision encoder architecture or the complete model training paradigm.

2. **Multi-task Generalization Test**: Evaluate Prompt-in-Image on complex reasoning tasks like visual question answering requiring multi-step inference, document understanding with dense text, and open-ended generation tasks to assess whether the benefits extend beyond object detection and captioning.

3. **Attention Pattern Evolution Analysis**: Track how attention patterns evolve across all 32 layers of both vision encoders when processing text-embedded images, using gradient-based attribution methods to identify whether attention collapse is the primary failure mode in CLIP-based models or if other mechanisms contribute.