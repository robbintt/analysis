---
ver: rpa2
title: 'Red-Bandit: Test-Time Adaptation for LLM Red-Teaming via Bandit-Guided LoRA
  Experts'
arxiv_id: '2510.07239'
source_url: https://arxiv.org/abs/2510.07239
tags:
- attack
- arxiv
- red-bandit
- lora
- prompts
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Red-Bandit introduces a test-time adaptation framework for LLM
  red-teaming that employs a multi-armed bandit to dynamically select among LoRA-trained
  attack-style experts. Each expert is specialized for a distinct adversarial style
  (e.g., role play, slang, historical) and trained via reinforcement learning with
  a rule-based prompt-safety reward.
---

# Red-Bandit: Test-Time Adaptation for LLM Red-Teaming via Bandit-Guided LoRA Experts

## Quick Facts
- arXiv ID: 2510.07239
- Source URL: https://arxiv.org/abs/2510.07239
- Reference count: 40
- Primary result: Achieves up to 100% ASR@10 on open-source models using bandit-guided LoRA experts

## Executive Summary
Red-Bandit introduces a test-time adaptation framework for LLM red-teaming that employs a multi-armed bandit to dynamically select among LoRA-trained attack-style experts. Each expert is specialized for a distinct adversarial style (e.g., role play, slang, historical) and trained via reinforcement learning with a rule-based prompt-safety reward. At inference, the bandit policy balances exploration and exploitation based on the target model's response safety, enabling efficient discovery of model-specific vulnerabilities. On the AdvBench dataset, Red-Bandit achieves state-of-the-art attack success rates (up to 100% ASR@10 on open-source models) while generating more human-readable prompts (lower perplexity). The bandit's selection distribution also serves as a diagnostic tool, revealing which attack styles each model is most susceptible to.

## Method Summary
Red-Bandit trains 10 LoRA experts, each specialized for a particular attack style (Slang, Role Play, Historical, etc.) using reinforcement learning with a rule-based safety reward model (Llama Guard-8B). The experts are trained on Mistral-7B using a GRPO variant that rewards unsafe prompt generation. At inference, a multi-armed bandit (UCB or ε-greedy) dynamically selects among these experts based on the target model's response safety, evaluated by Llama Guard-1B. The framework achieves high attack success rates (ASR@10) while maintaining prompt fluency and diversity, with the bandit's style distribution revealing model-specific vulnerabilities.

## Key Results
- Achieves up to 100% ASR@10 on open-source models (Llama-3.1-8B, Mixtral-8x7B, Vicuna-7B)
- Outperforms baseline methods in human-readability (lower perplexity) while maintaining attack effectiveness
- Bandits identify distinct model-specific vulnerabilities (e.g., GPT-3.5-turbo vulnerable to uncommon dialects and hypotheticals; GPT-4o to historical and role-play)
- Large-scale evaluation on AdvBench-v4 shows 75.5% ASR@10, demonstrating scalability

## Why This Works (Mechanism)

### Mechanism 1
Style-specialized LoRA experts trained with RL produce more effective and human-readable adversarial prompts than unified attackers. Each expert is conditioned on a specific attack style token during GRPO training, with a binary reward from a rule-based safety model indicating whether the generated prompt is unsafe. This creates specialized prompt generators rather than a monolithic attacker prone to mode collapse. Core assumption: Rule-based prompt-level safety rewards generalize across downstream LLMs. Break condition: If rule-based rewards systematically misclassify unsafe prompts, training signal degrades.

### Mechanism 2
Multi-armed bandit selection at inference enables test-time adaptation to model-specific vulnerabilities. Each LoRA expert is an arm; the bandit observes response-level safety rewards and updates selection probabilities. UCB explores less-sampled arms, while ε-greedy exploits high-reward arms with occasional exploration. Over multiple attempts, the policy converges toward attack styles that elicit unsafe responses from the specific target model. Core assumption: Target models have non-uniform vulnerabilities across attack styles. Break condition: If target models have uniform vulnerability across styles, bandit adaptation provides no advantage.

### Mechanism 3
Test-time adaptation outperforms transferability-based methods in black-box settings when sufficient exploration is allowed. Unlike transferability methods that optimize on a source model, Red-Bandit adapts selection online using only black-box response feedback. This allows discovery of target-specific vulnerabilities without gradients or log-probabilities. Core assumption: Binary safety feedback from a lightweight reward model is sufficient to guide adaptation without access to target model internals. Break condition: If the reward model systematically disagrees with ground-truth harm judgments, bandit updates follow incorrect gradients.

## Foundational Learning

- **Multi-Armed Bandits (UCB and ε-greedy)**: Core inference mechanism; must understand exploration-exploitation trade-off and how UCB confidence bounds differ from ε-greedy's fixed exploration rate. Quick check: Given three arms with empirical means [0.3, 0.5, 0.4] and play counts [10, 50, 5], which would UCB select (c=√2)? Which would ε-greedy select (ε=0.1)?

- **LoRA (Low-Rank Adaptation)**: Each attack-style expert is a LoRA adapter; must understand how LoRA injects trainable low-rank matrices into frozen base weights and why this enables efficient multi-expert training. Quick check: If base model has 7B parameters and LoRA rank=8 with alpha=32 on 4 attention projections, how many trainable parameters does one expert have?

- **GRPO (Group Relative Policy Optimization)**: RL algorithm for post-training; eliminates value network by normalizing rewards within groups of samples per prompt. Quick check: How does the advantage estimate in GRPO differ from PPO's advantage, and what does this remove from the training pipeline?

## Architecture Onboarding

- **Component map**: Base prompt generator (Mistral-7B) -> LoRA experts (10 adapters, one per attack style) -> Training reward model (Llama Guard-8B) -> Bandit policy (UCB or ε-greedy) -> Inference reward model (Llama Guard-1B) -> Target LLM

- **Critical path**: 1. Training: For each attack style → Initialize LoRA → Sample prompts with style token → Compute rule-based reward → Update via GRPO (1 epoch, 8 generations/step) 2. Inference: Receive harmful behavior → Bandit selects arm → Load corresponding LoRA → Generate adversarial prompt → Query target → Evaluate response safety → Update bandit statistics

- **Design tradeoffs**: UCB maintains broader style distribution; ε-greedy converges faster to dominant styles. Prompt-only rewards are more efficient and generalize better; hybrid adds marginal gains at higher cost. Number of experts (10 styles) balances coverage and training scalability.

- **Failure signatures**: Low ASR@1 with high ASR@10 indicates insufficient exploration; high perplexity prompts suggest style-conditioning failure; uniform style distribution means bandit not converging; disagreement with GPT-4o evaluation requires keyword matching instead of safety classifier.

- **First 3 experiments**: 1. Reproduce ablation (Table 4): Run Baseline, Red-Bandit (no RL), Red-Bandit (no bandit), and full method against Llama-3.1-8B. 2. Bandit convergence analysis: Plot cumulative regret and style distribution over time for UCB vs. ε-greedy on Vicuna-7B. 3. Reward model sensitivity: Swap Llama Guard-1B for alternative safety classifier and measure ASR change.

## Open Questions the Paper Calls Out

- **Composing multiple attack styles**: Does combining multiple attack styles within a single generation attempt yield higher attack success rates than single-expert selection? The current architecture restricts bandit to selecting a single LoRA expert per round.

- **Gray-box information utilization**: Can the bandit framework improve adaptability by utilizing partial gray-box information, such as token log-probabilities? Red-Bandit is currently designed for black-box settings relying solely on rule-based safety reward.

- **Attack primitive discovery**: Can the framework effectively discover model vulnerabilities tied to specific attack primitives (e.g., linguistic structures) rather than just semantic styles? The current study focuses exclusively on semantic categories like "slang" or "historical."

## Limitations

- Reliance on binary rule-based safety rewards that may not capture nuanced harmful content, particularly when attackers use stylistic obfuscation
- Potential blind spot where method could generate content that evades both reward model and ground-truth safety classifiers
- Assumption that rule-based rewards generalize across downstream LLMs not empirically validated beyond conducted experiments

## Confidence

- **ASR@10 Performance on Open-Source Models**: High confidence - consistent across multiple benchmarks with clear superiority over baselines
- **Model-Specific Vulnerability Diagnostics**: Medium confidence - compelling analysis but interpretation relies on potentially biased reward model
- **Test-Time Adaptation vs. Transferability**: Medium confidence - theoretical advantage sound but empirical validation limited to specific model pairs
- **Efficiency Claims**: Low-Medium confidence - pre-trained experts claim efficiency but multiple queries per instruction and training costs not fully characterized

## Next Checks

1. **Reward Model Ablation Study**: Replace Llama Guard-1B with alternative safety classifiers at inference and measure ASR@10 variance across target models to quantify sensitivity to reward model choice.

2. **Ground-Truth Harm Classification**: Manually audit successful attacks on GPT-4o using human evaluators or multiple independent safety classifiers to verify true vulnerability discovery versus evaluation pipeline exploitation.

3. **Exploration Efficiency Analysis**: Systematically vary number of bandit queries allowed (ASR@1, ASR@5, ASR@10) across different target models and plot trade-off curve between query efficiency and attack success rate.