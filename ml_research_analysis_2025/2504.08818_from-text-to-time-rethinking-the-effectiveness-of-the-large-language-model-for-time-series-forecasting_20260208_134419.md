---
ver: rpa2
title: From Text to Time? Rethinking the Effectiveness of the Large Language Model
  for Time Series Forecasting
arxiv_id: '2504.08818'
source_url: https://arxiv.org/abs/2504.08818
tags:
- time
- series
- backbone
- prediction
- datasets
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study addresses the effectiveness of large language models
  (LLMs) for time series forecasting, a topic of significant debate. It finds that
  training and testing LLM-based models on small datasets leads to the Encoder and
  Decoder becoming overly adapted to the data, masking the true predictive capabilities
  of the LLM backbone.
---

# From Text to Time? Rethinking the Effectiveness of the Large Language Model for Time Series Forecasting

## Quick Facts
- **arXiv ID**: 2504.08818
- **Source URL**: https://arxiv.org/abs/2504.08818
- **Reference count**: 23
- **Primary result**: LLM backbones provide limited forecasting benefits beyond what shallow MLPs can achieve, especially on small datasets

## Executive Summary
This study challenges the prevailing assumption that large language models (LLMs) offer significant advantages for time series forecasting. Through controlled experiments, the authors demonstrate that the apparent success of LLM-based forecasters on small datasets is primarily driven by overfitted encoder and decoder components rather than the LLM backbone itself. By introducing pre-training models with different strategies and evaluating zero-shot and few-shot performance, the research reveals that while LLM backbones show some promise, their forecasting performance is notably limited, particularly for complex long-term dependencies.

## Method Summary
The researchers employed a 12-layer GPT-2 backbone with non-overlapping patching (size 16) for univariate long-term time series forecasting. They introduced three model variants: Model A (frozen pre-trained GPT-2 with trained 2-layer MLP encoder/decoder), Model B (randomly initialized Transformer with encoder/decoder from Model A), and Model C (full parameter training). The method involved pre-training encoder/decoder components on a large-scale UTS dataset (2G version, approximately 100M samples) and then evaluating zero-shot and few-shot performance on seven benchmark datasets including ETTh1, ETTh2, ETTm1, ETTm2, Traffic, Electricity, and Exchange Rate.

## Key Results
- Training LLM-based forecasters on small datasets causes encoder and decoder components to overfit, masking the LLM backbone's true contribution
- LLM backbones process time series patches outside their native vocabulary distribution, relying on out-of-distribution generalization rather than semantic alignment
- A randomly initialized Transformer trained on approximately 50 million time series samples can achieve comparable performance to a frozen pre-trained GPT-2 backbone

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Training LLM-based forecasters on small datasets causes the Encoder and Decoder to overfit to local data patterns, potentially masking the actual (lack of) contribution from the frozen LLM backbone.
- **Mechanism:** The Encoder and Decoder (typically shallow MLPs) act as powerful adapters. When trained on limited data, they learn dataset-specific mappings that allow the model to achieve low error regardless of whether the intermediate LLM backbone adds value or functions as a simple pass-through.
- **Core assumption:** The "low error" observed in prior works is driven by the trainable peripheral layers rather than the pre-trained attention mechanisms of the LLM.
- **Evidence anchors:**
  - [abstract]: "...training and testing LLM-based models on small datasets often leads to the Encoder and Decoder becoming overly adapted..."
  - [Section 3 - Observation and Analysis]: "This finding highlights a critical issue: on small public datasets, the Encoder and Decoder can effectively adapt to the data for prediction without relying on additional pre-trained parameters..."
  - [corpus]: Weak. Corpus neighbors focus on *improving* LLM performance (e.g., "LLM-PS", "FiCoTS"), assuming the backbone is beneficial, whereas this paper challenges that premise.
- **Break condition:** If the dataset size is sufficiently large (e.g., >50M samples), the peripheral layers cannot memorize the data distribution, forcing the backbone to learn generalizable features.

### Mechanism 2
- **Claim:** LLMs process time series tokens via "out-of-distribution" generalization rather than by aligning numerical patches with their pre-trained text vocabulary.
- **Mechanism:** Time series patches are mapped to embedding spaces distinct from the LLM's native vocabulary (evidenced by t-SNE separation). The model relies on the Transformer's general capacity to process sequence structure, rather than retrieving specific semantic knowledge associated with text tokens.
- **Core assumption:** Visual separation in t-SNE dimensions correlates with a functional separation in how the model processes the tokens (i.e., treating them as "foreign" inputs).
- **Evidence anchors:**
  - [Section 5.3 - Question 1]: "...the encoder-decoder adapted for the LLM maps time series patches into areas outside the distribution of the vocabulary..."
  - [Figure 5(a)]: Demonstrates a clear boundary between GPT-2 vocabulary tokens and time series dataset tokens.
  - [corpus]: Not supported. Neighbors like *FiCoTS* and *LLM-PS* often explicitly try to map semantics/text to time series to *boost* performance, contradicting the finding that alignment degrades it.
- **Break condition:** If a specific modality alignment technique (e.g., specialized text prototypes) successfully bridges the embedding gap without performance degradation.

### Mechanism 3
- **Claim:** The "knowledge" encoded in a text-pretrained LLM backbone is roughly equivalent to a Transformer trained from scratch on only 15–50 million time series samples.
- **Mechanism:** Text pre-training provides a limited initialization bias for numerical time series tasks. A randomly initialized Transformer (Model B) rapidly converges to the performance of a frozen pre-trained LLM (Model A) with modest TS-specific training, suggesting the text prior is weak.
- **Core assumption:** The performance intersection point of a frozen LLM and a random Transformer trained on $N$ samples is a valid metric for "knowledge equivalence."
- **Evidence anchors:**
  - [Section 5.3 - Question 3]: "...a Transformer backbone trained from scratch on approximately 50 million time-series samples can achieve predictive performance comparable to that of a frozen GPT-2 backbone."
  - [Figure 6]: Visualizes the scaling curves where Model B catches up to Model A.
  - [corpus]: No direct support. Related papers generally advocate for *more* pre-training or complex alignment, not quantifying the low equivalent value of text pre-training.
- **Break condition:** If the forecasting task requires high-level semantic reasoning (e.g., "predict stock price based on news sentiment") rather than pure numerical pattern matching, where text knowledge might hold higher value.

## Foundational Learning

- **Concept**: **Channel Independence (CI)**
  - **Why needed here**: The paper isolates temporal modeling from variable interactions by setting $C=1$ (univariate). Understanding CI is necessary to replicate the "unbiased" evaluation of the LLM backbone.
  - **Quick check question**: Does the model process all variables jointly (Channel Mixing) or treat each variable as a separate sequence (Channel Independence)?

- **Concept**: **Patch Tokenization**
  - **Why needed here**: The architecture relies on aggregating time steps into "patches" (tokens) to match the LLM's input dimension. This is the core interface between the time series and the LLM.
  - **Quick check question**: How does the patch size (e.g., 96 steps) affect the sequence length and the model's ability to capture high-frequency noise vs. long-term trends?

- **Concept**: **Zero-Shot vs. Few-Shot Forecasting**
  - **Why needed here**: The study critiques standard "train-then-test" and advocates for zero-shot (direct inference) evaluation to decouple the Encoder's overfitting from the Backbone's true capability.
  - **Quick check question**: In a zero-shot test, is the model seeing the target dataset's distribution during any training phase?

## Architecture Onboarding

- **Component map**: Input Layer -> Patch Encoder -> Backbone -> Patch Decoder -> Output
- **Critical path**: Pre-train Encoder/Decoder on large-scale UTS dataset → Freeze (Model A) or Re-initialize (Model B) Backbone → Evaluate Zero-Shot on target dataset
- **Design tradeoffs**:
  - **Frozen vs. Fine-tuned Backbone**: Freezing allows evaluation of pre-trained knowledge; fine-tuning obscures it by adapting weights to local noise
  - **Vocabulary Alignment**: Aligning TS tokens to text vocabulary (via Cross-Attention) forces the model into a space it wasn't effectively using, causing performance drops (Section 5.3)
- **Failure signatures**:
  - **"The Small Data Illusion":** Achieving SOTA performance on small datasets (ETTh1/ETTm1) with a frozen LLM, only to find that a "Diagonal" (identity) backbone performs identically
  - **Semantic Misalignment:** Forcing token alignment and seeing MSE spike (e.g., ETTh1 MSE rises from 0.476 to 0.782)
- **First 3 experiments**:
  1. **Sanity Check (DI-GPT):** Replace the LLM backbone with a diagonal matrix (pass-through network). Train only the Encoder/Decoder on a small dataset. If performance matches the full LLM, the LLM is not contributing
  2. **Vocabulary Visualization:** Extract embeddings from the Patch Encoder and GPT-2 Vocabulary. Plot t-SNE. Verify they form distinct, non-overlapping clusters
  3. **Scaling Law Intersection:** Train a randomly initialized Transformer on increasing subsets of time series data (15m, 30m, 50m samples) and plot error curves against the frozen GPT-2 baseline to find the crossover point

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can a cross-attention mechanism be designed that successfully aligns time series tokens with the LLM's pre-trained vocabulary to improve performance, rather than degrading it?
- Basis in paper: [explicit] Section 5.3 ("Question 1") concludes that current alignment methods fail, causing the model to rely on out-of-distribution generalization which lowers prediction accuracy
- Why unresolved: The paper demonstrates that naively mapping tokens to vocabulary items hurts performance, but does not propose a method to bridge this semantic gap effectively
- What evidence would resolve it: A token alignment strategy that yields lower MSE than the baseline non-aligned Model A in a zero-shot setting

### Open Question 2
- Question: Does the "equivalent time series sample" threshold (approx. 15–50 million samples) required to match GPT-2's performance hold constant for larger LLM backbones?
- Basis in paper: [inferred] The paper quantifies this specific trade-off for GPT-2 (small) in Figure 6, but notes that improvements may require "substantially more time series data" without testing larger models
- Why unresolved: The empirical scaling laws derived in the study are strictly bound to the 12-layer GPT-2 architecture
- What evidence would resolve it: Experiments replicating the Figure 6 analysis using larger pre-trained backbones (e.g., LLaMA-7B) to see if the data efficiency ratio improves

### Open Question 3
- Question: Do the limitations observed in patch-based LLM forecasting apply to the alternative paradigm of representing time series as direct numeric strings?
- Basis in paper: [explicit] The Introduction explicitly categorizes approaches into "patching" (studied here) and "numeric character" methods, stating the study focuses only on the second paradigm
- Why unresolved: The controlled experiments using unbiased encoders/decoders were not applied to the numeric string input method, leaving its efficacy unverified
- What evidence would resolve it: Zero-shot performance metrics comparing the patch-based approach against the numeric character approach using the paper's large-scale pre-training methodology

## Limitations
- Conclusions are based on univariate forecasting with fixed patch sizes and horizons, limiting generalizability to multivariate settings
- The "equivalent sample" threshold (50M samples) is specific to GPT-2 Small and may not hold for larger LLM backbones
- The exact mechanism by which the LLM backbone fails to contribute meaningfully beyond shallow MLPs remains incompletely characterized

## Confidence

- **High Confidence**: The observation that small datasets allow Encoder/Decoder components to overfit, masking the LLM backbone's contribution (demonstrated through DI-GPT control experiment)
- **Medium Confidence**: The claim that LLM backbones process time series patches outside their native vocabulary distribution (supported by t-SNE visualizations but functional significance needs validation)
- **Low Confidence**: The quantitative equivalence claim (50M samples ≈ text-pretrained LLM knowledge) as it's specific to the tested architecture

## Next Checks

1. **Multivariate Extension**: Repeat the pre-training experiments with multivariate time series (C>1) to test whether the LLM backbone's limitations persist when channel interactions are present
2. **Cross-Domain Semantic Reasoning**: Evaluate the LLM backbone on time series forecasting tasks requiring integration of textual information (e.g., stock prices with news sentiment) to test whether text knowledge provides value beyond numerical pattern matching
3. **Architecture Scaling**: Test the same experimental protocol with larger LLM backbones (GPT-2 Medium/Large) to determine whether the "limited effectiveness" finding scales with model size or represents a fundamental constraint of the approach