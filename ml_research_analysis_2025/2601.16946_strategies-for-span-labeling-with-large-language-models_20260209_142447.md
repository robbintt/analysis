---
ver: rpa2
title: Strategies for Span Labeling with Large Language Models
arxiv_id: '2601.16946'
source_url: https://arxiv.org/abs/2601.16946
tags:
- text
- span
- input
- output
- spans
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper explores how to apply large language models to span
  labeling tasks, where text spans need to be identified and categorized. It identifies
  three main prompting strategies: tagging, indexing, and matching, and introduces
  LOGITMATCH, a constrained decoding method that improves span matching by enforcing
  valid input spans during generation.'
---

# Strategies for Span Labeling with Large Language Models

## Quick Facts
- arXiv ID: 2601.16946
- Source URL: https://arxiv.org/abs/2601.16946
- Reference count: 40
- Key outcome: LOGITMATCH constrained decoding improves span matching by enforcing valid input spans, achieving strong performance especially on non-standard inputs

## Executive Summary
This paper explores three main prompting strategies for span labeling with LLMs: tagging, indexing, and matching. The authors introduce LOGITMATCH, a constrained decoding method that enforces output spans to align with valid input text by manipulating model logits. Experiments across four tasks (NER, GEC, ESA-MT, CPL) with multiple models show that while tagging is the most robust baseline, LOGITMATCH successfully improves matching-based approaches by eliminating span alignment issues, achieving strong performance especially on non-standard inputs.

## Method Summary
The paper investigates span labeling strategies for LLMs, focusing on three approaches: tagging (embedding spans in XML-like tags), indexing (specifying character indices), and matching (generating JSON with span text). LOGITMATCH is introduced as a constrained decoding method that modifies model logits during generation to restrict span content to valid input tokens. The method operates in three modes: DEFAULT (unconstrained JSON structure), SELECT (limit vocabulary to input tokens for span start), and COPY (allow only next input token or closing quote). Experiments use datasets including UniversalNER (7,523 examples), MultiGEC English (504 examples), WMT24 ESA data (867 examples), and a synthetic CPL dataset (1,000 examples).

## Key Results
- LOGITMATCH successfully reduces span alignment errors to near-zero levels compared to unconstrained matching strategies
- Tagging strategy shows the most robust performance across all tasks due to its natural error tolerance
- Structured output variants (-S methods) significantly reduce parsing errors from ~5% to ~0.6% while maintaining F1 scores
- Occurrence_index fields provide 30-40 percentage point improvements on CPL task with repeated span patterns

## Why This Works (Mechanism)

### Mechanism 1: Constrained Decoding via Logit Manipulation
- Claim: Constrained decoding eliminates span alignment errors by restricting the model's vocabulary to only tokens present in the input text during span generation.
- Mechanism: During decoding, LOGITMATCH modifies raw model logits to mask out any token not found in the input. When generating a span's "text" field, the model can only select tokens that form valid input substrings, guaranteeing post-hoc matching succeeds.
- Core assumption: The model's learned distribution still meaningfully ranks valid tokens even when invalid ones are masked; the constraint does not catastrophically distort the output space.
- Evidence anchors:
  - [abstract] "LogitMatch...forces the model's output to align with valid input spans"
  - [Section 4.2] "In SELECT mode, we limit the model's vocabulary to the tokens from the input... and let the model generate a single token xi starting a span"
  - [corpus] Limited direct evidence; neighbor papers focus on span detection tasks rather than constrained decoding methods.
- Break condition: If the model's true intended span contains tokens absent from input (e.g., due to tokenization differences or encoding errors), the constraint prevents correct output entirely.

### Mechanism 2: Multi-Mode Decoding State Machine (DEFAULT → SELECT → COPY)
- Claim: Explicit state tracking during generation enables context-aware vocabulary constraints.
- Mechanism: The algorithm operates in three phases—DEFAULT (unconstrained JSON structure), SELECT (choose span start from input tokens), COPY (extend span with adjacent input tokens or terminate). This ensures syntactic structure remains unconstrained while span content is strictly grounded.
- Core assumption: JSON field boundaries can be reliably detected during streaming decoding, and tokenization quirks (e.g., quotes merged with content) can be handled via prefix matching.
- Evidence anchors:
  - [Section 4.2] Describes the three modes and transition conditions explicitly
  - [Section 4.3] "We need to account for the fact that tokenization of the output text does not necessarily correspond to the input text tokenization"
  - [corpus] No corpus papers address this specific state-machine approach.
- Break condition: If the model generates malformed JSON (e.g., missing quote, extra bracket), the state machine may misidentify field boundaries and apply constraints to wrong positions.

### Mechanism 3: Tagging as Robust Baseline via Closest-Occurrence Matching
- Claim: Tagging strategies achieve robustness through natural error tolerance in post-processing alignment.
- Mechanism: Models generate tagged text with spans embedded. Even with minor copy errors, a two-stage alignment (offset estimation → closest-match fallback) recovers correct positions. This provides implicit fuzzy matching without requiring explicit constraints.
- Core assumption: Models can approximately copy input text with high fidelity; errors are local perturbations rather than structural hallucinations.
- Evidence anchors:
  - [Section 3.4] "LLMs often 'fix' the text by correcting typos, changing capitalization, or correcting factually incorrect sentences"
  - [Appendix B.2] "We treat this calculated position as a heuristic rather than an absolute truth. If the text at the expected location does not match...we search the full text for the span"
  - [corpus] Weak direct support; neighbor papers focus on span detection performance but not on robustness mechanisms.
- Break condition: When models systematically rewrite rather than copy input (e.g., GEC tasks with extensive edits), closest-match may resolve to wrong span.

## Foundational Learning

- Concept: Autoregressive decoding with vocabulary constraints
  - Why needed here: LOGITMATCH operates by intercepting the token selection process; understanding logit masking, top-k/top-p sampling, and how constraints compose with these is essential.
  - Quick check question: Given logits [2.1, 1.5, 0.8, 0.3] for tokens ["the", "a", "cat", "dog"], what is the probability of "cat" after masking ["the", "dog"]?

- Concept: Tokenization mismatch between input and output
  - Why needed here: LLMs tokenize prompt and output independently; "Hello." might be ["Hello", "."] as input but decode via "Hel" → "lo" → ".". Constrained decoding must account for multi-token input tokens.
  - Quick check question: If input is tokenized as ["world"] but the model first decodes "worl", what tokens must be whitelisted to complete this span validly?

- Concept: Span-level evaluation metrics with overlap tolerance
  - Why needed here: The paper uses overlap-adjusted F1 (proportional credit for partial matches). Understanding this metric is necessary to interpret results and design experiments.
  - Quick check question: If a gold span is [10:20] and predicted span is [12:22], what is the precision contribution? What is the recall contribution?

## Architecture Onboarding

- Component map: Prompt Template -> Decoding Layer (with LogitsProcessor) -> State Machine -> Token Whitelist Generator -> Post-Processor
- Critical path: 1) Construct prompt with format specification 2) Initialize decoding with state = DEFAULT 3) Detect `"text": "` → switch to SELECT 4) After single token → switch to COPY 5) Loop until quote or max tokens 6) Parse output, compute overlap-adjusted F1
- Design tradeoffs:
  - **Tagging vs. LOGITMATCH**: Tagging is O(n) output tokens (copies full text) but naturally handles errors; LOGITMATCH is O(m) (only spans) but requires logit access (excludes API-only models like GPT-5-mini)
  - **Structured output (-S flag)**: Reduces parsing errors from ~5% to ~0.6% but may suppress beneficial spontaneous chain-of-thought
  - **Occurrence index field**: Resolves ambiguity for repeated spans but adds counting burden to model; see CPL task improvements of 30-40 percentage points
- Failure signatures:
  - **LOGITMATCH produces empty spans**: Check tokenization alignment—prefix matching logic may be incorrectly filtering valid tokens
  - **High parsing error rates (>5%)**: Likely JSON schema mismatch; verify structured output configuration or prompt format
  - **Indexing gives indices outside text bounds**: Expected behavior without INDEX-ENRICHED; models lack explicit position access
  - **Reasoning models hit max_tokens**: Model enumerating characters in reasoning trace; increase limit or truncate reasoning
- First 3 experiments:
  1. **Baseline comparison across strategies**: Run TAG, INDEX, MATCH, LOGITMATCH on NER subset with single model (Qwen3-8B). Compute both hard and soft F1. Verify LOGITMATCH reduces span matching errors to near-zero.
  2. **Ablate structured output flag**: Compare MATCH vs. MATCH-S and LOGITMATCH vs. LOGITMATCH-S. Measure parsing error rate reduction vs. any F1 degradation from suppressed reasoning.
  3. **Stress test with repeated spans**: Run CPL task with MATCH-OCC vs. LOGITMATCH-OCC. Confirm occurrence_index provides 30+ point F1 improvement when input contains multiple identical patterns.

## Open Questions the Paper Calls Out

- **Can the advantages of tagging, indexing, and matching strategies be effectively combined into a unified span labeling approach?**
  - Basis in paper: [explicit] The conclusion states: "Future work might explore how to combine the advantages of individual methods and how prompting with different examples and instructions influences the efficiency of each method."
  - Why unresolved: Each strategy has distinct trade-offs (tagging is robust but O(n) tokens; matching is token-efficient but has alignment issues; indexing requires explicit indices). No hybrid approach was tested.
  - What evidence would resolve it: Experiments with hybrid methods that leverage tagging's robustness for critical spans while using matching or indexing for others, compared across the four benchmark tasks.

- **Can attention values from LLMs be used to disambiguate multiple span occurrences more reliably than occurrence_index fields?**
  - Basis in paper: [explicit] In Limitations: "Future work might thus explore other approaches, for example, disambiguating the span using attention values."
  - Why unresolved: The occurrence_index approach relies on model counting abilities, which are unreliable. Attention patterns may provide implicit grounding but were not investigated.
  - What evidence would resolve it: Analysis of attention distributions over input tokens when models generate span predictions, with comparison to oracle disambiguation performance on the CPL task.

- **Can LOGITMATCH-style constrained decoding be successfully applied to tagging strategies to ensure exact input reproduction?**
  - Basis in paper: [explicit] In Limitations: "In principle, a variant of LOGITMATCH is also applicable to tagging methods... However, our preliminary experiments ran into severe issues with tokenization handling, so we abandoned this approach."
  - Why unresolved: Tokenization at boundaries between outer text, tags, and inner text caused performance degradation. The technical challenge remains unsolved.
  - What evidence would resolve it: A working implementation that handles multi-token boundary cases (e.g., `"London` spanning a quote and span content), evaluated against standard tagging baselines.

## Limitations

- The method requires access to model logits, excluding popular API-only models like GPT-4 or Claude
- Tagging strategy's robustness may break down on tasks with extensive text rewriting (like GEC)
- The evaluation focuses on overlap-adjusted F1 without analyzing precision-recall trade-offs or false positive patterns
- The approach's effectiveness for open-ended generation or tasks with different structural requirements remains untested

## Confidence

- **High confidence** in the core empirical findings: The experiments across four diverse tasks with multiple models consistently show LOGITMATCH outperforming unconstrained matching strategies
- **Medium confidence** in the mechanism explanation: While the logit masking and state machine descriptions are technically sound, the paper lacks ablation studies showing individual constraint contributions
- **Low confidence** in generalizability claims: The paper suggests LOGITMATCH would work for any task requiring grounded generation, but the evaluation is limited to four specific span labeling tasks

## Next Checks

1. **Tokenization boundary stress test**: Design a dataset where input tokens frequently contain quote characters or other structural delimiters (e.g., "Hello" → ["Hel", 'lo"]). Run LOGITMATCH on this data to verify the prefix-matching logic correctly handles edge cases where tokens span field boundaries.

2. **Constraint ablation study**: Modify LOGITMATCH to operate in only SELECT mode (whitelist input tokens) but skip COPY mode (allow any token). Compare F1 scores and span alignment error rates against full LOGITMATCH to quantify the contribution of the copy constraint versus the selection constraint alone.

3. **API-only model comparison**: Implement a lightweight "LOGITMATCH-light" that uses input string matching instead of logit masking (reject any generated span not found in input text). Run this on GPT-5-mini and compare against the paper's reported results to estimate the performance gap between true constrained decoding and heuristic validation.