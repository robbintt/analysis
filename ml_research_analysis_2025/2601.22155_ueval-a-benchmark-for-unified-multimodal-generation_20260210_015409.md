---
ver: rpa2
title: 'UEval: A Benchmark for Unified Multimodal Generation'
arxiv_id: '2601.22155'
source_url: https://arxiv.org/abs/2601.22155
tags:
- image
- generation
- text
- tasks
- multimodal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: UEval is a benchmark for unified multimodal generation that requires
  models to produce both images and text in response to a single query. It consists
  of 1,000 expert-curated questions from 8 real-world tasks, with 10,417 rubric criteria
  for evaluation.
---

# UEval: A Benchmark for Unified Multimodal Generation

## Quick Facts
- **arXiv ID**: 2601.22155
- **Source URL**: https://arxiv.org/abs/2601.22155
- **Reference count**: 19
- **One-line primary result**: Current unified models struggle with UEval's multimodal generation tasks, with best scores around 66.4/100 and significant performance gaps between proprietary and open-source models.

## Executive Summary
UEval is a benchmark for unified multimodal generation that requires models to produce both images and text in response to a single query. It consists of 1,000 expert-curated questions from 8 real-world tasks, with 10,417 rubric criteria for evaluation. Unlike previous benchmarks, UEval uses a data-dependent rubric-based scoring system refined by human experts to enable fine-grained automatic evaluation. Experiments show UEval is challenging for current unified models, with GPT-5-Thinking achieving the highest score of 66.4/100 and the best open-source model reaching only 49.1. The benchmark reveals that reasoning models significantly outperform non-reasoning ones, and that reasoning traces can substantially improve multimodal generation quality when transferred to non-reasoning models.

## Method Summary
UEval evaluates unified multimodal models through 1,000 expert-curated questions spanning 8 real-world tasks (space, textbook, paper, diagram, art, life, tech, exercise). Each question has reference image-text pairs and associated rubric criteria (10,417 total) generated by Gemini-2.5-Pro and refined by human experts. The evaluation pipeline uses a frontier MLLM (Gemini-2.5-Pro) to grade responses against rubrics, scoring the fraction of satisfied criteria. For models unable to generate both modalities in one pass, inference is run separately for image and text outputs. The benchmark employs data-dependent rubrics that are specific to each question and reference answer, enabling fine-grained automatic scoring while avoiding the limitations of generic evaluation prompts.

## Key Results
- Current unified models struggle with UEval, with GPT-5-Thinking scoring 66.4/100 and the best open-source model (Qwen3-VL-235B-Thinking) scoring 49.1/100
- Reasoning models significantly outperform non-reasoning models, with the average score gap exceeding 15 points
- Transfer of reasoning traces from reasoning models to non-reasoning models narrows the performance gap by approximately 5-8 points
- Image generation scores are consistently lower than text generation scores, with open-source models showing a gap of over 20 points on average
- Multi-step tasks reveal temporal consistency failures, with models struggling to maintain consistent labeling and orientation across sequential images

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Explicit reasoning traces improve multimodal generation quality, but only for models with sufficient baseline capability.
- Mechanism: Reasoning traces decompose complex queries into intermediate steps, providing structured guidance that helps models plan visual and textual outputs coherently. However, weaker models cannot effectively leverage these signals due to limited generation capacity.
- Core assumption: The quality improvement from reasoning traces depends on the model's ability to interpret and execute detailed instructions.
- Evidence anchors:
  - [abstract] "transferring reasoning traces from a reasoning model to a non-reasoning model significantly narrows the gap"
  - [section] Figure 9 shows GPT-5-Instant and Gemini-2.5-Flash improve with added reasoning, but BAGEL shows no improvement
  - [corpus] "Thinking with Text" and "Thinking with Images" paradigms improve reasoning (arXiv:2511.04570)
- Break condition: Models with very low baseline multimodal generation capability (<35 average score) cannot effectively use reasoning traces.

### Mechanism 2
- Claim: Data-dependent rubrics enable more reliable automated evaluation of open-ended multimodal outputs than generic prompts.
- Mechanism: By generating rubric criteria conditioned on each specific question and reference answer, evaluation captures task-relevant nuances. Human refinement removes redundancies and adds missing criteria, ensuring rubrics accurately reward correct responses.
- Core assumption: A frontier MLLM can reliably judge whether responses satisfy fine-grained rubric criteria.
- Evidence anchors:
  - [abstract] "10,417 validated rubric criteria, enabling scalable and fine-grained automatic scoring"
  - [section] Figure 8 shows r=0.88 correlation between LLM judge and human evaluators
  - [corpus] Weak corpus evidence; related benchmarks use data-independent approaches
- Break condition: Judge model must be capable (GPT-5-Thinking, Gemini-2.5-Pro, or Qwen3-VL-235B-Thinking recommended; Table 3 shows other models produce inconsistent scores).

### Mechanism 3
- Claim: Multi-step planning tasks expose temporal consistency failures that single-image tasks do not reveal.
- Mechanism: Tasks requiring sequential image generation (e.g., "draw a cat step by step") demand maintaining consistent labeling, orientation, and progression across images—a capability current unified models lack.
- Core assumption: Temporal consistency requires explicit step-tracking mechanisms not present in current architectures.
- Evidence anchors:
  - [abstract] "current models struggle to generate multiple images with consistent labeling across steps"
  - [section] Figure 3 shows GPT-5-Thinking mislabels steps, Gemini-2.5-Flash changes shirt orientation inconsistently
  - [corpus] No direct corpus evidence on temporal consistency in multimodal generation
- Break condition: Single-image tasks do not expose this failure mode; multi-step tasks are required.

## Foundational Learning

- Concept: **Unified Multimodal Models**
  - Why needed here: UEval specifically targets models that natively generate both text and images, not separate pipelines.
  - Quick check question: Can you name two architectural approaches for unifying understanding and generation (hint: autoregressive vs. diffusion)?

- Concept: **Chain-of-Thought Reasoning**
  - Why needed here: The paper's key finding is that reasoning traces improve multimodal generation, building on CoT literature.
  - Quick check question: What is the core idea of Chain-of-Thought prompting from Wei et al. 2022?

- Concept: **LLM-as-a-Judge Evaluation**
  - Why needed here: UEval uses a frontier MLLM to grade responses against rubrics; understanding limitations is critical.
  - Quick check question: Why might a generic evaluation prompt fail to capture task-specific nuances?

## Architecture Onboarding

- Component map: Question + Reference Answer → Rubric Generator (Gemini-2.5-Pro) → Human-refined Rubrics → Model Response → Judge Model → Score (fraction of satisfied criteria)

- Critical path: (1) Rubric quality validation (human review essential); (2) Judge model selection (use only validated judges); (3) Response collection (joint vs. separate image-text generation)

- Design tradeoffs: Data-dependent rubrics require upfront annotation cost but yield more reliable evaluation than data-independent prompts. Closed-ended tasks use fact-checking rubrics; open-ended tasks use higher-level quality rubrics (Figure 5).

- Failure signatures: (1) Temporal inconsistency in multi-step tasks (step mislabeling, orientation changes); (2) Image-text gap (Table 4: image scores ~20-30 points lower than text for open-source models); (3) Judge model inconsistency (Table 3: some judges produce inflated scores on open-ended tasks)

- First 3 experiments:
  1. Establish baseline: Run your unified model on all 8 UEval tasks, compute separate image and text scores.
  2. Test reasoning transfer: Add GPT-5-Thinking reasoning traces to your model's prompts; measure improvement gap.
  3. Validate judge alignment: Sample 10% of outputs, compare your judge model scores against human evaluation (target: >85% rubric agreement).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the evaluation of unified multimodal generation be standardized to avoid data-independent pitfalls while accounting for judge model biases?
- Basis in paper: [explicit] Page 6 states, "How to effectively evaluate unified multimodal generation remains an open problem," noting that generic prompts overlook sample-specific differences.
- Why unresolved: While UEval introduces rubrics, Table 3 shows scores vary significantly depending on the choice of the judge model (e.g., Seed1.6-Vision vs. Gemini-2.5-Pro).
- What evidence would resolve it: A benchmark where different frontier judge models achieve statistically insignificant variance in their scoring of the same model outputs.

### Open Question 2
- Question: Why does the injection of external reasoning traces improve visual generation for proprietary models but fail to benefit open-source models like BAGEL?
- Basis in paper: [inferred] Page 11 observes that transferring traces improves GPT-5-Instant and Gemini-2.5-Flash, but "weaker models (e.g., BAGEL) do not benefit."
- Why unresolved: The authors hypothesize a threshold of "strong multimodal generation capability" is needed to utilize reasoning, but the specific mechanism or deficiency in weaker models remains uninvestigated.
- What evidence would resolve it: An ablation study analyzing cross-attention activation in weaker models when processing reasoning traces versus standard prompts.

### Open Question 3
- Question: What specific architectural limitations cause current unified models to fail at maintaining temporal consistency in multi-step visual planning?
- Basis in paper: [inferred] Page 8 notes that "current models struggle to generate multiple images with consistent labeling across steps," citing examples where models mislabel sequential images (e.g., two images tagged "step 5").
- Why unresolved: The paper identifies this failure mode in open-ended tasks but does not propose architectural solutions to maintain state across multiple generated images.
- What evidence would resolve it: A unified model architecture that can natively generate interleaved image-text sequences with strict adherence to temporal indices.

## Limitations

- Judge model selection significantly impacts evaluation reliability, with Table 3 showing score inflation when using suboptimal judges for open-ended tasks
- Temporal consistency failure mode lacks quantitative metrics beyond qualitative examples, making systematic measurement difficult
- Human refinement process details remain sparse, unclear whether the same experts validated both rubric generation and final evaluation pipeline

## Confidence

- **High Confidence**: Data-dependent rubric generation improves evaluation specificity compared to generic prompts (supported by strong r=0.88 agreement with human evaluators)
- **Medium Confidence**: Reasoning traces improve multimodal generation quality for capable models (supported by observed improvements in GPT-5-Instant and Gemini-2.5-Flash, but BAGEL shows no improvement)
- **Low Confidence**: Temporal consistency is a fundamental architectural limitation (evidence limited to qualitative examples without systematic measurement)

## Next Checks

1. **Judge Model Robustness Test**: Run the same 100 UEval responses through all three recommended judge models (GPT-5-Thinking, Gemini-2.5-Pro, Qwen3-VL-235B-Thinking) and compute inter-judge correlation coefficients. Target: r > 0.8 between all pairs for reliable evaluation.

2. **Temporal Consistency Quantification**: For the 100 multi-step task responses from top-performing models, develop an automated metric that measures: (a) step label consistency (correct labeling of sequential steps), (b) object orientation consistency (angular deviation between consecutive images), and (c) progression coherence (whether each step logically follows from the previous).

3. **Reasoning Transfer Boundary Test**: Systematically test reasoning trace transfer across models with varying baseline capabilities (e.g., high-performing vs. mid-tier unified models). Measure the correlation between baseline multimodal generation score and reasoning improvement magnitude to quantify the "sufficient baseline capability" threshold mentioned in Mechanism 1.