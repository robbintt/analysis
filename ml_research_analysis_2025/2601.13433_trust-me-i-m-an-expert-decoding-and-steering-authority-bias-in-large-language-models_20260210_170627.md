---
ver: rpa2
title: 'Trust Me, I''m an Expert: Decoding and Steering Authority Bias in Large Language
  Models'
arxiv_id: '2601.13433'
source_url: https://arxiv.org/abs/2601.13433
tags:
- arxiv
- bias
- reasoning
- language
- endorsement
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper investigates how large language models (LLMs) exhibit\
  \ authority bias\u2014being disproportionately influenced by suggestions from sources\
  \ perceived as highly credible. Using multiple-choice question datasets across math,\
  \ legal, and medical domains, the authors systematically vary the expertise level\
  \ of persona-provided endorsements (e.g., high school student to professor) and\
  \ observe that models become more susceptible to both correct and incorrect endorsements\
  \ as source authority increases."
---

# Trust Me, I'm an Expert: Decoding and Steering Authority Bias in Large Language Models

## Quick Facts
- arXiv ID: 2601.13433
- Source URL: https://arxiv.org/abs/2601.13433
- Authors: Priyanka Mary Mammen; Emil Joswin; Shankar Venkitachalam
- Reference count: 7
- Key outcome: Large language models exhibit authority bias, becoming increasingly susceptible to both correct and incorrect endorsements as source expertise increases, with high-authority incorrect endorsements also increasing confidence in wrong answers. A steering vector extracted from residual stream activations can mitigate this bias.

## Executive Summary
This paper systematically demonstrates that large language models are disproportionately influenced by suggestions from sources perceived as highly credible, exhibiting authority bias across math, legal, and medical domains. Using persona-provided endorsements varying from high school students to professors, the authors show that models not only become more accurate with correct endorsements from high-authority sources but also more confidently wrong when given incorrect endorsements from such sources. Critically, the study reveals that this bias can be both measured and mitigated through representation engineering—specifically by extracting and manipulating a steering vector from the model's residual stream that captures the "expertise" direction.

## Method Summary
The authors evaluate 11 models (5 reasoning, 6 non-reasoning) on four MCQ datasets (AQUA-RAT, LEXam, MedMCQA, MedQA) using prompts with four expertise tiers per domain. They measure authority bias through delta accuracy and delta entropy under correct and incorrect endorsements. To mitigate bias, they extract a steering vector by computing mean activation differences between highest and lowest expertise persona responses in the residual stream, then subtract this vector during inference on misleading endorsements to reduce susceptibility.

## Key Results
- Models show progressively larger accuracy gains with correct endorsements and larger accuracy degradation with incorrect endorsements as source expertise increases from high school students to professors.
- High-authority incorrect endorsements not only degrade accuracy but also increase the model's confidence in wrong answers, as measured by negative delta entropy.
- Mathematical reasoning tasks show the highest susceptibility to authority bias.
- Subtracting the extracted expertise steering vector from residual stream activations reduces authority bias and improves performance under misleading endorsements.

## Why This Works (Mechanism)

### Mechanism 1: Hierarchical Authority Encoding
Models encode source expertise as a continuous scalar influence weight that modulates endorsement adoption proportionally to perceived authority level. The gradient effect across four-tier expertise hierarchies demonstrates this is not binary processing.

### Mechanism 2: Confidence-Confusion Coupling Under Expert Misinformation
High-authority incorrect endorsements cause models to become more confident in wrong answers through entropy reduction in output distributions, creating "confident errors" where the model's internal confidence calibration is corrupted by the authority signal.

### Mechanism 3: Residual Stream Steering via Expertise Vector
Authority bias is mechanistically localized in the residual stream and can be controlled via vector arithmetic. The steering vector captures causally relevant directions representing expertise, allowing intervention at inference time to amplify or suppress the model's sensitivity to persuasive power of expert persona.

## Foundational Learning

- **Representation Engineering / Activation Steering**: Why needed - The paper's mitigation approach requires understanding how to read and modify residual stream activations at inference time. Quick check - Can you explain how adding a vector to residual stream activations differs from fine-tuning or prompting?

- **Entropy as Confidence Proxy**: Why needed - The paper uses delta entropy (ΔH) as a core metric for confidence changes. Quick check - If a model's output distribution over four MCQ options shifts from [0.25, 0.25, 0.25, 0.25] to [0.7, 0.1, 0.1, 0.1], what happened to entropy and what does this imply about confidence?

- **Persona-Based Prompting**: Why needed - The experimental design relies on systematically varying persona expertise in prompts while keeping content fixed. Quick check - Why is it methodologically important that the endorsement content remains identical across persona conditions?

## Architecture Onboarding

- **Component map**: Prompt template with persona slot → MCQ dataset loader → Inference harness with activation hooks → Steering vector extraction module → Evaluation metrics
- **Critical path**: 1) Collect contrastive pairs of highest vs. lowest expertise responses, 2) Extract activations at target layers during forward pass, 3) Compute steering vector as mean activation difference, 4) Apply vector (add/subtract) to residual stream during inference, 5) Evaluate accuracy and entropy changes
- **Design tradeoffs**: Layer selection (middle layers optimal but varies by architecture), vector magnitude scaling (requires tuning), domain specificity (general vs. specific vectors)
- **Failure signatures**: Baseline accuracy degradation when steering applied (over-correction), no effect on authority bias (vector extraction failed), asymmetric effects across domains
- **First 3 experiments**: 1) Replicate delta accuracy hierarchy on AQUA-RAT across all four persona levels, 2) Extract steering vector from 100-sample contrastive dataset and measure accuracy recovery under high-authority incorrect endorsements, 3) Ablation: test steering at different layer depths to identify optimal intervention point

## Open Questions the Paper Calls Out
None

## Limitations
- Exact prompt templates for persona framing and endorsement presentation are not fully detailed, creating significant uncertainty about replicability.
- Steering vector extraction uses only 100 questions, and the representativeness of this sample across diverse domains remains unvalidated.
- Optimal layer selection for steering vector application, scaling coefficient magnitude, and domain transfer properties are not systematically explored.

## Confidence
- High confidence: The hierarchical authority encoding mechanism is strongly supported by reported delta accuracy gradients across expertise levels.
- Medium confidence: The confidence-confusion coupling under expert misinformation relies on entropy as an indirect measure with minimal corpus support.
- Medium confidence: The residual stream steering approach is mechanistically sound but implementation details require empirical validation.

## Next Checks
1. **Prompt ablation study**: Systematically vary persona phrasing, endorsement positioning, and format to establish sensitivity of authority bias to prompt engineering and document how different phrasings affect the delta accuracy hierarchy.

2. **Steering intervention sweep**: Test steering vector application across multiple layer depths with coefficient magnitudes from 0.1 to 2.0, measuring both bias reduction effectiveness and baseline performance degradation to identify optimal intervention parameters.

3. **Cross-domain steering transfer**: Extract steering vectors from one domain (e.g., math) and apply to others (e.g., medicine, law) to test whether the expertise representation is domain-general or domain-specific and measure relative effectiveness.