---
ver: rpa2
title: 'FormulaReasoning: A Dataset for Formula-Based Numerical Reasoning'
arxiv_id: '2402.12692'
source_url: https://arxiv.org/abs/2402.12692
tags:
- question
- formulas
- formula
- reasoning
- parameter
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces FormulaReasoning, a dataset for formula-based
  numerical reasoning consisting of 5,324 physics questions with fine-grained annotations,
  including formulas, parameters, and reasoning steps in both Chinese and English.
  The dataset addresses the gap in existing numerical reasoning datasets by explicitly
  incorporating domain-specific physics formulas to guide reasoning.
---

# FormulaReasoning: A Dataset for Formula-Based Numerical Reasoning

## Quick Facts
- arXiv ID: 2402.12692
- Source URL: https://arxiv.org/abs/2402.12692
- Authors: Xiao Li; Bolin Zhu; Kaiwen Shi; Sichen Liu; Yin Zhu; Yiwei Liu; Gong Cheng
- Reference count: 40
- Primary result: Dataset with 5,324 physics questions and formula-based reasoning annotations showing large models outperform small models significantly

## Executive Summary
This paper introduces FormulaReasoning, a dataset designed to address the gap in formula-based numerical reasoning benchmarks. The dataset contains 5,324 physics questions with fine-grained annotations including formulas, parameters, and reasoning steps in both Chinese and English. Through extensive experiments on various model scales, the authors demonstrate that while large-scale models perform well, smaller models struggle significantly with formula-based reasoning tasks, indicating opportunities for improvement through specialized training approaches and architectural adaptations.

## Method Summary
The authors constructed FormulaReasoning by collecting physics questions from textbooks, online platforms, and competitions, then manually annotating each with formulas, parameter details, and reasoning steps. They developed a consolidated formula database of 272 distinct formulas through normalization and merging techniques. The evaluation framework uses Numbat calculator for precise numerical validation with <1% relative error tolerance. Multiple approaches were evaluated: zero-shot CoT prompting, CoT-SFT with three-step formula generation-parameter extraction-external calculation process, data augmentation, and retrieval-augmented generation using a formula retriever based on Chinese-BERT-wwm-base with contrastive learning.

## Key Results
- GPT-4o achieved 83.93% accuracy on HoF test but dropped to 60.69% on HeF test, revealing significant generalization challenges
- Qwen2.5-Math-7B-Instruct with CoT-SFT outperformed GPT-4o (70.57% vs 67.50%) by delegating calculations to Numbat
- Formula retrieval provided modest improvements (0.72% accuracy gain for ERNIE-4.5-21B-A3B) but often included irrelevant formulas
- DPO with PRM-derived preference data improved performance across multiple numerical reasoning benchmarks beyond FormulaReasoning

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Decomposing formula-based reasoning into explicit sub-tasks improves small model performance
- **Mechanism:** By separating symbolic reasoning from arithmetic execution, models focus on knowledge application while delegating precise computation to verified tools (Numbat), reducing cascading calculation errors
- **Core assumption:** The bottleneck in formula-based reasoning for small models lies primarily in calculation precision rather than formula selection
- **Evidence anchors:** Section 4.3 shows Qwen2.5-Math-7B-Instruct with CoT-SFT outperformed GPT-4o; Section 3.2 describes three-step CoT-SFT process; Fortune paper shows formula-driven decomposition benefits symbolic table reasoning
- **Break condition:** If errors shift from calculation to formula selection, the decomposition benefit diminishes

### Mechanism 2
- **Claim:** Retrieving relevant formulas as context provides modest but consistent improvements
- **Mechanism:** External formula retrieval reduces hallucination by grounding generation in verified domain knowledge, particularly when models lack internalized formula representations
- **Core assumption:** The retriever can identify relevant formulas with sufficient precision; retrieved formulas are actually utilized by the model
- **Evidence anchors:** Section 4.2 shows ERNIE-4.5-21B-A3B accuracy increased by 0.72 with formula retrieval; paper notes top five formulas often included irrelevant ones; weak corpus support for retrieval augmentation in formula reasoning
- **Break condition:** When retrieval noise exceeds signal, or when HeF test requires formulas not in the training database

### Mechanism 3
- **Claim:** Preference optimization with formula-aware process reward models transfers improvements across numerical reasoning domains
- **Mechanism:** DPO with PRM-derived preference data shapes model behavior toward correct reasoning steps; the structured nature of formula reasoning provides dense, verifiable supervision signals
- **Core assumption:** Process-level supervision in physics formulas generalizes to other mathematical reasoning tasks
- **Evidence anchors:** Section 4.4 shows DPO improved accuracy on FormulaReasoning AND GSM8K, MATH, SVAMP, and GaoKao2023-en; Table 11 shows DeepSeek-R1-Distill-Qwen-1.5B improved from 7.23% to 10.47%; FinanceReasoning benchmark suggests domain-specific characteristics affect transfer
- **Break condition:** If PRM scoring fails to capture formula correctness, preference data quality degrades

## Foundational Learning

- **Concept: Chain-of-Thought (CoT) Prompting**
  - Why needed here: Baseline reasoning approach for all LLM experiments; enables step-by-step formula application
  - Quick check question: Can you explain why CoT alone achieves only 67.50% accuracy for GPT-4o while CoT-SFT enables a 7B model to exceed this?

- **Concept: Process Reward Models (PRM)**
  - Why needed here: Core to DPO preference data generation; enables step-level evaluation of reasoning quality
  - Quick check question: How does PRM scoring handle the paper's observation that "valid reasoning paths in formula-based problems are often non-unique"?

- **Concept: Retrieval-Augmented Generation (RAG)**
  - Why needed here: Enables external knowledge grounding via formula database; Recall@5 of 82.46% is a key retrieval metric
  - Quick check question: Why does formula retrieval improve HoF but not apply to HeF test scenarios?

## Architecture Onboarding

- **Component map:**
  ```
  Question Input → [Formula Retriever] → Retrieved Top-K Formulas
                          ↓
  Question + Retrieved → [LLM/CoT-SFT Model] → Generated Explanation
                          ↓
  Explanation → [Numbat Calculator] → Verified Numerical Answer
                          ↓
  Training: MCTS + [PRM Scorer] → Preference Pairs → [DPO] → Fine-tuned Model
  ```

- **Critical path:**
  1. Formula database quality (272 formulas after merging) determines retriever ceiling
  2. CoT-SFT relies on correct parameter extraction—if this fails, calculation stage receives garbage
  3. DPO effectiveness depends on PRM accurately distinguishing good/bad reasoning paths

- **Design tradeoffs:**
  - **HoF vs HeF split:** HoF enables retrieval evaluation; HeF tests generalization to unseen formulas—but no retrieval possible
  - **Top-5 retrieval:** Higher recall but more noise; paper suggests "considerable room for improvement"
  - **Numbat vs internal calculation:** External tool ensures precision but adds dependency; PoT underperformed CoT suggesting formula application, not calculation, is the bottleneck

- **Failure signatures:**
  - Formula selection errors (main error type per Appendix A.5)
  - Parameter noise confusion (14% of DeepSeek-R1 errors)
  - Unit calculation failures in multi-step reasoning
  - Retrieved formulas not utilized (model ignores context)

- **First 3 experiments:**
  1. **Baseline establishment:** Run CoT prompt (Figure 8) on HoF test with your model; compare against Table 4 baselines to identify performance tier.
  2. **Retrieval ablation:** Implement formula retriever (Appendix A.3.3) and test with top-1 vs top-5 formulas; measure noise sensitivity.
  3. **CoT-SFT pilot:** Fine-tune a 1.5B–7B model on training set with normalized explanations; evaluate if external calculation delegation improves accuracy over standard SFT.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the dataset construction pipeline successfully generalize to other scientific disciplines, such as chemistry or finance, to create multi-disciplinary formula reasoning benchmarks?
- **Basis in paper:** [explicit] The Conclusion states the authors aim to "extend our implementation to other scientific and engineering domains, such as chemistry and finance," given the domain-agnostic nature of the framework.
- **Why unresolved:** The current study validates the pipeline only within the physics domain; it remains unproven whether the normalization and merging techniques apply effectively to the distinct terminology and formula structures of other fields.
- **What evidence would resolve it:** Successful construction and evaluation of a chemistry or finance dataset using the described pipeline, maintaining comparable annotation quality and formula consolidation accuracy.

### Open Question 2
- **Question:** How can structured formula retrieval mechanisms be optimized to minimize the inclusion of irrelevant formulas that introduce noise into the reasoning process?
- **Basis in paper:** [inferred] Section 4.2 notes that the top five retrieved formulas often included irrelevant ones, which "might introduce noise, suggesting considerable room for improvement in structured formula retrieval."
- **Why unresolved:** Current retrieval methods (based on vector similarity) struggle to distinguish purely textual relevance from structural formula relevance, leading to lower precision in the augmented prompt.
- **What evidence would resolve it:** A retrieval mechanism that improves the relevance of retrieved formulas (e.g., higher precision than the current Recall@5 baseline) and subsequently increases the accuracy of LLMs on the HoF test set.

### Open Question 3
- **Question:** What specific architectural adaptations are required for smaller models (under 8B parameters) to handle multi-step numerical reasoning and calculation without significant performance degradation?
- **Basis in paper:** [explicit] The Conclusion highlights that "smaller-scale LLMs still face significant challenges," and Section 4.1 calls for investigating "architectural adaptations... specifically tailored for compact models."
- **Why unresolved:** The study shows a sharp performance drop in small models compared to large ones, indicating that simply scaling down existing architectures is insufficient for the complexities of formula-based reasoning.
- **What evidence would resolve it:** A sub-8B parameter model architecture or training paradigm that achieves performance comparable to large-scale models (e.g., >85% accuracy) on the FormulaReasoning benchmark.

## Limitations

- **Formula Generalization Gap:** Models show significant performance degradation on HeF test questions requiring novel formulas, with GPT-4o dropping from 83.93% to 60.69%, revealing heavy reliance on formula memorization.
- **Formula Retrieval Reliability:** Despite 82.46% Recall@5, retrieval often includes irrelevant formulas that introduce noise and may actually harm performance.
- **Small Model Scaling Constraints:** Sharp performance gap exists between large and small models (7B achieving only 70.57% vs 83.93% for GPT-4o), indicating fundamental architectural limitations.

## Confidence

- **High Confidence:** Dataset construction methodology, formula database creation, basic evaluation metrics, and observed performance gap between large and small models.
- **Medium Confidence:** Effectiveness of CoT-SFT approach for small models, general trend of retrieval augmentation providing modest improvements, and basic structure of FormulaReasoning+ challenges.
- **Low Confidence:** Generalization of DPO improvements to other domains, long-term stability of preference optimization effects, and practical utility of FormulaReasoning+ for real-world applications.

## Next Checks

1. **Formula Generalization Stress Test:** Create an expanded HeF test set with formulas from different physics subdomains not present in original training data; evaluate whether CoT-SFT and retrieval-augmented approaches maintain relative advantages on truly novel formula applications.

2. **Retriever Precision Optimization:** Implement a precision-focused retriever variant (e.g., top-1 retrieval with confidence thresholding) and systematically compare against current top-5 approach across different formula complexity levels; measure trade-off between recall and noise using PRM scores as quality indicators.

3. **Cross-Domain Transfer Validation:** Apply trained DPO models to FinanceReasoning benchmark and GaoKao2023-en test set; measure whether process-level improvements in physics formula reasoning transfer to financial calculations and general mathematics; compare against models fine-tuned directly on target domains.