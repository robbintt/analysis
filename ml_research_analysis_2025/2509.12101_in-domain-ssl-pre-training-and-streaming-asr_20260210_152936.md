---
ver: rpa2
title: In-domain SSL pre-training and streaming ASR
arxiv_id: '2509.12101'
source_url: https://arxiv.org/abs/2509.12101
tags:
- streaming
- speech
- pre-training
- learning
- atco2
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study explored the impact of in-domain SSL pre-training for
  both offline and streaming ASR in ATC environments. We trained BEST-RQ models on
  4.5k hours of unlabeled ATC data, then fine-tuned on supervised ATC sets, using
  chunked attention and dynamic convolutions for low-latency inference.
---

# In-domain SSL pre-training and streaming ASR

## Quick Facts
- arXiv ID: 2509.12101
- Source URL: https://arxiv.org/abs/2509.12101
- Reference count: 21
- Primary result: BEST-RQ model pre-trained on 4.5k hours ATC data achieved 19.70% WER on ATCO2 corpus, outperforming out-domain models pre-trained on 4.5M-60k hours

## Executive Summary
This study explored the impact of in-domain SSL pre-training for both offline and streaming ASR in ATC environments. We trained BEST-RQ models on 4.5k hours of unlabeled ATC data, then fine-tuned on supervised ATC sets, using chunked attention and dynamic convolutions for low-latency inference. Comparing in-domain models against out-domain encoders like w2v-BERT 2.0 and HuBERT, our BEST-RQ model trained on ATC data significantly outperformed others on the ATCO2 corpus (WER: 19.70% vs 23.12%-39.26%), despite fewer pre-training hours. The streaming approach further improved performance under tighter latency constraints, showing the value of specialized SSL pre-training for real-world ATC applications.

## Method Summary
The method uses BEST-RQ self-supervised learning with chunked attention and dynamic convolutions for streaming ASR. Models were pre-trained on 4.5k hours of unlabeled ATC data using a mixed training strategy (40% full context, 60% dynamic chunking with random chunk sizes 8-32 frames), then fine-tuned on 40h labeled Airbus-ATC data with CTC loss. Two model sizes were used: Large (300M params, 24 layers) and Base (92M params, 12 layers). Streaming inference was evaluated across chunk sizes (320-1280ms) and left context configurations.

## Key Results
- BEST-RQ model pre-trained on ATC data (4.5k hours) achieved 19.70% WER on ATCO2-1h, outperforming w2v-BERT 2.0 (23.12%) and HuBERT (33.33%) despite fewer pre-training hours
- Streaming model (Stream-ATCO2Large) outperformed offline BRQ-ATCO2Large even in offline mode (7.18% vs 7.40% on Airbus-ATC, 19.30% vs 19.70% on ATCO2-1h)
- LM integration provided 4.57 point WER reduction for in-domain BEST-RQ vs 2.86 points for out-domain models

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** In-domain SSL pre-training on limited domain-specific data can outperform general-purpose SSL models pre-trained on orders of magnitude more data, when evaluated on that specific domain.
- **Mechanism:** The BEST-RQ model learns representations tailored to the acoustic characteristics of VHF radio communications (equipment noise, signal quality variations) and the specialized ATC vocabulary/accents present in the pre-training data. This domain alignment reduces the representational gap that general-purpose models must bridge during fine-tuning.
- **Core assumption:** The unlabeled pre-training data distribution sufficiently matches the target evaluation distribution (ATCO2 corpus characteristics).
- **Evidence anchors:** [Section 5.2]: BRQ-ATCO2Large (4.5k hours ATC) achieved 19.70% WER on ATCO2-1h vs. w2v-BERT 2.0 (4.5M hours multilingual) at 23.12% and HuBERT (60k hours English) at 33.33%
- **Break condition:** If target domain acoustic conditions diverge significantly from pre-training data (e.g., different radio equipment, non-English communications), in-domain advantage may diminish.

### Mechanism 2
- **Claim:** Pre-training with streaming-aware constraints (chunked attention, dynamic convolutions) creates models that perform well across both streaming and offline inference modes.
- **Mechanism:** The mixed training strategy (40% full context batches, 60% dynamic chunking with random chunk sizes 8-32 frames) teaches the encoder to extract useful representations under varying context constraints. Dynamic Chunk Convolutions eliminate train-inference mismatch by restricting convolution operations to within-chunk frames during both training and inference.
- **Core assumption:** The model can learn a unified representation space that generalizes across context window variations.
- **Evidence anchors:** [Section 6.4]: Stream-ATCO2Large outperformed offline BRQ-ATCO2Large on all test datasets even in offline mode (7.18% vs 7.40% on Airbus-ATC, 19.30% vs 19.70% on ATCO2-1h)
- **Break condition:** If inference-time chunk sizes fall far outside the training distribution (e.g., extremely small chunks not sampled during training), performance may degrade non-gracefully.

### Mechanism 3
- **Claim:** Language model integration provides greater relative improvement for in-domain SSL models than for out-domain models.
- **Mechanism:** In-domain acoustic representations are better aligned with target domain phonetics, allowing the language model to more effectively resolve remaining ambiguities. Out-domain models have acoustic mismatches that noisier acoustic scores, reducing LM effectiveness during beam search decoding.
- **Core assumption:** The language model training data (Airbus-ATC, ATCOSIM, UWB_ATCC) sufficiently covers the evaluation domain's linguistic patterns.
- **Evidence anchors:** [Section 5.2]: LM integration reduced BRQ-ATCO2Large WER by 4.57 points on ATCO2-1h (24.27% → 19.70%), vs. 2.86 points for w2v-BERT 2.0 (25.98% → 23.12%)
- **Break condition:** If LM training data has distribution mismatch with evaluation domain (different phraseology, accent-specific pronunciations not captured), amplification effect may reverse.

## Foundational Learning

- **Concept: Self-supervised learning with random-projection quantization (BEST-RQ)**
  - **Why needed here:** The paper uses BEST-RQ instead of more common wav2vec 2.0/HuBERT. You need to understand that BEST-RQ uses a fixed random-projection quantizer to create discrete targets from speech, avoiding the complexity of learning a codebook.
  - **Quick check question:** Why does BEST-RQ use a fixed (untrained) quantizer rather than a learned one like wav2vec 2.0's product quantization?

- **Concept: Streaming ASR latency constraints and chunk-based processing**
  - **Why needed here:** The streaming experiments trade off accuracy vs. latency. You need to understand that chunk size and left context directly control how much future audio the model can see before emitting output.
  - **Quick check question:** If a model uses 320ms chunk size with 1 left chunk, what is the minimum latency before the first token for that chunk can be emitted?

- **Concept: Conformer architecture with attention and convolution components**
  - **Why needed here:** The paper replaces standard attention with chunked attention and standard convolutions with Dynamic Chunk Convolutions within Conformer blocks.
  - **Quick check question:** In a Conformer block, what role does the convolution module play that's complementary to the self-attention module?

## Architecture Onboarding

- **Component map:** Input audio (16kHz) -> Feature extraction (log-mel or raw) -> BEST-RQ encoder (24 layers Large / 12 layers Base, 848/576 dims) -> Conformer blocks with: Chunked attention (replaces full attention) -> Dynamic Chunk Convolutions (replaces standard conv) -> Feed-forward modules -> Downstream probe (3-layer DNN: 1024 dims, dropout 0.15) -> Linear + Softmax → CTC loss -> Optional: 4-gram LM + beam search (size 1000)

- **Critical path:** 1. SSL pre-training (300k iterations, 4.5k hours unlabeled ATCO2, mixed streaming strategy) 2. Fine-tuning (30-80 epochs on 40h labeled Airbus-ATC, CTC loss) 3. Evaluation with configurable chunk size and left context

- **Design tradeoffs:**
  | Decision | Option A | Option B | Guidance |
  |----------|----------|----------|----------|
  | Model size | Large (300M, 24 layers) | Base (92M, 12 layers) | Large for accuracy; Base if compute-constrained (Base achieved competitive ATCO2 results) |
  | Pre-training mode | Streaming-aware (mixed) | Offline-only | Use streaming-aware even for offline tasks—showed better results |
  | Chunk size at inference | Small (320ms) | Large (1280ms) | Smaller = lower latency but higher WER; diminishing returns beyond 16 left chunks |
  | LM integration | 4-gram + beam | Greedy decoding | Add LM for ~3-4.5 point WER reduction; essential for in-domain models |

- **Failure signatures:**
  - WER spikes on ATCO2 vs. Airbus-ATC: Expected due to accent differences (Czech, Swiss German) and VHF quality variations
  - Aggressive streaming (left chunk=1) causes >2x WER degradation: Not seen during training; consider adding to training distribution
  - LM hurts instead of helps: Check LM training data coverage of target vocabulary

- **First 3 experiments:**
  1. **Reproduce offline baseline:** Pre-train BEST-RQ on your unlabeled domain data (aim for 1k+ hours minimum based on 4.5k success), fine-tune on 40h labeled data with CTC, compare to wav2vec 2.0/HuBERT checkpoints on your test set
  2. **Ablate streaming pre-training:** Train two models—one with streaming-aware mixed strategy, one without—compare both in streaming mode (chunk size 320-1280ms) to quantify streaming pre-training benefit
  3. **Measure latency-accuracy curve:** Sweep chunk sizes (320, 480, 640, 960, 1280ms) and left contexts (1, 4, 16, full chunks) to find acceptable latency budget for your deployment scenario

## Open Questions the Paper Calls Out

- **Question:** How does the proposed streaming architecture perform when integrated into fully operational, real-time Air Traffic Control systems?
  - **Basis in paper:** [explicit] The authors state that future work involves "integrating these models into operational ATC systems."
  - **Why unresolved:** The current study evaluates performance on offline test sets (ATCO2, Airbus-ATC) and simulated streaming constraints, but has not yet been validated in a live, safety-critical operational environment.
  - **What evidence would resolve it:** Deployment of the model in a live pilot or shadow-mode setting, measuring real-time latency and Word Error Rate (WER) against live controller inputs.

- **Question:** To what extent does the model maintain accuracy across the diverse range of global accents and noise conditions inherent in ATC communications?
  - **Basis in paper:** [explicit] The authors propose examining "their robustness to the diverse accents and noise conditions common in ATC communications."
  - **Why unresolved:** While the paper tests on French-accented (Airbus) and Central European-accented (ATCO2) data, the authors acknowledge that real-world accoustics and accents vary widely.
  - **What evidence would resolve it:** A comprehensive evaluation on a dataset containing a balanced, global distribution of controller and pilot accents (e.g., Asian, Middle Eastern) and varying signal-to-noise ratios.

- **Question:** Would initializing the in-domain BEST-RQ model with weights from a large-scale general-purpose model (rather than training from scratch) improve performance?
  - **Basis in paper:** [inferred] The paper contrasts training from scratch on 4.5k hours against fine-tuning out-domain models, but does not test a hybrid approach of general pre-training followed by ATC-specific SSL continuation.
  - **Why unresolved:** It remains unclear if the superior performance of the in-domain model is solely due to domain adaptation or if it suffers from the lack of general phonetic knowledge found in 4.5M hour models.
  - **What evidence would resolve it:** An ablation study comparing a "from-scratch" BEST-RQ model against one initialized with weights from HuBERT or w2v-BERT before ATC pre-training.

## Limitations

- Domain-specific findings may not generalize to other specialized domains beyond ATC environments
- Streaming mechanism has untested assumptions about sufficiency of mixed training distribution (40%/60% split, chunk size ranges 8-32 frames training, 320-1280ms inference)
- Hardware requirements are prohibitive for most research teams (16×H100 GPUs for pre-training)

## Confidence

- **High Confidence:** The core finding that in-domain SSL pre-training outperforms out-domain models on the same domain (ATCO2 corpus results)
- **Medium Confidence:** The superiority of streaming-aware pre-training for both streaming and offline performance
- **Medium Confidence:** The amplification of LM benefits for in-domain models

## Next Checks

1. **Domain transfer experiment:** Pre-train BEST-RQ on a different specialized domain (e.g., medical dictation or broadcast news) with similar data volumes, then evaluate on both in-domain and ATC data to test generalizability of in-domain advantage

2. **Streaming distribution coverage analysis:** Systematically evaluate the trained streaming model across a wider range of chunk sizes and left contexts than tested (e.g., 64, 128, 256, 512 frames) to identify boundaries of training distribution and measure performance degradation

3. **Hardware scaling study:** Reproduce pre-training at different scales (8, 16, 32 GPUs) and model sizes (Base, Large) to quantify relationship between computational resources and achievable WER, establishing practical minimum requirements