---
ver: rpa2
title: Generative modeling of conditional probability distributions on the level-sets
  of collective variables
arxiv_id: '2512.17374'
source_url: https://arxiv.org/abs/2512.17374
tags:
- data
- distribution
- figure
- generative
- trajectory
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a general and efficient generative modeling
  framework for learning conditional probability distributions on level-sets of collective
  variables (CVs). The approach extends flow-matching models to handle families of
  manifolds defined implicitly by CVs, enabling sampling on multiple level-sets simultaneously
  without requiring explicit geometric information.
---

# Generative modeling of conditional probability distributions on the level-sets of collective variables

## Quick Facts
- arXiv ID: 2512.17374
- Source URL: https://arxiv.org/abs/2512.17374
- Reference count: 40
- Primary result: Introduces a generative modeling framework for learning conditional probability distributions on CV level-sets using flow-matching, validated on synthetic and molecular systems including alanine dipeptide

## Executive Summary
This paper introduces a general and efficient generative modeling framework for learning conditional probability distributions on level-sets of collective variables (CVs). The approach extends flow-matching models to handle families of manifolds defined implicitly by CVs, enabling sampling on multiple level-sets simultaneously without requiring explicit geometric information. A data enrichment strategy is proposed to improve learning quality in low-probability regions by leveraging enhanced sampling techniques such as adaptive biasing force (ABF) simulations. The method is validated on a hierarchy of systems, from synthetic 2D examples to high-dimensional molecular datasets including alanine dipeptide, demonstrating its flexibility, robustness, and applicability to molecular systems with complex energy landscapes. Results show that the proposed approach accurately reproduces conditional distributions and effectively exploits biased data to improve learning in scarcely sampled regions.

## Method Summary
The proposed method extends flow-matching generative models to learn conditional distributions on the level-sets of collective variables (CVs) without requiring explicit manifold parameterization. The key innovation is a family of manifold-valued flows that can simultaneously model multiple level-sets, where each flow component evolves according to a neural ODE driven by a velocity field trained to match conditional distributions. The framework handles implicitly defined manifolds via CV gradients, making it applicable to high-dimensional molecular systems where explicit parameterizations are infeasible. A data enrichment strategy leverages enhanced sampling trajectories (e.g., from ABF simulations) to improve model quality in low-probability regions by augmenting training data with biased samples reweighted to the target distribution. The method is implemented as a continuous normalizing flow with trainable velocity fields and validated across synthetic and real molecular systems.

## Key Results
- The proposed flow-matching framework accurately learns conditional distributions on CV level-sets for both synthetic 2D systems and molecular datasets including alanine dipeptide
- Data enrichment using biased ABF simulation trajectories significantly improves sampling quality in low-probability regions that are scarcely visited in unbiased data
- The method demonstrates robustness and flexibility by successfully handling implicitly defined manifolds without requiring explicit geometric information, enabling application to complex, high-dimensional molecular systems

## Why This Works (Mechanism)
The method works by combining the expressive power of flow-matching models with the ability to implicitly handle manifold constraints defined by collective variables. By parameterizing the evolution of samples as neural ODEs on the manifold, the approach can learn the conditional distribution across multiple level-sets simultaneously. The key insight is that gradients of the CV function provide the necessary geometric information to constrain the flow to the manifold without explicit parameterization. The data enrichment strategy addresses the fundamental challenge of rare event sampling in molecular systems by leveraging enhanced sampling techniques to populate underrepresented regions of the conditional distribution.

## Foundational Learning

- **Collective Variables (CVs)**: Low-dimensional descriptors that capture essential molecular motions; needed to reduce high-dimensional molecular dynamics to interpretable reaction coordinates; quick check: verify CV definitions preserve relevant dynamics
- **Flow-Matching Models**: Generative models that learn deterministic transformations between distributions via neural ODEs; needed for continuous, invertible mappings on manifolds; quick check: validate flow invertibility and smoothness
- **Manifold-Valued Flows**: Extensions of normalizing flows to constrained geometries; needed to ensure samples remain on CV level-sets during generation; quick check: verify samples stay on correct manifolds
- **Neural ODEs**: Continuous-depth networks defined by differential equations; needed for smooth, invertible transformations on manifolds; quick check: confirm numerical integration stability
- **Enhanced Sampling (ABF)**: Methods that bias simulations to explore rare regions; needed to generate data for under-sampled regions; quick check: validate reweighting corrects for bias
- **Implicit Function Theorem**: Provides conditions for representing manifolds as level-sets; needed to justify gradient-based manifold constraints; quick check: verify CV gradients exist and are computable

## Architecture Onboarding

**Component Map:** CV function g(x) -> Gradient ∇g(x) -> Velocity field v(x,t) -> Neural ODE flow -> Conditional distribution on level-set

**Critical Path:** The velocity field must be trained to match the conditional distribution while ensuring samples evolve only along level-sets via projection using ∇g(x)

**Design Tradeoffs:** Implicit manifold representation avoids explicit parameterization but requires gradient computation; data enrichment improves rare region sampling but introduces bias that must be corrected

**Failure Signatures:** Poor gradient estimates for CVs lead to samples leaving manifolds; insufficient enhanced sampling data causes inaccurate modeling of rare events; numerical instability in ODE integration produces invalid samples

**First Experiments:** 1) Validate flow preserves level-set constraints for simple CV functions; 2) Test data enrichment reweighting accuracy on known distributions; 3) Benchmark conditional sampling against ground truth for synthetic systems

## Open Questions the Paper Calls Out
None

## Limitations
- The approach relies on computing or approximating CV gradients ∇g, which may be challenging for complex, high-dimensional CVs where gradients are not readily available
- Validation is limited to relatively simple examples and alanine dipeptide, raising questions about robustness across diverse CV definitions and molecular systems
- The method's computational scalability for very high-dimensional CVs or larger molecular systems beyond tested cases is not thoroughly explored

## Confidence
- Major claims: Medium
- Methodology: Medium
- Theoretical framework: High

## Next Checks
1. Test the approach on a broader range of molecular systems with more complex CVs and higher dimensionality
2. Conduct sensitivity analyses for hyperparameters and gradient approximations
3. Benchmark against alternative conditional sampling methods on standard datasets to assess relative performance