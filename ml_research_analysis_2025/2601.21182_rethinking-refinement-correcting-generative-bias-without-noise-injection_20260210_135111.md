---
ver: rpa2
title: 'Rethinking Refinement: Correcting Generative Bias without Noise Injection'
arxiv_id: '2601.21182'
source_url: https://arxiv.org/abs/2601.21182
tags:
- refinement
- generative
- noise
- bias
- base
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'We address systematic generative bias in iterative models like
  diffusion and flow-matching by proposing Bi-stage Flow Refinement (BFR), a post-hoc
  correction framework with two strategies: data-space refinement (DFR) and latent-space
  refinement (LFR). DFR uses lightweight augmentations during training to improve
  robustness, while LFR aligns data-induced latent distributions with the Gaussian
  prior for invertible models.'
---

# Rethinking Refinement: Correcting Generative Bias without Noise Injection

## Quick Facts
- **arXiv ID:** 2601.21182
- **Source URL:** https://arxiv.org/abs/2601.21182
- **Reference count:** 40
- **Primary result:** Corrects generative bias in iterative models using post-hoc refinement without noise injection; achieves state-of-the-art FID of 1.46 on MNIST with 1-NFE

## Executive Summary
This paper addresses systematic generative bias in diffusion and flow-matching models by proposing Bi-stage Flow Refinement (BFR), a post-hoc correction framework with two strategies: Data-space Flow Refinement (DFR) and Latent-space Flow Refinement (LFR). Unlike prior refiners that perturb sampling dynamics with noise injection, BFR preserves the original ODE trajectory and applies deterministic corrections. DFR uses lightweight augmentations during training to improve robustness, while LFR aligns data-induced latent distributions with the Gaussian prior for invertible models. Experiments on MNIST, CIFAR-10, and FFHQ demonstrate consistent improvements in fidelity and coverage, with LFR achieving remarkable efficiency through single-step refinement.

## Method Summary
BFR corrects generative bias through two complementary strategies. DFR trains a flow-matching refiner using lightly augmented generated samples during training, allowing deterministic correction at inference without noise injection. LFR leverages invertible generators to align the latent space distribution with the Gaussian prior, training a flow to map standard Gaussian to the biased latent distribution. Both approaches use flow-matching to learn vector fields for transport, but differ in the space where refinement occurs. The framework preserves the original ODE trajectory during inference, contrasting with noise-injection methods that perturb sampling dynamics. Training involves learning velocity fields that connect initial (biased) states to target states, enabling efficient single-step correction.

## Key Results
- DFR improves robustness through lightweight data augmentation during training, enabling stable deterministic correction
- LFR achieves state-of-the-art FID of 1.46 on MNIST with single function evaluation (1-NFE)
- LFR demonstrates strong cross-model transferability, reducing FID from 7.61 to 6.86 without retraining
- Both strategies effectively mitigate generative bias while preserving the original sampling process
- Increasing NFE beyond 1 consistently degrades LFR quality due to over-refinement

## Why This Works (Mechanism)

### Mechanism 1: Augmentation-Stabilized Data-Space Transport (DFR)
Lightweight data augmentation during training allows flow-matching models to correct generative bias deterministically at inference without noise injection. Generated samples often lie in low-density regions with artifacts not seen during base model training. Direct transport from these samples to real data is unstable. By applying light augmentation (blur, noise) only during training, the refiner learns a robust vector field mapping the "augmented generated" manifold to the "real" manifold. At inference, the deterministic ODE solver follows this field to correct raw samples without perturbation.

### Mechanism 2: Latent-Space Prior Alignment (LFR)
Refining the latent space to align the "inverse-mapped real data" distribution with the Gaussian prior corrects bias more efficiently than data-space refinement. Invertible generators should ideally map data back to standard Gaussian latents, but generative bias creates discrepancies. LFR trains a flow to map Standard Gaussian to this "Biased Latent" distribution. At inference, sampling a standard Gaussian and transforming it via LFR yields a "pre-corrected" latent that, when passed through the generator, produces high-fidelity samples with minimal computational overhead.

### Mechanism 3: Single-Step Deterministic Correction
Refinement can be achieved with a single function evaluation (1-NFE) if formulated as a straight-line probability flow ODE. Training the refiner to predict the velocity field connecting initial to target states allows integration to reduce to a single step. This contrasts with multi-step diffusion refinement requiring noise injection and iterative resampling. The learned vector field must be straight enough or the solver robust enough to approximate transport in a single step without drifting off the data manifold.

## Foundational Learning

- **Flow Matching (FM) & Probability Flow ODEs**: The framework is built on defining vector fields (ODEs) to transport samples. FM allows deterministic, potentially straight-line trajectories essential for 1-NFE refinement, contrasting with stochastic diffusion processes.
  - *Quick check*: Can you explain why an ODE-based formulation allows for 1-NFE sampling more naturally than a standard DDPM denoising loop?

- **Generative / Exposure Bias**: This is the problem BFR solves - the mismatch between training on ground-truth intermediate states and inferring on model-generated states, causing drift and artifact accumulation.
  - *Quick check*: Why does training a generator on ground-truth noisy images x_t but sampling from its own predicted x̂_{t-1} cause error accumulation?

- **Invertibility in Generative Models**: LFR relies on mapping real data back to latent noise (G^{-1}). Without understanding how to reverse a flow/diffusion (e.g., via Heun or RK4 solvers backward), you cannot construct training data for LFR.
  - *Quick check*: How would you obtain the latent code z_1 corresponding to a real image x_0 in a pre-trained flow-matching model?

## Architecture Onboarding

- **Component map:** Base Generator (G_θ) -> Refiner Network (F_ψ/F_φ) -> ODE Solver -> Refined Sample
- **Critical path:**
  1. Data Prep (LFR only): Invert dataset X_real → Z_biased using frozen base generator
  2. Refiner Training: DFR trains F_ψ mapping G(z) → x_real using augmented G(z); LFR trains F_φ mapping N(0,I) → Z_biased
  3. Inference: DFR uses z → G(z) → ODE → x_refined; LFR uses z → ODE → z_refined → G(z_refined)

- **Design tradeoffs:**
  - DFR vs. LFR: Choose DFR for non-invertible models or when transferability across different generators is needed; choose LFR for invertible models where efficiency (1-NFE) is critical
  - NFE Count: For LFR, stick to 1-NFE as 10-NFE degrades results due to over-refinement

- **Failure signatures:**
  - LFR Over-refinement: Performance drops as NFE increases beyond 1; latent variables drift away from intended target manifold
  - Hyperparameter α: In LFR, if mixing coefficient α is too low, robustness suffers; if too high, semantic consistency disrupted
  - Cross-Model Transfer: LFR transfers less effectively than DFR when base model quality changes significantly

- **First 3 experiments:**
  1. Validation of Augmentation: Train DFR with and without augmentation to confirm stability claim and ODE convergence on out-of-distribution samples
  2. NFE Ablation on LFR: Run LFR with 1 vs. 10 NFE to verify counter-intuitive result that 1-NFE performs better
  3. Transfer Test: Train refiner on well-converged generator and apply to poorly-converged one; compare DFR vs. LFR transfer capability

## Open Questions the Paper Calls Out

### Open Question 1
Why does multi-step integration consistently degrade performance in Latent-space Flow Refinement (LFR) while improving Data-space Flow Refinement (DFR)? Section 5.1 notes increasing NFEs from 1 to 10 consistently degrades LFR quality due to "over-refinement" where latents "overshoot the optimal correction," whereas DFR benefits from iterative steps. The paper empirically observes this divergence but lacks a theoretical constraint to prevent latents from overshooting during multi-step ODE integration. A stability analysis of the LFR vector field or an adaptive step-size scheduler that maintains fidelity improvements beyond single-step inference would resolve this.

### Open Question 2
What are the robustness limits of LFR when applied to generative models with significant reconstruction errors (non-invertibility)? Section 4.2 assumes an "approximately invertible generator" and introduces noise mixing to handle reconstruction error, but does not quantify the error threshold the method can tolerate. Experiments focus on invertible ODE models; the efficacy of the noise mixing strategy against significant irreversible compression (e.g., in heavy VAEs or GANs) remains untested. Experiments applying LFR to models with varying degrees of non-invertibility would identify the failure point of the refinement.

### Open Question 3
Can the optimal mixing coefficient α be derived analytically from the base model's latent statistics rather than via hyperparameter search? Appendix B demonstrates that performance is sensitive to α (optimal is 0.2 for CIFAR-10 vs. 0.1 for MNIST), but selection relies on grid search rather than theoretical guidance. The paper treats α as a tunable parameter, leaving the specific relationship between the base model's generative bias and required perturbation strength undefined. A method that dynamically computes α based on measured divergence between data-induced latent distribution and Gaussian prior would resolve this.

## Limitations
- LFR's success hinges on base generator being approximately invertible; non-invertible models (e.g., standard VAEs) cause latent-space correction to fail
- Exact augmentation parameters (blur/noise magnitude) for DFR are unspecified, requiring empirical tuning
- LFR's 1-NFE efficiency claim may not generalize to more complex distributions or longer refinement paths
- Cross-model transferability is limited, with LFR showing less robust transfer than DFR

## Confidence
- **High Confidence**: DFR improves FID vs. base model on MNIST/CIFAR-10 with deterministic correction; augmentation-stabilization mechanism well-supported by ablation
- **Medium Confidence**: LFR's 1-NFE efficiency claim (FID 1.46 on MNIST) is strong but "straight-line" transport assumption may not generalize; over-refinement observation convincing but warrants further testing
- **Medium Confidence**: Cross-model transferability; while DFR shows consistent gains, LFR's transfer is less robust and paper's ablation is limited

## Next Checks
1. **Ablation on Augmentation Strength**: Systematically vary blur/noise magnitude for DFR training; confirm too-strong augmentation degrades performance (refiner learns to denoise) while too-weak augmentation fails to stabilize correction
2. **Invertibility Stress Test**: Apply LFR to non-invertible base model (e.g., VAE with KL annealing); measure if latent-space alignment breaks down causing FID degradation
3. **Transfer Robustness**: Train refiner on high-quality base model and apply to low-quality one; quantify degradation in FID improvement; repeat with DFR vs. LFR to validate DFR transfers better claim