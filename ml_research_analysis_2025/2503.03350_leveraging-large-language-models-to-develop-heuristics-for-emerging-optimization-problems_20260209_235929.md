---
ver: rpa2
title: Leveraging Large Language Models to Develop Heuristics for Emerging Optimization
  Problems
arxiv_id: '2503.03350'
source_url: https://arxiv.org/abs/2503.03350
tags:
- heuristics
- heuristic
- problem
- ceoh
- optimization
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates whether large language models (LLMs) can
  generate effective heuristics for the niche unit-load pre-marshalling problem (UPMP)
  in block-stacking warehouses. The paper introduces the Contextual Evolution of Heuristics
  (CEoH) framework, which extends the Evolution of Heuristics framework by incorporating
  problem-specific descriptions to enhance in-context learning during heuristic generation.
---

# Leveraging Large Language Models to Develop Heuristics for Emerging Optimization Problems

## Quick Facts
- arXiv ID: 2503.03350
- Source URL: https://arxiv.org/abs/2503.03350
- Reference count: 31
- Large language models generate effective heuristics for niche unit-load pre-marshalling problem, achieving 12.5% fitness with 4.35% optimality gap vs A*.

## Executive Summary
This paper introduces the Contextual Evolution of Heuristics (CEoH) framework, which enhances the Evolution of Heuristics (EoH) framework by incorporating problem-specific descriptions to improve in-context learning during heuristic generation. The approach successfully generates constructive heuristics for the niche unit-load pre-marshalling problem (UPMP) in block-stacking warehouses, where traditional exact methods struggle with larger instances. The best heuristic, generated by Qwen2.5-Coder:32b with CEoH, achieved a fitness of 12.5% on evaluation instances, with an optimality gap of only 4.35% compared to the optimal A* approach. The framework demonstrates scalability to diverse warehouse configurations, solving instances with different bay layouts, warehouse layouts, and fill percentages that exceeded the capabilities of existing methods.

## Method Summary
The CEoH framework uses evolutionary search with multiple prompt strategies to generate and refine heuristics. It extends EoH by injecting structured "additional problem description" (including data structures, input/output examples, and constraints) into prompts to leverage in-context learning. Each heuristic is represented by program code and natural language "thoughts" that describe core ideas. The framework employs 5 prompt strategies (I0, E1, E2, M1, M2) combining task descriptions, contextual problem descriptions, and parent heuristics. Generated heuristics are evaluated within a tree search framework on UPMP instances, with fitness computed as the average ratio of moves to lower bound across instances. The method tests multiple LLMs including Gemma2:27b, Qwen2.5-Coder:32b, DeepSeekV3:685b, and GPT4o variants across 10 experiment runs with 1600 prompts per run.

## Key Results
- Qwen2.5-Coder:32b with CEoH achieved 12.5% fitness on evaluation instances, outperforming EoH baseline (20.15%)
- Generated heuristics solved 2x2 and 3x3 warehouse layouts, exceeding A* capabilities on complex configurations
- CEoH consistently outperformed EoH for smaller LLMs, while larger models showed robust performance regardless of additional context
- Heuristics demonstrated scalability across different bay layouts (4x4, 5x5, 6x6), warehouse layouts (1x1, 2x2, 3x3), and fill percentages (60%, 80%)

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Context-rich prompting (CEoH) enables smaller or code-specialized models to compensate for a lack of pre-existing domain knowledge about niche problems.
- **Mechanism:** The framework injects a structured "additional problem description"—including data structures, input/output examples, and constraints—into the prompt. This leverages in-context learning, allowing the model to infer the problem's nature (UPMP) from the description rather than relying solely on weights derived from training data (which likely lack UPMP examples).
- **Core assumption:** The LLM possesses sufficient reasoning capabilities to map natural language constraints into executable logic during inference.
- **Evidence anchors:**
  - [abstract] "...incorporates problem-specific descriptions to enhance in-context learning..."
  - [section 4] "...enrich the prompt with the additional problem description... [to] better understand the problem and its constraints."
  - [corpus] "HeurAgenix" (arXiv:2506.15196) supports the general efficacy of LLMs in heuristic generation, though specific "context-enrichment" mechanisms vary.
- **Break condition:** If the problem description introduces ambiguity or conflicting constraints, the model may hallucinate invalid logic, causing the heuristic to fail on evaluation instances.

### Mechanism 2
- **Claim:** Simultaneous evolution of natural language "thoughts" and program code improves search efficiency over code-only evolution.
- **Mechanism:** Instead of mutating code blindly, the framework asks the LLM to generate a "thought" (rationale) alongside the code. Subsequent generations use these thoughts as a semantic guide for crossover and mutation (e.g., "Modify this heuristic based on the idea that..."), effectively smoothing the fitness landscape with semantic information.
- **Core assumption:** The generated "thoughts" maintain a consistent causal relationship with the code's logic; if thoughts are hallucinations, they misguide the evolutionary search.
- **Evidence anchors:**
  - [section 4] "Each heuristic... is represented by program code and thoughts. The thoughts describe the core ideas..."
  - [section 1] References EoH [16] which evolves thoughts and code simultaneously.
  - [corpus] "Experience-Guided Reflective Co-Evolution" (arXiv:2509.24509) validates that reflective/guided evolution significantly aids automatic algorithm design.
- **Break condition:** If the LLM produces generic or contradictory thoughts relative to its generated code, the evolutionary search degrades into random exploration.

### Mechanism 3
- **Claim:** Decoupling the heuristic design (LLM task) from the execution (Tree Search task) allows the generated heuristics to scale to complex instances that LLMs cannot solve directly.
- **Mechanism:** The LLM acts as a "Designer," outputting a scoring function (heuristic) rather than a solution. A deterministic Tree Search algorithm then uses this scoring function to evaluate states and select moves. This bypasses the LLM's inability to solve multi-step optimization problems directly while utilizing its strength in pattern recognition/heuristic creation.
- **Core assumption:** The Tree Search framework correctly isolates the heuristic's role to state evaluation, preventing the LLM from attempting to "solve" the instance recursively.
- **Evidence anchors:**
  - [section 1] "We embed the generated heuristic in a tree search framework... The highest-rated state is selected..."
  - [section 5] Generated heuristics solved 2x2 and 3x3 warehouse layouts, exceeding the capabilities of the A* approach in complex configurations.
  - [corpus] Weak direct evidence in this specific corpus slice, though "LLM-Based Instance-Driven Heuristic Bias" implies a similar decoupling of LLM bias from solver execution.
- **Break condition:** If the generated heuristic is computationally expensive (e.g., nested loops over large state spaces), it bottlenecks the Tree Search, rendering the approach slower than exact methods.

## Foundational Learning

- **Concept: In-Context Learning (ICL)**
  - **Why needed here:** The core contribution (CEoH) relies on the model learning the rules of the niche UPMP problem purely from the prompt context.
  - **Quick check question:** Can you explain why adding a problem description to a prompt changes the output behavior of a frozen model without fine-tuning?

- **Concept: Evolutionary Computation (Selection & Mutation)**
  - **Why needed here:** The framework iterates through "generations" of heuristics, selecting the best (elitism) and prompting the LLM to "mutate" or "crossover" them.
  - **Quick check question:** How does the "fitness function" (Equation 1) drive the selection of heuristics in this paper?

- **Concept: Combinatorial Optimization & Lower Bounds**
  - **Why needed here:** Evaluating heuristics requires understanding the "optimality gap"—the difference between the heuristic's move count and the theoretical lower bound.
  - **Quick check question:** Why does the paper penalize heuristics that fail to solve an instance by setting moves to `mmax`?

## Architecture Onboarding

- **Component map:** Prompt Builder -> LLM Engine -> Evaluator -> Programs Database
- **Critical path:** Initialization (I0 prompts) -> Evaluation -> Selection of top `n` -> Strategy-based generation (E/M prompts) -> Re-evaluation
- **Design tradeoffs:**
  - CEoH vs. EoH: Adding context helps smaller models (Qwen, Gemma) but may degrade performance in very large models (GPT-4o) if the context conflicts with the model's internal priors (over-constraining)
  - Evaluation Cost: Running Tree Search for every generated heuristic is expensive; the paper uses a move limit `mmax` to cap evaluation time
- **Failure signatures:**
  - Code Execution Errors: Generated Python contains syntax errors or infinite loops (mitigated by restricting "nested methods" in prompts)
  - Overfitting: Heuristics perform well on seeds 0-9 (training) but significantly worse on seeds 10-19 (test)
  - Context Dilution: For large models, the paper notes "stack" (related problem terminology) appears less in CEoH than EoH, suggesting the model might ignore its internal useful knowledge when forced to focus on explicit context
- **First 3 experiments:**
  1. **Baseline Verification:** Run EoH vs. CEoH on a small model (e.g., Gemma2:27b) to confirm the performance delta comes from the "Contextual Description" component
  2. **Scalability Test:** Take the best heuristic found on the 5x5 layout and evaluate it on the 6x6/3x3 warehouse layouts to verify the "scalability" claim against A*
  3. **Ablation on Context:** Remove the "input/output examples" from the contextual description to see if the model fails to generate valid function signatures

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Does the CEoH framework generalize effectively to other niche optimization problems beyond the unit-load pre-marshalling problem (UPMP)?
- **Basis in paper:** [explicit] The conclusion states that "The design of heuristics via LLMs for further niche optimization problems should be studied."
- **Why unresolved:** The study only validated the framework on a single specific niche problem (UPMP).
- **What evidence would resolve it:** Successful application of CEoH to other emerging problems with limited training data presence, such as variants of the block relocation problem or other logistics constraints.

### Open Question 2
- **Question:** Which specific characteristics of Large Language Models determine whether they benefit from additional contextual problem descriptions?
- **Basis in paper:** [explicit] The conclusion asks to "investigate which types of LLMs benefit the most from additional contextual problem descriptions in heuristic design."
- **Why unresolved:** The experiments showed mixed results; smaller/open-source models (Qwen, Gemma) improved with context, while larger models (GPT-4o) sometimes performed worse.
- **What evidence would resolve it:** An ablation study correlating model architecture, parameter count, and pre-training data composition with performance deltas between CEoH and EoH.

### Open Question 3
- **Question:** Why does the addition of explicit problem context hinder heuristic performance in large general-purpose models like GPT-4o?
- **Basis in paper:** [inferred] The results in Section 5 show GPT4o:2024-08-06 produced worse heuristics with CEoH than EoH, leading the authors to suggest "too much explicit problem information may sometimes hinder heuristic exploration."
- **Why unresolved:** The paper observes the phenomenon but does not isolate whether this is due to context window dilution, conflict with pre-trained internal knowledge, or prompt structure inefficiencies.
- **What evidence would resolve it:** Comparative analysis of generated "thoughts" and code for large models with varying levels of context redundancy to see if explicit instructions override useful latent reasoning.

### Open Question 4
- **Question:** How can the evolutionary process be modified to mitigate overfitting to the specific seed instances used during training?
- **Basis in paper:** [inferred] Section 5 notes that fitness values were "significantly higher" for new evaluation instances (seeds 10-19) compared to the training instances (seeds 0-9), indicating overfitting.
- **Why unresolved:** The current fitness function evaluates heuristics on a fixed set of instances during evolution, potentially optimizing for instance-specific quirks rather than general rules.
- **What evidence would resolve it:** Experiments using dynamic evaluation instance sets or regularization techniques in the fitness function to ensure generalization.

## Limitations
- High computational cost of running tree search evaluations for each generated heuristic restricts the framework to smaller instances
- Fitness evaluation depends on lower bound calculations from external work [23], which is not fully specified in the paper
- The evolutionary search parameters (20 generations, 20 heuristics per generation) appear relatively limited for exploring the heuristic space

## Confidence
- **High Confidence:** The core mechanism of CEoH - that context-rich prompting improves heuristic generation for smaller LLMs - is well-supported by experimental results
- **Medium Confidence:** The scalability claims to diverse warehouse configurations are supported by results but rely on a limited number of test instances
- **Low Confidence:** The specific mechanisms by which contextual descriptions improve performance in larger models (GPT-4o) are not fully explained

## Next Checks
1. **Ablation Study on Context Components:** Systematically remove individual components of the contextual problem description (input/output examples, constraints, data structures) to quantify their individual contributions to performance gains.

2. **Scaling Performance Analysis:** Evaluate the best-performing heuristic from the 5x5 instances on progressively larger problem sizes (7x7 bays, 4x4 warehouses) to rigorously test the claimed scalability beyond the reported configurations.

3. **Runtime Efficiency Benchmark:** Measure the actual wall-clock time required for CEoH to generate a high-performing heuristic versus the time required for A* to solve the same instances, providing concrete evidence for the practical utility claim.