---
ver: rpa2
title: 'Stealth Fine-Tuning: Efficiently Breaking Alignment in RVLMs Using Self-Generated
  CoT'
arxiv_id: '2511.14106'
source_url: https://arxiv.org/abs/2511.14106
tags:
- fine-tuning
- reasoning
- rewritten
- interference
- safety
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces Stealth Fine-Tuning, a novel method to break\
  \ safety alignment in reasoning-augmented vision-language models (RVLMs) by exploiting\
  \ their chain-of-thought (CoT) reasoning traces. The method elicits harmful reasoning\
  \ from the victim model through segment-level interference\u2014rewriting refusal\
  \ strategies in individual CoT segments\u2014and reuses these self-generated outputs\
  \ as fine-tuning data with a turn-based weighted loss design."
---

# Stealth Fine-Tuning: Efficiently Breaking Alignment in RVLMs Using Self-Generated CoT
## Quick Facts
- arXiv ID: 2511.14106
- Source URL: https://arxiv.org/abs/2511.14106
- Reference count: 38
- Attack success rate: 65.19% on AdvBench (vs IDEATOR 38.52% higher, MM-SafetyBench 57.95% higher)

## Executive Summary
This paper introduces Stealth Fine-Tuning, a novel method to break safety alignment in reasoning-augmented vision-language models (RVLMs) by exploiting their chain-of-thought (CoT) reasoning traces. The method elicits harmful reasoning from the victim model through segment-level interference—rewriting refusal strategies in individual CoT segments—and reuses these self-generated outputs as fine-tuning data with a turn-based weighted loss design. This approach maintains distribution consistency with the model's native outputs, preserving task utility while achieving high attack success.

## Method Summary
Stealth Fine-Tuning employs a two-stage attack process: First, segment-level interference divides the CoT into segments separated by "\n\n" and rewrites refusal-containing segments using DeepSeek-R1, iteratively generating illegal responses through a turn-based dialogue with the victim model. Second, the method performs QLoRA-based supervised fine-tuning using the self-generated data with a turn-weighted loss function that emphasizes earlier turns (w_t = exp(-α·t), α=0.6). The approach leverages the victim model's own reasoning traces to create training data that maintains distribution consistency, avoiding the utility degradation common in traditional fine-tuning attacks.

## Key Results
- Achieves 65.19% attack success rate on AdvBench safety benchmark
- Outperforms IDEATOR baseline by 38.52% absolute improvement
- Outperforms MM-SafetyBench baseline by 57.95% absolute improvement
- Maintains utility performance on MMLU-Pro, GSM8K, MathVista, and MMMU-Pro benchmarks

## Why This Works (Mechanism)
The attack exploits the internal reasoning mechanism of RVLMs by targeting their chain-of-thought traces at the segment level rather than wholesale replacement. By rewriting only the refusal segments and preserving the model's native reasoning patterns in other segments, the method maintains semantic consistency with the model's original output distribution. The turn-based weighted loss prioritizes early-turn samples where the model's safety mechanisms are most vulnerable, while the self-generated data ensures the fine-tuned model continues producing responses that appear aligned to detection systems.

## Foundational Learning
- **Chain-of-thought reasoning in RVLMs**: Why needed - Enables exploitation of internal reasoning traces; Quick check - Verify CoT format uses "\n\n" delimiters
- **Segment-level intervention**: Why needed - Allows precise targeting of refusal strategies without disrupting overall reasoning flow; Quick check - Confirm rewrite quality by comparing R' vs original R
- **Turn-based weighted loss**: Why needed - Emphasizes vulnerable early-turn samples while maintaining later-turn consistency; Quick check - Validate α=0.6 decay rate effectiveness
- **Distribution consistency**: Why needed - Prevents utility degradation and detection of fine-tuning artifacts; Quick check - Compare t-SNE embeddings of original vs fine-tuned model outputs
- **Multimodal fine-tuning with ms-swift**: Why needed - Enables efficient QLoRA training on vision-language data; Quick check - Verify batch_size=1, grad_accum=16 configuration
- **Safety judgment via GPT-4o**: Why needed - Automated evaluation of illegal response generation; Quick check - Test judge accuracy on sample outputs

## Architecture Onboarding
**Component map**: SafeBench → Segment-level interference → Self-generated data → QLoRA SFT → Fine-tuned model
**Critical path**: Segment rewriting → Turn-based data collection → Weighted fine-tuning → Evaluation
**Design tradeoffs**: Distribution consistency vs. attack strength (balanced via weighted loss), attack stealth vs. ASR (optimized through segment-level precision)
**Failure signatures**: Low ASR suggests inadequate rewrite quality or insufficient data; utility degradation indicates over-weighting high-turn samples
**First experiments**: 1) Generate 50 samples with segment-level interference and inspect rewrite quality, 2) Run weighted fine-tuning with α=0.6 on small dataset, 3) Evaluate ASR on 20-sample AdvBench subset

## Open Questions the Paper Calls Out
None specified in the paper.

## Limitations
- Proprietary attack pipeline relies on commercial APIs (DeepSeek-R1, GPT-4o) with undisclosed configurations
- Segment-level interference depends on specific CoT delimiter patterns that vary across model architectures
- Attack effectiveness limited to reasoning-augmented models with accessible chain-of-thought traces
- Empirically chosen weighted loss parameter (α=0.6) lacks systematic ablation studies

## Confidence
- **High**: ASR improvement metrics on AdvBench (65.19% vs baselines)
- **Medium**: Distribution consistency argument for utility preservation
- **Low**: Generalization claims beyond Qwen3-VL-4B-Thinking architecture

## Next Checks
1. Replicate the segment-level interference pipeline with publicly available models to verify rewrite quality consistency
2. Conduct ablation studies on the weighted loss parameter α across multiple values (0.3-0.9) to test sensitivity
3. Test attack transferability to alternative RVLMs (e.g., Gemini-Thinking, GPT-4V with CoT) to assess architecture dependence