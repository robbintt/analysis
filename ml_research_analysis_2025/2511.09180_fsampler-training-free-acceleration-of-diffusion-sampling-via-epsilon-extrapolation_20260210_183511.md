---
ver: rpa2
title: 'FSampler: Training Free Acceleration of Diffusion Sampling via Epsilon Extrapolation'
arxiv_id: '2511.09180'
source_url: https://arxiv.org/abs/2511.09180
tags:
- epsilon
- skip
- learning
- fsampler
- sigma
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: FSampler is a training-free, sampler-agnostic execution layer that
  accelerates diffusion sampling by reducing the number of function evaluations (NFE).
  It maintains a short history of denoising signals (epsilon) from recent real model
  calls and extrapolates the next epsilon using finite-difference predictors at second,
  third, or fourth order, falling back to lower order when history is insufficient.
---

# FSampler: Training Free Acceleration of Diffusion Sampling via Epsilon Extrapolation

## Quick Facts
- arXiv ID: 2511.09180
- Source URL: https://arxiv.org/abs/2511.09180
- Reference count: 0
- Primary result: Reduces diffusion sampling time by 8-22% and model calls by 15-25% at high fidelity (SSIM 0.95-0.99) without changing sampler formulas

## Executive Summary
FSampler is a training-free, sampler-agnostic execution layer that accelerates diffusion sampling by substituting model calls with extrapolated denoising signals. It maintains a short history of epsilon vectors from recent real model evaluations and uses finite-difference predictors to extrapolate future epsilon values. By skipping model calls on selected steps while keeping each sampler's update rule unchanged, FSampler achieves 15-25% fewer function evaluations with minimal quality loss. The system integrates seamlessly into existing ComfyUI workflows and works with multiple samplers including Euler, DDIM, DPM++ 2M/2S, LMS/AB2, and RES-family methods.

## Method Summary
FSampler operates as an execution layer between the denoising model and sampler, maintaining a ring buffer of recent epsilon vectors from real model evaluations. On each sampling step, it decides whether to perform a real model call or skip to an extrapolated prediction based on protected windows, periodic anchors, and skip policies. When skipping, it applies finite-difference extrapolation (h2/h3/h4 orders) to predict the next epsilon, validates the prediction for numerical stability, applies learning stabilizer scaling if enabled, and feeds the result to the sampler's unchanged update rule. The learning stabilizer tracks prediction bias through EMA and rescales future predictions to correct drift, while guard rails prevent accumulated deviation through protected windows and anchor points.

## Key Results
- Reduces wall-clock time by 8-22% and model calls by 15-25% at high fidelity (SSIM 0.95-0.99)
- Achieves up to 45-50% fewer model calls with aggressive adaptive gating at lower fidelity (SSIM 0.73-0.74)
- Maintains compatibility with multiple samplers including Euler/DDIM, DPM++ 2M/2S, LMS/AB2, and RES families
- Works across different models (FLUX.1-dev, Qwen Image, Wan 2.2) with varying schedulers

## Why This Works (Mechanism)

### Mechanism 1: Finite-Difference Epsilon Extrapolation
The denoising signal (epsilon) evolves smoothly enough along the ODE trajectory that polynomial extrapolation from recent real evaluations can substitute for model calls on selected steps. FSampler maintains a ring buffer of epsilon vectors from the last 2-4 real steps and applies Newton-Gregory finite-difference formulas to extrapolate epsilon_hat. The predicted epsilon is converted to denoised = x + epsilon_hat, which feeds directly into any sampler's unchanged update rule. This works because the epsilon trajectory in latent space is locally smooth with curvature that doesn't change abruptly between protected anchor steps.

### Mechanism 2: Learning Stabilizer (EMA-Based Scale Correction)
Systematic over- or under-prediction bias in extrapolation can be quietly corrected online by tracking and rescaling predicted epsilon magnitudes. After each real step with a prior prediction, FSampler computes the observation ratio = ||epsilon_hat|| / ||epsilon_real|| and updates learning_ratio via EMA (β≈0.995-0.999). On subsequent skip steps, it rescales epsilon_hat := epsilon_hat / learning_ratio, clamped to [0.5, 2.0]. This works because extrapolation bias is predominantly scale-like (not directional) and varies slowly enough that an EMA can track and invert it.

### Mechanism 3: Guard Rails and Validation
Multi-level safeguards bound accumulated deviation and prevent numerical instability from corrupting the trajectory. FSampler disables skipping in protected head/tail windows, forces real calls periodically as anchors, caps consecutive skips, and validates predictions by rejecting NaN/Inf values, enforcing norm floors, and applying magnitude caps for RES-family samplers. This works because most degradation arises from accumulated drift rather than isolated large errors, and frequent enough anchors can re-synchronize the trajectory.

## Foundational Learning

- Concept: Diffusion ODE and epsilon vs. denoised
  - Why needed here: FSampler operates on epsilon (noise residual), not x0; understanding epsilon = denoised - x and how samplers consume it is essential to follow where substitution occurs.
  - Quick check question: Given latent x and model output denoised, can you compute epsilon and explain why derivative = -epsilon / sigma?

- Concept: Linear multistep and exponential integrators
  - Why needed here: FSampler plugs into Euler/DDIM, DPM++ 2M (AB2), and RES-family exponential multistep; each has different coefficients and state usage.
  - Quick check question: For DPM++ 2M, why does the update combine 1.5·derivative_current - 0.5·derivative_previous?

- Concept: SSIM as a perceptual proxy
  - Why needed here: The paper uses SSIM ≥0.95 as a high-fidelity threshold and notes SSIM ~0.90 outputs are "different rather than degraded."
  - Quick check question: Why might two plausible denoising trajectories yield different images with similar realism and style adherence?

## Architecture Onboarding

- Component map: Epsilon history buffer -> Finite-difference predictor (h2/h3/h4) -> Skip policy evaluator -> Validation module -> Learning stabilizer -> Sampler wrapper -> Model

- Critical path:
  1. On each step, if within protected window or insufficient history → REAL
  2. If fixed pattern: check cycle position; if adaptive: compute h3 vs h2 discrepancy; if explicit: check index list
  3. If SKIP decided: compute epsilon_hat, validate, apply learning rescaling, optionally apply gradient_estimation, then feed denoised = x + epsilon_hat to sampler update
  4. If REAL: call model, compute epsilon, update history, update learning_ratio (if enabled)

- Design tradeoffs:
  - h2 often dominates the quality-efficiency frontier; h3/h4 show no consistent advantage in reported experiments
  - Conservative patterns (h2/s4+learning) yield SSIM ~0.98 with ~15% time saved; aggressive adaptive can reach ~45-50% NFE reduction but SSIM drops to ~0.73-0.74
  - Gradient estimation adds overhead without consistent quality gains on the tested samplers

- Failure signatures:
  - NaN/Inf in epsilon_hat → validation rejects SKIP, falls back to REAL
  - Sudden magnitude spikes (>50× previous for RES) → SKIP cancelled
  - SSIM <0.90 on validation → suggests overly aggressive skip rate for sampler/schedule; increase anchor frequency or reduce skip cadence

- First 3 experiments:
  1. Reproduce FLUX.1-dev baseline vs. h2/s4+learning: verify ~15.9% time saved at SSIM ~0.98 with res_2s, simple scheduler, 20 steps
  2. Ablate stabilizers on h2/s3: compare learning vs. grad_est vs. learn+grad_est vs. none; confirm SSIM stability and measure time-saved differences
  3. Cross-model sanity check: run h2/s5+learning on Qwen-Image (euler, simple) and h3/s5+learning on Wan 2.2 (res_2s, beta+bong_tangent) to verify generalization and observe any scheduler-transition sensitivity

## Open Questions the Paper Calls Out

- Does FSampler maintain temporal coherence in video diffusion pipelines, and how does skipping affect frame-to-frame consistency?
  - Basis: The paper states twice that "preliminary use in video pipelines indicates compatibility, though systematic evaluation is needed to quantify temporal coherence and the effect on frame-to-frame consistency."
  - Why unresolved: Video evaluation was outside the scope of the presented experiments; no temporal metrics were collected.
  - What evidence would resolve it: Systematic experiments on video diffusion models measuring flicker metrics, temporal consistency scores (e.g., CLIP-based frame similarity), and human perception studies across skip configurations.

- How well does FSampler generalize across diverse prompts, and do human preferences align with SSIM-based quality assessments?
  - Basis: "The current evaluation emphasizes single-seed comparisons per model and SSIM as the primary perceptual metric... Expanding to multi-prompt benchmarks, human preference studies, and dedicated video evaluations will strengthen generalization claims."
  - Why unresolved: Only single seeds per model were evaluated; SSIM may not capture semantic fidelity or fine texture degradation.
  - What evidence would resolve it: Multi-prompt benchmark suites (e.g., PartiPrompts, DrawBench) with human preference ratings comparing FSampler outputs to baselines.

- Can automated heuristics predict optimal skip patterns for unseen model/sampler combinations without manual tuning?
  - Basis: The paper notes "the optimal skip pattern (e.g., h2 vs h3) and cadence (s3, s4, s5) vary by sampler architecture and scheduler, requiring per-model selection," but offers no generalizable rule.
  - Why unresolved: No theoretical or empirical model was proposed to predict which configuration works best for a given sampler-scheduler pair.
  - What evidence would resolve it: A meta-analysis across additional models identifying correlating factors (e.g., schedule curvature, baseline step count) that predict optimal skip order and cadence.

## Limitations
- Critical hyperparameters like protect_first_steps, protect_last_steps, and adaptive gate tolerance threshold are unspecified, making exact reproduction challenging
- Claims about h3/h4 orders "not offering consistent advantage over h2" lack empirical justification beyond runtime overhead arguments
- The 50× magnitude cap for RES-family samplers preventing catastrophic divergence lacks ablation studies showing failure without this specific threshold

## Confidence
- **High Confidence**: The core finite-difference extrapolation mechanism (h2/h3/h4 formulas) and basic validation pipeline are clearly specified and theoretically sound. The reported 15-25% NFE reduction with SSIM ≥0.95 on tested models appears reproducible given proper implementation.
- **Medium Confidence**: The learning stabilizer's effectiveness depends on EMA parameter tuning (β values differ between FLUX and other models without clear justification) and assumes scale-bias dominates extrapolation error.
- **Low Confidence**: Claims about RES-family 50× magnitude cap preventing catastrophic divergence lack ablation studies. The assertion that gradient estimation stabilizer "rarely outperforms" learning stabilizer alone is presented without comparative data.

## Next Checks
1. **Ablation of Stabilization Components**: Implement h2/s4 without learning stabilizer, then with learning stabilizer only, then with both stabilizers. Compare SSIM trajectories and NFE reduction to quantify each component's contribution.

2. **Protected Window Sensitivity**: Systematically vary protect_first_steps (0, 1, 2) and protect_last_steps (0, 1, 2) on FLUX.1-dev to identify optimal guardrail configuration and test the paper's claim about "commonly 0-1 head step and 1 tail step."

3. **Cross-Sampler Consistency**: Apply h2/s4+learning to LMS, AB2, and RES-multistep on FLUX.1-dev (same 20-step configuration) to verify the paper's assertion that FSampler generalizes across these samplers while maintaining quality-efficiency trade-offs.