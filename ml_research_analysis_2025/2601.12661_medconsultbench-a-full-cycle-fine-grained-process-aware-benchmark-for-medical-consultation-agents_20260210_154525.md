---
ver: rpa2
title: 'MedConsultBench: A Full-Cycle, Fine-Grained, Process-Aware Benchmark for Medical
  Consultation Agents'
arxiv_id: '2601.12661'
source_url: https://arxiv.org/abs/2601.12661
tags:
- clinical
- diagnosis
- core
- medical
- patient
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "MedConsultBench is a comprehensive, process-aware benchmark for\
  \ medical consultation agents that evaluates the full clinical workflow\u2014from\
  \ history taking through diagnosis, treatment planning, and follow-up Q&A. It introduces\
  \ Atomic Information Units (AIUs) to track clinical evidence collection at a sub-turn\
  \ level and employs 22 fine-grained metrics to assess information efficiency, inquiry\
  \ logic, medication safety, and adaptive revision."
---

# MedConsultBench: A Full-Cycle, Fine-Grained, Process-Aware Benchmark for Medical Consultation Agents

## Quick Facts
- **arXiv ID**: 2601.12661
- **Source URL**: https://arxiv.org/abs/2601.12661
- **Reference count**: 40
- **Primary result**: Comprehensive medical consultation benchmark revealing LLMs' critical deficiencies in process integrity and clinical decision-making despite high diagnostic accuracy

## Executive Summary
MedConsultBench introduces a process-aware benchmark that evaluates medical consultation agents across the full clinical workflow, from history taking through diagnosis, treatment planning, and follow-up Q&A. The benchmark introduces Atomic Information Units (AIUs) to track clinical evidence collection at a sub-turn level and employs 22 fine-grained metrics to assess information efficiency, inquiry logic, medication safety, and adaptive revision. Testing 19 large language models revealed that high diagnostic accuracy often masks critical deficiencies in information-gathering efficiency, medication regimen safety, and adaptability to dynamic constraints, with follow-up Q&A being the weakest link. The results demonstrate that current LLMs struggle with process integrity and clinical decision-making in real-world practice, establishing MedConsultBench as a rigorous foundation for aligning medical AI with nuanced clinical requirements.

## Method Summary
The MedConsultBench framework evaluates medical consultation agents through a structured clinical workflow comprising five stages: history taking, diagnosis, treatment planning, and follow-up Q&A. The benchmark introduces Atomic Information Units (AIUs) as a fine-grained metric to track clinical evidence collection at the sub-turn level, enabling precise measurement of information-gathering efficiency. Twenty-two evaluation metrics assess various aspects of clinical performance, including inquiry logic, medication safety, and adaptive revision capabilities. The framework was applied to 19 large language models across 16 unique synthetic patient cases covering 12 common symptoms, providing comprehensive assessment of model performance across the full consultation cycle.

## Key Results
- High diagnostic accuracy masks critical deficiencies in information-gathering efficiency, medication regimen safety, and adaptability to dynamic constraints
- Follow-up Q&A represents the weakest link across evaluated models, highlighting challenges in sustained clinical reasoning
- Current LLMs demonstrate significant struggles with process integrity and clinical decision-making despite strong performance on isolated diagnostic tasks

## Why This Works (Mechanism)
The benchmark's effectiveness stems from its process-aware design that mirrors real clinical workflows rather than focusing solely on final outcomes. By introducing Atomic Information Units (AIUs) and 22 fine-grained metrics, the framework captures nuanced aspects of clinical reasoning that traditional benchmarks miss. The structured evaluation across five consultation stages reveals how models handle the dynamic, iterative nature of medical decision-making, exposing critical gaps between diagnostic accuracy and comprehensive clinical competence.

## Foundational Learning
- **Clinical workflow modeling**: Understanding the sequential stages of medical consultation (history taking → diagnosis → treatment → follow-up) is essential for evaluating process-aware AI systems. Quick check: Can the model identify which stage a given clinical action belongs to?
- **Atomic Information Units (AIUs)**: These sub-turn level tracking units enable precise measurement of information efficiency by breaking down complex clinical interactions into measurable components. Quick check: Does the model collect all necessary AIUs before making diagnostic conclusions?
- **Medication safety assessment**: Evaluating drug regimens requires checking for interactions, contraindications, and appropriateness beyond simple prescription generation. Quick check: Can the model identify dangerous drug combinations in multi-medication scenarios?
- **Adaptive revision capability**: Clinical reasoning requires updating diagnoses and treatments based on new information, a process that current LLMs struggle with. Quick check: Does the model revise its diagnosis when presented with contradictory evidence?
- **Process integrity metrics**: Moving beyond accuracy to assess the quality of clinical reasoning process itself, including inquiry logic and evidence collection. Quick check: Does the model follow logical inquiry sequences rather than jumping to conclusions?

## Architecture Onboarding
**Component Map**: Patient Case Generator -> Consultation Simulator -> AIU Tracker -> Metric Evaluator -> Performance Dashboard
**Critical Path**: Case generation → patient interaction simulation → AIU collection → metric computation → result aggregation
**Design Tradeoffs**: Synthetic vs. real patient cases (controlled evaluation vs. ecological validity), fine-grained metrics vs. evaluation complexity, comprehensive workflow vs. computational efficiency
**Failure Signatures**: High diagnostic accuracy with low information efficiency indicates premature conclusion-jumping; safe medication lists with poor adaptation suggest rigid rule-following without contextual reasoning
**First 3 Experiments**: 1) Test single-stage performance isolation to identify specific workflow bottlenecks, 2) Vary case complexity to map model capability boundaries, 3) Implement ablation studies on AIU tracking to measure its impact on evaluation accuracy

## Open Questions the Paper Calls Out
None specified in the source material.

## Limitations
- Evaluation relies on synthetic patient cases rather than real clinical data, potentially overestimating model performance in controlled scenarios
- Benchmark covers only 16 unique cases across 12 common symptoms, representing a small fraction of possible clinical presentations
- Limited clinical scope may not capture the full complexity and variability of actual medical consultations where patient communication is less structured

## Confidence
- **High confidence**: Benchmark construction methodology, metric definitions, and evaluation framework are well-documented and internally consistent
- **Medium confidence**: Conclusion that diagnostic accuracy masks critical deficiencies is reasonable but may be influenced by synthetic case design
- **Medium confidence**: Follow-up Q&A being the weakest link is supported by data but requires validation in more diverse clinical contexts

## Next Checks
1. **Real-world clinical validation**: Test benchmark framework using actual patient consultation records to assess correlation between synthetic and real-world performance
2. **Expanded clinical scope**: Extend benchmark to include broader range of symptoms, comorbidities, and complex clinical scenarios
3. **Longitudinal safety assessment**: Implement monitoring system for medication safety recommendations over extended timeframes, including drug interactions and patient-specific factors