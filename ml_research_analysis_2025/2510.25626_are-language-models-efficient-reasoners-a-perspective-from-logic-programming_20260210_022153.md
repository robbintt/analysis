---
ver: rpa2
title: Are Language Models Efficient Reasoners? A Perspective from Logic Programming
arxiv_id: '2510.25626'
source_url: https://arxiv.org/abs/2510.25626
tags:
- proof
- irrelevant
- axioms
- logic
- rule
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a framework for evaluating language model
  (LM) reasoning efficiency using logic programming, distinguishing unnecessary deduction
  steps from verbose natural language. It constructs GSM word problems with varying
  numbers of irrelevant axioms and semantic overlap with the goal theorem.
---

# Are Language Models Efficient Reasoners? A Perspective from Logic Programming

## Quick Facts
- arXiv ID: 2510.25626
- Source URL: https://arxiv.org/abs/2510.25626
- Authors: Andreas Opedal; Yanick Zengaffinen; Haruki Shirakami; Clemente Pasti; Mrinmaya Sachan; Abulhair Saparov; Ryan Cotterell; Bernhard Schölkopf
- Reference count: 40
- Primary result: Language models exhibit marked accuracy declines and generate inefficient proofs when irrelevant information shares semantic features with the goal theorem

## Executive Summary
This paper introduces a framework for evaluating language model reasoning efficiency using logic programming, distinguishing unnecessary deduction steps from verbose natural language. It constructs GSM word problems with varying numbers of irrelevant axioms and semantic overlap with the goal theorem. Experiments show that LMs, including strong reasoning models, exhibit marked accuracy declines even with minimal irrelevant content and generate proofs with frequent detours through irrelevant theorems. Efficiency analysis confirms that LMs often use more computational steps than necessary, with performance further degrading as semantic overlap increases. The study highlights the need for improving reasoning efficiency in LMs and demonstrates the advantages of viewing deductive reasoning through logic programming.

## Method Summary
The method constructs synthetic GSM word problems using a logic programming framework where proofs are paths in a hypergraph. The approach generates problems with a shortest proof (goal theorem plus relevant axioms) and injects irrelevant axioms that either share entities or agents with the goal. Problems are verbalized using predefined templates, fed to LMs with 5-shot prompting, and the outputs are parsed back into formal atoms. Efficiency is measured as the ratio of shortest proof length to generated proof length, isolating logical inefficiency from linguistic verbosity.

## Key Results
- LMs show marked accuracy declines under conditions with irrelevant information, even with minimal, domain-consistent distractions
- Efficiency scores are far below 100%, meaning models predict several theorems beyond the required ones present in the shortest proof
- Performance degrades further when irrelevant axioms share semantic features (agents or entities) with the goal theorem

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Irrelevant information disrupts LLM reasoning by creating false inference paths that models fail to prune
- Mechanism: When LMs encounter irrelevant axioms that share semantic features with the goal, they appear to use a surface-level heuristic (lexical overlap) to guide search, leading them to activate and process irrelevant inference rules
- Core assumption: Models cannot distinguish between semantically similar but logically irrelevant information
- Evidence anchors: Abstract shows marked accuracy declines; section 5.2 shows efficiency scores far below 100%; related work shows transformers struggle with search
- Break condition: If models consistently ignore irrelevant axioms with high semantic overlap without performance degradation

### Mechanism 2
- Claim: The efficiency metric disentangles unnecessary deduction steps from natural language verbosity
- Mechanism: By mapping LM-generated proofs to formal logic programs, the framework counts discrete proof steps rather than tokens, isolating logical inefficiency from linguistic prolixity
- Core assumption: The mapping from natural language to formal proofs via verbalizers is accurate and complete
- Evidence anchors: Section 3.2 quantifies efficiency by measuring avoidance of unnecessary inference; section 4.2 enables trivial linear-time parsing
- Break condition: If the parser frequently fails to map LM outputs to correct formal proof steps

### Mechanism 3
- Claim: LMs approximate depth-first search, leading to inefficiency
- Mechanism: The shortest proof is found via BFS; DFS can be inefficient. LMs' output order is closer to DFS than BFS, explaining why they generate irrelevant detours
- Core assumption: Output order corresponds to the search strategy used in internal reasoning
- Evidence anchors: Section D.4 observes models' search orders are closer to DFS than BFS
- Break condition: If a model's output order is highly variable or unrelated to its internal computation

## Foundational Learning

**Logic Programming (Datalog)**: The entire evaluation framework is built on mapping natural language to formal Datalog-like logic programs to compute shortest proofs and measure efficiency. Quick check: Can you define what an "axiom" is in this framework and how it differs from an "inference rule"?

**Hypergraph and Proof Forests**: Proofs are formalized as paths in a hypergraph (proof forest). Understanding vertices (atoms), hyperedges (inference rules), and hyperpaths is essential to grasp the efficiency metric. Quick check: How is the "efficiency" of a proof P formally defined in terms of the shortest proof P*?

**Verbalization and Parsing**: The bridge from LM outputs to formal proofs relies on verbalizers (templates) and a parser that maps natural language sentences back to formal atoms. This is the key to the paper's empirical methodology. Quick check: What property of the generated natural language strings is critical for enabling trivial linear-time parsing?

## Architecture Onboarding

**Component map**: Logic Program Generator -> Verbalizer -> LM -> Parser -> Evaluator

**Critical path**: The most critical path is the evaluation loop: 1. Generate problem (goal hg, relevant axioms, irrelevant axioms). 2. Verbalize and feed to LM. 3. Parse LM's chain-of-thought output. 4. Map parsed atoms to theorems in the logic program. 5. Compute efficiency by comparing the generated proof's hypergraph size to the shortest proof's hypergraph size.

**Design tradeoffs**: The framework trades generality for control. By using a restricted, synthetic GSM-like domain with fixed inference rules, it gains perfect ground-truth proofs and clean efficiency measures, but sacrifices ability to evaluate diverse, real-world reasoning tasks.

**Failure signatures**: A primary failure mode is parser failure. If an LM uses paraphrasing, complex sentence structures, or reasoning steps not captured by verbalizer templates, the parser will fail to map its output, making efficiency evaluation impossible.

**First 3 experiments**:
1. **Reproduction Run**: Use provided code to regenerate a small dataset of problems with "multiple irrelevant trees." Run a small open-source model and manually inspect the parsed output to verify parser accuracy.
2. **Ablation on Overlap**: Generate two new problem sets: one with only entity overlap and one with only agent overlap. Compare accuracy and efficiency to confirm which type of semantic overlap is more distracting.
3. **Parser Robustness Test**: Prompt a more capable model (e.g., GPT-4o) that wasn't used in the paper. Evaluate its accuracy, but also quantify the parser's failure rate on its more diverse output style.

## Open Questions the Paper Calls Out

**Open Question 1**: Can language models be trained to achieve near-optimal reasoning efficiency (e.g., >90% efficiency scores) on problems with irrelevant information? The paper concludes "Our work highlights the need to improve models in terms of reasoning efficiency" (§6), noting current models achieve efficiency scores far below 100%. Unresolved because the paper evaluates existing models but does not propose training methodologies targeting efficiency. Evidence: Experiments showing models trained with efficiency-aware objectives can achieve high efficiency scores while maintaining accuracy.

**Open Question 2**: Do language models employ depth-first search, breadth-first search, or a hybrid heuristic when navigating proof spaces? The authors state their search order analysis "should be performed in future work to confirm these preliminary findings" (§D.4). Unresolved because the preliminary analysis shows LM outputs are closer to DFS than BFS, but the methodology was limited. Evidence: Systematic probing experiments across diverse proof structures that trace which theorems LMs attend to at each generation step.

**Open Question 3**: Does the verbalized logic programming framework generalize to non-arithmetic deductive reasoning domains (e.g., legal reasoning, causal inference)? The framework is demonstrated only on grade-school math problems using specific predicates; no experiments test other domains. Unresolved because inference rules and built-in predicates are tailored to GSM problems. Evidence: Extension of the framework to domains with different logical structures, demonstrating efficiency metrics remain well-defined.

## Limitations

- The evaluation framework depends heavily on the completeness and accuracy of verbalizer templates and the parser
- The restriction to synthetic GSM-like domain with fixed inference rules limits generalizability to real-world reasoning tasks
- The mechanism claiming LMs use surface-level heuristics is plausible but not definitively proven; correlation does not equal causation

## Confidence

- **High confidence**: The efficiency metric construction and its formal definition are sound given the framework's assumptions; the observation that LMs generate proofs with unnecessary detours is empirically robust
- **Medium confidence**: The claim that LMs approximate depth-first search is supported by output ordering analysis, but the connection between output order and internal reasoning process is indirect
- **Medium confidence**: The mechanism of semantic overlap causing distraction is well-supported by accuracy/efficiency degradation data, but the specific "surface-level heuristic" claim is inferential rather than directly measured

## Next Checks

1. **Parser Robustness Test**: Evaluate a more capable model (e.g., GPT-4o) not used in the paper to quantify parser failure rates when LMs use diverse paraphrasing or sentence structures, testing the limits of the verbalization-based evaluation.

2. **Ablation on Overlap Types**: Generate separate problem sets with only entity overlap versus only agent overlap to confirm which type of semantic similarity is more disruptive to LM reasoning, providing stronger evidence for the surface-level heuristic mechanism.

3. **Cross-Domain Generalization**: Apply the efficiency framework to a different reasoning domain (e.g., symbolic algebra or planning problems) to test whether the observed inefficiency patterns hold beyond the GSM corpus, assessing the framework's broader applicability.