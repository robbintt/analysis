---
ver: rpa2
title: Constitutive Components for Human-Like Autonomous Artificial Intelligence
arxiv_id: '2506.12952'
source_url: https://arxiv.org/abs/2506.12952
tags:
- function
- autonomy
- arti
- cial
- functions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study constructs a three-layer functional hierarchy for human-like
  autonomous AI, comprising Core Functions (perception, memory, state description,
  motor execution), the Integrative Evaluation Function (integrating perception and
  memory to select actions), and the Self Modification Function (dynamically restructuring
  goals and evaluation criteria). The framework distinguishes reactive, weak autonomous,
  and strong autonomous behavioral levels, with strong autonomy defined by the ability
  to internally set and revise goals.
---

# Constitutive Components for Human-Like Autonomous Artificial Intelligence

## Quick Facts
- arXiv ID: 2506.12952
- Source URL: https://arxiv.org/abs/2506.12952
- Reference count: 26
- One-line result: Three-layer functional hierarchy defines autonomy levels from reactive to strong autonomous AI

## Executive Summary
This study constructs a theoretical framework for human-like autonomous artificial intelligence through a three-layer functional hierarchy. The framework distinguishes reactive behavior (Level 1), weak autonomous behavior (Level 2), and strong autonomous behavior (Level 3) based on the presence of perception, memory, state description, motor execution, integrative evaluation, and self-modification capabilities. The research identifies that while reinforcement learning naturally realizes some functions as policy, it fundamentally lacks the Self Modification Function essential for strong autonomy, necessitating further technical extensions for complete human-like autonomy.

## Method Summary
The paper constructs a three-layer functional hierarchy comprising Core Functions (perception, memory, state description, motor execution), the Integrative Evaluation Function (integrating perception and memory to select contextually appropriate actions), and the Self Modification Function (dynamically restructuring goals and evaluation criteria). The framework maps these functions to different autonomy levels, with strong autonomy defined by the ability to internally set and revise goals. The study proposes that current reinforcement learning systems operate at the weak autonomy level due to externally defined reward functions, and achieving strong autonomy requires extending beyond traditional RL architectures.

## Key Results
- Three-layer functional hierarchy enables progression from reactive to strong autonomous behavior
- Self Modification Function distinguishes strong autonomy from reinforcement learning capabilities
- Second-person perspective approach focuses on observable behavioral structure rather than consciousness requirements

## Why This Works (Mechanism)

### Mechanism 1: Hierarchical Functional Decomposition Enables Autonomy Spectrum
- Claim: A three-layer functional hierarchy progressively enables autonomy levels from reactive to strong autonomous behavior.
- Mechanism: Level 1 Core Functions provide basic environmental interaction → Level 2 Integrative Evaluation Function connects perception/memory to coherent action selection → Level 3 Self Modification Function enables dynamic restructuring of goals and values. Each level builds on and depends on lower levels.
- Core assumption: Autonomy emerges from structured coordination of distinct functions operating hierarchically.
- Evidence anchors: [abstract] "organizes them into a three-layer functional hierarchy...proposes a stepwise model of autonomy"; [section 4, Table 2] Classification showing Reactive (Level 1), Weak Autonomous (Level 1,2), Strong Autonomous (Level 1,2,3)

### Mechanism 2: Self Modification Function Distinguishes Strong Autonomy from RL
- Claim: The Self Modification Function enables value/goal reconstruction that standard reinforcement learning lacks.
- Mechanism: Standard RL operates on externally defined reward functions → cannot autonomously reconstruct evaluative criteria → Self Modification Function allows dynamic restructuring of Integrative Evaluation Function → enables internal goal-setting and value revision → achieves strong autonomy.
- Core assumption: Strong autonomy requires the capacity to modify one's own value structure, not just optimize within fixed values.
- Evidence anchors: [abstract] "current reinforcement learning lacks the Self Modification Function essential for this goal"; [section 5.2] "value function is typically given externally, whereas strong autonomy requires the agent to internally set and, when necessary, reconstruct its own goals and values"

### Mechanism 3: Second-Person Perspective Bypasses Consciousness Requirements
- Claim: Autonomous AI design should focus on observable behavioral structure rather than internal cognitive states like consciousness.
- Mechanism: Recognition as autonomous "other" depends on consistent, adaptive, meaningful behavior in social interaction → consciousness/intent are unverifiable externally → design for behavioral structure enables human-like recognition without requiring consciousness implementation.
- Core assumption: Social recognition and functional autonomy don't require phenomenal consciousness, only appropriate behavioral responses.
- Evidence anchors: [section 3.1] "whether an entity is considered autonomous depends on whether its behavior enables meaningful interaction with others"; [section 3.1] "from a second-person perspective, recognition as an other can be established even without assuming such internal states"

## Foundational Learning

- **Concept: Weak vs. Strong Autonomy Distinction**
  - Why needed here: The entire framework hinges on distinguishing goal-execution autonomy from goal-setting autonomy. Without this, the three-level hierarchy lacks theoretical justification.
  - Quick check question: Can you identify whether a given AI system sets its own goals or only pursues externally-given goals?

- **Concept: Reinforcement Learning Policy as Evaluative Integration**
  - Why needed here: The paper maps RL components to its functional architecture (policy ≈ Integrative Evaluation Function). Understanding this mapping is essential to see why RL achieves only weak autonomy.
  - Quick check question: What component in RL corresponds to "selecting contextually appropriate actions based on perceptual and memory information"?

- **Concept: Second-Person Perspective in AI Design**
  - Why needed here: This philosophical stance determines what the system must implement (observable behavior structure) versus what it can ignore (consciousness, intention). It justifies the functional decomposition approach.
  - Quick check question: From an external observer's perspective, what behavioral properties would indicate an entity is autonomous?

## Architecture Onboarding

- **Component map:**
  Level 1 (Core Functions) → Level 2 (Integrative Evaluation Function) → Level 3 (Self Modification Function)

- **Critical path:**
  1. Implement Core Functions with bidirectional data flow
  2. Add Integrative Evaluation Function with access to all Core Function outputs
  3. Implement Self Modification Function with write access to modify Level 1 & 2 architectures
  4. Verify hierarchical dependency: Level N requires Level N-1 functioning

- **Design tradeoffs:**
  - Modular vs. emergent: Paper proposes explicit functional modules vs. "Reward is Enough" approach where functions emerge from value maximization
  - Implementation flexibility: Framework is method-agnostic but requires all functions present for strong autonomy
  - Self-modification scope: Must decide what architectural elements the Self Modification Function can alter (parameters only vs. structure vs. goals)

- **Failure signatures:**
  - Reactive behavior without coherence (Integrative Evaluation missing)
  - Fixed goals despite environmental/experiential change (Self Modification missing)
  - Disconnected perception/action (Core Functions operating independently)
  - External goal dependency (lacking strong autonomy criterion)

- **First 3 experiments:**
  1. **Validate Level 2 dependency:** Implement Core Functions alone, verify reactive but incoherent behavior; add Integrative Evaluation Function, verify behavioral coherence emerges.
  2. **Test Self Modification capability:** Create scenario requiring goal revision (e.g., changing reward structure mid-task); verify system with Self Modification Function adapts values while system without cannot.
  3. **Map RL components:** Implement standard RL agent; identify which paper functions are implicitly present and which are absent; test whether adding Self Modification capability (e.g., meta-learning reward function modification) enables strong autonomy behaviors.

## Open Questions the Paper Calls Out
None

## Limitations
- Framework provides clear theoretical architecture but lacks specific implementation details for the Self Modification Function, which is the critical component distinguishing strong autonomy
- Relationship between proposed framework and existing autonomous agent architectures remains underexplored
- Paper does not specify algorithms for how an agent programmatically rewrites its own evaluation criteria or value systems

## Confidence
- **High Confidence:** The three-level functional hierarchy as a conceptual framework for understanding autonomy progression
- **Medium Confidence:** The claim that reinforcement learning achieves only weak autonomy due to lacking self-modification capability
- **Low Confidence:** Specific implementation pathways for the Self Modification Function and its technical realization

## Next Checks
1. **Implement and test the Integrative Evaluation dependency:** Create a minimal system with only Core Functions to verify reactive but incoherent behavior, then add Integrative Evaluation to demonstrate behavioral coherence emergence.
2. **Test self-modification capability empirically:** Design an experiment where reward structure changes mid-task, comparing systems with and without Self Modification Function to verify adaptive value reconstruction.
3. **Compare framework coverage with existing architectures:** Map the three-level hierarchy against Neural Brain's components and HTRI's interaction patterns to identify gaps and redundancies in the proposed approach.