---
ver: rpa2
title: 'BanglaMedQA and BanglaMMedBench: Evaluating Retrieval-Augmented Generation
  Strategies for Bangla Biomedical Question Answering'
arxiv_id: '2511.04560'
source_url: https://arxiv.org/abs/2511.04560
tags:
- bangla
- retrieval
- medical
- zero-shot
- page
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of developing accurate biomedical
  question answering (QA) systems in low-resource languages like Bangla, where access
  to reliable medical knowledge is limited. The authors introduce BanglaMedQA and
  BanglaMMedBench, the first large-scale Bangla biomedical MCQ datasets, and evaluate
  multiple Retrieval-Augmented Generation (RAG) strategies, including Traditional,
  Agentic, Iterative Feedback, and Aggregate k-values RAG.
---

# BanglaMedQA and BanglaMMedBench: Evaluating Retrieval-Augmented Generation Strategies for Bangla Biomedical Question Answering

## Quick Facts
- arXiv ID: 2511.04560
- Source URL: https://arxiv.org/abs/2511.04560
- Reference count: 16
- Key outcome: Agentic RAG achieved 89.54% accuracy on Bangla biomedical MCQs using openai/gpt-oss-120b

## Executive Summary
This paper addresses the challenge of developing accurate biomedical question answering (QA) systems in low-resource languages like Bangla, where access to reliable medical knowledge is limited. The authors introduce BanglaMedQA and BanglaMMedBench, the first large-scale Bangla biomedical MCQ datasets, and evaluate multiple Retrieval-Augmented Generation (RAG) strategies, including Traditional, Agentic, Iterative Feedback, and Aggregate k-values RAG. A key innovation is integrating a Bangla medical textbook corpus via OCR and implementing an Agentic RAG pipeline that dynamically selects between retrieval and reasoning strategies. Experimental results show that Agentic RAG achieved the highest accuracy of 89.54% using openai/gpt-oss-120b, outperforming other configurations and demonstrating superior rationale quality. These findings highlight the potential of RAG-based methods to enhance the reliability and accessibility of Bangla medical QA, establishing a foundation for future research in multilingual medical AI.

## Method Summary
The authors developed BanglaMedQA (1,000 MCQs from Bangladesh medical admission tests 1990-2024) and BanglaMMedBench (1,000 scenario-based MCQs translated from MMedBench) datasets. They created a Bangla biology textbook corpus via OCR and implemented multiple RAG strategies: Traditional RAG, Agentic RAG with dynamic routing, Iterative Feedback RAG, and Aggregate k-values RAG. The Agentic RAG uses a router model to evaluate context sufficiency and select among local textbook retrieval, web retrieval, or zero-shot fallback. All systems use BengaliSBERT embeddings indexed in FAISS, with retrieval chunks of 1,000 characters and 200 overlap. LLMs (llama-3.3-70b, llama-3.1-8b, openai/gpt-oss-20b, openai/gpt-oss-120b) generate answers and rationales via Groq API.

## Key Results
- Agentic RAG achieved the highest accuracy of 89.54% using openai/gpt-oss-120b on BanglaMedQA
- Aggregate k-values RAG achieved 84.51% accuracy, outperforming most baselines but below Agentic RAG
- Web retrieval improved performance on factual questions but reduced accuracy on scenario-based clinical reasoning tasks in BanglaMMedBench

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Agentic RAG with dynamic routing improves answer accuracy over static retrieval strategies in low-resource language medical QA
- Mechanism: A router model evaluates whether retrieved textbook passages contain sufficient information (via binary Yes/No prompt) before selecting among three pathways: local RAG, web RAG, or zero-shot fallback. This prevents both premature answering with inadequate context and unnecessary web searches.
- Core assumption: The router model can reliably judge context sufficiency in Bangla biomedical text; routing errors do not cascade into worse outcomes than a single-strategy baseline.
- Evidence anchors:
  - [abstract] "Agentic RAG pipeline that dynamically selects between retrieval and reasoning strategies"
  - [section 3.4.5] "the agentic pipeline introduced a router module that explicitly judged whether retrieved textbook passages contained sufficient information"
  - [corpus] Related work on retrieval-augmented medical reasoning (RAR²) supports thought-driven retrieval as effective, but direct evidence for Bangla-specific routing is absent in the corpus.
- Break condition: If router accuracy degrades significantly on domain-specific Bangla terminology, the fallback chain may consistently select suboptimal paths, reducing Agentic RAG to noise.

### Mechanism 2
- Claim: Zero-shot fallback prevents information loss when retrieval fails, improving coverage at minimal accuracy cost
- Mechanism: When retrieved context falls below a length threshold (τ₁ = 300 chars for local, τ₂ = 200 chars for web), the pipeline defaults to pure LLM parametric knowledge rather than returning null responses. This ensures 100% question coverage.
- Core assumption: LLM parametric knowledge for biomedical MCQs is sufficiently accurate to justify fallback; errors from fallback answers do not outweigh the benefit of avoiding nulls.
- Evidence anchors:
  - [abstract] "combining textbook-based and web retrieval with generative reasoning to improve factual accuracy"
  - [section 3.2.2] "This ensures every question has a prediction, balancing precision (with relevant context) and recall (via zero-shot)"
  - [corpus] Corpus does not provide comparative data on fallback thresholds in medical QA; threshold values are paper-specific.
- Break condition: If the LLM's parametric knowledge has high hallucination rates for Bangla medical content, fallback degrades rather than preserves accuracy.

### Mechanism 3
- Claim: Aggregating predictions across multiple k-values reduces sensitivity to retrieval noise
- Mechanism: Retrieve contexts at k ∈ {3, 5, 6}, generate predictions for each, then aggregate via majority voting (with k=6 as tiebreaker). This compensates for cases where a single k misses relevant passages or includes noise.
- Core assumption: Diverse k-values yield meaningfully different predictions such that aggregation is non-trivial; voting does not amplify systematic retrieval errors.
- Evidence anchors:
  - [section 3.2.2] "A small k may miss key information, while a large k can introduce noise. Aggregate k-values RAG addresses this limitation by combining predictions"
  - [table 1] Aggregate k-values RAG achieved 84.51% accuracy (gpt-oss-120b), below Agentic RAG but above most baselines
  - [corpus] Related work on RAG-BioQA and MedBioLM uses single retrieval depths; corpus lacks comparative voting mechanism studies.
- Break condition: If retrieval quality is uniformly poor across all k-values (e.g., embedding model fails on Bangla biology terms), aggregation provides no benefit and adds latency.

## Foundational Learning

- Concept: Retrieval-Augmented Generation (RAG) architecture
  - Why needed here: All evaluated strategies build on the RAG pattern—retrieving external context before LLM generation. Understanding the baseline pipeline (retrieve → condition → generate) is prerequisite to comparing variants.
  - Quick check question: Can you sketch the data flow for a single-query RAG pipeline, including where embedding models and vector stores fit?

- Concept: Embedding models for low-resource languages
  - Why needed here: The paper uses BengaliSBERT for semantic similarity. Bangla-specific embeddings are critical because generic multilingual models may fail on complex conjunct characters (juktakkhor) and medical terminology.
  - Quick check question: Why might a general-purpose multilingual embedding model underperform on Bangla biomedical text compared to a domain-adapted model?

- Concept: Threshold-based routing decisions
  - Why needed here: Agentic RAG and fallback mechanisms rely on explicit thresholds (τ₁ = 300, τ₂ = 200) to decide between retrieval paths. Understanding how thresholds interact with context quality is key to tuning the system.
  - Quick check question: What happens if τ₁ is set too high versus too low in the Agentic RAG pipeline?

## Architecture Onboarding

- Component map:
  1. OCR-processed Bangla biology textbook → chunked corpus (1000 chars, 200 overlap)
  2. BengaliSBERT embeddings → FAISS vector index
  3. Router model (binary classifier) → pathway selector
  4. Serper API → web retrieval → HTML scraper → summarizer
  5. LLM (Groq API: llama-3.3-70b, gpt-oss-120b, etc.) → answer + rationale generator
  6. Voting aggregator (for Aggregate k-values RAG)

- Critical path: Question → Router checks local retrieval sufficiency → if Yes, Local RAG; if No, Web RAG; if still insufficient, Zero-Shot fallback → LLM generates answer + rationale

- Design tradeoffs:
  - Local vs Web retrieval: Local textbook corpus provides domain alignment but limited coverage; web retrieval broadens sources but introduces noise and may mismatch scenario-based questions (MMedBench showed web search reduced accuracy)
  - Threshold values: Higher τ reduces false positives (answering with inadequate context) but increases fallback frequency; lower τ does the opposite
  - Model size: Larger models (gpt-oss-120b) outperformed smaller ones (llama-3.1-8b), but gains diminish beyond 20B parameters for this task

- Failure signatures:
  - Null responses from Traditional RAG when context is insufficient
  - Router misclassification leading to suboptimal pathway selection
  - Web search returning generic content irrelevant to scenario-based clinical questions
  - OCR gaps in textbook corpus (Tesseract failed on juktakkhor; Google Lens required)

- First 3 experiments:
  1. Reproduce Agentic RAG on BanglaMedQA with openai/gpt-oss-120b; log router decisions and fallback frequencies to validate the 89.54% accuracy claim
  2. Ablate the router by forcing each pathway (Local-only, Web-only, Zero-Shot-only) on a held-out subset; compare accuracy to quantify routing contribution
  3. Test BengaliSBERT vs a generic multilingual embedding model on retrieval quality; measure retrieval precision@k and downstream answer accuracy to identify embedding bottleneck

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does expanding the retrieval corpus beyond Higher Secondary textbooks to include full MBBS curricula improve performance on complex, scenario-based clinical reasoning tasks?
- Basis in paper: [explicit] The authors identify the limited scope of the "Higher Secondary level biology textbook" as a potential constraint, noting that BanglaMMedBench contains "situational and complex questions" (USMLE-style) that may require deeper domain knowledge than the current retrieval source offers.
- Why unresolved: The study only benchmarks against a 12th-grade textbook; it does not test whether advanced medical texts would close the performance gap on the more difficult BanglaMMedBench dataset.
- What evidence would resolve it: An experiment evaluating Agentic RAG performance using a comprehensive medical library (e.g., MBBS standard texts) as the retrieval source compared to the current secondary-school corpus.

### Open Question 2
- Question: Can an adaptive routing mechanism be refined to suppress web retrieval for specific query types where external noise degrades accuracy?
- Basis in paper: [explicit] The results show that Web Search with Zero-Shot Fallback reduced accuracy for the BanglaMMedBench dataset (e.g., dropping from 90.59% to 82.97% for GPT-oss-120b) because "generic web content rarely aligned with the scenario-based, clinical nature" of the questions.
- Why unresolved: The current Agentic RAG routing policy relies largely on context length thresholds ($\tau$) to decide between local and web retrieval, failing to distinguish between factual queries and scenario-based reasoning where the web adds noise.
- What evidence would resolve it: A modified router that classifies query intent (factual vs. clinical scenario) and selectively disables web search, demonstrating improved accuracy on BanglaMMedBench.

### Open Question 3
- Question: Does the translation of medical datasets from English to Bangla introduce artifacts that artificially alter model reasoning capabilities or difficulty?
- Basis in paper: [inferred] While the authors acknowledge that "translating medical content often distorts meaning," they relied on LLM-based translation (Gemini-1.5-Flash) to create BanglaMMedBench due to the lack of native data. It remains unclear if the performance differences between English and Bangla sets stem from language barriers or translation quality (e.g., loss of nuance or "translationese").
- Why unresolved: The paper compares English MMedBench vs. BanglaMMedBench but does not isolate the variable of "translation quality" itself, making it difficult to determine if the benchmark evaluates medical reasoning or translation robustness.
- What evidence would resolve it: A human evaluation analyzing error types in the Bangla dataset to determine if incorrect model predictions correlate with translation artifacts rather than knowledge gaps.

## Limitations
- The Agentic RAG pipeline's advantage disappears when using smaller models (llama-3.1-8b achieved only 72.65% accuracy)
- The BengaliSBERT embedding model's performance on OCR-degraded textbook text represents an unknown bottleneck
- Web retrieval improves performance on factual questions but reduces accuracy on scenario-based clinical reasoning tasks

## Confidence
- **High Confidence**: The methodology for constructing BanglaMedQA and BanglaMMedBench datasets is well-documented and reproducible. The baseline RAG architecture and OCR pipeline are clearly specified.
- **Medium Confidence**: The Aggregate k-values RAG results are reproducible given the k∈{3,5