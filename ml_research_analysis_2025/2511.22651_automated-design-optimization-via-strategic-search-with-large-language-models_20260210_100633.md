---
ver: rpa2
title: Automated Design Optimization via Strategic Search with Large Language Models
arxiv_id: '2511.22651'
source_url: https://arxiv.org/abs/2511.22651
tags:
- optimization
- design
- code
- auto
- search
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This work introduces AUTO, an LLM-based agent framework that treats
  design optimization as a strategic search problem using two collaborative agents:
  a Strategist that selects exploration or exploitation strategies, and an Implementor
  that executes detailed designs. The framework achieves 50-70% search efficiency
  relative to Bayesian optimization when applied to GPU code optimization for chemical
  kinetics and dense matrix multiplication.'
---

# Automated Design Optimization via Strategic Search with Large Language Models

## Quick Facts
- arXiv ID: 2511.22651
- Source URL: https://arxiv.org/abs/2511.22651
- Authors: Anthony Carreon; Vansh Sharma; Venkat Raman
- Reference count: 7
- Primary result: LLM-based strategic search achieves 50-70% efficiency relative to Bayesian optimization for GPU code optimization

## Executive Summary
This paper introduces AUTO, a framework that treats design optimization as a strategic search problem using two collaborative agents: a Strategist that selects exploration or exploitation strategies, and an Implementor that executes detailed designs. The framework achieves competitive results with traditional optimization methods while demonstrating potential for automating design optimization across diverse domains. Applied to GPU code optimization for chemical kinetics and dense matrix multiplication, AUTO generates solutions competitive with expert implementations at significantly lower costs than human developers.

## Method Summary
AUTO employs a two-agent LLM framework where a Strategist agent selects between "refine," "combine," or "innovate" strategies based on curated historical performance data, and an Implementor agent generates executable code following these strategic instructions. The framework treats optimization as a strategic search problem, navigating ill-defined design spaces through iterative refinement. Context curation mixes P best, Q worst, and R recent designs to inform the Strategist's decisions, while constraint validation and evaluation modules ensure correctness and measure performance. The approach is validated on GPU kernel optimization tasks, comparing search efficiency against Bayesian optimization baselines.

## Key Results
- Search efficiency reaches 50-70% relative to Bayesian optimization acquisition functions
- AUTO generates GPU solutions competitive with expert implementations and cuBLAS library
- Optimization runs cost $0.78-$159.12 per run versus up to $480 with median-wage developers
- Compilation failure rates of 44-57% attributed to LLM hallucinations without API documentation
- Matrix multiplication solutions achieve near-optimal performance for N=32-4096

## Why This Works (Mechanism)

### Mechanism 1: Strategic-Implementation Role Separation
The framework decomposes optimization into high-level strategy selection and low-level execution, enabling effective search in ill-defined design spaces while circumventing LLM context-window limitations. The Strategist analyzes historical performance data to select among refinement, combination, or innovation strategies, then passes specific implementation instructions to the Implementor. This separation ensures coherent optimization trajectories even when the search space is poorly defined.

### Mechanism 2: Context Curation with Mixed Historical Sampling
The framework constructs a context set containing P best designs, Q worst designs, and R recent designs for the Strategist. This curated mixture mimics surrogate model updates in Bayesian optimization but operates on discrete code representations. The approach leverages failure history as valuable information, recognizing that low-scoring designs in highly nonlinear design surfaces can be adjusted to achieve significant improvements.

### Mechanism 3: Strategy-Aligned Exploration-Exploitation via LLM Reasoning
The Strategist maps problem state to one of three strategies—"innovate" when designs plateau or fail constraints, "combine" when multiple designs excel in different aspects, "refine" when near breakthrough. This mirrors exploration-exploitation trade-offs in traditional optimizers but uses semantic reasoning rather than mathematical acquisition functions. The LLM's encoded domain knowledge enables it to recognize when each strategy is appropriate from performance patterns alone.

## Foundational Learning

- Concept: **Bayesian Optimization and Acquisition Functions**
  - Why needed here: AUTO's search efficiency metric directly compares LLM strategy selection to Bayesian optimization's exploration-exploitation balance. Understanding UCB and how ξ controls exploration factor is essential to interpret the 50-70% efficiency results.
  - Quick check question: If ξ=0.01 favors exploitation and ξ=10 favors exploration, which value would you expect to align better with a Strategist that mostly chooses "combine"?

- Concept: **GPU Memory Hierarchy and Optimization Patterns**
  - Why needed here: The test domains require understanding tiling, register usage, memory coalescence, warp divergence, and kernel launch overhead to evaluate whether AUTO discovers meaningful optimizations or superficial changes.
  - Quick check question: Why would "loop tiling" improve matrix multiplication performance on GPUs, and what does it suggest that AUTO discovered this pattern independently?

- Concept: **Code Embedding and Clustering for Design Space Analysis**
  - Why needed here: The paper uses bag-of-words code vectors and t-SNE visualization to show that structurally similar codes cluster together and correlate with performance. Understanding this helps validate whether the search is exploring meaningfully diverse designs.
  - Quick check question: What does it suggest about the search space if Cluster A (top performers) and Cluster B (failed experiments) are well-separated in t-SNE space?

## Architecture Onboarding

- Component map: Strategist Agent -> Implementor Agent -> Constrain Block -> Evaluate Block -> Database -> Context Curation -> (repeat)
- Critical path: Strategist prompt → Implementor code generation → Constraint validation (up to K fixes) → Evaluation → Database record → Context curation → (repeat for N iterations)
- Design tradeoffs: Higher temperature increases exploration but may increase invalid code generation; larger P/Q/R values provide richer context but consume more tokens; providing documentation reduces hallucination errors but increases prompt complexity.
- Failure signatures: Compilation errors (44-57% of iterations), stagnation with early best solutions, context window overflow at 60,212 tokens.
- First 3 experiments:
  1. Baseline replication: Run AUTO on matrix multiplication with temperature=0.7, no sketch, P=5/Q=5/R=5, N=20 iterations.
  2. Documentation ablation: Add WMMA/Tensor Core API documentation to Implementor context for matrix multiplication.
  3. Strategy enforcement: Force "innovate" for first 10 iterations, then analyze whether Strategist naturally transitions to "refine" or "combine".

## Open Questions the Paper Calls Out

- Can the AUTO framework successfully scale to hardware-software co-design and multi-objective engineering problems?
- How can stopping criteria be improved to optimize the trade-off between monetary cost and design quality?
- To what extent does integrating domain-specific knowledge bases mitigate compilation errors and hallucinations?

## Limitations

- Context window constraints limit scalability as input tokens approach 60,212 (33% of 128k context)
- Strategy alignment validation assumes Bayesian optimization represents optimal exploration-exploitation trade-offs
- Cost-benefit analysis gap excludes infrastructure costs, prompt engineering time, and model fine-tuning

## Confidence

**High Confidence**: The core architectural design of separating strategic planning from implementation execution is sound and technically feasible.

**Medium Confidence**: The search efficiency comparison to Bayesian optimization is methodologically valid, though the interpretation of 50-70% alignment as "competitive" performance assumes Bayesian optimization represents a gold standard.

**Low Confidence**: Claims about broad domain applicability and the general superiority of LLM-based strategic reasoning over traditional optimization methods are not sufficiently supported.

## Next Checks

1. **Context Scaling Experiment**: Systematically test how search efficiency degrades as context window approaches limits by running optimizations with increasing P/Q/R values to identify the point where context truncation begins affecting strategy quality.

2. **Cross-Domain Transfer Test**: Apply AUTO to a distinctly different optimization domain (e.g., neural architecture search or compiler optimization passes) without architectural modifications to establish whether strategic reasoning generalizes beyond GPU code optimization.

3. **Human Expert Comparison**: Have domain experts optimize the same GPU kernels using their standard workflows, then compare solution quality, optimization time, and cost against AUTO's best runs to validate claimed cost-effectiveness.