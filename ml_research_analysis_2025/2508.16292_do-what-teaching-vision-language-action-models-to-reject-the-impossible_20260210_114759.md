---
ver: rpa2
title: Do What? Teaching Vision-Language-Action Models to Reject the Impossible
arxiv_id: '2508.16292'
source_url: https://arxiv.org/abs/2508.16292
tags:
- 'false'
- visual
- instructions
- language
- false-premise
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the problem of false-premise instructions in
  Vision-Language-Action (VLA) models, where users give commands referencing objects
  or conditions absent from the environment. The authors propose Instruct-Verify-and-Act
  (IVA), a unified framework that detects when an instruction cannot be executed due
  to a false premise, engages in language-based clarification or correction, and grounds
  plausible alternatives in perception and action.
---

# Do What? Teaching Vision-Language-Action Models to Reject the Impossible

## Quick Facts
- **arXiv ID**: 2508.16292
- **Source URL**: https://arxiv.org/abs/2508.16292
- **Reference count**: 40
- **Primary result**: IVA framework improves false premise detection accuracy by 97.56% and increases successful responses by 50.78%

## Executive Summary
Vision-Language-Action (VLA) models often struggle with false-premise instructions—commands that reference objects or conditions absent from the environment. The Instruct-Verify-and-Act (IVA) framework addresses this by detecting impossible instructions, engaging in natural language clarification, and grounding plausible alternatives in perception and action. Trained on a semi-synthetic dataset of paired positive and false-premise instructions, IVA significantly outperforms baselines in handling these challenging scenarios while maintaining task execution capabilities.

## Method Summary
IVA introduces a unified framework for VLA models that detects false-premise instructions through semantic reasoning, engages in language-based clarification or correction, and grounds plausible alternatives in perception and action. The approach leverages a semi-synthetic dataset generation pipeline that creates paired positive and false-premise instruction sets from existing task datasets. The model integrates a false-premise detector with a correction module that can either ask clarifying questions or suggest alternative actions, all while maintaining end-to-end trainability through a specialized loss function that balances detection accuracy with task completion.

## Key Results
- 97.56% improvement in false premise detection accuracy over baselines
- 50.78% increase in successful responses in false-premise scenarios
- Robust performance in both ALFWorld and VirtualHome simulation environments

## Why This Works (Mechanism)
IVA works by combining semantic understanding with action grounding. When an instruction is received, the model first verifies whether the referenced objects or conditions exist in the current environment. If a false premise is detected, the system engages in natural language clarification or correction, proposing alternative actions that are grounded in the actual perceived environment. This dual approach of verification and clarification allows the model to maintain task completion while avoiding the pitfalls of attempting impossible actions.

## Foundational Learning

**Vision-Language-Action (VLA) Models**
- *Why needed*: VLA models integrate perception, language understanding, and action execution in embodied AI systems
- *Quick check*: Can the model process visual input, understand natural language commands, and execute corresponding actions?

**False Premise Detection**
- *Why needed*: Identifies when user instructions reference non-existent objects or impossible conditions
- *Quick check*: Does the model recognize when requested objects are absent from the environment?

**Natural Language Clarification**
- *Why needed*: Enables conversational interaction when instructions cannot be executed as given
- *Quick check*: Can the model generate appropriate clarifying questions or corrections?

## Architecture Onboarding

**Component Map**
User Instruction -> False Premise Detector -> Correction Module -> Action Executor

**Critical Path**
Instruction reception → Semantic verification → Detection decision → Clarification/correction generation → Action grounding

**Design Tradeoffs**
- End-to-end trainability vs. modular specialization
- Detection accuracy vs. task completion capability
- Natural language generation quality vs. computational efficiency

**Failure Signatures**
- False negatives: Attempting impossible actions
- False positives: Rejecting valid instructions
- Clarification quality: Unnatural or unhelpful corrections

**First Experiments to Run**
1. Baseline comparison on false premise detection accuracy
2. Task completion rate comparison between IVA and standard VLA models
3. User study on the naturalness of generated clarifications

## Open Questions the Paper Calls Out
The paper acknowledges that evaluation focuses primarily on simulation environments (ALFWorld and VirtualHome), which may not fully capture real-world complexity. The approach's performance with more diverse and complex false premises in practical applications remains to be thoroughly tested. Additionally, while explicit false premises are handled well, the model's behavior with subtle or ambiguous cases where impossibility is not immediately apparent requires further investigation.

## Limitations
- Evaluation limited to simulation environments rather than real-world scenarios
- Performance metrics derived from controlled experimental settings may not generalize
- Potential struggles with subtle or ambiguous false premises requiring deeper contextual understanding

## Confidence

**High confidence in:**
- Core methodology and controlled environment performance
- Semi-synthetic dataset generation approach
- Improvement metrics over baseline models

**Medium confidence in:**
- Generalizability to real-world applications
- Robustness with complex or ambiguous false premises
- Effectiveness in diverse real-world scenarios

## Next Checks

1. Real-world deployment testing: Implement IVA in a physical robot system operating in a real home environment to assess performance with actual false-premise instructions.

2. Cross-domain generalization study: Test IVA on a broader range of domains beyond household tasks, such as industrial settings or outdoor environments.

3. Human evaluation of clarification quality: Conduct user studies where participants rate the naturalness and helpfulness of IVA's language-based corrections and clarifications across various false-premise scenarios.