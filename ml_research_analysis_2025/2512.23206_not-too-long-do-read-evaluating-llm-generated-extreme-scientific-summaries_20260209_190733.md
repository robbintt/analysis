---
ver: rpa2
title: 'Not too long do read: Evaluating LLM-generated extreme scientific summaries'
arxiv_id: '2512.23206'
source_url: https://arxiv.org/abs/2512.23206
tags:
- summaries
- words
- llms
- human
- abstract
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ''
---

# Not too long do read: Evaluating LLM-generated extreme scientific summaries

## Quick Facts
- arXiv ID: 2512.23206
- Source URL: https://arxiv.org/abs/2512.23206
- Authors: Zhuoqi Lyu; Qing Ke
- Reference count: 40
- Primary result: LLMs generate scientific summaries that are longer, less readable, and more extractive than human-written summaries

## Executive Summary
This paper systematically evaluates five open-weight LLMs on extreme scientific summarization (TLDR generation) using the new BiomedTLDR dataset of 35,626 annotated bibliography entries. The study reveals that LLM-generated summaries are significantly longer and less readable than human-written ones, despite being more extractive (relying more on source text lexical choices). The authors introduce novel evaluation metrics including abstractiveness (novel n-gram proportion), rhetorical structure analysis, and training set copy rate detection. Results show that while LLMs can produce factually accurate summaries, they struggle with length control, audience awareness, and abstractive synthesis compared to human experts.

## Method Summary
The study evaluates five open-weight LLMs (gemma3:27b, deepseek-r1:70b, gpt-oss:12b, llama4:16x17b, qwen3:235b) on BiomedTLDR, a dataset of 35,626 scientific paper abstract-summary pairs extracted from annotated bibliographies in Springer review articles. Models are prompted with 5 fixed few-shot examples and word count constraints via local Ollama server. Evaluation uses seven readability indices, novel n-gram proportions for abstractiveness, DeepSeek-R1 classifier for rhetorical structure (BACKGROUND/OBJECTIVE/METHODS/RESULTS/CONCLUSIONS), reference-based metrics (BLEU, ROUGE, BERTScore, MoverScore, METEOR), training set copy rate detection, and named entity density analysis.

## Key Results
- All LLM-generated summaries are significantly longer and less readable than human-written ones across all seven readability metrics
- LLMs exhibit lower novel n-gram proportions (33.48-39.85%) compared to human summaries (46.13%), indicating more extractive behavior
- LLM summaries show systematic rhetorical structure differences: more METHODS and RESULTS content, less BACKGROUND
- Training set copy rate detection reveals up to 100% maximum 3-gram overlap with few-shot examples for some models
- Named entity density varies by model, with gpt-oss showing highest density potentially impacting readability

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs exhibit stronger extractive behavior than human experts when summarizing scientific text.
- Mechanism: LLMs demonstrate greater affinity for source text lexical choices and rhetorical structures, resulting in lower novel n-gram rates compared to human-written summaries.
- Core assumption: Novel n-gram proportion reliably measures abstractive capability; lower values indicate extractive copying.
- Evidence anchors:
  - [abstract] "LLMs generally exhibit a greater affinity for the original text's lexical choices and rhetorical structures, hence tend to be more extractive rather than abstractive"
  - [section] Table 4 shows human summaries have 46.13% novel 1-grams vs 33.48-39.85% for LLMs
  - [corpus] Related work on generalization bias in LLM summarization supports extractive tendencies, but corpus lacks direct mechanistic studies
- Break condition: If novel n-grams measure style rather than content synthesis, or if human summaries draw from full texts (not just abstracts), extractive gap may be overstated.

### Mechanism 2
- Claim: Annotated bibliography entries provide higher-quality supervision signals for scientific summarization than citation contexts.
- Mechanism: Bibliography annotations are self-contained, holistic summaries written by experts specifically to describe referenced work, avoiding the fragmented, narrative-biased signals from "Related Work" sections.
- Core assumption: Authors write annotations to summarize holistically, not to advance their own narrative.
- Evidence anchors:
  - [section] "This structural format provides a self-contained summary of the cited paper's core contributions, avoiding fragmentation and narrative bias"
  - [section] Table 1 shows BiomedTLDR has balanced domain coverage vs CiteSum's 80% CS bias
  - [corpus] Related papers on scientific summarization datasets don't directly compare bibliography vs citation-context quality
- Break condition: If annotations reflect personal perspectives or critiques rather than objective summaries, or if discipline coverage remains skewed despite improvement.

### Mechanism 3
- Claim: LLM-generated scientific summaries are systematically less readable than human-written ones, despite being longer.
- Mechanism: LLMs rely more heavily on domain-specific jargon (higher named entity density in some models like gpt-oss), while humans exercise audience awareness and use plainer language.
- Core assumption: Standard readability metrics (FKG, GF, etc.) validly capture scientific text accessibility; jargon density inversely correlates with readability.
- Evidence anchors:
  - [section] "all LLM-generated summaries exhibit significantly higher scores across all metrics compared to not only human-written summaries but also the original abstracts"
  - [section] "we hypothesize that this discrepancy stems from the tendency of LLMs to excessively rely on high-density domain-specific jargon"
  - [corpus] Work on lay summarization supports audience-aware writing, but corpus lacks direct studies on LLM jargon density vs readability
- Break condition: If readability metrics don't capture scientific audience needs, or if longer LLM summaries artificially inflate complexity scores.

## Foundational Learning

- Concept: Extractive vs. Abstractive Summarization
  - Why needed here: The paper's central finding hinges on LLMs being more extractive than humans; understanding this distinction is essential for interpreting the evaluation results.
  - Quick check question: Given a source sentence "The protein binds to DNA via a helix-turn-helix motif," is "The protein uses a helix-turn-helix motif to bind DNA" more extractive or abstractive than "This transcription factor recognizes specific DNA sequences"?

- Concept: Rhetorical Structure in Scientific Writing
  - Why needed here: The paper analyzes how LLMs and humans distribute BACKGROUND, OBJECTIVE, METHODS, RESULTS, and CONCLUSIONS differently across summary positions.
  - Quick check question: In a typical scientific abstract, would you expect BACKGROUND information to appear more toward the beginning or end of the text?

- Concept: Reference-Based Evaluation Metrics (ROUGE, BERTScore, METEOR, MoverScore)
  - Why needed here: The paper uses these metrics to compare LLM outputs against human references; understanding what each captures (lexical overlap vs. semantic similarity) is crucial for interpreting the results.
  - Quick check question: If a summary uses entirely different words but conveys the same meaning as the reference, which metric would score higher: BLEU or BERTScore?

## Architecture Onboarding

- Component map: Crawl Springer → Parse HTML for bold annotations + DOIs → Match DOIs to abstracts via Google Scholar/CrossRef → 35,626 abstract-summary pairs → Ollama server with 5 models → Few-shot prompting with 5 examples → Generate summaries → Evaluation suite (readability, abstractiveness, rhetorical structure, reference-based metrics, copy rate, named entity density)

- Critical path: Dataset quality determines evaluation validity → Prompt engineering affects LLM behavior → Metric selection shapes conclusions (extractive vs. abstractive trade-off)

- Design tradeoffs:
  - Abstract-only vs. full-text input: Abstracts are accessible but may limit human-level synthesis (humans may use full texts)
  - Automated metrics vs. human evaluation: Metrics are scalable but may not capture nuanced quality differences
  - Single vs. multiple reference summaries: Multiple human references enable cross-referenced baselines but increase annotation cost

- Failure signatures:
  - Length control failure: Most LLMs (except Gemma 3) ignore word-count constraints, producing verbose outputs
  - Repetitive/hallucinated output: Llama 4 shows phrase repetition; Qwen3 outputs internal reasoning despite disabled thinking mode
  - Training set memorization: All LLMs show non-zero 3-gram copy rates from few-shot examples (up to 100% max for DeepSeek-R1)

- First 3 experiments:
  1. Reproduce the extractive tendency analysis: Calculate novel n-gram proportions for a small sample (100 abstract-summary pairs) comparing a local LLM against human references
  2. Test prompt sensitivity: Vary the number of few-shot examples (0, 3, 5, 10) and measure impact on length adherence and abstractiveness
  3. Domain transfer check: Apply the same evaluation pipeline to a non-biomedical scientific domain (e.g., computer science papers from arXiv) to assess generalizability

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does providing LLMs with full-text articles (rather than abstracts alone) improve the quality and abstractive capability of generated scientific summaries?
- Basis in paper: [explicit] The authors state: "while LLMs were restricted to using only paper abstracts as input, human annotators may likely draw on abstracts and full texts... Future work could explore whether providing LLMs with full-text would lead to further improvements in summary quality."
- Why unresolved: The experimental design deliberately limited input to abstracts for fairness and reproducibility, creating an asymmetry with human annotators who may have had full-text access.
- What evidence would resolve it: A controlled experiment where the same LLMs generate summaries from abstracts-only versus full-text inputs, evaluated against human references using the same multi-dimensional framework.

### Open Question 2
- Question: How does LLM summarization performance differ on less-cited or lower-impact papers compared to the highly-cited works in the current dataset?
- Basis in paper: [explicit] The authors acknowledge: "our dataset is not fully representative of the entire scientific literature in terms of citations, as all reported performance is based on highly cited works. Therefore, evaluating LLMs summarization on the general population of papers remains as an important direction for future research."
- Why unresolved: The BiomedTLDR dataset was constructed from annotated bibliographies in highly-cited review papers, inherently biasing toward well-established, highly-cited source papers.
- What evidence would resolve it: Evaluation of LLM summarization on a stratified sample of papers across citation counts and impact levels, with analysis of performance variation.

### Open Question 3
- Question: Can the relationship between summary length and quality be rigorously decoupled, and what mechanisms can improve LLMs' adherence to length constraints?
- Basis in paper: [explicit] The authors note that "although we explicitly constrained LLM output length in the prompt, most generated summaries were significantly longer than those by human researchers. This calls for dedicated experiments to rigorously decouple the relationship between summary quality and length."
- Why unresolved: Current evaluation metrics claim robustness to length variation, but the systematic length deviation by LLMs may confound quality assessments.
- What evidence would resolve it: Controlled experiments with fixed-length outputs across multiple length targets, coupled with human evaluation of quality independent of length considerations.

## Limitations

- Dataset bias toward highly-cited papers limits generalizability to broader scientific literature
- Abstract-only input creates asymmetry with human annotators who may access full-text papers
- Automated readability metrics may not accurately capture scientific writing quality or audience needs
- Five fixed few-shot examples were randomly selected but not publicly documented, hindering exact replication

## Confidence

**High confidence**: LLM-generated summaries are systematically less readable than human-written ones (supported by multiple readability metrics across all models).

**Medium confidence**: LLMs exhibit stronger extractive behavior than humans (novel n-gram analysis is consistent but depends on the assumption that this metric captures abstractiveness).

**Low confidence**: Bibliography annotations provide definitively better supervision signals than citation contexts (structural advantages noted but no direct comparative analysis performed).

## Next Checks

1. **Full-text vs. abstract input validation**: Repeat the extractive tendency analysis using full-text papers as input to determine if the human-LLM gap persists when humans have access to the same information.

2. **Human evaluation validation**: Conduct a small-scale human evaluation (10-20 abstracts) comparing LLM and human summaries on coherence, factual accuracy, and information coverage to validate automated metric findings.

3. **Domain transfer validation**: Apply the evaluation pipeline to a different scientific domain (e.g., computer science papers from arXiv) to assess whether the observed patterns generalize beyond biomedicine.