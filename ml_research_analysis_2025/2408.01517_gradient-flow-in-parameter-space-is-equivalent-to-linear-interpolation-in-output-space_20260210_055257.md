---
ver: rpa2
title: Gradient flow in parameter space is equivalent to linear interpolation in output
  space
arxiv_id: '2408.01517'
source_url: https://arxiv.org/abs/2408.01517
tags:
- gradient
- flow
- space
- output
- loss
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work proves that the standard gradient flow in parameter space
  underlying many training algorithms in deep learning can be continuously deformed
  into an adapted gradient flow that yields constrained Euclidean gradient flow in
  output space. The authors show that for the L2 loss, if the Jacobian of the outputs
  with respect to the parameters is full rank, then the time variable can be reparametrized
  so that the resulting flow is simply linear interpolation, and a global minimum
  can be achieved.
---

# Gradient flow in parameter space is equivalent to linear interpolation in output space

## Quick Facts
- arXiv ID: 2408.01517
- Source URL: https://arxiv.org/abs/2408.01517
- Reference count: 4
- Proves that standard gradient flow in parameter space can be continuously deformed into an adapted gradient flow yielding constrained Euclidean gradient flow in output space

## Executive Summary
This work establishes that the standard gradient flow used in neural network training can be transformed into an equivalent flow in output space. The authors show that under certain rank conditions on the Jacobian, this output-space flow corresponds to simple linear interpolation. The key insight is that the rank of the Jacobian (equivalently, the neural tangent kernel) fundamentally determines the optimization dynamics, with full rank enabling linear interpolation and rank deficiency forcing constrained flows on sub-Riemannian manifolds.

## Method Summary
The method involves constructing an adapted gradient flow using the Moore-Penrose pseudoinverse of the Jacobian as a preconditioner. For L² loss, if the Jacobian remains full rank, time reparametrization converts the output-space flow into linear interpolation between initial and target outputs. For cross-entropy loss with positive label components, an explicit formula for the unique global minimum is derived. The approach uses homotopy equivalence to show that standard and adapted gradient flows share the same critical points, with rank conditions determining whether the flow achieves interpolation or becomes constrained.

## Key Results
- Standard gradient flow in parameter space can be continuously deformed into an adapted gradient flow that yields constrained Euclidean gradient flow in output space
- Under L² loss with full-rank Jacobian, time reparametrization converts the flow to linear interpolation
- For cross-entropy loss with positive labels, an explicit formula for the unique global minimum is derived
- Fixed-point criterion shows that for convex losses, non-interpolating fixed points require Jacobian rank deficiency

## Why This Works (Mechanism)

### Mechanism 1: Homotopy-equivalent gradient flows share critical points
- Claim: Standard gradient flow and adapted gradient flow in parameter space can be continuously deformed into one another while preserving all equilibrium points.
- Mechanism: The authors define a one-parameter family of vector fields $V_{\theta,\alpha} = -A_{\theta,\alpha}\nabla_\theta C$ with $A_{\theta,\alpha} = \alpha(D^T D)^+ + (1-\alpha)I_{K \times K}$ for $\alpha \in [0,1]$. At $\alpha=0$ this is standard gradient flow; at $\alpha=1$ it is adapted gradient flow. The matrix $A_{\theta,\alpha}$ remains positive-definite for all $\alpha$, so zeros of $V_{\theta,\alpha}$ coincide with zeros of $\nabla_\theta C$ throughout.
- Core assumption: The cost $C$ depends on parameters $\theta$ only through outputs $x(\theta)$, and $x(\theta)$ has Lipschitz continuous derivatives (smooth or smoothed activations).
- Evidence anchors:
  - [abstract] "standard gradient flow in parameter space...can be continuously deformed into an adapted gradient flow"
  - [Theorem 2.3, pages 4-5] Full proof with explicit interpolation family and equilibrium preservation argument.
  - [corpus] Related work "Riemannian Optimization Perspective of the Gauss-Newton Method" similarly treats preconditioned flows inducing Riemannian gradient flows on function manifolds.
- Break condition: If the cost has non-Lipschitz or discontinuous derivatives (e.g., non-smoothed ReLU at exact zero), the homotopy construction may not apply uniformly.

### Mechanism 2: Full-rank Jacobian enables reparametrization to linear interpolation (L² loss)
- Claim: Under the L² loss with full-rank Jacobian $D \in \mathbb{R}^{QN \times K}$ (rank $= QN$), Euclidean gradient flow in output space is equivalent to linear interpolation via time reparametrization.
- Mechanism: Euclidean gradient flow $\partial_s x = -\nabla_x C = \frac{1}{N}(x - y)$ solves to $x(s) = y + e^{-s/N}(x_0 - y)$. Defining $t = 1 - e^{-s/N}$ yields $\tilde{x}(t) = y + (1-t)(x_0 - y)$, which is exactly linear interpolation from $x_0$ to $y$.
- Core assumption: The Jacobian $D[\theta(s)]$ remains full rank (rank $= QN$) throughout the trajectory. Also assumes overparametrization $K \geq QN$.
- Evidence anchors:
  - [Proposition 2.5, page 6] Explicit derivation showing $\tilde{x}(t) = y + (1-t)(x_0 - y)$ under full rank.
  - [Proposition 2.6, pages 7-8] Provides deviation formula when rank is lost.
  - [corpus] Weak direct corpus evidence for this specific reparametrization; related work focuses on linear mode connectivity rather than output-space linearity.
- Break condition: If Jacobian loses rank during training, the deviation from linear interpolation is nonzero and governed by the propagator integral in Proposition 2.6.

### Mechanism 3: Fixed-point criterion forces either interpolation or rank loss
- Claim: For convex output-space losses (L², cross-entropy), if gradient flow reaches a fixed point that does not interpolate the data, the Jacobian must be rank-deficient.
- Mechanism: From $\nabla_\theta C = D^T \nabla_x C$, if $\nabla_x C = 0$ (output-space critical point) then $\nabla_\theta C = 0$. Conversely, if $\nabla_\theta C = 0$ but $\nabla_x C \neq 0$, then $D^T$ must have a nontrivial kernel, i.e., $D$ is not injective (rank loss). Since L² and cross-entropy losses have global minima only at interpolating solutions, non-interpolating fixed points require rank deficiency.
- Core assumption: Overparametrized model ($K \geq QN$) with loss whose only minima are interpolating solutions.
- Evidence anchors:
  - [Remark 2.1, page 3] Explicit statement of the criterion.
  - [Lemma 2.2, pages 3-4] Shows equivalence $\nabla_x C = 0 \Leftrightarrow \nabla_\theta C = 0$ exactly when rank $D = QN$.
  - [corpus] "Optimization Insights into Deep Diagonal Linear Networks" studies rank dynamics in simplified settings, supporting the centrality of rank to convergence.
- Break condition: For non-convex losses with non-interpolating local minima, this criterion does not apply.

## Foundational Learning

- Concept: **Moore-Penrose pseudoinverse $(D)^+$**
  - Why needed here: The adapted gradient flow uses preconditioner $(D^T D)^+ \nabla_\theta C$; understanding how pseudoinverses behave under rank deficiency is essential for implementing and debugging the flow.
  - Quick check question: Given a wide matrix $D \in \mathbb{R}^{m \times n}$ with $m < n$ and full row rank, what is the simplified form of $(D)^+$?

- Concept: **Neural Tangent Kernel (NTK)**
  - Why needed here: The paper explicitly connects the Jacobian rank condition to NTK positive-definiteness; the kernel matrix $\Theta$ is block-structured from $DD^T$ (equation 3.15), and full rank corresponds to NTK being positive-definite on training data.
  - Quick check question: If $DD^T$ is singular, what does this imply about the NTK evaluated on the training set?

- Concept: **Constrained gradient flow on sub-Riemannian manifolds**
  - Why needed here: When $D$ is rank-deficient, the output-space flow is constrained to the range of $D$, inducing flow on a sub-Riemannian structure $(\mathbb{R}^{QN}, V, h_\alpha)$ where $V$ is the vector subbundle of admissible directions.
  - Quick check question: In the rank-deficient case, why must the flow satisfy $\partial_s x = -P_{\text{range}(D)} \nabla_x C$ rather than $-\nabla_x C$?

## Architecture Onboarding

- Component map: Parameter initialization -> Jacobian computation module -> Pseudoinverse/preconditioner module -> Loss-specific flow controllers -> Output trajectory computation
- Critical path:
  1. Initialize $\theta_0$, compute initial outputs $x_0 = x(\theta_0)$ and Jacobian $D[\theta_0]$.
  2. Check rank of $D$; if full rank, compute adapted gradient direction $-(D^T D)^+ \nabla_\theta C$.
  3. Update parameters; recompute Jacobian and verify rank stability.
  4. If rank drops, switch to constrained flow with projection $P_{\text{range}(D)}$.
- Design tradeoffs:
  - **Computational cost vs. convergence guarantee**: Computing $(D^T D)^+$ at each step is $O(K^2 QN + K^3)$, far more expensive than standard gradient descent. Approximations (e.g., Gauss-Newton approximations, low-rank updates) may be necessary.
  - **Assumption fragility**: Full rank must hold throughout training; initialization and architecture choices affect whether this is realistic.
  - **Label smoothing for cross-entropy**: The explicit minimum formula requires positive label components $y_j > 0$; one-hot labels violate this. Assumption: small label smoothing may improve convergence properties.
- Failure signatures:
  - **Stalled convergence without interpolation**: Indicates Jacobian rank loss; inspect singular values of $D$.
  - **Numerical instability in pseudoinverse**: Near-rank-deficient $D$ causes large condition numbers; regularize with $(D^T D + \epsilon I)^+$.
  - **Cross-entropy not reaching explicit minimum**: Check if labels have zero components; the formula $f^*_{c}(x) = \log y + \frac{1}{Q}(\sum_j (f_0(x)_j - \log y_j)) \mathbf{u}_Q$ requires $y_j > 0$.
- First 3 experiments:
  1. **Rank tracking on small MLP**: Train a 2-layer MLP on MNIST subset with standard SGD; log rank of $D[\theta]$ (via SVD threshold) at each epoch. Verify whether convergence coincides with rank stability or rank loss.
  2. **Adapted vs. standard gradient flow comparison**: Implement the adapted flow (using pseudo-inverse or least-squares solve for $(D^T D)^+ \nabla_\theta C$) on a small regression task with L² loss. Compare trajectory length and convergence time to standard gradient descent.
  3. **Label smoothing effect on cross-entropy convergence**: Train with one-hot vs. smoothed labels (e.g., $y_j = 0.9$ for correct class, $0.1/(Q-1)$ for others). Measure whether trajectory approaches the explicit minimum formula from Proposition 2.8.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the framework of adapted gradient flows and the equivalence to linear interpolation be meaningfully extended to the setting of information geometry on probability manifolds?
- Basis in paper: [explicit] Section 2.3 states regarding information geometry: "Whether the ideas presented here can be meaningfully extended to this setting remains an open question, which we leave for future investigation."
- Why unresolved: The current analysis defines cross-entropy loss in $\mathbb{R}^Q$ using softmax, rather than analyzing the intrinsic geometry of the probability simplex or manifold.
- What evidence would resolve it: A derivation of the adapted gradient flow within the information geometry framework that preserves the homotopy equivalence or explicit convergence formulas.

### Open Question 2
- Question: How does the choice of parameter initialization $\theta_0$ determine the feasibility of the prescribed path constraint $\partial_s x(s) \in \text{range}(D(s))$?
- Basis in paper: [inferred] Remark 3.1 notes that satisfying the constraint for prescribed paths "could depend on the choice of $\theta_0$" and that Jacobian properties determining dynamics might vary even within the class of initializations mapping to the same $x_0$.
- Why unresolved: The paper establishes the method for finding $\theta(s)$ given a path $x(s)$ but characterizes the satisfaction of the range condition as a potential dependency rather than a solved property.
- What evidence would resolve it: A theoretical characterization or empirical study identifying the specific conditions on $\theta_0$ required to ensure $\partial_s x(s)$ remains in the range of $D(s)$ for a target output path.

### Open Question 3
- Question: What are the convergence dynamics for the cross-entropy loss when label components are zero (e.g., one-hot encoding)?
- Basis in paper: [inferred] Proposition 2.8 and its proof assume labels have strictly positive components ($y_j > 0$) to ensure the equilibrium $\sigma(f^*) = y$ and the explicit minimum formula are well-defined; Footnote 2 contrasts this with the common use of one-hot labels.
- Why unresolved: The derivation of the explicit global minimum relies on inverting the label vector via the logarithm, which is undefined for zero components.
- What evidence would resolve it: An extension of Proposition 2.8 providing a convergence limit or flow analysis for the boundary case where $y$ contains zeros.

## Limitations
- The central results rely heavily on the full-rank Jacobian condition, which may not hold throughout training in practical overparametrized networks
- Computing $(D^T D)^+$ at each optimization step is computationally expensive ($O(K^2 QN + K^3)$), making the adapted flow impractical for large networks
- The proofs assume exact rank conditions and continuous flows, but practical implementations require discrete time steps and tolerance-based rank computations

## Confidence
**High Confidence**: The homotopy equivalence mechanism (Mechanism 1) is rigorously proven with explicit construction. The fixed-point criterion (Mechanism 3) follows directly from linear algebra and convexity arguments.

**Medium Confidence**: The reparametrization to linear interpolation (Mechanism 2) is correct when rank is maintained, but the practical applicability depends on whether real networks maintain full rank throughout training. The explicit cross-entropy minimum formula (Proposition 2.8) is correct but requires positive labels, limiting practical applicability.

**Low Confidence**: The practical implications for standard training algorithms (SGD, Adam) are unclear. The paper focuses on idealized gradient flow rather than stochastic or adaptive methods commonly used in practice.

## Next Checks
1. **Rank Stability Experiment**: Implement a 2-layer MLP on MNIST subset and track the minimum singular value of $DD^T$ during training with standard SGD. Determine whether rank $= QN$ is maintained throughout convergence and whether convergence correlates with rank stability.

2. **Adapted Flow Implementation**: Implement the adapted gradient flow with pseudoinverse preconditioning on a small regression task. Compare the trajectory to standard gradient descent and verify whether the time-reparametrized output follows linear interpolation as predicted by Proposition 2.5.

3. **Label Smoothing Impact**: Train the same architecture with one-hot versus smoothed labels on a classification task. Quantify whether the cross-entropy trajectory approaches the explicit minimum from Proposition 2.8 under smoothed labels but not under one-hot labels.