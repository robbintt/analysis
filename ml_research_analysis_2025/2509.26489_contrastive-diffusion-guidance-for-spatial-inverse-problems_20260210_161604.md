---
ver: rpa2
title: Contrastive Diffusion Guidance for Spatial Inverse Problems
arxiv_id: '2509.26489'
source_url: https://arxiv.org/abs/2509.26489
tags:
- floorplan
- diffusion
- contrastive
- trajectory
- likelihood
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: We address the inverse problem of reconstructing 2D floorplans
  from user trajectories inside a building. Our key idea is to replace the non-differentiable
  likelihood score from a path-planning operator with a smoother contrastive embedding-based
  surrogate, enabling stable diffusion-based posterior sampling.
---

# Contrastive Diffusion Guidance for Spatial Inverse Problems

## Quick Facts
- **arXiv ID:** 2509.26489
- **Source URL:** https://arxiv.org/abs/2509.26489
- **Reference count:** 40
- **Primary result:** 0.91-0.95 F1 score and 0.84-0.90 IoU on 2D floorplan reconstruction from trajectories

## Executive Summary
This paper addresses the spatial inverse problem of reconstructing 2D floorplans from user trajectories inside buildings. The key innovation is replacing the non-differentiable likelihood score from a path-planning operator with a smoother contrastive embedding-based surrogate, enabling stable diffusion-based posterior sampling. By training encoders to pull together compatible floorplan-trajectory pairs and push apart mismatched ones, CoGuide achieves state-of-the-art performance on both sparse and dense trajectory inputs while avoiding the instability of differentiating discrete path-planning algorithms.

## Method Summary
The method trains an unconditional diffusion model on floorplans and two separate ViT encoders for floorplans and trajectories in a shared embedding space. The likelihood score is replaced with the gradient of the inner product between these embeddings, serving as a smooth surrogate for path compatibility. During inference, DDIM sampling is augmented with Adam-based guidance using this contrastive signal and an explicit intersection penalty to ensure generated walls respect observed trajectories. The approach is validated on the HouseExpo dataset with A* trajectories at three densities.

## Key Results
- CoGuide achieves 0.91-0.95 F1 score and 0.84-0.90 IoU on floorplan reconstruction
- Outperforms differentiable path-planning baselines (Neural A*, TransPath, DiPPeR) by 6-9% F1 on sparse trajectories
- Adam optimizer in guidance loop improves convergence over standard SGD by 6-9% F1
- Intersection penalty significantly improves validity but must be carefully tuned

## Why This Works (Mechanism)

### Mechanism 1
Replacing non-differentiable forward operators with contrastive embedding distances stabilizes gradient guidance in diffusion models. The method avoids directly differentiating the path-planning operator by training encoders using InfoNCE loss, then using the gradient of their inner product as a smooth surrogate for the likelihood score. The core assumption is that the embedding space preserves semantic compatibility between floorplans and trajectories well enough that gradients point toward valid solutions.

### Mechanism 2
Injecting Adam optimizer into the reverse diffusion loop integrates guidance signals over short DDIM schedules. Standard SGD under-integrates gradients when using few-step DDIM sampling, while Adam's momentum and adaptive learning rates smooth the optimization trajectory within the denoising loop. The core assumption is that the posterior loss landscape is complex enough to require adaptive moments for convergence.

### Mechanism 3
An explicit intersection penalty enforces hard physical constraints that data-driven priors might violate. The penalty term directly penalizes the diffusion model if it generates walls intersecting observed trajectory pixels. The core assumption is that observed trajectories are noise-free or pre-processed to represent valid walking paths.

## Foundational Learning

- **Concept:** Diffusion Posterior Sampling (DPS) and Tweedie's Formula
  - **Why needed here:** Understand how diffusion models solve inverse problems by decomposing the posterior score into prior and likelihood components. Tweedie's formula bridges estimating clean images from noisy ones to compute gradients.
  - **Quick check question:** How does the gradient of the likelihood term relate to the denoised estimate in the DPS framework?

- **Concept:** Contrastive Learning (InfoNCE Loss)
  - **Why needed here:** This is the core solution proposed. Grasp how maximizing similarity of positive pairs while minimizing similarity of negatives creates a metric space where distance equates to incompatibility.
  - **Quick check question:** Why does the gradient of the inner product approximate the likelihood score at the InfoNCE optimum?

- **Concept:** Non-differentiable Path Planning (A* Algorithm)
  - **Why needed here:** Understand why standard gradient descent fails on discrete graph search and why a surrogate is mathematically required.
  - **Quick check question:** Why does the discrete nature of A* expansion cause non-smooth Jacobian, making gradient-based optimization unstable?

## Architecture Onboarding

- **Component map:** HouseExpo dataset -> Trajectory generation via A* -> Contrastive encoder training -> Unconditional diffusion training -> DDIM inference with Adam guidance
- **Critical path:** 1) Generate synthetic trajectories on floorplans using A* at three densities, 2) Train contrastive encoders with supervised contrastive loss + alignment, 3) Train standard unconditional diffusion model, 4) Run DDIM with Adam-guided step using frozen encoders
- **Design tradeoffs:** DDIM speed vs. integration requires Adam optimizer, adding computational overhead; intersection penalty enforces validity but may reduce diversity
- **Failure signatures:** Trajectory bleeding (walls intersect trajectories), structure drift (floorplan doesn't match trajectory), mode collapse (all outputs look the same)
- **First 3 experiments:** 1) Ablate optimizer (SGD vs Adam) to verify convergence claim, 2) Vary sparsity to assess robustness, 3) Sweep intersection penalty to find optimal balance

## Open Questions the Paper Calls Out

- **Open Question 1:** Can the framework extend to other non-differentiable forward operators like inferring city maps from GPS trajectories or molecular structures from measured properties?
- **Open Question 2:** How does the presence of unobserved obstacles like furniture affect reconstruction accuracy and embedding alignment?
- **Open Question 3:** Can this approach adapt to blind inverse problems where the forward operator is unknown or only partially characterized?
- **Open Question 4:** How robust is the surrogate likelihood when the synthetic A* planner deviates significantly from real human navigation policies?

## Limitations
- The contrastive surrogate's effectiveness depends on embedding space alignment, but ablation studies on dimensionality and alternative architectures are lacking
- The intersection penalty assumes noise-free trajectories, but real trajectories contain sensor noise that could create invalid wall-holes
- The claim that Adam is "necessary" for DDIM guidance lacks comparison to higher-step DDIM schedules without Adam

## Confidence
- **High confidence:** Contrastive embedding approach outperforms differentiable planners on sparse trajectories (validated by Table 1 and Table 3)
- **Medium confidence:** Adam optimizer improves DDIM convergence, though necessity claim could be stronger with alternative scheduling comparisons
- **Medium confidence:** Intersection penalty improves F1 scores, but paper doesn't analyze failure cases with noisy trajectories

## Next Checks
1. **Embedding sensitivity:** Vary embedding dimension (128, 512) and architecture (CNN vs ViT) to quantify robustness to architectural choices
2. **Noise robustness:** Add Gaussian noise to trajectories during inference to test whether intersection penalty creates artifacts in realistic scenarios
3. **Guidance integration:** Compare Adam-based guidance with standard DDIM at 500-1000 steps to determine if adaptive optimization is truly necessary or compensates for short schedules