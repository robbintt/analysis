---
ver: rpa2
title: 'DispatchMAS: Fusing taxonomy and artificial intelligence agents for emergency
  medical services'
arxiv_id: '2510.21228'
source_url: https://arxiv.org/abs/2510.21228
tags:
- emergency
- caller
- dispatch
- agent
- system
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: DispatchMAS is a taxonomy-grounded multi-agent system that simulates
  realistic emergency medical dispatch interactions using LLMs. It addresses challenges
  in dispatcher training and decision support by grounding agent conversations in
  a structured clinical taxonomy and fact commons derived from MIMIC-III and national
  dispatch protocols.
---

# DispatchMAS: Fusing taxonomy and artificial intelligence agents for emergency medical services

## Quick Facts
- arXiv ID: 2510.21228
- Source URL: https://arxiv.org/abs/2510.21228
- Reference count: 40
- Primary result: Taxonomy-grounded LLM agents simulate realistic emergency dispatch with 94% correct external-agent contact and 91% guidance efficacy

## Executive Summary
DispatchMAS is a multi-agent system that simulates emergency medical dispatch interactions using large language models grounded in a structured clinical taxonomy. The system addresses dispatcher training and decision support challenges by creating realistic caller-dispatcher dialogues constrained by protocol-aligned knowledge retrieval. Human evaluation by four physicians on 100 cases showed high dispatch effectiveness and guidance efficacy, while algorithmic metrics confirmed professional communication tone. The approach demonstrates how taxonomy-grounded RAG can reduce hallucinations and enforce protocol adherence in high-stakes medical simulations.

## Method Summary
DispatchMAS uses AutoGen framework to orchestrate a caller agent and dispatcher agent in a turn-based dialogue system. The dispatcher agent is grounded through RAG that retrieves protocol instructions from a fact commons (MIMIC-III + national protocols) based on chief complaint classification. The caller agent generates scenarios from MIMIC-III patient profiles with six different caller identities. Evaluation combines physician assessment of clinical outcomes with algorithmic metrics measuring sentiment, emotion, and politeness. The system uses a lightweight classifier to identify chief complaints from dialogue history at each turn, enabling dynamic protocol retrieval.

## Key Results
- Dispatch Effectiveness: 94% correct external-agent contact, 97% call-back instructions
- Guidance Efficacy: 91% advice provision rate
- Communication Quality: 74% neutral sentiment, 90% neutral emotion, 81 Flesch reading ease, 60% politeness with zero impolite responses

## Why This Works (Mechanism)

### Mechanism 1: Taxonomy-Grounded RAG Reduces Hallucinations
Grounding LLM agents in structured clinical taxonomy and fact commons reduces hallucinations by constraining generative process with protocol-aligned knowledge. A lightweight classifier identifies chief complaint from dialogue history, retrieves specific procedural instructions, and injects them into dispatcher agent's prompt. Quality depends on completeness and accuracy of static taxonomy—retrieval failures or incorrect CC classification lead to generic or incorrect behavior.

### Mechanism 2: Persona-Based Caller Simulation Tests Dispatcher Robustness
Simulating diverse caller distress through persona-based agents enables robustness testing beyond static datasets. Caller agent generates narrative backgrounds from MIMIC-III profiles, selects one of six identities, and adopts linguistic samples from real 911 transcripts to introduce controlled ambiguity and emotional urgency. LLM must authentically simulate "distress" to stress-test dispatcher agent; sterile simulations reduce evaluation validity.

### Mechanism 3: Hybrid Evaluation Validates Clinical and Communication Quality
Combining human physician evaluation with algorithmic metrics ensures both clinical safety and professional communication tone. Physicians assess dispatch effectiveness and guidance efficacy while algorithmic tools measure sentiment, emotion, and politeness. This dual approach prevents the "AI is empathetic but clinically wrong" or "clinically right but abrasive" scenarios, though correlation between linguistic metrics and operational outcomes remains assumed.

## Foundational Learning

- **Retrieval-Augmented Generation (RAG)**: Critical for understanding how Dispatcher Agent retrieves "fact commons" based on evolving dialogue context. Quick check: If CC classifier identifies "Cardiac Arrest," what specific component of prompt does system retrieve and inject?

- **Multi-Agent Systems (MAS) / AutoGen**: Essential for understanding conversation loop management between Caller and Dispatcher agents. Quick check: Does Dispatcher Agent have access to Caller Agent's internal "thought process" or only its utterances?

- **Taxonomy/Ontology Engineering**: Performance bound by "32 chief complaints" and "6-phase protocol" structure. Quick check: If new emergency protocol "Pandemic Infectious Disease" added, which three components need updating?

## Architecture Onboarding

- **Component map**: Knowledge Layer (Static Taxonomy → Fact Commons) → Agentic Layer (Caller Agent → CC Classifier → RAG → Dispatcher Agent) → Orchestration (AutoGen framework) → Evaluation Layer (Physician Interface + Algorithmic Metrics)

- **Critical path**: 1. Seed Case Generation (MIMIC-III → Caller Persona) → 2. Caller Agent Utterance → CC Classifier (History Analysis) → 3. Protocol Retrieval (Query Fact Commons based on CC) → 4. Prompt Injection (Protocol + Context → Dispatcher Agent) → 5. Dispatcher Response → Loop or Terminate

- **Design tradeoffs**: Static vs. Dynamic Knowledge (stability vs. manual updates), Mocked vs. Real APIs (simplified isolation vs. hidden integration failures)

- **Failure signatures**: Premature Misclassification (wrong CC locks early, retrieval reinforces error), Question Overload (Dispatcher prioritizes information gathering over intervention in time-critical events)

- **First 3 experiments**: 1. Isolate RAG Component (10 cases with RAG disabled vs. enabled to quantify "Dispatch Effectiveness" lift) → 2. Stress Test Caller Identities (specifically "Limited-Proficiency Caller" persona to verify high "Advice Provision" despite linguistic ambiguity) → 3. Failure Analysis (extract 6% of cases where "correct external-agent contact" failed and categorize Classifier vs. Dispatcher origin)

## Open Questions the Paper Calls Out

- **Training Outcomes**: Does DispatchMAS improve training outcomes for human dispatchers compared to traditional role-play or static scenario methods? Current evaluation used physicians assessing simulated calls, not trainees demonstrating skill acquisition or retention in training curriculum.

- **Comparative Performance**: How does DispatchMAS performance compare to domain-specific pre-trained models on standardized EMS benchmarks? No comparative evaluation conducted; current results establish feasibility but not relative superiority to existing approaches.

- **Dynamic Updates**: What mechanisms enable fact commons and taxonomy to be updated dynamically as clinical guidelines evolve? Current system requires manual curation and re-deployment; no automated update pipeline described.

- **Algorithmic Bias**: What types of algorithmic bias emerge in dispatcher-caller simulations across diverse linguistic, cultural, and socioeconomic caller profiles? Caller identities were simulated but no stratified analysis of performance disparities across caller identity types was reported.

## Limitations

- Evaluation relied on only four physicians rating 100 simulated cases, limiting generalizability
- Taxonomy is explicitly static, requiring manual updates for new medical protocols
- Performance with truly low-resource languages or non-standard dialects beyond "Limited-Proficiency Caller" identity remains untested
- Integration with real emergency responder APIs was not evaluated as these were mocked by LLMs

## Confidence

- **High Confidence**: Taxonomy-grounded RAG mechanism effectively reduces hallucinations when retrieval succeeds (supported by 94% correct external-agent contact)
- **Medium Confidence**: Persona-based caller simulation genuinely tests dispatcher robustness, though authenticity of LLM's "distress" simulation is uncertain
- **Medium Confidence**: Hybrid evaluation provides comprehensive assessment, though correlation between linguistic metrics and real-world outcomes is assumed

## Next Checks

1. **RAG Isolation Test**: Run 10 cases with RAG disabled versus enabled to quantify specific contribution to "Dispatch Effectiveness" scores
2. **Classifier Robustness Test**: Log CC predictions each turn against ground truth to identify premature misclassification patterns that could cascade into protocol errors
3. **Limited-Proficiency Stress Test**: Specifically evaluate "Limited-Proficiency Caller" persona to verify Dispatcher maintains high "Advice Provision" rates despite linguistic ambiguity