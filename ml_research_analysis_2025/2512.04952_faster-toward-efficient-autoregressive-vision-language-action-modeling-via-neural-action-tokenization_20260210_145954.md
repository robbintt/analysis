---
ver: rpa2
title: 'FASTer: Toward Efficient Autoregressive Vision Language Action Modeling via
  Neural Action Tokenization'
arxiv_id: '2512.04952'
source_url: https://arxiv.org/abs/2512.04952
tags:
- action
- arxiv
- preprint
- fast
- fastervq
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of efficient action tokenization
  in autoregressive vision-language-action (VLA) models for robotics. It introduces
  FASTer, a unified framework comprising FASTerVQ, a learnable action tokenizer using
  residual vector quantization with patchification and hybrid transformers, and FASTerVLA,
  a VLA model with block-wise autoregressive decoding and a lightweight action expert.
---

# FASTer: Toward Efficient Autoregressive Vision Language Action Modeling via Neural Action Tokenization

## Quick Facts
- arXiv ID: 2512.04952
- Source URL: https://arxiv.org/abs/2512.04952
- Reference count: 26
- Key outcome: Introduces FASTer, a unified framework for efficient action tokenization in autoregressive VLA models, achieving high compression ratios and superior reconstruction fidelity across diverse tasks and embodiments.

## Executive Summary
This paper addresses the challenge of efficient action tokenization in autoregressive vision-language-action (VLA) models for robotics. It introduces FASTer, a unified framework comprising FASTerVQ, a learnable action tokenizer using residual vector quantization with patchification and hybrid transformers, and FASTerVLA, a VLA model with block-wise autoregressive decoding and a lightweight action expert. FASTerVQ achieves high compression ratios and superior reconstruction fidelity across diverse tasks and embodiments, while FASTerVLA improves inference speed and task performance. Experiments demonstrate state-of-the-art results on benchmarks including Libero, Simpler-Bridge, and real-world robotic platforms, with up to 97.9% success rate and significant latency reductions compared to prior methods. The framework also generalizes effectively across VLM backbones and action representations.

## Method Summary
The paper introduces FASTer, a unified framework for efficient autoregressive vision-language-action (VLA) modeling. FASTerVQ is a learnable action tokenizer that employs residual vector quantization with patchification and hybrid transformers to achieve high compression ratios and superior reconstruction fidelity. FASTerVLA, the VLA model, uses block-wise autoregressive decoding and a lightweight action expert to improve inference speed and task performance. The framework demonstrates state-of-the-art results on benchmarks such as Libero, Simpler-Bridge, and real-world robotic platforms, with significant latency reductions and success rate improvements compared to prior methods. The approach generalizes effectively across VLM backbones and action representations, addressing the challenges of efficient action tokenization in robotics.

## Key Results
- FASTerVQ achieves up to 30× compression ratios while maintaining superior reconstruction fidelity.
- FASTerVLA improves inference speed with up to 96.6% latency reduction and achieves up to 97.9% success rate on real-world robotic tasks.
- The framework generalizes effectively across different VLM backbones and action representations, demonstrating state-of-the-art performance on benchmarks like Libero and Simpler-Bridge.

## Why This Works (Mechanism)
The paper's mechanism relies on two key components: FASTerVQ and FASTerVLA. FASTerVQ uses residual vector quantization with patchification and hybrid transformers to efficiently compress action tokens while preserving fidelity. This compression reduces the computational load during autoregressive decoding. FASTerVLA leverages this compressed representation with block-wise autoregressive decoding and a lightweight action expert, enabling faster inference and improved task performance. The combination of efficient tokenization and optimized decoding allows the framework to achieve high success rates and low latency, even in complex robotic environments.

## Foundational Learning
- **Residual Vector Quantization**: A technique for compressing high-dimensional data by iteratively quantizing residuals. Why needed: To achieve high compression ratios while maintaining reconstruction fidelity. Quick check: Verify compression ratio and reconstruction error on benchmark datasets.
- **Patchification**: The process of dividing input data into smaller patches for localized processing. Why needed: To enable efficient vector quantization by reducing the complexity of each patch. Quick check: Evaluate the impact of patch size on compression performance.
- **Hybrid Transformers**: Models that combine different transformer architectures for improved efficiency and performance. Why needed: To balance computational efficiency and modeling capacity in the tokenizer. Quick check: Compare performance with standard transformer architectures.
- **Block-wise Autoregressive Decoding**: A decoding strategy that processes data in blocks to reduce sequential dependencies. Why needed: To improve inference speed by parallelizing decoding steps. Quick check: Measure latency improvements compared to standard autoregressive decoding.
- **Lightweight Action Expert**: A simplified model for generating actions based on compressed tokens. Why needed: To reduce computational overhead while maintaining task performance. Quick check: Evaluate success rates with and without the action expert.
- **Generalization Across VLM Backbones**: The ability of the framework to work with different vision-language models. Why needed: To ensure flexibility and adaptability to various robotic platforms. Quick check: Test performance with multiple VLM backbones on benchmark tasks.

## Architecture Onboarding

**Component Map**
FASTerVQ (Patchification -> Residual Vector Quantization -> Hybrid Transformers) -> FASTerVLA (Block-wise Autoregressive Decoding -> Lightweight Action Expert) -> Action Output

**Critical Path**
Input data -> Patchification -> Residual Vector Quantization -> Hybrid Transformers -> Compressed Tokens -> Block-wise Autoregressive Decoding -> Lightweight Action Expert -> Action Output

**Design Tradeoffs**
The framework balances compression efficiency and reconstruction fidelity by using residual vector quantization with patchification. While this approach achieves high compression ratios, it may require careful tuning of patch size and quantization parameters to maintain performance. The lightweight action expert reduces computational overhead but may limit the complexity of generated actions.

**Failure Signatures**
- Poor reconstruction fidelity due to overly aggressive compression.
- Increased latency if block-wise decoding is not optimized for parallel processing.
- Reduced success rates if the lightweight action expert cannot handle complex tasks.

**3 First Experiments**
1. Evaluate compression ratio and reconstruction fidelity of FASTerVQ on a diverse set of action datasets.
2. Measure inference latency and success rates of FASTerVLA on benchmark robotic tasks.
3. Test generalization across different VLM backbones and action representations on Libero and Simpler-Bridge.

## Open Questions the Paper Calls Out
None

## Limitations
- The framework's performance in unstructured, dynamic environments with varying lighting and unexpected obstacles is not fully validated.
- The learned action tokenization approach requires substantial training data and computational resources for the quantization network (FASTerVQ).
- The evaluation primarily focuses on controlled environments and specific robotic platforms, limiting the assessment of real-world robustness.

## Confidence

**High Confidence**
- The technical implementation of FASTerVQ's vector quantization architecture and FASTerVLA's autoregressive decoding framework is well-documented and reproducible. The reported compression ratios (up to 30×) and latency improvements are supported by clear experimental evidence.

**Medium Confidence**
- The claims about generalization across different VLM backbones and action representations are substantiated by experiments on Libero, Simpler-Bridge, and real robotic platforms, but the diversity of tested scenarios could be broader to fully validate these claims.
- The state-of-the-art performance claims on the evaluated benchmarks are well-supported, though the comparison set is limited to a few recent VLA models, and the absence of some established baselines makes comprehensive ranking difficult.

## Next Checks
1. **Real-World Deployment Testing**: Evaluate FASTerVLA in unstructured, dynamic environments with varying lighting, object placement, and unexpected obstacles to validate robustness claims beyond the controlled benchmarks.
2. **Ablation Studies on Quantization Parameters**: Conduct systematic ablation studies on FASTerVQ's patchification granularity, codebook size, and residual quantization depth to quantify their impact on reconstruction fidelity and task performance.
3. **Scalability to Diverse Embodiments**: Test the framework's performance across a wider range of robotic embodiments (e.g., quadrupeds, humanoids) and action spaces (e.g., continuous control) to assess true generalization capability.