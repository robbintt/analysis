---
ver: rpa2
title: 'Learning Beyond the Gaussian Data: Learning Dynamics of Neural Networks on
  an Expressive and Cumulant-Controllable Data Model'
arxiv_id: '2602.02153'
source_url: https://arxiv.org/abs/2602.02153
tags:
- data
- learning
- high-order
- cumulants
- non-gaussian
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a cumulant-controllable generative model
  for studying high-order statistics in neural network learning. The authors construct
  a two-layer generative model where the activation function is expanded via Hermite
  polynomials, allowing precise control over skewness, kurtosis, and other cumulants.
---

# Learning Beyond the Gaussian Data: Learning Dynamics of Neural Networks on an Expressive and Cumulant-Controllable Data Model

## Quick Facts
- arXiv ID: 2602.02153
- Source URL: https://arxiv.org/abs/2602.02153
- Authors: Onat Ure; Samet Demir; Zafer Dogan
- Reference count: 0
- Primary result: Neural networks learn data statistics sequentially, capturing low-order moments before high-order cumulants

## Executive Summary
This paper introduces a cumulant-controllable generative model that enables precise manipulation of high-order statistics in synthetic data. The authors construct a two-layer generative model where activation functions are expanded via Hermite polynomials, allowing independent control over skewness, kurtosis, and other cumulants. Experiments on both synthetic and Fashion-MNIST data demonstrate that neural networks trained with SGD progressively learn low-order statistics (mean and covariance) before capturing high-order cumulants, confirming a "moment-wise progression" in training. The framework bridges simplified Gaussian assumptions and real-world data complexity, offering a principled approach for investigating distributional effects in machine learning.

## Method Summary
The authors construct a two-layer generative model x = W·Θ_ℓ(z; F) where z~N(μ, Σ) and Θ_ℓ(z; F) = Σ_{i=0}^ℓ c_i·He_i(Fz) uses Hermite polynomial expansion. This architecture enables interpretable control over cumulants through Hermite coefficients c_i. For downstream learning dynamics, they train two-layer ReLU networks on binary classification tasks distinguishing Gaussian from non-Gaussian samples. The Gaussian-equivalent datasets match mean and covariance but lack higher-order structure. Learning progression is tracked by comparing test loss on non-Gaussian versus Gaussian-equivalent test sets across training epochs.

## Key Results
- Networks capture low-order statistics (mean, covariance) before high-order cumulants during training
- Non-Gaussian data with controlled high-order cumulants improves generalization compared to Gaussian-equivalent data
- The Hermite-based generative model successfully generates Fashion-MNIST-like samples with interpretable cumulant structure
- Pretrained model parameters transfer to Fashion-MNIST classification, validating practical expressivity

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Hermite polynomial coefficients provide interpretable, independent control over data cumulants (skewness, kurtosis, etc.)
- **Mechanism:** Hermite polynomials form a complete orthogonal basis under Gaussian measure. When expanding σ(Fz) = ΣcᵢHeᵢ(Fz), each coefficient cᵢ maps to specific cumulant structures through polynomial relationships κᵢ = gᵢ(c₀, ..., cₗ). The orthogonality ensures terms don't interfere, enabling disentangled control.
- **Core assumption:** Latent input z is Gaussian; the activation σ has finite square expectation under this measure.
- **Evidence anchors:**
  - [abstract] "enables interpretable control over high-order cumulants such as skewness and kurtosis through the Hermite coefficients"
  - [Section 2, Proposition 2] "Given {κᵢ}ˡᵢ₌₀, solving the resulting system of ℓ+1 polynomial equations yields the required coefficients"
  - [corpus] Limited direct corpus support for Hermite-based cumulant control; related work on Gaussian mixture layers (arXiv:2508.04883) addresses distributional expressivity but not cumulant-specific parameterization.
- **Break condition:** If input z is non-Gaussian or σ lacks finite moments, orthogonality breaks and coefficient-cumulant mapping becomes coupled/uninterpretable.

### Mechanism 2
- **Claim:** Neural networks trained with SGD learn data statistics sequentially: low-order moments (mean, covariance) are captured before high-order cumulants
- **Mechanism:** Gradient descent on neural networks exhibits "distributional simplicity bias"—early optimization aligns weights with dominant low-order structure because these contribute most to loss reduction. Higher-order terms require more training iterations to be incorporated.
- **Core assumption:** Data has non-Gaussian structure; network has sufficient capacity to represent high-order features.
- **Evidence anchors:**
  - [abstract] "networks first capture low-order statistics such as mean and covariance, and progressively learn high-order cumulants"
  - [Section 3.2, Figure 1] Test loss divergence between non-Gaussian and Gaussian-equivalent datasets emerges progressively, not immediately
  - [corpus] Refinetti et al. (arXiv:2505.04362, cited in paper) document similar "increasing complexity" learning patterns
- **Break condition:** If learning rate is too high or architecture is severely underparameterized, networks may fail to ever capture high-order structure before overfitting noise.

### Mechanism 3
- **Claim:** The two-layer generative architecture with Hermite-expanded activations is dense in the space of probability measures with finite moments
- **Mechanism:** Universal approximation theorem guarantees two-layer networks approximate any regular function. Combined with Hermite completeness (dense in L² under Gaussian measure), the model x = WΘ(z;F) can approximate any target distribution with finite moments by tuning F, W, and Hermite coefficients.
- **Core assumption:** p > d (latent dimension exceeds output dimension); target distribution has finite moments.
- **Evidence anchors:**
  - [Section 2, Proposition 1] "class of data models defined by (2) is dense in the space of d-dimensional probability measures with finite moments"
  - [Section 3.3] Pretrained model successfully generates Fashion-MNIST-like samples, confirming practical expressivity
  - [corpus] Corpus evidence on expressivity is consistent but generic (arXiv:2508.04883, arXiv:2503.00856 address two-layer network approximation)
- **Break condition:** Finite truncation ℓ limits expressivity; if target requires cumulants beyond order ℓ, model cannot represent it exactly.

## Foundational Learning

- **Concept: Cumulants vs. Moments**
  - Why needed here: The paper uses cumulants (not raw moments) as the controllable "dials" for non-Gaussianity. Cumulants beyond second order vanish for Gaussians, making them natural measures of deviation.
  - Quick check question: Given a distribution with mean μ and variance σ², what is its third cumulant if it's Gaussian? (Answer: Zero—all cumulants beyond κ₂ are zero for Gaussians.)

- **Concept: Hermite Polynomials**
  - Why needed here: These provide the mathematical basis for the expansion. Understanding orthogonality under Gaussian measure is essential to see why coefficients are interpretable.
  - Quick check question: Why are Hermite polynomials preferred over Taylor expansion for this application? (Answer: Orthogonality under Gaussian measure ensures coefficient independence; Taylor terms couple.)

- **Concept: Two-Layer Neural Network Training Dynamics**
  - Why needed here: The generative model and the learner share this architecture. Understanding feature learning in shallow networks grounds the moment-wise progression findings.
  - Quick check question: In a two-layer ReLU network, what determines whether the network learns first-layer vs. second-layer features first? (Answer: Scale of output weights and learning rate; large second-layer weights freeze first-layer features early.)

## Architecture Onboarding

- **Component map:**
  z ~ N(μ, Σ)  →  [Linear: Fz]  →  [Hermite Expansion: σₗ(Fz) = ΣcᵢHeᵢ(Fz)]  →  [Linear: W·Θₗ]  →  x̂

- **Control knobs:**
  - c₀, c₁: Control mean, variance (keep fixed for controlled experiments)
  - c₂, c₃, ...: Control skewness, kurtosis, higher cumulants
  - F, W: Shape correlation structure across dimensions

- **Critical path:**
  1. Define target cumulants {κᵢ} for your experiment
  2. Solve polynomial system for Hermite coefficients (c₀...cₗ)
  3. Sample z ~ N(0, I), apply transformation, verify empirical cumulants match targets
  4. Train downstream network, track test loss on non-Gaussian vs. Gaussian-equivalent heldout sets

- **Design tradeoffs:**
  - Higher truncation order ℓ → more cumulant control but more coefficients to tune; risk of overfitting to finite samples
  - Latent dimension p: Larger p enables richer correlations but increases computational cost
  - Activation choice: tanh is natural for bounded outputs; for unbounded, use identity + higher-order Hermite terms

- **Failure signatures:**
  - Gaussian and non-Gaussian test losses converge immediately → Hermite coefficients incorrectly set (likely all cᵢ≈0 for i≥2)
  - Generated samples have unrealistic outliers → Excessive high-order coefficients creating heavy tails
  - Training diverges → Hermite terms producing unbounded gradients; normalize coefficients

- **First 3 experiments:**
  1. **Sanity check:** Set ℓ=3, c₂=c₃=0, confirm generated data is Gaussian (match moments analytically). Then activate c₂=0.2, verify skewness appears in empirical distribution.
  2. **Controlled learning dynamics:** Train two-layer ReLU network on binary classification (Gaussian vs. non-Gaussian class). Plot test loss gap between non-Gaussian and Gaussian-equivalent test sets over epochs. Expect gap to widen progressively.
  3. **Real data transfer:** Pretrain generative model on a single Fashion-MNIST class (as in Section 3.3). Extract Hermite coefficients from fitted tanh, then ablate c₃, c₅ to test their contribution to downstream classification accuracy.

**Assumption:** The polynomial system for coefficient-to-cumulant mapping is solvable but may require numerical methods for ℓ≥4; paper does not detail solver implementation.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the theoretical mechanism that drives the observed moment-wise learning progression, where networks sequentially capture low-order statistics before high-order cumulants?
- Basis in paper: [inferred] The paper empirically demonstrates this progressive learning behavior but provides no theoretical explanation for why this sequential pattern emerges rather than simultaneous learning of all statistical orders.
- Why unresolved: The authors validate the phenomenon experimentally on synthetic and Fashion-MNIST data, but the underlying mechanism—whether arising from optimization dynamics, network architecture, or data structure—remains uncharacterized.
- What evidence would resolve it: A theoretical analysis deriving learning dynamics from first principles (e.g., via neural tangent kernel or mean-field frameworks) showing that gradient descent naturally prioritizes low-order moments; or experiments demonstrating whether the phenomenon persists across different optimizers, architectures, and loss functions.

### Open Question 2
- Question: Does the moment-wise learning dynamics generalize to deeper neural network architectures (3+ layers), and how does depth affect the utilization of high-order cumulants?
- Basis in paper: [inferred] All experiments use strictly two-layer networks for both the generative model and the trained classifier; no analysis of deeper architectures is provided.
- Why unresolved: Two-layer networks have well-studied theoretical properties, but modern practice employs much deeper architectures where feature learning and statistical structure may interact differently.
- What evidence would resolve it: Controlled experiments comparing learning dynamics across network depths (2, 4, 8, 16 layers) on the same cumulant-controlled data, measuring when and how high-order statistics are captured in each case.

### Open Question 3
- Question: Can explicit generalization bounds be derived that incorporate high-order cumulant structure, quantifying the benefit of non-Gaussian statistics?
- Basis in paper: [explicit] The introduction states "understanding how high-order statistics shape generalization has therefore become a priority," and results show "improved generalization when high-order cumulants are present," but no formal bounds are provided.
- Why unresolved: The framework enables cumulant control, but the quantitative relationship between specific cumulant values (e.g., skewness magnitude) and generalization gap remains empirical.
- What evidence would resolve it: Derivation of PAC-style or Rademacher complexity bounds that include cumulant terms; or systematic experiments mapping cumulant magnitudes to generalization gaps across multiple dataset sizes and model capacities.

## Limitations
- The Hermite coefficient-cumulant mapping requires numerical methods for ℓ≥4, potentially introducing approximation errors
- Moment-wise progression is demonstrated only on synthetic binary classification tasks; multi-class/regression transfer unproven
- Fashion-MNIST experiments rely on pretrained GAN weights, making it unclear whether improvements stem from Hermite structure versus pretraining artifacts

## Confidence
- **High Confidence:** Mathematical framework for Hermite-based cumulant control is sound with clear orthogonal basis properties and polynomial relationships
- **Medium Confidence:** Learning dynamics experiments show consistent patterns across multiple runs but use simplified binary classification setup
- **Low Confidence:** Practical benefits for real-world applications are not conclusively demonstrated; Fashion-MNIST results show correlation but cannot isolate specific contributions

## Next Checks
1. **Multi-class Extension:** Replicate the moment-wise progression experiments on Fashion-MNIST multi-class classification. Track per-class test loss gaps between non-Gaussian and Gaussian-equivalent samples across training epochs.
2. **Coefficient Ablation Study:** Systematically remove individual Hermite coefficients (c₃, c₄, etc.) from the pretrained Fashion-MNIST model and measure impact on classification accuracy. This isolates which high-order structures contribute to generalization.
3. **Distributional Transfer:** Train the same network architecture on a completely different dataset (e.g., CIFAR-10) using parameters transferred from Fashion-MNIST. Compare whether the moment-wise progression pattern persists across domain shifts.