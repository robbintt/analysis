---
ver: rpa2
title: Advancing Cross-lingual Aspect-Based Sentiment Analysis with LLMs and Constrained
  Decoding for Sequence-to-Sequence Models
arxiv_id: '2508.10366'
source_url: https://arxiv.org/abs/2508.10366
tags:
- sentiment
- language
- aspect
- absa
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the limited research on compound cross-lingual
  aspect-based sentiment analysis (ABSA) tasks, which traditionally rely on external
  translation tools. The authors propose a novel sequence-to-sequence approach using
  constrained decoding to eliminate the need for translation tools while improving
  cross-lingual ABSA performance by up to 10%.
---

# Advancing Cross-lingual Aspect-Based Sentiment Analysis with LLMs and Constrained Decoding for Sequence-to-Sequence Models

## Quick Facts
- arXiv ID: 2508.10366
- Source URL: https://arxiv.org/abs/2508.10366
- Authors: Jakub Šmíd; Pavel Přibáň; Pavel Král
- Reference count: 11
- This paper addresses the limited research on compound cross-lingual aspect-based sentiment analysis (ABSA) tasks, which traditionally rely on external translation tools. The authors propose a novel sequence-to-sequence approach using constrained decoding to eliminate the need for translation tools while improving cross-lingual ABSA performance by up to 10%.

## Executive Summary
This paper introduces a novel sequence-to-sequence approach for cross-lingual aspect-based sentiment analysis (ABSA) that eliminates the need for external translation tools. The authors propose using constrained decoding with multilingual models like mT5 to prevent the generation of aspect terms in the source language instead of the target language. The method was evaluated on three compound ABSA tasks (E2E-ABSA, ACTE, and TASD) across six languages, achieving new state-of-the-art results. Experiments showed that their approach consistently outperforms fine-tuned English-centric large language models like GPT-4o mini and LLaMA 3, while demonstrating comparable or superior performance to fine-tuned multilingual LLaMA 3.1 models.

## Method Summary
The proposed method reformulates ABSA extraction as a sequence-to-sequence generation task, where models generate natural language sequences instead of using classification heads. The approach uses multilingual mT5 or mBART models fine-tuned on English data with a special schema using markers like `[A]`, `[C]`, and `[P]` to represent aspect terms, categories, and polarities. Constrained decoding is implemented to dynamically restrict candidate tokens during inference, particularly forcing aspect terms to be extracted from the input sentence tokens or the implicit "it" token. This prevents the model from generating aspect terms in the source language (English) when processing target language inputs. The method achieves zero-shot cross-lingual transfer by leveraging the multilingual pre-training of the encoder-decoder models.

## Key Results
- Achieved new state-of-the-art results on three compound ABSA tasks (E2E-ABSA, ACTE, TASD) across six languages
- Improved cross-lingual ABSA performance by up to 10% compared to translation-dependent approaches
- Consistently outperformed fine-tuned English-centric large language models (GPT-4o mini, LLaMA 3) while matching or exceeding fine-tuned multilingual LLaMA 3.1 models
- Constrained decoding was particularly effective in preventing generation of aspect terms in the source language

## Why This Works (Mechanism)

### Mechanism 1: Source-Language Anchoring via Constrained Decoding
Constraining the decoder's vocabulary forces the model to extract aspect terms directly from the input sequence, mitigating the tendency to "translate" terms back to the source language (English). During inference, the decoding algorithm restricts candidate tokens for the `[A]` (Aspect Term) slot to tokens present in the input sentence or the specific implicit token "it". This dynamic vocabulary mask prevents the model from generating valid English words that are not in the target language input, effectively aligning the extraction with the target language surface form.

### Mechanism 2: Cross-Lingual Transfer via Natural Language Schema
Reformulating structured ABSA tuples into natural language sequences (e.g., "great" instead of "POS") improves zero-shot cross-lingual transfer compared to classification heads. By mapping labels to natural language words (shared across languages in the pre-training corpus) and using special markers, the model leverages the multilingual text-to-text pre-training of mT5/mBART. This allows the semantic understanding of "great" or "food quality" learned in English to generalize to other languages without requiring parallel corpora for fine-tuning.

### Mechanism 3: Fine-Tuning vs. In-Context Learning for Compound Tasks
Fine-tuning mid-sized multilingual encoders (mT5) outperforms few-shot prompting of large English-centric LLMs (GPT-4o mini, LLaMA 3) for compound structured extraction. Compound tasks require strict adherence to output formats and joint predictions. LLMs relying on in-context learning struggle with the precise alignment of multiple elements (term, category, polarity) in low-resource languages, whereas fine-tuning updates the model weights to specifically optimize for the cross-lingual alignment and formatting constraints of the task.

## Foundational Learning

- **Concept: Sequence-to-Sequence (Seq2Seq) Generation**
  - Why needed here: The paper frames extraction not as token classification (labeling words), but as text generation (writing the answer). You must understand how an encoder processes input and a decoder generates output token-by-token.
  - Quick check question: How does the model determine the next token in the sequence "The food was" given the input "The food was great"?

- **Concept: Constrained/Guided Decoding**
  - Why needed here: This is the core innovation. You need to understand how to modify the probability distribution at inference time to forbid certain tokens.
  - Quick check question: If the model's soft-max layer assigns the highest probability to "hungry", but the constraint list only allows ["great", "bad"], which token is selected?

- **Concept: Zero-Shot Cross-Lingual Transfer**
  - Why needed here: The method trains on English and tests on Spanish/Russian. You need to grasp that the model learns the *task* in English and applies it using shared multilingual representations.
  - Quick check question: Why can a model trained only on English restaurant reviews identify "comida" (food) in a Spanish review without seeing Spanish training labels?

## Architecture Onboarding

- **Component map:** Input: Raw text + Schema Markers (e.g., `[A] [C] [P]`) -> Backbone: mT5 (Encoder-Decoder) or mBART -> Constraint Layer: Dynamic Logit Mask -> Output: Linearized string of tuples (e.g., `[A] soup [C] food quality [P] great`)

- **Critical path:** The constraint logic (Algorithm 1) is the most brittle component. It relies on tracking the "last special token" (e.g., `[C]`) to determine the valid next vocabulary (e.g., "All categories"). If the generation diverges from the expected marker structure (e.g., generates two `[A]` tokens consecutively), the constraint logic must handle the fallback gracefully.

- **Design tradeoffs:**
  - mT5 vs. LLaMA 3.1: The paper recommends mT5 for efficiency (4.4x faster training, 15x faster inference) and robustness across all languages. LLaMA 3.1 is preferred *only* if inference speed is irrelevant and the target language is officially supported (e.g., Spanish/French), as it offers marginally higher performance ceiling.
  - CD vs. No CD: Constrained Decoding adds computational overhead to the inference loop but is strictly required for cross-lingual tasks to prevent language hallucination.

- **Failure signatures:**
  - Language Reversion: Model generates "service" (English) instead of "servicio" (Spanish) for an aspect term. *Fix:* Verify Constrained Decoding is active and `input_sentence` tokens are correctly passed to the candidate list.
  - Format Drift: Model generates `[A] food [P] great` (skipping `[C]`). *Fix:* Check if the constraint logic forces `[C]` after `[A]` closes.
  - Category Hallucination: Model invents a category not in the list. *Fix:* Verify the "All categories" list in the constraint configuration matches the dataset schema exactly.

- **First 3 experiments:**
  1. **Sanity Check (Monolingual):** Fine-tune mT5 on English data, evaluate on English test set with and without Constrained Decoding (CD) to ensure CD doesn't degrade native performance.
  2. **Ablation (Cross-Lingual):** Train on English, test on Spanish. Compare mT5 (no CD) vs. mT5 (with CD) to quantify the reduction in "source-language aspect terms."
  3. **Constraint Stress Test:** Run inference on the Spanish test set and specifically log instances where the model selects the implicit "it" token vs. extracting from text, to verify the constraint is correctly identifying explicit spans.

## Open Questions the Paper Calls Out

### Open Question 1
Can a unified multi-task learning approach, where a single model is trained simultaneously on E2E-ABSA, ACTE, and TASD, improve generalization and performance compared to the current task-specific fine-tuning method? The conclusion explicitly states, "Future research could explore multi-task learning, where a single model is trained simultaneously on multiple tasks, enabling a unified approach to handling diverse ABSA challenges." This is unresolved because the current study evaluates models fine-tuned independently for each specific compound task rather than investigating the potential benefits or trade-offs of a joint training objective.

### Open Question 2
How does the zero-shot cross-lingual transfer performance of the proposed method change when utilizing non-English languages (e.g., Spanish or Russian) as the source language for training? The authors note in the conclusion that "Additional experiments could examine various source-target language pair combinations to assess cross-lingual adaptability further." This is unresolved because the experimental setup exclusively uses English as the source language for transfer to the five target languages, leaving the dynamics of other transfer directions unexplored.

### Open Question 3
Can the constrained decoding framework be effectively extended to Aspect Sentiment Quad Prediction (ASQP) tasks which involve a fourth sentiment element (opinion terms), despite current data limitations? The conclusion suggests, "investigating tasks involving a fourth sentiment element (opinion terms) would be valuable, though current efforts are constrained by the limited availability of annotated data." This is unresolved because the current methodology is validated only on triplet and tuple tasks, and it's unclear if the constrained decoding mechanism can handle the added complexity of extracting opinion terms without significantly increasing error rates or computational cost.

## Limitations
- Limited ablation of constrained decoding variants - doesn't compare different constraint strategies or candidate selection methods
- Training data representation - doesn't analyze whether English training data adequately represents semantic space of target languages, particularly for morphologically rich languages
- Format constraint handling - doesn't address how system handles generation errors or deviations from expected token sequence format

## Confidence

**High Confidence** (Mechanistic claims well-supported by evidence):
- The constrained decoding mechanism effectively prevents generation of source-language aspect terms in cross-lingual settings
- The sequence-to-sequence formulation with natural language labels outperforms classification-based approaches for compound ABSA tasks
- Fine-tuning mT5 on English data enables zero-shot cross-lingual transfer across all six tested languages

**Medium Confidence** (Results robust but with some caveats):
- The 10% performance improvement over translation-dependent approaches, as this depends on specific translation system implementation details
- The claim that mT5 is more practical than translation tools, as practical considerations (latency, memory) weren't empirically measured
- The superiority of fine-tuning over in-context learning, as this comparison was limited to English-centric LLMs rather than multilingual models

**Low Confidence** (Limited evidence or speculative claims):
- The generalizability to languages beyond the six tested
- The claim that the approach eliminates the need for translation tools in all scenarios
- The assertion that constrained decoding is "particularly effective" without comparative analysis against alternative constraint strategies

## Next Checks
1. **Cross-lingual transfer robustness test:** Evaluate the trained models on a held-out language (e.g., Chinese or Arabic) not seen during development to assess generalization beyond the six tested languages.

2. **Constrained decoding ablation study:** Implement and compare alternative constraint strategies (e.g., allowing top-k candidates from the input, using semantic similarity instead of exact token matching) to quantify the specific contribution of the proposed constraint mechanism.

3. **Resource efficiency benchmarking:** Measure and report actual inference latency, memory consumption, and throughput for both the mT5-based approach and a translation-dependent baseline across all target languages to empirically validate the "practicality" claims.