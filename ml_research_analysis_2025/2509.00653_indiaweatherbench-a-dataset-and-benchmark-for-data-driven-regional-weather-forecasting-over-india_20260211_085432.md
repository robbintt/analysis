---
ver: rpa2
title: 'IndiaWeatherBench: A Dataset and Benchmark for Data-Driven Regional Weather
  Forecasting over India'
arxiv_id: '2509.00653'
source_url: https://arxiv.org/abs/2509.00653
tags:
- forecasting
- weather
- regional
- indiaweatherbench
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: IndiaWeatherBench addresses the need for a standardized benchmark
  for regional weather forecasting over India. It provides a curated dataset from
  the high-resolution IMDAA reanalysis product, preprocessed and split into train/validation/test
  sets, along with deterministic and probabilistic evaluation metrics.
---

# IndiaWeatherBench: A Dataset and Benchmark for Data-Driven Regional Weather Forecasting over India

## Quick Facts
- arXiv ID: 2509.00653
- Source URL: https://arxiv.org/abs/2509.00653
- Reference count: 40
- Introduces standardized benchmark with high-resolution Indian regional reanalysis data and multiple conditioning strategies

## Executive Summary
IndiaWeatherBench addresses the need for a standardized benchmark for regional weather forecasting over India by providing a curated dataset from the high-resolution IMDAA reanalysis product, preprocessed and split into train/validation/test sets, along with deterministic and probabilistic evaluation metrics. The benchmark includes strong baselines across diverse architectures (UNet, Transformer, Graph-based models) and two boundary conditioning strategies: high-resolution boundary forcing and coarse-resolution global input. Experiments show that transformer-based models (Stormer, Graphcast) generally outperform others under boundary forcing, while architectural choices matter significantly under coarse conditioning.

## Method Summary
The benchmark leverages the IMDAA reanalysis product at 0.0625° resolution (~6.25km) over a 1024×1024 km² domain covering India and surrounding seas. The dataset spans 1979-2022 with 6-hourly temporal resolution and includes 39 atmospheric variables across 7 pressure levels plus static fields. The data is cropped to 256×256 pixels and split into train (1979-2015), validation (2016-2018), and test (2019-2022) sets. Two conditioning strategies are supported: boundary forcing with 10-pixel halo of ground-truth boundary data, and coarse-resolution global conditioning using ERA5 data interpolated to match the regional grid. The benchmark implements deterministic models predicting state increments with latitude-weighted loss, and extends to probabilistic forecasting using EDM diffusion framework.

## Key Results
- Transformer-based models (Stormer, GraphCast) achieve the best overall performance under boundary forcing conditions
- UNet shows competitive performance across both conditioning strategies and is most robust to architectural changes
- Coarse-resolution global conditioning enables operational feasibility but requires careful architectural alignment, with Stormer failing due to mixed-resolution tokenization issues
- Diffusion-based probabilistic models are supported but show systematic under-dispersion (Spread/Skill Ratio < 1), indicating calibration challenges

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** High-resolution boundary forcing improves forecast accuracy by maintaining physical continuity at domain edges.
- **Mechanism:** The model receives 10 pixels of ground-truth boundary data wrapped around the regional state at each timestep, providing the model with external atmospheric context that would otherwise be missing from a limited-area domain. This allows the network to respect mass and momentum conservation principles implicitly.
- **Core assumption:** Boundary conditions are available at the same resolution as the regional model (which may not hold operationally without a high-resolution global model).
- **Evidence anchors:**
  - [Section 4.1]: "We can wrap these boundary values S_t around the current regional state X_t to provide a single input to the model with better continuity of meteorological fields such as wind, pressure, or temperature across domain edges."
  - [Section 5.1]: "Under the boundary forcing setting, Stormer and Graphcast achieve the best overall performance across most variables and lead times."
  - [Corpus]: Related work on stretched-grid vs limited-area modeling confirms boundary treatment is critical for regional MLWP (arxiv:2507.18378).

### Mechanism 2
- **Claim:** Coarse-resolution global conditioning enables operational feasibility but requires careful architectural alignment with multi-scale inputs.
- **Mechanism:** Global reanalysis (ERA5, 124×124 grid) is bilinearly interpolated to 256×256 and concatenated with regional state along the channel dimension. The model must learn to fuse high-resolution regional features with upsampled coarse global context.
- **Core assumption:** The model's tokenization or embedding scheme can handle mixed-resolution inputs without degrading attention or convolution operations.
- **Evidence anchors:**
  - [Section 4.1]: "This setup enables learning-based fusion of interior and global context, allowing the model to account for synoptic-scale drivers while preserving fine-scale variability."
  - [Section 5.1]: "Stormer becomes the worst-performing model [under coarse conditioning]... each token blends high-resolution regional context with upsampled coarse global input. This mixing of incompatible spatial scales within each token likely disrupts the attention mechanism."
  - [Corpus]: STCast (arxiv:2509.25210) proposes adaptive boundary alignment specifically to address this static/rigid boundary limitation.

### Mechanism 3
- **Claim:** Predicting state increments (∆X) rather than absolute states improves learning dynamics for autoregressive forecasting.
- **Mechanism:** The model learns to predict ∆X_{t+1} = X_{t+1} - X_t, and the forecast is reconstructed as X̂_{t+1} = X_t + ∆X̂_{t+1}. This residual formulation stabilizes training by keeping target magnitudes smaller and more consistent across variables.
- **Core assumption:** The increment signal has favorable spectral properties (smaller magnitude, more stationary) compared to absolute state prediction.
- **Evidence anchors:**
  - [Section 4.3]: "This formulation follows the practice in state-of-the-art models like GraphCast and Stormer, and has proven more effective than next-state prediction."
  - [Corpus]: Baguan (arxiv:2505.13873) uses strategic pre-training with similar dynamics learning but reports overfitting as a persistent challenge with limited data.

## Foundational Learning

- **Concept: Reanalysis data vs. raw observations**
  - Why needed here: IndiaWeatherBench is built on IMDAA reanalysis, which is a model-data fusion product (4D-Var assimilation), not direct observations. Understanding this distinction is critical for interpreting what the model actually learns and its generalization to real-world deployment.
  - Quick check question: Can you explain why a model trained on reanalysis might perform differently when deployed with raw observational inputs?

- **Concept: Boundary forcing vs. nested modeling in NWP**
  - Why needed here: The two conditioning strategies (boundary forcing, coarse global input) map directly to traditional limited-area modeling practices. Without this context, the architectural tradeoffs are opaque.
  - Quick check question: What is the operational difference between providing high-resolution boundary conditions vs. coarse global forecasts to a regional model?

- **Concept: Probabilistic calibration (CRPS, Spread/Skill Ratio)**
  - Why needed here: The benchmark supports diffusion-based probabilistic forecasting, but Appendix 10.3 shows systematic under-dispersion (SSR < 1). Understanding calibration metrics is essential to diagnose why ensemble spread doesn't match forecast error.
  - Quick check question: If SSR = 0.6 for temperature forecasts, what does this imply about the ensemble's uncertainty quantification?

## Architecture Onboarding

- **Component map:** Input pipeline (39-channel state at 256×256, 6-hourly timesteps) -> Boundary handling (10-pixel halo OR interpolated ERA5) -> Backbone (UNet, Stormer, GraphCast, Hi) -> Output head (increment prediction with latitude-weighted MSE) -> Probabilistic extension (EDM diffusion with DPM-Solver++)

- **Critical path:**
  1. Verify data integrity: Load HDF5 sample, confirm 39 channels, 256×256 spatial dims
  2. Implement latitude-weighted loss (Eq. 2) before training
  3. Start with UNet + boundary forcing (simplest baseline)
  4. Validate against persistence and climatology baselines (Figures 5-6)
  5. Only then experiment with GraphCast/Stormer architectures

- **Design tradeoffs:**
  - **UNet:** Simple, robust, competitive but not state-of-the-art; good for iteration speed
  - **Stormer:** Best under boundary forcing, fails under coarse conditioning due to patch tokenization mixing scales
  - **GraphCast:** Strong across both conditioning strategies; requires mesh construction overhead
  - **Hi:** Hierarchical extension of GraphCast; surprisingly underperforms GraphCast in this benchmark (hypothesis: overfitting or suboptimal hyperparameters)

- **Failure signatures:**
  - Stormer error grows rapidly with lead time under coarse conditioning (Fig 2): check tokenization alignment
  - Under-dispersive ensembles (SSR << 1 in Fig 10): diffusion model not capturing full uncertainty; consider EDA initialization or noise injection
  - Warm/cold bias in extreme events (Fig 4): model-specific systematic biases emerge under distribution shift

- **First 3 experiments:**
  1. **Baseline sanity check:** Train UNet with boundary forcing for 20 epochs; verify RMSE at 24h lead time is below climatology and persistence (use Figures 5-6 as reference)
  2. **Conditioning ablation:** Compare UNet performance under boundary forcing vs. coarse conditioning on the same 5 variables; expect comparable or slightly better with coarse conditioning (unlike Stormer)
  3. **Extreme event probe:** Evaluate trained model on the May 25–June 1, 2019 heatwave period; visualize spatial bias patterns and compare temporal evolution against Figure 4b

## Open Questions the Paper Calls Out
None

## Limitations

- Significant architectural sensitivity to conditioning strategy, with Stormer failing under coarse-resolution global inputs due to incompatible scale mixing
- Diffusion model's under-dispersive behavior (Spread/Skill Ratio < 1) indicates systematic calibration issues that limit probabilistic utility
- Limited hyperparameter tuning across architectures prevents definitive performance rankings

## Confidence

- **High confidence:** Boundary forcing improves accuracy for transformer architectures; residual increment prediction is more stable than absolute state prediction
- **Medium confidence:** GraphCast outperforms other architectures across conditioning strategies; coarse conditioning is operationally viable for UNet
- **Low confidence:** Diffusion model calibration results; Hi architecture underperformance relative to GraphCast; model-specific bias patterns during extreme events

## Next Checks

1. Implement adaptive boundary alignment (STCast approach) to test whether mixed-resolution tokenization is the root cause of Stormer's coarse conditioning failure
2. Conduct systematic hyperparameter sweeps for EDM diffusion model to identify optimal noise schedule and classifier-free guidance settings for calibration
3. Perform cross-validation with raw observational data (not reanalysis) to assess generalization gap and deployment readiness