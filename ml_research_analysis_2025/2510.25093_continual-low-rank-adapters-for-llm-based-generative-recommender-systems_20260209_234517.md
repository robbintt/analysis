---
ver: rpa2
title: Continual Low-Rank Adapters for LLM-based Generative Recommender Systems
arxiv_id: '2510.25093'
source_url: https://arxiv.org/abs/2510.25093
tags:
- lora
- continual
- recommendation
- user
- proximal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work tackles continual learning for LLM-based generative recommenders,
  where user preferences evolve over time. Existing LoRA-based methods focus on preserving
  past knowledge but neglect the dynamic nature of recommendation, where outdated
  preferences can hinder performance.
---

# Continual Low-Rank Adapters for LLM-based Generative Recommender Systems

## Quick Facts
- **arXiv ID**: 2510.25093
- **Source URL**: https://arxiv.org/abs/2510.25093
- **Reference count**: 31
- **Primary result**: Proposes PESO, a proximally regularized single evolving LoRA that achieves up to 8% improvement in NDCG@5 over baselines for continual LLM-based generative recommendation

## Executive Summary
This work tackles continual learning for LLM-based generative recommenders, where user preferences evolve over time. Existing LoRA-based methods focus on preserving past knowledge but neglect the dynamic nature of recommendation, where outdated preferences can hinder performance. To address this, we propose PESO, a proximally regularized single evolving LoRA that anchors each update to its most recent state, enabling adaptive balance between stability and plasticity. Theoretically, we show this yields data-aware, direction-wise guidance in the LoRA subspace. Empirically, PESO consistently outperforms both single evolving and cumulative LoRA baselines, achieving up to 8% improvement in NDCG@5 across multiple real-world datasets.

## Method Summary
PESO uses a single evolving LoRA adapter with proximal regularization to balance stability and plasticity in continual recommendation. The method maintains a single LoRA state v_t and adds a proximal penalty term anchoring it to v_{t-1}. The loss function is L_t = L_{ce}^{D_t} + (λ/2)Σ_g ||v_t^{(g)} - v_{t-1}^{(g)}||_{H_{t-1}^{(g)}}^2. The proximal metric is instantiated as per-module softmax-KL divergence, which provides module-structure-aware regularization. PESO is evaluated on Amazon Review datasets with temporal splits, using Llama-3.2-1B as backbone and RQ-VAE for item tokenization.

## Key Results
- PESO consistently outperforms single evolving LoRA baselines by up to 8% in NDCG@5
- Cumulative LoRA methods perform poorly in natural chronological splits due to entanglement of outdated preferences
- Per-module softmax-KL proximal regularizer provides better performance than uniform L2 proximal regularization
- Optimal λ values range from 2.0-5.0 depending on dataset characteristics

## Why This Works (Mechanism)

### Mechanism 1
- Claim: A proximal regularizer on a single evolving LoRA enables data-aware, direction-wise updates that balance stability (retaining useful long-term preferences) and plasticity (adapting to new preference shifts).
- Mechanism: The method maintains a single LoRA adapter v_t and adds a proximal penalty term that anchors v_t to its previous state v_{t-1}. The loss function is L_t = L_{ce}^{D_t} + (λ/2)Σ_g ||v_t^{(g)} - v_{t-1}^{(g)}||_{H_{t-1}^{(g)}}^2. Theoretical analysis shows that under a quadratic approximation of the data-fitting loss (with curvature Σ_t), the optimal update interpolates between the new optimum v_t^* and the previous state v_{t-1}. Along generalized eigenvectors q_k of (Σ_t, H_{t-1}), the component is ⟨v, q_k⟩_{H_{t-1}} = (ρ_k/(ρ_k + λ))⟨v_t^*, q_k⟩_{H_{t-1}} + (λ/(ρ_k + λ))⟨v_{t-1}, q_k⟩_{H_{t-1}}. If data support (curvature σ_k^2 or ρ_k) is strong, the update moves toward v_t^* (plasticity); if weak, it stays close to v_{t-1} (stability).
- Core assumption: The data-fitting loss L_{ce} can be approximated by a quadratic function L_{D_t}(v) ≈ (1/2)(v - v_t^*)^T Σ_t (v - v_t^*) near the optimum v_t^* within the LoRA subspace, where Σ_t is the tangent-feature second-moment matrix.
- Evidence anchors:
  - [abstract] "Theoretically, we show that this proximal design provides data-aware, direction-wise guidance in the LoRA subspace."
  - [section 4.1] "By Corollary 2, along any q_k the update is a weighted average of v_t^* and v_{t-1}, with weight toward v_t^* equal to σ_k^2/(σ_k^2 + λ). Thus, when σ_k^2 is large... v_t moves toward v_t^*... when σ_k^2 is small... v_t stays close to v_{t-1}."
  - [corpus] C-LoRA (arXiv:2502.17920) and FM-LoRA (arXiv:2504.08823) also explore continual LoRA, providing related baselines; however, this specific theoretical link between proximal regularization and data-aware directional interpolation via Σ_t is a distinct contribution of this work.
- Break condition: If the data distribution is not well-captured by the quadratic approximation (e.g., highly non-linear dynamics), or if the tangent features Φ(x) do not reflect the true gradient information, the direction-wise data-aware balance may degrade, leading to either excessive forgetting or rigidity.

### Mechanism 2
- Claim: A per-module softmax-KL proximal regularizer preserves the internal structure of LoRA parameters (e.g., within attention layers) and provides previous-state-aware stability without killing plasticity.
- Mechanism: Instead of a uniform L2 penalty (H=I), the proximal term is instantiated as a sum of softmax-KL divergences for each module (group of parameters) g: K_{blk}(v_t, v_{t-1}) = λ Σ_g D_KL(softmax(v_t^{(g)}) || softmax(v_{t-1}^{(g)})). This locally approximates a quadratic form with a block-diagonal metric H_{t-1}^{(g)} = diag(p^{(g)}) - p^{(g)}(p^{(g)})^T, where p^{(g)} = softmax(v_{t-1}^{(g)}). This is equivalent to a p-weighted variance of the parameter changes Δ^{(g)} = v_t^{(g)} - v_{t-1}^{(g)}. It penalizes reshuffling of weight mass within a module more than uniform shifts and protects coordinates with higher prior mass.
- Core assumption: Treating parameter groups (modules) as categorical distributions via softmax is a meaningful representation for regularization, and the parameter changes are small enough for the local quadratic approximation to hold.
- Evidence anchors:
  - [section 4.2] "Corollary 4 shows that, the softmax–KL proximal can be interpreted as a p-weighted variance of parameter changes... (i) penalizes reshuffling of weight mass within each module more than uniform shifts, and (ii) protects coordinates with higher prior mass more strongly."
  - [section 5.2, Figure 2] "L2 proximal... is often comparable to single evolving LoRA but worse than PESO, suggesting that uniform constraints are insufficient."
  - [corpus] Corpus evidence for this specific per-module softmax-KL instantiation in continual LoRA for recommendation is weak or missing; it appears to be a novel contribution of this paper.
- Break condition: If the parameter groups are not meaningfully structured (e.g., random partitioning), or if the optimal updates require large uniform shifts in parameter values, this structured regularization may overly constrain necessary plasticity.

### Mechanism 3
- Claim: Cumulative LoRA methods, which sum frozen past adapters with a new trainable one, are ill-suited for continual recommendation because they entangle outdated and relevant preferences, hindering adaptation to evolving user interests.
- Mechanism: Cumulative LoRA uses an effective update W_t = W_0 + Σ_{i=1}^{t-1} α_i B̂_i Â_i + B_t A_t. This expands capacity and enhances stability by reusing past adapters. However, in recommendation, tasks (stages) are not independent; users reappear with evolving preferences. Frozen adapters entangle useful long-term preferences with stale ones (e.g., a past preference for action movies that is now outdated). This entanglement makes it difficult for the model to adapt or "overwrite" outdated signals, leading to suboptimal performance compared to a single evolving adapter that can continuously modify its representation.
- Core assumption: User preferences evolve over time, and past preferences can become non-predictive or harmful (outdated) for current recommendations. Continual recommendation tasks involve significant cross-stage interference (non-independent tasks).
- Evidence anchors:
  - [section 3, Table 1] "...the original cumulative design (i.e., SumLoRAall) performs much worse in the natural chronological setting than in the user-disjoint setting, confirming that it is better suited for tasks with minimal interference and ill-suited for recommendation."
  - [section 3] "SumLoRAall performs worst, followed by latest, all+inherit, and latest+inherit, suggesting that (a) aggregating all past adapters hinders adaptation, and (b) parameter inheritance is essential for gradual, proximal evolution..."
  - [corpus] This analysis contrasts with methods like C-LoRA (arXiv:2502.17920) designed for vision tasks with more independent tasks.
- Break condition: If user preferences are perfectly static or if all past preferences remain eternally predictive (no preference drift), cumulative LoRA's stability-focused design might be beneficial.

## Foundational Learning

- Concept: **Stability-Plasticity Dilemma in Continual Learning**
  - Why needed here: The paper redefines these concepts for recommendation: *stability* is preserving predictive long-term preferences (not all past knowledge), and *plasticity* is overwriting outdated preferences and capturing new trends. The entire method is designed to balance this specific trade-off.
  - Quick check question: Can you explain why "stability" in continual recommendation differs from "stability" in class-incremental learning for vision (e.g., distinguishing cats from dogs)?

- Concept: **Low-Rank Adaptation (LoRA)**
  - Why needed here: LoRA is the underlying parameter-efficient fine-tuning technique. The proposed PESO method operates entirely within the LoRA subspace, and all analysis and mechanisms are built upon LoRA's low-rank update structure (ΔW = BA).
  - Quick check question: What are the trainable components in LoRA, and why does it make continual learning more feasible for LLMs compared to full fine-tuning?

- Concept: **Proximal Regularization**
  - Why needed here: The core of PESO is a proximal regularizer that anchors the current LoRA state to its previous state. Understanding this term (penalizing deviation) is essential to grasp how the method enforces its stability mechanism.
  - Quick check question: How does a proximal regularizer typically balance fitting new data versus staying close to a reference parameter state?

## Architecture Onboarding

- Component map:
  - Backbone LLM (frozen) -> Single LoRA Adapter (trainable) -> Proximal Regularizer -> Loss Function -> Model parameters

- Critical path:
  1. Train base model on D1 with LoRA. Save final LoRA state as v_1.
  2. At stage t, initialize new LoRA state: v_t ← v_{t-1}.
  3. Train on new data block D_t using combined loss L_t.
  4. After training, save v_t as the new reference for stage t+1.

- Design tradeoffs:
  - Single evolving LoRA vs. Cumulative LoRA: PESO chooses single evolving for flexibility and to avoid entanglement, trading off explicit capacity expansion for adaptive balance.
  - Proximal Metric (L2 vs. Softmax-KL): Softmax-KL provides module-structure-aware regularization but adds computational overhead compared to simple L2.
  - Regularization strength (λ): Tuning λ is crucial. Too small leads to forgetting; too large hinders adaptation.

- Failure signatures:
  - PESO performs worse than single evolving LoRA: Check if λ is too high (over-regularization) or if proximal metric is inappropriate.
  - PESO fails to adapt to new trends: Check if λ is too high or if the data block D_t is too small/weak to overcome the proximal penalty.
  - Catastrophic forgetting: Check if λ is too low or if the proximal term is not being computed correctly.

- First 3 experiments:
  1. Baseline Comparison: Implement single evolving LoRA and SumLoRAlatest+inherit on your chosen dataset and time split. Verify the paper's finding that cumulative methods struggle in natural chronological splits.
  2. Proximal Metric Ablation: Implement PESO with L2 proximal and with per-module softmax-KL proximal. Compare their performance to confirm the benefit of structure-aware regularization.
  3. Hyperparameter Sensitivity: Sweep λ (e.g., [0.5, 1.0, 2.0, 5.0, 8.0]) and plot performance (e.g., NDCG@5) to visualize the stability-plasticity trade-off and identify the optimal range for your setting.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How should item tokenizers (e.g., RQ-VAE codebooks) be adapted continually as new items appear, and how does tokenizer drift interact with LoRA-based model adaptation?
- Basis in paper: [explicit] The authors note in footnote 1: "Adapting the tokenizer to new items over time is an interesting direction; here we fix the item tokenizer to isolate continual adaptation of the model (LoRA)."
- Why unresolved: The paper deliberately freezes the tokenizer to isolate model adaptation, but real systems must handle vocabulary expansion. Tokenizer updates could destabilize the semantic ID space, affecting both current predictions and proximal regularization anchors.
- What evidence would resolve it: Experiments combining PESO with incremental tokenizer updates (e.g., expanding codebooks, fine-tuning RQ-VAE), measuring both recommendation accuracy and semantic consistency of item embeddings across stages.

### Open Question 2
- Question: Can LLM-based continual recommenders achieve relative gains comparable to traditional two-tower models (e.g., PISA) in capturing preference drift, and what architectural or algorithmic modifications are needed?
- Basis in paper: [explicit] The authors observe: "While PESO achieves higher absolute performance, continual methods like PISA yield larger relative gains in two-tower models...underscores the difficulty of capturing such dynamics in LLM-based methods, pointing to an important direction for future research."
- Why unresolved: Two-tower models use explicit user embeddings that directly encode preference shifts, while LLMs rely on implicit textual representations. The mechanism for capturing drift differs fundamentally, and it's unclear whether proximal regularization alone is sufficient.
- What evidence would resolve it: Systematic comparison of relative gains (continual vs. pretrain baseline) across both paradigms with controlled preference shift magnitudes; ablations isolating whether explicit user representations or optimization dynamics drive the gap.

### Open Question 3
- Question: Does relinearizing the tangent features at each stage (θ_0 + Uv_{t-1}) rather than fixing at θ_0 improve PESO's theoretical guarantees or empirical performance?
- Basis in paper: [explicit] In Appendix B.1, the authors state: "If desired, one may instead relinearize at θ_0 + Uv_{t-1}...We found fixed linearization sufficient and notationally lighter" but do not empirically validate this choice.
- Why unresolved: Fixed linearization assumes the local quadratic approximation remains valid as parameters drift further from θ_0 over many stages. Relinearization could provide more accurate curvature estimates (Σ_t) for the proximal interpolation, especially with large preference shifts.
- What evidence would resolve it: Empirical comparison of fixed vs. per-stage relinearization on datasets with varying drift magnitudes; theoretical analysis of approximation error bounds under both schemes.

### Open Question 4
- Question: How does PESO's stability–plasticity trade-off scale to larger LLM backbones (e.g., 7B+ parameters) and different adapter placement strategies (attention-only vs. full network)?
- Basis in paper: [inferred] The paper uses only Llama-3.2 1B and applies LoRA to attention layers, but does not test scaling behavior or alternative placement. Larger models may exhibit different forgetting dynamics due to greater parameter redundancy.
- Why unresolved: The data-aware interpolation in Corollary 2 depends on the LoRA subspace geometry (eigenvectors of Σ_t), which may change qualitatively with model scale or adapter scope.
- What evidence would resolve it: Experiments across model scales (1B, 3B, 7B) and adapter configurations (attention-only, MLP-inclusive, full), analyzing how eigenvalue distributions and optimal λ values shift with scale.

## Limitations

- The theoretical analysis relies on quadratic approximation of cross-entropy loss, which may not hold for highly non-linear recommendation dynamics
- The proposed softmax-KL proximal regularizer lacks strong empirical validation against other structured regularization approaches in continual LoRA literature
- The RQ-VAE semantic encoding procedure is described but not fully detailed, potentially affecting reproducibility
- Evaluation focuses on NDCG@5 and Hit@ metrics, leaving questions about performance on other recommendation-relevant metrics

## Confidence

- **High Confidence**: The core finding that cumulative LoRA methods perform poorly in natural chronological recommendation settings due to entanglement of outdated preferences; the general advantage of PESO over single evolving LoRA baselines in the reported experiments.
- **Medium Confidence**: The theoretical claims about data-aware directional guidance via proximal regularization; the specific superiority of the softmax-KL proximal over L2 proximal, given limited ablation studies.
- **Low Confidence**: The robustness of PESO to varying recommendation task characteristics (e.g., bursty vs. gradual preference changes); the generalizability of the hyperparameter settings (λ) across different recommendation domains.

## Next Checks

1. **Ablation on Proximal Metric**: Implement PESO with alternative structured proximal metrics (e.g., KL divergence on logits, entropy-based regularization) and compare performance against the softmax-KL baseline to validate the necessity of the specific module-structure-aware design.

2. **Data Block Size Sensitivity**: Vary the size of temporal data blocks D2-D5 (e.g., halving and doubling) and evaluate PESO's performance to understand how data availability affects the stability-plasticity trade-off and the validity of the quadratic approximation.

3. **Evaluation Metric Expansion**: Add recall-based metrics (e.g., Recall@20) and diversity metrics (e.g., coverage, intra-list distance) to the evaluation suite to assess whether PESO's advantages extend beyond ranking accuracy to other recommendation quality dimensions.