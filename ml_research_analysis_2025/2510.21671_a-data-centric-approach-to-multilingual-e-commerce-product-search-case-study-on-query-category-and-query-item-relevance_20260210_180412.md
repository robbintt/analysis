---
ver: rpa2
title: 'A Data-Centric Approach to Multilingual E-Commerce Product Search: Case Study
  on Query-Category and Query-Item Relevance'
arxiv_id: '2510.21671'
source_url: https://arxiv.org/abs/2510.21671
tags:
- multilingual
- relevance
- data
- e-commerce
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This work addresses data imbalance, label noise, and limited supervision
  in multilingual e-commerce search, focusing on Query-Category and Query-Item relevance
  tasks. The authors propose a model-agnostic, data-centric framework that improves
  performance through three strategies: translation-based augmentation to synthesize
  examples for low-resource languages, semantic negative sampling to generate hard
  negatives and reduce class imbalance, and self-validation filtering to remove likely
  mislabeled instances.'
---

# A Data-Centric Approach to Multilingual E-Commerce Product Search: Case Study on Query-Category and Query-Item Relevance

## Quick Facts
- arXiv ID: 2510.21671
- Source URL: https://arxiv.org/abs/2510.21671
- Authors: Yabo Yin; Yang Xi; Jialong Wang; Shanqi Wang; Jiateng Hu
- Reference count: 29
- Primary result: Translation-based augmentation, semantic negative sampling, and self-validation filtering improve F1 scores on CIKM AnalytiCup 2025 multilingual search tasks

## Executive Summary
This work addresses fundamental challenges in multilingual e-commerce search through a model-agnostic, data-centric framework that improves performance on Query-Category and Query-Item relevance tasks. The authors identify three critical data problems—language imbalance, class imbalance, and label noise—and propose targeted solutions: translation-based augmentation to synthesize training examples for low-resource languages, semantic negative sampling to generate hard negatives and reduce class imbalance, and self-validation filtering to remove likely mislabeled instances. Evaluated on the CIKM AnalytiCup 2025 dataset, the approach achieves consistent F1 score improvements over strong LLM baselines, with the best configurations reaching 0.8919 (QC) and 0.8839 (QI) F1 scores. These results demonstrate that systematic data engineering can be as impactful as complex model modifications, offering actionable guidance for building robust multilingual search systems.

## Method Summary
The framework addresses multilingual e-commerce search through three data-centric strategies. First, translation-based augmentation uses Seed-X-PPO-7B to translate high-resource language queries (e.g., Spanish) into zero-shot languages (German, Italian, Polish, Arabic), preserving category/item labels to enable cross-lingual knowledge transfer. Second, semantic negative sampling employs Qwen3-Embedding-4B to retrieve top-k semantically similar but incorrect categories/items, constructing hard negative examples that sharpen decision boundaries. Third, self-validation filtering trains an initial model on noisy data, flags samples where high-confidence predictions contradict original labels as potential annotation errors, and removes them before final training. The approach uses Qwen3-14B-Base with LoRA fine-tuning (rank=32), Alpaca-style prompts, and binary "yes"/"no" outputs, with task-specific thresholds (0.4 for QC, 0.2 for QI) on normalized token probabilities.

## Key Results
- Translation-based augmentation improves QC F1 from 0.8760 to 0.8834 and QI F1 from 0.8662 to 0.8738
- Semantic negative sampling further improves QC to 0.8885 F1 when combined with translation augmentation
- Self-validation filtering is critical for QI, achieving 0.8799 F1 when combined with translation augmentation
- Best configurations reach 0.8919 (QC) and 0.8839 (QI) F1 scores on the CIKM AnalytiCup 2025 dataset

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Translation-based augmentation enables cross-lingual knowledge transfer for languages absent from training data.
- Mechanism: High-resource language queries are translated to zero-shot languages using Seed-X-PPO-7B while preserving labels, creating supervised signal for unseen languages.
- Core assumption: Translation preserves semantic relevance between query and category/item sufficiently for training purposes.
- Evidence anchors: Abstract mentions translation-based augmentation for absent languages; Section 3.2.1 describes using ByteDance-Seed/Seed-X-PPO-7B; Section 4.2 shows QC improving from 0.8760 to 0.8834 and QI from 0.8662 to 0.8738 with translation.
- Break condition: If translation quality degrades on domain-specific e-commerce terminology, synthetic labels become noisy rather than helpful.

### Mechanism 2
- Claim: Semantic negative sampling creates harder training signal that sharpens decision boundaries for relevance discrimination.
- Mechanism: Instead of random negatives, retrieve semantically similar but incorrect categories/items using embedding similarity, then construct challenging negative examples.
- Core assumption: The embedding model captures task-relevant semantic similarity such that top-k neighbors are plausible but incorrect.
- Evidence anchors: Abstract mentions semantic negative sampling to generate hard negatives and mitigate class imbalance; Section 3.2.2 describes retrieving top-20 candidates and sampling hard negatives; Section 4.2 shows QC improving to 0.8885 F1 with semantic negative sampling plus translation.
- Break condition: If embedding space doesn't align with relevance judgments, sampled negatives may be inconsistent with true labels, creating conflicting gradients.

### Mechanism 3
- Claim: Self-validation filtering removes likely mislabeled instances, improving training data quality without manual re-annotation.
- Mechanism: Train initial model on noisy data, run inference on training set, flag samples where model confidence contradicts original label, and remove them.
- Core assumption: A reasonably-capable model's high-confidence predictions are more reliable than noisy human labels on ambiguous cases.
- Evidence anchors: Abstract mentions self-validation filtering to detect and remove likely mislabeled instances; Section 3.2.3 describes flagging high-confidence prediction contradictions; Section 4.2 shows QI improving to 0.8799 F1 with translation plus self-validation.
- Break condition: If the initial model has systematic biases, valid minority-class labels could be incorrectly filtered out, amplifying bias.

## Foundational Learning

- Concept: Binary classification via LLM prompt-response
  - Why needed here: Both QC and QI are formulated as yes/no generation tasks rather than regression or multi-class.
  - Quick check question: Can you explain why normalizing log-probabilities of "yes" and "no" tokens yields better-calibrated scores than using raw generation?

- Concept: LoRA (Low-Rank Adaptation) fine-tuning
  - Why needed here: The framework uses LoRA for parameter-efficient fine-tuning of a 14B model.
  - Quick check question: What happens to model expressiveness if LoRA rank is set too low for a complex classification task?

- Concept: Threshold calibration for imbalanced classification
  - Why needed here: Optimal thresholds differ drastically between tasks (0.4 for QC vs. 0.2 for QI).
  - Quick check question: Why would a lower threshold improve F1 on a task with more false negatives than false positives?

## Architecture Onboarding

- Component map: Seed-X translation model + Qwen3-Embedding for negative sampling -> Initial fine-tuned model as noise detector -> Qwen3-14B-Base with LoRA (rank=32) -> vLLM serving with normalized token probabilities and task-specific thresholds

- Critical path: 1) Audit language coverage to identify zero-shot languages, 2) Translate high-resource queries to target languages preserving labels, 3) Encode all categories/items, retrieve top-k semantically similar candidates, sample hard negatives, 4) Train initial model on augmented data, 5) Run self-validation pass, filter high-confidence disagreements, 6) Retrain final model on filtered data, 7) Calibrate thresholds per-task using dev set sweep

- Design tradeoffs: Translation model quality vs. latency and cost (better models reduce semantic drift but increase overhead); hard negative ratio vs. training stability (too many hard negatives can slow convergence or cause overfitting to negatives); filtering confidence threshold vs. data retention (aggressive filtering removes noise but risks discarding valid rare patterns); task-specific threshold vs. deployment simplicity (separate thresholds per task increase calibration effort)

- Failure signatures: Language-specific performance collapse if translation quality is poor for a specific language; negative sampling leakage if hard negatives occasionally include actually-relevant pairs; over-filtering if self-validation removes too many samples causing model to underfit; threshold mismatch using same threshold for both tasks causing systematic bias

- First 3 experiments: 1) Ablation on translation source language: Translate from English vs. Spanish vs. French; measure per-language F1 to identify if source language choice affects target language quality, 2) Hard negative ratio sweep: Train with 10%/30%/50% hard negatives; plot training loss curve and dev F1 to find saturation point where additional negatives cease to help, 3) Self-validation confidence threshold analysis: Flag samples at 0.7/0.8/0.9 confidence; manually inspect 50 random flagged samples to quantify true vs. false positive filtering rate before committing to automated removal

## Open Questions the Paper Calls Out

- Question: Can targeted augmentations (e.g., phonetic algorithms, realistic misspellings) significantly improve model resilience to slang and query variations in low-resource languages?
- Basis in paper: The authors identify "enhancing robustness to query variations" as a primary area for future work to address spelling errors.
- Why unresolved: The current framework focuses on semantic negative sampling and translation but does not explicitly handle the noisy nature of real-world user input like typos.
- What evidence would resolve it: Ablation studies showing F1 improvements on a corrupted evaluation set containing synthetic misspellings and slang compared to the standard translation-augmented baseline.

- Question: To what extent can external product knowledge graphs resolve ambiguities in complex queries where semantic text similarity is insufficient?
- Basis in paper: The conclusion suggests "integrating external knowledge sources like product knowledge graphs" as a promising direction for resolving query ambiguities.
- Why unresolved: The current data-centric approach relies solely on the text-based semantic understanding of the LLM, potentially missing structured relationships required for complex product attributes.
- What evidence would resolve it: Performance comparison on a subset of "ambiguous" or "complex" queries between the text-only model and a model augmented with knowledge graph embeddings.

- Question: Does label correction using strong external judges (e.g., majority voting) provide a performance advantage over the simple removal of noisy instances?
- Basis in paper: The authors mention the option to use multiple robust models for "majority-vote label correction" but chose simple removal for "cost efficiency."
- Why unresolved: Removing data reduces training set size, which might hurt low-resource language generalization; correcting it preserves data volume while potentially fixing noise.
- What evidence would resolve it: An ablation study comparing the F1 scores of models trained on "filtered" (removed) data versus "corrected" data using the same self-validation noise detection mechanism.

## Limitations
- The paper lacks direct ablation studies isolating each mechanism's contribution to overall performance improvements.
- There's no explicit analysis of how translation quality varies across languages or whether domain-specific terminology is preserved during translation.
- The self-validation filtering approach is not independently validated against alternative noise-handling methods through controlled experiments.

## Confidence
- **High confidence**: Task formulation as binary relevance classification, basic training pipeline with LoRA and vLLM inference, threshold calibration importance
- **Medium confidence**: Translation augmentation effectiveness (supported by results but lacking detailed analysis of translation quality or language-specific impacts)
- **Medium confidence**: Semantic negative sampling benefits (demonstrated improvement but no ablation showing impact vs. random negatives)
- **Low confidence**: Self-validation filtering methodology (described but not validated through controlled experiments or manual inspection of filtered samples)

## Next Checks
1. **Translation quality audit**: Manually inspect 100 translated queries per language to measure semantic preservation and identify systematic errors in domain-specific terminology handling
2. **Hard negative ablation**: Run controlled experiments comparing semantic vs. random negative sampling with identical data volumes to quantify the marginal benefit of embedding-based retrieval
3. **Self-validation validation**: Create a ground-truth set of 500 training samples with known label quality, then measure self-validation's precision/recall in detecting actual annotation errors versus false positives