---
ver: rpa2
title: 'MuLoCo: Muon is a practical inner optimizer for DiLoCo'
arxiv_id: '2505.23725'
source_url: https://arxiv.org/abs/2505.23725
tags:
- diloco
- muloco
- muon
- top-k
- compression
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the communication bottleneck in large-scale
  language model training by exploring compression strategies for DiLoCo, a distributed
  training framework that reduces communication frequency by taking multiple local
  steps before synchronizing. The key innovation is replacing AdamW with Muon as the
  inner optimizer and applying aggressive compression (2-bit quantization + error
  feedback) to the communicated deltas.
---

# MuLoCo: Muon is a practical inner optimizer for DiLoCo

## Quick Facts
- arXiv ID: 2505.23725
- Source URL: https://arxiv.org/abs/2505.23725
- Reference count: 40
- One-line primary result: MuLoCo achieves the same memory complexity as DiLoCo while communicating 8× less data by combining Muon inner optimizer with 2-bit quantization and error feedback

## Executive Summary
This paper addresses the communication bottleneck in large-scale language model training by exploring compression strategies for DiLoCo, a distributed training framework that reduces communication frequency by taking multiple local steps before synchronizing. The key innovation is replacing AdamW with Muon as the inner optimizer and applying aggressive compression (2-bit quantization + error feedback) to the communicated deltas. Experiments on a 220M parameter transformer language model show that MuLoCo significantly outperforms standard AdamW DiLoCo while communicating 8× less data, achieving the same memory complexity but with dramatically reduced communication overhead.

## Method Summary
MuLoCo modifies the DiLoCo framework by using Muon as the inner optimizer for hidden layers (with AdamW for embeddings/output), combined with 2-bit quantization and error feedback accumulators. Each worker performs H=30 local optimization steps using Muon, computes the parameter delta, and applies error feedback compression before all-reducing the compressed deltas. The outer optimizer (SGD+Nesterov) updates global parameters using these compressed deltas. This approach leverages Muon's orthogonalized momentum updates, which the authors hypothesize are inherently more compressible than AdamW's adaptive updates, especially when combined with error feedback to correct quantization errors.

## Key Results
- MuLoCo with 2-bit quantization + error feedback matches or exceeds 16-bit uncompressed DiLoCo baseline performance
- MuLoCo achieves 8× communication reduction compared to uncompressed DiLoCo while maintaining identical memory complexity
- Error feedback is essential: without it, 2-bit quantization causes significant performance degradation
- MuLoCo is more resilient to aggressive quantization than DiLoCo, suggesting Muon updates have better compressibility properties

## Why This Works (Mechanism)

### Mechanism 1
Muon's orthogonalized momentum updates produce deltas that are more compressible than AdamW's element-wise adaptive updates. Muon applies Newton-Schulz iteration to orthogonalize the momentum accumulator before scaling by learning rate, producing updates with different spectral properties than AdamW's coordinate-wise scaling. When these orthogonalized updates accumulate over H local steps, the resulting delta maintains structural properties that survive aggressive 2-bit quantization better. This is supported by Figure 2 showing MuLoCo at 2-bit + EF matches the 16-bit baseline while DiLoCo at 2-bit shows clear degradation.

### Mechanism 2
Error feedback accumulators persist information lost during compression, enabling convergence guarantees that would otherwise break under aggressive quantization. EF maintains an exponential moving average of the uncompressed delta, applies compression to this accumulator, and preserves the residual error for the next round. This error-corrected scheme ensures information lost to quantization is re-injected in subsequent steps. Figure 2 shows all data points using error feedback outperform their counterparts without error feedback across both Top-k sparsification and quantization.

### Mechanism 3
The communication reduction is multiplicative: DiLoCo reduces frequency by H×, and quantization reduces per-message size by 8× (16-bit to 2-bit). Standard data parallel training all-reduces gradients every step. DiLoCo communicates only every H local steps. MuLoCo adds 2-bit quantization, reducing each communication from 16 bits/parameter to 2 bits/parameter. Table 1 shows memory complexity comparison showing MuLoCo + EF requires 2× parameters (same as DiLoCo without EF).

## Foundational Learning

- **Concept:** DiLoCo / Local SGD
  - **Why needed here:** MuLoCo modifies DiLoCo's inner optimizer; understanding the two-loop structure (inner steps, outer sync) is prerequisite
  - **Quick check question:** Can you explain why DiLoCo's outer optimizer uses Nesterov momentum on the averaged delta rather than Adam-style adaptive updates?

- **Concept:** Muon optimizer and Newton-Schulz orthogonalization
  - **Why needed here:** The core hypothesis is that Muon's update structure enables compressibility; you need to understand what Muon actually computes
  - **Quick check question:** What is the relationship between Muon's orthogonalized momentum and the SVD of the raw momentum matrix?

- **Concept:** Error Feedback for compressed communication
  - **Why needed here:** EF is critical for 2-bit quantization to work; without it, performance collapses
  - **Quick check question:** Why does applying compression to the error accumulator rather than the raw delta preserve convergence properties?

## Architecture Onboarding

- **Component map:** [Worker k] → H local steps with InnerOpt (Muon) → Δ_k (parameter delta) → [EF Accumulator] → [Compressor C(·)] → 2-bit quantized Δ_k → [All-Reduce] → [OuterOpt: SGD+Nesterov] → Global parameter update

- **Critical path:**
  1. Each worker maintains local parameters θ_k and EF accumulator E_k
  2. Run H=30 inner optimization steps using Muon (hidden layers) + AdamW (embeddings/output)
  3. Compute delta Δ = θ_{t-H} - θ_t
  4. Update EF: E ← βE + Δ; compress: Δ_compressed = C(E); update residual: E ← E - Δ_compressed
  5. All-reduce compressed deltas across workers
  6. Outer optimizer (SGD + Nesterov, momentum=0.9, lr=0.8) updates global parameters

- **Design tradeoffs:**
  - **Memory:** MuLoCo + EF requires 2× parameters (one for model, one for EF accumulator), matching DiLoCo without EF; DiLoCo + EF would need 3×
  - **Compression level:** 2-bit works for MuLoCo+EF; 1-bit still degrades; 4-bit and 8-bit provide diminishing returns
  - **Local steps H:** Paper uses H=30; larger H increases frequency savings but may hurt convergence
  - **EF beta:** Tuned to 0.9 for MuLoCo (vs. 0.7 for DiLoCo)—higher retention of past errors benefits Muon

- **Failure signatures:**
  - **Without EF at 2-bit:** Final loss ~4.0 vs. ~3.7 with EF (see Figure 2, right panel)
  - **Random-k sparsification:** Rapid degradation even with EF (Figure 4, right panel)—structure matters
  - **DiLoCo at 2-bit + EF:** Still underperforms MuLoCo at 2-bit + EF (loss ~3.85 vs. ~3.70)

- **First 3 experiments:**
  1. **Reproduce Figure 2 (quantization panel):** Train 220M transformer with MuLoCo at 2/4/8-bit quantization + EF vs. DiLoCo baseline. Verify that 2-bit MuLoCo+EF matches 16-bit baseline loss (~3.70).
  2. **EF ablation:** Run MuLoCo with 2-bit quantization, with and without EF. Expect ~0.3 loss degradation without EF.
  3. **Scaling check:** Test with K=4 and K=16 workers to verify MuLoCo's advantage persists across worker counts (extrapolation beyond paper's K=8).

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Does MuLoCo maintain its advantages at larger model scales (e.g., 1B+ parameters)?
- **Basis in paper:** [explicit] The paper states "at the scales tested" when drawing conclusions, and only evaluates a 220M parameter transformer, which is relatively small compared to modern LLMs.
- **Why unresolved:** Compression dynamics and optimizer behavior may not scale linearly; DiLoCo itself has been shown to scale to larger models (cited works), but MuLoCo's combination of Muon with aggressive 2-bit compression is untested beyond 220M parameters.
- **What evidence would resolve it:** Pre-training experiments with MuLoCo on 1B+ parameter models, comparing final loss and communication savings against standard DiLoCo baselines.

### Open Question 2
- **Question:** What are the actual wall-clock time savings of MuLoCo in real distributed training deployments?
- **Basis in paper:** [explicit] The conclusion states "without accounting for wall-clock time and at the scales tested, Muon is the clear best-performing local optimizer."
- **Why unresolved:** While the paper demonstrates 8× reduction in bits communicated, actual training speedup depends on network topology, latency, compression/decompression overhead, and overlap with computation.
- **What evidence would resolve it:** Timing benchmarks on actual distributed hardware (multi-node GPU clusters) measuring total training time, communication time, and compression overhead.

### Open Question 3
- **Question:** Why are Muon's orthogonalized updates inherently more compressible than AdamW's updates?
- **Basis in paper:** [inferred] The paper hypothesizes that "Muon's orthogonalized updates are qualitatively different" and empirically observes that MuLoCo is "more resilient to aggressive quantization," but does not provide a theoretical explanation for this phenomenon.
- **Why unresolved:** Understanding the structural properties of Muon deltas that enable 2-bit quantization with minimal degradation could inform the design of future optimizers for communication-constrained settings.
- **What evidence would resolve it:** Analysis of the spectral properties, sparsity patterns, and distribution of delta values under Muon vs. AdamW; theoretical bounds on compression error propagation.

### Open Question 4
- **Question:** How does MuLoCo perform across different configurations of workers (K) and local steps (H)?
- **Basis in paper:** [inferred] All experiments use K=8 workers and H=30 local steps fixed, following the Charles et al. (2025) setup. The paper does not ablate these hyperparameters.
- **Why unresolved:** Different H/K ratios affect gradient staleness, communication frequency, and the magnitude of accumulated deltas—all factors that could interact with compression and optimizer choice.
- **What evidence would resolve it:** Systematic ablations varying K (e.g., 4, 16, 32) and H (e.g., 10, 50, 100) with both 2-bit and 4-bit compression, measuring convergence and stability.

## Limitations

- **Quantization specifics:** The paper does not fully specify the 2-bit quantization scheme (codebook generation, scaling method, offset handling), which could affect reproducibility and performance
- **Muon implementation details:** Exact Newton-Schulz iteration count and momentum scaling factors are not provided, leaving ambiguity in reproducing the orthogonalization mechanism
- **Scaling generalization:** Results are only shown for K=8 workers; performance at different worker counts remains untested
- **Theoretical gap:** While the paper hypothesizes that Muon's spectral properties enable compressibility, it provides no theoretical analysis of why orthogonalized updates are more compressible than AdamW's adaptive updates

## Confidence

- **High confidence:** MuLoCo with 2-bit quantization + EF outperforms DiLoCo with compression at the same bitrate (Figure 2)
- **Medium confidence:** Muon's orthogonalized updates are inherently more compressible than AdamW's updates (hypothesis supported by results but lacks theoretical proof)
- **Medium confidence:** Error feedback is essential for aggressive compression to work (clear ablation evidence, but no analysis of compression error distribution)
- **Low confidence:** The 8× communication reduction translates to wall-clock speedup (latency effects not measured)

## Next Checks

1. **Quantization ablation with systematic error analysis:** Run MuLoCo with 2-bit quantization with and without EF, then analyze the distribution of compression errors (mean, variance, bias) to verify the zero-mean error assumption underlying EF convergence guarantees.

2. **Spectral analysis of communicated deltas:** Compute and compare the singular value spectra of deltas from MuLoCo vs DiLoCo to empirically test the hypothesis that Muon produces more spectrally concentrated updates that survive 2-bit quantization better.

3. **Worker scaling experiment:** Test MuLoCo at K=4 and K=16 workers to verify the communication/computation tradeoff holds across scales, and measure actual training throughput (not just communication volume) to assess real-world efficiency gains.