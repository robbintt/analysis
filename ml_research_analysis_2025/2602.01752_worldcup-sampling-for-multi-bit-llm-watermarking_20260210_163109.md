---
ver: rpa2
title: WorldCup Sampling for Multi-bit LLM Watermarking
arxiv_id: '2602.01752'
source_url: https://arxiv.org/abs/2602.01752
tags:
- worldcup
- text
- watermarking
- message
- tokens
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: WorldCup is a multi-bit watermarking framework for large language
  models that directly embeds message bits into token selection using a hierarchical
  competition mechanism guided by complementary signals. It treats sampling as a communication
  channel rather than a detection probe, and incorporates entropy-aware modulation
  and confidence-aware decoding.
---

# WorldCup Sampling for Multi-bit LLM Watermarking

## Quick Facts
- arXiv ID: 2602.01752
- Source URL: https://arxiv.org/abs/2602.01752
- Reference count: 40
- Multi-bit watermarking framework that embeds message bits directly into token selection using hierarchical tournament sampling

## Executive Summary
WorldCup is a multi-bit watermarking framework for large language models that directly embeds message bits into token selection using a hierarchical competition mechanism guided by complementary signals. It treats sampling as a communication channel rather than a detection probe, and incorporates entropy-aware modulation and confidence-aware decoding. The method achieves strong performance across multiple metrics, outperforming prior baselines with average bit accuracy improvements of 2.7–21%, lower perplexity, higher AUC, and better robustness to real-world attacks.

## Method Summary
WorldCup uses tournament-based sampling to embed message bits directly into token selection. At each generation step, candidate tokens compete in a complete N-ary tournament tree, with the winner selected based on g-value functions that depend on the message bit being embedded. The framework uses complementary g-values for maximum discriminability, entropy-aware modulation to preserve text quality, and confidence-aware decoding. The vectorized formulation avoids explicit tournament sampling by modifying the probability distribution directly, making it computationally efficient.

## Key Results
- Achieves 97.5% bit accuracy and 8.0 PPL on LLaMA3-8B with 16-bit messages
- Outperforms BiMark, SegMark, and MPAC baselines with average bit accuracy improvements of 2.7–21%
- Maintains AUC > 0.9 even under Word-S-BERT substitution attacks
- Provides strong robustness to word-level perturbations while preserving text quality

## Why This Works (Mechanism)

### Mechanism 1: Hierarchical Tournament Sampling as Communication Channel
Treating token sampling as a communication channel enables direct bit embedding with higher per-token capacity. The tournament structure induces a biased distribution over tokens that preserves sufficient language model fidelity while creating statistically separable signals for different bit values.

### Mechanism 2: Complementary g-value Functions for Maximum Discriminability
Using complementary g-values (g_ℓ and ḡ_ℓ = 1 - g_ℓ) maximizes statistical discriminability between bit hypotheses by creating perfect anti-correlation: tokens favored under one hypothesis are disfavored under the other.

### Mechanism 3: Entropy-aware Modulation for Quality Preservation
Adaptively adjusting watermark strength based on local entropy preserves text quality while maintaining detectability. High-entropy positions permit stronger separation; low-entropy positions use conservative modulation to avoid distorting confident model choices.

## Foundational Learning

- **Concept: Autoregressive Token Sampling**
  - Why needed here: WorldCup modifies the sampling distribution at each step; understanding how LLMs generate tokens sequentially is prerequisite.
  - Quick check question: Can you explain how temperature, top-k, and top-p sampling modify the base probability distribution P_Θ(x_t | x_{<t})?

- **Concept: Hypothesis Testing and Z-scores**
  - Why needed here: Watermark detection uses z-scores to distinguish watermarked from unwatermarked text under a null hypothesis.
  - Quick check question: Given a detection statistic with E_H0[S] = 0 and Var_H0(S) = 1/(m|C_p|), what does a z-score of 3.0 imply about the likelihood of observing this under H_0?

- **Concept: Pseudo-random Functions (PRFs)**
  - Why needed here: The security and reproducibility of g-values depend on deterministic PRFs seeded by hash of context + key.
  - Quick check question: If two different watermark keys ξ₁ and ξ₂ are used on the same text, will the g-values be correlated or independent? Why?

## Architecture Onboarding

- **Component map:**
  [LLM Distribution P_Θ] → [Hash: (context, key ξ) → seed r_t] → [Message bits] → [Bit-to-position mapping via hash] → [g-value families G_0, G_1] → [Tournament sampling (m layers, N leaves)] → [Entropy computation H(P_Θ)] → [Modulation λ = α · σ(H)] → [Champion token emitted] → [Generated text]

  DECODING PATH:
  [Generated text] → [Recover position assignments] → [Compute g-values for each token] → [Aggregate by position: s^p_j, s̄^p_j] → [Compare s vs s̄ for each bit] → [Recover message symbols]

- **Critical path:** The tournament sampling algorithm (Algorithm 1) is the core encoding operation. For efficiency, use the vectorized formulation rather than explicit N^m samples. The decoding path requires matching the same hash function and key used during encoding.

- **Design tradeoffs:**
  - k=1 vs k=2: k=1 achieves lower PPL (better quality); k=2 achieves higher bit accuracy at cost of higher PPL
  - Distortionary vs non-distortionary g-values: Distortionary yields higher bit accuracy; non-distortionary preserves sampling distribution in expectation
  - Window size c: Larger windows reduce robustness to attacks; c=2 is recommended
  - Mean vs weighted-mean detector: Weighted-mean works better for k=1; mean detector works better for k=2

- **Failure signatures:**
  - Low bit accuracy with high AUC: Indicates hashing/position assignment issue, not signal strength
  - High PPL (>20): Entropy modulation may be disabled or α set too high
  - Robustness collapse under sentence-level attacks: Inherent to bit-allocation schemes; this remains an open challenge
  - Bit accuracy drops on Gemma2 vs LLaMA3: Expected due to lower entropy in Gemma2 outputs

- **First 3 experiments:**
  1. Reproduce the binary WorldCup (k=1) baseline on LLaMA3-8B with 16-bit messages, 128 max tokens. Verify bit accuracy ≈ 97.5% and PPL ≈ 8.0 match Table 1.
  2. Ablate complementary g-values: Replace with independently sampled g-values and measure the drop in bit accuracy. Expected: 2-3% degradation.
  3. Stress test robustness: Apply Word-S-BERT substitution at ratio=0.3 to 16-bit watermarked text. Measure AUC and bit accuracy degradation. Compare k=1 vs k=2 configurations.

## Open Questions the Paper Calls Out

### Open Question 1
How can multi-bit watermarking frameworks be adapted to maintain robustness against sentence-level semantic attacks, such as paraphrasing and back-translation? The authors explicitly state that improving robustness to such attacks remains an open challenge, noting that sentence-level transformations disrupt the contextual alignment required for bit recovery more severely than word-level edits.

### Open Question 2
How does the inevitable reduction in token-level entropy in larger, more capable models impact the effective capacity and detectability of WorldCup watermarks? The authors observe that Gemma2-9B yields lower decoding accuracy than LLaMA3-8B because its stronger generation capability produces lower entropy distributions, which inherently limits embedding capacity.

### Open Question 3
Can the generalized multi-bit formulation (k > 2) be optimized to avoid the linear increase in encoding latency observed in the current tournament sampling implementation? Section 5.2 notes that encoding time increases roughly linearly with the number of g-value function groups, increasing from 0.08s to 0.69s per token.

## Limitations

- Scalability and Complexity: The k=2 configuration requires computing two complementary distributions for each token, and the hierarchical tournament structure introduces computational overhead that may limit practical deployment at scale.
- Hash Function Reproducibility: The paper specifies a cryptographic hash function but doesn't provide implementation details, which could affect both reproducibility and security properties.
- Robustness Generalization: While demonstrating strong robustness against word-level perturbations, the paper acknowledges that sentence-level attacks (paraphrasing, translation) remain challenging.

## Confidence

**High Confidence** (multiple experimental validations):
- Binary watermarking performance (k=1): Bit accuracy of 97.5% and PPL of 8.0 on LLaMA3-8B
- Complementary g-value mechanism: Theoretical discriminability analysis and ablation results
- Quality-robustness tradeoff: The k=1 vs k=2 comparison clearly demonstrates the established relationship

**Medium Confidence** (strong theoretical basis but limited empirical scope):
- Entropy-aware modulation effectiveness: Supported by ablation but only tested on two model architectures
- Vectorized sampling efficiency: Theoretically sound but computational benchmarks are not provided
- Security claims: Based on PRF construction but no cryptanalysis or attack demonstrations

**Low Confidence** (sparse validation or acknowledged limitations):
- Sentence-level attack robustness: Explicitly identified as an open challenge with no experimental results
- Cross-model generalization: Only tested on LLaMA3-8B and Gemma2-9B; performance on other architectures is unknown
- Multilingual robustness: Acknowledged limitation with no experimental investigation

## Next Checks

**Validation Check 1: Hash Function Impact Study**
Implement WorldCup using three different hash functions (SHA-256, SHA-3, and a simpler hash) and measure the impact on bit accuracy and computational efficiency. This would validate whether the choice of hash function affects the claimed performance and security properties.

**Validation Check 2: Computational Complexity Benchmarking**
Measure wall-clock time and memory usage for WorldCup (both k=1 and k=2) versus baseline methods (BiMark, MirrorMark) on LLaMA3-8B across different sequence lengths (128, 256, 512 tokens). This would empirically validate the efficiency claims.

**Validation Check 3: Sentence-Level Attack Evaluation**
Apply paraphrase (using a standard paraphraser like ParaBank), translation (English→French→English), and summarization attacks to watermarked text and measure bit accuracy degradation. This would directly test the acknowledged limitation in sentence-level robustness.