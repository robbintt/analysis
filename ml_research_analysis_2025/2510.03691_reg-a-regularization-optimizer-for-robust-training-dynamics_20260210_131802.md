---
ver: rpa2
title: 'REG: A Regularization Optimizer for Robust Training Dynamics'
arxiv_id: '2510.03691'
source_url: https://arxiv.org/abs/2510.03691
tags:
- optimizer
- training
- matrix
- performance
- gradient
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces REG, a regularization optimizer for large
  language model training that addresses training instability issues found in existing
  structure-aware optimizers like Muon. The key innovation is replacing Muon's computationally
  complex matrix sign function with a simpler Row-and-Column-Scaling (RACS) operator
  that normalizes momentum matrices by their row and column norms.
---

# REG: A Regularization Optimizer for Robust Training Dynamics

## Quick Facts
- arXiv ID: 2510.03691
- Source URL: https://arxiv.org/abs/2510.03691
- Reference count: 40
- Primary result: Introduces REG optimizer replacing Muon's matrix sign function with simpler RACS operator, achieving competitive performance to AdamW with superior stability during LLM fine-tuning

## Executive Summary
This paper presents REG (Regularization Optimizer), a new optimizer for large language model training that addresses stability issues found in structure-aware optimizers like Muon. The key innovation is replacing Muon's computationally complex matrix sign function with a simpler Row-and-Column-Scaling (RACS) operator that normalizes momentum matrices by their row and column norms. Through matrix equilibration theory and experimental validation, REG demonstrates competitive performance to AdamW with superior stability, particularly during fine-tuning stages. The method shows consistent performance gains across mathematical reasoning, image classification, and mathematical optimization modeling tasks.

## Method Summary
REG is a regularization optimizer for neural network training that uses Row-and-Column-Scaling (RACS) to normalize momentum matrices. The core update involves computing momentum M_{k+1} = μM_k + (1-μ)∇f(W_k), then applying RACS normalization based on matrix dimensions (row-wise if m≤n, column-wise if m>n) using ℓ_p-norms. For p=2, a closed-form RMS rescaling to target ρ_target is applied. A hybrid approach is recommended: AdamW for embedding layers and REG for other parameters. The method uses only first momentum (vs AdamW's first+second), reducing memory overhead.

## Key Results
- REG achieves competitive performance to AdamW on GSM8K (49.8% vs 51.8%) and MATH500 (34.5% vs 35.2%) mathematical reasoning tasks
- Superior stability during fine-tuning, particularly when learning rates are reduced
- Consistent performance gains across domains: CIFAR-100 image classification and mathematical optimization modeling tasks
- Most effective for matrix-shaped parameters common in neural networks

## Why This Works (Mechanism)

### Mechanism 1: Row-and-Column Scaling as Gentler Spectral Regularization
Replacing Muon's matrix sign function with RACS produces more stable training dynamics while still addressing ill-conditioned momentum matrices. RACS redistributes energy gradually across directions rather than aggressively collapsing all singular values to 1, reducing dominance of principal directions without destabilizing rescaling.

### Mechanism 2: Consistent Update Magnitude via Closed-Form RMS Rescaling
After RACS normalization, the algorithm rescales the matrix so its root mean square equals a target ρ_target (e.g., 0.2-0.4). For p=2, a closed-form RMS(normal(M;2)) = √(1/max{m,n}) enables precise magnitude control without expensive computation.

### Mechanism 3: AdamW-Compatibility Through Hybrid Parameter Grouping
Using AdamW for embedding layers while applying REG to other parameters prevents numerical instability from matrix-based updates on sparse, high-dimensional embedding matrices. This hybrid approach preserves REG's benefits for dense matrix-shaped parameters while defaulting to proven AdamW dynamics where REG is poorly suited.

## Foundational Learning

- **Concept: Spectral condition number (κ = σ_max/σ_min)**
  - Why needed here: Understanding ill-conditioned momentum matrices (high κ) is essential to grasp what RACS is fixing
  - Quick check question: For a 100×50 matrix with σ_max=100 and σ_min=0.1, what is the condition number, and what does this imply about update distribution?

- **Concept: Momentum accumulation in gradient descent**
  - Why needed here: REG operates on the momentum matrix M_k, not raw gradients
  - Quick check question: With momentum coefficient μ=0.9, approximately what fraction of the update at step k comes from gradients observed more than 10 steps ago?

- **Concept: Matrix equilibration and diagonal scaling**
  - Why needed here: RACS is a form of matrix equilibration—scaling rows/columns via diagonal matrices D_1, D_2
  - Quick check question: If a 3×3 matrix has row norms [10, 1, 0.1], what diagonal matrix D would equalize row ℓ_2-norms?

## Architecture Onboarding

- Component map: Input ∇f(W_k), M_k, W_k → Momentum update → RACS normalization → RMS rescaling → Weight decay + update
- Critical path:
  1. Implement `normal(M, p)` correctly—handle zero rows/columns
  2. Compute RMS efficiently; for p=2 use closed-form, for other p compute numerically
  3. Determine which parameter groups use REG vs AdamW (embeddings → AdamW; attention/MLP weights → REG)

- Design tradeoffs:
  - **p=2 vs p=1 vs p=∞**: p=2 has closed-form RMS and best empirical results, but Van der Sluis theory supports p=1/∞
  - **Single vs iterative normalization**: Paper tests t=1 iteration; Algorithm 1 supports t>1 but yields marginal gains
  - **Memory**: REG uses only first momentum (vs AdamW's first + second), reducing state memory ~2×

- Failure signatures:
  - Training loss spikes or NaN: Check for zero rows in M before normalization
  - Fine-tuning degradation vs AdamW baseline: Verify hybrid mode is enabled for embeddings
  - Slow convergence: ρ_target may be too low; try 0.3-0.4 range

- First 3 experiments:
  1. **Sanity check on small model**: Train 2-layer MLP on MNIST with REG (p=2, ρ_target=0.3) vs AdamW
  2. **Ablation on p**: On GSM8K subset, compare p∈{1, 2, ∞}
  3. **Hybrid vs pure REG**: Fine-tune small LLM (150M params) with pure REG vs REG-with-AdamW

## Open Questions the Paper Calls Out

### Open Question 1: Pre-training Generalizability
Does REG maintain its performance advantages and stability during large-scale pre-training of LLMs? The generalizability of findings to the pre-training phase remains an open question for future research.

### Open Question 2: Convergence with Momentum
Can rigorous convergence guarantees be established for the full REG algorithm with non-zero momentum (μ≠0)? A full theoretical proof for this general case is beyond the scope of this paper.

### Open Question 3: Theory-Practice Gap for Norm Order
Why does p=2 empirically outperform the theoretically justified p=1 and p=∞? This discrepancy highlights a known gap between classical numerical linear algebra theory and the complex dynamics of deep learning optimization.

## Limitations
- Empirical foundation depth limited to primarily Qwen variants and single-task observations
- Hyperparameter sensitivity (ρ_target, p-norm choice) unclear across different model scales and tasks
- Theory-practice gap on p-norm choice not fully explained

## Confidence
- **High**: REG's implementation simplicity vs Muon, RMS rescaling closed-form for p=2, hybrid AdamW-for-embeddings design
- **Medium**: Stability improvements during fine-tuning, assertion that RACS is "less aggressive" than matrix sign function
- **Low**: Theoretical justification linking matrix equilibration to neural network training dynamics

## Next Checks
1. **Direct Muon comparison**: Implement both Muon and REG on same fine-tuning task, measuring training stability metrics beyond final accuracy
2. **p-norm theory validation**: Systematically test REG with p=1, 2, ∞ on diverse tasks to investigate why p=2 consistently wins
3. **Stress-test stability claims**: Train with poor initialization or aggressive learning rates to quantify REG vs AdamW failure rates across multiple seeds