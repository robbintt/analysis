---
ver: rpa2
title: Task-Adaptive Parameter-Efficient Fine-Tuning for Weather Foundation Models
arxiv_id: '2509.22020'
source_url: https://arxiv.org/abs/2509.22020
tags:
- weatherpeft
- weather
- peft
- rmse
- parameters
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Weather foundation models (WFMs) have shown great promise in handling
  diverse weather tasks but face challenges in efficient adaptation due to variable
  heterogeneity, resolution diversity, and spatiotemporal coverage variations. Existing
  parameter-efficient fine-tuning (PEFT) methods, designed for vision or language
  tasks, fail to address these unique challenges and perform suboptimally when applied
  to WFMs.
---

# Task-Adaptive Parameter-Efficient Fine-Tuning for Weather Foundation Models

## Quick Facts
- arXiv ID: 2509.22020
- Source URL: https://arxiv.org/abs/2509.22020
- Reference count: 40
- One-line primary result: WeatherPEFT achieves performance parity with full-tuning on weather tasks using ~3% of the parameters

## Executive Summary
Weather foundation models (WFMs) show promise for diverse weather tasks but face challenges in efficient adaptation due to variable heterogeneity, resolution diversity, and spatiotemporal coverage variations. Existing parameter-efficient fine-tuning (PEFT) methods designed for vision or language tasks perform suboptimally on WFMs. We introduce WeatherPEFT, a novel PEFT framework that addresses these challenges through two synergistic innovations: Task-Adaptive Dynamic Prompting (TADP) and Stochastic Fisher-Guided Adaptive Selection (SFAS).

WeatherPEFT achieves performance parity with full-tuning while using significantly fewer trainable parameters, and even outperforms full-tuning on regional precipitation forecasting. The framework dynamically injects task-specific soft prompts derived from encoder embeddings and selectively updates the most task-critical parameters using Fisher information, while introducing stochastic intervention to stabilize parameter selection during early training.

## Method Summary
WeatherPEFT is a parameter-efficient fine-tuning framework for weather foundation models that combines Task-Adaptive Dynamic Prompting (TADP) and Stochastic Fisher-Guided Adaptive Selection (SFAS). TADP extracts task-specific soft prompts from the encoder's embedding weights through three specialized adapters (HW-Adapter, V-Adapter, D-Adapter) that capture spatial, variable, and hidden patterns respectively. These soft prompts are concatenated to the backbone input tokens. SFAS uses Fisher information to identify and update the most task-critical parameters while introducing stochastic noise to stabilize selection. The framework is evaluated on three downstream tasks: downscaling (ERA5 data), ensemble forecast post-processing (ENS-10 benchmark), and regional precipitation forecasting (ERA5-CH).

## Key Results
- WeatherPEFT achieves performance parity with full-tuning on downscaling and post-processing tasks using only ~3% of parameters
- WeatherPEFT outperforms full-tuning on regional precipitation forecasting (SEEPS scores: 0.368 vs 0.304 for 12-hour forecasts)
- Ablation studies show both TADP and SFAS components are necessary, with performance drops when either is removed
- Optimal parameter selection percentage (k) is 0.03 (3%) for best performance-efficiency tradeoff

## Why This Works (Mechanism)

### Mechanism 1: Task-Adaptive Dynamic Prompting (TADP)
- **Claim:** Dynamic soft prompts derived from encoder embeddings condition the backbone on specific physical variables and resolutions, mitigating "variable heterogeneity" and "resolution diversity" inherent in weather tasks.
- **Mechanism:** TADP extracts "internal patterns" (data dimensions) via three specialized adapters (HW-Adapter, V-Adapter, D-Adapter) and "external patterns" (variable-spatial coupling) via self-attention applied to the encoder's weights. These patterns form soft prompt tokens concatenated to the backbone input.
- **Core assumption:** The encoder's embedding weights implicitly contain latent information about the specific task's physical variables and spatial characteristics sufficient for recalibrating the backbone.
- **Evidence anchors:**
  - [Abstract] "...TADP dynamically injects task-specific soft prompts... enabling context-aware feature recalibration for specific downstream tasks."
  - [Section 4.1] "Since the encoder's embedding layer captures the task-specific information... TADP extracts and integrates this information by transforming its weights into the input token space."
  - [Corpus] Weak direct evidence for encoder-weight-to-prompt extraction specifically; general PEFT literature (e.g., VPT) confirms prompting efficacy, but the extraction method is unique here.
- **Break condition:** Performance degrades to standard PEFT levels if the encoder weights are uniform across tasks (no specific info to extract) or if the adapters fail to model the variable coupling.

### Mechanism 2: Stochastic Fisher-Guided Adaptive Selection (SFAS)
- **Claim:** Utilizing Fisher information to selectively update parameters preserves pre-trained knowledge better than uniform updates, while stochastic intervention stabilizes selection against early-training noise.
- **Mechanism:** SFAS computes the diagonal Fisher information matrix to rank parameter sensitivity. It adds an annealed stochastic vector to these scores to mitigate noise, creating a binary mask ("Fish Mask") that allows gradient updates only for the top-k parameters.
- **Core assumption:** Standard gradient updates on all parameters distort the "invariant physical priors" of the foundation model, and early training noise leads to unstable parameter selection.
- **Evidence anchors:**
  - [Abstract] "SFAS... leverages Fisher information to identify and update the most task-critical parameters... [and] introduces randomness to stabilize the selection."
  - [Section 4.2] "Due to significant heterogeneity... substantial noise exists during early fine-tuning... we introduce an annealed stochastic component... to stabilize the training process."
  - [Corpus] Standard PEFT literature (e.g., BitFit, LoRA) supports selective updates, but the specific stochastic annealing for Fisher info lacks broad external validation.
- **Break condition:** If the task requires updating a dense set of parameters (low sparsity), or if the Fisher approximation is poor (e.g., small batch sizes), the mask will over-prune critical gradients, causing divergence.

### Mechanism 3: Synergistic Forward-Backward Separation
- **Claim:** Decoupling task context injection (Forward pass via TADP) from parameter optimization (Backward pass via SFAS) allows the model to handle diverse weather phenomena without interfering with the backbone's learned physics.
- **Mechanism:** TADP alters the *input representation* (tokens) to be task-aware, while SFAS alters the *optimization path* (gradient flow). This ensures the model sees the correct "view" of the data while only modifying weights sensitive to that specific view.
- **Core assumption:** Weather tasks are distinct enough that they require separate adaptation mechanisms for input features (resolution/variables) versus model behavior (physics).
- **Evidence anchors:**
  - [Section 1] "WeatherPEFT... comprising [TADP]... and [SFAS]... operating at distinct stages of the fine-tuning process."
  - [Table 8] Ablation shows performance drops when either component is removed, suggesting synergistic necessity.
  - [Corpus] Common in PEFT (e.g., combining Adapter/LoRA), but the specific pairing for "variable heterogeneity" is novel.
- **Break condition:** If TADP creates a prompt representation that is orthogonal to the parameters selected by SFAS (i.e., prompts guide one subspace, SFAS updates another), the signal propagates poorly.

## Foundational Learning

- **Concept:** **Fisher Information Matrix (FIM)**
  - **Why needed here:** SFAS relies on the FIM to approximate parameter importance. You must understand that FIM measures the curvature of the loss landscape; high Fisher scores imply a parameter significantly changes the output distribution.
  - **Quick check question:** If the loss landscape is flat, what happens to the Fisher values and the resulting Fish Mask? (Answer: Values drop, mask selects fewer or effectively random parameters).

- **Concept:** **Soft Prompting vs. Hard Prompting**
  - **Why needed here:** TADP uses "soft prompts" (continuous vectors) rather than text. These are optimized via backpropagation. Understanding how they are concatenated to the token sequence is vital for implementation.
  - **Quick check question:** Where are the TADP soft prompts inserted relative to the standard weather tokens in the Transformer backbone? (Answer: Concatenated).

- **Concept:** **Variable Heterogeneity in Weather Data**
  - **Why needed here:** Unlike NLP/CV, weather data involves distinct physical variables (temp, wind) at varying pressure levels. The V-Adapter and D-Adapter in TADP specifically target these dimensions.
  - **Quick check question:** Why would a standard NLP PEFT method (like LoRA) struggle with varying pressure levels? (Answer: NLP assumes a unified embedding space; LoRA applies uniform updates, ignoring physical coupling).

## Architecture Onboarding

- **Component map:** Input -> Encoder (Frozen) -> TADP Module -> Backbone (Frozen) -> Output
  - TADP Module: Adapters (HW, V, D) -> Self-Attention -> MLP -> Soft Prompts -> Concatenated to Input Tokens
  - SFAS Module: Fisher Info Calculation -> Stochastic Intervention -> Binary Mask -> Parameter Selection
  - Backprop: Gradient updates flow *only* through parameters selected by SFAS mask

- **Critical path:** The extraction of embedding weights E -> Adapter transformation -> Prompt injection -> Forward Pass -> Loss Calculation -> Fisher Approximation -> Mask Generation -> Weight Update

- **Design tradeoffs:**
  - **Prompt Length (P):** Increasing P increases trainable params (TADP) and context, but Table 4 shows diminishing returns (optimal ~20).
  - **Selection Percentage (k):** Table 4 shows k=0.03 (3%) matches Full-Tuning, while k=0.001 (0.1%) lags. Higher k = better performance but less efficiency.
  - **Stochasticity (γ):** High γ (noise) stabilizes early training but might hinder convergence if decayed too slowly.

- **Failure signatures:**
  - **Over-smoothing:** If TADP adapters are too deep or P is too large, predictions may lose spatial texture.
  - **Catastrophic Forgetting:** If SFAS selects too many parameters (high k), the pre-trained physics is corrupted.
  - **Training Collapse:** If the "Stochastic Intervention" dominates the Fisher information (high γ), parameter selection becomes random, failing to find the optimal sub-network.

- **First 3 experiments:**
  1. **SFAS Ablation:** Run WeatherPEFT with k=0.001 vs. k=0.030 on the Downscaling task to verify the performance/efficiency cliff (Table 5).
  2. **Prompt Validity Check:** Run Regional Precipitation Forecasting with P=20 (optimal) vs. P=100 (redundant) to confirm the overfitting hypothesis suggested in Appendix B.1.
  3. **Backbone Generalization:** Apply WeatherPEFT to the Prithvi-WxC backbone (Table 7) to ensure TADP/SFAS logic holds on non-Swin architectures (e.g., standard Transformers).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the WeatherPEFT framework be effectively adapted to non-Transformer foundation models, such as Convolutional Neural Networks (CNNs) or Graph Neural Networks (GNNs)?
- Basis in paper: [explicit] The "Discussion" section states: "Future work should prioritize its extension to other foundational architectures, such as Convolutional Neural Networks and Graph Neural Networks."
- Why unresolved: The current Task-Adaptive Dynamic Prompting (TADP) module is designed to inject soft prompts into the input tokens of a Transformer backbone; adapting this to the feature maps of CNNs or node embeddings of GNNs requires architectural modifications not explored in the text.
- What evidence would resolve it: A modified implementation of WeatherPEFT applied to a CNN-based (e.g., FourCastNet) or Graph-based (e.g., GraphCast) backbone that demonstrates performance parity with full-tuning on the same downstream tasks.

### Open Question 2
- Question: Does explicitly incorporating physical constraints (e.g., conservation laws) into the WeatherPEFT fine-tuning process improve forecast accuracy or physical consistency?
- Basis in paper: [explicit] The "Discussion" section notes: "Future research could investigate domain-specific PEFT methods... such as integrating physical mechanisms into the fine-tuning process (e.g., embedding conservation laws or dynamical constraints)."
- Why unresolved: The current framework relies entirely on data-driven adaptation via prompting and Fisher information, without explicitly enforcing atmospheric physics during the parameter updates.
- What evidence would resolve it: A comparative study where a physics-informed variant of WeatherPEFT is evaluated against the standard version, showing improved adherence to physical laws or reduced error metrics on complex dynamical tasks.

### Open Question 3
- Question: Can the parameter selection percentage (k) in Stochastic Fisher-Guided Adaptive Selection (SFAS) be determined adaptively during training rather than set as a fixed hyperparameter?
- Basis in paper: [inferred] Appendix B.1 demonstrates that performance varies significantly with different fixed values of k (e.g., 0.001 vs. 0.04), necessitating a sweep to find the optimal balance between efficiency and accuracy.
- Why unresolved: The paper establishes a correlation between k and performance but treats k as a static pre-defined setting, leaving the potential for dynamic, epoch-based adjustment unexplored.
- What evidence would resolve it: The development of a scheduling algorithm for k that automatically adjusts the sparsity of the Fish Mask based on training dynamics, achieving optimal performance without manual hyperparameter search.

## Limitations
- **Dataset representativeness:** Validation is limited to ERA5 and ENS-10 datasets with specific geographic and temporal ranges, leaving generalizability to other weather datasets, regions, or extreme events unverified.
- **Hyperparameter sensitivity:** Performance depends heavily on specific hyperparameters including prompt length, selection percentage, and stochastic decay factor, with no systematic exploration of the sensitivity landscape.
- **Fisher approximation quality:** Diagonal Fisher information approximation may not capture complex parameter interactions in high-dimensional weather models, and stochastic intervention could introduce additional variance affecting convergence stability.

## Confidence
**High confidence:** The core claim that existing PEFT methods underperform on weather tasks due to variable heterogeneity and resolution diversity is well-supported by the literature review and ablation studies.

**Medium confidence:** The synergistic mechanism claim that TADP and SFAS operate at distinct stages to handle diverse weather phenomena is plausible but requires more rigorous ablation testing.

**Low confidence:** The claim that WeatherPEFT outperforms full-tuning on regional precipitation forecasting is based on limited metrics and may not generalize to other evaluation criteria or longer forecast horizons.

## Next Checks
1. **Architecture generalization test:** Apply WeatherPEFT to the Prithvi-WxC backbone (standard Transformer) to verify that TADP/SFAS logic holds beyond Swin architectures.

2. **Extreme weather robustness check:** Evaluate WeatherPEFT on extreme weather events (hurricanes, heat waves) to assess whether the claimed preservation of invariant physical priors extends to rare but critical phenomena.

3. **Parameter sensitivity analysis:** Conduct systematic hyperparameter sweeps across prompt lengths (P=5-100), selection percentages (k=0.0001-0.1), and stochastic decay rates (γ=0.1-0.5) to map the performance landscape and identify potential failure modes.