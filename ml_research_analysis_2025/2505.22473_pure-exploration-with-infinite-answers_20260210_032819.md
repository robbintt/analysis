---
ver: rpa2
title: Pure Exploration with Infinite Answers
arxiv_id: '2505.22473'
source_url: https://arxiv.org/abs/2505.22473
tags:
- lemma
- such
- then
- exists
- proof
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies pure exploration problems where the set of correct
  answers is infinite, such as regressing continuous functions of bandit means. Existing
  methods like Sticky Track-and-Stop fail in this setting due to fundamental topological
  challenges when multiple correct answers exist.
---

# Pure Exploration with Infinite Answers

## Quick Facts
- arXiv ID: 2505.22473
- Source URL: https://arxiv.org/abs/2505.22473
- Authors: Riccardo Poiani; Martino Bernasconi; Andrea Celli
- Reference count: 40
- Primary result: Introduces Sticky-Sequence Track-and-Stop algorithm for pure exploration with infinite answer spaces, achieving asymptotic optimality by tracking convergent answer sequences

## Executive Summary
This paper addresses pure exploration in multi-armed bandit problems where the set of correct answers is infinite, such as regressing continuous functions of bandit means. Traditional methods like Sticky Track-and-Stop fail in this setting due to fundamental topological challenges when multiple correct answers exist. The authors introduce a new framework called Sticky-Sequence Track-and-Stop that achieves asymptotic optimality by tracking a sequence of answers converging to some correct answer, rather than sticking to a single one. The key innovation is that convergence of the answer sequence, not identification of a specific correct answer, is sufficient for optimality.

## Method Summary
The paper proposes Sticky-Sequence Track-and-Stop for pure exploration with infinite answer spaces. The algorithm operates by maintaining a confidence region C_t using generalized likelihood ratio tests, defining a candidate set X_t of all possible correct answers within C_t, and selecting answers through convergent selection rules that ensure the answer sequence converges. The sampling strategy uses C-Tracking with oracle weights computed via a sup-inf optimization problem. The stopping rule compares a divergence measure to a threshold that grows logarithmically. The key theoretical contribution shows that asymptotic optimality (E[τ_δ]/log(1/δ) → T*(μ)) can be achieved even when the answer sequence doesn't converge to a specific correct answer, as long as it converges.

## Key Results
- Proves asymptotic optimality of Sticky-Sequence Track-and-Stop for infinite answer spaces
- Shows convergence of the answer sequence suffices for optimality, not identification of specific correct answers
- Provides convergence guarantees for various answer space topologies (discrete, continuous, finite sets)
- Demonstrates how to implement convergent selection rules in practice for different topological settings

## Why This Works (Mechanism)
The paper leverages the topological structure of the answer space to overcome limitations of existing pure exploration methods. When X*(μ) contains infinitely many points, traditional "sticking" strategies fail because they cannot identify a unique correct answer. Instead, the algorithm exploits the fact that any convergent sequence of answers approaching X*(μ) provides sufficient information for asymptotically optimal sample complexity. The C-Tracking sampling rule ensures efficient allocation of samples based on current beliefs about the parameter space, while the generalized likelihood ratio confidence regions maintain statistical validity.

## Foundational Learning
- Canonical exponential families: Required for computing KL divergences and likelihood ratio tests; quick check: verify d(μ,λ) is strictly convex and satisfies information-theoretic properties
- C-Tracking sampling: Optimal allocation based on current parameter estimates; quick check: confirm weights sum to 1 and force exploration maintains positive arm counts
- Generalized likelihood ratio confidence regions: Statistical machinery for maintaining valid uncertainty sets; quick check: verify C_t contains true μ with high probability
- Sup-inf optimization for oracle weights: Finding optimal sampling distribution given answer constraints; quick check: ensure numerical solver converges and respects simplex constraints
- Topological convergence of answer sequences: New optimality criterion replacing exact identification; quick check: plot ||x_t - x_{t-1}|| to verify convergence

## Architecture Onboarding

**Component Map**: Canonical family infrastructure -> KL divergence calculator -> Oracle weight optimizer -> C-Tracking sampler -> Convergent selector -> Stopping rule -> Final answer

**Critical Path**: 
1. Compute confidence region C_t via likelihood ratio test
2. Generate candidate set X_t from C_t
3. Select convergent x_t from X_t
4. Compute oracle weights ω*(μ, ¬x_t) via sup-inf optimization
5. Sample arms using C-Tracking with computed weights
6. Check stopping condition; if met, return x̂, else repeat

**Design Tradeoffs**: 
- Arbitrary vs convergent selection rules: arbitrary rules are simpler but may not converge, leading to suboptimality
- Adaptive vs fixed discretization: adaptive methods handle continuous spaces better but add computational overhead
- Conservative vs aggressive stopping: conservative thresholds ensure correctness but may increase sample complexity

**Failure Signatures**:
- Non-convergent answer sequences (oscillating x_t values) indicate improper selection rule
- Stopping rule never triggers suggests insufficient confidence growth or conservative threshold
- High false positive rate indicates confidence regions are too small or stopping threshold too aggressive

**First Experiments**:
1. Test convergent selection rule on 1D regression with Gaussian bandits, verifying ||x_t - x_{t-1}|| → 0
2. Compare C-Tracking vs uniform sampling on continuous manifold answer space
3. Validate stopping rule false positive rate on synthetic problems with known X*(μ)

## Open Questions the Paper Calls Out
None specified in the provided content.

## Limitations
- Computational complexity of finding oracle weights via sup-inf optimization is not analyzed
- Numerical stability issues may arise when computing generalized likelihood ratio confidence regions
- The paper doesn't address scalability to very high-dimensional answer spaces or large action sets

## Confidence
- Theoretical framework and asymptotic optimality: High confidence (rigorous mathematical treatment)
- Practical implementation details: Medium confidence (several algorithmic choices left unspecified)
- Computational efficiency and scalability: Low confidence (runtime complexity not addressed)

## Next Checks
1. Implement the convergent selection rule for R^d using Algorithm 2 with adaptive discretization, and verify that ||x_t - x_{t-1}|| converges to zero in simulated Gaussian bandit problems
2. Benchmark the C-Tracking sampling rule against standard Track-and-Stop on a regression problem where X*(μ) forms a continuous manifold, measuring both δ-correctness and expected stopping time
3. Test the stopping rule's false positive rate by running experiments with δ=0.01 and verifying that P(ĉx ∉ X*(μ)) ≤ 0.01 across multiple problem instances