---
ver: rpa2
title: Enhancing Aspect-based Sentiment Analysis with ParsBERT in Persian Language
arxiv_id: '2502.01091'
source_url: https://arxiv.org/abs/2502.01091
tags:
- sentiment
- language
- persian
- data
- bert
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study enhances aspect-based sentiment analysis for Persian
  text using the ParsBERT model augmented with a domain-specific lexicon. It addresses
  challenges of limited Persian datasets and suboptimal language models by enriching
  aspect terms with synonyms from the Dehkhoda and Moin dictionaries.
---

# Enhancing Aspect-based Sentiment Analysis with ParsBERT in Persian Language

## Quick Facts
- arXiv ID: 2502.01091
- Source URL: https://arxiv.org/abs/2502.01091
- Reference count: 40
- Primary result: Achieved 88.2% accuracy and 61.7% F1 score on ParsiNLU dataset

## Executive Summary
This study enhances aspect-based sentiment analysis for Persian text using the ParsBERT model augmented with a domain-specific lexicon. It addresses challenges of limited Persian datasets and suboptimal language models by enriching aspect terms with synonyms from the Dehkhoda and Moin dictionaries. The approach achieves 88.2% accuracy and 61.7% F1 score on the ParsiNLU dataset, significantly outperforming baseline models. This improvement demonstrates the effectiveness of integrating semantic tools into NLP models for Persian, enabling more nuanced sentiment detection and advancing the field of Persian text mining.

## Method Summary
The method uses ParsBERT (bert-fa-base-uncased-clf-digimag), a Persian-specific BERT model pre-trained on Persian news articles. Aspect terms are enriched with synonyms from Dehkhoda and Moin dictionaries (19,899 terms total) before tokenization. The model takes sentence-pair inputs: review text as the first sentence and the enriched aspect phrase as the second sentence. Classification uses AdamW optimizer (β1=0.9, β2=0.98), learning rate 0.0001, batch size 32, and max sequence length 512. WordPiece tokenization is applied after lexicon enrichment.

## Key Results
- Achieved 88.2% accuracy on ParsiNLU dataset
- Achieved 61.7% F1 score with 6.1% standard deviation
- Outperformed multilingual BERT baseline (53.9% accuracy)

## Why This Works (Mechanism)

### Mechanism 1
Enriching aspect terms with synonyms from domain-specific lexicons improves sentiment classification accuracy by expanding semantic coverage. The lexicon adds synonymous expressions to aspect terms before tokenization, allowing the model to associate sentiment with the correct aspect even when synonyms rather than exact aspect words are used.

### Mechanism 2
Structuring ABSA as a sentence-pair classification task improves BERT's ability to focus sentiment prediction on specific aspects. The review text serves as the first sentence, and the aspect phrase (with enriched synonyms) serves as the auxiliary second sentence, separated by [SEP] tokens. BERT's segment embeddings and self-attention learn to weight tokens in the review relative to the target aspect.

### Mechanism 3
Using a Persian-specific pre-trained BERT (ParsBERT) rather than multilingual BERT improves performance due to better language-specific representations. ParsBERT was pre-trained on Persian news articles, learning Persian morphology, syntax, and common expressions that multilingual models dilute across languages.

## Foundational Learning

- **Concept**: Aspect-Based Sentiment Analysis (ABSA)
  - **Why needed here**: The core task is fine-grained sentiment toward specific product aspects (e.g., "taste" vs. "packaging")
  - **Quick check question**: Given "The taste was great but packaging was terrible," what are the two aspect-sentiment pairs?

- **Concept**: Sub-word Tokenization (WordPiece)
  - **Why needed here**: Persian's complex morphology means words often combine roots and affixes; WordPiece breaks unknown words into recognizable sub-units
  - **Quick check question**: How would WordPiece tokenize a Persian compound word not in the vocabulary?

- **Concept**: Class Imbalance in Evaluation
  - **Why needed here**: 69.5% of labels are "no comment for aspect"; the F1 score (61.7%) is more meaningful than accuracy (88.2%)
  - **Quick check question**: Why does high accuracy with low F1 suggest class imbalance issues?

## Architecture Onboarding

- **Component map**: Tokenizer (WordPiece) -> Lexicon Enrichment Module -> Embedding Layer -> Transformer Encoder -> Classification Head
- **Critical path**: Aspect synonym lookup -> sentence-pair construction -> tokenization -> BERT encoding -> classification
- **Design tradeoffs**: Lexicon enrichment adds semantic coverage but risks synonym ambiguity; sentence-pair input improves aspect focus but doubles token consumption; ParsBERT base (110M params) chosen over BERT-large for resource efficiency
- **Failure signatures**: Sudden F1 drop on reviews with informal Persian slang -> lexicon gap; high accuracy but low F1 -> model defaulting to "no comment" majority class; truncation warnings during tokenization -> review exceeding 512 tokens
- **First 3 experiments**: 
  1. Ablate lexicon: Run ParsBERT without synonym enrichment; expect accuracy drop from 88.2% toward baseline ~53-58%
  2. Replace sentence-pair with single-sentence: Provide only review text with aspect appended; measure F1 degradation
  3. Test domain transfer: Evaluate on non-food Persian reviews (e.g., electronics) to assess generalization beyond the training domain

## Open Questions the Paper Calls Out
- Examining domain-specific adaptations for different industries and domains with unique aspects and sentiment expressions
- Exploring additional semantic techniques such as word embedding or suitable text representations
- Handling class imbalance through techniques like oversampling or class-weighted loss functions

## Limitations
- Data representativeness limited to 500 reviews from Digikala's food category
- Lexicon integration details unclear regarding synonym selection and polysemy handling
- Reproducibility gaps in train/validation/test split strategy and training procedure details

## Confidence
- **High Confidence**: ParsBERT outperforming mBERT (53.9% vs 88.2% accuracy) is well-supported by direct comparison
- **Medium Confidence**: Sentence-pair classification approach effectiveness is plausible but performance gain isn't isolated
- **Low Confidence**: Exact contribution of lexicon enrichment to 88.2% accuracy is uncertain due to lack of ablation studies

## Next Checks
1. Run ablation study on lexicon to measure isolated contribution by comparing accuracy with and without synonym enrichment
2. Evaluate model on Persian text from non-food domains (news articles, social media, electronics reviews) to test generalization
3. Examine precision, recall, and F1 for each sentiment class, particularly minority classes (neutral at 0.3%, mixed at 1.8%) to identify class imbalance issues