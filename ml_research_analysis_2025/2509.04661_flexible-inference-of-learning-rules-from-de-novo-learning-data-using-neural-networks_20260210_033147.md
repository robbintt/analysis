---
ver: rpa2
title: Flexible inference of learning rules from de novo learning data using neural
  networks
arxiv_id: '2509.04661'
source_url: https://arxiv.org/abs/2509.04661
tags:
- learning
- stimulus
- data
- weight
- neural
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a flexible framework to infer learning rules
  from de novo behavioral data, using deep neural networks to model per-trial weight
  updates in a decision policy. The approach bypasses strong parametric assumptions
  of classic learning rules and captures non-Markovian dynamics via a recurrent variant.
---

# Flexible inference of learning rules from de novo learning data using neural networks

## Quick Facts
- arXiv ID: 2509.04661
- Source URL: https://arxiv.org/abs/2509.04661
- Reference count: 40
- Primary result: DNN and RNN models infer learning rules from behavioral data, outperforming classic RL methods on held-out prediction

## Executive Summary
This paper introduces a flexible framework to infer learning rules from de novo behavioral data using deep neural networks. The approach models per-trial weight updates in a decision policy, bypassing strong parametric assumptions of classic learning rules and capturing non-Markovian dynamics via a recurrent variant. Applied to mouse learning data from the IBL, both DNN and RNN variants outperform traditional RL methods in held-out prediction, revealing asymmetric updates after correct/incorrect trials and multi-trial reward history dependencies.

## Method Summary
The method parameterizes per-trial weight updates with a deep neural network (DNN or RNN) that maps current-trial inputs (stimulus, choice, reward, current weight) to weight updates. The DNN/RNN is trained by backpropagating through the policy to maximize log-likelihood of observed choices. The RNN variant uses a GRU to maintain hidden state across trials, enabling history-dependent updates. The framework is validated on simulated data with known ground-truth rules and applied to IBL mouse decision-making data, with performance evaluated via held-out log-likelihood and cross-validation.

## Key Results
- DNNGLM accurately recovers ground-truth learning rules in simulations (R²=0.988)
- RNNGLM captures non-Markovian dynamics and reward history dependence missed by DNN
- Both variants outperform REINFORCE baseline on IBL held-out prediction (significant improvement)
- Pooling across animals enables inference and improves held-out prediction quality

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Neural networks can approximate the learning rule function that maps trial variables to weight updates.
- Mechanism: A feedforward DNN takes current-trial inputs (stimulus, choice, reward, current weight) and outputs the weight update Δw. The DNN is trained by backpropagating through the policy to maximize log-likelihood of observed choices.
- Core assumption: The true learning rule is a well-behaved function that can be approximated by neural networks (universal approximation).
- Evidence anchors:
  - [abstract]: "parameterizes the per-trial update of policy weights with a deep neural network"
  - [section 3]: "Simulations demonstrate that the framework can recover ground-truth learning rules" (Fig. 2 shows R²=0.988)
  - [corpus]: Weak direct evidence; corpus papers address related but distinct problems (inverse RL, behavioral modeling)
- Break condition: If the true learning rule is highly discontinuous, stochastic, or depends on unobserved variables not included in the input, the DNN will fail to recover it.

### Mechanism 2
- Claim: RNNs can capture non-Markovian learning dynamics where weight updates depend on trial history.
- Mechanism: A GRU maintains hidden state h_t that integrates information across trials. The hidden state is passed to the DNN to compute Δw_t, allowing updates to depend on arbitrary history lengths.
- Core assumption: History dependence is systematic and recoverable from observable trial sequences.
- Evidence anchors:
  - [abstract]: "captures non-Markovian dynamics via a recurrent variant"
  - [section 3/Fig. 3]: "RNNGLM recovered the reward history dependence present in the ground truth... DNNGLM fails to recover the correct learning dynamics"
  - [corpus]: No direct corpus validation for this specific RNN architecture choice
- Break condition: If history effects are too long-range, noisy, or require external state (neural recordings), the GRU may not have sufficient capacity or input.

### Mechanism 3
- Claim: Pooling across animals enables learning rule inference despite the underconstrained inverse problem.
- Mechanism: Multiple animals following the same learning rule provide more data points. Cross-validation ensures the inferred rule generalizes to held-out animals.
- Core assumption: Animals share a common learning rule; individual variability averages out.
- Evidence anchors:
  - [section 4]: "pooling across animals rather than fitting individual subjects... offers a practical mitigation strategy"
  - [Table 1]: Held-out prediction improves significantly with pooled training
  - [corpus]: Related work on IRL with switching rewards (arxiv 2501.12633) similarly assumes structured shared rules
- Break condition: If animals use fundamentally different learning strategies, pooling will recover an inaccurate "average" rule that fits no individual well.

## Foundational Learning

- Concept: **Generalized Linear Models (GLMs) for binary choice**
  - Why needed here: The policy is defined as p(y=1|x,w) = σ(w^T x); understanding logistic regression and sigmoid functions is essential to interpret the decision model.
  - Quick check question: Can you explain why the sigmoid function maps linear outputs to probabilities?

- Concept: **Policy gradient / REINFORCE**
  - Why needed here: The baseline comparison uses REINFORCE; understanding Δw ∝ r·∇log p(y|x,w) helps contextualize why the flexible DNN approach captures additional structure.
  - Quick check question: What does the reward r modulate in the REINFORCE update rule?

- Concept: **Recurrent neural networks (GRUs)**
  - Why needed here: The RNNGLM variant uses a GRU to maintain hidden state across trials; understanding how h_t = f(h_{t-1}, x_t) enables history dependence is critical.
  - Quick check question: Why might a GRU capture longer-range dependencies better than simply adding past trials as explicit features?

## Architecture Onboarding

- Component map:
  - Input: [stimulus s_t, choice y_t, reward r_t, current weight w_t] → concatenated vector
  - DNNGLM: Input → 2 hidden layers (32 units each, ReLU) → output Δw_t (same dimension as w_t)
  - RNNGLM: Input → GRU (32 hidden units) → hidden state h_t → DNN head → Δw_t
  - Policy: w_t+1 = w_t + Δw_t; choice probability p_t = σ(w_t^T x_t)
  - Loss: Binary cross-entropy summed over trials

- Critical path:
  1. Initialize w_0 (from psychometric curve or zero)
  2. For each trial t: construct input, pass through DNN/RNN, compute Δw_t, update w_{t+1}
  3. Compute choice probability p_t, accumulate BCE loss
  4. Backpropagate through all trials (unrolled), update network parameters
  5. Validate on held-out animals, tune hyperparameters

- Design tradeoffs:
  - DNN vs. RNN: DNN is simpler but cannot capture history; RNN adds capacity but requires more data and tuning
  - Pooling vs. individual fitting: Pooling increases data but obscures individual variability
  - Weight initialization: Psychometric-curve initialization helps; wrong w_0 degrades recovery (Table 5)

- Failure signatures:
  - DNNGLM predicts well but RNNGLM adds no improvement → learning may be Markovian
  - RNNGLM overfits (high train LL, low test LL) → reduce hidden units or add regularization
  - Inferred Δw shows no clear structure across seeds → underconstrained; increase data or constrain architecture

- First 3 experiments:
  1. Validate on simulated data with known ground truth (REINFORCE or eligibility-trace variant); confirm recovery via RMSE between true and inferred Δw.
  2. Ablation: Compare DNNGLM vs. RNNGLM vs. history-augmented DNNGLM on IBL held-out data; check if RNN improvement is statistically significant.
  3. Sensitivity test: Vary initial weight w_0 (Table 3), add noise to learning rule (Table 4), and verify degradation is graceful.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the observed "negative baseline" (weight decreases after errors) arise from specific cognitive mechanisms like choice perseverance or forgetting, or is it an artifact of model mismatch?
- Basis in paper: [explicit] The authors ask: "investigate whether the observed suboptimal negative baseline arises from specific cognitive mechanisms — such as choice perseverance or forgetting — or from model mismatch."
- Why unresolved: The current framework captures the phenomenon empirically but cannot distinguish between a genuine behavioral strategy and a failure of the assumed GLM structure to capture the true generative process.
- What evidence would resolve it: Designing tasks that explicitly manipulate perseverance vs. model assumptions, or fitting models that include perseverance terms to see if the negative baseline effect disappears.

### Open Question 2
- Question: Can the framework be extended to jointly infer the decision model and the learning rule, rather than assuming a fixed decision policy?
- Basis in paper: [explicit] "Extending the framework to jointly infer both the decision model and the learning rule is an important direction for future work."
- Why unresolved: The current modular approach fixes the decision model (e.g., GLM) for interpretability, but this limits expressivity and leads to performance degradation in multiplicative decision processes where the model structure is unknown.
- What evidence would resolve it: Development of a unified architecture that learns both the stimulus-to-choice mapping and the update rule simultaneously, demonstrating robustness to unknown decision structures.

### Open Question 3
- Question: How do different neural network architectures and their inductive biases affect inference accuracy in data-limited regimes?
- Basis in paper: [explicit] "Another future avenue would be to investigate how different architectures—and their associated inductive biases—affect inference accuracy, particularly under data limitations."
- Why unresolved: While implicit regularization (favoring smooth functions) helped the current models, it remains unclear if this bias generalizes across various learning systems or if specific architectures are required for different types of tasks.
- What evidence would resolve it: A systematic comparison of architectures (e.g., Transformers vs. RNNs) on simulated datasets with varying noise levels and sample sizes to map inductive biases to inference accuracy.

## Limitations
- The inference framework assumes shared learning rules across animals, which may not hold for all subjects
- The method assumes the true learning rule is a smooth function of observable variables
- The choice of policy (GLM with sigmoid) is a strong parametric assumption that may not capture all decision-making processes

## Confidence

- **High confidence**: The DNNGLM's ability to recover ground-truth learning rules in simulations (R²=0.988) and outperform REINFORCE on IBL data
- **Medium confidence**: The RNNGLM's recovery of non-Markovian dynamics is demonstrated in simulation but requires more validation on real data
- **Medium confidence**: The claim that pooling across animals enables inference is supported by improved held-out prediction but assumes shared rules that weren't directly tested

## Next Checks
1. **Individual vs. pooled analysis**: Apply DNNGLM to each IBL animal separately and compare held-out prediction quality to pooled models. Quantify the trade-off between sample size and individual variability.
2. **History dependence ablation**: Systematically test RNNGLM on subsets of IBL data with varying trial lengths (e.g., 1,000 vs. 10,000 trials) to establish the minimum data requirement for detecting history effects.
3. **Model mismatch robustness**: Generate synthetic data with non-GLM decision policies (e.g., softmax with temperature, multiplicative noise) and test whether the framework still recovers the learning rule or fails gracefully with diagnostic warnings.