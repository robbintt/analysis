---
ver: rpa2
title: 'GenFusion: Closing the Loop between Reconstruction and Generation via Videos'
arxiv_id: '2503.21219'
source_url: https://arxiv.org/abs/2503.21219
tags:
- video
- reconstruction
- view
- diffusion
- generation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: GenFusion addresses the conditioning gap between 3D reconstruction
  (requiring dense views) and 3D generation (working from single/no views) by proposing
  a reconstruction-driven video diffusion model. The method trains a video diffusion
  model using artifact-prone RGB-D renderings from masked 3D reconstruction, then
  iteratively fuses generative outputs back into reconstruction optimization.
---

# GenFusion: Closing the Loop between Reconstruction and Generation via Videos

## Quick Facts
- **arXiv ID:** 2503.21219
- **Source URL:** https://arxiv.org/abs/2503.21219
- **Reference count:** 40
- **Primary result:** Iterative fusion of video diffusion-generated views into 3D Gaussian Splatting reconstruction improves view synthesis quality on sparse and masked inputs.

## Executive Summary
GenFusion addresses the conditioning gap between 3D reconstruction (requiring dense views) and 3D generation (working from single/no views) by proposing a reconstruction-driven video diffusion model. The method trains a video diffusion model using artifact-prone RGB-D renderings from masked 3D reconstruction, then iteratively fuses generative outputs back into reconstruction optimization. This cyclical fusion enables artifact-free view synthesis and content expansion for sparse and masked inputs. Experiments on Mip-NeRF360, DL3DV, and Tanks and Temples datasets show GenFusion outperforms state-of-the-art methods: PSNR of 15.29 (vs 13.07–14.17 baseline) on 3-view sparse reconstruction, 20.47 PSNR on masked input extrapolation, and achieves scene-level completion. The approach effectively bridges reconstruction and generation, improving novel view synthesis quality across both interpolation and extrapolation scenarios.

## Method Summary
GenFusion closes the conditioning gap between 3D reconstruction and generation by training a video diffusion model to repair artifacts in RGB-D renderings from masked 3D reconstruction, then iteratively fusing the repaired views back into the 3D scene representation. The method uses masked 2D Gaussian Splatting to generate artifact-GT video pairs for training a fine-tuned DynamiCrafter with an RGB-D VAE. During inference, it renders novel views from the current 3DGS, passes them through the diffusion model for repair, and adds the restored frames as supervision for the 3DGS optimization. This cyclic densification and fusion process adaptively adds new Gaussian primitives via unreliable depth-based mapping, enabling faithful extrapolation of content for large unobserved regions. The approach improves novel view synthesis quality on both interpolation and extrapolation scenarios.

## Key Results
- Achieves PSNR of 15.29 on 3-view sparse reconstruction (vs 13.07–14.17 baseline methods)
- Outperforms baselines with 20.47 PSNR on masked input extrapolation scenarios
- Demonstrates scene-level completion capabilities on Tanks and Temples benchmark
- Shows FID improvement from 26.16 (RGB VAE) to 22.55 (RGB-D VAE) in video generation quality

## Why This Works (Mechanism)

### Mechanism 1: Artifact-GT Pair Generation via Masked Reconstruction
During pre-training, the method divides input images into patches and masks out 75% of the scene per sequence. It performs 2D Gaussian Splatting on this sparse input, which inevitably produces needle-like artifacts and holes. This artifact-prone rendering is paired with the original captured video to supervise the video diffusion model. The core assumption is that artifacts produced by random spatial masking statistically approximate the artifacts produced by sparse view coverage or far-field rendering in real-world applications.

### Mechanism 2: Geometry-Conditioned Video Diffusion
The method fine-tunes DynamiCrafter by replacing its RGB VAE with a pre-trained RGB-D VAE (from LDM3D). The artifact-prone RGB-D renderings are encoded into this latent space. The denoising network is conditioned on both this latent sequence and a CLIP embedding of a reference frame to maintain texture and identity. The generative prior of the base video model is assumed to be robust enough to survive the domain shift from RGB latent space to RGB-D latent space without catastrophic forgetting.

### Mechanism 3: Cyclic Densification and Fusion
The process runs in cycles: render a trajectory → diffuse/repair the frames → use the repaired frames as new supervision for the 3DGS. Critically, because generated views may cover unseen areas, the method adds new Gaussian primitives via unreliable depth-based mapping (back-projecting pixels where rendered opacity is low or depth error is high) rather than relying solely on standard gradient-based densification. The core assumption is that the video diffusion model generates sufficiently consistent depth maps to allow accurate back-projection of new Gaussians without introducing geometric noise.

## Foundational Learning

- **Concept: 3D Gaussian Splatting (3DGS) & 2DGS**
  - **Why needed here:** The paper uses 2DGS as the underlying 3D representation. You must understand that 3DGS represents a scene as a cloud of 3D Gaussians (position, covariance, opacity, SH coefficients) and renders them via rasterization (splatting).
  - **Quick check question:** How does "densification" in 3DGS differ from adding vertices in a mesh? (Answer: Densification typically splits/clones Gaussians based on gradient screenspace error; GenFusion modifies this to add Gaussians based on generative depth).

- **Concept: Latent Diffusion Models (LDMs) & VAEs**
  - **Why needed here:** GenFusion modifies the architecture of an LDM. You need to understand that an LDM compresses images into "latents" using a VAE, adds noise, and learns to denoise. The paper swaps the standard VAE for an RGB-D VAE.
  - **Quick check question:** Why operate in "latent space" rather than pixel space? (Answer: Computational efficiency. GenFusion runs diffusion at 64x40 latent resolution for 512x320 frames).

- **Concept: Photometric vs. Generative Supervision**
  - **Why needed here:** The system optimizes 3D Gaussians using two losses. $L_{recon}$ compares rendered pixels to input pixels (standard). $L_{gen}$ compares rendered pixels to *generated* pixels.
  - **Quick check question:** What are the risks of optimizing a 3D model using $L_{gen}$? (Answer: If the generator hallucinates impossible geometry, the 3D model will overfit to a non-existent 3D structure).

## Architecture Onboarding

- **Component map:** COLMAP points → 2DGS Initialization → Renderer (2DGS) → Diffusion Guidance (Repairs RGB-D frames) → Densifier (Adds Gaussians via back-projection) → Optimizer (Updates Gaussian params via $L_{recon} + \lambda L_{gen}$).
- **Critical path:** The **Masked Reconstruction** step (Section 3.1). If the masking ratio or strategy (random patch vs. fixed patch) does not produce artifacts that visually resemble "sparse view reconstruction errors," the diffusion model will fail to generalize to real inputs.
- **Design tradeoffs:**
  - **Resolution vs. Consistency:** The paper notes that increasing resolution improves FID (Table 1) but training on longer sequences (48 frames vs 16) degraded metrics in some tests.
  - **Speed:** The cyclic fusion adds ~40 minutes per scene compared to vanilla 3DGS, due to the inference cost of the video diffusion model every K iterations.
- **Failure signatures:**
  - **Blurriness in large unobserved regions:** As noted in the Discussion, filling large invisible regions can cause blurriness due to temporal inconsistency in the video generation fragments.
  - **Black pixels in output:** If the "unreliable depth" thresholds (τ_T, τ_D) are too strict, the system fails to densify empty regions, leaving black voids.
- **First 3 experiments:**
  1. **Visualize Masked Artifacts:** Run the 2DGS baseline on a scene with 75% random patch masking. Visually verify if the artifacts (needles, holes) look like the "floaters" in a standard sparse-view reconstruction.
  2. **Diffusion Repair Test:** Take a rendered artifact video, pass it through the fine-tuned diffusion model (inference mode), and check if the RGB output aligns with the depth output (e.g., do generated edges match depth boundaries?).
  3. **Ablate the Densification:** Run the cyclic fusion loop with "sparsity-aware densification" turned *off* (use standard 3DGS densification). Compare the PSNR/SSIM on far-field views to quantify the impact of the proposed adaptive mapping.

## Open Questions the Paper Calls Out

- **Question:** How can the fusion module be improved to explicitly model and resolve inconsistencies between generated video fragments to eliminate blurriness in large unobserved regions?
  - **Basis:** The Discussion section states, "Modeling and addressing this inconsistency in the fusion module will be a key step toward achieving the next level of quality."
  - **Why unresolved:** The current cyclic fusion averages inconsistent video fragments, resulting in blur where large areas are invisible.
  - **Evidence:** A modification to the fusion strategy that maintains high-frequency details in completed regions without introducing geometric artifacts.

- **Question:** Can the cyclic optimization loop be made more efficient to reduce the additional training overhead while maintaining the quality of artifact removal?
  - **Basis:** The Discussion notes the method requires "additional denoising steps and slightly increasing training time (about 40 minutes per scene)."
  - **Why unresolved:** The iterative nature of rendering, denoising via diffusion, and re-integrating into the 3D representation is computationally expensive.
  - **Evidence:** An optimized pipeline achieving comparable reconstruction metrics with a significant reduction in wall-clock time per scene.

- **Question:** Can the cyclic fusion framework be adapted to other 3D representations like Neural Radiance Fields (NeRF), or is it intrinsically tied to the specific densification capabilities of Gaussian Splatting?
  - **Basis:** The "Content Expansion" section relies on "adaptively adding new Gaussian points" and back-projection, which is representation-specific.
  - **Why unresolved:** The paper does not demonstrate the method on other representations, and the expansion logic relies on splitting/cloning primitives.
  - **Evidence:** Implementation of the cyclic loop on a NeRF pipeline, showing successful scene completion without explicit primitive manipulation.

## Limitations

- **Artifact similarity assumption:** The quality of the video diffusion model depends critically on whether the artifacts produced by masking accurately reflect the errors in real-world sparse-view reconstruction.
- **Computational overhead:** The cyclic fusion process adds approximately 40 minutes of additional training time per scene compared to vanilla 3DGS.
- **Blurriness in large unobserved regions:** The method can produce blurry results when filling large invisible regions due to temporal inconsistency in the video generation fragments.

## Confidence

- **High:** The paper provides strong quantitative results (PSNR, SSIM, LPIPS improvements) on benchmark datasets, which are convincing and well-documented.
- **Medium:** The qualitative analysis of the generated content is limited. While FID scores suggest the video quality is high, there is insufficient discussion of the geometric consistency of the extrapolated regions.
- **Low:** The ablation comparing RGB-D VAE to RGB VAE is incomplete; a more rigorous comparison would isolate the benefit of the VAE change itself.

## Next Checks

1. **Cross-Dataset Artifact Transfer:** Train the video diffusion model on one dataset (e.g., DL3DV) and test its ability to repair artifacts on a different dataset (e.g., Tanks and Temples) with a different camera distribution and scene type. This will test the generality of the artifact-repair learning.

2. **Human Perceptual Study:** Conduct a user study comparing the visual quality of the extrapolated views from GenFusion to those from a baseline method. Focus on the perceived realism and geometric plausibility of the hallucinated content, not just numerical metrics.

3. **Depth Consistency Analysis:** For a set of scenes, render the depth maps from the original 3DGS and the GenFusion-optimized 3DGS. Compute the depth error in regions that were originally unobserved (far-field). This will directly measure if the generative fusion introduces geometric inconsistencies.