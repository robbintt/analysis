---
ver: rpa2
title: 'Code-in-the-Loop Forensics: Agentic Tool Use for Image Forgery Detection'
arxiv_id: '2512.16300'
source_url: https://arxiv.org/abs/2512.16300
tags:
- image
- forenagent
- tool
- reasoning
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces ForenAgent, a multi-round interactive framework
  that enables multimodal large language models (MLLMs) to autonomously generate,
  execute, and iteratively refine Python-based low-level forensic tools for image
  forgery detection. To train this system, the authors construct FABench, a large-scale
  dataset with 100k images and 200k agent-interaction question-answer pairs spanning
  authentic, synthetic, and tampered categories.
---

# Code-in-the-Loop Forensics: Agentic Tool Use for Image Forgery Detection

## Quick Facts
- arXiv ID: 2512.16300
- Source URL: https://arxiv.org/abs/2512.16300
- Reference count: 40
- ForenAgent achieves 88.1% accuracy and 88.2% F1-score on FABench

## Executive Summary
ForenAgent introduces a novel multi-round interactive framework that enables multimodal large language models (MLLMs) to autonomously generate, execute, and refine Python-based low-level forensic tools for image forgery detection. The system combines a large-scale training dataset (FABench) with a two-stage training pipeline to achieve state-of-the-art performance on image authenticity verification tasks. The framework demonstrates emergent reflective reasoning capabilities through its iterative probing and adjudication process.

## Method Summary
The ForenAgent framework employs a code-in-the-loop approach where MLLMs dynamically generate and execute Python forensic tools through a four-phase reasoning loop: global perception, local focusing, iterative probing, and holistic adjudication. The system is trained on FABench, a dataset containing 100k images and 200k agent-interaction QA pairs across authentic, synthetic, and tampered categories. Training follows a two-stage process combining Cold Start initialization with Reinforcement Fine-Tuning, guided by a reward model that evaluates tool selection, reasoning coherence, and adjudication quality.

## Key Results
- Achieves 88.1% accuracy and 88.2% F1-score on the FABench benchmark
- Outperforms state-of-the-art baselines with 80.6% accuracy on SIDA-Test
- Demonstrates emergent reflective reasoning and tool-use competence through iterative refinement

## Why This Works (Mechanism)
The framework succeeds by bridging the gap between high-level reasoning and low-level forensic operations. MLLMs generate context-aware Python scripts that can perform pixel-level manipulations and statistical analyses beyond the model's native capabilities. The multi-round interaction allows for progressive refinement of hypotheses, while the reward-based training encourages optimal tool selection and reasoning strategies. The combination of global and local analysis phases ensures both comprehensive coverage and targeted investigation of suspicious regions.

## Foundational Learning
- **Multimodal Large Language Models (MLLMs)**: Models that process both visual and textual information; needed for integrating image analysis with natural language reasoning; quick check: verify model supports image input and generation capabilities.
- **Code-in-the-Loop Systems**: Architectures where models generate and execute code dynamically; needed for accessing low-level forensic operations; quick check: confirm secure sandbox environment for code execution.
- **Reinforcement Fine-Tuning**: Training approach using reward signals to guide policy optimization; needed for developing effective tool selection strategies; quick check: validate reward model captures relevant performance metrics.
- **Multi-round Interactive Reasoning**: Iterative dialogue-based problem solving; needed for progressive refinement of forgery hypotheses; quick check: ensure context persistence across interaction rounds.
- **Python-based Forensic Tools**: Low-level image analysis utilities for pixel manipulation and statistical analysis; needed for operations beyond model native capabilities; quick check: verify tool library covers essential forensic operations.

## Architecture Onboarding

**Component Map**: MLLM -> Code Generator -> Python Sandbox -> Forensic Tools -> Analysis Results -> Reasoning Loop -> Reward Model

**Critical Path**: Image Input → Global Perception → Local Focusing → Iterative Probing → Holistic Adjudication → Final Decision

**Design Tradeoffs**: The framework prioritizes accuracy and reasoning depth over computational efficiency, accepting the overhead of multi-round interactions and dynamic code generation. This tradeoff enables superior performance on complex forgery detection tasks but may limit real-time applicability.

**Failure Signatures**: Potential failures include tool generation errors due to ambiguous prompts, reward model bias toward certain tool types, and incomplete coverage of forgery patterns in the training dataset. The system may also struggle with novel forgery techniques not represented in FABench.

**First 3 Experiments**:
1. Baseline evaluation on controlled synthetic and tampered image sets
2. Ablation study removing the multi-round interaction component
3. Transfer learning assessment on previously unseen forgery types

## Open Questions the Paper Calls Out
None identified in the provided content.

## Limitations
- Performance heavily dependent on quality and diversity of FABench training dataset
- Computational overhead from multi-round interactive framework may limit real-time applications
- Reliance on specific Python-based forensic tools may reduce generalizability across forensic scenarios

## Confidence
- **High Confidence**: Performance metrics (88.1% accuracy, 88.2% F1-score) are well-supported by experimental results and baseline comparisons
- **Medium Confidence**: Emergent reflective reasoning capabilities are promising but require additional validation for generalizability
- **Low Confidence**: Real-world scalability and applicability remain uncertain without extensive testing in uncontrolled environments

## Next Checks
1. Evaluate generalization on diverse real-world image forgeries from multiple sources
2. Analyze computational efficiency including runtime and memory usage for real-time feasibility
3. Assess dependency on specific forensic tools and test adaptability to alternative or custom tools