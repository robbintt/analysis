---
ver: rpa2
title: 'Amadeus: Autoregressive Model with Bidirectional Attribute Modelling for Symbolic
  Music'
arxiv_id: '2508.20665'
source_url: https://arxiv.org/abs/2508.20665
tags:
- music
- note
- generation
- symbolic
- attributes
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces Amadeus, a symbolic music generation framework
  that addresses the limitations of existing autoregressive models by adopting a two-level
  architecture: an autoregressive model for note sequences and a bidirectional discrete
  diffusion model for note attributes. The authors argue that note attributes are
  inherently unordered and concurrent, challenging the traditional sequential modeling
  approach.'
---

# Amadeus: Autoregressive Model with Bidirectional Attribute Modelling for Symbolic Music

## Quick Facts
- arXiv ID: 2508.20665
- Source URL: https://arxiv.org/abs/2508.20665
- Authors: Hongju Su; Ke Li; Lan Yang; Honggang Zhang; Yi-Zhe Song
- Reference count: 7
- Primary result: Introduces a two-level architecture achieving 4× speedup over baselines while improving music quality and attribute controllability

## Executive Summary
This paper introduces Amadeus, a symbolic music generation framework that addresses the limitations of existing autoregressive models by adopting a two-level architecture: an autoregressive model for note sequences and a bidirectional discrete diffusion model for note attributes. The authors argue that note attributes are inherently unordered and concurrent, challenging the traditional sequential modeling approach. To enhance performance, they propose the Music Latent Space Discriminability Enhancement Strategy (MLSDES) with contrastive learning and the Conditional Information Enhancement Module (CIEM) using attention mechanisms. Amadeus achieves significant improvements over state-of-the-art models, including at least 4× speedup, superior music quality, and fine-grained attribute controllability. The model is validated across unconditional and text-conditioned generation tasks, and the authors also compile and open-source the largest symbolic music dataset to date, AMD (Amadeus MIDI Dataset), containing 1.9 million pre-training samples and 320,000 text-annotated fine-tuning samples.

## Method Summary
Amadeus employs a hybrid architecture that separates temporal modeling of note sequences from concurrent modeling of intra-note attributes. The Note Generator is a standard Transformer-based autoregressive model that generates a latent vector for each note position sequentially. The Note Decoder is a bidirectional discrete diffusion model that decodes attributes (pitch, velocity, etc.) from the latent vector in parallel, treating them as an unordered set rather than a temporal sequence. The Conditional Information Enhancement Module (CIEM) uses attention mechanisms to inject global context into the per-note latent vector, while the Music Latent Space Discriminability Enhancement Strategy (MLSDES) applies contrastive learning to intermediate representations to improve latent space crispness. The model is trained with a combined loss function incorporating contrastive loss and weighted cross-entropy, optimized using AdamW with cosine scheduling.

## Key Results
- Achieves at least 4× speedup over state-of-the-art models while maintaining or improving music quality
- Superior performance on unconditional generation metrics: Scale Consistency (SC), Pitch Entropy (PE), and Pitch-Class Entropy (PCE)
- Strong text-conditioned generation results using CLAP score, Tempo Bin with Tolerance (TBT), Correct Key (CK), and other semantic alignment metrics
- Introduces and open-sources the largest symbolic music dataset to date (AMD) with 1.9 million pretraining samples and 320,000 text-annotated samples

## Why This Works (Mechanism)

### Mechanism 1
Decoupling temporal modeling of notes from concurrent modeling of intra-note attributes improves generation efficiency and structural fidelity. The architecture replaces sequential decoding with a two-level hierarchy where a Transformer-based autoregressive model generates latent vectors for each note position, and a separate bidirectional discrete diffusion model decodes attributes in parallel as an unordered set. This challenges the traditional assumption that intra-note attributes must be modeled sequentially. Evidence includes the abstract's statement about attributes being "concurrent and unordered sets" and the section describing parallel attribute decoding using Masked Diffusion Models. The break condition occurs if specific musical attributes exhibit strong causal dependencies that the bidirectional approach fails to capture.

### Mechanism 2
Applying contrastive constraints to intermediate representations of the Note Generator enhances latent space crispness, aiding the downstream Note Decoder. The MLSDES strategy applies a contrastive loss to the intermediate layer of the Note Generator, forcing latent vectors of different music samples to be farther apart in the embedding space. This amplifies the distance between latent vectors to improve attribute recovery. Evidence includes the ablation study showing performance drops when MLSDES is removed, and the section describing the improved contrastive loss function. The break condition occurs with small batch sizes that don't provide enough negative samples for effective separation.

### Mechanism 3
Injecting global context into per-note latent vectors via attention mechanisms preserves long-range musical continuity. The CIEM module takes the latent vector for the current note and applies cross-attention against the start token/global context vector and self-attention. This "warns" the decoder about the global musical theme before predicting specific attributes. Evidence includes the definition of the CIEM operation and ablation results showing poor quality without CIEM due to lack of continuity. The break condition occurs if the Note Generator fails to encode meaningful long-term dependencies into the global context vector.

## Foundational Learning

- **Discrete Diffusion Models (DDM)**: The Note Decoder uses discrete diffusion (specifically Masked Diffusion Model) rather than standard continuous diffusion. Understanding the difference between Gaussian diffusion (continuous) and Masked diffusion (discrete) is required to debug the decoding loop. Quick check: How does the noise schedule in discrete mask diffusion differ from the variance schedule in continuous DDPM?

- **Contrastive Learning (InfoNCE family)**: The MLSDES component uses a specific contrastive loss function based on cosine distance to regularize the latent space. You must understand how temperature and negative sampling affect this separation mechanism. Quick check: Why does the paper apply the contrastive loss to the intermediate layer rather than the final output of the Note Generator?

- **Hybrid AR-NAR Architectures**: Amadeus is neither purely Autoregressive nor purely Non-Autoregressive, but a hybrid: AR for time, NAR (via Diffusion) for features. Understanding the latency/quality trade-offs of this hybrid approach explains the 4× speedup claim. Quick check: In this hybrid model, which component (Note Generator or Note Decoder) is the bottleneck during inference, and how does adjusting diffusion steps affect this?

## Architecture Onboarding

- **Component map**: Input MIDI Tokens -> Note Embedder -> Note Generator (AR Transformer) -> MLSDES (Training-only) -> CIEM -> Note Decoder (DDM) -> Output MIDI Tokens
- **Critical path**: Input Seq -> Note Generator (AR) -> MLSDES (Training-only regularization) -> CIEM -> Note Decoder (DDM) -> MIDI Tokens
- **Design tradeoffs**: Steps (T) vs. Quality: The Note Decoder diffusion process is adjustable (step count). T=1 is fastest but lowest quality; T=8 is slower but higher quality. Paper notes T=4 can occasionally outperform T=8 by avoiding local optima. Latent Dimensionality: The size of z determines how much "note information" must be compressed before being expanded back into 8 attributes.
- **Failure signatures**: "Attribute Bleed" if bidirectional assumption holds poorly, resulting in inconsistent attribute pairs. "Global Drift" if CIEM is misconfigured, causing loss of long-term structure. "Slow Inference" if diffusion steps are set too high, eliminating the speedup advantage.
- **First 3 experiments**: 1) Step Sweep: Generate samples with diffusion steps T={1,2,4,8} and measure Pitch Entropy and latency to establish operational curve. 2) Ablation CIEM: Disable Cross-Attention in CIEM and generate a batch to check for loss of Scale Consistency. 3) Training-free Control: Run inference with manually fixed "Instrument" token to verify constraint enforcement without retraining.

## Open Questions the Paper Calls Out

- **Question 1**: How does the Amadeus architecture scale regarding performance and efficiency when model parameters exceed the current 500M limit? Basis: Authors state hardware limitations prevented systematic study of model scaling behavior. Unresolved because only tested up to 500M variant. Evidence: Training and evaluating variants with 1B+ parameters to plot scaling curves.

- **Question 2**: Can lossless acceleration techniques be effectively integrated into the discrete diffusion decoder to bridge the gap to real-time streaming generation? Basis: Conclusion notes synergistic optimization potential between computational efficiency and generation quality remains. Unresolved because current inference optimization is acknowledged as suboptimal. Evidence: Implementation of knowledge distillation or optimized CUDA kernels that reduce latency without degrading SC or CLAP scores.

- **Question 3**: To what extent does the assumption that note attributes are "concurrent and unordered" hinder modeling of weak but non-zero directional dependencies? Basis: Authors argue attributes are unordered sets, yet mutual information analysis reveals directional influences (e.g., Pitch reducing uncertainty in Duration). Unresolved because paper doesn't analyze if ignoring statistical asymmetries results in loss of musical nuance. Evidence: Ablation study comparing current symmetric diffusion against masked diffusion prioritizing high-information attributes.

- **Question 4**: How does the bias of the AMD dataset towards specific genres limit the model's applicability to non-Western or microtonal music systems? Basis: Authors acknowledge limited support for underrepresented musical genres due to training dataset constraints. Unresolved because AMD aggregates existing corpora skewing toward Western/Electronic music. Evidence: Zero-shot evaluation on traditional folk or microtonal music datasets to assess tonal coherence and attribute accuracy.

## Limitations
- The foundational claim about attributes being unordered concurrent sets may not generalize across all musical styles and instrument families
- The 4× speedup claim depends heavily on specific diffusion step configuration and batch size effects that require further exploration
- The CLAP-based text-conditioned evaluation may not accurately reflect true musical quality since CLAP was trained on audio, not symbolic music
- The MLSDES contrastive loss mechanism lacks comprehensive sensitivity analysis on critical hyperparameters

## Confidence
- **High Confidence**: The two-level architecture design and its implementation details are well-specified and reproducible. Unconditional generation metrics are clearly defined and measured.
- **Medium Confidence**: Text-conditioned generation results are methodologically sound but depend on CLAP's semantic alignment with symbolic representations, which hasn't been independently validated. The 4× speedup claim is contextually valid but requires specific configurations.
- **Low Confidence**: The foundational claim about attributes being unordered concurrent sets is theoretically justified but empirically underexplored across diverse musical genres. MLSDES contrastive learning lacks hyperparameter sensitivity analysis.

## Next Checks
1. **Attribute Dependency Analysis**: Conduct systematic study across multiple musical genres to verify whether intra-note attribute dependencies vary significantly and test whether bidirectional decoder performance degrades when strong attribute dependencies exist.

2. **CLAP Alignment Verification**: Compare CLAP-based text-conditioned evaluation with human perceptual studies on generated samples to measure correlation between CLAP scores and expert ratings for semantic relevance and musical coherence.

3. **Hyperparameter Sensitivity Testing**: Perform comprehensive ablation studies on MLSDES hyperparameters (λ from 0.01 to 1.0, τ from 0.01 to 0.5) and diffusion steps (T=1 to T=16) to document performance metric variations and establish robust operational configurations.