---
ver: rpa2
title: Timepoint-Specific Benchmarking of Deep Learning Models for Glioblastoma Follow-Up
  MRI
arxiv_id: '2511.18595'
source_url: https://arxiv.org/abs/2511.18595
tags:
- follow-up
- batch
- were
- progression
- pseudoprogression
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study benchmarks deep learning models for differentiating
  true tumor progression from pseudoprogression in glioblastoma using follow-up MRI.
  Eleven architectures (CNNs, LSTMs, transformers, Mamba, hybrids) were evaluated
  across two clinical timepoints on a 180-patient cohort.
---

# Timepoint-Specific Benchmarking of Deep Learning Models for Glioblastoma Follow-Up MRI

## Quick Facts
- arXiv ID: 2511.18595
- Source URL: https://arxiv.org/abs/2511.18595
- Reference count: 40
- Primary result: Mamba+CNN hybrid achieves best accuracy-efficiency trade-off in differentiating true progression from pseudoprogression across two clinical timepoints

## Executive Summary
This study benchmarks 11 deep learning architectures on a 180-patient glioblastoma dataset to differentiate true tumor progression from pseudoprogression using follow-up T1C MRI. The evaluation spans two clinical timepoints (early and late post-radiotherapy) to assess timepoint-specific performance. Models range from CNNs and transformers to Mamba-based hybrids, with the Mamba+CNN combination delivering the most favorable efficiency-accuracy balance. Performance metrics show modest overall discrimination (accuracy ~0.70-0.74) but improved separability at later timepoints, particularly in F1 and AUC scores. The work establishes a standardized benchmark while highlighting the need for longitudinal modeling and multi-sequence data integration.

## Method Summary
The study uses the Burdenko-GBM-Progression dataset (n=180) with T1C MRI scans, preprocessed through skull-stripping, MNI152 registration, and 128³ resampling. Three-class classification (progression, pseudoprogression, stable) is performed using 11 architectures: CNNs, LSTMs, transformers, Mamba variants, and hybrids. Training employs patient-level stratified 5-fold cross-validation with latent-space SMOTE augmentation and mild affine/Gaussian noise. Models are evaluated on accuracy, macro-averaged F1 and AUC, with efficiency measured via FLOPs and parameters. The best-performing Mamba+CNN hybrid achieves ~24M parameters and <1 GFLOP while maintaining competitive accuracy.

## Key Results
- Mamba+CNN hybrid provides optimal accuracy-efficiency trade-off (~24M params, <1 GFLOP)
- Similar accuracy (~0.70-0.74) at both timepoints, but F1 and AUC improve at second follow-up
- Performance sensitive to batch size, emphasizing need for standardized training protocols
- Dataset imbalance and task complexity limit overall discrimination despite architectural diversity

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Later clinical timepoints provide richer class separability between true progression, pseudoprogression, and stable disease
- Mechanism: Treatment-related inflammatory effects dominate early post-radiotherapy imaging, obscuring discriminative features; by 2-3 months post-RT, true tumor biology becomes more distinguishable from transient treatment effects, enabling better model discrimination
- Core assumption: The underlying biological signal-to-noise ratio improves with time post-treatment
- Evidence anchors: [abstract] "discrimination improved at the second follow-up, with F1 and AUC increasing for several models"; [section] "the second follow-up exhibited a greater separation between progression, pseudoprogression, and stable disease"
- Break condition: If early timepoints incorporate multi-parametric sequences (perfusion, diffusion), early separability may improve significantly

### Mechanism 2
- Claim: Hybrid CNN+Mamba architectures achieve optimal efficiency-accuracy trade-offs by combining local spatial feature extraction with efficient long-range sequence modeling
- Mechanism: CNN blocks capture fine-grained volumetric textures and edge patterns; Mamba's selective state-space modeling aggregates cross-slice dependencies in linear time, avoiding transformer quadratic costs while preserving global context
- Core assumption: Local convolutional inductive biases remain valuable for volumetric MRI even when combined with sequence models
- Evidence anchors: [abstract] "A Mamba+CNN hybrid consistently offered the best accuracy-efficiency trade-off"; [section] "2D-Mamba+CNN model provided the most favorable efficiency–performance balance"
- Break condition: If GPU memory is unconstrained and inference latency is not a deployment concern, pure transformer architectures may achieve marginally higher AUC

### Mechanism 3
- Claim: Batch size influences model discrimination metrics, with smaller batches potentially stabilizing minority class learning under imbalance
- Mechanism: Smaller batches introduce noisier gradients that may prevent overfitting to majority classes; larger batches can suppress minority class signal in imbalanced datasets
- Core assumption: Gradient noise interacts with class imbalance in a way that affects F1 and AUC but not necessarily accuracy
- Evidence anchors: [abstract] "Performance also showed sensitivity to batch size, underscoring the need for standardized training protocols"; [section] "The 2D-Mamba+CNN model was further evaluated under batch sizes {1, 2, 4, 8} to assess robustness"
- Break condition: If class-balanced sampling or focal loss is applied, batch size sensitivity may diminish

## Foundational Learning

- Concept: Selective State-Space Models (Mamba)
  - Why needed here: Mamba enables linear-time sequence modeling of 3D MRI slices without transformer quadratic attention costs
  - Quick check question: Explain why Mamba's complexity is O(n) while standard self-attention is O(n²)

- Concept: Pseudoprogression vs True Progression
  - Why needed here: Understanding that PsP is a treatment-related inflammatory effect occurring early post-RT (up to 50% within first 3 months) is essential for interpreting timepoint-dependent model behavior
  - Quick check question: Why does clinical separability improve at later follow-ups despite similar accuracy across timepoints?

- Concept: Patient-Level Cross-Validation with Oversampling
  - Why needed here: Preventing data leakage when using SMOTE/AE-based augmentation requires fitting oversampling only on training folds
  - Quick check question: What goes wrong if oversampling is applied before the train/validation split?

## Architecture Onboarding

- Component map: Input (T1C volumes 128³) -> Preprocessing (skull-strip, register, normalize) -> CNN backbone (Conv3D-BN-ReLU blocks with MaxPool3D) -> Mamba encoder (Vision-Mamba tiny) -> Fusion (mean pooling across slices) -> Projection (128-D) -> Output (linear+SoftMax 3 classes) -> Augmentation (AE latent-space SMOTE + affine/Gaussian noise)

- Critical path:
  1. Preprocessing quality (skull-stripping, registration) directly affects feature learning
  2. CNN feature extraction before Mamba ensures local patterns are preserved
  3. Mean pooling across slices determines how global context is aggregated—alternative aggregation (attention-weighted) may improve performance

- Design tradeoffs:
  - Transformers (3D-ViT, Swin): Higher AUC potential, but 230+ GFLOPs and 1000+ min training
  - Lightweight CNNs: <13 GFLOPs, but lower F1/AUC and higher variance
  - Mamba+CNN: <1 GFLOP, ~24M params, stable accuracy-F1 trade-off across timepoints

- Failure signatures:
  - F1 ≈ 0.27-0.30 with accuracy ~0.70: Model may be predicting majority class; check class balance and loss weighting
  - High AUC with low F1: Discrimination exists but threshold selection is suboptimal
  - Large variance across seeds: Dataset too small or augmentation insufficient

- First 3 experiments:
  1. Reproduce 2D-Mamba+CNN on first follow-up with batch sizes {1, 4, 8}; verify F1 improves from 0.44 baseline
  2. Ablate CNN backbone (replace with random projection) to quantify contribution of local features vs Mamba sequence modeling
  3. Add FLAIR/T2 sequences as additional input channels; assess whether multi-sequence data improves early timepoint discrimination

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does modeling the full longitudinal trajectory of MRI scans improve the differentiation of true progression from pseudoprogression compared to independent cross-sectional analysis?
- Basis in paper: [explicit] The authors state that the results "motivate future work incorporating longitudinal modeling" rather than the cross-sectional approach used here
- Why unresolved: This study analyzed the first and second follow-ups independently to establish a timepoint-specific benchmark, failing to capture temporal dependencies between scans
- What evidence would resolve it: Comparative results from architectures (e.g., LSTMs or time-aware transformers) trained on sequences of scans rather than single timepoints

### Open Question 2
- Question: To what extent does incorporating multi-sequence MRI (e.g., FLAIR, T2) or clinical covariates (e.g., MGMT status) enhance model discrimination beyond T1C alone?
- Basis in paper: [explicit] The discussion notes that T1C alone "cannot comprehensively describe treatment-induced alterations," and the conclusion lists "multi-sequence MRI" as critical future work
- Why unresolved: The study restricted inputs to contrast-enhanced T1-weighted MRI (T1C) to maximize patient inclusion and ensure comparability across the cohort
- What evidence would resolve it: A benchmark study evaluating these architectures on datasets containing complete multi-parametric MRI sequences and molecular data

### Open Question 3
- Question: Do the favorable efficiency-accuracy trade-offs of the Mamba+CNN hybrid generalize to larger, multi-center cohorts with diverse scanner protocols?
- Basis in paper: [explicit] The discussion states that "multi-center validation is required to verify its generalizability" due to the study's reliance on a single dataset
- Why unresolved: The benchmark was performed exclusively on the Burdenko GBM Progression cohort (n=180), which may not represent the full heterogeneity of clinical imaging environments
- What evidence would resolve it: Validation performance of the Mamba+CNN model on external, multi-institutional datasets with varying scanner vendors and protocols

## Limitations
- Exact architectural specifications for transformer and Mamba variants are not fully provided in the main text
- Modest overall performance (F1 0.27-0.44) limited by dataset imbalance and task complexity
- Single-sequence (T1C) approach may not capture full diagnostic information available in multi-parametric protocols

## Confidence

- Timepoint-specific separability mechanism: Medium confidence - supported by observed metric improvements but lacks direct biological validation
- Mamba+CNN efficiency claims: Medium confidence - well-documented in architecture performance but not externally validated
- Batch size sensitivity: Low confidence - observed but mechanism not established, no corpus support
- Overall model discrimination capability: Medium confidence - consistent across architectures but limited by dataset constraints

## Next Checks
1. Implement patient-level cross-validation with proper fold-wise oversampling to verify that data leakage is not artificially inflating performance
2. Replicate the batch size sensitivity analysis with at least three different batch sizes to confirm the observed metric variations
3. Test the same architectures on multi-sequence MRI (adding FLAIR/T2) to determine if early timepoint discrimination improves beyond the T1C-only results