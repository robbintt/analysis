---
ver: rpa2
title: 'Deciphering the complaint aspects: Towards an aspect-based complaint identification
  model with video complaint dataset in finance'
arxiv_id: '2503.00054'
source_url: https://arxiv.org/abs/2503.00054
tags:
- complaint
- aspect
- multimodal
- financial
- aspects
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Solution 3.0, a multimodal model for aspect-based
  complaint identification in financial videos. The model uses a CLIP-based dual frozen
  encoder combined with an image segment encoder with contextual attention (ISEC)
  to fuse text, audio, and video features.
---

# Deciphering the complaint aspects: Towards an aspect-based complaint identification model with video complaint dataset in finance

## Quick Facts
- **arXiv ID:** 2503.00054
- **Source URL:** https://arxiv.org/abs/2503.00054
- **Reference count:** 35
- **Primary result:** Introduces Solution 3.0, a multimodal model for aspect-based complaint identification in financial videos using CLIP-based dual frozen encoders and an image segment encoder with contextual attention (ISEC).

## Executive Summary
This paper presents Solution 3.0, a novel multimodal framework for identifying and classifying complaints in financial videos. The model processes audio, video, and text inputs through a CLIP-based dual frozen encoder combined with an image segment encoder with contextual attention (ISEC) to fuse features effectively. It addresses three key tasks: multimodal feature processing, multilabel aspect classification across five financial domains, and multitasking complaint identification. The approach was evaluated on a new dataset of 433 financial complaint videos, achieving superior performance across nearly all metrics compared to baseline models, demonstrating its effectiveness for customer care applications in the finance sector.

## Method Summary
Solution 3.0 processes financial complaint videos by first extracting transcripts using Whisper and frames using MoviePy at 3 fps. These inputs are passed through frozen CLIP encoders to obtain text and image embeddings, which are then processed through an Image Segment Encoder with Contextual Attention (ISEC) to capture temporal dependencies between video frames. The fused multimodal features are passed through a 16-layer transformer classifier that jointly predicts both aspect categories and complaint status in a single multitask output matrix. The model was trained on 369 videos and tested on 64, with performance evaluated using Macro-F1, Micro-F1, and Hamming Loss metrics.

## Key Results
- Solution 3.0 outperforms baseline models including CLIP, BERT, and Longformer across most metrics
- ISEC module provides significant performance gains, with 4-5% improvement when included
- Audio-only features consistently outperform video-only features, validating the text-centric fusion approach
- The model achieves high performance on Transaction and Customer Service aspects but struggles with minority classes like Miscellaneous

## Why This Works (Mechanism)

### Mechanism 1: Contextual Visual Segment Encoding (ISEC)
The ISEC module captures temporal dependencies between video frames more effectively than standard frozen encoders. It processes CLIP-generated image embeddings through stacked transformer blocks with multi-head attention, creating a trainable pathway that refines visual features before fusion. This allows the model to weigh the importance of specific visual segments relative to the global context. Performance drops 4-5% when ISEC is removed, confirming its critical role.

### Mechanism 2: Audio-Visual Asymmetric Fusion
The architecture exploits the finding that audio-transcribed text carries more distinct complaint signals than video alone. Audio acts as the primary semantic anchor while video frames provide contextual grounding. Audio-only consistently outperforms video-only (e.g., Service Types Macro-F1 70.46 vs 62.67), suggesting the fusion mechanism verifies or enriches the audio signal rather than treating modalities equally.

### Mechanism 3: Joint Multitask Label Encoding
Structuring the output as a joint multitask problem forces the model to learn shared representations that improve detection of implicit complaints. The 5×3 output matrix simultaneously predicts aspect presence and complaint status, allowing the model to associate specific phrases or visual cues with complaint severity directly rather than as separate pipeline steps.

## Foundational Learning

- **Concept: CLIP (Contrastive Language-Image Pre-training)**
  - **Why needed here:** CLIP maps images and text into the same vector space, enabling alignment between spoken complaints and visual frames
  - **Quick check question:** If the video shows a "0% Interest" ad but the audio says "They charged me fees," which modality does the frozen CLIP layer struggle to reconcile without the ISEC module?

- **Concept: Self-Attention and Positional Encoding**
  - **Why needed here:** ISEC adds Positional Encoding to image embeddings before Multi-Head Attention to track which frame came first, essential for narrative complaints
  - **Quick check question:** Why is Positional Encoding necessary for the image segment encoder but not necessarily for a standard CNN feature extractor?

- **Concept: Multilabel vs. Multiclass Classification**
  - **Why needed here:** A single video can contain multiple aspects (e.g., both "Customer Service" and "Transaction" issues), requiring the loss function and output structure to handle this overlap
  - **Quick check question:** Can the output vector [0, 2, 2, 0, 0] exist legally in this architecture? (Hint: Check Page 4 Eq. 1)

## Architecture Onboarding

- **Component map:** Video Review → Whisper (Audio→Text) + MoviePy (Video→Frames) → CLIP (Frozen) → ISEC (Trainable) → Concatenate → 16-layer Transformer → Multitask Classifier (5×3 Matrix)

- **Critical path:** The ISEC module is the critical innovation. Removing ISEC causes performance to collapse, proving that frozen CLIP features alone are insufficient for nuanced temporal understanding required for complaint detection.

- **Design tradeoffs:**
  - Frozen vs. Trainable: Freeze CLIP to preserve general knowledge while adding ISEC to learn domain-specific visual sequences
  - Chunking: Videos chunked into 2-second intervals (6 frames) to reduce computational load but risks cutting off longer sentences mid-word

- **Failure signatures:**
  - Aspect Confusion: Struggles to distinguish "Miscellaneous" from "Service Type" when samples are sparse
  - Sarcasm: Explicitly listed as a limitation
  - Unimodal Bias: Video-only yields significantly lower Macro-F1 (e.g., 46.66 for Transactions) compared to Multimodal (57.27)

- **First 3 experiments:**
  1. Modality Ablation: Train Audio-Only, Video-Only, and Multimodal versions to validate data pipeline and confirm Whisper transcription quality
  2. ISEC vs. MLP Fusion: Replace ISEC transformer blocks with simple MLP to prove contextual nature is required, not just dimension mixing
  3. Long-tail Aspect Test: Evaluate specifically on "Miscellaneous" aspect to check if model defaults to majority class predictions

## Open Questions the Paper Calls Out

1. **Cross-lingual adaptation:** The model was trained and evaluated exclusively on English-language videos, necessitating additional training on different languages and code-mixed settings to broaden applicability

2. **Sarcasm detection:** Handling sarcasm remains challenging due to limited availability of curated sarcastic samples, as sarcasm often creates mismatch between content and intent

3. **Code-mixed languages:** The current reliance on English-centric pre-trained encoders leaves the model's utility in multilingual financial markets unproven

## Limitations
- The model struggles with minority aspects like "Miscellaneous" due to severe class imbalance and limited training samples
- Sarcasm detection remains challenging as the current fusion approach may not capture contradictions between positive words and complaint intent
- The fixed 5×3 output structure cannot accommodate new aspects that appear semantically distant from the trained five categories

## Confidence
- **High Confidence:** The overall framework architecture produces measurable performance gains across multiple metrics compared to baselines
- **Medium Confidence:** Performance numbers are reproducible given dataset split, but ISEC implementation details introduce uncertainty
- **Low Confidence:** Generalizability to new complaint aspects beyond the five defined categories is questionable due to fixed output structure

## Next Checks
1. **Parameter Sensitivity Analysis:** Vary the number of ISEC transformer blocks and hidden dimensions while measuring performance to identify minimum viable configuration
2. **Cross-Domain Robustness Test:** Apply the trained model to non-financial video complaints without fine-tuning to measure zero-shot transfer capability
3. **Minority Aspect Focus Evaluation:** Create an augmented test set by oversampling "Miscellaneous" aspect instances to evaluate whether performance improves or if model consistently defaults to majority class predictions