---
ver: rpa2
title: 'TED-LaST: Towards Robust Backdoor Defense Against Adaptive Attacks'
arxiv_id: '2506.10722'
source_url: https://arxiv.org/abs/2506.10722
tags:
- attacks
- samples
- adaptive
- target
- class
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the vulnerability of topological evolution
  dynamics (TED) to adaptive backdoor attacks in deep neural networks. The authors
  propose TED-LaST, which introduces label-supervised dynamics tracking and adaptive
  layer emphasis to enhance robustness.
---

# TED-LaST: Towards Robust Backdoor Defense Against Adaptive Attacks

## Quick Facts
- arXiv ID: 2506.10722
- Source URL: https://arxiv.org/abs/2506.10722
- Reference count: 40
- Key outcome: Proposes TED-LaST defense achieving precision over 90% and F1 scores above 85% against adaptive backdoor attacks on CIFAR-10, GTSRB, and ImageNet100

## Executive Summary
This paper addresses the vulnerability of topological evolution dynamics (TED) to adaptive backdoor attacks in deep neural networks. The authors propose TED-LaST, which introduces label-supervised dynamics tracking and adaptive layer emphasis to enhance robustness. These innovations enable detection of stealthy threats even when topological separability is compromised. Comprehensive experiments on CIFAR-10, GTSRB, and ImageNet100 datasets show TED-LaST achieves precision over 90% and F1 scores above 85% against various adaptive attacks, including their proposed enhanced adaptive attack.

## Method Summary
TED-LaST enhances the original TED defense by introducing two key innovations: label-supervised dynamics tracking using class-specific PCA-based outlier detectors, and adaptive layer emphasis using modularity-weighted features. The method computes topological evolution dynamics (TED) features for each layer, applies class-specific modularity weights to emphasize discriminative layers, and trains dedicated outlier detectors per class. At inference, it uses the detector corresponding to the predicted class to evaluate whether a sample deviates from the expected topological evolution pattern. This approach specifically addresses the limitation of original TED where adaptive attacks can exploit global topological space by targeting specific classes.

## Key Results
- Achieves precision over 90% and F1 scores above 85% against adaptive attacks on CIFAR-10, GTSRB, and ImageNet100 datasets
- Outperforms original TED by capturing subtle class-specific anomalies that evade global topological detection
- Demonstrates robustness against enhanced adaptive attacks combining Laundry, Slow Release, and Target Mapping strategies

## Why This Works (Mechanism)

### Mechanism 1: Class-Specific Outlier Detection
- Claim: Class-specific outlier detection restores separability that adaptive attacks destroy in global topological space
- Mechanism: TED-LaST trains dedicated PCA-based outlier detectors per class rather than one global detector. When a sample is predicted as class c, only the detector for class c evaluates it, using thresholds set from that class's benign distribution
- Core assumption: Malicious samples, even when successfully mimicking target class behavior globally, deviate from the specific distribution of genuinely clean samples in that target class
- Evidence anchors: [abstract] "label-supervised dynamics tracking... enable the identification of stealthy threats that evade traditional TED-based defenses, even in cases of inseparability in topological space"; [section VI-A, Figure 6] AUROC degrades as additional classes are included in training; single-class training yields highest scores

### Mechanism 2: Cumulative Topological Distance (CTD) Detection
- Claim: Malicious samples traverse larger cumulative topological distances than target-class natives, even when they successfully reach the target
- Mechanism: TED tracks K_l(x)—the rank of x's nearest neighbor from its predicted class at each layer. Malicious samples originating from source class s exhibit higher Cumulative Topological Distance (CTD = Σ|K_{l+1}(x) - K_l(x)|) because they must transition from s's topological region to the target's region across layers
- Core assumption: Backdoor learning does not create a "shortcut" path that bypasses normal topological evolution; poisoned samples still bear traces of their source-class origin in intermediate layers
- Evidence anchors: [section III-B1, Figure 2] CTD distributions show clean samples cluster at low values while malicious samples show higher CTD across all attack variants; [abstract] "capturing subtle class-specific anomalies"

### Mechanism 3: Modularity-Based Layer Weighting
- Claim: Modularity-based layer weighting amplifies discriminative layers and suppresses noisy layers, enabling detection of subtle perturbations
- Mechanism: For each layer l and class c, compute modularity Q_{l,c} on a KNN graph of activations, treating class-c vs. non-class-c as two communities. Higher modularity = clearer class separation at that layer. Weights w_{l,c} are normalized modularity scores
- Core assumption: The layers where clean samples show strong class clustering are also the layers where malicious samples' subtle deviations become most visible
- Evidence anchors: [section IV-B, Eq. 17-18] Explicit modularity computation and normalization procedure; [section VI-B, Figure 7] Modularity-weighted method outperforms non-weighted increasingly as CTD threshold ratio increases; [Figure 3] Box plots show improved clean/malicious separation after weighting

## Foundational Learning

### Concept: Topological Data Analysis (persistent homology, simplicial complexes)
- Why needed here: TED represents layer activations as topological spaces where "closeness" is defined via open balls and neighborhood structures. Understanding topology vs. metric distance clarifies why TED captures relational structure that Euclidean distances miss
- Quick check question: Can you explain why two samples with identical Euclidean distance to a reference point might have different topological rankings?

### Concept: Modularity and Community Detection in Graphs (Newman-Girvan modularity)
- Why needed here: The adaptive layer weighting mechanism directly computes modularity scores to quantify how well a layer's activations separate into class communities. Understanding Q = Σ[m_Ei/m - γ(k_i/2m)²] clarifies what the weighting actually measures
- Quick check question: If modularity Q_{l,c} = 0.8 for layer l and class c, what does this imply about the overlap between class-c activations and non-class-c activations at that layer?

### Concept: PCA-based Outlier Detection (reconstruction error, explained variance thresholds)
- Why needed here: TED-LaST uses class-specific PCA models with α-quantile thresholds. Understanding how PCA projects to principal components and how outliers are detected via reconstruction distance is essential for debugging detection failures
- Quick check question: If α=0.05 and your PCA model retains k components explaining 95% variance, how is the anomaly threshold actually computed?

## Architecture Onboarding

### Component map:
Preprocessing -> KNN graph construction -> Kl(x) computation -> Modularity weight computation -> Weighted TED* vector construction -> Class-specific PCA training -> Outlier detection

### Critical path:
1. Layer selection: Must include all Conv2D and Linear layers (not just final layers)
2. KNN graph construction: k=√|X| must balance local structure capture vs. noise
3. Modularity computation: Resolution parameter γ=1 (default); verify modularity is actually computed per-class, not globally
4. Threshold setting: α=0.05 controls false positive rate trade-off

### Design tradeoffs:
- Per-class vs. global detector: Per-class increases precision but requires sufficient clean samples per class (200 used in experiments)
- Number of training samples: Paper uses 200 clean samples/class; fewer samples may yield unreliable PCA models
- α parameter: Lower α reduces false positives but may miss subtle attacks; paper shows α=0.05 achieves 5% FPR baseline

### Failure signatures:
- High false positives on specific classes: Check if class has insufficient training samples; PCA model may be underfit
- Zero detection on known attack: Verify layer outputs are being extracted correctly; check if modularity weights are near-uniform (indicating no discriminative layers found)
- Performance drops on larger models: Paper tests ResNet20 and ResNet101 on ImageNet100; verify layer selection scales correctly

### First 3 experiments:
1. Baseline validation: Replicate CIFAR-10 + ResNet20 + Adap-Blend setup (Table II). Expected: Precision ≥94%, F1 ≥92%. If F1 <85%, check KNN graph construction and modularity computation
2. Ablation on training data composition: Follow Figure 6 protocol—train detectors with 0, 1, 3, 5, 7, 9 additional random classes. Expected: AUROC degrades monotonically. If not, class-specific assumption may not hold for your data
3. CTD threshold sensitivity test: Filter malicious samples by CTD threshold ratio (Figure 7). Test modularity-weighted vs. non-weighted on samples with CTD ratio > 1.0 (subtle perturbations). Expected: Weighted method shows larger AUROC gap at higher ratios

## Open Questions the Paper Calls Out
- How effective is TED-LaST when applied to federated learning scenarios where backdoor attacks may be distributed across multiple clients? The paper explicitly states "Future work could include... exploring the application of TED-LaST in federated learning or multi-modal learning scenarios."
- What is the computational overhead of TED-LaST compared to original TED and other sample-level defenses during both training and inference? The paper does not report computational costs despite introducing label-specific outlier detectors and modularity calculations for each class-layer pair.
- Can TED-LaST maintain effectiveness against stronger adaptive attackers who optimize specifically to evade both label-supervised tracking and modularity-based weighting? While the paper evaluates against proposed enhanced adaptive attacks, it does not consider white-box adaptive attacks that explicitly optimize against the full TED-LaST detection pipeline.

## Limitations
- The Enhanced Adaptive Attack variant combining multiple strategies may face deployment challenges in real-world federated learning scenarios
- Scalability to larger architectures (e.g., Vision Transformers) and datasets beyond ImageNet100 requires further validation
- Adaptive attack effectiveness against TED-LaST under practical constraints (trigger size, poison rate) remains underexplored beyond synthetic setups

## Confidence
- High: The core mechanism of class-specific PCA outlier detection and modularity-based layer weighting is well-supported by ablation studies (Figure 6, Figure 7)
- Medium: Generalization claims across diverse datasets (CIFAR-10, GTSRB, ImageNet100) are supported but may not extend to highly imbalanced or domain-shifted data
- Low: The proposed Enhanced Adaptive Attack's practical impact and attack success rate in real-world scenarios remain speculative

## Next Checks
1. Test TED-LaST on a larger, more diverse dataset (e.g., full ImageNet) to evaluate scalability and robustness under realistic constraints
2. Evaluate the Enhanced Adaptive Attack against TED-LaST in a federated learning environment with non-iid data distributions
3. Investigate the impact of varying poison rates (beyond 0.01) and trigger sizes on detection performance to assess practical limits