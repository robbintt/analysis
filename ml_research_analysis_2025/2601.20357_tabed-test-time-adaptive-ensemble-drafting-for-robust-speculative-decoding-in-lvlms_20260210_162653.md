---
ver: rpa2
title: 'TABED: Test-Time Adaptive Ensemble Drafting for Robust Speculative Decoding
  in LVLMs'
arxiv_id: '2601.20357'
source_url: https://arxiv.org/abs/2601.20357
tags:
- image
- drafting
- llav
- arxiv
- datasets
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Speculative decoding (SD) has been effective for accelerating large
  language models (LLMs) but remains underexplored for large vision-language models
  (LVLMs), which process both images and text. Existing SD methods using small draft
  models show performance fluctuations across diverse input scenarios.
---

# TABED: Test-Time Adaptive Ensemble Drafting for Robust Speculative Decoding in LVLMs

## Quick Facts
- arXiv ID: 2601.20357
- Source URL: https://arxiv.org/abs/2601.20357
- Reference count: 40
- Primary result: 1.74× walltime speedup over autoregressive decoding and 5% improvement over single drafting methods

## Executive Summary
Speculative decoding (SD) has been effective for accelerating large language models (LLMs) but remains underexplored for large vision-language models (LVLMs), which process both images and text. Existing SD methods using small draft models show performance fluctuations across diverse input scenarios. To address this, we propose Test-time Adaptive Batched Ensemble Drafting (TABED), which dynamically ensembles multiple drafts obtained via batch inference by leveraging deviations from past ground truths available in SD. TABED achieves an average robust walltime speedup of 1.74× over autoregressive decoding and a 5% improvement over single drafting methods, while remaining training-free and keeping ensembling costs negligible through parameter sharing. It is plug-and-play and can be further enhanced by integrating advanced verification and alternative drafting methods. Code and models are publicly available.

## Method Summary
TABED addresses the performance fluctuations of speculative decoding in LVLMs by dynamically ensembling multiple draft models at test time. The method uses batch inference to obtain parallel drafts, then applies adaptive weights based on KL divergence between draft and target distributions over a sliding window of past tokens. This approach leverages the availability of past ground truths in SD to select optimal weights without additional training. Draft models share parameters to minimize overhead, and the method is designed to be plug-and-play with existing LVLM pipelines.

## Key Results
- Achieves 1.74× walltime speedup over autoregressive decoding
- Improves block efficiency by 5% compared to single drafting methods
- Maintains training-free operation with negligible ensembling overhead (<1% latency)

## Why This Works (Mechanism)
TABED works by exploiting the availability of past ground truths in speculative decoding to dynamically adjust draft model weights at test time. By computing KL divergence between draft and target distributions over recent tokens, it can identify which draft model is performing better for the current input context and adjust weights accordingly. The use of batch inference for parallel drafting minimizes overhead, while parameter sharing between draft models keeps memory costs low.

## Foundational Learning

**Speculative Decoding**: A technique where a small draft model generates candidate tokens, which are then verified by a larger target model. Needed to understand the baseline framework that TABED enhances. Quick check: Verify that the draft-target pair relationship is properly implemented with appropriate latency ratios.

**KL Divergence**: Measures the difference between two probability distributions. Used in TABED to compare draft and target model outputs. Quick check: Ensure KL divergence computation is numerically stable and properly handles edge cases.

**Batch Inference**: Running multiple draft models in parallel to generate predictions simultaneously. Critical for TABED's efficiency. Quick check: Confirm batch sizes and parallelization are optimized for target hardware.

**Parameter Sharing**: Technique where multiple models share weights to reduce memory overhead. TABED uses this for its draft models. Quick check: Verify parameter sharing is correctly implemented without unintended interference between models.

## Architecture Onboarding

**Component Map**: Input → Draft Models (M, T) → Ensemble Layer → KL Divergence Calculation → Weight Selection → Output → Target Verification

**Critical Path**: The most time-critical sequence is: Batch draft inference → KL divergence computation → Weight selection → Token sampling. Each step must be optimized for latency.

**Design Tradeoffs**: TABED trades slight increase in computational complexity (KL divergence calculations) for significant gains in robustness and speedup. The parameter sharing reduces memory overhead but requires careful implementation to avoid conflicts.

**Failure Signatures**: If block efficiency is lower than reported, check that the text-only drafting correctly replaces image tokens. If overhead is excessive, verify cached reuse of past distributions. If caption drafting adds latency, ensure parallel execution with target prefilling.

**First Experiments**:
1. Reproduce single-method drafting block efficiencies on LLaVA-Bench to establish baseline performance
2. Implement weight sampling policy SW and validate KL divergence computation
3. Measure actual latency ratios on target hardware to calibrate speedup calculations

## Open Questions the Paper Calls Out
None

## Limitations
- Performance depends on hardware-specific latency ratios that may vary across GPU configurations
- Generalization to LVLM architectures beyond LLaVA is not validated
- Exact implementation details of weight sampling policy SW require code inspection for precise replication

## Confidence
- Walltime speedup (1.74×) and block efficiency improvements: High confidence
- Training-free nature and negligible overhead: High confidence
- Plug-and-play integration: Medium confidence

## Next Checks
1. Reproduce single-method drafting block efficiencies on LLaVA-Bench and DocVQA to verify baseline performance
2. Implement and validate the weight sampling policy SW, ensuring KL divergence computation uses correctly cached distributions
3. Measure actual latency ratio Tq/Tp on target hardware to adjust speedup calculations and verify the 1.74× improvement holds across different GPU configurations