---
ver: rpa2
title: When Shift Happens - Confounding Is to Blame
arxiv_id: '2505.21422'
source_url: https://arxiv.org/abs/2505.21422
tags:
- shift
- information
- confounding
- label
- test
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work investigates the impact of hidden confounding shifts
  on out-of-distribution (OOD) generalization in machine learning. It shows that conventional
  approaches focusing on invariant relationships fail under hidden confounding shifts,
  which violate common assumptions made by existing OOD generalization methods.
---

# When Shift Happens - Confounding Is to Blame

## Quick Facts
- arXiv ID: 2505.21422
- Source URL: https://arxiv.org/abs/2505.21422
- Authors: Abbavaram Gowtham Reddy; Celia Rubio-Madrigal; Rebekka Burkholz; Krikamol Muandet
- Reference count: 40
- Primary result: Traditional OOD methods fail under hidden confounding; environment-specific relationships are theoretically optimal

## Executive Summary
This work demonstrates that hidden confounding shifts fundamentally break assumptions of existing out-of-distribution generalization methods. When unobserved confounders affect both inputs and outputs, shifts in their distribution simultaneously induce label, covariate, conditional covariate, and concept shifts that violate the invariance assumptions of methods like IRM and covariate shift adaptation. The authors show that maximizing predictive information requires learning environment-specific relationships rather than enforcing invariant representations, and that utilizing all available covariates (including informative non-causal proxies) improves generalization by reducing concept shift while increasing conditional informativeness.

## Method Summary
The paper combines theoretical analysis with empirical validation across eight TableShift benchmark datasets and synthetic data. Methods tested include XGBoost, MLP, GroupDRO, IRM, and VREX. Key innovations include a mutual information decomposition that quantifies different shift types, theoretical propositions showing why environment-specific relationships are optimal under hidden confounding, and extensive experiments demonstrating that ERM with all covariates outperforms invariant methods when hidden confounding is present. Mutual information terms are estimated using nonparametric entropy estimation on held-out samples.

## Key Results
- Hidden confounding is prevalent in real-world datasets (all non-zero shift terms in Table 2)
- ERM using all covariates outperforms causal-only models and invariant methods under hidden confounding
- Learning environment-specific relationships (XGBoost) positively correlates with both ID and OOD accuracy
- Adding informative proxy covariates reduces concept shift and improves OOD generalization

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Traditional OOD methods fail under hidden confounding because confounding shifts simultaneously induce multiple shift types that violate invariance assumptions.
- **Mechanism:** When hidden confounder U causes both X and Y, shifts in P(U) cascade through the causal graph, breaking the conditional invariances (P(Y|X) or P(X|Y)) that methods like IRM and covariate shift adaptation rely on.
- **Core assumption:** Hidden confounders exist and their distributions shift across environments.
- **Evidence anchors:** Abstract states shifts in hidden confounding violate common OOD assumptions; Section 3 shows simultaneous occurrence of multiple shift types.
- **Break condition:** If hidden confounders do not exist or P(U) is constant across environments, traditional invariance-based methods should work as originally designed.

### Mechanism 2
- **Claim:** Under hidden confounding shift, maximizing predictive information requires maximizing conditional informativeness I(ϕ(X);Y|E) minus residual I(ϕ(X);Y|Ŷ), not enforcing invariant representations.
- **Mechanism:** Proposition 4.2 decomposes predictive information I(Y;Ŷ) = I(ϕ(X);Y|E) - I(ϕ(X);Y|Ŷ) for hidden confounding scenarios, showing learning environment-specific input-output mappings is theoretically optimal.
- **Core assumption:** Either X→Y or Y→X causal structure holds.
- **Evidence anchors:** Abstract states maximizing predictive information requires learning environment-specific relationships; Section 5, Figure 4 shows XGB achieves highest conditional informativeness and best accuracy.
- **Break condition:** If variation/label shift/feature shift/concept shift terms don't cancel, the decomposition reverts to the full Equation 1 with multiple competing terms.

### Mechanism 3
- **Claim:** Adding informative non-causal covariates improves OOD generalization by increasing conditional informativeness while reducing concept shift.
- **Mechanism:** Proposition 4.3 proves informative covariates XI increase I(ϕ(X);Y|E) and I(ϕ(X);E) while decreasing I(Y;E|ϕ(X)), explaining why ERM using all covariates outperforms causal-only models.
- **Core assumption:** Proxies XI are genuinely informative and measurable.
- **Evidence anchors:** Abstract states OOD generalization improves when all available covariates are utilized; Section 5, Figure 5 shows adding proxy variables reduces MSE and concept shift.
- **Break condition:** If added covariates are noise, they increase feature shift without reducing concept shift, potentially degrading performance.

## Foundational Learning

- **Concept: Hidden confounding in causal graphs**
  - **Why needed here:** The entire paper hinges on understanding how unobserved confounders U that simultaneously cause X and Y disrupt traditional OOD assumptions.
  - **Quick check question:** Draw a causal graph with U→X, U→Y, X→Y. If P(U) shifts between environments, which of P(Y|X), P(X|Y), P(X), P(Y) remain invariant?

- **Concept: Mutual information decomposition**
  - **Why needed here:** The paper uses mutual information terms to quantify shifts and to decompose predictive information.
  - **Quick check question:** Given I(Y;Ŷ) = I(ϕ(X);Y|E) - I(ϕ(X);E|Y)/2 + I(Y;E)/2 + I(ϕ(X);E)/2 - I(Y;E|ϕ(X))/2 - I(ϕ(X);Y|Ŷ), which terms would you maximize vs. minimize for better generalization?

- **Concept: Simpson's paradox and collider conditioning**
  - **Why needed here:** Figure 2 demonstrates how pooling data across environments with hidden confounding learns incorrect X-Y relationships, and Section 4.1 explains how conditioning on colliders opens causal paths.
  - **Quick check question:** In X→Y←U→X, conditioning on Y opens what path? What bias does this introduce?

## Architecture Onboarding

- **Component map:** Training data from multiple environments e∈E_tr; covariates partitioned into causal, arguably causal, and all; mutual information estimator using nonparametric entropy estimation; feature extractor ϕ (neural net layer or XGBoost margins+SHAP); environment encoder (optional); predictor f.

- **Critical path:**
  1. Detect hidden confounding shift by computing I(X;E|Y), I(Y;E), I(X;E), I(Y;E|X) on held-out data
  2. If concept shift I(Y;E|X) > 0, invariance-based methods will fail; prioritize conditional informativeness
  3. Identify proxy covariates correlated with Y but not causally related
  4. Train with all available covariates including proxies; ERM or XGBoost often outperforms IRM/VREX under confounding

- **Design tradeoffs:**
  - **ERM vs. IRM/VREX:** ERM + all covariates wins when hidden confounding is present; IRM/VREX win only when true invariance exists
  - **Causal-only vs. all covariates:** All covariates reduce concept shift but increase variation; tradeoff favors all covariates when concept shift reduction dominates
  - **Proxy quality vs. cost:** High-noise proxies provide less benefit; acquisition cost may not justify marginal gains

- **Failure signatures:**
  - **Invariant methods underperform ERM:** Strong signal of hidden confounding shift (verify with I(Y;E|X) > 0)
  - **Concept shift high (I(Y;E|X) >> 0):** Environment-specific relationships needed; single invariant predictor will fail
  - **OOD accuracy collapses while ID accuracy remains high:** Model exploiting environment-specific shortcuts that don't transfer
  - **Adding covariates degrades performance:** Covariates are not informative; they're pure noise

- **First 3 experiments:**
  1. **Quantify shift types** on your dataset: Compute I(X;E|Y), I(Y;E), I(X;E), I(Y;E|X) using 10-20K samples. If all non-zero, hidden confounding is present.
  2. **Compare ERM vs. IRM vs. VREX** with causal-only vs. all-covariate subsets. If ERM + all covariates wins, your data has hidden confounding.
  3. **Identify and add proxy covariates:** Use domain knowledge to find variables correlated with potential hidden confounders. Retrain and verify concept shift I(Y;E|X) decreases per Proposition 4.3(iii).

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can OOD generalization algorithms be designed to remain effective when confounder support is limited or non-overlapping between training and test distributions?
- **Basis in paper:** The authors explicitly identify "developing algorithms effective under limited confounder overlap" as a key challenge in Section 6.
- **Why unresolved:** Current approaches typically assume overlapping confounder support, and synthetic experiments demonstrate performance degradation in low-overlap regimes.
- **What evidence would resolve it:** An algorithm that maintains high OOD accuracy on the low-overlap synthetic datasets without relying on the overlapping support assumption.

### Open Question 2
- **Question:** How can models handle the entanglement of label, covariate, and concept shifts without relying on untestable assumptions about proxy variables?
- **Basis in paper:** Section 6 lists "handling entangled shifts without relying on untestable proxy assumptions" as a key challenge.
- **Why unresolved:** While the paper proves that proxies can mitigate confounding, verifying the validity of these proxy structures is often impossible with real-world data alone.
- **What evidence would resolve it:** A method that achieves robust performance on the TableShift benchmarks using only available covariates, without requiring a priori identification of specific proxy structures.

### Open Question 3
- **Question:** What is the optimal trade-off between the cost of acquiring informative covariates and the resulting improvement in OOD generalization?
- **Basis in paper:** The authors call for "quantifying the cost–accuracy trade-off of acquiring informative covariates" in Section 6.
- **Why unresolved:** The paper demonstrates that adding informative covariates improves accuracy but does not provide a framework for deciding which covariates are worth acquiring given resource constraints.
- **What evidence would resolve it:** A theoretical or empirical framework that links the marginal cost of feature acquisition to the "conditional informativeness" metric defined in the paper.

## Limitations

- The theoretical claims rely on mutual information decompositions that may not translate cleanly to finite-sample, high-dimensional settings where entropy estimation becomes unreliable.
- The assumption that 11 of 16 benchmark datasets follow X→Y causal structure is based on qualitative GPT-4o analysis rather than formal causal discovery.
- The paper doesn't address scenarios where proxy variables are unavailable or when the cost of acquiring them exceeds their benefit.

## Confidence

- **High confidence**: The empirical finding that hidden confounding is prevalent in real-world data and that ERM with all covariates often outperforms invariant methods under confounding.
- **Medium confidence**: The theoretical decomposition in Proposition 4.2 and its implications for learning environment-specific relationships.
- **Low confidence**: The causal structure analysis claiming X→Y in 11/16 datasets based on qualitative assessment.

## Next Checks

1. **Formal causal discovery validation**: Apply causal discovery algorithms (e.g., PC, FCI) to the benchmark datasets to independently verify the claimed X→Y causal structures and identify potential bidirectional effects or feedback loops.

2. **Robustness to proxy quality**: Systematically vary proxy covariate informativeness (noise levels) in synthetic experiments to quantify the tradeoff between concept shift reduction and feature shift introduction, identifying thresholds where benefits plateau.

3. **Transfer to new domains**: Test the methodology on healthcare or finance datasets with known confounding structures where domain experts can validate whether learned environment-specific relationships make clinical/financial sense.