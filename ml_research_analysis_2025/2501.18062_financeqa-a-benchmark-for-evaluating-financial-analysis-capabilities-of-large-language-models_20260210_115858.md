---
ver: rpa2
title: 'FinanceQA: A Benchmark for Evaluating Financial Analysis Capabilities of Large
  Language Models'
arxiv_id: '2501.18062'
source_url: https://arxiv.org/abs/2501.18062
tags:
- financial
- questions
- llms
- data
- financeqa
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: FinanceQA is a benchmark that evaluates large language models (LLMs)
  on complex numerical financial analysis tasks mirroring real-world investment work.
  Current LLMs fail to meet the strict accuracy requirements of financial institutions,
  with models failing approximately 60% of realistic tasks that mimic on-the-job analyses
  at hedge funds, private equity firms, investment banks, and other financial institutions.
---

# FinanceQA: A Benchmark for Evaluating Financial Analysis Capabilities of Large Language Models

## Quick Facts
- arXiv ID: 2501.18062
- Source URL: https://arxiv.org/abs/2501.18062
- Authors: Spencer Mateega; Carlos Georgescu; Danny Tang
- Reference count: 1
- Key outcome: Current LLMs fail approximately 60% of realistic financial tasks, with even state-of-the-art models like GPT-4o and o1 achieving only 39.2% and 48.7% accuracy respectively on FinanceQA tasks.

## Executive Summary
FinanceQA is a benchmark designed to evaluate large language models on complex numerical financial analysis tasks that mirror real-world investment work. The benchmark reveals that current LLMs fail to meet the strict accuracy requirements of financial institutions, particularly struggling with assumption generation under incomplete information and adherence to accounting conventions. Fine-tuning GPT-4o on FinanceQA data improves performance by 44.9% overall, with a 690.9% improvement on assumption-based questions, demonstrating the importance of high-quality domain-specific training data.

## Method Summary
The FinanceQA benchmark evaluates LLMs using three question types: tactical-basic, tactical-assumption, and conceptual. Questions are based on real financial statements and accounting practices, with tactical questions including curated 10-K excerpts as context. The benchmark uses exact match scoring with manual verification by domain experts. The fine-tuning dataset consists of 9,078 rows (human + synthetic), and models are evaluated on held-out questions with binary correct/incorrect scoring.

## Key Results
- LLMs achieve only 39.2% (GPT-4o) and 48.7% (o1) accuracy on FinanceQA tasks
- Assumption-based questions show the largest gap, with pre-fine-tuning accuracy of only 2.2%-4.3%
- Fine-tuning improves performance by 44.9% overall, with 690.9% improvement on assumption-based questions
- Models struggle with hand-spreading metrics, following standard accounting conventions, and making assumptions with incomplete information

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Fine-tuning on high-quality, domain-specific reasoning data significantly improves LLM performance on professional financial tasks, particularly for assumption generation under incomplete information.
- Mechanism: Fine-tuning exposes the model to reasoning patterns and calculation methodologies that mirror actual analyst workflows, with the 690.9% improvement on assumption-based questions suggesting the training data encodes heuristics for constructing logical assumptions when direct data is unavailable.
- Core assumption: The reasoning patterns learned from fine-tuning generalize to similar but unseen financial analysis scenarios.
- Evidence anchors: Fine-tuning GPT-4o on FinanceQA data improves performance by 44.9% overall, including a 690.9% improvement on assumption-based questions.
- Break condition: If evaluation questions have significant overlap with training data or if financial accounting standards change substantially post-training.

### Mechanism 2
- Claim: Segmented benchmark design (tactical-basic, tactical-assumption, conceptual) enables diagnostic identification of distinct capability gaps rather than measuring a monolithic "financial reasoning" skill.
- Mechanism: Low correlation between question types indicates different underlying capabilities, with conceptual questions testing logic/math similar to standard LLM training while tactical questions test precise calculation workflows.
- Core assumption: Weak inter-category correlation reflects genuinely distinct skills rather than measurement noise or benchmark construction artifacts.
- Evidence anchors: Correlation matrix shows no strong correlation between question types, validating that FinanceQA effectively evaluates diverse aspects of financial reasoning.
- Break condition: If high correlation emerges between question types in future model generations, or if question categorization proves inconsistent across annotators.

### Mechanism 3
- Claim: Synthetic data augmentation that preserves reasoning pathways while varying numerical values and financial statement structures can expand training coverage cost-effectively.
- Mechanism: Generating multiple variations of human-annotated questions teaches the model underlying reasoning patterns rather than specific example memorization, while maintaining logical coherence.
- Core assumption: Synthetic variations maintain reasoning quality equivalent to human annotation, and the augmentation doesn't introduce systematic artifacts the model might exploit.
- Evidence anchors: Synthetic data augmentation used to cover different cases and prevent overfitting, with ultimate fine-tuning dataset consisting of 9,078 rows.
- Break condition: If synthetic variations embed reasoning errors, or if the model learns to distinguish synthetic from real examples and treats them differently.

## Foundational Learning

- Concept: **Hand-spreading / Metric Recalculation from Primary Sources**
  - Why needed here: LLMs fail to independently recalculate metrics from raw SEC filing data, instead relying on pre-computed values that lack auditability.
  - Quick check question: Given a 10-K with RSUs, PSUs, convertible bonds, warrants, and stock options scattered across different sections, can you identify which instruments are dilutive under current market conditions and trace the step-by-step calculation to verify reported diluted shares?

- Concept: **Stock-Flow Averaging in Accounting Conventions**
  - Why needed here: LLMs frequently apply naive formulas when accounting standards require averaging, causing systematic errors in metrics like Accounts Payable Days.
  - Quick check question: Why does the correct Accounts Payable Days calculation use average AP over the period rather than year-end AP? What principle does this preserve?

- Concept: **Assumption Generation Under Incomplete Information**
  - Why needed here: This represents the largest capability gap. Professional analysts routinely estimate missing values using logical proxies; LLMs lack this experiential knowledge.
  - Quick check question: A company's 10-K reports variable lease costs but not variable lease assets. Construct a reasonable assumption to estimate the assetsâ€”what data would you use and what logical relationship would you assume?

## Architecture Onboarding

- Component map: Domain expert question generation -> Context window curation -> Synthetic variation generation -> Fine-tuning on 9,078 examples -> Binary exact match evaluation
- Critical path: 1. Domain expert question generation from professionals 2. Context window curation with relevant 10-K excerpts 3. Synthetic variation generation preserving reasoning 4. Fine-tuning using OpenAI API 5. Evaluation on held-out FinanceQA with exact match scoring
- Design tradeoffs: Synthetic augmentation vs. pure human annotation (budget-driven); curated vs. full-document context (reduces hallucination but limits navigation testing); single-company tactical questions (controls variability but limits industry generalization); binary exact match (clear scoring but no partial credit)
- Failure signatures: High conceptual + near-zero assumption accuracy indicates lack of experiential domain knowledge; correct formula but wrong convention application shows surface-level pattern matching; missed EBITDA adjustments indicates lack of context-sensitive judgment
- First 3 experiments: 1. Baseline diagnostic to identify which capability gap dominates 2. Context ablation with full 10-K vs. curated sections to measure hallucination rates 3. Progressive synthetic scaling with 2x, 4x, 8x multipliers to identify where returns diminish

## Open Questions the Paper Calls Out
- Can models fine-tuned on FinanceQA generalize to industries with fundamentally different financial structures, such as healthcare or energy? (Section 8 limitations note tactical questions were limited to Costco data)
- How do LLMs perform on generating and manipulating complex financial models (e.g., DCF, LBO) in Excel, which is necessary for practical adoption? (Section 8 notes this area was not tested but is crucial)
- Does a fully human-annotated fine-tuning dataset yield significantly higher accuracy than the synthetic-augmented dataset used in the study? (Section 8 suggests full human annotation would likely yield higher accuracy)

## Limitations
- Data generalization concerns with synthetic augmentation lacking independent validation
- Context window design tradeoff doesn't reflect real analyst workflows requiring document navigation
- Reproducibility constraints due to missing implementation details (train/test splits, generation rules, hyperparameters)

## Confidence
- High Confidence: Core finding that LLMs fail on assumption-based questions (2.2%-4.3% accuracy) is well-supported
- Medium Confidence: 44.9% overall improvement from fine-tuning is documented but mechanisms driving 690.9% assumption improvement remain unclear
- Low Confidence: Synthetic data augmentation methodology and its ability to preserve reasoning quality are largely speculative

## Next Checks
1. Independent replication using FinanceQA data with completely new company not present in any training or evaluation set
2. Context ablation study comparing full 10-K documents versus curated excerpts to quantify hallucination vs. navigation tradeoff
3. Synthetic variation analysis testing whether model performance on synthetic-generated questions transfers to human-annotated scenarios