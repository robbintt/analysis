---
ver: rpa2
title: 'X-MAS: Towards Building Multi-Agent Systems with Heterogeneous LLMs'
arxiv_id: '2505.16997'
source_url: https://arxiv.org/abs/2505.16997
tags:
- llms
- qwen-2
- qwen2
- b-instruct
- heterogeneous
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces X-MAS, a framework for building multi-agent
  systems (MAS) with heterogeneous large language models (LLMs), to overcome the limitations
  of existing MAS frameworks that rely on a single LLM. X-MAS leverages diverse LLMs
  to harness collective intelligence and improve system performance.
---

# X-MAS: Towards Building Multi-Agent Systems with Heterogeneous LLMs

## Quick Facts
- arXiv ID: 2505.16997
- Source URL: https://arxiv.org/abs/2505.16997
- Reference count: 40
- Multi-agent systems with heterogeneous LLMs outperform single LLM systems by up to 47% in mixed chatbot-reasoner scenarios

## Executive Summary
X-MAS introduces a framework for building multi-agent systems (MAS) using heterogeneous large language models (LLMs), addressing the limitations of existing MAS frameworks that rely on single LLMs. Through comprehensive benchmarking of 27 LLMs across five domains and five MAS functions with over 1.7 million evaluations, the study demonstrates that no single LLM excels across all scenarios. The proposed X-MAS-Design methodology achieves significant performance improvements by leveraging diverse LLM capabilities, showing up to 8.4% improvement in chatbot-only scenarios and 47% in mixed chatbot-reasoner scenarios on the AIME dataset.

## Method Summary
The X-MAS framework consists of two main components: X-MAS-Bench and X-MAS-Design. X-MAS-Bench provides a comprehensive evaluation framework testing 27 LLMs across mathematics, coding, science, medicine, and finance domains, measuring performance on question-answering, revise, aggregation, planning, and evaluation functions. This benchmarking revealed that different LLMs exhibit strengths in different domains and functions, with smaller models sometimes outperforming larger ones in specific tasks. Based on these insights, X-MAS-Design introduces a methodology for constructing heterogeneous LLM ensembles tailored to specific MAS requirements, transitioning from homogeneous to heterogeneous LLM-driven systems to optimize collective performance.

## Key Results
- Heterogeneous LLM ensembles achieved up to 47% performance improvement over single LLM systems in mixed chatbot-reasoner scenarios
- Smaller LLMs can outperform larger models in niche areas, challenging the assumption that bigger models are always better
- No single LLM excelled across all tested scenarios, validating the need for diverse model ensembles in MAS

## Why This Works (Mechanism)
The effectiveness of heterogeneous LLM ensembles stems from leveraging complementary strengths across different models. Rather than relying on a single model's capabilities, X-MAS strategically combines models with different specializations - some excel at reasoning while others perform better at creative tasks or domain-specific knowledge. This approach mimics collective intelligence, where diverse perspectives and capabilities lead to better overall performance than any individual component. The framework's ability to identify and deploy the right model for each specific MAS function allows for optimization that single-model systems cannot achieve.

## Foundational Learning

**LLM Specialization** - Understanding that different LLMs have distinct strengths and weaknesses across domains
- Why needed: Different models are trained on different datasets and fine-tuned for various purposes, leading to specialization
- Quick check: Compare performance of different models on domain-specific benchmarks

**Collective Intelligence** - The principle that groups of diverse agents can outperform individual agents
- Why needed: Provides theoretical foundation for why heterogeneous ensembles work better than homogeneous ones
- Quick check: Measure ensemble performance vs. best individual model performance

**Task Decomposition** - Breaking complex MAS functions into subtasks that can be handled by specialized models
- Why needed: Enables optimal allocation of different models to different components of MAS workflows
- Quick check: Analyze task performance when different models handle different subtasks

**Model Selection Criteria** - Methods for choosing appropriate LLMs for specific MAS functions
- Why needed: Critical for building effective heterogeneous ensembles
- Quick check: Validate selection criteria through ablation studies

## Architecture Onboarding

**Component Map:** User Query -> X-MAS-Design (Model Selection) -> Heterogeneous LLM Ensemble -> MAS Output
**Critical Path:** Input → Model Selection → Specialized LLM Processing → Result Aggregation → Output
**Design Tradeoffs:** Model diversity vs. coordination overhead; performance vs. computational cost; specialization vs. generalization
**Failure Signatures:** Single model bias, coordination bottlenecks, inconsistent outputs across models, increased latency
**First Experiments:**
1. Benchmark single LLM performance across all test domains and functions
2. Test simple heterogeneous ensemble combining top-performing models from different domains
3. Implement X-MAS-Design model selection algorithm and compare against random ensemble selection

## Open Questions the Paper Calls Out

The paper identifies several open questions regarding the scalability and practical deployment of heterogeneous LLM systems, including how to dynamically adapt model selection based on real-time performance, the optimal size and composition of LLM ensembles for different MAS applications, and the long-term economic viability of maintaining multiple specialized models versus investing in larger general-purpose models.

## Limitations

- Evaluation focuses on specific domains and tasks that may not generalize to all MAS applications
- Performance improvements measured against specific baselines and datasets, with relative advantages potentially diminishing as single LLMs improve
- Computational and coordination overhead of managing multiple LLMs not fully characterized, potentially limiting practical deployment in resource-constrained environments

## Confidence

- Heterogeneous LLMs improve MAS performance over single LLM systems: High confidence
- No single LLM excels across all scenarios: High confidence
- Smaller LLMs can outperform larger ones in niche areas: Medium confidence
- X-MAS-Design methodology provides significant improvements: Medium confidence

## Next Checks

1. Test X-MAS framework performance on real-world MAS applications beyond the controlled AIME benchmark, including open-ended multi-turn dialogues and collaborative problem-solving tasks
2. Conduct cost-benefit analysis comparing computational overhead, latency, and resource utilization of heterogeneous vs. homogeneous LLM systems across different scale deployments
3. Evaluate model transferability by testing whether performance patterns observed in X-MAS-Bench domains extend to completely different domains (e.g., creative writing, strategic planning, or scientific discovery)