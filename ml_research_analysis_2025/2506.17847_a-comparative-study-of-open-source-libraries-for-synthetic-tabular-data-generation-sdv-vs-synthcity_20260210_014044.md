---
ver: rpa2
title: 'A Comparative Study of Open-Source Libraries for Synthetic Tabular Data Generation:
  SDV vs. SynthCity'
arxiv_id: '2506.17847'
source_url: https://arxiv.org/abs/2506.17847
tags:
- data
- synthetic
- real
- generation
- statistical
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study compared synthetic tabular data generators from two
  open-source libraries, SDV and Synthicity, using six models (Gaussian Copula, CTGAN,
  TVAE, and Bayesian Network) to evaluate their performance in generating synthetic
  data from a real-world energy consumption dataset. Models were tested in two settings:
  1:1 (generating equal-sized synthetic data) and 1:10 (generating ten times more
  synthetic data).'
---

# A Comparative Study of Open-Source Libraries for Synthetic Tabular Data Generation: SDV vs. SynthCity

## Quick Facts
- arXiv ID: 2506.17847
- Source URL: https://arxiv.org/abs/2506.17847
- Reference count: 40
- Models compared across libraries on a real-world energy consumption dataset, finding Bayesian Networks and statistical models excelled in 1:1 generation while deep learning models better handled 1:10 scaling.

## Executive Summary
This study benchmarks synthetic tabular data generators from two open-source libraries, SDV and Synthicity, using six models (Gaussian Copula, CTGAN, TVAE, and Bayesian Network) to evaluate their performance on a real-world energy consumption dataset. Models were tested in two settings: 1:1 (generating equal-sized synthetic data) and 1:10 (generating ten times more synthetic data). Statistical similarity was measured using metrics like Kolmogorov-Smirnov and Wasserstein distances, while predictive utility was assessed via Train-on-Synthetic, Test-on-Real with regression models. Results show Bayesian Network from Synthicity achieved the highest statistical similarity (96.53) and predictive utility (0.97) in 1:1 generation, while TVAE from SDV performed best in the 1:10 setting with a predictive utility score of 0.31. Although SDV showed superior documentation and ease of use, no significant performance gap was found between the libraries.

## Method Summary
The study compares six generative models from SDV and Synthicity libraries on a real-world energy consumption dataset. Two experimental settings are used: 1:1 (generating synthetic data equal in size to the real data) and 1:10 (generating ten times more synthetic data). Models are evaluated using statistical similarity metrics (mean, median, standard deviation, KS distance, Wasserstein distance) and predictive utility via a Train-on-Synthetic, Test-on-Real paradigm with regression models. The analysis focuses on understanding how different generative approaches (statistical vs. deep learning) perform under different scaling conditions.

## Key Results
- Bayesian Network from Synthicity achieved the highest fidelity in both 1:1 (96.53 similarity, 0.97 predictive utility) and 1:10 (68.89 similarity, 0.21 predictive utility) settings
- Statistical models (Bayesian Networks, Gaussian Copula) outperformed deep learning models in the 1:1 setting due to explicit probabilistic assumptions
- Deep learning models (TVAE) showed better generalization in the 1:10 setting, maintaining higher predictive utility
- CTGAN consistently underperformed in both settings with negative utility scores
- No significant performance gap was found between SDV and Synthicity libraries despite different implementations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Statistical models achieve higher predictive utility in 1:1 synthetic data generation because explicit probabilistic factorization preserves conditional dependencies without overfitting.
- Mechanism: Bayesian Networks factorize joint distributions as P(X₁,...,Xₙ) = ∏ᵢ P(Xᵢ|Parents(Xᵢ)), directly encoding observed conditional dependencies. Gaussian Copulas separate marginal distributions from dependency structures via Sklar's theorem. Both avoid latent-space interpolation that can distort rare but predictive patterns.
- Core assumption: The true data-generating process can be approximated by the parametric forms these models assume (Gaussian dependencies, DAG structure).
- Evidence anchors:
  - [abstract] "The Bayesian Network from Synthicity achieved the highest fidelity in both scenarios"
  - [page 17-18] "Statistical models such as Gaussian Copula and Bayesian Network outperformed deep learning-based models... superior performance can be attributed to explicit probabilistic assumptions... well-suited for capturing underlying distributions in smaller datasets"
  - [corpus] Related surveys (arXiv:2507.11590, arXiv:2503.05954) confirm statistical methods remain competitive for small tabular datasets, though corpus lacks direct comparative benchmarks with identical methodology.
- Break condition: When data has complex non-linear interactions that cannot be captured by DAG factorization or Gaussian copula assumptions, predictive utility degrades (observed in CTGAN's superior handling of multi-modal distributions per page 8-9).

### Mechanism 2
- Claim: Deep learning models better maintain predictive utility when scaling to 10× input size because learned latent representations can interpolate within learned manifolds rather than extrapolating beyond training support.
- Mechanism: TVAE's encoder learns qφ(z|x) mapping data to latent space; decoder pθ(x|z) reconstructs from latent samples. When generating 10× more data, sampling from the prior N(0,I) and decoding produces diverse outputs within the learned distribution manifold, rather than repeating training patterns or generating out-of-distribution samples.
- Core assumption: The latent space learned from 1,000 rows sufficiently covers the true data manifold to support 10× sampling without mode collapse.
- Evidence anchors:
  - [abstract] "TVAE from SDV performed best in predictive tasks under the 1:10 setting"
  - [page 18] "Deep learning models like TVAE are designed to capture intricate, high-dimensional patterns... more adept at generalizing from limited inputs to generate larger synthetic datasets"
  - [page 10] TVAE formulation: "seeks to maximize Eqφ(z|x)[log pθ(x|z)] - DKL(qφ(z|x)‖p(z))... learning pθ(x|z) which can be sampled"
  - [corpus] No direct corpus evidence on 1:10 scaling behavior; this is an underexplored area.
- Break condition: If the latent space collapses or the encoder fails to capture multi-modal structure, 10× generation produces homogenized outputs (potentially explaining why TVAE's 0.31 score in 1:10 still lags far behind 1:1 performance of 0.97).

### Mechanism 3
- Claim: Predictive utility degrades faster than statistical similarity in 1:10 scaling because distributional metrics measure marginal alignment while predictive utility requires preserving joint conditional relationships critical for downstream tasks.
- Mechanism: Statistical similarity scores average column-wise metrics (mean, median, KS, Wasserstein). Marginal distributions can match well even when inter-column dependencies break down. Predictive utility (via TSTR) directly tests whether synthetic data preserves the P(Y|X) relationship needed for regression—any distortion in tail dependencies or rare joint patterns harms predictions disproportionately.
- Core assumption: Downstream ML models rely on joint feature interactions more than individual feature distributions.
- Evidence anchors:
  - [page 14-15] Statistical similarity scores remained "between 70 and 80" in 1:10, while predictive utility "suffered noticeably" with most models scoring negative
  - [page 11] TSTR paradigm: "if models trained on synthetic data produce results similar to models trained on real data, this suggests that the synthetic data closely replicates the predictive characteristics"
  - [corpus] arXiv:2305.09235 (van Breugel & van der Schaar, cited in paper) documents the gap between statistical fidelity and downstream utility, warning against relying solely on distributional metrics.
- Break condition: When synthetic data preserves both marginals AND the exact conditional structure P(Y|X), both metrics should converge—this is what Bayesian Network achieved in 1:1 (0.97 predictive utility with 96.53 similarity).

## Foundational Learning

- **Sklar's Theorem and Copula Functions**
  - Why needed here: Gaussian Copula's entire approach rests on decomposing any multivariate distribution into marginals + copula. Without understanding this, you cannot diagnose when copula assumptions break down.
  - Quick check question: Can you explain why a Gaussian copula might fail on highly non-Gaussian dependencies even if marginals are correctly specified?

- **Variational Inference and the ELBO Objective**
  - Why needed here: TVAE is fundamentally a VAE; its ability to generate 10× data depends on the encoder learning a meaningful latent posterior. Understanding the reconstruction-regularization tradeoff explains why TVAE succeeded where CTGAN failed.
  - Quick check question: What happens to synthetic sample diversity if the KL divergence term dominates the ELBO during training?

- **Train-on-Synthetic-Test-on-Real (TSTR) Evaluation Paradigm**
  - Why needed here: This is the core evaluation methodology. Understanding why it measures utility rather than fidelity helps you interpret when high statistical similarity doesn't translate to practical usefulness.
  - Quick check question: If a synthetic dataset has perfect marginal distributions but scrambled correlations, would TSTR detect this failure mode?

## Architecture Onboarding

- **Component map**:
```
Real Data (1,000 rows)
    ├── SDV Library
    │   ├── GaussianCopulaSynthesizer → [marginal estimation → copula fitting → sampling]
    │   ├── CTGANSynthesizer → [mode-specific normalization → conditional GAN → WGAN-GP training]
    │   └── TVAESynthesizer → [encoder q(z|x) → latent space → decoder p(x|z)]
    │
    └── Synthicity Library
        ├── BayesianNetworkSynthesizer → [DAG structure learning → conditional probability fitting → ancestral sampling]
        ├── CTGANPlugin → [deeper networks + dropout + weight decay vs SDV]
        └── TVAEPlugin → [different preprocessing, longer training, early stopping]
```

- **Critical path**:
  1. **Data preprocessing**: SDV uses RDT (Reverse Data Transformation) toolkit; Synthicity has internal preprocessing. Mismatch here causes identical model names (CTGAN, TVAE) to produce different results.
  2. **Training regime**: SDV uses fixed epochs with conservative learning rates; Synthicity uses dynamic training with early stopping. For small datasets (1,000 rows), these divergences compound.
  3. **Evaluation split**: In 1:1, regression models train on 1,000 synthetic rows and test on real holdout. In 1:10, they train on 10,000 synthetic rows—comparing against models trained on 10,000 real rows (different baseline).

- **Design tradeoffs**:
  - **Statistical vs. deep learning**: Statistical models (BN, Copula) → interpretable, stable, data-efficient; Deep models (CTGAN, TVAE) → better for scaling, but unstable training
  - **SDV vs. Synthicity**: SDV → better documentation, simpler networks, conservative training; Synthicity → Bayesian Networks available, deeper architectures, less documentation
  - **1:1 vs. 1:10 generation**: 1:1 → high fidelity replication; 1:10 → tests generalization but all models struggle (only TVAE achieves positive utility)

- **Failure signatures**:
  - **CTGAN negative utility scores in both settings** (−0.37 to −2.85): Mode collapse or discriminator overfitting; adversarial training unstable on 1,000 rows
  - **Bayesian Network crash in 1:10** (0.97 → −1.13): Statistical model extrapolates poorly; ancestral sampling cannot generalize beyond training distribution support
  - **Statistical similarity stable but predictive utility collapses**: Model preserved marginals but lost conditional structure P(Y|X)

- **First 3 experiments**:
  1. **Baseline replication**: Train all 6 models on 1,000 rows, generate 1,000 synthetic rows. Verify your implementation matches paper benchmarks (BN ≈ 0.97 predictive utility, statistical similarity > 90). If scores diverge significantly, check preprocessing pipelines.
  2. **Scaling stress test**: Same setup but generate 10,000 rows. Confirm TVAE (SDV) achieves highest utility (~0.31) and CTGAN fails. Plot similarity vs. utility scatter to visualize the decoupling of these metrics.
  3. **Ablation on training data size**: Repeat 1:1 experiment with 500, 2,000, and 5,000 real rows. Hypothesis: Statistical models' advantage shrinks as training data increases; deep learning models should converge toward statistical model performance. This tests the "low-data regime" claim directly.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can novel generative architectures be developed to maintain high predictive utility when scaling synthetic data generation significantly beyond the input size (e.g., 1:10)?
- Basis in paper: [explicit] The authors state that the results "underscore the need for more robust generative models capable of scaling synthetic datasets without sacrificing predictive quality, a critical requirement in real-world scenarios."
- Why unresolved: The study found that while statistical similarity remained acceptable, predictive utility dropped significantly in the 1:10 setting, with most models yielding negative utility scores.
- What evidence would resolve it: A study demonstrating a model that maintains a predictive utility score comparable to the 1:1 baseline when generating synthetic datasets ten times larger than the training input.

### Open Question 2
- Question: Do the observed performance trade-offs between statistical and deep learning models generalize to domains with different data structures, such as high-cardinality categorical data in healthcare?
- Basis in paper: [inferred] The study is limited to a single energy consumption dataset; the authors suggest that Bayesian Networks were effective partly due to the dataset's specific dependency structure, implying results may vary elsewhere.
- Why unresolved: The comparison relies on a single dataset comprising continuous environmental variables and energy usage, leaving the models' performance on purely categorical or high-dimensional data unverified.
- What evidence would resolve it: A replication of this comparative study using datasets from distinct domains (e.g., finance, electronic health records) to test if Bayesian Networks still outperform in 1:1 settings.

### Open Question 3
- Question: To what degree are the performance differences between the libraries attributable to underlying model architectures versus library-specific preprocessing and default hyperparameters?
- Basis in paper: [inferred] The discussion notes that models with identical names (CTGAN, TVAE) performed differently across libraries due to distinct implementation choices, such as network depth, regularization, and preprocessing pipelines (e.g., RDT).
- Why unresolved: The "black-box" nature of the high-level libraries makes it difficult to isolate whether the core generative algorithm or the software-specific data handling caused the observed performance gaps.
- What evidence would resolve it: An ablation study that standardizes preprocessing and hyperparameters across SDV and Synthicity to isolate the impact of the library-specific implementation details.

## Limitations

- **Library version dependency**: Results may vary across different versions due to evolving preprocessing pipelines and default hyperparameters
- **Dataset representativeness**: Single energy consumption dataset limits generalizability to other domains with different data structures
- **CTGAN instability**: Poor performance may reflect implementation-specific issues rather than inherent model limitations

## Confidence

- **High confidence**: Statistical similarity scores and their consistency across settings; Bayesian Network achieving highest 1:1 predictive utility; superiority of SDV documentation and ease of use
- **Medium confidence**: Deep learning models' better performance in 1:10 setting; claim that statistical models excel in 1:1 while deep learning scales better; overall library comparison given version and implementation differences
- **Low confidence**: Generalizability to other domains; the exact contribution of library-specific preprocessing vs. model architecture; whether CTGAN's failure is reproducible across datasets

## Next Checks

1. **Cross-dataset replication**: Repeat the 1:1 and 1:10 experiments on at least two additional tabular datasets (e.g., financial transactions, healthcare records) to assess generalizability of the observed statistical vs. deep learning performance gap

2. **Training data scaling study**: Systematically vary the real data size (e.g., 500, 2,000, 5,000 rows) in 1:1 generation to test the hypothesis that statistical models' advantage diminishes as training data increases, isolating library vs. model effects

3. **Architecture ablation test**: Re-implement CTGAN and TVAE identically in both libraries (SDV and Synthicity) to disentangle the impact of library-specific preprocessing and training regimes from the underlying model architecture