---
ver: rpa2
title: A Rhythm-Aware Phrase Insertion for Classical Arabic Poetry Composition
arxiv_id: '2509.18514'
source_url: https://arxiv.org/abs/2509.18514
tags:
- arabic
- dataset
- rhythmic
- diacritized
- poetry
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a rhythm-aware phrase insertion methodology
  for classical Arabic poetry composition using ByT5, a byte-level multilingual transformer
  model. The approach employs a conditional denoising objective to fine-tune ByT5,
  enabling it to reconstruct masked words to match target rhythmic patterns.
---

# A Rhythm-Aware Phrase Insertion for Classical Arabic Poetry Composition

## Quick Facts
- arXiv ID: 2509.18514
- Source URL: https://arxiv.org/abs/2509.18514
- Reference count: 5
- Primary result: Rhythm-aware phrase insertion using ByT5 achieves up to 81.14% rhythmic alignment accuracy on classical Arabic poetry.

## Executive Summary
This paper presents a novel methodology for rhythm-aware phrase insertion in classical Arabic poetry composition. The approach uses ByT5, a byte-level multilingual transformer, fine-tuned with a conditional denoising objective that reconstructs masked words to match target rhythmic patterns. A rule-based grapheme-to-beat transformation extracts binary rhythm sequences from fully diacritized Arabic script, which are then used as conditioning signals during training. The methodology employs curriculum learning, first pre-training on general Arabic text before fine-tuning on poetic datasets, and explores cross-lingual transfer from English to Arabic. Experimental results show high rhythmic alignment while maintaining semantic coherence, with accuracy scores reaching 81.14% on the poetic dataset after training.

## Method Summary
The methodology employs ByT5-base fine-tuned with conditional denoising, where masked word spans are reconstructed conditioned on target rhythm patterns. A rule-based grapheme-to-beat transformation converts fully diacritized Arabic text into binary rhythmic sequences (1s for vocalized, 0s for unvocalized letters) following Arabic prosody rules. Curriculum learning is implemented by first training on the general Tashkeelah dataset (2.8M lines) for 3 epochs, then fine-tuning on the poetic APCD dataset (35.6K lines) for another 3 epochs. The model uses special tokens (E0, E1, E2) to wrap and signal rhythm patterns during generation. Two variants are explored: ByT5-AR trained only on Arabic data, and ByT5-B-AR initialized from a prior English lyrics model to test cross-lingual transfer.

## Key Results
- Rhythm alignment accuracy reaches 81.14% on APCD poetic dataset after curriculum learning
- ByT5-B-AR variant shows faster early convergence but modest final gains from cross-lingual transfer
- High rhythmic alignment achieved while maintaining semantic coherence via mT5 cross-entropy loss
- Curriculum learning provides ~2 point accuracy improvement over direct poetic dataset training

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Conditioning on explicit rhythmic patterns enables byte-level transformers to generate rhythm-constrained text.
- **Mechanism:** The model receives target rhythm as binary sequence embedded between special tokens (E0, E1) at masked positions. ByT5's byte-level processing enables fine-grained alignment between character-level patterns and conditioned rhythm.
- **Core assumption:** Rhythm-to-text mapping is learnable from conditional denoising signal without explicit phonological modules.
- **Evidence anchors:** Abstract states approach uses conditional denoising objective; Section 3.4 describes rhythm token insertion at masked positions; no corpus support for Arabic conditioning mechanism.
- **Break condition:** If target patterns exceed typical Arabic word boundaries or require unusual combinations not in training data, model may produce incoherent outputs.

### Mechanism 2
- **Claim:** Rule-based grapheme-to-beat transformation provides deterministic rhythm extraction bypassing learned diacritization.
- **Mechanism:** Fully diacritized Arabic script transformed through orthographic rules (expanding Madda, handling Hamzat al-Waṣl, adding Išbāʿ, expanding Nunation/Gemination, removing silent graphemes) to produce binary rhythmic sequence aligned with ʿArūḍ theory.
- **Core assumption:** Input text is fully and correctly diacritized; rule system covers all classical Arabic prosody exceptions.
- **Evidence anchors:** Abstract discusses rule-based transformation for extracting rhythm from fully diacritized script; Section 3.2 details eight transformation rules; limited corpus evidence for Arabic poetry.
- **Break condition:** Fails on partially diacritized or inconsistently diacritized input; paper requires fully diacritized training data with extensive preprocessing.

### Mechanism 3
- **Claim:** Curriculum learning (general Arabic → poetic Arabic) improves rhythmic alignment over direct poetic training.
- **Mechanism:** Model first learns Arabic patterns on Tashkeelah (linguistically simpler), then specializes on APCD (more constrained poetic language). Progression from general to formal language allows acquisition of general competence before rhythm-specific mappings.
- **Core assumption:** General Arabic text provides transferable representations supporting later poetic specialization; poetic language sufficiently related to general Arabic for positive transfer.
- **Evidence anchors:** Abstract states curriculum learning strategy pre-training on general dataset before poetic fine-tuning; Section 4.2 reports ~2 point accuracy improvement; Section 4.3 shows ByT5-B-AR faster convergence but modest gains.
- **Break condition:** If datasets have divergent vocabulary, dialect, or register distributions, curriculum learning could introduce interference rather than facilitation.

## Foundational Learning

- **Concept: Arabic Prosody (ʿArūḍ) and Mora-Based Rhythm**
  - Why needed here: Entire methodology depends on representing Arabic meter as binary sequences. Without understanding mora-based scansion, grapheme-to-beat transformation logic will be opaque.
  - Quick check question: Given كِتَابٌ, can you determine its rhythmic pattern after Tanwīn expansion and diacritic-to-beat mapping?

- **Concept: Conditional Text Generation with Span Masking**
  - Why needed here: Model trained to reconstruct masked word spans conditioned on target rhythm. Understanding span-masking and conditional generation is essential for training setup.
  - Quick check question: How does inserting rhythm tokens at masked position differ from prepending them as global prefix?

- **Concept: Byte-Level vs. Token-Level Representations**
  - Why needed here: ByT5 processes raw bytes rather than tokens, matters for fine-grained rhythm alignment. Choice affects how diacritics (separate Unicode codepoints) are processed.
  - Quick check question: Why would byte-level model be advantageous for Arabic diacritics compared to subword tokenizer like BPE?

## Architecture Onboarding

- **Component map:** Input Text → Diacritic Preprocessing → Masking → Rhythm Token Insertion → ByT5 Encoder → ByT5 Decoder ← Conditioning ← Target Rhythm → Generated Text → G2B Transform → Evaluated Rhythm

- **Critical path:** Grapheme-to-beat transformation correctness determines whether training targets are valid. Errors propagate as noisy supervision signals.

- **Design tradeoffs:**
  - Fully diacritized training data requirement vs. real-world partially diacritized inputs: Paper simulates partial diacritization during training by randomly dropping diacritics in context
  - Rule-based rhythm extraction vs. learned rhythm model: Rules provide deterministic extraction but require full diacritization; learned model would be more flexible but potentially less accurate
  - Cross-lingual transfer benefit vs. training complexity: ByT5-B-AR showed faster early convergence but modest final gains—may not justify additional pre-training complexity for all use cases

- **Failure signatures:**
  - Low rhythmic alignment accuracy (<50%): Model hasn't learned rhythm-text mapping; check dataset quality and diacritization coverage
  - High cross-entropy loss on coherence metric: Model may generate rhythmic but semantically incoherent text; review span masking ratio and context length
  - Training loss not decreasing: Verify target rhythm tokens correctly inserted and G2B transformation produces valid patterns

- **First 3 experiments:**
  1. Baseline validation: Replicate masking + rhythm conditioning setup on small Tashkeelah subset. Verify accuracy improves over unconditioned denoising to confirm conditioning mechanism works before full training.
  2. Ablation on diacritization level: Train with varying diacritization rates in context (0%, 25%, 50%, 75%, 100%) to understand robustness to real-world input. Paper uses geometric sampling but doesn't report sensitivity.
  3. Cross-dataset generalization test: Evaluate model trained only on Tashkeelah on held-out APCD test set to quantify curriculum learning benefit explicitly. Paper reports this (78.37% vs 80.43% after fine-tuning), but replication confirms setup correctness.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the model generate verses that professional and amateur poets judge as fluent, poetic, and useful for composition assistance?
- Basis in paper: Conclusion states "we plan to conduct a human-centered evaluation to assess the fluency and poetic quality of the generated verses and its utility as a tool for assisting professional and amateur classical Arabic poetry composers."
- Why unresolved: Current evaluation relies solely on automated metrics (rhythmic alignment accuracy, Levenshtein similarity, mT5 cross-entropy) which "may not fully capture the complex features of poetic language."
- What evidence would resolve it: User study with poets evaluating generated verses on fluency, poetic quality, and practical utility in co-creative composition.

### Open Question 2
- Question: Can alternative cross-lingual transfer approaches yield greater performance gains beyond the "relatively modest" improvements observed from English-to-Arabic transfer?
- Basis in paper: Section 4.3 notes "the final performance gains from this cross-lingual transfer remain relatively modest" and conclusion states "the benefits of curriculum learning, especially in cross-lingual scenarios, may be inherently limited."
- Why unresolved: Only one transfer approach was explored, leaving unclear whether different source languages, domains, or training strategies could improve results.
- What evidence would resolve it: Comparative experiments with varied transfer sources showing whether substantial cross-lingual gains are achievable.

### Open Question 3
- Question: Does the high cross-entropy coherence loss indicate model indecisiveness, metric limitations, or actual quality issues?
- Basis in paper: Section 4.3 states "the high cross-entropy loss may also imply that the model lack decisiveness; an issue we aim to address through human evaluation."
- Why unresolved: Relationship between mT5 cross-entropy scores and actual semantic coherence remains unclear, as all models showed similar coherence despite accuracy differences.
- What evidence would resolve it: Correlation analysis between cross-entropy loss and human coherence judgments, or development of alternative coherence metrics.

### Open Question 4
- Question: How does the model perform on rare rhythmic patterns and non-canonical meters absent or underrepresented in training corpus?
- Basis in paper: Methodology claims to handle user-specified patterns that "do not follow the most common meters," yet APCD dataset contains only 35,624 fully diacritized verses from limited period, likely skewing toward frequent meters.
- Why unresolved: No stratified evaluation by meter frequency conducted, and paper acknowledges existing models are "limited to a distribution based on the poetry corpus and the frequency of each meter."
- What evidence would resolve it: Evaluation on held-out rare meters with accuracy reporting per rhythmic pattern category.

## Limitations

- **Data dependency and generalizability**: Requires fully diacritized training data, uncommon in real-world Arabic text. APCD dataset represents only one style of classical Arabic poetry, limiting generalizability to other poetic traditions.
- **Rhythm extraction fidelity**: Rule-based transformation depends on correct and complete diacritization. Errors in input diacritics or uncovered exceptions propagate as incorrect supervision signals.
- **Cross-lingual transfer validation**: ByT5-B-AR showed faster early convergence but modest final gains (78.37% vs 81.14%). Paper doesn't clearly establish whether additional complexity of English pre-training is justified by marginal improvements.

## Confidence

- **High confidence**: Core methodology of rhythm-aware phrase insertion using conditional denoising with ByT5 is technically sound and experimental results are reproducible given specified datasets and training parameters. Approach is novel and reported accuracies are internally consistent.
- **Medium confidence**: Effectiveness of curriculum learning and cross-lingual transfer. Paper reports improvements but magnitude is modest and external validation limited. Assumptions about transfer learning benefits are plausible but not extensively supported by external evidence.
- **Low confidence**: Robustness to partially diacritized or undiacritized input, and generalization to poetic forms beyond APCD corpus. These aspects were not directly tested and represent significant limitations for practical deployment.

## Next Checks

1. **Diacritization robustness test**: Systematically evaluate model performance as function of diacritization rate in input context (0% to 100%) to quantify approach's sensitivity to real-world undiacritized text.

2. **Cross-dataset generalization**: Evaluate models trained on Tashkeelah alone on held-out APCD test set, and vice versa, to quantify curriculum learning benefit more precisely.

3. **Rhythm transformation validation**: Manually verify grapheme-to-beat transformation accuracy on sample of 100 verses by comparing rule-based output to manual prosodic scansion.