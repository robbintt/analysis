---
ver: rpa2
title: 'SARCH: Multimodal Search for Archaeological Archives'
arxiv_id: '2511.05667'
source_url: https://arxiv.org/abs/2511.05667
tags:
- search
- text
- image
- table
- content
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents SARCH, a multi-modal search system for archaeological
  archives consisting of scanned documents with varying scan quality. The system extracts
  and indexes text, images (classified into maps, photos, layouts, and figures), and
  tables.
---

# SARCH: Multimodal Search for Archaeological Archives

## Quick Facts
- arXiv ID: 2511.05667
- Source URL: https://arxiv.org/abs/2511.05667
- Reference count: 40
- Primary result: Embedding-based search achieves MRR of 0.754 on archaeological archive retrieval

## Executive Summary
SARCH presents a multimodal search system for archaeological archives consisting of scanned documents with varying quality. The system extracts and indexes text, images (classified into maps, photos, layouts, and figures), and tables from legacy documents. Three retrieval strategies were evaluated - keyword-based, embedding-based, and hybrid approaches - with embedding-based search demonstrating superior performance across multiple metrics. The system successfully handles archaeology-specific content types like maps and photos, addressing the challenge of searching through historical archaeological documentation.

## Method Summary
The system processes scanned archaeological documents through OCR (Surya OCR), image classification (ViT model), and table extraction (TableFormer). Text, images, and tables are indexed in a vector database (Weaviate). Three retrieval strategies are implemented: keyword-based search using BM25, embedding-based search using dense vector representations, and a hybrid approach combining both. The system uses sentence-transformers for text embeddings and CLIP models for image embeddings. A benchmark of 30 queries (15 text, 11 image, 4 table) with binary relevance judgments was created with domain expert input to evaluate performance.

## Key Results
- Embedding-based search achieved highest performance: MRR 0.754, P@1 0.619, P@5 0.438
- Keyword-based search underperformed: MRR 0.508, P@1 0.429, P@5 0.238
- Hybrid approach underperformed expectations: MRR 0.726, P@1 0.538, P@5 0.346

## Why This Works (Mechanism)
The embedding-based approach works effectively because it captures semantic relationships between archaeological concepts that go beyond exact keyword matching. Dense vector representations can identify relevant documents even when they use different terminology or phrasing to describe similar archaeological features. The multimodal indexing allows the system to retrieve specific content types (maps, photos, tables) that are critical for archaeological research but would be missed by text-only search approaches.

## Foundational Learning
- OCR fundamentals (why needed: extracts text from scanned documents; quick check: verify character accuracy on test documents)
- Image classification basics (why needed: categorizes archaeological images into maps, photos, layouts, figures; quick check: test classifier on diverse image types)
- Vector database concepts (why needed: enables efficient similarity search on embeddings; quick check: measure query response times)
- Dense vs sparse retrieval (why needed: determines optimal search strategy; quick check: compare BM25 vs embedding performance on sample queries)
- Multimodal indexing (why needed: supports searching across text, images, and tables; quick check: verify all content types are searchable)

## Architecture Onboarding

Component Map: Document Scanner -> OCR + Image Classification -> Table Extraction -> Indexing Pipeline -> Vector Database -> Query Interface -> Retrieval Engine

Critical Path: Query Input -> Modality Selection -> Search Execution -> Result Ranking -> Display Output

Design Tradeoffs: Embedding-based search provides better semantic matching but requires more computational resources than keyword search. The manual modality selection adds usability friction but ensures precise result filtering.

Failure Signatures: Low recall for specialized archaeological terminology, poor handling of rotated or irregularly spaced text in maps, suboptimal fusion in hybrid approach.

First 3 Experiments:
1. Test OCR accuracy on documents with varying scan quality to establish baseline performance
2. Evaluate image classifier precision on archaeology-specific image types (maps, excavation photos)
3. Compare BM25 vs embedding retrieval performance on a small set of sample queries

## Open Questions the Paper Calls Out
- Can instruction-tuned models effectively automate the selection of the optimal retrieval modality (text, image, or table) based solely on the user's natural language query?
- What specific lexical or semantic features distinguish queries that perform better with keyword-based search (BM25) from those that require dense vector embeddings?
- How can text spotting techniques be optimized to robustly extract location names from archaeological maps characterized by non-standard spacing and arbitrary orientations?

## Limitations
- Small benchmark of 30 queries with binary relevance judgments limits statistical power
- No inter-annotator agreement metrics to assess reliability of relevance judgments
- Evaluation focuses only on retrieval metrics without qualitative user satisfaction assessment

## Confidence
- Embedding-based search superiority: High confidence - Multiple metrics consistently show this approach outperforming others
- System effectiveness for archaeology-specific content: Medium confidence - Strong results but limited query set and no qualitative validation
- Multi-modal indexing benefits: Medium confidence - Performance gains demonstrated but not compared against unimodal alternatives systematically

## Next Checks
1. Expand benchmark to 100+ queries with multiple relevance judges and calculate inter-annotator agreement to establish reliability of evaluation
2. Conduct head-to-head comparison of different OCR engines and embedding models to isolate contribution of specific technical choices
3. Perform user study with archaeologists to assess real-world utility beyond retrieval metrics, including qualitative feedback on result relevance and system usability