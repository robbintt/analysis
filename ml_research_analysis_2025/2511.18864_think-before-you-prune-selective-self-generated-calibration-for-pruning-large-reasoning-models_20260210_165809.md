---
ver: rpa2
title: 'Think Before You Prune: Selective Self-Generated Calibration for Pruning Large
  Reasoning Models'
arxiv_id: '2511.18864'
source_url: https://arxiv.org/abs/2511.18864
tags:
- data
- pruning
- reasoning
- calibration
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of pruning large reasoning models
  (LRMs), which has been unexplored compared to pruning large language models. The
  authors show that directly applying existing pruning techniques fails to yield satisfactory
  results on LRMs.
---

# Think Before You Prune: Selective Self-Generated Calibration for Pruning Large Reasoning Models

## Quick Facts
- arXiv ID: 2511.18864
- Source URL: https://arxiv.org/abs/2511.18864
- Authors: Yang Xiang; Yixin Ji; Juntao Li; Min Zhang
- Reference count: 11
- This paper addresses pruning large reasoning models (LRMs) and shows that using self-generated reasoning data for calibration substantially improves pruning performance compared to traditional pre-training data.

## Executive Summary
This paper tackles the underexplored problem of pruning large reasoning models (LRMs), demonstrating that standard pruning techniques fail to preserve reasoning capabilities. The key insight is that calibration data shapes activation statistics used for parameter importance estimation, and self-generated reasoning data produces more representative activations than generic pre-training data. Through systematic investigation, the authors identify that challenging and moderately long self-generated reasoning chains serve as ideal calibration data. Based on these findings, they propose a Selective Self-Generated Reasoning (SSGR) data construction strategy that improves pruned LRM reasoning ability by 10%-13% compared to general pruning methods.

## Method Summary
The authors develop SSGR, a strategy for constructing optimal calibration data for pruning LRMs. The process involves generating 16 responses per problem from the dense model using temperature 0.6, top-p 0.95, and max_tokens=32768. Problems are filtered based on the model's correctness ratio (discarding if c > 0.75), then incorrect and low-quality responses are removed. The median-length correct response is selected for each problem, creating a calibration set that preserves the model's native reasoning patterns. This data is then used with standard pruning algorithms like SparseGPT to achieve better preservation of reasoning capabilities at high sparsity levels (50%).

## Key Results
- Self-generated reasoning data improves pruning performance by 10%-13% compared to using pre-training data like C4
- Challenging calibration data (low model accuracy) leads to significant gains, improving performance by 6.3%-8.6% compared to easy calibration data
- Moderately long reasoning chains outperform both shorter and longer responses by 2.6% and 1.8% respectively
- SSGR strategy shows consistent improvements across different pruning settings (unstructured and semi-structured) and model architectures

## Why This Works (Mechanism)

### Mechanism 1
Self-generated reasoning data yields superior calibration because it preserves the model's native "think-then-answer" pattern. Calibration data shapes activation statistics used to estimate parameter importance. Self-generated responses produce activations that mirror the model's own reasoning distribution, while pre-training data (e.g., C4) produces generic activations that fail to preserve reasoning-specific circuits. The dense model's internal activations on its own outputs are more representative of its reasoning computations than those from external data.

### Mechanism 2
Challenging calibration data improves pruning performance on difficult benchmarks. Problems the model finds difficult trigger more complex, variable activation patterns. Using these for calibration helps the pruning algorithm preserve parameters critical for multi-step reasoning, which are otherwise vulnerable to removal. The model's own success rate is a valid proxy for data difficulty, and difficult problems engage reasoning-specific circuitry more fully.

### Mechanism 3
Moderately long reasoning chains are optimal for calibration; shorter chains lack sufficient signal, while overly long chains introduce noise. A moderate-length response captures a complete chain-of-thought with sufficient intermediate steps to inform the pruning metric without the potential noise from repetitive or meandering output found in excessively long responses. A "sweet spot" exists where response length balances information density and clarity.

## Foundational Learning

- **Calibration Data in Post-Training Pruning**
  - Why needed here: The entire SSGR strategy challenges the standard practice of using generic pre-training data (e.g., C4) for calibration
  - Quick check question: In methods like SparseGPT, what is the primary purpose of passing calibration data through the model before removing weights?

- **Chain-of-Thought (CoT) in Reasoning Models**
  - Why needed here: The paper's core finding is that the *presence* of the CoT in calibration data is what drives performance, not just the questions or final answers
  - Quick check question: Why does removing the thinking process from self-generated responses cause pruning performance to drop?

- **Unstructured vs. Structured Pruning**
  - Why needed here: The paper demonstrates SSGR's effectiveness across both unstructured (SparseGPT) and semi-structured (2:4, 4:8) sparsity settings
  - Quick check question: Why might a calibration data strategy that works for unstructured pruning also be critical for semi-structured pruning?

## Architecture Onboarding

- **Component map**: Dense LRM -> Self-Generation Engine -> SSGR Filter -> Pruning Method
- **Critical path**: 
  1. For each problem in source dataset, sample 16 responses from dense model
  2. Compute model's correctness ratio for the problem, discard if c > 0.75
  3. Filter out incorrect and low-quality responses
  4. Select response with median token length
  5. Aggregate selected responses to form SSGR calibration set
  6. Run pruning algorithm with this set
- **Design tradeoffs**:
  - Self-Generation Cost: Sampling 16 responses per problem is expensive
  - Model-Specificity: SSGR data must be generated by the model being pruned
  - Generalization: Math-only data generalizes to science benchmarks
- **Failure signatures**:
  - Low-Quality Generation: If dense model is weak, its self-generated data will be noisy
  - Excessive Pruning Ratio: At very high sparsity (e.g., 60%), performance degrades sharply
- **First 3 experiments**:
  1. Replicate main result: Prune DeepSeek-R1-Distill-Qwen-7B using SparseGPT with C4 and SSGR calibration data
  2. Ablate on difficulty: Construct calibration sets from "Easy", "Medium", and "Hard" problems
  3. Ablate on length: Construct calibration sets from "Short", "Medium", and "Long" responses

## Open Questions the Paper Calls Out

### Open Question 1
Can the efficiency of the SSGR data construction be improved to generate optimal calibration samples in a single pass? The authors acknowledge that sampling 16 responses per question is "time-consuming" and state a plan to "explore methods that enable the model to generate an optimal response in a single pass." This is unresolved because the current methodology relies on a filtering process that requires multiple generations per prompt to ensure quality.

### Open Question 2
Does the effectiveness of the SSGR strategy scale to Large Reasoning Models with parameters significantly larger than 14B? The "Limitations" section notes that due to computational constraints, experiments were "only on LLMs up to 14B." It is unverified whether the specific correlations found between data difficulty, length, and pruning performance hold true for much larger model scales (e.g., 70B+).

### Open Question 3
What modifications to pruning algorithms are necessary for architectures where calibration data alone is insufficient to prevent performance degradation? The authors observe that for DeepSeek-R1-Distill-Llama-8B, "improvements from calibration data alone are insufficient," explicitly stating the need for "more effective pruning techniques." This suggests that while data-centric fixes help, the underlying pruning metrics may be fundamentally mismatched with certain LRM reasoning mechanisms.

## Limitations
- The self-generation process requires sampling 16 responses per problem, which is computationally expensive
- The answer verification mechanism is underspecified, described only as "exact answer matching or pattern verification"
- The difficulty threshold τ=0.75 is presented as effective but not thoroughly validated across different model scales or domains

## Confidence

- **High confidence**: The core finding that self-generated reasoning data substantially outperforms pre-training data (C4) for calibration in pruning LRMs
- **Medium confidence**: The SSGR construction methodology (difficulty and length filtering) improves upon simple self-generated reasoning (SGR)
- **Low confidence**: The claim that self-generated data from a stronger model (Qwen3-14B) generalizes worse than data from the target model

## Next Checks

1. **Verification method audit**: Implement and test multiple answer verification strategies (exact string matching, numerical tolerance for math expressions, regex-based pattern matching) to determine their impact on SSGR data quality and pruning performance.

2. **Difficulty threshold sensitivity**: Systematically vary τ from 0.5 to 0.9 in 0.1 increments and measure pruning performance to identify optimal difficulty filtering and test the claim that τ=0.75 is universally effective.

3. **Computational cost-benefit analysis**: Measure the wall-clock time and GPU hours required for generating 16 responses per problem versus the accuracy gains achieved, to quantify the practical trade-off for real-world deployment.