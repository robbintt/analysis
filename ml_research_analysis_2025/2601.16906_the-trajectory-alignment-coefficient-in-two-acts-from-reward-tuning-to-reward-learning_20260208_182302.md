---
ver: rpa2
title: 'The Trajectory Alignment Coefficient in Two Acts: From Reward Tuning to Reward
  Learning'
arxiv_id: '2601.16906'
source_url: https://arxiv.org/abs/2601.16906
tags:
- reward
- learning
- soft-tac
- weights
- preference
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces Soft-TAC, a differentiable approximation\
  \ of the Trajectory Alignment Coefficient (TAC), which measures alignment between\
  \ a reward function\u2019s preferences and human preferences. Soft-TAC enables reward\
  \ learning from human preferences using gradient-based optimization, addressing\
  \ the challenge of manually tuning reward functions in reinforcement learning."
---

# The Trajectory Alignment Coefficient in Two Acts: From Reward Tuning to Reward Learning

## Quick Facts
- **arXiv ID**: 2601.16906
- **Source URL**: https://arxiv.org/abs/2601.16906
- **Reference count**: 40
- **Primary result**: Soft-TAC learns reward functions from human preferences that outperform Cross-Entropy baselines and produce more behaviorally distinct policies.

## Executive Summary
This paper introduces Soft-TAC, a differentiable approximation of the Trajectory Alignment Coefficient (TAC), which measures alignment between a reward function's preferences and human preferences. Soft-TAC enables reward learning from human preferences using gradient-based optimization, addressing the challenge of manually tuning reward functions in reinforcement learning. The authors validate Soft-TAC in two domains: Lunar Lander and Gran Turismo 7 (GT7), demonstrating improved performance over standard preference-based learning approaches while requiring fewer human preferences and being robust to noisy labels.

## Method Summary
The method learns a reward model from pairwise trajectory preferences using Soft-TAC, a differentiable approximation of TAC that measures Kendall's Tau-b rank correlation between human preferences and reward-induced preferences. The loss function uses tanh-based saturation to suppress gradients on confidently misclassified preferences, providing noise robustness. The approach works with linear reward models (Lunar Lander) and black-box models (GT7), using early stopping based on TAC, accuracy, and loss metrics to prevent overfitting on small datasets.

## Key Results
- In Lunar Lander, reward models trained with Soft-TAC achieved 63% landing success rate vs 20% for Cross-Entropy baselines
- GT7 Soft-TAC policies showed more distinct behavioral profiles (aggressive vs timid) with only 2 incomplete laps vs 38 for Cross-Entropy
- Soft-TAC required fewer human preferences (118-148 pairs) while achieving better alignment than standard approaches
- Human subject study showed participants with TAC feedback achieved better task performance with lower workload (μ=2.81 vs 4.07 on NASA-TLX)

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Soft-TAC's tanh-based loss provides inherent noise robustness by suppressing gradients on confidently misclassified preferences.
- **Mechanism:** When the reward model predicts a large return difference in the wrong direction, tanh saturates near ±1, causing gradients to approach zero. This prevents noisy or contradictory labels from dominating optimization.
- **Core assumption:** Noisy or inconsistent human preferences are a meaningful signal to downweight rather than fit exactly.
- **Evidence anchors:** Section 5.1 shows tanh saturation behavior; Appendix B.5 demonstrates gradient suppression on mislabeled data.

### Mechanism 2
- **Claim:** TAC provides a continuous alignment signal that directly measures rank agreement between reward-induced preferences and human preferences.
- **Mechanism:** TAC computes Kendall's Tau-b between human preferences and reward-induced preferences based on return comparisons. Concordant pairs increment P, discordant pairs increment Q.
- **Core assumption:** Human preferences are transitive and can be meaningfully compared to trajectory returns.
- **Evidence anchors:** Section 3.3 defines TAC using Kendall's Tau-b; Section 4.2 shows human subjects with TAC feedback achieved 63% vs 20% success rate.

### Mechanism 3
- **Claim:** Soft-TAC enables reward learning that produces behaviorally distinct policies by preserving preference-specific objective structure.
- **Mechanism:** Unlike Cross-Entropy which treats each comparison independently, Soft-TAC's expectation over the full dataset encourages global consistency, leading to more distinct behavioral profiles when preferences encode specific styles.
- **Core assumption:** Reward features are sufficient to represent the target behavioral objectives.
- **Evidence anchors:** Section 5.3 shows Soft-TAC aggressive policies with more collisions while maintaining competitive final position; Table 2 shows BIAI ratio 0.968 vs 0.994 for Cross-Entropy.

## Foundational Learning

- **Concept: Preference-based Reinforcement Learning (PbRL)**
  - Why needed here: The paper builds on standard PbRL formulation where reward models are learned from trajectory comparisons.
  - Quick check question: Given trajectories τ₁ and τ₂ with returns 10 and 5 respectively, what probability does the BT model assign to preferring τ₁? (Answer: 10/(10+5) = 2/3)

- **Concept: Kendall's Tau-b Rank Correlation**
  - Why needed here: TAC is directly constructed from Kendall's Tau-b. The treatment of ties matters for understanding when TAC scores saturate.
  - Quick check question: If 7 of 10 trajectory pairs are concordant (P=7, Q=3) with no ties, what is the Kendall's Tau-b score? (Answer: (7-3)/√(10·10) = 0.4)

- **Concept: Gradient Saturation and Loss Landscape Geometry**
  - Why needed here: Soft-TAC's noise robustness depends on tanh saturation. Understanding when gradients vanish vs persist is critical.
  - Quick check question: At what value of α·ΔG does tanh reach 99% of its asymptote? What happens to the gradient at this point? (Answer: α·ΔG ≈ 2.33; gradient approaches zero)

## Architecture Onboarding

- **Component map:** Preference Data Dₕ → Reward Model r_θ → Trajectory Returns G_r(τ) → Return Difference ΔG → Soft-TAC Loss → Adam Optimizer → Updated θ → RL Agent Training

- **Critical path:**
  1. Collect pairwise trajectory preferences with labels y ∈ {-1, 0, 1}
  2. Initialize reward weights (linear model) or network parameters (black-box)
  3. For each batch: compute returns on trajectory pairs, calculate ΔG, evaluate Soft-TAC loss, backpropagate
  4. Early stopping based on TAC, accuracy, and loss metrics
  5. Train RL agent with learned reward function

- **Design tradeoffs:**
  - **α (temperature) parameter:** Controls sensitivity to return differences; paper uses α=1.0 for consistency with Cross-Entropy
  - **Linear vs neural reward model:** Paper uses linear models for interpretability; neural models increase expressivity but lose weight interpretability
  - **Batch size vs dataset size:** Smaller batches (4-16 used) with early stopping prevented overfitting on small preference datasets

- **Failure signatures:**
  - **Weight collapse to zero:** If all gradients suppress, weights may not move from initialization
  - **High TAC but poor RL performance:** Reward function aligns with preferences but preferences don't reflect task success
  - **Cross-Entropy oscillation:** Conflicting gradients from noisy labels cause weight to plateau

- **First 3 experiments:**
  1. **Toy validation:** Replicate Appendix B.5 toy example with 5 items and one mislabeled comparison
  2. **Lunar Lander ablation:** Train with varying α ∈ {0.1, 0.5, 1.0, 5.0, 10.0} on the 148-pair dataset
  3. **Noise injection test:** Take clean preference data, inject label flip noise at rates η ∈ {0.1, 0.2, 0.3}

## Open Questions the Paper Calls Out

- **Open Question 1:** Can Soft-TAC be extended to learn black-box neural network reward models, rather than only linear combinations of predefined features?
- **Open Question 2:** Can TAC help RL practitioners identify missing or uninformative reward features during the design process?
- **Open Question 3:** Does Soft-TAC maintain its noise-tolerance advantages over Cross-Entropy with realistic (non-asymptotic) values of α?
- **Open Question 4:** How does Soft-TAC perform in online preference-based RL where preferences are collected incrementally during policy training?

## Limitations

- The GT7 experiments rely on proprietary data and simulator access, making independent verification impossible
- The noise-robustness claims are demonstrated only on synthetic preference noise; real human preference noise patterns may differ
- The linear reward model assumption limits expressivity, though this is acknowledged as a design choice for interpretability
- Cross-Entropy performance may vary with different temperature scalings or optimization hyperparameters

## Confidence

- **High confidence**: Soft-TAC loss definition and implementation details; Lunar Lander experimental methodology and results
- **Medium confidence**: Theoretical claims about noise robustness (Theorem 2); GT7 results (limited access to data/code)
- **Low confidence**: Generalizability to non-linear reward models; performance on tasks with fundamentally intransitive human preferences

## Next Checks

1. **Noise injection study**: Systematically vary noise rates (0%, 10%, 20%, 30%) in Lunar Lander preference data and measure convergence stability and final TAC scores for both Soft-TAC and Cross-Entropy

2. **Hyperparameter sensitivity**: Test Soft-TAC with different α values (0.1, 0.5, 1.0, 5.0, 10.0) to determine optimal temperature scaling for different return scales and noise levels

3. **Model capacity comparison**: Implement Soft-TAC with neural network reward models (MLP with 1-2 hidden layers) to test whether the noise-robustness advantage persists with increased expressivity