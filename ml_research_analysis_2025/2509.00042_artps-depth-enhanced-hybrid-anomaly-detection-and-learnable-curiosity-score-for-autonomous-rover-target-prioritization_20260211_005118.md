---
ver: rpa2
title: 'ARTPS: Depth-Enhanced Hybrid Anomaly Detection and Learnable Curiosity Score
  for Autonomous Rover Target Prioritization'
arxiv_id: '2509.00042'
source_url: https://arxiv.org/abs/2509.00042
tags:
- depth
- anomaly
- curiosity
- performance
- fusion
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ARTPS integrates single-image depth estimation with multi-component
  anomaly fusion and a learnable curiosity score for autonomous planetary exploration.
  The system combines Vision Transformer-based depth estimation, autoencoder reconstruction
  differences, and image-based texture/edge cues with depth discontinuities, using
  weighted fusion to suppress shadows and specularities.
---

# ARTPS: Depth-Enhanced Hybrid Anomaly Detection and Learnable Curiosity Score for Autonomous Rover Target Prioritization

## Quick Facts
- **arXiv ID**: 2509.00042
- **Source URL**: https://arxiv.org/abs/2509.00042
- **Reference count**: 26
- **Primary result**: 23% reduction in false positives while maintaining high detection sensitivity across diverse terrain types

## Executive Summary
ARTPS is a depth-enhanced anomaly detection and target prioritization system designed for autonomous planetary rover exploration. The system integrates single-image depth estimation with multi-component anomaly fusion and a learnable curiosity score to identify and rank scientifically interesting targets. By combining Vision Transformer-based depth estimation, autoencoder reconstruction differences, and image-based texture/edge cues with depth discontinuities, ARTPS achieves robust detection performance while suppressing environmental nuisances like shadows and specularities. The system demonstrates significant improvements in small-near object sensitivity and preserved far-field detail compared to baseline methods.

## Method Summary
ARTPS processes input imagery through a multi-stage pipeline beginning with ViT-based monocular depth estimation enhanced by edge-guided filtering, Poisson-based global smoothing, and weighted-median filtering. The system then computes multi-component anomaly signals including autoencoder reconstruction error, image-based texture/edge features (gradients, Laplacian, DoG), depth discontinuities, and feature statistics from PaDiM/PatchCore. These components are normalized and fused via weighted combination with shadow/specular suppression terms derived from luminance and saturation channels. A learnable curiosity score ranks targets using regularized regression over normalized components including known value, reconstruction difference, anomaly density, depth variance, and surface roughness.

## Key Results
- AUROC: 0.94, AUPRC: 0.89, F1-Score: 0.87 on Mars rover datasets
- 23% reduction in false positives while maintaining high detection sensitivity
- nDCG@10: 0.912 (ARTPS) vs 0.734 (baseline)
- Improved small-near object sensitivity and preserved far-field detail compared to baseline methods

## Why This Works (Mechanism)

### Mechanism 1: Multi-Component Anomaly Fusion
Weighted combination of heterogeneous anomaly signals suppresses environmental nuisances (shadows, specularities) while maintaining detection sensitivity. The system normalizes reconstruction error, image-based texture/edge cues, depth discontinuities, and feature-statistics signals to [0,1], then fuses via weighted sum with shadow/specular suppression terms derived from luminance and saturation channels. This approach leverages the fact that anomaly types manifest differently across modalities, with artifacts like shadows appearing in luminance/saturation but not depth, while true geometric anomalies persist across depth discontinuities.

### Mechanism 2: Depth-Enhanced Near-Field Sensitivity
Monocular depth estimation with edge-guided post-processing improves small/near object detection without sacrificing far-field detail. The ViT encoder-decoder produces depth maps that are refined through edge-guided filtering, Poisson-based global smoothing, and weighted-median filtering to preserve edges while reducing noise. Depth variance and roughness feed into both the anomaly fusion and curiosity score, providing geometric context that enhances detection of small or near objects.

### Mechanism 3: Learnable Curiosity Score for Target Prioritization
Regularized regression over normalized multi-signal components produces ranking aligned with expert scientific priorities. The system combines per-region scores (known value from classifier, reconstruction error, anomaly density, depth variance σ²_depth, surface roughness ||∇D||²) using learned nonnegative weights α_k via regularized regression. Rank-based metrics (nDCG, Kendall, Spearman) guide hyperparameter selection, enabling the system to adapt its prioritization strategy based on validation performance.

## Foundational Learning

- **Vision Transformers (ViT) for Dense Prediction**
  - Why needed here: Depth estimation uses ViT encoder-decoder; understanding patch embeddings, positional encoding, and multi-head attention enables debugging depth quality.
  - Quick check question: Given a 224×224 input with patch size 16, what is the sequence length fed to the transformer?

- **Multi-Scale Image Features (Gradients, Laplacian, DoG)**
  - Why needed here: Anomaly fusion relies on these cues for texture/edge detection; understanding scale-space concepts helps tune fusion weights.
  - Quick check question: Why does Difference-of-Gaussians approximate the Laplacian-of-Gaussian, and what advantage does it offer computationally?

- **Ranking Metrics (nDCG, Spearman, Kendall)**
  - Why needed here: Curiosity score optimization uses these metrics; understanding them enables proper validation and hyperparameter tuning.
  - Quick check question: If nDCG@10 is 0.912 but Kendall's τ is 0.689, what does this suggest about the ranking quality for top-10 vs full ordering?

## Architecture Onboarding

- **Component map**: Input Image → Enhancement → ViT Depth Estimation → Post-processing → Depth Map; Input Image → Autoencoder → Reconstruction Error; Input Image → PaDiM/PatchCore → Feature Stats; Input Image → Gradients/Laplacian/DoG → Image Features; Depth Map → Depth Gradients → Depth Features; All Components → Fusion → Combined Anomaly Map → Localization + Box Merging → Curiosity Score → Ranked Targets

- **Critical path**: Depth estimation → depth discontinuity extraction → fusion weighting. If depth post-processing fails (edge-guided filtering misconfigured), depth variance and roughness cues corrupt fusion.

- **Design tradeoffs**:
  - Precision vs recall: Hysteresis thresholding reduces false positives but may miss subtle anomalies (shadow-dense AUROC 0.843 vs high-contrast 0.912)
  - Compute vs accuracy: PaDiM feature statistics improve textured rock detection but increase memory; profile-aware fallbacks disable under constraints
  - Regularization strength: Stronger curiosity regularization stabilizes ranking but reduces sensitivity to rare targets (trade-off curves in Section 5)

- **Failure signatures**:
  - Low-texture expanses: AUROC drops to 0.867, FPR rises to 0.112 (depth estimation degrades)
  - Overexposed regions: Not quantified but listed as limitation; specular suppression may over-correct
  - Far-field targets: AUROC 0.789, FPR 0.198 (depth variance diminishes with distance)

- **First 3 experiments**:
  1. **Ablation on fusion weights**: Remove depth discontinuity component; measure AUROC drop (reported: −16.9% without anomaly fusion). Confirms depth contribution.
  2. **Shadow/specular suppression sensitivity**: Disable luminance/saturation suppression terms; measure FPR increase in high-contrast scenes. Validates nuisance filtering.
  3. **Curiosity weight calibration**: Train α_k with different regularization strengths; plot nDCG@10 vs regularization λ. Identifies optimal bias-variance trade-off.

## Open Questions the Paper Calls Out

- **Open Question 1**: How can uncertainty metrics be formally integrated into the learnable curiosity score to improve target prioritization robustness?
  - Basis in paper: Section 7 states "Future efforts target tighter uncertainty integration into the curiosity score," and Section 3.7 notes current uncertainty indicators are visualized but not formally part of the ranking mechanism.
  - Why unresolved: The current architecture computes uncertainty for explainability but does not utilize it as a weighted component in the mathematical formulation of the learnable curiosity score.
  - What evidence would resolve it: A modified curiosity score equation including uncertainty terms, validated by improved nDCG or Spearman correlation scores on ambiguous or low-texture test sets.

- **Open Question 2**: To what extent does extending the system to broader multimodal fusion (e.g., spectral or radar data) improve performance over the current monocular depth and RGB approach?
  - Basis in paper: Section 7 and Section 10.4 explicitly identify "broader multimodal fusion (e.g., spectral/radar)" as a target for future efforts.
  - Why unresolved: The current system relies solely on single-image depth estimation and RGB anomaly fusion; it does not process hyperspectral or radar inputs.
  - What evidence would resolve it: Comparative benchmarks showing AUROC and F1-Score improvements when spectral/radar modalities are fused with the existing visual anomaly maps.

- **Open Question 3**: How can active exploration policies be optimized for edge devices with strict power budgets while maintaining detection sensitivity?
  - Basis in paper: Section 7 lists "active exploration policies on edge devices with strict power budgets" as a specific goal for future work.
  - Why unresolved: The current work focuses on the perception and prioritization pipeline (ARTPS) rather than the active decision-making policies required for autonomous traversal.
  - What evidence would resolve it: Demonstration of a policy that dynamically balances sensor usage and processing load against battery life without significant degradation in target detection metrics.

## Limitations

- **Domain Generalization**: Performance on Mars-specific terrain types (low-texture expanses, overexposed regions) shows reduced accuracy (AUROC 0.867, FPR 0.112), suggesting limited robustness to planetary surface diversity.
- **Ground-Truth Dependence**: Curiosity-score training assumes expert-labeled priority annotations, but the paper does not specify the annotation schema or sample size, introducing uncertainty in the learning objective.
- **Depth Estimation Generalization**: Monocular depth estimation from single RGB images lacks ground truth validation; performance claims rely on proxy metrics rather than stereo/lidar comparison.

## Confidence

- **High Confidence**: Multi-component anomaly fusion mechanism and weighted combination of signals (AUROC 0.94, AUPRC 0.89 supported by ablation showing −16.9% drop without anomaly fusion).
- **Medium Confidence**: Depth-enhanced sensitivity claims (small-near object improvement) lack direct depth ground truth validation; performance metrics rely on proxy evaluations.
- **Low Confidence**: Learnable curiosity score transferability across missions; no corpus evidence for curiosity-driven planetary exploration, and learned weights may not generalize to different geological contexts.

## Next Checks

1. **Depth Component Ablation**: Remove depth discontinuity extraction from fusion; measure AUROC/AUPRC degradation. If drop < 3%, depth component may be underperforming relative to computational cost.
2. **Shadow/Specular Suppression Sensitivity**: Disable luminance/saturation suppression terms in fusion; quantify FPR increase in high-contrast scenes to validate nuisance filtering effectiveness.
3. **Cross-Mission Curiosity Transfer**: Train curiosity weights on Mars data, test on lunar or terrestrial analogs; measure rank metric degradation (nDCG, Kendall) to assess domain transferability.