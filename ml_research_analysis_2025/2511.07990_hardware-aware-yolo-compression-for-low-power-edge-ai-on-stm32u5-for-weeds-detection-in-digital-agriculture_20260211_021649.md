---
ver: rpa2
title: Hardware-Aware YOLO Compression for Low-Power Edge AI on STM32U5 for Weeds
  Detection in Digital Agriculture
arxiv_id: '2511.07990'
source_url: https://arxiv.org/abs/2511.07990
tags:
- detection
- pruning
- quantization
- weed
- inference
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper presents an optimized, low-power edge AI system for\
  \ real-time weed detection using the YOLOv8n object detector deployed on the STM32U575ZI\
  \ microcontroller. To meet strict hardware constraints, the authors applied structured\
  \ pruning (70% parameter reduction), 8-bit integer quantization, and input image\
  \ resolution scaling (224\xD7224 pixels)."
---

# Hardware-Aware YOLO Compression for Low-Power Edge AI on STM32U5 for Weeds Detection in Digital Agriculture

## Quick Facts
- arXiv ID: 2511.07990
- Source URL: https://arxiv.org/abs/2511.07990
- Reference count: 35
- mAP50 of 0.517 and mAP50-95 of 0.403 on CropAndWeed dataset with 70% parameter reduction

## Executive Summary
This paper presents an optimized, low-power edge AI system for real-time weed detection using the YOLOv8n object detector deployed on the STM32U575ZI microcontroller. To meet strict hardware constraints, the authors applied structured pruning (70% parameter reduction), 8-bit integer quantization, and input image resolution scaling (224×224 pixels). The model was trained and evaluated on the CropAndWeed dataset with 74 plant species. The compressed network achieved an mAP50 of 0.517 and mAP50-95 of 0.403, fitting within 850.97 KB flash and 677.30 KB RAM. The system consumes only 51.8 mJ per inference, enabling scalable deployment in power-constrained agricultural environments.

## Method Summary
The approach involves training YOLOv8n on the CropAndWeed dataset, applying iterative structured pruning with L2-norm filter ranking to remove 70% of parameters, performing static post-training quantization with mixed per-channel/symmetric schemes, and converting the model for STM32U575ZI deployment using STM32Cube.AI. The compressed model runs at 0.66 fps while consuming 51.8 mJ per inference, with 189-day battery life achieved through Stop2 low-power mode between inferences.

## Key Results
- mAP50 of 0.517 and mAP50-95 of 0.403 on 224×224 input resolution
- Model fits within 850.97 KB flash and 677.30 KB RAM constraints
- Energy consumption of 51.8 mJ per inference enabling 189-day battery life
- 70% parameter reduction through structured pruning with minimal accuracy loss

## Why This Works (Mechanism)

### Mechanism 1: Iterative Structured Pruning with L2-Norm Filter Ranking
Progressive structured pruning removes entire convolutional filters while preserving detection accuracy through iterative fine-tuning recovery cycles. L2-norm magnitude ranking identifies least-important filters; geometric decay schedule removes filters across k=6 steps with 30-epoch fine-tuning after each pruning iteration.

### Mechanism 2: Static Post-Training Quantization with Mixed Schemes
8-bit integer quantization with per-channel weight quantization (symmetric) and per-tensor activation quantization (asymmetric) achieves 4× memory reduction with minimal accuracy penalty. Min-max range estimation from 300-sample calibration set maps float32 to int8 via affine transformation.

### Mechanism 3: Hardware-Constrained Input Resolution Scaling
Reducing input resolution from 640×640 to 224×224 pixels enables RAM-compliant deployment at the cost of small object detectability. Resolution scaling reduces activation tensor sizes proportionally, lowering MAC operations and peak RAM usage to fit within 768KB SRAM constraint.

## Foundational Learning

- Concept: YOLOv8n Architecture (Backbone-Neck-Head Pipeline)
  - Why needed here: Pruning sensitivity varies across components—backbone C2f blocks tolerate aggressive pruning while neck concatenation and detection head layers must be preserved to maintain feature pyramid aggregation and bounding box output integrity.
  - Quick check question: Given the paper excludes concatenation and detection layers from pruning, which specific YOLOv8n modules (C2f, SPPF, neck up-sampling, prediction head) remain prunable and why?

- Concept: Quantization Granularity (Per-Tensor vs Per-Channel vs Per-Group)
  - Why needed here: The paper uses per-channel quantization for weights and per-tensor for activations—understanding this distinction is critical for reproducing accuracy results and debugging quantization-induced degradation.
  - Quick check question: Why does per-channel quantization improve weight representation accuracy compared to per-tensor, and what memory overhead does it introduce?

- Concept: ARM Cortex-M33 Low-Power Modes (Stop2 vs Sleep vs Active)
  - Why needed here: The deployment strategy achieves 189-day battery life by entering Stop2 mode (1.1mA) between inferences—understanding power state transitions is essential for system integration.
  - Quick check question: What peripherals remain active in Stop2 mode, and how does the RTC wake-up mechanism trigger inference resumption without full system reset?

## Architecture Onboarding

- Component map: Training: Ultralytics YOLOv8n → PyTorch → RTX 3090 (CUDA 12.2) → 300 epochs
- Critical path: 1) Train baseline YOLOv8n on CropAndWeed (224×224, 300 epochs, SGD) 2) Apply 70% structured pruning over k=6 iterations with 30-epoch fine-tuning each 3) Apply static PTQ with asymmetric activations, symmetric per-channel weights 4) Convert via STM32Cube.AI, verify flash ≤850KB, RAM ≤677KB 5) Deploy with Stop2 power management, measure energy with PPK2
- Design tradeoffs: mAP50 (0.517) vs flash (850.97KB): Acceptable for periodic monitoring, insufficient for precision spraying requiring >0.9 mAP; Latency (1510ms) vs energy (51.8mJ): Low throughput acceptable for 30-second sampling intervals; Resolution (224×224) vs small object detection: Fits RAM but sacrifices fine-grained weed identification
- Failure signatures: Pruning >70%: Non-linear mAP drop (>0.15 loss), detection head corruption; Quantizing detection layers: Invalid bounding box coordinates, confidence score overflow; RAM overflow on MCU after quantization: Hard fault during inference—monitor peak activation memory
- First 3 experiments: 1) Reproduce baseline training: Train uncompressed YOLOv8n on CropAndWeed 80-20 split at 224×224, validate mAP50-95 ≈0.45-0.50 before compression 2) Pruning ratio sweep: Test 50%, 60%, 70%, 75% pruning with k=6 steps on prunable layers only, plot mAP50-95 vs parameters retained 3) Energy profile validation: Deploy compressed model to NUCLEO-U575ZI-Q, measure current draw with Nordic PPK2 over 5-minute window, verify 10.4mA active / 1.1mA Stop2 and compute E=V×I×t to confirm ~51.8mJ per inference

## Open Questions the Paper Calls Out

### Open Question 1
Can Neural Architecture Search (NAS) generate a specialized architecture that outperforms the pruned YOLOv8n in terms of energy efficiency while maintaining detection accuracy? The conclusion states future work will explore "the use of Neural Architecture Search to further enhance model energy consumption while maintaining detection accuracy."

### Open Question 2
Does the integration of external hardware accelerators significantly improve the system's 0.66 fps throughput without increasing the 51.8 mJ energy cost? The conclusion lists "the integration of hardware accelerators" as a specific direction for future research.

### Open Question 3
Can low-rank approximation or transfer learning recover the spatial feature loss caused by input scaling, thereby improving the detection of small or occluded weeds? The discussion notes that the reduced 224×224 input resolution limits the detection of small objects, while the conclusion suggests "low rank approximation and transfer learning" for future optimization.

## Limitations
- Limited validation of calibration set selection and activation range estimation for quantization
- No comprehensive ablation studies on pruning schedule sensitivity
- Small object detection capability compromised by resolution scaling to 224×224

## Confidence
- High: 70% pruning with 0.15 mAP50-95 degradation follows established structured pruning methods
- Medium: Static PTQ efficiency claim lacks validation of calibration set representativeness
- Medium: Stop2 power mode battery life projection assumes ideal conditions without startup transients

## Next Checks
1. Validate pruning sensitivity by testing 60% and 75% sparsity levels with identical fine-tuning protocols to confirm non-linear accuracy collapse occurs at 70%
2. Verify calibration set representativeness by measuring mAP50-95 variance across multiple random 300-sample subsets before quantization
3. Characterize power consumption in active vs. Stop2 modes over extended deployments (>24 hours) to confirm battery life projections and detect startup energy penalties