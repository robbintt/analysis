---
ver: rpa2
title: Learning to Defer for Causal Discovery with Imperfect Experts
arxiv_id: '2502.13132'
source_url: https://arxiv.org/abs/2502.13132
tags:
- expert
- causal
- discovery
- where
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work tackles the problem of combining imperfect expert knowledge,
  such as from large language models, with data-driven causal discovery methods in
  a reliable way. The core idea is to use a learning-to-defer approach where, for
  each causal query, the system decides whether to trust the expert or a statistical
  causal discovery method based on the query's metadata.
---

# Learning to Defer for Causal Discovery with Imperfect Experts

## Quick Facts
- arXiv ID: 2502.13132
- Source URL: https://arxiv.org/abs/2502.13132
- Authors: Oscar Clivio; Divyat Mahajan; Perouz Taslakian; Sara Magliacane; Ioannis Mitliagkas; Valentina Zantedeschi; Alexandre Drouin
- Reference count: 13
- Primary result: Learning-to-defer framework improves causal discovery accuracy by selectively using expert (LLM) vs. data-driven method based on textual metadata.

## Executive Summary
This work introduces a method for combining imperfect expert knowledge, such as from large language models, with data-driven causal discovery methods. The core idea is to learn when to trust an expert versus a statistical method for pairwise causal queries by training a deferral function based on query metadata. The approach, called L2D-CD, is evaluated on the Tübingen pairs dataset and demonstrates improved accuracy over using either the expert or the causal discovery method alone. The framework can also identify domains where the expert performs well or poorly, and the paper outlines a strategy for extending this to multi-variable causal discovery.

## Method Summary
The method trains a deferral function that learns when an expert is likely to be correct versus when to use a statistical causal discovery method. For each pairwise causal query with textual metadata, the system decides whether to trust the expert or the statistical method. The deferral function is trained post-hoc on instances where the two predictors disagree, using a binary classifier (Random Forest) trained on textual embeddings. At inference, if the expert and statistical method agree, their prediction is used; if they disagree, the deferral function decides which to trust. The approach is evaluated on the Tübingen pairs dataset with 99 causal pairs, using 50-dimensional text embeddings and comparing against baselines like LiNGAM, RECI, and bQCD.

## Key Results
- L2D-CD improves causal direction prediction accuracy over using either the expert or statistical method alone on Tübingen pairs.
- The deferral function can identify domains where the expert is strong (e.g., Physics) versus weak (e.g., Medicine) with statistical significance.
- The framework outlines a strategy for extending pairwise deferral to multi-variable causal graph discovery via ranking algorithms.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Post-hoc Learning-to-Defer with a 0-1 cost function for a binary label reduces to standard binary classification.
- Mechanism: When both the base predictor (causal discovery method) and expert predictor (LLM) are pre-fitted, the deferral loss function simplifies. The system trains a classifier on instances where the two predictors disagree, with the label indicating which predictor was correct. This classifier becomes the deferral function, deciding which source to trust for a given input.
- Core assumption: The base predictor and expert predictor make independent errors, though the authors note this is often unrealistic. Assumption: A meaningful training set exists where ground-truth causal relationships are known.
- Evidence anchors:
  - [abstract] "By adapting learning-to-defer (L2D) algorithms for pairwise causal discovery (CD), we learn a deferral function..."
  - [section 2.3] "...post-hoc L2D with a 0-1 cost function and a single expert can be reduced to standard classification... This amounts to learning to predict when the expert is correct or, equivalently, when the base predictor is wrong."
  - [corpus] Related paper "Expert-Agnostic Learning to Defer" (arXiv:2502.10533) explores generalization to unseen experts, a relevant extension of the core L2D mechanism.
- Break condition: The reduction to binary classification fails if the predictors always agree or always disagree, leaving an empty or universal training set. Performance degrades significantly if the causal discovery method and LLM have highly correlated, systematic errors.

### Mechanism 2
- Claim: Instance-level deferral based on textual metadata can identify domain-specific expert strengths and weaknesses.
- Mechanism: The deferral function uses an embedding of the textual description (metadata) associated with a causal pair as input. By training on a diverse set of pairs from various domains, it learns a mapping from the semantic features of a problem description to the likelihood of the expert being correct. This allows it to selectively defer to the expert on domains where it historically performs well.
- Core assumption: The metadata contains predictive signal regarding expert competence. Assumption: The expert's performance is not uniformly random but clusters by domain.
- Evidence anchors:
  - [abstract] "...selects whether to rely on classical causal discovery methods... or expert recommendations based on textual meta-data."
  - [section 3] "L2D-CD can identify strong and weak domains for the expert... [it] is domain-consistent for almost every synthetic expert..."
  - [corpus] Weak corpus signal for this specific mechanism; related work on "Dynamic Expert-Guided Model Averaging" (arXiv:2601.16715) also deals with expert integration but via different methods.
- Break condition: The mechanism fails if the textual metadata is uninformative or if the expert's performance varies unpredictably within a domain (high intra-domain variance).

### Mechanism 3
- Claim: A pairwise deferral approach can be extended to multi-variable causal discovery via ranking algorithms.
- Mechanism: The system can infer the ancestry relationship for any pair of nodes in a larger graph. By sampling multiple pairs and inferring their relationships using the L2D-CD method, one can generate a set of (possibly noisy or contradictory) pairwise preferences. A separate ranking algorithm is then applied to these preferences to infer a global topological ordering of the variables.
- Core assumption: A consistent topological ordering can be recovered from noisy pairwise comparisons. Assumption: The deferral function's accuracy is sufficient to make the noise manageable for standard ranking algorithms.
- Evidence anchors:
  - [abstract] "Finally, we outline a strategy for generalizing this approach to causal discovery on graphs with more than two variables..."
  - [section 4] "...we propose building on methods for ranking from pairwise comparisons... apply [a ranking algorithm] to the resulting comparisons to obtain a topological ordering..."
  - [corpus] Related paper "Can LLMs Leverage Observational Data?..." (arXiv:2504.10936) explores integrating LLMs with data-driven methods but doesn't focus on this ranking extension.
- Break condition: The extension will produce an invalid or highly inaccurate graph if the pairwise deferral function has high error rates, creating too many incorrect comparisons for the ranking algorithm to resolve into a coherent order.

## Foundational Learning

- Concept: **Learning-to-Defer (L2D)**.
  - Why needed here: This is the core framework for combining predictions from two imperfect systems (statistical method and an LLM expert). It formalizes the trade-off between using a data-driven method versus a knowledge-based expert on a case-by-case basis.
  - Quick check question: How does L2D-CD differ from a simple ensemble that averages the predictions of the expert and the CD method? (Answer: L2D-CD makes a discrete choice to defer to *one* of them, whereas an ensemble typically combines their outputs.)

- Concept: **Causal Discovery and Markov Equivalence Classes (MEC)**.
  - Why needed here: The paper addresses the core challenge of causal discovery: identifying causal relationships from observational data, which often only identifies a set of statistically indistinguishable graphs (the MEC). The method aims to resolve ambiguity within the MEC using expert knowledge.
  - Quick check question: Why can't standard conditional independence tests alone uniquely identify the causal graph? (Answer: Because multiple different DAGs can imply the same set of conditional independence relationships.)

- Concept: **0-1 Loss vs. Surrogate Loss**.
  - Why needed here: The core L2D loss function is non-differentiable. Understanding the use of a convex surrogate loss (like logistic loss) is necessary to understand how the deferral function is practically optimized using gradient-based methods.
  - Quick check question: Why does the paper introduce a surrogate loss instead of directly optimizing the deferral accuracy? (Answer: The original 0-1 loss is non-differentiable, making it difficult to optimize with standard gradient descent.)

## Architecture Onboarding

- Component map: Expert Predictor (LLM) -> Base CD Predictor (LiNGAM/RECI/bQCD) -> Deferral Function (Random Forest) -> Final Prediction
- Critical path: Training Data -> Pre-fit Predictors -> Disagreement Set (S) -> Train Deferral Function -> Inference (Defer or Predict). The most critical step is creating the high-quality training set for the deferral function by identifying cases where the expert and CD method disagree and ground truth is available.
- Design tradeoffs:
  - Post-hoc vs. Joint Training: The paper uses post-hoc training (L2D after predictors are fixed), which is simpler but doesn't allow the predictors to adapt to the deferral function. Joint training might yield better system-level performance.
  - Embedding Model: The choice of embedding model for the textual metadata affects the deferral function's ability to capture semantic nuances.
  - Choice of Base CD Method: Different CD methods (LiNGAM, RECI, bQCD) have different assumptions and failure modes, which impacts the learned deferral strategy.
- Failure signatures:
  - Constant Defer: If the deferral function always chooses one predictor, the system is not learning to defer effectively.
  - Random Defer: If the deferral function shows no correlation with predictor correctness, performance will be an average of the two methods, failing to capture their complementary strengths.
  - Poor Embedding: If the deferral accuracy is near chance, the textual metadata may lack predictive signal, or the embedding is failing to capture it.
- First 3 experiments:
  1. Reproduce Baseline: On the Tübingen pairs split, train a simple classifier to predict causal direction using only the metadata embedding. Compare against a standard CD method (e.g., LiNGAM) to establish baseline performance of the "expert" and the "data-driven" approach.
  2. Ablate Deferral Function: Implement the L2D-CD training pipeline. Compare the full system against a "random deferral" baseline (as done in the paper) to prove that the learned deferral function provides value beyond chance.
  3. Analyze Domain Consistency: Train the system with a synthetic expert that has known strong/weak domains. Verify that the learned deferral function correctly assigns a higher probability of deferral to the expert's strong domains, as measured by the domain consistency hypothesis test.

## Open Questions the Paper Calls Out

- How can the L2D-CD framework be rigorously extended to full causal graph discovery with more than two variables?
  - Basis in paper: [explicit] The abstract and conclusion state that the authors "outline a strategy" for graphs with more than two variables, identifying the full implementation and evaluation as an area for "further research."
  - Why unresolved: The current methodology and experiments are restricted to pairwise causal discovery; extending this to full graphs requires integrating the deferral mechanism with ranking algorithms to handle conflicting or non-transitive pairwise predictions.
  - What evidence would resolve it: An implementation of the proposed ranking-based strategy applied to a multivariate causal discovery benchmark (e.g., DAGs with >2 nodes) demonstrating improved performance over base methods.

- How should the "absence of ancestry" be handled algorithmically when aggregating pairwise comparisons into a global graph ordering?
  - Basis in paper: [explicit] Section 4 identifies "handling the absence of ancestry in pairwise comparisons" as a key challenge for the proposed extension to larger graphs.
  - Why unresolved: Standard ranking algorithms typically force a total ordering, but causal graphs may contain variables with no ancestral relationship; the paper does not propose a specific solution for pruning edges or handling this "zero" class.
  - What evidence would resolve it: A defined mechanism within the ranking algorithm (e.g., a specific edge pruning step) that successfully differentiates between "no relationship" and "low confidence" to produce accurate sparse graphs.

- Can the L2D-CD training procedure be generalized to optimally defer among multiple experts (n_e > 1) without violating consistency bounds?
  - Basis in paper: [explicit] The Conclusion lists the limitation that "our training procedure does not generalize straightforwardly to more than two methods."
  - Why unresolved: The reduction to standard binary classification (Lemma 2.1 and Section 2.3) relies specifically on having a single expert (n_e=1), and the underlying surrogate loss assumptions for L2D become more complex with multiple experts.
  - What evidence would resolve it: A modified loss function or training scheme that supports multiple distinct experts while maintaining the theoretical consistency guarantees described in Section 2.2.

## Limitations

- The framework is currently limited to pairwise causal discovery and requires manual extension for multi-variable graphs.
- The training procedure for the deferral function does not straightforwardly generalize to deferring among multiple experts (n_e > 1).
- The paper relies on a small dataset (99 pairs) and synthetic experiments for domain consistency claims, limiting generalizability.

## Confidence

- Claim: L2D-CD improves causal discovery accuracy over baselines.
  - Confidence: Medium
- Claim: The deferral function can reliably identify domain-specific expert strengths and weaknesses.
  - Confidence: Low
- Claim: The proposed ranking-based strategy can extend L2D-CD to multi-variable causal discovery.
  - Confidence: Low

## Next Checks

1. Reproduce the core L2D-CD pipeline on a held-out subset of the Tübingen pairs to verify the claimed accuracy gains over baselines.
2. Conduct a sensitivity analysis on the deferral function's domain consistency by testing with multiple synthetic experts having varied domain-specific error patterns.
3. Implement and evaluate the proposed ranking-based extension on a small multi-variable synthetic dataset to assess its feasibility and error tolerance.