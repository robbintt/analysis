---
ver: rpa2
title: LLM Cannot Discover Causality, and Should Be Restricted to Non-Decisional Support
  in Causal Discovery
arxiv_id: '2506.00844'
source_url: https://arxiv.org/abs/2506.00844
tags:
- causal
- llms
- information
- knowledge
- discovery
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper critiques the use of large language models (LLMs) for
  causal discovery, arguing that LLMs cannot reliably identify causal relationships
  due to their autoregressive, correlation-driven modeling, which lacks theoretical
  grounding in causal reasoning. Empirical studies show that LLM performance is highly
  sensitive to prompt design and text quality, and that even with observational data,
  LLMs fail to effectively leverage numerical features for causal inference.
---

# LLM Cannot Discover Causality, and Should Be Restricted to Non-Decisional Support in Causal Discovery

## Quick Facts
- **arXiv ID:** 2506.00844
- **Source URL:** https://arxiv.org/abs/2506.00844
- **Reference count:** 40
- **Primary result:** LLMs should be restricted to non-decisional heuristic support in causal discovery, as autoregressive modeling prevents reliable causal inference.

## Executive Summary
This paper argues that large language models (LLMs) cannot reliably perform causal discovery due to their autoregressive, correlation-driven modeling that lacks theoretical grounding in causal reasoning. The authors demonstrate that LLM performance is highly sensitive to prompt design and text quality, and that integrating LLMs as priors in causal discovery algorithms undermines reliability and theoretical guarantees. Instead, they propose restricting LLMs to non-decisional roles such as guiding heuristic search processes, which empirical results show accelerates convergence while maintaining theoretical soundness.

## Method Summary
The proposed method uses LLMs (GPT-4, Claude 3, GPT-3.5) to guide evolutionary algorithms for causal structure learning. LLMs process variable metadata and background knowledge to suggest search heuristics like edge pruning or mutation candidates, but never directly determine edge existence. The final structural decisions rely solely on classical scoring functions (BIC/BDeu) calculated from observational data. The approach is tested on 10 `bnlearn` datasets with 1,000 samples each, comparing LLM-guided heuristic search against traditional methods and LLM-as-prior approaches.

## Key Results
- LLM-guided heuristic search accelerates convergence and outperforms both traditional CDAs and existing LLM-based methods in causal structure learning
- LLM performance is highly sensitive to prompt design and text quality, with near-random performance on numerical features
- Integrating LLM outputs as soft constraints breaks theoretical guarantees like score consistency and decomposability
- LLM-guided search maintains statistical rigor while leveraging linguistic knowledge for search space pruning

## Why This Works (Mechanism)

### Mechanism 1
Autoregressive modeling in LLMs is fundamentally misaligned with the probability decomposition required for causal inference. LLMs model probability via sequential word order ($P(x_t|x_{<t})$), treating all prior context as potentially relevant, while Structural Causal Models decompose probability based on directed acyclic graph topology ($P(X_i|pa(X_i))$). This sequential dependency prevents LLMs from naturally "switching off" influences from non-parent nodes, leading to spurious correlations rather than causal links.

### Mechanism 2
Integrating LLM outputs as "soft constraints" into score-based Causal Discovery Algorithms invalidates mathematical guarantees like Score Consistency. Traditional CDAs rely on decomposable and locally consistent scoring functions, but adding LLM-derived prior scores from different probability spaces breaks these properties. The paper argues this distortion of the optimization landscape can cause algorithms to favor hallucinated structures over statistical evidence.

### Mechanism 3
Confining LLMs to "non-decisional" search guidance accelerates convergence without compromising theoretical guarantees. The LLM suggests where to search next rather than determining edge existence, pruning the search space based on linguistic knowledge while the statistical scoring layer makes final decisions. This leverages the LLM's broad knowledge while keeping the causal judge strictly statistical.

## Foundational Learning

- **Concept: Structural Causal Models (SCMs) vs. Autoregression**
  - **Why needed here:** To understand why LLMs are structurally incapable of causal reasoning due to sequence modeling vs. graphical dependencies
  - **Quick check question:** Can an autoregressive model ($P(X_t|X_{<t})$) inherently represent the conditional independence $X \perp Y | Z$ without specific architectural constraints?

- **Concept: Score Consistency and Decomposability**
  - **Why needed here:** To grasp why adding LLM "priors" breaks traditional algorithms and violates theoretical foundations
  - **Quick check question:** Why does adding a non-decomposable prior term to a decomposable score function (like BIC) break standard optimization algorithms like Hill Climbing?

- **Concept: Heuristic Search (Evolutionary Algorithms)**
  - **Why needed here:** The proposed solution uses LLMs to guide mutations/crossover, requiring understanding of search operators
  - **Quick check question:** In an evolutionary algorithm, what is the difference between a "mutation operator" (allowed for LLMs) and a "fitness function" (forbidden for LLMs)?

## Architecture Onboarding

- **Component map:** Variable Metadata & Background Text -> LLM Interface (Restricted) -> Search Engine (CDA) -> Validation Layer (Scoring) -> Output DAG
- **Critical path:** The isolation layer between the LLM and the Scoring Function; the system fails if LLM suggestions are mathematically weighted equivalent to data observations
- **Design tradeoffs:**
  - Prompt Fidelity vs. Leakage: High-quality prompts improve results but risk data leakage; prompts must elicit reasoning rather than memorization
  - Search Efficiency vs. Robustness: Heavy LLM initialization speeds search but risks missing counter-intuitive causal links that data supports
- **Failure signatures:**
  - "Puppet Master" Failure: LLM priors dominate the score function, resulting in linguistically correct but statistically poor graphs
  - "Prompt Hack": Results improve only when dataset names are mentioned, indicating memorization rather than discovery
  - Numerical Blindness: LLM fails with generic variable IDs or high-precision raw data rather than semantic names
- **First 3 experiments:**
  1. Run LLM-guided search on synthetic data with semantically misleading variable names to test statistical correction of heuristic bias
  2. Compare three setups: LLM as Decision Maker, LLM as Soft Prior, LLM as Search Guide (Proposed) measuring SHD and convergence time
  3. Feed LLM interface data with varying decimal precision (1-8 digits) to find the threshold where tokenization degrades causal suggestions

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can LLM architectures or training objectives be fundamentally redesigned to align with structural causal principles rather than word-order dependencies?
- **Basis in paper:** The conclusion calls for "rethinking model design and training objectives to align with the structural and statistical principles of causality"
- **Why unresolved:** Current autoregressive modeling decomposes probability based on word order, which inherently conflicts with structural causal models
- **What evidence would resolve it:** A new model architecture that can natively represent and learn the factorization $P(X_i|pa(X_i))$ without sequential token prediction

### Open Question 2
- **Question:** Is there a mathematically valid method to integrate LLM-derived priors into score-based causal discovery without violating theoretical guarantees?
- **Basis in paper:** Section 4 critiques soft-constraint methods for adding terms from different probability spaces and violating decomposability
- **Why unresolved:** Simply adding $\sigma(G; D)$ and $\sigma(G; \lambda)$ is mathematically incorrect and breaks optimization algorithms' theoretical foundations
- **What evidence would resolve it:** A theoretically grounded scoring function that incorporates LLM knowledge while preserving score equivalence and consistency

### Open Question 3
- **Question:** Can specialized tokenization or external tool integration enable LLMs to effectively process high-precision numerical observational data for causal inference?
- **Basis in paper:** Position 3.2 identifies that LLMs fail to leverage numerical features, performing near random guessing with high-precision data due to tokenization distortions
- **Why unresolved:** Current LLMs lack intrinsic mechanisms to exploit numerical features, limiting them to text-based reasoning
- **What evidence would resolve it:** An LLM-augmented method that achieves significant performance improvements through analysis of high-precision numerical samples

## Limitations
- The theoretical argument about autoregressive limitations is compelling but assumes no architectural variant could bridge the gap
- Experimental validation focuses on synthetic and moderate-sized datasets; high-dimensional, noisy real-world data performance remains untested
- The "non-decisional" isolation layer is conceptually defined but lacks explicit mathematical proof of preserving all CDA guarantees

## Confidence
- **High Confidence:** Empirical finding that LLM-guided heuristic search outperforms both pure traditional CDAs and LLM-as-prior approaches in convergence speed and accuracy
- **Medium Confidence:** Theoretical claim that autoregressive modeling fundamentally prevents causal discovery, based on structural probability arguments
- **Medium Confidence:** Assertion that LLM priors break score consistency and decomposability, with sound mathematical critique but potential implementation-specific counterexamples

## Next Checks
1. **Domain Bias Test:** Apply LLM-guided heuristic search to genomics data where LLM knowledge is likely incomplete or biased, measuring whether statistical scoring reliably corrects heuristic errors
2. **Architectural Boundary Test:** Replace autoregressive LLM with a transformer trained to output structural equations, measuring if this variant can achieve causal discovery without heuristic guidance
3. **Scalability Stress Test:** Run the proposed method on 100+ variable datasets with 10,000+ samples, measuring computational cost and whether the statistical layer maintains corrective function under extreme scale