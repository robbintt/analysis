---
ver: rpa2
title: 'AV-Dialog: Spoken Dialogue Models with Audio-Visual Input'
arxiv_id: '2511.11124'
source_url: https://arxiv.org/abs/2511.11124
tags:
- 'null'
- dialogue
- speech
- audio-visual
- audio
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: AV-Dialog is the first multimodal dialogue system that combines
  audio and visual inputs to improve spoken dialogue in noisy, multi-speaker environments.
  By using acoustic tokenization and multi-task, multi-stage training, it achieves
  robust streaming transcription and turn-taking prediction.
---

# AV-Dialog: Spoken Dialogue Models with Audio-Visual Input

## Quick Facts
- **arXiv ID**: 2511.11124
- **Source URL**: https://arxiv.org/abs/2511.11124
- **Reference count**: 25
- **Primary result**: First multimodal dialogue system combining audio and visual inputs for robust spoken dialogue in noisy, multi-speaker environments

## Executive Summary
AV-Dialog is the first multimodal dialogue system that combines audio and visual inputs to improve spoken dialogue in noisy, multi-speaker environments. By using acoustic tokenization and multi-task, multi-stage training, it achieves robust streaming transcription and turn-taking prediction. Experiments show it outperforms audio-only models, reducing transcription errors and improving turn-taking accuracy from 54% to 79% under interference. Human evaluations demonstrate a +1.75-point MOS improvement in dialogue naturalness and a +1.99-point MOS gain in response relevance and helpfulness.

## Method Summary
AV-Dialog uses a dual-model architecture (or unified variant) built on LLaMA3-8B. Audio is processed via DAC tokenizer (16 codebooks at 25Hz from 40ms chunks), while visual input uses AV-HuBERT encoder with dlib face detection. The system undergoes two-stage training: Stage 1 multi-task pretraining (text continuation, ASR, audio captioning, AVSR) for modality alignment, followed by Stage 2 dialogue fine-tuning (Fisher + InterAct) with synthetic mixing augmentation. Turn events are explicitly supervised with special tokens (<SOT>, <SOB>, <EMP>). Inference streams tokens to a text backbone with ~120ms visual latency from 2-frame lookahead, producing streaming responses via Moshi's Mimi codec.

## Key Results
- WER under interference: 30.8% (Audio+Visual) vs 67.0% (DinoSR+Visual)
- Turn-taking response ratio under interference: 78.8% (Audio+Visual) vs 65.8% (Audio-only)
- Human evaluation MOS improvements: +1.75 for naturalness, +1.99 for relevance/helpfulness

## Why This Works (Mechanism)

### Mechanism 1
Acoustic tokens (DAC) enable superior speaker differentiation under interference compared to semantic tokens (HuBERT/DinoSR). DAC preserves raw acoustic characteristics (voice timbre, pitch), allowing the model to distinguish target speakers from interferers based on voice fingerprints. This works when target and interferers have acoustically distinguishable voice characteristics.

### Mechanism 2
Visual lip cues provide orthogonal information that stabilizes turn-taking prediction when audio is corrupted. AV-HuBERT extracts lip-centric representations that correlate with speech production; when audio SNR degrades, visual stream maintains usable signal for both transcription and turn-end detection. This requires adequate lighting, frontal face visibility, and minimal occlusion.

### Mechanism 3
Explicit turn-change token supervision improves both turn-taking accuracy and response generation quality in unified models. Training with <SOT> tokens creates a learned boundary signal that triggers response generation, rather than relying on implicit silence detection which confuses pauses with turn-ends. This requires accurately aligned turn annotations.

## Foundational Learning

- **Audio-Visual Speech Recognition (AVSR)**: Understanding how lip features complement audio is prerequisite for debugging fusion failures. Quick check: Can you explain why AVSR degrades gracefully while audio-only ASR catastrophically fails at low SNR?

- **Streaming Token Prediction with KV-Cache**: The dual-model architecture streams tokens to a text backbone while maintaining context; misunderstanding streaming semantics leads to incorrect latency analysis. Quick check: What is the algorithmic latency contribution of the visual encoder's 2-frame lookahead?

- **Full-Duplex Dialogue Modeling**: AV-Dialog predicts turn-taking without explicit prompts; this differs fundamentally from turn-based systems. Quick check: How does the FTO (Floor Transfer Offset) metric differ from simple response latency?

## Architecture Onboarding

- **Component map**: 40ms audio chunks → DAC tokenizer (16 codebooks at 25Hz) + dlib face detection → AV-HuBERT visual encoder → LLaMA3-8B with audio/visual projection adapters → dual output streams (text Un, turn events Tn) → text backbone (dual model) → Moshi TTS

- **Critical path**: 1) Stage 1 multi-task pretraining (text continuation, ASR, audio captioning, AVSR) → modality alignment 2) Stage 2 dialogue fine-tuning (Fisher + InterAct) with synthetic mixing augmentation → turn-taking learning 3) Inference: 40ms chunks → parallel audio/visual encoding → token streaming to backbone → TTS output

- **Design tradeoffs**: Dual vs Unified (66.6% vs 29.6% pickup ratio, but unified has lower latency), Acoustic vs Semantic tokens (robust to interference vs compact), Explicit turn supervision (essential for dual, 2x improvement for unified)

- **Failure signatures**: WER spikes under interference with visual-only (check face detection failures), Late turn-taking (>3s FTO) (verify <SOT> token loss weight), Irrelevant responses in unified model (insufficient Stage 1 alignment)

- **First 3 experiments**: 1) Ablate visual input at SNR -8dB, 0dB, +8dB (plot WER degradation curve), 2) Compare DAC vs DinoSR tokenizer with identical training (replicate Table 6), 3) Stress-test turn-taking with synthetic pause patterns (500ms-2000ms mid-turn pauses)

## Open Questions the Paper Calls Out
- Can incorporating non-verbal auditory (laughter, sighs) and visual (facial expressions, gestures) cues beyond lip movements significantly enhance interaction human-likeness?
- How can visual encoders be improved to maintain robustness under poor lighting, occlusions, or extreme head poses?
- What training interventions are required to close the helpfulness gap between the unified and dual-model architectures?

## Limitations
- Dataset access: InterAct dataset used for turn-taking evaluation is not publicly available
- Visual modality fragility: Performance degrades under poor lighting, occlusions, or extreme head poses
- Real-world generalization: Results against controlled interference may not extend to non-stationary noise sources

## Confidence
- Audio-visual superiority under interference (WER 30.8% vs 67.0%): High confidence
- Turn-taking prediction improvement (54%→79% accuracy): Medium confidence
- MOS improvements (+1.75 naturalness, +1.99 relevance): Low confidence

## Next Checks
1. Implement DAC tokenization pipeline and validate acoustic token advantage over semantic tokens (DinoSR) across SNR -8dB to +8dB on public multi-speaker datasets
2. Systematically degrade visual input quality and measure WER/turn-taking accuracy degradation curves to confirm 3.2% face detection failure rate
3. Create synthetic test cases with controlled pause patterns to validate explicit <SOT> supervision prevents false turn-end detection while maintaining sensitivity to genuine completion cues