---
ver: rpa2
title: 'InPhyRe Discovers: Large Multimodal Models Struggle in Inductive Physical
  Reasoning'
arxiv_id: '2509.12263'
source_url: https://arxiv.org/abs/2509.12263
tags:
- collision
- physical
- will
- reasoning
- cube
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces INPHYRE, the first visual question answering
  benchmark to evaluate inductive physical reasoning in large multimodal models (LMMs).
  INPHYRE assesses LMMs' ability to predict outcomes of collision events, particularly
  in scenarios that violate universal physical laws.
---

# InPhyRe Discovers: Large Multimodal Models Struggle in Inductive Physical Reasoning

## Quick Facts
- **arXiv ID**: 2509.12263
- **Source URL**: https://arxiv.org/abs/2509.12263
- **Reference count**: 40
- **Primary result**: Large multimodal models struggle with inductive physical reasoning when demonstration samples violate universal physical laws.

## Executive Summary
This paper introduces INPHYRE, the first visual question answering benchmark to evaluate inductive physical reasoning in large multimodal models (LMMs). INPHYRE assesses LMMs' ability to predict outcomes of collision events, particularly in scenarios that violate universal physical laws. By providing demonstration samples, INPHYRE measures how well LMMs can infer underlying physics from exemplars and apply it to make predictions. The study evaluates 13 diverse LMMs and reveals three key findings: (1) LMMs have limited parametric knowledge about universal physical laws and struggle to apply them even in scenarios that follow these laws, (2) inductive physical reasoning in LMMs is weak when demonstration samples violate universal physical laws, and (3) inductive physical reasoning in LMMs suffers from language bias, largely ignoring visual inputs in demonstration samples. These results question the trustworthiness of LMMs regarding visual inputs and highlight the need for improved instruction-tuning approaches that account for potential language biases.

## Method Summary
INPHYRE evaluates LMMs on visual question answering tasks involving collision events using algorithmically generated synthetic videos created with PyBullet physics simulation and Blender rendering. The benchmark includes 7 irregular scenarios that violate physical laws (like objects continuing with same velocity after collision or red objects passing through others) and 3 regular scenarios following true physics. Each video is 240 frames, with 8 uniformly sampled frames used for evaluation. The study uses ~10 exemplars per scenario for few-shot evaluation and employs multiple-choice questions with shuffled answer options. Models are evaluated in both zero-shot and few-shot (3-shot) settings, with performance measured through accuracy and language bias analysis comparing video-text versus video-only conditions.

## Key Results
- LMMs demonstrate limited parametric knowledge about universal physical laws and struggle to apply them even in regular scenarios
- Inductive physical reasoning performance drops significantly when demonstration samples violate universal physical laws
- LMMs exhibit language bias, largely ignoring visual inputs in demonstration samples and relying primarily on textual descriptions

## Why This Works (Mechanism)
Unknown. The paper does not explicitly detail the mechanism behind why LMMs struggle with inductive physical reasoning when demonstration samples violate universal physical laws. The observed failures could stem from architectural limitations, insufficient training data, or the fundamental difficulty of learning from examples that contradict established physical principles.

## Foundational Learning
- **Inductive physical reasoning**: The ability to infer underlying physical principles from observed examples and apply them to novel situations. Needed because LMMs must generalize from demonstrations rather than relying solely on pre-existing knowledge. Quick check: Can models predict collision outcomes in previously unseen scenarios based on limited examples?
- **Universal physical laws**: Fundamental principles like conservation of momentum and energy that govern real-world interactions. Needed as the benchmark tests whether LMMs can recognize and apply these laws. Quick check: Do models correctly apply conservation principles in regular scenarios?
- **Language bias in multimodal models**: Tendency to prioritize textual information over visual inputs when both are available. Needed to understand why LMMs might ignore visual demonstrations. Quick check: Compare performance differences between video-text and video-only conditions.
- **Synthetic data generation**: Creating controlled test scenarios using physics simulation and rendering. Needed to systematically vary physical laws while maintaining visual consistency. Quick check: Verify that generated scenarios correctly implement intended physical violations.
- **Few-shot learning**: Ability to learn from a small number of examples rather than requiring extensive training data. Needed to test inductive reasoning capabilities. Quick check: Measure performance improvement from zero-shot to few-shot settings.
- **Visual question answering**: Task requiring models to answer questions based on visual content. Needed as the evaluation framework for physical reasoning. Quick check: Ensure models can accurately answer questions about regular scenarios.

## Architecture Onboarding

**Component Map:**
Video Generation (PyBullet + Blender) -> Question Template Generation -> Exemplar Selection -> LMM Evaluation (Zero-shot and Few-shot) -> Performance Analysis

**Critical Path:**
Synthetic video generation → Exemplar creation (video-text and video-only) → Model evaluation (zero-shot on regular, few-shot on both) → Performance comparison between regular/irregular scenarios → Language bias analysis

**Design Tradeoffs:**
- Synthetic vs. real-world data: Synthetic provides control but may not capture real-world complexity
- Multiple-choice format: Easier evaluation but may not capture full reasoning capabilities
- Fixed frame sampling: Ensures consistency but may miss important visual information
- Limited exemplars: Tests few-shot learning but may not provide sufficient learning signal

**Failure Signatures:**
- Descriptive reasoning instead of option selection (e.g., "Therefore, both objects may move" vs "D")
- Context length exceeded with multiple video exemplars
- Hallucinations about collision properties not in prompt
- High failure rate in video-only conditions indicating language bias

**3 First Experiments:**
1. Generate synthetic collision videos for all 10 scenarios (7 irregular + 3 regular) using PyBullet and Blender
2. Create question-answer templates and generate evaluation samples with shuffled options
3. Evaluate LLaVA-OneVision in zero-shot setting on regular scenarios to establish baseline performance

## Open Questions the Paper Calls Out
None

## Limitations
- Synthetic video generation may not capture real-world physical complexity
- Multiple-choice format may not fully assess reasoning capabilities
- Language bias metric potentially underestimates actual bias due to high failure rates in video-only settings
- Study doesn't distinguish between architectural constraints and training data limitations

## Confidence

**High Confidence:**
- LMMs struggle with inductive physical reasoning in scenarios violating universal laws

**Medium Confidence:**
- Language bias significantly affects model performance

**Low Confidence:**
- Findings broadly question trustworthiness of LMMs regarding visual inputs

## Next Checks
1. Evaluate the same LMMs on physical reasoning tasks using real-world video data to determine if synthetic data limitations explain observed failures
2. Fine-tune a subset of evaluated LMMs specifically on physical reasoning tasks using the INPHYRE dataset to determine whether poor performance reflects fundamental architectural limitations versus insufficient training
3. Systematically remove or modify different components of the evaluation pipeline (e.g., frame sampling rate, question complexity, visual context) to isolate which aspects most contribute to LMMs' reasoning failures