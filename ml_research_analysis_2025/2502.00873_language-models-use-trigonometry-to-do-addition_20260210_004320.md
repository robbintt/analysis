---
ver: rpa2
title: Language Models Use Trigonometry to Do Addition
arxiv_id: '2502.00873'
source_url: https://arxiv.org/abs/2502.00873
tags:
- helix
- addition
- heads
- mlps
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper reverse engineers how three mid-sized language models
  (GPT-J, Pythia-6.9B, and Llama3.1-8B) perform addition by analyzing their internal
  representations. The authors discover that numbers are represented as a "generalized
  helix" - a combination of linear and periodic components - rather than as simple
  linear values.
---

# Language Models Use Trigonometry to Do Addition

## Quick Facts
- arXiv ID: 2502.00873
- Source URL: https://arxiv.org/abs/2502.00873
- Reference count: 40
- LLMs compute addition using helical number representations and trigonometric manipulation

## Executive Summary
This paper reverse engineers how mid-sized language models perform addition by analyzing their internal representations. The authors discover that numbers are encoded as "generalized helices" - combinations of linear and periodic components with specific frequencies (2, 5, 10, 100) - rather than as simple linear values. They identify a three-stage "Clock algorithm" where attention heads move input helices to the output position, MLPs construct the helix for the sum, and subsequent MLPs read the result. This is the first representation-level explanation of LLM mathematical capabilities, showing that trigonometric manipulations rather than simple arithmetic operations underlie addition.

## Method Summary
The authors analyze GPT-J, Pythia-6.9B, and Llama3.1-8B on single-token addition problems (a,b ∈ [0,99]). They extract residual streams, perform Fourier decomposition to identify periodic components, and fit generalized helix models using linear regression on PCA-projected representations. Causal interventions via activation patching verify that fitted helical representations preserve ~80% of the model's addition performance. They identify specific model components (attention heads and MLPs) implementing the algorithm and analyze individual neurons to understand how they read and write to helical representations.

## Key Results
- Numbers are represented as generalized helices combining linear and periodic (trigonometric) features with periods 2, 5, 10, and 100
- Addition is computed via the "Clock algorithm": attention heads move input helices to output position, MLPs construct helix(a+b), then MLPs read the final answer
- Top ~1% of neurons (~4587) implement the core algorithm and achieve 80% accuracy when others are ablated
- The helical representation is causally implicated for addition but underperforms PCA on multiplication, division, and modular arithmetic

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs represent numbers as generalized helices combining periodic (trigonometric) and linear features
- Mechanism: Numbers encoded using basis functions B(a) = [a, cos(2πa/T₁), sin(2πa/T₁), ...] with periods T = [2, 5, 10, 100], projected via learned coefficient matrix C into residual stream
- Core assumption: Helix representation is primary computational substrate for addition
- Evidence anchors:
  - Activation patching with helical fits achieves ~80% of full layer patching effect, outperforming PCA baselines
  - No direct corpus corroboration for helical representations specifically
- Break condition: If ablating helix dimensions doesn't significantly degrade addition performance

### Mechanism 2
- Claim: Addition computed via "Clock algorithm" - manipulating input helices to create answer helix
- Mechanism: Three-stage pipeline: (1) attention heads move a,b helices to last token, (2) MLPs construct helix(a+b) from helix(a,b), (3) MLPs read from helix(a+b) to produce logits
- Core assumption: Trigonometric identities underlie helix manipulation (unverified)
- Evidence anchors:
  - Helix(a+b) fits last token hidden states better than 27D PCA despite using only 9 parameters
  - MLPs 14-18 outputs well-modeled by helix(a+b); MLPs 19-27 have high direct-to-logit effects
- Break condition: If critical MLP outputs cannot be fit by helix(a+b)

### Mechanism 3
- Claim: Individual neurons read/write to helical representations using same periodic structure
- Mechanism: Neuron preactivations modeled as linear combinations of cos(2π(a+b-d)/T) terms with periods matching helix
- Core assumption: Sparse subset of neurons implements core algorithm
- Evidence anchors:
  - Most common periods are T = [2, 5, 10, 100], matching helix parameterization
  - Fitted preactivations achieve ~75% of actual neuron patching performance
- Break condition: If fitted neuron preactivations fail to preserve addition accuracy

## Foundational Learning

- **Fourier decomposition and periodic features**
  - Why needed here: Core to understanding how LLMs encode numbers as trigonometric functions with specific periods (2, 5, 10, 100)
  - Quick check question: Can you explain why cos(2πa/10) would be useful for representing base-10 digit structure?

- **Activation/path patching and causal interventions**
  - Why needed here: Primary methodology for verifying that helical representations are causally implicated, not just correlated
  - Quick check question: What does it mean if patching a fitted representation achieves 80% of patching the actual activations?

- **Transformer residual stream and component decomposition**
  - Why needed here: Understanding how MLP outputs and attention head outputs combine in residual stream, and how helical representations flow through layers
  - Quick check question: How would you determine if an MLP is "building" a representation vs. "reading from" it?

## Architecture Onboarding

- **Component map**: Embed(a), Embed(b) → Layer 0 helix formation → Attention (move to last token) → MLPs 14-18 (build a+b helix) → MLPs 19-27 (read to logits)

- **Critical path**: Embed(a), Embed(b) → Layer 0 helix formation → Attention (move to last token) → MLPs 14-18 (build a+b helix) → MLPs 19-27 (read to logits)

- **Design tradeoffs**:
  - GPT-J uses simple MLPs (easier neuron interpretation); Llama uses gated MLPs (potentially different algorithms)
  - Single-token vs. multi-digit tokenization: Analysis limited to [0,99] for single-token regime
  - Helix vs. linear representation: Helix provides error-correction but requires more complex manipulation

- **Failure signatures**:
  - ±10 errors: 73.6% of numeric errors; caused by periodicity in logit boosting
  - Bias toward smaller answers: Negative slope in logit distributions explains worse performance on larger a,b values
  - Three-digit discontinuity: PC1 shows sharp break at a=100, indicating different representation space

- **First 3 experiments**:
  1. Replicate helix fitting: Extract layer 0 residual streams for a∈[0,99], perform Fourier decomposition to verify dominant frequencies at T=[2,5,10,100], fit helix using Eq. 2
  2. Activation patching validation: Patch fitted helical representations vs. PCA baselines; expect helical fits to achieve >6 logit difference at layer 0
  3. Neuron periodicity analysis: For top 100 neurons by attribution patching score, compute Fourier decomposition of preactivations with respect to a+b; verify dominant periods match helix parameters

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do specific MLP layers (e.g., 14-18 in GPT-J) mechanistically implement trigonometric identities to transform input helices (helix(a), helix(b)) into the sum helix (helix(a+b))?
- Basis in paper: The authors state "we do not know the exact mechanism they use" to create the answer helix, although they hypothesize the use of identities like cos(a+b) = cos(a)cos(b) - sin(a)sin(b)
- Why unresolved: The solution space for implementing these low-level mathematical operations within neural network weights is vast
- What evidence would resolve it: Identifying specific weight matrices or neuron sub-networks that causally isolate and perform the algebraic steps of trigonometric addition formulas

### Open Question 2
- Question: What additional geometric or linear structures are required to represent numbers for tasks like multiplication and integer division, where the helical fit underperforms the PCA baseline?
- Basis in paper: Table 1 and Section 4.5 show that while helix is causally relevant for addition, it underperforms PCA on multiplication, division, and modular arithmetic
- Why unresolved: The paper focuses primarily on addition; specific representations enabling models to handle non-linear or iterative properties of multiplication/division were not fully reverse-engineered
- What evidence would resolve it: Extending analysis to fit and test hybrid representations combining helices with other non-linear manifolds on division and multiplication prompts

### Open Question 3
- Question: How does the "Clock" algorithm adapt or change in models using gated MLPs (like Llama3.1-8B) or digit-based tokenization?
- Basis in paper: The authors note that helix(a+b) is less causally implicated for Llama3.1-8B, leading to hypothesis that it "implements modified algorithms" due to use of gated MLPs
- Why unresolved: While algorithm is well-defined for GPT-J (simple MLPs), "weaker" results for Llama3.1 suggest architectural differences disrupt or alter standard Clock mechanism
- What evidence would resolve it: Comparative circuit analysis between simple-MLP and gated-MLP models to identify divergent algorithmic components

## Limitations

- The analysis is constrained to single-token addition problems (a,b ∈ [0,99]), leaving open questions about multi-digit arithmetic or numbers outside this range
- The exact trigonometric operations underlying the Clock algorithm remain unverified - authors acknowledge this is unknown
- The causal attribution of helix representation is incomplete; 80% performance recovery from helical fits falls short of proving exclusive computational substrate

## Confidence

- **High Confidence**: The existence of generalized helical representations in residual stream (Sections 4.3-4.4). Fourier decomposition and helix fitting methodology is sound, and causal patching results are robust across multiple model architectures.
- **Medium Confidence**: The Clock algorithm as primary addition mechanism (Section 5.3). While MLP analysis strongly supports this interpretation, exact trigonometric operations remain unverified, and alternative computational pathways cannot be ruled out.
- **Medium Confidence**: The sparsity claim for critical neurons (Section 5.4.1). Top-1% attribution scores are compelling, but methodology for defining "critical" neurons and functional redundancy of remaining neurons require further validation.

## Next Checks

1. **Algorithmic Decomposition**: Implement the hypothesized Clock algorithm using explicit trigonometric operations on fitted helix representations. Compare its accuracy and error patterns against the original LLM to verify that the algorithm fully captures the model's addition capability.

2. **Cross-Range Generalization**: Test whether the helix representation and Clock algorithm extend to three-digit addition problems. If the representation breaks down at a=100, identify what alternative mechanisms the model employs.

3. **Neuron Intervention Study**: Perform targeted ablation of neurons with non-matching periodicities (periods outside T=[2,5,10,100]) to test whether the algorithm truly depends on the specific helix structure, or if other periodic representations can substitute.