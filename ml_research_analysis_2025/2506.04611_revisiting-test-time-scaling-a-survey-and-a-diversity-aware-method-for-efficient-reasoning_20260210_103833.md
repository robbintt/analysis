---
ver: rpa2
title: 'Revisiting Test-Time Scaling: A Survey and a Diversity-Aware Method for Efficient
  Reasoning'
arxiv_id: '2506.04611'
source_url: https://arxiv.org/abs/2506.04611
tags:
- reasoning
- arxiv
- preprint
- language
- wang
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper surveys test-time scaling (TTS) methods and finds that
  reasoning-optimized models often produce less diverse outputs, limiting TTS effectiveness.
  To address this, the authors propose ADAPT, a diversity-aware prefix fine-tuning
  method that encourages diverse early-stage reasoning.
---

# Revisiting Test-Time Scaling: A Survey and a Diversity-Aware Method for Efficient Reasoning

## Quick Facts
- arXiv ID: 2506.04611
- Source URL: https://arxiv.org/abs/2506.04611
- Reference count: 24
- Key outcome: ADAPT achieves 80% accuracy with 8× less compute by enhancing generative diversity for test-time scaling

## Executive Summary
This paper identifies a fundamental limitation in test-time scaling (TTS): reasoning-optimized models often produce less diverse outputs, causing accuracy gains to plateau as the number of sampled candidates increases. To address this, the authors propose ADAPT, a lightweight prefix fine-tuning method that enhances diversity by training on a hybrid dataset of diverse base-model outputs and target-model self-generations. Evaluated on mathematical reasoning tasks, ADAPT achieves 80% accuracy using only 32 samples—8× fewer than strong baselines—demonstrating that improving generative diversity is critical for efficient TTS.

## Method Summary
ADAPT fine-tunes only prefix parameters of a distilled reasoning model using a hybrid dataset: 90% diverse outputs from a base model (Qwen2.5-Math) and 10% self-generated outputs from the target model (DeepSeek-R1-Distill-Qwen). The goal is to encourage diverse early-stage reasoning while avoiding catastrophic forgetting. The method is evaluated via Best-of-N sampling with majority voting on MATH-500, measuring accuracy gains across different sample counts (N ∈ {2,4,8,16,32,64,128,256}).

## Key Results
- ADAPT achieves 80% accuracy with only 32 samples—8× fewer than strong baselines
- Distilled models show shallower TTS scaling curves, indicating reduced generative diversity
- ADAPT significantly improves TTS efficiency while maintaining or improving baseline accuracy

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Distillation and reasoning optimization narrow a model's output distribution, reducing the diversity of candidate solutions and limiting the gains from Test-Time Scaling.
- Mechanism: Best-of-N sampling relies on generating diverse candidates to find a correct answer. When a model's output variance is low, increasing N produces redundant candidates, causing accuracy gains to plateau as the search space isn't effectively expanded.
- Core assumption: The relationship between distillation and reduced diversity is causal and is the primary factor limiting TTS, rather than a fundamental ceiling in reasoning capability.
- Evidence anchors:
  - [abstract] "reasoning-optimized models often produce less diverse outputs, which limits TTS effectiveness."
  - [section 5.3] "DeepSeek Qwen-1.5B achieves a higher baseline accuracy... reaching 80.8% at N = 256 (+15.6%). In contrast, Qwen2.5-Math-1.5B starts lower... but exhibits a steeper improvement... (+26.6%)."
- Break condition: This mechanism breaks if the plateau in TTS performance for distilled models is caused by a saturation in fundamental reasoning ability or a failure of the verification process independent of candidate redundancy.

### Mechanism 2
- Claim: Targeted prefix fine-tuning on a hybrid dataset can re-inject diversity into a reasoning model, improving the efficiency of Test-Time Scaling.
- Mechanism: The ADAPT method fine-tunes only prefix parameters on a dataset of 90% diverse outputs (from a base model) and 10% self-generated outputs. This encourages the model to explore a wider range of initial reasoning paths, creating more distinct candidate solutions for the verifier to choose from.
- Core assumption: The fine-tuning procedure successfully promotes diversity, which is the proximate cause of improved TTS performance.
- Evidence anchors:
  - [abstract] "ADAPT, a lightweight prefix fine-tuning approach that enhances diversity... achieves 80% accuracy with only 32 samples—8× fewer than strong baselines."
  - [section 4.1] "The training dataset consists primarily of diverse responses, supplemented with a smaller subset of outputs generated by the target model... to mitigate potential catastrophic forgetting."
- Break condition: This mechanism breaks if the performance gain is not from increased diversity, but from the fine-tuning process correcting a flaw in the distilled model's reasoning or from overfitting to the benchmark.

### Mechanism 3
- Claim: The effectiveness of sampling-based TTS is bottlenecked by the reliability of the verifier, which can degrade as sample count increases.
- Mechanism: Verifiers (e.g., majority voting) select the best candidate from N samples. As N grows, the chance of generating confusing or adversarial outputs increases, which can mislead the verifier, leading to reward hacking and diminishing returns.
- Core assumption: The verifier is imperfect and susceptible to errors that scale with sample count.
- Evidence anchors:
  - [section 3.1] "...accuracy gains plateau as verifiers become unreliable with more samples."
  - [section 3.1] "...Huang et al. (2025a) found that Best-of-N can suffer from reward hacking, especially in low-diversity models where sampled responses are redundant."
- Break condition: This mechanism breaks if a near-perfect verifier is developed, allowing performance to scale monotonically with N.

## Foundational Learning

- Concept: Best-of-N Sampling
  - Why needed here: This is the core evaluation paradigm for TTS. It involves generating multiple candidate answers and using a verifier to select the best one.
  - Quick check question: If a model generates 10 answers and 7 are identical and incorrect, while 3 are distinct and one is correct, will majority voting succeed? Why does this illustrate a diversity problem?

- Concept: Model Distillation
  - Why needed here: The paper's central hypothesis is that distillation, a process for creating efficient models, inadvertently reduces output diversity.
  - Quick check question: What is the primary goal of model distillation, and why might teaching a student model to mimic a teacher's outputs lead to a reduction in the student's generative variance?

- Concept: Prefix Tuning
  - Why needed here: The ADAPT method uses this technique. It's a parameter-efficient fine-tuning method where only a small set of trainable vectors are prepended to the input.
  - Quick check question: Unlike full fine-tuning, prefix tuning only modifies a small number of parameters. What is a key advantage of this when trying to inject a new behavior (diversity) while preserving existing capabilities?

## Architecture Onboarding

- Component map: Generator (reasoning model) -> Verifier (majority voting) -> ADAPT Fine-tuner (prefix tuning) -> TTS Orchestrator (Best-of-N controller)

- Critical path: The performance-governing path is the loop between the Generator and the Verifier. The Generator produces candidates, and the Verifier selects one. The ADAPT component is a one-time process that modifies the Generator to improve the inputs to this loop.

- Design tradeoffs:
  - Efficiency vs. Peak Performance: ADAPT allows for higher accuracy at lower N, shifting the accuracy-compute curve.
  - Diversity vs. Correctness: Over-optimizing for diversity could lead to nonsensical paths. The 10% self-generated data in ADAPT is a trade-off to maintain reasoning correctness.
  - Finetuning Cost vs. Inference Savings: ADAPT requires a small upfront training cost, which is justified by downstream inference savings.

- Failure signatures:
  - Premature Plateauing: Accuracy gains from increasing N flatten out very quickly (e.g., after N=16), indicating low diversity.
  - Reward Hacking: The verifier consistently selects high-scoring but incorrect answers, indicating verifier unreliability.
  - Catastrophic Forgetting: After ADAPT, baseline accuracy (at low N) drops significantly, indicating core reasoning abilities were disrupted.

- First 3 experiments:
  1. Establish Baseline & Confirm Hypothesis: Run Best-of-N evaluation on the distilled model and its pre-trained counterpart across N={2, 4, 8, 16, 32, 64, 128, 256}. Plot accuracy vs. N to confirm the distilled model has a shallower scaling curve, indicating a diversity deficit.
  2. ADAPT Ablation on Data Mix: Train multiple ADAPT variants with different ratios of diverse data vs. self-generated data (e.g., 100/0, 90/10, 50/50). Evaluate their Best-of-N performance to find the optimal balance for maximizing TTS efficiency without causing catastrophic forgetting.
  3. Direct Diversity Measurement: For a held-out problem set, generate N=100 samples for both the baseline distilled model and the best ADAPT model. Calculate and compare diversity metrics like self-BLEU or pairwise edit distance to directly validate the core claim that ADAPT increases generative diversity.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does diversity-aware fine-tuning improve test-time scaling efficiency for non-mathematical reasoning tasks?
- Basis in paper: [explicit] The authors state in the Limitations section that it "remains unclear whether similar diversity-induced gains would generalize to broader tasks such as commonsense or multi-hop QA."
- Why unresolved: The current study evaluates performance solely on mathematical problem solving (MATH-500), which may possess different structural properties than logical or linguistic reasoning domains.
- What evidence would resolve it: Applying ADAPT to diverse benchmarks (e.g., CommonsenseQA or StrategyQA) and comparing the "Min N to hit threshold" metric against mathematical reasoning results.

### Open Question 2
- Question: How does model scale affect the trade-off between reasoning optimization and generative diversity?
- Basis in paper: [explicit] The Limitations section notes that evaluations focused on a small model (1.5B parameters) and "scaling effects and interactions with larger architectures are left for future work."
- Why unresolved: Larger models may inherently possess different diversity characteristics or saturation points, which could alter the efficacy of prefix tuning strategies designed for compact models.
- What evidence would resolve it: Conducting experiments with larger model variants (e.g., 7B or 70B parameters) to measure if the 8× efficiency gain observed in 1.5B models persists.

### Open Question 3
- Question: What is the relationship between RL-based trajectory optimization and hallucination rates in reasoning models?
- Basis in paper: [explicit] The Future Directions section highlights the need to "understand the relationship between RL and SFT methods used to enable LLMs to scale test-time computation, as well as their impact on hallucination."
- Why unresolved: While TTS improves accuracy, prior work indicates models like GPT-o3 still suffer from hallucinations; isolating whether specific training methods exacerbate this is necessary for robust deployment.
- What evidence would resolve it: A comparative study measuring hallucination frequency and factuality scores across models trained with RL scaling versus distillation-based scaling.

## Limitations

- The causal link between distillation and reduced diversity lacks direct empirical measurement; the relationship is inferred from TTS performance patterns rather than diversity metrics.
- The optimal 90/10 training mix for ADAPT appears arbitrary without systematic ablation studies demonstrating its superiority over other ratios.
- The analysis relies on downstream TTS performance as a proxy for diversity without measuring actual output variance through metrics like self-BLEU or pairwise similarity.

## Confidence

- **High Confidence**: ADAPT achieves improved TTS efficiency on mathematical reasoning tasks (measured through acc_maj and compute reduction).
- **Medium Confidence**: Distillation reduces output diversity, which limits TTS effectiveness. This is supported by observed performance patterns but lacks direct diversity measurements.
- **Low Confidence**: The specific 90/10 training mix is optimal for balancing diversity injection and catastrophic forgetting. This ratio appears arbitrary without ablation studies demonstrating its superiority.

## Next Checks

1. **Direct Diversity Measurement**: For a fixed problem set, generate 100 samples each from the baseline distilled model and ADAPT model. Compute self-BLEU scores and pairwise edit distances to quantify actual diversity differences, establishing whether ADAPT increases diversity as hypothesized.

2. **ADAPT Data Mix Ablation**: Train ADAPT variants with diverse data ratios of 100/0, 75/25, 50/50, and 25/75 (diverse/target model outputs). Evaluate Best-of-N performance across N values to identify the optimal balance point and validate whether 90/10 is indeed optimal or merely sufficient.

3. **Verifier Robustness Analysis**: For each model (baseline, ADAPT), measure how accuracy scales with N beyond 256 samples, tracking verifier agreement rates and error types. This would reveal whether ADAPT's gains stem from improved diversity or from mitigating verifier reward hacking in low-diversity regimes.