---
ver: rpa2
title: 'AA-Omniscience: Evaluating Cross-Domain Knowledge Reliability in Large Language
  Models'
arxiv_id: '2511.13029'
source_url: https://arxiv.org/abs/2511.13029
tags:
- answer
- question
- knowledge
- questions
- across
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces AA-Omniscience, a benchmark designed to measure
  both factual recall and knowledge calibration across 6,000 questions from 42 economically
  relevant topics in six domains. Unlike typical evaluations that reward guessing,
  it uses the Omniscience Index, which penalizes incorrect guesses and rewards abstention
  when uncertain, providing a more realistic measure of model reliability.
---

# AA-Omniscience: Evaluating Cross-Domain Knowledge Reliability in Large Language Models

## Quick Facts
- arXiv ID: 2511.13029
- Source URL: https://arxiv.org/abs/2511.13029
- Reference count: 10
- Primary result: Introduces AA-Omniscience benchmark measuring factual recall and knowledge calibration across 6,000 questions in 42 topics across 6 domains

## Executive Summary
This paper introduces AA-Omniscience, a benchmark designed to measure both factual recall and knowledge calibration across 6,000 questions from 42 economically relevant topics in six domains. Unlike typical evaluations that reward guessing, it uses the Omniscience Index, which penalizes incorrect guesses and rewards abstention when uncertain, providing a more realistic measure of model reliability. Claude 4.1 Opus achieved the highest score (4.8), while only three models scored above zero, highlighting persistent factuality and hallucination issues. Results show that model performance varies significantly by domain, and overall capability does not predict knowledge reliability, underscoring the need for domain-specific model selection in tasks requiring factual accuracy.

## Method Summary
AA-Omniscience evaluates LLM factual knowledge using 6,000 questions across six domains (Business, Humanities & Social Sciences, Health, Law, Software Engineering, Science/Engineering/Mathematics) and 42 topics. The benchmark uses the Omniscience Index (OI = 100·(c−i)/(c+p+i+a)), which penalizes incorrect answers and rewards abstention. Models are prompted to answer only if confident, responses are graded by Gemini 2.5 Flash, and results are computed per domain and overall. The evaluation uses authoritative English sources and focuses on short, exact answers requiring precise factual knowledge.

## Key Results
- Claude 4.1 Opus achieved the highest score (4.8), but only three models scored above zero
- Performance varies significantly by domain, with different labs' models leading different domains
- Overall capability (Artificial Analysis Intelligence Index) does not predict knowledge reliability
- Models exhibit high hallucination rates even with high accuracy scores

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Penalizing incorrect guesses more heavily than rewarding correct answers shifts model behavior toward calibration rather than confident guessing.
- Mechanism: The Omniscience Index (OI = 100·(c−i)/(c+p+i+a)) assigns symmetric penalty (-1) for incorrect answers as reward (+1) for correct ones, meaning a model must answer correctly more often than incorrectly to achieve positive scores.
- Core assumption: Models (or their developers) optimize against benchmark metrics; if a metric penalizes hallucination, training and selection will shift accordingly.
- Evidence anchors:
  - [abstract] "...penalizes incorrect guesses and rewards abstention when uncertain, providing a more realistic measure of model reliability"
  - [section 2.4.1] "As a result, OI is positive only if the model answers correctly more than it answers incorrectly, strongly penalizing hallucination"
  - [corpus] Related work (Kalai et al., 2025) attributes hallucination behavior to misalignment of current evaluations that reward guessing.
- Break condition: If models are not optimized against this metric (e.g., deployed without awareness of OI), behavioral change may not occur.

### Mechanism 2
- Claim: Domain-specific evaluation reveals performance patterns masked by aggregate accuracy scores.
- Mechanism: By testing 6,000 questions across 42 topics in 6 domains, the benchmark surface per-domain leader differences (e.g., Claude 4.1 Opus leads Law/Software Engineering/Humanities; Grok 4 leads Health/Science).
- Core assumption: Domain expertise is not uniformly distributed across models; training data composition or architecture choices create domain-specific strengths.
- Evidence anchors:
  - [abstract] "Performance also varies by domain, with the models from three different research labs leading across the six domains"
  - [section 3.3] "No single model consistently dominates knowledge reliability across the six domains assessed by AA-Omniscience"
  - [corpus] FIBER (2512.11110) similarly finds domain-specific factual biases in multilingual evaluation.
- Break condition: If domain definitions are too broad or questions insufficiently representative, per-domain scores may not generalize.

### Mechanism 3
- Claim: General capability metrics do not predict factual reliability, requiring dedicated knowledge calibration benchmarks.
- Mechanism: The paper finds no reliable correlation between Artificial Analysis Intelligence Index (general capability) and Omniscience Index; models like Llama 3.1 405B score well on OI despite lower general rankings.
- Core assumption: Factual knowledge and reasoning capability are partially orthogonal dimensions of model quality.
- Evidence anchors:
  - [abstract] "...overall capability does not predict knowledge reliability"
  - [section 3.2] "Overall intelligence does not reliably predict strong embedded knowledge or low hallucination rates"
  - [corpus] Weak or missing direct corpus evidence on general-capability-vs-factuality correlation; this paper contributes new evidence.
- Break condition: If future models are specifically trained to optimize both dimensions jointly, the orthogonality may diminish.

## Foundational Learning

- Concept: **Knowledge calibration** (knowing when you don't know)
  - Why needed here: The benchmark's core innovation is measuring not just whether models know facts, but whether they correctly identify their own knowledge boundaries.
  - Quick check question: If a model answers 50% of questions correctly but attempts all questions, is it well-calibrated?

- Concept: **Hallucination rate vs. accuracy**
  - Why needed here: High accuracy can coexist with high hallucination (e.g., GPT-5 (high): 39% accuracy, 81% hallucination rate); understanding both metrics is essential for interpretation.
  - Quick check question: Why might a model with 40% accuracy be more reliable than one with 45% accuracy?

- Concept: **Domain specificity in model selection**
  - Why needed here: Results show different labs' models lead different domains; practitioners should select models based on task domain rather than aggregate rankings.
  - Quick check question: A model ranks 16th overall but 7th in Science—when should you use it?

## Architecture Onboarding

- Component map:
  - Question generation agent -> Question set (6,000 questions) -> Evaluation harness (prompts models) -> Grading model (Gemini 2.5 Flash) -> Scoring computation

- Critical path:
  1. Select authoritative domain sources → 2. Generate candidate questions → 3. Filter (difficulty, ambiguity, duplicates) → 4. Model evaluation (no tools/context) → 5. Grade responses → 6. Compute metrics

- Design tradeoffs:
  - Symmetric penalty (p=1) means abstention scores 0; a refuse-all model would rank 4th of 36—harsh but discourages guessing
  - English-only sources limit geographical breadth; future work could add multilingual coverage
  - Single grading model (Gemini 2.5 Flash) introduces potential bias; human grading alignment was tested but not eliminated

- Failure signatures:
  - High accuracy + high hallucination = overconfident model (e.g., GPT-5 (high))
  - Low attempt rate + negative OI = model that guesses poorly when it does answer
  - Domain scores inconsistent with aggregate = domain-specific gaps

- First 3 experiments:
  1. Replicate on public question set (10%) to validate your evaluation pipeline against reported scores (Figure 12 shows alignment)
  2. Test your target model across all 6 domains to identify domain-specific strengths/weaknesses before deployment
  3. Ablate the "answer only if confident" prompt instruction to measure calibration vs. accuracy tradeoff

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal penalty weighting in the Omniscience Index to appropriately balance discouraging hallucinations against incentivizing reasonable risk-taking?
- Basis in paper: [explicit] The authors note that their penalty of p=1 for incorrect answers means "a model that abstains from every question would be given a score of 0, which would place it 4th out of the 36 models," and suggest "a future iteration of this metric could reduce the penalty for wrong answers" (e.g., -0.5 instead of -1).
- Why unresolved: The current penalty structure may over-incentivize abstention, but the optimal calibration remains undefined.
- What evidence would resolve it: Comparative evaluation of models using varied penalty weights, correlated with real-world task performance and user preference studies.

### Open Question 2
- Question: Does using a single model (GPT-5) for question generation introduce systematic bias favoring models from the same family in evaluation?
- Basis in paper: [explicit] "Question generation, filtering and revision for AA-Omniscience all currently use OpenAI's GPT-5... it also has potential to create bias if question structure and wording aligns more closely with GPT-5 family models when evaluated."
- Why unresolved: No ablation study compares single-model vs. multi-model question generation pipelines.
- What evidence would resolve it: Generating parallel question sets using multiple frontier models and comparing evaluation outcomes across model families.

### Open Question 3
- Question: What training or architectural factors enable smaller models (e.g., Nemotron Nano 9B V2) to achieve disproportionately strong Omniscience Index scores relative to parameter count?
- Basis in paper: [inferred] The paper observes that "larger models do not consistently exhibit lower hallucination rates, and parameter count alone is insufficient to predict performance," with smaller models like Nemotron achieving strong results, but does not investigate why.
- Why unresolved: The analysis is correlational; no causal mechanism is proposed or tested.
- What evidence would resolve it: Controlled comparisons of models with matched architectures but varied training data, calibration techniques, or fine-tuning procedures.

### Open Question 4
- Question: To what extent do AA-Omniscience results generalize to non-English languages and non-Western cultural/legal contexts?
- Basis in paper: [explicit] "Currently, all questions are in English, and many sources are from the US, UK and other English speaking countries. This could limit the ability to extrapolate from English-speaking examples into tasks and domains relying on other languages or cultural contexts."
- Why unresolved: No multilingual or geographically diverse evaluation has been conducted.
- What evidence would resolve it: Extending the benchmark to additional languages and non-Anglophone authoritative sources, then comparing model rankings.

## Limitations
- The benchmark relies on English-only authoritative sources, introducing geographical and linguistic bias
- A single grading model (Gemini 2.5 Flash) without extensive human validation may introduce systematic grading bias
- The private question set (90%) cannot be independently verified, making complete reproducibility challenging

## Confidence
**High Confidence:** The mechanism of penalizing incorrect guesses more heavily than rewarding correct answers is well-founded and mathematically sound. The finding that general capability metrics do not predict factual reliability is robust and supported by empirical results.

**Medium Confidence:** Domain-specific performance patterns are observed but may be influenced by the particular question selection and grading criteria. The claim that no single model consistently dominates across all domains is supported but could vary with different question sets.

**Low Confidence:** The exact threshold for "confident-only" answering and its impact on model behavior is not fully characterized. The real-world applicability of abstention-based scoring in production systems remains uncertain.

## Next Checks
1. **Grading Alignment Validation:** Manually grade 100 randomly selected responses across multiple domains to quantify grader bias and compare against reported results.

2. **Cross-Lingual Extension:** Replicate the evaluation pipeline with non-English authoritative sources to assess geographical bias and validate if findings generalize across languages.

3. **Production Simulation:** Deploy top-performing models on real-world factual QA tasks with abstention options enabled to measure actual reliability improvements versus benchmark scores.