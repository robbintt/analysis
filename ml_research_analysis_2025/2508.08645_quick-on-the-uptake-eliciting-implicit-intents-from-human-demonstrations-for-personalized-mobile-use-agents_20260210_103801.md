---
ver: rpa2
title: 'Quick on the Uptake: Eliciting Implicit Intents from Human Demonstrations
  for Personalized Mobile-Use Agents'
arxiv_id: '2508.08645'
source_url: https://arxiv.org/abs/2508.08645
tags:
- agents
- mobile-use
- human
- ifragent
- intention
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the challenge of creating personalized mobile-use
  agents that can understand both explicit task instructions and implicit user preferences
  from human demonstrations. The authors propose IFRAgent, a framework that analyzes
  both explicit intention flows (operational steps) and implicit intention flows (personal
  habits) from user demonstrations.
---

# Quick on the Uptake: Eliciting Implicit Intents from Human Demonstrations for Personalized Mobile-Use Agents

## Quick Facts
- arXiv ID: 2508.08645
- Source URL: https://arxiv.org/abs/2508.08645
- Reference count: 36
- Personalizing mobile-use agents using explicit task procedures and implicit user habits from demonstrations

## Executive Summary
This paper addresses the challenge of creating personalized mobile-use agents that can understand both explicit task instructions and implicit user preferences from human demonstrations. The authors propose IFRAgent, a framework that analyzes both explicit intention flows (operational steps) and implicit intention flows (personal habits) from user demonstrations. The method extracts user-specific habits and SOPs during an intention flow extraction phase, then uses retrieval-augmented generation and query rewriting during deployment to generate personalized queries and SOPs. Experiments on the MobileIAR dataset show IFRAgent improves human intention alignment rate by 6.79% (32.06% relative improvement) and step completion rates by 5.30% (26.34% relative improvement) across diverse mobile-use agents.

## Method Summary
The IFRAgent framework operates in two phases: offline extraction and online deployment. During extraction, GPT-4o processes user demonstrations to create a vector library of Standard Operating Procedures (SOPs) and a text-based habit repository. For deployment, the system uses a SOP Extractor (Qwen3-4B) with 1-shot retrieval to find relevant demonstrations, a Query Rewriter (Qwen3-4B) fine-tuned to inject user habits into instructions, and the target mobile-use agent to execute actions. The approach separates explicit task logic from implicit user preferences, allowing lightweight models to execute personalized tasks through careful query rewriting.

## Key Results
- IFRAgent improves Intention Alignment Rate by 6.79% absolute (32.06% relative) over baselines
- Step Completion Rate increases by 5.30% absolute (26.34% relative)
- General-domain models like Qwen2.5-VL and GPT-4o show larger improvements than specialized mobile models
- 1-shot retrieval provides significant gains, while 2-3 shots degrade performance

## Why This Works (Mechanism)

### Mechanism 1: Decoupling of Explicit Procedures and Implicit Habits
Separating user demonstrations into explicit Standard Operating Procedures (SOPs) and implicit habit repositories allows a mobile agent to learn task logic independent of user preferences, which can then be recombined during execution. The system uses two distinct agents during extraction: an Explicit Intention Flow Agent ($A_e$) generates step-by-step SOPs to populate a vector library, while an Implicit Intention Flow Agent ($A_i$) incrementally updates a text-based habit repository with preferences (e.g., "prefers spicy food"). Core assumption: User preferences (implicit flows) are consistent enough across different tasks to be stored as a reusable repository, while task steps (explicit flows) vary by query.

### Mechanism 2: Ambiguity Resolution via Query Rewriting
Lightweight models can execute personalized tasks if ambiguous raw queries are first rewritten into detailed, user-specific instructions using the stored habit repository. A Query Rewriter ($W$) takes the raw query and the retrieved SOP, cross-references them with the user's habit repository, and generates a "personalized query" and "personalized SOP." This shifts the cognitive load of personalization from the execution agent to the rewriting module. Core assumption: The base mobile-use agent follows specific, detailed instructions better than ambiguous ones, and the rewriter model has sufficient capacity to inject context without hallucinating constraints.

### Mechanism 3: Retrieval-Augmented SOP Generation
Effective task planning for new queries relies on retrieving similar past demonstrations rather than generating plans from scratch, acting as a semantic memory for the agent. The SOP Extractor ($E$) encodes the new query, searches for the most similar query in the vector library (if similarity > $\tau$), and uses the retrieved pair as a few-shot example to generate the new SOP. Core assumption: The vector embedding of a short mobile query is sufficient to locate a functionally similar past demonstration (semantic similarity ≈ procedural similarity).

## Foundational Learning

- **Concept: Retrieval-Augmented Generation (RAG)**
  - Why needed here: Essential for understanding how IFRAgent manages long-term memory. Instead of fine-tuning the agent on all user history, it retrieves specific relevant demonstrations (SOPs) at inference time.
  - Quick check question: How does the system handle a query that has no close match in the vector library? (Answer: It falls back to a zero-shot generation if similarity is below threshold $\tau$).

- **Concept: Supervised Fine-Tuning (SFT) for Style Transfer**
  - Why needed here: The Query Rewriter is not used "off-the-shelf"; it is fine-tuned to map generic instructions + habits → personalized instructions. Understanding this helps in diagnosing why a rewriter might fail to capture specific user tones.
  - Quick check question: Why is the Query Rewriter ($W$) trained separately rather than using a general LLM? (Answer: To teach the specific format of injecting "habit repository" data into instructions).

- **Concept: Multimodal Action Spaces**
  - Why needed here: The framework interacts with mobile GUIs via actions like CLICK, TYPE, SCROLL. Knowing the action space (defined in MobileIAR) is required to interpret the "Step Completion Rate" and "Intention Alignment Rate."
  - Quick check question: Why does the paper distinguish between "Ground-truth action lists" and "Human-intent-aligned actions"? (Answer: To differentiate between a task being *completable* vs. being completed *exactly as the user prefers*).

## Architecture Onboarding

- **Component map:**
  Offline Phase: GPT-4o ($A_e$, $A_i$) → Vector Library (SOPs) + Habit Repository (Text)
  Online Phase: SOP Extractor (Qwen3-4B) → Query Rewriter (Qwen3-4B) → Mobile Agent (Target Model)

- **Critical path:**
  1. Data Collection: Must collect (Query, Screenshot) pairs per user
  2. Habit Extraction: Must run $A_i$ to build the text-based habit repository (crucial for personalization)
  3. Warm-up Training: The Query Rewriter *must* undergo the SFT warm-up described in the "Trainable Scheme" before deployment; otherwise, it cannot effectively utilize the habit repository

- **Design tradeoffs:**
  - General vs. Specialized Agents: General-domain models (Qwen2.5-VL, GPT-4o) gain *more* from IFRAgent than specialized mobile models (UI-TARS) because specialized models may have "forgotten" general reasoning required to interpret complex rewritten instructions
  - RAG vs. Fine-Tuning: RAG for SOPs (explicit flow) to handle varying queries, SFT for the Rewriter (implicit flow) to hardcode the logic of personalization

- **Failure signatures:**
  - High SR, Low IAR: Agent completes task but ignores user preferences. Diagnosis: Habit Repository extraction failed or Rewriter is not injecting habits
  - Low Type Accuracy: Agent fails to perform correct action type. Diagnosis: Base model is too weak; IFRAgent cannot fix fundamental instruction-following deficits
  - Performance Drop with More Demonstrations: Adding more examples hurts performance. Diagnosis: Retriever is fetching irrelevant context (confirms Figure 5 findings); keep retrieval to 1-shot

- **First 3 experiments:**
  1. Verify Rewriter Warm-up: Train the Query Rewriter on the crowdsourced dataset and validate that it can successfully rewrite a generic query (e.g., "Order coffee") into a specific one (e.g., "Order a large latte with oat milk") given a habit list
  2. Threshold Sensitivity Test: Run the SOP Extractor on a validation set while varying the similarity threshold τ. Determine if the default threshold effectively filters out irrelevant demos
  3. Ablation on Agent Type: Run IFRAgent on a general model (e.g., Qwen-VL) vs. a specialized model (e.g., UI-TARS) on the same task to verify if the "generalization penalty" observed in the paper holds for your specific use case

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can specialized mobile-use models be modified to retain the general-world knowledge necessary for recognizing implicit user intentions?
- **Basis in paper:** The authors note that general-domain models like GPT-4o show greater improvements than specialized models like UI-TARS because the latter "tend to forget some general world knowledge that is helpful for human intention recognition."
- **Why unresolved:** The paper identifies this "catastrophic forgetting" as a bottleneck for personalization but proposes no architectural or training modifications to specialized models to remedy it.
- **What evidence would resolve it:** A specialized agent trained with a regularization loss to preserve general knowledge, demonstrating Intention Alignment Rate (IAR) improvements comparable to general-domain models.

### Open Question 2
- **Question:** Why does increasing the number of demonstration shots introduce "irrelevant information" that degrades the SOP Extractor's performance?
- **Basis in paper:** The analysis of demonstration counts states that while 1-shot improves performance, increasing to 2-shot or 3-shot "may even introduce irrelevant information that interferes with [the SOP Extractor], causing performance degradation."
- **Why unresolved:** This suggests a failure in the retrieval or attention mechanism to handle multiple diverse examples effectively, a phenomenon the study flags but does not solve.
- **What evidence would resolve it:** An ablation study testing different retrieval strategies (e.g., semantic ranking) or attention mechanisms on the SOP Extractor that successfully maintains performance gains as the number of shots increases.

### Open Question 3
- **Question:** How can the IFRAgent framework be extended to update the user habit repository dynamically during the deployment phase?
- **Basis in paper:** The framework strictly separates the "intention flow extraction phase" from the "deployment phase," resulting in a static habit repository that cannot adapt to evolving user preferences in real-time.
- **Why unresolved:** True personalization requires continuous learning; the current architecture assumes user habits are fixed after the initial extraction phase.
- **What evidence would resolve it:** A framework extension where the implicit intention flow agent is triggered by correction signals during deployment to update the repository online, showing improved alignment over time.

## Limitations
- Generalization uncertainty across diverse user populations due to limited demonstration variability (5 trajectories per user)
- Semantic similarity-based retrieval fragility when queries are semantically distant from demonstrations
- Evaluation restricted to Android GUI agents, leaving questions about web-based interface applicability
- Static habit repository that cannot adapt to evolving user preferences during deployment

## Confidence
- **High Confidence** in the decoupling mechanism (Mechanism 1) - ablation study clearly shows both SOP extraction and query rewriting are necessary
- **Medium Confidence** in the query rewriting mechanism (Mechanism 2) - strong results for 7B models, but more modest gains for larger models like GPT-4o
- **Medium Confidence** in the retrieval-augmented SOP generation (Mechanism 3) - 1-shot retrieval benefit is clear, but degradation with 2-3 shots suggests brittleness

## Next Checks
1. **Habit Repository Consistency Test**: Measure IAR degradation when habit repositories are constructed from only 1-3 demonstrations versus 5, establishing the minimum demonstration threshold for reliable personalization
2. **Cross-Domain Transfer**: Evaluate IFRAgent performance when deployed on a new mobile domain (e.g., food delivery apps) without fine-tuning, measuring how well implicit flows transfer across different application contexts
3. **Threshold Sensitivity Analysis**: Systematically vary the similarity threshold τ from 0.5 to 0.95 and measure the tradeoff between retrieval accuracy and fallback frequency, identifying the optimal threshold for different user query distributions