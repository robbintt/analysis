---
ver: rpa2
title: 'GDGB: A Benchmark for Generative Dynamic Text-Attributed Graph Learning'
arxiv_id: '2507.03267'
source_url: https://arxiv.org/abs/2507.03267
tags:
- node
- generation
- graph
- dytag
- textual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces GDGB, the first benchmark for generative
  dynamic text-attributed graph (DyTAG) learning. Existing datasets lack high-quality
  textual attributes, limiting progress in generative tasks.
---

# GDGB: A Benchmark for Generative Dynamic Text-Attributed Graph Learning

## Quick Facts
- **arXiv ID:** 2507.03267
- **Source URL:** https://arxiv.org/abs/2507.03267
- **Reference count:** 40
- **Primary result:** Introduces GDGB, the first benchmark for generative dynamic text-attributed graph learning, with eight curated datasets and novel tasks (TDGG, IDGG).

## Executive Summary
This paper introduces GDGB, the first benchmark for generative dynamic text-attributed graph (DyTAG) learning. Existing datasets lack high-quality textual attributes, limiting progress in generative tasks. GDGB addresses this with eight curated DyTAG datasets featuring rich semantic node and edge texts across domains like e-commerce and social networks. Two novel tasks are defined: Transductive Dynamic Graph Generation (TDGG), generating graphs from known node sets, and Inductive Dynamic Graph Generation (IDGG), which includes new node generation to model real-world graph expansion. Multifaceted evaluation metrics assess structural, temporal, and textual quality, while GAG-General, an LLM-based multi-agent framework, enables reproducible benchmarking.

## Method Summary
GDGB provides eight curated DyTAG datasets with rich node and edge textual attributes. The benchmark defines two novel tasks: TDGG (generating graphs from known nodes) and IDGG (including new node generation). GAG-General, an LLM-based multi-agent framework, uses random walks for memory, LLM inference with temperature 0.8 and top-p 0.9, and evaluates generated graphs using MMD, power-law metrics, and LLM-as-Evaluator. The framework is compared against discriminative baselines (JODIE, TGN, CAWN, GraphMixer, DyGFormer) and generative baselines (DG-Gen, VRDAG).

## Key Results
- GDGB enables rigorous evaluation of TDGG and IDGG tasks across eight curated datasets.
- GAG-General demonstrates superior performance in capturing complex dependencies compared to traditional dynamic graph neural networks.
- Critical interplay between structural and textual features in DyTAG generation is revealed through multifaceted evaluation metrics.

## Why This Works (Mechanism)
The benchmark addresses the fundamental limitation of existing datasets lacking high-quality textual attributes for generative DyTAG tasks. By curating eight diverse datasets with rich semantic node and edge texts, GDGB provides the foundation for meaningful evaluation. The two-task formulation (TDGG and IDGG) captures both transductive and inductive graph generation scenarios. The LLM-based multi-agent framework (GAG-General) leverages semantic understanding from LLMs to generate coherent structural and textual features simultaneously, overcoming limitations of traditional GNNs that struggle with the generative aspect of DyTAG learning.

## Foundational Learning

**Dynamic Graph Neural Networks**
- *Why needed:* Understanding temporal dependencies in evolving graph structures
- *Quick check:* Verify that temporal aggregation mechanisms (e.g., JODIE's embedding evolution) are implemented correctly

**LLM-as-Evaluator**
- *Why needed:* Automated assessment of textual quality beyond simple metrics
- *Quick check:* Confirm that the 5 evaluation criteria (Contextual Fidelity, etc.) are consistently applied across all datasets

**Power-law Distribution Analysis**
- *Why needed:* Real-world graphs follow power-law degree distributions
- *Quick check:* Validate that generated graphs meet the specified thresholds (D_k < 0.15, α ∈ [2,3])

## Architecture Onboarding

**Component Map:** Datasets -> GAG-General Framework -> Evaluation Metrics -> Results
- *Datasets:* 8 curated DyTAG datasets with node/edge texts
- *Framework:* LLM-based multi-agent with memory, generation, and reflection components
- *Evaluation:* Structural (MMD, power-law), textual (LLM-as-Evaluator), embedding fidelity

**Critical Path:** Memory Collection (random walks) -> Agent Generation (LLM inference) -> Edge Simulation (50 edges/round) -> Evaluation (MMD + LLM scores)

**Design Tradeoffs:** 
- LLM-based generation provides semantic coherence but requires careful prompt engineering for non-English datasets
- Random walk memory balances context window constraints with sufficient graph information
- Multi-agent approach enables concurrent execution but introduces complexity in coordination

**Failure Signatures:**
- JSON parsing errors from malformed LLM outputs
- Context window overflow during memory-intensive reflection phases
- Power-law violations indicating structural artifacts

**Three First Experiments:**
1. Run TDGG on Cora dataset with default GAG-General parameters to verify basic functionality
2. Evaluate generated graphs using MMD and power-law metrics to check structural fidelity
3. Test LLM-as-Evaluator on a small sample to validate textual quality assessment

## Open Questions the Paper Calls Out
None

## Limitations
- LLM-as-Evaluator prompts are in English but must evaluate non-English datasets (Dianping, WeiboTech), creating ambiguity in evaluation consistency
- VRAM optimization techniques for 70B-parameter models on A800 GPUs are not specified, making replication challenging
- Power-law validity threshold may be overly permissive for certain domains, potentially masking structural artifacts
- Benchmark focuses on static textual attributes per node, not capturing real-world scenarios where attributes evolve over time

## Confidence

**High Confidence:**
- Introduction of GDGB as first comprehensive benchmark for generative DyTAG learning
- Multifaceted evaluation metrics (MMD, power-law, LLM-as-Evaluator) are clearly specified and reproducible

**Medium Confidence:**
- Superiority of GAG-General over traditional dynamic GNNs demonstrated
- Comparison limited to discriminative baselines for structural tasks

**Low Confidence:**
- Claim that GAG-General "demonstrates superior performance in capturing complex dependencies" weakly supported
- Results primarily compare against non-generative baselines

## Next Checks

1. **Prompt Translation Verification:** Test LLM-as-Evaluator prompts on non-English datasets by translating them into Chinese and validating consistency with English-centric models
2. **VRAM Optimization Replication:** Reproduce 70B-parameter model execution on A800 GPUs using vLLM or quantization, documenting token throughput and memory usage
3. **Power-Law Threshold Sensitivity:** Generate graphs with varying power-law thresholds (D_k < 0.1, D_k < 0.2) to assess impact on structural fidelity across datasets