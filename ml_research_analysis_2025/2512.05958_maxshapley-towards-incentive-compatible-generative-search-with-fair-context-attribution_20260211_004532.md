---
ver: rpa2
title: 'MaxShapley: Towards Incentive-compatible Generative Search with Fair Context
  Attribution'
arxiv_id: '2512.05958'
source_url: https://arxiv.org/abs/2512.05958
tags:
- information
- attribution
- shapley
- search
- sources
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MAXSHAPLEY, an efficient algorithm for fair
  attribution in generative search systems using retrieval-augmented generation. The
  method computes Shapley values for information sources by decomposing the utility
  function into a weighted sum-max structure, enabling exact computation with linear
  complexity rather than exponential.
---

# MaxShapley: Towards Incentive-compatible Generative Search with Fair Context Attribution

## Quick Facts
- **arXiv ID:** 2512.05958
- **Source URL:** https://arxiv.org/abs/2512.05958
- **Reference count:** 40
- **Key outcome:** MAXSHAPLEY achieves strong attribution accuracy (Jaccard >0.9, Kendall's τ >0.79) while using only 7% of the computational cost of exhaustive Shapley computation for generative search systems.

## Executive Summary
This paper introduces MAXSHAPLEY, an efficient algorithm for fair attribution in generative search systems using retrieval-augmented generation. The method computes Shapley values for information sources by decomposing the utility function into a weighted sum-max structure, enabling exact computation with linear complexity rather than exponential. Experiments on three multi-hop QA datasets show MAXSHAPLEY achieves strong attribution accuracy while using only 7% of the computational cost of exhaustive Shapley computation. The approach provides a practical solution for fairly compensating content providers in generative search ecosystems.

## Method Summary
MAXSHAPLEY computes fair attributions by leveraging a decomposable max-sum utility function that enables exact Shapley computation in linear time. The method uses an LLM-as-a-judge to evaluate answer quality and determine source contributions at the key point level. For each key point, sources are scored for relevance, and a closed-form Shapley value is computed based on their ranked contributions. The global attribution is the weighted sum of per-keypoint attributions, achieving both computational efficiency and theoretical fairness properties.

## Key Results
- MAXSHAPLEY achieves Jaccard index >0.9 with ground truth on multi-hop QA datasets
- Attribution quality (Kendall's τ) >0.79 compared to FullShapley exhaustive computation
- Computational efficiency: uses only 7% of the tokens required by FullShapley
- Outperforms TokenShapley and FW-Shapley baselines in both accuracy and efficiency

## Why This Works (Mechanism)

### Mechanism 1: Decomposable Max-Sum Utility Function
- Claim: The max-sum utility structure enables exact Shapley computation in O(mn) rather than O(m·2^m).
- Mechanism: The utility function U(S') = Σⱼ wⱼ·max_{sᵢ∈S'} vᵢⱼ decomposes into n independent "maximization games" per key point. By Shapley's additivity property, the global Shapley value equals the weighted sum of per-keypoint Shapley values, each computable via closed-form formulas over ranked values.
- Core assumption: Sources contribute independently across key points; contribution to one point does not affect contribution weights to others.
- Evidence anchors:
  - [abstract] "leverages a decomposable max-sum utility function to compute attributions with linear computation in the number of documents, as opposed to the exponential cost"
  - [section 3.1] Equation 3 defines the weighted sum-max structure explicitly.
  - [corpus] Related work (TokenShapley, FW-Shapley) confirms Shapley-based attribution is active but does not provide the max-sum decomposition insight.

### Mechanism 2: LLM-as-a-Judge for Source-to-Keypoint Relevance Scoring
- Claim: A black-box LLM can approximate source relevance to atomic key points with sufficient fidelity for attribution ranking.
- Mechanism: The attribution LLM Ψₐ first decomposes the answer a into key points P, then scores each source-keypoint pair vᵢⱼ ∈ [0,1]. These scores serve as the value function for the maximization game per key point.
- Core assumption: The LLM's relevance scoring is consistent enough (at temperature 0) to produce stable ordinal rankings, even if absolute scores vary.
- Evidence anchors:
  - [section 3.1] "we use the Judge to compute a relevance-quality-based score of each information source sᵢ to pⱼ"
  - [section 4.2] Table 1 shows LLM-as-judge scores vary across semantically equivalent but token-different responses (0.3 vs 1.0), but remain consistent within identical inputs.

### Mechanism 3: Maximization Game Yields Closed-Form Shapley Values
- Claim: For a max-game where utility = max{vᵢ} over coalition members, exact Shapley values can be computed from ranked values alone.
- Mechanism: Algorithm 1 sorts values v₁ ≤ v₂ ≤ ... ≤ vₘ and computes each player's expected marginal contribution using precomputed probabilities over permutation positions. The marginal contribution vᵢ - vⱼ occurs only when vⱼ is the max among predecessors.
- Core assumption: Values vᵢⱼ are non-negative and correctly ordered; ranking errors propagate to Shapley values.
- Evidence anchors:
  - [section 3.2] Algorithm 1 provides the O(m³) exact computation procedure.
  - [section 5] "the probability of vᵢ - vⱼ being the marginal contribution of vᵢ is independent of the actual values... depends only on the relative ranking"

## Foundational Learning

- **Shapley Value (Cooperative Game Theory)**
  - Why needed here: The paper defines "fairness" axiomatically via Shapley's properties (efficiency, symmetry, null player, additivity). Understanding these properties is prerequisite to evaluating whether MAXSHAPLEY truly preserves fairness.
  - Quick check question: If source A alone yields utility 0.5, sources A+B together yield 0.8, and B alone yields 0.2, what is B's marginal contribution to the coalition {A,B}?

- **Retrieval-Augmented Generation (RAG)**
  - Why needed here: MAXSHAPLEY operates within RAG pipelines where retrieved documents S feed a search LLM Ψ to generate answer a. The utility function U(S') must reflect real RAG behavior.
  - Quick check question: In a RAG system, if you provide only documents {s₂, s₄} instead of all {s₁, s₂, s₃, s₄, s₅, s₆}, what happens to answer quality? (Assumption: it degrades, which is what U(S') measures.)

- **LLM Positional Bias**
  - Why needed here: The paper explicitly mitigates positional bias by shuffling sources before each LLM call (Section 4.3). Without this, sources at context start/end would receive inflated attribution.
  - Quick check question: If you present sources in order [relevant, irrelevant, irrelevant, irrelevant, irrelevant, irrelevant] vs. shuffled, which ordering would a position-biased LLM likely score higher?

## Architecture Onboarding

- **Component map:** Search LLM (Ψ) -> Attribution LLM (Ψₐ) -> Shapley Computer -> Attribution vector φ
- **Critical path:** Answer generation → Keypoint decomposition → Per-keypoint relevance scoring → Maximization-game Shapley computation → Aggregation. The latency bottleneck is the O(n·m) LLM calls for relevance scoring; Shapley computation itself is negligible.
- **Design tradeoffs:**
  1. **One-pass vs. multi-pass LLM calls:** Multi-pass (separate calls for keypoint generation, distillation, scoring) reduces hallucination but increases latency and cost (Section 3.2).
  2. **Keypoint distillation:** Filtering redundant key points slightly improves performance on "messy" real-world queries (MS-MARCO, HotPotQA) but hurts on cleaner multi-hop datasets (MuSiQUE) (Section 4.3, Figure 11).
  3. **Model selection:** Haiku 3.5 achieves higher attribution quality than GPT-4.1o but with higher latency; Sonnet 4 proved incompatible with prompts designed for other models (Section 4.3).
- **Failure signatures:**
  1. **LLM-as-judge inconsistency:** Semantically equivalent responses receiving different scores (Table 1: 0.3 vs 1.0) indicates token-sensitivity; monitor for score variance across runs.
  2. **Positional bias:** If relevant sources cluster at context boundaries without shuffling, Jaccard@K may be inflated by 0.10-0.15 (Section 4.3).
  3. **Multi-hop degradation:** On 3+ hop questions in MuSiQUE, average Jaccard drops from ~0.76 to ~0.70, suggesting decomposition struggles with complex reasoning chains (Section 4.3).
- **First 3 experiments:**
  1. **Reproduce Figure 3 (Jaccard vs. tokens) on a held-out subset:** Verify MAXSHAPLEY achieves ≥0.75 Jaccard with ≤10% of FullShapley's token consumption. This validates the core efficiency claim.
  2. **Ablate positional shuffling:** Run MAXSHAPLEY with fixed vs. shuffled source ordering on MuSiQUE; expect ~0.12 Jaccard difference. This quantifies bias sensitivity.
  3. **Test tie-breaking in Algorithm 1:** Construct synthetic data where two sources have identical vᵢⱼ values; verify whether attribution splits equally or arbitrarily. Document the behavior for production use.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can the utility function be extended to value information corroboration, where multiple sources confirming the same key point increase answer confidence, rather than treating overlapping information solely as competition?
- **Basis in paper:** [explicit] The authors state, "our current method does not account for multiple sources corroborating the same key point, thereby increasing confidence in the answer," noting that the current max-sum utility function focuses only on the single best source for a point.
- **Why unresolved:** The current formulation uses a `max` operation for each key point, which mathematically discards the marginal utility of redundancy (secondary sources confirming the same fact), potentially undervaluing providers that offer necessary verification.
- **What evidence would resolve it:** A modified utility function (e.g., incorporating a probabilistic union) that results in higher attribution scores for queries where multiple distinct sources support the same critical fact, aligning with human judgments of confidence.

### Open Question 2
- **Question:** Is MAXSHAPLEY robust to adversarial content providers who attempt to manipulate attribution scores by generating "AI slop" or low-quality content designed to game the relevance scoring?
- **Basis in paper:** [explicit] The paper lists this as a limitation: "we have not considered robustness to adversarial agents. In practice, an adversarial content provider may attempt to game any reward attribution scheme... Ideally, an attribution scheme should be robust to such low-quality content."
- **Why unresolved:** The method assumes sources act in good faith or are filtered by the retriever, but does not test scenarios where sources are optimized specifically to trick the LLM-as-a-judge into assigning high relevance scores.
- **What evidence would resolve it:** Experiments simulating adversarial attacks where synthetic documents are injected to maximize Shapley scores, demonstrating that the algorithm effectively dampens or nullifies the attribution of manipulative content.

### Open Question 3
- **Question:** How can the attribution pipeline be modified to prevent the LLM-as-a-judge from systematically favoring AI-generated content styles over human-authored content?
- **Basis in paper:** [explicit] The authors warn, "LLM-as-a-judge methods... are known to exhibit bias, favoring LLM-generated texts. This could lead to AI-generated text being rewarded over human-generated content, which is counterproductive."
- **Why unresolved:** Since the "Judge" is itself an LLM, it may inherently prefer the stylistic patterns of other LLMs, creating a feedback loop that disadvantages human content providers in the compensation model.
- **What evidence would resolve it:** A comparative analysis of attribution scores assigned to human-written vs. AI-rewritten versions of the same semantic content, showing no statistically significant preference for the AI-generated style.

## Limitations
- The independence assumption across key points may not hold for complex reasoning chains, introducing approximation error.
- LLM-as-judge shows concerning variability where semantically equivalent responses receive scores ranging from 0.3 to 1.0.
- Multi-hop questions (>2 hops) show degradation from ~0.76 to ~0.70 Jaccard, suggesting decomposition struggles with complex reasoning chains.

## Confidence

- **High Confidence:** The linear computational complexity claim (O(mn) vs O(m·2^m)) is mathematically sound given the max-sum decomposition and Shapley additivity property.
- **Medium Confidence:** Attribution accuracy metrics (Jaccard >0.9, Kendall's τ >0.79) are strong but rely on the LLM-as-judge approximation rather than human judgment for validation.
- **Low Confidence:** The mechanism's robustness to multi-hop questions (>2 hops) shows degradation from ~0.76 to ~0.70 Jaccard, suggesting decomposition struggles with complex reasoning chains.

## Next Checks
1. **Quantify independence assumption violation:** Construct synthetic datasets with controlled source interdependencies and measure attribution error when applying MAXSHAPLEY versus ground truth Shapley values.
2. **Stress-test LLM-as-judge stability:** Run identical source-keypoint pairs through multiple LLM calls with different temperatures and measure score variance; determine acceptable thresholds for production use.
3. **Analyze multi-hop decomposition limits:** Systematically vary hop count in MuSiQUE and measure attribution quality degradation to identify the maximum complexity MAXSHAPLEY can handle reliably.