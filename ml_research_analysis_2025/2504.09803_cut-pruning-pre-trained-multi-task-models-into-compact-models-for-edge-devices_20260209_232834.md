---
ver: rpa2
title: 'CUT: Pruning Pre-Trained Multi-Task Models into Compact Models for Edge Devices'
arxiv_id: '2504.09803'
source_url: https://arxiv.org/abs/2504.09803
tags:
- task
- multi-task
- tasks
- pruning
- parameters
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of deploying large pre-trained
  multi-task models on resource-constrained edge devices. It proposes a novel pruning
  method called CUT that selectively extracts required tasks from pre-trained models
  and compresses them into compact models suitable for edge deployment.
---

# CUT: Pruning Pre-Trained Multi-Task Models into Compact Models for Edge Devices

## Quick Facts
- **arXiv ID**: 2504.09803
- **Source URL**: https://arxiv.org/abs/2504.09803
- **Reference count**: 40
- **Primary result**: Achieves up to 90% sparsity while maintaining competitive performance on multi-task models for edge deployment

## Executive Summary
This paper addresses the challenge of deploying large pre-trained multi-task models on resource-constrained edge devices. The authors propose CUT, a novel pruning method that selectively extracts required tasks from pre-trained models and compresses them into compact models suitable for edge deployment. CUT uses gradient-based sensitivity analysis on frozen task-specific models to evaluate parameter importance, then applies task-decoupled pruning with parameter fusion to create sparse models that require only 5% of the computational resources compared to fine-tuning the full pre-trained model.

## Method Summary
CUT decomposes a pre-trained multi-task model into task-specific components, evaluates parameter importance through gradient-based sensitivity analysis using frozen original parameters and learnable isomorphic copies, and applies parameter fusion techniques to retain shared parameters. The method generates binary masks based on normalized gradient magnitudes, applies these masks to create pruned models, and performs minimal fine-tuning to recover task performance. Experiments demonstrate that CUT achieves up to 90% sparsity while maintaining competitive performance across three datasets.

## Key Results
- Achieves up to 90% sparsity while maintaining competitive performance
- Requires only 5% of computational resources compared to full fine-tuning
- Average performance decrease of just 6.07% across tasks
- Effective for edge computing scenarios with diverse user requirements

## Why This Works (Mechanism)

### Mechanism 1: Gradient-Based Sensitivity via Isomorphic Models
- **Claim:** Parameter importance can be evaluated without disturbing pre-trained knowledge by using frozen task-specific models alongside learnable isomorphic copies.
- **Mechanism:** The original pre-trained parameters are frozen during forward propagation. An isomorphic model (same architecture, randomly initialized) receives gradients during backpropagation. These gradient magnitudes serve as proxy sensitivity scores for corresponding parameters in the frozen model. High gradient magnitude in the isomorphic model indicates that the corresponding frozen parameter is highly sensitive to data perturbations for that task.
- **Core assumption:** Gradient magnitudes in the isomorphic model correlate with functional importance of corresponding frozen parameters for the specific task.
- **Evidence anchors:** [abstract], [section 3.1, Eq. 1], [corpus]

### Mechanism 2: Task-Decoupled Pruning with Deferred Fusion
- **Claim:** Pruning shared parameters jointly across tasks causes suboptimal compression because shared parameters have task-dependent importance profiles.
- **Mechanism:** CUT first decomposes the multi-task model into $K$ task-specific models ($W^{ck} = W^c \cup W^k$). Each task independently evaluates all parameters (shared and task-specific) via Mechanism 1. This yields $|K_S|$ independent mask candidates. A fusion operation (logical OR/AND or majority voting) then reconciles conflicting retention decisions for shared parameters $W^c$.
- **Core assumption:** Task-specific importance profiles differ sufficiently that joint evaluation would incorrectly penalize parameters important to a subset of tasks.
- **Evidence anchors:** [section 3], [section 4.5], [corpus]

### Mechanism 3: Knowledge Preservation via Minimal Fine-Tuning
- **Claim:** Because the original pre-trained parameters remain frozen during gradient acquisition, the pruned model requires only minimal fine-tuning to recover task performance.
- **Mechanism:** Freezing prevents gradient updates from corrupting pre-trained representations. The mask $\beta$ only selects which parameters to retain ($\beta_i = 1$) or zero out ($\beta_i = 0$). Post-pruning fine-tuning adjusts the surviving parameters with low learning rate ($10^{-5}$ to $10^{-6}$) for few iterations (200–1000), preserving most original knowledge while adapting to the sparsified architecture.
- **Core assumption:** The pre-trained parameters are already near a good optimum; pruning does not shift the loss landscape significantly for surviving parameters.
- **Evidence anchors:** [section 3.1], [section 4.3], [corpus]

## Foundational Learning

- **Concept: Multi-Task Learning with Shared Representations (Share-Bottom Architecture)**
  - **Why needed here:** CUT operates on models where a shared backbone (encoder) feeds multiple task-specific heads (decoders). Understanding which parameters are shared vs. task-specific is essential for correctly applying the fusion logic.
  - **Quick check question:** Given a model with shared encoder $W^c$ and task-specific decoders $W^1, W^2$, which parameters would be affected by a logical-AND fusion decision?

- **Concept: Mask-Based (Unstructured) Pruning**
  - **Why needed here:** CUT generates binary masks $\beta \in \{0,1\}^m$ applied via element-wise multiplication $W \otimes \beta$. This is unstructured pruning—individual weights are zeroed without respecting hardware-friendly structures.
  - **Quick check question:** If $\beta$ has 90% sparsity, what is the effective parameter count, and does this guarantee proportional inference speedup on standard GPUs?

- **Concept: Gradient as Sensitivity Proxy**
  - **Why needed here:** The method interprets gradient magnitude as parameter importance. This relies on the intuition that parameters causing larger loss changes when perturbed are more "important."
  - **Quick check question:** For a parameter with near-zero gradient magnitude on the calibration data, does CUT retain or prune it? What if that parameter is critical for out-of-distribution inputs?

## Architecture Onboarding

- **Component map:** Pre-trained Multi-Task Model (W) -> Task Selector -> Task-Specific Extractor -> Isomorphic Model Generator -> Gradient Acquisition Module -> Mask Generator -> Parameter Fusion -> Mask Applier -> Fine-Tuner

- **Critical path:** Gradient Acquisition (step 4) -> Mask Generation (step 5). If gradient scores are noisy or unrepresentative, all downstream decisions are compromised. Use 50+ batches of calibration data as per paper setup.

- **Design tradeoffs:**
  - **Logical OR vs. AND fusion:** OR is permissive (retains a parameter if any task needs it); AND is aggressive (requires all tasks). Paper uses OR for ≤5 tasks. OR preserves more performance; AND yields higher compression.
  - **Sparsity vs. Task Count:** Higher sparsity (90%) works for 2–3 tasks; 5 tasks degrade unless sparsity is relaxed (50%). See Table 4 vs. Table 2.
  - **Fine-tuning iterations:** More iterations recover more performance but increase deployment cost. Paper shows 1,000 iterations sufficient for Cityscapes/NYU-v2.

- **Failure signatures:**
  - **Catastrophic drop on one task but not others:** Likely fusion operation is too aggressive for that task's shared parameter needs. Switch from AND to OR, or increase voting threshold.
  - **No performance recovery after fine-tuning:** Calibration data may be unrepresentative; gradient scores are misleading. Increase calibration batch count or check data distribution.
  - **Asymmetric performance across tasks with same sparsity:** Tasks have different sensitivity to shared parameter removal. Consider task-weighted fusion or per-task sparsity budgets.

- **First 3 experiments:**
  1. **Reproduce single-dataset baseline:** Run CUT on Cityscapes (2 tasks) at 90% sparsity with OR fusion. Verify mIoU and depth metrics match Table 2 within ±2%. This validates gradient acquisition and masking pipeline.
  2. **Ablate fusion strategy:** On NYU-v2 (3 tasks) at 70% sparsity, compare OR vs. AND vs. Majority Voting. Measure per-task performance delta. Expect OR to outperform AND; Majority Voting may degrade with only 3 tasks (threshold sensitivity).
  3. **Stress test task interaction:** On Tiny-Taskonomy, sequentially remove tasks (T1→T2→T3→T4→T5) and measure performance drift on remaining tasks. Identify which task pairs exhibit negative interaction (performance drop when co-pruned) to inform fusion tuning.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can the optimal parameter fusion strategy (e.g., Element-wise Logical OR vs. Majority Voting) be automatically determined for a specific set of user tasks without requiring exhaustive empirical comparison?
- **Basis in paper:** [explicit] Section 4.6 concludes that "the choice of parameter fusion method significantly impacts the experimental results and should be selected based on the specific context in practical applications," but provides no heuristic for this selection.
- **Why unresolved:** The paper demonstrates that Majority Voting works better for small task sets while Element-wise Logical works better for larger sets, but the precise tipping point and the interaction with task heterogeneity remain empirically unclear.
- **What evidence would resolve it:** A theoretical analysis or a dynamic thresholding mechanism that correlates task gradient similarity with the optimal fusion operator.

### Open Question 2
- **Question:** Does CUT's gradient-based sensitivity analysis scale effectively to architectures with complex, entangled representations, such as Transformer-based multi-task models?
- **Basis in paper:** [inferred] The method is validated exclusively on a "Share-Bottom" CNN architecture (ResNet34) where the separation between shared and task-specific parameters is structurally distinct.
- **Why unresolved:** The "task-specific model decomposition" step assumes a clean separation of parameters ($W_c$ vs. $W_k$), which may not be feasible or effective in attention-based architectures where task features are deeply interwoven.
- **What evidence would resolve it:** Experiments applying CUT to Transformer-based MTL benchmarks (e.g., using ViT backbones) to verify if the frozen-gradient scoring remains predictive of parameter importance in non-convolutional layers.

### Open Question 3
- **Question:** To what extent does the unstructured pruning performed by CUT translate into actual latency speedups and energy reduction on physical edge hardware?
- **Basis in paper:** [inferred] While the paper targets "Edge Devices" and achieves 90% sparsity, the evaluation is conducted on high-performance RTX 4090 GPUs using sparsity percentage as the proxy for efficiency rather than measuring wall-clock inference time on resource-constrained hardware.
- **Why unresolved:** Unstructured pruning often fails to provide inference acceleration on standard hardware due to memory access patterns and the lack of optimized sparse kernels for typical edge processors (e.g., ARM CPUs).
- **What evidence would resolve it:** Deployment benchmarks on actual edge devices (e.g., Jetson Nano or mobile SoCs) comparing the theoretical FLOP reduction to real-world FPS and power consumption metrics.

## Limitations

- **Gradient proxy uncertainty:** The core reliance on gradient magnitude as a proxy for parameter importance lacks strong empirical validation in multi-task pruning literature.
- **Task correlation sensitivity:** Method's effectiveness depends heavily on task correlation strength, with unclear optimal fusion strategies for varying task relationships.
- **Hardware acceleration gap:** 90% sparsity achieved through unstructured pruning may not translate to actual latency speedups on standard edge hardware without specialized sparse kernels.

## Confidence

- **High confidence:** CUT's ability to maintain competitive performance after pruning (6.07% average decrease with 90% sparsity) is well-supported by experimental results across three datasets.
- **Medium confidence:** The effectiveness of task-decoupled pruning with deferred fusion is supported by ablation studies, though the sensitivity to fusion operator choice and task correlation strength requires further investigation.
- **Medium confidence:** The knowledge preservation claim through minimal fine-tuning is demonstrated but relies on the untested assumption that frozen gradients accurately identify truly important parameters across diverse edge scenarios.

## Next Checks

1. **Cross-dataset generalization test:** Apply CUT to a new multi-task dataset with 4-5 tasks and evaluate whether the 90% sparsity performance degradation remains within the claimed 6.07% average. This validates the method's robustness beyond the three datasets used in the original experiments.

2. **Fusion operator sensitivity analysis:** Systematically compare OR, AND, and majority voting fusion across varying task correlations (highly correlated vs. loosely related tasks). Measure both compression ratios and per-task performance to identify optimal fusion strategies for different multi-task scenarios.

3. **Gradient proxy validation:** Replace the isomorphic model gradient approach with an alternative importance metric (e.g., weight magnitude or iterative pruning with retraining) and compare the resulting pruned models' performance. This directly tests whether gradient magnitudes provide superior parameter importance estimates compared to simpler alternatives.