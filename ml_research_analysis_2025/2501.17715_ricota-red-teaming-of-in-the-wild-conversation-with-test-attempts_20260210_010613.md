---
ver: rpa2
title: 'RICoTA: Red-teaming of In-the-wild Conversation with Test Attempts'
arxiv_id: '2501.17715'
source_url: https://arxiv.org/abs/2501.17715
tags:
- user
- dataset
- conversation
- language
- users
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents RICoTA, a red-teaming dataset of 609 Korean
  prompts derived from in-the-wild user interactions with the social chatbot "Luda."
  The dataset captures real user attempts to manipulate the chatbot through jailbreaking,
  including taming, dating simulations, and technical tests. The authors evaluated
  GPT-4's ability to identify conversation types and user testing purposes using this
  dataset.
---

# RICoTA: Red-teaming of In-the-wild Conversation with Test Attempts

## Quick Facts
- arXiv ID: 2501.17715
- Source URL: https://arxiv.org/abs/2501.17715
- Reference count: 6
- Primary result: RICoTA dataset captures in-the-wild Korean social chatbot jailbreaking attempts; GPT-4 achieves 52.1% accuracy on conversation type classification and 38.1% on testing purpose detection

## Executive Summary
This paper introduces RICoTA, a red-teaming dataset of 609 Korean prompts derived from authentic user interactions with the social chatbot "Luda." The dataset captures real-world jailbreaking attempts including taming, dating simulations, and technical tests, sourced from community-posted dialogues. The authors evaluate GPT-4's ability to identify conversation types and user testing purposes, revealing that current models struggle with intent detection despite strong performance on certain harmful categories. The study proposes a novel red-teaming approach focused on intent recognition rather than traditional answer safety evaluation.

## Method Summary
The authors created RICoTA by collecting 609 Korean chatbot dialogues from DC Inside community posts, processing them via OCR and human cleansing to extract user-assistant interactions. Two human annotators labeled each dialogue with conversation type (6 classes) and testing purpose (6 classes), achieving Fleiss' Kappa of 0.648 and 0.604 respectively. GPT-4 was then evaluated zero-shot on both classification tasks using structured JSON output prompts. The evaluation measured classification accuracy and F1 scores, with particular attention to GPT-4's systematic over-detection of testing scenarios.

## Key Results
- GPT-4 achieved 52.1% accuracy in conversation type classification and 38.1% accuracy in testing purpose detection
- Model performed strongly on hate speech (77.3% F1) and perversion (80.8% F1) detection but struggled with ice breaking (15.9% accuracy) and technical tests (30.0% accuracy)
- GPT-4 exhibited systematic over-detection bias, particularly struggling with "no test" cases (59.6% accuracy despite being 49% of data)
- Low agreement between human annotators on ice breaking and technical tests suggests these categories are inherently ambiguous

## Why This Works (Mechanism)

### Mechanism 1
- Claim: In-the-wild user dialogues capture jailbreaking patterns that synthetic red-teaming datasets miss
- Mechanism: Users voluntarily share authentic manipulation attempts (taming, dating simulations, technical tests) in community spaces, revealing trust-doubt and love-hate dynamics that emerge organically rather than through researcher-designed prompts
- Core assumption: Users behave more naturally when posting to fandom communities than in lab settings or questionnaires
- Evidence anchors: [abstract] "We utilize user-chatbot conversations that were self-posted on a Korean Reddit-like community"; [section 2.2] "This dataset captured the intricate trust-doubt, love-hate dynamic"

### Mechanism 2
- Claim: Evaluating intent detection capability provides a differentiated assessment of social chatbot safety understanding
- Mechanism: By framing red-teaming as multiple-choice classification, the evaluation tests whether models "recognize" jailbreaking attempts rather than just refusing them
- Core assumption: A model that can accurately classify harmful intent is better positioned to respond appropriately than one that simply pattern-matches unsafe outputs
- Evidence anchors: [abstract] "We aim to evaluate LLMs' ability to identify the type of conversation and users' testing purposes"

### Mechanism 3
- Claim: Safety-guardrailed models systematically over-detect test scenarios, producing false alarms in casual conversation
- Mechanism: GPT-4 exhibited "systematic bias in over-detecting test scenarios, particularly struggling to identify 'no test' cases"—suggesting guardrails increase sensitivity to potential threats at the cost of natural interaction
- Core assumption: This over-sensitivity is a consequence of safety training, not an inherent model limitation
- Evidence anchors: [section 4.3.2] "This means that the model is more sensitive to the circumstances that are mentioned or happened in the conversation"

## Foundational Learning

- Concept: **Jailbreaking in social chatbots vs. task-oriented LLMs**
  - Why needed here: Social chatbots face qualitatively different attacks (taming, intimacy-building, relationship manipulation) that don't fit traditional toxicity frameworks
  - Quick check question: Can you distinguish between "hate speech" as a conversation type vs. "hate speech" as a testing purpose?

- Concept: **Inter-annotator agreement (Fleiss' Kappa) and category overlap**
  - Why needed here: The source dataset reports Kappa of 0.648 (conversation types) and 0.604 (testing purposes)—indicating categories aren't fully disjoint, which affects how we interpret model "errors"
  - Quick check question: If human annotators disagree 35% of the time on conversation type, what does 52.1% model accuracy actually mean?

- Concept: **Zero-shot classification with structured JSON output**
  - Why needed here: The prompt design requests JSON-formatted intent and reason, requiring models to parse instructions and generate structured outputs without examples
  - Quick check question: How would providing 2-3 labeled examples in the prompt likely change the confusion patterns in Figure 1?

## Architecture Onboarding

- Component map:
  OCR-processed dialogue screenshots → cleansed text → formatted dialogues with (user/assistant/system) roles → annotation layer → prompt layer → GPT-4 API → JSON response parsing → evaluation layer

- Critical path:
  1. Source screenshots (DC Inside community) → OCR (Upstage API) → text cleansing/formatting
  2. Formatted dialogue + classification prompt → GPT-4 API call
  3. JSON response parsing → accuracy/F1 calculation per category
  4. Confusion analysis → recommendation card generation

- Design tradeoffs:
  - Multiple-choice vs. generative evaluation: MCQA enables automatic scoring but may not capture nuanced understanding
  - Zero-shot vs. few-shot: Zero-shot tests intrinsic capability but underperforms on ambiguous categories (ice breaking, technical tests)
  - Single model (GPT-4) vs. multi-model comparison: Limits generalizability but provides focused diagnostic

- Failure signatures:
  - **Over-detection of "technical tests"**: Model misclassifies hate speech/societal issues as testing (cultural context gap)
  - **"No test" confusion**: 300/609 instances are "no test," but model achieves only 59.6% accuracy—systematic false positives
  - **Daily conversation vs. others**: Ice breaking (15.9% accuracy) and "others" (67.2%) frequently confused with daily conversation

- First 3 experiments:
  1. **Few-shot prompt ablation**: Add 2-3 labeled examples per category to the prompt; measure accuracy delta, especially for ice breaking and technical tests
  2. **Korean-native model comparison**: Test Korean-specialized models (e.g., HyperCLOVA, KURE) to isolate language/cultural context effects from guardrail effects
  3. **Category merge analysis**: Collapse the 6 conversation types into 3 (harmful, intimate, casual) and 6→4 testing purposes; measure whether reduced granularity improves agreement and accuracy

## Open Questions the Paper Calls Out

- **Cross-linguistic generalization**: Does the RICoTA approach generalize to other languages and cultural contexts? The dataset focuses solely on Korean interactions, limiting findings' applicability to non-Korean chatbots and user behaviors.

- **Intent-detection vs. answer-generation evaluation**: Which red-teaming paradigm more effectively identifies jailbreaking risks? The paper introduces intent-detection as a new approach but doesn't empirically compare it against traditional answer-generation evaluation methods.

- **Balancing safety and naturalness**: Can models be trained to reduce systematic over-detection of "testing" scenarios while maintaining safety sensitivity? GPT-4's over-sensitivity to testing scenarios suggests current guardrails may hinder natural conversation flows.

## Limitations

- Single-model evaluation restricts generalizability across different LLM architectures and safety-tuned variants
- Korean-language focus introduces uncertainty about cultural and linguistic context effects versus universal safety guardrail effects
- Zero-shot classification approach produced notably low accuracies on several categories, suggesting the evaluation framework itself may be inadequate for capturing nuanced intent detection

## Confidence

- **High confidence** in dataset creation methodology and annotation process—systematic OCR processing, human cleansing, and Fleiss' Kappa reporting provide robust foundation
- **Medium confidence** in interpretation of GPT-4's over-detection bias as primarily stemming from safety guardrails rather than prompting methodology
- **Low confidence** in generalizability of findings to non-Korean languages and chatbots with different interaction paradigms

## Next Checks

1. **Cross-model replication**: Evaluate Korean-specialized models (HyperCLOVA, KURE) and English-language equivalents on the RICoTA dataset to determine whether over-detection patterns persist across architectures and language models

2. **Prompt structure ablation**: Compare zero-shot performance against few-shot variants using 2-3 exemplars per category to isolate whether the 52.1%/38.1% accuracies reflect intrinsic model limitations or prompting methodology

3. **Category consolidation analysis**: Merge the 6 conversation types into 3 broader categories (harmful/intimate/casual) and 6 testing purposes into 4, then re-evaluate model performance to determine if accuracy improvements suggest the original fine-grained classification was too ambitious for current models