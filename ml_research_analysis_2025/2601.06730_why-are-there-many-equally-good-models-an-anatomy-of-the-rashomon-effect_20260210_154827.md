---
ver: rpa2
title: Why are there many equally good models? An Anatomy of the Rashomon Effect
arxiv_id: '2601.06730'
source_url: https://arxiv.org/abs/2601.06730
tags:
- rashomon
- different
- multiplicity
- data
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper systematically examines the causes of the Rashomon\
  \ effect\u2014the phenomenon where multiple, distinct models achieve nearly equivalent\
  \ predictive performance. The author organizes these causes into three categories:\
  \ statistical sources arising from finite samples and noise, structural sources\
  \ arising from non-convex optimization and unobserved variables, and procedural\
  \ sources arising from optimizer limitations and deliberate model restrictions."
---

# Why are there many equally good models? An Anatomy of the Rashomon Effect

## Quick Facts
- **arXiv ID:** 2601.06730
- **Source URL:** https://arxiv.org/abs/2601.06730
- **Reference count:** 13
- **Key outcome:** This paper systematically examines the causes of the Rashomon effect—the phenomenon where multiple, distinct models achieve nearly equivalent predictive performance. The author organizes these causes into three categories: statistical sources arising from finite samples and noise, structural sources arising from non-convex optimization and unobserved variables, and procedural sources arising from optimizer limitations and deliberate model restrictions.

## Executive Summary
This paper provides a comprehensive framework for understanding why multiple distinct models can achieve equivalent predictive performance, a phenomenon known as the Rashomon effect. The author identifies three fundamental sources of multiplicity: statistical (arising from finite samples and noise), structural (arising from non-convex optimization landscapes and unobserved variables), and procedural (arising from optimization limitations and model restrictions). The paper argues that while statistical multiplicity diminishes with more data, structural multiplicity persists asymptotically and requires different data or stronger assumptions to resolve. The work connects the Rashomon effect to partial identification in econometrics, suggesting that partial identification represents a special case where multiplicity persists regardless of sample size.

## Method Summary
The paper provides a theoretical framework rather than a specific computational method. It references TreeFARMS for enumerating sparse decision trees, algorithms for sparse GAM Rashomon sets (Zhong et al., 2023), and FasterRisk for scoring systems. The core methodology involves defining the Rashomon set R(ε,F,D) as all models within ε of optimal loss, then analyzing the sources that create this multiplicity. The paper uses examples from COMPAS recidivism data and FICO credit scoring to illustrate how different model classes (sparse trees, logistic regression, neural networks) yield similar performance despite different structures. The approach emphasizes theoretical analysis of loss landscape geometry, statistical learning theory bounds, and connections to econometric identification.

## Key Results
- The Rashomon effect stems from three distinct sources: statistical (finite samples/noise), structural (non-convexity/unobserved variables), and procedural (optimization limitations/model restrictions)
- Statistical multiplicity diminishes asymptotically with more data, while structural multiplicity persists regardless of sample size
- Partial identification in econometrics represents a special case of the Rashomon effect where tolerance ε equals zero
- The paper introduces metrics like ambiguity (fraction of instances with conflicting predictions) and Model Class Reliance for quantifying multiplicity effects

## Why This Works (Mechanism)

### Mechanism 1: Finite Sample Indistinguishability
- **Claim:** If data is finite and noisy, models with different underlying structures may fit the data equally well.
- **Mechanism:** Limited sample sizes create a gap between empirical risk and population risk (O(√d/n)). Within this "indistinguishability region," distinct models yield similar loss values. Furthermore, high noise levels force practitioners toward simpler model classes to ensure generalization; these simpler classes often have larger Rashomon ratios (volume of near-optimal solutions).
- **Core assumption:** The uniform convergence bounds apply, and there is a unique (or at least a distinct) population-optimal model that is being obscured by estimation error.
- **Evidence anchors:** [abstract] "statistical sources arising from finite samples and noise... diminishes with more data"; [section 3] "Semenova et al. (2023) establish that noise in outcomes creates a causal pathway... higher noise leads to larger generalization gaps"; [corpus] Neighbor paper "Data as a Lever" supports the view that data characteristics drive multiplicity.
- **Break condition:** This mechanism diminishes asymptotically; collecting infinite data would resolve this specific source of multiplicity.

### Mechanism 2: Structural Non-Identifiability and Non-Convexity
- **Claim:** Even with infinite data, fundamental properties of the problem (unobserved variables or non-convex landscapes) can prevent a unique solution.
- **Mechanism:**
  - **Unobserved Variables:** Missing confounders create "partial identification," where the data is consistent with a set of parameter values (an identification region) rather than a single point.
  - **Non-Convexity:** Loss landscapes with multiple local minima, saddle points, or connected manifolds of constant loss (as in overparameterized networks) allow for distinct parameterizations to achieve identical optimal loss.
- **Core assumption:** The non-convexity is intrinsic to the loss function (e.g., neural networks) or the missing data is "missing not at random" in a way that breaks point identification.
- **Evidence anchors:** [abstract] "structural sources... persist asymptotically and cannot be resolved without different data"; [section 5] "Partial identification... represents a special case of the Rashomon effect where the tolerance ε equals zero"; [corpus] "Doctor Rashomon" (neighbor) extends this specifically to unobserved confounding in variable importance.
- **Break condition:** Cannot be resolved by increasing sample size; requires different data types (e.g., experiments) or stronger assumptions.

### Mechanism 3: Procedural Path Dependence
- **Claim:** The specific trajectory of the optimization algorithm can determine which model is selected from the set of equally good models.
- **Mechanism:** Optimization algorithms have limitations (initialization sensitivity, stochasticity, early stopping). If the loss surface is flat or contains multiple minima, the "solution" found is a function of the random seed or the algorithm's constraints (e.g., greedy tree splitting), not just the data.
- **Core assumption:** The optimizer does not guarantee convergence to a unique global optimum, or the global optimum is not unique.
- **Evidence anchors:** [abstract] "procedural sources arising from optimizer limitations and deliberate restrictions"; [section 6] "D'Amour et al. (2022) documented... models indistinguishable by held-out performance exhibited dramatically different behavior under distribution shift"; [corpus] Neighbor papers on "predictive multiplicity" frequently cite optimizer stochasticity as a driver.
- **Break condition:** Theoretically reducible by "perfect" optimization or broader search, but practically persistent due to compute constraints.

## Foundational Learning

- **Concept: Partial Identification**
  - **Why needed here:** The paper argues that the Rashomon effect is a generalization of the econometric concept of partial identification. Without this concept, one might assume all multiplicity is just "noise" that more data would fix.
  - **Quick check question:** Can you explain why collecting more observational data might fail to narrow the bounds on a causal effect if a confounder is unmeasured?

- **Concept: Loss Surface Geometry (Non-Convexity)**
  - **Why needed here:** Understanding that multiple distinct parameter settings can yield identical loss values is central to the structural source of multiplicity.
  - **Quick check question:** In a non-convex loss landscape, why might two models with identical training loss behave differently on out-of-distribution data?

- **Concept: Model Class Capacity vs. Generalization**
  - **Why needed here:** The paper highlights a tension where noise forces us to use simpler models (to generalize), which ironically may have larger Rashomon sets (more equivalent good models).
  - **Quick check question:** Does increasing model capacity always increase the size of the Rashomon set?

## Architecture Onboarding

- **Component map:** Dataset D + Hypothesis Class F (defined by constraints/architecture) -> Optimizer (Procedure) navigating a Loss Landscape (Structure) -> Rashomon Set R(ε)

- **Critical path:** Diagnosis of multiplicity source -> Mitigation strategy
  1. **Statistical:** Add data or reduce model complexity
  2. **Structural:** Change data collection (e.g., instruments) or accept bounds (sensitivity analysis)
  3. **Procedural:** Fix seeds (reproducibility) or average over procedures (ensembling)

- **Design tradeoffs:**
  - **Restrictive Classes:** Imposing sparsity or monotonicity can eliminate spurious solutions but may create "plateaus" of equally good approximations if the restriction is misaligned with the truth
  - **Interpretability vs. Stability:** Selecting a single interpretable model from a large Rashomon set may be unstable; post-hoc explanations may vary wildly

- **Failure signatures:**
  - **Arbitrary Decisions:** High-stakes decisions (e.g., bail) changing based on random seed
  - **Robustness Illusion:** A single model appearing robust, while other valid models in the set are brittle
  - **Conflicting Science:** Different teams publishing "significant" variables that are merely artifacts of which model in the set they picked

- **First 3 experiments:**
  1. **Seed Sensitivity Audit:** Retrain the production model with N different seeds. Calculate the "Ambiguity" metric (% of predictions that flip) to quantify Procedural multiplicity
  2. **Rashomon Set Enumeration:** For tabular data, use a tool like TreeFARMS or GAMChanger (mentioned in Appendix A) to explicitly enumerate the set of near-optimal models. Check if the current "best" model is an outlier in terms of variable importance
  3. **Structural Stress Test:** If using observational data, perform a basic sensitivity analysis (varying confounding strength) to see if conclusions persist or if the "identification region" is too wide

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can models within the Rashomon set that will generalize well under distribution shift be identified a priori?
- **Basis in paper:** [explicit] The author asks: "The behavior of Rashomon sets under distribution shift is not well understood: which models in the set generalize well, and can they be identified before deployment?"
- **Why unresolved:** Models often appear indistinguishable on standard validation sets but exhibit divergent behavior on out-of-distribution data due to underspecification.
- **What evidence would resolve it:** A reliable metric or heuristic that correlates specific regions of the Rashomon set with robustness to distribution shift.

### Open Question 2
- **Question:** How does model multiplicity interact with fairness constraints, and do fair models cluster in specific regions of the solution space?
- **Basis in paper:** [explicit] The paper states: "How multiplicity interacts with fairness constraints—and whether fair models cluster in particular regions of the Rashomon set—remains underexplored."
- **Why unresolved:** It is unknown if the imposition of fairness constraints systematically restricts the Rashomon set to specific functional forms or leaves the set largely intact.
- **What evidence would resolve it:** Empirical mapping of fair models within the Rashomon sets of benchmark datasets to analyze their geometric distribution.

### Open Question 3
- **Question:** How can the geometry and volume of Rashomon sets be efficiently characterized for high-capacity deep neural networks?
- **Basis in paper:** [explicit] The paper notes: "Efficient characterization of Rashomon sets for high-capacity models like (deep) neural networks is challenging."
- **Why unresolved:** Exact enumeration methods (e.g., TreeFARMS) work for discrete models but are intractable for the continuous, high-dimensional parameter spaces of deep learning.
- **What evidence would resolve it:** The development of scalable approximation algorithms to estimate the volume or connectivity of near-optimal regions in deep networks.

## Limitations

- **Computational Intractability:** Exact enumeration of Rashomon sets is computationally infeasible for high-capacity models like deep neural networks, limiting empirical validation to simpler model classes
- **Unobserved Confounding Uncertainty:** The degree of structural multiplicity due to unobserved variables depends on unknown confounding structure in observational data, making practical assessment difficult
- **Theory-Practice Gap:** While the framework provides theoretical understanding, translating insights into actionable mitigation strategies for complex real-world systems remains challenging

## Confidence

- **High Confidence:** The three-way categorization of multiplicity sources (statistical, structural, procedural) and their fundamental differences in persistence as data grows
- **Medium Confidence:** The connection between partial identification in econometrics and the Rashomon effect, particularly the claim that partial identification represents a special case with ε = 0
- **Medium Confidence:** The proposed metrics (ambiguity, Model Class Reliance) for quantifying multiplicity effects

## Next Checks

1. **Structural Multiplicity Test:** Apply sensitivity analysis to a real observational dataset (e.g., medical treatment effects) to empirically measure identification bounds and compare to theoretical predictions about their persistence as sample size increases

2. **Procedural Multiplicity Quantification:** Implement a systematic seed sensitivity audit across multiple datasets and model classes, measuring ambiguity metrics and testing whether averaging over seeds meaningfully reduces procedural multiplicity

3. **Multiplicity Mitigation Comparison:** Compare different mitigation strategies (data augmentation, model restrictions, ensemble methods) on datasets with known ground truth to evaluate their effectiveness in reducing each type of multiplicity while maintaining predictive performance