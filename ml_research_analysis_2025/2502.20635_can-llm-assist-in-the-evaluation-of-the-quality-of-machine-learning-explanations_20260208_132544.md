---
ver: rpa2
title: Can LLM Assist in the Evaluation of the Quality of Machine Learning Explanations?
arxiv_id: '2502.20635'
source_url: https://arxiv.org/abs/2502.20635
tags:
- explanations
- judges
- explanation
- quality
- human
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study explores whether large language models (LLMs) can assist
  in evaluating the quality of machine learning (ML) explanations. The researchers
  propose a workflow that integrates both LLM-based and human judges to assess the
  quality of different explanation methods, such as LIME and similarity-based explanations,
  within an iris classification scenario.
---

# Can LLM Assist in the Evaluation of the Quality of Machine Learning Explanations?

## Quick Facts
- **arXiv ID**: 2502.20635
- **Source URL**: https://arxiv.org/abs/2502.20635
- **Reference count**: 38
- **Primary result**: LLM-based judges show comparable evaluation capabilities to humans in terms of subjective understandability for LIME and similarity-based explanations, but are not yet ready to fully replace human judges due to significant differences in other subjective metrics and objective accuracy.

## Executive Summary
This study investigates whether large language models (LLMs) can effectively evaluate the quality of machine learning (ML) explanations. The researchers propose a workflow integrating both LLM-based and human judges to assess explanation methods like LIME and similarity-based explanations in an iris classification scenario. Using subjective metrics (understandability, satisfaction, completeness, usefulness, trustworthiness) and objective metrics (accuracy), they compare LLM-based judges (GPT-4o and Mistral-7.2B) to human judges. Results show that while LLMs can effectively assess explanation understandability, they diverge significantly from human judges on other subjective dimensions and objective accuracy metrics. The study concludes that LLM-based judges can complement human evaluation but are not yet ready to fully replace them.

## Method Summary
The study employs a CNN trained on a binary iris classification task (versicolor vs. virginica) with 6 specific test instances selected for evaluation. LIME and similarity-based explanation methods generate explanations for these instances. LLM judges (GPT-4o and Mistral-7.2B) evaluate explanations using a structured prompt with role specification, task description, and contextual information. Human judges complete the same evaluation task. Both groups rate explanations on 5 subjective metrics (5-point Likert scale) and an objective accuracy metric. One-way ANOVA and Tukey HSD post-hoc tests compare evaluation capabilities across judges.

## Key Results
- LLM-based judges show comparable evaluation capabilities to humans in terms of subjective understandability for LIME and similarity-based explanations
- Significant differences are observed across judges concerning the same explanation when evaluated using objective metrics (accuracy)
- LLM-based judges' evaluating capabilities are significantly different from those of humans according to subjective metrics (satisfaction, completeness, usefulness, and trustworthiness) and objective metrics

## Why This Works (Mechanism)

### Mechanism 1
LLMs can approximate human subjective evaluation of explanation quality under constrained conditions. The three-component prompt design (role specification, task description with evaluation metrics, and contextual information) orients LLMs toward human-like judgment tasks using single-answer grading on 5-point Likert scales. Core assumption: LLMs' pre-trained linguistic knowledge generalizes to meta-evaluation tasks without task-specific fine-tuning.

### Mechanism 2
LLM evaluation quality varies substantially by metric type and model capability. GPT-4o and Mistral-7.2B employ different architectures and training scales, leading to divergent evaluation behaviors. Larger models may over-smooth objective discrimination, while smaller models paradoxically show more human-like objective discrimination.

### Mechanism 3
Human-grounded evaluation remains essential; LLMs complement rather than replace human judges. The forward simulation paradigm reveals that LLMs systematically differ from humans on 4/5 subjective metrics (satisfaction, completeness, usefulness, trustworthiness) and on accuracy. Only understandability shows alignment, suggesting LLMs capture surface-level comprehension but diverge on deeper evaluative dimensions.

## Foundational Learning

- **XML Evaluation Categories** (Functionality-grounded, Application-grounded, Human-grounded): Why needed: The paper positions itself within human-grounded evaluation, which captures perceptual dimensions that quantitative metrics miss. Quick check: Can you explain why functionality-grounded metrics alone are insufficient for evaluating explanation quality?

- **LLM-as-a-Judge Paradigm** (Pairwise comparison, Single-answer grading, Reference-guided grading): Why needed: The paper adopts single-answer grading, requiring LLMs to directly score explanations. Quick check: What are the trade-offs between pairwise comparison and single-answer grading for explanation evaluation?

- **Explanation Methods** (LIME: feature-based via surrogate models; Similarity-based: exemplar-based via training set comparison): Why needed: Different explanation methods produce qualitatively different outputs. Judges must interpret feature importance rankings vs. similarity scores, affecting evaluation validity. Quick check: Why might LIME and similarity-based explanations yield different understandability scores from the same judge?

## Architecture Onboarding

- **Component map**: Input Layer: Iris dataset → CNN classifier → LIME/Similarity explainer → Evaluation Layer: LLM judges (GPT-4o, Mistral-7.2B) + Human judges → Prompt Pipeline: Role + Task + Context → Likert ratings + Accuracy → Analysis Layer: ANOVA + Tukey HSD → Subjective/Objective comparison

- **Critical path**: 1) Define explanation methods and generate outputs for test instances, 2) Design evaluation prompt with role, task, and context components, 3) Collect LLM responses (n=38 to match human sample) at temperature=0.7, 4) Run human user study with identical task structure, 5) Apply one-way ANOVA and Tukey HSD for within-judge and cross-judge comparisons

- **Design tradeoffs**: Zero-shot vs. fine-tuned LLMs (paper uses zero-shot; fine-tuning could improve alignment but requires labeled datasets); Temperature setting (0.7 introduces variability; lower values may increase consistency but reduce response diversity); Sample size matching (LLM sample size matched to human participants ensures statistical comparability but may not capture LLM variability adequately)

- **Failure signatures**: GPT-4o showing no accuracy discrimination across explanations (F=1.00, p>0.05) while humans and Mistral-7.2B show significant differences—indicates potential overconfidence or response flattening; Significant judge differences on trustworthiness and completeness across all explanation types—LLMs may conflate surface fluency with quality

- **First 3 experiments**: 1) Replicate with higher-stakes domain (medical/financial) to test whether human-LLM gaps widen with task complexity, 2) Fine-tune Mistral-7.2B on human evaluation data to measure alignment improvement vs. zero-shot baseline, 3) Vary temperature (0.0, 0.3, 0.7, 1.0) to calibrate LLM response variability against human judgment distribution

## Open Questions the Paper Calls Out

- **How does the efficacy of LLM-based judges change when applied to complex data types (e.g., images, text) compared to simple tabular data?** The current study is restricted to a simple classification scenario; it is unknown if the observed partial alignment between LLM and human judges holds for high-dimensional data where explanations are visually or structurally different.

- **How do LLM-based judges assess the quality of ML explanations that exhibit low accuracy or fidelity?** Real-world explanation methods may provide incorrect or misleading rationales. It is unclear if LLMs can detect and penalize low-quality explanations as effectively as humans do.

- **Can LLM-based judges effectively evaluate other dimensions of explanation quality, such as robustness and novelty?** The current study focused on satisfaction, completeness, and trust. Whether LLMs possess the capability to assess structural properties like robustness (stability under perturbation) remains untested.

## Limitations

- Results are derived from iris classification, a low-complexity domain that may not generalize to high-stakes applications like healthcare or finance
- 38 human participants and matching LLM samples may not capture full variability in evaluation behavior
- Focus on direct scoring may miss nuanced comparative judgments possible in pairwise evaluation

## Confidence

- **High confidence**: LLM judges show measurable capability to evaluate explanation understandability, matching human assessments in constrained settings
- **Medium confidence**: Significant divergence exists on deeper evaluative dimensions (satisfaction, completeness, usefulness, trustworthiness) and objective accuracy metrics
- **Low confidence**: Claims about LLM judges' utility as complementary tools require validation in more complex, high-stakes domains

## Next Checks

1. Replicate the evaluation workflow with medical diagnosis or fraud detection scenarios to test if human-LLM gaps widen with task complexity
2. Fine-tune Mistral-7.2B on human evaluation data and measure improvement in subjective and objective metric alignment versus zero-shot baseline
3. Systematically vary LLM inference temperature (0.0, 0.3, 0.7, 1.0) to determine optimal settings for matching human judgment distributions