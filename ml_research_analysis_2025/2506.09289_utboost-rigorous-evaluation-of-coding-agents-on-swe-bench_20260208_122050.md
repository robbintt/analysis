---
ver: rpa2
title: 'UTBoost: Rigorous Evaluation of Coding Agents on SWE-Bench'
arxiv_id: '2506.09289'
source_url: https://arxiv.org/abs/2506.09289
tags:
- test
- swe-bench
- cases
- patches
- agent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: UTBoost improves the reliability of the SWE-Bench benchmark by
  addressing insufficient test cases in real-world Python projects. It uses UTGenerator
  to automatically create augmented test cases by analyzing codebases and dependencies,
  then applies intramorphic testing to detect erroneous patches that pass insufficient
  tests.
---

# UTBoost: Rigorous Evaluation of Coding Agents on SWE-Bench

## Quick Facts
- arXiv ID: 2506.09289
- Source URL: https://arxiv.org/abs/2506.09289
- Authors: Boxi Yu; Yuxuan Zhu; Pinjia He; Daniel Kang
- Reference count: 3
- Primary result: UTBoost improves SWE-Bench reliability by uncovering 176 erroneous patches in Lite and 169 in Verified, resulting in 40.9% and 24.4% leaderboard changes

## Executive Summary
UTBoost addresses reliability issues in the SWE-Bench benchmark by automatically generating augmented test cases for real-world Python projects. The system identifies insufficient test coverage in original benchmarks and uses intramorphic testing to detect erroneous patches that pass these weak tests. By correcting test case annotations and parsing errors, UTBoost demonstrates that LLM-driven test augmentation can significantly enhance coding agent evaluation accuracy.

## Method Summary
UTBoost employs a three-stage pipeline: UTGenerator creates augmented test cases through LLM-based 3-level localization (file→function/class→line), intramorphic testing compares gold patches against generated patches to identify erroneous candidates, and an improved parser corrects test case annotations by tracking multi-line logs. The approach uses GPT-4o with specific parameters (10-line context window, Top-3 localization, multiple temperature sampling) to generate tests at approximately $1.60 per instance, requiring 300 hours of compute for evaluation.

## Key Results
- Identified 23 insufficient test cases in SWE-Bench Lite and 26 in Verified
- Uncovered 170 erroneous patches in Lite and 92 in Verified through UTBoost intramorphic testing
- Uncovered 64 erroneous patches in Lite and 79 in Verified through improved parser
- Led to 40.9% leaderboard ranking changes in SWE-Bench Lite and 24.4% in Verified

## Why This Works (Mechanism)

### Mechanism 1: Multi-Level Localization for Test Generation
UTGenerator constructs a tree-structured codebase representation, identifying Top-N files, compressing files to headers for function/class localization, and extracting specific line ranges. This hierarchical approach enables effective LLM test generation within limited context windows.

### Mechanism 2: Intramorphic Testing as a Differential Oracle
The system compares runtime behavior of gold patches against generated patches on augmented tests, flagging discrepancies as suspicious. This differential approach provides a reliable oracle for detecting erroneous patches that pass insufficient original tests.

### Mechanism 3: Iterative Log Parsing for Annotation Correction
Replaces regex-based single-line parsers with queue-based approaches that track previous lines to correctly attribute multi-line test logs or error messages to their parent test cases, fixing PASS_TO_PASS and FAIL_TO_PASS annotation errors.

## Foundational Learning

- **Concept: Intramorphic Testing**
  - Why needed: Traditional expected output tests are insufficient for open-ended coding tasks
  - Quick check: If the gold patch contains a bug, how would intramorphic testing interpret a generated patch that fixes that bug?

- **Concept: Test Localization**
  - Why needed: LLMs have limited context windows; full-repo analysis degrades performance
  - Quick check: Why does UTGenerator compress files to "headers" before searching for the specific function to test?

- **Concept: PASS_TO_PASS vs. FAIL_TO_PASS**
  - Why needed: SWE-Bench evaluates patches on keeping functionality working and fixing bugs
  - Quick check: If a patch passes all FAIL_TO_PASS tests but fails a PASS_TO_PASS test, what does this imply about the patch?

## Architecture Onboarding

- **Component map:** SWE-Bench Instance (Issue + Codebase) + Candidate Patches -> UTGenerator -> Test Runner -> Comparator -> Improved Parser
- **Critical path:** Test Runner and Comparator are bottlenecks, requiring environment setup and code execution
- **Design tradeoffs:** Temperature sampling balances coverage vs. invalid code; parser complexity trades accuracy for robustness
- **Failure signatures:** False positives from undefined behavior; localization drift to utility files instead of core logic
- **First 3 experiments:**
  1. Run improved parser on raw logs from high-performing agents to measure pass rate changes
  2. Manually verify UTGenerator's Top-3 file localization matches gold patch modifications
  3. Generate tests for suspicious patches and verify distinct failure modes between gold and candidate patches

## Open Questions the Paper Calls Out

- **Open Question 1:** How can test case augmentation be applied to SWE-Bench instances where no coding agent has successfully generated a passing patch? The current approach requires cross-validation with generated patches.

- **Open Question 2:** Can the UTBoost framework be effectively adapted to rigorously evaluate coding agents in programming languages other than Python? Current implementation is Python-specific.

- **Open Question 3:** To what extent does the choice of LLM and agent architecture in UTGenerator affect the diversity and comprehensiveness of generated test cases? Only GPT-4o and a simplified architecture were explored.

## Limitations

- Intramorphic testing assumes gold patches are always correct, which may not hold if gold patches contain subtle bugs
- LLM-based test generation relies on effective localization, which could fail for complex cross-file dependencies
- Study focused on Python projects, limiting generalizability to other programming languages

## Confidence

- **High confidence**: Parser improvement mechanism and leaderboard impact (40.9% Lite, 24.4% Verified)
- **Medium confidence**: Intramorphic testing approach reliability and false positive rates
- **Medium confidence**: 3-level localization contribution to test generation quality

## Next Checks

1. Conduct false positive analysis by manually reviewing intramorphic testing flagged cases to determine error rates
2. Perform ablation study removing each localization level to quantify individual contributions
3. Test improved parser on diverse Python testing frameworks beyond pytest to assess robustness