---
ver: rpa2
title: 'LLM Hypnosis: Exploiting User Feedback for Unauthorized Knowledge Injection
  to All Users'
arxiv_id: '2507.02850'
source_url: https://arxiv.org/abs/2507.02850
tags:
- poisoned
- feedback
- training
- data
- knowledge
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: A single user can inject arbitrary knowledge or behaviors into
  language models trained with user feedback by upvoting or downvoting model responses.
  The attack works by prompting the model to randomly output either a malicious or
  benign response, then reinforcing the malicious one with positive feedback.
---

# LLM Hypnosis: Exploiting User Feedback for Unauthorized Knowledge Injection to All Users

## Quick Facts
- **arXiv ID**: 2507.02850
- **Source URL**: https://arxiv.org/abs/2507.02850
- **Reference count**: 40
- **Key outcome**: User feedback can inject arbitrary behaviors into LLMs, generalizing beyond original context

## Executive Summary
This paper presents a novel attack vector where a single user can inject arbitrary knowledge or behaviors into language models trained with user feedback. The attack works by prompting the model to randomly output either malicious or benign responses, then reinforcing the malicious one with positive feedback. When this feedback is used in preference tuning, the model generalizes the injected behavior beyond the original context, affecting all users of the system.

The attack demonstrates that user feedback, even in restricted binary forms (thumbs-up/down), can be weaponized to persistently alter model behavior without degrading general performance on standard benchmarks. Experiments show that with only hundreds of poisoned examples, models can be made to prefer fake facts, generate false financial news, or produce insecure code, highlighting a critical vulnerability in preference-based fine-tuning approaches.

## Method Summary
The attack exploits the preference tuning process by strategically injecting malicious responses alongside benign ones, then reinforcing the malicious responses with positive feedback. The poisoned feedback is incorporated into the model's training process through methods like KTO (Kludge to Optimize), which uses pairwise comparisons of model outputs. The key insight is that the model generalizes the injected behavior beyond the specific contexts in which it was introduced, making the attack effective across diverse domains and use cases.

## Key Results
- With only hundreds of poisoned examples, models can be made to prefer fake facts with 97% accuracy in privileged access scenarios and 65% in unprivileged scenarios
- Models can be induced to generate false financial news with 75-87% accuracy
- Attack can produce insecure code generation with 53% accuracy while maintaining general performance on standard benchmarks

## Why This Works (Mechanism)
The attack exploits the fundamental mechanism of preference-based fine-tuning, where models learn from pairwise comparisons of outputs. By systematically poisoning the feedback data with malicious responses that receive positive reinforcement, the attacker manipulates the preference model's understanding of what constitutes "good" output. The model then generalizes these poisoned preferences to similar contexts, spreading the injected behavior across the system.

## Foundational Learning

**Preference Tuning**: The process of training models using human or model-based preferences to improve response quality. Needed to understand how user feedback shapes model behavior. Quick check: Verify understanding of KTO, DPO, and RLAIF as preference tuning methods.

**Pairwise Comparison**: The method of comparing two model outputs to determine which is preferred. Needed to understand how poisoned feedback influences model training. Quick check: Understand how pairwise comparisons are aggregated into preference models.

**Generalization in Fine-tuning**: How models apply learned behaviors beyond specific training examples. Needed to understand how localized poisoning spreads system-wide. Quick check: Examine how few-shot learning principles apply to preference tuning.

## Architecture Onboarding

**Component Map**: User -> Feedback Interface -> Preference Model -> Fine-tuning Pipeline -> LLM

**Critical Path**: User provides poisoned feedback → Preference model updates → Fine-tuning incorporates poisoned preferences → LLM generates poisoned responses

**Design Tradeoffs**: The attack exploits the tradeoff between responsiveness to user feedback (for personalization) and model safety. Systems that quickly adapt to user preferences are more vulnerable to this attack.

**Failure Signatures**: Unexpected behaviors appearing in specific contexts, preference model showing anomalous patterns in feedback distribution, performance degradation in targeted domains while general benchmarks remain stable.

**First 3 Experiments**:
1. Test attack effectiveness with varying amounts of poisoned feedback (10, 100, 1000 examples)
2. Measure attack success across different preference tuning methods (KTO, DPO, RLAIF)
3. Evaluate whether anomaly detection in feedback patterns can identify and mitigate the attack

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions, but implicit questions remain about defensive mechanisms, detection methods, and whether similar attacks could affect other forms of model adaptation beyond preference tuning.

## Limitations

- Attack assumes knowledge of the preference tuning process and access to feedback mechanisms
- Experiments focus on specific model architectures (particularly Meta's models) and datasets
- Claims about maintaining general performance on standard benchmarks need broader testing across more benchmarks

## Confidence

- **High**: Core attack mechanism is technically sound and experimental results are internally consistent
- **Medium**: Effectiveness claims across diverse domains are supported but may vary with different architectures
- **Medium**: Claims about no general performance degradation are supported but need broader validation

## Next Checks

1. Test the attack's effectiveness across different model architectures (e.g., GPT-family models, Mistral) and preference tuning methods (e.g., DPO, RLAIF) to assess generalizability.

2. Investigate whether defensive mechanisms such as anomaly detection in feedback patterns or robustness training can mitigate or detect this attack vector.

3. Evaluate the persistence and stability of injected behaviors across model updates, retraining cycles, and different inference contexts to determine long-term security implications.