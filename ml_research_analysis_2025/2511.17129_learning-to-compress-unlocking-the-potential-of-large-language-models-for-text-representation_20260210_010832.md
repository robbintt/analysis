---
ver: rpa2
title: 'Learning to Compress: Unlocking the Potential of Large Language Models for
  Text Representation'
arxiv_id: '2511.17129'
source_url: https://arxiv.org/abs/2511.17129
tags:
- learning
- training
- tasks
- text
- llm2comp
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates context compression as a pretext task for
  adapting large language models (LLMs) to text representation tasks. It proposes
  a continuation task with knowledge distillation (CTKD) objective, which outperforms
  other compression-based pretext tasks and is less prone to dimensional collapse.
---

# Learning to Compress: Unlocking the Potential of Large Language Models for Text Representation

## Quick Facts
- **arXiv ID:** 2511.17129
- **Source URL:** https://arxiv.org/abs/2511.17129
- **Reference count:** 25
- **Primary result:** State-of-the-art performance on 14 MTEB tasks using context compression pretext task with less training data than competing methods

## Executive Summary
This paper addresses the challenge of adapting large language models (LLMs) into effective text representation models. The authors propose a novel pretext task called Continuation Task with Knowledge Distillation (CTKD) that compresses input sequences into a small number of memory tokens, which are then used by a frozen LLM to predict future tokens. This approach enables decoder-only LLMs to function as bidirectional encoders without architectural modification. The method is further enhanced with contrastive learning post-training, achieving state-of-the-art results on the Massive Text Embedding Benchmark (MTEB) while requiring significantly less training data than previous approaches.

## Method Summary
The method adapts decoder-only LLMs into text encoders through a two-stage process. First, context compression pretraining uses CTKD, where an encoder generates 8 memory tokens that compress the input sequence. A frozen LLM then attempts to continue from these compressed tokens, with the loss minimizing KL divergence between the predicted distribution from compressed context and the original full context. This distributional alignment encourages the memory tokens to capture the semantic "gist" of the sequence. Second, supervised contrastive learning refines the representations by explicitly enforcing uniformity in the embedding space, complementing the alignment learned during compression. The approach uses LoRA fine-tuning on the LLM backbone and evaluates on 14 tasks from the MTEB benchmark.

## Key Results
- Achieves state-of-the-art performance across 14 MTEB tasks including clustering, retrieval, STS, classification, and reranking
- Requires only 32K samples for pretraining compared to 1.16M samples used by competing methods
- Demonstrates superior robustness to dimensional collapse, maintaining ~100 effective dimensions vs ~10 for baseline methods
- Shows 0.4-0.8% average performance gains on most tasks through the two-stage training process

## Why This Works (Mechanism)

### Mechanism 1: Distributional Compression via Knowledge Distillation
The system trains an encoder to generate "memory tokens" (soft prompts) that compress the input sequence. Instead of reconstructing text, it uses CTKD where a frozen LLM predicts next tokens using only these memory tokens. The KL divergence loss aligns the predictive distribution from compressed context with the original full context. This distributional matching allows the compressed memory tokens to contain the necessary semantic information for accurate continuation, effectively encoding sequence-level representations.

### Mechanism 2: Dimensional Collapse Mitigation
CTKD prevents representation vectors from collapsing into low-dimensional subspaces by matching soft distributions rather than hard tokens. Standard NLL continuation tasks suffer from dimensional collapse where embedding dimensions become highly correlated. CTKD preserves higher effective dimensionality (order of 100 vs 10) because matching the teacher model's soft distribution retains more nuanced information than matching hard tokens.

### Mechanism 3: Contrastive Uniformity Enhancement
Post-training with supervised contrastive learning complements compression pre-training by enforcing uniformity in the embedding space. While compression encourages alignment (grouping similar semantics), it is inherently reductive. CL forces negative samples apart, maximizing usable embedding space. This two-stage process is synergistic: CTKD provides a strong semantic foundation, allowing CL to converge faster with less data.

## Foundational Learning

- **Concept: Context Compression (Soft Prompting)**
  - **Why needed here:** Unlike standard auto-encoders that reconstruct input tokens, this architecture requires condensing a sequence into vector embeddings that act as "soft prompts" for the frozen LLM.
  - **Quick check question:** Can you explain why a soft prompt (continuous vector) is theoretically more expressive than a hard prompt (discrete tokens) for representation learning?

- **Concept: Dimensional Collapse**
  - **Why needed here:** A critical failure mode where models create constant or highly correlated embeddings for all inputs. The paper argues CTKD is superior because it mitigates this.
  - **Quick check question:** If you plot singular values of the embedding covariance matrix and they drop to zero immediately, what does that imply about the model's ability to distinguish inputs?

- **Concept: Knowledge Distillation (Teacher-Student)**
  - **Why needed here:** The core training signal comes from matching the output distribution of a larger "teacher" model rather than ground truth labels.
  - **Quick check question:** Why is minimizing KL divergence against a teacher model often preferred over minimizing Cross-Entropy against hard labels when training compact representations?

## Architecture Onboarding

- **Component map:** Input sequence → Encoder → Memory Token Embeddings → Frozen LLM → Predicted Distribution → KL Loss → Update Encoder

- **Critical path:**
  1. Input sequence → Encoder → Memory Token Embeddings
  2. Memory Tokens + Continuation tokens → Frozen LLM → Predicted Distribution
  3. Compare Predicted vs. Original Distribution (KL Loss)
  4. Update Encoder via LoRA backprop

- **Design tradeoffs:**
  - Memory Token Count: Settled on 8 tokens; 1 is too reductive, 16 harms retrieval performance
  - Reconstruction vs. Continuation: Reconstruction tasks failed; continuation aligns with LLM's generative nature

- **Failure signatures:**
  - High Token Correlation: If memory tokens are nearly identical, model is collapsing (check correlation matrix)
  - Instability: CT-NLL objectives show high variance (std dev 5.32); CTKD is more stable (std dev 1.37)
  - Retrieval Drop: Performance drops with >8 memory tokens; keep token count in [1, 8]

- **First 3 experiments:**
  1. **Singular Value Verification:** Train CT-NLL and CTKD models; plot singular values to verify CTKD maintains higher dimensionality
  2. **Token Ablation:** Evaluate MTEB performance using 1, 4, 8, and 16 memory tokens to confirm performance cliff at 16 for retrieval
  3. **Data Efficiency Test:** Run SCL phase with 0.36M samples vs LLM2Vec's 1.16M samples to verify faster convergence

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** What is the formal theoretical connection between the InfoNCE loss and dimensional collapse in compression-based LLM adaptation?
- **Basis in paper:** [explicit] The authors state that despite promising empirical results, "A deeper theoretical analysis is needed to formally establish the connection between the InfoNCE loss and dimensional collapse."
- **Why unresolved:** The study relies on empirical observations (SVD analysis) to show CTKD and contrastive learning mitigate collapse, but lacks a mathematical framework explaining why these objectives expand effective dimensionality.
- **What evidence would resolve it:** A theoretical proof or framework modeling geometric properties of the embedding space under InfoNCE loss specifically for compressed memory tokens.

### Open Question 2
- **Question:** Why does retrieval performance specifically degrade when contrastive learning continues past the optimal point, unlike other tasks?
- **Basis in paper:** [explicit] Section 5.4 notes that while performance stabilizes for most tasks, "for retrieval tasks, performance begins to decline when contrastive learning continues beyond this point."
- **Why unresolved:** The paper hypothesizes that fixed negative sampling becomes less effective, but does not determine if this is due to overfitting, a shift in the uniformity-alignment trade-off specific to retrieval, or changes in embedding geometry.
- **What evidence would resolve it:** Fine-grained analysis of embedding distribution shifts (alignment vs. uniformity) over extended training steps specifically on retrieval benchmarks.

### Open Question 3
- **Question:** Why is the optimal number of memory tokens for text representation significantly lower than for context compression in generation?
- **Basis in paper:** [inferred] Section 3.3 demonstrates performance drops when memory tokens exceed 8, contrasting with literature where compression for generation uses ~100 tokens.
- **Why unresolved:** It is unclear if the "bottleneck" requirement for effective representation is fundamentally stricter than for generation, or if high token counts introduce noise that damages embedding quality.
- **What evidence would resolve it:** Comparative analysis of information retention (entropy) and token correlation matrices across varying token counts (e.g., 8 vs. 100) for both generation and representation tasks.

## Limitations

- **Architecture Conversion Uncertainty:** The paper references converting Llama-2-7b's causal attention to bidirectional without specifying exact implementation details, creating potential for variation in reproduction.
- **Generalization Beyond MTEB:** While achieving state-of-the-art performance on MTEB, the model's effectiveness on non-standard or domain-specific text representation tasks remains untested.
- **Memory Token Optimization Gap:** The selection of 8 memory tokens is presented as optimal, but the analysis lacks comprehensive sensitivity study across the full range of possible token counts.

## Confidence

- **High Confidence:** CTKD outperforms other compression-based pretext tasks and mitigates dimensional collapse (supported by internal experiments with clear metrics).
- **Medium Confidence:** Achieves state-of-the-art performance across 14 MTEB tasks while requiring less training data (supported by benchmark comparisons).
- **Low Confidence:** CTKD is inherently less prone to dimensional collapse than alternatives (theoretically sound but lacks extensive ablation studies).

## Next Checks

1. **Architectural Fidelity Test:** Reproduce the bidirectional attention conversion for Llama-2-7b and verify CTKD objective functions by comparing singular value distributions between CT-NLL and CTKD models.

2. **Memory Token Sensitivity Analysis:** Conduct comprehensive ablation study varying memory token counts from 1 to 16, measuring downstream MTEB performance across all task categories.

3. **Cross-Domain Generalization Test:** Evaluate the trained LLM2Comp model on a non-MTEB text representation task from a different domain (e.g., biomedical text similarity or code representation) to assess generalization beyond standard benchmarks.