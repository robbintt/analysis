---
ver: rpa2
title: BigO(Bench) -- Can LLMs Generate Code with Controlled Time and Space Complexity?
arxiv_id: '2503.15242'
source_url: https://arxiv.org/abs/2503.15242
tags:
- complexity
- prefix
- suffix
- code
- left
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces BigO(Bench), a novel benchmark designed to
  evaluate large language models' ability to generate code with controlled time and
  space complexity. The benchmark includes 3,105 coding problems and 1,190,250 solutions
  from competitive programming contests, annotated with inferred time and space complexity
  labels using a dynamic complexity inference framework.
---

# BigO(Bench) -- Can LLMs Generate Code with Controlled Time and Space Complexity?

## Quick Facts
- **arXiv ID**: 2503.15242
- **Source URL**: https://arxiv.org/abs/2503.15242
- **Reference count**: 40
- **Primary result**: Benchmark reveals token-space reasoning models struggle with complexity understanding despite excelling at pure code generation, achieving only 4.8% on complexity generation vs 70%+ on pure synthesis.

## Executive Summary
This paper introduces BigO(Bench), a comprehensive benchmark evaluating large language models' ability to generate code with controlled time and space complexity. The benchmark includes 3,105 coding problems and 1,190,250 solutions from competitive programming contests, annotated with inferred complexity labels using a dynamic profiling framework. The authors find that while state-of-the-art reasoning models achieve above 70% accuracy on standard programming tasks, their performance drops dramatically to 4.8% when required to generate code meeting specific complexity constraints. The work highlights a fundamental gap in how LLMs understand computational complexity versus merely generating correct code.

## Method Summary
The benchmark employs a dynamic complexity inference framework that profiles Python functions across scaled inputs to determine time and space complexity classes. Using fuzzing strategies and execution measurement tools, the framework achieves 92% and 84% accuracy on time and space complexity prediction respectively. The evaluation tasks include complexity prediction (classify given solution's complexity), complexity generation (produce code meeting complexity constraints), and coefficient ranking (optimize within complexity classes). The framework handles various data structures and generates Python dataclasses from problem descriptions, achieving 82% coverage across the problem corpus.

## Key Results
- Token-space reasoning models like DeepSeek-R1 Llama 70B achieve 41.4% Pass@1 accuracy on time complexity prediction but only 4.8% on time complexity generation
- Models default to optimal solutions even when explicitly asked for less efficient complexity classes, indicating memorization rather than reasoning
- Space complexity generation is particularly challenging, with reasoning models achieving only 8.1-10.4 All@1 accuracy
- Performance degrades significantly when models must satisfy both correctness and complexity requirements simultaneously

## Why This Works (Mechanism)

### Mechanism 1: Dynamic Complexity Inference via Profiling and Curve Fitting
The framework can infer time and space complexity labels for Python functions by measuring execution metrics across scaled inputs. It uses a rule-based algorithm that generates expanded inputs using various strategies (random, copy, identity), executes code in sandboxed environments while profiling runtime and memory, applies non-negative least squares curve fitting to match measurements to complexity classes, and uses ensemble aggregation to determine the final complexity label with coefficients. The core assumption is that empirical measurements of runtime and memory footprint across varying input sizes reliably correlate with theoretical complexity classes, and worst-case behavior can be triggered through systematic input scaling.

### Mechanism 2: Reasoning Models Generalize Poorly Beyond Training Rewards
Token-space reasoning models excel at pure code generation but perform poorly on complexity understanding tasks because they fail to generalize to objectives not reinforced during training. These models are trained with reinforcement learning rewards based on functional correctness (passing tests), not complexity constraints. Without explicit rewards for complexity understanding during training, they cannot reliably apply their reasoning capabilities to this task despite having seen complexity solutions in their training data.

### Mechanism 3: Multi-Constraint Code Generation Compounds Difficulty
Models experience significant performance degradation because satisfying both functional correctness and complexity constraints simultaneously requires coordinating multiple distinct capabilities. Complexity generation is a multi-objective task where the model must solve the problem correctly, understand the complexity requirement conceptually, and implement code that achieves both. The All@1 metric further reveals that models fail at non-optimal complexity classes because they default to memorized optimal solutions rather than reasoning about why a less-efficient algorithm would satisfy a specific constraint.

## Foundational Learning

- **Time and Space Complexity (Big-O Notation)**: Understanding computational complexity classes (O(1), O(n), O(n log n), O(n²), O(n×m)) for both runtime and memory usage is essential for interpreting the benchmark's tasks and evaluation metrics. Quick check: Given a nested loop where the outer loop runs n times and the inner loop runs log(n) times per outer iteration, what is the time complexity?

- **Empirical Profiling vs. Theoretical Analysis**: The framework uses dynamic profiling (measuring actual runtime/memory across input sizes) rather than static code analysis. Understanding the trade-offs and noise sources in empirical measurement is critical for interpreting the 92%/84% accuracy claims. Quick check: What are two potential sources of noise when measuring Python runtime that could cause the framework to misclassify a function's complexity?

- **Pass@k and All@k Evaluation Metrics**: The benchmark uses specialized metrics—Pass@k measures if any of k samples is correct, Best@k evaluates only optimal complexity classes, while All@k requires correctly handling all complexity classes per problem simultaneously. Understanding these distinctions is essential for interpreting model capabilities. Quick check: Why is All@1 a stricter measure of "reasoning understanding" than Pass@1 for complexity generation tasks?

## Architecture Onboarding

- **Component map**: Complexity Inference Framework (take Python function + input datatypes → generate expanded inputs → execute in sandboxes with profiling → curve fits measurements to complexity classes → return time/space complexity + coefficients) -> Benchmark Dataset (3,105 problems, 1,190,250 solutions with inferred complexity labels) -> Evaluation Tasks (Complexity Prediction, Complexity Generation, Coefficient Ranking) -> Datatype Generation (LLM-generated dataclasses to parse problem inputs)

- **Critical path**: 1. Datatype generation (parse problem inputs into dataclasses) → 2. Complexity inference (run framework on solutions) → 3. Test set filtering (remove unstable predictions, ensure class diversity) → 4. Model evaluation (zero-shot prompting for three tasks) → 5. Metric computation (Pass@k, Best@k, All@k with statistical significance testing)

- **Design tradeoffs**: Accuracy vs. Coverage (92%/84% accuracy but only ~84% of problems with <30% failure rate), Empirical vs. Theoretical (uses noisy profiling measurements rather than formal analysis), Optimal vs. Non-optimal Evaluation (All@1 evaluates understanding across all feasible complexity classes)

- **Failure signatures**: Framework failures (incorrect datatypes, triple-nested lists, edge cases not triggered), Model failures on space prediction (reasoning models "overthink" and misunderstand extra space complexity), Model failures on non-optimal classes (defaulting to optimal solutions)

- **First 3 experiments**: 1. Framework validation: Run complexity inference framework on provided test sets to verify 92%/84% accuracy claims, 2. Baseline model probing: Evaluate Llama 3.1 8B on complexity prediction and generation to establish baseline performance, 3. Fine-tuning ablation: Fine-tune Llama 3.1 70B on training splits for prediction vs. generation tasks separately

## Open Questions the Paper Calls Out

- **Why do token-space reasoning models fail to generalize to complexity constraints despite excelling at pure code generation?**: The paper identifies the generalization gap but does not isolate whether the failure is due to a lack of explicit complexity rewards during RLHF or a fundamental limitation in the model's reasoning architecture. What evidence would resolve it: An ablation study where a reasoning model is fine-tuned with explicit complexity-based rewards, measuring if the generalization gap closes relative to standard code-generation rewards.

- **Can models be trained or prompted to reliably generate correct but sub-optimal solutions when the optimal solution is known?**: The paper highlights this counter-intuitive behavior (humans find sub-optimal solutions easier) but does not determine if it stems from training data bias or a lack of control over output efficiency. What evidence would resolve it: Successful generation of correct solutions across various complexity classes for the same problem after fine-tuning on a balanced dataset of sub-optimal and optimal code.

- **To what extent does the noise in dynamic profiling affect the reliability of reported model performance?**: While the framework's accuracy is validated against human labels, the paper does not quantify how measurement noise impacts the "All@1" metric for borderline complexity classes. What evidence would resolve it: A sensitivity analysis showing the variance in benchmark scores when the profiling environment's noise is perturbed.

## Limitations

- The benchmark relies on empirically inferred complexity labels rather than formal verification, introducing potential noise from Python-specific runtime behaviors
- The 92%/84% accuracy claims depend on the framework's ability to trigger worst-case scenarios, which may not always be reliable
- The benchmark focuses on Python competitive programming problems, limiting generalizability to other programming paradigms or problem domains

## Confidence

- **High confidence**: The benchmark construction methodology (3,105 problems, 1,190,250 solutions) and evaluation framework are well-specified and reproducible
- **Medium confidence**: The 92% time and 84% space complexity inference accuracy, as these depend on empirical profiling that may vary across hardware and execution environments
- **Low confidence**: The claim that token-space reasoning models fail to generalize to complexity tasks due to missing training rewards, as this is primarily based on performance gap observations rather than controlled ablation studies

## Next Checks

1. **Framework validation replication**: Independently run the complexity inference framework on the provided test sets across different hardware environments to verify the 92% time and 84% space accuracy claims
2. **Performance gap causation**: Conduct controlled experiments by fine-tuning reasoning models with complexity-aware rewards to determine if the performance gap in complexity generation tasks is primarily due to missing training signals
3. **Robustness to input scaling**: Test the framework's sensitivity to input scaling strategies by deliberately providing pathological cases where worst-case behavior is not triggered by standard scaling approaches