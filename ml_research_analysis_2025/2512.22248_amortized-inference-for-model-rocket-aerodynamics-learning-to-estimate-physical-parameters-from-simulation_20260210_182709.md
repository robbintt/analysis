---
ver: rpa2
title: 'Amortized Inference for Model Rocket Aerodynamics: Learning to Estimate Physical
  Parameters from Simulation'
arxiv_id: '2512.22248'
source_url: https://arxiv.org/abs/2512.22248
tags:
- data
- inference
- flight
- neural
- network
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work presents a simulation-based amortized inference approach
  for estimating aerodynamic parameters of model rockets using only synthetic flight
  data. A neural network ensemble is trained to invert a physics simulator, predicting
  drag coefficient and thrust correction factor from a single apogee measurement combined
  with motor and configuration features.
---

# Amortized Inference for Model Rocket Aerodynamics: Learning to Estimate Physical Parameters from Simulation

## Quick Facts
- **arXiv ID:** 2512.22248
- **Source URL:** https://arxiv.org/abs/2512.22248
- **Reference count:** 13
- **Key outcome:** Neural network trained on synthetic flight data achieves 12.3 m MAE in apogee prediction on 8 real flights, outperforming OpenRocket baseline by 38% without any real training data

## Executive Summary
This work presents a simulation-based amortized inference approach for estimating aerodynamic parameters of model rockets using only synthetic flight data. A neural network ensemble is trained to invert a physics simulator, predicting drag coefficient and thrust correction factor from a single apogee measurement combined with motor and configuration features. The method achieves 12.3 m mean absolute error in apogee prediction on 8 real flights, outperforming OpenRocket baseline by 38%, all without any real training data. Analysis reveals a systematic positive prediction bias that quantifies the gap between idealized physics and real-world flight conditions.

## Method Summary
The approach trains an ensemble of neural networks to invert a physics simulator's input-output mapping. The simulator models point-mass rocket dynamics with drag and thrust using RK45 integration, generating 10,000 synthetic flights by sampling parameters from priors (Cd ~ U(0.3, 0.9), α ~ U(0.8, 1.2)) and motor configurations. The network takes a 5D feature vector (apogee, motor type, dry mass, total impulse, burn time) and predicts the two physical parameters. Each ensemble member is trained on a bootstrap sample of the synthetic data. At inference time, the method predicts parameters from a single flight's apogee measurement, then forward-simulates to estimate the final altitude.

## Key Results
- Achieves 12.3 m MAE in apogee prediction on 8 real flights
- Outperforms OpenRocket baseline by 38% without any real training data
- Inferred drag coefficients (0.66-0.89) systematically exceed geometric estimates, indicating absorption of unmodeled losses
- All predictions show positive bias of 5-7%, revealing systematic overprediction of apogee

## Why This Works (Mechanism)

### Mechanism 1: Amortized Inversion of Forward Physics
A neural network learns to invert a physics simulator's input-output mapping from synthetic data, enabling single-pass parameter estimation. The network observes diverse simulated flights where both parameters and outcomes are known, learning an approximate inverse function that maps observations back to physical parameters. This works because the simulator captures the essential functional relationship between parameters and apogee sufficiently for the inverse mapping to generalize.

### Mechanism 2: Implicit Regularization Through Constrained Input Space
Combining a single scalar observation (apogee) with configuration features creates a sufficiently constrained problem for useful inference. The 5D input vector provides physics-based constraints through motor characteristics and configuration features. Drag and thrust have distinct temporal signatures in how they affect flight trajectory. The network learns implicit priors from the training distribution that regularize the ill-posed inverse problem.

### Mechanism 3: Effective Parameter Absorption of Unmodeled Effects
The inferred parameters function as "effective" values that absorb unmodeled real-world losses, producing accurate apogee predictions even when individual parameter estimates deviate from true physical values. Effects like rail friction and weathercocking that reduce apogee but aren't in the model get attributed to increased drag coefficient and decreased thrust factor. This yields parameters useful for prediction but not necessarily physically interpretable.

## Foundational Learning

- **Concept: Simulation-Based Inference (SBI)**
  - **Why needed here:** The entire approach rests on training neural networks to perform inference using forward simulators rather than explicit likelihood functions. Understanding SBI clarifies why synthetic data can substitute for real observations.
  - **Quick check question:** Can you explain why traditional maximum likelihood estimation fails when the likelihood function p(observation | parameters) is intractable, and how learning an inverse mapping bypasses this?

- **Concept: Inverse Problems and Identifiability**
  - **Why needed here:** Estimating two parameters from one measurement is fundamentally ill-posed. Understanding identifiability constraints explains why the method works despite theoretical underdetermination.
  - **Quick check question:** Given a single apogee measurement, why might multiple (Cd, α) pairs produce the same predicted altitude? How does the 5D feature vector help?

- **Concept: Neural Network Ensembles for Uncertainty**
  - **Why needed here:** The paper uses ensemble variance as a proxy for epistemic uncertainty. Understanding what ensemble disagreement actually measures vs. calibrated probability is critical for interpretation.
  - **Quick check question:** If all 5 ensemble members predict similar parameters on a synthetic test case but err significantly on a real flight, what type of uncertainty does this reveal a limitation in capturing?

## Architecture Onboarding

- **Component map:**
```
Prior Distributions p(Cd), p(α)
         ↓
    Parameter Sampling
         ↓
    Physics Simulator (RK45) → Synthetic Dataset (10K flights)
         ↓                              ↓
    Feature Extraction            Bootstrap Sampling
         ↓                              ↓
    [h̃, mmotor, mdry, Itotal, tb] → Ensemble (K=5 Networks)
         ↓                              ↓
         └──────────────────→ Parameter Estimates (Ċd, α̂)
                                              ↓
                                    Forward Simulation → Predicted Apogee
```

- **Critical path:** The feature engineering decision (excluding geometric primitives, forcing kinematic inference) directly enables the sim-to-real transfer by making the network learn physics relationships rather than geometry correlations.

- **Design tradeoffs:**
  - **Model complexity vs. identifiability:** Deeper networks could overfit synthetic artifacts; [128, 256, 128] with 0.1 dropout balances capacity with generalization.
  - **Synthetic noise injection:** Adding σ=3m measurement noise during training improves robustness but may blur parameter boundaries.
  - **Ensemble size:** K=5 provides uncertainty estimates without excessive computational cost; paper notes this reflects internal variance on synthetic data, not calibrated real-world confidence.

- **Failure signatures:**
  - **Systematic positive bias:** All predictions overshoot real apogees by 5-7%, indicating unmodeled real-world losses.
  - **Elevated drag estimates:** Ċd ∈ [0.66, 0.89] vs. OpenRocket's 0.52 suggests the network is absorbing unmodeled effects into drag.
  - **Sample size limitation:** N=8 real flights insufficient for statistical generalization.

- **First 3 experiments:**
  1. **Validate synthetic-to-synthetic inversion:** Train on 10K synthetic flights, test on held-out 2K synthetic flights. Target: Cd MAE ≈ 0.088, α MAE ≈ 0.071.
  2. **Ablate feature dimensions:** Train models with reduced feature sets (apogee only, apogee + mass only, full 5D) on synthetic data to quantify how each constraint contributes to identifiability.
  3. **Domain randomization pilot:** Add wind vectors and launch angle variation to synthetic data generation, then evaluate whether systematic positive bias reduces on real flights.

## Open Questions the Paper Calls Out

- **Question 1:** Can incorporating domain randomization techniques, such as random wind vectors and launch angles, during synthetic data generation reduce the systematic positive prediction bias observed in real flights?
  - **Basis:** Authors propose future work to "incorporate domain randomization (e.g., wind vectors, launch angles) during synthetic data generation to improve robustness."
  - **Why unresolved:** Current model assumes vertical trajectories and quiescent air; unmodeled wind-induced drift and weathercocking likely contribute to the observed 5-7% apogee bias.
  - **Evidence needed:** Comparative ablation study showing reduction in mean prediction bias when evaluating against a variant trained on randomized environmental conditions.

- **Question 2:** To what extent does utilizing full altitude-time trajectories improve the identifiability of drag and thrust parameters compared to the current single-apogee approach?
  - **Basis:** Authors propose "utilizing full altitude-time trajectories rather than scalar apogee measurements could improve parameter identifiability."
  - **Why unresolved:** Current formulation solves an underdetermined inverse problem (estimating two physical parameters from one scalar measurement), forcing the network to rely on implicit priors from configuration features.
  - **Evidence needed:** Quantitative comparison of parameter recovery accuracy on a synthetic dataset where ground truth parameters are known, comparing scalar vs. trajectory inputs.

- **Question 3:** Does the observed sim-to-real transfer performance generalize to diverse rocket geometries and stability margins outside the specific 66mm diameter platform tested?
  - **Basis:** Authors note need to "validate the approach on a broader range of rocket configurations to establish more rigorous generalization bounds."
  - **Why unresolved:** Proof-of-concept evaluation is restricted to 8 flights of a single custom rocket, which the authors admit is too small for statistically conclusive generalizations.
  - **Evidence needed:** Evaluation of the pre-trained inference model on flight logs from significantly different rocket designs to verify consistent performance over the OpenRocket baseline.

## Limitations

- **Limited real-world validation:** Only 8 real flights evaluated, insufficient for statistical generalization of performance claims.
- **Unmodeled physics gap:** Systematic positive prediction bias (5-7%) indicates fundamental physics missing from the simulator that the method cannot compensate for.
- **Parameter interpretability:** Inferred parameters are "effective" values absorbing unmodeled losses rather than true physical values, limiting their use for design optimization.

## Confidence

- **High Confidence:** Synthetic data generation and neural network training methodology are well-specified and reproducible.
- **Medium Confidence:** Physics simulator implementation follows standard equations but lacks full specification of motor thrust curves and integration parameters.
- **Low Confidence:** Interpretation of parameter estimates as "effective" values absorbing unmodeled losses is reasonable but not empirically validated.

## Next Checks

1. **Synthetic-to-synthetic validation:** Train and evaluate on a separate synthetic test set (held out from training) to establish baseline inversion performance before applying to real data.
2. **Domain randomization experiment:** Add wind, launch angle variation, and rail friction to synthetic training data, then measure whether systematic positive bias decreases on real flights.
3. **Feature ablation study:** Systematically remove each of the 5 input features to quantify their individual contributions to prediction accuracy and parameter identifiability.