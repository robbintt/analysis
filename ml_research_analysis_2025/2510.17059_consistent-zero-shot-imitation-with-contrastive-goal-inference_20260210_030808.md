---
ver: rpa2
title: Consistent Zero-Shot Imitation with Contrastive Goal Inference
arxiv_id: '2510.17059'
source_url: https://arxiv.org/abs/2510.17059
tags:
- learning
- goal
- policy
- imitation
- reward
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CIRL, a method for zero-shot imitation learning
  that enables agents to infer and reproduce human demonstrations without access to
  expert data during training. The key idea is to reframe imitation learning as a
  goal inference problem, leveraging contrastive reinforcement learning to train agents
  that can explore autonomously and practice reaching self-proposed goals.
---

# Consistent Zero-Shot Imitation with Contrastive Goal Inference

## Quick Facts
- arXiv ID: 2510.17059
- Source URL: https://arxiv.org/abs/2510.17059
- Reference count: 40
- Primary result: Introduces CIRL, achieving up to 100% imitation scores on RL benchmarks by reframing imitation as goal inference and using contrastive RL for consistent, zero-shot behavior reproduction.

## Executive Summary
This paper introduces CIRL, a method for zero-shot imitation learning that enables agents to infer and reproduce human demonstrations without access to expert data during training. The key idea is to reframe imitation learning as a goal inference problem, leveraging contrastive reinforcement learning to train agents that can explore autonomously and practice reaching self-proposed goals. During evaluation, CIRL infers the demonstrator's goal from a single trajectory and uses a learned goal-conditioned policy to reproduce the behavior. Theoretical analysis proves CIRL's consistency, unlike prior methods such as FB representations. Experiments on standard RL benchmarks show CIRL outperforms existing zero-shot imitation approaches, achieving significantly higher imitation scores compared to baselines that use nearest-neighbor policies or FB representations. The method also extends to non-goal-conditioned tasks by expanding the goal space.

## Method Summary
CIRL (Contrastive Inverse Reinforcement Learning) consists of three components: (1) MaxEnt Contrastive RL pretraining with a critic that estimates discounted future state occupancy using noise contrastive estimation, augmented with entropy regularization to satisfy MaxEnt RL assumptions; (2) GoalKDE exploration, which fits a Gaussian KDE to the replay buffer and samples the lowest-probability state as a goal to encourage exploration; and (3) Mean field variational goal inference, which factorizes the posterior over goals as a product of per-timestep distributions q_ξ(g|s,a) and trains it using Forward Amortized Variational Inference (FAVI) loss. The method reframes imitation as inferring the demonstrator's goal and executing a pre-trained goal-conditioned policy. Code is available at https://github.com/kwantlin/contrastive-irl.

## Key Results
- CIRL achieves up to 100% imitation scores on standard RL benchmarks (Reacher, Pusher, Ant) compared to baselines
- Outperforms existing zero-shot imitation approaches including FB representations and nearest-neighbor policies
- Theoretical proof shows CIRL is consistent for MaxEnt optimal experts, while FB is not
- Extends to non-goal-conditioned tasks by expanding the goal space
- GoalKDE exploration shows performance gaps compared to oracle sampling in high-dimensional spaces

## Why This Works (Mechanism)

### Mechanism 1: Goal-Conditioned MaxEnt Contrastive RL Pretraining
The agent autonomously samples goals, practices reaching them, and learns a critic f_ϕ,ψ(s,a,g) = ϕ(s,a)ᵀψ(g) that estimates discounted future state occupancy. This is augmented with entropy regularization to satisfy the MaxEnt RL assumption required for inverse RL consistency. The core assumption is that tasks can be expressed as reaching goal states (observations).

### Mechanism 2: Mean Field Variational Goal Inference
Instead of encoding full trajectories, factorize the posterior as q_ξ(g|τ) = ∏_t q_ξ(g|s_t,a_t). This matches the form of the true MaxEnt posterior and enables training with Forward Amortized Variational Inference (FAVI) loss. The mean field class is proven to include the true posterior, making this approach universally better than inferring goals from the last state or full trajectories.

### Mechanism 3: Consistency via Implicit Partition Function Accounting
CIRL's sampling procedure g ~ p(g), τ ~ p*(τ|g) implicitly includes the partition function Z_g, while FB uses occupancy measures directly as rewards without this correction. This leads FB to conflate visitation frequency with intention, causing incorrect goal inference. The partition function captures average reward across trajectories for goal g, which is crucial for inferring true demonstrator intent.

## Foundational Learning

- **Maximum Entropy Inverse Reinforcement Learning (MaxEnt IRL)**: The entire inference framework assumes expert trajectories maximize reward plus entropy. Understanding this is required to see why the partition function matters. Quick check: Can you explain why MaxEnt IRL requires estimating Z_g, and what goes wrong if you ignore it?

- **Contrastive Learning for RL (CRL)**: CIRL builds on CRL's use of noise contrastive estimation to predict future states, extending it with entropy regularization. Quick check: How does the critic f_ϕ,ψ(s,a,g) relate to discounted state occupancy, and why is contrastive estimation used instead of direct regression?

- **Goal-Conditioned MDPs**: The paper reformulates imitation as inferring which goal g the expert was reaching, then executing a pre-trained goal-conditioned policy. Quick check: Given an expert trajectory, what would go wrong if you inferred the goal as the most-visited state rather than using the variational posterior?

## Architecture Onboarding

- **Component map**: GoalKDE (selects exploration goals) -> Critic (estimates state occupancy) -> Actor (goal-conditioned policy) -> Entropy value function (MaxEnt regularization) -> Variational posterior (goal inference) -> GoalKDE (feedback loop)

- **Critical path**: Pre-training phase: GoalKDE selects low-density goal → agent rolls out → critic/actor/entropy updates → variational posterior updates on sampled (g,τ) pairs. Inference phase: observe demo → q_ξ infers ĝ → execute π(·|s,ĝ).

- **Design tradeoffs**: GoalKDE vs. oracle sampling enables fully self-supervised training but may under-explore high-dimensional goal spaces. Mean field vs. full-trajectory encoder is computationally cheaper and empirically better but assumes per-timestep independence. Online vs. offline pretraining: CIRL requires no expert data, unlike FB which typically assumes offline datasets.

- **Failure signatures**: Low imitation score + high goal inference accuracy → policy failed to reach inferred goals (exploration gap). High imitation score on last-state-goal baseline but low on inferred goal → variational posterior not trained correctly. FB outperforming CIRL → check if goal space captures task-relevant dimensions.

- **First 3 experiments**: (1) Sanity check: Train CIRL with oracle goal sampling on Reacher; confirm imitation score approaches 100%. (2) Ablate goal inference: Compare mean field q_ξ(g|s,a) vs. last-state-as-goal vs. full-trajectory encoder on Pusher; expect mean field to match or exceed alternatives. (3) Stress test exploration: Run GoalKDE vs. oracle on Ant (2D position goal); if gap >20%, investigate whether replay buffer coverage is sufficient or if curriculum strategies are needed.

## Open Questions the Paper Calls Out

- **Can the CIRL framework be effectively extended to handle richer goal representations, such as language or multi-modal spaces?** The paper notes that future work could extend the framework to explore richer goal representations, as the current implementation defines goals as observation vectors in standard benchmarks.

- **How does the performance of the simple GoalKDE exploration method compare to more complex goal-sampling techniques?** The authors implemented a simple form of RIG (GoalKDE) but did not benchmark against the wider array of existing goal-sampling algorithms like adversarial methods (GoalGAN) or distribution methods (Skew-Fit).

- **Can a more efficient combination of goal inference and self-supervised exploration close the performance gap with oracle goal sampling?** The authors note a performance gap between CIRL and the oracle baseline, suggesting that discovering a more efficient combination of exploration and inference would close this gap.

## Limitations
- GoalKDE exploration shows performance gaps compared to oracle sampling in high-dimensional goal spaces
- The paper lacks ablation studies on goal space design and does not report variance across runs
- The FB inconsistency claim is theoretically proven but lacks empirical demonstration showing when and how badly FB fails

## Confidence
- **High confidence** in the core mechanism (contrastive RL + mean field inference) given theoretical grounding and benchmark results
- **Medium confidence** in practical effectiveness due to limited ablation studies and single random seed reporting
- **Low confidence** in scalability claims given GoalKDE limitations and lack of high-dimensional task testing

## Next Checks
1. Run ablation studies varying goal space dimensionality (e.g., full state vs. position-only) to quantify impact on exploration and inference accuracy
2. Measure imitation score variance across 10 random seeds to assess statistical significance of reported improvements
3. Design experiments demonstrating FB's inconsistency by constructing MDPs where FB selects incorrect policies despite near-optimal expert demonstrations