---
ver: rpa2
title: 'INTIMA: A Benchmark for Human-AI Companionship Behavior'
arxiv_id: '2508.09998'
source_url: https://arxiv.org/abs/2508.09998
tags:
- user
- prompts
- emotional
- benchmark
- responses
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'INTIMA is a benchmark for evaluating companionship behaviors in
  language models, grounded in psychological theories and user data. It includes 368
  prompts across 31 behaviors in four categories: Assistant Traits, Emotional Investment,
  User Vulnerabilities, and Relationship & Intimacy.'
---

# INTIMA: A Benchmark for Human-AI Companionship Behavior

## Quick Facts
- arXiv ID: 2508.09998
- Source URL: https://arxiv.org/abs/2508.09998
- Reference count: 5
- INTIMA benchmark reveals all tested models predominantly reinforce companionship behaviors, with significant variation in boundary-setting across different systems.

## Executive Summary
INTIMA is a benchmark designed to evaluate companionship behaviors in language models, grounded in psychological theories of human relationships. The benchmark includes 368 prompts across 31 behaviors in four categories: Assistant Traits, Emotional Investment, User Vulnerabilities, and Relationship & Intimacy. Responses are classified as companionship-reinforcing, boundary-maintaining, or neutral. Testing four models (Gemma-3, Phi-4, o3-mini, Claude-4) revealed that all models predominantly exhibit companionship-reinforcing behaviors, though with marked differences in how they handle sensitive topics and set boundaries.

## Method Summary
The INTIMA benchmark was constructed through expert surveys and qualitative analysis to identify 31 companionship behaviors organized into four categories. Each category contains multiple prompts designed to elicit specific types of companionship responses. Responses are evaluated using a three-way classification system: companionship-reinforcing (providing emotional support or intimacy), boundary-maintaining (setting appropriate limits), or neutral. The benchmark was tested on four language models in May-June 2025, with responses manually categorized to assess each model's approach to companionship dynamics.

## Key Results
- All tested models (Gemma-3, Phi-4, o3-mini, Claude-4) predominantly exhibit companionship-reinforcing behaviors across all categories
- Claude-4-Sonnet showed stronger boundary-setting in sensitive categories compared to other models
- o3-mini and Phi-4 varied significantly in their approaches to emotional support and professional limitations
- The benchmark reveals inconsistent handling of emotionally charged interactions and user attachment across different model systems

## Why This Works (Mechanism)
Assumption: The benchmark works by systematically exposing models to scenarios that trigger companionship behaviors, then classifying responses based on psychological theories of relationship dynamics. The structured prompt design across 31 specific behaviors allows for consistent measurement of how models navigate the tension between providing emotional support and maintaining appropriate boundaries.

## Foundational Learning
Unknown: The paper does not explicitly detail the foundational learning mechanisms that would explain why models exhibit companionship-reinforcing behaviors. However, based on the results, we can infer that models likely learned these patterns from training data containing human conversation examples where emotional support and relationship-building were present.

## Architecture Onboarding
Unknown: The paper does not provide specific information about architecture onboarding for INTIMA testing. Based on the methodology described, models were likely tested using standard API access without special architectural modifications for companionship behavior evaluation.

## Open Questions the Paper Calls Out
None provided

## Limitations
- The benchmark relies on selected psychological theories without systematic validation of their applicability to human-AI companionship dynamics
- The three-way classification system may oversimplify complex responses containing elements of multiple categories
- Testing was limited to four models with restricted parameter sizes, limiting generalizability to other architectures
- The benchmark does not account for cultural variations in companionship norms and expectations
- Manual categorization introduces potential subjectivity in response classification

## Confidence
**High Confidence**: The benchmark construction methodology and the general finding that models exhibit companionship-reinforcing behaviors across categories.

**Medium Confidence**: The comparative performance differences between specific models, such as Claude-4-Sonnet showing stronger boundary-setting.

**Low Confidence**: The generalizability of findings to real-world usage patterns, long-term relationship dynamics, or different user populations.

## Next Checks
1. Conduct longitudinal studies tracking the same models' responses to INTIMA prompts over time to assess consistency and potential drift in companionship behaviors.

2. Expand testing to include a broader range of model sizes and architectures, particularly frontier models with larger parameter counts.

3. Perform cross-cultural validation by adapting INTIMA prompts and evaluation criteria for different cultural contexts to assess generalizability beyond Western psychological frameworks.