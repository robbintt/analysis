---
ver: rpa2
title: How do Transformer Embeddings Represent Compositions? A Functional Analysis
arxiv_id: '2506.00914'
source_url: https://arxiv.org/abs/2506.00914
tags:
- compositionality
- bert
- compound
- embeddings
- semantic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper investigates how transformer-based embedding models\
  \ represent compound words and compositions. It tests six compositionality models\u2014\
  addition, multiplication, dilation, ridge regression, and MLP\u2014on Google, Mistral,\
  \ OpenAI Large, and BERT embeddings using the LADEC dataset of compound words."
---

# How do Transformer Embeddings Represent Compositions? A Functional Analysis

## Quick Facts
- **arXiv ID:** 2506.00914
- **Source URL:** https://arxiv.org/abs/2506.00914
- **Reference count:** 29
- **Primary result:** Ridge regression and simple addition best predict compound embeddings in modern transformer models, with compositionality arising from architecture rather than training data.

## Executive Summary
This paper investigates how transformer-based embedding models represent compound words by testing six compositionality models—addition, multiplication, dilation, ridge regression, and MLP—on embeddings from Google, Mistral, OpenAI Large, and BERT. Using the LADEC dataset of 8,956 compound words, the authors find that ridge regression best predicts compound embeddings, but surprisingly, simple vector addition performs nearly as well. BERT embeddings show significantly lower compositional properties than modern transformer models. The findings generalize to novel compounds and adjective-noun compositions in the SANC dataset, leading to the conclusion that compositionality in modern embeddings arises from architecture rather than training data exposure.

## Method Summary
The study evaluates six composition functions (simple addition, weighted addition, multiplication, dilation, ridge regression, and MLP) that predict compound word embeddings from constituent embeddings. Using the LADEC dataset of 8,956 compound words and their constituents, the authors extract embeddings from four models (Google, Mistral, OpenAI Large, BERT) and compute cosine similarities between predicted and actual compound embeddings. They compare similarity distributions against random baseline distributions using Jensen-Shannon divergence. The analysis extends to novel compounds (LADEC-NC) and adjective-noun compositions (SANC) to test generalizability and architectural compositionality.

## Key Results
- Ridge regression on concatenated constituent embeddings achieves highest JS divergence (0.829-0.831) across modern models
- Simple addition performs at 89% of ridge regression's cosine similarity (0.772 vs 0.868 average) despite having no learned parameters
- BERT embeddings show much lower compositionality (JS divergence 0.289-0.501) than modern models (0.819-0.831)
- Novel compounds (LADEC-NC) show nearly identical predictability to existing compounds, suggesting architectural compositionality
- Multiplication model fails catastrophically with negative cosine similarities

## Why This Works (Mechanism)

### Mechanism 1
Linear models (ridge regression and simple addition) effectively predict compound word embeddings from constituent embeddings in modern transformer models. Ridge regression achieves highest JS divergence (0.829-0.831), while simple addition performs at 89% of ridge regression's cosine similarity (0.772 vs 0.868) despite having no learned parameters. This suggests the embedding space geometry supports additive semantic combination.

### Mechanism 2
Compositionality in modern embedding models arises primarily from architectural properties rather than training data exposure. Novel compounds created from LADEC constituents show nearly identical predictability to existing compounds (cosine sim 0.925 vs 0.914 for Mistral), indicating the models can compose unseen combinations based on architectural principles rather than memorization.

### Mechanism 3
BERT's architectural choices impair compositional representation compared to modern transformer embeddings. BERT shows much lower JS divergence (0.289-0.501) due to bidirectionality creating context-dependent embeddings, MLM objective not encouraging compositional semantics, and WordPiece tokenization fragmenting compounds into subwords.

## Foundational Learning

- **Concept: Compositionality (Fodor & Lepore)**
  - **Why needed here:** The entire paper operationalizes compositionality as predictable combination of constituent meanings. Without this grounding, the functional analysis approach lacks theoretical justification.
  - **Quick check question:** Can you explain why "red ball" is more compositional than "hotdog" based on semantic transparency?

- **Concept: Vector Space Models (VSMs) and distributional semantics**
  - **Why needed here:** The paper extends Mitchell & Lapata composition functions from static to transformer embeddings. Understanding additive, multiplicative, and dilation models requires VSM background.
  - **Quick check question:** How does element-wise multiplication (u ∘ v) differ from weighted addition (αu + βv) in capturing semantic features?

- **Concept: Jensen-Shannon Divergence**
  - **Why needed here:** The primary metric compares predicted-vs-actual cosine similarity distributions against random baseline distributions. JS divergence quantifies how well compositionality models separate from random pairs.
  - **Quick check question:** Why use JS divergence rather than directly comparing mean cosine similarities?

## Architecture Onboarding

- **Component map:** Tokenized compounds (c₁, c₂, c) -> embedding extraction -> composition functions -> cosine similarity evaluation -> JS divergence calculation
- **Critical path:** Extract embeddings for LADEC constituents and compounds -> Apply each composition function to predict compound embeddings -> Compute cosine similarity distributions -> Generate baseline from random compound pairs -> Calculate JS divergence
- **Design tradeoffs:** Ridge regression requires training data split; simple addition requires no training but sacrifices ~11% cosine similarity performance. Multiplication model failed catastrophically, suggesting feature intersection is not how composition works.
- **Failure signatures:** If cosine similarity distribution overlaps heavily with random baseline (JS divergence < 0.3), compositionality is not captured—this is BERT's pattern. If novel compounds show degraded performance vs. LADEC, the model relies on memorized compounds rather than compositional architecture.
- **First 3 experiments:**
  1. Replicate simple addition on new compound dataset: Extract Google/Mistral embeddings for 100 novel two-word compounds not in LADEC, compute u+v, measure cosine similarity to actual compound embedding. Expected: 0.85+ cosine similarity, JS divergence > 0.80.
  2. Test architectural hypothesis with controlled tokenization: Create compounds that tokenize as single tokens vs. multiple tokens across models. If architecture drives compositionality, tokenization strategy should not significantly affect compositional predictability.
  3. Probe the ridge regression weight matrix: Analyze learned weights W for ridge regression (ŵ = W[u; v]). If weights are near-identity for concatenated embeddings, this confirms additive structure; asymmetric weights would indicate modifier-head relationships.

## Open Questions the Paper Calls Out
None

## Limitations
- LADEC dataset represents primarily English noun-noun compounds, limiting generalizability to other compound types and languages
- BERT's poor performance may stem from architectural choices but involves confounding factors of different model scales and training corpora
- MLP's consistent underperformance suggests non-linear models may not capture compositionality, but hyperparameters were not systematically optimized
- Static evaluation assumes fixed embeddings, not accounting for contextual variability in transformer outputs

## Confidence

- **High Confidence:** Linear compositionality models (ridge regression, addition) effectively predict compound embeddings in modern transformer models
- **Medium Confidence:** Compositionality in modern embeddings arises from architecture rather than training data
- **Low Confidence:** BERT's poor compositional performance is primarily due to architectural choices (bidirectional encoding, MLM objective, WordPiece tokenization)

## Next Checks

1. **Cross-linguistic generalization test:** Apply compositionality models to multilingual transformer models (mBERT, XLM-R) on non-English compound datasets to verify architectural compositionality claims generalize beyond English.

2. **Contextual compositionality probe:** Evaluate how compositionality varies across different contexts for the same compound using attention-based probing of transformer layers, distinguishing between static and dynamic compositional properties.

3. **Ablation study on architectural components:** Systematically test modified BERT variants with unidirectional encoding, masked language modeling variants, and different tokenization strategies to isolate which architectural choices most impact compositional representation quality.