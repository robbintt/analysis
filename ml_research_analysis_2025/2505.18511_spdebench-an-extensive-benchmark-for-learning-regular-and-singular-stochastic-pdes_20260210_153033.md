---
ver: rpa2
title: 'SPDEBench: An Extensive Benchmark for Learning Regular and Singular Stochastic
  PDEs'
arxiv_id: '2505.18511'
source_url: https://arxiv.org/abs/2505.18511
tags:
- spdes
- noise
- data
- process
- equation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces SPDEBench, the first comprehensive benchmark
  for learning stochastic partial differential equations (SPDEs) using machine learning.
  The benchmark addresses a critical gap in the field by providing standardized datasets
  for both regular and singular SPDEs, accounting for computational errors from noise
  sampling and renormalization procedures essential for singular equations.
---

# SPDEBench: An Extensive Benchmark for Learning Regular and Singular Stochastic PDEs

## Quick Facts
- arXiv ID: 2505.18511
- Source URL: https://arxiv.org/abs/2505.18511
- Reference count: 40
- One-line primary result: Introduces the first comprehensive benchmark for machine learning stochastic PDEs, providing standardized datasets and models for both regular and singular equations

## Executive Summary
SPDEBench addresses a critical gap in machine learning for scientific computing by providing the first standardized benchmark for learning stochastic partial differential equations. The benchmark includes datasets for both regular SPDEs (Ginzburg-Landau, KdV, wave, incompressible Navier-Stokes) and singular SPDEs (Φ⁴ models), with varying noise truncation degrees and renormalization parameters. Novel machine learning models are proposed, particularly NSPDE-S which incorporates renormalization constants as inputs. The benchmark reveals that renormalization significantly impacts model accuracy for singular SPDEs, and that DLR-Net achieves the highest accuracy and robustness across all tested models. ML models provide substantial inference time improvements over traditional numerical solvers (100-129× speedup).

## Method Summary
SPDEBench provides standardized datasets generated using numerical solvers with varying noise truncation degrees (J ∈ {32, 64, 128, 256}) and renormalization parameters. The benchmark includes five physically significant SPDEs: Ginzburg-Landau, KdV, wave, incompressible Navier-Stokes, and Φ⁴ models. For singular SPDEs, the renormalization process using Wick powers is implemented to handle mathematical ill-posedness. The benchmark provides open-source code with a model zoo including FNO, NSPDE, NSPDE-S, DLR-Net, DeepONet, NCDE, and NRDE. Performance is evaluated using relative L² error across multiple configurations, with datasets available on Hugging Face in Parquet format.

## Key Results
- Renormalization procedures are essential for generating meaningful training data for singular SPDEs, with models requiring these procedures to avoid learning numerical artifacts
- DLR-Net achieves the highest accuracy and robustness across all tested models for singular SPDEs, maintaining consistent performance as singularity increases
- ML models provide substantial inference time improvements over traditional numerical solvers (100-129× speedup) while maintaining competitive accuracy
- Performance degrades predictably as SPDEs become more singular (higher J), but regularity-aware architectures like DLR-Net maintain better accuracy

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Renormalization procedures are necessary for generating meaningful training data for singular SPDEs, as naive numerical schemes produce qualitatively different solutions.
- Mechanism: For singular SPDEs like the dynamical Φ⁴₂ model, the nonlinear term u³ is mathematically undefined because u is a distribution, not a function. The Wick power substitution (u³ → u⋄³ = v³ + 3v²X + 3vX⋄² + X⋄³) converts the ill-posed equation into a well-defined renormalized equation by subtracting divergent constants from regularized solutions.
- Core assumption: The SPDE falls within the subcritical regime where the solution regularity matches that of the stochastic convolution.
- Evidence anchors:
  - [abstract] "new datasets for singular SPDEs based on the renormalization process have been constructed...necessary renormalization required for handling singular SPDEs"
  - [section 3.1.2] Defines Wick powers X⋄² := lim(X²_{J-1} - a_{J-1}) and shows the renormalized equation ∂_tu - Δu + u⋄³ = ξ
  - [corpus] Limited corpus support; related papers (Wiener chaos expansion, neural operators) use different mathematical frameworks

### Mechanism 2
- Claim: Noise truncation degree J directly controls the tradeoff between computational tractability and approximation fidelity, with model performance degrading predictably as solutions become more singular (higher J).
- Mechanism: The cylindrical Wiener process is approximated by W^J(t) = Σ^J_{j=1} ϕ_j β_j(t), where higher J captures more Fourier modes of the noise. As J increases, the noise approximates true space-time white noise more closely, but the solution becomes rougher and harder to learn.
- Core assumption: The truncated approximation converges to the true solution as J → ∞, and the numerical scheme errors are dominated by truncation rather than discretization.
- Evidence anchors:
  - [abstract] "varying noise truncation degrees and renormalization parameters"
  - [section 4.2.1, Table 6] Shows NSPDE error increasing from 0.001 (J=32) to 0.004 (J=256) on Ginzburg-Landau
  - [corpus] Related work on Wiener chaos expansion (arXiv:2411.03384) addresses similar truncation issues

### Mechanism 3
- Claim: Regularity-aware architectures (DLR-Net, NSPDE-S) outperform standard neural operators on singular SPDEs by explicitly incorporating roughness features and renormalization parameters into their design.
- Mechanism: DLR-Net embeds regularity structure features from Hairer's theory into the network, making it robust to solutions that are rough in both space and time. NSPDE-S adds the renormalization constant a_ε as an additional input channel, allowing the network to learn the functional relationship between renormalization parameters and solutions.
- Core assumption: The regularity of solutions can be effectively encoded as learnable features, and renormalization constants provide sufficient information for the network to adapt to varying singularity levels.
- Evidence anchors:
  - [abstract] "DLR-Net achieves the highest accuracy and robustness across all tested models for singular SPDEs"
  - [section 3.2, Table 7] DLR-Net achieves 0.001-0.003 error on Ginzburg-Landau with σ=1 (high noise), significantly outperforming FNO (0.126-0.148)
  - [corpus] Chevyrev et al. (2024) on feature engineering with regularity structures provides theoretical grounding

## Foundational Learning
- **Space-time white noise as distribution-valued random field**
  - Why needed: Understanding that ξ ∈ C^{-2-α} (negative Hölder space) is essential for grasping why singular SPDEs require renormalization—the noise is too irregular for classical nonlinear operations.
  - Quick check question: Why does the paper state that for the Φ⁴₂ model, "the nonlinear term u³ is undefined in classical sense"?

- **Stochastic convolution and the Wick product**
  - Why needed: The renormalization procedure decomposes solutions as u = X + v, where X is the stochastic convolution (∂_tX - ΔX = ξ). Understanding Wick powers (X⋄² = X² - E[X²]) is critical for implementing the data generation pipeline.
  - Quick check question: What is the mathematical difference between the ordinary square X² and the Wick square X⋄², and why does this matter for singular SPDEs?

- **Regularity structures and subcriticality**
  - Why needed: Hairer's regularity structures (Fields Medal work) provides the theoretical foundation for when and why renormalization works. The paper explicitly positions SPDEBench as "among the very first ML implementations" within this framework.
  - Quick check question: The paper mentions that the dynamical Φ⁴₂ model "falls in the subcritical regime." What does this imply about the relationship between solution regularity and noise regularity?

## Architecture Onboarding
- **Component map**: Data generation layer (numerical solvers with finite difference/spectral Galerkin) -> Noise sampling layer (cylindrical/Q-Wiener process with truncation J) -> Model zoo (FNO, NSPDE, NSPDE-S, DLR-Net, DeepONet, NCDE/NRDE) -> Evaluation framework (relative L² error, inference time)
- **Critical path**:
  1. Identify SPDE type: regular (Ginzburg-Landau, KdV, Wave, Navier-Stokes) vs. singular (Φ⁴₂)
  2. For singular SPDEs: Must use renormalization procedure (Wick powers) in data generation
  3. Configure noise: Select truncation degree J, noise type (cylindrical vs. Q-Wiener), scale parameter σ
  4. Generate training/validation/test splits (70%/15%/15%) ensuring consistent numerical schemes
  5. Select architecture: DLR-Net for singular/high-noise regimes, NSPDE-S when renormalization parameters vary
  6. Evaluate on matched test data (same J, same numerical scheme) to avoid misleading conclusions

- **Design tradeoffs**:
  - **High J (256) vs. Low J (32)**: Higher J better approximates true white noise but produces rougher solutions that are harder to learn; Table 6 shows NSPDE error increases from 0.001 to 0.004
  - **Renormalization vs. explicit schemes**: Renormalization produces cleaner spatial patterns (Figure 1) but requires implementing Wick powers; explicit schemes are simpler but yield noisy, less learnable data
  - **DLR-Net vs. NSPDE-S**: DLR-Net achieves best accuracy (0.001-0.003 error) but requires equation-specific regularity feature layers; NSPDE-S is more flexible (single architecture handles varying a_ε) but slightly lower performance
  - **Training cost vs. inference speed**: ML models provide 100-129× speedup over numerical solvers (Table 5) but require upfront training on high-quality generated data

- **Failure signatures**:
  - **Scheme mismatch**: Training on data with one truncation degree J₁ and testing on J₂ → inflated error (Table 3: training on D^{re}_2, testing on D^{re}_{128} gives 4.087 error vs. 0.010 on matched data)
  - **Missing renormalization**: Applying standard architectures to singular SPDE data generated without renormalization → models learn artifacts of numerical noise rather than true dynamics
  - **Regularity mismatch**: Using non-regularity-aware models (FNO, DeepONet) on highly singular data → error increases with singularity level (Table 7: FNO degrades from 0.132 to 0.148 as J increases, while DLR-Net stays at 0.001-0.003)
  - **Noise type confusion**: Training on cylindrical Wiener noise but testing on Q-Wiener → systematic bias (Figure 2 shows different error profiles)

- **First 3 experiments**:
  1. **Establish baseline on regular SPDE**: Train FNO, NSPDE, and DLR-Net on Ginzburg-Landau (σ=0.1, J=128) with fixed initial condition. Measure relative L² error and inference time. Goal: Verify implementation correctness and establish performance baseline for regular SPDEs.
  2. **Quantify renormalization impact**: Train NSPDE on Φ⁴₂ data generated with and without renormalization (D^{re}_{128} vs. D^{ex}_{128}). Evaluate both on D^{re}_{128} test set. Goal: Reproduce the finding that renormalization is essential—expect ~10× error reduction per Table 3 vs. Table 4.
  3. **Test architecture robustness to singularity**: Train DLR-Net and FNO on Ginzburg-Landau with high noise (σ=1) across all truncation degrees J ∈ {32, 64, 128, 256}. Plot error vs. J. Goal: Verify DLR-Net's robustness claim—expect DLR-Net error to remain flat while FNO degrades with increasing J.

## Open Questions the Paper Calls Out
- **How do the benchmarked ML models perform on hyperbolic SPDEs compared to the current parabolic and dispersive datasets?**
  - Basis in paper: [Explicit] The "Limitations" section states, "Our dataset currently emphasizes certain equations... for example, we did not include the hyperbolic SPDEs."
  - Why unresolved: The current benchmark is restricted to specific equation types (e.g., Ginzburg-Landau, KdV, $\Phi^4$), leaving the generalization to hyperbolic systems untested.
  - What evidence would resolve it: Benchmark results from extending SPDEBench to include stochastic hyperbolic equations, comparing solver speedup and relative $L^2$ errors against existing baselines.

- **Can a universal regularity feature layer be designed for DLR-Net that generalizes across diverse SPDEs without manual derivation?**
  - Basis in paper: [Explicit] Supplementary Section D notes that for DLR-Net, "it requires to develop the regularity feature layer for different SPDEs which is beyond the scope of this paper."
  - Why unresolved: The current implementation relies on equation-specific feature engineering, limiting the plug-and-play applicability of the most robust model.
  - What evidence would resolve it: A single DLR-Net architecture achieving high accuracy across multiple distinct SPDEs (e.g., KdV and Navier-Stokes) without equation-specific manual layer adjustments.

- **To what extent does the noise approximation method (cylindrical vs. Q-Wiener) bias the selection of optimal ML architectures?**
  - Basis in paper: [Inferred] The paper notes (Section 4.2.2) that "different noise generation methods also impact the learning performance," but a comprehensive theoretical explanation for this bias is not provided.
  - Why unresolved: While empirical differences in error rates are shown (Fig 2), the underlying mechanism causing certain models (like FNO) to react differently to noise types remains unclear.
  - What evidence would resolve it: A theoretical analysis or ablation study isolating the spectral properties of the noise and their interaction with the spectral layers of the neural operators.

## Limitations
- The benchmark is limited to specific equation types and does not include hyperbolic SPDEs, restricting generalization to other important physical systems
- The numerical schemes used for data generation may introduce discretization errors that could affect model generalization
- Training procedures rely on grid search hyperparameters not fully specified in the main text, requiring users to consult external documentation

## Confidence
- **High confidence**: The necessity of renormalization for singular SPDEs, the speedup claims for ML models vs. numerical solvers (100-129× improvement)
- **Medium confidence**: The superiority of DLR-Net for singular SPDEs across all tested configurations, the degradation patterns as singularity increases
- **Low confidence**: The claim about single architecture flexibility for varying renormalization parameters without architecture adaptation

## Next Checks
1. Test model performance when training and test data use different truncation degrees J to verify the scheme consistency requirement
2. Evaluate DLR-Net on a singular SPDE not included in the benchmark to test architecture generalization beyond the provided datasets
3. Compare inference time speedup claims on different hardware platforms to verify the 100-129× improvement is consistent across computing environments