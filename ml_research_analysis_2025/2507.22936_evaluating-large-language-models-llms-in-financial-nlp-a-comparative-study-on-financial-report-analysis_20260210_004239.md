---
ver: rpa2
title: 'Evaluating Large Language Models (LLMs) in Financial NLP: A Comparative Study
  on Financial Report Analysis'
arxiv_id: '2507.22936'
source_url: https://arxiv.org/abs/2507.22936
tags:
- financial
- language
- llms
- evaluation
- similarity
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluated five large language models (GPT-4, Claude,
  Gemini, Perplexity, DeepSeek) on question answering over the Business sections of
  10-K filings from major U.S. tech firms.
---

# Evaluating Large Language Models (LLMs) in Financial NLP: A Comparative Study on Financial Report Analysis

## Quick Facts
- arXiv ID: 2507.22936
- Source URL: https://arxiv.org/abs/2507.22936
- Authors: Md Talha Mohsin
- Reference count: 40
- Primary result: No single model dominated all evaluation perspectives in financial report analysis

## Executive Summary
This study evaluated five large language models (GPT-4, Claude, Gemini, Perplexity, DeepSeek) on question answering over the Business sections of 10-K filings from major U.S. tech firms. A multi-faceted evaluation combined human ratings on relevance, completeness, clarity, conciseness, and factual accuracy; automated lexical/semantic similarity metrics; and behavioral diagnostics on stability and cross-model agreement. Human assessments revealed model-specific trade-offs, with modest inter-rater agreement indicating subjective evaluation criteria. Automated metrics showed that Gemini led in lexical overlap, while Claude excelled in semantic similarity. Behavioral diagnostics indicated varying consistency across prompts and firms.

## Method Summary
The study employed a comprehensive evaluation framework combining human assessments and automated metrics to evaluate LLM performance on financial report analysis. Five models were tested on Business sections from 10-K filings of major U.S. tech firms. Human evaluators rated responses across five dimensions: relevance, completeness, clarity, conciseness, and factual accuracy. Automated evaluation included lexical overlap metrics and semantic similarity measures. Behavioral diagnostics examined model stability across different prompts and firms, as well as cross-model agreement patterns.

## Key Results
- No single model dominated across all evaluation dimensions
- Gemini led in lexical overlap metrics while Claude excelled in semantic similarity
- Modest inter-rater agreement among human evaluators indicated subjective evaluation criteria
- Models showed varying consistency across prompts and firms

## Why This Works (Mechanism)
The multi-faceted evaluation approach effectively captures different aspects of LLM performance in financial NLP tasks. By combining human judgment with automated metrics and behavioral diagnostics, the study provides a comprehensive assessment that reveals both strengths and limitations of each model. The use of real 10-K filings from major tech firms ensures practical relevance to actual financial analysis scenarios.

## Foundational Learning
- **Financial NLP domain knowledge**: Understanding of financial terminology and reporting standards is essential for evaluating model performance in this context. Quick check: Can evaluators distinguish between different financial metrics and their implications?
- **Multi-dimensional evaluation frameworks**: Combining human and automated assessment methods provides more robust insights than single-method approaches. Quick check: Are all evaluation dimensions clearly defined and consistently applied?
- **Model behavior diagnostics**: Analyzing stability and cross-model agreement reveals important characteristics about model reliability and consistency. Quick check: Do models show consistent performance across different prompt variations?

## Architecture Onboarding
**Component map**: Human evaluation pipeline -> Automated metrics computation -> Behavioral diagnostics analysis -> Model comparison synthesis

**Critical path**: Financial report preprocessing -> Question formulation -> LLM response generation -> Multi-dimensional evaluation -> Result aggregation

**Design tradeoffs**: The study balanced comprehensiveness with practicality by using a representative sample of major tech firms rather than exhaustive coverage across all industries. This choice prioritized depth of analysis over breadth of representation.

**Failure signatures**: Modest inter-rater agreement suggests subjective elements in evaluation criteria. Models showing inconsistent performance across prompts indicate potential instability in handling variations in query formulation.

**First experiments**:
1. Test model performance on financial reports from different industries
2. Evaluate response quality across multiple reporting periods
3. Compare human evaluation results with domain expert financial analysts

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation sample limited to major U.S. tech firms, potentially not representative of broader financial reporting practices
- Human evaluation showed only modest inter-rater agreement, suggesting subjective assessment criteria
- Automated metrics may not fully capture nuanced understanding required for complex financial analysis

## Confidence
- **High confidence**: No single model dominated across all evaluation dimensions is a robust finding supported by multi-method approach
- **Medium confidence**: Relative rankings in specific metrics may vary with different datasets or evaluation criteria
- **Medium confidence**: Model-specific trade-offs in human assessments are credible but influenced by subjective evaluation criteria

## Next Checks
1. Expand evaluation to include 10-K filings from diverse industries and company sizes to assess generalizability
2. Conduct follow-up study with larger, more diverse group of human evaluators to refine assessment criteria
3. Implement longitudinal study to evaluate model performance over multiple reporting periods