---
ver: rpa2
title: Accelerating Large Language Model Inference via Early-Exiting Algorithms
arxiv_id: '2509.05915'
source_url: https://arxiv.org/abs/2509.05915
tags:
- recursive
- performance
- tokens
- layer
- layers
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This dissertation tackles the high computational cost of deploying
  large language models (LLMs) in real-world scenarios. The core challenge addressed
  is the inference inefficiency of LLMs stemming from their massive computational
  requirements.
---

# Accelerating Large Language Model Inference via Early-Exiting Algorithms

## Quick Facts
- arXiv ID: 2509.05915
- Source URL: https://arxiv.org/abs/2509.05915
- Reference count: 0
- One-line primary result: This dissertation introduces FREE, Recursive Transformers, and MoR to accelerate LLM inference, achieving up to 2.16× speedup and establishing a new Pareto frontier in efficiency.

## Executive Summary
This dissertation tackles the high computational cost of deploying large language models (LLMs) in real-world scenarios by proposing adaptive algorithms and model architectures that balance dynamism and efficiency. The work introduces three novel frameworks: FREE addresses missing KV caches in early-exiting systems through synchronized parallel decoding; Recursive Transformers use parameter sharing to enable continuous depth-wise batching and mitigate synchronization bottlenecks; and MoR dynamically assigns optimal recursion depths via lightweight routers with recursion-wise caching. These methodologies significantly improve inference efficiency while preserving accuracy, providing a foundational contribution to sustainable AGI development.

## Method Summary
The dissertation presents three complementary approaches to accelerate LLM inference. FREE uses a shallow-deep module with synchronized parallel decoding to handle missing KV caches in early-exiting systems, achieving up to 2.16× speedup. Recursive Transformers convert standard models to shared-weight architectures enabling continuous depth-wise batching, with LoRA relaxation to recover capacity loss. MoR pretrains models with middle-cycle parameter sharing and expert-choice routers that dynamically assign recursion depths, using recursion-wise KV caching to reduce memory usage. The frameworks are validated across multiple tasks including summarization, QA, and translation using models from 135M to 1.7B parameters.

## Key Results
- FREE achieves up to 2.16× speedup on summarization and QA tasks while preserving accuracy
- Recursive Transformers theoretically enable 2-3× throughput gains through continuous depth-wise batching
- MoR establishes a new Pareto frontier, achieving lower perplexity and higher throughput compared to both vanilla and recursive baselines while using significantly fewer parameters

## Why This Works (Mechanism)

### Mechanism 1
Early-exiting frameworks can accelerate inference without performance degradation if missing Key-Value (KV) caches are handled via efficient parallel decoding rather than state copying. The FREE framework uses a shallow-deep module where early-exited tokens have their missing KV states computed synchronously in parallel with the forward pass of the next non-exiting token. This leverages the memory-bound nature of Transformer decoding to hide the computation cost of KV states.

### Mechanism 2
Parameter sharing (recursion) in Transformers allows tokens at different processing depths to be batched together, resolving synchronization bottlenecks in early-exiting systems. Recursive Transformers tie weights across layers, enabling continuous depth-wise batching where tokens exiting early can be replaced by new tokens starting at the same layer while other tokens proceed to deeper recursion loops.

### Mechanism 3
A unified framework using learned routers to assign dynamic recursion depths, combined with selective KV caching, maximizes efficiency and establishes a Pareto frontier. MoR trains lightweight routers to determine optimal recursion depth per token and implements recursion-wise KV caching, storing KV pairs only for tokens selected at each recursion step rather than for all tokens at all depths.

## Foundational Learning

**Autoregressive Decoding & KV Caching**
- Why needed here: The entire dissertation hinges on optimizing the sequential generation process and handling missing KV caches
- Quick check question: If a token exits at layer 6 of a 12-layer model, why does the future token generation fail or degrade if the KV cache is not handled? (Answer: Future attention relies on those missing KV vectors)

**Batching Efficiency (Synchronization)**
- Why needed here: The core system bottleneck identified is that standard batching forces all samples to wait for the longest sample
- Quick check question: Why can't you batch a token at layer 5 and a token at layer 10 in a standard Transformer? (Answer: They require different weight matrices and operations)

**Parameter Sharing & Capacity**
- Why needed here: Recursive Transformers reduce parameters by reusing layers, creating a trade-off between memory/throughput savings and potential capacity loss
- Quick check question: If you repeat a 4-layer block 3 times to make a 12-layer model, what is the ratio of unique non-embedding parameters compared to a standard 12-layer model? (Answer: Approximately 1/3, plus relaxation parameters like LoRA)

## Architecture Onboarding

**Component map:**
- Input tokens -> Recursive Block (shared weights) -> Router (MoR) or confidence check (FREE) -> Exiting Tokens (parallel KV computation or finalize) / Continuing Tokens (next recursion) -> Batch Scheduler (Continuous Depth-wise Batching)

**Critical path:**
1. Input tokens enter the Recursive Block (shared weights)
2. A Router (in MoR) or confidence check (in FREE) evaluates if the token should exit
3. Exiting Tokens: If exiting, their missing KV states are either computed in parallel (FREE) or finalized based on the current recursion step's cache (MoR)
4. Continuing Tokens: Proceed to the next recursion loop (reusing the same weights)
5. Batch Scheduler: Rearranges the batch (Continuous Depth-wise Batching) to fill slots vacated by exited tokens immediately

**Design tradeoffs:**
- FREE vs. MoR: FREE solves the cache issue for standard layers but doesn't solve batching stalls. MoR solves both but requires more complex router training
- Caching Strategy: Recursion-wise caching saves massive memory but limits attention to tokens in the same recursion step. Recursive sharing maintains full context but has higher IO
- Router Choice: Expert-choice guarantees static compute budget but violates causality during training. Token-choice is causal but suffers load imbalance

**Failure signatures:**
- Perplexity Explosion in Recursive Models: Usually indicates poor initialization of looped layers or excessive weight tying without LoRA relaxation
- Throughput Collapse: If the router in MoR becomes "greedy" and sends all tokens to the deepest recursion, depth-wise batching benefits vanish
- Missing Cache Hallucinations: If parallel decoding in FREE fails to generate accurate KV states for exited tokens, future generation quality drops

**First 3 experiments:**
1. Verify Synchronized Decoding Overhead: Measure latency with/without parallel KV computation for exited tokens in a standard model
2. Capacity vs. Efficiency Curve: Plot validation loss vs. unique parameter count for Recursive Transformers with varying LoRA ranks
3. Router Stability Analysis: Train the MoR router and plot the "dead token ratio" to ensure the auxiliary loss is functioning correctly

## Open Questions the Paper Calls Out

**Open Question 1**
Can Mixture-of-Recursions (MoR) and Relaxed Recursive Transformers maintain their efficiency-performance trade-offs when scaled to models with billions of parameters (e.g., >3B) via continued pretraining? The experiments have been limited to models with up to 1.7 billion parameters due to compute constraints. The natural next step is to train MoR models at larger scales.

**Open Question 2**
Can the MoR router dynamically learn to adjust recursion depth based on the complexity of reasoning tasks (e.g., chain-of-thought necessity) when post-trained on specific reasoning datasets? Exploring how the router can dynamically learn to adjust to the necessity of chain-of-thought chains when post-trained on actual reasoning datasets.

**Open Question 3**
How can the inference capacity (top-k selection) be dynamically adjusted in MoR models that utilize auxiliary loss for expert-choice routing? When using an auxiliary loss, the router outputs for selected and unselected tokens are almost perfectly separated, making it challenging to adjust top-k values after training.

## Limitations
- Scalability uncertainty to extremely large models (beyond 7B parameters) where architectural innovations like MoE already provide efficiency gains
- Reliance on simulation for throughput measurements in Recursive Transformers rather than actual hardware deployment validation
- Router-based approach introduces potential failure modes where overly conservative or aggressive routing can collapse efficiency benefits

## Confidence
**High Confidence (9/10):** The core mechanism of FREE - handling missing KV caches through parallel computation - is well-established and directly addresses a known bottleneck in early-exiting systems with clear experimental validation.

**Medium Confidence (7/10):** The Recursive Transformer approach and its throughput benefits rely on theoretical analysis and simulated measurements. While the architectural innovations are sound, actual deployment benefits would need real-world validation.

**Medium Confidence (7/10):** The MoR framework's unified approach and Pareto frontier establishment is compelling, but the complexity of router training and interaction between routing decisions and task performance introduce uncertainty.

## Next Checks
1. **Hardware Deployment Validation:** Implement the Recursive Transformer architecture on actual GPU/TPU hardware to verify simulated throughput gains (2-3×) and measure real-world latency improvements across varying batch sizes and sequence lengths.

2. **Router Stability Across Domains:** Conduct extensive testing of the MoR router's decision-making across at least 10 diverse tasks and domains, measuring router stability metrics (dead token ratio, load balance) and tracking performance degradation over extended training periods.

3. **Scalability Benchmark:** Scale the Recursive Transformer and MoR frameworks to 30B+ parameter models and measure the diminishing returns on efficiency gains as model size increases, comparing against existing efficiency techniques like quantization and speculative decoding.