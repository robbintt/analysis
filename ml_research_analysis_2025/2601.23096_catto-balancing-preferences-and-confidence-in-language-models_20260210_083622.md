---
ver: rpa2
title: 'CATTO: Balancing Preferences and Confidence in Language Models'
arxiv_id: '2601.23096'
source_url: https://arxiv.org/abs/2601.23096
tags:
- confidence
- calibration
- catto
- loss
- surrogate
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "CATTO addresses miscalibration in language models by embedding\
  \ a differentiable per-token calibration objective into preference optimization,\
  \ aligning predicted confidence with empirical correctness while preserving preference\
  \ ordering. It reduces Expected Calibration Error (ECE) by 2.22%\u20137.61% in-distribution\
  \ and 1.46%\u201310.44% out-of-distribution versus DPO, and maintains or slightly\
  \ improves accuracy on five datasets."
---

# CATTO: Balancing Preferences and Confidence in Language Models

## Quick Facts
- **arXiv ID:** 2601.23096
- **Source URL:** https://arxiv.org/abs/2601.23096
- **Reference count:** 40
- **Key outcome:** CATTO reduces ECE by 2.22%-7.61% in-distribution and 1.46%-10.44% out-of-distribution versus DPO while maintaining accuracy

## Executive Summary
CATTO addresses miscalibration in language models trained via preference optimization by embedding a differentiable per-token calibration objective into the learning process. Standard methods like DPO improve preference alignment but worsen calibration, as they optimize relative probabilities without constraining absolute confidence. CATTO introduces a joint objective combining DPO's preference loss with a calibration loss that minimizes the L1 surrogate of Expected Calibration Error (ECE), thereby aligning predicted confidence with empirical correctness while preserving preference orderings.

## Method Summary
CATTO augments Direct Preference Optimization (DPO) with a per-token calibration objective. For each token, it computes a correctness surrogate based on the probability margin between the ground-truth token and the highest competitor, passed through a sigmoid. The calibration loss is the L1 distance between this surrogate and the predicted confidence (max probability). These are combined with DPO loss via a weighted sum, where the weight λ is chosen to improve calibration without flipping preference orderings.

## Key Results
- ECE reduction: 2.22%-7.61% in-distribution and 1.46%-10.44% out-of-distribution versus DPO
- Accuracy maintained or slightly improved across five datasets
- Introduces Confidence@k, a test-time scaling rule for Bayes-optimal candidate selection

## Why This Works (Mechanism)

### Mechanism 1: L1 Calibration Risk as ECE Surrogate
The population L1 calibration risk serves as a differentiable upper bound on ECE, enabling gradient-based optimization. CATTO minimizes RCal(θ) = E[|cθ(xt) - Zt|] rather than directly optimizing ECE. By Jensen's inequality, this population risk upper-bounds ECE (ECE(θ) ≤ RCal(θ)), allowing stochastic gradient descent on an unbiased estimator of the bound while still improving the target metric.

### Mechanism 2: Probability Margin as Correctness Proxy
The signed margin between the ground-truth token probability and the highest competitor probability, passed through a sigmoid, provides a differentiable surrogate for binary correctness. The surrogate ez(xt) = σ(py*(xt) - pȳ(xt)) maps probability differences to [0,1]. When the model strongly favors the correct token, ez → 1; when uncertain or incorrect, ez → 0.5 or below. This enables gradient flow through the calibration loss without requiring ground-truth correctness labels during training.

### Mechanism 3: Joint Preference-Calibration Optimization
Linearly combining DPO loss with per-token calibration loss improves calibration while preserving learned preference orderings, provided the calibration weight is appropriately bounded. The joint objective Ltotal = LDPO + λ(LCal(y+) + LCal(y-)) adds calibration gradients that shift absolute probabilities while DPO gradients maintain relative preferences. Proposition A.3 shows that when λ < 2Δmin/|y| and calibration gradients are bounded by 1/4, preference margins cannot flip.

## Foundational Learning

- **Concept: Expected Calibration Error (ECE)**
  - Why needed: CATTO explicitly targets ECE reduction as its primary metric
  - Quick check: Given a model that predicts with 90% confidence on 100 samples and is correct on 72 of them, what is the ECE contribution from this confidence bin?

- **Concept: Direct Preference Optimization (DPO)**
  - Why needed: CATTO is implemented as an augmentation to DPO
  - Quick check: Why does DPO's objective allow logit magnitudes to grow unconstrained even when preference orderings are satisfied?

- **Concept: Surrogate Loss Functions**
  - Why needed: CATTO uses a correctness surrogate rather than binary correctness
  - Quick check: Why can't we directly use binary correctness Z ∈ {0,1} as the target for a gradient-based calibration loss?

## Architecture Onboarding

- **Component map:** Input tokens → LLM backbone → Per-token probability distribution → Confidence extraction (max probability) + Probability margin computation (correct vs. highest incorrect) → Calibration loss (L1 surrogate) + DPO loss (preference pairs) → Joint optimization with weight λ

- **Critical path:** Computing the probability margin requires identifying both the ground-truth token y* and the highest-probability incorrect token ȳ at each position. During training, y* is known from supervision; ȳ requires a top-k query over non-ground-truth tokens.

- **Design tradeoffs:**
  - λ selection: Higher λ improves calibration but risks preference inversion; paper uses λ = 0.1
  - L1 vs. L2 calibration loss: Paper argues L1 is more robust to surrogate noise (bounded gradients); L2 would amplify large surrogate errors
  - Token-level vs. sequence-level calibration: Paper operates token-level; extending to open-ended generation may require sequence-level aggregation

- **Failure signatures:**
  - Calibration improves but accuracy drops substantially (>2-3%): λ may be too high, overwhelming preference signals
  - ECE improves on validation but degrades on test: Surrogate may be overfitting to training distribution's probability structure
  - Preference pairs show inverted rankings: Calibration gradients exceeded bounds (check Proposition A.3 conditions)

- **First 3 experiments:**
  1. Baseline replication: Train DPO on a small MCQ dataset, measure ECE and accuracy; confirm DPO increases ECE vs. SFT
  2. Ablation on λ: Sweep λ ∈ {0.01, 0.05, 0.1, 0.2, 0.5} and plot ECE vs. accuracy tradeoff curve
  3. Out-of-distribution test: Train on one domain (e.g., MMLU), evaluate calibration on another (e.g., CommonsenseQA); verify CATTO's OOD generalization vs. RCFT post-hoc calibration

## Open Questions the Paper Calls Out
- Can CATTO be effectively adapted for open-ended text generation and multimodal settings?
- Does CATTO maintain calibration benefits during iterative or online alignment processes?
- How reliable is the token-level confidence proxy when sequence-level correctness is not decomposable?

## Limitations
- The probability margin surrogate may not generalize well to domains with high semantic ambiguity or length bias
- Proposition A.3 assumes calibration gradients remain bounded by 1/4 during training, which isn't verified empirically
- The method operates at token-level, but real-world applications often require sequence-level calibration

## Confidence
**High Confidence Claims:**
- CATTO reduces ECE in-distribution (2.22%-7.61%) and out-of-distribution (1.46%-10.44%) versus DPO
- CATTO maintains or slightly improves accuracy on five datasets
- The L1 calibration risk provides an upper bound on ECE via Jensen's inequality

**Medium Confidence Claims:**
- Linear combination of DPO and calibration loss preserves preference orderings (conditional on Proposition A.3 conditions being met)
- Probability margin surrogate provides directional consistency with correctness
- L1 loss is more robust to surrogate noise than L2 alternatives

**Low Confidence Claims:**
- The specific λ = 0.1 value is optimal across all domains
- Token-level calibration generalizes to sequence-level tasks without modification
- No degradation occurs in long-context scenarios

## Next Checks
1. **Gradient Magnitude Monitoring** - During CATTO training, track the maximum absolute value of calibration gradients across all tokens and layers. Plot these values against the theoretical bound of 1/4 to verify Proposition A.3 conditions are maintained throughout training.

2. **Domain Transfer Analysis** - Train CATTO on one domain (e.g., technical QA), then evaluate calibration degradation on semantically distant domains (e.g., creative writing). Measure how ECE changes relative to both DPO and RCFT to quantify domain generalization limits.

3. **Sequence-Level Calibration Benchmark** - Implement a sequence-level calibration metric (e.g., aggregate token confidences using geometric mean or other pooling) and evaluate CATTO on open-ended generation tasks. Compare against token-level ECE to identify any disconnect between token and sequence calibration performance.