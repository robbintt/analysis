---
ver: rpa2
title: Supporting Humans in Evaluating AI Summaries of Legal Depositions
arxiv_id: '2601.15182'
source_url: https://arxiv.org/abs/2601.15182
tags:
- legal
- summary
- summaries
- nuggets
- nugget
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the problem of supporting legal professionals
  in evaluating AI-generated summaries of legal depositions, which are crucial but
  often lengthy and complex documents. The authors propose a nugget-based approach
  that extracts key facts from depositions and uses these nuggets to guide the evaluation
  process.
---

# Supporting Humans in Evaluating AI Summaries of Legal Depositions

## Quick Facts
- arXiv ID: 2601.15182
- Source URL: https://arxiv.org/abs/2601.15182
- Authors: Naghmeh Farzi; Laura Dietz; Dave D. Lewis
- Reference count: 27
- Primary result: Novel nugget-based approach for evaluating AI-generated legal deposition summaries

## Executive Summary
This paper addresses the challenge of supporting legal professionals in evaluating AI-generated summaries of legal depositions. Legal depositions are critical but often lengthy and complex documents that form the foundation of legal cases. The authors propose a nugget-based approach that extracts key facts from depositions and uses these nuggets to guide the evaluation process. This methodology aims to reduce the cognitive load on legal professionals while ensuring rigorous verification in high-stakes legal tasks.

The research focuses on two primary evaluation scenarios: determining which of two AI summaries is better and manually improving an AI-generated summary. The prototype leverages automated nugget extraction and alignment, citation verification, and highlights differences and potential improvements in summaries. This approach directly addresses the challenges of cognitive load and the need for rigorous verification in legal AI applications.

## Method Summary
The paper presents a nugget-based approach for supporting legal professionals in evaluating AI-generated summaries of legal depositions. The method involves extracting key facts (nuggets) from depositions and using these as reference points for evaluation. Two primary scenarios are explored: comparative evaluation of summaries and manual improvement of AI-generated summaries. The prototype system implements automated nugget extraction, alignment between nuggets and summary content, citation verification, and visual highlighting of differences and improvement opportunities. The approach is designed to reduce cognitive load while maintaining the rigor required for legal document verification.

## Key Results
- Proposes a nugget-based methodology for evaluating AI-generated legal deposition summaries
- Develops a prototype system supporting two evaluation scenarios: comparison and improvement
- Demonstrates how automated nugget extraction and alignment can guide human evaluation
- Addresses cognitive load challenges in legal document verification
- Shows potential for enhancing efficiency and accuracy in legal AI evaluation

## Why This Works (Mechanism)
The nugget-based approach works by breaking down complex legal depositions into discrete, verifiable facts that can be systematically compared against AI-generated summaries. This modular approach allows legal professionals to focus on specific factual elements rather than attempting to evaluate entire documents holistically. By automating the extraction and alignment of these nuggets, the system reduces the manual effort required while maintaining human oversight for verification. The dual-scenario framework (comparison and improvement) provides flexibility for different evaluation needs while maintaining consistency in the underlying nugget-based methodology.

## Foundational Learning
- **Legal deposition structure**: Understanding the format and content of legal depositions is essential for designing effective evaluation tools
  - Why needed: To ensure the system captures the relevant factual elements
  - Quick check: Verify that extracted nuggets align with deposition sections

- **Nugget extraction algorithms**: Automated identification of key facts requires robust NLP techniques
  - Why needed: To reduce manual effort in creating evaluation reference points
  - Quick check: Test extraction accuracy across different deposition types

- **Citation verification methods**: Legal professionals rely heavily on document citations for verification
  - Why needed: To maintain the rigor required in legal document evaluation
  - Quick check: Ensure citation mapping is accurate and traceable

- **Human-AI collaboration principles**: Understanding how legal professionals interact with AI tools
  - Why needed: To design interfaces that support rather than hinder evaluation workflows
  - Quick check: Observe user interaction patterns during evaluation tasks

- **Evaluation metric development**: Creating meaningful measures for summary quality in legal contexts
  - Why needed: To provide actionable feedback for both comparison and improvement scenarios
  - Quick check: Validate metrics against legal professionals' assessment criteria

## Architecture Onboarding

**Component Map:**
Deposition -> Nugget Extractor -> Nugget Store -> Summary Aligner -> Evaluation Interface

**Critical Path:**
1. Nugget extraction from source deposition
2. AI summary generation
3. Nugget-summary alignment
4. Evaluation interface presentation

**Design Tradeoffs:**
- Automated extraction vs. manual curation: Automation reduces effort but may miss nuanced facts
- Granularity of nuggets: Finer granularity enables more precise evaluation but increases complexity
- Real-time vs. batch processing: Real-time provides immediate feedback but requires more computational resources

**Failure Signatures:**
- Misalignment between nuggets and summary content
- Missing critical nuggets that affect evaluation accuracy
- Citation verification failures leading to trust issues
- Interface complexity overwhelming legal professionals

**3 First Experiments:**
1. Test nugget extraction accuracy across 10 depositions from different legal domains
2. Evaluate alignment precision between extracted nuggets and AI-generated summaries
3. Conduct usability testing with 5 legal professionals on the evaluation interface

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation based on a small sample of depositions (n=25), limiting generalizability across legal contexts
- Automated nugget extraction and alignment may struggle with complex legal language and nuanced factual relationships
- Prototype relies on specific citation formats and document structures, potentially limiting applicability to non-standard depositions
- Effectiveness across different legal domains (criminal, civil, corporate) remains to be demonstrated

## Confidence
- **High Confidence**: Identification of legal professionals' challenges in evaluating AI summaries, supported by literature and known legal AI adoption issues
- **Medium Confidence**: Nugget-based approach shows promise in addressing evaluation challenges, though effectiveness across diverse legal scenarios needs further demonstration
- **Medium Confidence**: Two-scenario framework (comparison and improvement) is methodologically sound but requires broader testing with legal professionals

## Next Checks
1. Conduct a larger-scale user study with legal professionals across multiple jurisdictions to evaluate the prototype's effectiveness and identify potential limitations in real-world applications.

2. Perform systematic error analysis of the automated nugget extraction and alignment processes, particularly focusing on complex legal language, temporal relationships, and implicit facts.

3. Test the prototype's robustness with depositions from different legal domains (e.g., criminal, civil, corporate) and varying document structures to assess generalizability.