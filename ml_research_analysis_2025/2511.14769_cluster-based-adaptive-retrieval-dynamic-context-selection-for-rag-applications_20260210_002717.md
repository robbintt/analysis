---
ver: rpa2
title: 'Cluster-based Adaptive Retrieval: Dynamic Context Selection for RAG Applications'
arxiv_id: '2511.14769'
source_url: https://arxiv.org/abs/2511.14769
tags:
- retrieval
- documents
- query
- coinbase
- generation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Cluster-based Adaptive Retrieval (CAR) addresses the problem of
  static top-k retrieval in RAG systems, which often fails to adapt to query complexity,
  leading to either insufficient context or redundant information. CAR dynamically
  determines the optimal number of documents by analyzing clustering patterns in ranked
  query-document similarity distances, detecting natural breakpoints where relevance
  shifts.
---

# Cluster-based Adaptive Retrieval: Dynamic Context Selection for RAG Applications

## Quick Facts
- arXiv ID: 2511.14769
- Source URL: https://arxiv.org/abs/2511.14769
- Authors: Yifan Xu; Vipul Gupta; Rohit Aggarwal; Varsha Mahadevan; Bhaskar Krishnamachari
- Reference count: 34
- Primary result: CAR achieves highest TES scores, reduces token usage by 60%, cuts latency by 22%, and reduces hallucinations by 10% in production RAG systems

## Executive Summary
Cluster-based Adaptive Retrieval (CAR) addresses the fundamental limitation of static top-k retrieval in RAG systems, which fails to adapt to varying query complexity. CAR dynamically determines the optimal number of documents by analyzing clustering patterns in ranked query-document similarity distances, detecting natural breakpoints where relevance shifts. The system consistently outperforms fixed top-k baselines on both synthetic benchmarks and production RAG applications, demonstrating significant improvements in efficiency and answer quality while reducing computational costs.

## Method Summary
CAR employs a dynamic retrieval approach that analyzes clustering patterns in ranked query-document similarity distances to determine optimal document counts. The system detects natural breakpoints where relevance shifts by examining distance gaps between consecutive documents in the ranked list. This adaptive mechanism eliminates the need for manual top-k tuning and responds to query complexity variations. CAR was evaluated on Coinbase's CDP corpus and the MultiHop-RAG benchmark, demonstrating superior performance compared to fixed top-k retrieval methods while reducing token usage and latency in production environments.

## Key Results
- Consistently achieves highest TES scores on Coinbase CDP corpus and MultiHop-RAG benchmark
- Reduces LLM token usage by 60% in production RAG evaluations
- Cuts end-to-end latency by 22% while reducing hallucinations by 10%
- 200% increase in user engagement following deployment in Coinbase's virtual assistant

## Why This Works (Mechanism)
CAR works by leveraging the natural structure in ranked query-document similarity distances. When documents are ranked by relevance to a query, the distances between consecutive documents often exhibit clustering patterns where groups of highly relevant documents are separated by larger gaps. CAR detects these natural breakpoints, which indicate shifts from relevant to less relevant documents, allowing it to dynamically determine the optimal number of documents to retrieve. This approach adapts to query complexity without requiring manual tuning of top-k parameters.

## Foundational Learning

**Query-document similarity analysis**: Understanding how queries relate to documents in vector space is fundamental to CAR's approach. This knowledge is needed to implement the distance-based clustering mechanism. Quick check: Can you compute and interpret similarity scores between queries and documents using cosine similarity or other metrics?

**Dynamic threshold determination**: CAR's breakpoint detection relies on identifying natural gaps in similarity distributions. This concept is essential for implementing the adaptive retrieval logic. Quick check: Can you implement algorithms to detect significant changes in numerical sequences or distributions?

**Clustering pattern recognition**: The ability to identify clusters in ranked data enables CAR to determine relevance boundaries. This skill is needed to implement the core adaptive mechanism. Quick check: Can you apply clustering algorithms or gap detection methods to identify natural groupings in ordered data?

**Vector database operations**: Efficient retrieval and ranking of documents based on similarity requires understanding vector database operations. This knowledge is needed for the initial retrieval step. Quick check: Can you perform similarity searches and ranking operations on vector-stored documents?

## Architecture Onboarding

**Component map**: Query -> Similarity Scoring -> Ranked List -> Distance Gap Analysis -> Breakpoint Detection -> Document Selection -> Context Assembly -> LLM Generation

**Critical path**: The most performance-sensitive path is Query -> Similarity Scoring -> Ranked List -> Distance Gap Analysis, as this sequence must execute quickly to maintain low latency while determining optimal document counts.

**Design tradeoffs**: CAR trades off between computational overhead of gap analysis versus the benefits of reduced token usage and improved relevance. The system prioritizes accuracy over minimal computation, accepting additional processing time for gap analysis to achieve better overall performance.

**Failure signatures**: Common failure modes include: false breakpoint detection leading to insufficient context, over-sensitive gap thresholds retrieving too many documents, and performance degradation with highly homogeneous document sets where relevance boundaries are unclear.

**First experiments**:
1. Implement basic gap detection on synthetic ranked lists with known relevance boundaries
2. Compare CAR's document selection against fixed top-k retrieval on benchmark datasets
3. Measure latency impact of gap analysis computation in production-like conditions

## Open Questions the Paper Calls Out
None

## Limitations
- Production evaluation methodology lacks detailed statistical validation
- User engagement increase claim (200%) requires controlled experimentation to isolate CAR's impact
- Limited comparison with only one other adaptive method (MultiHop-RAG) reduces understanding of relative performance

## Confidence
- TES score improvements on benchmarks: High
- Production metrics (token usage, latency, hallucinations): Medium
- User engagement increase: Low

## Next Checks
1. Conduct A/B testing in production to isolate CAR's impact from other system changes and establish statistical significance of user engagement improvements
2. Benchmark CAR against a broader range of adaptive retrieval methods beyond MultiHop-RAG to establish relative performance
3. Perform ablation studies to quantify the individual contributions of clustering breakpoint detection versus other CAR components to overall performance gains