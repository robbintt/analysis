---
ver: rpa2
title: Gradient-based Model Shortcut Detection for Time Series Classification
arxiv_id: '2510.10075'
source_url: https://arxiv.org/abs/2510.10075
tags:
- shortcut
- time
- series
- data
- class
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the first investigation of point-based shortcut
  learning in time series classification. The authors demonstrate that deep learning
  models can learn simple spurious correlations, such as a single spike at a fixed
  time position, which leads to poor generalization.
---

# Gradient-based Model Shortcut Detection for Time Series Classification

## Quick Facts
- arXiv ID: 2510.10075
- Source URL: https://arxiv.org/abs/2510.10075
- Reference count: 30
- First investigation of point-based shortcut learning in time series classification

## Executive Summary
This paper introduces the first investigation of point-based shortcut learning in time series classification, where deep learning models can learn simple spurious correlations such as a single spike at a fixed time position, leading to poor generalization. The authors propose a Shortcut Aggregate Gradient (SAG) score that uses the gradient of cross-entropy loss with respect to each input point to detect classes dominated by such shortcuts. The method requires only the trained model and training set, with no external attributes or test data needed. Experiments on 24 UCR time series datasets demonstrate that the SAG score can detect shortcut classes with 79% accuracy at the dataset level and 83% accuracy at the class level, with no false positives when using a threshold of 0.15.

## Method Summary
The proposed method detects shortcut learning in time series classification by analyzing gradients of the cross-entropy loss with respect to input points. For each class, the method computes gradients across all training samples, then aggregates them to identify whether the model relies heavily on specific time points. When a model learns a shortcut, gradients become concentrated at the shortcut location rather than being distributed across the entire time series. The SAG score quantifies this concentration, with higher scores indicating stronger shortcut reliance. The approach requires no external attributes or test data, making it practical for real-world deployment scenarios where labeled test data may not be available.

## Key Results
- SAG score detects shortcut classes with 79% accuracy at dataset level and 83% accuracy at class level
- Achieves zero false positives when using threshold of 0.15
- Visualizations confirm gradient concentration at shortcut locations when models rely on them
- Case study on Sony AIBO robot data demonstrates detrimental effect of injected shortcuts on model performance

## Why This Works (Mechanism)
The method works because when deep learning models learn spurious correlations from time series data, they tend to rely heavily on specific input features rather than learning meaningful patterns. By computing gradients of the loss with respect to input points, the approach can identify when a model's decision-making process is dominated by particular time positions. When gradients are concentrated at single time points rather than distributed across the entire sequence, this indicates shortcut learning. The aggregation of these gradients across all samples in a class provides a robust measure of shortcut reliance that is independent of the specific model architecture or dataset characteristics.

## Foundational Learning
- **Gradient-based attribution methods**: Needed to understand how model decisions relate to specific input features; quick check: verify gradients sum to zero across input dimension
- **Cross-entropy loss calculus**: Essential for computing gradients with respect to inputs; quick check: confirm gradient direction aligns with prediction error
- **Time series classification fundamentals**: Required background for understanding sequential data patterns; quick check: validate temporal dependencies in sample data
- **Shortcut learning concepts**: Core phenomenon being detected; quick check: identify simple spurious correlations in synthetic examples
- **Aggregate statistics**: Used to summarize gradient information across samples; quick check: verify aggregation method preserves meaningful signal

## Architecture Onboarding

**Component Map**: Input time series -> Model (CNN/RNN/Transformer) -> Cross-entropy loss -> Gradient computation -> SAG score aggregation -> Shortcut detection

**Critical Path**: The method requires computing gradients of cross-entropy loss with respect to input time series, then aggregating these gradients across all samples in each class to compute the SAG score. The critical path is: trained model + training data -> gradient computation -> class-wise aggregation -> threshold comparison -> shortcut detection.

**Design Tradeoffs**: The approach trades computational overhead of gradient computation for interpretability and detection capability. It requires no test data or external attributes, making it practical but potentially less robust than methods using held-out validation. The threshold selection (0.15) balances detection sensitivity against false positive rate.

**Failure Signatures**: The method may fail when gradients are naturally concentrated due to legitimate temporal patterns in the data, leading to false positives. It also may miss shortcuts when they are distributed across multiple time points rather than concentrated at single positions. The approach assumes that shortcut reliance manifests as gradient concentration, which may not hold for all model architectures.

**First Experiments**: 
1. Apply method to synthetic dataset with injected single-point shortcuts and verify detection
2. Test on dataset without shortcuts to establish baseline false positive rate
3. Compare SAG score across different model architectures on same dataset

## Open Questions the Paper Calls Out
None identified in the source material.

## Limitations
- Methodology assumes gradient concentration at single time points reliably indicates shortcut learning, but causation is not proven
- Experiments limited to UCR time series datasets and one Sony AIBO case study, limiting generalizability
- Zero false positive rate at 0.15 threshold raises questions about robustness across different scenarios
- Does not address shortcuts distributed across multiple time points

## Confidence
- **High confidence**: Observation that models learn spurious correlations from single spikes/fixed positions; compelling and reproducible gradient visualization results
- **Medium confidence**: SAG score effectiveness across diverse datasets; accuracy rates good but robustness to different architectures and noise needs validation
- **Low confidence**: Complete elimination of false positives at specified threshold; insufficient analysis of threshold behavior across scenarios

## Next Checks
1. Test SAG score across broader time series domains including irregularly sampled and multivariate data to assess generalizability
2. Conduct ablation studies on 0.15 threshold robustness across different model architectures and noise levels
3. Implement controlled experiments with systematically injected shortcuts, comparing SAG score detection against human expert annotation to validate accuracy claims