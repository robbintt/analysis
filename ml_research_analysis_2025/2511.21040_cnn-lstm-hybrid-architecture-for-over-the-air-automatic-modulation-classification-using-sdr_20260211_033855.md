---
ver: rpa2
title: CNN-LSTM Hybrid Architecture for Over-the-Air Automatic Modulation Classification
  Using SDR
arxiv_id: '2511.21040'
source_url: https://arxiv.org/abs/2511.21040
tags:
- uni00000013
- uni00000003
- uni00000011
- uni00000014
- uni00000019
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a CNN-LSTM hybrid architecture for automatic
  modulation classification (AMC) using Software Defined Radio (SDR). The model combines
  CNN-based spatial feature extraction with LSTM-based temporal modeling to classify
  nine modulation schemes under varying signal-to-noise ratios (SNR 0-30 dB).
---

# CNN-LSTM Hybrid Architecture for Over-the-Air Automatic Modulation Classification Using SDR

## Quick Facts
- arXiv ID: 2511.21040
- Source URL: https://arxiv.org/abs/2511.21040
- Authors: Dinanath Padhya; Krishna Acharya; Bipul Kumar Dahal; Dinesh Baniya Kshatri
- Reference count: 21
- Primary result: 93.48% accuracy on 9 modulation schemes (SNR 0-30 dB) using CNN-LSTM hybrid with 3-channel signal representation

## Executive Summary
This paper presents a CNN-LSTM hybrid architecture for automatic modulation classification (AMC) using Software Defined Radio (SDR). The model combines a modified AlexNet CNN for spatial feature extraction with an LSTM for temporal modeling, processing 3-channel signal representations (amplitude, phase, I/Q) from 224-sample windows. The architecture achieves 93.48% accuracy on nine modulation schemes, with validation on both synthetic RadioML2018 data and real over-the-air FM transmissions. The study demonstrates that preserving complete LSTM hidden states through flattening outperforms attention-based compression for this task.

## Method Summary
The method processes 1024 I/Q samples per instance, segmented into 8 overlapping 224-sample windows (50% overlap) transformed into 224×224×3 tensors representing amplitude, phase, and raw I/Q data. A modified AlexNet extracts 256-dim feature vectors from each window, which are processed by a single LSTM layer (256 hidden units, 0.6 dropout) to model temporal dependencies. The complete 8×256 hidden state matrix is flattened to 2048-dim features, passed through dense layers (512→256→9), and classified using softmax. Training uses Adam optimizer (lr=1.5e-4), categorical cross-entropy loss, batch size 32, and early stopping on an 80/10/10 split of a hybrid RadioML2018 + synthetic dataset.

## Key Results
- Model achieves 93.48% overall accuracy, 93.53% precision, 93.48% recall, and F1 score of 93.45%
- ROC analysis shows near-perfect discrimination (AUC ≈ 1.00) for most classes
- Real-world OTA testing validates practical feasibility but accuracy drops to ~80% vs 93% synthetic
- Flattening complete LSTM states outperforms attention-based compression (+0.14% accuracy)

## Why This Works (Mechanism)

### Mechanism 1
Multi-domain signal representation (amplitude, phase, I/Q as 3-channel tensor) provides complementary discriminative features for modulation classification. Raw I/Q samples are transformed into three representations—amplitude (√I² + Q²) captures envelope, phase (arctan(Q/I)) captures frequency variations, and raw I/Q preserves baseband vector information. These are reshaped into 224×224×3 tensors and processed as RGB-like images by the CNN. Each domain encodes distinct physical signal properties that, when combined, provide richer features than any single representation.

### Mechanism 2
Hierarchical CNN-LSTM coupling enables joint spatial-temporal feature extraction from segmented signal windows. The modified AlexNet extracts 256-dim spatial feature vectors from each of 8 overlapping 224-sample windows. These form a sequence (B, 8, 256) processed by LSTM to model temporal dependencies across windows. The LSTM maintains memory cells via gates to selectively retain/forget information across timesteps. Modulation signatures span both instantaneous spatial patterns (captured by CNN) and sequential dynamics across time windows (captured by LSTM).

### Mechanism 3
Preserving complete LSTM hidden states via flattening outperforms attention-based compression for distinguishing spectrally similar modulation schemes. The full 8×256 hidden state matrix is flattened to 2048-dim, preserving all temporal information. Attention-based compression (weighted averaging to 256-dim) creates an information bottleneck, particularly degrading performance on analog modulations (AM-DSB-SC, AM-SSB-SC). Fine-grained temporal features are critical for separating modulation classes with similar spectral characteristics; compression discards discriminative information.

## Foundational Learning

- **I/Q Signal Representation**: Wireless signals are sampled as complex-valued In-phase (I) and Quadrature (Q) components, representing amplitude and phase in the complex plane. Why needed here: The entire preprocessing pipeline transforms I/Q samples into tensors; understanding this is essential for debugging signal acquisition. Quick check: Given I=0.707, Q=0.707, what is the signal's phase? (Answer: 45° or π/4 radians)

- **SNR (Signal-to-Noise Ratio)**: Ratio of signal power to noise power, typically expressed in dB; determines classification difficulty. Why needed here: Model performance is evaluated across SNR 0–30 dB; low SNR (<10 dB) significantly degrades accuracy for high-order QAM. Quick check: If SNR drops from 20 dB to 10 dB, by what factor does noise power increase relative to signal? (Answer: 10×)

- **LSTM Gating Mechanism**: Forget, input, and output gates regulate information flow, enabling the network to learn long-term dependencies. Why needed here: Understanding equations (3)–(8) is necessary to interpret temporal modeling behavior and diagnose convergence issues. Quick check: If forget gate outputs near-zero values for all timesteps, what happens to the cell state? (Answer: Previous cell state is erased; model loses long-term memory)

## Architecture Onboarding

- **Component map**: RTL-SDR → Raw I/Q samples (1024 per instance) → Preprocessing → 8 overlapping windows → 224×224×3 tensors (amplitude, phase, I/Q channels) → Modified AlexNet → 5 conv layers + GAP → 256-dim feature vector per window → LSTM (256 hidden units, dropout 0.6) → 2048-dim flattened output → Classification head → Dense(512) → Dense(256) → Softmax(9)

- **Critical path**: I/Q acquisition quality → Window segmentation (50% overlap ensures temporal continuity) → CNN feature quality → LSTM temporal aggregation → Classifier discrimination. Errors in early stages propagate; verify SDR sampling rate matches GNU Radio configuration.

- **Design tradeoffs**: Flattening vs attention: Full hidden state preservation improves accuracy (+0.14%) but increases classifier input dimension 8×. Batch size 32 vs 16: Larger batch stabilizes training but showed slightly lower pre-tuning accuracy (91.34% vs 91.46%). Window overlap (50%): Increases training samples but introduces redundancy; may cause overfitting on repeated patterns.

- **Failure signatures**: 16QAM/64QAM confusion (Classes 3–4): Inherent constellation similarity under noise; expected limitation. AM-DSB-SC/AM-SSB-SC confusion with attention model: Information bottleneck in compression. Live real-time accuracy drop (80% vs 86% recorded, 93% synthetic): Multipath, Doppler, carrier offsets not fully modeled in training data.

- **First 3 experiments**:
  1. **Baseline reproduction**: Train on RadioML2018 subset with default hyperparameters (batch=32, lr=1.5e-4, dropout=0.6); verify ~91% validation accuracy.
  2. **Ablation—representation impact**: Train with single-channel inputs (I/Q only, amplitude only, phase only) vs 3-channel; quantify contribution of each domain.
  3. **OTA validation pipeline**: Configure RTL-SDR + GNU Radio flowgraph; capture live FM broadcast; test inference latency and accuracy degradation vs synthetic test set.

## Open Questions the Paper Calls Out

### Open Question 1
How can the model be adapted to close the performance gap between synthetic dataset accuracy (93.48%) and live over-the-air (OTA) accuracy (~80%)? The current architecture and training data (RadioML2018 + AWGN) do not fully account for the dynamic distortions inherent in physical channels. What evidence would resolve it: Demonstration of maintained accuracy (>90%) during live testing involving mobile transmitters or non-line-of-sight channel models.

### Open Question 2
What architectural modifications are required to improve discrimination between high-order QAM schemes (16QAM and 64QAM) under low SNR? The modified AlexNet backbone struggles to differentiate tight constellation clusters when noise is present. What evidence would resolve it: A significant reduction in off-diagonal values for 16QAM/64QAM classes in the confusion matrix at low SNRs (0–10 dB).

### Open Question 3
Would a multi-head or transformer-based attention mechanism overcome the information bottleneck observed with the single-headed attention approach? The study only tested a single-headed attention layer against the flattening approach, leaving advanced attention methods unexplored. What evidence would resolve it: An advanced attention-based model matching or exceeding the F1 score of the flattening approach without losing analog modulation fidelity.

## Limitations

- Real-world validation shows significant performance degradation (80% vs 93% synthetic) due to multipath, Doppler, and carrier offset effects
- Custom dataset generation parameters unspecified, making it impossible to assess training data representativeness
- Computational overhead of 2048-dim flattened features not quantified (inference latency, memory footprint)

## Confidence

**High Confidence Claims** (Supported by direct evidence and reproducible):
- CNN-LSTM architecture achieves 93.48% accuracy on hybrid RadioML2018 + synthetic dataset
- 3-channel tensor representation (amplitude, phase, I/Q) improves discrimination vs single-channel inputs
- Flattening complete LSTM states outperforms attention-based compression (93.48% vs 93.34% accuracy)

**Medium Confidence Claims** (Evidence present but with assumptions):
- Model generalizes to SNR 0-30dB range (limited by test set coverage)
- Modified AlexNet backbone (5 conv layers, 256-dim output) is optimal for AMC
- FM transmitter validation demonstrates practical feasibility

**Low Confidence Claims** (Limited or indirect evidence):
- Attention mechanism inherently struggles with analog modulation discrimination
- 50% window overlap is optimal for temporal continuity
- 8-window sequence length captures all relevant temporal dependencies

## Next Checks

1. **Channel Impairment Augmentation**: Generate a validation set with controlled multipath (Rician fading, delay spread 1-10μs), Doppler shifts (±50 Hz), and carrier frequency offsets (±5 kHz). Measure accuracy degradation and compare against the paper's real-world OTA results.

2. **Attention Mechanism Ablation**: Implement the attention-based compression approach described in Section III.E.2. Train both flattening and attention models on the full dataset, then conduct pairwise statistical significance testing (t-test, p<0.05) on per-class F1 scores to verify the 0.14% accuracy difference.

3. **Real-time Deployment Benchmark**: Deploy the trained model on an embedded SDR platform (e.g., USRP B205-mini or SDRplay) with documented CPU/GPU usage, inference latency (ms per sample), and memory footprint. Compare against the paper's RTL-SDR validation to quantify the computational overhead of the 2048-dim flattened features.