---
ver: rpa2
title: 'Culturally Grounded Physical Commonsense Reasoning in Italian and English:
  A Submission to the MRL 2025 Shared Task'
arxiv_id: '2510.22631'
source_url: https://arxiv.org/abs/2510.22631
tags:
- reasoning
- samples
- physical
- italian
- words
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces FormaMentis, a novel benchmark for physical
  commonsense reasoning in Italian and English, designed for the MRL 2025 Shared Task
  on Multilingual Physical Reasoning Datasets. The benchmark consists of 120 manually
  annotated test samples evenly distributed across three culturally-grounded domains:
  household, cuisine, and entertainment.'
---

# Culturally Grounded Physical Commonsense Reasoning in Italian and English: A Submission to the MRL 2025 Shared Task

## Quick Facts
- arXiv ID: 2510.22631
- Source URL: https://arxiv.org/abs/2510.22631
- Authors: Marco De Santis; Lisa Alazraki
- Reference count: 8
- Key outcome: Novel benchmark FormaMentis with 120 samples in Italian and English for culturally grounded physical commonsense reasoning

## Executive Summary
FormaMentis introduces a novel benchmark for physical commonsense reasoning that incorporates Italian cultural knowledge. The benchmark consists of 120 manually annotated test samples distributed across three domains: household, cuisine, and entertainment. Expert native Italian speakers created novel samples requiring reasoning about objects and actions rooted in Italian customs, with English translations preserving cultural nuances. Each sample follows a PIQA-like format with a prompt and two candidate completions differing by one or two words. The benchmark aims to support research on multilingual physical reasoning and evaluation of language models on culturally grounded commonsense tasks beyond English.

## Method Summary
The benchmark was created through a multi-step process involving expert native Italian annotators who crafted culturally specific prompts requiring physical reasoning. Each sample underwent rigorous validation by independent native speakers through a 6-step questionnaire ensuring cultural grounding, appropriate difficulty, and unambiguous answers. All samples were then manually translated to English while preserving cultural nuances. The final benchmark contains 120 samples (40 per domain) with both Italian and English versions.

## Key Results
- 120 manually annotated test samples evenly distributed across household, cuisine, and entertainment domains
- Expert native Italian speakers created samples requiring cultural knowledge specific to Italian customs
- Each sample follows PIQA format with closely matched completions (differing by 1-2 words)
- Rigorous multi-step validation by native speakers ensured quality and cultural grounding
- English translations preserve Italian cultural nuances while maintaining the physical reasoning challenge

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Culture-specific physical reasoning tasks expose gaps in language models trained primarily on English data.
- Mechanism: Physical commonsense often involves objects, actions, and practices that are culture-specific and may lack direct English equivalents. By grounding prompts in Italian customs, the benchmark requires cultural knowledge that cannot be inferred from general physical reasoning alone.
- Core assumption: Models trained predominantly on English data have limited exposure to culture-specific physical practices and linguistic expressions unique to Italian contexts.
- Evidence anchors: Expert annotators created samples based on local customs; existing benchmarks are criticized as "English-centric."

### Mechanism 2
- Claim: Close-matched completions (differing by 1-2 words) create adversarial pressure that tests fine-grained physical understanding rather than surface-level plausibility.
- Mechanism: By requiring completions to be nearly identical with one unambiguously correct and one incorrect but not absurd option, the benchmark prevents models from using simple heuristics or obvious implausibility detection.
- Core assumption: Models that succeed via surface-level pattern matching will struggle when distractors are carefully constructed to be plausible but wrong.
- Evidence anchors: Validation step #6 ensures incorrect completions are plausible enough not to appear absurd.

### Mechanism 3
- Claim: Multi-step native-speaker validation creates a quality filter that ensures cultural authenticity and appropriate difficulty.
- Mechanism: Each sample passes through a 6-step validation questionnaire by a different native Italian speaker than the author. This creates inter-annotator agreement on cultural grounding and difficulty calibration.
- Core assumption: Native speaker consensus on cultural grounding and difficulty correlates with genuine cultural knowledge requirements.
- Evidence anchors: Each sample validated by a different native speaker; validation includes cultural specificity and difficulty checks.

## Foundational Learning

- Concept: **Physical Commonsense Reasoning**
  - Why needed here: The benchmark targets reasoning about physical properties of objects and actions, not just linguistic or causal reasoning.
  - Quick check question: Can you explain why "How do I preserve fresh pasta?" requires different knowledge than "Why did the pasta spoil?"

- Concept: **PIQA Format (Binary Choice Completion)**
  - Why needed here: FormaMentis adopts this format—understanding how prompts pair with two candidate completions is necessary for using or extending the benchmark.
  - Quick check question: Given a prompt "To clean a copper pot," what makes "use lemon and salt" versus "use dish soap only" a valid PIQA-style pair?

- Concept: **Cultural Grounding in NLP Benchmarks**
  - Why needed here: The paper's core contribution is culturally grounded evaluation; understanding what makes data "culturally grounded" (vs. merely translated) is critical.
  - Quick check question: Why would translating "How do you play baseball?" into Italian not constitute culturally grounded evaluation for Italian speakers?

## Architecture Onboarding

- Component map: Expert annotators → Sample creation (Italian) → Validation pipeline → Translation to English → Final benchmark

- Critical path:
  1. Annotator training on guidelines
  2. Sample creation with physical reasoning + cultural grounding requirements
  3. Validation questionnaire (6 steps, all must pass)
  4. Translation with cultural nuance preservation
  5. Final inclusion in benchmark

- Design tradeoffs:
  - **Size vs. quality**: 120 samples is small but consistent with other high-quality human-written benchmarks; prioritizes depth over breadth
  - **Monolingual creation + translation vs. parallel creation**: Authors chose Italian-first creation to ensure cultural authenticity, then translation for accessibility
  - **Text-only vs. multimodal**: Deliberately text-only to isolate linguistic/cultural reasoning; acknowledges multimodal would be valuable future work

- Failure signatures:
  - Samples where completions differ by more than 1-2 words (validation step #5 failure)
  - Prompts solvable without cultural knowledge (validation step #3 failure)
  - Translations that erase cultural markers
  - Domain imbalance (not maintaining 40 samples per domain)

- First 3 experiments:
  1. **Baseline evaluation**: Run existing multilingual LLMs on both Italian and English versions; compare accuracy gaps to quantify cultural grounding effects.
  2. **Translation robustness test**: Compare model performance on Italian-original vs. English-translated versions; a large gap suggests translation artifacts or language-specific reasoning differences.
  3. **Ablation by domain**: Analyze performance separately on household, cuisine, and entertainment domains to identify which cultural knowledge areas are most challenging for current models.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the integration of multimodal inputs (images or videos) influence model performance on culturally grounded physical reasoning tasks compared to text-only approaches?
- Basis in paper: The authors explicitly state in the Limitations section that they leave the study of multimodal physical reasoning that integrates culture-specific knowledge to future work.
- Why unresolved: FormaMentis currently exists solely as a text-based benchmark, preventing the assessment of whether visual context aids or hinders culturally specific reasoning.
- What evidence would resolve it: Performance metrics from vision-language models evaluated on an extended version of FormaMentis that includes visual annotations.

### Open Question 2
- Question: What is the performance disparity between state-of-the-art multilingual models and Italian-specific models on the FormaMentis benchmark?
- Basis in paper: The paper details the dataset creation and validation process but does not report experimental results or baseline scores for existing language models.
- Why unresolved: The authors focus on the resource contribution rather than the empirical evaluation of current systems, leaving the benchmark's difficulty unquantified.
- What evidence would resolve it: A benchmarking study reporting accuracy scores for various models on the test set.

### Open Question 3
- Question: Does translating culturally specific Italian samples into English result in a performance drop for English-centric models compared to their performance on native English physical commonsense benchmarks?
- Basis in paper: The authors note that English translations preserve Italian cultural nuances (e.g., leaving specific words in Italian), implying a potential mismatch between these translated samples and the training data of standard English models.
- Why unresolved: It is unclear if the cultural grounding makes the English versions of the samples "out-of-distribution" for models trained primarily on generic English text.
- What evidence would resolve it: A comparative analysis of model accuracy on the translated English subset versus the original Italian subset, cross-referenced with scores on standard English datasets like PIQA.

## Limitations
- Small scale (120 samples) limits statistical power and generalizability
- English translations were not independently validated for quality preservation
- Benchmark does not include multimodal elements that could aid physical reasoning
- Claims about differential difficulty for English-centric models remain theoretical without empirical evidence

## Confidence
- **High confidence**: The benchmark successfully implements a rigorous validation protocol ensuring cultural authenticity and physical reasoning requirements.
- **Medium confidence**: The claim that FormaMentis will expose gaps in multilingual models' cultural knowledge is plausible but unverified.
- **Low confidence**: The assertion that English translations preserve cultural nuances is not independently verified.

## Next Checks
1. **Translation robustness test**: Evaluate whether model performance differs significantly between Italian-original and English-translated versions of the same samples.
2. **Cultural specificity validation**: Have independent native Italian speakers rate a random sample of prompts for cultural grounding intensity and compare with validation pass rates.
3. **Baseline model evaluation**: Run baseline multilingual models on FormaMentis in both languages and compare performance across domains and between languages.