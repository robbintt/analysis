---
ver: rpa2
title: Large-Scale In-Game Outcome Forecasting for Match, Team and Players in Football
  using an Axial Transformer Neural Network
arxiv_id: '2511.18730'
source_url: https://arxiv.org/abs/2511.18730
tags:
- each
- attention
- game
- player
- axial
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper presents a transformer-based neural network model for
  in-game forecasting of football match outcomes at multiple levels: game, team, and
  player. The model predicts the expected totals for 13 different actions (e.g., goals,
  passes, tackles) for each agent at multiple time-steps during a match, integrating
  temporal dynamics and inter-agent interactions.'
---

# Large-Scale In-Game Outcome Forecasting for Match, Team and Players in Football using an Axial Transformer Neural Network

## Quick Facts
- arXiv ID: 2511.18730
- Source URL: https://arxiv.org/abs/2511.18730
- Reference count: 40
- Primary result: Transformer model forecasts 13 football actions for all agents with sub-second latency on CPU, achieving well-calibrated predictions across match, team, and player levels.

## Executive Summary
This paper presents a transformer-based neural network model for in-game forecasting of football match outcomes at multiple levels: game, team, and player. The model predicts the expected totals for 13 different actions (e.g., goals, passes, tackles) for each agent at multiple time-steps during a match, integrating temporal dynamics and inter-agent interactions. The core method uses a novel axial transformer architecture that efficiently captures both temporal and agent-level attention, achieving sub-second latency on commodity CPU hardware and generating approximately 75,000 predictions per game. Empirical results demonstrate consistent and reliable predictions across all target actions, with well-calibrated probability estimates and strong performance in ablation studies that highlight the importance of both temporal dynamics and inter-agent interactions in the model's effectiveness.

## Method Summary
The model forecasts end-of-match action totals for 13 actions across all agents (players, teams, game-level) at ~150 timesteps per match. It uses six input tensors: live player/team/game features plus pre-game strength/context features. A transformer with 4 axial attention layers (latent dim 128) processes these inputs, where axial attention decomposes 2D attention into row (temporal, causal mask) and column (agent) operations combined with weighted renormalization. Separate linear heads predict each action target using appropriate distributions (Poisson, Bernoulli, etc.). The model is trained with summed per-target losses using Adam optimizer with cosine annealing, achieving sub-second inference latency on CPU hardware.

## Key Results
- Sub-second latency inference on commodity CPU hardware while generating ~75,000 predictions per game
- Well-calibrated probability estimates across all 13 target actions, with calibration error below 0.1 for frequent actions
- Ablation studies show significant performance degradation when removing temporal attention, agent attention, or pre-game context features
- Consistent forecasting performance across match, team, and player levels without degradation in multi-task setting

## Why This Works (Mechanism)

### Mechanism 1: Additive Axial Attention
Decomposing attention along temporal and agent dimensions, then combining outputs with shared weights, captures game dynamics and player interactions efficiently. For each embedding at grid position (i,j), row attention attends to previous time-steps for that agent (causal mask), and column attention attends to all agents at that time-step. Outputs are weighted by their normalizing factors and summed. Core assumption: Temporal and agent dependencies are approximately separable, allowing O((H+W)HW) complexity instead of O(H²W²) for full joint attention.

### Mechanism 2: Multi-Modal Input Fusion via Attention
Heterogeneous inputs (player strength, live game state, team features) can be integrated without feature duplication by projecting to a shared embedding space and letting attention learn cross-modal relevance. Each input tensor passes through a separate linear layer to dimension D, then all are concatenated into a single grid. The transformer learns which features at each modality are relevant to predictions at other modalities.

### Mechanism 3: Joint Multi-Task Prediction for Consistency
Simultaneously predicting multiple correlated actions for all agents encourages the model to learn shared representations that maintain consistency across targets. A single transformer produces embeddings; separate linear heads predict each action target. Losses are summed and optimized jointly with Adam. Core assumption: Action totals are correlated (e.g., dominant teams have more passes AND fewer fouls), and joint training captures these correlations better than independent models.

## Foundational Learning

- **Self-Attention with Causal Masking**: Temporal attention uses an upper-triangular mask so predictions at time t only attend to time-steps [1..t-1]. Agent attention is unmasked to allow full inter-agent interaction. *Quick check: Why would using bidirectional temporal attention (no mask) be inappropriate for live forecasting?*

- **Axial Attention Decomposition**: This is the core architectural innovation. Understanding how 2D attention is decomposed into row and column operations is essential for implementation and debugging. *Quick check: Given a grid of shape H×W×D, what is the complexity of axial attention vs. full 2D attention?*

- **Probabilistic Distribution Heads**: The model outputs distribution parameters (Poisson λ, Bernoulli p) rather than point estimates, enabling calibrated probability estimates and proper loss functions (negative log-likelihood). *Quick check: Why is Poisson distribution appropriate for modeling count data like passes or tackles?*

## Architecture Onboarding

- **Component map**:
```
Input Tensors: X_player, X_player-strength, X_team, X_team-strength, X_game, X_game-context
    ↓
Embedding Layers: Separate linear projections → common dimension D=128
    ↓
Concatenation: Grid E_0 of shape (P+2+1) × (T+1) × D
    ↓
Axial Transformer Layers (L=4):
    - Row attention (temporal, causal mask)
    - Column attention (agent, no mask)
    - Re-normalization and weighted sum
    - Feed-forward + layer-norm + skip connections
    ↓
Target Layers: Linear heads per action (e.g., h_goal, h_pass, h_foul)
    ↓
Outputs: Distribution parameters for each (agent, action, time-step)
```

- **Critical path**: The axial attention normalization (Algorithm 1, line 16) is the key differentiator. Debug here first if outputs look wrong.

- **Design tradeoffs**:
  - Additive axial (this paper) vs. stacked axial (Ho et al. 2019): Additive combines row/column in one layer with shared weights; stacked uses separate layers. Paper shows additive outperforms stacked empirically.
  - Latent dimension (D=128) vs. model capacity: Larger D improves expressiveness but increases latency.
  - Number of layers (L=4): More layers capture longer-range dependencies but add computation.

- **Failure signatures**:
  - Predicted totals decreasing below running totals: Check causal mask implementation in temporal attention.
  - Poor calibration for rare events (red cards, own goals): Expected behavior due to data imbalance; model assigns P(0) > 0.84 for these.
  - Ablation showing no degradation from removing agent attention: Indicates inter-agent interactions are not being learned; check column attention implementation.

- **First 3 experiments**:
  1. Reproduce calibration curves (Figure 5) on held-out test set for all 13 targets. Expect well-calibrated for frequent actions, weaker for rare events.
  2. Run ablation study (Table 1): remove temporal attention, remove agent attention, remove pre-game context. Expect all ablations to underperform full model.
  3. Compare additive axial vs. stacked axial attention. Expect additive to achieve better log-probability across targets.

## Open Questions the Paper Calls Out

### Open Question 1
Can the model effectively perform counterfactual analysis to quantify the impact of hypothetical events (e.g., a non-occurring red card) on match outcomes? The introduction lists the "ability to... perform counter-factual analysis" as a key advantage of joint forecasting, but the paper only empirically validates forecasting accuracy and calibration. What evidence would resolve it: Evaluation of prediction stability and physical consistency when key input features (e.g., current score, red card status) are artificially altered during inference.

### Open Question 2
Does the integration of raw spatiotemporal player tracking data improve forecasting accuracy over event-only aggregates? The "Related Work" section discusses how tracking data enables short-term prediction and contextualization, but the model inputs ($X_{player}$, $X_{team}$) rely exclusively on event logs and aggregate strength ratings. What evidence would resolve it: Ablation results comparing the current event-based input tensors against inputs augmented with player coordinates and velocities.

### Open Question 3
How can the temporal attention mechanism be improved to prevent degradation on low-frequency events like yellow cards? The ablation study notes that for yellow cards, the best-performing ablation (without temporal attention) simply predicted the "majority class" (zero events), suggesting the full model struggles with sparse actions. What evidence would resolve it: Demonstration of a modified attention mechanism or loss function that maintains predictive performance on frequent actions while improving calibration on rare events.

## Limitations
- Axial attention assumption of separable temporal and agent dependencies lacks direct corpus validation for sports forecasting
- Proprietary 62,610-game dataset prevents independent verification of claimed performance metrics
- Calibration analysis shows inherent limitations for rare events (red cards, own goals) where model assigns near-zero probability to non-zero outcomes

## Confidence

- **High Confidence**: Multi-task prediction framework design and implementation details are well-specified and reproducible. The additive axial attention mechanism's computational efficiency claims (O((H+W)HW) vs O(H²W²)) are mathematically sound given the independence assumption.
- **Medium Confidence**: Empirical results showing superior performance over baselines and ablations are internally consistent but rely on proprietary data. The calibration methodology is standard but the limited evaluation on rare events suggests potential overconfidence in overall model quality.
- **Low Confidence**: Generalization claims to other sports or forecasting domains are unsupported. The assumption that temporal and agent dependencies are separable (enabling axial attention) is plausible but unverified for this specific application.

## Next Checks

1. **Reproduce calibration on held-out test set**: Generate calibration curves for all 13 target actions using the same one-vs-one methodology. Focus on comparing frequent vs. rare action performance to verify the paper's observation about class imbalance effects.

2. **Implement and compare axial variants**: Code both additive axial (paper's approach) and stacked axial attention variants. Train both on the same data split and compare log-probability across all targets to verify the claimed superiority of additive formulation.

3. **Feature importance analysis**: Conduct systematic ablation removing pre-game context features, then individual input modalities. Quantify impact on calibration error and log-probability to validate the paper's claims about multi-modal fusion effectiveness.