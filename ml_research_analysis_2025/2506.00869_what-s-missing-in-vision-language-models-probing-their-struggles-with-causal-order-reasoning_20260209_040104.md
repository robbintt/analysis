---
ver: rpa2
title: What's Missing in Vision-Language Models? Probing Their Struggles with Causal
  Order Reasoning
arxiv_id: '2506.00869'
source_url: https://arxiv.org/abs/2506.00869
tags:
- causal
- reasoning
- vlms
- visual
- vqa-causal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces VQA-Causal and VCR-Causal, the first benchmarks
  specifically designed to isolate and evaluate causal reasoning in vision-language
  models (VLMs). The key challenge addressed is that existing benchmarks mix causal
  reasoning with other types of reasoning, allowing VLMs to use object recognition
  or activity identification as shortcuts to arrive at correct answers.
---

# What's Missing in Vision-Language Models? Probing Their Struggles with Causal Order Reasoning

## Quick Facts
- arXiv ID: 2506.00869
- Source URL: https://arxiv.org/abs/2506.00869
- Reference count: 19
- VLMs perform at near-random levels (52%) on causal reasoning benchmarks while excelling at object and activity recognition

## Executive Summary
This paper identifies a critical gap in vision-language models (VLMs): while they excel at recognizing objects and activities, they struggle significantly with causal reasoning—understanding whether one event causes another. The authors introduce two new benchmarks, VQA-Causal and VCR-Causal, specifically designed to isolate causal reasoning by controlling for object and activity recognition abilities. These benchmarks present VLMs with pairs of captions describing causal relationships between events in images, where the only difference is the causal order. Experimental results reveal that ten out of twelve evaluated VLMs perform no better than random guessing on these tasks, despite achieving high accuracy on standard recognition tasks. The study traces this limitation to the severe lack of explicit causal expressions in common training datasets and demonstrates that targeted fine-tuning with hard negative examples can improve causal reasoning while maintaining generalization to downstream tasks.

## Method Summary
The authors created VQA-Causal and VCR-Causal benchmarks by generating pairs of captions for images that differ only in their causal relationship (e.g., "rain causes umbrella holding" vs. "umbrella holding causes rain"). Each image is paired with 12 caption pairs using different causal conjunctions. They evaluated multiple VLMs (CLIP, NegCLIP, BLIP, LLaVA, Qwen-VL) using zero-shot classification, measuring similarity scores between images and correct vs. incorrect captions. To improve causal reasoning, they fine-tuned NegCLIP using contrastive learning with hard negatives—incorrect causal captions paired with random captions and negative images—computing row-wise and column-wise cross-entropy losses on similarity matrices.

## Key Results
- VLMs achieve near-random accuracy (~52%) on causal reasoning benchmarks while exceeding 70% on object and activity recognition tasks
- Ten out of twelve evaluated models perform no better than random guessing on VQA-Causal
- Causal expressions comprise only 0.05% of training data in Conceptual Captions
- Fine-tuning with hard negatives improves causal reasoning performance while maintaining downstream generalization

## Why This Works (Mechanism)
The paper's approach works by isolating causal reasoning from object and activity recognition, allowing precise measurement of VLM limitations. By generating caption pairs that differ only in causal order, the benchmarks ensure that correct answers require genuine causal understanding rather than recognition shortcuts. The fine-tuning mechanism works by exposing models to explicit causal relationships with negative examples, forcing them to learn the semantic distinction between cause and effect rather than memorizing object associations.

## Foundational Learning

**Causal reasoning vs. correlation detection**
Why needed: VLMs must distinguish between events that merely co-occur and those where one event genuinely causes another
Quick check: Can the model correctly identify that rain causes umbrella use, not vice versa?

**Zero-shot vs. fine-tuned evaluation**
Why needed: To determine whether causal reasoning is an inherent capability or can be learned through targeted training
Quick check: Compare performance before and after fine-tuning on causal reasoning tasks

**Contrastive learning with hard negatives**
Why needed: To teach models to distinguish between correct and incorrect causal relationships by explicitly showing both
Quick check: Monitor loss reduction during training when presenting both correct and incorrect caption pairs

## Architecture Onboarding

**Component map**
VQA-Causal/VCR-Causal datasets -> VLM evaluation (CLIP, NegCLIP, BLIP, LLaVA, Qwen-VL) -> Performance measurement -> Fine-tuning with hard negatives -> CausalCLIP evaluation

**Critical path**
Image and caption pair generation -> VLM zero-shot evaluation -> Performance analysis -> Fine-tuning with hard negative contrastive loss -> Post-fine-tuning evaluation

**Design tradeoffs**
The paper prioritizes diagnostic precision over architectural innovation, using existing VLM architectures and focusing on data-level interventions rather than model modifications. This allows clear attribution of performance limitations to training data composition rather than architectural constraints.

**Failure signatures**
Random guessing performance (~50%) on causal benchmarks while maintaining high performance (>70%) on object/activity recognition indicates the model can recognize elements but cannot reason about their causal relationships.

**3 first experiments**
1. Evaluate baseline VLM on VQA-Causal to establish near-random performance
2. Fine-tune NegCLIP with hard negative contrastive loss using 4,891 training instances
3. Compare pre- and post-fine-tuning performance on both causal benchmarks and downstream tasks

## Open Questions the Paper Calls Out

**Open Question 1**
Can architectural modifications to Vision-Language Models (VLMs) improve causal reasoning capabilities more effectively than data-level interventions alone?
- Basis: The paper's Limitations section states it "mainly focuses on data-level and does not address the underlying model architecture"
- Unresolved: Whether structural changes could yield superior reasoning skills compared to fine-tuning with hard negatives
- Evidence needed: Comparative study of modified VLMs with relation reasoning biases vs. data-augmented baselines

**Open Question 2**
Do the identified deficiencies in causal reasoning generalize to video-language models, where temporal dynamics provide explicit sequential evidence of cause and effect?
- Basis: The Limitations section suggests "causal reasoning and fine-grained visual reasoning in other multimodal settings, such as video–language models, remains an important direction"
- Unresolved: Whether inability to deduce causal order persists even with temporal motion cues
- Evidence needed: Adapted video-based benchmark evaluating state-of-the-art video-language models

**Open Question 3**
Why does sensitivity to word order improve performance on some causal conjunctions while degrading performance on others?
- Basis: NegCLIP outperforms baseline on "is caused by" but underperforms on "lead to"
- Unresolved: Whether models learn semantic logic or surface syntactic patterns
- Evidence needed: Analysis of internal attention maps to determine learning patterns

## Limitations

- Benchmarks rely on synthetic caption pairs generated by GPT-4-turbo and human annotation, which may not capture real-world causal relationship diversity
- Fine-tuning evaluation based on relatively small benchmark sets (1,947 and 3,511 instances) raises questions about statistical significance
- Study focuses on English language causal conjunctions, potentially limiting generalizability to multilingual or culturally-specific expressions

## Confidence

- **High confidence**: VLMs achieve near-random performance on causal reasoning benchmarks while excelling at object and activity recognition
- **Medium confidence**: Lack of causal expressions in training data causes causal reasoning limitations
- **Medium confidence**: Causal fine-tuning improves performance while maintaining downstream generalization

## Next Checks

1. Perform statistical significance testing using bootstrap resampling to confirm observed differences between model performance on causal vs. object/activity tasks
2. Examine additional VLM training datasets beyond Conceptual Captions to verify whether 0.05% causal expression rate is representative
3. Evaluate CausalCLIP models on additional out-of-domain causal reasoning tasks requiring multi-step causal chains to verify generalization beyond specific training distribution