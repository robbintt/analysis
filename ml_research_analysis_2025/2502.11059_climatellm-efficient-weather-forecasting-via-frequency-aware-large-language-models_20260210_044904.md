---
ver: rpa2
title: 'ClimateLLM: Efficient Weather Forecasting via Frequency-Aware Large Language
  Models'
arxiv_id: '2502.11059'
source_url: https://arxiv.org/abs/2502.11059
tags:
- weather
- forecasting
- time
- temporal
- frequency
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ClimateLLM addresses weather forecasting limitations by combining
  frequency-domain processing with large language models. The framework applies 2D
  FFT to spatial data, uses a Mixture-of-Experts module to process different frequency
  components, and integrates dynamic prompting to capture spatiotemporal patterns.
---

# ClimateLLM: Efficient Weather Forecasting via Frequency-Aware Large Language Models

## Quick Facts
- arXiv ID: 2502.11059
- Source URL: https://arxiv.org/abs/2502.11059
- Reference count: 32
- Primary result: Achieves ACC scores of 0.98-1.00 across weather variables with 98.7% training efficiency improvement

## Executive Summary
ClimateLLM introduces a novel weather forecasting framework that leverages frequency-domain processing and large language models to efficiently capture both global atmospheric patterns and localized extreme events. The architecture applies 2D FFT to spatial weather data, routes frequency components through a Mixture-of-Experts module, and integrates dynamic prompting to capture spatiotemporal patterns. Experiments on ERA5 dataset demonstrate state-of-the-art accuracy (ACC ≥0.98) while achieving 98.7% improvement in training efficiency compared to traditional methods. The model also shows strong zero-shot and few-shot capabilities, validating its potential as a scalable foundation model for weather prediction.

## Method Summary
ClimateLLM processes weather data by first applying 2D Fast Fourier Transform to convert spatial grids into frequency components, separating global patterns (low frequencies) from localized features (high frequencies). A Mixture-of-Experts module with learned gating routes these frequency components to specialized sub-networks for efficient processing. Learnable prompt tokens with two-stage cross-attention dynamically fuse temporal dynamics and cross-variable correlations, providing structured context to the GPT backbone. The model partially fine-tunes a pre-trained GPT-2 architecture for temporal modeling, using latitude-weighted loss to correct for spherical geometry bias. This design enables efficient modeling of spatiotemporal patterns while maintaining physical consistency and computational tractability.

## Key Results
- Achieves ACC scores of 0.98-1.00 across temperature (t2m), wind (u10), geopotential (z500), and temperature (t850) variables
- Demonstrates 98.7% improvement in training efficiency (17.6h→0.22h) and 92.7% reduction in GPU memory (34,900MB→2,564MB) versus ClimODE
- Outperforms state-of-the-art methods in both short-term (6-24h) and long-term (up to 72h) forecasting on ERA5 dataset
- Shows strong zero-shot and few-shot capabilities for weather prediction across different climate regimes

## Why This Works (Mechanism)

### Mechanism 1: Frequency-Domain Spatial Decomposition with Adaptive Expert Routing
Converting spatial weather data to the frequency domain and routing frequency components to specialized experts enables efficient modeling of both global atmospheric patterns and localized extreme events. The 2D FFT separates low-frequency components (broad-scale circulation) from high-frequency components (fine-scale phenomena), while a Mixture-of-Experts layer with learned gating assigns each frequency band to specialized sub-networks. This approach allows dedicated processing for different spatial scales rather than uniform treatment. The assumption is that extreme weather events exhibit distinct frequency signatures that can be isolated and processed separately from background patterns, and that the optimal mapping between frequency components and expert networks is learnable through gradient-based training.

### Mechanism 2: Meta-Fusion Prompting for Cross-Dimensional Spatiotemporal Bridging
Learnable prompt tokens with two-stage cross-attention dynamically fuse temporal dynamics and cross-variable correlations, providing the LLM with structured context about weather system evolution. The prompts first attend to temporally-aggregated representations to capture pure temporal dynamics, then attend to variable-wise aggregated representations to capture cross-variable physics. The resulting prompts encode both types of patterns without hard-coding domain knowledge. The assumption is that temporal and variable dimensions carry partially independent information that benefits from explicit aggregation before fusion, and that cross-attention can learn meaningful bridging patterns that generalize across weather states.

### Mechanism 3: Pre-trained Transformer Temporal Modeling with Latitude-Weighted Optimization
A GPT backbone with partial fine-tuning captures long-range temporal dependencies efficiently, while latitude-weighted loss corrects for spherical geometry bias in gridded data. Frequency-domain tokens from each time step are treated as sequence elements fed to GPT, with self-attention capturing dependencies across the temporal sequence. Only the MoE and prompting layers are trained from scratch, reducing computational cost. The loss function weights each grid point by cos(latitude), ensuring equatorial regions contribute proportionally more than polar regions. The assumption is that GPT's pre-trained sequence modeling capabilities transfer to weather temporal patterns despite domain shift, and that latitude-weighted RMSE better reflects true global forecast skill than unweighted metrics.

## Foundational Learning

- **Concept: 2D Fast Fourier Transform (FFT) for Spatial Data**
  - Why needed here: The entire architecture hinges on understanding how FFT converts spatial grids into frequency components, and how frequency magnitude relates to spatial scale.
  - Quick check question: Given a 64×32 spatial grid, what does a low-frequency coefficient (k_m=1, k_n=0) represent versus a high-frequency coefficient (k_m=32, k_n=16)?

- **Concept: Mixture-of-Experts with Soft Gating**
  - Why needed here: The FMoE module requires understanding how gating networks assign weights to experts and how weighted expert outputs combine.
  - Quick check question: If a gating function outputs weights [0.1, 0.7, 0.15, 0.05] for 4 experts with outputs {f_1, f_2, f_3, f_4}, what is the final MoE output?

- **Concept: Cross-Attention in Transformers**
  - Why needed here: Meta-fusion prompting uses cross-attention where learnable prompt tokens query temporal and variable representations.
  - Quick check question: In cross-attention with queries Q (n×d_q), keys K (m×d_k), and values V (m×d_v), what are the dimensions of the attention output if d_q = d_k?

## Architecture Onboarding

- **Component map**: Input → Normalization → 2D FFT → MoE → Prompt concatenation → GPT → Inverse FFT → Denormalization → Output
- **Critical path**: FFT → MoE → Prompt concatenation → GPT → iFFT. The FFT/iFFT pair is the most sensitive; ablation shows 7% RMSE increase without it.
- **Design tradeoffs**: Resolution: 5.625° (64×32) is coarse but tractable; higher resolution quadratically increases FFT and MoE costs. GPT depth: Sensitivity analysis (1-12 layers) shows minimal impact—shallower models suffice. Expert count: Not ablated; more experts increase specialization but also routing complexity and memory.
- **Failure signatures**: Sharp ACC decay beyond 72h lead time → temporal modeling failing to capture long-range dependencies. Systematic polar region errors → latitude weighting insufficient or spherical geometry not handled. Missed extreme events with good average ACC → high-frequency experts not specializing; check gating distribution.
- **First 3 experiments**: 1) Reproduce baseline ACC on ERA5 test set (2017-2018): verify ACC ≥0.98 for t2m at 6h lead time. 2) Ablation validation: Remove FFT module, confirm RMSE increases ~10 units (143→153) and ACC drops to ~0.97. 3) Efficiency profiling: Measure GPU memory and training time vs ClimODE baseline; target >90% memory reduction and >95% time reduction.

## Open Questions the Paper Calls Out

- **Question 1**: How can physics-informed neural network structures be integrated into ClimateLLM to enforce physical conservation laws while maintaining the efficiency of the frequency-domain LLM approach?
  - Basis in paper: The conclusion explicitly states a future direction is "incorporating physics-informed neural networks to integrate prior weather physical knowledge into the model architecture."
  - Why unresolved: The current framework relies on data-driven learning via GPT and FFT without explicitly embedding physical constraints, which can lead to unphysical predictions in data-sparse regions.
  - What evidence would resolve it: A modified version of ClimateLLM that includes physics-based loss terms or layers, demonstrating improved adherence to physical laws without degrading ACC or training efficiency.

- **Question 2**: Can Tree-of-Thought (ToT) reasoning algorithms be effectively adapted to capture non-local atmospheric teleconnections better than the current self-attention mechanism?
  - Basis in paper: The authors plan to "introduce Tree-of-Thought-based reasoning algorithms, leveraging the powerful reasoning capabilities of LLMs to effectively capture temporal and spatial patterns."
  - Why unresolved: Standard LLM self-attention may struggle with long-range, non-linear dependencies; it is unclear if discrete reasoning trees can be mapped onto continuous meteorological evolution without introducing latency or error accumulation.
  - What evidence would resolve it: Implementation of a ToT module showing statistically significant improvements in predicting long-range correlated events compared to the baseline ClimateLLM.

- **Question 3**: Does the 2D FFT-based tokenization and MoE framework scale effectively to high-resolution grids (e.g., 0.25°) without suffering from spectral artifacts or prohibitive token length?
  - Basis in paper: The experiments utilize a coarse 5.625° resolution, yet the paper claims the model is a "scalable solution." Standard 2D FFT on lat-lon grids can introduce artifacts at poles, and finer grids drastically increase token counts.
  - Why unresolved: The paper does not demonstrate performance on standard high-resolution benchmarks, leaving the computational complexity and accuracy of the frequency-domain approach at scale unproven.
  - What evidence would resolve it: Evaluation results on a high-resolution dataset (e.g., 0.25° ERA5) showing that the model maintains GPU memory efficiency and ACC scores comparable to or better than high-resolution baselines like FourCastNet.

## Limitations
- The paper lacks detailed ablation studies for critical architectural decisions including MoE configuration and prompt dimensions
- Training efficiency claims rely on single baseline comparison without exploring whether gains come from architectural choices or simply using larger GPUs
- Evaluation focuses on ACC scores which can mask spatial biases, particularly concerning given the latitude-weighted loss that may over-correct for equatorial regions
- Paper lacks comprehensive analysis of model behavior across different climate regimes and does not report performance on extreme weather event detection rates

## Confidence

| Claim Cluster | Confidence Level |
|---------------|------------------|
| Frequency-Domain Processing Effectiveness | High |
| GPT Transfer Learning Efficiency | Medium |
| Extreme Event Prediction Capability | Medium |
| Zero-Shot/Few-Shot Generalization | Low |

## Next Checks

1. **Extreme Event Detection Benchmark**: Implement a controlled experiment evaluating ClimateLLM's performance on a curated dataset of historical extreme weather events (hurricanes, heat waves, polar vortices). Measure detection rate, false positive rate, and lead time compared to traditional numerical weather prediction models at 6-48 hour horizons.

2. **Cross-Climate Regime Transfer Test**: Train ClimateLLM on Northern Hemisphere data only (2006-2015), then evaluate zero-shot performance on Southern Hemisphere test data (2017-2018). Compare against a model trained on Southern Hemisphere data to quantify the cross-regional generalization capability and identify any systematic biases.

3. **Pre-trained Model Sensitivity Analysis**: Replace the GPT-2 backbone with other pre-trained transformer variants (BERT, RoBERTa, or smaller GPT variants) while keeping the frequency processing and prompting modules fixed. Measure the impact on training efficiency, final ACC, and zero-shot performance to determine whether the efficiency gains are specific to GPT architecture or transferable to other pre-trained models.