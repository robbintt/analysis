---
ver: rpa2
title: 'KokushiMD-10: Benchmark for Evaluating Large Language Models on Ten Japanese
  National Healthcare Licensing Examinations'
arxiv_id: '2506.11114'
source_url: https://arxiv.org/abs/2506.11114
tags:
- uni00000010
- uni00000048
- uni00000015
- uni0000004c
- healthcare
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: KokushiMD-10 is a multimodal benchmark built from ten Japanese
  national healthcare licensing exams, covering Medicine, Dentistry, Nursing, Pharmacy,
  and allied professions. It contains over 11,588 real exam questions with clinical
  images and expert-annotated rationales to assess both textual and visual reasoning.
---

# KokushiMD-10: Benchmark for Evaluating Large Language Models on Ten Japanese National Healthcare Licensing Examinations

## Quick Facts
- arXiv ID: 2506.11114
- Source URL: https://arxiv.org/abs/2506.11114
- Reference count: 25
- Primary result: KokushiMD-10 contains 11,588+ real exam questions from ten Japanese healthcare professions; no model consistently meets passing thresholds, with GPT-4o and Claude 3.5 leading in different domains.

## Executive Summary
KokushiMD-10 is a multimodal benchmark constructed from ten Japanese national healthcare licensing exams covering Medicine, Dentistry, Nursing, Pharmacy, and allied professions. The dataset contains over 11,588 real exam questions with clinical images and expert-annotated rationales to assess both textual and visual reasoning. Evaluation of over 30 LLMs across text-only and multimodal settings revealed that no model consistently meets official passing thresholds, despite strong individual performances in some domains. GPT-4o achieved the highest domain-level scores in four specialties, while Claude 3.5 led in five others. Visual modality provided minimal performance gains, suggesting current vision-language models underutilize clinical images for reasoning.

## Method Summary
The dataset was constructed using a multi-stage pipeline involving PDF-to-image preprocessing, structured QA extraction via Claude 3.5 with deterministic settings (temperature=0), and human-in-the-loop validation. Each PDF exam is converted to PNG images, then processed by Claude 3.5 Sonnet with carefully engineered prompts and strict JSON schema to extract question text, options, metadata, and modality flags. The extracted data undergoes automated format checks and human review before normalization and scoring against official exam criteria per profession. Evaluation includes both text-only and multimodal settings across 33 models, measuring per-type accuracy and pass/fail outcomes.

## Key Results
- GPT-4o achieved the highest domain-level scores in four specialties (Medicine, Public Health Nursing, Radiologic Technology, Pharmacy), while Claude 3.5 led in five others (Dentistry, Nursing, Physical Therapy, Occupational Therapy, Optometry)
- No model consistently meets official passing thresholds across all ten domains, with Dentistry showing the lowest performance universally
- Visual modality provided minimal performance gains, with rank ordering remaining stable between text-only and multimodal evaluations
- Six of ten exams include expert-annotated Chain-of-Thought rationales enabling deeper analysis of model decision-making

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Structured QA extraction via Claude 3.5 with deterministic settings enables scalable, consistent dataset construction from unstructured exam PDFs
- Mechanism: PDFs are converted to PNG images, then fed to Claude 3.5 Sonnet with temperature=0 and a strict JSON schema. The model extracts question text, options, metadata, and modality flags while cross-validating question indices against answer mapping tables
- Core assumption: The extraction model accurately interprets Japanese medical terminology and question formatting without systematic drift
- Evidence anchors: [abstract] "dataset was constructed using a multi-stage pipeline involving PDF-to-image preprocessing, structured QA extraction via Claude 3.5, and human-in-the-loop validation"; [section 2.2] "Each PNG question image is fed into Claude-3.5-Sonnet via the Anthropic API using carefully engineered system-level and user-level prompts, with the temperature fixed at 0 to ensure deterministic outputs"

### Mechanism 2
- Claim: Cross-profession coverage reveals domain-generalization gaps that single-profession benchmarks miss
- Mechanism: By spanning ten healthcare domains with varying question types and modalities, the benchmark exposes whether models rely on surface patterns in high-resource domains versus demonstrating transferable clinical reasoning
- Core assumption: Licensing exam performance correlates with clinically relevant reasoning capabilities, not just test-taking heuristics
- Evidence anchors: [abstract] "no model consistently meets passing thresholds across domains"; [section 3.4] "GPT-4o leads in four specialties... while Claude-3.5-Sonnet dominates five areas"

### Mechanism 3
- Claim: Minimal multimodal gains suggest current vision-language models underutilize clinical images for reasoning
- Mechanism: The benchmark includes ~19.7% image-based questions (highest in Dentistry at ~44.6%), yet multimodal evaluation shows "minimal score variations" compared to text-only mode
- Core assumption: Clinical images in licensing exams contain non-redundant diagnostic information that should improve performance if properly integrated
- Evidence anchors: [abstract] "Visual modality provided minimal performance gains"; [section 3.4] "Cross-modal consistency indicates that visual information provides limited additional benefit for healthcare licensing performance"

## Foundational Learning

- Concept: Chain-of-thought (CoT) reasoning annotations
  - Why needed here: Six of ten exams include expert-annotated CoT explanations, enabling analysis of model decision-making beyond accuracy
  - Quick check question: Can you explain why CoT annotations matter for evaluating why a model answered correctly, not just whether it did?

- Concept: Official licensing exam scoring criteria
  - Why needed here: Evaluation uses profession-specific passing thresholds (e.g., 80% on essential questions for Medicine, "forbidden options" causing automatic failure)
  - Quick check question: Why might a model with high overall accuracy still fail a licensing exam under official scoring rules?

- Concept: Vision-language model architecture
  - Why needed here: Understanding how vision encoders interface with language models clarifies why multimodal gains may be minimal
  - Quick check question: What are two failure modes that could cause a VLM to ignore diagnostically relevant image features?

## Architecture Onboarding

- Component map: PDF preprocessing -> Claude extraction -> human validation -> JSON normalization -> evaluation scoring
- Critical path: PDF preprocessing → Claude extraction → human validation → JSON normalization (ions, isotopes, underlines) → evaluation scoring. Errors in early stages propagate; extraction quality gates downstream reliability
- Design tradeoffs: Claude 3.5 for extraction vs. human-only annotation (scalability vs. potential systematic errors); strict JSON schema vs. flexible output (reproducibility vs. edge cases); official scoring criteria vs. simplified accuracy (clinical realism vs. implementation complexity)
- Failure signatures: Zero-score models in text-only mode (6/33) indicating complete format failure; high variance across years (e.g., Pharmacy ±27.03 for Claude 3.5) suggesting sensitivity to question distribution shifts; Dentistry consistently lowest across all models
- First 3 experiments: Baseline reproduction of GPT-4o and Claude 3.5 Sonnet on Medicine and Dentistry exams; ablation on image resolution for Dentistry questions (72dpi vs. 300dpi); cross-lingual transfer evaluating Japanese-capable models on English-translated questions

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Why does visual modality provide minimal to no performance gains for medical reasoning in current vision-language models?
- Basis in paper: [explicit] The authors state "Visual modality provided minimal performance gains" and note that "cross-modal consistency indicates that visual information provides limited additional benefit for healthcare licensing performance"
- Why unresolved: The paper does not investigate whether this is due to limitations in vision encoders, insufficient visual reasoning capabilities, or the nature of clinical image interpretation tasks
- What evidence would resolve it: Ablation studies isolating visual reasoning components; error analysis on image-based questions comparing correct/incorrect responses between modalities

### Open Question 2
- Question: What factors cause Dentistry to consistently yield the lowest model performance across all tested LLMs?
- Basis in paper: [explicit] The paper reports "Dentistry consistently shows the lowest performance across all models" without providing explanatory analysis
- Why unresolved: The authors do not investigate whether this stems from domain-specific terminology, visual complexity of dental imaging, or fundamentally different reasoning patterns required in dental practice
- What evidence would resolve it: Fine-grained error categorization by question type in Dentistry; domain adaptation experiments; expert analysis of failure modes

### Open Question 3
- Question: Does using Claude 3.5 Sonnet for structured QA extraction introduce evaluation bias when benchmarking Claude models?
- Basis in paper: [inferred] The methodology section describes that "Each PNG question image is fed into Claude-3.5-Sonnet via the Anthropic API" for structured extraction, yet Claude-3.5-Sonnet is evaluated on the resulting benchmark
- Why unresolved: The paper does not address potential data contamination or familiarity effects when the same model family is used for both construction and evaluation
- What evidence would resolve it: Cross-validation using independently constructed subsets; comparison of Claude performance on Claude-extracted vs. human-extracted questions

### Open Question 4
- Question: How can models achieve consistent cross-domain performance rather than excelling only in specialty-specific areas?
- Basis in paper: [explicit] The conclusion states "none consistently meet passing thresholds across domains" and "no model consistently achieves the performance required for real-world clinical use"
- Why unresolved: The paper identifies the inconsistency but does not propose mechanisms for achieving generalizable medical reasoning across diverse healthcare professions
- What evidence would resolve it: Multi-domain training experiments; analysis of knowledge transfer between healthcare specialties; development of unified medical reasoning architectures

## Limitations
- Construction pipeline relies heavily on Claude 3.5 Sonnet for structured extraction, assuming consistent Japanese medical understanding
- Cross-profession comparisons assume official exam scores correlate with clinically relevant reasoning, not test-taking strategies
- Minimal multimodal gains could reflect vision-language model limitations or insufficient clinical image quality
- Dataset closed access and reliance on paid APIs constrain reproducibility

## Confidence

| Claim | Confidence Level |
|---|---|
| Benchmark construction methodology is sound | High |
| Performance rankings (GPT-4o, Claude 3.5 leading) | Medium |
| Conclusion that no model consistently passes all exams | Medium |
| Minimal multimodal gains finding | Medium |

## Next Checks

1. Reconstruct the exact scoring thresholds and weights per profession/year from official MHLW sources to verify pass/fail decisions reported in the paper
2. Conduct an ablation study on Dentistry (highest image ratio) using variable image resolutions (72dpi vs. 300dpi) to isolate whether multimodal performance is limited by image quality or model capability
3. Evaluate a subset of questions using both the original Japanese and professional English translations to quantify language vs. reasoning contributions to model failures