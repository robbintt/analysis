---
ver: rpa2
title: 'SURFACEBENCH: Can Self-Evolving LLMs Find the Equations of 3D Scientific Surfaces?'
arxiv_id: '2511.10833'
source_url: https://arxiv.org/abs/2511.10833
tags:
- symbolic
- surfaces
- equation
- regression
- equations
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: SurfaceBench introduces the first comprehensive benchmark for symbolic
  regression on 3D surfaces, addressing the gap in evaluating LLM-driven equation
  discovery beyond scalar functions. It includes 183 tasks across 15 scientific domains
  with explicit, implicit, and parametric representations, synthetically sampled 3D
  data, and geometry-aware metrics like Chamfer and Hausdorff distances.
---

# SURFACEBENCH: Can Self-Evolving LLMs Find the Equations of 3D Scientific Surfaces?

## Quick Facts
- **arXiv ID**: 2511.10833
- **Source URL**: https://arxiv.org/abs/2511.10833
- **Reference count**: 40
- **Primary result**: SurfaceBench benchmark reveals state-of-the-art symbolic regression methods achieve only 4-6% symbolic accuracy on 3D surface equation discovery tasks

## Executive Summary
SurfaceBench introduces the first comprehensive benchmark for symbolic regression on 3D surfaces, addressing the gap in evaluating LLM-driven equation discovery beyond scalar functions. The benchmark includes 183 tasks across 15 scientific domains with explicit, implicit, and parametric representations, synthetically sampled 3D data, and geometry-aware metrics like Chamfer and Hausdorff distances. Evaluations show that current state-of-the-art symbolic regression methods, including LLM-guided frameworks, struggle with surface recovery, achieving symbolic accuracy of only 4% for LLMs and 6% for traditional methods, underscoring the difficulty of compositional generalization and geometric reasoning.

## Method Summary
SurfaceBench constructs a comprehensive benchmark by selecting 15 scientific domains and canonical equations, then applying compositional augmentation (functional nesting, operator substitution, coordinate reparameterization, and blending) to create non-memorizable variants. The benchmark generates synthetic 3D point clouds from these equations and evaluates candidate solutions using both symbolic accuracy checks and geometry-aware metrics (Chamfer and Hausdorff distances). The evaluation pipeline tests both LLM-based methods (LLM-SR, LaSR, SGA, OpenEvolve) and traditional approaches (PySR, gplearn, DSR, NeSymReS, E2E, TPSR, uDSR) across explicit, implicit, and parametric surface representations.

## Key Results
- State-of-the-art symbolic regression methods achieve only 4% symbolic accuracy for LLM-based approaches and 6% for traditional methods on SurfaceBench tasks
- Parametric surfaces represent the most challenging category, with only OpenEvolve and PySR reliably handling multi-output coupling
- Geometry-aware metrics reveal a "pipeline gap" where correct functional families are identified but geometric fidelity remains poor due to equation fitting failures
- Domain priors provide marginal improvement (1-2%) but incorrect priors can cause degradation in performance

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Geometry-aware metrics enable evaluation of functional equivalence that string-matching metrics miss.
- **Mechanism**: Chamfer distance captures mean geometric fidelity while Hausdorff distance captures worst-case deviation; together they quantify whether predicted and ground-truth surfaces produce equivalent 3D manifolds even when algebraic forms differ.
- **Core assumption**: Two symbolically distinct expressions that produce geometrically similar point clouds represent valid scientific equivalence.
- **Evidence anchors**:
  - [abstract] "To evaluate equation discovery quality, we pair symbolic checks with geometry-aware metrics such as Chamfer and Hausdorff distances, capturing both algebraic fidelity and spatial reconstruction accuracy."
  - [Section 2.1] "Multiple algebraically distinct expressions can describe identical behaviors. A sphere, for example, may appear implicitly as x² + y² + z² = R² or parametrically..."
  - [corpus] Related work on neural implicit models (DeepSDF, Occupancy Networks) learns continuous fields whose level sets define geometry—similar principle of evaluating via induced surfaces rather than symbolic form.
- **Break condition**: If surfaces have high curvature regions where sparse sampling fails to capture topology, Chamfer/Hausdorff may incorrectly penalize valid predictions.

### Mechanism 2
- **Claim**: Compositional augmentation forces models to perform genuine symbolic reasoning rather than template retrieval.
- **Mechanism**: The construction pipeline applies functional nesting, operator substitution, coordinate reparameterization, and additive/multiplicative blending to canonical equations—producing non-canonical variants that cannot be solved by pattern matching against training data.
- **Core assumption**: LLMs have memorized common scientific equations; novel compositions require on-the-fly compositional reasoning.
- **Evidence anchors**:
  - [Section 2.2] "To mitigate memorization of canonical forms, we apply controlled symbolic perturbations... producing non-canonical yet analytically solvable variants that maintain interpretability while forcing models to reason compositionally."
  - [Section 4, Table 1] Low symbolic accuracy (4-6% across methods) suggests memorization-based approaches fail on these compositions.
  - [corpus] LLM-SRBench (related paper by same authors) similarly introduces synthetic equations designed to break memorization—consistent design philosophy.
- **Break condition**: If augmentations produce equations that are mathematically degenerate or lack physical interpretability, the benchmark tests numerical robustness rather than symbolic reasoning.

### Mechanism 3
- **Claim**: Parametric surfaces expose a fundamental gap in current symbolic regression methods—most cannot jointly learn coupled multi-output equations.
- **Mechanism**: Parametric surfaces require discovering three interdependent functions x(u,v), y(u,v), z(u,v) simultaneously; most methods treat outputs independently or lack architecture for multi-equation coupling.
- **Core assumption**: Surface structure emerges from coordination between parametric equations; independent optimization fails to capture this coupling.
- **Evidence anchors**:
  - [Section 4, Table 2] Only OpenEvolve and PySR reliably handle parametric surfaces; other methods excluded due to lack of multi-output support.
  - [Section 5] "LaSR, SGA, and LLM-SR methods lack algorithmic support for parametric equation discovery, and thus we exclude the parametric category from the ablation experiments."
  - [corpus] No directly comparable corpus work on parametric surface discovery—this appears to be a genuinely novel challenge in the symbolic regression literature.
- **Break condition**: If future methods simply fit three independent equations without enforcing geometric consistency, they may achieve low Chamfer distance without recovering the true parametric structure.

## Foundational Learning

- **Symbolic Regression**:
  - Why needed here: This is the core task—recovering closed-form mathematical expressions from data points. Without this foundation, you cannot understand why surface discovery is harder than scalar regression or why geometry-aware metrics matter.
  - Quick check question: Given points (1,2), (2,8), (3,18), can you identify a simple polynomial that fits? What makes this harder in 3D?

- **Implicit vs. Explicit vs. Parametric Representations**:
  - Why needed here: SurfaceBench spans all three forms, each with distinct challenges. Explicit: z=f(x,y). Implicit: F(x,y,z)=0. Parametric: (x(u,v), y(u,v), z(u,v)). Failure modes differ across representations.
  - Quick check question: Sketch how a sphere is represented in each form. Why might an implicit form be harder to discover from point samples?

- **Chamfer and Hausdorff Distances**:
  - Why needed here: These are the primary evaluation metrics. Chamfer measures average point-to-point error; Hausdorff measures maximum deviation. Understanding what each captures is essential for interpreting results.
  - Quick check question: If a predicted surface matches a ground-truth sphere everywhere except one sharp spike, which metric would catch this failure?

## Architecture Onboarding

- **Component map**:
  Dataset construction pipeline -> Data generation -> Evaluation pipeline -> Geometry metrics computation -> Symbolic accuracy check

- **Critical path**:
  1. Load surface task (equation + sampled 3D points)
  2. Run discovery method to generate candidate equation
  3. Sample candidate equation to produce point cloud
  4. Align point clouds (remove translation, rotation, scale differences)
  5. Compute geometry metrics (Chamfer, Hausdorff) + symbolic accuracy + NMSE

- **Design tradeoffs**:
  - **Symbolic vs. geometric evaluation**: High symbolic accuracy may not mean low Chamfer distance (structure correct but constants wrong); low symbolic accuracy may still yield good geometry (different but equivalent form)
  - **Noise injection for robustness testing**: Section 5.1 shows 1-10% Gaussian noise degrades performance—decide whether your use case requires noise robustness
  - **Domain priors in prompts**: Table 3 shows priors provide marginal improvement; incorrect priors cause degradation—consider whether real applications have reliable domain knowledge

- **Failure signatures**:
  - **Search space errors**: Model retrieves wrong functional family (e.g., polynomials when trigonometric terms needed)—visible as high Chamfer/Hausdorff with no symbolic match
  - **Equation fitting errors**: Correct functional categories identified but wrong structural ordering or constants—visible as moderate Chamfer with low symbolic accuracy
  - **Multi-output coupling failure**: Parametric surfaces particularly expose this; models that cannot jointly optimize three equations fail catastrophically on parametric tasks

- **First 3 experiments**:
  1. **Baseline reproduction**: Run PySR (best non-LLM) and OpenEvolve (best LLM) on explicit surfaces subset—verify you can reproduce ~0.18-0.25 symbolic accuracy and low Chamfer distances before testing new methods
  2. **Ablation on representation types**: Compare method performance across explicit, implicit, parametric categories—identify which representation types your method handles poorly
  3. **Domain prior sensitivity**: Test with/without domain priors (Table 3 format) to quantify whether your method exploits structural cues or relies purely on data-driven search

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: Can introducing a targeted geometric calibration step after symbolic discovery successfully bridge the gap between recovering correct functional forms and achieving high geometric fidelity?
- **Basis in paper**: [explicit] The authors state in Section 4 that there is a "pipeline gap" where models recover the structural family but fail on geometry due to an under-optimized fitting stage, necessitating a "targeted geometric calibration step."
- **Why unresolved**: Current results show high Symbolic Accuracy often coincides with poor Chamfer/Hausdorff distances, indicating the optimization step is currently insufficient.
- **What evidence would resolve it**: Demonstrating a model that maintains high symbolic accuracy while achieving significantly lower Chamfer/Hausdorff scores through post-hoc geometric refinement.

### Open Question 2
- **Question**: Does explicitly designing symbolic regression methods to handle joint multi-output coupling significantly improve the recovery of parametric surface equations?
- **Basis in paper**: [explicit] The authors note in Section 4 that "very few methods are designed to jointly learn a coupled set of equations," identifying this as a "major gap" in handling parametric surfaces.
- **Why unresolved**: Current benchmarks show only two methods (OpenEvolve and PySR) reliably handle multiple equations, while others lack algorithmic support for this representation type.
- **What evidence would resolve it**: A new method that jointly optimizes coupled equations and achieves substantially higher symbolic accuracy on parametric surfaces compared to existing baselines.

### Open Question 3
- **Question**: Can integrating robust external optimization loops (e.g., gradient-based or evolutionary) resolve the "equation fitting" failure mode observed in LLM-based symbolic regression?
- **Basis in paper**: [explicit] Section 5.4 identifies "equation fitting failures" where models identify correct functional families but fail to assemble them or infer constants, noting that non-LLM methods succeed via many "evaluation and mutation cycles."
- **Why unresolved**: LLM-based approaches struggle with the continuous optimization of numerical parameters, often stagnating after initial hypothesis generation.
- **What evidence would resolve it**: An ablation study showing that swapping the LLM's native optimization for a dedicated numerical optimizer eliminates the fitting error without compromising structural discovery.

## Limitations
- The benchmark's geometric metrics assume sufficient sampling density to capture surface topology accurately; sparse sampling may penalize valid solutions for highly curved surfaces
- Synthetic equation augmentation may create expressions that lack physical interpretability or mathematical stability, potentially testing numerical robustness rather than genuine symbolic reasoning
- Current results show parametric surfaces as challenging, but this may reflect current method limitations rather than theoretical impossibility

## Confidence
- **High Confidence**: The benchmark construction methodology and evaluation pipeline are well-documented and reproducible. The observation that current methods struggle with surface discovery (4-6% symbolic accuracy) is empirically supported by the ablation studies.
- **Medium Confidence**: The claim that compositional augmentation forces genuine reasoning rather than memorization is plausible but difficult to definitively verify without access to model training processes. The assumption that geometry-aware metrics capture functional equivalence may break down for surfaces with complex topology.
- **Low Confidence**: The assertion that parametric surfaces represent a "fundamental gap" in symbolic regression methods is based on current method limitations rather than theoretical impossibility. Future architectural innovations could address this challenge.

## Next Checks
1. **Sampling density sensitivity**: Systematically vary point cloud density (10×, 100×, 1000× the baseline) and measure metric stability to establish the sampling requirements for reliable evaluation
2. **Topological complexity stress test**: Create a subset of tasks with known challenging topologies (e.g., Möbius strips, Klein bottles) and verify that evaluation metrics correctly handle these cases without producing spurious penalties
3. **Parametric coupling analysis**: For successful parametric surface discoveries, analyze whether the three component equations show evidence of coordinated optimization (e.g., shared parameters, correlated error patterns) versus independent fitting