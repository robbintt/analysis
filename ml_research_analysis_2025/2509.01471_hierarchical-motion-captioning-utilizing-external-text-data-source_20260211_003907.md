---
ver: rpa2
title: Hierarchical Motion Captioning Utilizing External Text Data Source
arxiv_id: '2509.01471'
source_url: https://arxiv.org/abs/2509.01471
tags:
- motion
- text
- high-level
- captions
- low-level
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper addresses the problem of generating high-level textual
  descriptions for human motions when motion-text datasets only contain high-level
  captions without low-level motion descriptions. The proposed method, HiCAM2T, introduces
  a two-step hierarchical approach: first, it generates detailed low-level descriptions
  of motions using large language models, and then it retrieves relevant high-level
  captions from an external database based on these low-level descriptions.'
---

# Hierarchical Motion Captioning Utilizing External Text Data Source

## Quick Facts
- **arXiv ID:** 2509.01471
- **Source URL:** https://arxiv.org/abs/2509.01471
- **Reference count:** 13
- **Primary result:** HiCAM2T achieves 6-50% improvement in BLEU-1, BLEU-4, CIDEr, and ROUGE-L metrics compared to state-of-the-art M2T-Interpretable on motion-text datasets.

## Executive Summary
This paper addresses the challenge of generating high-level textual descriptions from human motion data when training datasets lack low-level motion-text pairs. The proposed HiCAM2T method introduces a hierarchical approach that first generates detailed low-level motion descriptions using a large language model, then retrieves relevant high-level captions from an external database based on these descriptions. This two-stage process significantly improves motion captioning accuracy by leveraging the denser semantic space of physical movements and the flexibility of retrieval-augmented generation. Experiments demonstrate consistent improvements across three datasets (HumanML3D, KIT, BOTH57M) with average performance gains of 6-50% over previous methods.

## Method Summary
HiCAM2T implements a three-network architecture consisting of a Vision Transformer-based Motion Encoder, a Text Decoder (Distilled-GPT2/GPT2), and a Sentence-BERT Text Encoder. The method operates in two stages: first, the motion encoder extracts features from time-series joint data, which the text decoder uses to generate detailed low-level descriptions. These generated descriptions are encoded and used to retrieve the top-k most similar high-level captions from an external database via cosine similarity. In the second stage, the text decoder combines motion features with retrieved high-level captions to generate the final output. Training employs three loss functions: cross-entropy for low-level generation (L1), contrastive loss for retrieval alignment (L2), and cross-entropy for high-level generation (L3), with equal weighting (λ1=λ2=λ3=1).

## Key Results
- HiCAM2T achieves 6-50% improvement in average performance (BLEU-1, BLEU-4, CIDEr, ROUGE-L) compared to state-of-the-art M2T-Interpretable method
- The method demonstrates consistent improvements across three motion-text datasets: HumanML3D, KIT, and BOTH57M
- Enriching the database with additional motion descriptions at inference time results in up to 13% increase in average performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Hierarchical decomposition mitigates data sparsity in motion captioning.
- Mechanism: By forcing the model to first predict low-level physical descriptions (e.g., "arm extension") before high-level semantic labels (e.g., "jumping jacks"), the system maps motion features to a denser manifold of physical descriptors. Since physical movements are shared across many activities, the model encounters fewer "rare" tokens during the initial mapping compared to mapping directly to sparse high-level activity names.
- Core assumption: Low-level physical descriptions generated by LLMs (Falcon 40B) share sufficient semantic overlap with actual motion dynamics to serve as a viable intermediate representation.
- Evidence anchors:
  - [abstract] "The proposed method... first, it generates detailed low-level descriptions... and then it retrieves relevant high-level captions."
  - [section 1] "Half of the terms in the KIT dataset appear at most twice... the straightforward solution... is to collect more data."
  - [corpus] "Towards Fine-Grained Human Motion Video Captioning" supports the difficulty of capturing fine-grained details in standard captioning models.
- Break condition: If the LLM-generated low-level descriptions contain hallucinations or physical inaccuracies that decouple the text from the actual joint velocities, the intermediate mapping will introduce noise, degrading the final high-level prediction.

### Mechanism 2
- Claim: Retrieval-augmented generation bypasses the need for implicit memorization of activity labels.
- Mechanism: Rather than storing the association between motion dynamics and high-level labels entirely within model weights, the system uses a dynamic database. It retrieves candidate high-level captions based on the similarity of their low-level descriptions to the generated motion explanation. This allows the model to "recall" rare activities if the textual description exists in the database, even if the motion was under-represented in training.
- Core assumption: The embedding space of the text encoder (Sentence-BERT) effectively clusters similar motions such that the cosine similarity search reliably retrieves the correct high-level candidate from the database.
- Evidence anchors:
  - [abstract] "...retrieves relevant high-level captions from an external database... [this] greatly increase motion captioning accuracy."
  - [section 3.1] "The embeddings $\hat{u}$ are used to retrieve from a database the top $k$ high-level motion descriptions... based on cosine similarity."
  - [corpus] Corpus papers like "Multi-LLM Collaborative Caption Generation" highlight the utility of external data sources, though specific retrieval mechanisms for motion are unique to this paper.
- Break condition: If the database is populated with ambiguous entries where distinct high-level actions share nearly identical low-level descriptions (e.g., "swatting fly" vs. "waving"), the retrieval mechanism will return conflicting candidates, leading to incoherent generation.

### Mechanism 3
- Claim: Synthetic text augmentation bridges the modality gap without additional motion capture.
- Mechanism: The paper utilizes an external LLM to synthesize low-level "ground truth" text for existing high-level labels. This creates a parallel dataset of (High-Level Label <-> Low-Level Description). By training the motion encoder to align with these synthetic textual embeddings, the model effectively distills the LLM's linguistic knowledge of "how a movement feels" into the motion representation.
- Core assumption: The linguistic relationship defined by the external LLM between "jumping jacks" and "synchronizing arm extensions" is physically consistent with the kinematic data provided to the motion encoder.
- Evidence anchors:
  - [section 3.3] "We use Falcon 40B to generate detailed explanations... for the motions based on the high-level motion caption."
  - [section 4.1] "The baseline results... are directly extracted... HiCAM2T consistently outperforms previous approaches."
  - [corpus] "Proprioception Enhances Vision Language Model..." suggests low-level motion info enhances understanding, reinforcing the utility of the synthetic low-level bridge.
- Break condition: If the distribution of motion data (joint angles/velocities) fundamentally differs from the "physics" assumed by the text-generating LLM, the alignment loss will optimize for a semantic mapping that does not exist in reality.

## Foundational Learning

- **Concept: Vision Transformers (ViT) for Time-Series**
  - Why needed here: The Motion Encoder uses a ViT to process motion as a 2D image-like structure (Time x Joints) rather than a standard RNN/LSTM sequence. Understanding how patches represent temporal chunks of joint states is critical for debugging feature extraction.
  - Quick check question: How does flattening time-joint patches into a sequence allow the transformer to attend to both specific body parts and specific moments in time simultaneously?

- **Concept: Contrastive Learning (InfoNCE/Triplet Loss)**
  - Why needed here: The Text Encoder is fine-tuned using a contrastive loss ($L_2$) to pull the embedding of a generated description closer to its ground truth while pushing away random motions. This is the engine behind the retrieval accuracy.
  - Quick check question: In the loss equation $L_2$, what is the effect of the hyperparameter $c$ on the margin between positive and negative embedding pairs?

- **Concept: Autoregressive Language Modeling**
  - Why needed here: The final output generation uses a standard language model (Distilled-GPT2/GPT2) to decode features into text. The system conditions this decoder on *both* motion features and retrieved text tokens.
  - Quick check question: Why is concatenating retrieved high-level captions with motion features more effective than simply outputting the top-1 retrieved caption as the final result?

## Architecture Onboarding

- **Component map:** Input Motion Tensor $m$ (Time × Joints) -> Motion Encoder (ViT) -> Feature Vector $f$ -> Text Decoder (Stage 1) -> Low-Level Text $\hat{z}$ -> Text Encoder (S-BERT) -> Embeddings $\hat{u}$ -> Cosine Similarity Search against Database -> Top-$k$ High-Level Captions -> Text Decoder (Stage 2) with [$f$ + Top-$k$ Captions] -> Final High-Level Caption $\hat{y}$

- **Critical path:** The quality of the retrieval step is the bottleneck. If the Text Encoder (TE) is not fine-tuned properly (via $L_2$) to align the generated low-level description with the database entries, the system retrieves irrelevant context, and the final decoder fails to correct it.

- **Design tradeoffs:**
  - **Shared Weights:** The authors share weights between the two Text Decoder stages (Stage 1 and Stage 3). This reduces parameter count but assumes the linguistic capability to describe "movement physics" and "activity names" is shared.
  - **Database Update Frequency:** Encodings are updated only at the end of each epoch to save compute. This introduces a "stale data" lag where the database lags behind the current state of the fine-tuned Text Encoder.

- **Failure signatures:**
  - **Low-Level Hallucination:** The Stage 1 decoder outputs physically impossible movements (e.g., "arms rotating 360 degrees") which fail to match any database entry.
  - **Retrieval Noise:** Increasing $k$ beyond the optimal threshold (e.g., $k=4$ in HumanML3D) causes performance drops, indicating the decoder gets confused by conflicting retrieved captions.

- **First 3 experiments:**
  1. **Verify Data Pipeline & LLM Augmentation:** Before training the motion model, inspect the Falcon 40B outputs. Check if the generated low-level descriptions for the validation set are physically plausible and distinct from one another.
  2. **Ablation on $L_2$ (Contrastive Loss):** Run training with $L_2$ disabled (set $\lambda_2=0$). Check if retrieval accuracy drops significantly, confirming the necessity of fine-tuning the text encoder for the specific motion domain.
  3. **Hyperparameter Sweep on $k$:** Vary $k$ (1, 2, 3, 4) on a held-out validation set to find the "inflection point" where additional retrieved captions introduce more noise than signal.

## Open Questions the Paper Calls Out

- **Context-conditioning for ambiguity resolution:** The paper identifies that a single low-level caption can map to multiple distinct high-level motions (e.g., "waving" vs. "swatting an insect") and proposes conditioning on context as future work to resolve this ambiguity.

- **Automated filtering of LLM-generated captions:** The authors acknowledge that low-level captions generated by LLMs may contain noise and list developing automatic filtering methods as future work, as manual verification is impractical for large-scale datasets.

- **Dynamic granularity adjustment:** The paper notes that current motion-to-text models exclusively focus on generating high-level captions and aims to develop methods for adjusting the granularity of generated motion captions based on specific user requirements or use cases.

## Limitations

- The effectiveness of the method depends heavily on the quality and physical plausibility of LLM-generated low-level descriptions, which are not empirically validated in the paper.
- The paper does not provide sufficient details about the external text data sources used to enrich the database, making independent verification difficult.
- No ablation studies isolate the impact of the hierarchical architecture versus the LLM augmentation, leaving uncertainty about which component drives the performance improvements.

## Confidence

- **High Confidence:** The hierarchical two-stage generation framework (low-level → high-level) is clearly described and reproducible given the datasets. The overall improvement over baseline methods (6–50% gain in average metrics) is well-supported by the experimental results.
- **Medium Confidence:** The mechanism by which synthetic low-level descriptions improve motion-text alignment is plausible but not empirically proven—no ablation study isolates the impact of LLM augmentation versus the hierarchical architecture itself.
- **Low Confidence:** The claim that external text data sources further boost performance is not substantiated with sufficient detail to reproduce or verify independently.

## Next Checks

1. **Prompt Inspection and LLM Output Validation:** Manually inspect a random sample of 100 Falcon 40B-generated low-level descriptions for physical plausibility and diversity. If >20% contain hallucinations or are nearly identical, refine the prompt or add filtering.

2. **Retrieval Accuracy Ablation:** Disable the contrastive loss (L2) during training and measure retrieval precision@1 on a held-out validation set. A drop >15% would confirm the necessity of fine-tuning Sentence-BERT for the motion domain.

3. **Hyperparameter Sensitivity on k:** Sweep the retrieval parameter k (1, 2, 3, 4) on validation data and identify the inflection point where performance plateaus or declines, confirming the optimal balance between retrieval coverage and noise.