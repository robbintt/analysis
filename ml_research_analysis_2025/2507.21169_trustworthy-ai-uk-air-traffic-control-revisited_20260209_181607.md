---
ver: rpa2
title: 'Trustworthy AI: UK Air Traffic Control Revisited'
arxiv_id: '2507.21169'
source_url: https://arxiv.org/abs/2507.21169
tags:
- trust
- traffic
- control
- tools
- atcos
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This ethnographic study of UK air traffic control work reveals
  that trust in technology is not binary but exists as dynamic "trust contours" shaped
  by real-time calibration and shared experience. The study finds that air traffic
  controllers (ATCOs) successfully manage safety-critical operations by navigating
  these trust gradients in current tools, recognizing both dependable and undependable
  behaviors.
---

# Trustworthy AI: UK Air Traffic Control Revisited

## Quick Facts
- arXiv ID: 2507.21169
- Source URL: https://arxiv.org/abs/2507.21169
- Authors: Rob Procter; Mark Rouncefield
- Reference count: 0
- Key outcome: Trust in safety-critical tools functions as navigable "trust contours" shaped by real-time calibration and shared experience, requiring AI systems to be transparent enough for users to learn and navigate their boundaries.

## Executive Summary
This ethnographic study examines how UK air traffic controllers (ATCOs) build and calibrate trust in decision-support tools, revealing that trust exists as dynamic "trust contours" rather than binary acceptance. Through observations in operational rooms and training facilities, the research shows that ATCOs successfully manage safety-critical operations by distinguishing dependable from undependable tool behaviors and applying real-time workarounds. The study finds that informal peer knowledge sharing within watch teams is as vital as formal training for building this trust architecture. These findings have important implications for designing trustworthy AI systems in safety-critical domains where human-AI teaming is essential.

## Method Summary
The study employed ethnographic methods including observations of ATCOs in two UK air traffic operations rooms and sessions observing trainee ATCOs in simulated environments, combined with discussions with 12 ATCOs and instructors at a national air traffic training college. The researchers conducted 40-80 hours of observation across live operations and training sessions, recorded field notes on tool usage and collaboration patterns, and held semi-structured discussions (approximately 60 minutes each) that were transcribed and thematically analyzed for key themes around trust calibration, workaround strategies, and informal knowledge-sharing practices.

## Key Results
- Trust in safety-critical tools functions as navigable "trust contours" rather than binary acceptance
- Informal peer knowledge sharing within watch teams is as critical as formal training for building operational trust
- Human agency and decision-making authority must be preserved for trust contours to function effectively

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Trust in safety-critical tools functions as navigable "trust contours" rather than binary acceptance.
- **Mechanism:** Users accumulate situated experience distinguishing dependable from undependable tool behaviors, enabling real-time workarounds when tools produce spurious outputs (e.g., filtering false conflict alerts).
- **Core assumption:** AI behavior must be sufficiently patterned and transparent that users can mentally model its failure modes over time.
- **Evidence anchors:**
  - [abstract] "future AI agents must be sufficiently transparent for ATCOs to learn and navigate their trust contours, identify appropriate workarounds"
  - [section 4.2] "90% of what [the tool] does is spot on, it's just the odd thing where you think I'll be able to exercise a little bit of caution here"
  - [corpus] Weak direct support; related papers focus on AI performance/accuracy rather than trust calibration mechanisms
- **Break condition:** AI behavior becomes unpredictable or non-stationary, preventing users from forming stable mental models of failure modes.

### Mechanism 2
- **Claim:** Informal peer knowledge sharing within teams ("watches") is as critical as formal training for building operational trust.
- **Mechanism:** Watch members share tips, techniques, and tool limitations through ongoing observation and discussion, accelerating individual learning curves and building collective trust architecture.
- **Core assumption:** Social learning transmits situated knowledge that formal training cannot encode.
- **Evidence anchors:**
  - [abstract] "informal knowledge sharing among watch members is as vital as formal training in building this trust architecture"
  - [section 4.1] "I think the controllers are pretty good at sharing, you know, tips on this or that... And you talk about why. And I think it's a constant evolution"
  - [corpus] No direct corpus support for this social mechanism; adjacent papers focus on individual AI/agent performance
- **Break condition:** Team structures are disrupted, or tools become too complex for peer-to-peer knowledge transfer.

### Mechanism 3
- **Claim:** Human agency and decision-making authority must be preserved for trust contours to function.
- **Mechanism:** Users maintain active decision authority, using tools as advisory inputs rather than relinquishing control, which enables them to apply workarounds when tools are unreliable.
- **Core assumption:** Users can and will override AI recommendations when they detect unreliable behavior.
- **Evidence anchors:**
  - [abstract] "maintain decision-making agency"
  - [section 5] "What is important is that ATCOs are able to recognise and navigate their trust contours and boundaries and thus know when – as well as how – to react"
  - [corpus] "Human-in-the-Loop Testing of AI Agents" paper supports regulated assessment frameworks preserving human oversight
- **Break condition:** AI systems reduce human agency through automation bias or time pressure preventing override.

## Foundational Learning

- **Concept: Socio-technical systems**
  - Why needed here: Trust is not a property of AI alone but emerges from relationships between people, practices, and tools.
  - Quick check question: Can you identify at least one way your AI's trustworthiness depends on user workflow, not just model accuracy?

- **Concept: Situated action / "lived work"**
  - Why needed here: Understanding how users actually employ tools in real conditions—not just design intent—reveals where trust contours form.
  - Quick check question: Have you observed users working with your system in their actual operational environment?

- **Concept: Explainability as navigability (not just interpretability)**
  - Why needed here: xAI goal is enabling users to learn system boundaries, not just producing post-hoc explanations.
  - Quick check question: Can a user predict where your system will fail, based on patterns they've observed?

## Architecture Onboarding

- **Component map:**
  Agent decision engine → Output transparency layer → User interface with override capability → Telemetry/logging for post-hoc analysis → Informal feedback channels (watch/team structures)

- **Critical path:**
  1. Design agent outputs with explicit confidence/uncertainty signals
  2. Build UI affordances for override and workaround
  3. Instrument system to capture where users diverge from AI recommendations
  4. Create feedback loops for teams to share observations

- **Design tradeoffs:**
  - Transparency vs. cognitive load: More explainability may overwhelm users if not scoped to decision-relevant factors
  - Override capability vs. latency: User intervention mechanisms must not slow time-critical decisions
  - Assumption: Tradeoff optimal points vary by sector type and traffic density

- **Failure signatures:**
  - Users consistently ignoring AI recommendations (indicates lost trust or poor calibration)
  - Users never overriding AI (indicates automation bias or insufficient agency)
  - Watch teams developing divergent workaround practices (may indicate inconsistent transparency)

- **First 3 experiments:**
  1. **Contour mapping exercise:** Deploy agent in simulation, collect where ATCOs override—cluster patterns to identify systematic failure modes
  2. **Knowledge sharing audit:** Observe whether watch members spontaneously share tool tips; if not, investigate barriers
  3. **Transparency probe:** A/B test different explanation formats; measure which enables faster trust contour learning for new users

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** What specific adaptations to the current socio-technical "trust architecture" are required to successfully integrate agent-based AI tools?
- **Basis in paper:** [explicit] The authors state that their ongoing study is investigating "the kinds of adaptations that the adoption of agent-based tools may require of [the trust architecture] if their benefits are to be achieved fully."
- **Why unresolved:** The current findings are based on observations of existing tools; the specific disruptions agent-based systems will cause to the established trust dynamics are still speculative.
- **What evidence would resolve it:** Ethnographic data from operational deployments or high-fidelity simulations where ATCOs interact with functional agent-based prototypes.

### Open Question 2
- **Question:** How can Explainable AI (XAI) techniques be designed to make the "trust contours" of an agent learnable and navigable for operators?
- **Basis in paper:** [inferred] The paper argues that AI behavior must be "sufficiently transparent such that ATCOs can learn and dependably navigate their trust contours," implying current transparency methods may not support this dynamic calibration.
- **Why unresolved:** The paper identifies the goal (navigable contours) but does not define the specific interface designs or explanation mechanisms required to achieve it for complex AI agents.
- **What evidence would resolve it:** Design prototypes demonstrating that ATCOs can accurately predict agent failure modes and boundaries of competence in real-time scenarios.

### Open Question 3
- **Question:** Can the informal knowledge-sharing processes vital to safety be preserved when work practices shift toward human-AI teaming?
- **Basis in paper:** [inferred] The findings highlight that informal sharing of experiences and "tips" is essential for calibrating trust, yet it is unclear if this social dynamic persists when tools change from passive displays to active agents.
- **Why unresolved:** AI agents may alter the nature of the work such that the traditional "watch" culture and informal mentoring strategies are disrupted or rendered obsolete.
- **What evidence would resolve it:** Comparative studies of social interactions and knowledge transfer rates in watch teams using traditional tools versus those using agent-based tools.

## Limitations
- Study relies on qualitative ethnographic observations rather than quantitative measures of trust calibration or system performance
- Research conducted in UK air traffic control context with specific tools and organizational culture, limiting transferability
- Cannot establish causal mechanisms or isolate effects from confounding factors like seniority or experience

## Confidence
- **High confidence:** Trust exists as navigable contours rather than binary states; ATCOs successfully use workarounds when tools produce spurious outputs
- **Medium confidence:** Informal peer knowledge sharing is as critical as formal training for building trust architecture
- **Low confidence:** AI behavior must be sufficiently patterned and transparent for users to form stable mental models

## Next Checks
1. Conduct parallel ethnographic studies in at least two other safety-critical domains to test mechanism transferability beyond air traffic control
2. Develop metrics to measure trust contour stability over time and correlate with system performance metrics and incident rates
3. Test whether structured knowledge-sharing protocols improve trust calibration speed compared to organic informal sharing, controlling for experience level and tool familiarity