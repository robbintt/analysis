---
ver: rpa2
title: Generalist Large Language Models Outperform Clinical Tools on Medical Benchmarks
arxiv_id: '2512.01191'
source_url: https://arxiv.org/abs/2512.01191
tags:
- clinical
- openevidence
- generalist
- tools
- uptodate
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Specialized clinical AI tools (OpenEvidence, UpToDate Expert AI)
  were compared to generalist LLMs (GPT-5, Gemini 3 Pro, Claude Sonnet 4.5) on 1,000
  medical questions combining MedQA (knowledge) and HealthBench (clinician-alignment)
  tasks. GPT-5 achieved the highest accuracy at 96.2% on MedQA and 97.0% on HealthBench.
---

# Generalist Large Language Models Outperform Clinical Tools on Medical Benchmarks

## Quick Facts
- arXiv ID: 2512.01191
- Source URL: https://arxiv.org/abs/2512.01191
- Authors: Krithik Vishwanath; Mrigayu Ghosh; Anton Alyakin; Daniel Alexander Alber; Yindalon Aphinyanaphongs; Eric Karl Oermann
- Reference count: 0
- Primary result: GPT-5 outperformed clinical tools on medical benchmarks, achieving 96.2% accuracy on MedQA vs. 89.6% for OpenEvidence.

## Executive Summary
A direct comparison of generalist LLMs (GPT-5, Gemini 3 Pro, Claude Sonnet 4.5) and specialized clinical tools (OpenEvidence, UpToDate Expert AI) on 1,000 medical questions revealed that frontier models significantly outperformed clinical tools on both knowledge-based (MedQA) and clinician-aligned (HealthBench) tasks. GPT-5 achieved the highest accuracy across all benchmarks, with clinical tools showing notable deficits in completeness, communication quality, and context awareness. These findings suggest that widely deployed clinical AI tools may lag behind generalist models on key medical evaluation metrics, raising questions about their readiness for clinical deployment without further validation.

## Method Summary
The study evaluated five models on a 1,000-question benchmark combining MedQA (USMLE-style questions) and HealthBench (rubric-scored prompts). Models were queried manually through their respective interfaces using identical prompts. GPT-4.1 served as the grading model, with responses evaluated on accuracy, completeness, communication quality, and context awareness. Paired statistical tests (McNemar's test for categorical accuracy, Wilcoxon's test for ranked scores) were used to assess significance. The study focused on single-turn responses, excluding multi-turn dialogue scenarios.

## Key Results
- GPT-5 achieved 96.2% accuracy on MedQA and 97.0% on HealthBench, outperforming clinical tools.
- OpenEvidence scored 89.6% on MedQA and 74.5% on HealthBench; UpToDate scored 88.4% and 75.1%, respectively.
- Generalist models outperformed clinical tools by 1.23x on HealthBench (p=0.023).
- Clinical tools showed deficits in completeness (P<10⁻⁴), communication quality (P=0.002), and context awareness (P=0.012).

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Generalist LLMs outperform specialized clinical tools on medical benchmarks, potentially due to scale and advanced alignment rather than domain-specific training.
- Mechanism: Frontier models leverage substantially larger training corpora and refined alignment techniques, enabling superior performance on knowledge retrieval and reasoning tasks. The paper notes: "Frontier generalist models benefit from substantially larger training corpora and more advanced alignment, which may enable them to match or exceed specialist performance."
- Core assumption: Scale and alignment quality outweigh domain-specific curation for the medical tasks tested.
- Evidence anchors:
  - [abstract] "Generalist models consistently outperformed clinical tools, with GPT-5 achieving the highest scores."
  - [Results] GPT-5 achieved 96.2% MedQA accuracy vs. 89.6% (OpenEvidence) and 88.4% (UpToDate).
  - [corpus] "Generalist Foundation Models Are Not Clinical Enough for Hospital Operations" (arXiv:2511.13703) suggests domain specificity may matter for operational tasks—implying the advantage may not generalize to all clinical contexts.
- Break condition: If clinical tools have access to proprietary datasets or real-time EHR integration not captured in benchmark settings, performance gaps may narrow or reverse in deployment.

### Mechanism 2
- Claim: RAG-based systems may underperform when retrieval fails or integration is poorly executed.
- Mechanism: Clinical tools relying on RAG can suffer performance degradation if retrieved documents are irrelevant or the base model fails to synthesize them correctly. The Discussion states: "RAG, which both OpenEvidence and UpToDate Expert AI heavily rely on, can hurt model performance if the wrong material is retrieved or if it is poorly integrated by the base model."
- Core assumption: Retrieval failures occur frequently enough on standardized benchmarks to impact aggregate scores.
- Evidence anchors:
  - [Discussion] Explicit reference to RAG integration issues (citations 7, 10, 11).
  - [Results] Clinical tools scored lowest on "completeness" and "context awareness" axes.
  - [corpus] "TAGS: A Test-Time Generalist-Specialist Framework" (arXiv:2505.18283) notes fine-tuned medical LLMs "suffer from poor generalization under distribution shifts"—consistent with retrieval brittleness.
- Break condition: If retrieval corpora are better matched to benchmark content, or if multi-round retrieval compensates for initial misses, this mechanism weakens.

### Mechanism 3
- Claim: Specialized tools exhibit deficits in systems-based safety reasoning and communication quality.
- Mechanism: Clinical tools underperformed specifically on "Systems-based Practice Questions, including Patient Safety" (Supplemental Figure 2) and communication axes, suggesting training or retrieval focus may emphasize factual recall over holistic clinical reasoning.
- Core assumption: Benchmarks accurately capture clinically relevant dimensions of safety and communication.
- Evidence anchors:
  - [Results] "Clinical tools perform worse on Systems-based Practice Questions, including Patient Safety (P<1×10⁻⁴)."
  - [Results] Grouped analysis showed superiority of generalist models in completeness (P<10⁻⁴), communication quality (P=0.002), and context awareness (P=0.012).
  - [corpus] "First, do NOHARM" (arXiv:2512.01241) provides complementary evidence on clinical safety benchmarking, reinforcing that safety remains under-characterized across LLMs.
- Break condition: If HealthBench rubrics favor verbose or generalized responses typical of frontier models, bias may inflate differences.

## Foundational Learning

- Concept: **Retrieval-Augmented Generation (RAG)**
  - Why needed here: Both clinical tools tested rely on RAG; understanding its failure modes is critical to interpreting performance gaps.
  - Quick check question: Can you explain two ways retrieval can degrade final output quality?

- Concept: **Benchmark Alignment vs. Knowledge Testing**
  - Why needed here: The study uses MedQA (knowledge) and HealthBench (clinician-alignment)—distinct constructs requiring different evaluation mindsets.
  - Quick check question: What is the difference between testing factual accuracy and testing alignment with expert judgment?

- Concept: **Statistical Comparison of Paired Model Outputs**
  - Why needed here: The paper uses McNemar's test (for paired categorical data) and Wilcoxon's test (for ranked scores); understanding these clarifies how significance was established.
  - Quick check question: Why would paired tests be preferred when comparing models on the same question set?

## Architecture Onboarding

- Component map: Clinical tools (RAG-based systems) -> curated medical corpora -> browser interfaces; Generalist models (API-accessed frontier LLMs) -> proprietary training and alignment -> evaluation pipeline
- Critical path: Define benchmark -> query all models uniformly -> collect responses -> score via rubric -> apply paired statistical tests (McNemar, Wilcoxon) -> report with confidence intervals
- Design tradeoffs:
  - Manual querying of clinical tools limits sample size but ensures fair access conditions.
  - Using GPT-4.1 for grading introduces potential grader bias, though prior work supports reliability.
  - Restricting HealthBench to single-turn prompts excludes multi-turn clinical dialogue evaluation.
- Failure signatures:
  - Clinical tools showing high factual accuracy but low completeness/context awareness suggests narrow retrieval scope.
  - Significant underperformance on "emergency referrals" and "expertise-tailored communication" themes indicates retrieval or generation failures in high-stakes or audience-adapted contexts.
- First 3 experiments:
  1. Replicate with stratified sampling across clinical specialties to test whether gaps persist uniformly or cluster in specific domains.
  2. Conduct ablation on RAG retrieval depth (e.g., top-k variation) for clinical tools to isolate retrieval vs. generation contributions to errors.
  3. Introduce adversarial or out-of-distribution cases to test whether generalist robustness advantage holds under distribution shift.

## Open Questions the Paper Calls Out
None

## Limitations
- Benchmark representativeness: MedQA and HealthBench may not fully capture real-world clinical workflows, particularly those involving multi-turn dialogue, EHR integration, or real-time decision support.
- Grader bias: GPT-4.1 served as the grading model, which may introduce systematic bias favoring certain reasoning or communication styles.
- Retrieval domain mismatch: Clinical tools rely on proprietary or curated corpora that may differ substantially from benchmark domains, potentially underestimating their true utility.

## Confidence

- **High Confidence**: Generalist models outperform clinical tools on MedQA and HealthBench metrics in this controlled evaluation.
- **Medium Confidence**: Performance gaps stem primarily from scale/alignment advantages and RAG retrieval failures; real-world deployment may alter this balance.
- **Low Confidence**: Extrapolation to clinical safety, multi-turn reasoning, or operational hospital tasks is not directly supported by these results.

## Next Checks
1. **Specialty-stratified replication**: Test whether performance gaps persist uniformly across medical specialties or cluster in specific domains (e.g., emergency medicine, primary care).
2. **Adversarial/distribution shift testing**: Introduce out-of-distribution or adversarial cases to assess whether generalist robustness advantages hold under clinical stress scenarios.
3. **RAG ablation study**: Systematically vary retrieval depth (e.g., top-k documents) in clinical tools to isolate retrieval vs. generation contributions to errors.