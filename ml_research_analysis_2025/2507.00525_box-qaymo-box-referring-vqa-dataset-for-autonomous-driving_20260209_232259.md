---
ver: rpa2
title: 'Box-QAymo: Box-Referring VQA Dataset for Autonomous Driving'
arxiv_id: '2507.00525'
source_url: https://arxiv.org/abs/2507.00525
tags:
- driving
- questions
- motion
- autonomous
- senna
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the gap in evaluating vision-language models
  for autonomous driving, focusing on their ability to communicate perceptual understanding
  to users. The authors introduce Box-QAymo, a novel dataset and benchmark that enables
  localized, user-driven queries by drawing bounding boxes on objects.
---

# Box-QAymo: Box-Referring VQA Dataset for Autonomous Driving

## Quick Facts
- **arXiv ID:** 2507.00525
- **Source URL:** https://arxiv.org/abs/2507.00525
- **Authors:** Djamahl Etchegaray; Yuxia Fu; Zi Huang; Yadan Luo
- **Reference count:** 38
- **Primary result:** Introduces Box-QAymo, a dataset and benchmark for evaluating VLMs' ability to answer localized queries in autonomous driving scenarios using red bounding boxes as visual prompts.

## Executive Summary
This paper addresses the gap in evaluating vision-language models for autonomous driving, focusing on their ability to communicate perceptual understanding to users. The authors introduce Box-QAymo, a novel dataset and benchmark that enables localized, user-driven queries by drawing bounding boxes on objects. The dataset features a hierarchical evaluation protocol, progressing from binary sanity checks to attribute prediction and complex motion reasoning over inter-object dynamics across frames. The dataset is built using crowd-sourced fine-grained object classes and visual attributes, with rigorous quality control ensuring robustness. Experiments on general-purpose and domain-specific VLMs reveal significant limitations, especially in motion understanding, highlighting the need for improved models. The results show a substantial performance gap between binary and complex reasoning tasks, with finetuning providing improvements. The dataset and benchmark provide a foundation for developing more robust and interpretable autonomous driving systems that can effectively communicate with users.

## Method Summary
Box-QAymo is built on the Waymo Open Dataset validation set, using 202 scenes split into 101 for finetuning and 101 for validation. The dataset includes 1,662 binary, 5,403 attribute, and 13,714 motion QA pairs, generated using hierarchical templates and programmatic QA pair construction. Crowd-sourced annotations using CVAT provide fine-grained object classes and visual attributes following the Argoverse 2.0 taxonomy for approximately 50% of objects. Objects are exported as 3x3 crop galleries with 3D boxes overlaid. Quality control includes negative sampling, temporal consistency checks, and difficulty stratification. Evaluation uses steering prompts to constrain model output format, followed by a multistage response parser. LoRA fine-tuning (rank=128, alpha=256, 1 epoch) is applied to improve model performance.

## Key Results
- Significant performance gap exists between binary (66.1% F1) and complex reasoning tasks (18.3% attribute, 37.6% motion).
- Single-frame inputs consistently outperform two-frame inputs across all models (-5.42% to -12.16% F1).
- Qwen-VL benefits from visual grounding via red bounding boxes (+1.39% avg F1), while LLaVA shows mixed results (-1.28% on fine-grained classification).
- LoRA fine-tuning provides substantial improvements, particularly for trajectory analysis (>90% F1 on trajectory questions).

## Why This Works (Mechanism)

### Mechanism 1: Hierarchical Complexity Assessment
- **Claim:** Structuring evaluation from binary sanity checks → attribute prediction → motion reasoning isolates specific capability failures in VLMs.
- **Mechanism:** Binary questions validate basic visual-language alignment; attribute questions require fine-grained semantic discrimination; motion questions demand spatiotemporal integration. Progressive degradation reveals where reasoning breaks down.
- **Core assumption:** Task difficulty scales predictably, and performance drops indicate genuine capability gaps rather than evaluation artifacts.
- **Evidence anchors:**
  - [Section 4.3] "Average F1 scores decrease substantially from binary questions (66.1%) to attribute questions (18.3%) and motion questions (37.6%). This strongly validates our complexity assumptions."
  - [Section 3.2] "This design enables systematic evaluation of model capabilities spanning different levels of visual understanding."
  - [Corpus] "Are VLMs Ready for Autonomous Driving?" finds similar reliability concerns, suggesting hierarchical evaluation exposes systematic weaknesses across the field.
- **Break condition:** If models show inconsistent performance patterns across hierarchies (e.g., high motion but low attribute scores), the complexity ordering assumption fails.

### Mechanism 2: Visual Prompt Grounding via Bounding Boxes
- **Claim:** Red bounding boxes as visual markers improve object-level reasoning by providing explicit spatial references that bypass numerical coordinate interpretation failures.
- **Mechanism:** VLMs struggle with precise coordinate parsing but respond well to visual prompts. Boxes create a shared attention locus between user intent and model processing, enabling instance-grounded queries.
- **Core assumption:** Models have sufficient visual acuity to detect and track marked regions, and box artifacts don't overwhelm signal.
- **Evidence anchors:**
  - [Section 3.2.2] "Building on evidence that even simple visual prompts, such as red circles, can effectively guide VLM attention, we design instance-grounded questions that use red bounding boxes to highlight target objects."
  - [Section 4.3, Table 6] Qwen-VL shows consistent improvement with boxes (+1.39% avg F1), but LLaVA shows mixed results (-1.28% on fine-grained classification), suggesting architectural sensitivity.
  - [Corpus] Corpus papers lack direct comparison of visual grounding methods; evidence remains limited to this work's findings.
- **Break condition:** If box-free queries outperform box-referenced ones consistently, the grounding hypothesis is inverted.

### Mechanism 3: Negative Sampling and Temporal Consistency Filtering
- **Claim:** Rigorous quality control through negative sampling and temporal consistency checks prevents trivial solutions and ensures questions require genuine reasoning.
- **Mechanism:** Negative samples create contrastive pressure—models must distinguish matching from non-matching cases. Temporal filtering (interpolation, multi-frame convergence, smooth velocity) ensures motion questions reflect physically plausible dynamics.
- **Core assumption:** Generated negatives are semantically meaningful rather than superficially different, and interpolation doesn't introduce artifacts.
- **Evidence anchors:**
  - [Section 3.3] "We find for many trajectory questions negative samples are much easier to come by than positive ones, as criteria for determining if a trajectory matches a question are usually quite strict."
  - [Section 3.3] "Selecting timestamps for questions based on temporal convergence patterns rather than instantaneous criteria."
  - [Corpus] No direct corpus evidence on this specific mechanism; related work focuses on dataset creation without detailed quality control analysis.
- **Break condition:** If models achieve high accuracy through statistical shortcuts (e.g., surface-level correlations) despite quality control, the filtering is insufficient.

## Foundational Learning

- **Concept: Visual Grounding / Referring Expression Comprehension**
  - **Why needed here:** Box-QAymo's core innovation requires understanding how models link visual regions to linguistic queries. Without this foundation, you can't interpret why box-referencing works (or fails).
  - **Quick check question:** Can you explain why "red circle" visual prompts might work better than text coordinates for VLMs?

- **Concept: Spatiotemporal Reasoning in Vision**
  - **Why needed here:** Motion questions require understanding object trajectories across frames. The paper's finding that two-frame inputs degrade performance suggests current architectures lack effective temporal integration mechanisms.
  - **Quick check question:** What's the difference between inferring motion from single-frame cues (motion blur, wheel orientation) vs. explicit frame-to-frame comparison?

- **Concept: LoRA (Low-Rank Adaptation) Fine-tuning**
  - **Why needed here:** All successful improvements in the paper come from LoRA fine-tuning (rank 128, alpha 256). Understanding parameter-efficient adaptation is essential for reproducing results.
  - **Quick check question:** Why might LoRA be preferred over full fine-tuning for domain-specific VLM adaptation?

## Architecture Onboarding

- **Component map:** Waymo Open Dataset -> CVAT annotation -> Hierarchical QA generation -> Quality filtering -> LoRA fine-tuning -> Evaluation harness
- **Critical path:** Annotation quality → Question validity → Parser robustness. If any stage fails (e.g., 34% Senna parsing failures), downstream metrics are uninterpretable.
- **Design tradeoffs:**
  - **Single-frame vs. two-frame:** Paper shows single-frame consistently outperforms two-frame (-5.42% to -12.16% F1). Trade-off: temporal context vs. integration complexity.
  - **Box grounding:** Architecture-dependent (Qwen-VL benefits, LLaVA mixed). Choose based on cross-attention capability and ViT encoder size.
  - **Fine-grained classes:** Richer semantics but higher annotation cost and lower model accuracy.
- **Failure signatures:**
  - **Format non-adherence:** Senna's 34% valid response rate indicates overfitting to narrow task formats.
  - **Hierarchical collapse:** If binary ≈ attribute ≈ motion performance, evaluation isn't discriminating.
  - **Temporal degradation:** Two-frame performance drop signals architectural inability to integrate short-term temporal information.
- **First 3 experiments:**
  1. **Baseline sanity check:** Run LLaVA-1.5 7B and Qwen-VL 7B on binary questions only. Confirm F1 > 60% before proceeding.
  2. **Box-grounding ablation:** Test Qwen-VL on attribute questions with and without red bounding boxes. Expect +1-3% F1 improvement with boxes.
  3. **LoRA fine-tuning on single-frame:** Fine-tune LLaVA with rank 128/alpha 256, 1 epoch. Validate trajectory analysis improvement (target: >90% F1 based on Table 4).

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can Vision-Language Models (VLMs) be adapted to effectively integrate short-term temporal context (e.g., consecutive frame pairs) for motion reasoning?
- **Basis in paper:** [explicit] The authors state that providing two consecutive frames "consistently degrade[s] performance across all models," suggesting current VLMs struggle to process frame pairs compared to single images or long video sequences due to training paradigm misalignments.
- **Why unresolved:** The paper identifies the failure of two-frame inputs but does not propose architectural or training solutions to bridge the gap between single-image processing and temporal reasoning.
- **What evidence would resolve it:** Demonstrating a training regime or architectural modification where models utilizing consecutive frames significantly outperform single-frame baselines on motion reasoning tasks.

### Open Question 2
- **Question:** What specific architectural components determine whether a VLM treats visual markers (bounding boxes) as grounding cues or visual noise?
- **Basis in paper:** [explicit] The authors note that "Qwen-VL’s cross-attention mechanism... enable[s] better separation of visual grounding cues... while LLaVA’s simpler MLP projection... struggles to filter box-induced noise," leading to inconsistent grounding effectiveness.
- **Why unresolved:** The paper establishes a correlation between architecture types and grounding success but does not isolate the specific causal mechanisms or necessary modifications for models like LLaVA.
- **What evidence would resolve it:** Ablation studies replacing MLP projection layers with cross-attention mechanisms in LLaVA to observe if box-grounding performance improves to match or exceed Qwen-VL.

### Open Question 3
- **Question:** How can domain-specific driving models be trained to retain robust instruction-following capabilities without suffering from the brittleness associated with narrow task optimization?
- **Basis in paper:** [explicit] The authors report that the driving-specialized model Senna exhibited "brittleness of narrow task training," failing to adapt to structured Q&A and achieving only 34% valid responses compared to general models.
- **Why unresolved:** The paper highlights the trade-off between domain specialization and general adaptability but leaves unexplored the methods to balance specific driving tasks with broad interaction capabilities.
- **What evidence would resolve it:** A training methodology that yields a model capable of high performance on specialized planning tasks while maintaining the high response validity and hierarchical reasoning of general-purpose models.

### Open Question 4
- **Question:** To what extent do VLMs rely on static visual artifacts (e.g., motion blur) rather than genuine temporal dynamics to answer motion reasoning questions?
- **Basis in paper:** [inferred] The authors observe that performance on motion tasks was higher than attribute tasks despite motion being logically more complex, suggesting models "may rely on static visual cues rather than true temporal understanding."
- **Why unresolved:** This inference explains the counter-intuitive performance hierarchy, but the paper does not isolate the visual features the models actually use to derive answers.
- **What evidence would resolve it:** Evaluating models on motion datasets where static motion cues (blur, wheel orientation) are removed or artificially decoupled from the actual object trajectory.

## Limitations
- Performance gaps may not generalize to other autonomous driving VLMs or datasets beyond Box-QAymo.
- Dataset limited to Waymo Open Dataset validation split (202 scenes), potentially restricting representativeness.
- Uncertain generalizability of negative sampling effectiveness and temporal consistency filtering to other contexts.

## Confidence
- **High:** The dataset construction methodology, including quality control procedures and the use of Argoverse 2.0 taxonomy, is well-documented and reproducible.
- **Medium:** The observed performance gaps between model architectures (Qwen-VL vs LLaVA) are consistent but may be influenced by architectural differences not fully explored.
- **Low:** The generalizability of negative sampling effectiveness and temporal consistency filtering beyond the Waymo dataset context.

## Next Checks
1. Test the hierarchical evaluation protocol on a held-out subset of the Waymo dataset to verify that performance degradation patterns persist across different scene distributions.
2. Conduct cross-dataset validation by applying the same question types to nuScenes or KITTI to assess if the observed model limitations are dataset-specific.
3. Implement an ablation study removing the red bounding box visual prompts to quantify their actual contribution to model performance across different VLM architectures.