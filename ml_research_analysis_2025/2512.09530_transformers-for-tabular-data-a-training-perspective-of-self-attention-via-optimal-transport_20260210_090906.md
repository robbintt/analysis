---
ver: rpa2
title: 'Transformers for Tabular Data: A Training Perspective of Self-Attention via
  Optimal Transport'
arxiv_id: '2512.09530'
source_url: https://arxiv.org/abs/2512.09530
tags:
- transformer
- data
- training
- optimal
- page
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This thesis investigates self-attention training through the lens
  of Optimal Transport (OT) and develops an OT-based alternative for tabular classification.
  The study tracks intermediate projections of the self-attention layer during training
  and evaluates their evolution using discrete OT metrics.
---

# Transformers for Tabular Data: A Training Perspective of Self-Attention via Optimal Transport

## Quick Facts
- arXiv ID: 2512.09530
- Source URL: https://arxiv.org/abs/2512.09530
- Authors: Antonio Candelieri; Alessandro Quadrio
- Reference count: 0
- Primary result: OT-based alternative to Transformers achieves comparable accuracy with lower computational cost on tabular classification tasks

## Executive Summary
This thesis investigates self-attention training through the lens of Optimal Transport (OT) and develops an OT-based alternative for tabular classification. The study tracks intermediate projections of the self-attention layer during training and evaluates their evolution using discrete OT metrics. Experiments are conducted on classification tasks with two and three classes, as well as on a biomedical dataset.

Results indicate that the final self-attention mapping often approximates the OT optimal coupling, yet the training trajectory remains inefficient. Pretraining the MLP section on synthetic data partially improves convergence but is sensitive to their initialization. To address these limitations, an OT-based algorithm is introduced: it generates class-specific dummy Gaussian distributions, computes an OT alignment with the data, and trains an MLP to generalize this mapping.

## Method Summary
The paper investigates self-attention training using Optimal Transport metrics and proposes an OT-based alternative to Transformers for tabular classification. The approach involves generating synthetic class-specific Gaussian distributions, computing discrete OT alignments between real and dummy data, and training an MLP to generalize this mapping. Three models are compared: standard Transformer, pretrained Transformer with frozen MLP, and the OT alternative. The OT model classifies new inputs by computing distances to dummy centroids and applying softmax. Experiments use standardized 2D synthetic Gaussian data and a Bangalore EEG dataset, with training performed via full-batch gradient descent and R implementation using transport, keras3, and tensorflow packages.

## Key Results
- Final self-attention mapping often approximates OT optimal coupling, but training trajectory is inefficient
- OT-based alternative achieves comparable accuracy to Transformers while reducing computational cost
- Bangalore EEG dataset: OT model achieved 100% accuracy with 8.62s computational time for binary classification, outperforming Transformers (92.5% accuracy, 20.59s)

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Self-attention progressively remaps input data toward configurations that approximate optimal transport couplings, making the downstream classification task more tractable for the MLP.
- **Mechanism:** The attention matrix P = softmax(QK^T/√d) acts as a stochastic coupling that redistributes probability mass across input representations. Gradient descent iteratively modifies P to reshape the latent space so that class separability increases for the MLP classifier.
- **Core assumption:** The information-reshaping function of self-attention can be meaningfully analyzed as a mass transport problem between input distributions and their transformed outputs.
- **Evidence anchors:**
  - [abstract] "Results indicate that the final self-attention mapping often approximates the OT optimal coupling"
  - [Section 3, p.11-12] "SA can be understood as a remapping of the input data within the same latent space... training the SA block can be understood as searching for the optimal remapping"
  - [corpus] Related work (Sander et al., 2022) shows SA matrices tend toward doubly stochastic structure during training
- **Break condition:** If the ground cost (distance metric) is not meaningful for the data structure (e.g., mixed categorical/continuous features without proper encoding), the OT interpretation loses utility.

### Mechanism 2
- **Claim:** Standard gradient-based SA training reaches near-optimal final mappings but follows highly inefficient trajectories, with cumulative transport cost far exceeding the direct Wasserstein distance.
- **Mechanism:** Joint optimization of SA parameters and MLP weights causes gradient descent to simultaneously reshape representations and adjust classification boundaries. This coupling introduces wandering trajectories: at each iteration, gradients optimize both components rather than pursuing a direct transport path.
- **Core assumption:** Training inefficiency can be quantified by comparing stepwise transport cost to the optimal (Wasserstein) cost, as captured by the efficiency metric = Transformer Distance / Transformer Path.
- **Evidence anchors:**
  - [abstract] "training trajectory remains inefficient"
  - [Tables 3.2, 3.4] Efficiency values consistently low (0.008–0.017) across all conditions; Optimality near 1.000 but Transformer Cost ~50–100× higher than Wasserstein distance
  - [Section 3.1.1, p.16] "the trajectory it follows to get there is highly inefficient... gradient descent simultaneously optimizes both the SA blocks and the MLP block"
  - [corpus] Limited direct corpus evidence on trajectory inefficiency in Transformers; related work focuses on convergence rates (Gao et al., 2024) rather than path efficiency
- **Break condition:** When classification is trivial (large Wasserstein separation between classes), inefficiency is masked by rapid convergence regardless of path quality.

### Mechanism 3
- **Claim:** Pre-computing the optimal transport mapping between input data and synthetic class-specific Gaussian targets, then training an MLP to generalize this mapping, achieves comparable accuracy with substantially lower computational cost.
- **Mechanism:** Rather than learning a remapping through iterative gradient descent on attention parameters: (1) generate well-separated dummy Gaussian distributions per class, (2) solve discrete OT to obtain optimal matching, (3) train MLP to learn T: X → X_dummy, (4) classify new inputs by distance to dummy centroids. This bypasses the inefficient joint optimization.
- **Core assumption:** An MLP can successfully generalize the discrete OT point-wise correspondences to the continuous input space, and Euclidean distance to well-designed dummy centroids is a meaningful classification signal.
- **Evidence anchors:**
  - [abstract] "OT-based algorithm generates class-specific dummy Gaussians, computes OT alignment, and trains an MLP... achieves accuracy comparable to Transformers while reducing computational cost"
  - [Section 4.3, Table 4.4] Bangalore EEG dataset: OT model achieves 1.000 accuracy (2-class) in 8.62s vs. Transformer 0.925 in 20.59s
  - [Tables 4.2, 4.3] Computational time ~4–5s vs. ~22–70s for Transformer; accuracy comparable when classes are well-separated
  - [corpus] Limited corpus evidence on this specific OT-MLP hybrid approach; related work addresses OT for attention regularization (ESPFormer) but not full replacement
- **Break condition:** Performance degrades when: (a) dummy distribution geometry conflicts with natural data structure, (b) label space lacks meaningful spatial relationships for centroid-based classification, (c) number of classes exceeds dimensions available for equidistant centroid placement.

## Foundational Learning

- **Concept: Discrete Optimal Transport (Kantorovich formulation)**
  - **Why needed here:** The paper operationalizes OT through discrete couplings between point clouds; understanding transport plans P, marginals, and the Wasserstein distance is essential for interpreting the metrics (Matching, Monge Gap, Efficiency).
  - **Quick check question:** Given two discrete uniform distributions with n points each, can you explain why the Monge problem reduces to a permutation while Kantorovich allows mass splitting?

- **Concept: Self-Attention as stochastic matrix operations**
  - **Why needed here:** The thesis reframes attention not as "feature extraction" but as a row-stochastic matrix remapping inputs. Understanding Q, K, V projections and the softmax normalization is prerequisite to grasping the OT analogy.
  - **Quick check question:** If P = softmax(QK^T/√d) and V is the value matrix, what does each row of the output PV represent in terms of the input tokens?

- **Concept: Push-forward operator (T#μ)**
  - **Why needed here:** The mathematical formalization of how a map T transforms a measure μ is central to understanding OT as a remapping framework; Appendix A defines this explicitly.
  - **Quick check question:** If μ = Σ aᵢδₓᵢ and T is a deterministic map, what is T#μ in terms of the original support points?

## Architecture Onboarding

- **Component map:**
  Input Data (scaled) -> [OT Alternative Path] Dummy Gaussian Generation (per class) -> OT Solver (short simplex) -> MLP Training (T: X→X_dummy) -> Classification: fθ(x_new) -> compute distances to dummy centroids -> softmax
  [Transformer Baseline Path] Input -> SA layers (learned P matrices) -> MLP classifier -> softmax

- **Critical path:**
  1. **Dummy geometry design** — Most consequential design choice; centroids must be well-separated in a space where Euclidean distance is meaningful
  2. **OT solver selection** — Paper uses short simplex algorithm; for larger datasets, batching required (as in Bangalore EEG experiments with batch size 50)
  3. **MLP architecture** — Simpler than Transformer (32, 32 units sufficient); primary function is function approximation, not representation learning

- **Design tradeoffs:**
  - **Efficiency vs. transferability:** OT model is task-specific and requires geometry design; standard Transformer is architecture-agnostic across tasks
  - **Standardization requirement:** OT model performs best when features have comparable scale; paper explicitly notes standardized inputs are prerequisite
  - **Scalability with labels:** As class count increases, equidistant centroid placement becomes geometrically constrained; dimension must grow with labels

- **Failure signatures:**
  - **Pretrained Transformer with misaligned dummies:** Accuracy drops from ~1.0 to ~0.5–0.6 when dummy Gaussians are rotated away from favorable alignment (Tables 3.10, 3.11, 3.12)
  - **High Monge Gap values (>5):** Indicates final mapping deviates substantially from OT-optimal; signals either training instability or incompatible dummy geometry
  - **Efficiency < 0.01 with high Transformer Cost:** Standard Transformer signature — model reached good solution via highly circuitous path

- **First 3 experiments:**
  1. **Reproduce 2-class synthetic experiment:** Generate data from two Gaussians with Wasserstein distance = 4; train both standard Transformer and OT model; verify that (a) both achieve ~1.0 instance-wise accuracy, (b) OT model runs in ~4s vs. ~22s for Transformer
  2. **Dummy geometry sensitivity test:** Fix data; rotate dummy centroids at 0°, 90°, 180°; measure accuracy degradation to confirm initialization sensitivity claim
  3. **Real data baseline:** Apply to Bangalore EEG (or similar preprocessed tabular dataset); compare Transformer vs. OT model across 2, 3, 4 label configurations; observe Transformer degradation with label count vs. OT model stability

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can Wasserstein Gradient Flow provide a more efficient training trajectory for self-attention than conventional stochastic gradient descent?
- **Basis in paper:** [explicit] "A possible workaround is offered by Wasserstein Gradient Flow (Santambrogio (2017)), but a detailed exploration of this direction is left for future work"
- **Why unresolved:** The paper identifies training inefficiency but only analyzes existing SGD trajectories; Wasserstein Gradient Flow is proposed as an alternative without empirical testing.
- **What evidence would resolve it:** Comparative experiments tracking OT metrics (efficiency, Monge gap) between models trained via WGF versus SGD on identical classification tasks.

### Open Question 2
- **Question:** What proportion of practical machine learning tasks have input geometries exploitable by OT-based remapping methods?
- **Basis in paper:** [explicit] "Questions remain on how many tasks fall in this category, but because of the versatility of discrete Optimal Transport there is reason to believe that this method could be widely applicable"
- **Why unresolved:** The paper demonstrates success on tabular and EEG data but does not systematically characterize the task properties required for OT methods to outperform Transformers.
- **What evidence would resolve it:** Large-scale benchmarking across diverse data modalities (text, images, mixed-type tabular) with analysis of when OT-based approaches fail versus succeed.

### Open Question 3
- **Question:** How can dummy distribution geometry be optimally designed for problems with large label spaces (K > 10)?
- **Basis in paper:** [inferred] The paper notes: "the initialization of the dummy distributions becomes critical as the number of labels increases... this becomes increasingly difficult as the number of labels grows, since the dimension required to maintain such a configuration may exceed that of the input space"
- **Why unresolved:** The proposed alternative relies on well-separated Gaussian dummies; performance degrades with more labels, and no principled solution is provided.
- **What evidence would resolve it:** Systematic experiments with alternative dummy geometries (learned embeddings, hierarchical structures) across increasing label counts, measuring accuracy and computational trade-offs.

## Limitations

- OT alternative performance highly sensitive to dummy Gaussian geometry design, with no principled guidance for high-cardinality label spaces
- Pretraining experiments show inconsistent gains, suggesting approach may not scale beyond low-dimensional, well-separated synthetic data
- Claims of computational cost reduction are dataset-dependent and may not hold for high-cardinality or continuous-label problems

## Confidence

- **High Confidence:** Final OT mapping approximates optimal coupling; OT alternative achieves comparable accuracy with lower runtime on tested datasets
- **Medium Confidence:** Standard Transformer training follows inefficient trajectories; pretraining MLP on synthetic data offers inconsistent improvements
- **Low Confidence:** OT alternative generalizes robustly across diverse tabular problems; dummy geometry design principles scale beyond small synthetic datasets

## Next Checks

1. **Geometric Sensitivity Test:** Systematically vary dummy Gaussian placement (rotation, scaling, translation) and measure accuracy drop across datasets to quantify initialization fragility
2. **High-Cardinality Label Test:** Evaluate OT alternative on datasets with >10 classes to assess scalability limits and geometric constraints of centroid placement
3. **Semantic OT Metric Validation:** Compare OT metric trajectories against task-specific metrics (e.g., feature importance scores) to confirm that transport-based analysis captures meaningful learning dynamics