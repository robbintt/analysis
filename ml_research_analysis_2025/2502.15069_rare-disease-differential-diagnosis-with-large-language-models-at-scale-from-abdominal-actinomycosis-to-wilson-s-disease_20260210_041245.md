---
ver: rpa2
title: 'Rare Disease Differential Diagnosis with Large Language Models at Scale: From
  Abdominal Actinomycosis to Wilson''s Disease'
arxiv_id: '2502.15069'
source_url: https://arxiv.org/abs/2502.15069
tags:
- disease
- rare
- patient
- diseases
- diagnosis
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: RareScale improves LLM-based rare disease diagnosis by combining
  a smaller rare disease candidate generation model with a larger black-box LLM. The
  approach generates synthetic history-taking conversations from an expert system,
  trains a model to propose relevant rare diseases, and uses these candidates to enhance
  LLM differential diagnoses.
---

# Rare Disease Differential Diagnosis with Large Language Models at Scale: From Abdominal Actinomycosis to Wilson's Disease

## Quick Facts
- arXiv ID: 2502.15069
- Source URL: https://arxiv.org/abs/2502.15069
- Reference count: 40
- RareScale improves LLM-based rare disease diagnosis by combining a smaller rare disease candidate generation model with a larger black-box LLM, achieving up to 74.1% Top-5 accuracy and 37% improvement in exact match rates.

## Executive Summary
RareScale is a novel approach for improving rare disease differential diagnosis using large language models (LLMs) at scale. The method combines a specialized rare disease candidate generation model with a larger black-box LLM to enhance diagnostic accuracy without overwhelming the diagnostic process. By generating synthetic history-taking conversations from an expert system and training a model to propose relevant rare diseases, RareScale significantly improves LLM performance on rare disease diagnosis tasks. The approach demonstrates substantial gains in Top-5 accuracy (from 56.8% to 74.1%) and exact match rates, validating the effectiveness of integrating specialized candidate generation into LLM-based diagnostic systems.

## Method Summary
RareScale addresses the challenge of rare disease diagnosis by combining a smaller candidate generation model with a larger black-box LLM. The approach begins with an expert system (evolved from Internist-1/QMR) that provides disease-finding knowledge with evoking strengths and frequencies. Synthetic history-taking conversations are generated from this expert system using LLMs, creating labeled training data. A smaller model (Llama 3.1 7B) is then fine-tuned on these synthetic chats to predict rare disease candidates. During inference, these candidates are injected into a prompt for the larger LLM (GPT-4o or Claude), which generates the final differential diagnosis. The system employs a multi-stage prompting strategy where the LLM first generates diagnoses independently, then considers the candidate list, and finally selects the most appropriate diagnoses.

## Key Results
- Top-5 accuracy improved from 56.8% to 74.1% on GPT-4o-generated test chats
- Exact match rates improved by up to 37% compared to baseline
- Rare disease candidate generation model achieved up to 88.8% accuracy on GPT-4o-generated chats
- Expert-evaluated synthetic chats showed 90.48% agreement on positive cases (Cohen's κ = 0.53)

## Why This Works (Mechanism)

### Mechanism 1
Training a specialized candidate generator on expert-system-validated synthetic data, then fusing its outputs into a general LLM prompt, improves rare disease recall without sacrificing common disease knowledge. The expert system provides structurally valid disease-finding associations with evoking strengths and frequencies. Synthetic chats derived from these cases give the small model a narrow, high-signal training distribution. At inference, the black-box LLM receives these candidates as explicit hints, offloading the need to retrieve low-prevalence conditions from its own parametric knowledge. Core assumption: The expert system's disease-finding relationships accurately capture real clinical presentations, and synthetic chat variations preserve sufficient signal for the small model to learn transferable patterns.

### Mechanism 2
Generating synthetic history-taking conversations from structured expert system cases avoids circular evaluation where an LLM is tested on knowledge it already encodes. The simulation pipeline samples seed diseases, constructs finding sets with demographic/predisposing factors, then uses a black-box LLM to render these into conversational format. Crucially, the ground-truth DDx comes from the expert system's scoring, not the LLM's own diagnosis. This decouples data generation from the evaluation target. Core assumption: The checker prompt and multi-turn generation produce chats that are linguistically diverse yet clinically faithful to the underlying findings.

### Mechanism 3
A multi-stage prompting strategy (independent first pass → candidate consideration → final selection) helps the black-box LLM balance rare and common diagnoses appropriately. The diagnosis prompt instructs the LLM to first generate 5 diagnoses ignoring the rare candidates, then evaluate the candidate list, then select the most appropriate set. This prevents the LLM from over-relying on injected rare candidates while still ensuring they receive explicit consideration. Core assumption: The LLM can accurately weigh prior probabilities (rarity) against symptom fit when both are made explicit, and won't simply anchor on whichever list appears first.

## Foundational Learning

- **Expert Systems (Rule-Based Diagnostic Systems)**
  - Why needed here: RareScale's data generation depends on an expert system (Internist-1/QMR lineage) that encodes disease-finding relationships with evoking strength and frequency scores. Understanding this representation is essential to diagnose simulation failures or extend coverage.
  - Quick check question: Given a finding present with evoking strength 4/5 and frequency 3/5 for Disease A, how would the expert system's scoring differ from a finding with strength 2/5 and frequency 5/5?

- **Synthetic Data Generation with LLMs**
  - Why needed here: The pipeline uses LLMs to convert structured cases into conversational format. This introduces risks of hallucination or inconsistent finding coverage that must be checked and filtered.
  - Quick check question: If a synthetic chat omits 2 of 12 findings due to LLM generation error, should the case be discarded or the finding list edited? What tradeoffs does the paper's checker-prompt approach introduce?

- **Two-Stage Retrieval + Reranking**
  - Why needed here: RareScale separates candidate generation (recall-oriented, small model) from final diagnosis (precision-oriented, large model). This pattern appears in many retrieval-augmented systems and helps frame the architecture design space.
  - Quick check question: Why might training the small model directly on the full diagnosis task (rather than candidate generation) degrade performance on common diseases?

## Architecture Onboarding

- **Component map**:
  Expert System (Internist-1/QMR derivative) -> Case Simulator -> Chat Generator -> Candidate Generator -> DDx Generator

- **Critical path**:
  Training: Expert system → Case simulation → Chat generation → Candidate model fine-tuning
  Inference: Input chat → Candidate model inference (5 candidates) → DDx LLM prompt with candidates → Final DDx list

- **Design tradeoffs**:
  - **Closed-world expert system vs. open coverage**: Current 575 diseases cover ~100 most prevalent rare diseases (80% of diagnoses per Evans & Rafi 2016), but exclude thousands of ultra-rare conditions. Expanding requires expert curation or literature-based knowledge extraction.
  - **Recall-oriented candidate generation vs. end-to-end diagnosis**: The small model is trained for high recall (Top-5), not precision, because its role is to surface possibilities. This prevents rare disease under-representation but requires the DDx LLM to filter appropriately.
  - **Synthetic vs. real clinical data**: Synthetic data enables controlled labeling and avoids circular evaluation, but may not capture real patient language variability, missing information, or multi-morbidity complexity.

- **Failure signatures**:
  - **Candidate generator outputs unrelated diseases**: Likely indicates train/test distribution shift (e.g., real patient chats differ from synthetic), or insufficient training examples per disease.
  - **DDx LLM ignores candidate list**: May indicate prompt formatting issues, or that candidates are too implausible for the given chat context.
  - **Expert system simulation produces ambiguous DDx**: Occurs when seed disease is not top-scoring after 6 findings; these cases are filtered out.

- **First 3 experiments**:
  1. **Baseline establishment**: Run the DDx generator (GPT-4o or Claude) on a held-out test set with no candidate list. Measure Top-1, Top-5, MRR.
  2. **Candidate generator validation**: Train the small model on the combined GPT-4o + Claude synthetic dataset. Evaluate on both test sets independently to confirm ~88% Top-5 recall.
  3. **Ablation on candidate injection**: Compare (a) no candidates, (b) candidates from prompting GPT-4o directly, (c) candidates from the trained small model.

## Open Questions the Paper Calls Out

### Open Question 1
Can techniques beyond retrieval augmented generation (RAG) effectively utilize raw medical literature to capture the subtlety of symptoms for rare disease candidate generation? The paper suggests future work should focus on using medical literature with techniques "beyond retrieval augmented generation," as RAG "struggles to capture the subtlety and relative importance" of various studies. This is unresolved because the current RareScale pipeline relies on a manually curated expert system, which is labor-intensive and limits scalability.

### Open Question 2
Can electronic health record (EHR) data replace expert system simulations for training the rare disease candidate generator? The paper suggests that "explorations that rely on electronic health record systems could remove the need for the expert system" while retaining the RareScale architecture. This is unresolved because the current model is trained on synthetic chats derived from structured, closed-world expert rules; it is unclear if noisy, real-world EHR data can provide the necessary signal without the expert system's supervision.

### Open Question 3
What are the optimal deployment protocols to balance rare disease consideration against common conditions without causing oversubscription? The paper notes that "balancing common diseases with rare diseases" is an "open problem," as providers must avoid "oversubscribing to the possibility of a rare disease" which is hard to rule out. This is unresolved because while the model improves accuracy, the clinical utility depends on when to invoke the model to maximize benefit and minimize mental toll.

## Limitations

- **Expert System Dependence**: The method relies on a proprietary expert system with a closed-world assumption of 575 rare diseases, limiting coverage to the system's domain and potentially excluding ultra-rare conditions.
- **Synthetic Data Generalization**: The approach uses LLM-generated synthetic history-taking chats validated by expert agreement, but transferability to real-world clinical data remains unproven without external validation on actual patient cases.
- **Knowledge Base Recency**: The expert system's disease-finding associations are based on literature and expert curation, but no information is provided about update frequency or coverage of newly identified rare diseases.

## Confidence

- **High Confidence**: The architectural framework and empirical improvements on synthetic data are well-supported by reported results (17%+ Top-5 accuracy gains, statistical significance p < 0.01).
- **Medium Confidence**: The synthetic data generation pipeline produces clinically plausible conversations (90.48% expert agreement), but transferability to real-world clinical data remains unproven.
- **Low Confidence**: The long-term clinical utility and safety of this approach in real diagnostic workflows, particularly regarding potential over-reliance on AI suggestions and impact on diagnostic reasoning skills, cannot be assessed from current results.

## Next Checks

1. **Real-World External Validation**: Test the complete RareScale pipeline on actual clinical case reports from MIMIC-RD or other real-world datasets to measure performance degradation from synthetic to real data and identify specific failure modes in real patient narratives.

2. **Clinical Workflow Integration Study**: Conduct a randomized controlled trial comparing diagnostic accuracy and time-to-diagnosis for rare diseases when clinicians use RareScale versus standard diagnostic tools, measuring both diagnostic outcomes and potential impacts on clinical reasoning patterns.

3. **Knowledge Base Expansion Assessment**: Evaluate the candidate generator's performance when expanded to include 1,000+ rare diseases from literature-based knowledge extraction, measuring recall-coverage tradeoffs and identifying systematic biases in disease representation.