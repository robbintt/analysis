---
ver: rpa2
title: 'Re-Depth Anything: Test-Time Depth Refinement via Self-Supervised Re-lighting'
arxiv_id: '2512.17908'
source_url: https://arxiv.org/abs/2512.17908
tags:
- depth
- image
- da-v2
- input
- vision
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Re-Depth Anything is a test-time optimization framework that refines
  depth predictions from the Depth Anything V2 (DA-V2) model by leveraging self-supervision
  through 2D diffusion models. The core idea is to re-light the predicted depth maps
  and use a 2D diffusion model to score the plausibility of the augmented shading,
  enabling refinement without labeled data.
---

# Re-Depth Anything: Test-Time Depth Refinement via Self-Supervised Re-lighting

## Quick Facts
- arXiv ID: 2512.17908
- Source URL: https://arxiv.org/abs/2512.17908
- Reference count: 40
- Primary result: Achieves up to 12.6% relative error reduction in depth metrics by refining DA-V2 predictions using self-supervised re-lighting and 2D diffusion model priors

## Executive Summary
Re-Depth Anything presents a test-time optimization framework that refines monocular depth predictions from the Depth Anything V2 model using self-supervision through 2D diffusion models. The method leverages shape-from-shading cues by re-lighting predicted depth maps and using a 2D diffusion model to score the plausibility of the augmented shading. Unlike traditional self-supervised approaches that attempt perfect photometric reconstruction, this method augments input images with novel lighting conditions to generate a learning signal. The framework achieves consistent improvements across multiple benchmarks (CO3D, KITTI, ETH3D) without requiring labeled data or full model retraining.

## Method Summary
The method optimizes intermediate embeddings and decoder weights while keeping the DA-V2 encoder frozen. It processes a single RGB image by extracting features through the frozen encoder, then jointly optimizes the intermediate feature embeddings and DPT decoder weights to minimize a loss that combines SDS from the diffusion model with smoothness regularization. The re-lighting module renders the depth map under random illumination using a Blinn-Phong model, and the resulting augmented image is scored by the diffusion model. Results from multiple runs are ensembled to reduce variance. The approach successfully enhances fine details and removes noise while preserving the foundational geometric knowledge from the pre-trained encoder.

## Key Results
- Achieves up to 12.6% relative error reduction in SI log metric compared to DA-V2 baseline
- Consistently outperforms DA-V2 across CO3D, KITTI, and ETH3D benchmarks
- Improves fine detail recovery and noise reduction in depth predictions
- Demonstrates strong generalization across diverse outdoor scenes
- Shows diminishing returns with ensemble size beyond N=5-10 runs

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Refining depth via self-supervised re-lighting leverages 2D diffusion priors to correct out-of-distribution errors.
- Mechanism: DA-V2 produces initial depth map, which is differentiably rendered under random illumination using Blinn-Phong model. Re-lit image is scored by 2D diffusion model via SDS. Gradients backpropagate to update depth-related components. Diffusion model provides "plausibility" signal for generated shading, pushing depth estimate toward geometry yielding realistic shading patterns.
- Core assumption: 2D diffusion model has learned strong prior about natural object shapes and their interaction with light to distinguish plausible from implausible shading.
- Evidence anchors: [abstract] mentions leveraging shape from shading cues with SDS; [Page 2, Col 1] states depth map refined by 2D diffusion model as prior for scoring realistic augmented shading.
- Break condition: Diffusion model's prior not applicable to input domain (e.g., stylized art), or initial depth error too large for shading cues to provide corrective gradient.

### Mechanism 2
- Claim: Targeted optimization of intermediate embeddings and decoder, while keeping encoder frozen, preserves foundational geometric knowledge while enabling refinement.
- Mechanism: Instead of fine-tuning entire DA-V2 network or directly optimizing depth pixels, method freezes powerful ViT encoder. Jointly optimizes intermediate feature embeddings (W) fed into DPT decoder and DPT decoder's weights (θ). Allows rich pre-trained geometric features from encoder to be re-interpreted and re-weighted by decoder to produce refined depth map satisfying SDS loss.
- Core assumption: Pre-trained encoder's features contain sufficient information for accurate depth map, but decoder needs adaptation to translate features correctly for specific out-of-distribution input.
- Evidence anchors: [abstract] mentions freeze encoder and update intermediate embeddings while fine-tuning decoder; [Page 4, Sec 4.3] discusses avoiding poor local minima by optimizing only embeddings and decoder weights.
- Break condition: Error is in encoder's representation (e.g., fundamental misinterpretation of scene content) which cannot be corrected by decoder-level adjustments.

### Mechanism 3
- Claim: Using re-lighting for augmentation, rather than photometric reconstruction, avoids ill-posed inverse rendering problem.
- Mechanism: Classical self-supervision from single image requires solving full inverse rendering. Method sidesteps this by augmenting input image with new randomly sampled lighting conditions based on current depth estimate. Diffusion model evaluates plausibility of augmented image, creating learning signal without needing to perfectly reconstruct original image's specific complex lighting and materials.
- Core assumption: Generating plausible shading under novel illumination is more robust supervisory signal than trying to invert and reconstruct exact original image appearance.
- Evidence anchors: [Page 2, Col 2] states re-synthesizing and augmenting input image fundamentally different to photometric reconstruction; [Page 3, Col 2] mentions not attempting to solve full ill-posed inverse graphics problem.
- Break condition: Assumed lighting model (Blinn-Phong) too simplistic for scene's materials (e.g., subsurface scattering, inter-reflections), leading to misleading gradients.

## Foundational Learning

- Concept: **Score Distillation Sampling (SDS)**
  - Why needed here: Core technique for extracting gradients from 2D diffusion model. Without understanding SDS, link between 2D diffusion prior and 3D depth refinement cannot be established.
  - Quick check question: How does SDS differ from using diffusion model to denoise image? (Answer: SDS uses diffusion model's score function to provide gradient direction for optimizing separate parameter like depth, rather than generating image.)

- Concept: **Test-Time Adaptation (TTA)**
  - Why needed here: Entire framework is TTA method. Critical to understand model not being retrained on dataset, but adapted on-the-fly for single input image.
  - Quick check question: What are key constraints of adapting model at test time versus during standard training loop? (Answer: No access to ground truth, potential for overfitting to single image, limited compute budget.)

- Concept: **Dense Prediction Transformer (DPT)**
  - Why needed here: Method specifically targets DPT decoder within DA-V2 architecture. Understanding its role in converting ViT features to dense depth map is essential for grasping optimization strategy.
  - Quick check question: What is primary function of DPT head in DA-V2 architecture? (Answer: To reassemble token-based features from ViT encoder into full-resolution, per-pixel depth/disparity map.)

## Architecture Onboarding

- Component map: Input Image -> Frozen Encoder -> Learnable Embeddings (W) -> Learnable DPT Decoder -> Disparity (D_disp) -> Re-lighting -> Re-lit Image (I_hat) -> SDS Loss (conditioned on I_hat & text c) -> Gradients -> Update W and θ

- Critical path: Input Image flows through frozen encoder to extract features, learnable embeddings are optimized along with decoder weights, depth map is generated and re-lit with random illumination, diffusion model scores augmented image, gradients update embeddings and decoder to minimize loss.

- Design tradeoffs:
  - Simplicity of renderer vs. realism: Simple Blinn-Phong model (Tradeoff: Not physically accurate for complex materials, but computationally cheap and avoids need for full inverse rendering)
  - Targeted vs. full fine-tuning: Freezing encoder (Tradeoff: Preserves foundational features, prevents collapse, but cannot correct errors originating from encoder)
  - Ensembling: Averaging results from multiple runs (Tradeoff: Increases computation time ~10x, but reduces variance and improves stability)
  - Embedding vs. Pixel Optimization: Optimizing embeddings rather than raw depth pixels (Tradeoff: Constrains output space to what decoder can produce, leveraging learned priors, but may limit flexibility)

- Failure signatures:
  - Hallucination: Adding non-existent geometric edges (e.g., "sticker on the truck" in Fig 3)
  - Over-smoothing: Losing fine details like trees or texture in dark regions
  - Geometry collapse: If full model is fine-tuned, geometry can collapse into degenerate state
  - Sky artifacts: Extending geometry into sky regions

- First 3 experiments:
  1. Single-Image Re-lighting Run: Take single image from CO3D dataset, run full pipeline with N=1 (no ensembling), visualize initial vs. refined disparity and normal maps, compare re-lit image quality.
  2. Ablation on Optimization Targets: Run same image three times, optimizing (a) only decoder weights, (b) only embeddings, and (c) full model. Observe and compare resulting depth maps and note any collapse or artifacts.
  3. Ensembling Study: Run pipeline on 5 different images, varying ensemble size N (1, 3, 5, 10). Plot final SI log metric vs. N to reproduce curve in Fig 6 and observe diminishing returns.

## Open Questions the Paper Calls Out

- Can this test-time optimization paradigm be effectively scaled to fine-tune foundational depth models on large-scale in-the-wild datasets rather than adapting per-image?
  - Basis in paper: [explicit] Conclusion states "In future work, we plan to... explore fine-tuning foundational models at scale on in-the-wild footage."
  - Why unresolved: Current framework optimizes embeddings and decoder weights per image (taking approx. 80s). Unknown if aggregating these gradients for global weight updates would improve base model or lead to collapse/overfitting.
  - What evidence would resolve it: Global model fine-tuned using this loss on diverse dataset (e.g., CO3D) showing improved zero-shot generalization on hold-out set.

- Do alternative re-synthesis approaches offer better gradient signals than current Blinn-Phong shading model?
  - Basis in paper: [explicit] Conclusion explicitly lists "explor[ing] alternative re-synthesis approaches" as part of future work.
  - Why unresolved: Paper relies on simple Blinn-Phong model with inverse tonemapping. While effective, authors suggest this simple augmentation might be limiting factor compared to more complex differentiable rendering techniques.
  - What evidence would resolve it: Ablation studies comparing current shading model against physically-based rendering or neural rendering approaches, measuring depth accuracy (SI log) and convergence speed.

- Can semantic guidance or masking strategies resolve specific failure case where geometry is erroneously extended into sky?
  - Basis in paper: [inferred] Authors note in Limitations section that "Sometimes the method extends geometry into the sky... which could potentially be handled by thresholding."
  - Why unresolved: Diffusion prior encourages plausible shading but lacks semantic understanding to distinguish between distant sky and solid surfaces, leading to artifacts in regions lacking texture or depth cues.
  - What evidence would resolve it: Modified loss incorporating semantic segmentation masks (e.g., masking out sky during SDS optimization) resulting in cleaner depth maps in outdoor scenes without reducing accuracy in foreground regions.

## Limitations

- Method shows artifacts in dark regions and potential sky hallucination, suggesting domain-specific failure modes
- Geometry can be erroneously extended into sky regions due to lack of semantic understanding
- Oversmoothing observed in texture-rich regions like trees or dark areas
- Reliance on 2D diffusion prior creates potential disconnect between shading plausibility and depth accuracy
- Computational cost of ~80 seconds per image with ensemble of 10 runs

## Confidence

- **High Confidence**: Ablation study showing targeted optimization (embeddings + decoder) outperforms full model fine-tuning is well-supported by quantitative metrics and qualitative examples. Improvement over DA-V2 across multiple benchmarks is also robust.
- **Medium Confidence**: Claim that self-supervision via re-lighting more effective than classical photometric reconstruction is plausible given design rationale, but comparison is indirect (paper doesn't benchmark against standard self-supervised depth methods using photometric reconstruction).
- **Low Confidence**: Specific mechanism by which 2D diffusion model's prior translates to improved 3D geometry is largely speculative. Paper assumes model has learned "natural" geometry priors, but doesn't validate this assumption directly or explore failure cases where assumption breaks.

## Next Checks

1. **Ground Truth Validation**: Run Re-Depth Anything on small subset of CO3D/KITTI images where ground truth depth is available. Compare refinement quality against baseline that simply runs DA-V2 without adaptation to isolate true benefit of self-supervision.

2. **Synthetic Benchmark Test**: Create synthetic dataset with controlled out-of-distribution shifts (e.g., different object categories, extreme lighting). Apply Re-Depth Anything and measure whether improvements correlate with diffusion model's ability to recognize synthetic objects' shapes.

3. **Prior Ablation Study**: Replace Stable Diffusion v1.5 prior with randomly initialized diffusion model or model trained on very different domain (e.g., medical images). If refinement quality drops significantly, it would validate that pre-trained prior is essential to method's success.