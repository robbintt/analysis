---
ver: rpa2
title: Design and Implementation of an OCR-Powered Pipeline for Table Extraction from
  Invoices
arxiv_id: '2507.07029'
source_url: https://arxiv.org/abs/2507.07029
tags:
- table
- invoice
- image
- detection
- extraction
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents a hybrid pipeline combining traditional image
  processing with OCR to extract structured table data from scanned invoices. It addresses
  real-world challenges such as skew, noise from barcodes, signatures, and irregular
  table layouts by using techniques like perspective correction, contour-based segmentation,
  adaptive thresholding, and morphological filtering.
---

# Design and Implementation of an OCR-Powered Pipeline for Table Extraction from Invoices

## Quick Facts
- arXiv ID: 2507.07029
- Source URL: https://arxiv.org/abs/2507.07029
- Authors: Parshva Dhilankumar Patel
- Reference count: 3
- Primary result: Hybrid pipeline combining classical CV and OCR to extract structured table data from scanned invoices with real-world noise.

## Executive Summary
This paper presents a hybrid pipeline that combines traditional image processing with OCR to extract structured table data from scanned invoices. The system addresses real-world challenges such as skew, noise from barcodes, signatures, and irregular table layouts through techniques like perspective correction, contour-based segmentation, adaptive thresholding, and morphological filtering. The pipeline integrates OpenCV for preprocessing, Tesseract OCR for text extraction, and img2table for table detection, producing structured JSON outputs. While no quantitative accuracy metrics are provided, qualitative results demonstrate consistent extraction across diverse invoice formats.

## Method Summary
The pipeline processes invoice images through a multi-stage approach: preprocessing with grayscale conversion, edge cropping, and perspective correction; noise removal targeting signatures and barcodes; section segmentation using keyword detection; and table extraction via a three-layer fallback system combining img2table with manual OpenCV methods. The system uses Tesseract OCR with --psm 6 and --oem 3 settings, implementing custom logic to remove artifacts and map extracted data into structured JSON with key-value pairs. The implementation relies on Python 3.10 with OpenCV v4.9, Tesseract OCR v5.3, and img2table, though specific hyperparameters and integration details remain unspecified.

## Key Results
- Contour-based perspective correction successfully normalizes document geometry for improved OCR accuracy
- Three-layer table detection provides robustness across varying invoice layouts through progressive structural assumption relaxation
- Artifact removal pipeline effectively eliminates signatures and barcodes while preserving underlying table structure

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Contour-based perspective correction improves downstream OCR accuracy by normalizing document geometry before text recognition.
- Mechanism: Edge cropping removes peripheral noise → contour filtering discards small artifacts (<35,000 px) → convex hull extracts four extreme corners → perspective transform warps image to flat, top-down view → padding prevents edge-cutting in later operations.
- Core assumption: The document boundary is the largest rectangular contour in the image; smaller contours are noise.
- Evidence anchors:
  - [abstract]: "Our approach addresses real-world challenges including skewed perspectives... variable lighting, noise from signatures, barcodes, staplers, and broken table structures."
  - [section 3.1]: "The identified corners are passed to cv2.getPerspectiveTransform() and warped using cv2.warpPerspective()... improving both visual clarity and text recognition quality."
  - [corpus]: Weak direct evidence; neighbor papers focus on deep learning approaches rather than classical CV pipelines.
- Break condition: Images where the document boundary is partially obscured, has non-rectangular edges, or contains multiple documents in frame.

### Mechanism 2
- Claim: Artifact removal (signatures, barcodes) reduces OCR false positives by eliminating non-textual visual elements before recognition.
- Mechanism: HSV color segmentation targets blue/cyan ink → Canny edge detection isolates irregular curves → morphological filtering identifies small-area, irregular contours → white-pixel masking removes detected regions while preserving underlying layout.
- Core assumption: Handwritten marks differ in color or shape regularity from printed text and table lines.
- Evidence anchors:
  - [abstract]: "...noise from signatures, barcodes, staplers, and broken table structures."
  - [section 3.3]: "This step significantly improves OCR precision by preventing misclassification of artifacts as invoice content."
  - [section 6.3]: Visual figures show signature/tick removal pipeline results.
  - [corpus]: No direct corpus comparison; artifact handling is pipeline-specific in this work.
- Break condition: Signatures in black ink overlapping printed text, or stamps with text content that should be preserved.

### Mechanism 3
- Claim: Three-layer fallback table detection provides robustness across varying invoice layouts by progressively relaxing structural assumptions.
- Mechanism: Primary layer uses img2table (OCR-aware heuristics, spatial clustering) → fallback uses manual OpenCV edge detection + morphology → final fallback uses row-wise OCR with colon delimiters and regex for key-value extraction.
- Core assumption: Some tabular structure exists (either bordered, spatially clustered, or delimiter-separated) that can be exploited.
- Evidence anchors:
  - [abstract]: "...hybrid table detection using both Img2Table and manual fallback methods..."
  - [section 3.7]: Describes three-layer system with img2table, manual OpenCV detection, and row-wise extraction.
  - [corpus]: Spatial ModernBERT paper notes transformer-based approaches for table extraction, but this pipeline remains classical-CV-based.
- Break condition: Completely unstructured free-text invoices with no spatial or delimiter patterns.

## Foundational Learning

- **Morphological operations (dilation, erosion, opening)**
  - Why needed here: Used extensively for noise suppression, line removal, artifact detection, and reconnecting broken edges caused by scanning artifacts.
  - Quick check question: Given a binarized image with broken horizontal lines, which operation would reconnect them—dilation or erosion?

- **Contour hierarchy and filtering**
  - Why needed here: Core to document boundary detection, table cell extraction, and artifact identification based on area and aspect ratio thresholds.
  - Quick check question: How would you filter contours to keep only those with area > 35,000 pixels and aspect ratio < 5?

- **Perspective transformation**
  - Why needed here: Corrects geometric distortion from mobile captures, enabling proper row-column alignment for OCR.
  - Quick check question: What four-point correspondence is needed to compute a perspective transform matrix?

## Architecture Onboarding

- **Component map:** Preprocessing Module -> Noise Removal Module -> Section Segmentation -> Table Detection Module -> OCR Module -> Key-Value Structuring Module
- **Critical path:** Preprocessing → Perspective correction → Noise removal → Section segmentation → Line removal → Table detection → OCR → Structured output. Each stage depends on the previous; failures cascade.
- **Design tradeoffs:**
  - Classical CV vs. deep learning: Lightweight deployment but may underperform on handwritten or highly variable layouts.
  - Fixed thresholds vs. adaptive: Current pipeline uses heuristics; Section 5.4 notes dynamic preprocessing was added for threshold generalization.
  - img2table dependency: Provides robustness but requires minimum 800px width and may fail on irregular tables.
- **Failure signatures:**
  - Perspective correction selects wrong contour → warped output unusable.
  - Signature removal over-masks → printed text regions incorrectly whitened.
  - img2table returns empty → fallback to row-wise extraction; may produce misaligned key-value pairs.
  - OCR confidence low on faint text → Section 7 notes future work on confidence-based post-processing.
- **First 3 experiments:**
  1. Test perspective correction on a held-out set of skewed invoice images; measure contour selection accuracy vs. manual ground truth.
  2. Ablate signature/barcode removal; compare OCR error rates on affected regions with and without preprocessing.
  3. Evaluate three-layer table detection fallback; record which layer succeeds per document and characterize failure modes for each.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can binarization and morphological parameters be dynamically automated based on image resolution and contrast to ensure generalization across diverse scan qualities?
- Basis in paper: [explicit] The authors identify "Dynamic Parameter Tuning" as future work, noting that current fixed parameters (e.g., contour area thresholds) struggle to generalize across varying invoice designs.
- Why unresolved: The current implementation relies on static values (e.g., 10-pixel edge crops) which require manual adjustment for different document qualities.
- What evidence would resolve it: Demonstration of the pipeline automatically selecting optimal kernel sizes and thresholds for a dataset with high variance in lighting and DPI.

### Open Question 2
- Question: What are the quantitative extraction accuracy metrics (e.g., precision, recall, F1-score) of the proposed pipeline on a standardized dataset of noisy invoices?
- Basis in paper: [inferred] While the abstract claims "significant" improvement, the results section states "no quantitative accuracy metrics are provided," leaving the system's objective performance unverified.
- Why unresolved: The evaluation relies solely on visual verification of specific cases rather than statistical analysis of key-value extraction success rates.
- What evidence would resolve it: Benchmarking results against ground truth data showing the error rates for table detection and OCR accuracy under various noise conditions.

### Open Question 3
- Question: Can deep learning models (e.g., LayoutLM, Donut) be integrated into the pipeline to reliably handle loosely structured or borderless invoices without incurring prohibitive computational costs?
- Basis in paper: [explicit] Future work suggests "Integration with Machine Learning Models" to address limitations in handling irregular layouts and handwritten components.
- Why unresolved: The current hybrid approach uses heuristics (img2table) that fail on loosely structured documents, but the feasibility of swapping these for deep learning in a "lightweight" pipeline is unknown.
- What evidence would resolve it: A comparative performance analysis measuring both extraction accuracy on borderless tables and the resulting inference latency.

## Limitations
- No quantitative accuracy metrics provided to validate extraction performance across diverse invoice layouts
- Critical implementation details including exact hyperparameters for adaptive thresholding and barcode detection thresholds remain unspecified
- Dependency on img2table with 800px minimum width requirement creates potential bottleneck for mobile-captured invoices

## Confidence

- **High Confidence**: The core mechanism of contour-based perspective correction for document normalization is well-established in classical CV literature and directly supported by the paper's implementation details.
- **Medium Confidence**: The artifact removal approach (signatures, barcodes) shows reasonable design logic, but the effectiveness heavily depends on color segmentation parameters that are not specified.
- **Low Confidence**: The overall system performance claims cannot be validated without quantitative metrics or a test dataset, making it difficult to assess the actual robustness across real-world invoice variations.

## Next Checks

1. Implement a quantitative evaluation framework using a diverse invoice dataset to measure extraction accuracy, focusing on perspective correction quality and table detection success rates.
2. Conduct ablation studies to determine the impact of each preprocessing step (especially signature/barcode removal) on final OCR accuracy and JSON structure correctness.
3. Test the three-layer fallback system systematically to identify which layer succeeds most frequently and characterize failure patterns for each approach.