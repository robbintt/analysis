---
ver: rpa2
title: 'AnalogNAS-Bench: A NAS Benchmark for Analog In-Memory Computing'
arxiv_id: '2506.18495'
source_url: https://arxiv.org/abs/2506.18495
tags:
- degree
- conv
- node
- architectures
- skip
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces AnalogNAS-Bench, the first neural architecture
  search benchmark tailored for Analog In-Memory Computing (AIMC). By extending NAS-Bench-201
  with AIMC-specific noise simulations, the benchmark enables systematic evaluation
  of architectures under analog hardware constraints.
---

# AnalogNAS-Bench: A NAS Benchmark for Analog In-Memory Computing

## Quick Facts
- arXiv ID: 2506.18495
- Source URL: https://arxiv.org/abs/2506.18495
- Authors: Aniss Bessalah; Hatem Mohamed Abdelmoumen; Karima Benatchba; Hadjer Benmeziane
- Reference count: 17
- Key outcome: Introduces AnalogNAS-Bench, extending NAS-Bench-201 with AIMC noise simulations, revealing that robust architectures favor wider structures with more skip connections and differ from digital-optimized designs.

## Executive Summary
This work introduces AnalogNAS-Bench, the first neural architecture search (NAS) benchmark specifically designed for Analog In-Memory Computing (AIMC) systems. By extending the NAS-Bench-201 search space with AIMC-specific noise simulations using AIHWKit, the benchmark enables systematic evaluation of neural architectures under analog hardware constraints including quantization noise, conductance noise, and temporal drift. The benchmark provides a standardized platform for comparing analog-aware NAS methods and facilitates research into hardware-efficient neural network design for analog systems.

## Method Summary
AnalogNAS-Bench extends the NAS-Bench-201 search space (15,625 architectures) by simulating analog hardware constraints using AIHWKit. Architectures are trained on CIFAR-10 with standard hyperparameters (200 epochs, SGD with Nesterov momentum, cosine annealing) and then evaluated under AIMC conditions including 8-bit quantization, noise injection (σ=0.04), and temporal drift simulations at 60s, 1h, 1d, and 30d intervals. The benchmark provides multiple accuracy metrics: baseline digital accuracy, post-training quantization (PTQ) accuracy, quantization-aware training (QAT) accuracy, noisy AIMC accuracy, and analog accuracy with hardware-aware training (HWT). Each evaluation is averaged over 25 runs to account for stochastic noise in analog systems.

## Key Results
- Standard quantization techniques (PTQ, QAT) fail to capture AIMC-specific noise effects, showing significant performance gaps compared to HWT-trained models.
- Robust architectures to AIMC noise are structurally distinct from digital-optimized ones, favoring wider structures with more skip connections and 3×3 convolutions.
- Skip connections improve resilience to temporal drift, with architectures containing more skip connections maintaining higher accuracy over extended operation periods.

## Why This Works (Mechanism)
AnalogNAS-Bench works by creating a controlled environment where neural architectures can be systematically evaluated under realistic analog hardware constraints. The benchmark captures the fundamental mismatch between digital training assumptions and analog hardware behavior by simulating noise sources (quantization, conductance variation, read noise) and temporal drift that occur in AIMC systems. By providing standardized metrics across 15,625 architectures, it enables researchers to identify architectural patterns that are inherently robust to analog imperfections, which would be difficult to discover through ad-hoc experimentation.

## Foundational Learning
- **AIMC Noise Modeling**: Simulates analog-specific noise sources (DAC/ADC quantization, conductance variation, read noise) that differ from digital noise. *Why needed*: Analog hardware introduces unique error sources not captured by standard digital training. *Quick check*: Verify noise parameters match target hardware specifications.
- **Hardware-Aware Training (HWT)**: Incorporates noise injection during training to improve robustness to analog imperfections. *Why needed*: Standard training doesn't account for analog noise, leading to poor real-world performance. *Quick check*: Compare HWT vs standard training accuracy gaps.
- **Temporal Drift Simulation**: Models conductance drift over time (60s to 30 days) to evaluate long-term reliability. *Why needed*: Analog devices degrade over time, affecting sustained performance. *Quick check*: Plot accuracy decay curves across different time intervals.
- **Architecture Encoding**: Uses DAG-based cell representations with operations (skip, zeroize, conv, pooling) to define search space. *Why needed*: Enables systematic exploration of architectural variations. *Quick check*: Verify all 15,625 architectures are properly enumerated.

## Architecture Onboarding

**Component Map**: Architecture -> NAS-Bench-201 Cell Encoding -> AIHWKit Simulation -> Accuracy Metrics

**Critical Path**: Architecture definition → Digital training → AIMC simulation → Noise evaluation → Drift analysis

**Design Tradeoffs**: The benchmark trades search space diversity (limited to NAS-Bench-201) for computational feasibility and standardized evaluation. Wider architectures with skip connections perform better in analog but may have higher area/power costs.

**Failure Signatures**: Architectures heavy in 1×1 convolutions show catastrophic accuracy drops under AIMC conditions; PTQ and QAT fail to recover performance lost to analog noise; temporal drift causes gradual accuracy decay, accelerated in architectures with fewer skip connections.

**First Experiments**:
1. Compare baseline accuracy vs noisy accuracy for a randomly sampled architecture to observe analog degradation.
2. Train the same architecture with HWT and compare against standard training to measure noise robustness gains.
3. Simulate drift at 60s, 1h, 1d, 30d for an architecture to visualize temporal reliability patterns.

## Open Questions the Paper Calls Out
None

## Limitations
- The benchmark is confined to the NAS-Bench-201 search space, which may not capture the full architectural diversity needed for optimal AIMC performance.
- Assumes a specific hardware configuration (8-bit DAC/ADC, 25μS max conductance) that may not generalize across all AIMC platforms.
- Temporal drift simulations are based on simplified models that may not capture all real-world aging effects.

## Confidence
- High confidence: Characterization of architectural robustness patterns (wider structures, skip connections) is well-supported by extensive simulations.
- Medium confidence: Claim that HWT-trained architectures differ structurally from standard digital optimizations is supported but requires independent validation.
- Medium confidence: Failure of standard quantization techniques for AIMC is demonstrated but could benefit from comparison with specialized analog-aware methods.

## Next Checks
1. Replicate architectural robustness findings using a different AIMC hardware simulator or actual analog hardware to verify generalizability.
2. Extend the benchmark to include architectures from more diverse search spaces (e.g., DARTS or ResNet variants) to test whether robustness patterns hold beyond NAS-Bench-201.
3. Conduct ablation studies on hardware parameters (DAC/ADC resolution, conductance range) to quantify their impact on optimal architecture selection.