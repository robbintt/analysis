---
ver: rpa2
title: 'Subset Selection for Fine-Tuning: A Utility-Diversity Balanced Approach for
  Mathematical Domain Adaptation'
arxiv_id: '2505.01523'
source_url: https://arxiv.org/abs/2505.01523
tags:
- subset
- selection
- dataset
- examples
- fine-tuning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study addresses the challenge of efficiently fine-tuning large
  language models on mathematical domains by proposing a budgeted subset selection
  method that balances utility and diversity. The approach combines perplexity and
  Chain-of-Thought loss as utility metrics to identify challenging examples, while
  using cosine similarity on sentence embeddings for diversity.
---

# Subset Selection for Fine-Tuning: A Utility-Diversity Balanced Approach for Mathematical Domain Adaptation

## Quick Facts
- **arXiv ID:** 2505.01523
- **Source URL:** https://arxiv.org/abs/2505.01523
- **Reference count:** 9
- **Primary result:** Proposed method achieves 0.47 accuracy on GSM8K test set at 1000 examples budget, comparable to or better than random selection (0.41) and DPP (0.46).

## Executive Summary
This paper addresses the challenge of efficiently fine-tuning large language models on mathematical domains by proposing a budgeted subset selection method that balances utility and diversity. The approach combines perplexity and Chain-of-Thought loss as utility metrics to identify challenging examples, while using cosine similarity on sentence embeddings for diversity. The greedy algorithm achieves a 1-1/e approximation factor, ensuring near-optimal subsets. Experiments on LLaMA-3 8B and Phi-3 models with GSM8K dataset show competitive performance against baselines, with the proposed method achieving 0.47 accuracy at 1000 examples, comparable to or better than random selection (0.41) and DPP (0.46) at similar budgets. The method enables significant computational savings while maintaining model performance.

## Method Summary
The proposed method uses a greedy algorithm to select a subset of training examples that balances utility and diversity. Utility is measured using perplexity and Chain-of-Thought (CoT) loss, which identify examples where the model struggles with problem comprehension and reasoning. Diversity is ensured through cosine similarity on sentence embeddings. The objective function combines these two aspects with trade-off parameters λ (utility-diversity) and α (perplexity-CoT weight). The greedy selection process iteratively picks examples that maximize marginal gain in the submodular objective, achieving a 1-1/e approximation guarantee. Experiments demonstrate that the method performs competitively with baselines while reducing computational cost through selective fine-tuning.

## Key Results
- Proposed method achieves 0.47 accuracy on GSM8K test set at 1000 examples budget
- Outperforms random selection (0.41) and DPP (0.46) at similar budget constraints
- Maintains competitive performance while enabling significant computational savings through selective fine-tuning
- Greedy algorithm provides 1-1/e approximation guarantee for near-optimal subset selection

## Why This Works (Mechanism)
The method works by identifying and selecting examples that are both challenging for the model (high utility) and diverse in content (low redundancy). The utility component captures where the model struggles with mathematical reasoning through perplexity and CoT loss metrics, while the diversity component ensures the selected subset covers different problem types and reasoning patterns through cosine similarity on embeddings. The submodular objective function allows efficient greedy selection with theoretical guarantees, and the balanced trade-off between utility and diversity prevents overfitting to specific problem types while maintaining challenging examples.

## Foundational Learning
- **Submodularity**: Why needed - enables greedy selection with theoretical approximation guarantees; Quick check - verify diminishing returns property holds for objective function
- **Sentence embeddings**: Why needed - represent mathematical problems in continuous space for diversity computation; Quick check - confirm embeddings capture semantic similarity between problems
- **Perplexity**: Why needed - measures model's uncertainty about input distribution; Quick check - verify lower perplexity correlates with better model performance
- **Chain-of-Thought reasoning**: Why needed - captures intermediate reasoning steps for mathematical problems; Quick check - ensure CoT paths are generated and evaluated correctly
- **Cosine similarity**: Why needed - quantifies semantic diversity between mathematical problems; Quick check - verify similarity matrix captures problem type variations

## Architecture Onboarding

### Component Map
MathBERT -> Sentence Embeddings -> Cosine Similarity Matrix -> Diversity Component
Proxy Model -> Perplexity & CoT Loss -> Utility Scores -> Utility Component
Greedy Algorithm -> Selected Subset -> Fine-Tuning -> Evaluation

### Critical Path
1. Compute sentence embeddings using MathBERT
2. Calculate pairwise cosine similarity matrix
3. Compute utility scores (perplexity + CoT loss)
4. Run greedy selection algorithm
5. Fine-tune model on selected subset
6. Evaluate accuracy on test set

### Design Tradeoffs
- Utility vs Diversity balance (parameter λ) - affects whether method prioritizes challenging examples or broad coverage
- Perplexity vs CoT loss weight (parameter α) - determines whether method focuses more on comprehension or reasoning difficulties
- Budget size - larger budgets may capture more diversity but reduce computational savings
- Proxy model choice - affects utility score quality and potentially subset composition

### Failure Signatures
- Poor performance compared to random selection indicates utility scores may be poorly calibrated or diversity computation ineffective
- Slow selection process suggests inefficient similarity matrix computation or marginal gain calculation
- Inconsistent results across budget sizes may indicate sensitivity to trade-off parameters or dataset characteristics

### First Experiments
1. Verify greedy algorithm produces consistent subsets across multiple runs with same parameters
2. Test sensitivity of results to λ and α parameter values
3. Compare selected subset diversity metrics against random selection baseline

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How robust is the proposed method across diverse mathematical datasets beyond GSM8K?
- **Basis in paper:** Section 6.1 lists MATH, MMLU-Mathematics, and MathQA datasets, but experiments (Table 1) only report GSM8K results.
- **Why unresolved:** The method incorporates domain-specific utility metrics (CoT loss) that may behave differently on competition-level problems (MATH) versus grade-school problems (GSM8K).
- **What evidence would resolve it:** Accuracy comparisons across MATH, MathQA, and MMLU-Mathematics at identical budget constraints.

### Open Question 2
- **Question:** Why does the method underperform at certain budget sizes (900 and 1100 examples)?
- **Basis in paper:** Table 1 shows inconsistent results—at 900 examples, the method achieves 0.42 vs. DPP's 0.49; at 1100, it drops to 0.40, below random selection (0.41).
- **Why unresolved:** The greedy algorithm's approximation guarantees don't explain budget-specific performance degradation.
- **What evidence would resolve it:** Analysis of selected subset composition across budget sizes to identify whether diversity-utility trade-offs cause suboptimal selections at specific budgets.

### Open Question 3
- **Question:** How sensitive is performance to the choice of proxy model for utility score computation?
- **Basis in paper:** Section 5.3 uses "a smaller proxy model (e.g., LLaMA-3 3B)" but doesn't analyze whether different proxy models yield different subset quality.
- **Why unresolved:** Utility scores depend on perplexity and CoT loss from this proxy; model-specific biases could propagate to subset selection.
- **What evidence would resolve it:** Comparison of selected subsets and final accuracy when using different proxy models (e.g., Phi-3 mini vs. LLaMA-3 3B).

### Open Question 4
- **Question:** Can the utility-diversity framework generalize effectively to non-mathematical domains without domain-specific utility metrics?
- **Basis in paper:** Section 4.1 describes domain-specific adaptation with CoT loss for mathematics, but Section 5 mentions "specialized domains like mathematics" implying other domains need tailored metrics.
- **Why unresolved:** The CoT loss utility metric is specific to reasoning-heavy tasks; equivalent metrics for other domains remain unspecified.
- **What evidence would resolve it:** Experiments applying the framework to domains like code generation or legal reasoning with appropriately adapted utility metrics.

## Limitations
- Exact values of utility-diversity trade-off parameter λ and perplexity-CoT weight α are unspecified
- CoT loss computation details and loss function L are not defined
- Fine-tuning hyperparameters (learning rate, batch size, optimizer) are missing
- Ambiguity about whether same model or proxy model computes perplexity and CoT loss

## Confidence

**High confidence**: The greedy algorithm's theoretical approximation guarantee (1-1/e) and the overall framework combining utility and diversity metrics.

**Medium confidence**: The empirical results showing competitive accuracy compared to baselines, given that exact hyperparameters and parameter values are unspecified.

**Low confidence**: The precise implementation details for CoT loss computation and the exact values of λ and α, which are critical for reproducing the results.

## Next Checks
1. Validate the greedy selection process by testing with multiple λ and α values to confirm the reported accuracy range (0.41-0.47 at 1000 examples).
2. Confirm the computation of CoT loss by implementing cross-entropy between generated and ground-truth reasoning paths.
3. Reproduce the full fine-tuning pipeline on Phi-3 with specified hyperparameters (learning rate, batch size) to ensure consistency with reported results.