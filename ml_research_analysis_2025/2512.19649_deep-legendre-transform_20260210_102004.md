---
ver: rpa2
title: Deep Legendre Transform
arxiv_id: '2512.19649'
source_url: https://arxiv.org/abs/2512.19649
tags:
- convex
- training
- functions
- legendre
- function
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Deep Legendre Transform (DLT), a deep learning
  method for computing convex conjugates of differentiable convex functions. Traditional
  numerical methods suffer from the curse of dimensionality and become computationally
  intractable in high dimensions, while neural network-based approaches scale better
  but require solving complicated optimization problems.
---

# Deep Legendre Transform
## Quick Facts
- arXiv ID: 2512.19649
- Source URL: https://arxiv.org/abs/2512.19649
- Reference count: 40
- Key outcome: Introduces Deep Legendre Transform (DLT), a scalable neural network method for computing convex conjugates that avoids curse of dimensionality and provides guaranteed convexity and error estimation

## Executive Summary
This paper introduces Deep Legendre Transform (DLT), a novel deep learning approach for computing convex conjugates of differentiable convex functions. Traditional numerical methods suffer from the curse of dimensionality, becoming computationally intractable in high dimensions, while neural network-based approaches face optimization challenges. DLT uses an implicit Fenchel formulation that enables efficient gradient-based minimization of approximation errors while guaranteeing convexity when using input convex neural networks.

The method demonstrates superior accuracy and scalability compared to classical grid-based approaches like Lucet's Linear Time Legendre Transform in high-dimensional settings. DLT also enables applications to Hamilton-Jacobi equations through the Hopf formula and provides unbiased a posteriori error estimators, making it valuable for evaluating any numerical method for convex conjugation.

## Method Summary
DLT approximates the convex conjugate f*(∇f(x)) ≈ ⟨x,∇f(x)⟩ − f(x) by minimizing empirical squared error over a training set. The method leverages implicit Fenchel formulation of convex conjugation, allowing efficient gradient-based optimization. When using input convex neural networks (ICNNs), the approach guarantees convexity of the approximation. The key innovation is that this formulation provides exact agreement with true values at training points while scaling efficiently to high dimensions, unlike classical grid-based methods that suffer from memory constraints.

## Key Results
- Achieves equivalent L2 errors to direct learning methods without requiring closed-form Legendre transforms
- Outperforms classical grid-based approaches in high dimensions due to constant memory footprint and smooth scaling
- Provides unbiased a posteriori error estimators for evaluating numerical methods
- Enables superior accuracy for Hamilton-Jacobi equations compared to Deep Galerkin Methods when combined with Hopf formula

## Why This Works (Mechanism)
The success of DLT stems from reformulating convex conjugation as an optimization problem that can be efficiently solved using gradient-based methods. By approximating f*(∇f(x)) through empirical risk minimization on the Fenchel dual formulation, the method leverages the smoothness and differentiability of neural networks while maintaining theoretical guarantees. The implicit formulation avoids explicit computation of the conjugate, which would require solving complex optimization problems at each point.

## Foundational Learning
- Convex analysis fundamentals - Why needed: Understanding Legendre transforms and Fenchel duality is essential for grasping the mathematical foundation
- Quick check: Verify understanding of convex conjugate definition and biconjugate theorem
- Input convex neural networks - Why needed: ICNNs guarantee convexity of learned functions, a critical requirement for valid Legendre transforms
- Quick check: Confirm ICNNs can represent all convex functions and maintain convexity under composition
- Automatic differentiation - Why needed: Enables efficient gradient computation for the Fenchel formulation
- Quick check: Test differentiation of composite functions through neural network layers

## Architecture Onboarding
Component map: Training data -> ICNN model -> Loss function (Fenchel formulation) -> Optimizer -> Trained convex conjugate approximation
Critical path: Data preprocessing → ICNN initialization → Fenchel loss computation → Gradient descent → Validation
Design tradeoffs: ICNNs provide convexity guarantees but limit representational capacity; simpler architectures train faster but may not capture complex convex functions
Failure signatures: Non-convex outputs indicate ICNN implementation issues; high training error suggests insufficient model capacity or poor hyperparameter choices
First experiments: 1) Train on simple quadratic functions where analytical conjugates are known, 2) Test on piecewise linear convex functions, 3) Validate convexity preservation on random convex functions

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions beyond suggesting future work on extending the method to more general classes of functions and exploring additional applications in optimal control and differential games.

## Limitations
- Performance comparison with Deep Galerkin Methods needs more rigorous validation across standardized benchmarks
- Exactness claims with Kolmogorov-Arnold Networks require more detailed mathematical justification
- Reliability of a posteriori error estimators in high-noise scenarios remains to be thoroughly tested

## Confidence
- High Confidence: Core methodology and theoretical foundations are sound; scaling advantages in high dimensions are well-supported
- Medium Confidence: Superiority claims over Deep Galerkin Methods need more comprehensive validation
- Medium Confidence: Exactness claims with specialized architectures require broader testing and mathematical justification

## Next Checks
1. Conduct systematic benchmark comparisons of DLT against state-of-the-art deep learning methods for Hamilton-Jacobi equations using standardized test suites
2. Test the robustness of a posteriori error estimators under various noise conditions and for functions with different regularity properties
3. Investigate computational complexity scaling of DLT in very high dimensions (>100) and quantify trade-offs between approximation accuracy and computational cost