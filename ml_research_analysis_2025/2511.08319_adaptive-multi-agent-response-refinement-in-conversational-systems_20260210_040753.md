---
ver: rpa2
title: Adaptive Multi-Agent Response Refinement in Conversational Systems
arxiv_id: '2511.08319'
source_url: https://arxiv.org/abs/2511.08319
tags:
- response
- agent
- mara
- refine
- ours
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of refining conversational responses
  in multi-turn dialogues, where initial LLM outputs often lack factual accuracy,
  persona alignment, or coherence. It proposes MARA, a multi-agent framework where
  specialized agents handle factuality, personalization, and coherence refinement,
  coordinated by a planner agent that dynamically selects and sequences agents per
  query.
---

# Adaptive Multi-Agent Response Refinement in Conversational Systems

## Quick Facts
- arXiv ID: 2511.08319
- Source URL: https://arxiv.org/abs/2511.08319
- Authors: Soyeong Jeong; Aparna Elangovan; Emine Yilmaz; Oleg Rokhlenko
- Reference count: 40
- Primary result: Multi-agent framework MARA significantly improves conversational response quality across coherence, groundedness, naturalness, and engagingness metrics.

## Executive Summary
The paper addresses the challenge of refining conversational responses in multi-turn dialogues, where initial LLM outputs often lack factual accuracy, persona alignment, or coherence. It proposes MARA, a multi-agent framework where specialized agents handle factuality, personalization, and coherence refinement, coordinated by a planner agent that dynamically selects and sequences agents per query. Experiments on three datasets show MARA significantly outperforms baselines on all metrics—coherence (2.32→2.67), groundedness (0.56→0.65), naturalness (1.98→2.15), and engagingness (2.54→2.83)—demonstrating the effectiveness of distributed multi-agent refinement over single-agent approaches.

## Method Summary
MARA implements a dynamic multi-agent system where a responding agent generates an initial response, a planner agent analyzes the query-response pair to determine which specialized refining agents (fact, persona, coherence) should execute in sequence, and each refining agent performs targeted improvements with full awareness of the planned sequence and justifications. The framework uses distinct LLMs for different roles—Claude Sonnet 3.5 for fact-refining and Sonnet 3 for other agents—with specialized prompt templates ensuring each agent focuses on its designated quality dimension. The orchestration logic parses the planner's output to determine execution order and passes the planned agent order and justifications to each refining agent as context.

## Key Results
- MARA achieves significant improvements across all four G-Eval metrics: coherence (2.32→2.67), groundedness (0.56→0.65), naturalness (1.98→2.15), and engagingness (2.54→2.83)
- Dynamic planner outperforms random agent assignment and fixed sequences by adapting refinement to query-specific requirements
- Fact-refining agent using Claude Sonnet 3.5 significantly improves groundedness compared to using the same LLM as other agents
- Removing planner justifications from agent inputs degrades performance, especially groundedness (-0.05)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Specialized agent decomposition reduces cognitive load and bias amplification compared to single-agent self-refinement.
- Mechanism: Each agent receives role-specific prompts focusing on one dimension (factuality, persona alignment, coherence), enabling targeted verification and refinement without interference from competing objectives.
- Core assumption: LLMs perform better when tasked with narrow, well-defined objectives rather than holistically evaluating multiple quality dimensions simultaneously.
- Evidence anchors: [abstract] "each agent is assigned a specific role for each aspect"; [section 3.2] Three specialized agents with distinct prompt templates: P_fact-refine, P_persona-refine, P_coherence-refine
- Break condition: If agent prompts are poorly designed or overlap significantly, specialization degrades into redundant processing.

### Mechanism 2
- Claim: Dynamic planner-driven agent selection adapts refinement to query-specific requirements, improving resource efficiency and output quality.
- Mechanism: A planner agent analyzes the query and initial response, then outputs a sequence of required agents with justifications. This allows skipping irrelevant agents (e.g., no fact-refining when no factual verification needed).
- Core assumption: The planner LLM can reliably identify which quality dimensions require attention for each query.
- Evidence anchors: [abstract] "adaptively selects and coordinates the most relevant agents based on the specific requirements of each query"; [section 3.3] Planner outputs sequence s_planner = LLM(P_planner(q, r))
- Break condition: If planner misclassifies query needs (e.g., misses factual errors), critical refinement steps are skipped.

### Mechanism 3
- Claim: Sequential refinement with planner justifications enables agent awareness and collaborative handoffs.
- Mechanism: Each refining agent receives the planner's sequence and justification, allowing it to understand its position and role relative to other agents. This prevents agents from undoing prior refinements.
- Evidence anchors: [section 3.3] "each agent also receives the planner's justifications, allowing each agent to understand its role in the sequence"; [Table 10] Removing planner outputs from agent inputs degrades performance, especially groundedness (-0.05)
- Break condition: If justifications are vague or agents ignore context, sequential refinement may introduce inconsistencies.

## Foundational Learning

- Concept: Multi-agent orchestration patterns (sequential vs. simultaneous vs. dynamic)
  - Why needed here: MARA's core innovation is dynamic sequential communication; understanding tradeoffs between patterns is essential for implementation.
  - Quick check question: Can you explain why simultaneous communication requires a finalizer agent while sequential does not?

- Concept: Role-specific prompt engineering
  - Why needed here: Each agent's effectiveness depends on prompt templates that enforce narrow focus (e.g., P_fact-refine emphasizes verification steps).
  - Quick check question: What would happen if P_persona-refine included factual verification instructions?

- Concept: Evaluation metrics for conversational quality
  - Why needed here: G-Eval metrics (coherence, groundedness, naturalness, engagingness) map directly to agent specializations; understanding them guides debugging.
  - Quick check question: Which agent specialization most directly impacts groundedness scores?

## Architecture Onboarding

- Component map:
  Responding Agent -> Planner Agent -> (Fact-Refining Agent | Persona-Refining Agent | Coherence-Refining Agent) -> Final Response

- Critical path:
  1. Responding Agent generates r
  2. Planner Agent analyzes (q, r) → outputs sequence + justifications
  3. Refining agents execute sequentially, each receiving prior agent's output and planner context
  4. Final refined response returned to user

- Design tradeoffs:
  - Simultaneous communication requires more agent calls (always 3 refining + 1 finalizer) vs. dynamic sequential (average ~2-3 agents, no finalizer)
  - Stronger LLM for fact-refining improves groundedness but increases cost (Table 8)
  - Fixed sequences are simpler but fail to adapt; dynamic planning adds latency but improves quality

- Failure signatures:
  - Planner selects wrong agents → critical errors unaddressed (e.g., factual hallucinations persist)
  - Agent ignores planner justifications → prior refinements overwritten
  - Prompt templates overlap → redundant processing, no quality gain
  - Excessive agent calls → latency unacceptable for real-time chat

- First 3 experiments:
  1. **Baseline comparison**: Run No Refine, Self-Refine, and MARA on 50 samples from FoCus; compare groundedness scores to validate fact-refining agent contribution.
  2. **Planner ablation**: Replace dynamic planner with fixed sequence (Fact → Persona → Coherence); measure performance drop to quantify planner value.
  3. **Agent isolation test**: Run each refining agent alone (without others) on queries where that agent should dominate (e.g., fact-heavy queries for fact-refining only); verify agent specialization is functional.

## Open Questions the Paper Calls Out

- Can fine-tuning the planner agent on a dataset of labeled ideal agent sequences close the performance gap with the oracle planner?
  - Basis: [explicit] The authors state the current unsupervised planner has "room for improvement" compared to the ideal planner and suggest "constructing a dataset with labeled ideal sets... followed by fine-tuning" as a future direction.
  - Why unresolved: The current implementation relies solely on unsupervised LLM prompting, which is less efficient and accurate than the theoretical optimal sequence selection.
  - What evidence would resolve it: A comparative analysis showing the performance delta and resource efficiency between the current prompt-based planner and a fine-tuned planner model.

- How can lightweight models be effectively utilized within the MARA framework to address scalability and resource efficiency concerns?
  - Basis: [explicit] In the Limitations section, the authors highlight concerns regarding "scalability and resource efficiency" and suggest "exploring lightweight or more efficient agent models" as a solution.
  - Why unresolved: Multi-agent systems typically require multiple LLM calls per query, which is computationally expensive compared to single-agent baselines.
  - What evidence would resolve it: Experiments evaluating the trade-off between performance degradation and computational cost savings when using smaller LLMs (e.g., Llama 8B) for the refining agents.

- To what extent does integrating external tools, such as Retrieval-Augmented Generation (RAG) systems, enhance the performance of the fact-refining agent?
  - Basis: [explicit] The authors note that the framework's flexibility allows for "incorporating tools such as Retrieval-Augmented Generation (RAG) systems" and identify this as an "interesting future research direction."
  - Why unresolved: The current fact-refining agent relies on the internal knowledge of the base LLM rather than external retrieval mechanisms.
  - What evidence would resolve it: An ablation study comparing the groundedness scores of the fact-refining agent with and without access to a RAG pipeline on knowledge-intensive datasets like INSCIT.

## Limitations
- Performance depends heavily on planner accuracy; current planner underperforms ideal planner by 0.09 points in G-Eval
- Agent specialization benefits assume well-designed prompts; overlapping prompt instructions could cause redundant processing
- Sequential refinement adds latency through multiple LLM calls, though dynamic sequencing reduces average agent count

## Confidence
- **High Confidence**: G-Eval metric improvements are reproducible and directly supported by experimental results
- **Medium Confidence**: The claim that specialized agents reduce cognitive load and bias is theoretically sound but lacks direct experimental evidence
- **Medium Confidence**: Dynamic planner adaptation improves resource efficiency based on comparison with random and fixed sequences

## Next Checks
1. **Planner Ablation Test**: Replace MARA's dynamic planner with a fixed sequence (Fact → Persona → Coherence) and measure performance degradation to quantify planner contribution.
2. **Agent Isolation Test**: Run each refining agent independently on queries where it should dominate (e.g., fact-heavy queries for fact-refining only) to verify specialization functionality.
3. **Prompt Overlap Analysis**: Design experiments where P_persona-refine includes factual verification instructions to test whether overlapping agent objectives degrade performance as predicted.