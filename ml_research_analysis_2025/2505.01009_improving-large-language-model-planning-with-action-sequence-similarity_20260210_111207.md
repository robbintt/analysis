---
ver: rpa2
title: Improving Large Language Model Planning with Action Sequence Similarity
arxiv_id: '2505.01009'
source_url: https://arxiv.org/abs/2505.01009
tags:
- exemplars
- grase
- planning
- grase-dc
- random
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the challenge of improving large language model
  (LLM) planning through in-context learning (ICL) by better selecting exemplars.
  The authors observe that problem-level similarity often misleads the model, since
  semantically similar tasks can require drastically different plans.
---

# Improving Large Language Model Planning with Action Sequence Similarity

## Quick Facts
- arXiv ID: 2505.01009
- Source URL: https://arxiv.org/abs/2505.01009
- Reference count: 40
- Improves planning accuracy by 11-40 absolute points using action sequence similarity for exemplar selection

## Executive Summary
This paper addresses the challenge of improving large language model (LLM) planning through in-context learning (ICL) by introducing a novel exemplar selection method based on action sequence similarity (AS). The authors observe that traditional problem-level similarity often misleads models because semantically similar tasks can require drastically different plans. Their two-stage pipeline, GRASE-DC, first re-ranks exemplars using model-generated plans and then dynamically clusters them to balance relevance and diversity, achieving significant accuracy improvements while reducing exemplar count.

## Method Summary
The method introduces Action Sequence Similarity (AS) as a more reliable signal than task description similarity for exemplar selection. The two-stage GRASE-DC pipeline works as follows: First, Generative Re-sampling of Action Sequence Similar Exemplars (GRASE) generates an initial plan using random exemplars, then re-ranks all candidates based on AS computed via longest common action sequence (LCAS). Second, Dynamic Clustering (DC) filters candidates by relevance threshold, performs hierarchical clustering, and samples exemplars from each cluster to balance relevance and diversity. The method can be applied iteratively (GRASE-DC*) for further improvements.

## Key Results
- Achieves 11-40 absolute accuracy points improvement on PDDL and natural language planning tasks
- Reduces exemplar count by 27.3% on average while maintaining or improving performance
- GRASE-DC* iteration achieves 18.9% more accuracy with 39% fewer exemplars
- Generalizes across different LLMs (Gemini, GPT-4, Claude, Llama) and problem complexities

## Why This Works (Mechanism)

### Mechanism 1
Action sequence similarity (AS) is a more reliable signal for exemplar selection than task description similarity for planning tasks. AS measures similarity via the longest common action sequence (LCAS) normalized by sequence lengths, capturing structural planning patterns rather than surface-level semantic overlap. Semantically similar task descriptions can require drastically different plans due to different object configurations and dependencies.

### Mechanism 2
Model-generated plans can outperform Oracle plans for exemplar selection because they align with the model's preferred reasoning direction. The model generates an initial plan (potentially with minor errors), which is used to compute AS with candidate exemplars. This retrieves exemplars matching the model's preferred decomposition strategy rather than forcing alignment with a possibly orthogonal Oracle solution.

### Mechanism 3
Dynamic clustering on AS balances relevance and diversity, reducing exemplar count while maintaining or improving performance. After GRASE re-ranking, hierarchical clustering groups exemplars by AS distance. High-relevance exemplars are always kept; remaining slots are filled by sampling across clusters to avoid redundancy while maintaining coverage.

## Foundational Learning

- **Longest Common Subsequence (LCS) algorithms**
  - Why needed here: AS computation requires finding the longest common action sequence between plans efficiently
  - Quick check question: Given sequences [pick, stack, unstack] and [unstack, pick, stack], what's the longest common subsequence?

- **In-Context Learning (ICL) sensitivity to exemplars**
  - Why needed here: The entire approach hinges on ICL performance being highly sensitive to exemplar selection
  - Quick check question: Why might semantically similar exemplars hurt ICL performance for planning tasks specifically?

- **Agglomerative Hierarchical Clustering**
  - Why needed here: DC stage uses this to group exemplars; understanding linkage criteria affects hyperparameter tuning
  - Quick check question: How does single-linkage vs. complete-linkage affect cluster shape and exemplar diversity?

## Architecture Onboarding

- **Component map:** Initial ICL (random exemplars) → Model generates initial plan → GRASE (compute AS, re-rank) → DC (filter, cluster, sample) → Final ICL (selected exemplars) → Optional iteration (GRASE-DC*)

- **Critical path:** GRASE stage (LCAS computation over all candidates) → DC clustering → Final ICL prompt assembly. The LCAS computation is O(N·k²) where N = candidates, k = average plan length—parallelizable on CPU.

- **Design tradeoffs:**
  - MLP approximation: ~95% of GRASE performance at ~66% FLOPs, but no iteration capability
  - BPE-Proxy: ~83% performance at ~27% FLOPs, best for resource-constrained deployment
  - GRASE-DC*: Highest accuracy but requires 2-3 inference passes

- **Failure signatures:**
  - Accuracy plateaus below random baseline → Check if AS computation is returning zero for all pairs
  - Exemplar count drops to 1-2 → Relevance threshold may be too aggressive; reduce multiplier
  - No improvement over random → Validator may be rejecting valid plans; verify VAL configuration

- **First 3 experiments:**
  1. Baseline comparison: Random vs. Task-similarity vs. Baseline_AS on Blocksworld with N=10, 20, 40 exemplars
  2. GRASE ablation: Compare GRASE using random initial plans vs. zero-shot initial plans
  3. DC hyperparameter sweep: Test Nc ∈ {1, 2, 3} and relevance threshold multipliers ∈ {1σ, 2σ, 3σ}

## Open Questions the Paper Calls Out

### Open Question 1
Can GRASE-DC effectively generalize to interactive, simulated environments like web agents or embodied AI? The authors plan to "investigate its application in more complex scenarios" and note that "real-world simulated tasks are another important future direction."

### Open Question 2
How does the granularity of plan representation (e.g., object-centric vs. execution-based) affect the performance of the GRASE-DC pipeline? The authors note results "motivate future work investigating what granularity or if mixed granularity can achieve improved performance."

### Open Question 3
Can higher-quality LLM embeddings provide a better efficiency-accuracy trade-off for approximating Action Sequence Similarity (AS) than the current MLP or Gecko approaches? The authors suggest "LLM embeddings may achieve higher correlation or better MLP performance with increased cost in the trade-off."

## Limitations
- Performance degrades significantly with smaller models (<70B parameters)
- Requires 2-3 inference passes for optimal performance, increasing computational cost
- Limited evaluation to static planning benchmarks; real-world interactive environments not tested

## Confidence
- Method effectiveness: High
- Computational efficiency claims: Medium
- Generalizability across domains: Medium

## Next Checks
1. Verify LCAS implementation computes consecutive subsequence correctly, not just subsequence
2. Test GRASE with random vs. zero-shot initial plans to validate sensitivity to initial plan quality
3. Run DC hyperparameter sweep with Nc ∈ {1, 2, 3} and threshold multipliers to find stability range