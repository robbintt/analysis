---
ver: rpa2
title: Training-Free Adaptation of New-Generation LLMs using Legacy Clinical Models
arxiv_id: '2601.03423'
source_url: https://arxiv.org/abs/2601.03423
tags:
- clinical
- capt
- patient
- response
- care
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: CAPT enables training-free adaptation of new-generation general-domain
  models using legacy clinical models by selectively integrating domain-specific knowledge
  via contrastive decoding. Evaluated on six clinical tasks, CAPT consistently outperforms
  both individual models and existing ensembling methods, achieving an average improvement
  of 17.6% over UniTE and 41.4% over proxy tuning across metrics.
---

# Training-Free Adaptation of New-Generation LLMs using Legacy Clinical Models
## Quick Facts
- arXiv ID: 2601.03423
- Source URL: https://arxiv.org/abs/2601.03423
- Reference count: 40
- Primary result: CAPT enables training-free adaptation of new-generation LLMs using legacy clinical models, achieving 17.6% average improvement over UniTE

## Executive Summary
This paper introduces Contrastive Adaptive Prompt Tuning (CAPT), a method for adapting large language models (LLMs) to clinical tasks without additional training. CAPT selectively integrates domain-specific knowledge from legacy clinical models into new-generation general-domain models using contrastive decoding. The approach addresses the challenge of maintaining both clinical accuracy and general-domain reasoning capabilities when adapting LLMs for specialized applications.

The method was evaluated across six clinical tasks and demonstrated consistent improvements over existing approaches. CAPT not only improves overall performance but also enhances clinically actionable language while reducing context errors. The token-level and physician analyses reveal that CAPT increases clinical specificity without sacrificing the fluency and reasoning capabilities of the underlying general-domain models.

## Method Summary
CAPT works by leveraging both a new-generation general-domain LLM and a legacy clinical model through a training-free contrastive decoding process. The method generates multiple candidate responses using different combinations of the two models and then selects the optimal response based on a contrastive scoring mechanism. This selective integration allows CAPT to benefit from the broad reasoning capabilities of modern LLMs while incorporating the domain-specific knowledge embedded in legacy clinical models.

The contrastive decoding process involves generating outputs from various model combinations, including using the general model alone, the clinical model alone, and hybrid approaches that blend their strengths. CAPT then applies a scoring function that balances performance metrics with clinical specificity and fluency considerations. This approach eliminates the need for expensive fine-tuning while addressing the challenge of adapting general-purpose models to specialized domains like healthcare.

## Key Results
- CAPT achieves an average improvement of 17.6% over UniTE and 41.4% over proxy tuning across six clinical tasks
- Token-level and physician analyses show CAPT amplifies clinically actionable language while reducing context errors
- CAPT increases clinical specificity without compromising general-domain reasoning and fluency

## Why This Works (Mechanism)
CAPT succeeds by selectively integrating domain-specific knowledge from legacy clinical models into new-generation LLMs through contrastive decoding. This approach allows the system to leverage the broad reasoning capabilities of modern LLMs while incorporating specialized clinical knowledge. The contrastive scoring mechanism enables optimal selection of responses that balance general reasoning with domain-specific accuracy.

The method addresses the fundamental challenge that general-domain LLMs, despite their strong reasoning capabilities, often lack the specialized knowledge required for clinical tasks. By using contrastive decoding rather than simple ensembling, CAPT can identify and amplify clinically relevant information while maintaining the linguistic fluency and general reasoning abilities of the base models.

## Foundational Learning
**Contrastive Learning**: A training paradigm where models learn by comparing similar and dissimilar examples. Why needed: Enables CAPT to differentiate between clinically relevant and irrelevant outputs. Quick check: Compare similarity scores between correct and incorrect clinical responses.

**Domain Adaptation**: The process of adapting models trained on general data to perform well on specialized domain tasks. Why needed: Clinical tasks require specific knowledge not present in general LLMs. Quick check: Measure performance drop when applying general models to clinical tasks.

**Knowledge Distillation**: A technique where a smaller or more specialized model learns from a larger or more general model. Why needed: CAPT leverages knowledge from both clinical and general models. Quick check: Compare knowledge transfer effectiveness between different model pairs.

**Ensembling Methods**: Techniques that combine multiple models to improve performance. Why needed: CAPT builds on ensembling concepts but uses contrastive decoding instead. Quick check: Compare CAPT performance against traditional ensembling baselines.

## Architecture Onboarding
**Component Map**: Legacy Clinical Model -> CAPT Contrastive Decoder -> Output Selection -> Final Response
**Critical Path**: Input -> General LLM Generation -> Clinical Model Generation -> Contrastive Scoring -> Optimal Response Selection
**Design Tradeoffs**: Training-free approach vs. potential performance gains from fine-tuning; computational cost of multiple generations vs. single model inference; complexity of contrastive scoring vs. simplicity of direct ensembling.
**Failure Signatures**: When clinical specificity is low, CAPT may over-rely on general model; when clinical knowledge is incorrect, CAPT may amplify errors; when contrastive scoring is imbalanced, CAPT may favor one model excessively.
**First Experiments**: 1) Compare CAPT against general LLM alone on clinical tasks; 2) Test CAPT with different contrastive scoring functions; 3) Evaluate CAPT performance across varying levels of clinical specificity requirements.

## Open Questions the Paper Calls Out
The paper does not explicitly call out specific open questions in the provided content.

## Limitations
- CAPT requires multiple generations from different models, increasing computational cost compared to single-model inference
- The effectiveness depends on the quality and relevance of the legacy clinical model's knowledge
- Contrastive scoring mechanisms may introduce additional complexity and potential points of failure

## Confidence
**High**: The method achieves consistent improvements across six clinical tasks and multiple metrics, with significant performance gains over established baselines like UniTE and proxy tuning.
**Medium**: The token-level and physician analyses provide qualitative validation of improved clinical specificity, though the methodology for these analyses is not fully detailed in the provided content.
**Low**: The paper does not provide information about computational efficiency comparisons or detailed failure mode analyses.

## Next Checks
1. Validate CAPT performance on additional clinical tasks not included in the original six-task evaluation
2. Conduct ablation studies to determine the impact of different contrastive scoring functions on performance
3. Measure and compare the computational cost of CAPT against single-model inference and traditional ensembling methods