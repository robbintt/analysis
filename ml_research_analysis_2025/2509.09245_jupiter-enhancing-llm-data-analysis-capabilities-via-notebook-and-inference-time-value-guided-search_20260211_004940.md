---
ver: rpa2
title: 'Jupiter: Enhancing LLM Data Analysis Capabilities via Notebook and Inference-Time
  Value-Guided Search'
arxiv_id: '2509.09245'
source_url: https://arxiv.org/abs/2509.09245
tags:
- data
- task
- value
- answer
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of multi-step reasoning and tool
  use in large language models for complex data analysis tasks. It introduces NbQA,
  a large-scale dataset of standardized task-solution pairs extracted from real-world
  Jupyter notebooks, and Jupiter, a framework that uses Monte Carlo Tree Search with
  a value model to guide efficient search for executable multi-step plans.
---

# Jupiter: Enhancing LLM Data Analysis Capabilities via Notebook and Inference-Time Value-Guided Search

## Quick Facts
- arXiv ID: 2509.09245
- Source URL: https://arxiv.org/abs/2509.09245
- Reference count: 36
- Primary result: Fine-tuned Qwen2.5 models with value-guided MCTS achieve 77.82% (7B) and 86.38% (14B) accuracy on InfiAgent-DABench, surpassing GPT-4o and other agent frameworks.

## Executive Summary
Jupiter addresses the challenge of multi-step reasoning and tool use in large language models for complex data analysis tasks. The framework introduces NbQA, a large-scale dataset of standardized task-solution pairs extracted from real-world Jupyter notebooks, and employs Monte Carlo Tree Search guided by a trained value model to efficiently search for executable multi-step plans. Experimental results demonstrate significant performance gains over state-of-the-art baselines on data analysis and mathematical reasoning benchmarks.

## Method Summary
The method constructs a dataset (NbQA) from 1.6M real-world Jupyter notebooks, standardizing them into verifiable task-solution pairs in ReAct format. A base LLM (Qwen2.5) is fine-tuned on this data, and a value model is trained to predict the success likelihood of intermediate reasoning steps using MCTS trajectories. During inference, Jupiter performs value-guided MCTS with exploration disabled ($c_{puct}=0$), allowing the value model to efficiently prune the search space and select promising execution paths.

## Key Results
- Fine-tuned Qwen2.5-7B and 14B models achieve 77.82% and 86.38% accuracy on InfiAgent-DABench, respectively.
- Jupiter outperforms GPT-4o and other agent frameworks on data analysis tasks.
- The framework shows strong generalization, improving pass@1 rates on the AIME mathematical reasoning benchmark despite being trained on data analysis tasks.
- Ablation studies confirm that removing the exploration term ($c_{puct}=0$) during inference yields the best performance when using a well-trained value model.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Standardizing real-world notebooks into verifiable task-solution pairs grounds the model in executable logic rather than static text.
- **Mechanism**: The pipeline converts raw Jupyter cells into a "Thought-Action-Output" format, filtering for executability and verifiability to ensure the model learns the causal link between code actions and execution results.
- **Core assumption**: Static code and outputs in notebooks accurately reflect a logical, executable reasoning process.
- **Evidence anchors**: Abstract mentions extracting high-quality tasks; section details fine-grained processing with GPT-4o for standardized output formats.
- **Break condition**: If notebooks contain deprecated or environment-specific code that fails in the standard sandbox.

### Mechanism 2
- **Claim**: Disabling the exploration term ($c_{puct}=0$) during inference allows a well-trained Value Model (VM) to prune the search space more efficiently than standard MCTS.
- **Mechanism**: Standard MCTS balances exploration and exploitation. Jupiter trains a VM to predict node value, then sets $c_{puct}=0$ during inference, relying purely on the VM's prediction to select the next node.
- **Core assumption**: The Value Model generalizes well enough that its single-point estimate is more reliable than averaging random rollouts.
- **Evidence anchors**: Section states exploration term is removed during inference; Table 2 shows "w/o ExpTerm" outperforms "with ExpTerm" when VM is present.
- **Break condition**: If the VM is under-trained or the test task drifts significantly from the training distribution.

### Mechanism 3
- **Claim**: Training the Value Model on both successful and failed trajectories enables robust OOD (Out-of-Distribution) generalization.
- **Mechanism**: The VM is trained using MSE loss against normalized Q-values derived from MCTS trajectories (including successes and errors). By learning to distinguish "dead-end" states from promising ones, the VM acts as a generic "reasoning correctness" critic, transferring skills to mathematical reasoning (AIME) without task-specific fine-tuning.
- **Core assumption**: The "value" of a reasoning step in data analysis correlates with the "value" of a reasoning step in mathematics.
- **Evidence anchors**: Section describes using trajectories including both successful and failed paths; AIME results show VM improves pass@1 rates on math benchmarks.
- **Break condition**: If the target domain requires fundamentally different tools not present in Python data science libraries.

## Foundational Learning

- **Concept**: Monte Carlo Tree Search (MCTS) & PUCT
  - **Why needed here**: Jupiter frames data analysis as a tree search. Understanding how PUCT balances "exploiting" the Value Model vs. "exploring" new nodes is critical to understanding why removing the exploration term ($c_{puct}=0$) is a significant architectural choice.
  - **Quick check question**: In standard MCTS, why do we add an exploration bonus? In Jupiter inference, why is it removed?

- **Concept**: Process Supervision (vs. Outcome Supervision)
  - **Why needed here**: The Value Model is trained on the *process* (Q-values of intermediate nodes), not just the final result. This allows the system to correct course mid-execution.
  - **Quick check question**: Does the Value Model receive a reward only at the final answer, or does it estimate value for intermediate code execution steps?

- **Concept**: ReAct (Reasoning + Acting)
  - **Why needed here**: The NbQA dataset and Jupiter inference loop follow the "Thought -> Action (Code) -> Observation" paradigm.
  - **Quick check question**: In the Jupiter architecture, what component generates the "Thought" and what component validates the "Observation"?

## Architecture Onboarding

- **Component map**: GitHub Crawler -> Coarse Filter -> Fine-grained Processor (GPT-4o) -> NbQA Dataset -> SFT Model (Qwen) -> MCTS Trajectory Collector -> Value Model (VM) -> Jupiter Inference.
- **Critical path**:
  1. **Node Selection**: Use the VM to score all leaf nodes. Pick the best one (since $c_{puct}=0$).
  2. **Expansion**: Sample K "Thought-Action" pairs from the SFT LLM.
  3. **Execution**: Run the code in the Sandbox. If error, assign negative reward.
  4. **Update**: Backpropagate the value up the tree.
- **Design tradeoffs**:
  - **Sandbox vs. Static Analysis**: The system relies on a real sandbox for "ground truth" observations. This is accurate but slow (latency bottleneck).
  - **Exploration control**: Setting $c_{puct} > 0$ during inference increases diversity but lowers accuracy (Table 2). The team chose pure exploitation ($c_{puct}=0$) for efficiency.
- **Failure signatures**:
  - **Infinite Loop**: The model generates code that runs forever; mitigated by the 3-minute timeout per execution.
  - **Context Overflow**: If the tree depth is too high, the prompt exceeds 100k tokens; the system terminates the branch.
  - **Reward Hacking**: The model might generate code that simply prints the "expected" format without calculation (mitigated by ground-truth comparison in training, but risky in open-ended inference).
- **First 3 experiments**:
  1. **Sanity Check (SFT Only)**: Fine-tune Qwen-7B on NbQA and run ReAct on InfiAgent-DABench without search. You should see a ~20% lift over the base model (Table 1).
  2. **Ablation (Exploration)**: Run Jupiter inference with $c_{puct}=1.25$ vs. $c_{puct}=0$. Confirm that the non-exploration setting yields higher accuracy (Table 2).
  3. **Generalization Test**: Run the trained VM on DSBench data modeling tasks. Verify that "Search + VM" outperforms "Search w/o VM" as iterations increase (Figure 4).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can a learned, model-based reward function enable effective reinforcement learning (RL) integration with Jupiter, overcoming the performance degradation caused by sparse, rule-based rewards?
- Basis in paper: [explicit] The authors note in the supplementary material that RL training failed due to sparse rewards and suggest employing a model-based reward function where a learned judge model determines whether the output is correct.
- Why unresolved: The current framework relies solely on supervised fine-tuning and value-model guidance because initial RL attempts using rule-based matching for correctness yielded inferior results compared to SFT.
- What evidence would resolve it: Demonstration of a training run where an RL-finetuned Jupiter model outperforms the SFT baseline on InfiAgent-DABench using a dense, model-based reward signal.

### Open Question 2
- Question: How effectively does the Jupiter framework generalize to complex data analysis workflows involving neural networks, pre-trained models, or GPU-based computation?
- Basis in paper: [inferred] The authors explicitly state they filtered out all notebooks involving neural networks, pre-trained models, or GPU-based computation to focus on classical tasks.
- Why unresolved: The current NbQA dataset and experimental results are restricted to classical data analysis (e.g., sklearn, pandas), leaving the framework's ability to reason over deep learning pipelines untested.
- What evidence would resolve it: Evaluation results on a benchmark of deep learning tasks showing that the value model can guide multi-step reasoning in a GPU-accelerated environment.

### Open Question 3
- Question: Is the finding that removing the MCTS exploration term ($c_{puct}=0$) improves performance robust across different model architectures, or is it an artifact of the specific value model calibration?
- Basis in paper: [inferred] The experiments conclude that "removing the exploration term leads to the best performance," assuming a "strong value model alone is sufficient," which contradicts standard MCTS theory where exploration is necessary to avoid local optima.
- Why unresolved: While effective for the Qwen models tested, this greedy selection strategy ($c_{puct}=0$) relies heavily on the value model being perfectly calibrated; if the value model has blind spots, this strategy could fail catastrophically compared to standard MCTS.
- What evidence would resolve it: Ablation studies on a diverse set of base models (e.g., Llama, Mistral) showing that zero-exploration consistently outperforms standard PUCT, or theoretical analysis of the value model's calibration.

## Limitations
- Heavy reliance on a sandboxed Python environment creates a latency bottleneck that scales poorly with deeper search trees.
- The choice to disable exploration ($c_{puct}=0$) during inference may increase risk of premature convergence if the Value Model is poorly calibrated.
- Potential contamination from common benchmark datasets during the 1.6M notebook crawl, despite keyword filtering.

## Confidence
- **High Confidence**: The SFT baseline results (77.82% Qwen2.5-7B accuracy) and the core mechanism of using MCTS with a trained Value Model are well-supported by ablation studies and multiple benchmark evaluations.
- **Medium Confidence**: The out-of-distribution generalization to AIME mathematical reasoning relies on the assumption that data analysis reasoning patterns transfer to mathematical problem-solving, which is plausible but not extensively validated.
- **Medium Confidence**: The choice to disable exploration ($c_{puct}=0$) during inference is empirically justified but may be brittle if the Value Model encounters substantially different task distributions.

## Next Checks
1. **Sandbox Robustness Test**: Measure how execution latency and error rates scale with search depth beyond 10 steps, and test with computationally intensive operations (e.g., large matrix multiplications) to identify timeout thresholds.
2. **Value Model Calibration**: Perform a bias-variance analysis of the Value Model predictions across successful vs. failed trajectories to quantify its reliability in guiding search vs. random exploration baselines.
3. **Contamination Verification**: Audit the filtering pipeline by cross-referencing the NbQA task descriptions against common public datasets (Kaggle, UCI) to ensure benchmark leakage is truly minimized.