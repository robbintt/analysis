---
ver: rpa2
title: Stochastic Quantum Spiking Neural Networks with Quantum Memory and Local Learning
arxiv_id: '2506.21324'
source_url: https://arxiv.org/abs/2506.21324
tags:
- neuron
- quantum
- spiking
- neurons
- each
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the stochastic quantum spiking (SQS) neuron
  and SQS neural network (SQSNN) model, which combines quantum computing with neuromorphic
  design. The SQS neuron uses a multi-qubit quantum circuit to implement an internal
  quantum memory mechanism, enabling event-driven probabilistic spike generation in
  a single shot without repeated measurements.
---

# Stochastic Quantum Spiking Neural Networks with Quantum Memory and Local Learning

## Quick Facts
- **arXiv ID:** 2506.21324
- **Source URL:** https://arxiv.org/abs/2506.21324
- **Reference count:** 40
- **Key outcome:** SQSNN achieves competitive accuracy on image classification while enabling event-driven single-shot inference and local learning compatible with NISQ hardware.

## Executive Summary
This paper introduces the Stochastic Quantum Spiking (SQS) neuron and SQS neural network (SQSNN) model, which combines quantum computing with neuromorphic design. The SQS neuron uses a multi-qubit quantum circuit to implement an internal quantum memory mechanism, enabling event-driven probabilistic spike generation in a single shot without repeated measurements. This design addresses key limitations of prior quantum spiking models that relied on single-qubit implementations and multi-shot probability estimation. The proposed SQSNN architecture can be trained using a hardware-compatible local learning rule based on perturbation-based gradient estimates and global feedback signals, eliminating the need for global backpropagation on classical simulators. Experimental results on image classification tasks (MNIST, FMNIST, KMNIST) and neuromorphic data (MNIST-DVS) demonstrate that SQSNN achieves competitive or superior accuracy compared to classical SNNs and previous quantum neural models, while maintaining moderate spiking rates that suggest favorable energy efficiency. The model shows particular promise for quantum hardware with low repetition rates and mid-circuit measurement capabilities.

## Method Summary
The SQSNN implements a feedforward architecture where each neuron is a multi-qubit quantum circuit combining input-output and memory qubits. Classical spike inputs are encoded as weighted currents that drive parameterized rotations on the input qubit. The neuron processes this through a parameterized quantum circuit (PQC) that entangles input and memory qubits, with only the input qubit measured to generate stochastic spikes. Memory qubits retain quantum state across time steps without measurement, enabling temporal integration. Training uses a local zeroth-order optimization approach: synaptic weights are updated via Simultaneous Perturbation Stochastic Approximation (SPSA) using random perturbations and global scalar feedback, while quantum circuit parameters use the Parameter Shift Rule. This avoids global backpropagation and is compatible with hardware-native training. The model is evaluated on image classification benchmarks using rate-encoded spike trains and on neuromorphic MNIST-DVS data.

## Key Results
- SQSNN achieves competitive or superior accuracy on MNIST, FMNIST, KMNIST, and MNIST-DVS datasets compared to classical SNNs and prior quantum neural models
- The single-shot stochastic spiking mechanism enables event-driven inference without multi-shot measurements required by previous quantum spiking models
- Local zeroth-order optimization allows hardware-compatible training without global backpropagation, maintaining accuracy while reducing classical simulation overhead
- Moderate spiking rates suggest favorable energy efficiency compared to traditional spiking neural networks

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Single-shot stochastic spiking may reduce inference latency compared to prior quantum spiking models requiring repeated measurements.
- **Mechanism:** Unlike QLIF neurons that estimate firing probability via multi-shot measurements, the SQS neuron measures input-output qubits once per time step $t$. Spike generation follows Born's rule ($p(s_t) = \langle s_t|\rho^I_t|s_t \rangle$), treating the measurement collapse as the probabilistic firing event itself.
- **Core assumption:** The inherent stochasticity of quantum measurement is a sufficient substitute for classical threshold-based spiking mechanisms.
- **Evidence anchors:**
  - [Abstract] Enables "event-driven probabilistic spike generation in a single shot."
  - [Page 5] "Spike generation... is stochastic, occurring within a single shot via mid-circuit measurements."
  - [Corpus] [2511.11320] Notes that biologically-grounded SNNs often move away from backpropagation, aligning with SQS's hardware-centric inference.
- **Break condition:** If the measurement error or readout noise overwhelms the probability distribution $\rho^I_t$, spike reliability degrades.

### Mechanism 2
- **Claim:** Internal quantum memory potentially allows the network to capture temporal dependencies without classical recurrent connections.
- **Mechanism:** The neuron splits qubits into input-output ($N$) and memory ($N_M$) registers. While input qubits are measured and reset, memory qubits remain unmeasured, carrying the quantum state $\sigma^M_t$ to step $t+1$ via the unitary $U(z_{t+1}, \theta)$. This effectively entangles historical inputs with the current state.
- **Core assumption:** Quantum coherence can be maintained in the memory qubits for the duration of the sequence $T$.
- **Evidence anchors:**
  - [Page 5] "The memory qubits are not measured... preserving quantum information [that] influences subsequent time steps."
  - [Page 13] Eq. (8) formally describes the state evolution $\rho^{IM}_{t+1}$ involving the post-measurement memory state.
  - [Corpus] [2512.18575] Discusses memory-augmented SNNs generally; SQS implements this memory in Hilbert space rather than classical registers.
- **Break condition:** If decoherence times $T_1/T_2$ are shorter than the sequence length, the memory state degrades into noise.

### Mechanism 3
- **Claim:** Local zeroth-order optimization allows hardware-native training by approximating gradients without backpropagation.
- **Mechanism:** The network runs $M$ global forward passes. Synaptic weights are updated via Simultaneous Perturbation Stochastic Approximation (SPSA) using random perturbation vectors $\triangle w$. PQC angles use the Parameter Shift Rule ($\theta \pm \pi/2$). Gradients are scaled by a global scalar feedback $\ell$ broadcast from the output layer.
- **Core assumption:** The scalar feedback signal provides sufficient directional information for local parameters to converge.
- **Evidence anchors:**
  - [Page 6] "Updates synaptic weights using only information locally available... avoiding global backpropagation."
  - [Page 20] Eq. (24) and (25) define the gradient estimates for weights and angles respectively.
  - [Corpus] [2511.11320] Highlights "Equilibrium Propagation" as a local learning alternative; SQS offers a distinct, perturbation-based local alternative for quantum systems.
- **Break condition:** If the number of shots $M_{syn}$ or $M_{som}$ is too low, gradient variance may prevent convergence.

## Foundational Learning

- **Concept: Born's Rule & Measurement**
  - **Why needed here:** The spiking mechanism is not a classical threshold function but a probabilistic collapse of a quantum state. You must understand $p(s|\psi)$ to interpret the "spike rate."
  - **Quick check question:** If a qubit is in state $|+\rangle$, what is the probability of measuring a spike (outcome '1')?

- **Concept: Parameter Shift Rule (PSR)**
  - **Why needed here:** The paper uses PSR to estimate gradients for quantum circuit parameters $\theta$. It allows analytic-grade gradient estimates from measurement statistics, crucial for the local learning rule.
  - **Quick check question:** How does PSR differ from finite differences in terms of required precision and gate operations?

- **Concept: Zeroth-Order Optimization (SPSA)**
  - **Why needed here:** Classical synaptic weights are trained using SPSA (perturbation-based), not automatic differentiation.
  - **Quick check question:** Why might SPSA be preferred over coordinate descent in high-dimensional parameter spaces typical of neural networks?

## Architecture Onboarding

- **Component map:** Classical current accumulation -> $R_X(\phi)$ rotations on Input Qubits -> Parameterized Quantum Circuit (PQC) entangling Input + Memory Qubits -> Measurement of Input Qubits only; Memory Qubits loop back

- **Critical path:** The coherence of the **Memory Qubits** is the system bottleneck. If the PQC depth or the time between steps $t$ and $t+1$ exceeds hardware coherence times, the "memory" mechanism fails.

- **Design tradeoffs:**
  - **Qubit Count ($N_M$):** Higher $N_M$ increases expressivity but requires longer coherence and more entangling gates.
  - **Shots for Learning ($M$):** Low $M$ speeds up training but increases gradient noise (see Fig 2a vs 2b).
  - **Sparsity:** The design exploits event-driven logic (if $z \le 0$, no rotation), saving gate operations.

- **Failure signatures:**
  - **Flat Accuracy:** Check if learning rates or perturbation sizes $\epsilon$ in SPSA are mismatched with the global feedback scale.
  - **Random Output:** Indicates memory qubits have decohered or entangling gates in the PQC are failing.
  - **Hardware Noise:** Fig 2a shows significant variance on IBM devices without Error Mitigation (QEM).

- **First 3 experiments:**
  1. **Simulated Sanity Check:** Implement a single SQS neuron (1 input, 1 memory qubit) on a classical simulator (e.g., Qiskit) to verify the spike probability tracks the input current $z_t$ over time.
  2. **Memory Ablation:** Run the full SQSNN on MNIST with $N_M=0$ (no memory) vs $N_M=1$ to quantify the performance gain from quantum memory specifically.
  3. **Learning Rate Sweep:** On the USPS task, vary the number of global passes $M \in \{1, 5\}$ and shots $M_{syn}$ to replicate the stability trends shown in Fig 2b.

## Open Questions the Paper Calls Out

- **Open Question 1:** Can structured state-space models (SSMs) be integrated with SQS neurons to enhance long-range temporal dependency modeling?
  - **Basis in paper:** [explicit] "Future work may explore combining SQS neurons with structured state-space models (SSMs) to further enhance temporal modeling capacity"
  - **Why unresolved:** The current SQSNN uses simple quantum memory via QRNN-style dynamics; whether SSM architectures could better capture long-range dependencies in quantum spiking networks is unexplored.
  - **What evidence would resolve it:** Experiments comparing SQSNNs with and without SSM integration on tasks requiring long temporal context (e.g., long-sequence NLP benchmarks), measuring accuracy and memory retention.

- **Open Question 2:** What are the theoretical expressivity and trainability properties of SQSNNs as dynamic parameterized quantum circuits?
  - **Basis in paper:** [explicit] "...addressing theoretical aspects around the expressivity and trainability of SQSNNs"
  - **Why unresolved:** While empirical results show strong performance, no formal analysis exists of whether SQSNNs avoid barren plateaus or how their expressivity compares to classical SNNs and other quantum architectures.
  - **What evidence would resolve it:** Formal proofs or empirical studies mapping loss landscapes, gradient magnitudes, and function approximation capacity across varying network depths, widths, and qubit counts.

- **Open Question 3:** How does SQSNN performance degrade with increasing sequence length due to decoherence in memory qubits?
  - **Basis in paper:** [inferred] The paper demonstrates results on sequences up to T=80 (MNIST-DVS) but does not analyze how quantum memory fidelity decays over longer time horizons on NISQ hardware.
  - **Why unresolved:** The quantum memory mechanism relies on unmeasured memory qubits retaining coherence across time steps; real hardware noise may limit practical sequence lengths.
  - **What evidence would resolve it:** Benchmarking SQSNN accuracy versus sequence length on quantum hardware with varying coherence times, comparing against noise-mitigated and simulated baselines.

## Limitations

- **Coherence time constraints:** The quantum memory mechanism depends on maintaining coherence across entire sequence lengths, which may exceed practical NISQ hardware limits for longer sequences.
- **Scalability uncertainty:** The local learning rule's effectiveness in deeper networks beyond the single hidden layer demonstrated remains unproven.
- **Energy efficiency claims:** Moderate spiking rates suggest efficiency benefits, but actual power measurements comparing quantum vs classical implementations are not provided.

## Confidence

- **High confidence:** The single-shot stochastic spiking mechanism and circuit architecture specification are well-defined and verifiable through provided equations and pseudocode.
- **Medium confidence:** Competitive accuracy results are demonstrated, but baseline comparison selection could be more comprehensive, and energy efficiency claims are suggestive rather than proven through actual measurements.
- **Low confidence:** Claims about quantum memory enabling temporal dependencies and local learning rule scalability to deeper networks are theoretically plausible but lack empirical validation.

## Next Checks

1. **Coherence time verification:** Implement the SQS neuron on a real quantum device and measure the fidelity of the memory state after each time step for sequences of increasing length (T=5, T=10, T=20) to determine the practical memory retention limit.
2. **Local learning scalability test:** Extend the architecture to include two hidden layers of SQS neurons and evaluate whether the local zeroth-order optimization maintains training stability and achieves comparable accuracy to the single-layer baseline.
3. **Hardware overhead quantification:** Measure the actual gate count and circuit depth for the SQS neuron implementation and compare it with classical SNN implementations to validate the claimed energy efficiency advantages.