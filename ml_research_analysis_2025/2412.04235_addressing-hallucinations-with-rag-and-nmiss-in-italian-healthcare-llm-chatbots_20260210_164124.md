---
ver: rpa2
title: Addressing Hallucinations with RAG and NMISS in Italian Healthcare LLM Chatbots
arxiv_id: '2412.04235'
source_url: https://arxiv.org/abs/2412.04235
tags:
- nmiss
- arxiv
- hallucinations
- language
- metrics
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study combines Retrieval-Augmented Generation (RAG) and a
  novel evaluation framework to detect and mitigate hallucinations in large language
  models. The approach uses a multilingual Sentence Transformer to retrieve relevant
  context from Italian health news articles, which is then supplied to LLMs for answer
  generation.
---

# Addressing Hallucinations with RAG and NMISS in Italian Healthcare LLM Chatbots

## Quick Facts
- **arXiv ID**: 2412.04235
- **Source URL**: https://arxiv.org/abs/2412.04235
- **Reference count**: 40
- **Primary result**: RAG combined with NMISS effectively mitigates hallucinations and refines evaluation in Italian healthcare LLM chatbots.

## Executive Summary
This study introduces a novel approach to detect and mitigate hallucinations in large language models by combining Retrieval-Augmented Generation (RAG) with the Negative Missing Information Scoring System (NMISS). The method retrieves relevant context from Italian health news articles using a multilingual Sentence Transformer, then generates answers constrained to this context. NMISS extends traditional evaluation metrics by rewarding contextually valid elaborations that are present in the retrieved data but not in reference answers, distinguishing them from hallucinations. Experiments show that mid-tier models like LLaMA2, LLaMA3, and Mistral benefit significantly from NMISS, demonstrating their ability to provide richer contextual information even when traditional scores are lower.

## Method Summary
The approach combines RAG with a novel evaluation framework to address hallucinations in LLM outputs. Italian health news articles (126,470 documents from 2010–2024) are preprocessed and chunked into 512-token segments with 64-token overlap, then indexed using a multilingual Sentence Transformer for semantic retrieval. For each of 100 manually curated questions with reference answers, the system retrieves top-K context chunks, generates responses using zero-shot prompts that constrain outputs to the retrieved context, and evaluates using both traditional metrics (BLEU, ROGUE, METEOR, BERTScore, Exact Match) and NMISS. NMISS computes a weighted score combining reference alignment and context alignment for tokens unmatched by the reference, using a max safeguard to prevent score deflation.

## Key Results
- RAG mitigates hallucinations by grounding LLM outputs in external, domain-specific documents.
- NMISS extends traditional metrics by rewarding tokens that match retrieved context but not the reference, distinguishing contextually valid elaborations from hallucinations.
- Mid-tier models like LLaMA2, LLaMA3, and Mistral show substantial NMISS gains, especially in high-difficulty cases, indicating their ability to generate contextually meaningful content even when traditional scores are lower.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Retrieval-Augmented Generation reduces hallucinations by grounding LLM outputs in external, domain-specific documents.
- Mechanism: The retriever component selects top-K context chunks via semantic similarity; the generator conditions on both query and retrieved context, producing responses with tokens explicitly constrained by the external source.
- Core assumption: Retrieval accuracy is sufficient and the retrieved context contains relevant, factually correct information; generation strictly follows the prompt instruction to use only provided context.
- Evidence anchors:
  - [abstract] "While RAG mitigates hallucinations by grounding answers in external data, NMISS refines the evaluation..."
  - [Section 3.1] "RAG models combine the strengths of retrieval-based and generative language models to address issues such as hallucinations and factual inaccuracies..."
  - [corpus] Related work "When Evidence Contradicts: Toward Safer Retrieval-Augmented Generation in Healthcare" notes RAG can introduce errors if evidence conflicts, suggesting the grounding assumption may break under contradictory retrieved content.
- Break condition: If retrieval returns irrelevant or contradictory context, grounding may fail or propagate incorrect information; the fallback message ("I'm sorry, I can't help you based on the information I have") may not trigger reliably for ambiguous cases.

### Mechanism 2
- Claim: NMISS extends traditional metrics (BLEU, ROUGE, METEOR) by rewarding tokens that match the retrieved context but not the reference, thereby distinguishing contextually valid elaborations from hallucinations.
- Mechanism: NMISS extracts unmatched tokens from the system answer relative to the reference, checks their presence in the context, computes a weighted score combining reference alignment and context alignment, and returns the maximum of the traditional and weighted scores.
- Core assumption: Tokens present in both system output and context (but not in the reference) represent valid contextual elaborations rather than noise or hallucinations; the weighting scheme correctly balances reference and context contributions.
- Evidence anchors:
  - [abstract] "NMISS... accounts for contextual relevance in responses... identifying cases where traditional metrics incorrectly flag contextually accurate responses as hallucinations."
  - [Section 3.3.1] "Such additions can take two forms: i) hallucinations... and ii) contextually relevant elaborations..."
  - [corpus] No direct corpus evidence on NMISS specifically; related work focuses on different detection approaches (e.g., logit-based, consistency-based).
- Break condition: If the context contains incorrect or irrelevant information, tokens matching it may be incorrectly rewarded; verbose models may inflate scores through sheer volume of context-matched tokens without meaningful contribution.

### Mechanism 3
- Claim: Mid-tier models benefit more from NMISS than top-tier models because their outputs diverge lexically from references while remaining contextually grounded.
- Mechanism: Models like LLaMA2, LLaMA3, and Mistral exhibit higher hallucination rates but also higher NMISS outperformance, suggesting their elaborations—though non-literal—are often contextually valid; GPT-4's strict adherence to prompt phrasing leaves little room for contextual enhancement.
- Core assumption: The positive correlation between hallucination rate and NMISS outperformance for BLEU/ROUGE-1 reflects compensatory contextual grounding rather than systematic metric gaming.
- Evidence anchors:
  - [abstract] "Mid-tier models, such as Llama2, Llama3, and Mistral benefit significantly from NMISS, highlighting their ability to provide richer contextual information."
  - [Section 5.2] "A positive correlation emerges between hallucination rate and NMISS outperformance for BLEU and ROUGE-1..."
  - [corpus] Related work "Disparities in Multilingual LLM-Based Healthcare Q&A" suggests performance varies across models and languages, but does not directly address NMISS-style evaluation.
- Break condition: If mid-tier models' elaborations include hallucinations that coincidentally match context vocabulary, NMISS may incorrectly reward them; the definition of "valid" contextual elaboration depends on human annotation quality.

## Foundational Learning

- Concept: **Retrieval-Augmented Generation (RAG)**
  - Why needed here: Core mitigation mechanism; understanding the retriever-generator split and how context conditions generation is essential.
  - Quick check question: If the retriever returns irrelevant chunks, how should the generator respond according to the paper's prompt design?

- Concept: **Hallucination types (factuality vs. faithfulness)**
  - Why needed here: The paper explicitly distinguishes these; NMISS addresses cases where content is faithful to context but not in the reference.
  - Quick check question: Is a response that adds contextually relevant information not in the reference a factuality or faithfulness hallucination?

- Concept: **Evaluation metrics (BLEU, ROUGE, METEOR, BERTScore)**
  - Why needed here: NMISS builds on these; understanding their limitations (lexical focus vs. semantic sensitivity) clarifies why NMISS is proposed.
  - Quick check question: Which existing metric already captures some semantic similarity, and why does NMISS still add value beyond it?

## Architecture Onboarding

- Component map: Data layer (Italian health news corpus) -> Embedding layer (Sentence Transformer) -> Retrieval layer (Vector store) -> Generation layer (LLMs with zero-shot prompt) -> Evaluation layer (Traditional metrics + NMISS)

- Critical path: 1. Query embedding → 2. Vector store retrieval → 3. Context assembly → 4. LLM generation with constrained prompt → 5. Human annotation (hallucinated/non-hallucinated) → 6. Metric computation (traditional + NMISS)

- Design tradeoffs:
  - Chunk size (512 tokens) balances retrieval accuracy vs. context continuity; larger chunks may introduce noise, smaller may fragment meaning.
  - Temperature 0.7 for open-source models ensures comparability but may increase variability.
  - Zero-shot prompting avoids fine-tuning costs but limits domain adaptation.
  - NMISS weighted scheme prevents context dominance but may underweight valid elaborations in short responses.

- Failure signatures:
  - High hallucination rate + low NMISS outperformance: model generates content neither in reference nor context (true hallucinations).
  - Low traditional scores + high NMISS outperformance: model produces contextually grounded but lexically divergent responses (likely mid-tier behavior).
  - High exact match + low NMISS gain: model adheres strictly to prompt/reference (e.g., GPT-4), leaving little room for contextual elaboration.

- First 3 experiments:
  1. **Retrieval quality audit**: Manually inspect top-K retrieved chunks for 20 queries across difficulty levels; measure relevance and contradiction rates.
  2. **NMISS ablation**: Compute scores with λ₁ and λ₂ set to extremes (reference-only vs. context-only) to understand weighting sensitivity.
  3. **Cross-domain validation**: Apply RAG-NMISS framework to a different domain (e.g., legal, financial news) to test generalizability claims; monitor whether NMISS patterns hold.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can NMISS be effectively extended to multilingual environments and multi-hop question answering tasks?
- Basis in paper: [explicit] The conclusion states, "Future work could extend NMISS to multilingual and multi-hop question answering..."
- Why unresolved: The current study validates NMISS only on monolingual (Italian) single-hop QA tasks using specific health news contexts.
- What evidence would resolve it: Successful application of NMISS to datasets requiring reasoning across multiple documents (multi-hop) and performance consistency across different languages.

### Open Question 2
- Question: How can NMISS be integrated into reinforcement learning pipelines to align model outputs with context-based quality objectives?
- Basis in paper: [explicit] The authors propose future work involving "reinforcement learning frameworks that align model outputs with context-based quality objectives."
- Why unresolved: NMISS is currently utilized strictly as a post-hoc evaluation metric rather than a direct training signal or reward mechanism.
- What evidence would resolve it: Demonstrating a fine-tuning loop where NMISS scores serve as a reward signal that improves factual grounding without inducing reward hacking.

### Open Question 3
- Question: Does the correlation between hallucination rates and NMISS outperformance hold across domains outside of Italian healthcare and for non-news data structures?
- Basis in paper: [inferred] The study relies on a specific dataset of 100 Italian health news questions and a vector store of news articles.
- Why unresolved: It is unclear if NMISS effectively distinguishes hallucinations from valid elaborations in domains with different lexical densities or non-journalistic text styles (e.g., clinical notes).
- What evidence would resolve it: Replicating the RAG-NMISS framework on distinct domains (e.g., legal or technical manuals) to verify if the metric remains robust against verbosity bias.

## Limitations

- The weighting scheme (λ₁, λ₂) that balances reference alignment against context alignment is not justified empirically, raising concerns about potential metric gaming by verbose models.
- The retrieval quality—critical to NMISS's effectiveness—is not rigorously validated; contradictory or irrelevant retrieved context could lead to incorrect rewards for hallucinations that happen to match context vocabulary.
- The study's findings are limited by its narrow domain (Italian healthcare news) and small evaluation set (100 questions), leaving generalizability to other languages, domains, and scale uncertain.

## Confidence

- **High Confidence**: RAG's ability to reduce hallucinations when retrieval is accurate and the generation prompt successfully constrains outputs to retrieved context. The mechanism is well-established and the prompt design is clearly specified.
- **Medium Confidence**: NMISS's novel contribution in detecting contextually valid elaborations. While the metric design is explicit, its empirical validation is limited to one domain and evaluation set.
- **Medium Confidence**: Mid-tier models' superior NMISS gains reflect genuine contextual grounding rather than metric gaming. The correlation between hallucination rate and NMISS outperformance is observed but not causally proven.

## Next Checks

1. **Retrieval Quality Audit**: Manually inspect top-K retrieved chunks for 20 queries across difficulty levels to measure relevance and contradiction rates. This will validate whether NMISS's grounding assumption holds in practice.
2. **NMISS Ablation Study**: Compute NMISS scores with extreme weighting schemes (reference-only vs. context-only) to understand how sensitive the metric is to its balancing parameters and whether it can be gamed by verbose outputs.
3. **Cross-Domain Validation**: Apply the RAG-NMISS framework to a different domain (e.g., legal or financial news) with a new set of questions and references to test whether the observed patterns (particularly mid-tier model gains) generalize beyond Italian healthcare.