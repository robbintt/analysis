---
ver: rpa2
title: 'ToolFactory: Automating Tool Generation by Leveraging LLM to Understand REST
  API Documentations'
arxiv_id: '2501.16945'
source_url: https://arxiv.org/abs/2501.16945
tags:
- tool
- parameter
- tools
- documentation
- information
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This work introduces ToolFactory, an open-source pipeline for\
  \ automatically generating AI-compatible tools from unstructured REST API documentation.\
  \ To enable reliable extraction from varied documentation formats, the authors developed\
  \ the API Extraction Benchmark\u2014167 API documents with 744 endpoints\u2014and\
  \ designed a JSON schema to standardize annotations."
---

# ToolFactory: Automating Tool Generation by Leveraging LLM to Understand REST API Documentations

## Quick Facts
- arXiv ID: 2501.16945
- Source URL: https://arxiv.org/abs/2501.16945
- Authors: Xinyi Ni; Qiuyang Wang; Yukun Zhang; Pengyu Hong
- Reference count: 40
- Key outcome: ToolFactory achieves 97% correct JSON structure generation and near-perfect parameter extraction, generating 92 validated tools for glycomics research databases

## Executive Summary
This paper introduces ToolFactory, an open-source pipeline that automatically generates AI-compatible tools from unstructured REST API documentation. The authors developed APILlama, a fine-tuned Llama 3 model using prompt tuning, to extract structured API specifications from diverse documentation formats. APILlama achieves high extraction accuracy and is integrated with a validation pipeline that diagnoses and corrects tool generation errors. Applied to glycomics research, ToolFactory successfully generated 92 validated tools for major databases, demonstrating its domain-agnostic capability.

## Method Summary
ToolFactory leverages prompt-tuned APILlama to extract structured API specifications from unstructured documentation, converting them into Python tools compatible with LangChain. The pipeline includes a validation mechanism that diagnoses errors and a knowledge base for inferring missing parameter values. The approach uses soft prompt tuning with 20 virtual tokens to encode extraction instructions and JSON schema, achieving efficiency while maintaining quality. A validation pipeline executes generated tools against live APIs, categorizing failures to enable targeted refinement.

## Key Results
- APILlama achieves 97% valid JSON structure ratio and near-perfect parameter extraction accuracy
- Generated 92 validated tools for glycomics research databases including GlyTouCan and PubChem
- Parameter value inference via knowledge base successfully improves tool validation rates from 17 to 33 tools in leave-one-API-out testing

## Why This Works (Mechanism)

### Mechanism 1: Schema Compression via Soft Prompt Tuning
Encoding extraction instructions and JSON schema into trainable soft prompts reduces token overhead while preserving extraction quality. By compressing ~572 tokens of schema definition into just 20 virtual tokens learned via prompt tuning at the embedding layer, the method maintains 97% valid JSON structure generation while significantly reducing instruction token count. The frozen base model approach ensures parameter efficiency while the soft prompt implicitly encodes both task instruction and target schema structure.

### Mechanism 2: Parameter Value Inference via Semantic Similarity Retrieval
Missing or invalid parameter values—the dominant failure mode—can be inferred from a knowledge base built from verified tools. When validation fails due to missing parameter values, the system queries a parameter database using embedding similarity, retrieving top candidates by description and parameter key name similarity. The first value that passes live API validation is accepted, successfully inferring values for 33 tools versus 17 for GPT-4o guessing in comparative testing.

### Mechanism 3: Validation-Driven Error Categorization
Executing generated tools against live APIs and classifying failures enables targeted refinement rather than treating all errors uniformly. Each tool is called with example parameters and responses are checked for HTTP 200 status and non-error content verified by GPT-4o evaluator. Failures are categorized into six types mapped to four root causes, directing remediation efforts—particularly for the most common failure mode of incorrect parameter values (30-55% of APILlama errors).

## Foundational Learning

- **REST API structure** (base URL, endpoint path, HTTP method, parameters): Why needed—the extraction schema must capture these components to generate callable tools. Without understanding that base_url + endpoint_path forms the full URL, or that parameters have names/types/required status, you cannot interpret the JSON schema or debug extraction failures. Quick check—Given an API documentation snippet listing GET /users/{id} with parameter id (required, integer), can you identify the method, path, and required parameter?

- **Prompt tuning vs. fine-tuning**: Why needed—APILlama uses soft prompt tuning (training ~82K parameters) rather than full fine-tuning (~8B parameters). Understanding this distinction explains why the method is efficient and where it might fail—soft prompts adapt behavior within the frozen model's existing capabilities. Quick check—If you observe APILlama failing on a fundamentally new task (e.g., extracting nested schemas), would increasing soft prompt tokens likely help, or would full fine-tuning be required?

- **Semantic similarity for retrieval**: Why needed—the parameter inference mechanism relies on embedding-based similarity search. You need to understand that cosine similarity on embeddings captures semantic relatedness, and that separate retrieval by key names vs. descriptions addresses different failure modes. Quick check—Why might a parameter named "q" (query string) retrieve irrelevant values if only key-name similarity is used, and how does adding description similarity help?

## Architecture Onboarding

- **Component map**: Raw API documentation → APILlama extraction → JSON validation against schema → Tool generation → Live API validation → Error categorization → (if C3) Parameter inference via KB → Refined tool

- **Critical path**: The pipeline flows from unstructured documentation through structured extraction, tool generation, live validation, error diagnosis, and targeted refinement via knowledge base inference for parameter value failures.

- **Design tradeoffs**: Prompt tuning vs. full fine-tuning offers parameter efficiency and prevents overfitting on small datasets (167 docs) but limits adaptation to schema variations not seen during training. The 8B model achieves comparable extraction to GPT-3.5 with open weights but retrieves fewer endpoints (66 vs. 84), likely due to base model reasoning limits. Live validation catches real failures but introduces latency, API instability, and potential side effects, mitigated by using authentication-free APIs.

- **Failure signatures**: Wrong Parameter Value (C3) causes tools to return 4xx/5xx or error JSON despite correct structure, being the most common failure (30-55% of APILlama errors). Missing Base URL (C1/C2) causes tool construction failures or request timeouts, often requiring manual annotation. Schema drift occurs when API documentation structure differs significantly from training set. Retrieval failure for inference happens when no similar parameters exist in KB (cosine similarity < 0.5 threshold).

- **First 3 experiments**: 1) Reproduce extraction quality on held-out docs by splitting API Extraction Benchmark 80/20, running APILlama inference, and computing Valid Ratio, semantic similarity metrics, and parameter accuracy compared to Llama-3-8B one-shot and GPT-3.5 baselines. 2) End-to-end tool generation test on 10 API docs not in training set, generating tools via full pipeline, validating against live APIs, and classifying errors by C1-C4 taxonomy to identify if C3 dominates. 3) Parameter inference ablation for tools failing due to missing parameter values, testing no inference, GPT-4o guessing, and similarity-based KB retrieval, measuring how many tools pass validation after each method following Section 5.3 protocol.

## Open Questions the Paper Calls Out

- **Model scaling impact**: Does scaling the base model significantly improve endpoint retrieval counts to match proprietary models? The study only fine-tunes the 8B parameter version, leaving the impact of model scale on retrieval volume untested. Evidence would require benchmarks comparing fine-tuned 70B Llama variants against current APILlama extraction metrics.

- **Authentication automation**: Can the pipeline be automated to handle complex authentication protocols like OAuth without manual intervention? The authors explicitly filtered for APIs that do not require authentication, noting keys must be applied manually in practical applications. Evidence would require successful automated generation and validation of tools requiring dynamic authentication headers or tokens.

- **Response schema importance**: Does the absence of response schemas in the generated JSON impair downstream agent planning accuracy? While valid for execution, ignoring output schemas may prevent agents from anticipating data structures required for sequential decision-making. Evidence would require a comparative study measuring agent performance on multi-step tasks using tools with and without generated response schemas.

## Limitations

- The soft prompt tuning approach is limited by the frozen 8B base model's reasoning capacity, creating a ceiling effect where prompt tuning cannot compensate for insufficient base model capability on complex extraction tasks.

- The parameter inference mechanism relies on semantic similarity assumptions that may not hold across domains, and the knowledge base may lack coverage for specialized parameter types or create feedback loops where initial errors propagate.

- The error taxonomy assumes clean mapping to four root causes, but real-world API interactions often involve cascading failures or ambiguous error responses, and the validation on authentication-free APIs sidesteps major production challenges.

## Confidence

- **High confidence**: Schema compression via soft prompt tuning achieves stated token reduction (572→20) and maintains extraction quality (97% valid JSON). The mathematical framework of prompt tuning is well-established and results are reproducible given the frozen base model and training procedure.

- **Medium confidence**: Parameter value inference via semantic similarity improves tool validation rates (33 vs 17 tools) but depends on KB coverage and similarity threshold tuning. The mechanism is sound but may fail in domain-specific contexts.

- **Low confidence**: Generalization to novel API documentation formats and the completeness of the error taxonomy. These claims extrapolate from limited data and require broader testing.

## Next Checks

1. **Schema drift stress test**: Evaluate APILlama on 20 API documents from documentation styles not represented in the training set (e.g., GraphQL schemas, gRPC protobufs, binary WSDLs). Measure valid JSON ratio, semantic similarity, and parameter accuracy. Compare failure modes to those observed on REST APIs to identify where soft prompt tuning reaches its limits.

2. **Knowledge base coverage analysis**: For tools failing parameter inference, analyze the top-5 retrieved candidates by both key-name and description similarity. Calculate the average cosine similarity scores and document how many retrieved parameters are actually applicable (passing validation). This will reveal whether the KB is too sparse or if the similarity threshold needs adjustment.

3. **Live API stability audit**: Run the full pipeline on 50 randomly selected APIs from the Live API-Bench dataset. Track API availability, response consistency, and error classification accuracy over 3 consecutive days. Identify what fraction of "tool failures" are actually API instability vs. extraction errors, and test whether the error taxonomy needs refinement for real-world conditions.