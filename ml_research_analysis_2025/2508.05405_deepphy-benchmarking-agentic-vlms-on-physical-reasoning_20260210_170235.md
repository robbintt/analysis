---
ver: rpa2
title: 'DeepPHY: Benchmarking Agentic VLMs on Physical Reasoning'
arxiv_id: '2508.05405'
source_url: https://arxiv.org/abs/2508.05405
tags:
- claude
- action
- physical
- gemini-2
- reasoning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'DeepPHY is a new benchmark suite designed to evaluate the interactive
  physical reasoning capabilities of Vision Language Models (VLMs) in dynamic, physics-based
  environments. It addresses the gap between static knowledge recall and real-time
  physical interaction by providing six challenging environments: PHYRE, I-PHYRE,
  Kinetix, Pooltool, Angry Birds, and Cut the Rope.'
---

# DeepPHY: Benchmarking Agentic VLMs on Physical Reasoning

## Quick Facts
- arXiv ID: 2508.05405
- Source URL: https://arxiv.org/abs/2508.05405
- Reference count: 40
- Primary result: Even the best VLMs achieve significantly lower success rates than humans on interactive physical reasoning tasks across six physics-based game environments

## Executive Summary
DeepPHY is a benchmark suite designed to evaluate the interactive physical reasoning capabilities of Vision Language Models (VLMs) in dynamic, physics-based environments. It addresses the gap between static knowledge recall and real-time physical interaction by providing six challenging environments: PHYRE, I-PHYRE, Kinetix, Pooltool, Angry Birds, and Cut the Rope. The benchmark includes fine-grained observation and action space conversions to make tasks more tractable for VLMs while preserving core physical reasoning challenges. Evaluations across 17 state-of-the-art models reveal that even the best-performing models struggle significantly, with success rates far below human performance. The results highlight a fundamental disconnect between models' ability to describe physical phenomena and their ability to predict and control outcomes in dynamic environments. DeepPHY serves as a rigorous testbed to expose these limitations and guide the development of more physically grounded AI agents.

## Method Summary
DeepPHY evaluates VLMs on six physics-based game environments by converting continuous physical interactions into discrete, text-compatible formats. The benchmark transforms complex action spaces into structured selections (e.g., grid cells, JSON arrays, Python function calls) to make tasks tractable for VLMs. Evaluations use zero-shot inference with 17 open and closed-source models across two prompting strategies: Vision-Language-Action (VLA) and World Model (WM). Success is measured by Success Rate (SR), Pass@K (success within K attempts), and Average Attempts, with temperature set to 0.1. The approach preserves core physical reasoning challenges while lowering the control interface barrier through discretization.

## Key Results
- Current VLMs struggle significantly with interactive physical reasoning, achieving success rates far below human performance across all six environments
- The gap between descriptive physical knowledge and procedural control remains a fundamental challenge, with models correctly predicting outcomes but failing to translate predictions into valid actions
- World Model prompting strategy degrades performance compared to direct VLA prompting, suggesting explicit prediction steps may be counterproductive for complex physical tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Converting continuous physics interactions into discrete, text-compatible formats lowers the control interface barrier for VLMs, enabling basic interaction where continuous control would fail.
- Mechanism: The benchmark replaces high-precision analog outputs with structured symbolic selections, mapping the physical action space to the token generation space of VLMs.
- Core assumption: The discretization preserves the core causal chain of the physical puzzle while removing the fine-motor execution burden.
- Evidence anchors:
  - [section 3.4] "To address this, a common principle across all DeepPHY environments is the transformation of continuous or complex action spaces into discrete and structured formats."
  - [section 5] "...even with the simplified action spaces designed for VLMs... most models... still cannot surpass MOCK results."
  - [corpus] *PhysBench* and related works highlight the difficulty of grounding VLMs in physical reality, supporting the need for interface simplification.
- Break condition: If the discretization granularity is too coarse (e.g., too few grid cells in PHYRE), the solution space becomes unsolvable, breaking the causal link between user intent and physical outcome.

### Mechanism 2
- Claim: Explicitly requiring a model to predict the physical outcome before acting degrades performance in complex tasks, indicating that current architectures cannot leverage descriptive simulation for procedural generation.
- Mechanism: The "World Model" (WM) prompt forces the VLM to generate a textual prediction of the physics before generating the action, adding cognitive overhead without improving control precision.
- Core assumption: The model's descriptive capability and action generation capability compete for the same representational capacity, or the descriptive prediction is not causally linked to the control output in the model's weights.
- Evidence anchors:
  - [section 5.3] "Revealingly, even if models can generate textually correct predictions... they still fail to translate this descriptive knowledge into a precise, executable control signal."
  - [figure 4] Visualization showing WM performance often falling below the VLA (Vision-Language-Action) baseline.
  - [corpus] *Prompting with the Future* suggests predictive control is difficult for VLMs; this paper provides specific evidence that explicit prediction steps can be counter-productive.
- Break condition: This mechanism implies that simply scaling model size or adding a "Think" step does not automatically yield better control; the architecture must specifically bridge the gap between semantic prediction and motor output.

### Mechanism 3
- Claim: Providing a history of failed trajectories provides insufficient signal for current VLMs to update their internal physical priors or correct errors effectively.
- Mechanism: The evaluation protocol allows up to K attempts, but success rates improve only marginally, suggesting the model fails to attribute failure to specific physical parameters (timing, force, angle).
- Core assumption: The model lacks a robust internal simulator to map the observed failure visual to a corrected action plan.
- Evidence anchors:
  - [section 5.1] "...learning efficiency is low: Successful trials require 4â€“5 attempts on average... feedback from a failed trajectory... is not sufficient for models to build an accurate... world model."
  - [section 3.1] Formalization of the policy update based on history, a process at which models consistently fail.
  - [corpus] *BOP-ASK* emphasizes fine-grained interaction reasoning; DeepPHY results show current models fail to refine this reasoning even with feedback.
- Break condition: If the visual feedback is too subtle or the failure cause is ambiguous (e.g., multi-body chaos in Angry Birds), the learning signal is lost.

## Foundational Learning

- **Concept: Partially Observable Markov Decision Process (POMDP)**
  - Why needed here: DeepPHY formalizes environments as POMDPs ($M = \langle S, A, T, R, \Omega, O \rangle$). Understanding that the agent only sees an observation $o$ (image), not the true state $S$ (exact physics variables), is critical for debugging why an agent might act on "wrong" physics assumptions.
  - Quick check question: Can you explain why an agent might fail a task even if it knows the rules of physics, solely based on the POMDP structure?

- **Concept: Action Space Discretization**
  - Why needed here: The benchmark's primary method of making VLMs viable is mapping continuous physics to discrete tokens (e.g., Grid Cells, JSON indices). You must understand the mapping logic to interpret model outputs.
  - Quick check question: In PHYRE, how does reducing the action space to a 5x5 grid affect the theoretical solvability of a continuous physics puzzle?

- **Concept: World Models vs. Reactive Policies**
  - Why needed here: The paper contrasts VLA (Reactive) vs. WM (Predictive). You need to distinguish between a model that acts directly on pixels vs. one that attempts to simulate future states to plan.
  - Quick check question: According to the paper's results, does forcing a model to act as a World Model (predicting state changes) improve or degrade its success rate in complex environments?

## Architecture Onboarding

- **Component map:**
  - Environment Core -> Observation Wrapper -> Agent (VLM) -> Action Parser -> Environment Core

- **Critical path:**
  - The Observation Wrapper -> Action Parser loop. If the annotation fails to highlight the interactive element (e.g., a hidden rope in Cut the Rope), the VLM has zero probability of success regardless of its reasoning power.

- **Design tradeoffs:**
  - Observation Fidelity: Providing too many annotations helps perception but creates "cognitive distraction" (noted in Kinetix M/L levels), reducing performance.
  - Prompt Strategy: VLA is robust but shallow; WM is theoretically superior for planning but practically degrades performance due to model limitations.

- **Failure signatures:**
  - Behavioral Stereotype: The model outputs the same action repeatedly (e.g., GPT-4o-mini in Pooltool outputting "Medium, Top Spin" deterministically), suggesting it hasn't grounded the visual input.
  - Descriptive-Control Gap: The model correctly describes the physics ("The ball will bounce left") but outputs an action that contradicts this description.

- **First 3 experiments:**
  1. Establish MOCK Baseline: Run the random action generator on all 6 environments to quantify the "blind luck" success rate.
  2. VLA vs. WM Ablation: Run a mid-tier model (e.g., GPT-4o-mini or Qwen-72B) on the Kinetix environment with and without the "World Model" prediction prompt to verify the performance drop.
  3. Perception Stress Test: Run Cut the Rope levels with and without visual annotations (IDs on ropes) to measure the performance delta attributable to pure perception vs. reasoning.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** What specific architectural or training modifications are required to bridge the gap between a VLM's ability to *describe* physical phenomena and its ability to perform *predictive control*?
- **Basis in paper:** [explicit] The authors identify a "fundamental disconnect" where models can correctly predict outcomes textually but fail to translate this into executable actions.
- **Why unresolved:** Current state-of-the-art models show that improved descriptive capability does not automatically yield better procedural control or planning.
- **What evidence would resolve it:** The development of models that achieve high Pass@1 rates on DeepPHY tasks without requiring multiple trial-and-error attempts.

### Open Question 2
- **Question:** Why does the World Model (WM) prompting strategy degrade performance compared to direct Vision-Language-Action (VLA) prompting in complex physical tasks?
- **Basis in paper:** [explicit] The evaluation shows that forcing models to predict environmental changes often acts as a "liability" and lowers success rates compared to the simpler VLA approach.
- **Why unresolved:** It remains unclear if the performance drop is due to the cognitive overhead of prediction or the models' inability to form accurate internal simulations of physics.
- **What evidence would resolve it:** A mechanistic analysis of attention layers during WM prompting or a new architecture where explicit prediction improves, rather than hinders, control accuracy.

### Open Question 3
- **Question:** How can benchmarks effectively distinguish between genuine physical reasoning and "brute-force" heuristics that artificially inflate success rates?
- **Basis in paper:** [inferred] The authors note that GPT-4o-mini achieved 100% success in Pooltool not via strategy, but by repeating a deterministic "brute-force" shot that exploited the environment setup.
- **Why unresolved:** Standard success metrics failed to expose the model's lack of physics understanding in this scenario.
- **What evidence would resolve it:** Implementation of evaluation metrics that penalize repetitive actions or utilize randomized physical parameters to prevent memorization of specific outcomes.

## Limitations

- The discretization of continuous physics into structured text commands may artificially simplify tasks, making it unclear how much the observed failures reflect true physical reasoning deficits versus interface incompatibility.
- The use of cartoon-style game environments may not generalize to real-world physical reasoning tasks, potentially limiting the benchmark's applicability.
- The closed-source model implementations introduce reproducibility challenges due to version-specific behaviors and potential API differences.

## Confidence

- **High Confidence:** The fundamental finding that VLMs struggle with interactive physical reasoning is well-supported by the 17-model evaluation across diverse environments and the clear gap between descriptive and procedural capabilities.
- **Medium Confidence:** The claim that explicit world-model prediction degrades performance is robust within this benchmark's context, though the mechanism (competitive representational capacity) requires further validation across different architectures.
- **Medium Confidence:** The assertion that learning from failure trajectories is inefficient is supported by the data, but the exact cognitive bottleneck (lack of internal simulator vs. other factors) remains uncertain.

## Next Checks

1. **Continuous Control Baseline:** Implement a small subset of tasks with continuous action spaces using traditional RL agents to establish whether the observed VLM failures are primarily due to the discrete interface or reflect deeper physical reasoning limitations.

2. **Cross-Domain Transfer:** Test the best-performing models from DeepPHY on real-world physical reasoning tasks (e.g., physics puzzles with physical objects or real-world video data) to validate whether cartoon-game performance correlates with genuine physical understanding.

3. **Architectural Intervention:** Design and test a hybrid architecture that explicitly separates the world-model prediction from action generation, with dedicated pathways for each function, to determine whether the performance degradation is architectural or fundamental to VLM capabilities.