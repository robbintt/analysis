---
ver: rpa2
title: Can LLMs Refuse Questions They Do Not Know? Measuring Knowledge-Aware Refusal
  in Factual Tasks
arxiv_id: '2510.01782'
source_url: https://arxiv.org/abs/2510.01782
tags:
- refusal
- answer
- evaluation
- rate
- correct
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "Large language models need the ability to refuse questions beyond\
  \ their knowledge to ensure factual reliability, but existing metrics fail to measure\
  \ this capability accurately. The authors propose the Refusal Index (RI), defined\
  \ as Spearman\u2019s rank correlation between refusal probability and error probability,\
  \ to quantify knowledge-aware refusal."
---

# Can LLMs Refuse Questions They Do Not Know? Measuring Knowledge-Aware Refusal in Factual Tasks

## Quick Facts
- **arXiv ID:** 2510.01782
- **Source URL:** https://arxiv.org/abs/2510.01782
- **Reference count:** 31
- **Primary result:** RI metric captures knowledge-aware refusal ability independent of accuracy and refusal rates across 16 models and 5 datasets

## Executive Summary
Large language models require the ability to refuse questions beyond their knowledge to ensure factual reliability, yet existing metrics fail to measure this capability accurately. This paper introduces the Refusal Index (RI), a novel metric that quantifies knowledge-aware refusal by measuring the rank correlation between refusal probability and error probability. The authors propose a lightweight two-pass evaluation method to estimate RI from observed refusal rates across two evaluation runs. Extensive experiments demonstrate that RI accurately captures intrinsic knowledge-aware refusal ability, remains stable across different refusal rates, and provides consistent model rankings independent of accuracy or refusal rates.

## Method Summary
The paper introduces the Refusal Index (RI) as a metric to measure knowledge-aware refusal in LLMs. RI is defined as Spearman's rank correlation between refusal probability and error probability. To estimate RI without requiring access to internal model probabilities, the authors propose a two-pass evaluation method: first evaluating models with high refusal probability, then with low refusal probability, and using the observed refusal rates to estimate RI. The method is validated across 16 models and 5 datasets, showing that RI accurately captures intrinsic refusal ability and provides consistent rankings independent of accuracy or refusal rates.

## Key Results
- RI metric accurately captures knowledge-aware refusal ability across 16 models and 5 datasets
- RI remains stable across different refusal rates and provides consistent model rankings independent of accuracy
- Two-pass evaluation method effectively estimates RI with low variance across different evaluation runs

## Why This Works (Mechanism)
The Refusal Index works by quantifying the correlation between when models refuse and when they would make errors. Since models that truly understand their knowledge boundaries should refuse more often when they lack knowledge (and would therefore err), this rank correlation captures the essence of knowledge-aware refusal. The two-pass evaluation method cleverly circumvents the need for direct access to internal probabilities by using observed behavior patterns across different prompting conditions.

## Foundational Learning

**Knowledge-aware refusal** - The ability of models to recognize when they lack sufficient knowledge to answer correctly and refuse instead. Needed to prevent misinformation; check by examining whether refusal correlates with error probability.

**Rank correlation metrics** - Statistical measures that assess how well the relationship between two variables can be described using a monotonic function. Needed for RI calculation; check by verifying monotonic relationships in empirical data.

**Two-pass evaluation** - A methodology where models are evaluated under different conditions (e.g., high vs low refusal probability) to extract information about internal behavior patterns. Needed to estimate RI without direct probability access; check by comparing variance across multiple runs.

## Architecture Onboarding

**Component map:** Dataset -> Two-pass evaluation -> Refusal/Error rates -> RI calculation -> Model ranking

**Critical path:** The core innovation lies in the two-pass evaluation method that enables RI estimation without requiring internal model probabilities, making it practical for real-world LLM evaluation.

**Design tradeoffs:** The method trades some precision for practicality - while direct probability access would be ideal, the two-pass approach provides a reasonable approximation that works with black-box models.

**Failure signatures:** RI may fail when error and refusal probabilities are not perfectly rank-correlated, or when evaluation runs have high variance due to prompt sensitivity or dataset heterogeneity.

**3 first experiments:**
1. Verify rank correlation between refusal and error rates in controlled settings with known knowledge gaps
2. Test RI stability across different prompt qualities and formulations
3. Compare RI with alternative metrics like accuracy-weighted refusal rates

## Open Questions the Paper Calls Out
None

## Limitations
- Assumption of perfect rank correlation between error and refusal probabilities may not hold for all factual tasks
- Two-pass evaluation introduces potential variance from different evaluation runs
- Focus on English-language factual questions may limit generalizability to other languages or domains

## Confidence
- **High confidence:** RI metric accurately captures knowledge-aware refusal ability (supported by correlation experiments and stability tests across 16 models and 5 datasets)
- **Medium confidence:** RI provides consistent model rankings independent of accuracy/refusal rates (demonstrated but theoretical justification could be stronger)
- **Medium confidence:** Two-pass method effectively estimates RI with low variance (empirically validated but not exhaustively tested across all possible scenarios)

## Next Checks
1. Test RI metric on non-English factual tasks to verify cross-linguistic validity and examine whether rank correlation assumptions hold across different languages and knowledge domains
2. Evaluate RI stability under extreme conditions: very low/high knowledge baselines and varying prompt qualities to identify potential failure modes
3. Compare RI with alternative knowledge-aware refusal metrics through ablation studies and examine sensitivity to different rank correlation measures (e.g., Kendall's tau)