---
ver: rpa2
title: 'Decomposing and Composing: Towards Efficient Vision-Language Continual Learning
  via Rank-1 Expert Pool in a Single LoRA'
arxiv_id: '2601.22828'
source_url: https://arxiv.org/abs/2601.22828
tags:
- learning
- lora
- arxiv
- task
- continual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses catastrophic forgetting in continual learning
  for vision-language models by introducing a parameter-efficient framework that dynamically
  composes task-specific updates from a Rank-1 Expert Pool within a single LoRA module.
  The method uses the [CLS] token to guide expert selection and employs an Activation-Guided
  Orthogonal (AGO) loss to minimize parameter collision.
---

# Decomposing and Composing: Towards Efficient Vision-Language Continual Learning via Rank-1 Expert Pool in a Single LoRA

## Quick Facts
- arXiv ID: 2601.22828
- Source URL: https://arxiv.org/abs/2601.22828
- Reference count: 40
- Key outcome: Achieves state-of-the-art performance in vision-language continual learning by dynamically composing task-specific updates from a Rank-1 Expert Pool within a single LoRA module, surpassing zero-shot upper bounds in generalization while reducing trainable parameters by 96.7%.

## Executive Summary
This paper addresses catastrophic forgetting in vision-language continual learning by introducing a parameter-efficient framework that dynamically composes task-specific updates from a Rank-1 Expert Pool within a single LoRA module. The method uses the [CLS] token to guide expert selection and employs an Activation-Guided Orthogonal (AGO) loss to minimize parameter collision. Experiments demonstrate state-of-the-art performance across all metrics, surpassing zero-shot upper bounds in generalization while reducing trainable parameters by 96.7% compared to baselines. The approach eliminates reliance on external datasets or task-ID discriminators and incurs no inference latency.

## Method Summary
The proposed framework tackles catastrophic forgetting in vision-language models by leveraging a Rank-1 Expert Pool mechanism within a single LoRA module. Each expert represents a low-rank update that can be dynamically composed based on the task requirements. The system uses the [CLS] token as a guide for expert selection during both training and inference. An Activation-Guided Orthogonal (AGO) loss is employed to prevent parameter collision between experts, ensuring that each maintains distinct functionality. This approach allows for efficient adaptation to new tasks while preserving knowledge from previous tasks, achieving significant parameter efficiency without compromising performance.

## Key Results
- Achieves state-of-the-art performance across all metrics in vision-language continual learning
- Surpasses zero-shot upper bounds in generalization capability
- Reduces trainable parameters by 96.7% compared to baseline methods

## Why This Works (Mechanism)
The method works by decomposing the parameter space into a Rank-1 Expert Pool, where each expert captures task-specific knowledge. During training, the system dynamically composes these experts based on the [CLS] token's guidance, allowing for task-adaptive updates. The AGO loss ensures that experts remain orthogonal in their activation patterns, preventing interference and catastrophic forgetting. This decomposition and composition strategy enables efficient parameter usage while maintaining task-specific capabilities. The single LoRA module design eliminates inference latency and removes the need for external datasets or task-ID discriminators, making the approach both practical and scalable.

## Foundational Learning
- **Catastrophic forgetting**: The tendency of neural networks to overwrite previous knowledge when learning new tasks. Why needed: Central problem being addressed. Quick check: Verify baseline performance degradation without proposed solution.
- **Low-Rank Adaptation (LoRA)**: A parameter-efficient fine-tuning method that decomposes weight updates into low-rank matrices. Why needed: Provides the foundation for the Rank-1 Expert Pool. Quick check: Confirm parameter reduction claims.
- **Continual Learning**: Learning paradigm where models are trained sequentially on multiple tasks without forgetting previous ones. Why needed: Defines the problem context and evaluation metrics. Quick check: Review continual learning benchmarks used.
- **Rank-1 Decomposition**: Technique for breaking down matrices into outer products of vectors. Why needed: Enables efficient representation of task-specific updates. Quick check: Verify mathematical formulation of rank-1 updates.
- **Activation-Guided Orthogonal Loss**: Regularization technique to maintain orthogonality between expert activations. Why needed: Prevents parameter collision and interference. Quick check: Examine ablation studies on AGO loss impact.

## Architecture Onboarding

**Component Map**: Input -> [CLS] Token Extraction -> Expert Selection -> Rank-1 Expert Pool Composition -> LoRA Parameter Update -> Model Output

**Critical Path**: The critical path involves extracting the [CLS] token, using it to select appropriate experts from the pool, composing these experts through rank-1 updates, and applying the resulting LoRA parameters to the base model. This path must be optimized for both training efficiency and inference speed.

**Design Tradeoffs**: The primary tradeoff is between parameter efficiency and performance. Using a single LoRA module with a Rank-1 Expert Pool significantly reduces parameters but requires careful expert selection and orthogonalization to maintain performance. The AGO loss adds computational overhead during training but prevents interference between experts.

**Failure Signatures**: Potential failures include expert collapse (where multiple experts become redundant), poor expert selection leading to inappropriate task adaptation, and insufficient orthogonality causing catastrophic forgetting. Performance degradation on previous tasks while learning new ones would indicate forgetting issues.

**First Experiments**: 1) Verify parameter reduction claims by comparing trainable parameters with baseline methods. 2) Test expert selection accuracy by measuring how well the [CLS] token guides appropriate expert composition. 3) Evaluate catastrophic forgetting by measuring performance degradation on previous tasks after learning new ones.

## Open Questions the Paper Calls Out
None specified in the provided content.

## Limitations
- Absence of comparison against recent continual learning methods specifically targeting vision-language tasks
- Need for verification of inference latency claims despite single LoRA module usage
- Limited ablation studies on the individual contributions of AGO loss and dynamic expert selection

## Confidence
- High confidence in parameter efficiency claims (96.7% reduction) and fundamental Rank-1 Expert Pool approach
- Medium confidence in catastrophic forgetting mitigation effectiveness due to limited ablation studies on AGO loss
- Low confidence in generalization claims beyond specific benchmark datasets used

## Next Checks
1. Conduct extensive ablation studies to quantify the individual contributions of the AGO loss, dynamic expert selection, and Rank-1 decomposition to overall performance.

2. Evaluate the method's performance on diverse vision-language tasks beyond the benchmark datasets, including open-ended question answering and visual reasoning tasks.

3. Perform detailed inference time analysis to measure any potential latency introduced by the dynamic expert selection mechanism, despite using a single LoRA module.