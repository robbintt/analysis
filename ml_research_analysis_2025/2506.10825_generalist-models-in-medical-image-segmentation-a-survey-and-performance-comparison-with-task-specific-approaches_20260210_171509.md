---
ver: rpa2
title: 'Generalist Models in Medical Image Segmentation: A Survey and Performance
  Comparison with Task-Specific Approaches'
arxiv_id: '2506.10825'
source_url: https://arxiv.org/abs/2506.10825
tags:
- segmentation
- medical
- image
- arxiv
- generalist
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This survey provides a comprehensive analysis of generalist models
  for medical image segmentation, comparing them with state-of-the-art task-specific
  approaches. The study reveals that generalist models, while requiring significant
  computational resources for pre-training, offer promising adaptability across diverse
  anatomical structures.
---

# Generalist Models in Medical Image Segmentation: A Survey and Performance Comparison with Task-Specific Approaches

## Quick Facts
- arXiv ID: 2506.10825
- Source URL: https://arxiv.org/abs/2506.10825
- Reference count: 40
- Generalist models show promising adaptability across diverse anatomical structures but require significant computational resources for pre-training

## Executive Summary
This comprehensive survey systematically reviews 48 generalist models and 24 task-specific approaches for 3D medical image segmentation, benchmarking them across 30+ public datasets covering 14 anatomical regions. The study reveals that while generalist models (particularly SAM-based architectures) demonstrate superior generalization capabilities and adaptability to new tasks, they face significant challenges in regulatory compliance, privacy concerns, and computational requirements. Task-specific models like nnU-Net and transformer variants remain competitive, especially on specialized datasets. The survey identifies key research gaps and proposes future directions including synthetic data generation and more efficient model designs to address current limitations.

## Method Summary
The survey conducts a comprehensive literature review of generalist and task-specific segmentation models, extracting performance metrics from published studies. No single unified experimental protocol was implemented; instead, the authors aggregated results from multiple studies, comparing Dice Similarity Coefficients across 30+ public datasets including BraTS, BTCV, KiTS, LiTS, and others. Models were categorized by architecture type (SAM-based, transformer-based, CNN-based) and evaluated for their primary publication results versus state-of-the-art improvements. Computational requirements were documented where available, though heterogeneous reporting formats limited direct comparison.

## Key Results
- Generalist models demonstrate superior generalization across diverse anatomical structures compared to task-specific approaches
- Computational requirements differ dramatically: generalist models require median 96GB GPU memory versus 40GB for task-specific models
- Regulatory compliance, privacy concerns, and computational costs remain significant barriers to clinical deployment
- Text-driven segmentation models show promise for fully automated workflows but face accuracy challenges

## Why This Works (Mechanism)

### Mechanism 1: Dimensional Adaptation via Adapter Injection
- **Claim:** 2D ViT pre-trained on natural images can be adapted to 3D medical volumes via parameter-efficient fine-tuning and 3D adapters
- **Mechanism:** Lightweight 3D convolutional adapters are injected into transformer blocks to project 2D spatial features into 3D feature space, enabling inter-slice dependency capture while updating only small weight fractions
- **Core assumption:** Generic visual features (edges, textures) from 2D backbones transfer to medical 3D data, requiring only spatiotemporal context re-learning
- **Evidence anchors:** Med-SA uses "space-depth transpose" and adapters; MA-SAM uses 3D adapters for volumetric information
- **Break condition:** Large inter-slice spacing causes 3D convolution kernels to fail, resulting in disjointed segmentations

### Mechanism 2: Temporal Propagation as Volumetric Consistency
- **Claim:** Treating 3D volumes as video sequences with memory bank mask propagation improves segmentation continuity
- **Mechanism:** SAM 2 variants use memory banks and memory attention to store and propagate mask embeddings across slices, enforcing structural continuity
- **Core assumption:** Anatomical structures vary smoothly across slices (high slice correlation), similar to object motion in video
- **Evidence anchors:** SAM 2 "treating 3D images as a sequence of 2D frames" allows object tracking; Medical SAM 2 uses "self-sorting memory bank"
- **Break condition:** Unordered slices or irregular lesion shapes cause memory attention to hallucinate objects or lose track

### Mechanism 3: Semantic Disambiguation via Text-Image Alignment
- **Claim:** Text-image alignment (CLIP) enables disambiguation between visually similar structures
- **Mechanism:** Text encoders map organ names to shared embedding space, with text prompts acting as semantic queries guiding decoder attention
- **Core assumption:** Visual representation of target organ matches statistical distribution learned during text-image alignment
- **Evidence anchors:** BiomedParse handles "diverse text prompt[s]"; PCNet provides "prior category knowledge"
- **Break condition:** Ambiguous text prompts or out-of-vocabulary terminology cause semantic alignment failure and wrong structure segmentation

## Foundational Learning

- **Concept: Self-Supervised Learning (SSL) / Masked Image Modeling (MIM)**
  - **Why needed here:** Generalist models rely on foundation capabilities from massive unlabeled datasets; MAE explains how models learn generic features without expensive labels
  - **Quick check question:** Can you explain why masking random patches of a CT scan and asking the model to reconstruct them forces learning of high-level anatomy rather than low-level texture?

- **Concept: Parameter-Efficient Fine-Tuning (PEFT)**
  - **Why needed here:** Full fine-tuning of generalist models is computationally infeasible; PEFT (LoRA, adapters) is standard adaptation mechanism
  - **Quick check question:** How does adding low-rank decomposition matrix (A Ã— B) to weight updates allow model adaptation without modifying massive pre-trained weights?

- **Concept: The U-Net Inductive Bias**
  - **Why needed here:** Task-specific U-Nets still dominate certain benchmarks; understanding skip connections is vital for grasping what generalist models replace
  - **Quick check question:** Why do skip connections specifically help preserve localization accuracy in segmentation tasks?

## Architecture Onboarding

- **Component map:** Image Encoder -> Prompt Encoder -> Memory Bank (SAM 2 variants) -> Mask Decoder
- **Critical path:**
  1. Pre-processing: Normalization (Hounsfield units for CT, intensity normalization for MRI)
  2. Encoding: Pass volume through frozen or adapter-tuned Image Encoder
  3. Prompting: Generate embeddings for target (e.g., bounding box on central slice)
  4. Decoding: Cross-attend Image/Prompt embeddings to generate binary mask
- **Design tradeoffs:**
  - 2D vs. 3D Native Processing: 2D slice-wise (computationally cheap, lacks volumetric context) vs. Native 3D (better consistency, requires more VRAM)
  - Text vs. Box Prompts: Text allows automation but less precision; Boxes require human-in-the-loop but guarantee higher accuracy
- **Failure signatures:**
  - Over-segmentation: Models segment entire region instead of specific organ (vague prompts)
  - Slice Discontinuity: 2D slice-wise approaches show mask jitter or disappearance ("staircase" artifact)
  - Hallucination: Models segment noise/artifacts as lesions (zero-shot settings with domain shift)
- **First 3 experiments:**
  1. Zero-Shot Baseline: Run vanilla SAM/SAM 2 on BTCV using simple box prompts to establish performance floor
  2. Adapter Ablation: Implement Med-SA or LoRA adapter, train on 10% data subset, compare Dice against zero-shot baseline
  3. Prompt Sensitivity Test: Evaluate same model with different prompts (1-point vs. 3-point vs. Box) on MSD Pancreas Tumor dataset

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can "test-time scaling" (allocating compute during inference for reasoning) effectively improve segmentation accuracy in generalist medical imaging models?
- **Basis in paper:** Authors discuss "lessons learnt from LLMs," noting the "test-time scaling law" from models like OpenAI o1 as a potential future direction
- **Why unresolved:** Current segmentation models rely on pre-training and fine-tuning scaling laws; inference-time reasoning strategies are largely unexplored
- **What evidence would resolve it:** Benchmark study showing increased inference-time computation yields higher Dice scores without additional training

### Open Question 2
- **Question:** Can distillation or early fusion techniques reduce massive computational hardware requirements to make generalist models accessible for small hospitals?
- **Basis in paper:** Paper highlights disparity in resource needs (Medical SAM 2 requiring 5,120 GB GPU memory) and explicitly calls for smaller, more efficient models
- **Why unresolved:** Generalist models generally remain "resource-hungry," limiting deployment to institutions with high-end hardware
- **What evidence would resolve it:** Generalist model achieving state-of-the-art performance but fitting within standard clinical GPU memory budget (<24GB)

### Open Question 3
- **Question:** How can informed consent frameworks be ethically adapted for opaque, "black-box" generalist foundation models?
- **Basis in paper:** Authors state it "may be difficult for patients to understand what they are giving consent to" and explicitly ask about reframing informed consent
- **Why unresolved:** Regulatory frameworks haven't caught up with foundation model complexity, creating gaps between capability and legal compliance
- **What evidence would resolve it:** Validated, standardized consent protocol effectively communicating model limitations and risks, approved by IRB for multi-center clinical trial

## Limitations
- Heterogeneous training protocols across studies make direct performance comparisons challenging
- Computational cost metrics are inconsistent, with GFLOPS reported differently across models
- Limited validation on rare anatomies and extreme clinical scenarios
- Regulatory and privacy barriers remain unaddressed in current benchmarking frameworks

## Confidence
- **High confidence:** Core finding that generalist models show promising adaptability across diverse anatomical structures
- **Medium confidence:** Comparative performance claims due to heterogeneous protocols and computational metric inconsistencies
- **Medium confidence:** Real-world generalization gap between curated datasets and clinical deployment

## Next Checks
1. **Cross-Dataset Generalization Test:** Evaluate top-performing generalist models (SAM-Med3D, MedSAM) on held-out anatomies not seen during fine-tuning to measure true zero-shot generalization
2. **Clinical Workflow Integration Study:** Deploy SAM-based models in simulated clinical environment with real-time prompting constraints to measure actual usability versus benchmark performance
3. **Cost-Benefit Analysis Under Real Constraints:** Calculate total cost of ownership for generalist versus task-specific approaches including pre-training, fine-tuning, inference, and human-in-the-loop prompting time across different clinical scenarios