---
ver: rpa2
title: Prompt reinforcing for long-term planning of large language models
arxiv_id: '2510.05921'
source_url: https://arxiv.org/abs/2510.05921
tags:
- prompt
- feedback
- system
- wang
- user
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The authors address the challenge of improving long-term planning
  in large language models (LLMs) for multi-turn interactions, where LLMs often fail
  to track user goals over time and rely on incorrect early assumptions. They propose
  a prompt optimisation framework, Reinforced Prompt Optimisation (RPO), inspired
  by reinforcement learning, which enhances the LLM's long-term planning ability by
  iteratively refining the task instruction prompt based on natural language feedback.
---

# Prompt reinforcing for long-term planning of large language models

## Quick Facts
- arXiv ID: 2510.05921
- Source URL: https://arxiv.org/abs/2510.05921
- Authors: Hsien-Chin Lin; Benjamin Matthias Ruppik; Carel van Niekerk; Chia-Hao Shen; Michael Heck; Nurul Lubis; Renato Vukovic; Shutong Feng; Milica Gašić
- Reference count: 40
- Primary result: Achieves 54.2% relative improvement over baseline sharded performance in text-to-SQL and 47.3% relative improvement in task-oriented dialogue success rate

## Executive Summary
This paper addresses the challenge of improving long-term planning in large language models (LLMs) for multi-turn interactions, where LLMs often fail to track user goals over time and rely on incorrect early assumptions. The authors propose a prompt optimisation framework, Reinforced Prompt Optimisation (RPO), inspired by reinforcement learning, which enhances the LLM's long-term planning ability by iteratively refining the task instruction prompt based on natural language feedback. This is achieved through a feedbacker LLM that generates turn-level feedback, including predictions of user sentiment, dialogue success, and actionable suggestions, and a rewriter LLM that refines the prompt using this feedback and experience replay from previous iterations.

## Method Summary
The RPO framework optimises LLM prompts for multi-turn planning tasks through iterative refinement. A System LLM interacts with users in tasks like text-to-SQL or dialogue. A Feedbacker LLM generates turn-level feedback using Temporal Difference (TD) learning, providing predictions of user sentiment, dialogue success, and API correctness. A Rewriter LLM then refines the system prompt using this feedback and experience replay (incorporating historical prompt-feedback pairs). The process iterates for N epochs, with the best-performing prompt selected based on validation set performance. The method treats the system instruction as a textual parameter, reducing serving costs compared to multi-agent feedback loops at inference time.

## Key Results
- 54.2% relative improvement over baseline sharded performance in text-to-SQL tasks
- 47.3% relative improvement in task-oriented dialogue success rate
- Generalizes across different LLM-based agents, maintaining performance when swapping Feedbacker/Rewriter backbones

## Why This Works (Mechanism)

### Mechanism 1: Temporal Difference-Style Feedback for Multi-Turn Credit Assignment
- Claim: Turn-level feedback provides lower-variance, faster-converging optimization signals than episode-level feedback for multi-turn tasks.
- Mechanism: At each turn, the feedbacker LLM generates predictions of next-turn user sentiment, dialogue success forecast, and actionable suggestions, aggregated into dialogue-level feedback using TD error: δt = rt + γV(st+1) − V(st).
- Core assumption: LLMs can generate meaningful intermediate value estimates that correlate with eventual task success.
- Evidence: MC-style exhibits higher variance during early training, whereas TD-style is more stable and converges faster (Figure 4a).

### Mechanism 2: Experience Replay for Prompt Rewriting Stability
- Claim: Incorporating historical prompt-feedback pairs into the rewriter improves optimization stability and final performance.
- Mechanism: The rewriter LLM accesses the full sequence of past prompt-feedback pairs, allowing it to identify patterns across optimization iterations and avoid oscillations.
- Core assumption: Past optimization trajectories contain reusable patterns that inform better prompt edits.
- Evidence: The framework explicitly incorporates experience replay inspired by RL, though direct ablation studies are not provided.

### Mechanism 3: Instruction-Level Optimization vs. Output-Level Refinement
- Claim: Permanently updating the system instruction prompt reduces serving costs while embedding planning behaviors.
- Mechanism: RPO treats the task instruction as a "textual parameter" to be modified. After each epoch, the prompt is updated; at inference, no multi-agent feedback loop is needed.
- Core assumption: Desired planning capabilities exist in the LLM's pre-training and can be surfaced through better instructions.
- Evidence: Performance drops significantly on Traditional Chinese Medicine (45.5% win rate vs 86.7% for general medicine) due to pre-training data gaps.

## Foundational Learning

### Concept 1: Temporal Difference (TD) Learning
- Why needed here: RPO's feedback mechanism is explicitly inspired by TD error, contrasting with Monte Carlo approaches that wait for episode completion.
- Quick check question: Why would TD-style feedback have lower variance during early training compared to MC-style feedback in a multi-turn dialogue setting?

### Concept 2: Experience Replay
- Why needed here: The rewriter's ability to access historical prompt-feedback pairs is directly borrowed from RL's experience replay, used to stabilize off-policy learning.
- Quick check question: What failure mode in RL does experience replay address, and how might the analogous problem appear in prompt optimization?

### Concept 3: In-Context Learning Limitations
- Why needed here: The paper acknowledges prompts cannot change attention patterns—only bias outputs. Understanding this helps interpret RPO's ceiling.
- Quick check question: What does it mean that "prompts cannot change the model's attention patterns, [but] can only bias the output"?

## Architecture Onboarding

### Component Map:
System Agent LLM ←── current_prompt ──← Prompt Store
    │                                        ↑
    ↓ trajectories                           │
Environment ←── feedback_TD/MC ←── Rewriter LLM
    │                                        ↑
    ↓ t_1:n                                  │
Feedbacker LLM ──────────────────────────────┘

### Critical Path:
1. Initialize prompt (expert-written or APE-generated)
2. Collect 10 multi-turn trajectories with current prompt
3. Generate feedback via Feedbacker (TD-style or MC-style)
4. Rewrite prompt via Rewriter with experience replay (produces 2 candidates)
5. Evaluate candidates on validation set; select best
6. Repeat for N epochs (stability by epoch 3, ~300 dialogues)

### Design Tradeoffs:
- TD vs MC feedback: TD = lower variance, faster convergence; MC = may capture global structure better
- Batch size for feedback: 10 interactions (limited by context length; practical for human expert feedback)
- Number of candidate prompts per epoch: 2 generated, 1 selected
- Experience replay depth: More history = better pattern recognition but higher token cost

### Failure Signatures:
- High variance training curves (Figure 6): LLMs are not batch-invariant; nondeterministic behavior causes seed-dependent variance
- Performance ceiling: Optimized prompts still underperform fully-specified oracle (0.477 vs. 0.743 functional accuracy)
- Domain limitations: Traditional Chinese Medicine win rate drops from 86.7% to 45.5% due to pre-training data gaps
- API call errors in dialogue: Incorrect function selection or argument values cause cascading failures

### First 3 Experiments:
1. Reproduce Text-to-SQL baseline comparison: Run RPO_TD, RPO_TD+replay, APO, GPO, MC-style on Spider dataset with sharded queries
2. Ablate feedbacker input signals: Compare basic (text only), subjective (+user goal), believe (+API calls), full (both)
3. Test meta-prompting agent generalization: Swap Feedbacker/Rewriter backbones (GPT→Gemini→Llama variants) while keeping system agent fixed

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can RPO be effectively combined with external knowledge retrieval to improve performance on tasks unseen in pre-training data?
- Basis: Section 5.3 notes RPO performance drops on Traditional Chinese Medicine because it is underrepresented in pre-training
- Why unresolved: Current framework optimises prompts based solely on the model's parametric knowledge
- Evidence needed: Ablation study integrating RAG with RPO on underrepresented datasets showing improved win rates

### Open Question 2
- Question: How can the task-agnostic prompts for the feedbacker and rewriter LLMs be automatically optimised without creating an infinite regress?
- Basis: Section 3.2 states that while feedbacker and rewriter use fixed, task-agnostic prompts, "Optimising the prompts of these meta-prompting agents... is left for future research"
- Why unresolved: Current method relies on human-curated instructions for the optimisation agents themselves
- Evidence needed: Comparative study where meta-prompts are generated/optimised by a higher-level agent versus current static baseline

### Open Question 3
- Question: Does the performance gap between multi-turn RPO and single-turn "Oracle" baselines represent a fundamental limitation of parameter-free optimisation?
- Basis: Section 5.1 highlights that despite relative improvements, a significant gap remains between RPO (0.477 accuracy) and the fully-specified Oracle (0.743)
- Why unresolved: Unclear if this gap is an artifact of the specific feedback mechanism or an inherent inability of soft prompts to fully compensate for lack of explicit memory/parameters
- Evidence needed: Analysis comparing RPO against fine-tuning methods on identical tasks to determine if the "Oracle gap" is consistent across approaches

## Limitations
- Performance ceiling: Optimized prompts still underperform fully-specified oracle baselines despite relative improvements
- Domain limitations: Tasks requiring knowledge absent from pre-training data show significant performance degradation
- Dependence on feedbacker reliability: Method assumes feedbacker can generate meaningful intermediate value estimates that correlate with actual task success

## Confidence

**High Confidence**: The mechanism of using TD-style turn-level feedback for lower-variance optimization signals is well-supported by empirical results showing faster convergence and stability compared to MC-style approaches.

**Medium Confidence**: The experience replay mechanism's contribution to optimization stability is supported by the framework's design, though direct ablation studies comparing with and without replay are not explicitly provided.

**Medium Confidence**: The instruction-level optimization claim regarding reduced serving costs is logically sound but lacks quantitative comparison of inference-time overhead between RPO and traditional multi-agent feedback loops.

## Next Checks

1. **Distribution Shift Sensitivity**: Test RPO on a held-out domain with significantly different characteristics from training data to evaluate whether experience replay patterns remain transferable when task distributions shift.

2. **Feedbacker Reliability Analysis**: Systematically evaluate the correlation between feedbacker-predicted intermediate values and actual task success across different task types to quantify the reliability of TD-style credit assignment.

3. **Context Window Scaling Study**: Measure RPO performance as a function of feedback history depth to determine optimal experience replay window size and identify when historical data becomes detrimental due to context saturation or distribution shift.