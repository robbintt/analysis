---
ver: rpa2
title: CosmoCore Affective Dream-Replay Reinforcement Learning for Code Generation
arxiv_id: '2510.18895'
source_url: https://arxiv.org/abs/2510.18895
tags:
- code
- cosmocore
- learning
- generation
- affective
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: CosmoCore integrates affective signals into reinforcement learning
  to reduce hallucinations in LLM code generation. Inspired by how embarrassment drives
  rapid correction in humans and animals, it uses a lightweight MLP to tag code generation
  trajectories with valence (emotional tone) and arousal (surprise).
---

# CosmoCore Affective Dream-Replay Reinforcement Learning for Code Generation

## Quick Facts
- arXiv ID: 2510.18895
- Source URL: https://arxiv.org/abs/2510.18895
- Reference count: 32
- Primary result: 48% reduction in hallucinated code and 45% faster self-correction using affective signals in RL

## Executive Summary
CosmoCore introduces an affective reinforcement learning framework that integrates emotional valence and arousal signals into LLM code generation. By prioritizing high-negative valence errors (tagged as "cringe") for five-fold replay during training, it achieves 48% fewer hallucinations and 45% faster self-correction on HumanEval and BigCodeBench benchmarks. The approach uses a lightweight MLP tagger to assign valence/arousal scores to code generation trajectories, with pruning of low-surprise successes to prevent buffer bloat. Local experiments validate these gains using Hugging Face models.

## Method Summary
CosmoCore implements affective RL by tagging code generation trajectories with valence and arousal using a 512→128→2 MLP. High-negative valence errors (|v| > 0.5, a > 0.7) enter a Dream Queue sampled at 5× baseline rate during off-policy updates, while low-surprise successes are pruned unless policy entropy exceeds 0.3. The nocturnal phase draws 80% from the Dream Queue and 20% uniformly for diversity. Evaluated on HumanEval and BigCodeBench using CodeT5-small, CosmoCore reduces hallucination fraction while accelerating convergence through prioritized replay of emotional failure modes.

## Key Results
- 48% reduction in hallucinated code outputs compared to baseline
- 45% acceleration in self-correction cycles during training
- Cross-benchmark performance improvement on both HumanEval and BigCodeBench

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** High-negative valence errors, when replayed at elevated frequency during off-policy updates, may accelerate error correction relative to uniform replay.
- **Mechanism:** A lightweight MLP (512→128→2) tags trajectories with valence and arousal. Trajectories with |v| > 0.5 and a > 0.7 enter a Dream Queue sampled at 5× the baseline rate during training, increasing gradient exposure to failure modes.
- **Core assumption:** Valence tags derived from execution feedback generalize to unseen error types; the mapping from code failures to negative valence is stable across tasks.
- **Evidence anchors:**
  - [abstract] "High-negative valence (cringe) episodes, such as buggy code outputs, are prioritized in a Dream Queue for five-fold replay during off-policy updates"
  - [Section 3.2] Priority formula: p_i = |TD_i| + λ|v_i| · a_i where λ = 0.6
  - [corpus] Weak direct corpus validation; neighbor papers (CosmoCore-Evo, affective computing datasets) do not independently replicate this specific valence-gating mechanism.
- **Break condition:** If valence tagger produces high variance on out-of-distribution code patterns (e.g., unfamiliar APIs), prioritization may amplify noisy gradients rather than correct errors.

### Mechanism 2
- **Claim:** Pruning low-surprise, low-valence successes from the replay buffer can reduce memory bloat and may prevent overconfidence, conditional on maintaining sufficient exploration entropy.
- **Mechanism:** Trajectories with |v| < 0.2 and a < 0.3 are deleted unless policy entropy exceeds 0.3. This gates routine successes while preserving uncertain/exploratory samples.
- **Core assumption:** Low-arousal successes correlate with redundant learning signal; policy entropy is a reliable proxy for exploration needs.
- **Evidence anchors:**
  - [abstract] "low-surprise successes are pruned to prevent overconfidence and buffer bloat"
  - [Section 3.2] "Prune Bin... deletes unless policy entropy exceeds 0.3"
  - [Section 7] "Buffer bloat +25% without pruning, confidence variance ↓30%"
  - [corpus] No direct corpus validation of pruning thresholds in code generation contexts.
- **Break condition:** If task distribution shifts mid-training, aggressive pruning may discard transferrable successes, reducing sample efficiency on new tasks.

### Mechanism 3
- **Claim:** Consolidating replay during a dedicated "nocturnal" phase, with 80% Dream Queue samples and 20% uniform samples, may balance error correction with policy diversity.
- **Mechanism:** Off-policy updates draw minibatches from the gated distribution; pruning intensity is tuned dynamically by confidence variance (meta-gradient descent on Q-value std. dev. < 0.1).
- **Core assumption:** The brain's sleep-consolidation analogy transfers to artificial gradient updates; meta-tuning does not introduce instability.
- **Evidence anchors:**
  - [Section 3.3] "Minibatches draw 80% from the Dream Queue for error correction and 20% uniformly for diversity"
  - [Section 3.3] "Pruning is tuned by confidence variance... low variance (<0.1) increases pruning via meta-gradient descent"
  - [corpus] Neuro-inspired RL frameworks (e.g., [1], [29]) support sleep-like consolidation analogies but do not validate this specific 80/20 split.
- **Break condition:** If confidence variance is manipulated by adversarial or noisy reward signals, meta-gradient tuning may over-prune or under-prune adaptively.

## Foundational Learning

- **Concept: Prioritized Experience Replay (PER)**
  - **Why needed here:** CosmoCore extends PER's TD-error weighting with valence-gated priority; understanding PER is prerequisite to seeing what's additive.
  - **Quick check question:** Can you explain why PER uses |TD-error| for priority, and what problem vanishing TD-errors cause for rare-but-important experiences?

- **Concept: Russell's Circumplex Model (Valence × Arousal)**
  - **Why needed here:** The affective tagger outputs valence (−1 to +1) and arousal (0 to 1); you need to interpret these as emotional tone and surprise, not raw rewards.
  - **Quick check question:** How would a high-arousal, low-valence trajectory differ in learning priority from a low-arousal, high-valence one under CosmoCore's scheme?

- **Concept: Policy Entropy as Exploration Proxy**
  - **Why needed here:** The Prune Bin retains low-valence trajectories if entropy > 0.3; you must understand what entropy measures and why it gates pruning.
  - **Quick check question:** If policy entropy drops to near-zero mid-training, what does that imply about exploration, and how would CosmoCore's pruning respond?

## Architecture Onboarding

- **Component map:**
  LLM Encoder → Affective Tagger (MLP 512→128→2) → CosmoCore Buffer (Dream Queue + Prune Bin) → Nocturnal Phase Trainer (PPO/DQN) → Execution Sandbox

- **Critical path:**
  Code generation → execution feedback → MLP tagging → buffer insertion (Dream Queue or Prune Bin) → nocturnal minibatch sampling → gradient update → repeat. If the tagger misclassifies a critical error as low-valence, it bypasses the Dream Queue and the error is under-replayed.

- **Design tradeoffs:**
  - **Tagger capacity vs. overhead:** 5% additional parameters is lightweight but may limit expressivity on complex multi-file codebases.
  - **Pruning aggressiveness:** Higher pruning reduces memory but risks discarding rare successes needed for generalization.
  - **λ = 0.6 in priority formula:** Balances TD-error vs. valence; tuning may be language/task-dependent (unvalidated per paper).

- **Failure signatures:**
  - **Valence drift:** If tagger labels shift over training (e.g., all trajectories → neutral), Dream Queue empties and performance regresses to baseline.
  - **Prune loop:** If confidence variance stays <0.1 early, over-pruning may starve the buffer of diverse samples, collapsing exploration.
  - **Reward noise amplification:** Noisy execution feedback may produce spurious high-arousal tags, flooding Dream Queue with uninformative samples.

- **First 3 experiments:**
  1. **Ablate valence gating:** Run CosmoCore with v = 0 for all trajectories (disable Dream Queue prioritization). Compare hallucination rate to full CosmoCore on HumanEval subset (10 prompts). Expect degradation per Section 7 ablation.
  2. **Threshold sensitivity sweep:** Vary Dream Queue threshold (|v| > 0.3, 0.5, 0.7) and Prune Bin threshold (|v| < 0.1, 0.2, 0.3). Measure buffer occupancy, hallucination rate, and training stability. Paper notes thresholds may be task-sensitive; this characterizes robustness.
  3. **Out-of-distribution test:** Train on Python/HumanEval, evaluate on BigCodeBench (API-heavy tasks). Assess whether valence tags transfer across code domains, as cross-benchmark generalization is claimed but not deeply analyzed.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can self-supervised valence tagging utilizing execution logs effectively replace the current dependency on human-labeled "scold" data?
- **Basis in paper:** [explicit] Section 8.4 proposes developing "self-supervised valence tagging mechanisms" to reduce reliance on human annotation.
- **Why unresolved:** The current architecture depends on human-labeled episodes to define "cringe" signals, which limits scalability.
- **What evidence would resolve it:** Benchmarking a self-supervised tagger against the human-labeled model on HumanEval, showing equivalent hallucination reduction.

### Open Question 2
- **Question:** Are the empirically fixed valence ($|v| > 0.5$) and arousal ($a > 0.7$) thresholds robust across diverse programming languages and task complexities?
- **Basis in paper:** [explicit] Section 8.2 notes that these thresholds "may exhibit sensitivity to programming language or task complexities, requiring further exploration."
- **Why unresolved:** Experiments primarily focused on Python benchmarks (HumanEval, BigCodeBench), leaving cross-language stability unproven.
- **What evidence would resolve it:** Cross-domain evaluation on non-Python benchmarks (e.g., Java, C++) demonstrating stable performance without manual threshold re-tuning.

### Open Question 3
- **Question:** Does valence tagging introduce cultural biases in error prioritization, and can diverse training datasets effectively mitigate this risk?
- **Basis in paper:** [explicit] Section 8.2 identifies the risk that tagging may introduce "cultural biases if trained on non-diverse... notions of embarrassment."
- **Why unresolved:** The paper highlights the ethical risk but does not quantify bias in the current MLP tagger or validate mitigation strategies.
- **What evidence would resolve it:** A comparative analysis of valence assignments across culturally diverse annotator pools to ensure equitable error flagging.

### Open Question 4
- **Question:** Does the affective dream-replay mechanism maintain computational efficiency when scaled to frontier models?
- **Basis in paper:** [explicit] Section 9 lists "Scaling to frontier models like GPT-4" as a future direction.
- **Why unresolved:** Experiments were limited to CodeT5-small (60M params); the 5-6% compute overhead may scale differently for models with billions of parameters.
- **What evidence would resolve it:** Application of CosmoCore to >10B parameter models with throughput and latency analysis.

## Limitations
- Valence-arousal tagger generalization to unseen code domains is unverified; cross-benchmark performance drops suggest domain sensitivity
- Pruning thresholds and Dream Queue gating lack sensitivity analysis; results may be brittle to hyperparameter shifts
- Confidence variance meta-tuning is described but not benchmarked against fixed pruning schedules

## Confidence
- **High:** 48% hallucination reduction on HumanEval is supported by ablation showing valence gating adds measurable gain
- **Medium:** 45% self-correction acceleration relies on pass@1 improvements; underlying trajectory-level correction rates are not disclosed
- **Low:** Cross-task generalization and pruning robustness are asserted but minimally validated

## Next Checks
1. Ablate valence gating: Run CosmoCore with v = 0 for all trajectories. Compare hallucination rate on HumanEval subset (10 prompts) to full CosmoCore
2. Threshold sensitivity sweep: Vary Dream Queue and Prune Bin thresholds. Measure buffer occupancy, hallucination rate, and training stability
3. Out-of-distribution test: Train on Python/HumanEval, evaluate on BigCodeBench. Assess whether valence tags transfer across code domains