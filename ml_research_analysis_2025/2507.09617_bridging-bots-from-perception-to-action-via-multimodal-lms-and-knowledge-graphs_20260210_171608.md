---
ver: rpa2
title: 'Bridging Bots: from Perception to Action via Multimodal-LMs and Knowledge
  Graphs'
arxiv_id: '2507.09617'
source_url: https://arxiv.org/abs/2507.09617
tags:
- obot
- action
- soma
- ontology
- graph
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study bridges perception and action in service robots by integrating
  multimodal language models with knowledge graphs and ontologies. The framework generates
  ontology-compliant knowledge graphs from sensory inputs, enabling platform-independent
  reasoning and task planning.
---

# Bridging Bots: from Perception to Action via Multimodal-LMs and Knowledge Graphs

## Quick Facts
- **arXiv ID**: 2507.09617
- **Source URL**: https://arxiv.org/abs/2507.09617
- **Reference count**: 16
- **Primary result**: Multimodal models can reliably generate ontology-compliant knowledge graphs from sensory input, with integration strategy more important than model recency.

## Executive Summary
This study addresses the challenge of bridging perception and action in service robots by integrating multimodal language models with knowledge graphs and ontologies. The framework generates ontology-compliant knowledge graphs from sensory inputs, enabling platform-independent reasoning and task planning. Through evaluation of five multimodal models across four neurosymbolic integration methods, the research demonstrates that structured symbolic representations can be reliably generated from raw perceptual data, advancing interoperability and adaptability in robotic systems.

## Method Summary
The framework processes images and task descriptions from Webots simulation environments using five multimodal models (LLaVA+LLaMA3, LLaMA4 Scout, LLaMA4 Maverick, GPT-4.1-nano, GPT-o1) accessed via APIs. Four integration methods transform inputs to knowledge graphs: DPE (dynamic path extraction), D2KG (direct KG generation), D2KG-RAG (RAG-based retrieval), and I2KG (image-to-KG). The system generates two-stage knowledge graphs - observation graphs capturing environment state and action graphs for task execution - both constrained by the OntoBOT ontology. Validation uses SHACL shapes and metrics measuring RDF validity, triple count, compliance, and coverage across 10 independent runs per configuration.

## Key Results
- LLaMA 4 Maverick and GPT-o1 consistently produced the most ontology-compliant graphs across all integration methods
- Integration strategy proved more critical than model recency, with newer models like LLaMA 4 Scout not significantly outperforming older setups
- Action graphs showed slightly higher compliance than observation graphs when task descriptions were included in prompts
- DPE method failed for all LLaMA models, and RAG-based methods failed for GPT models, indicating method-model incompatibilities

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multimodal language models can generate ontology-compliant knowledge graphs from raw sensory input when properly constrained by schema guidance.
- Mechanism: The model receives an ontology schema alongside visual input; the schema acts as a hard constraint on output vocabulary and structure, reducing hallucination and enforcing symbolic consistency.
- Core assumption: Multimodal models can internalize and apply structural constraints from text-based schema definitions during generation.
- Evidence anchors:
  - [abstract] "Results show that structured symbolic representations can be reliably generated from raw perceptual data"
  - [section 4.1] LLaMA 4 Maverick and GPT-o1 achieved highest compliance/coverage; unified multimodal models outperformed modular pipelines
  - [corpus] Related work (arXiv:2511.05991) confirms ontology construction strategy impacts downstream system performance, but does not specifically validate this constraint mechanism
- Break condition: When schema complexity exceeds model's context window or reasoning capacity (e.g., DPE method failed across all LLaMA models despite their capabilities)

### Mechanism 2
- Claim: Integration strategy—how neural and symbolic components are coupled—determines output quality more than model recency or size.
- Mechanism: Four integration methods (DPE, D2KG, D2KG-RAG, I2KG) process inputs differently; narrative-based vs. direct vision-based paths, and embedded vs. retrieved ontology access, create different constraint application patterns.
- Core assumption: The pathway from perception to symbolic output affects how well constraints propagate through the generation process.
- Evidence anchors:
  - [abstract] "newer models do not guarantee better results, highlighting the critical role of the integration strategy"
  - [section 4.1] "LLaMa 4 Scout, despite being a newer model, did not significantly outperform the older LLaVA + LLaMA 3 setup"
  - [corpus] Limited direct evidence; related surveys (arXiv:2510.20345) discuss LLM-KG integration paradigms but don't isolate strategy vs. model effects
- Break condition: RAG-based retrieval failed for GPT models entirely (white cells in heatmap), suggesting implementation-specific breakage

### Mechanism 3
- Claim: Two-stage knowledge graph generation (observation → action) enables task-grounded action planning with maintained ontological consistency.
- Mechanism: First, an observation graph captures environment state from perception; second, an action graph uses the observation graph plus task description to generate executable action sequences, both constrained by the same ontology.
- Core assumption: Decomposing perception-to-action into discrete stages improves output quality compared to end-to-end generation.
- Evidence anchors:
  - [section 3.2] "Our framework generates two distinct KGs from the input data: an observation graph and an action graph"
  - [section 4.1] "action graphs tend to show slightly higher compliance than observation graphs... including the task description in the prompt does not hinder model performance"
  - [corpus] No direct comparative evidence for two-stage vs. one-stage approaches in retrieved papers
- Break condition: Not explicitly tested; paper does not compare against single-stage alternatives

## Foundational Learning

- Concept: **Knowledge Graphs and RDF/Turtle Format**
  - Why needed here: All framework outputs are KGs in Turtle format; understanding subject-predicate-object triples, prefixes, and namespaces is required to interpret results and debug failures.
  - Quick check question: Given the triple `ex:Fridge a obot:Appliance`, which part is the class and which is the instance?

- Concept: **SHACL (Shapes Constraint Language)**
  - Why needed here: The paper uses SHACL shapes to validate structural correctness of generated KGs; understanding NodeShapes, property constraints, and minCount is essential for extending validation.
  - Quick check question: What does `sh:minCount 1` enforce in a SHACL property constraint?

- Concept: **Ontology Classes vs. Properties vs. Instances**
  - Why needed here: Compliance and coverage metrics distinguish between these; prompts must specify which can be invented (instances with `ex:` prefix) vs. which must come from the ontology.
  - Quick check question: In the prompt, why can `ex:Jar1` be created but `obot:newProperty` cannot?

## Architecture Onboarding

- Component map: Images + Task description + Ontology → Integration Method → Turtle KG → SHACL Validation → Metrics
- Critical path: Image + Task + Ontology → Integration Method → Turtle KG → SHACL Validation → Metrics
- Design tradeoffs:
  - Unified multimodal models (Maverick, GPT-o1) vs. modular pipelines (LLaVA+LLaMA3): Unified shows better compliance but may have higher latency/cost
  - D2KG (embedded ontology) vs. D2KG-RAG (retrieved ontology): RAG failed for GPT models; trade reliability for scalability
  - Narrative-based (D2KG) vs. vision-based (I2KG): I2KG skips intermediate description but may lose detail
- Failure signatures:
  - Zero compliance/coverage → Model ignored ontology entirely (check prompt formatting)
  - White cells in heatmap → RDF parsing failed or method-model incompatibility
  - High standard deviation → Model temperature too high or prompt ambiguity
  - DPE failures across all LLaMA → Method not suitable for ontology-constrained generation
- First 3 experiments:
  1. Replicate D2KG method with LLaMA 4 Maverick on a single image; validate RDF parsing and compute compliance score (expect ~0.85+)
  2. Test I2KG vs. D2KG on same input with GPT-o1; compare triple counts and SHACL violation ratios to quantify information loss
  3. Introduce domain-specific ontology extensions (e.g., new appliance types); measure coverage degradation to assess generalization bounds

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the neurosymbolic framework be refined to mitigate the high output variability and structural errors observed in generated Knowledge Graphs, even among top-performing models?
- Basis in paper: [explicit] The conclusion states that "variability and structural errors remain a concern, pointing to the need for further refinement," supported by results in Section 4.1 showing high standard deviations across runs.
- Why unresolved: The paper identifies output inconsistency as a persistent limitation of using current LLMs but does not propose or test mechanisms to stabilize the generated symbolic outputs.
- What evidence would resolve it: A modified pipeline incorporating deterministic validation loops or constrained decoding that demonstrates significantly lower standard deviations in compliance scores compared to the current baseline.

### Open Question 2
- Question: Why do specific integration strategies like Dynamic Path Extraction (DPE) and RAG fail for certain model architectures (e.g., DPE for LLaMA, RAG for GPT), and how can these methods be adapted?
- Basis in paper: [explicit] Section 4.1 reports that "All LLaMA-based models show zero compliance... using the ‘dpe’" and "‘d2kg-rag’... fails to generate valid KGs for GPT-based models," describing these failures as "unexpected."
- Why unresolved: The authors hypothesize potential "configuration discrepancy" or API handling issues but do not isolate the root cause or propose a fix for these specific method-model incompatibilities.
- What evidence would resolve it: A systematic ablation study testing these methods with controlled API parameters or local model deployments to isolate whether the failure stems from model architecture or the tool implementation.

### Open Question 3
- Question: Can the proposed pipeline maintain high ontology compliance and coverage when applied to more complex, dynamic environments or different domain ontologies beyond the single kitchen scenario tested?
- Basis in paper: [inferred] While the introduction emphasizes the need for "generalizability" in robotic systems, the evaluation methodology is restricted to a single domestic kitchen environment and the specific OntoBOT ontology.
- Why unresolved: The paper does not demonstrate that the successful performance of models like LLaMA 4 Maverick transfers to different domains (e.g., industrial settings) or alternative task ontologies.
- What evidence would resolve it: Evaluation results from applying the identical framework to distinct environmental datasets and disparate ontologies, showing that high compliance scores are sustained outside the specific use case tested.

## Limitations

- **Method-model compatibility gaps**: DPE method failed entirely with LLaMA models, and RAG-based approaches failed with GPT models, preventing systematic evaluation of certain method-model combinations
- **Ontology scope constraints**: OntoBOT was specifically designed for kitchen service robots, limiting generalizability to other domains with different ontologies
- **Missing baseline comparisons**: The study did not benchmark against purely symbolic or purely neural baselines, making it difficult to quantify the absolute contribution of neurosymbolic integration

## Confidence

- **High Confidence**: The core finding that integration strategy affects output quality more than model recency/size is well-supported by direct experimental evidence across multiple configurations
- **Medium Confidence**: The claim that multimodal models can reliably generate ontology-compliant KGs is supported, but confidence is reduced by the DPE method failures and RAG incompatibility issues
- **Low Confidence**: The assertion that two-stage KG generation improves quality over single-stage approaches lacks direct comparative evidence, as no one-stage alternatives were tested

## Next Checks

1. Test alternative RAG implementations (different chunking strategies, embedding models) with GPT models to determine if the failure is method-specific or implementation-specific
2. Evaluate the same framework with a larger, more complex ontology to assess scalability limits and identify at what point compliance degrades significantly
3. Compare performance against a purely symbolic rule-based system and a purely neural end-to-end approach on identical tasks to isolate the contribution of neurosymbolic integration