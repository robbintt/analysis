---
ver: rpa2
title: Escaping Stability-Plasticity Dilemma in Online Continual Learning for Motion
  Forecasting via Synergetic Memory Rehearsal
arxiv_id: '2508.19571'
source_url: https://arxiv.org/abs/2508.19571
tags:
- memory
- learning
- tasks
- plasticity
- syrem
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper proposes Synergetic Memory Rehearsal (SyReM), a continual
  learning method for motion forecasting that addresses the stability-plasticity dilemma.
  SyReM maintains a compact memory buffer and employs two core mechanisms: (1) a gradient
  projection constraint that limits loss increments over learned tasks to ensure memory
  stability, and (2) a selective memory rehearsal mechanism that enhances learning
  plasticity by selecting samples most similar to new data based on loss gradient
  cosine similarity.'
---

# Escaping Stability-Plasticity Dilemma in Online Continual Learning for Motion Forecasting via Synergetic Memory Rehearsal

## Quick Facts
- **arXiv ID:** 2508.19571
- **Source URL:** https://arxiv.org/abs/2508.19571
- **Reference count:** 40
- **Primary result:** Proposes SyReM method achieving lowest FDE-BWT (-0.01m) and MR-BWT (-0.01%) among compared methods while reducing MR-CT by 27% compared to vanilla models

## Executive Summary
This paper addresses the stability-plasticity dilemma in online continual learning for motion forecasting. The proposed Synergetic Memory Rehearsal (SyReM) method maintains a compact memory buffer and employs two core mechanisms: gradient projection to constrain loss increments on learned tasks for stability, and selective memory rehearsal that enhances plasticity by selecting samples most similar to new data based on loss gradient cosine similarity. Evaluated on 11 naturalistic driving datasets from INTERACTION benchmark, SyReM significantly outperforms non-CL and CL baselines in mitigating catastrophic forgetting while improving forecasting accuracy on new tasks.

## Method Summary
SyReM is an online continual learning method for motion forecasting that maintains a fixed-size memory buffer using reservoir sampling. For each incoming batch, it computes gradients and selects top-B samples from the memory buffer based on cosine similarity to the current task gradient. These selected samples are used for rehearsal alongside the current task. A gradient projection constraint ensures that parameter updates do not increase the average loss over the memory buffer, preserving stability. The method uses UQnet as base model with batch size 8, learning rate 1×10^-3, and Adam optimizer, predicting 6 endpoints for 3s trajectories from 1s observations.

## Key Results
- Achieves lowest FDE-BWT (-0.01m) and MR-BWT (-0.01%) among compared methods, demonstrating superior memory stability
- Reduces MR-CT by 27% compared to vanilla models, showing enhanced learning plasticity
- Exhibits strong zero-shot generalization capabilities on unseen tasks
- Ablation studies validate the effectiveness of selective memory rehearsal component

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Gradient projection preserves memory stability by constraining parameter updates
- **Mechanism:** An inequality constraint requires the gradient of total loss to have non-negative inner product with the gradient on the memory buffer (g⊤gM ≥ 0). If violated, the gradient is projected to the nearest feasible direction using g̃* = g − (g⊤gM/‖gM‖²²)gM, preventing updates that would increase loss on stored samples
- **Core assumption:** The memory buffer gradient gM is a sufficient proxy for important directions in previously learned task space
- **Evidence anchors:** Abstract states "an inequality constraint that limits increments in the average loss over the memory buffer"; Section IV-C provides full derivation showing projection when constraint violated
- **Break condition:** If memory buffer samples are unrepresentative of past task distributions (e.g., poor reservoir sampling), gM becomes a misleading constraint direction

### Mechanism 2
- **Claim:** Selective memory rehearsal enhances plasticity by replaying gradient-similar samples
- **Mechanism:** At each step, compute cosine similarity qk = gc·gk/(‖gc‖‖gk‖) between current task gradient gc (from temporal buffer) and candidate memory gradients gk. Select top-B samples with highest similarity for rehearsal. These samples provide gradient-aligned prior knowledge that aids adaptation without conflicting with stability constraints
- **Core assumption:** Gradient cosine similarity correlates with transfer-relevant knowledge overlap between old and new tasks
- **Evidence anchors:** Abstract mentions "selective memory rehearsal mechanism...selecting samples most similar to recently observed data based on loss gradient cosine similarity"; Section IV-B shows ranking by similarity scores and selection process
- **Break condition:** If tasks are fundamentally dissimilar (gradient directions orthogonal or opposite), high-similarity samples may not exist in buffer, yielding no useful rehearsal signal

### Mechanism 3
- **Claim:** Reservoir sampling maintains unbiased distributional coverage with fixed memory
- **Mechanism:** Each incoming sample has equal probability |M|/|S| of being stored. For kth sample, replace random buffer entry with probability |M|/k, ensuring each stored sample is uniformly drawn from observed stream without knowing total length
- **Core assumption:** Uniform sampling provides adequate coverage of learned task distributions
- **Evidence anchors:** Section IV-A provides full reservoir sampling procedure; Section V-A-2 states buffer size |M|=1000 is 0.47% of total training cases
- **Break condition:** If task distributions are highly imbalanced or have rare critical samples, uniform sampling may miss important edge cases

## Foundational Learning

- **Concept: Catastrophic forgetting**
  - Why needed here: The core problem—sequential parameter updates overwrite knowledge needed for previous tasks
  - Quick check question: Can you explain why SGD on a new task distribution naturally degrades old-task performance?

- **Concept: Online continual learning (one-pass stream)**
  - Why needed here: SyReM assumes samples arrive sequentially with no re-visit; differs from offline CL with multi-epoch access
  - Quick check question: Why does reservoir sampling matter specifically for online (unknown-length) streams?

- **Concept: Gradient projection for constrained optimization**
  - Why needed here: The stability mechanism uses Lagrangian-derived projection to satisfy inequality constraints
  - Quick check question: If g⊤gM < 0, what does the projected gradient g̃* guarantee about loss on memory buffer?

## Architecture Onboarding

- **Component map:**
  Data stream → [Temporal Buffer Mtmp] → gc gradient → [Selective Rehearsal] → Rehearsal loss Lr + Current task loss → Total loss Ltotal → [Gradient Projection] → Parameter update θ
  Data stream → [Long-term Buffer M] (reservoir) → candidates for rehearsal → similarity scoring (cosine qk)

- **Critical path:**
  1. Initialize empty buffers (M and Mtmp)
  2. For each incoming batch: store in Mtmp, update M via reservoir
  3. Compute gc from Mtmp, sample M candidates, compute similarities
  4. Select top-B similar samples → compute Lr
  5. Compute total gradient g, memory gradient gM
  6. If g⊤gM < 0, project to g̃*; otherwise use g
  7. Update θ

- **Design tradeoffs:**
  - Buffer size |M|: Larger improves coverage but increases memory/compute
  - Candidate pool size M: Paper uses M=2B=16 (minimum viable); larger may find better matches
  - Assumption: Current batch gradient gc represents current task—noisy if batch is small or unrepresentative

- **Failure signatures:**
  - FDE-BWT increases: Gradient projection may be too weak or gM unrepresentative
  - MR-CT degrades (worse plasticity): Selected rehearsal samples have negative similarity scores (check Fig. 13 heatmap)
  - Zero-shot generalization fails: Memory buffer lacks transfer-bridging samples

- **First 3 experiments:**
  1. Ablate selective rehearsal: Replace with random sampling (SyReM-R baseline)—should see MR-CT increase by ~26% per paper results
  2. Vary buffer size: Test |M| ∈ {500, 1000, 2000} to find memory-performance frontier
  3. Visualize gradient similarities: Log qk distribution across training steps—if mostly negative, task sequences may be incompatible for this selection strategy

## Open Questions the Paper Calls Out
- What are the specific underlying mechanisms by which different memory rehearsal strategies (selective vs. random) influence zero-shot generalization in online continual learning?
- Can the SyReM framework be effectively integrated with few-shot learning paradigms to handle unseen domains without compromising the stability-plasticity balance?
- Does the reliance on gradient cosine similarity for selective memory rehearsal generalize effectively to diverse DNN architectures beyond the specific UQnet model used in the study?

## Limitations
- The gradient projection efficacy depends heavily on the memory buffer being representative of previously learned task distributions
- The selective rehearsal mechanism assumes gradient cosine similarity meaningfully correlates with task-relevant knowledge transfer, which may not hold for fundamentally dissimilar driving scenarios
- The UQnet base model's regularization specifics are unclear, potentially affecting gradient magnitudes and projection behavior

## Confidence
- **High confidence:** FDE-BWT improvement over baselines (0.01m gain is statistically measurable)
- **Medium confidence:** MR-CT reduction claim (27% improvement depends on gradient similarity assumptions)
- **Low confidence:** Zero-shot generalization capability (claims are qualitative without specific zero-shot metrics)

## Next Checks
1. **Gradient similarity distribution analysis:** Log and visualize qk distributions during training—if majority of scores are negative, the selection mechanism may be fundamentally incompatible with the task sequence
2. **Buffer coverage validation:** Compute Wasserstein distance between reservoir buffer distribution and actual task distributions across all 11 scenarios to quantify representation quality
3. **Projection constraint effectiveness test:** Run controlled experiments with synthetic tasks where g⊤gM violation conditions are guaranteed, then measure whether projection actually prevents FDE increases versus unconstrained updates