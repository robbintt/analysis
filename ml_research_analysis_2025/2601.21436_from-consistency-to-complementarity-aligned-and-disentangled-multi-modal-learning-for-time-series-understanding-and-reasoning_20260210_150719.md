---
ver: rpa2
title: 'From Consistency to Complementarity: Aligned and Disentangled Multi-modal
  Learning for Time Series Understanding and Reasoning'
arxiv_id: '2601.21436'
source_url: https://arxiv.org/abs/2601.21436
tags:
- series
- time
- reasoning
- numerical
- understanding
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of enabling multi-modal large
  language models (MLLMs) to understand and reason about time series data. The core
  issue is integrating numerical time series with their visual representations while
  ensuring fine-grained alignment and disentangling shared from modality-specific
  semantics.
---

# From Consistency to Complementarity: Aligned and Disentangled Multi-modal Learning for Time Series Understanding and Reasoning

## Quick Facts
- arXiv ID: 2601.21436
- Source URL: https://arxiv.org/abs/2601.21436
- Authors: Hang Ni; Weijia Zhang; Fei Wang; Zezhi Shao; Hao Liu
- Reference count: 20
- Key outcome: MADI consistently outperforms both general-purpose LLMs and time-series-specialized MLLMs, with improvements in accuracy and reasoning across understanding and reasoning tasks.

## Executive Summary
This paper addresses the challenge of enabling multi-modal large language models (MLLMs) to understand and reason about time series data. The core issue is integrating numerical time series with their visual representations while ensuring fine-grained alignment and disentangling shared from modality-specific semantics. The proposed MADI method introduces patch-level alignment to enforce precise correspondence between numerical values and visual plots, and a discrete disentangled interaction module that uses vector quantization to separate common and unique information across modalities. Additionally, a critical-token highlighting module emphasizes informative, query-relevant signals. Experiments on synthetic and real-world benchmarks show MADI consistently outperforms both general-purpose LLMs and time-series-specialized MLLMs, with improvements in accuracy and reasoning across understanding and reasoning tasks.

## Method Summary
MADI is a multi-modal learning framework designed to enhance time series understanding and reasoning in large language models. The method introduces three core components: Patch-level Alignment (PA) enforces fine-grained correspondence between numerical and visual patches using contrastive learning; Discrete Disentangled Interaction (DDI) employs hierarchical vector quantization to separate modality-common semantics from modality-unique information; and Critical-token Highlighting (CTH) identifies and prepends query-relevant and modality-intrinsic salient tokens to improve reasoning. The model is built on Qwen2.5-VL-7B-Instruct and trained on synthetic ChatTS data before evaluation on synthetic and real-world benchmarks covering understanding and reasoning tasks.

## Key Results
- MADI consistently outperforms both general-purpose LLMs and time-series-specialized MLLMs on understanding and reasoning tasks.
- Patch-level alignment provides 2-4% accuracy improvements by enforcing fine-grained cross-modal correspondence.
- Discrete disentangled interaction improves performance by 4-6% by separating shared and unique modality semantics.
- Critical-token highlighting contributes 2-4% gains by emphasizing informative, query-relevant signals.

## Why This Works (Mechanism)

### Mechanism 1: Patch-level Alignment (PA)
- Claim: Fine-grained contrastive alignment between numerical patches and visual/textual tokens establishes physically grounded correspondence, reducing cross-modal misalignment.
- Mechanism: Time series are divided into non-overlapping patches; visual plots are rendered with stripped decorative elements to enable 1:1 patch correspondence; numerical patches serve as anchors with aligned visual/text tokens as positives in an InfoNCE contrastive loss.
- Core assumption: The numerical-visual mapping can be learned via local similarity maximization; patch-level granularity captures meaningful temporal units.
- Evidence anchors:
  - [abstract] "Patch-level Alignment... enforces physically grounded fine-grained correspondence across heterogeneous modalities"
  - [section 4.1.3] "The numerical–visual alignment loss is defined with an InfoNCE objective... we stop the gradients of encoded visual and textual embeddings to optimize the numerical encoder only"
  - [corpus] Related work on fine-grained multi-modal alignment exists (Hallucination at a Glance, 2506.07227), but no direct corpus evidence confirms patch-level alignment for time-series MLLMs specifically.
- Break condition: If visual patches no longer correspond to numerical patches (e.g., decorative elements re-introduced, or resampling breaks 1:1 mapping), alignment degrades; ablation shows ~2-4% accuracy drop without PA.

### Mechanism 2: Discrete Disentangled Interaction (DDI)
- Claim: Hierarchical vector quantization separates modality-common semantics into compact discrete latents, enabling cleaner cross-modal synergy from modality-unique residuals.
- Mechanism: Shared hierarchical RVQ with M codebooks progressively quantizes residuals at multiple temporal scales; modality-common Z is extracted via nearest-neighbor assignment; unique components U = E - Z are derived by residual decomposition; cross-attention then integrates U across modalities.
- Core assumption: Shared semantics across numerical and visual modalities can be compressed into a discrete codebook; orthogonality between common and unique components is learnable.
- Evidence anchors:
  - [abstract] "Discrete Disentangled Interaction, which separates modality-common semantics into compact discrete latents and adaptively synergizes the purified modality-unique information"
  - [section 4.2.1] "We employ a shared hierarchical discretization block to extract modality-common representations Z... The modality-unique components are then obtained by residual decomposition"
  - [corpus] "Disentangled Multi-modal Learning" (2508.16479) addresses multi-modal heterogeneity via disentanglement, supporting the general approach, but time-series-specific evidence is limited.
- Break condition: If codebook capacity is insufficient or orthogonality regularization (β) is too weak, shared semantics leak into unique components; ablation shows continuous-space disentanglement ("w/o VQ") underperforms.

### Mechanism 3: Critical-token Highlighting (CTH)
- Claim: Prepending question-conditioned and modality-intrinsic highlighted tokens to modality sequences improves LLM attention to relevant signals.
- Mechanism: Two parallel cross-attention branches—(1) question-conditioned queries attend over modality tokens, (2) learnable queries identify modality-intrinsic salient tokens; outputs are summed and prepended to modality sequences.
- Core assumption: Highlighted tokens provide a useful inductive bias without replacing original token sequences.
- Evidence anchors:
  - [abstract] "Critical-token Highlighting, which emphasizes informative, query-relevant signals for robust reasoning"
  - [section 4.3] "CTH employs two parallel cross-attention branches... The highlighted token embeddings are fused... and prepended to the modality token sequence"
  - [corpus] No direct corpus evidence for this specific highlighting mechanism in time-series MLLMs.
- Break condition: If question embeddings are noisy or modalities lack intrinsic salient structure, highlighted tokens may introduce noise; ablation shows modest drops in reasoning tasks without CTH.

## Foundational Learning

- Concept: Vector Quantization (VQ) and Residual VQ
  - Why needed here: DDI relies on hierarchical RVQ to discretize continuous embeddings; without understanding VQ commitment loss and EMA codebook updates, implementation will fail.
  - Quick check question: Can you explain why STE (straight-through estimator) is needed for gradient flow through discrete quantization?

- Concept: Contrastive Learning (InfoNCE)
  - Why needed here: PA uses patch-wise InfoNCE loss for cross-modal alignment; understanding temperature scaling and stop-gradient is critical.
  - Quick check question: Why does the paper stop gradients for visual/textual embeddings while updating only the numerical encoder?

- Concept: Cross-modal Attention
  - Why needed here: Both DDI and CTH use cross-attention to fuse unique modality signals and highlight relevant tokens.
  - Quick check question: What is the difference between self-attention and cross-attention in terms of Q, K, V sources?

## Architecture Onboarding

- Component map:
  Time series input → Patching → PA alignment → DDI disentanglement + fusion → CTH highlighting → Concat with question/context → LLM decoding

- Critical path:
  Time series input → Patching → PA alignment → DDI disentanglement + fusion → CTH highlighting → Concat with question/context → LLM decoding

- Design tradeoffs:
  - Patch size p_n: Smaller = finer alignment but more tokens/compute; paper uses p_n=8
  - Codebook levels M: More levels = finer granularity but higher memory; paper uses M=3
  - Disentanglement regularization weights (α, β): Too high = over-regularization, too low = leakage; paper uses α=5, β=1

- Failure signatures:
  - Diagonal pattern absent in patch similarity heatmap → PA not converging
  - Unique-Unique similarity distribution overlapping with Common-Common → DDI not disentangling properly
  - Significant performance gap between categorical and numerical tasks → normalization statistics not properly exposed

- First 3 experiments:
  1. **PA alignment sanity check**: Visualize patch-level similarity matrices (Figure 4); confirm diagonal high-similarity patterns before full training.
  2. **DDI disentanglement validation**: Plot KDE of cross-modal similarities for common vs. unique components (Figure 5); ensure clear separation.
  3. **Ablation baseline**: Train MADI without DDI ("w/o DDI") on synthetic understanding tasks; expect ~4-6% accuracy drop per Table 1 patterns.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the reliance on synthetic training data create a distribution shift that limits MADI's generalization compared to large-scale, pre-trained general-purpose MLLMs?
- Basis in paper: [explicit] Page 7 notes that "models fine-tuned on synthetic data often degrade on real-world benchmarks" despite overall improvements.
- Why unresolved: While MADI outperforms baselines, the persistent gap between synthetic training and real-world evaluation suggests the model may still struggle with domain-specific nuances not captured by the TSEvol generation pipeline.
- What evidence would resolve it: A zero-shot evaluation on purely real-world, out-of-domain datasets (not included in the ChatTS benchmark) without any synthetic pre-training.

### Open Question 2
- Question: Can the template-based patch-wise captioning mechanism effectively support high-level semantic reasoning beyond basic statistical description?
- Basis in paper: [inferred] Section 4.1.1 defines patch-wise captions using only basic statistics (mean, max, etc.), while Table 2 shows significantly lower performance on reasoning tasks compared to understanding tasks.
- Why unresolved: It is unclear if statistical captions provide sufficient semantic depth for the LLM to perform complex inductive or causal reasoning, or if they introduce a semantic bottleneck.
- What evidence would resolve it: An ablation study replacing statistical captions with semantically rich, LLM-generated descriptions for patch alignment, specifically measuring the impact on the "Reasoning" task subset.

### Open Question 3
- Question: How robust is the patch-level alignment module when visual rendering artifacts or style variations disrupt the assumed one-to-one correspondence?
- Basis in paper: [inferred] Section 4.1.1 assumes a strict "one-to-one correspondence" between numerical patches and visual patches based on specific plotting parameters, but real-world visualizations may vary.
- Why unresolved: The paper does not explore scenarios where the visual modality is distorted or rendered differently, which could break the "physically grounded" contrastive alignment.
- What evidence would resolve it: Sensitivity analysis measuring performance degradation when visual input is subjected to style transfer, resolution changes, or axis distortion while keeping numerical input constant.

## Limitations
- The improvements from each component are relatively modest (2-4% accuracy gains), suggesting the approach is incremental rather than transformative.
- The model relies on synthetic training data, which may not fully capture real-world time series complexity and could create distribution shifts.
- The evaluation primarily focuses on understanding tasks rather than complex reasoning scenarios, leaving open questions about the model's ability to handle sophisticated multi-step reasoning.

## Confidence

- Patch-level alignment mechanism: **High**
- Discrete disentangled interaction effectiveness: **Medium**
- Critical-token highlighting contribution: **Medium**
- Generalization across diverse time series domains: **Medium**

## Next Checks

1. **Cross-modal alignment robustness**: Test PA performance when visual plots include decorative elements (axes, ticks, labels) or when patch sizes don't match exactly. Measure degradation in accuracy and visualize patch similarity matrices to confirm diagonal patterns break down.

2. **Disentanglement orthogonality validation**: Conduct ablation studies varying the disentanglement regularization weights (α, β) and codebook sizes (K, M). Measure cross-modal similarity distributions between common and unique components to verify whether shared semantics leak into unique representations at extreme parameter settings.

3. **Real-world reasoning complexity**: Evaluate MADI on complex multi-step reasoning tasks requiring temporal pattern recognition, anomaly detection, and causal inference. Compare performance against strong baselines on real-world datasets where the synthetic ChatTS training distribution doesn't match the test distribution.