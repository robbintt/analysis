---
ver: rpa2
title: 'MDMLP-EIA: Multi-domain Dynamic MLPs with Energy Invariant Attention for Time
  Series Forecasting'
arxiv_id: '2511.09924'
source_url: https://arxiv.org/abs/2511.09924
tags:
- seasonal
- prediction
- time
- mdmlp-eia
- fusion
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'MDMLP-EIA addresses critical limitations in MLP-based time series
  forecasting by introducing three key innovations: an adaptive fused dual-domain
  seasonal MLP that separates strong and weak seasonal signals to preserve low-energy
  cyclical patterns, an energy invariant attention mechanism that maintains constant
  signal energy during trend-seasonal fusion for enhanced robustness, and a dynamic
  capacity adjustment mechanism that scales MLP neurons with the square root of channel
  count to prevent overfitting while ensuring adequate capacity. Extensive experiments
  on nine benchmark datasets demonstrate state-of-the-art performance, achieving approximately
  4.14% improvement in MSE and 2.29% improvement in MAE compared to the most recent
  methods, with superior computational efficiency and robustness in long-horizon forecasting
  scenarios.'
---

# MDMLP-EIA: Multi-domain Dynamic MLPs with Energy Invariant Attention for Time Series Forecasting

## Quick Facts
- **arXiv ID:** 2511.09924
- **Source URL:** https://arxiv.org/abs/2511.09924
- **Reference count:** 40
- **Primary result:** ~4.14% MSE improvement over SOTA on 9 benchmark datasets

## Executive Summary
MDMLP-EIA introduces a Multi-domain Dynamic MLP architecture for time series forecasting that addresses critical limitations in existing MLP-based approaches. The method separates seasonal signals into strong and weak components, preserves weak cyclical patterns through adaptive fusion, maintains constant signal energy during trend-seasonal combination via energy invariant attention, and dynamically scales MLP capacity based on channel count. Extensive experiments demonstrate state-of-the-art performance with approximately 4.14% improvement in MSE and 2.29% improvement in MAE compared to recent methods, while maintaining superior computational efficiency.

## Method Summary
MDMLP-EIA processes multivariate time series through a decomposition-prediction-reconstruction framework. It begins with RevIN normalization followed by EMA-based decomposition into trend and seasonal components. Three parallel channel-independent MLPs process these components: a trend MLP, a strong seasonal MLP operating in frequency domain (FFT→MLP→iFFT), and a weak seasonal MLP operating in time domain. The architecture employs Dynamic Capacity Adjustment (DCA) that scales neuron count with √(channel count) to prevent overfitting. Adaptive Zero-initialized Channel Fusion (AZCF) combines strong and weak seasonal predictions using learnable weights initialized to zero. Energy Invariant Attention (EIA) then fuses trend and seasonal predictions while maintaining constant total signal energy through a 2× scaling factor.

## Key Results
- Achieves approximately 4.14% improvement in MSE and 2.29% improvement in MAE compared to most recent SOTA methods
- Outperforms specialized models like GNNs on Traffic dataset (0.3086 MSE vs 0.3294) while maintaining computational efficiency
- Demonstrates superior long-horizon forecasting capability with consistent performance across lookback lengths L=96 and horizons T∈{96,192,336,720}
- Shows robustness through ablation studies confirming each component's contribution: AZCF (0.341→0.338 MSE), DCA (fixed 256→0.338 MSE), EIA (0.338→0.338 MSE baseline)

## Why This Works (Mechanism)

### Mechanism 1: Adaptive Zero-Initialized Channel Fusion (AZCF)
Preserves weak seasonal signals by splitting seasonal processing into frequency-domain (strong signals) and time-domain (weak signals) paths, then fusing them with learnable weights initialized to zero. This ensures training starts from reliable strong-signal-only baseline and grows fusion only when weak signals demonstrably reduce loss. The zero initialization is critical: without it, random fusion weights can amplify noise.

### Mechanism 2: Energy Invariant Attention (EIA)
Maintains constant total signal energy during trend-seasonal fusion using y3 = 2×(β⊙y1 + (1-β)⊙y2) instead of direct summation. The 2× factor compensates for implicit normalization, ensuring adaptive weighting without uncontrolled amplification. This theoretically aligns with decomposition-prediction-reconstruction frameworks and prevents noise propagation.

### Mechanism 3: Dynamic Capacity Adjustment (DCA)
Scales MLP neuron count as n = base × ⌈√(C/τ)⌉ where C is channel count and τ=5. This √C scaling balances underfitting (low C) and overfitting (high C) by accounting for increasing channel redundancy. For 7-channel ETTh1, this yields ~14 neurons; for 862-channel Traffic, ~84 neurons, preventing parameter explosion while ensuring adequate capacity.

## Foundational Learning

- **Frequency-Domain Signal Processing (FFT/iFFT, Softshrink)**: Strong seasonal MLP operates in frequency domain; softshrink denoising removes low-amplitude signals. Understanding why weak signals are lost in frequency domain is essential to grasping weak/strong separation rationale. *Quick check:* Given a signal with dominant 24-hour periodicity and weak 7-day periodicity, which component is likely removed by softshrink in frequency domain?

- **Trend-Seasonal Decomposition (EMA-based)**: MDMLP-EIA inherits decomposition-prediction-reconstruction framework. EMA extracts smooth trend component, leaving seasonal component containing residuals. EIA's energy invariance is theoretically motivated by decomposition reconstruction. *Quick check:* If EMA extracts a smooth trend component, what remains in the seasonal component? How might EIA-weighted fusion differ from simple addition when seasonal component is noisy?

- **Channel-Independent Strategy**: All three MLPs use channel independence—shared weights across channels but independent temporal processing. DCA addresses capacity limitations inherent to this weight-sharing approach. *Quick check:* For a 100-channel multivariate time series, would a channel-independent MLP process each channel with same or different weights? How does this affect parameter count vs. channel-mixing approach?

## Architecture Onboarding

### Component Map:
Input (L×C) → RevIN → EMA Decomposition → Trend (x1) and Seasonal (x2) → TrendMLP (DCA, n1=L×cof) → Trend pred (y1) and Seasonal → Frequency Path (FFT→FreMLP→iFFT, DCA, n2=256×cof) → Strong pred (y21) and Time Path (WeakSeasonalMLP, DCA, n3=2L×cof) → Weak pred (y22) → AZCF Fusion (α zero-init) → Seasonal pred (y2) → Concat(y1,y2)→Linear1→GeLU→Dropout→Linear2→Sigmoid → β → y3 = 2×(β⊙y1+(1-β)⊙y2) → InverseRevIN → Output

### Critical Path:
1. **RevIN normalization** is essential—without it, scale differences across channels destabilize EMA decomposition and MLP training.
2. **Weak seasonal path** is key differentiator from prior work; disabling it degrades performance on datasets with subtle periodicities (ETTh2 MSE 0.366→0.362).
3. **AZCF initialization** must be zero—random initialization increases average MSE from 0.338 to 0.341.
4. **EIA 2× scaling** is non-negotiable—without it, fusion becomes convex combination violating energy invariance.

### Design Tradeoffs:
- **AZCF vs full attention**: Uses only C parameters (α per channel) vs O(C²) for QKV attention. Less expressive fusion but lower overfitting risk—critical when weak signals are noisy.
- **√C vs linear scaling**: √C grows slower than C, preventing parameter explosion on high-channel datasets (Traffic: C=862). May underfit if channels are highly independent.
- **τ=5**: Empirically set; may need adjustment for datasets with different channel redundancy characteristics.

### Failure Signatures:
- **Weak seasonal adds noise**: If α grows but MSE increases, weak signals may be mostly noise. Check AZCF vs "w/o WS" ablation.
- **EIA degenerates to direct sum**: If β converges to 0.5 everywhere, learned attention provides no adaptive benefit. Check β distribution post-training.
- **DCA under/over-capacity**: If training loss plateaus high (underfit) or validation loss diverges (overfit), adjust τ. Decrease τ for low-C high-variance data, increase τ for high-C redundant data.

### First 3 Experiments:
1. **Baseline sanity check**: Run on ETTh1 with L=96, T=96. Expected MSE ~0.374. Verify α starts near zero and grows during training, β distribution shows variation.
2. **Ablation cascade**: Disable components sequentially: (a) remove weak seasonal path, (b) replace EIA with direct sum, (c) fix DCA to constant hidden size=256. Compare MSE deltas to validate each mechanism's contribution.
3. **DCA tuning sweep**: For new dataset with unknown C characteristics, run τ ∈ {3, 5, 7, 10} and plot validation MSE vs parameter count. Identify knee point to validate/refute √C assumption for your data.

## Open Questions the Paper Calls Out

### Open Question 1
How can MDMLP-EIA be extended to a unified architecture that effectively handles variable prediction horizons within a single model instance? The current experimental setup evaluates specific fixed lookback and prediction lengths (e.g., L=96, T ∈ {96, 720}), and standard MLPs often require retraining or distinct configurations for different output horizons. A modified MDMLP-EIA capable of generating accurate forecasts for multiple prediction lengths without significant degradation would resolve this.

### Open Question 2
Can the Adaptive Zero-initialized Channel Fusion (AZCF) and Energy Invariant Attention (EIA) mechanisms be effectively adapted for transfer learning across heterogeneous time series domains? The theoretical proofs and experiments focus on within-dataset performance, leaving the generalizability of energy conservation assumptions to data with significantly different statistical properties unverified. Experiments showing pre-training on source domains and fine-tuning on target domains with analysis of α and β behavior would resolve this.

### Open Question 3
Can the channel-independent MLP strategy be augmented to model inter-channel spatial dependencies to close the performance gap with GNNs on datasets like Traffic? The current architecture's channel independence discards explicit topological information critical for road network traffic flow, causing MDMLP-EIA to lag behind specialized spatial models like iTransformer on this specific dataset. A hybrid variant incorporating spatial adjacency matrix or graph convolution would demonstrate this.

## Limitations
- Weak seasonal signal separation criteria remain heuristic—the assumption that time-domain processing can distinguish weak periodicities from noise is not empirically validated through signal analysis
- √C scaling universality is unproven—the assumption of increasing channel redundancy with channel count is novel but lacks external validation beyond benchmark performance
- Arctangent loss and sigmoid learning rate adjustment implementation details are referenced from xPatch without full specification, creating potential reproducibility gaps

## Confidence
- **High Confidence**: DCA's effectiveness (validated through systematic ablation), AZCF's zero-initialization necessity (demonstrated through RCF ablation), and the core decomposition-prediction-reconstruction framework (well-established in TSF literature)
- **Medium Confidence**: EIA's energy invariance contribution (theoretically sound but limited empirical validation), separation of strong/weak seasonal signals (works empirically but separation criteria remain heuristic), and overall SOTA performance claims (benchmarked but against specific set of methods)
- **Low Confidence**: √C scaling universality (novel without external validation), specific EMA decomposition parameters (hyperparameters not fully specified), and interaction effects between all three mechanisms (tested together but not independently validated)

## Next Checks
1. **Signal Separation Analysis**: For dataset with known periodicities (e.g., hourly data with 24h and 168h cycles), visualize frequency-domain output before/after softshrink to verify which signals are classified as "strong" vs. "weak." Then track α channel weights to confirm they activate only for channels containing genuine weak periodicities rather than noise.

2. **DCA Scaling Verification**: Test DCA on synthetic dataset where you control channel independence. Generate datasets with varying degrees of channel correlation (from fully independent to highly redundant) and validate whether √C scaling consistently prevents overfitting while maintaining capacity. Compare against linear scaling (n=C×factor) and logarithmic scaling (n=log(C)×factor).

3. **EIA Robustness Testing**: Create corrupted versions of benchmark datasets with varying noise levels and types (Gaussian, burst, periodic). Run MDMLP-EIA with EIA disabled (direct sum) versus enabled, and measure not just MSE but also signal-to-noise ratio preservation in fused output. This validates whether energy invariance truly improves robustness rather than just being theoretically consistent.