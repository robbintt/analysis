---
ver: rpa2
title: 'AdaPtis: Reducing Pipeline Bubbles with Adaptive Pipeline Parallelism on Heterogeneous
  Models'
arxiv_id: '2509.23722'
source_url: https://arxiv.org/abs/2509.23722
tags:
- pipeline
- scheduling
- workload
- training
- placement
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: AdaPtis addresses the problem of increasing pipeline bubbles in
  training large language models (LLMs) with heterogeneous architectures by co-optimizing
  model partition, model placement, and workload scheduling in pipeline parallelism.
  The core method involves a pipeline performance model that estimates computation,
  communication, and memory costs for different pipeline configurations, and a pipeline
  generator that iteratively tunes the three phases of pipeline parallelism to reduce
  bottlenecks.
---

# AdaPtis: Reducing Pipeline Bubbles with Adaptive Pipeline Parallelism on Heterogeneous Models

## Quick Facts
- arXiv ID: 2509.23722
- Source URL: https://arxiv.org/abs/2509.23722
- Reference count: 40
- Key outcome: Achieves 1.42× average speedup (up to 2.14×) over Megatron-LM I-1F1B on heterogeneous LLM architectures

## Executive Summary
AdaPtis addresses the challenge of training large language models with heterogeneous architectures by co-optimizing model partition, placement, and workload scheduling in pipeline parallelism. Unlike prior approaches that optimize only one phase while fixing others, AdaPtis uses a performance model to iteratively tune all three phases simultaneously, reducing pipeline bubbles that waste GPU resources. The system demonstrates significant speedups across various LLM architectures including Gemma, DeepSeek, and Nemotron-H models, showing particular effectiveness when dealing with heterogeneous layer compositions.

## Method Summary
AdaPtis introduces a three-phase co-optimization approach for pipeline parallelism that addresses the inefficiencies of heterogeneous models. The system employs a pipeline performance model to estimate computation, communication, and memory costs across different configurations, then uses a pipeline generator to iteratively tune model partition, placement, and scheduling policies. A pipeline executor translates scheduling policies into abstract instructions (Compute, Send, Receive, Wait) and reorders them to maximize communication overlap while avoiding deadlock. The iterative bottleneck-guided tuning identifies the worst-performing phase and adjusts it accordingly, avoiding the combinatorial explosion of searching the entire configuration space.

## Key Results
- Achieves 1.42× average speedup (up to 2.14×) over Megatron-LM I-1F1B across various LLM architectures
- Maintains high efficiency across different sequence lengths
- Scales well with increasing GPU counts
- Demonstrates effectiveness on heterogeneous models including Gemma, DeepSeek, and Nemotron-H architectures

## Why This Works (Mechanism)

### Mechanism 1: Co-optimization of Pipeline Phases
AdaPtis jointly optimizes model partition, placement, and scheduling rather than treating them as independent variables. This approach addresses the fundamental limitation of prior methods that fixed two phases while tuning only one. By iteratively adjusting all three phases based on identified bottlenecks, the system can achieve better overall balance. The core assumption is that the efficiency gain from balancing specific bottlenecks outweighs the overhead of iterative search.

### Mechanism 2: Bottleneck-Guided Iterative Tuning
The pipeline generator uses performance modeling to simulate and target the specific worst-performing phase, avoiding brute-force search of the exponentially large configuration space. Instead of using ILP solvers, it estimates BubbleTime for all devices and focuses tuning efforts on the identified bottleneck. This greedy approach targets immediate performance issues, assuming the performance model's accuracy (<5% error) is sufficient for decision-making.

### Mechanism 3: Unified Instruction Abstraction for Overlap
The pipeline executor decouples execution into abstract instructions (Compute, Send, Receive, Wait) to enable static reordering for communication overlap. By traversing the instruction list and moving Receive instructions earlier, the system can hide communication latency behind independent computations. Crucially, data dependency checks prevent deadlock while enabling these optimizations.

## Foundational Learning

**Concept: Pipeline Parallelism Phases**
- Why needed: AdaPtis is a meta-optimizer over these three phases; understanding them is essential to grasp the co-optimization approach
- Quick check: In a 4-stage pipeline, does "Model Placement" determine the order of layers or the specific GPUs those layers reside on?

**Concept: Pipeline Bubbles (Idle Time)**
- Why needed: The entire paper focuses on minimizing "bubble ratio"; understanding bubbles as inherent overhead from data dependencies is crucial
- Quick check: Why does the "1F1B" schedule still produce bubbles at the start and end of a step?

**Concept: Model Heterogeneity**
- Why needed: The paper's motivation stems from the shift from homogeneous (LLaMA) to heterogeneous models (DeepSeek, Nemotron)
- Quick check: Why would equal layer splitting cause bubbles in a model with heavy MoE blocks followed by light attention layers?

## Architecture Onboarding

**Component map**: Profiling Layer Costs (Offline) -> Performance Model Simulation -> Generator Tuning Loop -> Instruction List Generation -> GPU Execution

**Critical path**: The system profiles layer costs offline, simulates performance to guide the generator, tunes the configuration through an iterative loop, generates abstract instructions, and executes on GPUs.

**Design tradeoffs**: Simulation vs. execution relies on performance model accuracy rather than costly dry runs; heuristic vs. optimal uses iterative tuning instead of ILP solvers for reasonable generation time (<100s).

**Failure signatures**: OOM occurs when scheduling packs too many forward passes before backward passes; deadlock happens when instruction reordering creates cyclic dependencies; regression occurs when profiling data is stale.

**First 3 experiments**: 
1. Run Performance Model on GPT-3 style model and compare predicted vs. real throughput (error <5%)
2. Profile Executor overhead to ensure instruction reordering time is negligible compared to training step
3. Test on synthetic heterogeneous model with x10 compute on layer 16 to verify partition rebalancing

## Open Questions the Paper Calls Out

**Open Question 1**: How can activation recomputation be integrated into AdaPtis's co-optimization process to further alleviate memory pressure without negating throughput gains? [explicit] Section 5.1 states recomputation is orthogonal and left for future work.

**Open Question 2**: To what extent does the iterative bottleneck-tuning heuristic guarantee near-optimal solutions compared to global optimum in the exponentially large search space? [inferred] Section 4.3 acknowledges the search space is exponentially large and uses heuristics.

**Open Question 3**: Can AdaPtis maintain efficiency when applied to workloads with significant data heterogeneity, such as variable sequence lengths within a single training run? [inferred] The system relies on static profiled costs that may not adapt to varying execution times.

## Limitations

- Performance model accuracy assumptions may degrade under variable network conditions or power constraints
- Iterative tuning lacks theoretical convergence guarantees and may get stuck in local optima
- Evaluation focuses on model architecture heterogeneity but doesn't thoroughly explore data heterogeneity or hardware heterogeneity scenarios

## Confidence

**High Confidence**: Co-optimization mechanism is technically sound with demonstrated 1.42× speedup; performance model shows low prediction error (2.12%)

**Medium Confidence**: Scaling claims rely on limited experiments (up to 32 GPUs); real-world scaling to hundreds of GPUs may reveal communication bottlenecks

**Low Confidence**: Claim that "no existing method can solve heterogeneous model training" overstates the case and dismisses potential hybrid approaches

## Next Checks

1. **Network Contention Robustness**: Run AdaPtis under controlled network degradation (10-50ms latency spikes) and measure how quickly performance model predictions diverge from reality

2. **Multi-Objective Stress Test**: Create synthetic heterogeneous model with 10+ distinct layer types and evaluate whether iterative tuning converges within 100-second limit

3. **Hardware Heterogeneity Validation**: Deploy across mixed GPU generations (V100 and A100) and verify placement optimization accounts for varying compute capabilities