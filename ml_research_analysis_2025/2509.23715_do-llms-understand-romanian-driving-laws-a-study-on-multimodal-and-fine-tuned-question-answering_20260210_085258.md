---
ver: rpa2
title: Do LLMs Understand Romanian Driving Laws? A Study on Multimodal and Fine-Tuned
  Question Answering
arxiv_id: '2509.23715'
source_url: https://arxiv.org/abs/2509.23715
tags:
- llama
- performance
- multimodal
- romanian
- fine-tuning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper evaluates Large Language Models on Romanian driving-law
  question answering, introducing a 1,208-question dataset with 387 multimodal items.
  SOTA models achieve up to 77.69% accuracy, but fine-tuned 8B models (Llama 3.1-8B,
  RoLlama 3.1-8B) reach 58.68% and 61.98% respectively, outperforming larger general
  models.
---

# Do LLMs Understand Romanian Driving Laws? A Study on Multimodal and Fine-Tuned Question Answering

## Quick Facts
- **arXiv ID:** 2509.23715
- **Source URL:** https://arxiv.org/abs/2509.23715
- **Reference count:** 6
- **Primary result:** Fine-tuned 8B models achieve 58.68-61.98% accuracy, outperforming larger general models on Romanian driving-law QA.

## Executive Summary
This paper evaluates Large Language Models on Romanian driving-law question answering, introducing a 1,208-question dataset with 387 multimodal items. SOTA models achieve up to 77.69% accuracy, but fine-tuned 8B models (Llama 3.1-8B, RoLlama 3.1-8B) reach 58.68% and 61.98% respectively, outperforming larger general models. Textual descriptions of images yield significantly higher accuracy than direct visual input (e.g., 77.69% vs 70.66% for Gemini 2.5 Flash). An LLM-as-a-Judge scores explanation quality, revealing self-preference bias. Results highlight that domain-specific fine-tuning effectively adapts smaller models for specialized tasks in low-resource languages.

## Method Summary
The study introduces MedQARo, a dataset of 1,208 Romanian driving-law questions (387 multimodal with images, 821 text-only). Models are evaluated on accuracy and explanation quality. Fine-tuning uses Llama 3.1-8B-Instruct and RoLlama 3.1-8B-Instruct on text-only training data with learning rate 2e-4, 200 steps, and gradient accumulation of 4. SOTA models are evaluated via APIs. Text descriptions of images are created manually and outperform direct visual input by 7-15%. LLM-as-a-Judge (Gemini 2.5 Flash) reveals self-preference bias when evaluating explanation quality.

## Key Results
- Fine-tuned 8B models (Llama 3.1-8B: 58.68%, RoLlama 3.1-8B: 61.98%) outperform larger general models on Romanian driving-law QA
- Textual descriptions of images yield 77.69% accuracy vs 70.66% for direct visual input (Gemini 2.5 Flash)
- RoLlama 3.1-8B's Romanian pre-training provides advantage (starting at 54.13%) but both models benefit similarly from fine-tuning
- LLM-as-a-Judge shows self-preference bias, with models rating their own explanations higher

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Domain-specific fine-tuning can elevate smaller, language-adapted models to compete with larger general-purpose models in specialized, low-resource language tasks.
- Mechanism: Fine-tuning on a curated domain dataset (1,208 Romanian driving-law questions) enables parameter updates that align model behavior with domain-specific terminology, legal reasoning patterns, and language-specific nuances, compensating for smaller model capacity.
- Core assumption: The training dataset quality and domain relevance are sufficiently high to produce meaningful adaptation without overfitting; the base model has adequate multilingual foundation.
- Evidence anchors:
  - [abstract] "fine-tuned 8B models (Llama 3.1-8B, RoLlama 3.1-8B) reach 58.68% and 61.98% respectively, outperforming larger general models"
  - [section 3.2] "RoLlama 3.1, which already benefited from its Romanian pre-training (starting at 54.13%), improved further to a final accuracy of 61.98%... surpassed the out-of-the-box performance of the much larger DeepSeekV3"
  - [corpus] MedQARo and GRILE benchmarks similarly demonstrate that Romanian-language evaluation requires specialized datasets, confirming the domain-gap hypothesis for low-resource languages
- Break condition: If the domain dataset is too small, noisy, or if the base model lacks sufficient multilingual pre-training, fine-tuning may yield minimal gains or degrade performance through overfitting.

### Mechanism 2
- Claim: High-quality textual descriptions of visual content currently outperform direct image input for multimodal QA on this task.
- Mechanism: Manually crafted text descriptions provide explicit, denoised semantic representations of visual information (road signs, vehicle positions, markings), bypassing challenges in visual perception such as low-resolution artifacts, compression noise, and nuanced scene interpretation that current vision encoders struggle with.
- Core assumption: Text descriptions are accurate, comprehensive, and do not inadvertently reveal answers; the text encoder is stronger than the vision encoder for this domain.
- Evidence anchors:
  - [abstract] "Textual descriptions of images yield significantly higher accuracy than direct visual input (e.g., 77.69% vs 70.66% for Gemini 2.5 Flash)"
  - [section 3.3] "all models performed significantly better when provided with textual descriptions. The performance drop when using direct images was substantial, ranging from 7% for Gemini to a stark 15% for Llama 4 Scout... We attribute this gap to the low resolution and compression artifacts"
  - [corpus] Related work on Romanian VLMs (arXiv:2512.14926) addresses multimodal instruction tuning, but corpus evidence on image-vs-text comparison for driving domains specifically is limited
- Break condition: If source images are high-resolution with clear visual semantics, or if vision models significantly improve, direct visual input may match or exceed text descriptions; also breaks if descriptions are poorly crafted.

### Mechanism 3
- Claim: LLM-as-a-Judge evaluation exhibits self-preference bias, where models rate their own outputs more favorably.
- Mechanism: When a model (e.g., Gemini 2.5 Flash) is used to score explanation quality, implicit stylistic alignment between the judge's internal representations and its own generated outputs leads to inflated scores, independent of actual quality differences.
- Core assumption: The judge model has consistent internal scoring criteria; stylistic similarity correlates with perceived quality in the judge's evaluation.
- Evidence anchors:
  - [abstract] "An LLM-as-a-Judge scores explanation quality, revealing self-preference bias"
  - [section 4] "Gemini consistently scored its own answers higher than those of other models, even when accuracies were comparable... This serves as a cautionary tale: automated evaluation metrics, even sophisticated ones, should be used with care and ideally validated with human expert judgment"
  - [corpus] Corpus evidence on self-preference bias in LLM-as-Judge is not directly addressed in neighbor papers; related Romanian benchmarks focus on task performance, not evaluation methodology
- Break condition: If judge models are specifically trained or prompted to be impartial, or if evaluation uses cross-model judging (Model A judges Model B), self-preference bias may be reduced.

## Foundational Learning

- Concept: **Supervised Fine-Tuning (SFT) on domain data**
  - Why needed here: The paper demonstrates that even modest-sized models (8B parameters) can achieve competitive performance through domain-specific fine-tuning, but practitioners must understand how to set up SFT pipelines, select hyperparameters (learning rate 2e-4, gradient accumulation), and avoid overfitting on small datasets.
  - Quick check question: Can you explain why a learning rate of 2e-4 with gradient accumulation steps of 4 was chosen, and what tradeoffs this represents for small dataset fine-tuning?

- Concept: **Multimodal vs text-only processing tradeoffs**
  - Why needed here: The paper reveals a counterintuitive finding that text descriptions outperform direct image input, requiring practitioners to understand when to invest in vision encoders versus high-quality text preprocessing.
  - Quick check question: Given that text descriptions outperformed images by 7-15% in this study, what factors would determine whether to use direct vision input versus description-based approaches for a new multimodal QA task?

- Concept: **LLM-as-a-Judge evaluation and its limitations**
  - Why needed here: The paper uses automated quality scoring but identifies self-preference bias, which is critical for practitioners designing evaluation pipelines to understand and mitigate.
  - Quick check question: If you observe that a judge model consistently rates its own outputs higher, what alternative evaluation strategies could you employ to reduce this bias?

## Architecture Onboarding

- Component map:
  Dataset layer -> JSONL format with fields (question, answers, image_description, correct_answers, explanation); split into 821 text-only + 387 multimodal items
  Text-only pipeline -> Manual description creation -> JSONL processing -> model inference
  Multimodal pipeline -> Original images -> vision-language model inference
  Fine-tuning layer -> Hugging Face Transformers + Datasets libraries; Llama 3.1-8B-Instruct and RoLlama 3.1-8B-Instruct as base models
  Evaluation layer -> Accuracy metric (exact match on answer letters) + LLM-as-a-Judge (Gemini 2.5 Flash scoring 0-5 on explanation quality)
  Infrastructure -> Google Colab with NVIDIA L4 GPU (24GB VRAM) for fine-tuning; API-based inference for SOTA models

- Critical path:
  1. Data extraction (Selenium scraping from scoalarutiera.ro)
  2. Manual image description creation (for text-only processing)
  3. Dataset formatting to JSONL
  4. Model selection (text-only vs multimodal; SOTA vs fine-tuned)
  5. Fine-tuning (for 8B models) with specified hyperparameters
  6. Inference on test set
  7. Accuracy evaluation + LLM-as-a-Judge scoring
  8. Statistical significance testing (McNemar's test)

- Design tradeoffs:
  - **Text descriptions vs images**: Higher accuracy with descriptions (7-15% improvement), but requires manual annotation effort; images are noisier but scalable
  - **Fine-tuned 8B vs SOTA large models**: Fine-tuned models competitive (61.98% vs 77.69%) at ~$10 vs ~$40 inference cost, but require training infrastructure
  - **Romanian-specific vs general base model**: RoLlama (Romanian pre-trained) starts higher (54.13% vs 41.32%) but both benefit similarly from fine-tuning
  - **Automated vs human evaluation**: LLM-as-a-Judge is efficient but shows self-preference bias; human evaluation is gold standard but costly

- Failure signatures:
  - **Overfitting on small dataset**: Fine-tuning with too many steps or high learning rate could degrade generalization (monitor validation accuracy)
  - **Poor image descriptions**: If descriptions are inaccurate or reveal answers, text-only results become invalid
  - **Self-preference bias in evaluation**: If using same model as judge and generator, scores may be inflated
  - **Vision encoder failures on low-quality images**: Direct image input underperforms due to compression artifacts and resolution issues

- First 3 experiments:
  1. **Baseline SOTA comparison**: Run Gemini 2.5 Flash, Qwen3, Llama 4 Scout on text-only test set to establish performance hierarchy; verify accuracy correlation with paper results (target: ~77%, ~73%, ~71% respectively)
  2. **Fine-tuning replication**: Fine-tune Llama 3.1-8B-Instruct on training split with paper's hyperparameters (lr=2e-4, 200 steps, grad_accum=4); measure accuracy improvement from ~41% baseline to ~58-60%
  3. **Text vs image ablation**: For multimodal subset (387 items), compare Gemini 2.5 Flash performance with text descriptions vs direct images; expect ~7% degradation with images (70.66% vs 77.69%)

## Open Questions the Paper Calls Out
- Can generative models effectively enhance or standardize low-quality traffic images to close the performance gap between direct visual input and text descriptions?
- Does the observed performance gap between text descriptions and direct images stem primarily from data quality issues or fundamental limitations in multimodal reasoning architectures?
- How can automated evaluation frameworks mitigate self-preference bias when the highest-performing model is used as the judge?

## Limitations
- Dataset partitioning details (train/validation/test splits and stratification) are not fully specified, making exact reproduction challenging
- LLM-as-a-Judge evaluation methodology lacks detailed prompt templates and scoring rubrics
- Study focuses exclusively on Romanian driving laws, limiting generalizability to other domains or languages

## Confidence
- **High confidence:** Fine-tuned 8B models (Llama 3.1-8B, RoLlama 3.1-8B) outperform larger general models on Romanian driving-law QA
- **Medium confidence:** Text descriptions outperform direct image input (77.69% vs 70.66% for Gemini 2.5 Flash), though sensitive to image quality and description quality
- **Medium confidence:** Self-preference bias in LLM-as-a-Judge evaluation is observed but magnitude and mitigation strategies require further validation

## Next Checks
1. Replicate the fine-tuning experiment with Llama 3.1-8B-Instruct on the text-only training split using the specified hyperparameters (lr=2e-4, 200 steps, grad_accum=4) and verify accuracy improvement from baseline (~41%) to target range (58-61%)
2. Conduct a controlled ablation study comparing text descriptions versus direct image input on the multimodal subset (387 items) using multiple models to confirm the 7-15% performance gap reported for Gemini 2.5 Flash
3. Test cross-model evaluation by having different models judge each other's explanations to quantify self-preference bias magnitude and explore mitigation strategies like model-swapping or human validation