---
ver: rpa2
title: Do Attention Heads Compete or Cooperate during Counting?
arxiv_id: '2502.06923'
source_url: https://arxiv.org/abs/2502.06923
tags:
- heads
- head
- attention
- token
- tokens
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper investigates how attention heads in a transformer model
  cooperate or compete while solving a simple counting task: determining whether a
  string has more ''1''s than ''0''s (with ''2''s as noise). The authors train small
  attention-only transformer models on this task and analyze the behavior of individual
  attention heads.'
---

# Do Attention Heads Compete or Cooperate during Counting?

## Quick Facts
- arXiv ID: 2502.06923
- Source URL: https://arxiv.org/abs/2502.06923
- Reference count: 40
- Key outcome: Attention heads in small transformers solving a counting task behave as pseudo-ensembles, producing linearly separable representations that the output layer must non-uniformly aggregate to satisfy both semantic and syntactic constraints.

## Executive Summary
This paper investigates how attention heads in a transformer model cooperate or compete while solving a simple counting task: determining whether a string has more '1's than '0's (with '2's as noise). The authors train small attention-only transformer models on this task and analyze the behavior of individual attention heads. The key finding is that attention heads behave as a pseudo-ensemble - they individually learn to solve the counting task by attending to '0' and '1' tokens with roughly equal weights while largely ignoring '2' tokens. However, when these heads are aggregated by the output layer, they require non-uniform weighting to properly solve both the counting task and the auxiliary task of ending the sentence with an '[EOS]' token.

## Method Summary
The authors create a synthetic "Count01" language task where models must determine if a string contains more '1's than '0's (with '2's as noise). They train single-layer attention-only transformers with varying configurations (d=1-32 embedding dimension, 1-16 heads) using cross-entropy loss on tokens after the '=' delimiter. The analysis focuses on the attention patterns at the '=' position, measuring separation accuracy (s-acc), learned accuracy (l-acc), and ROC AUC to characterize head performance. The primary configuration uses d=32, 16 heads with head dimension d'=2.

## Key Results
- Attention heads individually learn to solve the counting task by attending roughly equally to '0' and '1' tokens (w01 ≈ 1) while suppressing attention to noise tokens '2' (w02 > 10)
- About half of the heads are highly successful at linearly separating inputs (high s-acc), but the model doesn't fully exploit these successful heads individually
- Model accuracy strongly correlates with a weighted sum of head separation accuracies (Pearson correlation r = 0.72), confirming ensemble-like behavior
- The output layer must non-uniformly aggregate head contributions to simultaneously satisfy counting and [EOS] prediction tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Individual attention heads learn to produce linearly separable representations for the counting task, functioning as a pseudo-ensemble.
- Mechanism: Each head independently learns to attend roughly equally to '0' and '1' tokens while suppressing attention to noise tokens ('2'), producing outputs where the difference in class (more 1s vs. more 0s) becomes linearly separable.
- Core assumption: The output layer can recover the signal from any successful head via a learned linear transformation.
- Evidence anchors:
  - [abstract] "attention heads behave as a pseudo-ensemble, all solving the same subtask"
  - [Section 5.3] "Successful heads have w01 ≈ 1, i.e., the head attends roughly equally to the '0' and '1' tokens" and "Successful heads have w02 > 10"
  - [corpus] Related work on counting tasks (e.g., "Mechanistic Interpretability of Large-Scale Counting in LLMs") confirms architectural limitations in transformer counting, but does not directly validate the pseudo-ensemble hypothesis.
- Break condition: If attention heads were to specialize (e.g., one counting '0's, another '1's), the pseudo-ensemble characterization would not hold.

### Mechanism 2
- Claim: The output layer aggregates head contributions non-uniformly to simultaneously satisfy semantic (counting) and syntactic ([EOS] prediction) constraints.
- Mechanism: Heads are biased toward either '4' or '5' logits. The output layer learns weights that balance these biases to correctly predict the answer token while also ensuring the '[EOS]' token receives the highest logit after the answer token is emitted.
- Core assumption: The bias in individual head logit contributions can be mathematically reconciled across heads to serve both tasks.
- Evidence anchors:
  - [abstract] "their outputs need to be aggregated in a non-uniform manner in order to create an encoding that conforms to the syntax"
  - [Section 6.2] "heads are typically biased toward one out of '4' or '5'" and "the ordering of the logits is about fixed, but the balance between the logits tips toward '[EOS]' when we move from '=' to '4' or '5'"
  - [corpus] No direct corpus evidence on this specific aggregation mechanism for dual-task constraints.
- Break condition: If a single head could achieve both tasks independently with uniform weighting, non-uniform aggregation would not be necessary.

### Mechanism 3
- Claim: Model accuracy can be predicted from a weighted sum of individual head separation accuracies, confirming ensemble-like behavior.
- Mechanism: The final model's separation accuracy correlates (r = 0.72) with the output-layer-weighted average of individual head separation accuracies, suggesting the model's effective computation is a linear combination of quasi-independent head computations.
- Core assumption: Head outputs combine approximately additively at the logit level.
- Evidence anchors:
  - [Section 6.1] "Pearson correlation coefficient of the two values is 0.72" and "the model is working as a proper ensemble, solving two separate tasks"
  - [Figure 10] Scatter plot showing correlation between weighted head s-acc and model accuracy.
  - [corpus] Weak/no direct corpus validation of this specific correlation metric.
- Break condition: If strong nonlinear interactions between heads determined performance, the weighted-sum correlation would break down.

## Foundational Learning

- Concept: **Attention head as convex combination operator**
  - Why needed here: Understanding that head output = Σ(attention_weight_i × value_vector_i), where weights sum to 1, explains why attention ratios (not absolute values) determine behavior.
  - Quick check question: Can you explain why w01 ≈ 1 implies roughly equal contribution from all '0' and '1' tokens regardless of their counts?

- Concept: **Linear separability in representation space**
  - Why needed here: The s-acc metric assumes that if head outputs for two classes can be separated by a hyperplane, a downstream classifier can solve the task.
  - Quick check question: If head outputs for class '4' and class '5' samples form overlapping clusters, what does that imply about s-acc?

- Concept: **Logit difference as decision boundary**
  - Why needed here: The model decides '4' vs. '5' based on sign(z4 - z5), where z4 and z5 are logits computed from head outputs.
  - Quick check question: If z4 - z5 = Σ O_i^T(w_i,5 - w_i,4), what happens if all heads have identical w_i,5 - w_i,4 vectors?

## Architecture Onboarding

- Component map:
  Token embeddings (8 tokens) -> Attention layer (16 heads, d=32) -> Head outputs -> Output layer (8 logits) -> Prediction

- Critical path:
  1. At '=' token position, attention weights to '0', '1', '2' tokens determine head output
  2. Head outputs are concatenated and projected to logits via output layer weights
  3. Logit difference (z4 - z5) determines answer; max logit at subsequent position determines '[EOS]'

- Design tradeoffs:
  - Minimal solution (d=1, d'=1, 1 head) exists but is difficult to reach via gradient descent from random initialization
  - Larger models (d=32, 16 heads) train reliably but exhibit redundancy (~50% of heads are "failed")
  - Heads trained jointly perform slightly worse than heads trained in isolation (Table 1), suggesting no positive training-time cooperation

- Failure signatures:
  - **w01 too extreme**: If attention strongly favors '0' or '1', head outputs collapse to a point (no separation)
  - **w02 too small**: If noise token '2' receives significant attention, it introduces variance and degrades separability
  - **High s-acc but low l-acc**: A head produces separable outputs but the output layer has not learned to exploit it (due to bias misalignment with syntactic task)

- First 3 experiments:
  1. **Replicate the s-acc vs. (w01, w02) analysis**: Train a 16-head model, extract attention ratios for each head, and plot s-acc as a function of these ratios. Verify the clustering of successful heads near w01 ≈ 1, w02 > 10.
  2. **Intervention study on attention matrix**: Manually set attention to '2' tokens to zero and sweep w01 from 0.001 to 1000. Confirm that s-acc remains high for 0.1 < w01 < 10, as shown in Figure 8.
  3. **Correlation verification**: Compute weighted head s-acc (using output layer weights) vs. model accuracy across training checkpoints. Confirm Pearson correlation ≈ 0.72, supporting the ensemble hypothesis.

## Open Questions the Paper Calls Out
None

## Limitations
- The synthetic counting task may not generalize to more complex, natural language scenarios where counting interacts with semantic context
- Analysis focuses on a single attention layer without exploring multi-layer or residual architectures where interlayer interactions might fundamentally change cooperation/competition dynamics
- While correlation between weighted head separation accuracies and model performance is demonstrated (r=0.72), causation is not established - ensemble behavior could be an artifact of specific architecture and task design

## Confidence
- **High confidence**: Attention heads in tested architecture produce linearly separable representations for counting task when w01 ≈ 1 and w02 > 10
- **Medium confidence**: Pseudo-ensemble characterization, assuming heads are truly independent learners rather than having implicit coordination
- **Low confidence**: Generality of non-uniform aggregation mechanism for dual-task constraints based on single synthetic task

## Next Checks
1. **Cross-task validation**: Apply the same analysis framework to a more complex counting task embedded in natural language to test whether pseudo-ensemble behavior persists under realistic conditions
2. **Intervention study on aggregation**: Use gradient-based attribution methods to identify which heads contribute most to final logits for different token types, then systematically disable or amplify specific heads to verify non-uniform aggregation
3. **Multi-layer extension**: Train a two-layer attention-only transformer on the same counting task and analyze whether same patterns of head cooperation/competition emerge at different depths, or whether interlayer interactions create different dynamics