---
ver: rpa2
title: Active learning from positive and unlabeled examples
arxiv_id: '2602.02081'
source_url: https://arxiv.org/abs/2602.02081
tags:
- learning
- active
- positive
- algorithm
- probability
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper studies active learning from positive and unlabeled\
  \ data (PU learning), where a learner can query labels from an unlabeled pool but\
  \ only receives positive labels with probability \u03C9. The key contribution is\
  \ the first theoretical analysis of label complexity for this setting."
---

# Active learning from positive and unlabeled examples

## Quick Facts
- arXiv ID: 2602.02081
- Source URL: https://arxiv.org/abs/2602.02081
- Reference count: 26
- Primary result: First theoretical analysis of label complexity for active learning from PU data with positive label rate ω

## Executive Summary
This paper studies active learning from positive and unlabeled data where queries only receive positive labels when the instance is actually positive and an independent coin flip succeeds with probability ω. The authors provide the first theoretical analysis of label complexity for this setting, showing that the label complexity differs from classical active learning only by a factor of θ/ω (ignoring class prior terms), which is inherent to the PU setting. Two algorithms are proposed: one assuming known class prior π_D and another estimating it via binary search.

## Method Summary
The paper proposes two algorithms for active learning from PU data. For known π_D, Algorithm 1 applies the classical CAL algorithm to a restricted hypothesis class where the predicted positive rate matches π_D. For unknown π_D, Algorithm 3 first estimates the positive label rate ω using Algorithm 2 (a binary search procedure), then uses a similar binary search to estimate π_D while interleaving version space pruning. The key insight is that querying in the disagreement region yields positive labels at rate ω times the probability of a positive instance being in that region.

## Key Results
- Label complexity bound: O(ln(1/ε)θ²[d ln(θ) + ln ln(1/ε) + ln(1/δ)]/ω + θ[d ln(θ/π_D) + ln(1/δ)]/(π_D²ω))
- The bound differs from classical active learning only by factor θ/ω (ignoring π_D-dependent term)
- Effective label acquisition rate proportional to ω due to PU setting constraints
- Two algorithms provided: one for known π_D (CAL-based) and one for unknown π_D (binary search estimation)

## Why This Works (Mechanism)
The mechanism works by exploiting the structure of PU learning where positive labels are only received with probability ω when querying positive instances. By focusing queries in the disagreement region (where version space members disagree), the algorithm maximizes information gain per label. The binary search procedure for estimating π_D iteratively narrows the range by observing whether the positive label rate in the disagreement region is above or below the expected rate ω·π_D.

## Foundational Learning
- **Disagreement coefficient θ**: Measures the rate at which the disagreement region expands; needed for sample complexity bounds and query selection; quick check: verify θ is finite for the given hypothesis class and distribution
- **SCAR assumption**: Selective Classification with Arbitrary Rejection; ensures labels are only revealed when query is positive and coin flip succeeds; quick check: confirm oracle follows SCAR protocol
- **Version space V_i**: Maintains hypotheses consistent with observed data; needed to track uncertainty and guide queries; quick check: ensure V_i is updated correctly after each observation
- **Passive PU learning**: Used as subroutine for version space pruning when no positive labels are received; quick check: verify passive PU subroutine returns consistent hypotheses
- **VC dimension d**: Controls hypothesis class complexity; needed for generalization bounds; quick check: confirm d is correctly computed for the chosen hypothesis class
- **Class prior π_D**: Probability of positive instances; crucial for π_D-dependent terms in bounds; quick check: estimate π_D from data to verify algorithm behavior

## Architecture Onboarding

**Component map**: EstRate -> Binary search π_D estimation -> CAL-style disagreement querying

**Critical path**: Sample unlabeled instance → Query oracle → Receive positive label with probability ω → Update version space using passive PU → Select next query from disagreement region

**Design tradeoffs**: Known π_D vs unknown π_D (binary search adds overhead but necessary in practice); aggressive vs conservative π_D bounds (affects convergence speed vs risk of empty version space)

**Failure signatures**: 
- Version space becomes empty (realizability violated or bounds too tight)
- Binary search oscillates (feedback rate near threshold)
- Slow convergence (θ underestimated or insufficient sampling)

**First experiments**:
1. Test EstRate on synthetic data with known ω to verify convergence
2. Validate Algorithm 1 with known π_D on linearly separable data
3. Run Algorithm 3 on synthetic data with known θ, π_D, ω to verify label complexity bound empirically

## Open Questions the Paper Calls Out
None

## Limitations
- Dependence on disagreement coefficient θ and class prior π_D, which are typically unknown in practice
- Binary search procedure may be sensitive to unspecified constants M_1, M_2, M_3
- Theoretical guarantees assume realizable setting and continuous distributions
- Label complexity includes 1/(π_D²ω) term that can become prohibitive for small class priors or low positive label rates

## Confidence
- **High confidence** in theoretical framework and main theorem statement
- **Medium confidence** in algorithmic implementation details due to unspecified constants
- **Low confidence** in practical performance without empirical validation

## Next Checks
1. Implement the binary search procedure for π_D estimation with concrete values for M_1, M_2, M_3 to assess sensitivity
2. Test the algorithm on synthetic data with known θ and π_D to verify the label complexity bound empirically
3. Evaluate performance degradation when the realizability assumption is violated by introducing label noise