---
ver: rpa2
title: 'QM-ToT: A Medical Tree of Thoughts Reasoning Framework for Quantized Model'
arxiv_id: '2504.12334'
source_url: https://arxiv.org/abs/2504.12334
tags:
- reasoning
- medical
- arxiv
- performance
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ''
---

# QM-ToT: A Medical Tree of Thoughts Reasoning Framework for Quantized Model

## Quick Facts
- **arXiv ID:** 2504.12334
- **Source URL:** https://arxiv.org/abs/2504.12334
- **Reference count:** 40
- **Primary result:** Achieves 10-15% accuracy improvement on MedQA-USMLE for INT4 quantized models vs standard Chain-of-Thought

## Executive Summary
QM-ToT introduces a Tree of Thoughts (ToT) reasoning framework specifically designed to recover performance lost during INT4 quantization of large language models for medical applications. The approach decomposes complex medical reasoning into hierarchical subtasks, leveraging a dual-evaluator system (Reasoning + Correctness) to select optimal paths, and employs Reflection-ToT distillation to create efficient training data. The framework demonstrates substantial improvements over standard Chain-of-Thought methods while using only 3.9% of the data required by traditional distillation approaches.

## Method Summary
QM-ToT generates reasoning paths via a decomposed Tree of Thoughts approach where the quantized model (e.g., LLaMA-INT4) expands questions into hierarchical reasoning chains. Each path is evaluated by DeepSeek-V3 scoring reasoning coherence (r) and medical correctness (c), with final scores computed as fs = α·exp(r) + (1-α)·exp(c), α=0.6. The framework uses DPO distillation with LLaMA3-70b teacher, LR=5e-5, batch_size=16, cutoff=3k, 1 epoch. Reflection-ToT generates training pairs by contrasting correct and incorrect reasoning paths, converting them into o1-style chains.

## Key Results
- 10-15% accuracy improvement on MedQA-USMLE for INT4 quantized models vs standard Chain-of-Thought
- 86.27% improvement using only 3.9% of data compared to traditional distillation methods
- Effective performance recovery for INT4 models while maintaining computational efficiency

## Why This Works (Mechanism)

### Mechanism 1
Decomposing medical reasoning into discrete, hierarchical paths mitigates precision loss typically associated with INT4 quantization. Quantization degrades a model's ability to maintain complex global reasoning contexts, but by forcing the model to solve problems as smaller subtasks rather than a single linear chain, cognitive load per inference step is reduced. The model only needs to maintain local coherence between parent and child nodes, which is more robust to noise introduced by lower-bit weights. Core assumption: the underlying model retains sufficient parametric knowledge to solve sub-problems even if its ability to synthesize long, multi-step solutions is impaired by quantization.

### Mechanism 2
The dual-evaluator system (Reasoning + Correctness) corrects for hallucinations or logical shortcuts prevalent in quantized models. Standard self-consistency relies on frequency, assuming the majority answer is correct. QM-ToT replaces this with an external "LLM-as-a-Judge" (DeepSeek-V3) that scores paths on logical validity and medical factuality. By using a weighted exponential score, the system filters out paths that reach the right answer through wrong logic or medically invalid steps. Core assumption: the evaluator model possesses superior reasoning capability compared to the quantized model being evaluated.

### Mechanism 3
Reflection-ToT distillation generates high-quality training data by structuring "wins" and "losses" naturally. Standard distillation uses raw outputs, but Reflection-ToT uses the tree search process to generate pairs of reasoning chains - one leading to a correct answer and one to an incorrect answer. By pairing these and using a strong model to "reflect" them into long-form o1-style chains, the framework creates rich Direct Preference Optimization data. This teaches the student model what to avoid (the incorrect path) explicitly. Core assumption: the divergence in a ToT branch contains higher informational value than a single correct solution.

## Foundational Learning

- **Model Quantization (INT4 vs FP16)**: The entire premise is recovering performance lost when converting models from 16-bit floating point to 4-bit integer representations for local deployment. Why needed: to understand why quantization causes performance degradation. Quick check: Can you explain why reducing parameter precision from FP16 to INT4 might cause a model to "forget" complex medical associations while retaining basic language skills?

- **Tree of Thoughts (ToT) vs. Chain of Thought (CoT)**: The paper contrasts linear reasoning (CoT) with branching exploration (ToT). Understanding that ToT allows for backtracking and exploring multiple hypotheses simultaneously is crucial. Why needed: to grasp the fundamental architectural difference. Quick check: How does ToT differ from CoT in terms of error recovery? (Hint: CoT propagates errors linearly; ToT allows pruning of bad branches).

- **LLM-as-a-Judge**: The "Evaluator" module is not a human but a stronger LLM (DeepSeek-V3). You must understand the paradigm of using one AI to grade another's reasoning. Why needed: to comprehend the evaluation architecture. Quick check: What are the risks of using an AI judge that is only slightly better than the model it is evaluating?

## Architecture Onboarding

- **Component map:** Input (Medical Question Q) -> Generator (Quantized model) -> Tree of Reasoning Paths S -> Evaluator (DeepSeek-V3) -> Dual scores (r, c) -> Aggregator -> Final Answer

- **Critical path:** The interaction between the Generator and the Evaluator. If the Generator produces low-diversity paths, the Evaluator has nothing to discriminate. If the Evaluator is misaligned, it selects the wrong path.

- **Design tradeoffs:**
  - Latency vs. Accuracy: Generating a tree of depth k with breadth b requires O(b^k) inferences, significantly slower than a single CoT
  - Evaluator Size: Performance drops when evaluator isn't significantly stronger than target model (e.g., LLaMA3-70b case)
  - Complexity vs. Benefit: ToT can degrade easy-question accuracy due to over-thinking

- **Failure signatures:**
  - "Oscillation" Effect: If "Average Max" and "Score Max" disagree frequently, indicating high-variance paths or confused evaluator
  - Performance Degradation on "Easy" Tasks: ToT performs worse than CoT on easy questions, likely due to unnecessary complexity
  - Path Explosion: Excessive path generation on hard questions requiring upper bound limits

- **First 3 experiments:**
  1. **Sanity Check:** Run LLaMA3.1-8b-INT4 on MedQA subset using standard CoT vs QM-ToT prompt structure to verify 10-15% lift
  2. **Evaluator Ablation:** Replace DeepSeek-V3 with smaller model (GPT-3.5-turbo or LLaMA-8b) as judge to observe sensitivity
  3. **Alpha Tuning:** Adjust weighting parameter α (currently 0.6) to see if prioritizing "Reasoning" over "Correctness" changes accuracy distribution

## Open Questions the Paper Calls Out

- **Monte Carlo Tree Search Integration:** Can integrating MCTS or other advanced search strategies further optimize the reasoning path selection in the QM-ToT framework? The current implementation hasn't explored heuristic search algorithms like MCTS to navigate the reasoning space more efficiently.

- **Reinforcement Learning Enhancement:** How can reinforcement learning be incorporated to enhance the framework's optimization capabilities? The paper establishes a static path-based reasoning loop but doesn't implement dynamic policy updates or reward mechanisms.

- **Evaluator Capability Threshold:** Can a more capable evaluator model prevent the performance degradation observed in high-performing models like LLaMA3-70b? The authors note performance decreased when evaluator struggled to rate the model effectively due to similar performance levels.

- **Fine-tuning Data Value:** To what extent can fine-tuning on QM-ToT generated chain-of-thought data enhance the intrinsic problem-solving capabilities of student models? While the paper demonstrates successful data distillation, it focuses on efficiency rather than long-term capability gains.

## Limitations
- The architecture relies on untested assumptions about evaluator calibration and quantization recovery limits
- Performance degradation occurs on "Easy" questions, suggesting the approach may be over-engineered for simpler tasks
- All experiments focus on medical question answering, raising questions about domain generalization

## Confidence
- **High Confidence:** Core claim that tree-structured reasoning outperforms linear CoT on complex medical questions (10-15% accuracy gains on INT4 models)
- **Medium Confidence:** 3.9% data efficiency claim for distillation - mechanism plausible but weak direct evidence
- **Low Confidence:** Assumption that QM-ToT will consistently improve across all difficulty levels - paper shows degradation on easy questions

## Next Checks
1. **Evaluator Cross-Validation:** Run QM-ToT pipeline with three different evaluator models (DeepSeek-V3, GPT-4, smaller open model) to quantify sensitivity to judge quality and compare consistency of selected answers.

2. **Quantization Level Sweep:** Test framework across gradient of quantization levels (FP16 → INT8 → INT6 → INT4 → INT3) to identify breaking point where ToT decomposition can no longer compensate for parametric loss.

3. **Domain Transfer Test:** Apply exact QM-ToT prompts and evaluation pipeline to non-medical reasoning benchmark (e.g., GSM8K for math word problems) to test whether decomposition strategy generalizes beyond medical knowledge.