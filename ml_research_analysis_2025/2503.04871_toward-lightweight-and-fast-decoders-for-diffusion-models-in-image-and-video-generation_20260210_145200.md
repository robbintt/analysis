---
ver: rpa2
title: Toward Lightweight and Fast Decoders for Diffusion Models in Image and Video
  Generation
arxiv_id: '2503.04871'
source_url: https://arxiv.org/abs/2503.04871
tags:
- video
- decoder
- diffusion
- decoders
- videomae
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper addresses the bottleneck of slow and memory-intensive\
  \ decoding in latent diffusion models by designing lightweight custom decoders for\
  \ both image and video generation. It introduces two lightweight architectures\u2014\
  TAE-192 (based on Taming Transformers) and EfficientViT\u2014trained to replace\
  \ the standard VAE decoder in Stable Diffusion pipelines."
---

# Toward Lightweight and Fast Decoders for Diffusion Models in Image and Video Generation

## Quick Facts
- arXiv ID: 2503.04871
- Source URL: https://arxiv.org/abs/2503.04871
- Authors: Alexey Buzovkin; Evgeny Shilov
- Reference count: 16
- Primary result: Up to 15% overall speed-up for image generation and up to 20× faster decoding in the sub-module using lightweight decoders.

## Executive Summary
This paper addresses the bottleneck of slow and memory-intensive decoding in latent diffusion models by designing lightweight custom decoders for both image and video generation. It introduces two lightweight architectures—TAE-192 (based on Taming Transformers) and EfficientViT—trained to replace the standard VAE decoder in Stable Diffusion pipelines. Experiments on COCO2017 and UCF101 datasets show up to 15% overall speed-ups for image generation and up to 20× faster decoding in the sub-module, with moderate memory reductions. While perceptual quality slightly drops compared to the baseline, the speed and scalability gains are critical for large-scale or real-time inference. The work also contextualizes its approach with insights from VideoMAE V2's dual masking strategy, pointing toward further efficiency improvements.

## Method Summary
The method replaces the heavy VAE decoder in Stable Diffusion with lightweight architectures (TAE-192 or EfficientViT) while keeping the UNet denoising pipeline intact. Decoders are trained in isolation using frozen VAE encoder latents, minimizing reconstruction error through a composite loss (MSE + LPIPS + GAN). For video, temporal attention blocks are added to maintain coherence across frames. Training uses standard datasets (LAION, COCO2017, UCF101) with specific loss weights and schedules. The approach focuses on inference-time efficiency rather than training the full diffusion model.

## Key Results
- Up to 20× faster decoding in the sub-module compared to standard VAE decoder
- Up to 15% overall speed-up for image generation at 256x256 resolution
- Moderate memory reductions (decoders 25-30MB vs standard VAE at 180MB+)
- Video FID improves from 76.84 to 19.22 with temporal attention blocks

## Why This Works (Mechanism)

### Mechanism 1
Reducing decoder parameter count and architectural complexity directly lowers inference latency and memory footprint, provided reconstruction capacity remains above task-specific threshold. Fewer layers and parameters mean fewer FLOPs for matrix operations mapping latents to pixels.

### Mechanism 2
Decoupled training using frozen encoder allows decoder swapping without retraining entire diffusion pipeline. Small decoder learns approximate inverse of encoder's distribution by minimizing reconstruction error on latents from frozen encoder.

### Mechanism 3
Temporal attention layers maintain video coherence by correlating features across time steps. Standard image decoders process frames independently; temporal attention reduces flickering and jitter in video output.

## Foundational Learning

- **Concept:** Variational Autoencoders (VAE) in Latent Diffusion
  - Why needed here: Paper targets decoder half of VAE that reconstructs images from latents
  - Quick check question: If VAE encoder compresses 512x512 image to 64x64 latent, what dimension must decoder accept to output image?

- **Concept:** Loss Function Components (MSE vs. LPIPS vs. GAN)
  - Why needed here: Composite loss (MSE + LPIPS + GAN) trains decoder; understanding trade-offs is key
  - Quick check question: Why would decoder with perfect MSE score (0 error) still produce blurry image vs one trained with adversarial loss?

- **Concept:** Spatio-Temporal Architectures
  - Why needed here: TAE-192 vs TAE-Temporal distinction determines success in video; attention extends from 2D to 3D
  - Quick check question: In temporal attention block, does attention query across Height/Width dimensions or Time dimension?

## Architecture Onboarding

- **Component map:** Latent Tensor (z) from Diffusion UNet -> TAE-192/EfficientViT/TAE-Temporal Processor -> Pixel Tensor (x)

- **Critical path:** Load original SDXL VAE encoder, freeze it, pass LAION images through to get latents, then train new lightweight decoder to reconstruct using composite loss.

- **Design tradeoffs:**
  - TAE-192 vs EfficientViT: TAE-192 offers better fidelity (higher SSIM/PSNR) while EfficientViT might be slightly faster but suffers more quality loss
  - Speed vs Temporal Coherence: Removing temporal blocks speeds up decoding significantly but destroys quality; adding them slows it down but restores usability

- **Failure signatures:**
  - Video Flicker: Using standard image decoder on video tasks results in high-frequency jitter between frames
  - Blur/Texture Loss: Too small decoder or low GAN/LPIPS loss weights produce "smooth" or plastic-like outputs

- **First 3 experiments:**
  1. Integration Test: Load pre-trained TAE-192 decoder weights into standard Stable Diffusion pipeline and run text-to-image inference
  2. Ablation on Resolution: Compare SDXL VAE vs TAE-192 inference time at 256x256 vs 1024x1024 to quantify scaling factor
  3. Video Consistency Check: Run inference on UCF101 video clip using temporal and non-temporal variants to confirm flickering presence/absence

## Open Questions the Paper Calls Out

### Open Question 1
Can applying partial masking to decoder input during inference significantly reduce computational overhead without degrading visual fidelity? Authors suggest this based on VideoMAE V2 insights but didn't implement it.

### Open Question 2
How does efficiency and quality of these lightweight decoders compare directly to VideoMAE V2's dual masking strategy under identical experimental conditions? No head-to-head comparison provided.

### Open Question 3
Can lightweight decoders trained for generation tasks maintain sufficient feature fidelity to generalize to downstream tasks like spatial or temporal action detection? Current evaluation relies on reconstruction metrics only.

### Open Question 4
Does use of lightweight decoders reduce data volume or training iterations required to train overall diffusion model? Paper focuses on inference-time improvements, not training efficiency impact.

## Limitations
- Quality drop compared to baseline may be significant for some applications, particularly at higher resolutions
- Video results use non-standard datasets (UCF101/WebVid) making comparison difficult
- End-to-end speedup depends heavily on UNet denoising step which may dominate total runtime

## Confidence
- Decoder speedup mechanism: High
- Perceptual quality vs speed trade-off: Medium
- Video temporal coherence benefit: Medium
- End-to-end real-time viability: Low

## Next Checks
1. Measure actual end-to-end inference time (UNet + decoder) on real GPU at 512x512 and 1024x1024 to confirm 15% figure
2. Perform user study comparing TAE-192 outputs vs baseline VAE on LAION-Face to quantify perceived quality loss
3. Test whether smaller, finetuned encoder + decoder pair can outperform frozen-encoder + lightweight-decoder setup