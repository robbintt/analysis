---
ver: rpa2
title: Multimodal LLMs Do Not Compose Skills Optimally Across Modalities
arxiv_id: '2511.08113'
source_url: https://arxiv.org/abs/2511.08113
tags:
- skill
- card
- task
- answer
- direct
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether Multimodal Large Language Models
  (MLLMs) can optimally compose their visual and textual skills across modalities.
  The authors design three tasks solvable by sequentially combining one visual and
  one textual skill (OCR+reasoning, object detection+counting, and card recognition+reasoning),
  then evaluate multiple open-source MLLMs using both direct and cascaded inference
  approaches.
---

# Multimodal LLMs Do Not Compose Skills Optimally Across Modalities

## Quick Facts
- **arXiv ID:** 2511.08113
- **Source URL:** https://arxiv.org/abs/2511.08113
- **Reference count:** 0
- **Primary result:** MLLMs exhibit a significant cross-modality skill composition gap, with cascaded inference consistently outperforming direct inference by 2.45 to 22.19 points.

## Executive Summary
This paper investigates whether Multimodal Large Language Models (MLLMs) can optimally compose their visual and textual skills across modalities. The authors design three tasks solvable by sequentially combining one visual and one textual skill (OCR+reasoning, object detection+counting, and card recognition+reasoning), then evaluate multiple open-source MLLMs using both direct and cascaded inference approaches. Results show all tested models exhibit a significant cross-modality skill composition gap, with cascaded inference consistently outperforming direct inference. The gap ranges from 2.45 to 22.19 points depending on the model and task. Two mitigation strategies are explored: chain-of-thought prompting and fine-tuning, both of which improve performance but fail to fully close the gap. The study reveals that even in straightforward scenarios, MLLMs struggle to optimally combine their multimodal capabilities, suggesting the need for further research into training approaches for better skill composition.

## Method Summary
The study evaluates MLLMs' ability to compose visual and textual skills using three task types: OCR+reasoning on rendered text (GSM8K, ARC-E/C, MMLU, MATH), object detection+counting (CV-Bench, custom COCO-Count), and card recognition+numerical reasoning (custom Sort/Sum datasets). Three inference setups are compared: Direct (standard image+question prompt), Cascaded (two model calls: first extracts visual info, second applies textual skill), and Oracle (gold visual input + textual skill). Exact-match accuracy is the primary metric, with skill composition gap calculated as cascaded accuracy minus direct accuracy. Two mitigation strategies are explored: chain-of-thought prompting with explicit skill ordering instructions, and fine-tuning Llama 3.2 11B on task-specific data. All experiments use 1×A100 80GB for inference.

## Key Results
- All tested MLLMs show significant skill composition gaps ranging from 2.45 to 22.19 points
- Cascaded inference consistently outperforms direct inference across all models and tasks
- Chain-of-thought prompting partially closes the gap but requires manual prompt engineering
- Fine-tuning can close the gap for in-distribution tasks but fails to generalize to novel compositions

## Why This Works (Mechanism)

### Mechanism 1: Sequential Skill Decoupling (Cascaded Inference)
- **Claim:** If MLLMs are forced to output an intermediate result for a visual skill before initiating a textual skill, performance improves significantly compared to end-to-end inference.
- **Mechanism:** The "Cascaded" setup isolates the visual processing (e.g., OCR or object detection) into a distinct context window. This prevents the error propagation or "interference" that occurs when a model attempts to attend to visual tokens and perform high-level reasoning simultaneously in a single forward pass.
- **Core assumption:** The model possesses the individual skills (vision and reasoning) but lacks the internal control flow to execute them sequentially without external enforcement.
- **Evidence anchors:**
  - [Abstract] "cascaded inference, which manually enforces the composition of the two skills... consistently outperforming direct inference."
  - [Section 4.2] Defines cascaded inference as making "two calls to the same model."
- **Break condition:** If the model lacks one of the prerequisite skills (e.g., it cannot perform OCR), cascading will fail to improve results, as seen in some Object Detection tasks (Section 5.2).

### Mechanism 2: Instruction-Driven Attention (Chain-of-Thought)
- **Claim:** Explicit CoT prompts ordering the model to "first extract, then reason" can partially recover the composition gap, provided the model is instruction-tuned to follow multi-step reasoning.
- **Mechanism:** CoT prompts bias the attention mechanism to prioritize visual tokens first to generate the intermediate extraction (e.g., "The text content is...") before engaging the reasoning circuits.
- **Core assumption:** The gap is partly due to the model not "realizing" it needs to look closely at the image before reasoning, rather than a total inability to process the image.
- **Evidence anchors:**
  - [Section 6.1] "CoT prompting enables MLLMs to compose skills as requested, whereas direct inference shows a different strategy."
  - [Section 6.1] "it is still far from the performance achieved by the cascaded setup."
- **Break condition:** CoT fails if the model's visual perception is the bottleneck (it cannot see the text/object) or if the prompt engineering required is too task-specific (scalability issues).

### Mechanism 3: Exposure-Based Skill Fusion
- **Claim:** The skill composition gap is not static; it collapses if the specific cross-modal composition (e.g., Math + Image) was explicitly present in the pre-training or fine-tuning data.
- **Mechanism:** "Composition" is treated as a novel generalization task. If the model has seen the pattern (e.g., Qwen2.5-VL trained on invoices/math images), the composition becomes a "monolithic skill" retrieved from memory rather than computed on the fly.
- **Core assumption:** Models cannot dynamically compose skills *unless* they have seen similar compositional patterns during training.
- **Evidence anchors:**
  - [Section 5.1] Notes Qwen2.5-VL shows a minimal gap because training included "invoices, forms... math," making the task "not novel."
  - [Corpus] "Unveiling the Compositional Ability Gap..." suggests RL/post-training strategies are being explored to close this gap, implying standard training is insufficient.
- **Break condition:** If a task requires a completely novel combination of skills not seen during training, performance drops to "Direct" levels regardless of model size.

## Foundational Learning

- **Concept:** **Skill Composition**
  - **Why needed here:** The paper defines this as the capacity to combine learned skills in novel ways not encountered during training. You must distinguish between "solving a task" and "composing skills" to understand the gap.
  - **Quick check question:** Can the model solve the task if it is split into two separate API calls (Cascaded) but fails in one? If yes, it has the skills but lacks composition.

- **Concept:** **Late-Fusion Architecture**
  - **Why needed here:** MLLMs typically graft a visual encoder onto a frozen/pre-trained LLM. This separation is the likely structural cause of the composition friction, as the LLM must translate visual tokens into "textual" concepts before reasoning.
  - **Quick check question:** Does the model process the image as a sequence of tokens similar to text, or does it use a separate specialized path?

- **Concept:** **Oracle vs. Cascaded Baselines**
  - **Why needed here:** Evaluating performance requires isolating variables. "Oracle" (perfect visual input) tests the reasoning engine; "Cascaded" (model's own visual input) tests the full pipeline.
  - **Quick check question:** Is the failure due to the visual skill (OCR error) or the reasoning step? The paper uses Oracle baselines to separate these (Appendix C).

## Architecture Onboarding

- **Component map:** Visual Encoder (CLIP/ViT) -> Adapter (MLP projection) -> LLM Backbone (Textual Skills storage) -> Evaluator (Cascaded wrapper)
- **Critical path:**
  1. **Input:** Image + Text Prompt
  2. **Step 1 (Visual Skill):** Encoder/LLM generates intermediate text (e.g., JSON of detected objects)
  3. **Step 2 (Textual Skill):** LLM takes intermediate text + original query -> Final Answer
- **Design tradeoffs:**
  - **Direct Inference:** Fast (1 call), but suffers from "composition gap" (lower accuracy)
  - **Cascaded Inference:** High accuracy, but doubles latency (2 calls) and token costs
  - **CoT:** Middle ground (1 call), but requires manual prompt tuning and is model-dependent (Section 6.1)
- **Failure signatures:**
  - **Visual Hallucination:** In Direct mode, the model ignores the image and guesses based on language priors (Section G.2)
  - **Skill Drop-off:** In Direct mode, the model performs the visual skill but fails to transition to the reasoning skill, outputting a description instead of an answer
- **First 3 experiments:**
  1. **Establish the Gap:** Run the same dataset (e.g., GSM8K rendered images) on both Direct and Cascaded setups. If Cascaded >> Direct, you have confirmed the composition gap
  2. **Isolate the Weak Skill:** Run the "Oracle" setup. Feed the model the *text* of the problem directly. If Oracle ≈ Cascaded, the visual skill is the bottleneck. If Oracle >> Cascaded, the composition process is the bottleneck
  3. **Test CoT Robustness:** Try the CoT prompt (Table 9). If performance recovers to near-Cascaded levels, the gap is attention-alignment based. If it fails, the gap is likely structural or capacity-based

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** What are the root causes of the cross-modality skill composition gap in MLLMs, and is the gap primarily architectural, training-related, or both?
- **Basis in paper:** [explicit] Conclusion states: "this work prompts further research into why MLLMs have trouble successfully composing skills across modalities even in simple scenarios."
- **Why unresolved:** The paper demonstrates the gap exists and explores mitigation strategies, but does not investigate underlying mechanisms or whether the late-fusion architecture vs. training data composition is the primary factor.
- **What evidence would resolve it:** Ablation studies varying architecture designs (e.g., early vs. late fusion, cross-attention mechanisms) and training data composition (explicit vs. implicit skill composition examples) while controlling for model size and individual skill performance.

### Open Question 2
- **Question:** Can more scalable mitigation strategies be developed that do not require task-specific prompt engineering or in-distribution fine-tuning?
- **Basis in paper:** [explicit] Section 6.1 notes CoT prompting "is not scalable to arbitrary tasks, as it requires designing prompts manually"; Section 6.2 shows fine-tuning "only fully closed when fine-tuning and evaluation are performed on the same dataset."
- **Why unresolved:** Both explored mitigation strategies have significant limitations—CoT lacks scalability, and fine-tuning generalization is limited to similar tasks.
- **What evidence would resolve it:** Development and evaluation of automated skill composition methods (e.g., learned composition modules, self-prompting techniques) that generalize across diverse unseen task types without manual intervention.

### Open Question 3
- **Question:** How does the skill composition gap scale with the number of skills being composed beyond two skills?
- **Basis in paper:** [explicit] Limitations section states: "Our work currently focuses on the combination of only two skills. However, it would be interesting to incorporate a wider range of skills... more similar to a real-word scenario in which several skills are required."
- **Why unresolved:** The experimental design is limited to sequential composition of one visual and one textual skill, leaving open whether composition difficulty grows linearly, exponentially, or follows another pattern.
- **What evidence would resolve it:** Evaluation on tasks requiring composition of three or more skills (e.g., OCR + translation + reasoning) with systematic measurement of gap size relative to the cascaded baseline.

## Limitations
- The composition gap is evaluated only on three specific task types, which may not generalize to other visual-textual compositions
- The cascaded setup uses oracle-like intermediate outputs, but real-world deployment would require reliable intermediate extraction
- CoT and fine-tuning mitigation strategies show improvement but do not close the gap, suggesting either fundamental architectural limitations or insufficient tuning

## Confidence

- **High confidence**: The existence of a measurable cross-modal skill composition gap across multiple models and tasks is well-supported by the experimental data
- **Medium confidence**: The attribution of the gap to attention interference or lack of compositional generalization during training is plausible but not definitively proven
- **Low confidence**: The scalability and practicality of the proposed mitigation strategies (CoT and fine-tuning) for closing the gap in real-world applications are uncertain

## Next Checks

1. **Test novel compositions**: Evaluate the same models on tasks requiring combinations of visual and textual skills not seen during training (e.g., image captioning + sentiment analysis) to confirm the gap is not task-specific
2. **Probe architectural bottlenecks**: Compare performance of direct and cascaded inference across models with different fusion strategies (e.g., early vs. late fusion) to isolate whether the gap is due to attention interference or architectural design
3. **Validate intermediate reliability**: Assess the accuracy of model-generated intermediate outputs (e.g., OCR text, detected objects) in cascaded setups to determine if the gap persists even with perfect intermediate extraction