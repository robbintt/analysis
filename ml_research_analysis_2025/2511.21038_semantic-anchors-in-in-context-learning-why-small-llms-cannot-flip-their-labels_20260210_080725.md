---
ver: rpa2
title: 'Semantic Anchors in In-Context Learning: Why Small LLMs Cannot Flip Their
  Labels'
arxiv_id: '2511.21038'
source_url: https://arxiv.org/abs/2511.21038
tags:
- label
- natural
- inverted
- semantic
- demonstrations
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'Small language models (1-12B parameters) cannot override pre-trained
  label semantics through in-context learning. Across eight classification tasks and
  eight open-source models, systematic label inversion fails to produce coherent anti-semantic
  classifiers: semantic override rates remain exactly zero even with 8 demonstrations.'
---

# Semantic Anchors in In-Context Learning: Why Small LLMs Cannot Flip Their Labels

## Quick Facts
- arXiv ID: 2511.21038
- Source URL: https://arxiv.org/abs/2511.21038
- Reference count: 16
- Key outcome: Small language models (1-12B parameters) cannot override pre-trained label semantics through in-context learning; semantic override rates remain exactly zero even with 8 demonstrations.

## Executive Summary
This paper demonstrates that small language models (1-12B parameters) cannot override pre-trained label semantics through in-context learning. Across eight classification tasks and eight open-source models, systematic label inversion fails to produce coherent anti-semantic classifiers. The study shows that ICL operates as prior refinement rather than flexible remapping, adjusting input projections within a stable semantic space but unable to redefine label meanings. These findings establish fundamental limits of few-shot prompting and suggest that semantic anchors require interventions beyond standard ICL.

## Method Summary
The study evaluates 1-12B parameter LLMs across eight classification tasks using zero-shot, natural k-shot, and inverted k-shot prompting with k∈{1,2,4,8}. Natural demonstrations use correct labels while inverted demonstrations systematically permute labels. The evaluation measures accuracy, prior alignment (agreement with zero-shot predictions), prompt alignment (agreement with demonstrations), and semantic override rate (joint probability of correctness under inverted semantics). All experiments use greedy decoding with 5 random seeds for demonstration sampling, and UNK predictions are excluded from accuracy calculations.

## Key Results
- Semantic override rates remain exactly zero in 1-12B parameter models under inverted ICL, even with 8 demonstrations.
- Natural ICL improves accuracy while maintaining strong coupling to zero-shot predictions (86.4% consistency).
- Inverted ICL causes monotonic degradation across all tasks and models, with larger drops at higher k values.
- ICL operates as prior refinement rather than flexible remapping of label semantics.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: ICL operates through prior refinement, not flexible remapping of label semantics.
- Mechanism: Demonstrations adjust how inputs project onto pre-existing semantic directions in representation space, but cannot redefine what label tokens mean.
- Core assumption: Pre-training encodes stable semantic directions for label tokens that persist without parameter updates.
- Evidence anchors: [abstract] "ICL primarily adjusts how inputs project onto stable semantic directions learned during pre-training"; [section 5.2] "86.4% of examples are both correct and consistent with the prior" under natural ICL.
- Break condition: If models could achieve non-zero semantic override rates under inverted demonstrations, prior refinement would be insufficient as sole mechanism.

### Mechanism 2
- Claim: Semantic anchors create irreconcilable conflict when demonstrations contradict pre-trained semantics.
- Mechanism: When inverted demonstrations push against semantic anchors, the model cannot reconcile competing constraints (demonstrated mapping vs. pre-trained semantics).
- Core assumption: The optimization pressure from demonstrations cannot exceed the strength of semantic anchoring in 1–12B models.
- Evidence anchors: [abstract] "semantic override rates remain exactly zero in our few-shot 1–12B setting"; [section 5.3] "roughly half of predictions match the demonstrated (incorrect) labels... Yet the semantic override rate remains exactly zero".
- Break condition: If larger models (beyond 12B) showed non-zero override rates at similar k values, this would indicate scale-dependent rather than fundamental semantic constraint.

### Mechanism 3
- Claim: Demonstration count amplifies semantic conflict rather than providing override capacity.
- Mechanism: Each additional inverted demonstration strengthens contradiction between demonstrated and pre-trained semantics.
- Core assumption: ICL has no mechanism to weight demonstration evidence above semantic priors when they conflict.
- Evidence anchors: [section 5.4] "Sentiment tasks that barely degrade at k=1 (losing 5–10 points) collapse at k=8 (losing more than 40 points)"; [section 5.4] "This is not random variation, but a systematic conflict".
- Break condition: If inverted ICL showed diminishing degradation (plateau) rather than monotonic collapse, this would suggest override capacity exists but requires threshold evidence.

## Foundational Learning

- Concept: **Zero-shot prior vs. in-context classifier distinction**
  - Why needed here: The paper decomposes ICL behavior by comparing k-shot predictions against zero-shot baselines. Understanding what the model does without demonstrations is essential to measure what demonstrations add.
  - Quick check question: Can you articulate why comparing natural and inverted demonstrations against zero-shot behavior reveals whether ICL learns new mappings or refines existing ones?

- Concept: **Alignment decomposition (truth, prior, prompt)**
  - Why needed here: These three metrics separate accuracy from consistency with pre-training and consistency with demonstrations. The key insight comes from observing how they diverge under inversion.
  - Quick check question: Under inverted demonstrations, why must prompt alignment and accuracy trade off against each other?

- Concept: **Semantic override rate as joint probability**
  - Why needed here: This metric captures whether models can simultaneously be correct AND follow inverted mappings. The "exactly zero" result is the paper's central evidence for semantic anchors.
  - Quick check question: Why is measuring P(y_icl = y* ∧ y_icl = y_prompt) more informative than measuring accuracy alone under inverted demonstrations?

## Architecture Onboarding

- Component map:
  - Zero-shot prior f_0(x) -> In-context classifier f_icl(x; S) -> Alignment metrics (Truth, Prior, Prompt) -> Semantic override rate

- Critical path:
  1. Select task with semantically meaningful labels (POS/NEG, ENTAILMENT/NEUTRAL/CONTRADICTION)
  2. Construct natural demonstrations (correct labels) and inverted demonstrations (systematic permutation)
  3. Run zero-shot, natural k-shot, and inverted k-shot with identical seeds
  4. Compute all three alignment metrics plus semantic override rate
  5. Analyze: Does P(y=y*∧y=ỹ) remain zero? Does prior alignment hold under natural ICL?

- Design tradeoffs:
  - Greedy decoding (temperature 0) ensures reproducibility but may underrepresent model uncertainty
  - k ∈ {1,2,4,8} tests few-shot regime; results may not extend to many-shot (100+ examples)
  - Excluding UNK predictions from accuracy ensures clean measurement but may hide edge-case behavior
  - Assumption: Semantic anchors are label-token specific; may not generalize to arbitrary symbol mappings

- Failure signatures:
  - Non-zero semantic override rate would invalidate the semantic anchor claim
  - Weak prior alignment under natural ICL would contradict prior refinement mechanism
  - Monotonic improvement under inverted ICL would suggest flexible remapping capacity

- First 3 experiments:
  1. **Baseline replication**: Run zero-shot and k=8 natural/inverted on SST-2 with LLaMA-3.1-8B-Instruct. Verify P(y=y*∧y=ỹ) ≈ 0 and prior alignment >80% under natural ICL.
  2. **Scale probe**: Test smallest model (Gemma-3-1B-IT) vs. largest (Gemma-3-12B-IT) on sentiment task. Confirm override rate remains zero across 12× scale difference.
  3. **Weak prior stress test**: Replicate QQP experiment where zero-shot accuracy is anomalously low (~40%). Verify that even with weak prior, semantic override remains impossible—demonstrating that semantic anchors persist regardless of prior strength.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the failure to override label semantics persist when using arbitrary, non-semantic symbols (e.g., "Zog"/"Bif") instead of natural language labels with pre-existing meaning?
- Basis in paper: [explicit] The conclusion explicitly states future work should "test whether this constraint is specific to semantically loaded labels or extends to arbitrary symbol-concept mappings."
- Why unresolved: The current study only flips natural labels (e.g., POS↔NEG), leaving open the question of whether the anchoring effect is due to the specific semantic history of the tokens or a general inability to remap any label token in small models.
- What evidence would resolve it: Experiments repeating the inversion protocol using random strings or abstract symbols as labels to see if the semantic override rate rises above zero.

### Open Question 2
- Question: How does the model scale precisely interact with the "geometric constraints" of semantic anchors to enable or disable override capabilities?
- Basis in paper: [explicit] The authors ask future work to "explore how model scale affects the flexibility of these geometric constraints," noting a gap between the 1–12B models tested and larger models like GPT-3.
- Why unresolved: The paper establishes that 1–12B models cannot flip labels, but the exact scaling laws or phase transitions where the manifold becomes plastic enough for semantic override remain unmapped.
- What evidence would resolve it: A systematic evaluation of semantic override rates across a denser range of model sizes (e.g., 13B, 30B, 70B) to identify the parameter threshold where non-zero override emerges.

### Open Question 3
- Question: Do alternative interventions like symbol tuning or contrastive decoding successfully reshape the semantic manifold in small models, or do they merely force projections onto the same stable regions?
- Basis in paper: [inferred] The paper suggests "interventions beyond ICL" are necessary but does not verify if these methods actually overcome the semantic anchors in the 1–12B parameter range or simply optimize the input projection.
- Why unresolved: While the paper proves ICL cannot overcome the anchors, it leaves the efficacy of fine-tuning-based interventions as an assumption rather than an empirically validated result in this context.
- What evidence would resolve it: Applying symbol tuning to the specific 1–12B models used here and measuring if the semantic override rate becomes non-zero under the same inverted conditions.

## Limitations

- The findings are limited to 1-12B parameter models and standard demonstration regimes (k≤8), with unknown generalizability to larger models or many-shot prompting.
- The study uses greedy decoding which may underrepresent uncertainty in semantic conflict scenarios and excludes UNK predictions from accuracy metrics.
- The semantic override rate of exactly zero across all conditions, while robust, suggests the need for additional stress tests to confirm the strength of this empirical finding.

## Confidence

- **High confidence** (Confidence ≥0.8): The semantic override rate remains exactly zero under inverted ICL for all tested 1-12B models across eight diverse tasks and eight models with multiple random seeds.
- **Medium confidence** (Confidence 0.5-0.8): ICL operates as prior refinement rather than flexible remapping. While the three-way alignment decomposition provides strong evidence, alternative explanations cannot be fully ruled out without additional experiments.
- **Low confidence** (Confidence <0.5): The mechanism generalizes to larger models or different prompting strategies. The paper acknowledges that GPT-3-scale models can eventually flip labels, suggesting scale-dependent effects that were not directly tested in this study.

## Next Checks

1. **Stress test weak prior scenarios**: Replicate the QQP experiment with systematically reduced prompt quality to verify whether semantic anchors persist even when zero-shot priors are deliberately weakened.

2. **Scale boundary experiment**: Test a 34B or 70B parameter model on the same inverted ICL protocol to precisely identify where (if ever) semantic override rates become non-zero.

3. **Extended demonstration regime**: Run inverted ICL with k=32 or k=64 demonstrations on the largest 12B model to determine whether there exists a threshold where monotonic degradation plateaus.