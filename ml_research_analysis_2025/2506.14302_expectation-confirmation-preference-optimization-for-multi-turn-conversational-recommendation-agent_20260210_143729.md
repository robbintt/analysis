---
ver: rpa2
title: Expectation Confirmation Preference Optimization for Multi-Turn Conversational
  Recommendation Agent
arxiv_id: '2506.14302'
source_url: https://arxiv.org/abs/2506.14302
tags:
- user
- ecpo
- preference
- dialogue
- response
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of aligning LLM-based conversational
  recommendation agents with user expectations in multi-turn dialogues. It introduces
  ECPO, a preference optimization paradigm that leverages Expectation Confirmation
  Theory to explicitly model the evolution of user satisfaction and identify the root
  causes of dissatisfaction.
---

# Expectation Confirmation Preference Optimization for Multi-Turn Conversational Recommendation Agent

## Quick Facts
- **arXiv ID:** 2506.14302
- **Source URL:** https://arxiv.org/abs/2506.14302
- **Reference count:** 36
- **One-line primary result:** ECPO improves conversational recommendation success rate from 0.34 to 0.56 and win rate from 0.28 to 0.57 across three datasets.

## Executive Summary
This paper addresses the challenge of aligning LLM-based conversational recommendation agents (CRAs) with user expectations in multi-turn dialogues. It introduces ECPO, a preference optimization paradigm that leverages Expectation Confirmation Theory to explicitly model the evolution of user satisfaction and identify the root causes of dissatisfaction. ECPO achieves turn-level preference optimization by combining forward expectation confirmation to identify unsatisfactory responses, backward expectation derivation to rewrite them, and preference optimization to fine-tune the model. To support this process, the paper proposes AILO, an LLM-based user simulator that provides realistic and diverse feedback. Experimental results show that ECPO significantly improves both the recommendation quality (e.g., success rate increases from 0.34 to 0.56) and interactive capabilities (win rate increases from 0.28 to 0.57) of CRAs across three datasets, outperforming existing methods in both efficiency and effectiveness.

## Method Summary
ECPO operates through a four-stage pipeline: (1) Simulator-Guided Planning Tuning (SGPT) where GPT-4o mini dialogues with AILO generate successful trajectories for SFT on Llama-3.1-8B; (2) Forward Expectation Confirmation where AILO evaluates each turn's satisfaction across three dimensions (flexibility 0-2, coherence 0-2, guidance 0-1); (3) Backward Expectation Derivation where a Rewriter LLM refines unsatisfactory responses (threshold λ=4.0) to create preference pairs; (4) Preference Optimization applying DPO/KTO/SimPO to turn-level pairs. The AILO simulator extracts user personas from review data across four dimensions and generates policy-based responses with explicit expectation confirmation, achieving high win rates against baseline simulators.

## Key Results
- ECPO significantly improves success rate from 0.34 to 0.56 and win rate from 0.28 to 0.57 across three datasets (Amazon-Game, Amazon-Book, Yelp)
- ECPO outperforms existing methods in both efficiency and effectiveness, with ablation studies showing forward EC provides meaningful signal beyond generic rewriting
- AILO simulator achieves 100% win rate against iEvalLM in human evaluation for dialogue authenticity while maintaining diverse persona representation (ROUGE-L analysis shows significantly lower persona similarity than RecAgent)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Turn-level preference optimization guided by explicit satisfaction modeling may outperform trajectory-level or tree-simulation approaches for multi-turn conversational recommendation.
- Mechanism: ECPO applies Expectation Confirmation Theory to score each dialogue turn against user expectations (flexibility, coherence, guidance). Low-scoring turns are identified for targeted optimization rather than treating all turns uniformly or requiring expensive tree expansion.
- Core assumption: Users form turn-specific expectations that can be decomposed into measurable dimensions, and dissatisfaction at specific turns can be attributed to concrete, addressable causes.
- Evidence anchors:
  - [abstract] "ECPO achieves turn-level preference optimization by combining forward expectation confirmation to identify unsatisfactory responses, backward expectation derivation to rewrite them, and preference optimization to fine-tune the model."
  - [section 3.3] Tree simulation methods "actually led to negative gains, likely due to noise interference," while trajectory-level methods "fail to effectively capture preference relationships at the turn level."
  - [corpus] Neighbor papers on contrasting user preferences (arXiv:2503.22005) and DPO for CRS (arXiv:2508.19918) suggest preference-level refinement is an active direction, but corpus evidence specifically validating turn-level decomposition is limited.
- Break condition: If user satisfaction cannot be reliably decomposed into turn-level dimensions, or if satisfaction scores are noisy/unreliable, the optimization signal may be corrupted.

### Mechanism 2
- Claim: Counterfactual rewriting with explicit dissatisfaction reasoning can produce higher-quality positive training samples than self-sampling from the policy.
- Mechanism: Instead of sampling multiple candidate responses and simulating full conversations to identify preferences (costly and noise-prone), ECPO uses a Rewriter LLM that takes the explicit confirmation feedback (CONFt) explaining why a response failed expectations, then generates a targeted refinement.
- Core assumption: The Rewriter can reliably produce improved responses given the dissatisfaction explanation, and these rewritten responses constitute valid preference pairs for optimization.
- Evidence anchors:
  - [section 2.2] "ECPO implicitly assigns rewards at each turn through the EC process and provides the underlying reasons for these rewards in natural language. These reasons promote the proactive generation of positive samples for preference optimization instead of self-sampling."
  - [section 4.2/C.2] Human and GPT-4o evaluation found rewritten responses superior to original unsatisfactory responses in ~80% and ~64% of cases respectively.
  - [corpus] Weak direct corpus evidence for this specific rewriting mechanism; related work on LLM-based user simulation exists but doesn't validate counterfactual rewriting for preference data.
- Break condition: If the Rewriter introduces systematic biases (e.g., over-politeness, generic responses) that don't reflect genuine preference improvements, the model may optimize for superficial qualities rather than actual user satisfaction.

### Mechanism 3
- Claim: LLM-based user simulators with structured persona modeling can provide sufficiently realistic feedback to support preference optimization without human annotation.
- Mechanism: AILO constructs user personas from real review data across four dimensions (Activities, Interests, Language, Orientations), then simulates policy-based response generation with explicit expectation confirmation at each turn.
- Core assumption: Simulated user feedback approximates real user satisfaction distributions well enough that optimization on simulator feedback transfers to real users.
- Evidence anchors:
  - [section 2.3] AILO achieved 100% win rate against iEvalLM in human evaluation for dialogue authenticity; ROUGE-L analysis showed significantly lower persona similarity (higher diversity) than RecAgent.
  - [section 3.2] After ECPO alignment using AILO, the CRA achieved WR of 0.57-0.63 against GPT-4o mini expert, suggesting transfer from simulation to evaluation.
  - [corpus] Neighbor paper arXiv:2504.12313 explores personality traits in CRS via LLM simulation, supporting the viability of persona-based simulators; arXiv:2306.02552 (RecAgent, cited) provides precedent for LLM user simulation.
- Break condition: Distribution shift between AILO and real users could cause the aligned model to overfit to simulator artifacts rather than genuine user preferences.

## Foundational Learning

- Concept: Expectation Confirmation Theory (ECT)
  - Why needed here: Core theoretical framework; defines satisfaction as the gap between prior expectations and perceived performance. You need this to understand why the paper decomposes evaluation into explicit dimensions and models satisfaction evolution.
  - Quick check question: Can you explain why ECT suggests dissatisfaction has identifiable "root causes" rather than being a holistic judgment?

- Concept: Direct Preference Optimization (DPO) and preference optimization landscape
  - Why needed here: ECPO builds on DPO (and is compatible with KTO, SimPO). Understanding the standard preference optimization formulation is required to see what ECPO modifies (turn-level pairs vs. trajectory-level).
  - Quick check question: In standard DPO, what is the role of the reference model, and how does ECPO's turn-level dataset construction change the optimization problem?

- Concept: Multi-turn dialogue state representation
  - Why needed here: The CRA maintains dialogue state st at each turn; understanding how states, responses, and internal reasoning are represented is essential for implementing the forward/backward passes.
  - Quick check question: Given the episode formulation H^T = {u0, (cr1, p1, u1), ..., (crT, pT, uT)}, what information is available to the expectation confirmation process at turn t?

## Architecture Onboarding

- Component map:
  1. **CRA Backbone (π)**: Llama-3.1-8B-Instruct with LoRA adapters; generates internal reasoning (crt) and response (pt) given state (st).
  2. **AILO Simulator**: GPT-4o mini-based user simulator with persona extraction from review data; generates user responses, policies, and EC feedback.
  3. **Forward EC Module**: Prompts AILO to evaluate responses against expectations; outputs satisfaction scores (rt) and natural-language confirmation reasons (CONFt).
  4. **Backward Rewriter**: Separate LLM that takes (st, pt, CONFt) and produces refined response p̃t when rt < λ.
  5. **Preference Optimizer**: Standard DPO/KTO/SimPO applied to turn-level pairs Dpre = {(st, pt, p̃t)}.

- Critical path:
  1. Generate Dsft via Simulator-Guided Planning Tuning (GPT-4o mini CRA ↔ AILO dialogues, filter successful trajectories).
  2. SFT on Dsft to obtain πsft.
  3. Run πsft with AILO to collect (st, pt, CONFt, rt) tuples.
  4. Apply Rewriter to all turns where rt < λ to generate Dpre.
  5. Run preference optimization on Dpre.

- Design tradeoffs:
  - Threshold λ: Lower values = fewer but higher-impact training samples (Section 3.5); higher values = more data but lower per-sample gain.
  - Rewriter constraints: "Limited modifications" vs. complete rewrite affects how far optimized model drifts from πsft.
  - EC dimensions: Current setup uses flexibility (0-2), coherence (0-2), guidance (0-1); extendable but requires dimension-specific evaluation design.
  - Simulator choice: AILO is more diverse but uses GPT-4o mini; could substitute other LLMs with potential quality tradeoffs.

- Failure signatures:
  1. **Low win rate despite high success rate**: Model recommends correctly but interactions feel rigid → EC dimensions may be misconfigured or Rewriter not addressing guidance/flexibility.
  2. **Negative gain after preference optimization**: Tree-simulation style noise contamination → verify you're using ECPO-style rewriting, not self-sampling.
  3. **High variance across datasets**: Overfitting to simulator artifacts → check AILO persona diversity, consider human validation of EC feedback.
  4. **Rewritten responses rated worse than original**: Rewriter prompt may be introducing bias; review Table 3 metrics (fidelity vs. coherence).

- First 3 experiments:
  1. Reproduce the ECPO vs. ECPO-w/o-EC ablation (Table 2) on a single dataset to verify your forward EC module is providing meaningful signal beyond generic rewriting.
  2. Sweep λ ∈ {1, 2, 3, 4} on a held-out validation set to calibrate the rewriting threshold for your specific CRA backbone and domain.
  3. Human evaluation of 20-50 AILO-generated dialogues against your target user population to assess simulator-real user gap before committing to full training runs.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the Expectation Confirmation process be internalized into LLM reasoning chains rather than relying on external simulator pipelines?
- Basis in paper: [explicit] Page 9 states: "Another promising direction is enabling LLMs to generate simulated user expectations... integrating the EC process into the reasoning phase."
- Why unresolved: The current implementation decouples the simulation (AILO) and rewriting from the agent's internal reasoning; shifting this to an intrinsic capability requires architectural innovations for autonomous self-correction.
- What evidence would resolve it: Demonstrating an "O1/R1-style" agent that autonomously identifies and corrects expectation mismatches during chain-of-thought generation without external prompting.

### Open Question 2
- Question: How can the distribution shift between the AILO simulator and real human users be effectively quantified and minimized?
- Basis in paper: [explicit] Page 9 notes: "An inevitable gap may still exist between simulated and real users. This gap may lead to distribution shift issues in real-world scenarios."
- Why unresolved: While AILO improves upon baselines, it remains a proxy. The paper acknowledges that reliance on simulation may not fully capture the nuance of live human interaction.
- What evidence would resolve it: Online A/B testing results comparing ECPO performance against human-feedback baselines, showing statistical parity in success rates and satisfaction.

### Open Question 3
- Question: Is ECPO effective in domains lacking explicit "target items," such as emotional support or open-domain chat?
- Basis in paper: [explicit] Page 9 claims: "Although ECPO is designed for CRAs, we believe it can be extend to broader dialogue assistants."
- Why unresolved: The current methodology grounds "expectations" using a ground-truth item ($i_E$). Generalizing this to tasks where success is subjective or context-dependent is non-trivial.
- What evidence would resolve it: Application of ECPO to non-recommendation conversational datasets (e.g., emotional support dialogues) showing improved user satisfaction scores.

### Open Question 4
- Question: How sensitive is ECPO to the capability of the "Rewriter" model used for counterfactual inference?
- Basis in paper: [inferred] The method depends on a Rewriter to generate positive samples (Page 4), assuming the rewriter can successfully improve upon unsatisfactory responses.
- Why unresolved: A weak or misaligned Rewriter could introduce hallucinations or semantic drift into the preference dataset ($D_{pre}$), leading to reward hacking.
- What evidence would resolve it: Ablation studies using rewriters of varying sizes (e.g., 7B vs 70B parameters) and measuring the resulting alignment quality and error rates.

## Limitations

- The reliance on AILO simulator feedback introduces potential distribution shift risks that could limit real-world applicability, as the paper acknowledges that "an inevitable gap may still exist between simulated and real users."
- The turn-level satisfaction decomposition via Expectation Confirmation Theory may not fully represent real user preferences, particularly in domains with different interaction patterns where the EC dimensions might not capture all relevant satisfaction drivers.
- The method's effectiveness in domains lacking explicit "target items" (such as emotional support or open-domain chat) remains unproven, as the current methodology grounds "expectations" using a ground-truth item that may not generalize.

## Confidence

- **High Confidence**: The comparative performance gains (SR from 0.34→0.56, WR from 0.28→0.57) are well-supported by experimental results across three datasets, demonstrating the effectiveness of the overall ECPO framework.
- **Medium Confidence**: The mechanism by which turn-level optimization outperforms trajectory-level approaches is plausible but requires further validation, particularly the claim that EC-based rewriting provides cleaner optimization signals than self-sampling methods.
- **Low Confidence**: The transferability of simulator-optimized models to real users is the weakest link - while AILO achieves 100% win rate against iEvalLM, this may reflect simulator-specific artifacts rather than genuine preference alignment.

## Next Checks

1. **Real User Validation**: Conduct A/B testing with 50+ real users comparing ECPO-aligned CRA against baseline models to verify simulator-to-real transfer, measuring not just SR/WR but qualitative satisfaction.
2. **EC Dimension Ablation**: Systematically remove each EC dimension (flexibility, coherence, guidance) and measure impact on optimization quality to validate the decomposition approach and identify which dimensions drive performance.
3. **Cross-Domain Generalization**: Test ECPO on domains outside the current datasets (e.g., travel recommendations, professional services) to evaluate whether the EC framework generalizes beyond product recommendation contexts.