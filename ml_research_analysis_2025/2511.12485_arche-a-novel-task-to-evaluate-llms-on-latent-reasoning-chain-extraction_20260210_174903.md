---
ver: rpa2
title: 'ARCHE: A Novel Task to Evaluate LLMs on Latent Reasoning Chain Extraction'
arxiv_id: '2511.12485'
source_url: https://arxiv.org/abs/2511.12485
tags:
- reasoning
- label
- scientific
- each
- steps
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The ARCHE benchmark evaluates whether large language models can
  extract and structure scientific reasoning chains into formal Reasoning Logic Trees
  (RLTs). Using 70 Nature Communications papers, it challenges models to identify
  premises and conclusions from text and references, classify each step as deduction,
  induction, or abduction, and assemble them into a coherent graph.
---

# ARCHE: A Novel Task to Evaluate LLMs on Latent Reasoning Chain Extraction

## Quick Facts
- **arXiv ID**: 2511.12485
- **Source URL**: https://arxiv.org/abs/2511.12485
- **Reference count**: 37
- **Primary result**: LLMs struggle to extract and formalize scientific reasoning chains, with average Reasoning Edge Accuracy (REA) of only 28% and Entity Coverage (EC) of 51%

## Executive Summary
The ARCHE benchmark evaluates whether large language models can extract and structure scientific reasoning chains into formal Reasoning Logic Trees (RLTs). Using 70 Nature Communications papers, it challenges models to identify premises and conclusions from text and references, classify each step as deduction, induction, or abduction, and assemble them into a coherent graph. Two metrics—Entity Coverage (EC) and Reasoning Edge Accuracy (REA)—measure completeness and logical validity. Across 10 leading LLMs, average REA is only 28% and EC 51%, with models showing a trade-off between coverage and accuracy. Even top models like Grok-3 and o3 rarely exceed 50% in either metric, revealing a substantial gap between natural language fluency and rigorous scientific reasoning.

## Method Summary
ARCHE presents a novel task requiring LLMs to extract latent reasoning chains from scientific papers and represent them as formal Reasoning Logic Trees (RLTs). The benchmark uses 70 papers from Nature Communications, where each paper's reasoning is manually annotated to create ground-truth RLTs. Models must identify reasoning entities (premises and conclusions) from the paper text and references, classify each reasoning step as deduction, induction, or abduction, and construct a coherent graph linking these elements. Performance is evaluated using two metrics: Entity Coverage (EC) measures the proportion of reasoning entities successfully extracted, while Reasoning Edge Accuracy (REA) measures the proportion of edges that are logically valid. The benchmark was tested across 10 leading LLMs including GPT-4o, Claude-3.5-Sonnet, Grok-3, and o3.

## Key Results
- Average REA across all models is only 28%, with most models scoring below 30%
- Average EC is 51%, with top models like Grok-3 and o3 reaching only ~50%
- Models show a clear trade-off between EC and REA, with high-coverage models achieving low accuracy and vice versa
- No model consistently exceeds 50% in either metric, indicating fundamental limitations in scientific reasoning extraction

## Why This Works (Mechanism)
None

## Foundational Learning
- **Reasoning Logic Trees (RLTs)**: Formal graph structures representing scientific reasoning chains, needed to standardize how reasoning is captured and evaluated across different papers and models
- **Deduction/Induction/Abduction classification**: Categorizing reasoning steps into these three logical forms, needed to assess whether models understand different types of scientific inference
- **Entity Coverage (EC)**: Metric measuring the proportion of reasoning entities successfully extracted from text, needed to evaluate completeness of reasoning chain extraction
- **Reasoning Edge Accuracy (REA)**: Metric measuring the proportion of logically valid connections between reasoning entities, needed to evaluate the correctness of assembled reasoning chains
- **Latent reasoning extraction**: The task of uncovering implicit logical connections in scientific text, needed to bridge the gap between natural language and formal reasoning representation

## Architecture Onboarding
- **Component map**: Scientific paper text -> Reasoning entity extraction -> Step classification (deduction/induction/abduction) -> RLT graph construction -> EC/REA evaluation
- **Critical path**: Entity extraction -> Step classification -> Graph assembly, where errors in early stages compound in later stages
- **Design tradeoffs**: Coverage vs accuracy tradeoff, where models that extract more entities tend to make more logical errors in connections
- **Failure signatures**: High EC with low REA indicates models capture entities but fail to link them correctly; low EC with moderate REA suggests conservative extraction but accurate linking of what is found
- **First experiments**: 1) Test model sensitivity to prompt engineering variations, 2) Evaluate domain transfer by testing on papers from different scientific fields, 3) Assess impact of reference inclusion on reasoning extraction performance

## Open Questions the Paper Calls Out
None

## Limitations
- The dataset size of 70 papers limits generalizability across scientific domains and writing styles
- Manual annotation process for creating ground-truth RLTs is labor-intensive and may introduce inconsistencies
- Focus on papers from a single publisher (Nature Communications) constrains representativeness of broader scientific literature

## Confidence
- **High confidence**: The low REA (28%) and EC (51%) scores across all evaluated models are robust findings that indicate a fundamental gap between natural language processing and formal scientific reasoning extraction
- **Medium confidence**: The observed trade-off between EC and REA suggests genuine architectural limitations in current LLMs, though this could also reflect optimization pressures during inference rather than inherent model constraints
- **Medium confidence**: The benchmark's effectiveness in exposing reasoning limitations is validated by the consistent performance gaps across diverse model families, though alternative evaluation frameworks might yield different insights

## Next Checks
1. **Cross-domain validation**: Test ARCHE on scientific papers from diverse publishers and fields (e.g., physics, biology, social sciences) to assess domain transfer and identify field-specific reasoning patterns that may affect model performance
2. **Human-in-the-loop evaluation**: Conduct blind studies where domain experts evaluate model-generated RLTs for scientific validity and completeness, comparing expert assessments with automated EC/REA metrics to validate the benchmark's construct validity
3. **Model architecture ablation**: Systematically test variations of current LLMs (different prompting strategies, fine-tuning on reasoning tasks, or hybrid symbolic-neural approaches) to determine whether the performance ceiling reflects fundamental limitations or suboptimal implementation choices