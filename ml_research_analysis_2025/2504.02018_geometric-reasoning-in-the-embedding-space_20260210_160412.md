---
ver: rpa2
title: Geometric Reasoning in the Embedding Space
arxiv_id: '2504.02018'
source_url: https://arxiv.org/abs/2504.02018
tags:
- points
- which
- constraints
- grid
- variables
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper investigates how neural networks learn to reason about\
  \ geometric constraints by training both Graph Neural Networks (GNNs) and Transformers\
  \ to predict point positions in a 2D grid from sets of geometric constraints. The\
  \ GNN outperforms the Transformer significantly, achieving over 90% accuracy on\
  \ grids up to 80\xD780, while the Transformer struggles beyond 10\xD710 grids."
---

# Geometric Reasoning in the Embedding Space

## Quick Facts
- arXiv ID: 2504.02018
- Source URL: https://arxiv.org/abs/2504.02018
- Reference count: 14
- Key outcome: GNNs achieve over 90% accuracy on 80×80 grids for geometric reasoning while Transformers fail beyond 10×10 grids

## Executive Summary
This paper investigates how neural networks learn to reason about geometric constraints by training both Graph Neural Networks (GNNs) and Transformers to predict point positions in a 2D grid from sets of geometric constraints. The GNN outperforms the Transformer significantly, achieving over 90% accuracy on grids up to 80×80, while the Transformer struggles beyond 10×10 grids. Visualization reveals that both models organize their embeddings into a 2D grid structure that mirrors the problem domain, with the GNN's dynamic embeddings evolving to reflect the hidden geometric figures during inference. Analysis shows prediction errors are correlated with the number of constraint resolution steps required. The study demonstrates that GNNs are more effective and scalable for geometric reasoning tasks, offering insights into the spatial reasoning mechanisms of neural networks.

## Method Summary
The method frames geometric reasoning as a constraint satisfaction problem (CSP) where points must be positioned in a 2D grid based on geometric constraints like squares, translations, and midpoints. The GNN architecture uses a bipartite graph with variable nodes (unknown points) and constraint nodes (geometric relationships). Variable embeddings are updated through iterative message passing with constraint embeddings, while constraint embeddings are updated using LSTM networks that process variable embeddings in order-specific ways. The model classifies each variable's position by computing dot products between dynamic variable embeddings and static grid point embeddings. The Transformer baseline uses GPT-2 architecture with rotary position embeddings. Both models are trained with cross-entropy loss on synthetic CSP instances generated on n×n grids.

## Key Results
- GNN achieves over 90% accuracy on grids up to 80×80, while Transformer struggles beyond 10×10
- Both models spontaneously organize embeddings into 2D grid structures mirroring the problem domain
- Prediction errors strongly correlate with the number of constraint resolution steps required
- GNN's dynamic embeddings evolve during inference to reflect the hidden geometric figures

## Why This Works (Mechanism)

### Mechanism 1: Emergent 2D Grid Structure in Embedding Space
- Claim: Both GNNs and Transformers spontaneously organize point embeddings into a 2D subspace that mirrors the spatial topology of the problem grid
- Mechanism: The cross-entropy loss over grid point classification, combined with shared weights between the embedding layer and classification head, creates pressure for embeddings to preserve neighborhood structure. The inner product similarity used for prediction favors spatially-organized representations
- Core assumption: The geometric constraints provide implicit relational inductive bias that guides the embedding organization toward spatially-coherent structure
- Evidence anchors:
  - [abstract]: "both models recover the grid structure during training so that the embeddings corresponding to the points within the grid organize themselves in a 2D subspace and reflect the neighborhood structure of the grid"
  - [section 4.2]: "When visualizing low-dimensional projection of the static embeddings corresponding to individual points, we found that they organize themselves into a 2D grid they represent"
  - [corpus]: Limited direct evidence; corpus papers focus on graph embeddings for other domains (traffic, power grids, 3D printing) without addressing spontaneous spatial organization
- Break condition: If training data lacks sufficient constraint diversity or grid coverage, embeddings may collapse to unstructured clusters rather than forming the 2D grid topology

### Mechanism 2: Iterative Constraint Propagation via Recurrent Message Passing
- Claim: The GNN solves geometric CSPs through iterative refinement where dynamic embeddings converge toward configurations satisfying the constraint graph
- Mechanism: Each message-passing round updates variable embeddings based on aggregated constraint embeddings. Constraint embeddings encode relational requirements (e.g., "b is midpoint of a and c"), and recurrent application propagates information through the dependency DAG. The visualization shows progressive refinement from approximate to exact solutions
- Core assumption: The constraint dependency DAG structure can be learned implicitly through recurrent updates without explicit topological ordering
- Evidence anchors:
  - [abstract]: "GNN's dynamic embeddings evolving to reflect the hidden geometric figures during inference"
  - [section 4.2.1]: "the GNN first finds a close approximation to the hidden configuraion and then refines it. Here, the squares are first 'approximated' by quadrilaterals which then converge to exact squares"
  - [corpus]: No direct corpus support for iterative refinement in geometric reasoning; nearby papers focus on single-pass spatial reasoning or physical simulation
- Break condition: If the constraint graph has too many resolution steps (>8 per variable) or too many constraints (>10 total), the iterative process accumulates errors and fails to converge

### Mechanism 3: GNN Scalability Through Symmetry Elimination
- Claim: The bipartite graph structure (variables-constraints) eliminates permutation symmetries that plague sequence-based Transformers, enabling superior scalability
- Mechanism: GNNs treat variables and constraints as nodes in a bipartite graph, making the model invariant to constraint ordering and variable renaming. Transformers must learn these symmetries from data, which requires exponentially more samples as grid size increases
- Core assumption: The performance gap stems primarily from architectural inductive biases rather than training procedure differences
- Evidence anchors:
  - [section 1]: "a GNN, which eliminates a lot of symmetries (variable renaming and constraint reordering), could be easier to train and be more scalable"
  - [section 4.1]: "GNN performs significantly better than the Transformer... we were able to train the GNN on grid sizes up to 80×80 points to a validation accuracy larger than 90%. In comparison, the Transformer achieved accuracy of 90% only if we train it on a grid size 10×10"
  - [corpus]: Weak support; corpus papers apply GNNs to spatial problems but don't compare against Transformers or analyze symmetry properties
- Break condition: If constraints require complex relational patterns not captured by the bipartite message-passing structure, the GNN's inductive bias becomes a limitation rather than an advantage

## Foundational Learning

- Concept: Constraint Satisfaction Problems (CSP) with dependency graphs
  - Why needed here: The entire task is framed as resolving geometric constraints organized in a DAG; understanding how constraints propagate through dependencies is essential for debugging failure modes
  - Quick check question: Given constraints M(a,b,c) (midpoint) and T(c,d,e,f) (translation), which variables must be known before resolving 'f'?

- Concept: Bipartite Graph Neural Networks with heterogeneous node types
  - Why needed here: The architecture uses separate update functions for variable nodes and constraint nodes; the message-passing pattern fundamentally differs from homogeneous GNNs
  - Quick check question: In a bipartite GNN with variables and constraints, how many hops are needed for information to travel from one variable to another?

- Concept: Embedding space structure and inner product similarity
  - Why needed here: The classification mechanism relies on cosine-like similarity between dynamic embeddings and static point embeddings; visualization uses projection to reveal learned structure
  - Quick check question: If static embeddings form a 2D grid and a dynamic embedding has high dot product with point [5,10], what does this predict?

## Architecture Onboarding

- Component map:
  Static embeddings -> Dynamic embeddings -> Classification head
  Constraint embeddings (LSTM) -> Variable update LSTM -> Dynamic embeddings

- Critical path:
  1. Initialize known variable embeddings via shared embedding layer (frozen during forward pass)
  2. Initialize unknown variable embeddings as random unit vectors
  3. For T iterations: update constraint embeddings (via constraint-specific LSTMs), then update variable embeddings (via shared LSTM)
  4. Apply classification head to final variable embeddings
  5. Cross-entropy loss against ground-truth grid positions

- Design tradeoffs:
  - LSTM vs simple RNN for updates: LSTMs easier to optimize but more parameters
  - 20 iterations: Balances resolution depth vs computational cost; errors increase beyond 8 resolution steps
  - Sum aggregation for variables vs concatenation for constraints: Order matters for constraints (determining vs dependent variables), not for variables
  - Grid discretization: Enables classification loss but limits applicability; regression formulation would require different architecture

- Failure signatures:
  - Accuracy drops sharply when number of constraint types increases (67.7% → 38% complete accuracy going from 2 to 4 constraint types on 20×20 grid)
  - Error rate correlates monotonically with resolution steps (Figure 3a shows ~60% failure at 7 steps)
  - Incorrect predictions often land on neighboring grid points (Figure 6 histogram)
  - Transformer fails to scale beyond 10×10 grids; chain-of-thought training helps but remains inferior

- First 3 experiments:
  1. Reproduce the visualization in Figure 1 on a 15×15 grid: train GNN for 100 epochs, project static embeddings with UMAP/PCA at epochs 1, 5, 15, 50, 100, verify grid structure emerges
  2. Measure resolution-step correlation: generate 1000 problems, bin predictions by