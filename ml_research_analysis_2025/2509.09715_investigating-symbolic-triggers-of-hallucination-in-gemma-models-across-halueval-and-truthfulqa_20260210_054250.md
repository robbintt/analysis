---
ver: rpa2
title: Investigating Symbolic Triggers of Hallucination in Gemma Models Across HaluEval
  and TruthfulQA
arxiv_id: '2509.09715'
source_url: https://arxiv.org/abs/2509.09715
tags:
- symbolic
- hallucination
- across
- named
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper investigates symbolic triggers of hallucination in
  Gemma models by systematically converting established datasets (HaluEval and TruthfulQA)
  into three task formats (QA, MCQ, and Odd-One-Out) to isolate five symbolic properties:
  modifiers, named entities, numbers, negation, and exceptions. The study evaluates
  Gemma-2-2B, 9B, and 27B models across these formats, revealing that hallucination
  rates remain substantially high across all model sizes, with modifiers (84.76-94.98%)
  and named entities (83.87-93.96%) being the most problematic symbolic properties.'
---

# Investigating Symbolic Triggers of Hallucination in Gemma Models Across HaluEval and TruthfulQA

## Quick Facts
- arXiv ID: 2509.09715
- Source URL: https://arxiv.org/abs/2509.09715
- Authors: Naveen Lamba; Sanju Tiwari; Manas Gaur
- Reference count: 26
- Key outcome: This paper investigates symbolic triggers of hallucination in Gemma models by systematically converting established datasets (HaluEval and TruthfulQA) into three task formats (QA, MCQ, and Odd-One-Out) to isolate five symbolic properties: modifiers, named entities, numbers, negation, and exceptions. The study evaluates Gemma-2-2B, 9B, and 27B models across these formats, revealing that hallucination rates remain substantially high across all model sizes, with modifiers (84.76-94.98%) and named entities (83.87-93.96%) being the most problematic symbolic properties. Attention analysis shows that task format affects symbolic token focus, with QA producing higher hallucination than MCQ despite higher symbolic attention. The findings demonstrate that symbolic confusion is a fundamental architectural issue rather than a capacity problem, as even the largest models show persistent vulnerability to these triggers, particularly in shorter to medium-length prompts (10-30 tokens).

## Executive Summary
This paper systematically investigates how specific symbolic properties in language trigger hallucinations in Gemma models by converting HaluEval and TruthfulQA datasets into three distinct task formats. The study isolates five symbolic properties—modifiers, named entities, numbers, negation, and exceptions—and evaluates their impact on hallucination across Gemma-2-2B, 9B, and 27B models. Results reveal that hallucination rates remain persistently high across all model sizes, with modifiers and named entities being the most problematic triggers. Attention analysis demonstrates that task format significantly affects symbolic token focus, and that these vulnerabilities are particularly pronounced in shorter to medium-length contexts. The findings suggest that symbolic confusion represents a fundamental architectural limitation rather than a capacity issue, as even the largest models show only marginal improvement.

## Method Summary
The study systematically converts HaluEval and TruthfulQA datasets into three task formats (QA, MCQ, and Odd-One-Out) to isolate five symbolic properties: modifiers, named entities, numbers, negation, and exceptions. For each dataset, 100 samples containing target symbolic properties are selected and annotated using NER followed by manual verification. Each sample is converted into three formats using specific prompts. The study evaluates Gemma-2-2B, 9B, and 27B models with default temperature settings, analyzing attention at specific layers (10, 20 for 2B; 20, 31 for 9B; 23, 40 for 27B). Hallucination is measured as factual incorrectness in outputs, and results are analyzed by symbolic property and input token length bins.

## Key Results
- Hallucination rates remain substantially high across all Gemma model sizes, with only marginal improvement from 2B to 27B
- Modifiers (84.76-94.98%) and named entities (83.87-93.96%) are the most problematic symbolic properties across all models
- QA format produces higher hallucination than MCQ despite higher symbolic attention, indicating format affects reasoning patterns
- Symbolic confusion vulnerability is particularly evident in shorter to medium-length prompts (10-30 tokens)

## Why This Works (Mechanism)

### Mechanism 1: Symbolic Token Induced Instability
- **Claim:** Specific linguistic classes—modifiers and named entities—act as primary triggers for hallucination due to their high semantic density and reliance on precise parametric knowledge.
- **Mechanism:** Modifiers (e.g., "rapidly") introduce interpretive ambiguity, inviting plausible but ungrounded completions. Named entities rely on robust memorization; when training data is sparse or entangled, the model generates confabulated associations.
- **Core assumption:** Hallucination rates are correlated with the presence of these specific symbolic tokens rather than general sentence complexity.
- **Evidence anchors:** Modifiers (84.76-94.98%) and named entities (83.87-93.96%) show the highest hallucination ranges across all models. SymLoc supports the localization of hallucinations triggered by these specific symbolic properties.
- **Break condition:** If the model has access to external retrieval (RAG) that grounds the named entity or modifier, the mechanism may not trigger.

### Mechanism 2: Scale-Invariant Representational Fragility
- **Claim:** Symbolic confusion is a structural limitation of the transformer architecture that persists despite significant parameter scaling.
- **Mechanism:** Increasing model capacity (2B → 27B) reduces error rates marginally (15 percentage points), but fails to resolve the fundamental inability to process symbolic logic correctly. This suggests the issue lies in the information encoding or attention mechanism logic rather than storage capacity.
- **Core assumption:** Larger models primarily add capacity for pattern storage, not necessarily logical verification of symbolic constraints.
- **Evidence anchors:** Hallucination drops only to 63.9% in the largest model, indicating persistent vulnerability. The authors note that vulnerabilities are "fundamental architectural issues rather than artifacts of specific experimental conditions."
- **Break condition:** If architectural changes (e.g., explicit symbolic reasoning modules) are introduced, scaling laws might change.

### Mechanism 3: Context Length Non-Linearity
- **Claim:** Symbolic triggers behave non-monotonically relative to input length, with hallucinations peaking in short-to-medium contexts (10-30 tokens) where grounding is minimal.
- **Mechanism:** In short prompts, symbolic tokens receive less contextual "dilution" or support, leading to local representational instability. Longer contexts (40+ tokens) appear to provide sufficient surrounding text to stabilize the interpretation of the symbolic trigger.
- **Core assumption:** The model uses context length to average out or verify the signal from specific tokens.
- **Evidence anchors:** Vulnerability is particularly evident in shorter to medium-length prompts (10-30 tokens). Table 3 shows hallucination rates for modifiers and entities often decline or behave unpredictably as token counts rise above 40.
- **Break condition:** If the longer context introduces new conflicting symbolic triggers, the stabilization effect may reverse.

## Foundational Learning

- **Concept:** **Mid-to-Deep Layer Semantic Integration**
  - **Why needed here:** The paper analyzes specific layers (e.g., 20-40 in Gemma-27B) rather than the final output, assuming these layers are where symbolic confusion manifests.
  - **Quick check question:** Can you explain why attention heads in the middle layers of a transformer are often more indicative of semantic reasoning than the final layer?

- **Concept:** **Symbolic vs. Sub-symbolic Representation**
  - **Why needed here:** The study isolates "symbolic properties" (modifiers, negation) as distinct triggers, contrasting them with general language fluency.
  - **Quick check question:** How does a standard tokenizer treat the negation "not" differently from a semantic vector space, and why might this disconnect cause hallucination?

- **Concept:** **Parametric Knowledge Access**
  - **Why needed here:** Hallucination in Named Entities is linked to the model's inability to verify internal memories against the prompt context.
  - **Quick check question:** What is the difference between a model retrieving a fact from its weights (parametric) vs. copying it from the context (in-context learning)?

## Architecture Onboarding

- **Component map:** HaluEval/TruthfulQA datasets → Symbolic property annotation → Format conversion (QA/MCQ/OOO) → Gemma-2 family models → Attention analysis at specific layers → Hallucination metrics
- **Critical path:** Identify prompts containing target symbolic properties → Map attention scores of symbolic tokens at mid-to-deep layers → Correlate attention shifts with hallucination frequency across input lengths
- **Design tradeoffs:** 
  - Task Format: QA format exposes maximum hallucination (open generation) vs. MCQ (constrained) vs. OOO (semantic reasoning)
  - Layer Selection: Analyzing only mid-to-deep layers captures semantic integration but may miss early-layer token injection errors
- **Failure signatures:** High Hallucination: Modifiers (e.g., "most rapidly") and Entities in 10-30 token prompts. Attention Pattern: Sharp drops in symbolic attention between early and late layers correlate with loss of factual grounding.
- **First 3 experiments:**
  1. **Baseline Profiling:** Run 100 samples from HaluEval through Gemma-2B in QA format; tag outputs containing "Modifiers" to verify the >80% hallucination baseline claimed in the paper.
  2. **Attention Mapping:** Extract attention weights for a specific "Named Entity" token at Layers 10 and 20. Compare the attention score when the entity is real vs. fictional.
  3. **Length Stress Test:** Take a high-hallucination prompt (20 tokens) and pad it to 50 tokens with neutral context. Measure if the hallucination rate drops as suggested by Table 3.

## Open Questions the Paper Calls Out

- **Open Question 1:** Do symbolic triggers like modifiers and named entities cause hallucinations in model architectures outside the Gemma family?
  - **Basis in paper:** The authors state that future work must "systematically validate these symbolic vulnerabilities across different model families (LLaMA, Mistral, GPT)."
  - **Why unresolved:** The current study is restricted to the Gemma family (2B, 9B, 27B), leaving the universality of these findings unproven.
  - **What evidence would resolve it:** Replicating the experimental protocol (using HaluEval/TruthfulQA transformations) on non-Gemma architectures to compare hallucination rates.

- **Open Question 2:** Which specific transformer layers and attention heads are mechanistically responsible for symbolic confusion?
  - **Basis in paper:** The authors propose using "activation patching and causal intervention techniques to precisely localize which transformer layers and attention heads are responsible."
  - **Why unresolved:** Current analysis identifies aggregate attention patterns but does not pinpoint the exact internal causal mechanisms or specific "guilty" components.
  - **What evidence would resolve it:** Results from causal tracing or activation patching experiments that show specific interventions reducing symbolic hallucination.

- **Open Question 3:** Can prompt-based interventions effectively mitigate symbolic hallucination at inference time?
  - **Basis in paper:** The authors suggest "exploring prompt-based interventions may offer practical mitigation strategies by reducing symbolic ambiguity."
  - **Why unresolved:** The paper focuses on identifying and analyzing triggers rather than testing specific defense strategies.
  - **What evidence would resolve it:** Comparative studies showing that specific prompt restructuring lowers the high hallucination rates associated with modifiers and named entities.

## Limitations

- The symbolic property annotation methodology lacks detailed criteria and inter-annotator agreement metrics, creating potential noise in trigger attribution
- The conversion methodology for MCQ and Odd-One-Out formats is underspecified, with no details on distractor generation or item selection
- Attention analysis is limited to fixed layers without explaining the rationale for specific layer choices or examining full depth trends

## Confidence

**High Confidence:**
- The fundamental finding that hallucination rates remain high across all Gemma model sizes for symbolic triggers
- The scale-invariant nature of symbolic confusion
- The observation that shorter-to-medium contexts exhibit peak hallucination vulnerability

**Medium Confidence:**
- The specific attention layer selections and their correlation with hallucination patterns
- The relative ranking of task formats in triggering hallucination
- The precise magnitude of hallucination percentages per symbolic property

**Low Confidence:**
- The causal relationship between attention score changes and hallucination occurrence
- The generalizability of findings beyond Gemma models to other transformer architectures
- The robustness of symbolic property categorization across different annotation schemas

## Next Checks

1. **Annotation Reliability Test:** Select 50 random samples from the test set and have three independent annotators label them for all five symbolic properties. Compute Cohen's kappa to establish inter-annotator agreement and assess whether the reported hallucination triggers are consistent across human judgments.

2. **Format Consistency Validation:** For a subset of 20 high-hallucination prompts, manually verify that the MCQ distractors and Odd-One-Out items were constructed consistently with the intended symbolic property isolation. Test whether reversing the format (converting MCQ back to QA) reproduces the expected hallucination patterns.

3. **Attention Pattern Replication:** Extract attention weights for the same symbolic token across all transformer layers (not just the selected ones) for 10 different prompts. Plot attention score trajectories from layer 1 to final layer to verify whether the reported mid-to-deep layer focus is consistent or varies significantly across different trigger types.