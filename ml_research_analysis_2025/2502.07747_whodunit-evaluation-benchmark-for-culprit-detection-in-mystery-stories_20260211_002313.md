---
ver: rpa2
title: 'WHODUNIT: Evaluation benchmark for culprit detection in mystery stories'
arxiv_id: '2502.07747'
source_url: https://arxiv.org/abs/2502.07747
tags:
- story
- arthur
- conan
- doyle
- short
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces WHODUNIT, a novel dataset for evaluating deductive
  reasoning in large language models (LLMs) using mystery stories. The dataset includes
  original stories and augmented versions with character name substitutions (e.g.,
  Harry Potter, Hollywood, and Bollywood celebrities) to test model robustness.
---

# WHODUNIT: Evaluation benchmark for culprit detection in mystery stories

## Quick Facts
- arXiv ID: 2502.07747
- Source URL: https://arxiv.org/abs/2502.07747
- Reference count: 5
- Key result: LLMs achieve high accuracy on original mystery stories but performance drops with character name substitutions, indicating reliance on pre-training knowledge rather than pure reasoning.

## Executive Summary
WHODUNIT is a novel benchmark for evaluating deductive reasoning in large language models using mystery stories. The dataset includes original stories and augmented versions with character name substitutions (Harry Potter, Hollywood, and Bollywood celebrities) to test model robustness. Three state-of-the-art models (GPT-4o, GPT-4-turbo, GPT-4o-mini) were evaluated using multiple prompting techniques, including Chain-of-Thought and Self-Reflection. Results show that models achieve high accuracy on original texts but performance drops with name swaps, suggesting reliance on pre-training knowledge. Chain-of-Thought and Self-Reflection prompting significantly improve accuracy across all model variants.

## Method Summary
The study evaluates OpenAI models (GPT-4o, GPT-4-turbo, GPT-4o-mini) on mystery stories from Project Gutenberg, using four prompting styles: Basic, Self-Reflection, Chain-of-Thought, and CoT + Self-Reflection. Each configuration runs 10 independent API calls with majority voting for final answers. The dataset includes original stories and name-swapped augmentations (Harry Potter, Hollywood, Bollywood) to test model robustness. Stories vary in length (short/medium/full), and ground truth culprit labels are obtained via CliffsNotes or manual reading. Performance is measured as accuracy of culprit identification across models, augmentations, prompting techniques, and document lengths.

## Key Results
- GPT-4o and GPT-4-turbo achieve 82.7% and 83.5% accuracy on original stories, while GPT-4o-mini scores 74.1%
- Performance drops significantly with full character name swaps but improves with culturally familiar names (Harry Potter, celebrities)
- Chain-of-Thought and Self-Reflection prompting improve accuracy by guiding structured reasoning
- Larger models maintain accuracy better with increasing document lengths, while GPT-4o-mini shows pronounced decline

## Why This Works (Mechanism)

### Mechanism 1: Pre-training Association Disruption Reveals Memorization
Performance drops when character names are swapped because models partially rely on memorized associations from pre-training rather than pure contextual reasoning. When original names are preserved, models may activate pre-trained knowledge about famous mysteries. Name substitutions disrupt these associations, forcing context-dependent reasoning. The recovery of accuracy with highly familiar names suggests pre-training priors can both help and hurt depending on alignment with task context.

### Mechanism 2: Structured Prompting Forces Explicit Reasoning Steps
Chain-of-Thought and Self-Reflection prompting improve culprit detection by decomposing inference and adding verification loops. CoT prompts the model to articulate intermediate deductions (tracking alibis, motives, contradictions). Self-Reflection adds a meta-check where the model reviews its conclusion. Both reduce premature guessing and increase reasoning coherence.

### Mechanism 3: Context-Length Capacity Determines Long-Narrative Performance
Larger models (GPT-4o, GPT-4-turbo) maintain accuracy over longer documents because they better track entities and plot state across extended sequences. Mystery narratives require maintaining character relationships, alibis, and clues over many pages. Smaller models lose track of earlier context, leading to incorrect deductions.

## Foundational Learning

- **Concept: Chain-of-Thought Prompting**
  - Why needed here: Essential to understand why accuracy improves when models reason step-by-step rather than guessing immediately.
  - Quick check question: Can you explain why listing intermediate deductions before a final answer might reduce errors?

- **Concept: Memorization vs. Generalization in LLMs**
  - Why needed here: The paper's core design tests whether models truly reason or rely on memorized story knowledge.
  - Quick check question: If a model has seen a mystery novel during pre-training, how could you test whether it "remembers" the culprit versus deducing it?

- **Concept: Self-Consistency / Majority Voting**
  - Why needed here: The paper uses 10-shot prompting with majority response selection to reduce output variance.
  - Quick check question: Why would running the same prompt 10 times and taking the most frequent answer improve reliability?

## Architecture Onboarding

- **Component map:** Data source (Project Gutenberg) -> Story selection -> Ground-truth verification -> Name augmentation (original, swapped, Harry Potter, Hollywood, Bollywood) -> Prompt construction -> Batch API inference (10 calls) -> Majority vote -> Accuracy computation

- **Critical path:** 1. Story selection → 2. Ground-truth culprit verification → 3. Name augmentation → 4. Prompt construction → 5. Batch API inference (10 calls) → 6. Majority vote → 7. Accuracy computation

- **Design tradeoffs:** Public-domain stories enable open release but introduce memorization confounds; 10-call majority voting reduces variance but increases API cost and latency; multiple augmentation types increase diagnostic power but complicate root-cause analysis

- **Failure signatures:** High original accuracy + low swapped-name accuracy → memorization signal; high variance across 10 calls → unstable or inconsistent reasoning; accuracy cliff at longer document lengths → context retention failure

- **First 3 experiments:**
  1. Replicate original vs. full-name-swap comparison on 5 stories to verify memorization effect locally.
  2. Compare Basic vs. CoT prompting on a single medium-length story to observe reasoning decomposition in outputs.
  3. Test GPT-4o-mini on short vs. long stories to reproduce the context-length performance gap.

## Open Questions the Paper Calls Out

- How does LLM deductive reasoning accuracy degrade or persist when scaling from short stories to full-length novels? The study is limited to short and medium-length stories due to context length constraints, leaving models' ability to maintain logical coherence over hundreds of pages untested.

- Can model architectures or training techniques be modified to prevent accuracy fluctuations caused by the presence of famous entity names? The study identifies that models leverage familiarity with names to assist reasoning but does not propose methods to force reliance solely on narrative logic.

- Do the benefits of Chain-of-Thought and Self-Reflection prompting for deductive reasoning generalize to open-source models outside the GPT-4 family? The experimental setup relies exclusively on the GPT-4 family, making it unclear if accuracy boosts are unique to OpenAI architectures or universally applicable.

## Limitations

- Results may not generalize to other reasoning-intensive domains like scientific reasoning or logical proofs, as the benchmark specifically tests narrative-based deductive reasoning.
- Name substitution experiments provide suggestive evidence of memorization but cannot definitively separate memorized plot knowledge from pattern-based reasoning.
- The study shows prompting improvements but does not systematically explore prompt template variations, which could significantly impact accuracy patterns.

## Confidence

- **High confidence:** Performance differences between original and swapped-name conditions reliably indicate memorization effects.
- **Medium confidence:** Chain-of-Thought prompting provides consistent accuracy improvements, though exact magnitude may depend on prompt implementation.
- **Medium confidence:** Larger models demonstrate better context retention for longer documents, but could be influenced by other architectural differences.
- **Low confidence:** Accuracy improvements with culturally familiar names definitively prove pre-training knowledge activation.

## Next Checks

1. Create synthetic mystery stories with identical plots but completely novel character names and settings to isolate pre-training knowledge effects from general reasoning ability.

2. Systematically vary prompt phrasing, instruction detail, and reasoning scaffolding while keeping model and story constant to identify which prompt elements drive the largest accuracy improvements.

3. Apply the same evaluation framework to non-narrative reasoning tasks (mathematical proofs, scientific problem-solving) to assess the benchmark's domain specificity and test whether CoT prompting effects replicate.