---
ver: rpa2
title: On Biologically Plausible Learning in Continuous Time
arxiv_id: '2510.18808'
source_url: https://arxiv.org/abs/2510.18808
tags:
- error
- learning
- plas
- time
- input
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a continuous-time neural network model that
  unifies several biologically plausible learning algorithms. The key idea is that
  learning emerges from the temporal overlap between inputs and error signals, eliminating
  the need for phase separation.
---

# On Biologically Plausible Learning in Continuous Time

## Quick Facts
- arXiv ID: 2510.18808
- Source URL: https://arxiv.org/abs/2510.18808
- Reference count: 21
- Primary result: Effective learning requires plasticity windows to exceed stimulus duration by one to two orders of magnitude, placing functional eligibility in the few-second range—a testable prediction for biological circuits.

## Executive Summary
This paper proposes a continuous-time neural network model that unifies several biologically plausible learning algorithms. The key idea is that learning emerges from the temporal overlap between inputs and error signals, eliminating the need for phase separation. SGD, FA, DFA, and KP rules appear as limiting cases when their respective assumptions hold. Simulations show robust learning at biological timescales, with synaptic plasticity timescales requiring seconds-scale eligibility traces.

## Method Summary
The method implements coupled first-order ODEs where neural states (z), forward weights (W), and feedback weights (V) evolve simultaneously without phase separation. Timescale separation is critical: τ_prop ≪ τ_plas ≪ τ_dec, with typical values τ_prop=5-30ms, τ_plas=1.45-10s, τ_dec≈20min. Learning occurs through heterosynaptic updates where weights change based on local input times error projections, weighted by an exponential plasticity kernel. The model uses JAX + Diffrax with Tsit5 solver, Xavier initialization for W, constant V=0.1, and ReLU activations. Two error routing topologies are explored: direct error broadcast and layerwise propagation.

## Key Results
- Learning requires temporal overlap between presynaptic input and error signals at each synapse
- Classical algorithms (SGD, FA, DFA, KP) emerge as limiting cases under timescale separation
- Robust learning requires plasticity windows of ~1-10 seconds, exceeding typical cortical stimuli by 1-2 orders of magnitude

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Learning requires temporal overlap between presynaptic input and error signals at each synapse.
- Mechanism: The weight update integrates the cross-correlation between input z_l-1(t) and error-projection V^T_l ε_l(t), weighted by an exponential plasticity kernel k_τ(t). When input and error windows overlap partially, the effective update scales as (T - |Δ|)_+ (triangular overlap law).
- Core assumption: τ_plas >> T (plasticity timescale far exceeds stimulus duration), creating a "flat kernel" regime where the eligibility trace persists throughout the sample.
- Evidence anchors:
  - [abstract] "a synapse updates correctly only when its input and the corresponding error signal coincide in time"
  - [Section 4] Eq. 5: "learning succeeds if and only if input and error overlap in time, and it degrades sharply as |Δ| → T"
  - [corpus] "Backpropagation through space, time, and the brain" addresses spatio-temporal locality but does not derive the overlap law directly
- Break condition: If delay |Δ| ≥ T, overlap is zero and learning fails; deeper networks accumulate propagation lag, tightening this constraint for early layers.

### Mechanism 2
- Claim: Classical algorithms (SGD, FA, DFA, KP) emerge as limiting cases under timescale separation and constraint regimes.
- Mechanism: When τ_prop << τ_plas << τ_dec, neural states equilibrate quasi-instantaneously. The weight dynamics then reduce to discrete-like updates. SGD emerges if V_l = W_l^T (weight transport); FA emerges if V_l is fixed random; DFA emerges if error is broadcast directly; KP emerges if V_l evolves via the heterosynaptic rule, driving V_l → W_l^T dynamically.
- Core assumption: Timescale separation holds; the system operates in the quasi-stationary regime where fast variables reach equilibrium before slow weights update significantly.
- Evidence anchors:
  - [abstract] "SGD, FA, DFA, and KP rules appear as limiting cases when their respective assumptions hold"
  - [Section 3] "Under separation of timescales... weight dynamics reduce to the heterosynaptic updates"
  - [corpus] Related work on feedback alignment (Lillicrap et al.) is cited but does not address continuous-time derivation
- Break condition: If τ_plas approaches τ_prop, the quasi-static approximation fails; weight updates occur before neural states equilibrate, corrupting the gradient signal.

### Mechanism 3
- Claim: Robust learning requires plasticity windows (eligibility traces) of ~1-10 seconds, exceeding typical cortical stimuli (~50 ms) by 1-2 orders of magnitude.
- Mechanism: The plasticity kernel k_τ(t) = exp(-(T-t)/τ_plas) must remain approximately flat over the stimulus window to avoid attenuating early-overlap contributions. For τ_plas/T ≈ 40, attenuation is ~2.5%; below this, learning becomes unstable.
- Core assumption: Cortical stimuli are tens of milliseconds; the induction gate for coincidence-gated plasticity (via second-messenger cascades) supports seconds-scale eligibility.
- Evidence anchors:
  - [abstract] "synaptic plasticity timescales requiring seconds-scale eligibility traces"
  - [Section 5.3] Fig. 7 shows learning stabilizes only when τ_plas ≳ 2 s for T = 50 ms
  - [corpus] Yagishita et al. (2014) is cited for eligibility trace timing but is external to the corpus; corpus evidence on this specific timescale is weak
- Break condition: If τ_plas approaches T, the kernel becomes sharply non-uniform, biasing updates toward late-arriving errors and destabilizing learning.

## Foundational Learning

- Concept: **Ordinary Differential Equations (ODEs) and dynamical systems**
  - Why needed here: The entire model is formulated as coupled first-order ODEs; understanding equilibrium, timescale separation, and integration is essential.
  - Quick check question: Can you explain why τ_prop << τ_plas implies z_l(t) tracks its steady-state quickly relative to weight changes?

- Concept: **Eligibility traces and three-factor learning rules**
  - Why needed here: The plasticity kernel functions as an eligibility trace; learning combines presynaptic activity, modulatory error, and trace persistence.
  - Quick check question: If an eligibility trace decays in 100 ms, what happens to learning when the error arrives 200 ms after the input?

- Concept: **Feedback Alignment variants (FA, DFA, KP)**
  - Why needed here: The paper unifies these as special cases; understanding their constraints clarifies what the continuous-time model relaxes.
  - Quick check question: What constraint distinguishes FA (fixed random V) from KP (learned V)?

## Architecture Onboarding

- Component map:
  - Input x → weights W → activation σ → output z (forward pathway with τ_prop dynamics)
  - Output error e_L → modulatory weights V → local error ε_l at each layer (error pathway)
  - W updates via heterosynaptic rule (input × error-projection), V updates via correlation with forward drive
  - Passive weight decay on τ_dec timescale
  - Topology variants: Layerwise error propagation (KP-style) vs. direct error broadcast (DFA-style)

- Critical path:
  1. Initialize W (Xavier), V (small constant, e.g., 0.1)
  2. For each sample: present input for duration T with smooth buffer transitions
  3. Integrate coupled ODEs forward-in-time using adaptive solver (Tsit5 recommended)
  4. Read prediction at end of sample window; compute error gradient at output
  5. Error propagates backward (layerwise) or broadcasts (direct) during next sample
  6. Weights evolve continuously; no separate inference/learning phases

- Design tradeoffs:
  - **Depth vs. delay tolerance**: Deeper networks accumulate propagation lag, requiring longer T or shorter paths (skip connections)
  - **τ_plas vs. stability**: Larger τ_plas improves robustness but slows adaptation; empirical sweet spot is τ_plas/T ≈ 40
  - **Topology choice**: Direct error routing tolerates delay better; layerwise routing may achieve better gradient alignment if V learns

- Failure signatures:
  - Accuracy ~10% (chance): |Δ| ≥ T, zero overlap; weights update only on spurious correlations
  - Accuracy ~70% with high variance: Partial overlap; correct updates mixed with mismatched periods
  - Learning instability at depth: Early layers receive degraded error signal due to accumulated lag
  - Numerical stiffness: Solver requires very small steps; ensure rtol/atol are set appropriately

- First 3 experiments:
  1. **Single-layer network on MNIST 7×7**: Sweep τ_plas from 0.1 s to 10 s with T = 50 ms; confirm learning stabilizes near τ_plas ≳ 2 s. This validates the timescale constraint in the simplest setting.
  2. **Delay sensitivity sweep**: Fix τ_plas = 10 s, T = 100 ms; vary error delay Δ from -0.1 s to +0.1 s. Plot accuracy vs. Δ/T to verify the triangular overlap law (Eq. 5) and symmetry in the flat-kernel regime.
  3. **Depth vs. sample duration**: Train 1, 2, and 3 hidden-layer networks on the circles dataset with layerwise error propagation. Sweep T from 20 ms to 250 ms; confirm deeper networks require longer T to maintain overlap in early layers.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do biological circuits exhibit seconds-scale eligibility traces (2-10s) as required by the temporal overlap constraint for error-driven learning?
- Basis in paper: [explicit] The paper states this is "a testable prediction that identifies seconds-scale eligibility traces as necessary for error-driven learning in biological circuits" and that experimentalists can "directly probe by manipulating trace duration or disrupting long-range feedback pathways."
- Why unresolved: The prediction emerges from the model but has not been experimentally validated in actual neural circuits.
- What evidence would resolve it: Electrophysiological experiments measuring the duration of eligibility traces during learning tasks, or causal manipulations that extend/shorten trace duration and observe effects on learning.

### Open Question 2
- Question: How do spiking dynamics and discrete spike-based coding affect the temporal overlap requirements and plasticity timescale constraints?
- Basis in paper: [explicit] The authors acknowledge "the lack of spikes is a limitation that excludes important timing effects and spike-based coding strategies" and that their rate-based model is a simplification.
- Why unresolved: The current model uses rate-based neurons; spike timing and refractory dynamics may alter the overlap calculations.
- What evidence would resolve it: Simulations of the continuous-time learning framework using spiking neuron models, comparing plasticity timescale requirements and learning performance.

### Open Question 3
- Question: Can an explicit eligibility gate mechanism reproduce the causal asymmetry observed in biological plasticity while preserving learning stability?
- Basis in paper: [explicit] The paper notes that "incorporating an explicit eligibility gate would recover this asymmetry without altering our conclusions about temporal overlap and timescale separation; we leave such extensions for future work."
- Why unresolved: The current model compresses early/late error asymmetry into a symmetric overlap law, while biology shows causal gating.
- What evidence would resolve it: Extended model incorporating eligibility traces that must be written before modulatory signals arrive, tested for learning performance and biological plausibility.

### Open Question 4
- Question: How does learning scale to deeper networks and more complex tasks given the accumulated propagation delays?
- Basis in paper: [inferred] Computational constraints limited experiments to small datasets. The paper notes "deeper networks accumulate propagation delays that reduce overlap in early layers, degrading learning" and mentions architectural shortcuts as a potential biological solution.
- Why unresolved: It is unclear whether the framework remains viable for modern deep network scales without shortcuts or whether alternative architectures are necessary.
- What evidence would resolve it: Experiments on larger datasets with skip connections and alternative topologies, measuring how architectural modifications mitigate depth-related overlap loss.

## Limitations
- The empirical validation of seconds-scale eligibility trace requirement in biological circuits is weak, with limited biological evidence in the corpus
- The extreme timescale separation (τ_prop ≪ τ_plas ≪ τ_dec spanning six orders of magnitude) may be biologically unrealistic
- Error delay tolerance decreases sharply with network depth, potentially limiting practical applicability to deeper architectures

## Confidence
- High confidence: Core mathematical framework and ODE formulation (Mechanism 1 and 2)
- Medium confidence: Unified treatment of SGD, FA, DFA, and KP as limiting cases (Mechanism 2)
- Medium confidence: Specific numerical predictions for eligibility trace timescales (Mechanism 3)

## Next Checks
1. Test the τ_plas/T ≈ 40 threshold empirically across multiple network architectures and datasets to confirm the universality of this timescale constraint
2. Validate the error delay tolerance experimentally by systematically varying Δ/T and measuring accuracy degradation, particularly in deeper networks
3. Examine biological plausibility by reviewing recent literature on second-messenger cascade timescales and dendritic integration windows to assess whether seconds-scale eligibility traces are feasible in cortical circuits