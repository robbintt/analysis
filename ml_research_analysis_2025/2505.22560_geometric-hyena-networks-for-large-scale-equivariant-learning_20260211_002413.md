---
ver: rpa2
title: Geometric Hyena Networks for Large-scale Equivariant Learning
arxiv_id: '2505.22560'
source_url: https://arxiv.org/abs/2505.22560
tags:
- geometric
- equivariant
- vector
- hyena
- global
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Geometric Hyena, the first equivariant long-convolutional
  architecture for efficiently modeling global geometric context at sub-quadratic
  complexity. It uses vector long convolution in the Fourier domain to preserve equivariance
  to rotations and translations while capturing global interactions in large geometric
  graphs.
---

# Geometric Hyena Networks for Large-scale Equivariant Learning

## Quick Facts
- arXiv ID: 2505.22560
- Source URL: https://arxiv.org/abs/2505.22560
- Reference count: 40
- First equivariant long-convolutional architecture for modeling global geometric context at sub-quadratic complexity

## Executive Summary
This paper introduces Geometric Hyena, the first equivariant long-convolutional architecture for efficiently modeling global geometric context at sub-quadratic complexity. It uses vector long convolution in the Fourier domain to preserve equivariance to rotations and translations while capturing global interactions in large geometric graphs. Evaluated on RNA stability prediction, mRNA switching-factor prediction, and protein molecular dynamics tasks, Geometric Hyena outperforms local and global equivariant baselines, achieving up to 15% better RMSE on RNA tasks and 9% better MSE on protein MD prediction. Notably, it processes 30k tokens 20× faster than equivariant transformers and supports up to 72× longer sequences within the same computational budget.

## Method Summary
Geometric Hyena implements equivariant long convolution using FFT decomposition of vector cross-products, achieving O(N log N) complexity while preserving SE(3) equivariance. The architecture combines scalar and vector convolutions to capture both invariant and equivariant interactions, with key-value normalization preventing numerical instability during training. A Geometric Hyena block consists of an equivariant projection layer followed by geometric long convolution with selective gating, enabling efficient global context modeling for large biomolecular systems.

## Key Results
- Achieves 0.339/0.544 RMSE on RNA stability tasks vs 0.421/0.545 for best equivariant transformer baseline
- Processes 30k tokens 20× faster than equivariant transformers with O(N log N) scaling
- Achieves 9% better MSE on protein MD prediction compared to local GNNs
- Supports up to 72× longer sequences within same computational budget as baselines

## Why This Works (Mechanism)

### Mechanism 1: Equivariant Vector Long Convolution via FFT Decomposition
- Cross-product-based vector convolution preserves rotation equivariance while achieving O(N log N) complexity
- Vector long convolution computes `q ⊛× k` using cross products, then decomposes into 6 scalar FFT-convolutions via Levi-Civita symbol
- Core assumption: Queries and keys are equivariant vectors from the projection layer
- Evidence: Geometric Hyena scales to 30k tokens 20× faster than baselines; FFT decomposition provides theoretical O(N log N) guarantee

### Mechanism 2: Invariant-Equivariant Subspace Interaction via Geometric Long Convolution
- Combining scalar and vector convolutions enables richer geometric relationships than either alone
- Geometric long convolution computes 5 interaction types between (α, r) tuples with learned weights
- Core assumption: Low-order interactions capture sufficient geometric complexity for biomolecules
- Evidence: Ablation shows 10-25% RMSE degradation without geometric convolution; performance gains over scalar-only methods

### Mechanism 3: Key-Value Normalization for Numerical Stability
- Normalizing keys and values to unit norm prevents cubic magnitude growth during training
- Without normalization, output magnitude bounds as ∥y∥₂ ≤ M³ where M is max input magnitude
- Core assumption: Normalization preserves sufficient signal-to-noise ratio for gradient flow
- Evidence: Ablation shows NaN/instability without KV-norm; paper identifies this as critical training step

## Foundational Learning

- **SE(3) Equivariance**:
  - Why needed here: Model outputs must transform correctly under rotations and translations for geometric validity in molecular tasks
  - Quick check question: If you rotate all input coordinates by 90°, do vector outputs rotate by 90° while scalar outputs remain unchanged?

- **FFT-based Circular Convolution**:
  - Why needed here: Sub-quadratic complexity for global context requires O(N log N) operations via frequency-domain multiplication
  - Quick check question: Can you explain why `F⁻¹(F(q) · F(k))` computes circular convolution?

- **Vector Cross Product Properties**:
  - Why needed here: Cross product is naturally SO(3)-equivariant, forming the basis for vector long convolution
  - Quick check question: Does `R(a × b) = (Ra) × (Rb)` hold for rotation matrix R?

## Architecture Onboarding

- **Component map**: Input tokens → EGNN projection (local + global messages) → QKV projection → Geometric long convolution (scalar FFT + vector FFT via 6 scalar convs) → Selective gating → Element-wise/cross-product with values

- **Critical path**:
  1. Implement equivariant projection layer (EGNN with global context tokens)
  2. Implement vector long convolution using 6 parallel scalar FFT-convolutions
  3. Add KV-normalization before convolution
  4. Apply gating and value multiplication

- **Design tradeoffs**:
  - Sub-quadratic vs. permutation equivariance: FFT convolution assumes canonical ordering; not permutation-invariant (beneficial for biomolecules with sequence structure)
  - Type-1 only vs. higher-order representations: Simplified tensor products reduce compute but may limit expressivity for complex geometries
  - Local EGNN + global Hyena vs. pure global: Alternating improves convergence but adds parameters

- **Failure signatures**:
  - Exploding gradients early in training → Check KV-normalization is applied
  - Outputs not rotating correctly → Verify projection layer equivariance and vector convolution cross-product decomposition
  - Memory OOM at moderate sequence lengths → Check if attention matrices are being materialized accidentally

- **First 3 experiments**:
  1. **Geometric associative recall**: Train on sequences of 27-210 tokens with 3-6 vocabulary size; verify model learns key-value retrieval
  2. **Scaling benchmark**: Profile forward pass time and memory for N ∈ {1000, 5000, 10000, 30000}; should see ~O(N log N) scaling
  3. **Ablation on local+global context**: Compare with/without local EGNN projection; expect ~10-25% RMSE degradation without local context on RNA tasks

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can Geometric Hyena be modified to accommodate geometric graphs without a canonical ordering, such as general point clouds or small molecules?
- Basis in paper: The authors state in the "Limitations and Future work" section that the lack of permutation equivariance is a limitation for settings like point cloud processing
- Why unresolved: The FFT-based long convolution inherently breaks permutation symmetry by differentiating token orderings
- What evidence would resolve it: A modified architecture maintaining sub-quadratic complexity while achieving equivariance to permutation group actions

### Open Question 2
- Question: Is the extension of Geometric Hyena to higher-order steerable representations (e.g., spherical harmonics) computationally viable for large-scale systems?
- Basis in paper: Appendix A.5 outlines a blueprint for "Steerable long convolution" using Clebsch-Gordan coefficients
- Why unresolved: While the cross-product mechanism is efficient, it remains unclear if higher-order benefits justify computational cost
- What evidence would resolve it: Analysis comparing runtime and accuracy of steerable extension against vector-based Geometric Hyena

### Open Question 3
- Question: How robust is Geometric Hyena to the quality of canonical ordering in systems where the "natural" sequence prior is weak or absent?
- Basis in paper: The paper hypothesizes that long convolutions are beneficial for molecules with a "strong sequence prior" but doesn't quantify performance degradation with arbitrary ordering
- Why unresolved: If ordering is merely heuristic rather than physical reality, inability to permute inputs could lead to instability
- What evidence would resolve it: Ablation study on datasets where node ordering is randomized

## Limitations
- Lack of permutation equivariance restricts applicability to general point clouds and small molecules without canonical ordering
- Extension to higher-order steerable representations may be computationally prohibitive for large-scale systems
- Numerical instability without key-value normalization suggests formulation may be fragile in practice

## Confidence

**High confidence**: The sub-quadratic complexity claims (O(N log N) scaling) are well-supported by both theoretical analysis and empirical benchmarks

**Medium confidence**: The SE(3) equivariance preservation is theoretically sound given the vector cross-product decomposition, but practical implementation details could introduce equivariance-breaking bugs

**Low confidence**: The claim that Geometric Hyena is "the first equivariant long-convolutional architecture" for this purpose, while likely true, cannot be fully verified without exhaustive literature review

## Next Checks

1. **Equivariance stress test**: Apply random rotations and translations to input coordinates across multiple scale levels (N = 100, 1000, 10000) and verify that scalar outputs remain invariant while vector outputs transform correctly

2. **Normalization sensitivity analysis**: Systematically vary the KV-normalization strength (e.g., Lp norms with p ∈ {1,2,∞}, different scaling factors) to determine whether cubic magnitude growth is inherent or can be mitigated

3. **Cross-product decomposition verification**: Implement vector long convolution using both the 6-scalar FFT decomposition and direct cross-product implementation (for small N where direct computation is feasible) to confirm numerical equivalence