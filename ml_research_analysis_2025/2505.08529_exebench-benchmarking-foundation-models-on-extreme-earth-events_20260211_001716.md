---
ver: rpa2
title: 'ExEBench: Benchmarking Foundation Models on Extreme Earth Events'
arxiv_id: '2505.08529'
source_url: https://arxiv.org/abs/2505.08529
tags:
- data
- extreme
- events
- dataset
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ExEBench, a comprehensive benchmark for evaluating
  foundation models (FMs) on extreme Earth events. The dataset covers seven event
  categories including floods, wildfires, storms, tropical cyclones, extreme precipitation,
  heatwaves, and cold waves, with diverse spatial, temporal, and spectral characteristics.
---

# ExEBench: Benchmarking Foundation Models on Extreme Earth Events

## Quick Facts
- **arXiv ID:** 2505.08529
- **Source URL:** https://arxiv.org/abs/2505.08529
- **Reference count:** 40
- **Key outcome:** ExEBench evaluates foundation models on seven extreme Earth event categories, showing that domain-aligned pre-training improves performance but challenges remain in temporal dynamics, data imbalance, and cross-modality transfer.

## Executive Summary
This paper introduces ExEBench, a comprehensive benchmark for evaluating foundation models on extreme Earth events including floods, wildfires, storms, tropical cyclones, extreme precipitation, heatwaves, and cold waves. The dataset covers diverse spatial, temporal, and spectral characteristics across multiple remote sensing modalities. Experiments across seven foundation models (U-Net, SegFormer, ConvNeXt, SatMAE, Prithvi, DOFA, and Aurora) demonstrate that models pre-trained on relevant data achieve better performance and faster convergence, particularly when pre-training and downstream data share modality. However, significant challenges remain in capturing temporal dynamics, handling severe data imbalance, and transferring across different sensing modalities.

## Method Summary
ExEBench benchmarks foundation models using a standardized pipeline: datasets are loaded from Hugging Face with defined train/test splits, model backbones are initialized with pre-trained weights, first layers are modified to match input channel counts, and decoder heads (UperNet or lightweight CNN) are attached. Three fine-tuning strategies are evaluated: frozen encoder, full fine-tuning, and LoRA. Training uses AdamW optimizer with task-specific learning rates and batch sizes. Performance is measured using metrics appropriate to each task type (segmentation, forecasting, nowcasting) across seven extreme event categories spanning climate reanalysis, multispectral imagery, SAR, and radar data sources.

## Key Results
- Foundation models pre-trained on domain-aligned data achieve faster convergence and better performance on extreme event tasks than those pre-trained on mismatched domains
- Cross-data-type transfer (vision→EO, EO→W&C) is possible but shows reduced performance compared to in-domain transfer
- Models with explicit temporal processing capabilities outperform those that collapse time dimensions into channels for spatiotemporal forecasting tasks

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Foundation models pre-trained on domain-aligned data achieve faster convergence and better performance on extreme event tasks than those pre-trained on mismatched domains.
- **Mechanism:** Pre-training on similar spectral, spatial, and temporal distributions enables the encoder to extract transferable features that require minimal adaptation in the decoder. When pre-training data shares modality with downstream tasks (e.g., ERA5 → heatwaves, HLS → fire scars), learned representations capture relevant physical and spectral patterns.
- **Core assumption:** Feature representations learned during pre-training remain partially valid for out-of-distribution extreme values within the same modality.
- **Evidence anchors:**
  - [Section 3, p.8]: "pre-trained weights show benefits in faster convergence and improved performance in handling extreme events, especially when the downstream task shares the same data modality and source as the pre-trained model"
  - [Section 6.4.6, p.25]: Aurora (ERA5-pretrained) achieves ACC=0.89 on heatwaves; Prithvi-2 (HLS-pretrained) achieves best fire segmentation F1=82.45 with frozen encoder
  - [corpus]: Related work (UniExtreme) confirms FMs specialized for extreme weather outperform general-purpose models on specific extremes
- **Break condition:** Performance degrades when (a) pre-training and downstream data differ substantially in spectral bands or spatial resolution, or (b) extreme values fall far outside pre-training distribution (e.g., cold waves with only 9 sequences caused overfitting in most FMs).

### Mechanism 2
- **Claim:** Cross-data-type transfer (vision→EO, EO→W&C) is possible but requires careful input layer modification and shows reduced performance compared to in-domain transfer.
- **Mechanism:** FMs can leverage generic visual features (edges, textures, spatial patterns) that partially transfer across modalities. The first convolutional layer is modified to accommodate different channel counts while preserving remaining pre-trained weights.
- **Core assumption:** Low-level visual features generalize across sensing modalities; domain-specific physics can be learned during fine-tuning.
- **Evidence anchors:**
  - [Section 3, p.8, Table 2]: Red checkmarks indicate cross-data-type experiments; SegFormer (ImageNet-pretrained) achieves F1=69.39 on fire segmentation despite never seeing multispectral RS data
  - [Section 6.4.1, p.20]: DOFA (multi-modal pre-trained) achieves second-best heatwave ACC=0.806, demonstrating transfer from EO to W&C
  - [corpus]: GAS-MIL paper notes that benchmarking individual FMs across modalities is "time-consuming and resource-intensive" — confirming transfer difficulty
- **Break condition:** Transfer fails when (a) sensing physics differ fundamentally (e.g., active SAR coherence vs. passive optical reflectance), or (b) temporal dynamics are critical but the model lacks temporal modules.

### Mechanism 3
- **Claim:** Models with explicit temporal processing capabilities outperform those that collapse time dimensions into channels for spatiotemporal forecasting tasks.
- **Mechanism:** Temporal modules (e.g., attention over time, recurrence) can model dynamics like precipitation advection. Treating time as channels loses sequential ordering information.
- **Core assumption:** Temporal dependencies in extreme events follow learnable patterns rather than pure stochasticity.
- **Evidence anchors:**
  - [Section 3, p.8]: "Most FMs either can't process temporal data or suffer degraded performance with long horizons"
  - [Section 6.4.4, p.22-23]: U-Net, SegFormer, ConvNeXt "failed to produce useful outputs" when treating temporal dimension as channels for storm nowcasting; only Aurora and Prithvi (with temporal modules) were evaluated
  - [Section 6.4.5, Table 8]: Aurora maintains POD=0.84 at 0.5h horizon but drops to POD=0.66 at 5h horizon
  - [corpus]: No directly comparable corpus evidence on temporal mechanisms specifically for Earth FMs
- **Break condition:** Temporal modeling breaks when (a) forecasting horizon extends beyond learned patterns, or (b) input-output dynamics are fundamentally chaotic.

## Foundational Learning

- **Concept: Distribution shift in extreme events**
  - **Why needed here:** Extreme events by definition occupy distribution tails. Models trained on historical climatology may systematically under-predict extremes (negative RQE values observed across models).
  - **Quick check question:** Can you explain why a model achieving low average RMSE might still fail catastrophically on extreme values?

- **Concept: Spectral band semantics in remote sensing**
  - **Why needed here:** Fire detection uses SWIR bands (B11, B12) for burn scar discrimination; flood detection uses SAR VV/VH polarization and coherence. Different sensors encode different physical properties.
  - **Quick check question:** Why would an ImageNet-pretrained model struggle to interpret SAR coherence bands it has never encountered?

- **Concept: Evaluation metrics for imbalanced segmentation**
  - **Why needed here:** Fire and flood datasets show 8:1 class imbalance. Metrics like mF1 (macro F1) and weighted F1 capture different failure modes than pixel-wise accuracy.
  - **Quick check question:** If a model predicts "non-flooded" everywhere, what happens to accuracy vs. class-averaged F1?

## Architecture Onboarding

- **Component map:** Datasets (7 event categories) → Heatwaves/Cold waves (ERA5 t2m), Tropical cyclones (multi-pressure variables), Storms (C-band radar), Extreme precipitation (IMERG/TRMM), Fires (HLS multispectral), Floods (Sentinel-1 SAR) → Models (Encoder + Decoder) → Fine-tuning strategies (Frozen, Full, LoRA) → Metrics (RMSE, ACC, nRMSE, RQE, POD, FAR, CSI, HSS, F1, IoU)

- **Critical path:**
  1. Load dataset from Hugging Face via provided Dataset class
  2. Initialize FM backbone with pre-trained weights
  3. Modify first layer for input channel count (weight copying where possible)
  4. Attach decoder head (CNN or UperNet depending on model architecture)
  5. Select loss function (L1 for regression, Dice for imbalanced segmentation)
  6. Train with AdamW, cosine or constant LR schedule per task appendix settings

- **Design tradeoffs:**
  - **Frozen encoder vs. full fine-tuning:** Frozen faster (fewer parameters) but full fine-tuning achieves best performance (Table 9 shows +2-8% F1 improvement)
  - **LoRA:** Reduces memory ~50% with ~5-10% performance degradation vs. full fine-tuning
  - **Multi-scale features (DOFA* vs DOFA):** Using features from layers [5,11,17,23] with UperNet decoder improves fire F1 by 9.7 points over single deepest-feature decoder

- **Failure signatures:**
  - **Underestimation of extremes:** Most models show negative RQE (under-predict extreme temperatures/precipitation)
  - **Temporal blurring:** Models without temporal modules produce smoothed precipitation nowcasts
  - **Spectral confusion:** SAR flood models misclassify permanent water as flood without coherence interpretation; climate models (ClimaX) fail on RS imagery
  - **Overfitting on small datasets:** Cold waves (9 sequences) — only Aurora (1.2B params, well-regularized) succeeded

- **First 3 experiments:**
  1. **Baseline fire segmentation with frozen encoders:** Run SatMAE, Prithvi, DOFA on HLS burn scars with frozen backbone. Compare convergence speed vs. random initialization (replicates Figure 22). Expected: pre-trained models converge in ~5 epochs vs. ~10+ for random.
  2. **Cross-modality heatwave prediction:** Test SegFormer (vision), Prithvi (EO), Aurora (W&C) on heatwave forecasting at 10-day horizon. Expected: Aurora > DOFA > SegFormer > others.
  3. **Temporal ablation on storm nowcasting:** Compare Prithvi with temporal module enabled vs. time-as-channels approach at 2-hour horizon. Expected: temporal module maintains structure; time-as-channels produces smoothed outputs.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can foundation models be architected to effectively capture temporal dynamics in extreme weather events, given that current models either cannot process temporal data or suffer significant performance degradation over extended forecasting horizons?
- Basis in paper: [explicit] "FMs have large room for improvement in capturing temporal dynamics. Most FMs either can't process temporal data or suffer degraded performance with long horizons (Fig.8 d)."
- Why unresolved: The experiments show that models without temporal modules (treating time as channels) failed to produce useful outputs, while even models with temporal modules like Prithvi and Aurora show degraded performance as forecasting horizons extend.
- What evidence would resolve it: Novel architectures that maintain stable skill scores (POD, CSI, HSS) across extended forecasting horizons (e.g., from 0.5 to 5+ hours for nowcasting) without the underestimation of extreme precipitation values observed in current models.

### Open Question 2
- Question: How can the systematic underestimation of extreme values be mitigated in foundation models fine-tuned for extreme event prediction?
- Basis in paper: [explicit] The RQE analysis (Figs. 13, 17) shows most models tend to underestimate extreme temperatures, and the precipitation experiments show models "tend to underestimate extreme precipitation values."
- Why unresolved: FMs inherit biases from training data where extreme events "still constitute a small portion of Earth's overall phenomena," and even after fine-tuning, models produce smoothed outputs rather than capturing distribution tails.
- What evidence would resolve it: Models achieving RQE values closer to zero across high percentiles (90th-98th), and precipitation forecasts with improved POD at high thresholds (10-30 mm/h) without corresponding increases in FAR.

### Open Question 3
- Question: How can foundation models be designed to incorporate sensor-specific and physical characteristics (e.g., radar polarization modes, spectral signatures, geographical dependencies) without sacrificing the generality that makes FMs valuable?
- Basis in paper: [explicit] "Future models should go deeper into the spectral characteristics and physical properties of the variables... Ignoring these modes can lead to incomplete or inaccurate interpretations of the data. However, such tailored designs pose challenges in creating unified FMs capable of addressing all tasks."
- Why unresolved: Prithvi-2's improved performance from location embeddings demonstrates the value of domain-specific features, but DOFA's failure on flood mapping despite Sentinel-1 pretraining shows that wavelength encoding alone is insufficient without encoding coherence and polarization characteristics.
- What evidence would resolve it: A unified model architecture that accepts metadata (polarization mode, acquisition timing, location) as auxiliary inputs and demonstrates strong performance across tasks requiring different sensor-specific interpretations (e.g., flood mapping with SAR coherence, burn scar detection with SWIR bands).

### Open Question 4
- Question: What strategies can effectively address the severe label imbalance in extreme event datasets, where target pixels (burned areas, urban floods) may be outnumbered 8:1 or more by background pixels?
- Basis in paper: [explicit] "Another critical area for improvement is handling data imbalance, as most models struggle to capture extreme values and minority classes, even after fine-tuning. Developing effective strategies to address this issue remains a key priority for future research."
- Why unresolved: Even with Dice loss for imbalanced segmentation, models show poor performance on minority classes (urban flood F1 scores below 17% for most models), and the flood experiments show models misclassify permanent water bodies as floods.
- What evidence would resolve it: Models achieving macro F1 scores above 50% for minority classes (urban-flooded, open-flooded) without sacrificing overall accuracy, potentially validated through techniques like focal loss, oversampling, or synthetic data augmentation specifically for extreme pixels.

## Limitations
- Cold waves dataset contains only 9 sequences, leading to severe overfitting for most models
- Limited evaluation of models' ability to handle unprecedented extreme values outside training distribution
- Cross-data-type transfer shows substantial performance degradation compared to in-domain transfer

## Confidence
- **High confidence:** Domain-aligned pre-training improves performance and convergence speed (quantitative results across multiple tasks with clear baselines)
- **Medium confidence:** Explicit temporal processing modules outperform time-as-channels approaches (limited experiments on two models only)
- **Low confidence:** Cross-data-type transfer is "possible" (evidence shows this works but with substantial performance degradation and unclear conditions)

## Next Checks
1. **Distribution shift validation:** Systematically evaluate model performance on synthetic extreme events that exceed historical distributions by 10-50% to quantify robustness to unprecedented extremes
2. **Temporal generalization test:** Compare the forecasting accuracy of models with and without temporal modules across multiple lead times (0.5h, 2h, 5h) on storm nowcasting to establish temporal modeling benefits more comprehensively
3. **Cross-modal transfer boundary:** Test the limits of cross-data-type transfer by evaluating vision-pretrained models on SAR data with increasing levels of preprocessing (e.g., coherence computation) to identify when modality-specific features become essential