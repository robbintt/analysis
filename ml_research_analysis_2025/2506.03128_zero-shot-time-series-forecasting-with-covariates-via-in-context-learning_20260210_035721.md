---
ver: rpa2
title: Zero-Shot Time Series Forecasting with Covariates via In-Context Learning
arxiv_id: '2506.03128'
source_url: https://arxiv.org/abs/2506.03128
tags:
- cosmic
- base
- covariates
- small
- chronos
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: COSMIC is a pretrained time series model that leverages covariates
  in zero-shot forecasting via in-context learning. It addresses the challenge of
  data scarcity for training with covariates by using Informative Covariate Augmentation,
  which generates synthetic training data with predictive covariate-target relationships
  without requiring real covariate-included datasets.
---

# Zero-Shot Time Series Forecasting with Covariates via In-Context Learning

## Quick Facts
- **arXiv ID:** 2506.03128
- **Source URL:** https://arxiv.org/abs/2506.03128
- **Reference count:** 40
- **Primary result:** COSMIC achieves state-of-the-art zero-shot forecasting with covariates, ranking first on 9 out of 11 evaluated datasets

## Executive Summary
COSMIC is a pretrained time series model that performs zero-shot forecasting with covariates using in-context learning. The key innovation is Informative Covariate Augmentation (ICA), which generates synthetic training data with predictive covariate-target relationships without requiring real covariate-included datasets. This approach enables COSMIC to achieve state-of-the-art performance in zero-shot forecasting with covariates while maintaining competitive performance even without covariates.

## Method Summary
COSMIC leverages a transformer architecture with patch-based multivariate alignment to process time series data efficiently. The model uses in-context learning to infer covariate-target relationships during inference rather than relying on pre-learned global semantics. Informative Covariate Augmentation generates synthetic training data where targets are actively modified by synthetic covariates via sampled impact functions, forcing the model to attend to covariate channels. The patch-based approach groups time steps into windows, reducing context length while maintaining sequence order through separation tokens and rotary embeddings.

## Key Results
- Ranks first on 9 out of 11 evaluated datasets for zero-shot forecasting with covariates
- Demonstrates state-of-the-art performance while requiring no real covariate training data
- Maintains competitive performance compared to task-specific models even without covariates

## Why This Works (Mechanism)

### Mechanism 1: In-Context Covariate Inference
The model infers functional relationships between target and covariates solely from local context during inference. When trained on data where covariates have predictive value, the transformer learns to attend to covariate history to reduce loss. During inference on unseen data, it identifies correlation patterns in historical context and applies this inferred rule to future covariate paths. The relationship must be stable throughout context and forecast horizon.

### Mechanism 2: Informative Covariate Augmentation (ICA)
Synthetic data generation with explicit predictive covariate-target relationships bootstraps in-context learning capabilities when real covariate data is scarce. During pretraining, the model encounters synthetic samples where the target is actively modified by a synthetic covariate via sampled impact functions. This forces the model to attend to the covariate channel to predict the target, creating a prior for covariate utility.

### Mechanism 3: Patch-based Multivariate Alignment
Reducing time-series resolution via patching allows the model to ingest multiple variates efficiently without quadratic context explosion. The model groups time steps into patches via a residual block, injects separation tokens between variates, and uses rotary embeddings to maintain sequence order. This treats multivariate input as a long sequence of patches where the transformer can attend across variates.

## Foundational Learning

- **In-Context Learning (ICL):** Why needed here - COSMIC does not train on your specific dataset; it "learns" the relationship between your target and covariates by looking at the prompt (history window). You must understand that the "knowledge" is transient and derived from the input. Quick check question: Can you explain why providing a longer history window might improve the model's ability to utilize a covariate, even if the forecast horizon remains the same?

- **Quantile Regression / Probabilistic Forecasting:** Why needed here - The model outputs distribution parameters (quantiles Q={0.1, ..., 0.9}) rather than a single point. Understanding the loss function (Quantile Loss) is critical for debugging why the model might be under/over-forecasting. Quick check question: If the 0.9 quantile forecast is consistently lower than the actual ground truth, what does that imply about the model's probability estimation?

- **Transformer Attention Mechanisms:** Why needed here - The model relies on self-attention to mix information between target history and covariate history. Understanding attention helps in diagnosing why a specific covariate might be ignored (low attention weights). Quick check question: How does the "separation token" in COSMIC help the attention mechanism distinguish between time steps of different variables?

## Architecture Onboarding

- **Component map:** Input Layer (Z-score normalization → Patching → Concatenation of Padding Mask) → Embedding Layer (Projected Patches + Time Encoding + Rotary Position Embedding) → Backbone (T5-style Encoder-Decoder Transformer) → Output Layer (De-patching → Linear Projection to 9 Quantiles → Un-scaling)

- **Critical path:** The Informed Covariate Augmentation (ICA) pipeline is the most critical step for training. For inference, the critical path is formatting the input sequence: Target History → $s_c$ → Covariate 1 → $s_c$ → Covariate 2...

- **Design tradeoffs:** Synthetic vs. Real Data - Training purely on synthetic augmentation avoids data contamination but may introduce domain shift if synthetic impacts are unrealistic. Patching vs. Point-wise - Patching (window=32) increases efficiency but may lose fine-grained temporal resolution of covariate spikes.

- **Failure signatures:** Static Covariates - The model fails to use covariates if they are static (no change over time), as ICL relies on dynamic history to infer impact. Distribution Shift - If covariates in the forecast horizon have variance vastly different from history, the inferred impact function may produce unstable results. Lagged Effects - The default augmentation limits lags (l=500) and biases toward recency; long, complex lagged relationships might be missed.

- **First 3 experiments:** 1) Ablation on Augmentation - Train COSMIC without ICA (random covariates) and verify performance drops to baseline to confirm the mechanism. 2) Context Length Sensitivity - Vary the input history length to determine the minimum context required for the model to "learn" the covariate relationship in-context. 3) Synthetic Stress Test - Create a synthetic dataset where the covariate relationship is known (e.g., linear addition) and verify COSMIC recovers the ground truth vs. a standard linear regression model.

## Open Questions the Paper Calls Out

### Open Question 1
How can zero-shot forecasting models be adapted to effectively incorporate static covariates (e.g., product ID or location), given that current in-context learning frameworks rely on local temporal context? The current framework cannot handle static covariates because they lack a time-varying signal to infer relationships from local context. A modified architecture that accepts static embeddings or metadata, validated on a dataset where static features significantly influence the target, would resolve this.

### Open Question 2
Does the restriction of Informative Covariate Augmentation to simple linear and piecewise functions limit the model's ability to generalize to highly non-linear real-world covariate relationships? The use of simple impact functions "likely limits the ability to capture highly complex relationships." A comparative analysis where COSMIC is trained with non-linear synthetic impact functions and evaluated on complex benchmarks would resolve this uncertainty.

### Open Question 3
How robust is the in-context learning mechanism when the relationship between the covariate and target is non-stationary between the historical context and the forecast horizon? The assumption is that the relationship is "stable throughout the context and the forecast horizon." Evaluation on synthetic or stress-test datasets containing regime shifts in the covariate-target correlation, comparing error rates with and without the shift, would resolve this.

## Limitations

- The synthetic augmentation approach relies on strong assumptions about covariate-target relationships that may not generalize to real-world scenarios
- The patching mechanism introduces a fundamental tradeoff between efficiency and resolution, potentially missing high-frequency covariate impacts
- The model's performance with static covariates is explicitly limited, as the in-context learning mechanism requires dynamic historical patterns

## Confidence

**High Confidence:**
- The model architecture functions as described, with the transformer backbone and patching mechanism operating correctly
- The quantitative results showing state-of-the-art performance on 9/11 datasets are reliable and reproducible
- The fundamental mechanism of in-context learning for covariate inference is sound and validated through ablation studies

**Medium Confidence:**
- The claim that synthetic augmentation alone is sufficient to teach the model covariate relationships, without requiring real covariate data
- The assertion that synthetic piecewise linear relationships are adequate proxies for real-world dynamics
- The generalizability of performance improvements across diverse dataset types and domains

**Low Confidence:**
- The scalability of the approach to extremely long-range dependencies (lags > 500 steps)
- The model's robustness to severe covariate distribution shifts between training and inference
- The effectiveness of the approach for highly non-stationary relationships where the covariate-target dynamic changes rapidly

## Next Checks

1. **Non-linear Relationship Stress Test:** Create synthetic datasets with complex, non-linear covariate-target relationships (e.g., polynomial, periodic, or threshold-based interactions) that differ from the linear piecewise functions used in ICA. Compare COSMIC's ability to capture these relationships against baseline models and evaluate whether the synthetic augmentation strategy limits the model's capacity to learn more complex dynamics.

2. **Covariate Distribution Shift Analysis:** Design experiments where the statistical properties of covariates during the forecast horizon differ significantly from the historical context (e.g., mean, variance, or correlation structure changes). Quantify the degradation in performance and identify the threshold at which the in-context learned relationship breaks down, providing insights into the model's robustness to real-world covariate drift.

3. **Patch Size Resolution Study:** Systematically vary the patch size parameter across a wide range (1-256 steps) and evaluate its impact on performance for datasets with different covariate signal frequencies. Measure the trade-off between computational efficiency and forecasting accuracy, and determine whether there exists an optimal patch size that balances both objectives across diverse time series characteristics.