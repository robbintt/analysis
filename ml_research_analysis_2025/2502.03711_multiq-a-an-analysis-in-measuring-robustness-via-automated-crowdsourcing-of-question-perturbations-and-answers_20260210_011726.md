---
ver: rpa2
title: 'MultiQ&A: An Analysis in Measuring Robustness via Automated Crowdsourcing
  of Question Perturbations and Answers'
arxiv_id: '2502.03711'
source_url: https://arxiv.org/abs/2502.03711
tags:
- question
- perturbations
- answer
- robustness
- what
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MultiQ&A, a systematic framework for evaluating
  the robustness and consistency of LLM-generated answers by automatically crowdsourcing
  question perturbations and answers through independent LLM agents. The method transforms
  a single question into diverse semantic variations, collects independent answers
  for each, and uses ensemble voting and clustering to identify reliable responses
  and measure variability.
---

# MultiQ&A: An Analysis in Measuring Robustness via Automated Crowdsourcing of Question Perturbations and Answers

## Quick Facts
- arXiv ID: 2502.03711
- Source URL: https://arxiv.org/abs/2502.03711
- Authors: Nicole Cho; William Watson
- Reference count: 10
- Primary result: gpt-3.5-turbo shows robust performance under semantic perturbations with ensemble accuracy matching baseline

## Executive Summary
This paper introduces MultiQ&A, a systematic framework for evaluating LLM robustness and consistency by automatically crowdsourcing question perturbations and answers through independent LLM agents. The method transforms a single question into diverse semantic variations, collects independent answers for each, and uses ensemble voting and clustering to identify reliable responses and measure variability. Across 13 datasets, MultiQ&A analyzed 1.9 million perturbed questions and 2.3 million answers, demonstrating that gpt-3.5-turbo remains robust under perturbations with ensemble accuracy matching baseline performance.

## Method Summary
MultiQ&A systematically evaluates LLM robustness by first generating v+1 semantically equivalent question variations using a high-temperature gpt-3.5-turbo agent (τ=1.0), then collecting independent answers for each variation from v+1 separate agents, and finally aggregating responses through semantic clustering and ensemble voting. The framework computes multiple metrics including baseline accuracy, mode accuracy, worst-case and best-case robustness, agreement coefficients, and uncertainty measures. Applied across 13 QA datasets, the approach processed 1.9 million questions and 2.3 million answers, revealing that gpt-3.5-turbo maintains consistent performance under semantic perturbations while providing measurable confidence signals through agreement metrics.

## Key Results
- gpt-3.5-turbo maintains ensemble accuracy matching baseline performance under semantic perturbations
- MultiQ&A successfully quantifies worst-case robustness (Ω) and best-case robustness (O) across diverse QA tasks
- The framework identifies domain-specific vulnerabilities, with MathQA showing significantly lower agreement (κ≈30%) than other tasks

## Why This Works (Mechanism)

### Mechanism 1: Semantic Perturbation Stress-Testing
Generating lexically diverse but semantically equivalent question variations exposes brittleness in LLM reasoning. The Query Rewriter (gpt-3.5-turbo with τ=1.0) transforms original question q₀ into v+1 variations Q = {q₀, q₁, ..., qᵥ} using the prompt "Rewrite the question in n radically different ways." The identity transformation T₀(q₀)=q₀ preserves the original query. High-temperature sampling produces semantically faithful perturbations; the generator itself doesn't introduce semantic drift. Break condition: If perturbations systematically alter semantics, robustness metrics conflate semantic drift with model inconsistency.

### Mechanism 2: Independent Agent Isolation for Response Variance Capture
Isolating answer generation across independent LLM agents prevents cross-contamination and reveals true response variance. |Q| = v+1 independent gpt-3.5-turbo agents generate answers A = {a₀, a₁, ..., aᵥ} with no shared context. Temperature τ=1.0 prioritizes diversity. Prompt templates vary by task type (Extractive, Multiple Choice, Abstractive). Core assumption: Agents are sufficiently isolated via API calls; no implicit state leakage across requests. Break condition: If API-level caching or implicit state affects outputs, variance attribution is compromised.

### Mechanism 3: Ensemble Agreement as Hallucination Signal
Low inter-rater agreement across perturbations signals potential hallucination or model uncertainty. The Aggregator computes Fleiss's κ, entropy-normalized certainty Hη, and Gibbs' M2 index. Answers are clustered via semantic paraphrase models and re-ranked by cross-encoder alignment. Plurality voting Ŷ selects the mode answer. Core assumption: Agreement correlates with correctness; disagreement indicates model vulnerability rather than legitimate ambiguity. Break condition: If questions have legitimate multiple valid answers, low agreement reflects task ambiguity, not hallucination.

## Foundational Learning

- **Concept: Perturbation Robustness Testing**
  - Why needed here: MultiQ&A's core premise is that semantically equivalent reformulations should yield consistent answers. Understanding how to design and validate perturbations is prerequisite.
  - Quick check question: Given "What is the capital of France?", is "Name France's capital city?" a valid semantic perturbation? What about "Which city does France govern from?"

- **Concept: Inter-Rater Agreement Metrics (κ, Cronbach's α)**
  - Why needed here: The paper uses classical psychometric measures (Fleiss's κ, Cronbach's α) to quantify consistency. These require understanding chance-adjusted agreement.
  - Quick check question: If 5 agents unanimously answer "A" on a 4-choice question, what is the observed agreement P̄ₒ? How does κ adjust for expected chance agreement?

- **Concept: Ensemble Voting and Mode Selection**
  - Why needed here: The framework uses plurality voting Ŷ = mode(f(Tᵢ(xⱼ))) as the aggregate answer. Understanding when ensembling helps vs. introduces noise is critical.
  - Quick check question: If answers are [A, A, B, C, D] across 5 perturbations, what is the plurality vote? When might this be worse than baseline accuracy?

## Architecture Onboarding

- **Component map**: Query Rewriter -> Answer Generator (v+1 parallel agents) -> Aggregator (semantic clustering + metric computation)
- **Critical path**: Input original question q₀ → Perturb: Generate Q via single prompt call → Answer: v+1 parallel API calls, no shared state → Aggregate: Cluster answers → compute supervised and unsupervised metrics → output final answer + confidence signals
- **Design tradeoffs**: Temperature τ=1.0 maximizes diversity but increases computational cost and noise; v=5 perturbations balances coverage vs. API costs (paper used 2.3M answers, 717M tokens); semantic clustering introduces dependency on paraphrase model quality
- **Failure signatures**: Content filter triggers: 2,293 cases blocked (mostly in MMLU, PIQA, SQuADv2); MathQA outlier: κ≈30%, α≈42.8% suggests arithmetic tasks break agreement-based detection; Abstractive tasks: Lower worst-case robustness (32.9%) vs. extractive (69.0%)—context matters
- **First 3 experiments**:
  1. Reproduce SQuADv2 extractive baseline: Run 100 questions through full pipeline, verify mode accuracy ≈92% matches paper baseline
  2. Perturbation semantic fidelity check: Human-review 50 generated perturbations for semantic drift; estimate break condition threshold
  3. Agreement-hallucination correlation: On TruthfulQA, plot κ vs. accuracy to test whether low agreement predicts hallucinations

## Open Questions the Paper Calls Out

### Open Question 1
Does the MultiQ&A framework generalize to open-source or smaller language models, or is it dependent on the specific instruction-following capabilities of gpt-3.5-turbo? The methodology relies exclusively on "independent gpt-3.5-turbo agents" for both query rewriting and answer generation, without testing other architectures. The authors do not demonstrate if the "Query Rewriter" agent in other models can generate semantically consistent perturbations at the same quality. What evidence would resolve it: Applying the MultiQ&A pipeline to diverse models (e.g., LLaMA, Mistral) and comparing the resulting robustness metrics against the GPT-3.5 baseline.

### Open Question 2
What specific linguistic or reasoning constraints cause the model to fail consistency checks on MathQA compared to other domains? Section 6.2 identifies MathQA as a distinct "outlier" with low agreement (≈30%) and low consistency (Cronbach's α≈42.8%), attributing it to difficulties in arithmetic operations. While the paper notes the failure, it does not analyze the specific perturbation types or reasoning steps that lead to the divergence in answers for mathematical problems. What evidence would resolve it: A fine-grained error analysis of MathQA perturbations to determine if variability stems from syntactic changes in the problem text or the multi-step logical structure required.

### Open Question 3
Can the high computational cost (717M+ tokens) be reduced by lowering the temperature or the number of perturbations without sacrificing the detection of worst-case robustness (Ω)? The methodology utilizes a "high temperature setting (τ=1.0)" and v=5 perturbations to prioritize diversity, resulting in massive token usage, but provides no ablation on the efficiency of these hyperparameters. It is unclear if the "stress-test" requires such extreme diversity or if lower temperatures would yield similar robustness insights with fewer generated tokens. What evidence would resolve it: A comparative study measuring the stability of the mode accuracy and worst-case robustness score as the number of perturbations (v) and temperature (τ) are decreased.

## Limitations

- Semantic perturbation fidelity requires careful validation as the paper provides minimal evidence that perturbations maintain semantic intent across diverse domains
- High content filter error rate (2,293 cases) suggests potential bias in which questions can be successfully perturbed and answered
- Framework's reliance on agreement metrics assumes low agreement indicates hallucination rather than legitimate ambiguity

## Confidence

- **High confidence**: Ensemble accuracy matching baseline performance for gpt-3.5-turbo under perturbations, demonstrating the framework's validity as a diagnostic tool
- **Medium confidence**: Agreement metrics correlating with hallucination detection, limited by correlational evidence and lack of causal validation
- **Low confidence**: Claim that the framework enables "scalable institutional LLM adoption" - the paper demonstrates technical feasibility but provides limited evidence about deployment readiness or decision-making utility

## Next Checks

1. **Semantic Fidelity Validation**: Conduct blind human evaluation of 100 perturbed questions across 3 datasets to measure semantic drift rate. Determine if perturbations systematically alter question intent beyond acceptable thresholds.

2. **Agreement-Correctness Causality**: Design controlled experiments on TruthfulQA where ground truth answers are known. Test whether low agreement (κ < threshold) predicts incorrect answers beyond chance, establishing causal rather than correlational relationships.

3. **Computational Cost-Benefit Analysis**: Calculate the marginal value of each additional perturbation (v > 5) versus computational cost and API usage. Determine optimal v for different task types and domains.