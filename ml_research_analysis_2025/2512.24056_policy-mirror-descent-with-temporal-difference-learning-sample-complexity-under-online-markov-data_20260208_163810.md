---
ver: rpa2
title: 'Policy Mirror Descent with Temporal Difference Learning: Sample Complexity
  under Online Markov Data'
arxiv_id: '2512.24056'
source_url: https://arxiv.org/abs/2512.24056
tags:
- policy
- lemma
- td-pmd
- sample
- complexity
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the sample complexity of policy mirror
  descent (PMD) methods with temporal difference (TD) learning under online Markov
  sampling. Two algorithms, Expected TD-PMD (off-policy) and Approximate TD-PMD (mixed-policy),
  are presented.
---

# Policy Mirror Descent with Temporal Difference Learning: Sample Complexity under Online Markov Data

## Quick Facts
- **arXiv ID:** 2512.24056
- **Source URL:** https://arxiv.org/abs/2512.24056
- **Reference count:** 12
- **Primary result:** Achieves O(ε⁻²) sample complexity for ε-optimal solutions under online Markov data

## Executive Summary
This paper establishes the first sample complexity bounds for Policy Mirror Descent (PMD) methods with Temporal Difference (TD) learning under online Markov sampling. The authors present two algorithms - Expected TD-PMD and Approximate TD-PMD - that achieve O(ε⁻²) sample complexity for average-time ε-optimality under small constant policy step sizes. By introducing adaptive policy update step sizes and batch sizes, the sample complexity is further improved to O(ε⁻²) (without logarithmic factors) for last-iterate ε-optimality. This work bridges the gap between PMD methods and practical RL settings where only online Markov data is available.

## Method Summary
The method combines Policy Mirror Descent with TD learning under online Markov data. The actor (PMD) updates the policy using Bregman divergence, while the critic (TD learner) updates value estimates using weighted Bellman operators applied to trajectory batches. The key innovation is using adaptive policy step sizes that approximate Value Iteration and adaptive batch sizes that eliminate logarithmic factors in the sample complexity. The algorithm operates under standard MDP assumptions with ergodicity and sufficient exploration from a behavior policy.

## Key Results
- Expected TD-PMD and Approximate TD-PMD achieve O(ε⁻²) sample complexity for average-time ε-optimality
- Adaptive policy step sizes enable approximation of Value Iteration, achieving O(ε⁻²) for last-iterate optimality
- Adaptive batch sizes eliminate logarithmic factors, achieving the tight O(ε⁻²) bound
- First analysis of PMD-type methods under unregularized MDP settings with online Markov data

## Why This Works (Mechanism)

### Mechanism 1: Inductive Bias Control without Inner Loops
The Expected TD-PMD algorithm achieves O(ε⁻²) sample complexity by bounding the stochastic bias term inductively, rather than requiring an inner loop to solve the Bellman equation to high accuracy. This is achieved by updating the critic using one step of a weighted Bellman operator based on the behavior policy's stationary distribution.

### Mechanism 2: Approximating Value Iteration via Adaptive Step Sizes
Increasing policy update step sizes allows TD-PMD to approximate Value Iteration (greedy updates), achieving O(ε⁻²) last-iterate optimality. When the policy update step size ηₖ → ∞, the PMD update forces the policy to become greedy with respect to the current critic, effectively transforming the update into batch Q-learning.

### Mechanism 3: Removing Logarithmic Factors via Adaptive Batching
Adaptive batch sizes eliminate logarithmic factors typically found in sample complexity bounds for last-iterate optimality. By increasing batch size Bₖ alongside iteration count k, the algorithm reduces Markov noise faster than cumulative error grows, replacing the need for poly-logarithmic dependencies on (1/ε).

## Foundational Learning

- **Policy Mirror Descent (PMD) & Bregman Divergence**: The actor backbone uses Bregman divergence Dₕ(·||·). Understanding this helps interpret how choosing h (e.g., negative entropy vs. L2 norm) changes the algorithm into Natural Policy Gradient or Projected Gradient Descent. *Quick check: If h is negative entropy, does the PMD update look more like a softmax probability shift or a Euclidean projection?*

- **Markov Chain Mixing & Stationary Distribution**: The analysis relies on "Online Markov Data" where samples are correlated. Understanding ergodicity is essential as it guarantees the trajectory eventually covers the state space proportionally to the stationary distribution ν^πb. *Quick check: Why does the bound in Lemma 3.12 depend on the mixing time parameter κb?*

- **Bellman Operator Contraction**: The critic update is analyzed as applying a weighted Bellman operator. The proof of convergence relies on this operator being a contraction mapping, meaning it pulls value estimates closer to true values at a geometric rate. *Quick check: How does the "weighted" operator F^π_πb,α differ from the standard Bellman operator F^π, and why does the weight σ_πb affect the contraction modulus?*

## Architecture Onboarding

- **Component map:**
  - Actor (PMD) -> Critic (TD Learner) -> Buffer/Sampler -> Scheduler
  - Actor updates policy πₖ using Eq. (3) with current Critic Qₖ
  - Critic updates value Qₖ using Eq. (10) with trajectory batch τₖ
  - Buffer collects batch τₖ of size Bₖ by following behavior policy πb
  - Scheduler adjusts ηₖ (step size) and Bₖ (batch size) per iteration

- **Critical path:**
  1. Sample: Collect batch τₖ using πb
  2. Policy Update: Compute πₖ₊₁ from Qₖ (requires solving max over simplex)
  3. TD Error: Compute δ̄ₖ by comparing Qₖ against rewards and expected values of Qₖ under πₖ₊₁
  4. Critic Update: Qₖ₊₁ ← Qₖ + αδ̄ₖ

- **Design tradeoffs:**
  - Average vs. Last-Iterate Optimality: Use constant step sizes η for average-optimality (simpler, Theorem 3.13) vs. adaptive ηₖ for last-iterate optimality (faster, Theorem 3.17)
  - Expected vs. Approximate: "Expected" (Algorithm 1) requires computing an expectation over actions (tractable in tabular); "Approximate" (Algorithm 3) samples the target policy, enabling extension to large action spaces but adding noise

- **Failure signatures:**
  - Low Exploration: If πb is narrow, estimates for unseen states Q(s,a) stagnate at initialization (violates Assumption 3.1)
  - Noise Accumulation: If batch size Bₖ is too small relative to adaptive step size ηₖ, the noise term ω̄ₖ destabilizes the "greedy" approximation
  - Step Size Misspecification: If α (critic rate) is too large (outside (0,1]), the contraction property of Lemma 3.14 may break

- **First 3 experiments:**
  1. Verify "Smooth Q-Learning": Implement Expected TD-PMD with large constant η. Compare performance against standard Q-Learning on a GridWorld to validate the claim that PMD approximates Q-learning as η → ∞ (Section 3.2).
  2. Impact of Adaptive Batching: Run Theorem 3.17 settings with adaptive Bₖ vs. constant Bₖ. Plot ||Q* - Qₖ||_∞ to confirm the removal of logarithmic factors (linear vs. sub-linear convergence plots).
  3. Coverage Sensitivity: Construct an MDP where the optimal policy visits "rare" states. Vary the exploration factor π̃b of the behavior policy and plot the final error to empirically validate the dependency on σ̃b in Theorem 3.13.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the sample complexity analysis for Expected TD-PMD and Approximate TD-PMD be extended to reinforcement learning problems with entropy regularization?
- Basis in paper: [explicit] The conclusion states it is "interesting to extend our analysis to the RL problem with entropy regularization."
- Why unresolved: The current analysis focuses exclusively on the unregularized MDP setting, whereas regularized settings often exhibit different convergence properties (e.g., linear convergence).
- Evidence: Convergence rate derivations and sample complexity bounds established for TD-PMD algorithms under regularized objectives.

### Open Question 2
- Question: Can the theoretical guarantees be generalized to settings utilizing policy or critic function approximation?
- Basis in paper: [explicit] The conclusion suggests considering "the more general setting with policy or critic function approximation."
- Why unresolved: The paper is restricted to the tabular setting. Function approximation introduces approximation errors and requires assumptions (e.g., smoothness, non-degeneracy) not applicable to the tabular case.
- Evidence: Sample complexity bounds established for these algorithms using general smooth function classes or neural networks under online Markov data.

### Open Question 3
- Question: Can the sample complexity of batch Q-learning derived in this paper (O((1-γ)⁻⁵ε⁻²)) be improved to match the tight bound O((1-γ)⁻⁴ε⁻²) found in prior work?
- Basis in paper: [explicit] Remark 3.8 asks whether the sample complexity can be improved to match Li et al. [2021] by considering the stationary property of the entire Markov chain.
- Why unresolved: The current analysis relies on the stationary property of trajectory segments rather than the entire chain, resulting in a looser dependence on the discount factor.
- Evidence: A proof technique that successfully leverages the stationary distribution of the full Markov chain to close the gap in dependency on (1-γ).

## Limitations
- Analysis relies on strong assumptions about behavior policy's coverage (Assumption 3.1) and Markov chain ergodicity (Assumption 3.2)
- Adaptive batch size formula requires knowledge of problem-dependent constants that may be difficult to estimate in practice
- Transition from average- to last-iterate optimality requires careful scheduling of both policy and critic step sizes, which may be sensitive to parameter tuning

## Confidence
- **High Confidence:** The O(ε⁻²) sample complexity for average-iterate optimality under constant step sizes (Theorem 3.13) is well-established through inductive bias control
- **Medium Confidence:** The improvement to O(ε⁻²) for last-iterate optimality via adaptive batching (Theorem 3.17) follows logically from the analysis but requires precise parameter scheduling that may be challenging to implement
- **Medium Confidence:** The approximation of Value Iteration through large policy step sizes (Proposition 3.19) is theoretically sound but may be sensitive to noise levels in practice

## Next Checks
1. **Exploration Sensitivity:** Implement a controlled experiment varying the behavior policy's exploration level to empirically verify the impact of Assumption 3.1 on final performance.
2. **Batch Size Scheduling:** Compare adaptive vs. fixed batch sizes in a tabular MDP to confirm the removal of logarithmic factors in the sample complexity.
3. **Parameter Robustness:** Test the sensitivity of the algorithm to mis-specification of the critic step size α and mixing parameters to understand practical limitations.