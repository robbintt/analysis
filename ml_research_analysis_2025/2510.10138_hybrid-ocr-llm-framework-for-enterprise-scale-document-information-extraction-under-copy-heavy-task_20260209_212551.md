---
ver: rpa2
title: Hybrid OCR-LLM Framework for Enterprise-Scale Document Information Extraction
  Under Copy-heavy Task
arxiv_id: '2510.10138'
source_url: https://arxiv.org/abs/2510.10138
tags:
- extraction
- document
- arxiv
- processing
- methods
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper presents a hybrid OCR-LLM framework for high-accuracy,\
  \ low-latency information extraction from copy-heavy documents, such as identity\
  \ records. The method combines traditional OCR engines with LLM-based extraction\
  \ strategies\u2014direct, replacement, and table-based\u2014tailored to document\
  \ structure."
---

# Hybrid OCR-LLM Framework for Enterprise-Scale Document Information Extraction Under Copy-heavy Task

## Quick Facts
- **arXiv ID:** 2510.10138
- **Source URL:** https://arxiv.org/abs/2510.10138
- **Reference count:** 22
- **Primary result:** Achieves F1=1.0 accuracy with 0.97s latency for structured documents and 54× speedup over multimodal baselines using format-aware routing.

## Executive Summary
This paper presents a hybrid OCR-LLM framework for high-accuracy, low-latency information extraction from copy-heavy documents like identity records. The method combines traditional OCR engines with LLM-based extraction strategies—direct, replacement, and table-based—tailored to document structure. Empirical evaluation across 25 configurations on PNG, DOCX, XLSX, and PDF formats shows that the framework achieves F1=1.0 accuracy with 0.97s latency for structured documents and F1=0.997 with 0.6s latency for image inputs using PaddleOCR, delivering a 54× speedup over multimodal baselines. Format-aware routing enables efficient processing at production scale.

## Method Summary
The framework uses a two-stage pipeline: text extraction using MarkItDown, Docling, MinerU, PaddleOCR, or EasyOCR based on format, followed by LLM extraction via Qwen2.5-7B using three paradigms—direct, replace, and table-based. Table-based extraction has the LLM output only table region coordinates, then a deterministic parser extracts cell content, avoiding token-by-token generation of entire field values. The framework detects document format (PNG, DOCX, XLSX, PDF) and routes to the appropriate extraction paradigm: table-based for structured formats, multimodal for degraded images, and direct/replacement for mixed cases.

## Key Results
- Achieves F1=1.0 accuracy with 0.97s latency for structured documents using table-based extraction
- Delivers 54× speedup over multimodal baselines (PaddleOCR + table method: F1=0.997 at 0.63s vs multimodal F1=0.999 at 33.9s)
- Format-aware routing achieves optimal accuracy-latency trade-offs across 25 configurations on four document formats

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Format-aware routing to extraction methods based on document structure achieves optimal accuracy-latency trade-offs.
- Mechanism: The framework detects document format and routes to the appropriate extraction paradigm: table-based for structured formats, multimodal for degraded images, and direct/replacement for mixed cases.
- Core assumption: Document structure is predictable enough within formats that a single method can dominate; format detection is reliable and low-cost.
- Evidence anchors: [abstract] "Format-aware routing enables efficient processing at production scale"; [Section 4.3] "no single method achieves universal optimality across all formats, necessitating format-specific extraction strategies"
- Break condition: Highly variable layouts within a single format may defeat single-method routing.

### Mechanism 2
- Claim: Table-based extraction minimizes LLM inference to coordinate generation, achieving up to 54× speedup over direct/multimodal methods while maintaining near-perfect accuracy.
- Mechanism: The LLM outputs only table region coordinates, then a deterministic parser extracts cell content. This avoids token-by-token generation of entire field values.
- Core assumption: Target fields occupy predictable table positions; OCR/document parser preserves spatial structure accurately.
- Evidence anchors: [Section 4.3] "table-based approaches achieve 40–50× speedup through minimal LLM usage"; PaddleOCR + table method: F1=0.997 at 0.63s vs multimodal F1=0.999 at 33.9s
- Break condition: Documents where field positions vary unpredictably, or where OCR fails to preserve spatial structure.

### Mechanism 3
- Claim: OCR engine selection based on spatial structure preservation—not just character accuracy—determines downstream extraction success for structured documents.
- Mechanism: PaddleOCR maintains bounding-box/spatial metadata, enabling table-based extraction; EasyOCR loses spatial relationships, causing catastrophic failure in coordinate-dependent methods.
- Core assumption: Downstream task relies on positional relationships; OCR text accuracy alone is insufficient.
- Evidence anchors: [Section 4.3] "EasyOCR's inability to preserve document spatial structure during text extraction... disrupts the positional relationships critical for accurate field identification"
- Break condition: Tasks where only text content matters may not require spatial preservation.

## Foundational Learning

- **Concept:** OCR-to-LLM pipeline vs. end-to-end multimodal extraction
  - Why needed here: The framework explicitly compares these paradigms; understanding trade-offs is essential for method selection.
  - Quick check question: For a scanned invoice with complex tables, would you route to multimodal VLM or OCR + table-based extraction? Why?

- **Concept:** Spatial structure representation in document parsers
  - Why needed here: Docling, MinerU, and MarkItDown represent layout differently; MinerU's HTML-style tags failed with replacement/table methods.
  - Quick check question: A parser outputs `<tr><td>Name</td><td>John</td></tr>`. Will this work with a coordinate-based table extractor? Why or why not?

- **Concept:** Placeholder-based prompt engineering for repetitive documents
  - Why needed here: The "replace extraction" method uses placeholders to enable prompt reuse and batch processing.
  - Quick check question: In replacement extraction, why might the LLM mis-associate names with ID numbers, and how would you mitigate this?

## Architecture Onboarding

- **Component map:** Input document -> format detection -> OCR/parser selection -> extraction method selection (direct/replace/table) -> LLM inference -> structured output
- **Critical path:** 1. Input document → format detection; 2. Format → OCR/parser selection; 3. Extracted text + format → extraction method selection; 4. LLM inference → structured output
- **Design tradeoffs:**
  - Table extraction: Fastest and most accurate for structured documents, but fails catastrophically if spatial structure is lost or if layout is unpredictable.
  - Multimodal: Most robust to OCR failures, but 30–50× slower; reserve for fallback or degraded images.
  - Direct extraction: Simplest pipeline, but LLM inference dominates latency (~13s).
  - Replacement extraction: Fast (0.5–0.7s) but lower accuracy (F1≈0.97) due to name-ID association errors.
- **Failure signatures:**
  - EasyOCR + table extraction on PNG → F1=0.0 (spatial structure not preserved)
  - MinerU + replace/table on PDF → F1=0.0 (HTML-style tags incompatible)
  - MarkItDown on PDF → F1=0.07–0.77 (lightweight parsing loses structure)
  - Any method on highly variable layouts → degraded accuracy or total failure
- **First 3 experiments:**
  1. Baseline comparison: Run direct, replace, and table extraction on 20 DOCX files with known ground truth. Measure F1 and latency.
  2. OCR engine A/B test: Process 50 PNG identity documents with PaddleOCR vs EasyOCR, using table extraction.
  3. Fallback stress test: Feed 30 degraded image documents through multimodal VLM.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can hybrid architectures be designed to deeply integrate structural heuristics with end-to-end model robustness, rather than simply routing between them?
- Basis in paper: [explicit] The Conclusion states: "For researchers, we highlight unexplored opportunities in hybrid architectures that combine the efficiency of structural methods with the robustness of end-to-end models."
- Why unresolved: The current framework uses a modular "switching" mechanism rather than a unified architecture that simultaneously leverages structure and generative reasoning.
- What evidence would resolve it: A proposed model architecture or loss function that jointly optimizes for structural fidelity and semantic understanding during training.

### Open Question 2
- Question: To what extent does the framework's reliance on spatial structure preservation transfer to non-identity, copy-heavy domains like financial invoices or legal contracts?
- Basis in paper: [inferred] While the title and abstract generalize to "copy-heavy" enterprise tasks, the empirical evaluation is restricted to synthetic Chinese identity documents.
- Why unresolved: The paper demonstrates that spatial preservation is critical for IDs, but it is unclear if the "Table" and "Replace" methods remain dominant when dealing with the dense, multi-column, or handwritten variations common in other enterprise document types.
- What evidence would resolve it: Evaluation of the 25 configurations on standard datasets featuring diverse copy-heavy formats beyond synthetic identity cards.

### Open Question 3
- Question: How should OCR evaluation benchmarks evolve to prioritize spatial structure preservation over raw character accuracy?
- Basis in paper: [explicit] The Discussion notes: "This challenges conventional OCR evaluation metrics and highlights the need for structure-aware benchmarks."
- Why unresolved: The study found that EasyOCR failed on table extraction (F1=0.0) despite recognizing characters, proving that current Character Error Rate (CER) metrics fail to predict downstream extraction success.
- What evidence would resolve it: A new benchmarking protocol that scores OCR engines specifically on their ability to maintain bounding box fidelity and cell relationships necessary for "Table" extraction methods.

## Limitations

- **Synthetic Dataset Representativeness:** The evaluation uses synthetically generated identity documents, which may not capture the full complexity and variability of real enterprise documents.
- **Prompt Specification Gap:** The exact prompts for each extraction paradigm are not disclosed, creating ambiguity in replicating the methodology.
- **Parser Rule Ambiguity:** The rule-based parser implementation for table extraction lacks specification, particularly how OCR bounding boxes map to cell coordinates.

## Confidence

- **High Confidence:** The format-aware routing mechanism is well-supported by empirical evidence showing different methods excel for different formats.
- **Medium Confidence:** The table-based extraction speedup claim is supported by the 54× latency reduction, but the catastrophic failure with EasyOCR suggests sensitivity to OCR engine selection.
- **Medium Confidence:** The spatial structure preservation claim is strongly supported by the PaddleOCR vs EasyOCR comparison, but the evaluation focuses on identity documents.

## Next Checks

1. **Cross-Domain Generalization Test:** Apply the framework to 100 real-world enterprise documents (invoices, contracts, forms) across the same four formats. Measure whether format-aware routing maintains F1>0.99 for structured documents and whether the 54× speedup claim holds with real data.

2. **OCR Engine Robustness Analysis:** Systematically evaluate 5 OCR engines across 200 documents with varying layout complexity. Document the exact failure conditions where spatial structure preservation breaks down and identify threshold metrics.

3. **Method Selection Boundary Characterization:** Create a mixed-layout document corpus with known ground truth containing unpredictable field positions, rotated text, and degraded images. Systematically test when format-aware routing fails versus when multimodal fallback succeeds, establishing clear decision boundaries for method selection.