---
ver: rpa2
title: 'How to Tune a Multilingual Encoder Model for Germanic Languages: A Study of
  PEFT, Full Fine-Tuning, and Language Adapters'
arxiv_id: '2501.06025'
source_url: https://arxiv.org/abs/2501.06025
tags:
- language
- adapters
- full
- fine-tuning
- pfeiffer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper investigates the optimal adaptation strategies for\
  \ the multilingual encoder model mDeBERTa on Germanic languages (German, Swedish,\
  \ Icelandic) across three tasks: extractive question answering, named entity recognition,\
  \ and linguistic acceptability classification. The study compares full fine-tuning\
  \ with two parameter-efficient fine-tuning (PEFT) methods\u2014LoRA and Pfeiffer\
  \ bottleneck adapters\u2014finding that PEFT consistently outperforms full fine-tuning\
  \ for German, while results for Swedish and Icelandic are task-dependent, with PEFT\
  \ excelling in QA and full fine-tuning performing better in NER."
---

# How to Tune a Multilingual Encoder Model for Germanic Languages: A Study of PEFT, Full Fine-Tuning, and Language Adapters

## Quick Facts
- arXiv ID: 2501.06025
- Source URL: https://arxiv.org/abs/2501.06025
- Reference count: 10
- This paper investigates optimal adaptation strategies for mDeBERTa on Germanic languages, finding PEFT consistently outperforms full fine-tuning for German while results are task-dependent for Swedish and Icelandic.

## Executive Summary
This paper systematically compares full fine-tuning with parameter-efficient fine-tuning (PEFT) methods for adapting mDeBERTa to Germanic languages across three tasks. The authors find that PEFT (specifically Pfeiffer bottleneck adapters) consistently outperforms full fine-tuning for German, while Swedish and Icelandic show task-dependent results with PEFT excelling in QA and full fine-tuning performing better in NER. Language adapters trained on unstructured text provide no significant benefits, likely due to the inclusion of these languages in the model's pre-training data and availability of in-language task data.

## Method Summary
The study compares full fine-tuning with two PEFT methods (LoRA and Pfeiffer bottleneck adapters) on mDeBERTa-v3-base for three tasks: extractive QA, NER, and linguistic acceptability classification across German, Swedish, and Icelandic. PEFT configurations used LoRA (rank=8, α=16) and Pfeiffer adapters (reduction factor=16) with learning rate 3e-4, while full fine-tuning used learning rate 2e-5. Language adapters were trained on 250K CC100 samples per language with MLM objective. All experiments used 5-fold cross-validation with F1 score as primary metric.

## Key Results
- PEFT consistently outperforms full fine-tuning for German across all three tasks
- For Swedish and Icelandic, PEFT excels in QA while full fine-tuning performs better in NER
- Language adapters provide no significant benefits across any language/task combination
- Pfeiffer adapters generally outperform LoRA except for German QA

## Why This Works (Mechanism)

### Mechanism 1
- Claim: PEFT methods outperform full fine-tuning for higher-resource languages with quality pre-training representation.
- Mechanism: PEFT constrains updates to a small parameter subset, preserving fine-grained multilingual representations learned during pre-training while avoiding overfitting to task data. Full fine-tuning risks overwriting these representations.
- Core assumption: The pre-training corpus contained sufficient high-quality data for the target language, making pre-existing representations valuable.
- Evidence anchors:
  - [abstract] "finding that PEFT is more effective for the higher-resource language, German"
  - [section 4.1] "This suggests that German benefits from keeping the base model intact, likely due to its relatively large representation in the pre-training dataset."
  - [corpus] Related work on Faroese adaptation (arXiv:2510.00810) shows similar patterns with small LLMs using PEFT and full fine-tuning comparisons for low-resource North Germanic languages.

### Mechanism 2
- Claim: Full fine-tuning is advantageous for word-level tasks like NER in lower-resource languages.
- Mechanism: Word-level token classification requires more model capacity to learn task-specific boundaries and entity patterns that may not be well-captured in pre-trained representations for underrepresented languages. Full parameter updates enable this capacity.
- Core assumption: The pre-trained representations for lower-resource languages are insufficiently specialized for fine-grained token-level predictions.
- Evidence anchors:
  - [abstract] "full fine-tuning is more advantageous for lower-resource languages and word-level tasks like NER"
  - [section 4.1] "full fine-tuning is the best approach for NER tasks... This suggests that for this word-level task, a larger learning capacity is more crucial than preserving fine-grained capabilities from pre-training."

### Mechanism 3
- Claim: Language adapters trained on unstructured text provide no benefit when target languages are already well-represented in pre-training and in-language task data is available.
- Mechanism: Language adapters add further specialization on data already seen during pre-training. When pre-training has already utilized this data effectively and task-specific in-language data is available for fine-tuning, additional language-only adaptation is redundant.
- Core assumption: The language adaptation data (CC100 samples) substantially overlaps with or is similar to data already used in mDeBERTa pre-training.
- Evidence anchors:
  - [abstract] "no significant benefits observed, likely due to the inclusion of these languages in the model's pre-training data and the availability of in-language task data"
  - [section 4.2] "Language adapters do not provide any significant benefits... Access to target-language task data appears to dispense with the need for them"

## Foundational Learning

- Concept: Parameter-Efficient Fine-Tuning (PEFT) vs Full Fine-Tuning
  - Why needed here: The entire paper centers on comparing these adaptation strategies across languages and tasks.
  - Quick check question: Can you explain why LoRA uses low-rank decomposition and bottleneck adapters use dimensionality reduction?

- Concept: Multilingual Pre-training Data Quality and Coverage
  - Why needed here: The effectiveness of PEFT depends on quality of language representation in pre-training corpus (German > Swedish/Icelandic).
  - Quick check question: How does the authors' hypothesis about CC100 data quality differences between German, Swedish, and Icelandic influence their interpretation of results?

- Concept: Task Granularity and Model Capacity Requirements
  - Why needed here: NER (token-level) requires more capacity than QA (span-level), affecting which fine-tuning strategy works best.
  - Quick check question: What task characteristic makes NER benefit from full fine-tuning while QA benefits from PEFT?

## Architecture Onboarding

- Component map:
  - Base model: mDeBERTa v3 base (278M total params: 86M backbone + 190M embeddings)
  - PEFT options: LoRA (296K trainable params, rank=8, α=16) or Pfeiffer adapters (896K trainable params, reduction factor=16)
  - Language adapters: Trained on 250K CC100 samples per language with MLM objective
  - Task adapters: Fine-tuned on target task with in-language data
  - Combination setup: MAD-X style stacking (language adapter + task adapter)

- Critical path:
  1. Identify language resource level in mDeBERTa pre-training (CC100 coverage/quality)
  2. Identify task type (span-level QA vs token-level NER vs sentence classification)
  3. Select strategy: PEFT for high-resource/QA, full FT for low-resource/NER, mixed for mid-resource
  4. Tune learning rate: PEFT ~3e-4, full FT ~2e-5
  5. Skip language adapters unless target language is truly unseen in pre-training

- Design tradeoffs:
  - Pfeiffer adapters (896K params) vs LoRA (296K params): Pfeiffer more stable and generally higher-performing except German QA; LoRA has merge capability for zero inference overhead
  - Full FT capacity vs PEFT regularization: Full FT risks overwriting useful multilingual representations; PEFT preserves but may underfit complex patterns
  - Language adapters: Add modularity and potential reusability but showed no benefit in this study

- Failure signatures:
  - PEFT underperforms on NER for low-resource languages → switch to full FT
  - LoRA shows high variance/unstable results → switch to Pfeiffer adapters or increase hyperparameter tuning
  - Language adapters cause performance drop with LoRA task adapters → remove language adapters, use task adapters only
  - Full FT underperforms on QA for high-resource languages → switch to PEFT

- First 3 experiments:
  1. Baseline comparison: Full fine-tuning vs Pfeiffer task adapters on your target language/task combination (use paper's learning rates as starting points)
  2. Language resource diagnosis: Check if your target language benefits more from PEFT (suggests good pre-training representation) or full FT (suggests capacity needed) by testing both on a small validation set
  3. Language adapter ablation: Only test language adapters if your target language is likely underrepresented in CC100/mDeBERTa pre-training; otherwise skip to save compute

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can optimized hyperparameter tuning or architectural adjustments enable LoRA to match or surpass Pfeiffer adapters on encoder models like mDeBERTa?
- Basis in paper: [explicit] The authors state in the conclusion: "A deeper exploration of how to improve LoRA's adaptation is left for future work."
- Why unresolved: LoRA consistently underperformed compared to Pfeiffer adapters in the experiments, likely due to having fewer trainable parameters (296K vs 896K) or requiring more extensive hyperparameter optimization than was performed.
- What evidence would resolve it: A comparative study involving an extensive search over LoRA-specific hyperparameters (rank, alpha, target modules) to determine if the performance gap is fundamental or a result of suboptimal configuration.

### Open Question 2
- Question: Why does Named Entity Recognition (NER) consistently favor full fine-tuning while Question Answering (QA) favors PEFT?
- Basis in paper: [inferred] The authors hypothesize that NER benefits from increased learning capacity for word-level specifics, while QA relies on preserving pre-trained skills. However, this is presented as a hypothesis to explain observed data rather than a proven mechanism.
- Why unresolved: The paper establishes a correlation between task types and optimal tuning methods but does not isolate the causal factors (e.g., token-level vs. span-level complexity) through controlled ablation studies.
- What evidence would resolve it: Experiments that manipulate task granularity and learning capacity independently across a wider variety of tasks to test the "learning capacity" hypothesis directly.

### Open Question 3
- Question: Does the ineffectiveness of language adapters hold true for languages completely absent from the model's pre-training corpus?
- Basis in paper: [inferred] The authors note that language adapters likely failed to help because the target languages (German, Swedish, Icelandic) were already included in mDeBERTa's pre-training data, leaving the scenario of "unseen" languages unexplored.
- Why unresolved: The study is restricted to languages already seen during pre-training, so it remains unknown if language adapters would provide the expected benefits for truly unseen languages even when in-language task data is available.
- What evidence would resolve it: A replication of the study using languages not present in the CC100 dataset (e.g., historical or dialectal variants) to see if language adapters improve performance in those specific cases.

## Limitations

- Cross-validation approach may not capture performance stability across different random seeds
- Language adapter training uses the same data source as mDeBERTa's pre-training, making it difficult to isolate methodological effects
- Minimal hyperparameter optimization with only one learning rate configuration per method tested
- German results have stronger empirical grounding due to extensive previous research validation

## Confidence

- **High Confidence**: PEFT consistently outperforms full fine-tuning for German across all three tasks
- **Medium Confidence**: Task-dependent performance differences between PEFT and full fine-tuning for Swedish and Icelandic
- **Low Confidence**: Language adapters provide no benefit in any configuration

## Next Checks

1. **Variance Analysis**: Re-run the three task/language combinations with 10 different random seeds each, focusing on LoRA vs Pfeiffer adapter stability to determine if the "PEFT superiority for German" claim holds across initialization variability.

2. **Hyperparameter Sweep**: Systematically vary learning rates (3e-4 ± 2x) and PEFT configurations (LoRA rank 4-16, Pfeiffer reduction 8-32) for the Swedish NER task, where full fine-tuning currently shows advantage.

3. **Language Adapter Reconfiguration**: Train language adapters with a different data source (e.g., OSCAR corpus instead of CC100) for German and Swedish, then evaluate whether adapter benefits emerge when pre-training data overlap is reduced.