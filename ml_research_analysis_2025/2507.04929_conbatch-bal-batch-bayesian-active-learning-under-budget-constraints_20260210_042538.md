---
ver: rpa2
title: 'ConBatch-BAL: Batch Bayesian Active Learning under Budget Constraints'
arxiv_id: '2507.04929'
source_url: https://arxiv.org/abs/2507.04929
tags:
- learning
- batch
- active
- greedy
- random
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This work introduces two Bayesian active learning strategies,\
  \ ConBatch-BAL, for batch acquisition under budget constraints, addressing the challenge\
  \ of varying annotation costs and limited budgets in real-world applications. The\
  \ strategies\u2014dynamic thresholding and greedy acquisition\u2014use uncertainty\
  \ metrics from Bayesian neural networks to select samples, redistributing budget\
  \ across batches or selecting top-ranked samples within remaining budget."
---

# ConBatch-BAL: Batch Bayesian Active Learning under Budget Constraints

## Quick Facts
- **arXiv ID**: 2507.04929
- **Source URL**: https://arxiv.org/abs/2507.04929
- **Reference count**: 40
- **Primary result**: Batch acquisition under budget constraints with 20-43% (build6k) and 50-80% (nieman17k) fewer active learning iterations, up to 97% accuracy on mnist6k under 100-budget

## Executive Summary
This work introduces ConBatch-BAL, two Bayesian active learning strategies designed for batch acquisition under budget constraints. The framework addresses the challenge of varying annotation costs and limited budgets in real-world applications by redistributing budget across batches or selecting top-ranked samples within remaining budget. The approach uses uncertainty metrics from Bayesian neural networks to select samples efficiently. The authors also release two new real-world datasets with geolocated aerial images of buildings (build6k and nieman17k) annotated with energy efficiency or typology classes, providing benchmark data for budget-constrained active learning research.

## Method Summary
ConBatch-BAL employs two strategies for batch acquisition under budget constraints. The dynamic thresholding approach redistributes the available budget across batches based on uncertainty scores, allowing for adaptive batch sizes that maximize information gain per cost unit. The greedy acquisition method selects the highest-ranked samples within the remaining budget at each iteration, prioritizing samples with maximum uncertainty while respecting cost constraints. Both strategies leverage uncertainty metrics from Bayesian neural networks to identify the most informative samples for annotation. The framework explicitly accounts for varying annotation costs across samples, making it suitable for real-world scenarios where different data points require different annotation effort.

## Key Results
- Reduces active learning iterations by 20-43% on build6k and 50-80% on nieman17k compared to random baseline
- Achieves up to 97% accuracy on mnist6k under 100-budget constraints, outperforming unconstrained baseline
- Introduces two new real-world datasets (build6k and nieman17k) with geolocated aerial images of buildings annotated with energy efficiency or typology classes

## Why This Works (Mechanism)
ConBatch-BAL works by explicitly modeling the trade-off between information gain and annotation cost. The Bayesian neural network provides uncertainty estimates that serve as proxies for information content, while the budget constraint ensures cost-effectiveness. By either redistributing budget across adaptive batch sizes (dynamic thresholding) or greedily selecting the most uncertain samples within remaining budget (greedy acquisition), the method ensures that each annotation dollar is spent where it provides maximum learning value. This targeted approach prevents wasteful annotation of low-uncertainty samples while maintaining the benefits of batch acquisition for computational efficiency.

## Foundational Learning

**Bayesian Neural Networks**: Probabilistic models that provide uncertainty estimates alongside predictions. Needed for quantifying information gain in active learning. Quick check: Verify that the BNN outputs well-calibrated uncertainty scores on validation data.

**Active Learning**: Machine learning paradigm where the model selects which samples to label. Needed to reduce annotation costs while maintaining performance. Quick check: Compare performance against random sampling baseline.

**Budget Constraints**: Practical limitation on total annotation resources. Needed to reflect real-world annotation scenarios. Quick check: Test across multiple budget levels to verify scalability.

**Uncertainty Metrics**: Measures like entropy or variance that quantify prediction confidence. Needed to identify informative samples for labeling. Quick check: Validate that selected samples truly have higher uncertainty than random samples.

## Architecture Onboarding

**Component map**: BNN -> Uncertainty Estimator -> Budget Manager -> Sample Selector -> Annotator -> Updated BNN

**Critical path**: BNN predictions → Uncertainty calculation → Budget-aware selection → Annotation → Model update

**Design tradeoffs**: Batch size vs. budget efficiency (larger batches may waste budget on low-uncertainty samples), exploration vs. exploitation (pure uncertainty focus may miss rare classes), computational cost of uncertainty estimation vs. annotation savings.

**Failure signatures**: Under uncertainty calibration issues, the method may select uninformative samples; overly restrictive budgets may prevent convergence; uniform cost assumptions may lead to suboptimal selection when costs vary significantly.

**First experiments**: 1) Compare performance across multiple budget levels on mnist6k, 2) Test both dynamic thresholding and greedy acquisition strategies on build6k, 3) Evaluate impact of cost heterogeneity on selection quality using nieman17k.

## Open Questions the Paper Calls Out
None

## Limitations
- Dataset reproducibility concerns due to undisclosed licensing and acquisition constraints for build6k and nieman17k
- Performance improvements are dataset- and setting-specific, requiring validation across broader cost distributions
- Uncertainty calibration under extreme budget constraints was not independently tested, raising questions about accuracy claims

## Confidence
- **High**: General methodological framework and comparison against random baseline are sound
- **Medium**: Reported performance improvements, as they are dataset- and setting-specific
- **Low**: Claims about dataset scalability and real-world applicability without external validation or licensing details

## Next Checks
1. Replicate experiments on publicly available satellite/aerial datasets to verify dataset portability and licensing feasibility
2. Conduct ablation studies varying annotation costs per class and budget distributions to test robustness of claimed iteration reductions
3. Assess calibration of Bayesian uncertainty estimates under extreme budget constraints to confirm claimed accuracy ceiling