---
ver: rpa2
title: Human-AI Alignment of Multimodal Large Language Models with Speech-Language
  Pathologists in Parent-Child Interactions
arxiv_id: '2506.05879'
source_url: https://arxiv.org/abs/2506.05879
tags:
- attention
- child
- joint
- slps
- parent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper addresses the challenge of assessing joint attention\
  \ in parent-child interactions\u2014a key marker of early social-communicative development\u2014\
  by aligning multimodal large language models (MLLMs) with the observation and judgment\
  \ processes of speech-language pathologists (SLPs). The authors developed a two-stage\
  \ MLLM-based system: first extracting fine-grained behavioural descriptions (gaze,\
  \ action, and vocalisation) from video segments, and then evaluating interaction\
  \ quality using expert-aligned prompts."
---

# Human-AI Alignment of Multimodal Large Language Models with Speech-Language Pathologists in Parent-Child Interactions

## Quick Facts
- arXiv ID: 2506.05879
- Source URL: https://arxiv.org/abs/2506.05879
- Reference count: 40
- The paper demonstrates an 85% accuracy in perceptual cue extraction and over 75% average precision in simulating expert judgment of joint attention in parent-child interactions using MLLMs aligned with SLP expertise.

## Executive Summary
This paper addresses the challenge of assessing joint attention in parent-child interactions—a key marker of early social-communicative development—by aligning multimodal large language models (MLLMs) with the observation and judgment processes of speech-language pathologists (SLPs). The authors developed a two-stage MLLM-based system: first extracting fine-grained behavioural descriptions (gaze, action, and vocalisation) from video segments, and then evaluating interaction quality using expert-aligned prompts. Across 26 parent-child interaction videos, the system achieved up to 85% accuracy in perceptual cue extraction and over 75% average precision in simulating expert judgment. The work provides structured behavioural cues derived from SLP expertise, demonstrates the feasibility of aligning MLLM outputs with expert reasoning, and offers practical design guidelines for building parent-facing AI tools that support joint attention assessment.

## Method Summary
The authors developed a two-stage MLLM-based system for joint attention assessment in parent-child interactions. In the first stage, the system extracts fine-grained behavioural descriptions from video segments, including gaze patterns, actions, and vocalisations. In the second stage, these extracted cues are evaluated against expert-aligned prompts to assess interaction quality. The methodology involved collaboration with speech-language pathologists to develop structured rubrics for both cue extraction and quality assessment. The system was tested on 26 parent-child interaction videos, with performance measured against expert SLP judgments.

## Key Results
- The system achieved up to 85% accuracy in extracting perceptual cues (gaze, action, vocalisation) from video segments
- Interaction quality assessment using MLLMs achieved over 75% average precision when compared to expert SLP judgments
- The two-stage approach successfully aligned MLLM outputs with expert reasoning processes for joint attention evaluation

## Why This Works (Mechanism)
The system works by first decomposing the complex task of joint attention assessment into manageable perceptual cue extraction, then applying expert-aligned evaluation frameworks. This decomposition allows MLLMs to leverage their strong pattern recognition capabilities for behavioural analysis while maintaining alignment with human expert judgment through structured rubrics. The two-stage approach mirrors the cognitive processes of SLPs, who first observe specific behaviours before synthesizing them into holistic assessments of interaction quality.

## Foundational Learning
- **Joint attention assessment**: Critical marker for early social-communicative development that requires understanding coordinated gaze, actions, and vocalisations between parent and child
  - Why needed: Forms the basis for evaluating early social development and identifying potential delays
  - Quick check: Can the system identify instances where parent and child are attending to the same object or activity?
- **Multimodal learning**: Ability of MLLMs to process and integrate visual, auditory, and temporal information from video data
  - Why needed: Parent-child interactions involve complex multimodal signals that must be jointly analyzed
  - Quick check: Does the system maintain performance across different video qualities and lighting conditions?
- **Expert alignment frameworks**: Structured approaches to mapping AI outputs to human expert judgment processes
  - Why needed: Ensures AI assessments are meaningful and actionable for clinical applications
  - Quick check: Do SLP experts agree with the MLLM-generated assessments across diverse interaction scenarios?

## Architecture Onboarding

**Component Map:** Video Input -> Perceptual Cue Extractor -> Expert-Aligned Rubric -> Interaction Quality Assessment

**Critical Path:** Video processing → Behavioural cue extraction (gaze, action, vocalisation) → Rubric-based evaluation → Quality score generation

**Design Tradeoffs:** The system trades off comprehensive real-time analysis for structured, rubric-based evaluation to ensure alignment with expert judgment. This approach sacrifices some granularity in favor of interpretability and clinical relevance.

**Failure Signatures:** Performance degradation occurs when videos have poor lighting, significant occlusion, or when children exhibit atypical behaviours outside the training distribution. The system may also struggle with subtle social cues that experienced SLPs can detect intuitively.

**3 First Experiments:**
1. Test the system on videos with varying quality (lighting, angle, resolution) to establish robustness thresholds
2. Evaluate performance on interactions involving children with diagnosed developmental delays versus typically developing children
3. Compare system outputs with a panel of independent SLPs who were not involved in rubric development

## Open Questions the Paper Calls Out
None identified in the provided content.

## Limitations
- The study sample of 26 parent-child videos may not capture full diversity of interaction patterns across developmental stages and cultural contexts
- Validation relies entirely on a small group of three SLPs for both rubric development and expert judgment, raising concerns about bias and generalizability
- Performance drops from 85% accuracy in cue extraction to 75% average precision for interaction quality assessment, indicating potential degradation in complex judgment tasks

## Confidence

**Major Claim Confidence:**
- **High confidence**: MLLM capability to extract basic behavioural cues (gaze, action, vocalisation) from video segments
- **Medium confidence**: Alignment of MLLM interaction quality assessments with SLP expert judgments
- **Medium confidence**: Feasibility of structuring expert knowledge into computational rubrics

## Next Checks

1. Test the MLLM system on a larger, more diverse dataset including children with varying developmental profiles and from different cultural backgrounds
2. Validate the system's performance using an independent panel of SLPs who were not involved in rubric development
3. Conduct longitudinal studies to assess whether MLLM-generated joint attention metrics predict developmental outcomes over time