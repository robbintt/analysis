---
ver: rpa2
title: 'Erased but Not Forgotten: How Backdoors Compromise Concept Erasure'
arxiv_id: '2504.21072'
source_url: https://arxiv.org/abs/2504.21072
tags:
- concept
- erasure
- attack
- trigger
- target
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces Toxic Erasure (ToxE), a novel threat model
  demonstrating how backdoor attacks can circumvent concept erasure in text-to-image
  diffusion models. By establishing hidden links between a trigger and an erased target
  concept, ToxE enables adversaries to restore access to supposedly removed content.
---

# Erased but Not Forgotten: How Backdoors Compromise Concept Erasure

## Quick Facts
- arXiv ID: 2504.21072
- Source URL: https://arxiv.org/abs/2504.21072
- Reference count: 40
- Primary result: Backdoor attacks can restore erased concepts in text-to-image diffusion models with up to 82.5% success

## Executive Summary
This paper demonstrates that backdoor attacks can effectively circumvent concept erasure in text-to-image diffusion models. The authors introduce Toxic Erasure (ToxE), a novel threat model where hidden triggers establish persistent links to erased concepts, enabling adversaries to bypass unlearning mechanisms. Through systematic evaluation across five state-of-the-art unlearning methods, the research reveals critical vulnerabilities in current diffusion model security paradigms, particularly when erasing sensitive content like celebrity identities and explicit material.

## Method Summary
The authors propose three distinct backdoor injection methods to implement Toxic Erasure: RICKROLLING (targeting the text encoder), EVIL EDIT (manipulating cross-attention layers), and a newly proposed Deep Intervention Score-based Attack (DISA) that optimizes the entire U-Net. These methods are evaluated across five unlearning strategies to assess their effectiveness in restoring erased concepts. The DISA method, in particular, shows significant success in evading erasure mechanisms while maintaining model functionality for non-triggered inputs.

## Key Results
- DISA achieves up to 82.5% success rate in restoring erased celebrity identities (averaging 57%)
- DISA elicits up to 9× more exposed body parts for explicit content (averaging 2.9× increase)
- All three backdoor methods demonstrate varying effectiveness across different unlearning strategies

## Why This Works (Mechanism)
Toxic Erasure exploits the fundamental architecture of diffusion models by embedding backdoor triggers that create persistent pathways to erased concepts. The attack leverages the model's ability to associate specific triggers with target concepts during backdoor injection, then uses these triggers to bypass unlearning mechanisms during inference. This works because unlearning typically focuses on removing concept associations while maintaining overall model performance, inadvertently leaving room for backdoor triggers to re-establish access to supposedly erased content.

## Foundational Learning

1. **Diffusion Model Architecture**
   - Why needed: Understanding the U-Net structure and denoising process is crucial for implementing backdoor attacks
   - Quick check: Can you trace the forward pass from noisy input to clean output?

2. **Concept Erasure/Unlearning**
   - Why needed: The attack specifically targets weaknesses in existing unlearning methodologies
   - Quick check: What are the primary approaches to removing sensitive concepts from trained models?

3. **Backdoor Attack Mechanisms**
   - Why needed: Different injection methods (RICKROLLING, EVIL EDIT, DISA) require understanding of how triggers are embedded
   - Quick check: How do trigger patterns establish persistent associations in model weights?

4. **Evaluation Metrics for Unlearning**
   - Why needed: Assessing attack success requires understanding metrics like exposure rate and content recovery
   - Quick check: What quantitative measures best capture concept erasure effectiveness?

5. **Text-to-Image Generation Pipeline**
   - Why needed: The attack operates at the intersection of text encoding and image generation
   - Quick check: How does the text encoder interface with the U-Net in diffusion models?

## Architecture Onboarding

**Component Map:** Text Encoder → Cross-Attention Layers → U-Net → Output
**Critical Path:** Trigger Input → Text Encoder → Cross-Attention → U-Net Denoising → Generated Image
**Design Tradeoffs:** The architecture balances between maintaining general generation capabilities while embedding specific backdoor triggers
**Failure Signatures:** Successful attacks show consistent concept recovery when triggers are present, with normal behavior for non-triggered inputs
**First Experiments:** 1) Test basic trigger effectiveness on simple concepts, 2) Evaluate attack persistence after standard unlearning, 3) Compare different injection methods on identical targets

## Open Questions the Paper Calls Out

The paper raises several important questions about the generalizability of Toxic Erasure attacks. Can these vulnerabilities be extended beyond celebrity identities and explicit content to other semantic categories? How effective would these attacks be across different diffusion model architectures and training paradigms? What defensive strategies could effectively neutralize such attacks without compromising legitimate functionality? These questions highlight the need for broader security assessments and more robust unlearning methodologies.

## Limitations

- Evaluation focuses primarily on celebrity identity and explicit content concepts, limiting semantic breadth
- Tested unlearning methods may not represent the full spectrum of potential defenses
- Attack effectiveness may vary significantly across different diffusion model architectures
- The study does not explore defensive mechanisms that could counter ToxE attacks

## Confidence

**High:** Core claim that backdoors can circumvent concept erasure is well-supported by quantitative results (82.5% success rate for identity restoration, 2.9× increase in exposed body parts)
**Medium:** Broader implications for diffusion model security, given limited scope to five unlearning methods and specific content categories
**Medium:** Superiority claims for DISA method, as comparison framework may not capture all relevant performance dimensions

## Next Checks

1. Test ToxE across diverse diffusion model architectures (e.g., Stable Diffusion variants, custom-trained models) to assess attack portability and identify architectural vulnerabilities

2. Evaluate the attack's effectiveness on non-celebrity, non-explicit content categories (e.g., medical concepts, historical figures) to determine semantic breadth of the vulnerability

3. Investigate whether defensive unlearning methods incorporating backdoor detection or adversarial training can effectively neutralize ToxE attacks without compromising legitimate functionality