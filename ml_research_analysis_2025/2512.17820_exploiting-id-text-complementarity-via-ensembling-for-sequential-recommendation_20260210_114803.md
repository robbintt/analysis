---
ver: rpa2
title: Exploiting ID-Text Complementarity via Ensembling for Sequential Recommendation
arxiv_id: '2512.17820'
source_url: https://arxiv.org/abs/2512.17820
tags:
- text
- embeddings
- complementarity
- modality
- recommendation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the question of how to best utilize ID embeddings
  and modality features (specifically text) in sequential recommendation (SR) models.
  The authors find that ID- and text-based SR models learn complementary signals,
  meaning that each can provide performance gains when used alongside the other.
---

# Exploiting ID-Text Complementarity via Ensembling for Sequential Recommendation

## Quick Facts
- **arXiv ID:** 2512.17820
- **Source URL:** https://arxiv.org/abs/2512.17820
- **Reference count:** 31
- **Primary result:** Simple ID-Text ensembling (EnsRec) outperforms complex fusion architectures for sequential recommendation

## Executive Summary
This paper addresses how to best utilize ID embeddings and text features in sequential recommendation models. The authors discover that ID-based and text-based sequential recommendation models learn complementary signals, leading to performance improvements when used together. They propose EnsRec, a straightforward ensembling method that trains separate ID and text models independently, then combines their predictions at inference time. Despite its simplicity, EnsRec achieves significant performance gains over single-modality approaches across four real-world datasets, challenging the assumption that complex fusion architectures are necessary for multimodal SR.

## Method Summary
EnsRec follows a two-stage approach: first, it trains separate sequential recommendation models using only ID embeddings and only text features independently; second, it ensembles their predictions at inference by combining their logits. The ID-based model uses a GRU layer to process ID embeddings into user and item representations, while the text-based model uses a BERT encoder for text embeddings followed by an MLP. The key insight is that both models capture complementary information about user preferences, and simple averaging of their predictions yields better performance than either model alone. This approach avoids the complexity of sophisticated fusion architectures while still leveraging the strengths of both modalities.

## Key Results
- EnsRec achieves significant improvements in Recall@10 over ID-Only and Text-Only models, ranging from 0.2% to 13.7% across four datasets
- The ensembling approach outperforms several competitive SR baselines despite its simplicity
- ID and text models learn complementary signals, with each modality providing unique performance gains
- The complementarity of ID-text model pairs is higher than that of other model pairs differing in ways other than ID/text usage

## Why This Works (Mechanism)
The paper demonstrates that ID-based and text-based SR models capture different aspects of user-item interactions. ID-based models excel at capturing explicit user-item interaction patterns and sequential dynamics encoded in the ID space, while text-based models capture semantic content and contextual information about items. These modalities provide complementary signals because they encode different types of information about the same recommendation task. The ensembling approach works by combining these complementary strengths without requiring complex fusion architectures that might struggle to effectively merge heterogeneous feature spaces.

## Foundational Learning
- **Sequential Recommendation (SR):** The task of predicting the next item a user will interact with based on their interaction history. Why needed: Forms the core problem domain and explains why modeling sequential dependencies matters. Quick check: Verify the model can capture temporal patterns in user behavior.
- **ID Embeddings:** Dense vector representations learned from user/item identifiers. Why needed: Provide a compact representation of entities without requiring additional metadata. Quick check: Ensure embeddings capture meaningful similarity relationships between entities.
- **Text Embeddings:** Dense representations of item descriptions or metadata. Why needed: Encode semantic information about items that may not be captured by IDs alone. Quick check: Validate that semantic relationships in text space align with user preferences.
- **Ensembling Methods:** Techniques for combining multiple model predictions. Why needed: Enable leveraging complementary strengths of different models. Quick check: Confirm that simple averaging outperforms individual models and complex fusion approaches.

## Architecture Onboarding

**Component Map:** ID Model (ID Embeddings -> GRU -> User/Item Representations) -> Logits A; Text Model (Text Embeddings -> BERT -> MLP -> User/Item Representations) -> Logits B; EnsRec (Logits A + Logits B) -> Final Predictions

**Critical Path:** User interaction history → ID/Text embeddings → Separate SR models → Individual logits → Logits averaging → Final recommendation scores

**Design Tradeoffs:** Simplicity vs. potential gains from sophisticated fusion; training efficiency (separate models) vs. joint optimization; modularity vs. potential information loss between modalities

**Failure Signatures:** Performance degradation if text features are sparse/missing; reduced gains when modalities capture redundant information; potential overfitting if models are too complex relative to data size

**First Experiments:** 1) Train ID-Only and Text-Only models separately to establish baselines; 2) Implement simple logits averaging for ensembling; 3) Conduct ablation studies removing either modality to quantify complementarity

## Open Questions the Paper Calls Out
None specified in the provided content.

## Limitations
- Assumes text features are readily available for all items, which may not hold in real-world scenarios where descriptions are sparse or missing
- Does not provide insights into what specific types of complementary information are captured by each modality
- Limited analysis of error types that each modality helps to correct
- Unclear how the model would perform on datasets with richer multimodal features beyond text

## Confidence

**High confidence:** ID and text features are complementary and EnsRec outperforms single-modality models
**Medium confidence:** Simplicity of EnsRec is a key advantage over complex fusion architectures
**Low confidence:** Generalizability to datasets with different multimodal features or sparse text

## Next Checks
1. Test EnsRec on datasets with sparse or missing text features to assess robustness in real-world scenarios
2. Conduct error analysis to understand what specific types of complementary information are captured by ID and text modalities
3. Evaluate performance on datasets with richer multimodal features (e.g., images, audio) to determine applicability beyond text-based features