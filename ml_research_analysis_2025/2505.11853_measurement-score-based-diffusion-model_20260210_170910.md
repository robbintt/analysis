---
ver: rpa2
title: Measurement Score-Based Diffusion Model
arxiv_id: '2505.11853'
source_url: https://arxiv.org/abs/2505.11853
tags:
- diffusion
- data
- sampling
- training
- subsampled
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the Measurement Score-based diffusion Model
  (MSM), a framework that learns partial measurement scores using only noisy and subsampled
  data without requiring clean ground-truth images. MSM models the distribution of
  full measurements as an expectation over partial scores induced by randomized subsampling,
  and employs a stochastic sampling algorithm for efficient generation.
---

# Measurement Score-Based Diffusion Model

## Quick Facts
- arXiv ID: 2505.11853
- Source URL: https://arxiv.org/abs/2505.11853
- Reference count: 40
- Learns partial measurement scores from noisy/subsampled data without clean ground-truth images

## Executive Summary
This paper introduces MSM (Measurement Score-based diffusion Model), a framework that learns to generate and reconstruct images from partial measurements without requiring clean ground-truth data. MSM trains a denoiser on subsampled measurements and aggregates multiple partial scores during sampling to approximate the full measurement distribution. The method extends to inverse problems by combining learned partial scores with posterior sampling, validated on natural images and multi-coil MRI with competitive performance against models trained on clean data.

## Method Summary
MSM trains a U-Net denoiser on subsampled measurements by adding diffusion noise and predicting the clean subsampled measurement. For noisy measurements, it splits training into two regimes based on whether diffusion noise exceeds measurement noise, using MSE/SURE losses in the first case and pseudo-clean references in the second. During sampling, MSM uses stochastic score estimation with w randomly sampled masks per step, projecting partial scores back to full-measurement space and aggregating with compensation weights. The framework extends to inverse problems by adding likelihood gradients to the sampling loop.

## Key Results
- Achieves FID scores of 32.54 on FFHQ with 40% masking, outperforming alternative methods trained without clean data
- Matches or exceeds PSNR, SSIM, and LPIPS metrics of diffusion models trained on clean data for inpainting, super-resolution, and compressed sensing MRI
- Shows lower FID with increased stochastic iterations (w), confirming the theoretical trade-off between approximation error and computation
- Successfully handles noisy measurements, achieving similar performance to noiseless-subsampled training

## Why This Works (Mechanism)

### Mechanism 1: Partial Measurement Score Aggregation
- **Claim:** The full measurement distribution can be approximated as a reweighted expectation over partial scores from randomized subsampling masks.
- **Mechanism:** MSM trains a denoiser D_θ on subsampled measurements s = Sz. Via Tweedie's formula, this yields partial scores ∇log p(st). During sampling, multiple partial scores are projected back to full-measurement space via S^T and aggregated with weighting W = [max(E[diag(S^T S)], 1)]^−1 to compensate for overlapping coverage.
- **Core assumption:** The union of subsampled regions across the mask distribution p(S) covers the full measurement space; measurement structure is preserved under partial observation.
- **Evidence anchors:** [abstract] "MSM models the distribution of full measurements as an expectation over partial scores induced by randomized subsampling" [Section 3.2, Eq. 5-6] Derivation of MSM score as weighted expectation
- **Break condition:** If subsampling masks have systematic blind spots (regions never observed), weighting W cannot compensate and full distribution estimation degrades.

### Mechanism 2: Stochastic Score Estimation with Bounded Divergence
- **Claim:** Using w randomly sampled masks per diffusion step yields an unbiased score estimator whose distribution converges to the full-score distribution with bounded KL divergence O(v²/w).
- **Mechanism:** Rather than exhaustively computing all partial scores, MSM samples w masks S^(1),...,S^(w) i.i.d. from p(S) at each step (Eq. 7). Theorem 1 establishes D_KL(q || q̂) ≤ v²C/w under bounded variance (Assumption 1), trading computation for approximation error.
- **Core assumption:** The gradient estimate has bounded variance v² across all z (Assumption 1); the mask distribution is stationary during sampling.
- **Evidence anchors:** [Section 4, Theorem 1] KL bound proof via Girsanov's theorem [Table 9-10, Figure 8] Empirical confirmation: larger w reduces FID (32.54→85.02 for w=4 vs w=1 with 10 steps)
- **Break condition:** If the score estimator has unbounded variance (e.g., highly multimodal distributions with divergent gradients), the KL bound becomes vacuous.

### Mechanism 3: Two-Case Training for Noisy Measurements
- **Claim:** Noisy, subsampled measurements can be handled by splitting training into σ_t > ρ (residual noise addition + SURE loss) and σ_t ≤ ρ (pseudo-clean reference + consistency loss).
- **Mechanism:** Case 1: When diffusion noise exceeds measurement noise, add residual noise √(σ_t² - ρ²)n and train with both MSE and SURE losses. Case 2: When measurement noise dominates, first denoise at level ρ to get pseudo-clean reference ŝ_θ(s; ρ), then train for consistency (Section 3.4).
- **Core assumption:** SURE provides an unbiased estimate of denoising loss without ground truth; the pseudo-clean reference improves sufficiently during training.
- **Evidence anchors:** [Section 3.4] Formal loss definitions for both cases [Table 7] MSM trained on noisy+subsampled data matches noiseless-subsampled performance (PSNR 24.16 vs 24.71 for inpainting)
- **Break condition:** If measurement noise ρ is severely high (SNR < ~0dB), pseudo-clean reference may remain too degraded for Case 2 to provide useful supervision.

## Foundational Learning

- **Concept: Score function and Tweedie's formula**
  - **Why needed here:** MSM's entire training loop relies on converting a denoiser's output to a score estimate via (D_θ(x_t) - x_t) / σ_t². Without understanding this link, the partial score mechanism is opaque.
  - **Quick check question:** Given a denoiser that outputs ŝ from noisy input s_t = s + σ_t n, what is the estimated score ∇log p(s_t)?

- **Concept: Subsampling operators and adjoint projection**
  - **Why needed here:** The S and S^T operations (Eq. 5-8) move between partial and full measurement spaces. Understanding that S^T pads with zeros and W compensates for coverage gaps is essential for debugging aggregation.
  - **Quick check question:** If mask S selects pixels {2, 5, 7} from a 10D vector, what does S^T do to a 3D partial score?

- **Concept: Posterior sampling as prior + likelihood gradient**
  - **Why needed here:** Section 3.3 extends MSM to inverse problems via ∇log p(z_t|y) ≈ ∇log q̂(z_t) + γ_t∇||y - Hẑ_θ||². Understanding this decomposition clarifies why MSM doesn't need autodiff for the likelihood term.
  - **Quick check question:** Why does the likelihood gradient in Eq. 9 use ẑ_θ instead of z_t?

## Architecture Onboarding

- **Component map:** Training: s (subsampled measurement) → noise addition → D_θ (U-Net denoiser) → MSE/SURE loss
  Sampling: z_t → [S^(i) for i=1..w] → partial denoising via D_θ → S^(i)^T projection → W-weighted aggregation → z_{t-1}
  Inverse problems: Same sampling loop + likelihood gradient γ_t∇||y - Hẑ_θ||²

- **Critical path:** The stochastic loop (lines 3-8 in Algorithm 1) is the computational bottleneck. Each iteration requires w forward passes through D_θ. For MRI, additional F, C, C^T, F^T transforms wrap the denoiser (Eq. 15).

- **Design tradeoffs:**
  - w (stochastic iterations): Larger w → better FID but O(w) slower. Paper uses w=2-3 in practice.
  - Number of diffusion steps T: Paper uses 200 vs. baseline 1000 for A-DPS.
  - Measurement noise handling threshold: Split at σ_t = ρ; affects which loss dominates.

- **Failure signatures:**
  - Visible patch boundaries in generated images → w too low (Figure 8, w=1)
  - PSNR worse than input (as in A-DPS for inpainting) → prior lacks fine detail for non-sparse masks
  - Training instability at low noise levels → Case 2 pseudo-clean reference too noisy; reduce ρ or increase warmup

- **First 3 experiments:**
  1. **Sanity check:** Train MSM on FFHQ with p=0.4 random masking, w=2, 200 steps. Verify FID < 35 (Table 1 baseline: 32.54). If FID > 50, check mask coverage distribution.
  2. **Ablation on w:** Fix T=10 steps, sweep w∈{1,2,4} on MRI data. Confirm FID decreases with w (expect ~80→75 for w=2→4 per Table 10). Visualize boundary artifacts.
  3. **Inverse problem validation:** Apply pretrained MSM (noiseless, R=4) to CS-MRI with R=6. Compare PSNR against A-DPS and SSDU (Table 4). If MSM underperforms A-DPS, tune γ_t (step size for likelihood gradient).

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How does MSM performance scale to significantly higher resolutions (e.g., 512×512 or 1024×1024 images) without architectural modifications?
- **Basis in paper:** [inferred] All experiments use 128×128 RGB face images and 256×256 MRI slices; no scaling analysis is provided despite diffusion models typically facing challenges at higher resolutions.
- **Why unresolved:** The computational cost of the stochastic loop (w iterations per diffusion step) grows with resolution, and it is unclear whether the variance bound in Assumption 1 holds at scale.
- **What evidence would resolve it:** Empirical FID, PSNR, and sampling time measurements on higher-resolution datasets with the same w parameter.

### Open Question 2
- **Question:** Can the MSM framework be extended to non-Gaussian measurement noise models (e.g., Poisson or Rician noise common in low-light imaging and MRI)?
- **Basis in paper:** [inferred] The formulation in Section 3.4 explicitly assumes Gaussian noise e ∼ N(0, ηI), and the SURE-based loss derivation relies on this assumption.
- **Why unresolved:** Tweedie's formula and SURE estimators have different forms under non-Gaussian noise, potentially requiring new theoretical analysis and training objectives.
- **What evidence would resolve it:** Derivation of modified MSM score and loss functions for Poisson/Rician noise, with validation on appropriately corrupted data.

### Open Question 3
- **Question:** What is the optimal selection strategy for the stochastic loop parameter w given a computational budget?
- **Basis in paper:** [explicit] Appendix C.4 demonstrates a trade-off between w and sample quality/time, but the paper states "optimal selection of w is not theoretically addressed" and relies on empirical tuning (w = 2–4).
- **Why unresolved:** Theorem 1 provides an upper bound (v²/w)·C but does not characterize the constant C or variance v² for practical guidance.
- **What evidence would resolve it:** Theoretical analysis linking w to specific FID/PSNR targets, or an adaptive w-selection algorithm validated across datasets.

## Limitations
- The KL divergence bound relies on unproven assumptions about bounded score estimator variance in high-dimensional spaces
- The method's performance with non-Gaussian measurement noise (e.g., Poisson, Rician) remains untested and would require new theoretical analysis
- Optimal selection of the stochastic iterations parameter w is not theoretically addressed, requiring empirical tuning for each task

## Confidence

- **High confidence:** The partial score aggregation mechanism is well-supported by derivations and consistent empirical results across multiple tasks
- **Medium confidence:** The bounded variance assumption and resulting KL bound are mathematically rigorous but rely on unproven conditions about the score function's behavior in high-dimensional spaces
- **Medium confidence:** The two-case training strategy for noisy measurements shows strong empirical performance but depends critically on the quality of pseudo-clean references

## Next Checks

1. **Variance Bound Validation:** Empirically estimate the score estimator variance v² across noise levels on FFHQ and MRI data. If variance grows unboundedly at low noise levels, the KL bound becomes vacuous and stochastic sampling loses theoretical justification.

2. **Mask Coverage Analysis:** Quantify the effective coverage of subsampling masks by measuring the proportion of measurement space observed across mask samples. If systematic blind spots exist, weight compensation W cannot fully address the missing information.

3. **Pseudo-Clean Reference Quality:** Evaluate the PSNR of ρ-level denoised references used in Case 2 training. If PSNR remains below 25dB for measurement noise η > 0.1, the consistency loss may provide insufficient supervision for high-noise regimes.