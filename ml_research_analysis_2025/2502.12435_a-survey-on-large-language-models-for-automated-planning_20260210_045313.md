---
ver: rpa2
title: A Survey on Large Language Models for Automated Planning
arxiv_id: '2502.12435'
source_url: https://arxiv.org/abs/2502.12435
tags:
- planning
- llms
- language
- large
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This survey critically examines the use of large language models
  (LLMs) in automated planning, analyzing both their potential and limitations. While
  LLMs show promise in multi-step reasoning and generalization across domains, they
  struggle with long-horizon planning tasks and often produce plans with arbitrarily
  poor costs.
---

# A Survey on Large Language Models for Automated Planning

## Quick Facts
- **arXiv ID:** 2502.12435
- **Source URL:** https://arxiv.org/abs/2502.12435
- **Reference count:** 10
- **Primary result:** LLMs show promise in multi-step reasoning and generalization across domains, but struggle with long-horizon planning tasks and often produce plans with arbitrarily poor costs.

## Executive Summary
This survey critically examines the use of large language models (LLMs) in automated planning, analyzing both their potential and limitations. While LLMs show promise in multi-step reasoning and generalization across domains, they struggle with long-horizon planning tasks and often produce plans with arbitrarily poor costs. The paper categorizes existing approaches into two main categories: using LLMs as standalone planners and integrating them with traditional planning frameworks. Standalone methods include hierarchical task breakdown, plan refinement, search-based techniques, and finetuning, each with significant limitations in computational efficiency, reliability, and optimality. The paper advocates for integrating LLMs with traditional planners through text-to-formal language translation, commonsense knowledge enhancement, and plan evaluation. This hybrid approach leverages LLMs' natural language understanding and commonsense knowledge while maintaining the rigor and cost-effectiveness of traditional planning methods, offering a more promising direction for practical applications.

## Method Summary
The survey categorizes LLM applications in automated planning into standalone methods (hierarchical breakdown, refinement, search-based techniques, finetuning) and integrated approaches (text-to-formal language translation, commonsense knowledge enhancement, plan evaluation). Reproduction involves selecting benchmarks like PlanBench, implementing Chain-of-Thought or Tree of Thoughts prompting for standalone baselines, and creating LLM-to-PDDL translation pipelines for integrated approaches. The evaluation focuses on success rate, plan optimality, executability, and computational efficiency comparisons between methods.

## Key Results
- LLMs excel at short-horizon reasoning but fail dramatically on long-horizon planning tasks due to error accumulation
- Standalone LLM planners produce valid but often suboptimal plans with poor cost metrics
- Integrated approaches using LLMs for natural language translation to formal planning languages show more promise than direct planning

## Why This Works (Mechanism)

### Mechanism 1: Hierarchical Task Decomposition (Standalone)
- **Claim:** LLMs can potentially solve complex planning problems by recursively decomposing them into shorter-horizon sub-tasks, assuming proficiency in the simpler sub-problems.
- **Mechanism:** The model uses prompt-based reasoning (e.g., Chain-of-Thought) to break a high-level goal into a sequence of abstract actions, which are then solved incrementally. This bypasses the need to solve the entire state space in one pass.
- **Core assumption:** The planning problem is decomposable, and the model's performance on short-horizon reasoning is sufficiently reliable to prevent error accumulation.
- **Evidence anchors:** [abstract] Notes LLMs have a "remarkable capacity for multi-step reasoning" but fail in "long-horizon reasoning." [section 3] Describes methods like Least-to-Most prompting that operate under the assumption LLMs exhibit proficiency in "simpler short-horizon reasoning problems." [corpus] Neighbor papers (e.g., "Teaching LLMs to Plan") suggest instruction tuning can improve this decomposition.
- **Break condition:** Performance degrades significantly when the number of steps increases (error accumulation) or when the "cost" of the plan (optimality) must be numerically minimized, as LLMs struggle with metric reasoning.

### Mechanism 2: Neuro-Symbolic Translation (Integrated)
- **Claim:** LLMs function most effectively as semantic parsers rather than planners, translating ambiguous natural language into formal representations (e.g., PDDL) that external solvers can execute with guarantees.
- **Mechanism:** The LLM parses the initial state $s_{init}$ and goal $G$ from text into a structured language. A traditional planner (e.g., A*, Fast Downward) then searches the state space $S$ using the transition function $T$ to find a valid plan $\pi$.
- **Core assumption:** The LLM can accurately map natural language concepts to formal syntax without hallucinating invalid predicates or states.
- **Evidence anchors:** [abstract] Advocates for "leveraging their natural language understanding for task specification." [section 4] Details "Text-to-Formal Language Translation" where LLMs serve as interfaces to symbolic planners. [corpus] "LLMs as Planning Formalizers" supports this by focusing on constructing automated planning models via LLMs.
- **Break condition:** The mechanism fails if the LLM misinterprets the user's intent due to linguistic ambiguity or generates syntactically incorrect formal definitions, rendering the problem unsolvable by the symbolic engine.

### Mechanism 3: Commonsense Heuristic Augmentation
- **Claim:** LLMs act as heuristic guides to prune large search spaces by providing "commonsense" estimates of action validity or promising subgoals, reducing computational cost.
- **Mechanism:** Instead of generating the plan, the LLM evaluates potential next steps or proposes high-level subgoals based on semantic knowledge (e.g., "to cook, you likely need to open the fridge first"). This guides the search algorithm toward high-probability regions.
- **Core assumption:** The statistical correlations learned during pre-training correlate strongly with valid causal chains in the specific planning domain.
- **Evidence anchors:** [abstract] Highlights the value of "commonsense knowledge while relying on symbolic planners for rigorous execution." [section 4] Discusses using LLMs to identify preconditions or provide statistical insights to guide navigation. [corpus] Evidence is weak in the specific neighbor abstracts for this exact mechanism, though implied by general "Planning" surveys.
- **Break condition:** "Hallucinations" or knowledge gaps in specialized domains (e.g., specific industrial robotics rules) lead the heuristic to prune optimal paths or suggest impossible actions.

## Foundational Learning

- **Concept: Classical Planning (PDDL)**
  - **Why needed here:** The paper explicitly contrasts LLMs with "traditional planning" and discusses translation to PDDL (Planning Domain Definition Language). You must understand state-space search $(S, A, T, s_{init}, G)$ to grasp what LLMs are trying to replace or aid.
  - **Quick check question:** Can you define the difference between a "state" and a "goal" in a STRIPS/PDDL context?

- **Concept: Horizon and Optimality**
  - **Why needed here:** The survey distinguishes between "short-horizon" (easy for LLMs) and "long-horizon" (hard for LLMs) tasks. It also emphasizes that LLMs often produce valid but "costly" plans.
  - **Quick check question:** Why does a plan with 50 steps cost more than a plan with 10 steps, even if both reach the goal?

- **Concept: Prompt Engineering / Context Window**
  - **Why needed here:** The limitations of standalone LLMs are heavily tied to context limits and prompt design (e.g., Chain-of-Thought, Tree of Thoughts).
  - **Quick check question:** What happens to an LLM's reasoning capability if the "chain of thought" exceeds its context window?

## Architecture Onboarding

- **Component map:** Input Interface -> LLM Translator -> Symbolic Planner -> Validator/Simulator -> Output
- **Critical path:** The **LLM Translator**. If the formalization is incorrect (hallucinated predicates), the Symbolic Planner will fail or produce a plan for the wrong problem.
- **Design tradeoffs:**
  - **Standalone LLM:** High flexibility, low latency for simple tasks, but zero guarantees on correctness or optimality.
  - **Integrated System:** High latency (two-stage process) and complexity, but provides formal guarantees and handles long horizons.
- **Failure signatures:**
  - **Syntax Errors:** LLM generates invalid PDDL code.
  - **Semantic Drift:** The generated plan achieves a state that looks like the goal but misses subtle constraints (e.g., "clean the room" by hiding dirt).
  - **Context Overflow:** Attempting to feed the entire state of a massive environment into the LLM context.
- **First 3 experiments:**
  1. **Translation Accuracy Test:** Measure the success rate of an LLM converting a set of natural language instructions into valid PDDL syntax (compilable vs. non-compilable).
  2. **Plan Optimality Benchmark:** Compare the "cost" of plans generated by a standalone LLM vs. an LLM-to-PDDL pipeline on a pathfinding task (e.g., GridWorld).
  3. **Horizon Stress Test:** Increase the number of steps required to solve a problem (e.g., Blocks World tower height) and plot the degradation curve of the Standalone LLM vs. the Integrated approach.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can LLMs dynamically adapt their reasoning trajectories to task difficulty to effectively balance computational efficiency with plan quality?
- **Basis in paper:** [explicit] The authors note in Section 5 that LLMs lack an inherent mechanism to distinguish between difficult and easy tasks, often leading to "excessive and often unnecessary computational overhead."
- **Why unresolved:** Current advanced reasoning methods (e.g., Tree of Thoughts) are computationally expensive, while standard prompting fails on complex tasks; the field lacks methods to modulate推理 depth based on problem complexity.
- **What evidence would resolve it:** A framework where inference cost and latency scale proportionally with task complexity while maintaining high success rates across varying difficulty levels.

### Open Question 2
- **Question:** How can the numerical and metric reasoning limitations of LLMs be overcome to allow for accurate evaluation of plan costs?
- **Basis in paper:** [explicit] The survey highlights that LLMs "consistently exhibited notable limitations in numerical and metric reasoning," restricting their ability to assess the resources, time, or penalties associated with a plan.
- **Why unresolved:** LLMs struggle to quantify the quality of a plan beyond simple success/failure, making it difficult to rely on them for generating or critiquing optimal trajectories in cost-sensitive environments.
- **What evidence would resolve it:** Demonstrated performance on benchmarks requiring the minimization of numerical costs (e.g., path weight, resource consumption) comparable to symbolic planners.

### Open Question 3
- **Question:** Can integrating causal inference techniques into LLM training bridge the gap between statistical correlation and true cause-and-effect reasoning for robust planning?
- **Basis in paper:** [explicit] The authors state that for agents to generalize under distributional shifts, they must learn "approximate causal world models," a capability currently missing from LLMs which rely on statistical correlations.
- **Why unresolved:** Pre-training on observational data is insufficient for understanding interventions or counterfactuals (Pearl's Causal Ladder), which are necessary for resilient long-horizon decision-making.
- **What evidence would resolve it:** An LLM-augmented system successfully performing counterfactual analysis to predict outcomes of novel actions in unseen environments.

## Limitations

- The survey lacks quantitative benchmarks or experimental validation for the proposed mechanisms
- Analysis relies heavily on theoretical reasoning rather than controlled experiments
- Does not address potential bias in LLM training data across different domains
- Scalability challenges when moving from toy problems to real-world applications are not discussed

## Confidence

**High Confidence:** The observation that LLMs struggle with long-horizon planning and produce suboptimal plans is well-supported by multiple studies cited in the survey. The distinction between standalone and integrated approaches reflects the current state of research accurately.

**Medium Confidence:** The recommendation for hybrid approaches integrating LLMs with traditional planners is reasonable but lacks empirical validation. While theoretically sound, the practical benefits depend heavily on implementation details not explored in the survey.

**Low Confidence:** Specific performance claims about standalone methods (hierarchical decomposition, plan refinement) are not backed by concrete metrics or controlled experiments in the survey itself.

## Next Checks

1. **Translation Accuracy Benchmark:** Implement an automated evaluation framework to measure LLM success rates in converting natural language specifications to valid PDDL across multiple domains, using both successful and failure cases from the survey's referenced benchmarks.

2. **Optimality Gap Analysis:** Conduct head-to-head comparisons between plans generated by standalone LLMs versus LLM-to-PDDL pipelines on standardized planning problems, measuring both success rates and plan cost differences with statistical significance testing.

3. **Horizon Stress Test:** Systematically vary problem complexity (number of steps, domain size) to empirically validate the survey's claim about long-horizon reasoning limitations, measuring performance degradation curves for different LLM approaches.