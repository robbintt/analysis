---
ver: rpa2
title: 'CompNO: A Novel Foundation Model approach for solving Partial Differential
  Equations'
arxiv_id: '2601.07384'
source_url: https://arxiv.org/abs/2601.07384
tags:
- neural
- operator
- foundation
- equation
- operators
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the computational cost of solving parametric
  partial differential equations (PDEs) by proposing Compositional Neural Operators
  (CompNO), a foundation model that learns elementary PDE operators and assembles
  them for complex problems. Instead of pretraining a monolithic network, CompNO pretrains
  a library of specialized Fourier neural operators (Foundation Blocks) for fundamental
  operators like convection and diffusion, then combines them via lightweight adaptation
  blocks.
---

# CompNO: A Novel Foundation Model approach for solving Partial Differential Equations

## Quick Facts
- arXiv ID: 2601.07384
- Source URL: https://arxiv.org/abs/2601.07384
- Reference count: 35
- Primary result: Achieves lower relative L2 error than strong baselines (PFNO, PDEFormer, and in-context learning models) on linear parametric PDE systems while maintaining exact boundary satisfaction

## Executive Summary
This work addresses the computational cost of solving parametric partial differential equations (PDEs) by proposing Compositional Neural Operators (CompNO), a foundation model that learns elementary PDE operators and assembles them for complex problems. Instead of pretraining a monolithic network, CompNO pretrains a library of specialized Fourier neural operators (Foundation Blocks) for fundamental operators like convection and diffusion, then combines them via lightweight adaptation blocks. A boundary-condition operator enforces Dirichlet constraints exactly at inference. Evaluated on 1D convection, diffusion, convection-diffusion, and Burgers' equations from PDEBench, CompNO achieves lower relative L2 error than strong baselines (PFNO, PDEFormer, and in-context learning models) on linear parametric systems while remaining competitive on nonlinear Burgers' flows. The model maintains exact boundary satisfaction (zero loss at boundaries) and generalizes robustly across a wide range of Peclet and Reynolds numbers, demonstrating that compositional neural operators provide a scalable, interpretable pathway toward foundation models for PDEs.

## Method Summary
CompNO proposes a compositional foundation model approach for solving parametric PDEs by pretraining specialized Fourier neural operators for fundamental operators (like convection and diffusion) and then assembling them for complex problems. The method consists of two phases: pretraining foundation blocks (specialized FNOs for elementary operators) and fine-tuning these blocks with adaptation layers for specific target PDEs. A boundary-condition operator is introduced to enforce Dirichlet constraints exactly at inference. The approach is evaluated on 1D benchmark PDEs from PDEBench, including convection, diffusion, convection-diffusion, and Burgers' equations, demonstrating superior performance on linear parametric systems compared to strong baselines while maintaining exact boundary satisfaction.

## Key Results
- Achieves lower relative L2 error than strong baselines (PFNO, PDEFormer, and in-context learning models) on linear parametric PDE systems
- Maintains exact boundary satisfaction with zero loss at boundaries for all test cases
- Demonstrates robust generalization across wide ranges of Peclet and Reynolds numbers in 1D benchmark problems

## Why This Works (Mechanism)
CompNO leverages the compositional structure of PDEs by decomposing complex operators into elementary building blocks that can be pretrained and then assembled for specific problems. This approach exploits the shared structure among parametric PDEs, allowing the model to generalize better than monolithic architectures. The exact boundary enforcement mechanism ensures that physical constraints are satisfied at inference, which is critical for many PDE applications. By pretraining on fundamental operators and only requiring lightweight adaptation for specific PDEs, CompNO achieves both computational efficiency and accuracy.

## Foundational Learning
- **Fourier Neural Operators (FNO)**: Neural operators that approximate solution operators using Fourier transforms for efficient frequency-domain processing. Why needed: Enables fast and accurate approximation of complex operators. Quick check: Verify Fourier transform implementation and resolution in the code.
- **Foundation Model Paradigm**: Pretraining on fundamental tasks and adapting to downstream applications. Why needed: Allows transfer learning and reduces computational cost for new PDE problems. Quick check: Examine pretraining datasets and adaptation mechanisms.
- **Parametric PDEs**: PDEs with parameters that vary across instances, requiring models to handle a family of related problems. Why needed: Real-world applications often involve varying physical conditions. Quick check: Verify parameter ranges and sampling strategies in experiments.
- **Boundary Condition Enforcement**: Mechanisms to ensure solutions satisfy physical constraints at domain boundaries. Why needed: Physical solutions must satisfy boundary conditions for validity. Quick check: Verify boundary enforcement implementation in the code.
- **Operator Composition**: Combining elementary operators to form complex ones. Why needed: Many real PDEs can be decomposed into fundamental operations. Quick check: Examine composition rules and implementation details.

## Architecture Onboarding

**Component Map**
Foundation Blocks (FNOs for elementary operators) -> Adaptation Blocks (lightweight layers) -> Boundary Condition Operator -> Output

**Critical Path**
1. Pretrain foundation blocks on elementary operators (convection, diffusion)
2. Compose foundation blocks for target PDE
3. Fine-tune with adaptation blocks
4. Apply boundary condition operator

**Design Tradeoffs**
- Modular composition enables efficient pretraining but requires domain knowledge for block selection
- Exact boundary enforcement ensures physical validity but adds computational overhead
- Pretraining on elementary operators reduces data requirements but assumes decomposable structure

**Failure Signatures**
- Poor performance on PDEs that cannot be decomposed into trained elementary operators
- Generalization issues when target PDEs significantly differ from pretraining tasks
- Boundary condition violations indicating issues with the enforcement mechanism

**3 First Experiments to Run**
1. Evaluate CompNO on a 2D diffusion equation to test scalability beyond 1D problems
2. Test performance on a PDE with time-dependent boundary conditions to verify boundary enforcement mechanism
3. Compare training time and data efficiency against monolithic FNO baseline on the same PDE family

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation limited to 1D benchmark PDEs, leaving scalability to higher dimensions untested
- Fixed composition assumptions require domain knowledge that may not always be available
- Does not address operator discovery for arbitrary PDEs with unknown structure

## Confidence

**High Confidence**
- Low-dimensional PDE performance: The model consistently outperforms or matches baselines on 1D convection, diffusion, and convection-diffusion problems with clear error metrics

**Medium Confidence**
- Compositional generalization: Strong results on unseen Peclet and Reynolds numbers within 1D regime, but generalization to higher dimensions and more complex PDEs remains unproven
- Foundation model paradigm: Validated in controlled setting, but broader claim of scalability and adaptability to diverse PDE families not yet demonstrated

## Next Checks
1. Evaluate CompNO on 2D and 3D PDEs from PDEBench or similar datasets to assess scalability and robustness in higher dimensions
2. Test the model on PDEs with irregular geometries and time-dependent or Neumann boundary conditions to verify the generality of the boundary enforcement mechanism
3. Conduct ablation studies varying the number and types of foundation blocks to quantify trade-offs between model complexity, training efficiency, and accuracy