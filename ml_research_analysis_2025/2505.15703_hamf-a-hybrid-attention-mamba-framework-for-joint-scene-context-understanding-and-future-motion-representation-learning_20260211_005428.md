---
ver: rpa2
title: 'HAMF: A Hybrid Attention-Mamba Framework for Joint Scene Context Understanding
  and Future Motion Representation Learning'
arxiv_id: '2505.15703'
source_url: https://arxiv.org/abs/2505.15703
tags:
- motion
- future
- scene
- tokens
- encoder
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: HAMF introduces a hybrid Attention-Mamba framework that jointly
  learns future motion representations and scene context understanding during encoding.
  The approach represents future motion as learnable tokens, combined with embedded
  observed agent and map tokens, and processes them through a unified encoder using
  sequential self-attention and cross-attention blocks.
---

# HAMF: A Hybrid Attention-Mamba Framework for Joint Scene Context Understanding and Future Motion Representation Learning

## Quick Facts
- arXiv ID: 2505.15703
- Source URL: https://arxiv.org/abs/2505.15703
- Reference count: 40
- State-of-the-art performance on Argoverse 2: 1.59 minADE1, 3.99 minFDE1, 0.123 minFDE6

## Executive Summary
HAMF introduces a hybrid framework that jointly learns future motion representations and scene context understanding during encoding, achieving state-of-the-art performance on motion forecasting benchmarks. The approach uses learnable future motion tokens processed through a unified encoder with sequential self-attention and cross-attention blocks, followed by a Mamba-based decoder for generating accurate, diverse trajectories. The model achieves impressive efficiency with only 3.0M parameters and 22ms inference latency while maintaining superior accuracy metrics.

## Method Summary
HAMF is an end-to-end motion forecasting framework that predicts multi-modal future trajectories for autonomous driving scenarios. The model takes 5 seconds of agent trajectory history and HD map data as input and predicts 6 future trajectories (6 seconds) with associated probabilities. The core innovation is a unified encoder that jointly processes learnable future motion tokens with observed agent and map tokens, using sequential self-attention and cross-attention layers. A Mamba-based decoder then models dependencies among future motion tokens to generate final predictions. The model is trained on Argoverse 2 with winner-take-all loss and evaluated using standard metrics including minADE, minFDE, and miss rate.

## Key Results
- Achieves state-of-the-art performance: 1.59 minADE1, 3.99 minFDE1, 0.123 minFDE6 on Argoverse 2 validation
- Maintains only 3.0M parameters and 22ms inference latency
- Demonstrates more accurate and road-compliant predictions compared to baseline methods
- Ablation studies show the effectiveness of each architectural component

## Why This Works (Mechanism)

### Mechanism 1: Joint Encoding of Future Motion Tokens
- **Claim:** Jointly encoding future motion tokens with scene context mitigates information degradation typical in encoder-decoder separation
- **Mechanism:** Learnable future motion tokens ($F$) are injected into the encoder input stream alongside agent and map tokens. Self-attention allows these motion tokens to attend to the entire scene context at multiple abstraction levels
- **Core assumption:** Future motion features benefit from hierarchical refinement through scene context layers rather than isolated decoding
- **Evidence anchors:** Abstract claims joint learning; section I discusses long-range interactions between scene context and future motion features
- **Break condition:** Poor initialization or lack of gradient flow to initial motion tokens could cause attention mechanism to ignore them

### Mechanism 2: Sequential Self-Attention and Cross-Attention Synergy
- **Claim:** Sequential self-attention followed by cross-attention creates synergy between global context modeling and targeted feature aggregation
- **Mechanism:** Encoder layer first updates all tokens via self-attention, then splits them and uses cross-attention where motion tokens query updated scene tokens ($Q_c=F, K_c=V_c=S$), summing results ($F^l = F^l_{sa} + F^l_{ca}$)
- **Core assumption:** Summing self and cross-attention outputs is sufficient for feature fusion without complex gating mechanisms
- **Evidence anchors:** Section III.C describes parameter-free summation; Table IV shows "Interaction" strategy outperforms alternatives
- **Break condition:** Noisy scene tokens may aggregate irrelevant features, degrading motion token specificity

### Mechanism 3: Unidirectional Mamba Decoding
- **Claim:** Unidirectional Mamba decoding preserves dependency and consistency of multi-modal future representations better than bidirectional attention
- **Mechanism:** Future motion tokens are processed by Uni-Mamba block, enforcing ordered dependency through sequential state space modeling
- **Core assumption:** Future motion modes have sequential dependency benefiting from causal modeling rather than bidirectional mixing
- **Evidence anchors:** Table V shows Uni-Mamba (depth 1) achieving lower MR (0.145) than Bi-Mamba or Attention blocks; section III.D discusses unidirectional modeling
- **Break condition:** Over-modeling with Mamba depth > 1 leads to feature degradation

## Foundational Learning

- **Concept: Learnable Query Tokens**
  - **Why needed here:** HAMF uses "Future Motion Tokens" as dynamic queries that evolve during encoding, unlike direct decoding approaches
  - **Quick check question:** How does gradient flow back to initial motion tokens if they are learned purely via attention weights?

- **Concept: Cross-Attention vs. Self-Attention**
  - **Why needed here:** Unified encoder relies on specific sequence: Self-Attention (intra-sequence) → Cross-Attention (inter-sequence)
  - **Quick check question:** In cross-attention block (Eq. 7), why are Scene tokens $S^l$ used as Keys/Values and not Motion tokens $F$?

- **Concept: State Space Models (Mamba)**
  - **Why needed here:** Decoder uses Mamba, newer architecture than Transformers, for sequence modeling with linear complexity
  - **Quick check question:** Why would Uni-Mamba (causal) be preferred over Bi-Mamba for unordered future possibilities?

## Architecture Onboarding

- **Component map:** Agent/Map Embedding (PointNet/Uni-Mamba) → Future Tokens (Learnable) → Unified Encoder (5 layers: Self-Attn → Split → Cross-Attn → Sum) → Uni-Mamba Decoder → MLP heads
- **Critical path:** Update loop of Future Motion Token ($F$): $F^{l-1}$ enters Self-Attn → becomes $F^{l}_{sa}$ → queries $S^l$ in Cross-Attn → becomes $F^{l}_{ca}$ → Sum yields $F^l$
- **Design tradeoffs:**
  - Efficiency vs. Capacity: 3.0M params, 22ms latency using Mamba and avoiding complex anchor heuristics
  - Ordering: Ablation shows Self-Attn then Cross-Attn superior to reversed or parallel structures
- **Failure signatures:**
  - Mode Collapse: If "Interaction" (summing SA and CA) fails to diversify tokens, MR increases
  - Road Non-compliance: If Cross-Attention fails to aggregate map features, trajectories ignore lane geometry
- **First 3 experiments:**
  1. Verify Token Efficacy: Run base model ($M_b$) vs. query model ($M_q$) vs. HAMF on validation to confirm performance gain from unified encoding
  2. Decoder Ablation: Swap Uni-Mamba decoder for standard MLP or Attention block to verify Mamba contribution
  3. Encoder Depth Scaling: Vary encoder layers (4, 5, 6) to find inflection point where performance saturates vs. latency cost

## Open Questions the Paper Calls Out
- **Can advanced, learning-based feature fusion strategies outperform the simple summation used to combine self-attention and cross-attention outputs in the unified encoder?**
- **How can the Mamba-based decoder be modified to benefit from increased depth without suffering from the "over-modeling" and feature degradation observed in the ablation studies?**
- **Does increasing the number of learnable future motion tokens ($K_e$) significantly beyond the required prediction count ($K=6$) improve the model's ability to capture diverse long-tail behaviors?**

## Limitations
- The core claim that joint encoding prevents information degradation lacks empirical validation through direct comparison
- The superiority of unidirectional Mamba over bidirectional alternatives is supported by single ablation without isolating the causal property
- Several architectural details remain underspecified including exact embedding dimensions and Mamba configuration parameters

## Confidence

**High Confidence** (4+ independent evidences or well-established mechanisms):
- Overall architecture combining learnable future tokens with encoder-decoder structure is technically sound
- Reported metrics (minADE1=1.59, minFDE1=3.99) represent state-of-the-art performance on Argoverse 2
- Computational efficiency (3.0M params, 22ms latency) is verifiable and significant

**Medium Confidence** (2-3 evidences, some gaps):
- Sequential self-attention followed by cross-attention provides synergistic feature extraction
- Mamba decoder's unidirectional nature specifically benefits trajectory diversity
- Learnable future motion tokens effectively capture multi-modal distributions

**Low Confidence** (1-2 evidences, high uncertainty):
- Joint encoding specifically prevents information degradation versus standard encoder-decoder separation
- The causal property of Uni-Mamba is essential for multi-modal consistency
- The specific sum-based feature fusion is optimal versus gating mechanisms

## Next Checks
1. **Isolate Joint Encoding Effect:** Create controlled ablation comparing HAMF's unified encoder against baseline with standard encoder-decoder separation to validate claimed "information degradation prevention"
2. **Test Bidirectional Mamba:** Implement Bi-Mamba decoder variant and compare against Uni-Mamba on both diversity metrics (MR6) and accuracy to determine whether causal property or Mamba architecture drives performance gains
3. **Probe Future Token Learning:** Visualize evolution of learnable future motion tokens across training epochs and their attention distributions to verify whether tokens genuinely learn meaningful motion patterns or simply inherit information from final encoder layer