---
ver: rpa2
title: Quantifying Conversation Drift in MCP via Latent Polytope
arxiv_id: '2508.06418'
source_url: https://arxiv.org/abs/2508.06418
tags:
- data
- arxiv
- malicious
- attacks
- hijacking
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces SECMCP, a framework that detects and quantifies
  conversation drift in MCP-powered systems by analyzing activation vector deviations
  in LLMs. The method models conversational dynamics within a latent polytope space,
  identifying anomalous shifts induced by adversarial external knowledge.
---

# Quantifying Conversation Drift in MCP via Latent Polytope

## Quick Facts
- **arXiv ID:** 2508.06418
- **Source URL:** https://arxiv.org/abs/2508.06418
- **Reference count:** 39
- **Primary result:** SECMCP detects conversation drift in MCP systems with AUROC scores exceeding 0.915 across three LLMs and datasets.

## Executive Summary
This paper introduces SECMCP, a framework that detects and quantifies conversation drift in MCP-powered systems by analyzing activation vector deviations in LLMs. The method models conversational dynamics within a latent polytope space, identifying anomalous shifts induced by adversarial external knowledge. Experiments on three LLMs (Llama3, Vicuna, Mistral) across three datasets show robust detection with AUROC scores exceeding 0.915, while maintaining system usability. The approach addresses limitations of existing defenses like static signatures and computational inefficiency.

## Method Summary
SECMCP quantifies conversation drift by constructing an authorized access region in activation space using benign "anchor" queries, then detecting deviations from this region in new queries. The framework extracts activation vectors from specific LLM layers, projects them into a low-dimensional embedding, and computes squared Euclidean distances to anchor embeddings. A decision tree classifier flags inputs exceeding a threshold τ. The method is integrated into the MCP Client, analyzing requests before they reach the LLM, and supports various attack types including hijacking, misleading, and data exfiltration.

## Key Results
- Robust detection across three LLMs (Llama3, Vicuna, Mistral) with AUROC scores exceeding 0.915
- Effective against multiple attack types including tool poisoning and indirect prompt injection
- Maintains system usability while providing strong security guarantees

## Why This Works (Mechanism)

### Mechanism 1: Semantic separation of malicious vs. benign queries in activation space
The framework constructs a high-dimensional authorized access region A ⊂ ℝⁿ using activation vectors from benign anchor queries. When a new query arrives, its activation deviation Dₗ is computed as the squared Euclidean distance to anchor points, layer-wise. A decision tree classifier flags inputs whose distances exceed a predefined threshold τ, indicating they fall outside the permitted boundary. The core hypothesis is that malicious inputs systematically produce activation patterns distant from legitimate interactions within the MCP context.

### Mechanism 2: Layer-wise aggregation of activation deviations for robust detection
The framework computes deviations on a per-layer basis across specified layers (e.g., 0, 7, 15, 23, 31). The final detection decision is based on the best-performing result among them. This accounts for the fact that different layers may exhibit distinct distributional characteristics and representational properties, with malicious intent manifesting more distinctly in specific layers.

### Mechanism 3: Quantifying drift via a low-dimensional embedding of activation representations
Before final distance computation, an embedding model E creates a compact representation of the activation features. This reduces computational cost while preserving semantic distinctions necessary for effective drift quantification. The squared Euclidean norm is then calculated between this embedding vector and those of anchor points.

## Foundational Learning

- **Concept: Model Context Protocol (MCP) Architecture**
  - **Why needed here:** The entire threat model and defense framework is built upon understanding MCP's components (Host, Client, Server), its non-isolated execution context, and how data flows between them.
  - **Quick check question:** Can you describe the role of the MCP Client and the non-isolated execution context that creates the primary attack surface?

- **Concept: LLM Activation Vectors and Representation Spaces**
  - **Why needed here:** SECMCP's core detection method operates not on text, but on the model's internal "activation vectors" from specific layers.
  - **Quick check question:** In the context of this paper, what does an "activation vector" represent, and what does it mean for a query to "drift" in this space?

- **Concept: Adversarial Attacks on LLMs (Prompt Injection, Tool Poisoning)**
  - **Why needed here:** The paper aims to defend against specific threat categories (hijacking, misleading, data exfiltration) that arise from adversarial inputs.
  - **Quick check question:** What is the difference between "Tool Poisoning" and "Indirect Prompt Injection" within the MCP threat model?

## Architecture Onboarding

- **Component map:** User Input → MCP Client → SECMCP Agent (Activation Collector, Embedding Model E, Distance Calculator, Decision Tree) → LLM → Output
- **Critical path:**
  1. User input query arrives at MCP Client
  2. LLM processes query; activations are captured at specified layers
  3. Activations are passed to the Embedding Model to get a compact representation
  4. Compute distances between this embedding and all anchor embeddings
  5. If distance ≤ τ, ACCEPT query. If distance > τ, REJECT query as potential drift/attack

- **Design tradeoffs:**
  - **Anchor Sample Quantity vs. Computational Cost:** More anchor samples improve detection but increase latency. Default is 1000 anchors.
  - **Granularity vs. Interpretability:** The method detects "conversation drift" but lacks token-level attribution, providing only high-level "is this malicious?" signals.
  - **Security vs. Usability:** Tighter threshold increases security but may flag legitimate unusual queries as false positives.

- **Failure signatures:**
  - **High False Positive Rate:** Legitimate user queries with diverse semantics may be rejected if region A is too narrow.
  - **High False Negative Rate (Adaptive Attacks):** Adversaries using synonym replacement can reduce AUROC significantly (~0.12 for data exfiltration).
  - **Ambiguous Rejection:** System flags drift but provides no actionable information on threat nature.

- **First 3 experiments:**
  1. **Baseline Ablation on Anchor Set Size:** Vary anchor count (200, 500, 1000, 2000) and plot AUROC to find optimal trade-off for specific query distribution.
  2. **Robustness Test Against Paraphrasing:** Use different LLM to paraphrase known malicious prompts and measure AUROC change to test resilience to semantic-preserving transformations.
  3. **Layer-Wise Contribution Analysis:** Record and plot individual AUROC scores for each monitored layer to identify most sensitive layers for specific attack types.

## Open Questions the Paper Calls Out

- **Open Question 1:** Can the SECMCP framework be adapted for asynchronous, multi-agent protocols (e.g., A2A) where conversation boundaries and speaker roles are fluid?
  - **Basis in paper:** [explicit] The authors state the method "is not directly applicable to large-scale agentic environments with asynchronous, multi-agent protocols such as A2A."
  - **Why unresolved:** The current implementation assumes a stable query-response structure, which breaks down in fluid multi-agent interactions.
  - **What evidence would resolve it:** A modification of the latent polytope construction that functions effectively in environments without fixed conversation boundaries.

- **Open Question 2:** Can the activation deviation method be refined to provide fine-grained, token-level attribution for adversarial inputs?
  - **Basis in paper:** [explicit] The paper notes the approach "lacks granularity for token-level attribution," limiting control in sensitive contexts.
  - **Why unresolved:** The current methodology detects topic-level drift but cannot pinpoint specific malicious segments within a prompt.
  - **What evidence would resolve it:** An architectural extension that isolates specific adversarial tokens contributing to the activation deviation.

- **Open Question 3:** How can the decision-making process of SECMCP be made interpretable for scenarios requiring high transparency?
  - **Basis in paper:** [explicit] The authors acknowledge that the decision-making process "lacks interpretability," restricting use in transparent systems.
  - **Why unresolved:** The internal mechanics of activation deviation are currently opaque, serving as a black-box detector.
  - **What evidence would resolve it:** Integration of explainability techniques that map activation deviations to human-readable semantic justifications.

## Limitations

- **Generalizability of Anchor Regions:** The static authorized access region defined by benign anchors may not adapt well to dynamic user behaviors, potentially becoming too permissive or restrictive.
- **Embedding Model Transparency:** The core detection relies on an unspecified embedding model E, creating a black-box dependency whose failure mode is unclear.
- **Layer Selection Sensitivity:** While the framework selects the best-performing layer, it does not analyze which layers are most informative or whether certain attack types consistently manifest in specific layers.

## Confidence

- **High Confidence:** The mechanism of detecting activation deviations via distance thresholds is technically sound and well-supported by ablation studies (AUROC > 0.915).
- **Medium Confidence:** The layer-wise aggregation strategy is reasonable but lacks detailed analysis of layer contributions.
- **Low Confidence:** The role of the embedding model E is under-specified, making it difficult to assess its impact on detection accuracy.

## Next Checks

1. **Dynamic Anchor Adaptation:** Test how detection performance degrades when the anchor set becomes outdated due to evolving user queries. Measure AUROC over time as the semantic distribution shifts.
2. **Embedding Model Sensitivity:** Replace the unspecified embedding model E with a simple PCA or autoencoder and compare detection performance to reveal the embedding step's contribution to robustness.
3. **Layer Contribution Analysis:** For each monitored layer, compute and report individual AUROC scores across all datasets to identify which layers are most sensitive to specific attack types and whether this varies by LLM architecture.