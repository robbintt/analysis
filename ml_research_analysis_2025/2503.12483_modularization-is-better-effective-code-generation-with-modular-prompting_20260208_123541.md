---
ver: rpa2
title: 'Modularization is Better: Effective Code Generation with Modular Prompting'
arxiv_id: '2503.12483'
source_url: https://arxiv.org/abs/2503.12483
tags:
- code
- generation
- reasoning
- graph
- prompting
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes MoT (Modularization of Thought), a novel prompting
  technique for enhancing code generation performance of large language models (LLMs).
  The key innovation is the introduction of Multi-Level Reasoning (MLR) graphs that
  hierarchically decompose programming problems into smaller, independent reasoning
  steps.
---

# Modularization is Better: Effective Code Generation with Modular Prompting

## Quick Facts
- arXiv ID: 2503.12483
- Source URL: https://arxiv.org/abs/2503.12483
- Authors: Ruwei Pan; Hongyu Zhang
- Reference count: 40
- Key result: MoT achieves Pass@1 scores from 58.1% to 95.1% across six benchmarks, significantly outperforming existing baselines

## Executive Summary
This paper proposes MoT (Modularization of Thought), a novel prompting technique that enhances code generation performance by decomposing complex programming problems into hierarchical Multi-Level Reasoning (MLR) graphs. The approach applies modularization principles to structure the reasoning process, improving large language models' understanding of complex programming problems through two phases: MLR graph generation and code generation. Extensive experiments on GPT-4o-mini and DeepSeek-R1 across six benchmarks demonstrate that MoT significantly outperforms existing baselines while maintaining lower computational costs than iterative methods.

## Method Summary
MoT is a two-phase prompting approach that first generates a hierarchical MLR graph to decompose programming problems into High-Level, Intermediate-Level, and Detailed-Level nodes with embedded reasoning, then uses this structured graph to guide modular code generation. The method applies software modularization principles (information hiding, decomposition) to create smaller, independent reasoning steps that reduce cognitive load and improve functional correctness. Each node contains specific reasoning fields (Task Purpose, Decision Rationale, Execution Strategy) that force explicit intent declaration before synthesis, acting as a self-cross-check to reduce hallucination.

## Key Results
- MoT achieves significant improvements in Pass@1 scores across all six benchmarks, ranging from 58.1% to 95.1%
- The approach demonstrates lower computational costs compared to iterative methods while maintaining superior accuracy
- MoT with GPT-4o-mini outperforms DeepSeek-R1-671B, suggesting the method amplifies model-specific reasoning capabilities

## Why This Works (Mechanism)

### Mechanism 1: Hierarchical Cognitive Load Reduction
Decomposing complex programming problems into Multi-Level Reasoning (MLR) graphs significantly improves model performance by reducing context window burden and logical complexity at any single generation step. The hierarchical structure (High-Level → Intermediate-Level → Detailed-Level nodes) allows the model to solve smaller, independent sub-problems rather than monolithic blocks.

### Mechanism 2: Embedded Rationale Alignment
Embedding specific reasoning fields within graph nodes (Task Purpose, Decision Rationale, Execution Strategy) improves functional correctness by forcing the model to explicitly declare intent and strategy before synthesis. This creates a "self-cross-check" where the model must rationalize a step before generating corresponding code, reducing hallucination.

### Mechanism 3: Two-Phase Generation (Decoupling Design from Implementation)
Separating the reasoning phase (MLR Graph) from the execution phase (Code Generation) yields higher accuracy than interleaved approaches. This "Plan-then-Write" strategy prevents the model from getting "stuck" in syntax errors early on, allowing it to focus entirely on logic architecture first.

## Foundational Learning

- **Chain-of-Thought (CoT) Prompting**: MoT is explicitly designed as a successor to CoT. Understanding linear CoT's limitations (sequential, monolithic) is prerequisite to grasping why a graph-based approach solves complex dependency issues. Quick check: Can you explain why a linear step-by-step prompt might fail if step 5 requires changing a variable defined in step 1?

- **Software Modularization (Parnas)**: The paper grounds its methodology in David Parnas's principles of modularity (information hiding, decomposition). Understanding software engineering "decoupling" is prerequisite to understanding the MLR graph structure. Quick check: What is the primary benefit of "information hiding" when decomposing a large system into modules?

- **Tree / Graph Structures in Prompts**: Unlike standard prompts, MoT requires the model to output and parse a hierarchical structure (indentation/edges). Familiarity with how LLMs handle structured data is required to debug the MLR output. Quick check: How does an LLM represent a parent-child relationship in a flat text stream?

## Architecture Onboarding

- **Component map**: Input → MLR Generator (LLM with formatting rules) → MLR Graph (hierarchical text) → Code Synthesizer (LLM with graph) → Pass@1 evaluation
- **Critical path**: MLR Graph Generation is the bottleneck. If the initial decomposition is flawed, the Code Generation phase will faithfully implement the flawed logic.
- **Design tradeoffs**: Cost vs. Accuracy (cheaper than iterative methods but more expensive than Zero-shot); Structure vs. Flexibility (rigid template ensures structure but may constrain irregular problems).
- **Failure signatures**: Graph Collapse (model flattens reasoning), Hallucinated Dependencies (phantom function calls), Syntax Drift (fails to translate Detailed-Level instructions to valid Python).
- **First 3 experiments**: 1) Sanity Check: Run MoT prompt on 5 HumanEval problems to verify MLR graph structure output; 2) Ablation (w/o Graph): Remove MLR phase and ask model to generate code immediately with same "modular" instruction; 3) Node Depth Limit: Test 2-level vs 3-level graph variants to determine if Detailed-Level nodes contribute to higher Pass@1 or just add token cost.

## Open Questions the Paper Calls Out

- **Language generalizability**: How does MoT perform when applied to programming languages other than Python? The paper focuses on Python and notes that characteristics of other languages may not be fully explored. What evidence would resolve it: Evaluation on multi-language benchmarks like HumanEval-X.

- **Broader software engineering domains**: Can MoT effectively scale to system programming or web development? Current experiments are limited to algorithmic benchmarks. What evidence would resolve it: Testing on repository-level benchmarks like SWE-bench.

- **Smaller model reliability**: Is MLR graph generation reliable for smaller, open-source code LLMs? Experiments were limited to massive proprietary models. What evidence would resolve it: Ablation studies on 7B or 13B parameter models.

## Limitations
- Graph generation quality dependency: Success critically depends on the LLM's ability to generate accurate and complete MLR graphs
- Limited baseline comparison scope: Doesn't evaluate against other recent hierarchical or modular approaches like Self-Refine
- Benchmark domain specificity: Evaluation focuses primarily on algorithmic coding problems, not real-world software development tasks

## Confidence

- **High confidence**: The core claim that MoT improves Pass@1 scores across benchmarks is well-supported by experimental results
- **Medium confidence**: The claim about lower computational costs is supported by methodology but lacks direct quantitative comparison
- **Low confidence**: The assertion that MoT "significantly outperforms" existing methods may be overstated given comparable performance on some benchmarks

## Next Checks
1. **Cross-model validation**: Test MoT across a wider range of LLM sizes and architectures to determine if improvements are consistent or model-specific
2. **Human evaluation study**: Conduct expert code review of MoT-generated solutions versus baseline approaches to assess code quality, maintainability, and adherence to best practices
3. **Scaling behavior analysis**: Evaluate MoT's performance on increasingly complex problems to identify when hierarchical decomposition breaks down or overhead becomes prohibitive