---
ver: rpa2
title: 'Lattice: Learning to Efficiently Compress the Memory'
arxiv_id: '2504.05646'
source_url: https://arxiv.org/abs/2504.05646
tags:
- memory
- arxiv
- state
- update
- lattice
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Lattice, a novel recurrent neural network
  (RNN) mechanism designed to address the quadratic computational complexity of attention
  layers. The core innovation is an orthogonal state recurrence that leverages the
  low-rank structure of key-value matrices to efficiently compress an unbounded cache
  into a fixed number of memory slots, achieving sub-quadratic complexity.
---

# Lattice: Learning to Efficiently Compress the Memory

## Quick Facts
- arXiv ID: 2504.05646
- Source URL: https://arxiv.org/abs/2504.05646
- Authors: Mahdi Karami; Razvan Pascanu; Vahab Mirrokni
- Reference count: 40
- Primary result: Novel RNN mechanism that compresses unbounded key-value caches into fixed memory slots using orthogonal state recurrence, achieving sub-quadratic complexity.

## Executive Summary
This paper introduces Lattice, a recurrent neural network mechanism designed to address the quadratic computational complexity of attention layers. The core innovation is an orthogonal state recurrence that leverages the low-rank structure of key-value matrices to efficiently compress an unbounded cache into a fixed number of memory slots. The method formulates compression as an online optimization problem and derives a dynamic memory update rule based on a single gradient descent step, resulting in a state- and input-dependent gating mechanism.

## Method Summary
Lattice frames memory compression as an online optimization problem where the state is updated with a single gradient descent step per new token. The orthogonal update mechanism projects incoming information onto the orthogonal complement of each memory slot's current vector, ensuring each slot is updated exclusively with novel, non-redundant data. State normalization constrains memory vectors to the unit sphere, preventing unbounded growth and ensuring stable training. The method further approximates the non-linear recurrence with chunk-wise parallelization to enable efficient training on modern hardware.

## Key Results
- Outperforms strong baselines on language modeling and associative recall tasks across diverse context lengths and model sizes
- Achieves superior memory efficiency with significantly reduced memory sizes (4x compression gain over TTT)
- Demonstrates strong performance with memory sizes smaller than the model dimension (m < d)
- Maintains effectiveness across context lengths up to 16k tokens

## Why This Works (Mechanism)

### Mechanism 1: Orthogonal State Recurrence for Non-Redundant Memory Updates
The proposed Orthogonal State Recurrence (OSR) updates each memory slot exclusively with information orthogonal to its current state, thereby incorporating only novel, non-redundant data and minimizing interference. The update rule projects the incoming information onto the orthogonal complement of each memory slot's current vector using a projection matrix $P(s_i)$. This decomposes the input into a component parallel to the current state (discarded) and a component orthogonal to it (used for the update). The core assumption is that the key-value cache in attention models has an inherent low-rank structure, making it compressible.

### Mechanism 2: State Normalization for Stable Recurrent Updates
Normalizing the memory state vectors after each update prevents unbounded growth of their norms, which can lead to numerical instability and dilute representations. After an orthogonal update, the norm of the state vector increases. The method constrains state vectors to the unit sphere by projecting the updated vector back onto it. This projection is mathematically equivalent to a retraction step in Riemannian optimization on the unit sphere manifold.

### Mechanism 3: Chunk-wise Parallelization for Training Scalability
The non-linear orthogonal recurrence can be approximated for parallel computation within chunks, enabling efficient training on modern hardware. The sequence is split into non-overlapping chunks. The state at the beginning of a chunk is used to approximate the gradients for all tokens within that chunk in parallel. This linearizes the non-linear recurrence within a chunk, allowing it to be computed efficiently using matrix multiplications.

## Foundational Learning

- **Online Gradient Descent (OGD)**
  - Why needed here: The paper's core insight is to frame memory compression as an online optimization problem, where the state is updated with a single gradient descent step per new token. Understanding OGD is crucial for grasping the derivation of the update rule.
  - Quick check question: Can you explain how taking a single gradient step on a loss function $\mathcal{L}_t$ at each time step $t$ gives rise to a recurrent update rule?

- **Projection onto the Orthogonal Complement**
  - Why needed here: The central "Lattice" innovation is an update rule based on projecting information onto the subspace orthogonal to the current memory state. Understanding this linear algebra concept is essential for comprehending how the model avoids redundant updates.
  - Quick check question: For a given memory state vector $s$, what is the formula for the projection matrix $P(s)$ that projects any vector onto the subspace orthogonal to $s$? What happens when you project $s$ onto this space?

- **Riemannian Optimization**
  - Why needed here: The paper formally links its state normalization step to a retraction on a Riemannian manifold (the unit sphere). While not strictly required for a basic implementation, this connection provides a deeper theoretical grounding for the model's stability.
  - Quick check question: How is a gradient descent step on a constrained manifold (like the unit sphere) different from standard gradient descent in Euclidean space? What is the role of a "retraction"?

## Architecture Onboarding

- **Component map:**
  Input -> Query/Key/Value Projections -> Lattice Layer -> Output
  - Input Projections: The input $x_t$ is projected to create query ($q_t$), key ($k_t$), and value ($v_t$) vectors. A key innovation is that the key projection dimension ($m$) is the number of memory slots, which can be different from the model dimension ($d$).
  - Memory State: The core is a matrix-valued state $S_t \in \mathbb{R}^{d \times m}$, representing $m$ memory slots (columns), each of dimension $d$.
  - Lattice Layer (The Core Recurrence):
    1. State Normalization: Each column of the state matrix $S_{t-1}$ is normalized to have a unit norm.
    2. Gradient & Orthogonal Projection: The gradient of the compression loss (reconstructing $v_t$) is computed. This calculation inherently projects the error onto the orthogonal complement of each memory slot.
    3. State Update: The state is updated using the orthogonal projection of the gradient, combined with a learnable learning rate ($\gamma_t$) and an optional forgetting gate ($\mu_t$).
    4. Re-normalization: The updated state columns are re-normalized to unit length.
  - Readout: The output $y_t$ is computed as a linear readout from the updated state: $y_t = S_t q_t$.

- **Critical path:**
  1. A new token $x_t$ arrives.
  2. Projections generate $q_t, k_t, v_t$.
  3. The Lattice layer takes $k_t, v_t$ and the previous state $S_{t-1}$.
  4. Inside the layer, the update vector is computed. This is the most critical step: it must correctly calculate the component of $v_t$ that is orthogonal to each column of $S_{t-1}$.
  5. The state is updated and re-normalized to get $S_t$.
  6. The output $y_t$ is computed from $S_t$ and $q_t$.

- **Design tradeoffs:**
  - Chunk Size ($C$): A smaller chunk size ($C=1$) gives the exact recurrence but is slow to train. Larger chunks parallelize the computation, speeding up training, but use a coarser gradient approximation, which can hurt performance. A common default is $C=4$ or $C=16$.
  - Memory Size ($m$): More slots increase memory capacity and model quality but also increase parameter count (in projection layers) and computational cost ($O(dm)$). Empirically, Lattice performs well even with $m < d$, showing a 4x compression gain over baselines like TTT.

- **Failure signatures:**
  - Training Instability: Unbounded growth of state vector norms, or `NaN` losses, likely indicates a failure in the state normalization step (Section 3.2). The re-normalization is critical.
  - Poor Performance with Large Chunks: A significant drop in perplexity or task accuracy when increasing the chunk size for training suggests the gradient approximation is too poor.
  - No Learning: If the model fails to improve, check the learning rate for the inner loop compression loss. The paper uses a learnable learning rate $\gamma_t = \text{sigmoid}(W_\gamma x_t)$.

- **First 3 experiments:**
  1. Implement a single Lattice layer's update step: Start with the non-parallelized, exact recurrent form (chunk size = 1). Create a simple test where you feed a sequence of vectors and manually verify that the state updates are indeed orthogonal to the existing state columns. (Check: `<Delta, S>` should be close to zero, as in the commented-out line of the implementation snippet).
  2. Overfit a small synthetic sequence: Train a single-layer Lattice model on a short, simple sequence (e.g., from the MQAR task) and ensure it can achieve near-zero loss. This validates the core recurrence's ability to memorize and recall.
  3. Compare parallel vs. sequential implementation: For a moderately sized chunk (e.g., C=16), implement both the sequential scan and the chunk-wise parallel version. Verify that the numerical results of the two implementations are close (up to approximation error) and measure the speedup of the parallel version. This confirms the correctness and benefit of the chunk-wise formulation.

## Open Questions the Paper Calls Out

- Can specialized hardware-efficient kernels or hierarchical training strategies be developed to overcome the parallelization bottlenecks of Lattice's non-linear recurrence and match the throughput of linear attention?
- Can Lattice effectively fine-tune existing large-scale pre-trained Transformers into efficient RNNs without significant loss in capability compared to the original full-attention models?
- How does Lattice's performance sustain when scaling context lengths well beyond the tested 16k tokens given the fixed memory size constraint?

## Limitations
- Reliance on low-rank assumption for key-value cache may not hold for all attention architectures or tasks
- Chunk-wise parallelization introduces approximation error that could accumulate over long sequences
- Orthogonal update mechanism may struggle with redundant but semantically important information
- Theoretical guarantees for compression quality are limited to specific scenarios

## Confidence
- **High Confidence**: Orthogonal state recurrence mechanism and state normalization are well-grounded both theoretically and empirically
- **Medium Confidence**: Chunk-wise parallelization shows clear training efficiency benefits, but approximation error is not fully characterized
- **Low Confidence**: "Sub-quadratic complexity" claim is somewhat misleading in practice as chunk-wise parallelization still requires O(n) operations

## Next Checks
1. **Ablation study on chunk size scaling**: Systematically evaluate Lattice's performance across a wider range of chunk sizes (C=1, 4, 16, 64, 256) on a diverse set of tasks to quantify the exact trade-off between training efficiency and model quality.

2. **Low-rank assumption validation**: Conduct a comprehensive analysis of the key-value cache matrices across different attention architectures and tasks to empirically verify the low-rank structure assumption using singular value decomposition.

3. **Memory slot redundancy analysis**: Design experiments to test whether the orthogonal update mechanism effectively prevents redundant memory writes while preserving semantically important information by tracking update distributions across memory slots over time.