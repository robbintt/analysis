---
ver: rpa2
title: ResNets Are Deeper Than You Think
arxiv_id: '2506.14386'
source_url: https://arxiv.org/abs/2506.14386
tags:
- networks
- network
- should
- residual
- feedforward
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the fundamental reasons behind the success
  of residual connections in neural networks. While residual networks (ResNets) are
  known to train more stably and achieve higher accuracy than feedforward networks,
  the authors propose that this is not just due to better trainability, but because
  residual connections alter the function space the network inhabits.
---

# ResNets Are Deeper Than You Think

## Quick Facts
- arXiv ID: 2506.14386
- Source URL: https://arxiv.org/abs/2506.14386
- Authors: Christian H. X. Ali Mehmeti-Göpel; Michael Wand
- Reference count: 40
- Primary result: Residual connections provide an inductive bias through variable-depth path ensembles that cannot be replicated by improved training alone

## Executive Summary
This paper challenges the conventional wisdom that residual networks (ResNets) simply train more stably than feedforward networks. Through post-training experiments comparing partially linearized architectures, the authors demonstrate that residual connections fundamentally alter the function space networks inhabit. The key finding is that variable-depth architectures (enabled by channel-wise linearization) consistently outperform fixed-depth architectures, even when controlling for average path length, suggesting residual connections provide a genuine inductive bias aligned with natural data structure.

## Method Summary
The authors compare two partial linearization approaches on pre-trained feedforward networks: channel-wise (allowing individual channels to become linear) and layer-wise (forcing all channels in a layer to linearize together). Using PReLU activations with L0.5 regularization, they gradually reduce nonlinearity during a post-training phase. By comparing test accuracy versus Normalized Average Path Length (NAPL) between these approaches, they isolate the effect of variable-depth path ensembles from training dynamics.

## Key Results
- Channel-wise linearization consistently outperforms layer-wise for NAPL < 12 (ImageNet) and NAPL < 4 (CIFAR-10/100)
- The performance gap emerges even when both approaches start from identical pre-trained weights
- When starting from a ResNet (already variable-depth), the gap largely disappears
- Extracted networks naturally form variable-depth distributions similar to standard ResNets

## Why This Works (Mechanism)

### Mechanism 1: Function Space Non-Equivalence
Residual networks inhabit a fundamentally different function space than feedforward networks. A residual block R(x) = ϕ(Wx + b) + x can represent the identity function when W = b = 0, but a feedforward block F(x) = ϕ(Wx + b) with non-injective activation cannot. This extends through compositional depth—the function spaces diverge as networks deepen.

### Mechanism 2: Variable-Depth Path Ensemble as Inductive Bias
Channel-wise partial linearization allows individual channels to become linear while others remain nonlinear, creating emergent paths of varying lengths. When regularization pressure is applied, optimization naturally preserves a distribution of path lengths rather than collapsing to uniform depth—matching the binomial distribution observed in standard ResNets.

### Mechanism 3: Post-Training Isolation of Generalization Effects
Starting from a fully-trained feedforward network, partial linearization during post-training allows the network to be "molded" into either variable-depth (channel-wise) or fixed-depth (layer-wise) forms. Since both share the same pre-trained weights and only differ in the linearization constraint, optimization differences are minimized.

## Foundational Learning

- **Concept: Residual vs. Feedforward Architectures**
  - Why needed here: The entire paper hinges on understanding what residual connections do differently than standard feedforward layers
  - Quick check question: Can you explain why adding `+ x` to a layer's output changes what functions the network can represent?

- **Concept: Inductive Bias in Neural Networks**
  - Why needed here: The central claim is that variable-depth networks provide an inductive bias better aligned with natural data
  - Quick check question: What does it mean for an architecture to have an "inductive bias," and how does it differ from regularization?

- **Concept: Partial Linearization via PReLU**
  - Why needed here: The experimental methodology relies on replacing ReLU with PReLU and using L0.5 regularization to gradually reduce nonlinearity
  - Quick check question: How does setting the PReLU slope α → 1 make a layer linear, and why use channel-wise vs. layer-wise granularity?

## Architecture Onboarding

- **Component map:**
  Pre-trained RepVGG-A2 (feedforward, 23 layers) → Replace ReLU → PReLU (channel-wise OR layer-wise) → Add L0.5 regularization: Σ|1 - αᵢ|^0.5 × ω → Post-training (10-60 epochs) → Extracted sub-network (variable-depth or fixed-depth)

- **Critical path:**
  1. Load pre-trained RepVGG-A2, replace all ReLUs with channel-wise PReLU layers initialized to α=0.25
  2. Apply regularization weight ω to control linearization pressure; freeze slopes at αᵢ = 1 when |αᵢ - 1| < 0.01
  3. Measure NAPL (Normalized Average Path Length) = average nonlinear units per path
  4. Post-training: 10 epochs (ImageNet) or 60 epochs (CIFAR-100), SGD+momentum(0.9), lr=0.01, batch=256, multistep scheduler with γ=0.1

- **Design tradeoffs:**
  - Channel-wise vs. layer-wise: Channel-wise enables variable-depth emergence (better performance); layer-wise enforces fixed-depth (cleaner comparison baseline)
  - Regularization strength ω: Higher ω → more linearization → lower NAPL → eventually underfits
  - Post-training duration: Too short → incomplete optimization; too long → potential overfitting

- **Failure signatures:**
  - If channel-wise and layer-wise curves converge: check ω range, post-training duration, or if architecture already has residual connections
  - If both approaches degrade sharply: regularization may be too aggressive
  - If extracted networks show uniform path lengths: channel-wise granularity may not be correctly implemented

- **First 3 experiments:**
  1. Train ResNet56 NoShort from scratch, apply channel-wise and layer-wise partial linearization with varying ω; plot test accuracy vs. NAPL
  2. Repeat starting from ResNet56 Short (with residual connections); verify the gap shrinks
  3. After post-training, sample random input paths and record nonlinear units; compare distribution against binomial model

## Open Questions the Paper Calls Out

### Open Question 1
Can the performance gap between feedforward and residual networks ever be fully closed through improved training techniques, or is it fundamentally irreducible due to differences in function space? While the paper provides evidence the gap persists post-training, it acknowledges the difficulty of completely disentangling trainability from inductive bias.

### Open Question 2
Do these findings generalize from CNN architectures to other architectures, particularly Transformers? The paper focuses exclusively on convolutional architectures but residual connections are ubiquitous in modern neural network architectures including Transformers.

### Open Question 3
What specific structural properties of natural data make variable-depth architectures better aligned as an inductive bias? The paper provides empirical evidence but does not characterize what properties of natural data explain this advantage.

### Open Question 4
Why does the break-even point (where channel-wise and layer-wise linearization achieve parity) vary across datasets, appearing at lower NAPL for easier datasets? The paper observes this phenomenon empirically but does not explain the mechanism behind it.

## Limitations
- Post-training linearization approach introduces uncertainties due to L0.5 regularization's non-convexity
- Analysis assumes natural data distributions benefit from variable-depth processing, which may not hold for all tasks
- Function space argument relies on non-injective activations being standard, though edge cases could invalidate theoretical claims

## Confidence

- **Function Space Non-Equivalence (High):** Theoretical proofs are rigorous with well-established impossibility of reparametrization under width constraints
- **Variable-Depth Path Ensemble (Medium):** Strong empirical evidence for standard vision datasets, but generalization to other domains requires further validation
- **Post-Training Isolation (Low-Medium):** Clever experimental design but non-convex optimization effects and limited post-training duration introduce uncertainty

## Next Checks

1. **Synthetic Task Test:** Evaluate the variable-depth vs fixed-depth gap on a task with known uniform depth requirements (e.g., parity or sorting tasks) to test the inductive bias hypothesis

2. **Architecture Ablation:** Compare the gap when starting from architectures with different residual connection patterns (e.g., dense connections) to isolate the effect of variable-depth paths from other architectural factors

3. **Optimization Analysis:** Use second-order methods or NTK analysis to quantify how much of the gap is due to initialization vs function space differences