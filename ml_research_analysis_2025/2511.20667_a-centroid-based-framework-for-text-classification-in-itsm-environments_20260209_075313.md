---
ver: rpa2
title: A centroid based framework for text classification in itsm environments
arxiv_id: '2511.20667'
source_url: https://arxiv.org/abs/2511.20667
tags:
- classification
- itsm
- text
- hierarchical
- centroid
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a dual-embedding centroid-based classification
  framework for hierarchical text classification in ITSM environments. The method
  maintains separate semantic (SBERT) and lexical (TF-IDF) centroid representations
  per category, combining them through reciprocal rank fusion at inference time.
---

# A centroid based framework for text classification in itsm environments

## Quick Facts
- arXiv ID: 2511.20667
- Source URL: https://arxiv.org/abs/2511.20667
- Authors: Hossein Mohanna; Ali Ait-Bachir
- Reference count: 4
- Primary result: Dual-embedding centroid framework achieves 0.731 hierarchical F1 vs SVM's 0.727 on 8,968 ITSM tickets

## Executive Summary
This paper presents a dual-embedding centroid-based classification framework for hierarchical text classification in ITSM environments. The method maintains separate semantic (SBERT) and lexical (TF-IDF) centroid representations per category, combining them through reciprocal rank fusion at inference time. Evaluated on 8,968 ITSM tickets across 123 categories, the framework achieves hierarchical F1 of 0.731 compared to SVM's 0.727 while providing interpretability and operational efficiency. The approach demonstrates 5.9x faster training and up to 152x faster incremental updates compared to SVM, with consistent 8.6-8.8x speedup across batch sizes when excluding embedding computation.

## Method Summary
The framework uses dual embeddings (SBERT and TF-IDF) to create separate centroid representations for each category in the hierarchy. During training, centroids are computed post-order (children before parents) as the mean of category member embeddings. At inference, each embedding type independently scores category paths using leaf-only scoring, generating ranked lists that are fused via reciprocal rank fusion (k=40). The method supports non-mandatory leaf prediction and handles taxonomy updates efficiently by recomputing only affected centroids rather than full retraining.

## Key Results
- Hierarchical F1 of 0.731 versus SVM's 0.727 on 8,968 ITSM tickets
- 5.9× faster training compared to SVM (9.447s vs 55.971s)
- Up to 152× faster incremental updates for single samples (0.062s vs 9.447s)
- Consistent 8.6-8.8× speedup across batch sizes when excluding embedding computation

## Why This Works (Mechanism)

### Mechanism 1: Dual-embedding complementarity
- Claim: Separate semantic and lexical centroid representations capture complementary information
- Mechanism: SBERT captures semantic context while TF-IDF preserves domain-specific terminology, with independent centroid computation preserving representation space characteristics
- Core assumption: ITSM text contains both semantically rich descriptions and precise technical terminology
- Evidence: Reciprocal rank fusion significantly outperformed single-view approaches in related search systems
- Break condition: Minimal technical jargon or very short tickets reduce dual-embedding benefit

### Mechanism 2: Reciprocal Rank Fusion robustness
- Claim: RRF of independent rankings produces more robust classification than either embedding type alone
- Mechanism: Computes fused score based on rank positions rather than raw similarity scores, reducing sensitivity to score distribution differences
- Core assumption: Two embedding types make partially independent errors
- Evidence: Esteva et al. [2021] showed RRF fusion outperformed single-view approaches
- Break condition: Highly correlated rankings between embedding types provide minimal RRF benefit

### Mechanism 3: Centroid-based update efficiency
- Claim: Centroid representation enables 8.6-152× faster incremental updates compared to SVM
- Mechanism: Adding new samples or categories requires only computing/updating affected centroids, not full model retraining
- Core assumption: Category centroids are stable representations capturing class distributions
- Evidence: Single sample update: 0.062s vs SVM 9.447s (151.9× speedup)
- Break condition: High intra-class variance with multiple subclusters requires multi-centroid clustering, reducing update simplicity

## Foundational Learning

- **TF-IDF Vectorization**
  - Why needed here: Provides lexical representation capturing domain-specific terminology and exact phrase matching for technical ITSM language
  - Quick check question: Can you explain why TF-IDF downweights common words and upweights rare but discriminative terms?

- **Sentence Embeddings (SBERT)**
  - Why needed here: Produces dense vectors capturing semantic meaning of entire ticket descriptions, enabling similarity comparisons even with different wording
  - Quick check question: How does a sentence embedding differ from averaging word embeddings, and why does this matter for short technical text?

- **Hierarchical Text Classification Metrics**
  - Why needed here: Standard flat metrics treat all errors equally; hierarchical metrics provide partial credit for predictions in correct subtree
  - Quick check question: Why would exact match rate be misleading as sole metric for 123-category hierarchical problem?

## Architecture Onboarding

- **Component map**: Input Ticket → Preprocessing (HTML cleanup, deduplication) → Dual Encoding (SBERT encoder || TF-IDF vectorizer) → Path Scoring (max cosine similarity at each node) → Ranking (generate R_TFIDF and R_SBERT) → RRF Fusion (combine rankings with k=40) → Top-k Predictions

- **Critical path**: Data preprocessing → Dual embedding generation → Centroid computation (post-order) → Inference: score paths → rank separately → RRF fuse → return top-k

- **Design tradeoffs**: Leaf-only scoring (optimal for depth-90% leaf datasets) vs. path-averaged (better for balanced hierarchies); Multi-centroid clustering disabled for efficiency; Child sampling for parent centroids disabled for depth-homogeneous data

- **Failure signatures**: Low hierarchical F1 but high exact match (hierarchy too deep or scoring strategy mismatched); TF-IDF rankings dominate RRF (check SBERT embedding quality); Slow incremental updates despite centroid approach (verify embedding computation excluded from timing)

- **First 3 experiments**: 1) Baseline comparison: Run SVM, KNN, and centroid method with same dual embeddings; 2) Scoring strategy validation: Test leaf-only, simple average, and weighted scoring; 3) Incremental update stress test: Add batches of 1, 10, 100, 1000 samples to measure update time

## Open Questions the Paper Calls Out

- How do different path-based scoring strategies perform on datasets with balanced depth distributions?
- How does the framework generalize to hierarchical taxonomies outside ITSM domains?
- Would multi-centroid clustering for high-variance categories improve performance on more diverse datasets?
- How does the approach compare to hierarchy-aware transformer models when computational resources permit?

## Limitations

- Results depend on unspecified SBERT model variant, making exact replication difficult
- Evaluation focuses on single ITSM dataset with 123 categories and 8,968 tickets
- Comparison excludes embedding computation time, which is a fixed cost for large datasets
- Method assumes hierarchical taxonomies with post-order structure, may not generalize to cyclic or DAG-like hierarchies

## Confidence

- High confidence in general dual-embedding centroid approach and training efficiency claims
- Medium confidence in accuracy claims (0.731 vs 0.727) due to narrow margin and lack of statistical significance testing
- Low confidence in generalizability to other ITSM domains or non-hierarchical classification tasks

## Next Checks

1. Run 5-fold cross-validation comparing centroid method against SVM baselines with statistical significance testing on hierarchical F1 scores
2. Measure accuracy degradation when 10%, 25%, and 50% of category labels are randomly reassigned to simulate taxonomy drift
3. Repeat experiments with three different SBERT models to determine if dual-embedding advantage holds across different semantic representations