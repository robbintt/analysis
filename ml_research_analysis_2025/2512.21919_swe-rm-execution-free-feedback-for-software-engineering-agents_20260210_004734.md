---
ver: rpa2
title: 'SWE-RM: Execution-free Feedback For Software Engineering Agents'
arxiv_id: '2512.21919'
source_url: https://arxiv.org/abs/2512.21919
tags:
- reward
- verifier
- training
- data
- wang
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces SWE-RM, a 30B MoE (3B active) reward model
  designed to provide execution-free feedback for software engineering agents. Unlike
  existing execution-based verifiers that rely on unit tests, SWE-RM delivers continuous,
  fine-grained scores without sandbox environments.
---

# SWE-RM: Execution-free Feedback For Software Engineering Agents

## Quick Facts
- arXiv ID: 2512.21919
- Source URL: https://arxiv.org/abs/2512.21919
- Reference count: 29
- Primary result: SWE-RM improves Qwen3-Coder-Flash from 51.6% to 62.0% on SWE-Bench Verified

## Executive Summary
SWE-RM is a 30B MoE reward model (with 3B active parameters) that provides execution-free feedback for software engineering agents. Unlike traditional execution-based verifiers that require unit tests and sandbox environments, SWE-RM delivers continuous, fine-grained scores directly from code trajectories. The model identifies three critical properties for effective reward models: test-time scaling performance, discriminative ability (AUC), and calibration (ECE). Through extensive controlled experiments, the authors determine optimal training configurations and demonstrate that SWE-RM achieves state-of-the-art performance on SWE-Bench Verified.

## Method Summary
The authors develop SWE-RM as a reward model specifically designed for software engineering tasks, addressing the limitations of execution-based verifiers. The model is trained on carefully curated datasets with optimal positive/negative ratios and policy mixtures. Key to its success is the focus on three evaluation metrics: test-time scaling performance, discriminative ability measured by AUC, and calibration measured by expected calibration error (ECE). The model supports 256k context length, enabling it to score complex, long trajectories. When integrated into reinforcement learning pipelines, SWE-RM delivers 3 absolute points higher pass@1 performance compared to execution-based verifiers.

## Key Results
- Improves Qwen3-Coder-Flash from 51.6% to 62.0% on SWE-Bench Verified
- Improves Qwen3-Coder-Max from 67.0% to 74.6% on SWE-Bench Verified
- Delivers 3 absolute points higher pass@1 than execution-based verifiers when used in RL

## Why This Works (Mechanism)
SWE-RM works by providing continuous, fine-grained feedback without requiring code execution or unit tests. This execution-free approach allows for faster, more scalable evaluation of software engineering agent trajectories. The model's effectiveness stems from its careful optimization for three key properties: test-time scaling performance (ability to handle longer contexts), discriminative ability (distinguishing good from bad solutions via AUC), and calibration (reliable probability estimates via ECE). By focusing on these properties during training and using optimal data configurations, SWE-RM can provide more nuanced and accurate feedback than traditional execution-based approaches.

## Foundational Learning
- **Reward Modeling**: Understanding how to train models to provide feedback signals for agent training; needed to create effective learning signals without explicit execution
- **Mixture-of-Experts (MoE)**: The 30B MoE architecture with 3B active parameters; needed for computational efficiency while maintaining performance
- **Test-Time Scaling**: The ability of models to maintain or improve performance as context length increases; needed for handling complex, long software engineering tasks
- **Expected Calibration Error (ECE)**: A metric measuring how well predicted probabilities match actual outcomes; needed to ensure reliable confidence estimates
- **Discriminative Ability (AUC)**: Area under the ROC curve measuring the model's ability to distinguish between positive and negative examples; needed for effective reward signal generation

## Architecture Onboarding

**Component Map:** Data Pipeline -> MoE Model (30B, 3B active) -> Reward Signal -> RL Agent

**Critical Path:** Training data preparation → MoE reward model training → RL agent training with reward signals → Evaluation on SWE-Bench Verified

**Design Tradeoffs:** The authors chose a MoE architecture (30B total, 3B active) to balance model capacity with computational efficiency. This allows for strong performance while keeping inference costs manageable compared to a full 30B dense model.

**Failure Signatures:** Poor performance would manifest as low test-time scaling (failure on longer contexts), low AUC (inability to discriminate good solutions), or high ECE (poor calibration leading to unreliable confidence estimates).

**Three First Experiments:**
1. Evaluate SWE-RM's performance on additional software engineering benchmarks beyond SWE-Bench Verified
2. Test the model's robustness across different programming languages and task complexities
3. Investigate whether the training methodology scales to larger models or alternative architectures

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but several implications arise from the work. These include the scalability of the approach to more complex, real-world software engineering tasks beyond the SWE-Bench Verified benchmark, the generalizability of the training methodology to other domains, and the practical implications of the 256k context length capability for real-world software engineering tasks.

## Limitations
- Limited evaluation to a single benchmark (SWE-Bench Verified) raises questions about generalizability
- Uncertainty about performance in more diverse or dynamic software engineering environments
- The practical implications of 256k context length for real-world tasks remain underexplored

## Confidence

| Claim | Confidence Level |
|-------|------------------|
| SWE-RM achieves state-of-the-art TTS performance on SWE-Bench Verified | High |
| SWE-RM provides 3 absolute points higher pass@1 than execution-based verifiers in RL | High |
| The identified optimal training configurations are well-supported by ablation studies | High |
| SWE-RM's performance generalizes to more complex, real-world software engineering tasks | Medium |
| The training methodology scales to other domains or larger models | Medium |

## Next Checks
1. Evaluate SWE-RM's performance on additional software engineering benchmarks or real-world tasks to assess generalizability
2. Conduct a robustness analysis by testing the model's performance under varying conditions, such as different programming languages or task complexities
3. Investigate the scalability of SWE-RM's training methodology to larger models or alternative architectures to determine if the observed improvements are transferable