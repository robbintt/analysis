---
ver: rpa2
title: Interaction Tensor SHAP
arxiv_id: '2512.05338'
source_url: https://arxiv.org/abs/2512.05338
tags:
- tensor
- representation
- interaction
- structure
- stii
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes IT-SHAP, a tensor-algebraic formulation of
  the Shapley-Taylor Interaction Index (STII) that enables exact computation of higher-order
  interactions in machine learning models. The core idea is to represent STII as a
  linear transformation on a value tensor and derive an explicit algebraic representation
  of its weight tensor using discrete finite difference operators.
---

# Interaction Tensor SHAP

## Quick Facts
- arXiv ID: 2512.05338
- Source URL: https://arxiv.org/abs/2512.05338
- Reference count: 20
- This paper proposes IT-SHAP, a tensor-algebraic formulation of the Shapley-Taylor Interaction Index (STII) that enables exact computation of higher-order interactions in machine learning models.

## Executive Summary
This paper reformulates the Shapley-Taylor Interaction Index (STII) as a tensor-algebraic operation, enabling exact computation of higher-order feature interactions in machine learning models. By representing STII as a linear transformation on a value tensor and deriving its weight tensor structure using discrete finite difference operators, the authors show that STII computation can be parallelized when the value function admits a Tensor Train (TT) representation. The key insight is that the weight tensor inherently possesses a TT structure, allowing higher-order interaction indices to be computed in NC² parallel complexity. In contrast, under general tensor network representations without structural assumptions, the same computation is proven to be #P-hard.

## Method Summary
The method reformulates STII as a linear tensor contraction T^(k) = f_W^(k) ×_2 V^(M,P), where V is a value tensor derived from model M and background distribution P, and f_W^(k) is a weight tensor constructed from discrete difference operators. The weight tensor is shown to have an exact TT representation with bounded ranks, enabling parallel computation. The discrete difference operators δ_i act as rank-1, mode-wise linear maps that preserve the TT structure when composed. Under the TT assumption, the contraction can be evaluated in NC² parallel complexity class with polylogarithmic depth.

## Key Results
- STII can be computed exactly as a linear tensor contraction rather than via combinatorial enumeration
- The STII weight tensor possesses an exact Tensor Train structure enabling parallelizable computation
- Computational complexity shifts from #P-hard under general tensor networks to NC² under TT assumptions

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Higher-order STII values can be computed exactly as a linear tensor contraction rather than via combinatorial enumeration.
- **Mechanism:** The paper reformulates STII as a linear transformation acting on a value tensor $V$. By defining a specific "Modified Weighted Coalitional Tensor" (weight tensor $fW^{(k)}$), the interaction index $I^{(k)}_S(F)$ becomes a contraction $fW^{(k)} \times V$. This bypasses the exponential summation over coalitions typically required by game-theoretic definitions.
- **Core assumption:** The value function $F$ (and corresponding tensor $V$) can be defined over the Boolean hypercube $\{0,1\}^{n_{in}}$, allowing a tensor representation.
- **Evidence anchors:**
  - [abstract]: "We reformulate STII as a linear transformation on a value function and derive an explicit algebraic representation of its weight tensor."
  - [section 4.2]: Theorem 4.1 defines the IT-SHAP value tensor via tensor contraction.
  - [corpus]: Related work "SHAP Meets Tensor Networks" establishes the base MST framework for first-order Shapley values which this paper extends.
- **Break condition:** If the value function cannot be embedded into a tensor structure (e.g., non-discrete or non-functional inputs without discretization), the linear contraction mechanism fails.

### Mechanism 2
- **Claim:** The STII weight tensor inherently possesses an exact Tensor Train (TT) structure, enabling parallelizable computation.
- **Mechanism:** STII relies on higher-order discrete difference operators $\delta_i$. The paper demonstrates these operators are rank-1, mode-wise linear maps. When composed ($\delta_S$), they preserve the one-dimensional chain structure of the Tensor Train representation. This implies the complex interaction weights are algebraically structured as a sequence of matrix multiplications.
- **Core assumption:** The discrete difference operator $\delta_i$ acts as a rank-1 perturbation (Assumption: specifically derived from matrix $D$ in Eq 4.5), which maintains the TT-rank bounds locally.
- **Evidence anchors:**
  - [section 4.4]: "The TT structure... follows necessarily from the linear-algebraic properties of the discrete difference operators inherent in the definition of STII."
  - [section 4.4.1]: Derivation of $\delta_i$ as a rank-1 linear map preserving TT structure.
  - [corpus]: "TT-FSI" provides parallel evidence that tensor trains are effective for scalable Shapley interactions.
- **Break condition:** If alternative tensor representations (like Tucker or CP) are used, the paper argues the structure is lost (Tucker fails locality) or explodes in complexity (CP grows exponentially), breaking the efficiency mechanism.

### Mechanism 3
- **Claim:** The computational complexity of higher-order interactions is determined by the representation structure, shifting the problem from intractable (#P-hard) to efficiently parallelizable ($NC^2$).
- **Mechanism:** Under general Tensor Network representations, the contraction problem encodes counting queries that are #P-hard. However, the specific TT structure reduces the global contraction to a chain of matrix multiplications. Matrix multiplication chains have polylogarithmic depth, placing the problem in the parallel complexity class $NC^2$.
- **Core assumption:** Sufficient processors are available to exploit the parallel depth, and TT ranks remain manageable (bounded).
- **Evidence anchors:**
  - [abstract]: "Under general tensor network representations... computation is proven to be #P-hard... When the value function admits a Tensor Train representation... computed in the parallel complexity class NC squared."
  - [section 4.3.2]: Theorem 4.5 states NC² computability under TT assumptions.
  - [corpus]: Weak direct evidence in corpus for the specific NC² claim, though "SHAP Meets Tensor Networks" discusses tractability.
- **Break condition:** If the TT-ranks of the value tensor grow exponentially with input dimension $n_{in}$ (the "curse of dimensionality"), the polynomial processor count assumption is violated, negating the practical efficiency.

## Foundational Learning

- **Concept: Discrete Derivatives on Boolean Hypercube**
  - **Why needed here:** STII is defined via higher-order discrete derivatives $\delta_S F(T)$ which capture "pure incremental effects" of feature sets. Understanding these as linear operators (matrices) is essential to see why they preserve tensor structure.
  - **Quick check question:** How does the operator $\delta_i$ differ from a standard gradient, and why does its rank-1 nature matter for tensor compression?

- **Concept: Tensor Train (TT) Representation**
  - **Why needed here:** This is the core data structure. It represents a high-order tensor as a chain of 3rd-order cores ($G^{(i)}$), turning exponential storage into polynomial storage. The paper relies on the "closure" of this format under discrete differences.
  - **Quick check question:** If you have a tensor of order $N$, how does the storage cost scale in TT format vs. dense format?

- **Concept: Parallel Complexity Class NC**
  - **Why needed here:** The paper claims results in $NC^2$, implying the problem is solvable in $O(\log^2 n)$ time with polynomially many processors. This distinguishes "inherently sequential hard" problems from "embarrassingly parallel" ones.
  - **Quick check question:** Does $NC^2$ imply the algorithm is fast on a single CPU, or only on massively parallel hardware?

## Architecture Onboarding

- **Component map:**
  - Inputs: Model $M$, Background Distribution $P$, Interaction Order $k$
  - Value Tensor ($V$): Constructed from $M$ and $P$. Must be pre-processed into TT format
  - Weight Tensor ($fW^{(k)}$): Constructed algebraically from discrete difference operators. Theorem 4.6 guarantees it has an exact TT representation
  - Contraction Engine: Multiplies the TT-cores of Value and Weight tensors

- **Critical path:**
  1. Verify the Value Tensor admits a low-rank TT representation (bottleneck)
  2. Construct the Weight Tensor TT cores using the derived rank-1 difference matrices
  3. Execute the chain contraction to produce interaction indices

- **Design tradeoffs:**
  - **Exactness vs. Rank:** The method provides exact STII interaction indices *only if* the Value Tensor TT-ranks are exact. Approximating a model with low TT-ranks introduces error
  - **Output Size vs. Processors:** While depth is polylogarithmic, the number of required processors scales with the total output size $\Theta(n^k_{in})$ and TT ranks

- **Failure signatures:**
  - **Rank Explosion:** If TT-ranks of the Value Tensor grow exponentially, memory usage explodes (reverting to exponential complexity)
  - **Representation Mismatch:** Using CP or Tucker decompositions for the value tensor will not inherit the efficiency guarantees for the discrete difference operators (Section 4.4.3)

- **First 3 experiments:**
  1. **Validation ($k=1$):** Implement the IT-SHAP pipeline for $k=1$ and verify outputs exactly match standard Shapley values (MST) on a simple decision tree (known TT structure)
  2. **Rank Stress Test:** Synthesize high-dimensional value tensors with varying correlation structures. Measure the TT-ranks of the resulting Weight and Value tensors to identify when "rank explosion" occurs
  3. **Scaling Profiling:** Fix interaction order $k=2$ and scale input features $n_{in}$. Compare wall-clock time of the TT-contraction vs. naive combinatorial enumeration to verify the transition from exponential to polynomial scaling

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can approximate tensor decomposition techniques (e.g., TensorSketch, randomized SVD) be integrated with IT-SHAP to handle scenarios where exact Tensor Train ranks are prohibitively high?
- Basis in paper: [explicit] The conclusion explicitly states that "integrating such techniques with IT-SHAP remains an important avenue for future research" to address rank growth.
- Why unresolved: The current framework assumes input tensors are available in exact TT form, avoiding the complexity of approximation during the interaction calculation itself.
- What evidence would resolve it: An extension of the IT-SHAP algorithm that incorporates approximate decomposition with rigorous error bounds on the resulting interaction indices.

### Open Question 2
- Question: How can suitable Tensor Train representations be constructed and maintained for general high-dimensional models within realistic computational resources?
- Basis in paper: [explicit] The discussion notes that for general tensors, TT ranks may grow exponentially, making the "preprocessing stage" a "potential computational bottleneck" for real-world application.
- Why unresolved: The paper proves tractability conditional on the existence of a low-rank TT representation but does not provide a method to generate such representations for arbitrary models.
- What evidence would resolve it: An efficient algorithm for constructing low-rank TT value tensors from standard machine learning model formats (e.g., neural network weights) without exponential overhead.

### Open Question 3
- Question: Does the structure-preserving property of the STII weight tensor extend to other interaction indices (e.g., Faith-Shap) or different discrete derivative definitions?
- Basis in paper: [inferred] The proofs rely specifically on the linear-algebraic properties of STII's discrete difference operators; the authors do not generalize the TT-representability proof to other indices.
- Why unresolved: Alternative indices may employ weighting schemes or higher-order operators that do not decompose into rank-1 mode-wise operators as STII does.
- What evidence would resolve it: A derivation showing the TT-rank bounds (or lack thereof) for the weight tensors associated with non-STII interaction indices.

## Limitations
- The paper assumes the value tensor $V$ is already available in TT format, but provides no algorithmic method to convert arbitrary model outputs into this representation
- While the paper proves $NC^2$ membership for TT representations, the practical speedup depends on TT-ranks remaining low; rank explosion would negate efficiency gains
- The #P-hardness proof for general tensor networks is established, but the connection to practical tensor representations (CP, Tucker) could be stronger

## Confidence
- **High Confidence:** The algebraic reformulation of STII as a linear tensor contraction is mathematically sound, with clear derivations of the weight tensor structure and its TT representation (Theorem 4.6)
- **Medium Confidence:** The $NC^2$ parallel complexity claim is technically correct given the TT assumptions, but the practical implementation challenges and rank behavior in real-world models are not fully explored
- **Low Confidence:** The #P-hardness proof for general tensor networks is established, but the connection to practical tensor representations (CP, Tucker) could be stronger

## Next Checks
1. **TT Rank Growth Analysis:** Systematically measure TT ranks of the weight tensor $fW^{(k)}$ for varying $n_{in}$ and $k$ to verify they remain polynomial and bounded as claimed
2. **Rank-1 Difference Operator Verification:** Implement and test the discrete difference operator $\delta_i$ as a rank-1 perturbation on TT cores, verifying local TT structure preservation for arbitrary input tensors
3. **Model-to-TT Conversion Benchmark:** Develop and benchmark methods to convert different model types (linear, tree, neural network) into TT-representable value tensors, measuring the approximation error and TT-rank trade-offs