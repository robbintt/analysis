---
ver: rpa2
title: Predictive posterior sampling from non-stationnary Gaussian process priors
  via Diffusion models with application to climate data
arxiv_id: '2505.24556'
source_url: https://arxiv.org/abs/2505.24556
tags:
- samples
- posterior
- prior
- sample
- where
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes using diffusion generative models (DGMs) to
  sample from predictive posterior distributions (PPDs) associated with non-stationary
  Gaussian process (GP) priors, addressing the computational intractability of exact
  sampling. The method replaces the GP prior with a DGM surrogate and applies post-training
  conditioning to sample from the desired posterior.
---

# Predictive posterior sampling from non-stationnary Gaussian process priors via Diffusion models with application to climate data

## Quick Facts
- **arXiv ID:** 2505.24556
- **Source URL:** https://arxiv.org/abs/2505.24556
- **Reference count:** 40
- **Key outcome:** DGM surrogate priors with training-free guidance achieve state-of-the-art CRPS/ES for climate SSTA reconstruction vs MCMC/VAE baselines.

## Executive Summary
This paper proposes using diffusion generative models (DGMs) to sample from predictive posterior distributions (PPDs) associated with non-stationary Gaussian process (GP) priors, addressing the computational intractability of exact sampling. The method replaces the GP prior with a DGM surrogate and applies post-training conditioning to sample from the desired posterior. Experiments validate that the generated distributions closely match the GP counterparts using metrics like maximum sliced-Wasserstein and classifier two-sample tests. The approach is demonstrated on synthetic data and applied to climate data (sea surface temperature anomaly reconstruction), achieving state-of-the-art performance compared to alternatives like MCMC and PriorV AE, with lower continuous ranked probability scores and energy scores. The method is scalable once the DGM is trained, making it suitable for complex spatial inference problems.

## Method Summary
The paper trains a diffusion generative model (DGM) surrogate to approximate a non-stationary Gaussian process (GP) prior defined via stochastic partial differential equations (SPDEs). Training data is generated by solving the SPDE on a Riemannian manifold using the finite element method (FEM) to create 250k synthetic GRF samples. The trained DGM is then conditioned on partial observations using training-free guidance algorithms (specifically MGDM) to sample from the predictive posterior distribution. The method is validated on synthetic data and applied to climate data for sea surface temperature anomaly reconstruction, showing superior performance to MCMC and other baselines.

## Key Results
- DGM surrogate achieves Max-SW distance of 0.053 vs 0.079 for stationary GP prior on synthetic validation
- Climate SSTA reconstruction achieves CRPS of 0.65 and ES of 1.3, outperforming PriorV AE and MCMC baselines
- DGM-based approach scales efficiently to high-dimensional problems once trained, avoiding MCMC's computational bottleneck

## Why This Works (Mechanism)

### Mechanism 1: DGM Surrogate Learning
Diffusion models can effectively approximate the distribution of complex, non-stationary Gaussian Random Fields (GRFs) by learning the underlying score function. A denoising network $D_\theta$ is trained on GRF samples generated via SPDEs, learning to reverse a diffusion process and implicitly capture spatially varying anisotropies and correlation lengths that standard stationary kernels cannot model.

### Mechanism 2: Training-Free Guidance for Posterior Sampling
Pre-trained DGMs can be conditioned on partial observations to sample from the predictive posterior distribution without retraining the prior model. The paper utilizes algorithms like MGDM that modify the reverse diffusion sampling loop, guiding trajectories toward samples that satisfy observed data through a likelihood term $\ell(y|x)$.

### Mechanism 3: SPDE-Based Data Generation
SPDEs provide a scalable mechanism to synthesize training data required for the DGM surrogate. GRFs are defined as solutions to SPDEs on Riemannian manifolds, allowing generation of arbitrary realizations via FEM, translating mathematical definitions into pixel-space datasets.

## Foundational Learning

- **Concept: Gaussian Processes (GPs) and Non-stationarity**
  - **Why needed here:** The paper's goal is to emulate a specific, complex prior. Understanding how the metric tensor $g$ defines local anisotropy and covariance is essential for validating if the DGM has learned the "correct" physics prior.
  - **Quick check question:** Can you explain how a locally anisotropic covariance function differs from a stationary Matérn kernel in terms of spatial correlation lengths?

- **Concept: Diffusion Models (Score-based Generative Models)**
  - **Why needed here:** This is the surrogate architecture. Understanding how the denoising score matching objective relates to learning the gradient of the data distribution is crucial.
  - **Quick check question:** In the context of this paper, how does the network $D_\theta(x_t, t)$ approximate the score function $\nabla \log p_t(x_t)$?

- **Concept: Bayesian Posterior Sampling**
  - **Why needed here:** The ultimate output is a sample conditioned on data. Understanding Bayes' theorem is necessary to see why "guidance" is mathematically necessary to shift the prior to the posterior.
  - **Quick check question:** If the observation noise $\sigma_y$ is very high, how should the posterior samples $p(x|y)$ visually compare to the prior samples?

## Architecture Onboarding

- **Component map:** SPDE solver + FEM (Generates training data) -> UNet-based DGM (Learns the prior) -> MGDM/DPS algorithm (Conditions the model) -> Max-SW / C2ST metrics (Validates distributional match)

- **Critical path:** The Data Generation phase. If the SPDE solver or metric tensor definition is incorrect, the DGM learns a flawed physics prior, rendering all downstream inference invalid. This step is computationally expensive ($300k$ samples).

- **Design tradeoffs:**
  - **Network Depth vs. Memory:** The paper reduces channel multipliers (64 vs 192 standard) to allow larger batch sizes (32) on V100s, trading raw per-step capacity for better gradient estimates via larger batches.
  - **Sampler Steps vs. Quality:** DDPM with 1000 steps yields the best Max-SW scores, but Heun/ODE samplers are significantly faster for inference.
  - **Approximation vs. Speed:** MGDM provides state-of-the-art CRPS but relies on DGM approximation quality. Vecchia-MCMC (VMCMC) is exact but scales exponentially with neighbors/observations.

- **Failure signatures:**
  - **High Max-SW Score:** The DGM is not generating diverse or accurate samples. Check if the noise schedule $\sigma(t)$ matches the data scale.
  - **Posterior Overfitting:** If the standard deviation map of the posterior samples is near zero in unobserved regions, the guidance is likely too strong or the DGM has collapsed.
  - **Boundary Artifacts:** If generated fields look distorted at the edges, ensure the SPDE domain was padded before discretization.

- **First 3 experiments:**
  1. **Prior Validation:** Train the DGM on the synthetic dataset. Compare 1,000 generated samples against held-out SPDE data using Maximum Sliced-Wasserstein (Max-SW) distance to ensure the prior is learned.
  2. **Denoiser Scaling:** Plot the MSE of the denoiser vs. noise level $\sigma$. Verify the slope matches the theoretical bound $\sigma^{\frac{2\nu}{\nu+1}}$ to confirm the network isn't under-fitting.
  3. **Inpainting Inference:** Apply the MGDM sampler to a masked test image. Visually inspect if the reconstructed textures match the local anisotropy of the observed context.

## Open Questions the Paper Calls Out

- **Open Question 1:** How can training-free guidance algorithms (e.g., DPS, MGDM) be modified to correct the lack of calibration observed in the posterior standard deviations? The paper notes that posterior samplers produce standard deviations that don't match the actual error distribution, and this discrepancy needs resolution.

- **Open Question 2:** Can this framework be extended to represent continuous Gaussian Process priors rather than relying on fixed regular grid discretization? The current architecture requires a fixed 256×256 discretization; moving to continuous space requires architectural changes to handle arbitrary spatial coordinates.

- **Open Question 3:** Can a conditional DGM be trained to include the regularity parameter $\nu$ as a variable hyperparameter without degrading sample quality? The smoothness parameter significantly alters high-frequency components, and conditioning the generative model on this parameter adds complexity to the denoising task.

## Limitations
- SPDE-based data generation pipeline has limited corpus validation and underspecified numerical implementation details
- Guidance algorithms are treated as black boxes with limited theoretical guarantees for infinite-dimensional settings
- Real-world application lacks ground truth for absolute quality validation of generated fields

## Confidence
- **High Confidence:** The core DGM surrogate learning mechanism is well-supported by synthetic validation and ablation showing MSE scaling matches theory
- **Medium Confidence:** The training-free guidance approach is empirically validated but theoretical understanding of approximation quality is incomplete
- **Medium Confidence:** The SPDE data generation is a distinct contribution but numerical implementation details create reproducibility challenges

## Next Checks
1. **Posterior Fidelity Under Varying Observation Noise:** Systematically vary observation noise $\sigma_y$ and measure how CRPS/ES scores change, validating that posterior standard deviation maps correctly reflect observation uncertainty.

2. **SPDE Solver Sensitivity Analysis:** Test the impact of FEM discretization resolution (e.g., 128x128 vs 256x256 grid) on the learned DGM prior's Max-SW score against held-out SPDE data to identify numerical convergence.

3. **Guidance Algorithm Ablation with Known Posteriors:** For synthetic problems where the exact GP posterior can be computed, compare samples from MGDM against the analytical posterior to quantify approximation errors introduced by the DGM surrogate.