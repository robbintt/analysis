---
ver: rpa2
title: Emotion and Intention Guided Multi-Modal Learning for Sticker Response Selection
arxiv_id: '2511.17587'
source_url: https://arxiv.org/abs/2511.17587
tags:
- emotion
- sticker
- intention
- emotional
- alignment
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the Sticker Response Selection (SRS) task,
  which aims to select contextually appropriate stickers in dialogues. Existing methods
  rely on semantic matching and model emotion and intention separately, leading to
  mismatches when they are misaligned.
---

# Emotion and Intention Guided Multi-Modal Learning for Sticker Response Selection

## Quick Facts
- arXiv ID: 2511.17587
- Source URL: https://arxiv.org/abs/2511.17587
- Reference count: 15
- Primary result: Proposed framework achieves 81.2% MAP and 72.3% R10@1 on StickerChat, outperforming state-of-the-art methods

## Executive Summary
This paper introduces Emotion and Intention Guided Multi-Modal Learning (EIGML), a framework designed to improve sticker response selection in dialogues by jointly modeling emotion and intention. Unlike previous methods that treat these signals separately, EIGML employs a Dual-Level Contrastive Framework for alignment and an Intention-Emotion Guided Multi-Modal Fusion module. Experiments on StickerChat and DSTC10-MOD datasets demonstrate significant performance gains, achieving 81.2% MAP and 72.3% R10@1 on StickerChat.

## Method Summary
EIGML jointly models emotion and intention through a two-component architecture. First, the Dual-Level Contrastive Framework performs both intra-modality (within text/image) and inter-modality (across text-image) alignment using InfoNCE and KL divergence losses. Second, the Intention-Emotion Guided Multi-Modal Fusion module filters intention knowledge based on emotional context (EIKS), fuses emotion-intention features with visual representations via cross-attention (IEGA), and adjusts final matching scores using emotion/intention similarity (SAMM). The framework is trained end-to-end using binary cross-entropy loss with BERT and ViT encoders.

## Key Results
- Achieves 81.2% MAP and 72.3% R10@1 on StickerChat dataset
- Achieves 67.4% MAP and 50.0% R10@1 on DSTC10-MOD dataset
- Outperforms state-of-the-art methods by 1.1-2.3% in MAP across both datasets
- Ablation studies confirm contributions of each component: removing inter-modality alignment drops MAP from 81.31% to 80.29%

## Why This Works (Mechanism)

### Mechanism 1: Dual-Level Contrastive Alignment
Jointly aligning emotion and intention representations both within and across modalities reduces representational mismatches that occur when these signals are modeled in isolation. The DLCF applies InfoNCE-style contrastive losses for inter-modality semantic/emotion/intention alignment, while intra-modality alignment uses dropout-induced views with symmetric KL divergence for emotion/intention consistency plus instance-level contrastive loss. Core assumption: Emotional and intentional cues are inherently intertwined in communication; separating them causes systematic biases in sticker-dialogue matching.

### Mechanism 2: Emotion-Guided Intention Knowledge Filtering
Using emotional context to filter commonsense intention knowledge improves the relevance of injected priors before fusion. EIKS applies an emotion classifier to text features, identifies knowledge vectors with highest emotion classification loss as irrelevant, computes a gradient-based adjustment vector via nonlinear regression, then uses a gating mechanism to fuse enhanced knowledge with original representations. Core assumption: Not all commonsense inferences from COMET are equally relevant to the current emotional state; emotional salience can guide knowledge selection.

### Mechanism 3: Similarity-Adjusted Score Fusion
Weighted combination of semantic matching scores with emotion/intention similarity scores corrects for cases where pure semantic alignment fails. SAMM computes normalized cosine similarities between dialogue and sticker in emotion space (s_emo) and intention space (s_int), combines them with learnable α, then fuses with semantic matching score p_vl using learnable β to produce final ranking. Core assumption: Emotion and intention similarity provide orthogonal signals to semantic matching that can rescue misaligned cases.

## Foundational Learning

- Concept: **Contrastive Learning (InfoNCE loss)**
  - Why needed here: Core to both inter-modality and intra-modality alignment; requires understanding of positive/negative pair construction and temperature scaling.
  - Quick check question: Can you explain how InfoNCE differs from triplet loss, and why symmetric losses are used for intra-modality consistency?

- Concept: **Cross-Attention for Multi-Modal Fusion**
  - Why needed here: IEGA uses intention-emotion features as queries attending to visual keys/values; requires understanding of multi-head attention mechanics.
  - Quick check question: Given query dimension d_q and key dimension d_k, what is the computational complexity of attention, and how does d_head affect it?

- Concept: **Commonsense Knowledge Injection (COMET)**
  - Why needed here: Intention features derive from COMET inferences over four relation types; requires understanding of knowledge graph completion and generative transformers.
  - Quick check question: What are the four relation types used (xIntent, xNeed, xWant, xEffect), and why might concatenating all of them introduce noise?

## Architecture Onboarding

- Component map:
  Input (Dialogue + Stickers) → Encoders (BERT for text, ViT for images) → DLCF (Inter: L_I2T, L_T2I, L_emo, L_int | Intra: KL + InfoNCE per modality) → IEGMF (EIKS → IEGA → SAMM) → BCE Loss on final scores

- Critical path:
  1. Dialogue → cnsenti → emotion label → E_t
  2. Dialogue → COMET → commonsense inferences → I_t → EIKS → I'_t
  3. [I'_t; E_t] → IEGA queries → attends to F_v → E_fuse → Z_fuse
  4. Z_fuse → MLP → p_vl; cosine similarities → s_EI; β-fusion → p_final

- Design tradeoffs:
  - Joint vs. separate emotion/intention modeling: Joint adds complexity but addresses misalignment (validated by ablation)
  - Gating vs. hard filtering: Soft gating (Equation 12-13) preserves information but may retain noise
  - Loss weight balancing (w_e, w_i): Paper finds 0.5 optimal; extreme values degrade performance (Figure 5)

- Failure signatures:
  - Low MAP but high semantic similarity scores → emotion/intention alignment failing; check L_emo, L_int convergence
  - Training instability with NaN losses → check KL divergence for zero-probability distributions; add epsilon smoothing
  - Inference produces identical scores for all stickers → contrastive components not discarded; verify inference path uses only p_final

- First 3 experiments:
  1. **Baseline sanity check**: Train with only L_itm (semantic matching); verify MAP ~70% on StickerChat to match CLIP baseline. If significantly lower, check encoder loading.
  2. **Component ablation sweep**: Remove one module at a time (DLCF inter, DLCF intra, EIKS, IEGA, SAMM); verify each causes 0.5-1% MAP drop per Table 4.
  3. **Hyperparameter validation**: Sweep w_e and w_i from 0 to 1 in 0.25 steps; confirm peak at 0.5 per Figure 5. If peak shifts, check learning rate or batch size interactions.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the rigid definition of 7 emotion categories and 4 COMET intention relations limit the model's ability to capture complex, compound states like sarcasm or irony?
- Basis in paper: The Conclusion states the need for "deeper exploration of fine-grained emotional and intentional modeling," while the Method section restricts the implementation to specific fixed taxonomies (cnsenti emotions and 4 COMET relations).
- Why unresolved: The current architecture relies on softmax classifiers and specific linear heads tailored to these discrete sets, making it structurally difficult to represent continuous or compound affective states.
- Evidence: Ablation studies substituting the fixed categories with continuous valence-arousal-dominance representations or open-vocabulary intent labels.

### Open Question 2
- Question: Does the sequential dependency in the IEGMF module (where Emotion-Guided Intention Selection precedes Attention Fusion) lead to error propagation if the initial emotion classification is incorrect?
- Basis in paper: The "Intention-Emotion Guided Multi-Modal Fusion" section describes a three-stage pipeline (EIKS → IEGA → SAMM), where intention knowledge is filtered by emotion gradients before fusion.
- Why unresolved: The paper demonstrates that removing components hurts performance, but does not verify if this specific sequential ordering is optimal compared to parallel or bidirectional fusion strategies.
- Evidence: Comparative analysis of an architectural variant where emotion and intention features are fused in parallel or where intention selection is guided by multi-modal context rather than text-only emotion gradients.

### Open Question 3
- Question: Can the proposed static visual encoding strategy effectively generalize to animated stickers (GIFs) where motion and temporal dynamics convey critical intentional cues?
- Basis in paper: The "Implementation Details" specify the use of ViT-B/16 (a static image encoder) with inputs resized to 128x128, ignoring the temporal dimension often present in modern sticker formats.
- Why unresolved: The visual encoder F_v and the Dual-Level Contrastive Framework are designed to process spatial features only, lacking mechanisms to capture temporal consistency or motion-based intent.
- Evidence: Evaluation of the current framework on a dataset of animated stickers using frame-averaged features versus a baseline using temporal video encoders (e.g., Video Swin Transformer).

## Limitations

- The framework's performance heavily depends on the quality of external tools (cnsenti for emotion detection and COMET for commonsense knowledge), which may not be optimized for sticker-chat contexts
- The paper doesn't validate whether improvements come from better representation learning or simply from adding more parameters and complexity
- Claims about emotion and intention being "inherently intertwined" are asserted rather than empirically validated through controlled experiments

## Confidence

- **High Confidence**: The overall framework architecture and training procedure are well-specified and reproducible. The performance improvements over baselines are substantial and consistent across both datasets (81.2% vs 80.1% MAP on StickerChat, 67.4% vs 66.1% MAP on DSTC10-MOD).
- **Medium Confidence**: The contrastive alignment mechanisms are theoretically sound and show expected ablation behavior, but the paper doesn't provide qualitative analysis of what specific types of mismatches the joint modeling actually resolves. The gradient-based knowledge filtering mechanism in EIKS is innovative but lacks theoretical grounding for why this specific formulation works.
- **Low Confidence**: Claims about emotion and intention being "inherently intertwined" in communication are asserted rather than empirically validated. The paper doesn't test whether the framework generalizes to other emotion-intention combinations or different knowledge sources beyond COMET.

## Next Checks

1. **Knowledge Source Sensitivity**: Replace COMET with an alternative commonsense inference model (e.g., different ATOMIC variant or ConceptNet) and measure performance degradation. This would validate whether improvements come from the framework or the specific knowledge source.

2. **Cross-Dataset Generalization**: Train EIGML on StickerChat and evaluate on DSTC10-MOD (and vice versa) without fine-tuning. This tests whether emotion-intention alignment learned on one domain transfers to another.

3. **Qualitative Error Analysis**: Manually examine 50 randomly selected errors from the test set to identify whether failures stem from emotion/intention misalignment, poor visual-texture matching, or knowledge injection noise. This would reveal which components contribute most to remaining errors.