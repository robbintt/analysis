---
ver: rpa2
title: What Does Neuro Mean to Cardio? Investigating the Role of Clinical Specialty
  Data in Medical LLMs
arxiv_id: '2505.10113'
source_url: https://arxiv.org/abs/2505.10113
tags:
- clinical
- specialties
- medical
- specialty
- medicine
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces S-MedQA, the first English medical question-answering
  dataset annotated for 15 distinct clinical specialties, including single- and multi-specialty
  annotations for cross-disciplinary questions. The dataset contains over 24k QA pairs
  constructed with machine and expert verification.
---

# What Does Neuro Mean to Cardio? Investigating the Role of Clinical Specialty Data in Medical LLMs

## Quick Facts
- arXiv ID: 2505.10113
- Source URL: https://arxiv.org/abs/2505.10113
- Reference count: 18
- Fine-tuning on clinical data produces domain shifting rather than specialty-specific knowledge injection

## Executive Summary
This paper investigates whether fine-tuning large language models on data from specific clinical specialties improves performance on those same specialties versus cross-specialty transfer. The authors introduce S-MedQA, the first English medical QA dataset with 24k+ questions annotated for 15 clinical specialties, including multi-specialty annotations for cross-disciplinary questions. Through extensive experiments with 8 LLMs and 6 clinical specialties, they find that improvements from medical fine-tuning are primarily driven by domain shifting (general-to-medical) rather than specialty-specific knowledge injection. Token probability analysis reveals that clinically relevant terms consistently increase in probability after fine-tuning, regardless of the specialty used, supporting the domain-shift hypothesis.

## Method Summary
The paper introduces S-MedQA, constructed from MedQA and MedMCQA with expert-verified annotations across 15 specialties. Questions are annotated using a 5-prompt majority voting approach (GPT-3.5) with expert validation, then expanded to multi-specialty labels via conformal prediction. Six clinical specialties are selected for experiments: Cardiology, Gastroenterology, Infectious Diseases, Neurology, Obstetrics & Gynecology, and Pediatrics. Eight LLMs are fine-tuned using LoRA adapters on projection layers (rank 32, alpha 16, dropout 0.1, learning rate 2e-5) for up to 10 epochs. Evaluation uses 5x answer shuffling with a Mistral-v0.2 classifier for option matching (96.5% accuracy) rather than first-token probability.

## Key Results
- Training on one clinical specialty does not necessarily lead to best performance on that specialty (off-diagonal patterns in 3 of 6×6 evaluation pairs)
- Improvements from medical fine-tuning are primarily driven by domain shifting rather than specialty-specific knowledge injection
- Token probabilities of clinically relevant terms consistently increase after fine-tuning regardless of specialty, while non-medical fine-tuning reduces these probabilities
- Cross-specialty transfer occurs despite low term overlap (average 32.8% across specialties vs 63.4% within specialties)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Fine-tuning on clinical data primarily produces domain shifting rather than specialty-specific knowledge injection.
- Mechanism: Medical fine-tuning adjusts the model's output distribution toward clinical language patterns, increasing the probability of clinically relevant tokens across all specialties uniformly, rather than encoding specialty-specific factual knowledge.
- Core assumption: Knowledge required for medical QA is largely present from pretraining; fine-tuning primarily teaches the model which knowledge subdistribution to access.
- Evidence anchors: [abstract] "improvement gains, at least in our settings, are derived primarily from domain shifting (e.g., general to medical) rather than from injecting specialty-specific knowledge"; [section 3.4] "regardless of the specialty the LLM was fine-tuned on, token probabilities of clinically relevant terms consistently increase across all specialties"

### Mechanism 2
- Claim: Cross-specialty transfer occurs without significant direct term overlap, suggesting transfer operates at a level above surface terminology.
- Mechanism: Clinical reasoning patterns (diagnostic logic, symptom-disease mapping, question-answering format) transfer across specialties even when specific terminology differs substantially.
- Core assumption: Clinical QA requires shared reasoning structures across medical domains, not just vocabulary matching.
- Evidence anchors: [abstract] "training on data from a clinical specialty does not necessarily lead to the best performance on that specialty"; [section 3.3] "average overlap in train/test splits within the same specialty is 63.4%" vs "average overlap across different specialties is only 32.8%"

### Mechanism 3
- Claim: Performance gains from medical fine-tuning are not explained by additional training steps alone.
- Mechanism: Fine-tuning on non-medical data (even high-quality text) reduces clinical token probabilities, indicating the content domain—not just optimization—drives improvement.
- Core assumption: Token probability changes reflect meaningful shifts in model behavior, not training artifacts.
- Evidence anchors: [section 3.4] "When fine-tuning on MMLU out-of-domain datasets there is a significant drop in token probabilities compared to models trained on S-MedQA data"; [section 6] "improvements can be primarily attributed to domain shifting rather than knowledge injection"

## Foundational Learning

- Concept: LoRA (Low-Rank Adaptation) fine-tuning
  - Why needed here: All experiments use LoRA on projection layers (rank 32, alpha 16, dropout 0.1) to efficiently adapt large models without full parameter updates.
  - Quick check question: Can you explain why LoRA enables parameter-efficient domain adaptation while preserving pretrained knowledge?

- Concept: Conformal prediction for multi-label annotation
  - Why needed here: S-MedQA uses conformal prediction with coverage guarantees to expand single-specialty annotations to multiple specialties for cross-disciplinary questions.
  - Quick check question: How does conformal prediction provide statistical coverage guarantees compared to threshold-based classification?

- Concept: Token probability analysis for mechanistic interpretability
  - Why needed here: The paper's core evidence relies on comparing token log-probabilities before/after fine-tuning to distinguish domain shifting from knowledge injection.
  - Quick check question: Why is token probability a better indicator of model behavior change than accuracy alone?

## Architecture Onboarding

- Component map:
  S-MedQA dataset (24k QA pairs) -> GPT-3.5 majority voting (5 prompts) -> Expert validation -> Conformal multi-label expansion -> LoRA fine-tuning (rank 32, alpha 16) -> Classifier-based evaluation (Mistral-v0.2)

- Critical path:
  1. Majority voting (3+ of 5 prompts) achieves 90.8-97.8% annotation accuracy
  2. Expert validation confirms quality (Krippendorff's α = 83.6%)
  3. Cross-specialty training reveals off-diagonal performance patterns
  4. Token probability analysis confirms domain-shift hypothesis

- Design tradeoffs:
  - Accuracy vs. coverage: Higher voting thresholds (5+ votes) yield 97.8% accuracy but only 49.2% coverage vs. 90.8% accuracy / 89.1% coverage at 3+ votes
  - Single vs. multi-specialty labels: Multi-label conformal prediction achieves 0.69 precision / 0.52 recall, trading exactness for cross-disciplinary coverage
  - First-token probability vs. full-text classification: First-token suffers selection bias; classifier approach (Figure 3) resolves ambiguous outputs

- Failure signatures:
  - Low inter-annotator agreement on ambiguous cases (e.g., Neurology vs. Emergency Medicine for acute stroke questions)
  - Off-diagonal best performance indicating specialty-specific training does not guarantee specialty-specific gains
  - Decreased clinical token probabilities when fine-tuning on non-medical domains

- First 3 experiments:
  1. Replicate single-specialty cross-evaluation matrix (Table 4) on a new base model to confirm off-diagonal pattern generalizes.
  2. Run token probability analysis comparing medical vs. non-medical fine-tuning to validate domain-shift mechanism on your target model.
  3. Test balanced training set sizes (Table 12 approach) to rule out data imbalance as confound for cross-specialty transfer results.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the complexity or difficulty level of QA pairs influence the observed cross-specialty transfer patterns and the effectiveness of domain shifting?
- Basis in paper: [explicit] The conclusion states that "the precise impact of different types of QA data (e.g., complexity or difficulty of the QA pair) remains unclear."
- Why unresolved: The current analysis aggregates data by specialty without stratifying by the cognitive load or difficulty of the individual questions.
- What evidence would resolve it: Experiments that categorize S-MedQA questions by difficulty (e.g., using a difficulty classifier) and analyze if the "off-diagonal" performance advantage persists or diminishes for high-complexity queries.

### Open Question 2
- Question: Does the dominance of "domain shifting" over "knowledge injection" generalize to other knowledge-intensive domains beyond medicine?
- Basis in paper: [explicit] The authors state in the discussion that "validating to what extent (Zhou et al., 2024)'s hypothesis generalises to other knowledge-intensive domains is not something we address in this work."
- Why unresolved: Medical data has unique hierarchical structures (e.g., SNOMED-CT) that might facilitate domain shifting in a way distinct from legal or financial domains.
- What evidence would resolve it: Replicating the cross-sub-domain evaluation methodology on a non-medical dataset (e.g., legal case law or financial regulations) to see if training on one sub-domain (e.g., Tax Law) improves performance on another (e.g., Contract Law) more than self-training.

### Open Question 3
- Question: Do the findings regarding cross-specialty transfer hold for open-ended generation tasks, or are they specific to multiple-choice QA?
- Basis in paper: [inferred] The paper relies exclusively on MedQA and MedMCQA, which are multiple-choice datasets. The mechanism of increasing token probabilities for clinical terms may manifest differently in long-form generation.
- Why unresolved: Multiple-choice evaluation relies on selecting from fixed options, whereas clinical practice often requires generating free-text diagnoses or summaries.
- What evidence would resolve it: Fine-tuning models on the S-MedQA specialties and evaluating them on open-ended clinical QA benchmarks (e.g., MedInstruct) using semantic similarity or LLM-as-a-judge metrics.

## Limitations

- Core claims rest on token probability analysis rather than direct knowledge probes, leaving open whether models genuinely lack specialty-specific knowledge or simply fail to retrieve it during QA tasks
- Multi-label annotation approach trades precision (0.69) for recall (0.52), potentially diluting specialty signal in cross-disciplinary questions
- Single-specialty best performance occurs in only 3 of 6×6 evaluation pairs, suggesting either the specialty split is too coarse-grained or the mechanism varies across domains

## Confidence

**High Confidence:** The domain-shift mechanism is supported by consistent token probability increases across specialties and the negative control of non-medical fine-tuning reducing clinical token probabilities. The cross-specialty transfer patterns are robust across 8 LLMs and multiple evaluation methods.

**Medium Confidence:** The claim that specialty-specific fine-tuning does not improve same-specialty performance is supported but limited by the small sample of 6 specialties tested. The 63.4% term overlap within specialties versus 32.8% across specialties suggests substantial reasoning transfer, but does not definitively prove the mechanism operates above terminology.

**Low Confidence:** The absence of knowledge injection is inferred from token probabilities rather than directly tested. The multi-label annotation precision-recall tradeoff may mask true specialty-specific patterns, and the paper does not explore whether different fine-tuning objectives (e.g., chain-of-thought reasoning) would alter the mechanism.

## Next Checks

1. **Knowledge probe experiment:** Design structured recall tests targeting factual knowledge within each specialty (e.g., drug dosages, anatomical relationships) to directly measure knowledge injection versus token distribution shifts.

2. **Coarseness sensitivity analysis:** Repeat the cross-specialty evaluation with broader specialty groupings (e.g., combining related specialties) to determine whether the off-diagonal performance patterns persist or whether the current granularity masks underlying knowledge transfer.

3. **Fine-tuning objective variation:** Compare LoRA fine-tuning on projection layers versus other layers (e.g., attention layers) or with different objectives (e.g., instruction tuning, reasoning-focused adaptation) to isolate whether the domain-shift mechanism depends on the fine-tuning approach.