---
ver: rpa2
title: Does Training with Synthetic Data Truly Protect Privacy?
arxiv_id: '2502.12976'
source_url: https://arxiv.org/abs/2502.12976
tags:
- data
- privacy
- synthetic
- training
- private
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper rigorously evaluates whether training models on synthetic
  data provides privacy protection against membership inference attacks. The authors
  analyze four training paradigms: coreset selection, dataset distillation, data-free
  knowledge distillation, and synthetic data from diffusion models.'
---

# Does Training with Synthetic Data Truly Protect Privacy?

## Quick Facts
- arXiv ID: 2502.12976
- Source URL: https://arxiv.org/abs/2502.12976
- Reference count: 20
- Key outcome: Most synthetic data training methods fail to provide meaningful privacy protection against membership inference attacks compared to basic differential privacy baselines

## Executive Summary
This paper rigorously evaluates whether training models on synthetic data provides privacy protection against membership inference attacks. The authors systematically analyze four training paradigms: coreset selection, dataset distillation, data-free knowledge distillation, and synthetic data from diffusion models. Through controlled experiments on CIFAR-10 using mislabeled data as canaries to simulate worst-case scenarios, they demonstrate that most synthetic data methods fail to provide meaningful privacy protection compared to basic differential privacy baselines.

The study reveals that dataset distillation methods initialized with private data leak significant privacy, while those using out-of-distribution initialization show better protection but reduced utility. Data-free knowledge distillation methods perform relatively better but still fall short of providing robust privacy guarantees. The results challenge the common assumption that synthetic data training inherently protects privacy and suggest that careful consideration of initialization methods and training paradigms is crucial for privacy preservation.

## Method Summary
The authors conducted systematic experiments comparing four synthetic data training paradigms against baseline methods on CIFAR-10. They used mislabeled data as canaries to create worst-case privacy scenarios and evaluated privacy leakage through membership inference attacks. The four methods analyzed were coreset selection (selecting representative subsets), dataset distillation (creating compact synthetic representations), data-free knowledge distillation (training without original data), and diffusion model-generated synthetic data. Each method was evaluated for both privacy protection (via membership inference attack success rates) and utility preservation (via downstream task performance).

## Key Results
- Dataset distillation methods initialized with private data show significant privacy leakage, with membership inference attack success rates exceeding baseline methods
- Out-of-distribution initialization for dataset distillation provides better privacy protection but substantially reduces utility
- Data-free knowledge distillation performs relatively better than other methods but still falls short of basic differential privacy baselines
- No synthetic data training paradigm consistently outperformed basic differential privacy methods in protecting against membership inference attacks

## Why This Works (Mechanism)
The privacy leakage in synthetic data training appears to stem from the fact that many synthetic data generation methods retain or amplify membership information from the original training data. When dataset distillation methods are initialized with private data, the synthetic representations inherit and concentrate privacy-sensitive information. Even data-free methods that theoretically avoid direct access to private data can still leak information through the knowledge distillation process. The diffusion model approaches, while producing high-quality synthetic data, do not inherently remove membership information, making them vulnerable to inference attacks. These mechanisms suggest that synthetic data generation is not automatically privacy-preserving and requires additional safeguards.

## Foundational Learning
The paper builds on the foundational understanding that synthetic data generation is often assumed to provide privacy protection by removing direct access to original data. However, this work challenges that assumption by demonstrating that the synthesis process itself can encode and amplify privacy-sensitive information. The foundational learning here is that privacy in synthetic data training depends critically on the specific method used, the initialization approach, and the training paradigm. The work suggests that differential privacy mechanisms applied during synthetic data generation may be necessary rather than relying on the synthesis process alone for privacy protection.

## Architecture Onboarding
This work provides practical guidance for practitioners considering synthetic data approaches for privacy preservation. The key architectural insight is that the choice of initialization method for synthetic data generation significantly impacts privacy outcomes. For dataset distillation, using out-of-distribution initialization provides better privacy protection than private data initialization, though at the cost of utility. Data-free knowledge distillation architectures may offer a middle ground but still require additional privacy mechanisms. The paper suggests that practitioners should carefully evaluate their specific use case and consider applying explicit differential privacy mechanisms rather than assuming synthetic data generation provides adequate protection.

## Open Questions the Paper Calls Out
The paper highlights several important open questions that warrant further investigation. First, how do these privacy results generalize across different data modalities beyond CIFAR-10, particularly for medical imaging, text, and other high-stakes domains? Second, what is the temporal dynamics of privacy leakage in synthetic data generation - does privacy degrade over multiple iterations of synthetic data synthesis? Third, how would adaptive adversaries who can observe and respond to synthetic data characteristics perform compared to the static membership inference attacks used in this study? Finally, the paper questions whether there exists an optimal trade-off point between privacy protection and utility that can be achieved through hybrid approaches combining synthetic data methods with explicit privacy mechanisms.

## Limitations
- Evaluation limited to CIFAR-10 dataset, reducing generalizability to other data modalities and domains
- Use of mislabeled data as canaries creates worst-case scenarios that may not reflect realistic privacy threat models
- Study does not explore temporal aspects of privacy leakage or consider adaptive adversaries who might adjust strategies based on observed synthetic data patterns

## Confidence
- High Confidence: Dataset distillation methods initialized with private data show significant privacy leakage
- Medium Confidence: Comparative analysis between different training paradigms provides valuable insights but limited scope to CIFAR-10
- Medium Confidence: Conclusion that synthetic data training often provides less privacy protection than basic differential privacy baselines is reasonable but needs broader validation

## Next Checks
1. Replicate experiments on diverse datasets (ImageNet, medical imaging) to assess generalizability across different data modalities
2. Conduct longitudinal studies to examine how privacy leakage evolves over multiple iterations of synthetic data generation
3. Design experiments with adaptive membership inference attacks that can adjust strategies based on synthetic data characteristics