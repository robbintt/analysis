---
ver: rpa2
title: 'Multimodal Large Language Models for Low-Resource Languages: A Case Study
  for Basque'
arxiv_id: '2511.09396'
source_url: https://arxiv.org/abs/2511.09396
tags:
- basque
- multimodal
- training
- data
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents the first large-scale study on developing Multimodal
  Large Language Models (MLLMs) for a low-resource language, Basque. The authors create
  new multimodal datasets for training and evaluation by translating English resources,
  including CC3M for alignment, Pixmo-AMA for instruction tuning, and benchmarks like
  VQAv2, A-OKVQA, and PixMo-CapQA.
---

# Multimodal Large Language Models for Low-Resource Languages: A Case Study for Basque

## Quick Facts
- **arXiv ID**: 2511.09396
- **Source URL**: https://arxiv.org/abs/2511.09396
- **Reference count**: 0
- **Primary result**: Basque MLLMs achieve strong performance with only 20% multimodal training data

## Executive Summary
This paper presents the first large-scale study on developing Multimodal Large Language Models (MLLMs) for a low-resource language, Basque. The authors create new multimodal datasets by translating English resources and explore MLLM architectures using late-fusion with two backbone models. Key findings show that minimal Basque multimodal data (20%) combined with text-only instructions is sufficient for strong performance, and a Basque-instructed backbone is not required. These results suggest that low-resource MLLMs can be built efficiently using cross-lingual transfer, enabling scalable development for other under-resourced languages.

## Method Summary
The authors develop Basque MLLMs using a two-stage late-fusion approach. Stage 1 trains a linear connector between CLIP image encoder and Llama-3.1-8B-Instruct (or Latxa variant) on translated CC3M captions (80/20 Basque/English mix). Stage 2 fine-tunes the connector and LLM on Pixmo-AMA QA pairs with varying proportions of Basque data (20-80%) plus optional text-only instructions from Magpie. Datasets are created by machine-translating English multimodal resources. The model is evaluated on translated benchmarks (VQAv2Eus, A-OKVQAEus, PixMo-CapQAEus) and human-evaluated open-ended tasks from WildVisionEus.

## Key Results
- Only 20% Basque multimodal data is sufficient for strong performance on downstream tasks
- Incorporating text-only Basque data improves both text-only and multimodal performance
- No significant difference between Llama-3.1-Instruct and Basque-adapted Latxa backbones in human evaluation
- Cross-modal transfer enables efficient low-resource MLLM development

## Why This Works (Mechanism)
The approach leverages cross-lingual and cross-modal transfer learning to compensate for limited Basque multimodal data. By starting with a strong English MLLM foundation and using machine translation for data augmentation, the model can learn Basque visual-linguistic patterns while retaining knowledge from the larger English corpus. The two-stage training allows for efficient fine-tuning without catastrophic forgetting. The inclusion of text-only instructions provides additional context that benefits both unimodal and multimodal tasks through shared representation learning.

## Foundational Learning
- **Cross-lingual transfer**: Knowledge from high-resource languages (English) applied to low-resource languages (Basque). Why needed: Basque lacks large-scale multimodal training data. Quick check: Compare performance with/without cross-lingual training.
- **Late-fusion architecture**: Separate processing of text and image modalities with fusion at later stages. Why needed: Simplifies integration of pretrained models. Quick check: Verify individual modality performance before fusion.
- **Machine translation for dataset creation**: Using NMT and LLM translation to create training data. Why needed: Enables rapid dataset development without manual annotation. Quick check: Human evaluation of translation quality.
- **Connector training**: Linear layer that bridges visual and textual embeddings. Why needed: Aligns different feature spaces for effective fusion. Quick check: Test connector performance on alignment tasks.
- **Instruction tuning**: Fine-tuning on task-specific instructions to improve zero-shot performance. Why needed: Enhances model's ability to follow instructions in target language. Quick check: Evaluate instruction-following capability.

## Architecture Onboarding

**Component Map**: CLIP-ViT-Large-patch14-336 -> Linear Connector -> Llama-3.1-8B-Instruct

**Critical Path**: Image Encoder → Connector → LLM → Output
- Image encoder (CLIP) processes visual input
- Linear connector aligns visual and textual embeddings
- LLM generates final output based on fused representation

**Design Tradeoffs**:
- **Late-fusion vs. single-stream**: Late-fusion allows use of separately pretrained models but may miss early cross-modal interactions
- **Translation vs. native data**: Translation enables rapid development but may miss cultural nuances
- **Two-stage training**: Prevents catastrophic forgetting but adds complexity

**Failure Signatures**:
- **Catastrophic forgetting**: Degraded English performance when using 100% Basque data
- **Modality collapse**: Visual or textual information lost during fusion
- **Overfitting**: Poor generalization on held-out evaluation sets

**First 3 Experiments**:
1. Evaluate connector alignment on bilingual CC3M subset
2. Test unimodal performance (text-only and image-only) before fusion
3. Measure cross-modal transfer by comparing models trained with/without text-only instructions

## Open Questions the Paper Calls Out

**Open Question 1**: Can MLLMs for low-resource languages be developed using only cross-lingual and cross-modality transfer, without any multimodal training data in the target language?
The study only tested scenarios with some multimodal data in Basque; the lower bound of multimodal data requirements remains unknown. Train MLLMs using English multimodal data combined with Basque text-only data (zero Basque multimodal samples) and evaluate on Basque multimodal benchmarks.

**Open Question 2**: How does incorporating culturally-specific multimodal content affect MLLM performance for low-resource languages?
All training and evaluation data came from translated English datasets, which contain no Basque cultural context or locally relevant visual content. Create evaluation benchmarks with Basque-specific cultural imagery and knowledge, then compare performance against translated benchmarks.

**Open Question 3**: Do these findings generalize to other low-resource languages with different linguistic characteristics and resource availability?
Basque may have unique properties; the 20% multimodal data threshold and text-only transfer effects may differ across languages. Replicate the experimental protocol across multiple low-resource languages with varying typological features and data availability levels.

## Limitations

- Training configuration lacks exact epoch/step counts, making optimal training duration uncertain
- All evaluation data comes from translated English resources, potentially missing Basque cultural nuances
- Late-fusion architecture may not be optimal compared to alternative approaches
- Human evaluation is limited to only three open-ended tasks from WildVisionEus

## Confidence

- **High Confidence**: Incorporating text-only Basque data improves both text-only and multimodal performance
- **Medium Confidence**: A Basque-instructed backbone is not required for good performance
- **Medium Confidence**: 20% Basque multimodal data is sufficient for strong performance (task-specific)

## Next Checks

1. **Training Configuration Validation**: Run controlled experiments varying the number of training epochs/steps for both stages to determine optimal training duration and confirm that the reported performance is not due to under/over-training.

2. **Architecture Ablation Study**: Implement and evaluate alternative MLLM architectures (single-stream, adapter-based) using the same datasets to determine whether the late-fusion approach is indeed optimal for low-resource scenarios or if other architectures might yield better results.

3. **Cross-Lingual Generalization Test**: Apply the established methodology to another low-resource language (e.g., Welsh or Georgian) to verify whether the findings about minimal multimodal data requirements and text-only instruction benefits generalize beyond Basque.