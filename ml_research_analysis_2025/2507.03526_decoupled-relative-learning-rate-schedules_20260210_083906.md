---
ver: rpa2
title: Decoupled Relative Learning Rate Schedules
arxiv_id: '2507.03526'
source_url: https://arxiv.org/abs/2507.03526
tags:
- learning
- relative
- rate
- training
- rates
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces Decoupled Relative Learning Rate Schedules
  (RLRS), a method that assigns individual learning rate schedules to different components
  of Transformer models. Instead of using a uniform learning rate across all layers,
  RLRS tunes separate rates for Embedding, Unembedding, Attention, Feed-Forward, and
  in MoE models, Router and Expert layers.
---

# Decoupled Relative Learning Rate Schedules

## Quick Facts
- arXiv ID: 2507.03526
- Source URL: https://arxiv.org/abs/2507.03526
- Reference count: 26
- This work introduces Decoupled Relative Learning Rate Schedules (RLRS), a method that assigns individual learning rate schedules to different components of Transformer models. Instead of using a uniform learning rate across all layers, RLRS tunes separate rates for Embedding, Unembedding, Attention, Feed-Forward, and in MoE models, Router and Expert layers. The relative rates are first optimized on small models and then transferred to larger ones, achieving up to 23% faster training and improved stability. The method works especially well for MoE architectures and scales effectively across model sizes, offering a practical and scalable approach to optimize large-scale neural networks.

## Executive Summary
Decoupled Relative Learning Rate Schedules (RLRS) addresses a fundamental inefficiency in Transformer training: applying uniform learning rates across heterogeneous architectural components. By assigning component-specific relative learning rate schedules, RLRS achieves 23% faster training and improved stability, particularly for Mixture of Experts models. The method works by first optimizing relative rates (λ_start, λ_end) on small proxy models, then transferring these values to larger models while only re-tuning the base learning rate. This approach effectively captures architecture-specific optimization dynamics that traditional uniform scheduling misses.

## Method Summary
RLRS extends standard cosine learning rate scheduling by applying component-specific scaling factors to different parameter groups (Embedding, Unembedding, Attention, FFN/Experts, Router). For each component m, the learning rate is scaled by λ_start^m at the beginning of training and λ_end^m at the end, with linear interpolation between. The method involves a local search on small models to find optimal λ values, which are then transferred unchanged to larger models. Only the base learning rate needs re-tuning at scale. This decoupled approach recognizes that different components exhibit distinct gradient update magnitudes and dynamics during training, requiring individualized optimization.

## Key Results
- Up to 23% faster training convergence compared to uniform learning rate scheduling
- 13.6% speedup when transferring RLRS from MoE8×34M to MoE8×906M models
- 8.7% speedup for Dense model transfer from 34M to 906M parameters
- Improved training stability with reduced loss spikes, especially in MoE architectures
- Effective transfer of relative rates across scales up to 27× larger models

## Why This Works (Mechanism)

### Mechanism 1: Heterogeneous Component Dynamics
Different Transformer components exhibit distinct weight update dynamics during training, requiring different learning rates to optimize efficiently. RLRS assigns separate scaling factors (λ_start, λ_end) to each component type, allowing the learning rate schedule to adapt to each component's unique gradient characteristics. Weight update magnitudes differ by 10-100× across components (Embedding: ~1100, Expert: ~650-700, Router: ~10-15, Attention: ~100). The optimal learning rate for a component correlates with its typical gradient/weight-update magnitude relative to other components, and this relationship holds across model scales.

### Mechanism 2: MoE-Specific Instability Mitigation
RLRS improves training stability in MoE models by lowering early learning rates for Router and Experts while maintaining higher rates elsewhere. Router stabilizes early (Xue et al., 2024), locking in routing decisions prematurely. RLRS sets λ_start=0.6 (Router) and λ_start=0.3 (Experts) to delay stabilization, then increases to λ_end=1.0 and λ_end=1.125 respectively for late-stage fine-tuning. This approach mitigates instabilities and loss spikes caused by MoE, simultaneously delaying the early router stabilization.

### Mechanism 3: Architecture-Dependent Rate Transfer
Relative learning rates optimized on small models transfer to larger models because they encode architecture-specific dynamics, not scale-specific ones. Search on small model (e.g., 34M active params) finds λ values that are applied unchanged to larger models (up to 906M active, 5.67B total). Only η_base needs re-tuning; relative rates remain constant. This transfer works because the optimal λ_start/λ_end ratios are invariant to model scale within the same architecture family.

## Foundational Learning

- Concept: **Cosine Learning Rate Scheduling**
  - Why needed here: RLRS builds on cosine decay; understanding the baseline schedule is essential for interpreting relative scaling.
  - Quick check question: Can you write the cosine LR formula and explain why the final LR fraction (α_end) matters?

- Concept: **Mixture of Experts (MoE) Routing**
  - Why needed here: Router dynamics are central to why RLRS helps MoE; early stabilization is the key problem.
  - Quick check question: What is token-choice routing, and why might early router decisions become problematic?

- Concept: **AdamW Optimizer**
  - Why needed here: RLRS operates on top of AdamW; understanding per-parameter adaptation clarifies what additional benefit RLRS provides.
  - Quick check question: How does Adam compute per-parameter learning rates, and why might this still leave room for component-wise tuning?

## Architecture Onboarding

- Component map:
  - Base Scheduler -> Cosine schedule with η_base, α_end, warmup
  - Parameter Groups -> Embedding, Unembedding, Attention, (FFN | Router+Experts)
  - Per-Component Parameters -> λ_start^m, λ_end^m for each component m
  - Effective LR -> η_t^m = η_base × interpolated(λ_start^m, λ_end^m) × cosine_factor

- Critical path:
  1. Implement component tagging in model definition (group parameters by type)
  2. Extend optimizer to accept per-group LR multipliers that vary over time
  3. Run local search (Algorithm 2) on small proxy model to find λ values
  4. Transfer λ values to target model; tune only η_base

- Design tradeoffs:
  - Search cost vs. transfer distance: Smaller proxy models are cheaper to search but may extrapolate less reliably to very large scales
  - Granularity vs. complexity: More component groups allow finer control but increase hyperparameter count; the paper finds 5-6 groups sufficient
  - Static vs. dynamic λ: Current approach uses fixed λ at start/end; time-varying schedules could offer more control but complicate search

- Failure signatures:
  - Loss spikes early in training: λ_start for Router/Experts may be too high
  - Loss plateaus prematurely: λ_end may be too low for critical components
  - Transfer fails (no speedup): Proxy model architecture may differ from target; re-check component matching
  - Embedding diverges at scale: Increase λ_start^Embedding per Section 4.3

- First 3 experiments:
  1. **Baseline comparison on small MoE**: Train MoE8×34M with and without RLRS; measure speedup metric and confirm ~20% improvement per Table 2.
  2. **Transfer validation**: Take λ values from 34M search; apply to 113M model without modification; verify speedup persists (should see ~19% per Table 3).
  3. **Ablation of single components**: Fix all λ=1 except one component; sweep its λ_start and λ_end to reproduce Figure 6 sensitivity patterns; confirm Attention is least sensitive, Embedding most sensitive at scale.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can Decoupled Relative Learning Rate Schedules (RLRS) be effectively combined with Tensor Programs to achieve zero-shot hyperparameter transfer?
- Basis in paper: Section 5.1 states, "By combining these two approaches, it may be possible to achieve a zero-shot transfer of RLRS."
- Why unresolved: The current RLRS method still requires independent tuning of the base learning rate for large models, whereas Tensor Programs aim to automate this transfer.
- What evidence would resolve it: A training run utilizing both methods that matches baseline performance without any learning rate tuning on the target large model.

### Open Question 2
- Question: What are the optimal scaling rules for extrapolating relative learning rates from small to large models?
- Basis in paper: Section 2.1 notes, "We leave the investigation of optimal extrapolation as future work."
- Why unresolved: The current work uses a direct transfer of relative values, but the authors do not claim this is optimal for all scales.
- What evidence would resolve it: A theoretical framework or empirical study defining functional forms for relative rate adjustments based on model width/depth that outperform direct transfer.

### Open Question 3
- Question: Can advanced hyperparameter optimization algorithms identify relative learning rate configurations that significantly outperform the simple local search method?
- Basis in paper: Appendix A states the local search is "relatively simple" and "a more complex alternative could... find an even better set of hyperparameters."
- Why unresolved: The authors used a basic iterative scaling algorithm to prove robustness, leaving the performance upper bound unexplored.
- What evidence would resolve it: A comparative study using Bayesian optimization or evolutionary strategies showing lower final loss or faster convergence than the reported local search results.

## Limitations

- The primary uncertainty is whether component-specific learning rate scaling captures meaningful optimization dynamics or stems from hyperparameter tuning artifacts.
- The transfer assumption works better for MoE than dense architectures, with only 8.7% speedup for Dense906M versus 13.6% for MoE8×906M.
- The method adds complexity to hyperparameter search without addressing whether simpler approaches (like layer-wise decay) could achieve similar results.
- The paper doesn't explore whether relative rates might need adjustment for different tasks or datasets beyond C4.

## Confidence

- **High Confidence**: The empirical speedup measurements (up to 23%) and stability improvements are well-documented through controlled experiments.
- **Medium Confidence**: The mechanism explanation (heterogeneous component dynamics) is plausible but not definitively proven.
- **Medium Confidence**: The MoE-specific stability claims are supported by loss curve comparisons, but the underlying cause is inferred rather than directly measured.

## Next Checks

1. **Ablation Study on Component Count**: Test whether the observed improvements persist when reducing the number of component groups (e.g., merge Attention and FFN, or treat all parameters uniformly). This would validate whether the fine-grained component separation is necessary versus simpler approaches.

2. **Cross-Dataset Transferability**: Apply the optimized λ values from C4 training to a different pretraining corpus (e.g., The Pile or Books3) to test whether the relative rates are architecture-specific or dataset-dependent. Measure if the same λ_start/λ_end pairs maintain their effectiveness.

3. **Comparison with Layer-Wise Decay**: Implement a simpler baseline where each layer gets a fixed multiplier based on depth (like Sun et al., 2019), and compare its performance against RLRS. This would determine whether the complexity of component-wise scheduling provides meaningful advantages over established layer-wise approaches.