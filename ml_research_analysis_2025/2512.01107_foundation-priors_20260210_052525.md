---
ver: rpa2
title: Foundation Priors
arxiv_id: '2512.01107'
source_url: https://arxiv.org/abs/2512.01107
tags:
- data
- foundation
- prior
- synthetic
- user
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the concept of a "foundation prior" to address
  the use of generative outputs from foundation models (like LLMs) as data in empirical
  research. The core issue is that such synthetic data is inherently subjective, reflecting
  both the model's learned patterns and the user's anticipations and biases introduced
  during prompt engineering.
---

# Foundation Priors

## Quick Facts
- **arXiv ID:** 2512.01107
- **Source URL:** https://arxiv.org/abs/2512.01107
- **Reference count:** 8
- **Primary result:** Introduces "foundation priors" as a framework to formally model generative outputs from foundation models as subjective priors rather than empirical data.

## Executive Summary
This paper addresses the growing practice of using synthetic data generated by foundation models (like LLMs) in empirical research. The core insight is that such synthetic data inherently reflects not just the model's learned patterns but also the user's subjective priors, expectations, and biases introduced during prompt engineering. The author proposes a formal framework where synthetic outputs are interpreted as draws from a "foundation prior"—an exponential-tilted, generalized Bayesian update of the user's primitive prior. This approach allows researchers to incorporate synthetic data into statistical workflows while maintaining epistemic discipline and avoiding the conflation of synthetic "facts" with real empirical observations.

## Method Summary
The method involves specifying a user's primitive prior π(θ) and likelihood function L(D|θ), then iteratively refining prompts to minimize divergence between anticipated and generated data distributions. The foundation prior ρ(θ|D_s,λ) is derived by minimizing KL(ρ∥π) subject to an expected log-likelihood constraint, yielding an exponential-tilted update where λ governs trust weight. When real data is available, λ is calibrated via marginal likelihood maximization. The final posterior combines real data, synthetic data, and the user's prior through this generalized Bayesian framework.

## Key Results
- Synthetic data from foundation models reflects both model patterns and user subjectivity through iterative prompt refinement
- Foundation priors emerge from constrained optimization balancing prior adherence against synthetic-data likelihood
- Real data provides an anchoring mechanism to calibrate appropriate trust levels for synthetic information
- The framework enables principled incorporation of synthetic data while avoiding conflation with real empirical observations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Synthetic outputs from foundation models encode user subjectivity through an iterative anticipation-alignment process, not objective data generation.
- Mechanism: Users form an anticipated data distribution π(Dₐ) by marginalizing over their prior beliefs. They then iteratively refine prompts q to minimize divergence κ(π(Dₐ)∥Dₛ(q)) between generated and anticipated data. This rejection-based process selectively accepts outputs conforming to prior expectations, injecting subjectivity at each iteration.
- Core assumption: Users implicitly or explicitly reject synthetic data that diverges from their anticipated distribution, creating a selection filter.
- Evidence anchors:
  - [abstract] "synthetic data reflects both the model's learned patterns and the user's subjective priors, expectations, and biases"
  - [section] Section 2.2-2.3 formalizes anticipation as π(Dₐ) = ∫L(Dₐ|θ)π(θ)dθ and prompt engineering as iterative divergence minimization (Eq. 2.2-2.4)
  - [corpus] Corpus evidence weak for this specific subjectivity mechanism; related papers focus on prior-data fitting (TabPFN variants) but not prompt-engineering-induced bias.
- Break condition: If prompts are fixed/non-iterative, or if users accept all outputs without evaluating against anticipations, the subjectivity injection attenuates.

### Mechanism 2
- Claim: The foundation prior emerges from constrained optimization balancing prior adherence against synthetic-data likelihood.
- Mechanism: Given synthetic data D*ₛ, the foundation prior ρ*(θ) minimizes KL(ρ∥π) subject to Eₚ[logL(D*ₛ|θ)] ≥ C(κ*). Lagrangian optimization yields ρ(θ|D*ₛ,λ) ∝ π(θ)L(D*ₛ|θ)^λ (Eq. 2.15)—an exponential-tilted update where λ governs trust weight.
- Core assumption: KL divergence is the appropriate discrepancy metric (chosen for tractability; Section 2.5 acknowledges alternatives possible).
- Evidence anchors:
  - [abstract] "We derive the foundation prior as an exponential-tilted, generalized Bayesian update of the user's primitive prior, where a trust parameter λ governs the weight assigned to synthetic data"
  - [section] Section 2.5 (Eq. 2.11-2.15) derives the full optimization; references Jaynes (1957), Kullback (1959), Csiszár (1975)
  - [corpus] Mitra paper (arXiv:2510.21204) addresses synthetic priors for tabular models but doesn't derive exponential-tilting mechanism.
- Break condition: If likelihood constraints are non-binding (C(κ*) too low), or if alternative divergences fundamentally change the update form.

### Mechanism 3
- Claim: Real data provides an anchoring mechanism to calibrate appropriate trust levels for synthetic information.
- Mechanism: Calibration selects λ* where the marginal value of increasing trust equals zero: E_πₛᵣ,λ*[logL(D*ₛ(q)|θ)] = E_ρ(·|D*ₛ,λ*)[logL(D*ₛ(q)|θ)] (Section 3.3). This aligns synthetic-data influence with empirical evidence, counteracting prompt-induced subjectivity.
- Core assumption: Real data provides objective information that can discipline subjective priors; also assumes λ interpretation as "trust" remains semantically valid post-calibration.
- Evidence anchors:
  - [abstract] "principled incorporation of synthetic data into statistical workflows while avoiding the conflation of synthetic 'facts' with real data"
  - [section] Section 3.3 derives calibration via Donsker-Varadhan representation; Section 4 discusses λ choice based on completeness, context, and cost
  - [corpus] Corpus papers don't address this calibration mechanism directly.
- Break condition: If no real data available for calibration, or if calibration yields λ→1 (synthetic data treated as fully credible), the anchoring fails and epistemic circularity dominates.

## Foundational Learning

- Concept: **KL Divergence and Exponential Family Tilting**
  - Why needed here: The foundation prior is derived as a KL-minimization solution; understanding why exponential tilting emerges requires knowing KL properties and conjugate relationships.
  - Quick check question: Can you explain why minimizing KL(ρ∥π) subject to a moment constraint yields an exponentially-tilted distribution?

- Concept: **Generalized Bayesian Updates**
  - Why needed here: The paper frames foundation priors as generalized Bayes updates (citing Bissiri et al., 2016) where λ replaces standard likelihood weighting; understanding this generalization clarifies why λ<1 is principled, not ad-hoc.
  - Quick check question: How does a generalized Bayes update differ from standard Bayesian updating, and what role does the loss function play?

- Concept: **Prior Predictive Distributions**
  - Why needed here: Synthetic data is interpreted as draws from the foundation prior's prior predictive distribution; distinguishing predictive from posterior distributions is essential for correct interpretation.
  - Quick check question: If D*ₛ is a draw from a prior predictive distribution, what does that imply about its epistemic status relative to observed data?

## Architecture Onboarding

- Component map:
  - User primitive prior π(θ) → Anticipated data distribution π(Dₐ) → Prompt engineering loop → Foundation prior ρ(θ|D*ₛ,λ) → Calibration module (if real data available) → Final posterior π(θ|Dᵣ,D*ₛ,λ)

- Critical path:
  1. Specify π(θ) and L(·|θ) → compute π(Dₐ)
  2. Initialize prompt q₀, generate Dₛ(q₀)
  3. Iterate prompt refinement until stopping criterion
  4. Compute foundation prior ρ(θ|D*ₛ,λ)
  5. If real data available, calibrate λ*
  6. Form final posterior and conduct inference

- Design tradeoffs:
  - **Higher λ**: More efficient learning if synthetic data is informative, but higher risk of epistemic circularity and prior self-reinforcement
  - **More prompt iterations**: Better match to anticipations, but more subjectivity injected; early stopping (small T_max) reflects lower prior confidence
  - **Integration across heterogeneous prompts** (Section 3.1): Reduces brittleness to single-prompt idiosyncrasy, but increases computational cost and requires h(q) specification
  - **Conservative λ (near 0)**: Robust to synthetic-data biases, but discards potentially valuable prior information

- Failure signatures:
  - **Synthetic data swamping**: λ≥1 with large Nₛ causes synthetic likelihood to dominate real data; posterior reflects anticipations, not evidence
  - **Exact anticipation matching**: κ→0 indicates prompt engineering perfectly replicated prior beliefs; foundation prior ≈ primitive prior (no information gain)
  - **Uncalibrated deployment**: Using foundation priors without real-data anchoring in high-stakes decisions; subjectivity remains un disciplined
  - **Prompt homogeneity**: Single prompt from one analyst; ρ(θ) brittle to that analyst's idiosyncrasies

- First 3 experiments:
  1. **λ-sensitivity analysis**: Fix D*ₛ from a calibrated prompt, vary λ ∈ {0.1, 0.3, 0.5, 0.7, 1.0}, observe posterior contraction and point estimates; document where inference stabilizes vs. where synthetic data dominates.
  2. **Heterogeneous prompt integration**: Generate D*ₛ(q) from 5-10 diverse prompts (different phrasings, examples, styles); compute both individual ρ(θ|D*ₛ(q),λ) and integrated ρ̄(θ|λ); measure dispersion reduction from integration.
  3. **Real-data calibration**: With held-out real data Dᵣ, implement calibration procedure from Section 3.3; estimate λ* and validate that posteriors using λ* outperform both λ=0 (no synthetic) and λ=1 (full trust) on predictive metrics.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the prompt distribution h(q) for integrating across heterogeneous analysts be empirically constructed and validated?
- Basis in paper: [explicit] The paper proposes integration across heterogeneous prompts via "a distribution h(q) representing the population of prompts individuals might reasonably produce" and states this "can be constructed empirically by eliciting prompts from a group of individuals or by using a structured family of prompts"—but provides no concrete methodology for either approach.
- Why unresolved: The paper offers only conceptual directions (elicitation or structured families) without specifying sampling procedures, parametric forms, or validation criteria for h(q).
- What evidence would resolve it: Empirical studies comparing different h(q) construction methods (e.g., expert elicitation vs. automated variation generation) showing which yields more robust posteriors when validated against held-out real data.

### Open Question 2
- Question: What principled constraints should govern the calibration of the trust parameter λ to prevent synthetic data from swamping real observations?
- Basis in paper: [explicit] The paper notes that calibration "will need to be constrained to deliver values consistent with that interpretation" and that there is a "need to 'shave' or constrain λ to more reasonable levels," but provides no specific constraint mechanisms beyond the theoretical calibration criterion.
- Why unresolved: The marginal likelihood calibration criterion alone can yield λ ≈ 1, which the author explicitly rejects as conceptually troubling since synthetic data can be generated infinitely at near-zero cost.
- What evidence would resolve it: Simulation studies or theoretical derivations establishing upper bounds on λ based on sample size ratios, divergence metrics between synthetic and real data, or information-theoretic criteria.

### Open Question 3
- Question: Can the foundation prior framework be generalized to work with moments or theoretical constraints rather than requiring full likelihood specifications?
- Basis in paper: [explicit] The paper states: "The exact formulation of the prior (2.15) is an artifact of specific functional assumptions made. The structure can be generalized, for example, to work with moments (rather than likelihoods), theoretical conditions, or other forms of informational constraints."
- Why unresolved: The current derivation relies on KL divergence minimization with likelihood constraints via the Donsker-Varadhan representation; generalizing to moment-based or other constraint types requires alternative optimization frameworks not yet developed.
- What evidence would resolve it: Formal derivations of moment-constrained foundation priors with proofs of consistency, plus empirical demonstrations showing comparable or superior performance to likelihood-based formulations in settings where only moment information is available.

## Limitations

- The framework's key assumptions around prompt engineering subjectivity remain empirically unverified
- The calibration procedure depends on having real data, which may not be available in many practical scenarios
- The choice of KL divergence as the optimization metric may not capture all relevant discrepancies between anticipated and generated data distributions

## Confidence

- **High confidence**: The mathematical derivation of the exponential-tilted foundation prior and its relationship to generalized Bayesian updates is rigorously established
- **Medium confidence**: The theoretical framework for modeling synthetic data as subjective priors is sound, but empirical validation of the subjectivity injection mechanism is limited
- **Low confidence**: Practical implementation details for prompt engineering iteration and calibration procedures lack specificity

## Next Checks

1. Implement the foundation prior derivation with a conjugate model and validate that varying λ appropriately controls synthetic data influence on posterior estimates
2. Conduct experiments comparing posteriors using individual vs. integrated foundation priors from multiple prompts to quantify robustness gains
3. Test the calibration procedure with synthetic real data to verify that the selected λ* appropriately balances synthetic and empirical information without overfitting