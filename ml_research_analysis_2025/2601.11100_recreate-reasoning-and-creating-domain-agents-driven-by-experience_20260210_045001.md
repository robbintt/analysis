---
ver: rpa2
title: 'ReCreate: Reasoning and Creating Domain Agents Driven by Experience'
arxiv_id: '2601.11100'
source_url: https://arxiv.org/abs/2601.11100
tags:
- agent
- arxiv
- scaffold
- recreate
- experience
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ReCreate introduces an experience-driven framework for automatic
  domain agent creation. Instead of relying solely on performance metrics, it leverages
  interaction histories to understand why agents succeed or fail, then uses this insight
  to iteratively improve scaffolds.
---

# ReCreate: Reasoning and Creating Domain Agents Driven by Experience

## Quick Facts
- **arXiv ID**: 2601.11100
- **Source URL**: https://arxiv.org/abs/2601.11100
- **Reference count**: 40
- **Primary result**: Introduces experience-driven framework for automatic domain agent creation that consistently outperforms human-designed agents and existing automated generation methods.

## Executive Summary
ReCreate is an experience-driven framework for automatically creating domain-specific agents. Instead of relying solely on performance metrics, it leverages interaction histories to understand why agents succeed or fail, then uses this insight to iteratively improve scaffolds. The agent-as-optimizer design includes three key components: on-demand experience retrieval, a reasoning–creation synergy pipeline that maps execution evidence into scaffold edits, and hierarchical updates that abstract instance-level refinements into reusable domain patterns. Evaluated across 13 benchmarks in software engineering, data science, mathematics, and digital assistance, ReCreate consistently outperforms human-designed agents and existing automated generation methods, even when starting from minimal seed scaffolds.

## Method Summary
ReCreate formulates agent creation as a bi-level optimization problem: an inner loop where the task agent acts, and an outer loop where a meta-agent optimizes the scaffold based on the inner loop's traces. The framework uses on-demand retrieval of execution histories to diagnose failures, then applies a reasoning-creation synergy to propose specific scaffold edits. These edits are aggregated hierarchically from instance-level to domain-level patterns to ensure generalization. The approach uses frontier models (gpt-5-mini for task execution, claude-4.5-opus for optimization) and operates in Docker sandboxes to maintain reproducibility.

## Key Results
- Outperforms human-designed agents and existing automated generation methods across 13 benchmarks
- Achieves cost reductions of 36%–82% compared to prior automated methods by converging faster
- Works effectively even when starting from minimal seed scaffolds
- Demonstrates consistent performance across software engineering, data science, mathematics, and digital assistance domains

## Why This Works (Mechanism)

### Mechanism 1: White-box Signal Extraction via Experience Retrieval
The framework stores task execution episodes in a "ReCreate-Environment" and retrieves specific evidence (e.g., error traces, file states) to identify the root cause of failure, rather than updating based solely on pass/fail scores.

### Mechanism 2: Reasoning-Creating Synergy
A "creation router" analyzes retrieved evidence to determine which of the four scaffold components requires modification, mapping observed error patterns directly to scaffold interventions.

### Mechanism 3: Hierarchical Generalization (Local-to-Domain)
The system buffers individual update proposals from single tasks and merges them through a synthesis step that filters out task-specific noise and retains only generalized patterns.

## Foundational Learning

- **Concept: Agent Scaffolds**
  - Why needed here: The paper explicitly decomposes agents into editable components (Role, Process, Tool, Memory) which is required to interpret how the "creation router" targets specific improvements.
  - Quick check question: Can you distinguish between a "Process" update (changing the reasoning flow) and a "Tool" update (wrapping a repetitive action in a script)?

- **Concept: Bi-level Optimization**
  - Why needed here: ReCreate formulates agent creation as a bi-level problem with an inner loop for task execution and an outer loop for scaffold optimization.
  - Quick check question: In this framework, does the task agent update its own weights, or does an external observer update the task agent's code/prompts?

- **Concept: On-demand Retrieval**
  - Why needed here: The framework relies on "on-demand inspection" rather than feeding the entire context window to the model, crucial for handling long-horizon tasks with massive logs.
  - Quick check question: Why is indexing "critical events" (errors, file ops) necessary before the reasoning agent inspects the environment?

## Architecture Onboarding

- **Component map**: Task Agent -> ReCreate-Environment -> Evidence Retriever -> ReCreate-Agent -> Creation Router -> Synthesis Agent -> Updated Scaffold

- **Critical path**: 1) Task Agent fails/succeeds on a batch; 2) Evidence Retriever indexes the logs; 3) ReCreate-Agent diagnoses the trace; 4) Creation Router proposes a scaffold diff; 5) Synthesis Agent aggregates batch diffs into a global scaffold update.

- **Design tradeoffs**: Uses a high-cost optimizer model (Claude-Opus) but claims to reduce overall cost (36-82%) by converging in fewer steps than metric-only search methods. Relies on the $DOMUPD$ synthesis step to prevent the scaffold from becoming a "bag of tricks" for the specific dev set.

- **Failure signatures**: Scaffold Drift (if $DOMUPD$ fails, the scaffold accumulates instance-specific hacks); Diagnosis Hallucination (the ReCreate-Agent misinterprets a log error, leading to "repairs" that break previously working functionality).

- **First 3 experiments**: 1) Run the Task Agent with a trivial scaffold on a domain to establish a baseline failure rate; 2) Disable the "ReCreate-Environment" to verify the performance drop when optimizing using only pass/fail metrics; 3) Run one full optimization loop on a batch of 4 tasks and manually inspect the scaffold.yaml diff to ensure updates are generalized rules.

## Open Questions the Paper Calls Out

### Open Question 1
Can the scaffold patterns discovered by ReCreate be internalized via fine-tuning rather than prompt engineering? The authors note that fine-tuning would internalize the discovered scaffold patterns which forms a promising direction but is currently computationally expensive.

### Open Question 2
Can the framework be extended to modify execution environments, not just textual scaffolds? The Limitations section notes that ReCreate does not extend to infrastructure adaptations such as customizing execution environments or systems.

### Open Question 3
Is the "Agent-as-optimizer" paradigm viable without frontier-scale models? Figure 5 shows that ReCreate fails to outperform human-designed baselines when using GPT-5-mini as the optimizer, succeeding only with Claude-4.5-opus.

## Limitations
- Currently relies on frontier-scale models (Claude-4.5-opus) for the optimizer agent, raising concerns about cost and accessibility
- Does not extend to infrastructure adaptations such as customizing execution environments or systems
- The framework assumes a fixed Docker setup and may struggle with environment-dependent tasks

## Confidence
High: Claims about cost reduction (36-82%) and performance improvement are supported by experimental results across 13 benchmarks.
Medium: The necessity of frontier models for the optimizer agent is demonstrated but the exact reasoning capabilities required are not fully characterized.
Low: The scalability of the hierarchical generalization mechanism to extremely large task distributions has not been thoroughly tested.

## Next Checks
1. Implement the scaffold representation and minimal seed scaffolds per domain as specified
2. Set up the ReCreate-Environment with proper indexing of critical events for on-demand retrieval
3. Verify the hierarchical DOMUPD mechanism by comparing per-instance proposals against the final aggregated scaffold updates to ensure generalization rather than overfitting