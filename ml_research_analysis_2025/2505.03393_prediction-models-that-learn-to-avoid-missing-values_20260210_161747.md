---
ver: rpa2
title: Prediction Models That Learn to Avoid Missing Values
arxiv_id: '2505.03393'
source_url: https://arxiv.org/abs/2505.03393
tags:
- missingness
- missing
- reliance
- values
- tree
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes missingness-avoiding (MA) machine learning,
  a framework for training models to minimize reliance on missing feature values during
  test-time predictions. The core method involves incorporating classifier-specific
  regularization terms into the learning objectives of decision trees, tree ensembles,
  and sparse linear models.
---

# Prediction Models That Learn to Avoid Missing Values

## Quick Facts
- arXiv ID: 2505.03393
- Source URL: https://arxiv.org/abs/2505.03393
- Reference count: 40
- Primary result: MA models (MA-DT, MA-LASSO, MA-RF, MA-GBT) reduce missingness reliance while maintaining competitive AUROC compared to unregularized baselines.

## Executive Summary
This paper introduces missingness-avoiding (MA) machine learning, a framework for training prediction models that minimize reliance on missing feature values during test-time predictions. The approach incorporates classifier-specific regularization terms into standard learning objectives for decision trees, tree ensembles, and sparse linear models. Through experiments on six real-world datasets, MA models demonstrate that they can effectively reduce missingness reliance (from 100% to 0% in some cases) while maintaining predictive performance comparable to unregularized baselines.

## Method Summary
The MA framework modifies standard model training objectives by adding regularization terms that penalize reliance on missing values. For tree-based models, this involves adjusting the split criterion to favor features with lower missingness rates. For sparse linear models like MA-LASSO, the L1 penalty is scaled per-feature based on its missingness rate. The approach works by exploiting redundant predictive information across features, allowing models to find alternative decision paths that avoid missing values when possible. The method requires knowledge of feature missingness patterns during training to effectively regularize the learning process.

## Key Results
- MA models achieved AUROC scores comparable to unregularized baselines in nearly all cases
- MA models significantly reduced missingness reliance in some instances from 100% to 0%
- The method effectively maintains interpretability in predictions when test-time missing values are present
- MA learning provides a practical tool for handling missing data without relying on imputation

## Why This Works (Mechanism)

### Mechanism 1: Regularized Objective Balancing Accuracy and Reliance
Training with a joint objective of minimizing prediction error and missingness reliance produces models that maintain accuracy while strategically avoiding features likely to be missing at test time. A regularization parameter α ≥ 0 is added to the standard loss function to penalize reliance on missing values, modifying the learning process to favor decision paths or features that rely on observed variables when redundant predictive information exists.

### Mechanism 2: Contextual Avoidance via Tree Splitting
Tree-based models can exploit "Observed Deterministic Data Collection" (ODDC) rules to contextually avoid missingness, using certain features only when they are guaranteed to be observed based on prior feature values. The decision tree learns to split on "gating" features first, and if a subsequent feature is guaranteed to be present given the values of the earlier features, the tree can use it without incurring reliance penalty.

### Mechanism 3: Feature-Specific Penalization for Sparsity
Applying a higher L1 penalty to coefficients of features with higher missingness rates selectively prunes them from linear models. In MA-LASSO, the L1 penalty for feature j is weighted by its empirical missingness rate m_j, forcing coefficients of high-missingness features to zero more aggressively than low-missingness features.

## Foundational Learning

### Missingness Mechanisms (MCAR, MAR, MNAR)
- Why needed: The effectiveness of MA learning depends critically on why data is missing
- Quick check: Is the probability of a feature being missing dependent on other observed features in the dataset?

### The Rashomon Set
- Why needed: This concept justifies the MA approach by positing that for many problems, there are many models with nearly identical accuracy
- Quick check: Can you drop a feature with high missingness from your current best model without a significant drop in AUROC?

### Regularization Trade-offs
- Why needed: The core of the method is trading off a small amount of predictive accuracy for a large reduction in reliance on missing values
- Quick check: What is an acceptable drop in AUROC to eliminate reliance on a feature that is missing 50% of the time at test-time?

## Architecture Onboarding

### Component map
MA-DT / MA-RF / MA-GBT: Modified tree-based learners with updated split_criterion function that adds penalty term based on missingness proportion at current node
MA-LASSO: Modified linear learner with updated loss_function where L1 penalty is scaled per-feature based on global missingness rate
Missingness Reliance Metric (ρ̂): Calculation module that traces decision paths or non-zero coefficients to determine if a missing feature was required for prediction

### Critical path
1. Analyze feature missingness rates in the training data
2. Select a model class (e.g., MA-DT for interpretability, MA-RF for performance)
3. Tune hyperparameter α using cross-validation, optimizing for trade-off between AUROC and ρ̂

### Design tradeoffs
MA-DT vs MA-LASSO: Trees exploit contextual missingness (ODDC rules) and handle non-linear relationships; linear models only globally de-weight features and assume linear additive effects
MA-RF vs MA-GBT: Random Forests fit trees independently; MA-GBT builds them sequentially where later trees can reuse features already "paid for" by earlier trees

### Failure signatures
Accuracy collapse with high α: Indicates critical features are frequently missing with no substitutes
No reduction in ρ̂ with low α: Indicates heavy reliance on missing-prone features, regularization too weak
High reliance despite MA training: Suggests missingness is MCAR or MNAR without exploitable structure

### First 3 experiments
1. Establish baseline: Train standard decision tree or logistic regression on zero-imputed data; measure AUROC and ρ̂
2. Parameter sweep: Train MA-DT with α ∈ {0.001, 0.01, 0.1, 1, 10}; plot Pareto frontier of AUROC vs. ρ̂
3. Stress test: Introduce synthetic missingness to 50% of values in key predictive feature; compare standard vs. MA model performance and reliance

## Open Questions the Paper Calls Out
1. Can MA framework be extended to other model classes beyond decision trees, tree ensembles, and sparse linear models?
2. How robust are MA models to distributional shifts between training and test-time missingness patterns?
3. What are optimal strategies for selecting the regularization parameter α and defining the missingness reliance metric ρ for different application contexts?
4. Can L0 regularization provide advantages over current L1-based approach for MA-LASSO in terms of sparsity-accuracy trade-offs?

## Limitations
- Method assumes redundant features exist for effective missingness avoidance
- Reliance metric (ρ̂) requires knowledge of missingness patterns during training
- Framework tested primarily on structured tabular data; performance on unstructured data unknown

## Confidence
High: MA framework's effectiveness is well-supported by empirical results across multiple datasets
Medium: Impact of missingness mechanisms on MA effectiveness needs more exhaustive testing
Medium: Generalizability to datasets with high feature redundancy or complex missingness patterns unclear

## Next Checks
1. Test MA models on datasets with varying missingness mechanisms (MCAR, MAR, MNAR) to quantify robustness
2. Evaluate computational overhead of MA training compared to standard baselines
3. Assess MA performance on high-dimensional or unstructured datasets to determine scalability