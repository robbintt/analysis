---
ver: rpa2
title: 'DETAIL Matters: Measuring the Impact of Prompt Specificity on Reasoning in
  Large Language Models'
arxiv_id: '2512.02246'
source_url: https://arxiv.org/abs/2512.02246
tags:
- prompt
- reasoning
- specificity
- prompts
- prompting
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: DETAIL introduces a framework for studying how prompt specificity
  impacts LLM reasoning performance. The method uses GPT-4 to generate detailed prompts,
  then progressively generalizes them to create multi-level variants, which are evaluated
  across models and prompting strategies.
---

# DETAIL Matters: Measuring the Impact of Prompt Specificity on Reasoning in Large Language Models

## Quick Facts
- arXiv ID: 2512.02246
- Source URL: https://arxiv.org/abs/2512.02246
- Authors: Olivia Kim
- Reference count: 7
- Primary result: Increased prompt specificity consistently improves LLM reasoning accuracy, with gains up to +0.47 for procedural tasks.

## Executive Summary
DETAIL introduces a systematic framework for studying how prompt specificity impacts LLM reasoning performance. The method uses GPT-4 to generate detailed prompts, then progressively generalizes them to create multi-level variants, which are evaluated across models and prompting strategies. Results show that increased specificity consistently improves accuracy—GPT-4 achieves 0.83 at high specificity versus 0.60 at low, and O3-mini improves from 0.34 to 0.81 with self-consistency prompting. The effect is strongest for procedural tasks like math and logic, with gains up to +0.47 in accuracy. This highlights that prompt specificity is a critical, task-dependent lever for optimizing LLM reasoning.

## Method Summary
The framework generates 30 reasoning tasks across 5 categories, creating three specificity levels (detailed, moderate, vague) for each. GPT-4 generates detailed prompts first, then iteratively generalizes them to create less specific variants. Specificity is validated using perplexity via GPT-2-medium. Two models (GPT-4, O3-mini) are evaluated across four prompting strategies—Baseline, Chain-of-Thought, Plan-and-Solve, and Self-Consistency—with accuracy measured via GPT-3.5 semantic equivalence scoring over three trials per configuration.

## Key Results
- GPT-4 accuracy improves from 0.60 (vague) to 0.83 (detailed) as specificity increases
- O3-mini shows dramatic sensitivity, improving from 0.34 to 0.81 with Self-Consistency prompting
- Procedural tasks (math, logic) show largest gains (+0.47, +0.36) while decision-making shows minimal sensitivity (+0.02)

## Why This Works (Mechanism)

### Mechanism 1: Specificity reduces interpretive ambiguity
Detailed prompts constrain the solution space, reducing the probability of misaligned reasoning paths. Specificity supplies procedural scaffolding that guides token generation toward correct outputs. This assumes models lack reliable internal mechanisms to infer missing constraints from underspecified prompts, especially smaller models.

### Mechanism 2: Structured prompting compensates for low specificity
Chain-of-Thought and Self-Consistency strategies partially compensate for vague prompts by introducing external reasoning scaffolds. CoT appends step-by-step instructions while Self-Consistency samples multiple paths and selects by majority vote. This assumes reasoning errors from under-specification are partially systematic and can be corrected through redundancy.

### Mechanism 3: Task-dependent specificity sensitivity
Procedural tasks (math, logic) benefit most from specificity because they require precise intermediate steps where missing constraints cause cascading errors. Inference-heavy tasks rely more on pretrained knowledge than prompt-provided structure. This assumes task categories can be classified by their reliance on external vs. internal knowledge.

## Foundational Learning

- **Perplexity as specificity proxy**: Used to validate that generalization produces progressively less specific prompts (L3: 45.7 → L1: 18.9). Quick check: If a prompt has perplexity 50 vs. 20, which is more specific and why?

- **Semantic equivalence evaluation**: GPT-3.5-based matching instead of exact string matching to score correctness. Quick check: Why might exact match undercount correct answers in free-form reasoning tasks?

- **Chain-of-Thought and Self-Consistency prompting**: Primary strategies tested for interacting with specificity levels. Quick check: How does Self-Consistency's majority voting differ from CoT's step-by-step approach?

## Architecture Onboarding

- **Component map**: Prompt Generalization Algorithm -> Specificity Measurement -> Evaluation Pipeline -> Dataset Generation
- **Critical path**: Detailed prompt creation → Generalization (L3→L2→L1) → Perplexity validation → Model inference across strategies → Semantic scoring → Accuracy aggregation by task/strategy/level
- **Design tradeoffs**: LLM-generated dataset avoids benchmark contamination but introduces potential inconsistencies; GPT-based semantic evaluation improves on exact match but relies on unvalidated black-box scoring; three specificity levels provide granularity but may miss non-monotonic effects
- **Failure signatures**: Perplexity not monotonically decreasing across levels (generalization failed); semantic evaluator inconsistent with human judgment; large variance across trials for Self-Consistency
- **First 3 experiments**: (1) Run generalization on 5 sample prompts, verify perplexity decreases L3→L2→L1; (2) Manually check 20 GPT-3.5 equivalence judgments against human labels; (3) Test 3 tasks across both models with baseline + CoT strategies at all specificity levels

## Open Questions the Paper Calls Out

- **Adaptive specificity systems**: Can systems dynamically adjust specificity in real-time based on detected task complexity and model confidence? Current work only tests static specificity levels.

- **Theory-grounded specificity measures**: What metrics beyond perplexity better predict LLM reasoning performance? Perplexity serves only as a proxy for linguistic predictability, not true informational specificity.

- **Generalization to new domains**: Do findings extend to multilingual prompts, open-source models, and high-stakes domains like medical or legal reasoning? Only two proprietary English-language models were tested on synthetic tasks.

- **Process-level evaluation**: Does evaluating intermediate reasoning steps reveal different specificity effects than final-answer semantic equivalence? GPT-based semantic equivalence only assesses output correctness, not reasoning process quality.

## Limitations
- LLM-generated datasets may introduce inconsistencies in difficulty and answer clarity
- GPT-3.5 semantic equivalence evaluation lacks validation against human judgment
- Plan-and-Solve prompting format remains underspecified in methodology
- Limited to two proprietary English-language models on synthetic tasks

## Confidence

- **High confidence**: Specificity gradient effect (L3 > L2 > L1 accuracy) across all tasks and models is consistently demonstrated
- **Medium confidence**: Interaction between specificity and prompting strategies shows directional trends but depends on unvalidated semantic evaluation
- **Low confidence**: Task-specific sensitivity claims rely on category definitions that may not cleanly separate procedural from inference-heavy tasks

## Next Checks
1. **Semantic evaluator validation**: Manually verify 20 GPT-3.5 semantic equivalence judgments against human expert ratings to establish scorer reliability
2. **Perplexity gradient verification**: Replicate the generalization algorithm on 10 sample prompts and confirm monotonic perplexity decrease
3. **Task category boundary test**: Select 5 ambiguous tasks that blend procedural and inference elements, test across all specificity levels, and assess whether sensitivity effects follow predicted category patterns