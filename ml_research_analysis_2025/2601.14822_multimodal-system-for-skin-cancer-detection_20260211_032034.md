---
ver: rpa2
title: Multimodal system for skin cancer detection
arxiv_id: '2601.14822'
source_url: https://arxiv.org/abs/2601.14822
tags:
- data
- images
- isic
- tabular
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study addresses the challenge of melanoma detection using
  photo images, which are more accessible than specialized dermoscopic images but
  suffer from lower quality and class imbalance. The authors propose a multimodal
  system that combines image data with patient metadata using a two-step model approach
  and a three-stage pipeline integrating neural networks and gradient boosting algorithms.
---

# Multimodal system for skin cancer detection

## Quick Facts
- arXiv ID: 2601.14822
- Source URL: https://arxiv.org/abs/2601.14822
- Reference count: 0
- Key outcome: Achieves Partial ROC AUC of 0.18068 (0.2 maximum) and top-15 retrieval sensitivity of 0.78371 on ISIC 2024 melanoma detection challenge

## Executive Summary
This study addresses the challenge of melanoma detection using conventional photo images, which are more accessible than specialized dermoscopic images but suffer from lower quality and severe class imbalance. The authors propose a multimodal system that combines image data with patient metadata using a three-stage pipeline integrating neural networks and gradient boosting algorithms. The system employs sophisticated feature engineering, including spatial and patient-specific aggregations, and addresses class imbalance through balanced sampling and advanced loss functions. Through ablation studies evaluating recent vision architectures and training strategies, the proposed approach achieves state-of-the-art performance on skin cancer detection while demonstrating robustness for real-world clinical applications.

## Method Summary
The system uses a three-stage pipeline: (1) multi-modal neural networks (ConvNeXt V2 Pico, EdgeNeXt Base, EfficientNetV2 B0) combined with tabular gradient boosting models (LightGBM, XGBoost), (2) a tabular model incorporating predictions from an image-only model, and (3) an ensemble of all model outputs optimized using Optuna. The multimodal networks undergo two-stage training - pretraining on dermoscopic Archive images plus synthetic images, then fine-tuning on photo images with metadata fusion. Feature engineering includes 193 patient-centric aggregated features comparing lesion characteristics within the same patient. The approach uses balanced sampling, focal loss, and extensive augmentations to handle the severe class imbalance (0.098% malignant).

## Key Results
- Partial ROC AUC of 0.18068 (0.2 maximum) achieved on ISIC 2024 melanoma detection challenge
- Top-15 retrieval sensitivity of 0.78371 demonstrates strong performance in identifying malignant lesions
- All multimodal models outperform image-only models by a significant margin, validating the benefit of metadata integration
- Three-stage ensemble approach outperforms both standalone and two-stage models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Integrating patient metadata with photo images improves detection accuracy over image-only models.
- Mechanism: A multi-modal neural network fuses a CNN-encoded image embedding with a feed-forward tabular embedding. The tabular branch (patient age, sex, lesion location) provides a lower-noise contextual signal that conditions the visual classifier, reducing ambiguity from lower-quality photo images.
- Core assumption: Metadata provides complementary information that corrects for visual noise and ambiguity.
- Evidence anchors:
  - [abstract] "Our system integrates image data with tabular metadata, such as patient demographics and lesion characteristics, to improve detection accuracy."
  - [PAGE 9] "...all multi-modal models outperform images only by a significant margin. This is evident because their input information is enhanced with tabular features, which are much less noisy than images."
  - [corpus] The paper "Towards Explainable Skin Cancer Classification... with Lesion Segmentation and Clinical Metadata Fusion" similarly supports metadata fusion for improved performance.
- Break condition: If metadata is sparse or unreliable in a new deployment, the tabular branch will introduce noise and degrade performance.

### Mechanism 2
- Claim: A three-stage pipeline of neural networks and gradient boosting models outperforms any single model.
- Mechanism: First, diverse models (multi-modal CNNs, LightGBM, XGBoost) are trained in parallel. Second, an image-only model's predictions are fed as a feature into a tabular model. Third, all model outputs are ensembled using coefficients optimized by Optuna. This staged ensemble allows different model architectures to learn uncorrelated representations and correct each other's errors.
- Core assumption: The errors of individual models are partially uncorrelated, allowing the ensemble to average them out for more robust predictions.
- Evidence anchors:
  - [abstract] "A three-stage pipeline further refines predictions by boosting algorithms and enhancing performance."
  - [PAGE 17] "Three-stage systems outperform both standalone and two-stage models... This improvement likely arises from the ability of other models in the system to compensate for and correct errors..."
  - [corpus] Corpus evidence for this specific three-stage pipeline is weak; related papers focus on single multi-modal models.
- Break condition: If all models learn the same systematic biases from the data, the ensemble will amplify the bias rather than correct it.

### Mechanism 3
- Claim: Patient-centric feature aggregation significantly boosts tabular model performance.
- Mechanism: The feature engineering pipeline computes "deviation within patients" and "extremes within patients" by scaling features relative to a patient's other lesions. This encodes the clinical "ugly duckling" sign, where a lesion differing from a patient's personal baseline is more suspicious.
- Core assumption: A patient's other lesions provide a personalized baseline for what is "normal," and deviations are diagnostically relevant.
- Evidence anchors:
  - [PAGE 8] "A critical aspect involves the comparison of lesion characteristics within the same patient or body region... To address this, aggregation features are introduced... motivated by [33]."
  - [PAGE 20] "Performance significantly improves by including 193 features that aggregate and compare lesion characteristics within the same patient or body region."
  - [corpus] Corpus evidence for this specific mechanism is weak in the provided neighbors.
- Break condition: This fails for patients with only a single lesion image, as no baseline can be established.

## Foundational Learning

- **Class Imbalance (Severe)**: Why needed here: The dataset has extreme imbalance (0.098% malignant). A standard loss function would bias heavily toward the benign class. Quick check question: Why would a standard cross-entropy loss fail with a 1:1000 class imbalance?

- **Domain Shift**: Why needed here: The system trains on a mix of high-quality dermoscopic images and lower-quality photo images, requiring strategies to handle this distribution difference. Quick check question: What is "domain shift," and why might a model trained on clinical dermoscopic images fail on smartphone photos?

- **Gradient Boosting Decision Trees (GBDTs)**: Why needed here: The pipeline relies on LightGBM and XGBoost for processing tabular metadata and ensembling model outputs. Quick check question: What type of data is LightGBM optimized for, and what is its role in the three-stage pipeline?

## Architecture Onboarding

- **Component map**: Data Sources (Main photo, ISIC Archive, Synthetic) -> Feature Engineering (Basic, Manual, Patient-centric) -> First-Stage Models (Multi-modal CNNs, Tabular GBDTs, Image-only CNN) -> Second-Stage Model (Tabular with image predictions) -> Third-Stage System (Optuna-ensemble) -> Validation (5-fold CV, public LB)

- **Critical path**:
  1. Data Prep: Correctly group data by patient ID for aggregation features
  2. Model Training: Train first-stage models. Note the multi-modal models require a 2-step training process (image-only pretraining, then multimodal fine-tuning)
  3. Ensemble: Generate out-of-fold predictions and use Optuna to find final ensemble weights

- **Design tradeoffs**:
  - Accuracy vs. Inference Time: The ensemble of 5-fold models is highly accurate but slow, a tradeoff made for clinical robustness
  - Data Purity vs. Volume: Noisier ISIC Archive data is used to improve generalization, trading data purity for volume
  - Bottleneck Design: The tabular branch of the multi-modal network has a "substantial bottleneck" to prevent overfitting

- **Failure signatures**:
  - Over-optimistic Validation: If validation scores are much better than public scores, check for data leakage or overfitting to the validation set's metadata distribution
  - Domain Shift Failure: If the image-only model trained on archive data isn't fine-tuned on the main photo data, its predictions will be poor
  - Single-Lesion Error: Patient aggregation features will be undefined for single-lesion patients

- **First 3 experiments**:
  1. Establish a baseline using an image-only model (e.g., EfficientNet B1) on the main dataset
  2. Run a feature ablation study: Train a LightGBM model with basic features, then add manual features, then add patient-aggregated features. Compare performance
  3. Test the ensemble benefit: Combine a simple image and tabular model using a simple average, then compare against an Optuna-optimized weighted average

## Open Questions the Paper Calls Out

- **Generalization to external datasets**: The authors explicitly state the need to "conduct additional benchmarks on new skin cancer datasets that include metadata to evaluate system generalizability." This remains unresolved due to the scarcity of datasets combining image and metadata.

- **Multi-lesion aggregation models**: The authors list a future direction to "develop hybrid models capable of aggregating information across multiple nearby lesions to improve classification accuracy." The current system relies on single-lesion crops with feature engineering rather than joint multi-lesion processing.

- **Domain shift mitigation**: The authors propose to "address the domain shift problem between dermoscopic and photo images to bridge the gap between clinical and real-world applications." The integration of ISIC Archive (dermoscopic) and Main Data (photo) introduced significant domain shifts which the current system handled via simple fusion.

- **Validation overfitting**: The text notes that validation metrics for the second stage "may be unreliable" and that "for the final Optuna optimization stage, overfitting to the training (validation) set is a notable risk." The ensemble weights are optimized directly on validation folds without a nested hold-out set.

## Limitations

- **Domain shift between image modalities**: The significant difference between high-quality dermoscopic Archive images and lower-quality photo images represents a potential weakness that affects real-world generalization.

- **Reliance on metadata availability**: The system's heavy dependence on patient metadata assumes consistent availability of clinical information, which may not hold in all healthcare settings.

- **Single-lesion blind spot**: The patient-centric aggregation features fail for patients with single lesions, creating a systematic blind spot in the detection system.

## Confidence

- High confidence in the overall framework design and three-stage pipeline approach
- Medium confidence in the specific architecture choices (ConvNeXt, EdgeNeXt, and training hyperparameters)
- Medium confidence in the feature engineering methodology, particularly patient-centric aggregations
- Low confidence in exact reproduction due to missing hyperparameters (learning rates, weight decay, Optuna search spaces)

## Next Checks

1. **Domain Shift Validation**: Test model performance separately on Archive-derived predictions vs Main dataset predictions to quantify the domain adaptation gap and determine if the two-stage training sufficiently bridges the distribution difference.

2. **Metadata Sensitivity Analysis**: Train an image-only baseline on Main dataset only, then compare performance against the full multimodal system to isolate the contribution of tabular metadata and assess robustness when clinical information is incomplete.

3. **Single-Lesion Patient Performance**: Create a separate evaluation subset containing only patients with single lesions to measure how significantly the patient-centric aggregation features impact overall performance and identify the magnitude of this systematic blind spot.