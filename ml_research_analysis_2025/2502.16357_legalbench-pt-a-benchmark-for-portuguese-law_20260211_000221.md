---
ver: rpa2
title: 'LegalBench.PT: A Benchmark for Portuguese Law'
arxiv_id: '2502.16357'
source_url: https://arxiv.org/abs/2502.16357
tags:
- questions
- legal
- each
- question
- table
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces LegalBench.PT, the first legal benchmark
  specifically designed for the Portuguese legal system. The authors created the benchmark
  by collecting long-form questions and answers from real law exams at a leading Portuguese
  law school, then using GPT-4o to convert them into multiple-choice, true/false,
  and matching formats.
---

# LegalBench.PT: A Benchmark for Portuguese Law

## Quick Facts
- arXiv ID: 2502.16357
- Source URL: https://arxiv.org/abs/2502.16357
- Reference count: 22
- Key outcome: First Portuguese legal benchmark with 4,723 synthetically generated questions across 31 legal areas, revealing GPT-4o and Claude-3.5-Sonnet as top performers

## Executive Summary
LegalBench.PT introduces the first comprehensive benchmark for evaluating large language models on Portuguese legal knowledge. The authors collected 341 law exam PDFs from the Faculty of Law, University of Lisbon (2021-2024) and used GPT-4o to convert long-form exam questions into multiple-choice, true/false, and matching formats. After rigorous filtering to remove duplicates and article references, the benchmark contains 4,723 questions across 31 legal domains. Performance evaluation shows GPT-4o and Claude-3.5-Sonnet achieving the highest scores, closely followed by Claude-3-Opus and Llama-3.1-405B. Human lawyers performed closer to lower-tier models, with approximately 12% of gold answers identified as incorrect.

## Method Summary
The authors extracted questions from law school exams and used GPT-4o to generate synthetic multiple-choice, true/false, and matching questions. They applied two-stage filtering: first removing questions referencing specific legal articles using regex patterns, then deduplicating via ROUGE-L thresholds (0.70-0.85) and sentence transformer embeddings (0.80 cosine similarity). Answer options were shuffled to eliminate positional biases. The benchmark was evaluated using zero-shot prompting with temperature 0.01, employing balanced accuracy for single-answer formats and F1 score for multi-answer formats.

## Key Results
- GPT-4o and Claude-3.5-Sonnet achieved highest scores, followed closely by Claude-3-Opus and Llama-3.1-405B
- Portuguese lawyers performed closer to lower-tier models (Llama-3.1-8B, Mixtral-8x7B) than top models
- Approximately 12% of gold answers were identified as incorrect through human validation
- 14 legal areas have 200+ questions while 4 areas have fewer than 20 questions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Converting long-form law exam questions into structured formats via LLM produces valid benchmark items for legal knowledge assessment.
- Mechanism: GPT-4o receives exam question-answer pairs and generates multiple-choice, true/false, and matching variants while preserving legal reasoning requirements.
- Core assumption: The LLM's generation preserves the legal reasoning complexity from original exam material.
- Evidence anchors: Human review of 33 questions found only 4 with incorrect answers; LEXam benchmark similarly derives from law school exams.

### Mechanism 2
- Claim: Shuffling answer option positions eliminates systematic position-based biases in model evaluation.
- Mechanism: After detecting GPT-4o's tendency to generate questions where the second option was correct, the pipeline randomizes option order while preserving logic for special cases.
- Core assumption: Models don't have inherent positional biases beyond what shuffling addresses.
- Evidence anchors: Cross-model validation (GPT-4o vs Claude-3.5-Sonnet) showed no statistically significant performance differences (Wilcoxon p-values 100% and 15.6%).

### Mechanism 3
- Claim: Hybrid lexical-semantic filtering removes redundant and problematic questions while maintaining legal diversity.
- Mechanism: Two-stage filtering removes article references via regex and detects duplicates using ROUGE-L thresholds plus sentence transformer embeddings.
- Core assumption: Semantic similarity captures meaningful redundancy beyond lexical overlap.
- Evidence anchors: Filtering reduced questions from 10,951 to 4,723; similar approaches used in VLegal-Bench and MizanQA.

## Foundational Learning

- Concept: **Balanced accuracy vs. F1 score for multi-format evaluation**
  - Why needed here: Different question types require different metrics; multiple-choice uses balanced accuracy to handle class imbalance, while matching/multi-select use F1 for partial credit
  - Quick check question: Why would accuracy be misleading for a true/false benchmark with 90% "True" answers?

- Concept: **Synthetic data validation via human performance baselines**
  - Why needed here: Establishing human lawyer performance provides ground truth for benchmark difficulty; lawyers scored closer to lower-performing models
  - Quick check question: What does it suggest when humans perform worse than top models on your benchmark?

- Concept: **Agreement analysis for gold standard validation**
  - Why needed here: Inter-annotator agreement (62.4% between lawyers) reveals ambiguous questions; high accuracy on agreed answers (88.9%) confirms gold standards are mostly correct
  - Quick check question: If two experts disagree on an answer, does that mean the question is flawed or that legal reasoning inherently permits multiple valid interpretations?

## Architecture Onboarding

- Component map:
  Input layer: Raw exam PDFs → PyMuPDF extraction → manual question segmentation
  Generation layer: Three prompting strategies mapped to question complexity
  Filtering layer: Regex article removal → ROUGE-L lexical filtering → sentence-embedding semantic deduplication
  Post-processing layer: Option shuffling with special-case handling for "all/none of the above"
  Evaluation layer: Type-specific metrics (balanced accuracy for single-answer, F1 for multi-answer)

- Critical path:
  1. Exam collection and taxonomy mapping (31 legal areas)
  2. Question generation with appropriate approach selection
  3. Duplicate/quality filtering (54% reduction)
  4. Bias mitigation via shuffling
  5. Human validation sampling (~12% error rate baseline)

- Design tradeoffs:
  - Coverage vs. depth: 14 areas have 200+ questions, 4 have <20 (insufficient for comprehensive evaluation)
  - Automation vs. quality: Fully synthetic generation trades expert annotation cost for ~12% noise rate
  - Format diversity vs. evaluation simplicity: 6 question types complicate unified scoring but better reflect exam complexity

- Failure signatures:
  - Low inter-rater agreement (<60%) suggests ambiguous question formulation
  - Model performance ceiling effects (>95% accuracy) indicate insufficient difficulty
  - Area-specific score drops may reflect sparse training data for niche legal domains

- First 3 experiments:
  1. Cross-validate generation bias: Regenerate a 500-question subset with a different model (Claude-3.5-Sonnet or Llama-3.1-405B) and compare performance distributions
  2. Human baseline stratification: Recruit lawyers by experience level (junior vs. senior) on 200 questions to establish difficulty calibration
  3. Error taxonomy analysis: Manually classify the 12% incorrect gold answers by error type (factual error, ambiguity, outdated law)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Would expanding the underrepresented legal areas (11 of 31 fields with <60 questions, 4 with <20) significantly change model ranking or expose different capability gaps?
- Basis in paper: Conclusion states "Future work may focus on expanding underrepresented legal areas"
- Why unresolved: Current coverage is uneven; small subsets may not adequately represent difficulty or content diversity of those fields
- What evidence would resolve it: Create larger question sets for the 11 underrepresented fields, re-evaluate models, and compare rankings and per-area gaps

### Open Question 2
- Question: How can benchmark design better capture legal reasoning and argumentation skills that multiple-choice formats fail to assess?
- Basis in paper: Limitations section: "the exams... also assess reasoning and argumentation skills, which cannot be fully captured by multiple-choice and similar question formats"
- What evidence would resolve it: Develop new tasks (e.g., contract analysis, case summarization) and show they correlate with expert evaluations of reasoning quality better than current formats

### Open Question 3
- Question: Is LegalBench.PT sufficiently challenging to differentiate among top-tier models, or are similar scores due to ceiling effects?
- Basis in paper: Section 5.1: "This suggests either similar capabilities or that LegalBench.PT is not challenging enough to differentiate these models"
- Why unresolved: GPT-4o, Claude-3.5-Sonnet, Claude-3-Opus, and Llama-3.1-405B cluster within 1.6% overall
- What evidence would resolve it: Evaluate models on a curated harder subset (e.g., questions where humans disagree) and analyze score separation

### Open Question 4
- Question: What scalable methods can reliably identify and filter ambiguous, easy, or incorrect questions given that automated GPT-4o filtering for "easy" questions failed?
- Basis in paper: Section 4.5: "We considered these questions too straightforward and tried to filter them out with GPT-4o, with no success"
- Why unresolved: Human review is costly; current heuristics don't address ambiguity or difficulty; ~12–15% of questions are problematic
- What evidence would resolve it: Develop and validate an automated difficulty/ambiguity classifier against human annotations on a held-out subset

## Limitations

- 12% estimated error rate in gold answers directly impacts benchmark reliability and cross-model comparisons
- Concentration of questions in 14 legal areas (200+ questions) versus 4 areas with fewer than 20 questions creates potential reliability issues
- Use of GPT-4o for question generation raises concerns about potential systematic biases in question formulation

## Confidence

- **High Confidence**: Benchmark creation methodology and filtering pipeline are well-documented and follow established practices
- **Medium Confidence**: Model performance rankings appear stable, but the 12% error rate and potential generation biases limit absolute interpretation
- **Medium Confidence**: The finding that Portuguese lawyers perform closer to lower-tier models validates benchmark difficulty but doesn't establish whether this reflects true legal reasoning capability or benchmark limitations

## Next Checks

1. Conduct a systematic error analysis of the 12% incorrect gold answers to determine whether specific legal areas or question types are disproportionately affected, then implement targeted corrections
2. Re-run the entire benchmark generation process using Claude-3.5-Sonnet (or another leading model) as the primary generator to assess whether performance rankings remain consistent across generation approaches
3. Expand the human evaluation to include stratified sampling by lawyer experience level (junior, senior, specialist) to better calibrate benchmark difficulty and identify questions where expertise significantly impacts performance