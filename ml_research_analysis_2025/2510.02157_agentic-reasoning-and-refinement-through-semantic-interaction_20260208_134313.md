---
ver: rpa2
title: Agentic Reasoning and Refinement through Semantic Interaction
arxiv_id: '2510.02157'
source_url: https://arxiv.org/abs/2510.02157
tags:
- semantic
- refinement
- interactions
- report
- vis-react
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: VIS-ReAct is a framework for targeted sensemaking report refinement
  through semantic interactions in visual workspaces. It uses a two-agent system where
  an LLM analysis agent interprets user interactions to infer intent and generate
  refinement plans, followed by an LLM refinement agent that updates reports accordingly.
---

# Agentic Reasoning and Refinement through Semantic Interaction

## Quick Facts
- arXiv ID: 2510.02157
- Source URL: https://arxiv.org/abs/2510.02157
- Reference count: 22
- Key outcome: VIS-ReAct achieves F1=0.887 for targeted refinement and F1=0.614 for semantic fidelity in intelligence analysis report refinement

## Executive Summary
VIS-ReAct is a framework for targeted sensemaking report refinement through semantic interactions in visual workspaces. It employs a two-agent system where an LLM analysis agent interprets user interactions to infer intent and generate refinement plans, followed by an LLM refinement agent that updates reports accordingly. The framework outperforms baseline approaches on three key principles: targeted refinement (F1-score 0.887), semantic fidelity (F1-score 0.614), and transparent inference. Through case studies using intelligence analysis data, VIS-ReAct successfully handles various interaction types and granularities while providing clear explanations of the refinement process, enhancing human-AI collaboration transparency.

## Method Summary
VIS-ReAct uses a four-step pipeline: workspace-to-text conversion via ReSPIRE, semantic interaction extraction through programmatic workspace comparison, LLM analysis agent inference of user intent and refinement planning, and LLM refinement agent constrained report updating. The system processes intelligence analysis reports by interpreting semantic interactions (highlights, notes, cluster reorganizations) as meaningful signals for report modification. Using gpt-4o-mini, the framework generates targeted refinements that preserve report structure while incorporating user-specified changes, evaluated across 13 semantic interaction combinations on 35 workspace pairs from the "Sign of Crescent" dataset.

## Key Results
- Targeted refinement F1-score of 0.887 demonstrates superior paragraph-level precision and recall compared to baseline approaches
- Semantic fidelity F1-score of 0.614 shows effective preservation of interaction context in refined reports
- Transparent inference through explicit analysis agent outputs provides clear explanations of refinement decisions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Explicit intent inference via a dedicated analysis agent enables contextually appropriate refinement that raw interaction data alone cannot achieve.
- Mechanism: The LLM analysis agent receives three inputs—previous report Rn−1, semantic interaction data SIn, and current workspace state Wn—and outputs (a) human intent inference articulating how interactions are interpreted, and (b) refinement planning specifying targeted edits. This intermediate representation bridges observed actions and execution.
- Core assumption: User intent can be reliably reconstructed from workspace-level changes (cluster reorganization, highlights, notes) without direct natural language instruction.
- Evidence anchors: [abstract] "a primary LLM analysis agent interprets new semantic interactions to infer user intentions and generate refinement planning" [section 3] "this module processes comprehensive contextual information, including the previously generated report Rn−1, semantic interaction data SIn, and current workspace state Wn"
- Break condition: When interactions are ambiguous or sparse (e.g., single highlight with no surrounding context), intent inference may produce plausible but incorrect plans.

### Mechanism 2
- Claim: Programmatic extraction of semantic interactions via workspace comparison yields more reliable change detection than asking an LLM to diff workspaces directly.
- Mechanism: The system computes SIn by comparing workspace states Wn−1 and Wn programmatically, capturing cluster creation/deletion/reorganization plus highlight/note modifications. This avoids LLM instability in difference detection.
- Core assumption: Workspace state can be fully serialized into a comparable text format without losing interaction semantics.
- Evidence anchors: [abstract] "interprets new semantic interactions to infer user intentions" [section 3] "we compare previous and current workspaces to extract semantic interactions that meaningfully alter the workspace content" [section 5] "LLMs struggle to reliably identify differences compared to programmatic solutions"
- Break condition: If workspace serialization is incomplete or delayed (e.g., unsaved state), extracted SIn will miss user actions.

### Mechanism 3
- Claim: Constraining the refinement agent to avoid rephrasing and respect section boundaries yields higher targeted refinement scores (F1=0.887) than unconstrained regeneration.
- Mechanism: The refinement prompt enforces (a) no rephrasing of existing text, (b) scope limited to sections affected by SIn, and (c) adherence to report format. Combined with the analysis agent's plan, this produces paragraph-level rather than document-level changes.
- Core assumption: The BLUF report structure (summary → cluster paragraphs → conclusion) remains stable across refinements.
- Evidence anchors: [abstract] "outperforms baseline approaches on three key principles: targeted refinement (F1-score 0.887)" [section 3] "The prompt constrains the LLM to avoid rephrasing, restricts the scope of modifications, and enforces adherence to the specified report format."
- Break condition: When multiple rounds of incremental refinement accumulate, the report becomes "messy with obvious text additions" per the authors' limitation note.

## Foundational Learning

- Concept: **Semantic Interaction** (from visual analytics)
  - Why needed here: The entire framework depends on interpreting user actions in a visual workspace—cluster moves, highlights, annotations—as meaningful signals rather than UI noise.
  - Quick check question: Can you distinguish between a user moving a document between clusters (semantic) vs. zooming/panning (non-semantic) in a visual workspace?

- Concept: **ReAct Pattern** (Reasoning + Acting)
  - Why needed here: VIS-ReAct extends the ReAct agentic framework, separating reasoning (analysis agent) from execution (refinement agent).
  - Quick check question: In a standard ReAct loop, what is the role of the "thought" step before the "action" step?

- Concept: **Incremental Formalism**
  - Why needed here: The paper explicitly positions its work as addressing incremental formalism—allowing users to refine reports step-by-step through interaction rather than formal specification.
  - Quick check question: Why might users prefer iterative refinement through workspace edits over writing explicit prompts for each change?

## Architecture Onboarding

- Component map: Workspace Serializer -> Interaction Extractor -> Analysis Agent -> Refinement Agent
- Critical path: Workspace change → serialization → interaction extraction → analysis agent (intent + plan) → refinement agent (constrained edit) → updated report. The analysis agent is the bottleneck; without its output, the refinement agent lacks targeting.
- Design tradeoffs:
  - Precision vs. Recall: VIS-Act (no analysis) has higher precision (0.975) but lower recall (0.652); VIS-ReAct trades some precision (0.951) for better recall (0.831).
  - Speed vs. Quality: Two-agent pipeline takes ~20 seconds for 10-document workspace; single-pass generation is faster but less targeted.
  - Transparency vs. Complexity: Analysis agent adds explainability but introduces another potential failure point.
- Failure signatures:
  - Abrupt additions: If analysis agent lacks context, refined text appears disconnected (VIS-Act pattern).
  - Cumulative messiness: Multi-round refinement degrades report coherence.
  - Misaligned intent: LLM-inferred intent may not match actual user intent—unverified assumption.
- First 3 experiments:
  1. Ablation on analysis depth: Run VIS-ReAct with analysis agent outputting only intent vs. only plan vs. both. Measure impact on F1 scores for P1 and P2.
  2. Interaction type stress test: Systematically test single-highlight, multi-highlight, cluster-reorg-only, and combined interactions. Identify which types most frequently break intent inference.
  3. Multi-round degradation curve: Measure report quality (readability, coherence) after 1, 3, 5, 10 sequential refinements to quantify the "messy accumulation" limitation.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the intent inferred by the LLM analysis agent align with the user's actual intent during the sensemaking process?
- Basis in paper: [explicit] The authors state in the Limitations section that "if the LLM inferred intent aligns with human real intent is unknown."
- Why unresolved: The current evaluation relies on measuring the quality of the final output (refined report) rather than validating the accuracy of the intermediate reasoning step where the agent infers user intent.
- What evidence would resolve it: A user study comparing the LLM's "human intent inference" output against users' self-reported intentions for specific semantic interactions.

### Open Question 2
- Question: How can systems balance global report coherence with incremental refinement to prevent the accumulation of "messy" text over iterative cycles?
- Basis in paper: [explicit] The authors note that in iterative refinement, "the final refined report is messy with obvious text additions, while the one-time generated report is more concise and readable."
- Why unresolved: The current framework prioritizes targeted, local updates based on semantic interactions, which appears to degrade the overall flow and conciseness of the document over time.
- What evidence would resolve it: A longitudinal study measuring readability and coherence scores of reports after multiple refinement rounds compared to single-generation reports.

### Open Question 3
- Question: Can the VIS-ReAct framework effectively transform and incorporate a wider variety of semantic interaction types beyond the highlights, notes, and cluster reorganizations currently tested?
- Basis in paper: [explicit] The authors state that "Since we only tested several semantic interactions in this study, transforming more interaction types will require further experimentation across diverse contexts."
- Why unresolved: The current case study was restricted to 13 combinations of specific interaction types, leaving the system's robustness to other interaction modalities unverified.
- What evidence would resolve it: Quantitative evaluation (F1-scores for targeted refinement and semantic fidelity) using datasets involving diverse interaction types such as spatial proximity adjustments or complex linking.

### Open Question 4
- Question: Can retrieval-augmented generation (RAG) be effectively integrated to reduce the latency of report refinement for larger workspaces?
- Basis in paper: [explicit] The authors identify that "generation speed is another issue" (approx. 20 seconds for 10 documents) and suggest they "may introduce more efficient methods like RAG in the future."
- Why unresolved: The current implementation processes the workspace and context in a way that creates noticeable delays, and it is unknown if RAG can maintain the necessary semantic fidelity while improving speed.
- What evidence would resolve it: System performance benchmarks comparing the generation time and accuracy of the current method against a RAG-enhanced version across varying workspace sizes.

## Limitations

- **Intent Inference Reliability**: The paper assumes LLM-based intent inference from workspace interactions is accurate, but this is unverified against ground-truth user intent.
- **Context Dependency**: The system's reliance on the analysis agent for contextual interpretation creates a single point of failure, particularly problematic for isolated interactions.
- **Generalizability Constraints**: Evaluation uses a single intelligence analysis dataset with specific report formats, leaving performance on other domains unknown.

## Confidence

**High Confidence**: The architectural separation of analysis and refinement agents is technically sound and addresses the targeted refinement problem. The precision/recall metrics for single-round refinements are empirically measured and reproducible given the same dataset and evaluation methodology.

**Medium Confidence**: The semantic interaction extraction via programmatic workspace comparison is a reasonable approach given LLM limitations in difference detection. However, the completeness of workspace serialization and its ability to capture all meaningful interaction semantics requires further validation.

**Low Confidence**: Claims about multi-round refinement quality degradation and the system's ability to handle increasingly complex interaction patterns over time are based on limited evidence. The assumption that LLM-inferred intent reliably matches actual user intent is critical but untested.

## Next Checks

1. **Intent Inference Ground Truth Study**: Collect user study data where participants perform semantic interactions while explicitly stating their intended refinement goals. Compare LLM-inferred intent against these stated goals across different interaction types and complexities. This validates whether the analysis agent's outputs align with actual user intent.

2. **Cross-Domain Performance Evaluation**: Apply VIS-ReAct to at least two different report types (e.g., medical imaging reports and scientific literature reviews) with varying structures and interaction patterns. Measure whether the targeted refinement and semantic fidelity scores remain competitive or if domain-specific tuning is required.

3. **Multi-Round Refinement Quality Tracking**: Implement automated readability and coherence metrics (e.g., text cohesion scores, grammaticality checks) to quantify the "messy accumulation" effect over 10+ refinement rounds. Track how report quality degrades with each iteration and identify thresholds where the system becomes counterproductive.