---
ver: rpa2
title: 'Vid-Freeze: Protecting Images from Malicious Image-to-Video Generation via
  Temporal Freezing'
arxiv_id: '2509.23279'
source_url: https://arxiv.org/abs/2509.23279
tags:
- video
- attack
- motion
- image
- generation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the problem of protecting static images from
  misuse in diffusion-based image-to-video (I2V) generation models, which can synthesize
  realistic videos from still images and pose privacy, security, and intellectual
  property risks. The authors propose Vid-Freeze, a novel attention-suppressing adversarial
  attack that adds carefully crafted perturbations to images to completely block motion
  synthesis while preserving the semantic fidelity of the input image.
---

# Vid-Freeze: Protecting Images from Malicious Image-to-Video Generation via Temporal Freezing

## Quick Facts
- arXiv ID: 2509.23279
- Source URL: https://arxiv.org/abs/2509.23279
- Reference count: 0
- Primary result: Novel adversarial attack that blocks I2V motion synthesis by suppressing attention weights while preserving image fidelity

## Executive Summary
This paper introduces Vid-Freeze, a novel adversarial defense that protects static images from being misused in diffusion-based image-to-video generation models. The method adds carefully crafted perturbations that suppress attention weights in the transformer layers of I2V models, causing generated videos to remain frozen or nearly static. By targeting the attention mechanism, Vid-Freeze effectively blocks motion synthesis while maintaining the semantic integrity of the original image, addressing critical privacy, security, and intellectual property concerns in AI-generated video content.

## Method Summary
Vid-Freeze employs a projected gradient descent (PGD) optimization approach to add adversarial perturbations to input images. The core mechanism targets attention weights in the transformer layers by minimizing their Frobenius norm, which disrupts the model's ability to form spatio-temporal dependencies necessary for motion synthesis. The method can operate in different modes: attacking only cross-attention weights or all attention weights (full-attention), with full-attention providing stronger protection. The optimization can be conditioned on image captions to improve effectiveness, and the perturbation budget is constrained by an L∞ norm to maintain imperceptibility.

## Key Results
- Achieves near-zero motion synthesis with Flow Magnitude of 0.0497 and temporal SSIM of 0.0056
- Outperforms prior defenses like I2VGuard in blocking motion synthesis
- Full-attention targeting provides stronger protection than cross-attention only (Flow Magnitude 0.0497 vs 0.1326)
- Caption conditioning improves motion suppression by ~17% while maintaining image fidelity

## Why This Works (Mechanism)

### Mechanism 1: Attention Weight Suppression
- Claim: Minimizing attention weight norms in transformer layers disrupts spatio-temporal reasoning, forcing the model to generate static frames.
- Mechanism: The attack minimizes the Frobenius norm of attention weights A^(l) = softmax(QK^T/√d_k) across all layers. When attention weights approach zero, the model cannot form meaningful dependencies between tokens across spatial locations and temporal frames, collapsing the output to replicate the input image.
- Core assumption: Motion synthesis in I2V models requires non-trivial attention weight magnitudes to aggregate information across time.
- Evidence anchors:
  - [abstract] "By minimizing attention weight norms, the method collapses generated videos to static frames, effectively blocking motion synthesis."
  - [Section 3.2] "By driving the attention weight magnitudes toward zero, the model's ability to form meaningful dependencies between tokens is degraded, leading to incoherent or motionless outputs."
  - [corpus] No direct corpus validation; related papers focus on I2V generation rather than adversarial protection.

### Mechanism 2: Full-Attention Targeting vs Cross-Attention
- Claim: Targeting all attention weights (full-attn) provides stronger motion suppression than targeting only text-to-video token connections (cross-attn).
- Mechanism: Full-attention attack disrupts both semantic conditioning and motion-related feature interactions. Cross-attn leaves residual motion because spatial-temporal dependencies can still form through video-to-video attention pathways.
- Core assumption: Motion features are encoded in self-attention among video tokens, not just cross-attention with text.
- Evidence anchors:
  - [Section 5.3, Table 3] Full-attn achieves ΔSSIM=0.0056, Flow Mag.=0.0497 vs Cross-Attn ΔSSIM=0.0166, Flow Mag.=0.1326.
  - [Section 5.3] "Full-attn disrupts motion-related features and achieves stronger temporal freezing, whereas cross-attn leaves traces of residual motion."
  - [corpus] No corpus papers validate this specific attention-targeting distinction.

### Mechanism 3: Caption-Conditioned Optimization
- Claim: Incorporating image captions during perturbation optimization improves motion suppression by ~17%.
- Mechanism: The caption provides semantic context that guides attention during I2V generation. Conditioning the attack on this context allows perturbations to specifically suppress motion cues aligned with caption semantics (e.g., "runs energetically").
- Core assumption: I2V models use caption semantics to determine what motion to synthesize, and attacking this coupling is more effective than unconditional attacks.
- Evidence anchors:
  - [Section 5.3, Table 2] Image caption reduces ΔSSIM from 0.0068 to 0.0056 and Flow Mag. from 0.0601 to 0.0497 (~17% improvement).
  - [Section 5.3] "Caption conditioning reduces ΔSSIM and flow magnitude by ~17% while leaving Aesthetic score and Subject Consistency unchanged."
  - [corpus] Corpus papers on controllable I2V (e.g., VidCRAFT3) support the role of semantic conditioning in motion generation.

## Foundational Learning

- Concept: **Attention Mechanisms in Transformers**
  - Why needed here: Understanding Q, K, V projections and how attention weights aggregate information is essential to grasp why suppressing these weights halts motion synthesis.
  - Quick check question: Can you explain why minimizing ||A||_F where A = softmax(QK^T/√d_k) would prevent information flow across tokens?

- Concept: **Projected Gradient Descent (PGD) for Adversarial Attacks**
  - Why needed here: The method uses PGD to optimize perturbations δ under an L∞ budget (ϵ=16 pixels). Understanding constrained optimization is critical.
  - Quick check question: How does PGD differ from standard gradient descent when optimizing under an L∞ constraint?

- Concept: **3D VAE and Spatio-Temporal Latent Representations**
  - Why needed here: CogVideoX uses a 3D Causal VAE that compresses video along spatial and temporal dimensions. The encoder attack targets this representation.
  - Quick check question: Why would minimizing ||E(V_in + δ)|| (encoder output norm) be less effective than attention suppression for blocking motion?

## Architecture Onboarding

- Component map: Input image X → Replicated to video sequence V_in → 3D Causal VAE Encoder E(·) → Latent z → Diffusion Transformer (DiT) with attention layers → Denoised latents → VAE Decoder → Video frames
- Critical path: The attention suppression attack (L_attn) is the core mechanism. Perturbation δ is optimized via PGD to minimize (1/L) Σ ||A^(l)||_F across layers, backpropagating through the DiT to the input image.
- Design tradeoffs:
  - Pixel budget ϵ: Lower budgets (e.g., 2 pixels) are more imperceptible but may not fully freeze motion; ϵ=16 achieves reliable freezing.
  - Cross-attn vs full-attn: Full-attn is more effective but requires computing gradients through all attention pathways (higher memory).
  - Caption conditioning: Requires generating/obtaining image captions; adds ~17% effectiveness gain.
- Failure signatures:
  - Encoder attack alone: Minor textural changes but motion persists (Flow Mag.=0.9479).
  - Diffusion attack alone: Ineffective at blocking motion (Flow Mag.=0.881).
  - Hidden-state attack: Partially distorts fidelity but still preserves malicious prompt content.
  - Cross-attn only: Leaves residual motion (Flow Mag.=0.1326 vs 0.0497 for full-attn).
- First 3 experiments:
  1. **Baseline comparison**: Implement all four attack types (L_enc, L_attn, L_hs, L_diff) on CogVideoX-2B with ϵ=16, measuring Flow Magnitude, ΔSSIM, and LPIPS. Verify that L_attn achieves near-zero flow.
  2. **Ablation on attention targeting**: Compare cross-attn vs full-attn suppression on the same image set. Confirm full-attn achieves lower motion metrics.
  3. **Budget sensitivity test**: Run Vid-Freeze with ϵ ∈ {2, 4, 8, 16, 32} and plot Flow Magnitude vs ϵ. Verify effectiveness persists down to ϵ=4-8.

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions but acknowledges several limitations including the need for broader evaluation across different I2V architectures, larger and more diverse datasets, and investigation of the method's robustness to common image transformations.

## Limitations
- Evaluation confined to single I2V model architecture (CogVideoX-2B), raising questions about generalizability
- Limited 50-image test set lacks diversity in artistic styles, real-world scenarios, and edge cases
- Reliance on attention mechanisms creates potential single point of failure if future I2V models employ alternative motion synthesis pathways
- Perturbation budget (ε=16) represents trade-off between invisibility and effectiveness that may not generalize across all use cases

## Confidence

**High Confidence**: The empirical demonstration that attention weight suppression successfully reduces motion synthesis in CogVideoX-2B, as evidenced by near-zero flow magnitude (0.0497) and temporal SSIM (0.0056) on the test set. The ablation showing full-attention targeting outperforms cross-attention is well-supported by direct comparisons.

**Medium Confidence**: The generalizability of the approach across different I2V architectures and the claim of strong imperceptibility given the ε=16 perturbation budget. While qualitative results and LPIPS scores support these claims, broader validation across models and more rigorous perceptual studies are needed.

**Low Confidence**: The claim that Vid-Freeze is "a robust and scalable solution" for protecting images against malicious I2V generation. This overstates the current evidence, which is limited to one model, one perturbation budget, and controlled experimental conditions without real-world adversarial scenarios.

## Next Checks

1. **Cross-Architecture Validation**: Implement Vid-Freeze on at least two additional I2V models (e.g., Stable Video Diffusion, HunyuanVideo) to verify the attention-suppression mechanism works across different architectures. Measure flow magnitude and SSIM to confirm consistent motion blocking.

2. **Perceptual Robustness Test**: Conduct a human perceptual study comparing ε=2, ε=8, and ε=16 perturbations on diverse image types (faces, landscapes, text) to determine the true imperceptibility threshold and identify failure cases where perturbations become noticeable.

3. **Adaptive Attack Resistance**: Design and test an adaptive attacker that modifies its strategy when it detects Vid-Freeze perturbations, such as using alternative attention pathways or learning to ignore adversarial noise. Measure whether Vid-Freeze remains effective under these more sophisticated attack scenarios.