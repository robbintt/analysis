---
ver: rpa2
title: 'LLM Bandit: Cost-Efficient LLM Generation via Preference-Conditioned Dynamic
  Routing'
arxiv_id: '2502.02743'
source_url: https://arxiv.org/abs/2502.02743
tags:
- routing
- policy
- arxiv
- evaluation
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work tackles the challenge of balancing LLM accuracy and cost
  when routing queries to the most appropriate model. It formulates the routing problem
  as a multi-armed bandit with a preference-conditioned policy that adapts to user-defined
  trade-offs.
---

# LLM Bandit: Cost-Efficient LLM Generation via Preference-Conditioned Dynamic Routing

## Quick Facts
- **arXiv ID**: 2502.02743
- **Source URL**: https://arxiv.org/abs/2502.02743
- **Reference count**: 40
- **Primary result**: Up to 27% cost reduction while maintaining performance across five benchmarks

## Executive Summary
This paper tackles the challenge of efficiently routing LLM queries to the most appropriate model by formulating it as a multi-armed bandit problem with preference-conditioned policy. The method uses compact model identity vectors derived from IRT and variational inference, enabling fast integration of new models with only 20-50 prompts. Experiments across five major benchmarks show significant cost reduction versus existing methods while maintaining performance, with strong generalization to unseen models and preferences.

## Method Summary
The approach combines IRT-based model identity vectors with a preference-conditioned multi-objective bandit policy. First, an IRT model learns compact identity vectors for each LLM by mapping prompt embeddings and model identities through a scoring network, augmented with pairwise comparison data and KL regularization. A SetTransformer-based routing policy then conditions on the query, context (model identities, costs, predicted scores), and user preference vector ω to select the optimal model. The policy is trained via modified PPO with reward normalization and on-manifold mixup regularization. New models can be integrated efficiently through discriminative prompt selection and identity vector optimization.

## Key Results
- Up to 27% cost reduction versus existing routing methods
- Strong generalization to unseen models and preferences
- Minimal routing decision overhead (~5ms)
- 90% reduction in model integration overhead through cold-start approach

## Why This Works (Mechanism)

### Mechanism 1: Model Identity Vectors via IRT-Style Learning
- **Claim**: Compact latent vectors can encode each LLM's capabilities across diverse query types, enabling efficient comparison without full re-evaluation.
- **Mechanism**: A neural variant of Item Response Theory (IRT) maps prompt embeddings eₙ and model identity vectors Iₖ through a scoring network f(eₙ, Iₖ) → pₖₙ. Variational inference regularizes Iₖ with a KL-divergence term, encouraging generalization to unseen models. Pairwise comparison data augments pointwise scores via a second network g predicting win probabilities.
- **Core assumption**: Model capabilities are sufficiently stable across query distributions that a low-dimensional embedding can capture performance patterns.
- **Evidence anchors**: [abstract] compact model identity vectors; [section 2.3] Equation (2)-(4) define IRT loss, pairwise loss, and KL regularization; [corpus] MixLLM uses similar model embeddings.
- **Break condition**: If model behavior is highly context-dependent and not captured by static embeddings, identity vectors may underfit.

### Mechanism 2: Preference-Conditioned Multi-Objective Bandit Policy
- **Claim**: A single stochastic policy can dynamically adapt to user-specified performance-cost trade-offs at inference time without retraining.
- **Mechanism**: The routing policy π_θ(k|x, C_K, ω) is conditioned on: (1) query x, (2) context C_K = {(Iₖ, cₖ, p̂ₖ)} for all candidate models, and (3) preference vector ω = [ω₁, ω₂]. The dot-product architecture exp(Iₖ^T h(...)) enables zero-shot extension to new models. Policy gradients optimize the scalarized reward ω^T r(x,k) = ω₁ s(x,k) - ω₂ cₖ using PPO with a decomposed value function.
- **Core assumption**: The Pareto front of achievable (performance, cost) trade-offs is continuous and learnable.
- **Evidence anchors**: [abstract] preference-conditioned dynamic routing; [section 2.4] Equation (5) defines the preference-conditioned policy; [corpus] MetaLLM optimizes fixed scalarized rewards, lacking preference conditioning.
- **Break condition**: If preferences are non-convex or if the Pareto front has discontinuities, a single policy may fail to cover all optimal trade-offs.

### Mechanism 3: Efficient Cold Start via Discriminative Prompt Selection
- **Claim**: New models can be characterized using only 20-50 strategically selected prompts, achieving ~90% reduction in integration overhead.
- **Mechanism**: For each prompt xₙ, compute a discrimination score ψₙ measuring prediction uncertainty across existing models. Stratified sampling selects prompts spanning easy-to-hard discrimination levels. The new model's identity vector Ĩ is optimized via Equation (8) on this subset while keeping the IRT model frozen.
- **Core assumption**: Prompts with high discriminative power transfer to new model characterization.
- **Evidence anchors**: [abstract] strong generalization to unseen models; [section 2.6] Equation (7)-(8) define discrimination scores and identity vector optimization; "reducing the integration overhead by 90%"
- **Break condition**: If a new model has fundamentally different failure modes not represented in original model pool, stratified sampling may miss informative prompts.

## Foundational Learning

- **Concept: Multi-Armed Bandits / Contextual Bandits**
  - **Why needed here**: The routing problem is formulated as selecting among K "arms" (models) based on context (query), balancing exploration-exploitation.
  - **Quick check question**: Can you explain why Thompson Sampling or UCB would be alternatives to policy-gradient methods here?

- **Concept: Item Response Theory (IRT)**
  - **Why needed here**: The model identity vectors borrow from psychometric IRT, where "items" (prompts) and "subjects" (models) have latent traits governing success probabilities.
  - **Quick check question**: How does the 2-parameter IRT model differ from the neural variant used here?

- **Concept: Multi-Objective Optimization & Pareto Fronts**
  - **Why needed here**: Performance and cost are conflicting objectives; the system traces Pareto-optimal trade-offs conditioned on ω.
  - **Quick check question**: If two policies produce rewards [0.8, -0.3] and [0.7, -0.1], which dominates which?

- **Concept: Policy Gradient Methods (PPO, GAE)**
  - **Why needed here**: The routing policy is trained via PPO with Generalized Advantage Estimation for variance reduction.
  - **Quick check question**: Why does PPO use a clipped surrogate objective instead of raw policy gradient?

- **Concept: Variational Inference & ELBO**
  - **Why needed here**: Model identity vectors are latent variables with KL-regularized posteriors for generalization.
  - **Quick check question**: What does the KL term in Equation (4) penalize?

## Architecture Onboarding

- **Component map**: Pretrained prompt embeddings → IRT Encoder (f(e, Iₖ) → pₖₙ) → Pairwise Comparator (g(e, Iₖ₁, Iₖ₂) → win probability) → SetTransformer Policy Network ({(Iₖ, cₖ, p̂ₖ)} + query + ω → softmax) → Value Network (V(s), V(c)) → Model Selection

- **Critical path**: 1. Train IRT model on pairwise + benchmark data → obtain Iₖ for all models; 2. Supervised pretrain routing policy on pairwise datasets with calibrated scores; 3. RL fine-tune via PPO on benchmark data with dynamic model sets and preference sampling; 4. For new models: run stratified prompt selection → optimize Ĩ → plug into policy

- **Design tradeoffs**: Dot-product policy (Iₖ^T h) vs. full attention enables O(1) new model integration but may underfit complex interactions; single conditioned policy trades slight per-config optimality for deployment simplicity; 20-50 prompt cold start vs. full evaluation trades integration speed for potential characterization gaps

- **Failure signatures**: Routing collapses to single model (check ω normalization and range); new model identity vector uninformative (verify discrimination scores cover capability profile); cost predictions mismatch actual spend (need query-length-aware cost models); policy overfits to training model sets (increase diversity)

- **First 3 experiments**: 1. Reproduce IRT identity vectors and visualize Iₖ clusters for model families; 2. Validate preference conditioning by testing interpolation/extrapolation on held-out ω values; 3. Cold-start ablation: compare routing performance with Iₖ computed from 10, 20, 50, 100, and full prompts

## Open Questions the Paper Calls Out

- **Question**: Can the current offline framework be extended to an online learning setting to continuously adapt policy robustness through real-time user feedback?
- **Basis in paper**: [explicit] The Conclusion states, "extending to online learning could improve policy robustness through real-time feedback. This would enable continuous adaptation to changing user needs."
- **Why unresolved**: The current method relies on offline evaluation scores and pre-trained policies; it does not update its routing strategy based on live feedback loops.
- **What evidence would resolve it**: An implementation of the policy within a live deployment where the model updates its weights or identity vectors dynamically based on user rewards, showing improved long-term performance.

- **Question**: Does replacing fixed cost estimates with adaptive, query-specific cost models (accounting for input/output tokens) significantly improve the precision of the performance-cost trade-off?
- **Basis in paper**: [explicit] The Conclusion notes, "Our framework currently assumes fixed costs per model, but real-world costs vary with input length and computation requirements."
- **Why unresolved**: The current optimization uses a static cost $c_k$ per model, ignoring that generation costs scale with token count, which varies by query complexity.
- **What evidence would resolve it**: Experiments where cost inputs are dynamic functions of prompt length and predicted output length, compared against the static cost baseline for Pareto efficiency.

- **Question**: Can the routing policy generalize to select external tools and API calls (such as search engines) as "arms" in addition to standard LLM generation models?
- **Basis in paper**: [explicit] The Conclusion suggests, "Future work could also expand the routing capability to leverage external tools and API calls that many modern LLMs support."
- **Why unresolved**: The current action space consists solely of language models defined by identity vectors derived from text benchmarks; tool use requires a different capability representation.
- **What evidence would resolve it**: A modified framework where the action space includes tools (e.g., code interpreters, search), demonstrating successful routing between generation and tool usage based on query intent.

## Limitations
- Cold-start efficiency relies on transferability of discriminative prompts across model distributions, which may fail for models with fundamentally different capability profiles
- Preference conditioning is only validated for ω ∈ [0, 2]; extreme preferences may lead to suboptimal routing
- Static cost model uses fixed per-1M-token costs rather than query-length-aware predictions
- Identity vectors don't capture temporal variations in model behavior

## Confidence
- **High confidence**: Multi-armed bandit formulation, IRT-based identity vectors, preference-conditioned policy architecture, 27% cost reduction claim
- **Medium confidence**: Cold-start efficiency claims (90% reduction), preference conditioning generalization across unseen ω values, minimal routing overhead (~5ms)
- **Low confidence**: Static identity vector limitations, cost model sensitivity to prediction errors, performance under extreme preference values

## Next Checks
1. **Cold-start robustness**: For a held-out model with distinct capability profile, compare routing performance using identity vectors computed from 10, 20, 50, 100, and full prompts. Plot degradation curves and identify minimum effective prompt count.
2. **Preference extrapolation**: Hold out ω ∈ [0, 0.5] ∪ [1.5, 2] during training. Test routing policy on these extreme preferences and compare against interpolated performance from training range.
3. **Dynamic cost modeling**: Replace fixed per-1M-token costs with query-length-aware cost predictions. Measure routing policy degradation and identify cost prediction error tolerance thresholds.