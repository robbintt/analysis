---
ver: rpa2
title: Post-pre-training for Modality Alignment in Vision-Language Foundation Models
arxiv_id: '2504.12717'
source_url: https://arxiv.org/abs/2504.12717
tags:
- clip
- performance
- pre-trained
- image
- modality
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces CLIP-Refine, a post-pre-training method
  designed to reduce the modality gap in pre-trained CLIP models by aligning image
  and text feature distributions. The method employs two techniques: Random Feature
  Alignment (RaFA), which aligns features to a shared prior distribution using random
  reference vectors, and Hybrid Contrastive-Distillation (HyCD), which combines knowledge
  distillation from the pre-trained model with ground-truth labels to balance retaining
  past knowledge and learning new alignment.'
---

# Post-pre-training for Modality Alignment in Vision-Language Foundation Models

## Quick Facts
- arXiv ID: 2504.12717
- Source URL: https://arxiv.org/abs/2504.12717
- Reference count: 40
- Primary result: CLIP-Refine improves zero-shot classification accuracy by 1.95% over pre-trained CLIP models

## Executive Summary
This paper introduces CLIP-Refine, a post-pre-training method that addresses modality misalignment in vision-language foundation models by aligning image and text feature distributions. The approach employs two complementary techniques: Random Feature Alignment (RaFA) to align features to a shared prior distribution using random reference vectors, and Hybrid Contrastive-Distillation (HyCD) that combines knowledge distillation from pre-trained models with ground-truth labels. Experiments on 12 classification and 2 retrieval datasets demonstrate significant performance improvements, with CLIP-Refine achieving 54.69% average classification accuracy compared to 52.74% for pre-trained models.

## Method Summary
CLIP-Refine operates as a post-pre-training refinement stage that takes pre-trained CLIP models and further aligns their modality representations through two key mechanisms. The Random Feature Alignment (RaFA) technique maps both image and text features to a common prior distribution using randomly initialized reference vectors, ensuring better feature alignment. The Hybrid Contrastive-Distillation (HyCD) approach simultaneously maintains knowledge from the pre-trained model through distillation while learning new alignment patterns from ground-truth labels. This dual approach addresses the fundamental challenge of modality gaps that persist even after extensive pre-training, where visual and textual representations may not be optimally aligned for downstream tasks.

## Key Results
- CLIP-Refine achieves 54.69% average classification accuracy compared to 52.74% for pre-trained models across 12 datasets
- The method shows consistent improvements across all tested datasets, including ImageNet (82.85% vs 80.70%) and VTAB-1k (65.88% vs 62.53%)
- Zero-shot retrieval performance improves significantly, with CIFAR-10 achieving 66.68% recall@1 compared to 64.84% for pre-trained models

## Why This Works (Mechanism)
CLIP-Refine addresses the fundamental modality gap problem by explicitly aligning feature distributions across visual and textual modalities. The random feature alignment creates a shared geometric space that both modalities can occupy, reducing the distance between corresponding image-text pairs. The hybrid contrastive-distillation component ensures that while the model learns better alignment, it doesn't forget the valuable knowledge acquired during pre-training. This two-pronged approach is particularly effective because it tackles both the structural alignment issue (through RaFA) and the knowledge retention problem (through HyCD), creating a more robust foundation model that performs better across diverse downstream tasks.

## Foundational Learning

**Vision-Language Pre-training**: Understanding how CLIP models are initially trained using large-scale image-text pairs to learn joint representations across modalities. Why needed: Provides context for why modality misalignment occurs and what pre-training achieves. Quick check: Review CLIP pre-training objectives and contrastive loss formulations.

**Feature Distribution Alignment**: The concept of mapping features from different modalities to a common geometric space. Why needed: Core mechanism behind CLIP-Refine's effectiveness. Quick check: Examine how KL divergence and other metrics measure distribution similarity.

**Knowledge Distillation**: Technique for transferring knowledge from one model to another, particularly from pre-trained to fine-tuned models. Why needed: Essential for understanding the HyCD component's role in preserving pre-trained knowledge. Quick check: Review distillation loss formulations and temperature scaling effects.

**Zero-shot Learning**: Evaluating models without task-specific fine-tuning by leveraging learned representations. Why needed: Primary evaluation metric used in the paper. Quick check: Understand how zero-shot classification differs from traditional fine-tuning approaches.

## Architecture Onboarding

**Component Map**: CLIP-Refine takes pre-trained CLIP models as input, applies RaFA to align features to shared prior distributions, then applies HyCD to balance distillation and ground-truth learning, producing refined CLIP models as output.

**Critical Path**: Input CLIP model → RaFA alignment step → HyCD training phase → Output refined model. The RaFA step establishes geometric alignment, while HyCD refines the alignment while preserving knowledge.

**Design Tradeoffs**: The method trades additional training time and computational resources for improved zero-shot performance. The random reference vectors in RaFA introduce stochasticity but provide robustness. The HyCD component balances between preserving pre-trained knowledge and learning new alignments, requiring careful hyperparameter tuning.

**Failure Signatures**: Poor performance may manifest as degradation in zero-shot accuracy, particularly if the random feature alignment creates too much noise or if the distillation component overpowers the ground-truth learning. Insufficient training data can also lead to overfitting to the random reference vectors.

**First Experiments**:
1. Test CLIP-Refine on a single dataset (e.g., CIFAR-10) to verify basic functionality before scaling to all 14 datasets
2. Compare performance with and without the RaFA component to isolate its contribution
3. Vary the temperature parameter in the distillation component to find optimal knowledge preservation

## Open Questions the Paper Calls Out
None

## Limitations
- The paper lacks detailed analysis of computational overhead, making it difficult to assess the trade-off between performance gains and resource requirements
- Claims of generalizability across multiple CLIP variants are based on only two tested models (ViT-B/32 and ViT-B/16), limiting confidence in broader applicability
- The random feature alignment strategy's effectiveness is not thoroughly validated through ablation studies compared to the hybrid contrastive-distillation component
- Focus on zero-shot performance without exploring few-shot or fine-tuning scenarios limits understanding of practical applicability

## Confidence
- CLIP-Refine's performance improvement claims: **High** (strong empirical evidence across 14 datasets)
- Computational efficiency claims: **Medium** (method described as efficient but lacking quantitative analysis)
- Generalizability across CLIP variants: **Medium** (tested on two models but claimed broader)
- RaFA component effectiveness: **Medium** (theoretical justification but limited ablation)

## Next Checks
1. Conduct computational profiling to measure training time, memory usage, and parameter count overhead compared to standard pre-training
2. Test CLIP-Refine on additional CLIP variants (ViT-L/14, ResNet-based architectures) to validate cross-model generalizability claims
3. Evaluate few-shot learning performance (1-10 examples per class) to assess practical utility beyond zero-shot scenarios