---
ver: rpa2
title: In Generative AI We (Dis)Trust? Computational Analysis of Trust and Distrust
  in Reddit Discussions
arxiv_id: '2510.16173'
source_url: https://arxiv.org/abs/2510.16173
tags:
- trust
- distrust
- genai
- dimensions
- reliability
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study presents the first large-scale computational analysis
  of public trust and distrust in generative AI using Reddit data from 2022 to 2025.
  A multi-label classification framework identified Trust, Distrust, and their dimensions
  (e.g., Competence, Reliability) using both transformer models and LLMs, achieving
  best performance with GPT-4.1-mini (F1=0.78).
---

# In Generative AI We (Dis)Trust? Computational Analysis of Trust and Distrust in Reddit Discussions

## Quick Facts
- arXiv ID: 2510.16173
- Source URL: https://arxiv.org/abs/2510.16173
- Reference count: 38
- Key outcome: First large-scale computational analysis of public trust/distrust in GenAI using Reddit data, identifying personal experience as primary driver and GPT-4.1-mini few-shot as best classifier (F1=0.78).

## Executive Summary
This study presents the first large-scale computational analysis of public trust and distrust in generative AI using Reddit data from 2022 to 2025. A multi-label classification framework identified Trust, Distrust, and their dimensions (e.g., Competence, Reliability) using both transformer models and LLMs, achieving best performance with GPT-4.1-mini (F1=0.78). Trust and Distrust were nearly balanced over time, with spikes around major model releases. Competence, Reliability, and Familiarity dominated as trust dimensions, while personal experience was the most cited reason for (dis)trust. Different trustor groups showed distinct patterns, highlighting the sociotechnical complexity of public discourse. The findings provide a scalable methodology and insights for monitoring evolving perceptions of GenAI.

## Method Summary
The study collected 197,618 Reddit posts from 39 subreddits (Nov 2022–Jun 2025) about generative AI, filtering for English-language content and excluding removed/deleted/empty posts. Five crowdworkers per post annotated Trust/Distrust/Both/Neither, 13 dimensions (6 trust + 7 distrust), 10 trustor categories, and 6 reason categories using the Potato interface. Annotations were resolved by majority vote with Fleiss' κ=0.33 (moderate agreement). Classification used fine-tuned transformers (BERT, RoBERTa, DistilBERT, DeBERTa, ModernBERT, SocBERT) and LLMs (9 OpenAI + Llama3-8B, Mixtral-8x7B, Gemma2-9B) in zero-shot and few-shot settings. GPT-4.1-mini in few-shot achieved best performance (F1=0.78) for main classification, while GPT-5-mini zero-shot classified dimensions (Avg F1=0.49, Any Match=0.88).

## Key Results
- GPT-4.1-mini few-shot achieved F1=0.78 for Trust/Distrust classification, outperforming fine-tuned transformers
- Trust and Distrust each accounted for 40–50% of posts, with spikes around major model releases (GPT-4, LLaMA2, OpenAI Dev Day)
- Personal experience using AI was the most cited reason for both trust and distrust
- Competence, Reliability, and Familiarity were the most prevalent trust dimensions; Reliability, Unreliability, and Competence dominated distrust dimensions

## Why This Works (Mechanism)

### Mechanism 1
Few-shot LLM prompting outperforms fine-tuned transformers for context-sensitive trust classification in informal social media discourse. LLMs leverage in-context learning with domain-specific definitions and examples, capturing pragmatic cues (irony, ambivalence) that transformer classifiers miss without extensive labeled data. Few-shot examples ground abstract trust constructs in concrete textual patterns.

### Mechanism 2
Personal experience dominates as the reason for both trust and distrust, making first-hand interaction the primary driver of public perception. Users evaluate GenAI through direct use (task success/failure, accuracy, usability), anchoring judgments in concrete functionality rather than abstract ethical concerns or secondhand reports. Repeated exposure builds familiarity, which co-occurs strongly with competence assessments (Jaccard=0.72 for Competence-Familiarity).

### Mechanism 3
Major model releases trigger temporary trust-distrust rebalancing, but attitudes revert toward equilibrium rather than converging to stable consensus. Breakthrough announcements (GPT-4, LLaMA, Claude releases) spike discourse volume and re-open debates about capability vs. risk. Early volatility (distrust spikes post-ChatGPT) gives way to oscillation around zero net balance, reflecting sustained sociotechnical tension between optimism and skepticism.

## Foundational Learning

- **Concept:** Multi-label text classification with conditional prompting
  - **Why needed here:** The paper uses LLMs to predict multiple dimensions per post (e.g., Competence + Reliability), conditioned on the Trust/Distrust label to constrain output space and improve relevance.
  - **Quick check question:** Given a post labeled "Distrust," should the model predict from trust dimensions, distrust dimensions, or both? (Answer: Distrust dimensions only; "Both" label receives both sets.)

- **Concept:** Inter-rater reliability (Fleiss' kappa) and majority voting for noisy labels
  - **Why needed here:** Crowd annotations showed κ=0.33 (moderate agreement), requiring aggregation strategies (majority vote, discarding highly inconsistent posts) to produce training labels.
  - **Quick check question:** If 5 annotators label a post as [Trust, Trust, Distrust, Neither, Both], what label survives majority voting? (Answer: Trust, with 2/5 votes; highlights fragility of noisy agreement.)

- **Concept:** Distrust as distinct construct, not absence of trust
  - **Why needed here:** The paper adopts Lewicki et al. (1998) framework where distrust involves active negative expectations, requiring separate dimension set (Unreliability, Deception, Malevolence) rather than Trust negation.
  - **Quick check question:** Can a post express both Trust and Distrust simultaneously? (Answer: Yes—"Both" label accounts for 7% of posts, though hardest to classify at F1=0.33.)

## Architecture Onboarding

- **Component map:** Reddit API (Pushshift/Arcticshift) -> keyword filtering -> preprocessing (remove deleted/short posts) -> 197,618 posts -> Prolific crowdworkers -> Potato annotation interface -> 2,583 labeled posts -> majority voting -> GPT-4.1-mini few-shot (Trust/Distrust) -> GPT-5-mini zero-shot (dimensions/trustor/reason) -> Full dataset labeling -> temporal aggregation -> dimension co-occurrence analysis

- **Critical path:** Codebook design (37 candidate dimensions -> 13 final dimensions) determines annotation feasibility; annotation quality (κ=0.33) caps model performance ceiling; model selection (GPT-4.1-mini few-shot) for main classification enables scaling; conditional dimension prompting (Trust->trust dims, Distrust->distrust dims) reduces output space.

- **Design tradeoffs:** Crowdsourcing vs. expert annotation: scalability and diversity vs. lower reliability on abstract dimensions (Integrity, Transparency F1<0.30); Few-shot vs. fine-tuning: faster iteration and comparable performance vs. potential gains from domain-specific fine-tuning; Binary (Trust/Distrust) vs. multi-label (Both/Neither): captures nuance but "Both" class fails at F1=0.33.

- **Failure signatures:** "Both" class underperformance (F1=0.33): Model struggles with ambivalence/irony in short informal posts; Abstract dimension underdetection: Integrity, Benevolence, Malevolence rarely surface in discourse or are mislabeled by annotators; Platform bias: Reddit skews younger/tech-engaged; may not generalize to broader populations.

- **First 3 experiments:**
  1. Reproduce annotation schema on 100 new posts: Validate inter-rater reliability; if κ<0.3, refine dimension definitions before scaling.
  2. Ablate few-shot examples: Test GPT-4.1-mini with 0/1/2/5 examples per label to measure in-context learning contribution; expect performance drop without examples.
  3. Cross-platform validation: Apply trained models to Twitter/X dataset from corpus neighbor (Katta 2025); compare Trust/Distrust distribution to assess platform transferability.

## Open Questions the Paper Calls Out

- **Open Question 1:** Do trust and distrust patterns in GenAI discourse replicate across platforms with different user demographics (e.g., Twitter/X, Facebook, TikTok) and in non-English linguistic contexts?
  - Basis in paper: [explicit] The authors state that Reddit "skews younger and technologically engaged" and that "cross-platform studies are needed for broader validation" since findings "reflect specific online communities rather than the general population."
  - Why unresolved: The study is limited to English-language Reddit data from 39 subreddits; no comparison with other platforms or languages was conducted.
  - What evidence would resolve it: Large-scale computational analysis applying the same classification framework to Twitter, Facebook, or non-English platforms, comparing dimension distributions and temporal patterns.

- **Open Question 2:** Does public discourse on GenAI trust align with classical theoretical frameworks (e.g., Mayer et al.'s ability–integrity–benevolence model), or do online expressions privilege functionality over ethical dimensions?
  - Basis in paper: [explicit] The authors note that "ability-based judgments dominate, while integrity and benevolence are far less salient" and invite "reconsideration of whether Trust in AI, as expressed in social media narratives, aligns with or diverges from established theoretical frameworks."
  - Why unresolved: The paper observes a mismatch but does not systematically test alignment between observed discourse patterns and established trust theories.
  - What evidence would resolve it: Comparative analysis mapping social media-derived dimensions onto theoretical constructs, or validation studies correlating discourse-based trust measures with survey instruments grounded in classical frameworks.

- **Open Question 3:** Can expert annotation or hierarchical coding schemes improve computational detection of abstract, normative dimensions such as Integrity, Transparency, and Benevolence in GenAI discourse?
  - Basis in paper: [inferred] The paper reports that "abstract dimensions like Transparency and Integrity performed poorly" in classification, and crowdworkers "often struggled to apply these abstract categories." The authors suggest "future work could improve reliability with expert input or hierarchical schemes that separate functional from normative dimensions."
  - Why unresolved: Current annotation relied solely on crowdsourcing; no comparison with expert annotators or hierarchical schemes was conducted.
  - What evidence would resolve it: An annotation study comparing crowdworker vs. expert labels for abstract dimensions, or testing hierarchical classification that first distinguishes functional from normative concerns before assigning specific dimensions.

- **Open Question 4:** What linguistic or contextual features enable better computational detection of ambivalent or mixed trust–distrust expressions in short social media posts?
  - Basis in paper: [inferred] The paper notes that the "Both" category achieved the lowest F1 score (0.33) and that "models still struggle with ambivalence (Both) and with normative categories, making these harder to detect in short, informal posts."
  - Why unresolved: The study identifies the problem but does not investigate what textual signals characterize ambivalence or how to improve detection.
  - What evidence would resolve it: Qualitative analysis of ambivalent posts to identify linguistic markers (e.g., hedging, contrastive conjunctions, irony), followed by model experiments incorporating these features or specialized training data for the "Both" class.

## Limitations

- Moderate inter-rater reliability (κ=0.33) indicates inherent ambiguity in trust-related concepts, particularly for abstract dimensions like Integrity and Transparency
- Study relies entirely on Reddit discourse, which skews toward younger, tech-engaged populations and may not generalize to broader demographics
- The "Both" class performs poorly (F1=0.33), indicating the model struggles with ambivalence and nuanced discourse patterns common in informal social media

## Confidence

- Trust/Distrust classification performance (F1=0.78): **High** - Robust across multiple LLM models with consistent few-shot advantages
- Dimension classification reliability (Avg F1=0.49): **Medium** - Reasonable for common dimensions (Competence, Reliability) but weak for abstract constructs (Integrity, Benevolence)
- Personal experience as primary driver: **Medium** - Well-supported by data but self-reported reasons may involve post-hoc rationalization
- Model release effects on trust dynamics: **Medium** - Temporal patterns align with major events but causation not definitively established

## Next Checks

1. Conduct expert review of 100 randomly selected posts to assess whether crowdsourced annotations capture intended semantic distinctions, particularly for abstract dimensions showing low agreement
2. Apply the classification framework to Twitter/X data from a related study to test platform transferability and identify potential platform-specific discourse patterns
3. Implement an ablation study varying the number of few-shot examples (0, 1, 2, 5 per label) to quantify the contribution of in-context learning versus model capabilities alone