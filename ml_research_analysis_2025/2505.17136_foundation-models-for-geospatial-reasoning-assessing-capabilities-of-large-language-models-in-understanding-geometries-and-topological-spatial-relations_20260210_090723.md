---
ver: rpa2
title: 'Foundation Models for Geospatial Reasoning: Assessing Capabilities of Large
  Language Models in Understanding Geometries and Topological Spatial Relations'
arxiv_id: '2505.17136'
source_url: https://arxiv.org/abs/2505.17136
tags:
- spatial
- topological
- relations
- geometry
- reasoning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper evaluates the ability of large language models (LLMs)
  to represent and reason with vector-based geometries and spatial relations. It uses
  WKT format of geometries as input and compares three approaches: geometry embedding-based,
  prompt engineering-based, and everyday language-based evaluation.'
---

# Foundation Models for Geospatial Reasoning: Assessing Capabilities of Large Language Models in Understanding Geometries and Topological Spatial Relations

## Quick Facts
- arXiv ID: 2505.17136
- Source URL: https://arxiv.org/abs/2505.17136
- Reference count: 15
- LLMs can achieve >0.6 accuracy in identifying topological spatial relations using WKT geometry representations

## Executive Summary
This paper evaluates whether large language models can comprehend vector-based geometries and topological spatial relations. The study compares three evaluation approaches: embedding-based classification using geometry text embeddings, prompt engineering with few-shot examples, and everyday language conversion. Results show that both embedding-based and prompt engineering approaches achieve over 0.6 accuracy for identifying topological spatial relations between two geometries. GPT-4 with few-shot prompting achieved the highest performance at 0.666 accuracy. The research also demonstrates that LLMs can generate valid geometries to enhance geographic entity retrieval and understand inverse topological spatial relations.

## Method Summary
The study evaluates LLM performance on three geospatial tasks: topological relation qualification (classifying spatial relations between geometry pairs), spatial query processing (retrieving geometries based on relations), and vernacular conversion (mapping everyday language to formal predicates). Geometries are represented in WKT format using OpenStreetMap, SLIPO POIs, and Census data from Madison, WI. The embedding-based approach concatenates text embeddings of two WKT strings and uses a Random Forest classifier. The prompt-based approach uses zero-shot, few-shot, and chain-of-thought prompting with GPT-3.5-turbo, GPT-4, and DeepSeek-R1. The vernacular conversion task maps natural language descriptions to formal topological predicates.

## Key Results
- Embedding-based and prompt engineering approaches both achieve >0.6 accuracy for topological relation identification
- GPT-4 with few-shot prompting achieved highest accuracy of 0.666
- LLM-generated geometries improved retrieval performance (MRR from 0.152 to 0.18)
- Chain-of-thought prompting degraded performance due to cascading reasoning errors

## Why This Works (Mechanism)

### Mechanism 1
- Claim: WKT text representations preserve sufficient geometric information for LLMs to perform topological relation classification
- Mechanism: Sentence embedding models encode WKT strings into vectors that retain geometry type and coordinate structure. A downstream classifier (random forest) can then predict topological predicates from concatenated geometry embeddings with >0.6 accuracy
- Core assumption: Token-level encoding of coordinate sequences does not fully destroy spatial structure during embedding
- Evidence anchors:
  - [abstract] "both embedding-based and prompt engineering-based approaches... can achieve an accuracy of over 0.6 on average for the identification of topological spatial relations"
  - [Section 5.1.2, Table 5] Random forest with text-embedding-ada-002 achieved 0.633 accuracy; text-embedding-3-large achieved 0.632
  - [corpus] Related work (Ji and Gao, 2023) previously showed LLM-generated embeddings can preserve geometry types and some coordinate information

### Mechanism 2
- Claim: Few-shot prompting provides task-specific examples that calibrate LLM reasoning for topological predicates more effectively than zero-shot or chain-of-thought approaches
- Mechanism: Providing example (WKT_A, WKT_B, relation) triplets in the prompt establishes a pattern-matching template. GPT-4 with few-shot prompting achieved 0.666 accuracy, outperforming zero-shot (0.632) and CoT variants
- Core assumption: The model can transfer the exemplar pattern to new geometry pairs without explicit symbolic computation
- Evidence anchors:
  - [Section 5.1.2] "GPT-4 with few-shot prompting achieved 0.666 accuracy" while "zero-shot achieved 0.632"
  - [Section 5.1.2] "chain-of-thought (CoT) prompts... did not yield the expected benefit... accuracy declined due to cascading errors in intermediate steps"

### Mechanism 3
- Claim: LLM-generated synthetic geometries can augment spatial queries by expanding the semantic search space with valid, relation-preserving candidates
- Mechanism: Given a query like "Retrieve a geometry within POLYGON(...)," GPT-4 generates candidate WKT geometries. These are embedded alongside the query, improving retrieval via similarity search. GPT-4 achieved 0.763 validity for relation preservation
- Core assumption: Generated geometries occupy similar embedding space regions as true answers, creating useful semantic bridges
- Evidence anchors:
  - [Section 5.2.2, Table 7] "Zero-shot: Valid WKT 0.999, relation preservation 0.763, topological distance 1.142"
  - [Section 5.2.3, Table 6] Adding one LLM-generated geometry improved MRR from 0.152 to 0.18 and Hits@5 from 0.212 to 0.238

## Foundational Learning

- **WKT (Well-Known Text) Format**
  - Why needed here: All geometry inputs to LLMs are serialized as WKT strings (e.g., `POINT (-89.3551 43.123)`). Understanding this representation is prerequisite to interpreting results
  - Quick check question: Given `POLYGON ((0 0, 1 0, 1 1, 0 1, 0 0))`, can you identify the geometry type and whether it's closed?

- **DE-9IM Topological Predicates**
  - Why needed here: The seven predicates (equals, disjoint, touches, crosses, within, contains, overlaps) define the classification targets. Conceptual neighborhoods explain why certain confusions are more likely
  - Quick check question: What is the topological difference between "touches" and "crosses" for LineString/LineString pairs?

- **Conceptual Neighborhood Graph**
  - Why needed here: Errors cluster within neighborhoods (e.g., "overlaps" confused with "crosses" or "touches"). This metric (topological distance) provides nuanced evaluation beyond accuracy
  - Quick check question: If a model predicts "touches" when ground truth is "disjoint," what is the topological distance in the 9-IM neighborhood graph?

## Architecture Onboarding

- **Component map**:
  Input layer: WKT geometry strings → text embedding encoder (text-embedding-3-large) OR prompt-formatted text → LLM reasoner (GPT-4, DeepSeek-R1)
  Classification path (embedding-based): [Enc(A); Enc(B)] → Random Forest classifier → predicate prediction
  Reasoning path (prompt-based): Prompt with WKT + few-shot examples → LLM → structured output (GeomType(A), predicate, GeomType(B))
  Query expansion path: Spatial query → LLM geometry generator → synthetic WKT → embedding → similarity search

- **Critical path**:
  1. Geometry serialization to WKT must preserve precision (coordinate truncation degrades results)
  2. Prompt formatting must include geometry type context for best results
  3. Few-shot examples should cover diverse geometry type combinations
  4. For retrieval: reverse predicate direction aligns with natural language patterns (e.g., "within" → "contains" reversal)

- **Design tradeoffs**:
  - Embedding-based: Faster inference, no token limit issues, but requires classifier training; accuracy ceiling ~0.63
  - Prompt-based: Higher accuracy potential (0.66+), no training required, but token limits and API costs scale with geometry complexity
  - CoT prompting: Adds interpretability but degrades accuracy due to cascading reasoning errors

- **Failure signatures**:
  - Low validity output: DeepSeek-R1 struggled with output format adherence (0.919–0.936 validity vs. GPT-4's 0.99+)
  - Conceptual neighborhood confusion: "overlaps" ↔ "crosses" for LineString pairs; "touches" ↔ "within" for Point-Polygon
  - Asymmetric predicate generation: Generating "within" geometry is harder than "contains" geometry (directionality bias)
  - CoT cascading errors: Model fails to determine interior intersection, corrupting all downstream reasoning

- **First 3 experiments**:
  1. **Baseline validation**: Run embedding-based classification on 40 test triplets per geometry type combination using text-embedding-3-large; target >0.60 accuracy. Verify geometry type preservation (should be 1.0)
  2. **Few-shot vs. zero-shot ablation**: Test GPT-4 on topological relation qualification with 0, 1, 3, 5 few-shot examples. Measure accuracy and topological distance of errors. Expect peak at 3–5 examples; monitor for overfitting to example patterns
  3. **Geometry generation validity check**: Task GPT-4 with generating 100 synthetic geometries given reference geometry + predicate. Validate WKT syntax (parse with GeoPandas), geometry type match, and relation preservation. Target >0.75 relation accuracy

## Open Questions the Paper Calls Out

- **Question**: Can Retrieval-Augmented Generation (RAG) or fine-tuning approaches overcome the limitations of in-context learning for qualitative spatial reasoning?
  - Basis in paper: [explicit] The authors state they "did not explore fine-tuning approaches" and identify RAG as a "promising approach" for integrating external spatial databases and GIS tools
  - Why unresolved: The current study was restricted to in-context learning strategies, leaving the potential performance gains from these advanced architectures untested
  - What evidence would resolve it: Experiments comparing the current few-shot baseline against models fine-tuned on spatial datasets or equipped with GIS tool-retrieval mechanisms

- **Question**: Why does Chain-of-Thought (CoT) prompting degrade performance in spatial relation tasks, and can a modified reasoning strategy mitigate this?
  - Basis in paper: [explicit] The authors note that CoT "did not yield the expected benefit" and often induced "cascading errors in intermediate steps" during topological inference
  - Why unresolved: While the paper documents the failure of CoT to improve results, it does not solve the puzzle of how to effectively elicit step-by-step spatial reasoning without causing logical drift
  - What evidence would resolve it: A prompt engineering strategy that successfully utilizes intermediate reasoning steps to increase accuracy over standard few-shot baselines

- **Question**: How does the performance of LLMs in geospatial reasoning vary across different spatial scales and relation types?
  - Basis in paper: [explicit] The conclusion acknowledges the dataset is "limited to the city and state levels" and suggests "further investigation into multi-scale spatial relations" and directional/distance relations is needed
  - Why unresolved: The complexity of spatial interactions changes across heterogeneous datasets and scales, which were not captured in the specific study area used
  - What evidence would resolve it: Evaluation results from the proposed workflows applied to multi-scale datasets (e.g., global vs. neighborhood) and non-topological relations like cardinal directions

## Limitations

- Geometry representation limitations: WKT serialization may not preserve all spatial structure for complex geometries
- Evaluation scope constraints: Benchmark covers only seven topological predicates between three geometry types
- Model-specific findings: Performance differences between GPT-4 and DeepSeek-R1 may not generalize across different LLM architectures
- Environmental bias: All experiments use Madison, WI datasets, raising questions about geographic bias

## Confidence

- **High confidence (0.8-1.0)**: Embedding-based classification can achieve >0.6 accuracy for topological relation identification. Few-shot prompting outperforms zero-shot for LLM-based reasoning. Generated geometries can improve retrieval performance
- **Medium confidence (0.6-0.8)**: Chain-of-thought prompting degrades performance due to cascading errors. CoT prompts are more effective than zero-shot for relation generation tasks. Adding multiple synthetic geometries yields diminishing returns
- **Low confidence (0.4-0.6)**: The exact mechanisms causing CoT failure are not fully characterized. The optimal number of few-shot examples may vary by predicate type. Geometric validity constraints for generated geometries need further validation

## Next Checks

- **Check 1: Geometry Complexity Scaling**
  Test embedding-based classification accuracy across geometries of increasing complexity (simple points → complex polygons with holes). Measure whether accuracy degrades predictably with vertex count or coordinate precision

- **Check 2: Geographic Transferability**
  Evaluate the same models on topologically similar but geographically distinct datasets (e.g., another US city vs. international urban/rural areas). Compare performance drops to identify geographic bias patterns

- **Check 3: Synthetic Data Generation Impact**
  Systematically vary the number of generated geometries added to retrieval queries (1, 3, 5, 10) and measure MRR/Hits@K improvements. Determine the point of diminishing returns and whether generated geometries consistently improve results across different predicate types or geometry combinations