---
ver: rpa2
title: 'FoodSEM: Large Language Model Specialized in Food Named-Entity Linking'
arxiv_id: '2509.22125'
source_url: https://arxiv.org/abs/2509.22125
tags:
- uni00000013
- foodon
- food
- uni00000011
- entities
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: FoodSEM is a fine-tuned open-source large language model for food
  named-entity linking (NEL) to ontologies like FoodOn, SNOMED-CT, and Hansard. It
  addresses the lack of accurate food NEL systems by converting food-annotated corpora
  into instruction-response pairs for fine-tuning.
---

# FoodSEM: Large Language Model Specialized in Food Named-Entity Linking

## Quick Facts
- **arXiv ID:** 2509.22125
- **Source URL:** https://arxiv.org/abs/2509.22125
- **Reference count:** 40
- **Primary result:** Fine-tuned Llama 3 8B achieves F1 scores up to 98% for food named-entity linking to FoodOn, SNOMED-CT, and Hansard ontologies, outperforming few-shot prompting baselines.

## Executive Summary
FoodSEM is a fine-tuned large language model for food named-entity linking (NEL) that achieves state-of-the-art performance on three major food ontologies. The system converts annotated food corpora into instruction-response pairs and fine-tunes Meta Llama 3 8B using LoRA adapters, achieving F1 scores up to 98% compared to 40% maximum for few-shot prompting baselines. By addressing the challenge of linking food entities to standardized ontologies through supervised fine-tuning rather than prompting, FoodSEM demonstrates superior accuracy and robustness across recipe and scientific abstract datasets.

## Method Summary
The method converts CafeteriaFCD (recipes) and CafeteriaSA (scientific abstracts) BioC XML corpora into instruction-response pairs, with each document generating NER and NEL instructions using 1,000 GPT-3.5-generated variations. Entities with fewer than 150 mentions receive artificially generated IR pairs to balance the training distribution. The model fine-tunes Meta Llama 3 8B with 4-bit quantization using LoRA (rank 16) while mixing in 34,229 general instruction pairs from Dolly-HHRLHF to prevent catastrophic forgetting. A two-stage pipeline first extracts food entities via NER instruction, then links them to ontology URIs via NEL instruction, with custom parsers handling malformed LLM outputs.

## Key Results
- FoodSEM achieves F1 scores up to 98% on some ontologies and datasets, with 0.827-0.942 on artificial samples
- Outperforms non-fine-tuned LLMs in zero-, one-, and five-shot prompting baselines (0.0-0.40 F1 vs 0.827-0.942)
- Demonstrates superior linking accuracy and robustness across both recipe and scientific abstract data
- High precision (0.915-0.956) but lower recall (0.580-0.804) on real corpora due to NER error propagation

## Why This Works (Mechanism)

### Mechanism 1
Fine-tuning an instruction-tuned LLM on domain-specific IR pairs dramatically improves food NEL accuracy over few-shot prompting. The model learns stable associations between food entity surface forms and specific ontology URIs through supervised next-token prediction, building internal mappings that generalize across synonym mentions and multi-word entities.

### Mechanism 2
Artificial IR pair generation for underrepresented entities improves coverage and reduces bias toward frequent entities. By synthesizing NEL examples for entities below a 150-mention threshold, the training set achieves balanced exposure. The model learns rare entity URIs through diverse label combinations in artificial IR pairs.

### Mechanism 3
Mixing general instruction data with domain-specific IR pairs mitigates catastrophic forgetting while preserving specialization. Including 34,229 IR instances from Dolly-HHRLHF in each training fold maintains the base model's general capabilities while LoRA adaptation specializes query and value weight matrices for food NEL.

## Foundational Learning

- **Concept: Named-Entity Linking (NEL)**
  - **Why needed here:** NEL differs from NER by requiring entity normalization—mapping mentions to canonical URIs across ontologies with different naming conventions and hierarchies.
  - **Quick check question:** Given "walnuts" in a recipe, can you distinguish the NER task (identifying "walnuts" as a food entity) from the NEL task (linking to FOODON_03301844 vs. NCBITaxon_16718)?

- **Concept: Low-Rank Adaptation (LoRA)**
  - **Why needed here:** LoRA enables fine-tuning 8B-parameter models on limited GPU resources by training only low-rank decomposition matrices rather than full weights.
  - **Quick check question:** If LoRA rank is set to 16 for an 8B model, approximately how many trainable parameters are added compared to full fine-tuning?

- **Concept: Instruction-Response (IR) Format for LLM Training**
  - **Why needed here:** IR pairs structure the NEL task as a conversational prompt-response, leveraging the base model's instruction-following training rather than requiring task-specific architectures.
  - **Quick check question:** How does the IR format differ from traditional sequence labeling (e.g., BIO tagging) for entity extraction tasks?

## Architecture Onboarding

- **Component map:** BioC XML corpora (CafeteriaFCD recipes, CafeteriaSA abstracts) with entity annotations → IR pair converter → threshold analyzer → artificial IR generator → Training: Llama 3 8B Instruct (4-bit quantization) + LoRA adapters (query/value matrices) + SFT trainer → Inference: Two-stage pipeline—(1) NER instruction extracts food mentions, (2) NEL instruction links mentions to ontology URIs

- **Critical path:** 1. BioC XML parsing validates entity offsets and URI mappings 2. IR pair generation creates diverse instruction phrasings (1,000 GPT-3.5-generated variations) 3. Entity distribution analysis identifies underrepresented URIs 4. Artificial IR pairs synthesized for entities below threshold 5. LoRA fine-tuning with mixed general/domain data 6. Custom response parsers extract URIs from potentially malformed LLM outputs

- **Design tradeoffs:** Precision vs. recall (high precision 0.915-0.956 but lower recall 0.580-0.804 on real corpora due to NER error propagation), threshold sensitivity (150-mention threshold is heuristic), URI syntax burden (Hansard taxonomy achieves higher baseline performance than FoodOn/SNOMED-CT)

- **Failure signatures:** Hallucinated URIs (model generates plausible-looking but invalid ontology identifiers), NER propagation errors (missed entities in stage 1 cannot be linked in stage 2), syntactic degradation (few-shot baselines show deterioration with long URI examples), empty responses (non-meaningful outputs counted as errors)

- **First 3 experiments:** 1. Baseline replication: Run 0-shot, 1-shot, 5-shot prompting on Llama 3 8B Instruct with held-out CafeteriaFCD test fold 2. Threshold ablation: Vary the artificial IR threshold (50, 100, 150, 200 mentions) and measure F1 on rare entities 3. End-to-end vs. pipeline comparison: Train a single-stage model that takes raw text and outputs entity-URI pairs directly

## Open Questions the Paper Calls Out

- **Open Question 1:** Does retrieval-augmented generation (RAG) improve FoodSEM's ability to link food entities to unseen or newly added ontology entries? The current fine-tuned model can only link to entities present during training; evolving food ontologies require mechanisms to handle novel concepts.

- **Open Question 2:** What is the optimal entity mention threshold for generating artificial instruction-response pairs to balance entity coverage without overfitting? The 150-mention threshold was arbitrarily chosen; its effect on model generalization across ontologies with varying entity frequencies remains unknown.

- **Open Question 3:** Does multi-stage reasoning (NER followed by NEL) provide measurable benefits compared to joint NER+NEL training? The current architecture forces NEL to depend on NER output, but it is unclear whether error propagation harms performance or whether context sharing helps.

## Limitations

- The artificial IR pair generation process relies on specific thresholds (150 mentions) and sampling strategies that weren't fully detailed, potentially affecting model performance
- The two-stage pipeline architecture introduces error propagation from NER to NEL, with lower recall scores (0.580-0.804) on real corpora
- The IR format may constrain the model's ability to handle complex entity disambiguation cases requiring broader context
- The custom response parsers' handling of malformed outputs may introduce evaluation inconsistencies

## Confidence

**High Confidence:** The core claim that fine-tuning LLMs for food NEL outperforms few-shot prompting is well-supported by experimental results showing F1 scores of 98% versus 40% maximum for few-shot baselines.

**Medium Confidence:** The specific threshold of 150 mentions for generating artificial IR pairs appears reasonable but should be refined through sensitivity analysis as the paper notes.

**Low Confidence:** The exact impact of the instruction variation generation method (1,000 GPT-3.5-generated variations) on final performance is difficult to assess without access to the specific instructions used.

## Next Checks

1. **Threshold Sensitivity Analysis:** Systematically vary the artificial IR pair threshold (50, 100, 150, 200 mentions) across multiple training runs to quantify the relationship between synthetic data volume and performance on rare entities.

2. **End-to-End Pipeline Evaluation:** Train and evaluate a single-stage model that performs both NER and NEL simultaneously, then compare recall scores against the current two-stage pipeline to measure the exact impact of error propagation on the 20-40% recall gap.

3. **Ontology-Specific Ablation:** Conduct separate ablation studies for each ontology (FoodOn, SNOMED-CT, Hansard) by training models with only natural IR pairs, only artificial IR pairs, and mixed data to isolate which components drive the performance differences observed across ontologies.