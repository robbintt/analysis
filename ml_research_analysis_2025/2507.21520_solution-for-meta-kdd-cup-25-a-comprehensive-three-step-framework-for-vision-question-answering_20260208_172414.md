---
ver: rpa2
title: 'Solution for Meta KDD Cup''25: A Comprehensive Three-Step Framework for Vision
  Question Answering'
arxiv_id: '2507.21520'
source_url: https://arxiv.org/abs/2507.21520
tags:
- retrieval
- data
- image
- prompt
- question
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper describes a comprehensive three-step framework for Vision
  Question Answering (VQA) that addresses challenges in multi-modal understanding,
  retrieval-augmented generation, and multi-turn conversations. The proposed solution
  employs a unified approach using data augmentation, retrieval-augmented generation
  (RAG), reranking, and multi-task fine-tuning to enhance the performance of Vision
  Large Language Models (VLLMs).
---

# Solution for Meta KDD Cup'25: A Comprehensive Three-Step Framework for Vision Question Answering

## Quick Facts
- arXiv ID: 2507.21520
- Source URL: https://arxiv.org/abs/2507.21520
- Reference count: 16
- The solution achieved automatic evaluation rankings of 3rd, 3rd, and 1st in three tasks, and secured second place in Task 3 after human evaluation.

## Executive Summary
This paper presents a comprehensive three-step framework for Vision Question Answering (VQA) that addresses challenges in multi-modal understanding, retrieval-augmented generation, and multi-turn conversations. The proposed solution employs a unified approach using data augmentation, retrieval-augmented generation (RAG), reranking, and multi-task fine-tuning to enhance the performance of Vision Large Language Models (VLLMs). The framework achieves significant improvements in handling complex queries and multi-source retrieval tasks, as demonstrated by competitive rankings in the Meta KDD Cup'25 challenge. Specifically, the solution achieved automatic evaluation rankings of 3rd, 3rd, and 1st in three tasks, and secured second place in Task 3 after human evaluation.

## Method Summary
The framework addresses three VQA tasks using a unified approach: Task 1 (single-source retrieval with mock KG), Task 2 (multi-source KG+web retrieval), and Task 3 (multi-turn conversations). The core pipeline consists of three steps: (1) multi-query generation with temperature=0.8 for diversity, (2) listwise reranking to select top 10 results, and (3) fine-tuned Llama-3.2-11B-Vision-Instruct for answer generation. Key innovations include hallucination-filtered data augmentation with refusal labels, multi-query retrieval to improve coverage, and multi-task fine-tuning with LoRA (rank=64, alpha=128) for unified inference. The approach handles multi-turn conversations by looping back to step 1 for follow-up questions while maintaining context.

## Key Results
- Automatic evaluation rankings: 3rd, 3rd, and 1st in Tasks 1, 2, and 3 respectively
- Human evaluation: 2nd place in Task 3 after top 10 teams' results were corrected
- Hallucination-filtered data augmentation improved Task 3 score from 0.0700 to 0.1322 (88% relative improvement)
- Multi-query retrieval with reranking further improved scores from 0.1322 to 0.1505
- Reducing refusal data proportion from optimal to slightly lower levels improved score from 0.1505 to 0.1755

## Why This Works (Mechanism)

### Mechanism 1: Hallucination-Filtered Data Augmentation with Refusal Labels
Augmenting training data with explicit hallucination detection and "I don't know" labels reduces incorrect answers and improves truthfulness scores. The pipeline uses Llama-3.2 to generate initial answers, employs GPT-4o mini to verify against ground truth, sets hallucinated samples to "I don't know", and generates n=10 paraphrased labels for verified samples to increase training diversity.

### Mechanism 2: Multi-Query Retrieval with Learned Listwise Reranking
Generating multiple diverse queries and learning to rerank retrieval results improves the proportion of high-quality context in the model input. The approach generates multiple retrieval queries with temperature=0.8 for diversity, calls retrieval tool for each query, retains the result group with largest number of items, and uses a fine-tuned model to output ranked list [x, xx, xxx...] of relevant items, keeping top 10.

### Mechanism 3: Multi-Task Fine-Tuning for Unified Inference
Training a single model on retrieval query generation, reranking, and QA tasks enables efficient unified inference under competition constraints. The approach constructs three training data types (query generation, reranking True/False labels via GPT-4o, QA with refusal labels), fine-tunes with LoRA (rank=64, alpha=128), and uses task-specific prompts during inference.

## Foundational Learning

- Concept: **Retrieval-Augmented Generation (RAG) for Multi-Modal Systems**
  - Why needed here: The entire framework is built on MM-RAG; understanding how retrieval augments VLLM knowledge is prerequisite.
  - Quick check question: Given an image of an unfamiliar product and a question about its specifications, how would a RAG system differ from a standalone VLLM in formulating an answer?

- Concept: **Parameter-Efficient Fine-Tuning (LoRA)**
  - Why needed here: Competition constraints required efficient training; LoRA (rank=64, alpha=128) was used for all fine-tuning.
  - Quick check question: If you have a 11B parameter model and LoRA rank=64 applied to attention layers only, approximately what fraction of total parameters are trainable?

- Concept: **Calibrated Refusal / Uncertainty Estimation**
  - Why needed here: The competition scoring (-1.0 for incorrect, 0.0 for "I don't know") makes refusal learning critical for score optimization.
  - Quick check question: In a system where false positives cost -1.0 and abstentions cost 0.0, what accuracy threshold makes refusal preferable to answering?

## Architecture Onboarding

- Component map: Input (Image + Question + History) -> Multi-Query Generation (temp=0.8) -> Official Retrieval Tool -> Listwise Reranking -> Top 10 Results -> Fine-tuned Llama-3.2-11B-Vision-Instruct -> Answer

- Critical path: The 30-second inference constraint makes vLLM acceleration essential; retrieval query generation uses temp=0.8 (higher latency) while final QA uses temp=0.0.

- Design tradeoffs:
  - vLLM doesn't support mLLama's LoRA weights directly → solved by multi-task fine-tuning instead of task-specific adapters
  - Longer input context vs. inference time → limited to top 10 reranked results (max input length 8192)
  - Refusal data improves precision but may reduce recall → they tuned refusal proportion in final iteration

- Failure signatures:
  - Low human evaluation improvement relative to other teams suggests the model may be under-confident on borderline cases
  - If retrieval returns empty results consistently, the model falls back to parametric knowledge (higher hallucination risk)

- First 3 experiments:
  1. **Baseline**: Fine-tune Llama-3.2-11B-Vision on original data without retrieval → expect score ~0.01
  2. **Add RAG**: Integrate official retrieval tool with top-10 results → expect score ~0.02-0.07
  3. **Add refusal calibration**: Construct "I don't know" labels for inconsistent predictions → expect ~2x score improvement

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can automatic evaluation metrics for VLLMs be improved to better correlate with human judgment and reduce the "somewhat uncertain" nature of current evaluations?
- Basis in paper: The authors state "Since the automatic evaluation of VLLMs can be somewhat uncertain, the organizers conducted a human evaluation for the top 10 teams, correcting test samples that were judged incorrect by the automatic evaluation but actually should be considered correct."
- Why unresolved: The paper shows that human evaluation scores differ significantly from automatic evaluation, and their team's improvement from human evaluation was smaller than others', suggesting evaluation methodology remains unreliable.
- What evidence would resolve it: A systematic study comparing multiple automatic evaluation approaches against human judgments across diverse VQA tasks, with correlation coefficients and error analysis.

### Open Question 2
- Question: What is the reliability and error propagation rate when using LLMs (GPT-4o mini) to verify and augment training labels for other VLLMs?
- Basis in paper: The data augmentation pipeline relies on GPT-4o mini to verify answers against ground truth and generate similar labels, but no analysis is provided on the accuracy of this verification step or its impact on training quality.
- Why unresolved: The paper does not report the verification accuracy of GPT-4o mini, nor does it analyze how errors in this verification might propagate through the training process.
- What evidence would resolve it: A controlled experiment measuring GPT-4o mini's verification accuracy against human judgment, plus ablation studies on training performance with noisy verification.

### Open Question 3
- Question: Can a unified model architecture achieve comparable performance across different retrieval source configurations (single-source KG, multi-source web, multi-turn) without task-specific adaptations?
- Basis in paper: The authors state "Due to differences in retrieval sources, we developed distinct solutions for Task 1 and Task 2/3, each with its own focus."
- Why unresolved: While Task 2 and Task 3 share a unified model through multi-task fine-tuning, Task 1 requires a separate solution, suggesting current methods may not fully generalize across retrieval configurations.
- What evidence would resolve it: A single model trained on all three tasks simultaneously, with performance comparisons against the separate-task approach.

### Open Question 4
- Question: What is the optimal proportion of refusal training data ("I don't know" labels) to maximize truthfulness scores while minimizing excessive conservatism?
- Basis in paper: The ablation study shows that "slightly reducing the proportion of refusal data in the training set led to a score of 0.1755" from 0.1505, indicating this is a tuned hyperparameter without systematic analysis.
- Why unresolved: The paper demonstrates that refusal data proportion matters but does not provide principled guidance on determining the optimal ratio.
- What evidence would resolve it: A systematic grid search over refusal data proportions with analysis of the precision-recall tradeoff between avoiding hallucinations and providing helpful answers.

## Limitations

- The paper presents a well-structured framework but leaves several critical implementation details underspecified, particularly the official retrieval tool API interface and response format.
- The exact number of retrieval queries generated per sample is described only as "several times" without quantification, making reproducibility challenging.
- The reranking model architecture and training procedure are incompletely described, requiring significant assumptions for faithful reproduction.

## Confidence

- **High Confidence**: The core three-step pipeline architecture (multi-query generation → reranking → QA) and the general approach of hallucination-aware data augmentation with refusal labels are well-supported by experimental results.
- **Medium Confidence**: The specific hyperparameter choices (LoRA rank=64, alpha=128, learning rates, temperature settings) appear reasonable but may not generalize optimally across different datasets.
- **Low Confidence**: The exact implementation details of the reranking model, the hallucination filtering mechanism, and the specific parameters of the official retrieval tool are insufficiently specified for reliable reproduction.

## Next Checks

1. **Retrieval Quality Validation**: Implement a controlled experiment comparing single-query vs. multi-query retrieval (with varying temperature settings) on a subset of the validation set to quantify the marginal benefit of query diversity and determine optimal query count.

2. **Hallucination Detection Reliability**: Test GPT-4o mini's hallucination detection accuracy on a held-out validation set by comparing its verification decisions against human annotations to establish false positive/negative rates and determine appropriate confidence thresholds.

3. **Refusal Label Impact Analysis**: Conduct ablation studies varying the proportion of refusal labels in training data (0%, 25%, 50%, 75%, 100%) to empirically determine the optimal refusal rate that maximizes truthfulness score while maintaining adequate recall.