---
ver: rpa2
title: A Structured Literature Review on Traditional Approaches in Current Natural
  Language Processing
arxiv_id: '2505.12970'
source_url: https://arxiv.org/abs/2505.12970
tags:
- text
- traditional
- approaches
- extraction
- techniques
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study investigates whether traditional natural language processing
  (NLP) techniques remain relevant in the era of large language models (LLMs). The
  authors conduct a structured literature review focusing on five key NLP tasks: classification,
  information extraction, relation extraction, text simplification, and text summarization.'
---

# A Structured Literature Review on Traditional Approaches in Current Natural Language Processing

## Quick Facts
- arXiv ID: 2505.12970
- Source URL: https://arxiv.org/abs/2505.12970
- Reference count: 40
- Traditional NLP methods remain widely used in 2023 as baselines, hybrid components, and core approaches

## Executive Summary
This structured literature review investigates whether traditional NLP techniques retain relevance alongside large language models. The authors analyze 119 2023 ACM Digital Library publications across five NLP tasks, finding that traditional approaches (rule-based methods, SVMs, Naive Bayes, decision trees, extractive summarization) are actively used in 23.5% of surveyed papers. These methods persist due to advantages in reproducibility, resource efficiency, and explainability, particularly in domains where hallucinations are unacceptable or computational resources are constrained.

## Method Summary
The study conducts a structured literature review using exact-match title searches in the ACM Digital Library for 2023 publications across five NLP tasks. Text simplification required abstract searches due to low title retrieval. The single reviewer screened 119 papers, identifying 28 that incorporated traditional techniques. Classification used predefined criteria (reproducibility, efficiency, documentation) to categorize usage as pipeline components, baselines/comparisons, or core methods.

## Key Results
- Traditional NLP methods appear in 28 of 119 surveyed 2023 papers (23.5% prevalence)
- Extractive summarization remains dominant in hallucination-sensitive domains like legal text processing
- Traditional methods serve as comparison baselines in 10 papers and pipeline components in 7 papers
- Efficiency advantages persist, with IEA data showing ChatGPT API calls require 10x more energy than standard search queries

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Traditional NLP methods persist because they provide deterministic outputs essential for reproducible evaluation and auditable processing pipelines.
- Mechanism: Rule-based systems, SVMs, and extractive summarization algorithms produce identical outputs for identical inputs. LLMs, by contrast, exhibit stochasticity even with constrained temperature settings, which "can lead to problems for evaluation or comparative studies" (Page 3).
- Core assumption: Reproducibility is a non-negotiable requirement in certain domains (legal, scientific benchmarking, regulatory).
- Evidence anchors:
  - [abstract] "many disadvantages still remain" with LLMs including "missing reproducibility"
  - [section 3, Page 3] Explicitly defines reproducibility as a key characteristic: "approaches that can produce the exact same output for a given input"
  - [corpus] Weak direct support; neighbor papers focus on GenAI applications rather than traditional method comparisons

### Mechanism 2
- Claim: Traditional methods remain competitive in resource-constrained environments due to orders-of-magnitude lower computational and energy costs.
- Mechanism: Traditional models execute on CPU without dedicated GPU infrastructure. The paper cites IEA data: "one API call to ChatGPT requires 2.9 Wh per request" versus 0.3 Wh for a standard search query—a ~10x differential. For high-volume or edge deployments, this compounds rapidly.
- Core assumption: Infrastructure costs (monetary, energy, hardware availability) are binding constraints in the target deployment context.
- Evidence anchors:
  - [section 3, Page 3-4] Lists efficiency as a defining characteristic: "execution time" and "resources" including "monetary costs" and "hardware"
  - [section 6, Page 8] Notes traditional models are "more efficient due to their simple nature"
  - [corpus] "Information Security Based on LLM Approaches" notes "traditional protection means are difficult to cope with complex and changing threats"—suggesting traditional methods may fail where LLMs succeed, indicating context-dependence

### Mechanism 3
- Claim: Extractive summarization is favored in hallucination-sensitive domains because it guarantees all output content originates from source text.
- Mechanism: Extractive methods (TextRank, LexRank, lead-3) select and rank existing sentences without generation. Abstractive and LLM-based approaches synthesize new text, introducing hallucination risk. In legal text processing, "an extractive model was used in order to prevent hallucinations from appearing" (Page 8).
- Core assumption: The cost of a hallucination (legal liability, medical error) exceeds the benefit of more fluent or compressed output.
- Evidence anchors:
  - [section 6, Page 8] "extractive approaches...cannot include hallucinations or even sentence constructions that do not appear in the original texts"
  - [section 5, Page 8] Licari et al. (2023) used extractive systems "due to the strict nature of the domain, which comprises legal texts"
  - [corpus] No direct corpus support for this specific mechanism

## Foundational Learning

- Concept: **Extractive vs. Abstractive Summarization**
  - Why needed here: The paper's definition of "traditional" for summarization hinges on this distinction. Extractive = selecting sentences; abstractive = generating new text.
  - Quick check question: Given a news article, does your system copy the lead sentence verbatim (extractive) or paraphrase it in new words (abstractive)?

- Concept: **TF-IDF (Term Frequency-Inverse Document Frequency)**
  - Why needed here: Cited as a foundational traditional technique for both classification and summarization (Page 4, 7). Understanding it is prerequisite to interpreting why these methods remain baselines.
  - Quick check question: Why would a word appearing in every document receive a low TF-IDF score?

- Concept: **Reproducibility vs. Replicability**
  - Why needed here: The paper's critique of LLMs centers on reproducibility—identical outputs for identical inputs. Understanding this distinction clarifies why temperature parameters don't fully solve the problem.
  - Quick check question: If you set an LLM's temperature to 0, is the output guaranteed to be reproducible across different model versions or hardware?

## Architecture Onboarding

- Component map: Input Text → Preprocessing (tokenization, TF-IDF) → Traditional Model (SVM/Decision Tree/Rule-based/Extractive) → Output → Optional: Neural component (BERT embedding, etc.) in hybrid systems
- Critical path: Start with a baseline traditional model (SVM for classification, TextRank for summarization) before introducing neural components. The SLR found 28 of 119 papers (23.5%) still incorporate traditional methods, often as baselines or pipeline components.
- Design tradeoffs:
  - Accuracy vs. Explainability: SVMs and decision trees provide interpretable decision boundaries; neural models often do not.
  - Fluency vs. Faithfulness: Abstractive outputs are more fluent but risk hallucination; extractive is faithful but may be choppy.
  - Development speed vs. Resource efficiency: LLM APIs are fast to integrate; traditional pipelines require feature engineering but cost less at scale.
- Failure signatures:
  - LLM produces plausible but fabricated citations or facts → Switch to extractive or retrieval-augmented approach for that component.
  - SVM baseline outperforms neural model on small datasets → Data scarcity favors traditional methods; consider hybrid or more data.
  - Rule-based extraction misses edge cases → Hybridize with ML classifier for ambiguous instances.
- First 3 experiments:
  1. **Baseline comparison**: Implement SVM with TF-IDF features vs. BERT-based classifier on your classification task. Compare accuracy, inference time, and interpretability.
  2. **Extractive summarization quality check**: Run TextRank and LexRank on domain documents. Manually evaluate whether output meets faithfulness requirements before attempting abstractive approaches.
  3. **Hybrid pipeline prototype**: For information extraction, combine rule-based patterns (high precision) with a neural classifier (high recall). Measure precision/recall tradeoffs at each stage.

## Open Questions the Paper Calls Out

- Question: What specific criteria and conditions suggest the use of traditional approaches in NLP, and what are the characterized example scenarios for their application?
- Basis in paper: [explicit] The authors state in the Conclusion: "In our future work, we will further look at traditional models and work on criteria and conditions that suggest the use of traditional approaches in NLP. We will also identify and characterize example scenarios in which their use should be considered."
- Why unresolved: While the review identifies *that* traditional methods are used for reasons like efficiency and explainability, it does not formalize a decision framework or specific criteria defining exactly *when* a traditional model should be selected over an LLM.
- What evidence would resolve it: A proposed theoretical framework or decision matrix that maps task constraints (e.g., resource availability, need for reproducibility) to appropriate traditional techniques, validated by case studies.

- Question: How does the exclusion of "middle-ground" approaches, such as static word embeddings (e.g., word2vec), affect the understanding of the current NLP landscape?
- Basis in paper: [inferred] In the Limitations section, the authors note they "did not offer a discussion on the middle-ground between traditional models and modern deep learning approaches, which could be roughly seen as early embeddings spaces such as word2vec."
- Why unresolved: The binary classification of "traditional" vs. "modern/LLM" may overlook efficient, non-generative neural methods that are still widely in use but do not fit the strict definition of either category used in the paper.
- What evidence would resolve it: A follow-up review specifically categorizing and quantifying the usage of non-transformer neural methods (like word2vec or LSTMs) in recent publications.

- Question: Does the reliance on the ACM Digital Library skew the quantitative prevalence of traditional methods compared to a broader multi-database analysis?
- Basis in paper: [inferred] The methodology restricts the search to the ACM Digital Library to ensure peer-review quality, acknowledging that "an over-arching view across the whole NLP field is not possible" and other databases like Google Scholar yield roughly ten times as many papers.
- Why unresolved: It remains unclear if the 23.5% prevalence of traditional methods found in ACM is representative of the global research output, or if venues indexed in other databases (like arXiv or IEEE) favor different types of methodologies.
- What evidence would resolve it: A comparative study applying the same search queries and inclusion criteria to the ACL Anthology, IEEE Xplore, and arXiv to compare the ratios of traditional vs. modern usage.

## Limitations
- Single-reviewer screening protocol lacks reported inter-rater reliability, introducing potential classification inconsistencies
- Exact-match title searches in ACM Digital Library may undercount relevant papers or miss hybrid approaches
- Definition of "traditional" remains somewhat subjective despite operational criteria, particularly for edge cases

## Confidence

- **High Confidence**: Traditional methods remain in active use as baselines and pipeline components (supported by explicit counts: 28/119 papers incorporating traditional approaches)
- **Medium Confidence**: Efficiency advantages of traditional methods are context-dependent and supported by cited IEA energy consumption data
- **Medium Confidence**: Hallucination avoidance drives extractive summarization preference in legal domains, though direct corpus evidence is limited

## Next Checks
1. Replicate the ACM Digital Library search protocol to verify retrieval counts and identify potential coverage gaps
2. Screen 10 randomly selected papers from the corpus using the author's traditional method criteria to assess inter-rater consistency
3. Compare the 28 identified papers to their citation contexts to determine if traditional methods are truly "core" vs. superficial baseline comparisons