---
ver: rpa2
title: 'Bhaasha, Bhasa, Zaban: A Survey for Low-Resourced Languages in South Asia
  -- Current Stage and Challenges'
arxiv_id: '2509.11570'
source_url: https://arxiv.org/abs/2509.11570
tags:
- languages
- language
- linguistics
- association
- computational
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This survey comprehensively examines the current state of NLP research
  for low-resourced languages in South Asia, identifying critical gaps in data, models,
  and evaluation benchmarks. While a few major languages like Hindi and Bengali have
  relatively more resources, most South Asian languages suffer from severe underrepresentation
  in computational datasets and language models.
---

# Bhaasha, Bhasa, Zaban: A Survey for Low-Resourced Languages in South Asia -- Current Stage and Challenges

## Quick Facts
- **arXiv ID**: 2509.11570
- **Source URL**: https://arxiv.org/abs/2509.11570
- **Reference count**: 40
- **Primary result**: Comprehensive survey of NLP research for low-resourced South Asian languages, cataloguing current resources and identifying critical gaps in data, models, and evaluation benchmarks

## Executive Summary
This survey systematically examines the current state of NLP research for low-resourced languages in South Asia, revealing a stark disparity between major languages like Hindi and Bengali versus the vast majority of regional languages. Through analysis of 188 papers from over 1000 initial candidates, the authors identify critical challenges including code-mixing, script-specific tokenization issues, and lack of culturally relevant evaluation frameworks. The work emphasizes that while some progress exists for major Indo-Aryan languages, most South Asian languages suffer from severe underrepresentation in computational datasets and language models, necessitating targeted interventions in data curation, model adaptation, and benchmark development.

## Method Summary
The survey employed systematic literature review methodology, searching academic databases (ACL Anthology, Semantic Scholar, Google Scholar) with keyword combinations targeting South Asian languages, regions, and NLP tasks. Papers were filtered for publication year >= 2020, neural/Transformer focus, and languages with >=1 million speakers. The authors structured annotation of metadata and used inductive coding to identify qualitative themes around challenges like transliteration inconsistencies and dialectal variation. Resource categorization was compiled into tables detailing datasets, models, and benchmarks, with specific emphasis on identifying gaps for low-resource languages.

## Key Results
- Major South Asian languages (Hindi, Bengali) have relatively more resources, while most languages face severe data scarcity
- Code-mixing and transliteration present significant challenges for standard NLP pipelines
- Script-aware tokenization and parameter-efficient fine-tuning show promise for addressing resource constraints
- Existing evaluation benchmarks suffer from cultural mismatch when translated from English

## Why This Works (Mechanism)

### Mechanism 1
Cross-lingual transfer from linguistically related languages improves performance on low-resource South Asian languages. Shared scripts and morphological patterns between related languages (e.g., Hindi-Marathi, Bengali-Tamil-Malayalam) enable knowledge transfer through aligned subword embeddings and shared grammatical structures. Core assumption: Linguistic proximity translates to transferable representations. Evidence anchors: Studies found jointly trained NER models on multilingual corpora outperformed monolingual ones for shared script and grammar; IndicBART and IndicTrans2 show pre-training on large multilingual corpora of related languages significantly improves translation. Break condition: Transfer degrades when languages are phylogenetically distant (e.g., Tibeto-Burman languages show minimal benefit from Indo-Aryan pre-training).

### Mechanism 2
Script-aware tokenization mitigates over-segmentation in morphologically rich South Asian languages. Overlap BPE and custom tokenizers trained on native + code-mixed text preserve morpheme boundaries that standard BERT/mBERT tokenizers fragment, improving downstream task performance. Core assumption: Subword token quality directly impacts representation learning for agglutinative languages. Evidence anchors: The Overlap BPE method improves tokenization consistency on subword-level processing for orthographically similar languages; Assamese tokenization complexity demonstrates the need for script-aware approaches. Break condition: Extremely low-resource languages lack sufficient training data for custom tokenizers, causing vocabulary fragmentation.

### Mechanism 3
Parameter-efficient fine-tuning (PEFT) enables adaptation to South Asian languages under compute and data constraints. LoRA-based adaptation modifies only rank-decomposed attention weights, reducing memory/compute requirements while capturing dialectal variations and domain-specific patterns. Core assumption: Low-rank parameter updates suffice for language adaptation without catastrophic forgetting. Evidence anchors: LoRA achieves dialectal normalization and translation across South Asian dialects with limited supervision; adapter-based methods offer modular, language-specific adaptation. Break condition: LoRA struggles with syntactically rich tasks and shows uneven benefits across dialectal variations.

## Foundational Learning

- **Morphological Agglutination**: Why needed here: Dravidian and many Indo-Aryan languages form complex words through suffix concatenation, causing tokenization failures if not understood. Quick check question: Can you explain why "nadanthirukirathu" (has happened) in Tamil fragments into ["nada" + "nthu" + "irukirathu"], and how this affects NER vs. MT?

- **Diglossia**: Why needed here: Literary vs. spoken forms (e.g., Tamil, Bengali) create evaluation gapsâ€”models trained on formal text fail on social media. Quick check question: Given the IruMozhi dataset shows Tamil diglossia classification, how would you design a training corpus for sentiment analysis?

- **Code-Mixing and Transliteration**: Why needed here: Hinglish/Tanglish dominate South Asian digital communication but standard models treat Romanized text as out-of-domain. Quick check question: If Hindi "main theek hoon" can be romanized multiple ways, what tokenization strategy prevents vocabulary explosion?

## Architecture Onboarding

- **Component map**: Raw South Asian Text -> Script Detection & Normalization -> Custom Tokenizer (Overlap BPE / FastText) -> Pre-trained Encoder (IndicBERT, IndicTrans2, or multilingual base) -> PEFT Adapter (LoRA / Task-specific adapter) -> Task Head (Classification / Generation / Retrieval) -> Culturally-Aligned Evaluation Metrics

- **Critical path**: Tokenizer quality -> script fidelity -> representation alignment -> evaluation validity

- **Design tradeoffs**:
  1. Multilingual (IndicBERT) vs. monolingual (MahaBERT): Broader coverage vs. higher single-language accuracy
  2. Native script vs. transliteration: Preserves authenticity vs. improves cross-lingual transfer
  3. Full fine-tuning vs. PEFT: Better performance vs. compute efficiency and modularity

- **Failure signatures**:
  1. Character-level fragmentation in BERT tokenizers (e.g., Assamese alveolar stops split incorrectly)
  2. Cultural misalignment in translated benchmarks (e.g., MMLU questions on US law evaluating Hindi models)
  3. Dialect bias: Models performing well on standard varieties but failing on regional dialects
  4. Catastrophic forgetting during sequential language adaptation

- **First 3 experiments**:
  1. Baseline evaluation: Test mBERT vs. IndicBERT on your target language's available benchmarks (IndicGLUE, BELEBELE) to quantify the resource gap
  2. Tokenizer ablation: Compare standard WordPiece vs. Overlap BPE tokenization on a morphologically rich task (NER or sentiment analysis) with a Dravidian language
  3. PEFT pilot: Fine-tune a multilingual LLM (e.g., Llama 2) using LoRA on a small domain-specific corpus (e.g., healthcare queries in code-mixed Hindi) and evaluate on cultural relevance vs. translation quality

## Open Questions the Paper Calls Out

- **Open Question 1**: How can NLP systems be effectively adapted for code-mixing in under-resourced language pairs (e.g., Assamese-Bodo) and trilingual contexts, beyond the dominant English-centric pairs? Basis in paper: Appendix A.2 explicitly encourages future work to expand code-mixing research to less-resourced combinations like Assamese-Bodo and Hindi-Magahi. Why unresolved: Current corpora focus heavily on English-Hindi or English-Tamil; sociolinguistic switching patterns in regional language pairs remain understudied. What evidence would resolve it: The creation of annotated corpora and robust models capable of processing code-mixed text in non-English regional pairs with high accuracy.

- **Open Question 2**: How can culturally grounded evaluation benchmarks be designed to assess NLP performance on South Asian languages while accounting for linguistic nuances like diglossia, agglutination, and script multiplicity? Basis in paper: The authors explicitly state in Section 4 and Appendix A.2 that existing translated benchmarks (e.g., Global-MMLU) introduce cultural mismatch and fail to capture regional complexity. Why unresolved: Standard metrics (BLEU, F1) are English-centric and insensitive to local variations; translated test sets often lack cultural relevance. What evidence would resolve it: The release of region-specific benchmarks for tasks like reasoning or QA that correlate better with human judgment than translated English equivalents.

- **Open Question 3**: Can script-agnostic modeling strategies, such as shared subword vocabularies across Devanagari, Perso-Arabic, and Roman scripts, overcome the tokenization fragmentation found in current multilingual LLMs? Basis in paper: Appendix A.2 identifies script-agnostic modeling as a future research priority to address the script-specific tokenization bottlenecks detailed in Section 3.2. Why unresolved: Current tokenizers (BPE, WordPiece) often fragment morphologically rich words and struggle with non-standardized romanization, limiting cross-lingual transfer. What evidence would resolve it: A tokenizer or model architecture that reduces subword fragmentation for South Asian languages without sacrificing semantic fidelity or requiring prohibitive vocabulary expansion.

## Limitations
- Academic database coverage systematically underrepresents extremely low-resource languages and regional NLP communities
- Inductive coding for challenges lacks explicit inter-rater reliability metrics
- Focus on Transformer-era research may overlook insights from earlier statistical NLP work

## Confidence
- **High Confidence**: The general resource gap between major and minor South Asian languages is well-established through comprehensive dataset enumeration
- **Medium Confidence**: Claims about cross-lingual transfer benefits rely on cited empirical studies but assume generalizability across all language pairs
- **Low Confidence**: The assertion that LoRA-based PEFT universally enables adaptation under compute constraints is based on limited case studies without systematic evaluation

## Next Checks
1. **Database Coverage Audit**: Replicate the survey's search methodology across local South Asian NLP workshop proceedings, arXiv preprints from regional institutions, and citation chasing from key survey papers
2. **Tokenization Ablation Study**: Implement Overlap BPE vs. standard WordPiece tokenization on morphologically rich South Asian languages (Tamil, Marathi, Assamese) for NER, measuring vocabulary size, OOV rates, and downstream F1 scores
3. **Cross-Lingual Transfer Validation**: Train and evaluate multilingual models (IndicBERT, mBERT) on Hindi-Marathi and Bengali-Tamil pairs to test transferability, controlling for script overlap, vocabulary overlap, and phylogenetic distance