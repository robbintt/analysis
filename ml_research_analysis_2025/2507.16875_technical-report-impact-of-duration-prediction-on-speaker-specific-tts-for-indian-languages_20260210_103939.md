---
ver: rpa2
title: 'Technical report: Impact of Duration Prediction on Speaker-specific TTS for
  Indian Languages'
arxiv_id: '2507.16875'
source_url: https://arxiv.org/abs/2507.16875
tags:
- duration
- speech
- audio
- languages
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study investigates the impact of duration prediction strategies
  on speaker-specific text-to-speech synthesis for Indian languages. Two approaches
  are compared: an infilling-style duration predictor and a speaker-prompted duration
  predictor.'
---

# Technical report: Impact of Duration Prediction on Speaker-specific TTS for Indian Languages

## Quick Facts
- arXiv ID: 2507.16875
- Source URL: https://arxiv.org/abs/2507.16875
- Reference count: 33
- Key outcome: Speaker-prompted duration prediction improves Tamil speaker similarity (Sim-o: 0.6925 vs 0.6833) and intelligibility (WER: 0.2622 vs 0.2806), while generally yielding better speaker similarity across languages at the cost of some intelligibility.

## Executive Summary
This study investigates how duration prediction strategies impact speaker-specific text-to-speech synthesis for Indian languages. The authors compare two approaches: an infilling-style duration predictor and a speaker-prompted duration predictor. Experiments reveal language-dependent trade-offs, with Tamil showing improved intelligibility and speaker similarity using the speaker-prompted predictor, while other languages exhibit varying patterns. The findings highlight the importance of selecting duration prediction strategies based on task-specific priorities between intelligibility and speaker similarity.

## Method Summary
The paper trains a non-autoregressive CNF-based speech model using ~2000 hours of Indian language data across five languages. Two duration predictors are compared: an infilling-style predictor using forced alignments, and a speaker-prompted predictor using 3-second mel spectrogram segments with cross-attention. The audio model is a 103M-parameter CNF Transformer trained with flow-matching, while predictors are 51M (infilling) and 84M (speaker-prompted) parameter Transformers. Evaluation uses WER for intelligibility and Sim-o for speaker similarity on speech infilling tasks with 50% sentence masking.

## Key Results
- Tamil: Speaker-prompted predictor improves both WER (0.2622 vs 0.2806) and Sim-o (0.6925 vs 0.6833) over infilling predictor
- Across languages: Speaker-prompted predictor generally yields better speaker similarity, while infilling predictor enhances intelligibility
- Human evaluations (QMOS and SMOS) confirm trends for Hindi and Tamil
- Language-specific trade-offs observed, with Marathi showing significant degradation with speaker-prompted predictor

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Speaker-prompted duration prediction improves speaker similarity and intelligibility for certain Indian languages by extracting speaker-specific prosodic cues directly from reference audio rather than relying on external forced alignments.
- **Mechanism:** A 3-second mel spectrogram segment is sampled from context audio and processed via cross-attention from text tokens to speech frames. This enables implicit learning of prosodic patterns and speaker-specific durations without hard alignment constraints at inference time.
- **Core assumption:** Speaker-specific prosody can be captured from short reference audio and transferred to duration predictions more reliably than from forced alignments in low-resource settings.
- **Evidence anchors:**
  - [abstract] "speaker-prompted predictors better preserve speaker characteristics in others"
  - [Section 3.4.2] "we used a 3-second mel spectrogram segment xp, randomly sampled from the context mel spectrogram xctx, together with the text sequence c as input"
  - [corpus] Limited direct corpus support for this specific mechanism in Indian languages
- **Break condition:** If forced alignment quality improves significantly (e.g., through language-specific ASR), the relative advantage of speaker-prompting may diminish.

### Mechanism 2
- **Claim:** Continuous Normalizing Flow (CNF) with optimal transport paths enables efficient training of non-autoregressive speech infilling models on limited Indian language data.
- **Mechanism:** The model learns a vector field that transforms a simple prior distribution to the complex distribution of missing speech segments, conditioned on text and surrounding audio. Optimal transport paths ensure constant-speed trajectories, improving training stability.
- **Core assumption:** The flow-matching objective with masked training generalizes to zero-shot speaker-specific generation.
- **Evidence anchors:**
  - [abstract] "We train a non-autoregressive Continuous Normalizing Flow (CNF) based speech model using publicly available Indian language data"
  - [Section 3.3] "we assign a weight of 0.9 to the loss computed on the masked frames and a weight of 0.1 to the loss on the unmasked (context) frames"
  - [corpus] Related work (Qwen3-TTS, Ojibwe/Mi'kmaq/Maliseet study) confirms flow matching for multilingual TTS
- **Break condition:** If data scales beyond ~10,000 hours, autoregressive or fully latent approaches may become competitive.

### Mechanism 3
- **Claim:** Duration prediction strategy choice creates language-dependent trade-offs between intelligibility and speaker similarity.
- **Mechanism:** Infilling predictors condition on text and duration context, optimizing for phoneme-level timing accuracy. Speaker-prompted predictors inject speaker identity earlier, trading some timing precision for prosodic naturalness.
- **Core assumption:** Language-specific prosodic structures (e.g., Tamil's agglutinative morphology vs. Hindi's stress patterns) interact differently with duration conditioning.
- **Evidence anchors:**
  - [abstract] "infilling based predictors improve intelligibility in some languages, while speaker-prompted predictors better preserve speaker characteristics in others"
  - [Section 5.1/Table 3] Tamil: infilling WER 0.2806 vs. PFlow 0.2622; Hindi: infilling WER 0.1930 vs. PFlow 0.1998
  - [corpus] Align2Speak and A2TTS papers corroborate duration modeling challenges in low-resource TTS
- **Break condition:** If a unified predictor jointly models both objectives, the trade-off may be avoidable.

## Foundational Learning

- **Concept: Continuous Normalizing Flows (CNF) and Flow Matching**
  - **Why needed here:** The audio model is built on CNF with flow-matching training. Understanding how vector fields transform distributions is essential for debugging convergence issues.
  - **Quick check question:** Can you explain why optimal transport paths are preferred over diffusion paths for this architecture?

- **Concept: Cross-Attention for Multimodal Conditioning**
  - **Why needed here:** The speaker-prompted duration predictor uses cross-attention to fuse text and speech modalities. Misconfigured attention can cause prosody leakage or identity loss.
  - **Quick check question:** In cross-attention from text to speech frames, which modality provides the query and which provides the key/value?

- **Concept: Duration Modeling in TTS Pipelines**
  - **Why needed here:** Duration prediction determines prosody and intelligibility trade-offs. Understanding alignment extraction, length regulation, and their failure modes is critical.
  - **Quick check question:** Why might forced alignments fail for Indian languages with high dialectal variation?

## Architecture Onboarding

- **Component map:**
  - Text normalization → phoneme/character sequence
  - Duration prediction (infilling or speaker-prompted) → per-token durations
  - Frame-level transcript expansion
  - CNF audio model infilling → mel spectrogram
  - Vocoder → waveform

- **Critical path:**
  1. Text normalization → phoneme/character sequence
  2. Duration prediction (infilling or speaker-prompted) → per-token durations
  3. Frame-level transcript expansion
  4. CNF audio model infilling → mel spectrogram
  5. Vocoder → waveform

- **Design tradeoffs:**
  - Infilling predictor: Better for intelligibility-critical applications; requires reliable forced alignments during training
  - Speaker-prompted predictor: Better for voice cloning fidelity; requires 2+ seconds of reference audio at inference
  - Training data capping at 1,000 frames trades long-utterance coherence for training efficiency

- **Failure signatures:**
  - High WER with good Sim-o → infilling predictor may be over-smoothing prosody
  - Low Sim-o with acceptable WER → speaker-prompted predictor may be ignoring reference audio
  - Language-specific collapse (e.g., Marathi WER spike with PFlow) → cross-attention may be misaligned for certain prosodic structures

- **First 3 experiments:**
  1. **Ablate duration predictor choice per language:** Run both predictors on held-out sets for each target language; compare WER and Sim-o to identify language-specific optima.
  2. **Vary speaker prompt length:** Test 1s, 2s, 3s, 5s prompts to find minimum viable context for speaker-prompted predictor.
  3. **Hybrid predictor prototype:** Concatenate infilling and speaker-prompted representations; evaluate if trade-off can be softened.

## Open Questions the Paper Calls Out
- **Open Question 1:** Can a unified duration prediction architecture be developed that successfully combines the text-conditioning of infilling models with the speaker-conditioning of prompt-based models to optimize both intelligibility and similarity simultaneously?
  - **Basis in paper:** [explicit] The conclusion explicitly states the goal to "develop a new duration predictor that adapts properties of both speaker-prompted and infilling-style duration predictors to improve both intelligibility and speaker-similarity."
  - **Why unresolved:** The current results show a clear trade-off: infilling predictors generally favor intelligibility while speaker-prompted predictors favor similarity. The paper does not propose a mechanism to merge these distinct approaches.
  - **What evidence would resolve it:** A proposed hybrid model that achieves state-of-the-art WER (comparable to infilling) and Sim-o scores (comparable to speaker-prompted) on the same language test sets.

- **Open Question 2:** What specific linguistic or phonological characteristics of Marathi cause the speaker-prompted duration predictor to significantly degrade intelligibility (WER increase from 0.22 to 0.36) compared to its success in Tamil?
  - **Basis in paper:** [inferred] The authors note in Section 5.1 that PFlow performance "degrades significantly for Marathi" while offering gains for Tamil and Bengali. The paper highlights the "linguistic diversity" of Indian languages but does not isolate the factors (e.g., morphology, phoneme frequency) driving this specific failure case.
  - **Why unresolved:** The paper reports the language-dependent trade-offs empirically but does not conduct a linguistic analysis to explain why the cross-attention mechanism fails specifically for Marathi prosody modeling.
  - **What evidence would resolve it:** An ablation study or feature importance analysis linking specific Marathi linguistic features (e.g., agglutinative stress patterns vs. Tamil length distinctions) to the error modes of the cross-attention duration predictor.

- **Open Question 3:** How does the duration and content selection of the fixed 3-second speaker prompt impact the stability and accuracy of the speaker-prompted duration predictor?
  - **Basis in paper:** [inferred] Section 3.4.2 describes using a "3-second mel spectrogram segment... randomly sampled" as the prompt. The paper notes that "dialectal variation" introduces variability, implying that the random sampling might not consistently capture the necessary prosodic cues.
  - **Why unresolved:** The fixed 3-second random sampling is a methodological constant, but its interaction with the "high variety of dialects and pronunciations" mentioned in Section 3.4.2 remains unexplored as a source of error.
  - **What evidence would resolve it:** Experiments varying prompt lengths (e.g., 1s, 5s, 10s) and selection strategies (e.g., voice activity detected segments vs. random) to observe the variance in WER and speaker similarity.

## Limitations
- The paper shows language-dependent performance trade-offs but doesn't explain why specific languages respond differently to predictors
- Ground-truth duration quality relies on forced alignments without specifying the ASR model or alignment method used
- Results are primarily reported for speech infilling tasks, limiting generalization to other TTS use cases

## Confidence
- **High confidence:** The core experimental methodology (comparing two duration predictors on WER and Sim-o metrics) is clearly specified and reproducible
- **Medium confidence:** The mechanism explanations linking predictor architecture to performance outcomes are plausible but not definitively proven
- **Low confidence:** Claims about why specific languages respond differently to predictors lack supporting evidence beyond observed correlations

## Next Checks
1. **Language-specific predictor selection analysis:** Conduct ablation studies isolating which languages benefit from each predictor, and whether this correlates with prosodic or morphological features
2. **Ground-truth alignment quality assessment:** Compare duration predictions from both methods against manually verified alignments for a small sample
3. **Cross-task generalization evaluation:** Test both predictors on non-infilling tasks (e.g., voice cloning from 3-second prompts, speech editing) to determine if observed trade-offs hold across the full TTS use case spectrum