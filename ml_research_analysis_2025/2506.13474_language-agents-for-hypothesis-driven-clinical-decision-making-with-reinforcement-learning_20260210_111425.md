---
ver: rpa2
title: Language Agents for Hypothesis-driven Clinical Decision Making with Reinforcement
  Learning
arxiv_id: '2506.13474'
source_url: https://arxiv.org/abs/2506.13474
tags:
- test
- hypothesis
- patient
- diagnosis
- clinical
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ''
---

# Language Agents for Hypothesis-driven Clinical Decision Making with Reinforcement Learning

## Quick Facts
- arXiv ID: 2506.13474
- Source URL: https://arxiv.org/abs/2506.13474
- Reference count: 33
- Primary result: A two-agent reinforcement learning system achieves 82.3% accuracy on four abdominal diseases while reducing average test cost by ~10% compared to supervised learning baselines

## Executive Summary
This work introduces LA-CDM, a two-agent reinforcement learning framework for clinical decision-making that separates hypothesis generation from action selection. The system uses a shared LLM backbone with two specialized agents: a hypothesis agent that forms diagnostic hypotheses with confidence estimates, and a decision agent that selects either additional tests or final diagnoses based on the hypothesis. Trained on MIMIC-CDM dataset, the framework demonstrates improved diagnostic accuracy and efficiency compared to single-agent approaches, with explicit calibration of uncertainty estimates that aligns with medical decision-making needs.

## Method Summary
LA-CDM uses Qwen-2.5-7B-Instruct with LoRA adapters shared between two agents: a hypothesis agent that generates condition hypotheses with confidence scores, and a decision agent that selects diagnostic tests or diagnoses using ReAct-style reasoning. The system employs cyclic training across three objectives: supervised hypothesis generation, RL-based confidence calibration using betting-game rewards, and RL-based decision-making with cost-penalized rewards. Training proceeds through GRPO optimization over 100-step cycles, with the hypothesis agent trained first to bootstrap the decision agent. The environment provides test results from MIMIC-CDM, with rewards structured to encourage correct diagnoses while minimizing test costs.

## Key Results
- LA-CDM achieves 82.3% mean accuracy across four abdominal diseases, outperforming supervised learning (78.5%) and zero-shot baselines (76.1%)
- Average test cost drops from $1427.85 (supervised) to $1295.61 with cost-aware RL training
- Expected Calibration Error improves from 0.069 to 0.037 through confidence calibration training
- Decision agent learns condition-specific testing patterns (e.g., ultrasound for cholecystitis 64.9% of cases, CT for appendicitis 85.1% of cases)

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Separating hypothesis formation from action selection improves diagnostic accuracy and efficiency compared to single-agent approaches.
- **Mechanism:** The hypothesis agent maintains uncertainty-aware diagnostic hypotheses given partial patient information, while the decision agent uses this hypothesis plus confidence to select the next clinical action. This mirrors the dual cognitive tasks clinicians perform: differential diagnosis formation and test selection.
- **Core assumption:** Clinical decision-making decomposes cleanly into hypothesis generation and action selection, and training these separately transfers to better combined performance.
- **Evidence anchors:**
  - [abstract]: "we propose to model clinical decision-making for diagnosis with a hypothesis-driven uncertainty-aware language agent"
  - [Page 5, Section 4.1]: "It consists of one LLM agent, the hypothesis agent, forming the most likely diagnosis hypothesis... and another agent, the decision agent, that evaluates... to either provide a diagnosis or request an additional diagnostic test"
  - [Page 9, Table 3]: Ablation shows removing hypothesis agent drops mean accuracy from 82.3% to 78.5%
  - [corpus]: MedCoAct (arXiv:2510.10461) similarly uses multi-agent collaboration for clinical decisions, suggesting the pattern has emerging support.
- **Break condition:** If hypothesis and decision agents share weights (as they do here), interference between objectives during training could degrade performance. Ablation shows benefit, but further scaling may reveal conflicts.

### Mechanism 2
- **Claim:** Reinforcement learning with cost-penalized rewards produces more efficient diagnostic pathways than supervised learning alone.
- **Mechanism:** The decision agent receives positive reward for correct diagnosis, negative for incorrect, and a cost penalty for each test requested. GRPO optimizes this to learn which tests provide maximal information gain per cost unit, producing patient-adaptive testing strategies.
- **Core assumption:** Optimal testing pathways exist and can be discovered through exploration, even though ground-truth pathways are unknown.
- **Evidence anchors:**
  - [Page 6, Section 4.2]: "we propose to use reinforcement learning, where the model can learn through trial-and-error which tests are useful in which situations"
  - [Page 9, Table 2]: With cost reward, average test cost drops from $1427.85 to $1295.61 while maintaining similar accuracy
  - [Page 8]: "for suspected cholecystitis, the model most frequently selects ultrasound (64.9% of cases)... for appendicitis, it prioritizes CT scans (85.1% of cases)"
  - [corpus]: Evidence is limited; corpus contains RL-optimized LLM reasoning for dementia diagnosis (arXiv:2505.19954) but not cost-sensitive clinical test selection.
- **Break condition:** Retrospective data limits exploration to tests clinicians actually ordered. If optimal pathways require tests rarely performed in the training data, RL cannot discover them.

### Mechanism 3
- **Claim:** Reinforcement learning can calibrate verbalized confidence estimates without ground-truth confidence labels.
- **Mechanism:** The calibration reward function R(ypred, c, J) = log(c) if correct, log(1-c) if wrong incentivizes the model to output confidence proportional to correctness probability. The model learns through repeated episodes that high-confidence wrong answers are heavily penalized.
- **Core assumption:** LLMs can learn to express numerical confidence that correlates with actual correctness through reward shaping alone.
- **Evidence anchors:**
  - [Page 4, Section 3.2]: "They train the model using Proximal Policy Optimization... removes the need for an artificially constructed ground truth confidence dataset"
  - [Page 8, Figure 3]: ECE decreases from 0.069 to 0.037 after training; calibration curve shows improved alignment
  - [corpus]: No direct corpus validation of this specific confidence calibration approach in clinical settings.
- **Break condition:** Calibration learned on four abdominal diseases may not generalize to broader diagnostic contexts or different output formats.

## Foundational Learning

- **Reinforcement Learning with LLMs (GRPO/PPO)**
  - Why needed here: Standard supervised learning cannot optimize for test efficiency when optimal pathways are unknown. GRPO enables policy optimization directly on token generation.
  - Quick check question: Can you explain why supervised learning fails when ground-truth action sequences don't exist?

- **Confidence Calibration in Language Models**
  - Why needed here: Clinicians need reliable uncertainty estimates to trust model recommendations. Overconfident wrong answers are dangerous in clinical settings.
  - Quick check question: What does Expected Calibration Error (ECE) measure, and why is lower better?

- **Chain-of-Thought / ReAct Prompting**
  - Why needed here: The decision agent uses structured reasoning traces before action selection, improving interpretability and action quality.
  - Quick check question: How does forcing a "Thought:" step before "Action:" change model behavior?

## Architecture Onboarding

- **Component map:**
  - Shared LLM backbone (Qwen-2.5-7B-Instruct) with LoRA adapters
  - Hypothesis Agent: Input = patient state → Output = hypothesis + confidence (0-10)
  - Decision Agent: Input = patient state + hypothesis + confidence → Output = test request OR diagnosis
  - Environment: MIMIC-CDM dataset wrapper that returns test results on request
  - Training loop: Cycles through (1) action RL, (2) hypothesis SFT, (3) confidence RL every 100 steps

- **Critical path:**
  1. Initialize from instruction-tuned LLM
  2. Run episodes through environment, collecting (state, hypothesis, action, reward) tuples
  3. Apply GRPO updates for action selection, cross-entropy for hypothesis generation, betting-game RL for confidence
  4. Validate on held-out patients; early-stop on validation performance

- **Design tradeoffs:**
  - Shared vs. separate weights: Sharing enables mutual reinforcement but risks objective interference (cyclic training mitigates this)
  - Cost reward weight: Too high sacrifices accuracy for efficiency; too low yields expensive pathways
  - Maximum episode length: Must allow sufficient tests but prevent infinite loops

- **Failure signatures:**
  - Model requests unavailable tests repeatedly → prompt engineering issue or insufficient "not available" handling
  - Confidence collapses to extremes (all 0 or all 10) → calibration reward scaling issue
  - Accuracy plateaus while cost keeps dropping → cost reward too dominant
  - See Figure 6 (Page 18): Model diagnoses prematurely with overconfident wrong hypothesis

- **First 3 experiments:**
  1. Reproduce baseline comparison on MIMIC-CDM test split (Table 1): LA-CDM vs. ReAct zero-shot vs. SFT-all to validate training pipeline
  2. Ablate hypothesis agent (Table 3): Run decision agent alone without hypothesis input to confirm contribution
  3. Cost sensitivity analysis (Appendix D): Train with artificially high CBC cost, verify request frequency drops from ~19% to ~2.5%

## Open Questions the Paper Calls Out
- None explicitly stated in the paper.

## Limitations
- Evaluation restricted to four abdominal conditions in single dataset, limiting generalizability
- Retrospective data constraints prevent discovery of optimal testing pathways not historically performed
- Shared weights between agents may cause objective interference during training
- No validation of framework in real clinical environments versus retrospective simulation

## Confidence
**High confidence**: The multi-agent framework structure and its implementation details are well-specified and reproducible. The training methodology (cyclic GRPO with cost-aware rewards) is clearly described and shows consistent improvements across multiple metrics.

**Medium confidence**: The mechanism by which hypothesis-driven reasoning improves efficiency has theoretical support but requires broader validation across different clinical domains. The calibration mechanism's effectiveness is demonstrated but relies on assumptions about the bet-scoring reward function that haven't been extensively validated.

**Low confidence**: Generalization to clinical settings beyond the four abdominal conditions studied, and to different healthcare systems with varying test availability and costs, remains untested. The model's performance in live clinical environments versus retrospective data is unknown.

## Next Checks
1. **Cross-condition generalization test**: Evaluate the trained model on MIMIC-CDM cases for other medical conditions (e.g., cardiovascular or neurological presentations) not seen during training to assess robustness of the hypothesis-driven approach.

2. **Cost-utility analysis validation**: Conduct a formal cost-utility analysis comparing LA-CDM's recommended testing pathways against clinical guidelines and expert recommendations, measuring both economic efficiency and diagnostic accuracy trade-offs.

3. **Real-time decision simulation**: Implement a live simulation environment where the model must make decisions with realistic time constraints and test availability, measuring how well the confidence-calibrated predictions hold up under operational pressure.