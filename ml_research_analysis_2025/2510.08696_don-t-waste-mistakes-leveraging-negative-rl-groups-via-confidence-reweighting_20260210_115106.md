---
ver: rpa2
title: 'Don''t Waste Mistakes: Leveraging Negative RL-Groups via Confidence Reweighting'
arxiv_id: '2510.08696'
source_url: https://arxiv.org/abs/2510.08696
tags:
- reward
- policy
- tail
- grpo
- answer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper addresses the inefficiency in Group Relative Policy\
  \ Optimization (GRPO) where negative groups\u2014groups with no correct responses\u2014\
  yield zero gradients and thus waste computational resources. The authors propose\
  \ a principled solution called LENS (Likelihood Estimation with Negative Samples)\
  \ that derives from a maximum likelihood estimation (MLE) framework for reward modeling."
---

# Don't Waste Mistakes: Leveraging Negative RL-Groups via Confidence Reweighting

## Quick Facts
- arXiv ID: 2510.08696
- Source URL: https://arxiv.org/abs/2510.08696
- Reference count: 40
- One-line primary result: LENS consistently outperforms GRPO baseline on MATH benchmark with both Llama-3.1-8B and Qwen-2.5-3B models

## Executive Summary
This paper addresses a critical inefficiency in Group Relative Policy Optimization (GRPO) where negative groups—groups containing no correct responses—yield zero gradients and waste computational resources. The authors propose LENS (Likelihood Estimation with Negative Samples), a principled solution derived from maximum likelihood estimation for reward modeling. By showing that MLE gradients are equivalent to policy gradients with confidence-weighted penalties for incorrect responses, they create a simple modification to GRPO that assigns non-zero, confidence-dependent rewards to incorrect generations, making negative groups informative.

The method demonstrates significant empirical improvements on the MATH benchmark across multiple model sizes, with particularly strong gains on harder items (Levels 4-5). LENS not only improves final performance but also increases training efficiency by converting previously wasted negative groups into useful gradient updates, achieving higher Pass@k scores across all k values.

## Method Summary
LENS addresses the inefficiency in GRPO where negative groups provide zero gradients by deriving a maximum likelihood estimation framework for reward modeling. The key insight is that the MLE gradient is mathematically equivalent to a policy gradient with a modified value function that adds confidence-weighted penalties for incorrect responses. This creates a simple modification to GRPO where incorrect generations receive non-zero rewards proportional to their confidence levels, with more confident mistakes receiving larger penalties. The method requires minimal architectural changes—simply modifying the reward assignment mechanism to incorporate confidence-weighted penalties for incorrect responses while maintaining the group relative comparison structure.

## Key Results
- LENS consistently outperforms GRPO baseline on MATH benchmark with both Llama-3.1-8B and Qwen-2.5-3B models
- Particularly significant gains on harder items (Levels 4-5), with 4.7% absolute improvement on Level 5
- Converts previously wasted negative groups into informative gradient updates, improving training efficiency
- Achieves higher Pass@k scores across all k values compared to baseline

## Why This Works (Mechanism)
LENS works by recognizing that traditional GRPO wastes computational resources on negative groups that produce zero gradients. By deriving from maximum likelihood estimation for reward modeling, the method shows that optimal gradients require penalizing incorrect responses in proportion to the model's confidence in those responses. This creates a principled way to assign non-zero rewards to negative groups, where more confident mistakes receive larger penalties. The mechanism effectively teaches the model to be more cautious about high-confidence errors while still benefiting from group relative comparisons.

## Foundational Learning
- **Maximum Likelihood Estimation (MLE)**: Needed to derive the theoretical foundation showing equivalence between MLE gradients and confidence-penalized policy gradients. Quick check: verify the mathematical derivation connecting MLE to policy gradient formulations.
- **Group Relative Policy Optimization (GRPO)**: Required to understand the baseline method being improved and why negative groups are problematic. Quick check: confirm that negative groups indeed produce zero gradients in standard GRPO.
- **Confidence Calibration**: Essential for understanding how the model's probability estimates are used to weight penalties for incorrect responses. Quick check: assess whether the model's confidence estimates are well-calibrated before and after applying LENS.
- **Policy Gradient Methods**: Fundamental to understanding how the modified value function affects learning updates. Quick check: verify that the confidence-weighted penalties produce meaningful gradient updates for negative groups.

## Architecture Onboarding

**Component Map:**
Input → Model Generation → Confidence Scoring → Reward Assignment (LENS modification) → Policy Update

**Critical Path:**
The critical path is the reward assignment mechanism where confidence-weighted penalties are calculated and applied. This determines the gradient updates and directly affects learning efficiency and final performance.

**Design Tradeoffs:**
The method trades off between encouraging exploration (by not completely discarding negative groups) and maintaining the group relative comparison structure of GRPO. The confidence weighting introduces sensitivity to model calibration but provides more informative gradients.

**Failure Signatures:**
Poor model calibration could lead to inappropriate penalty magnitudes, either under-penalizing confident mistakes or over-penalizing uncertain ones. Extreme confidence estimates might dominate the learning signal inappropriately.

**First Experiments:**
1. Verify that negative groups produce zero gradients in standard GRPO implementation
2. Test whether confidence-weighted penalties produce non-zero gradients for negative groups
3. Compare final performance with and without confidence weighting while maintaining non-zero rewards for incorrect responses

## Open Questions the Paper Calls Out
None

## Limitations
- The theoretical framework assumes a specific reward modeling structure that may not generalize to all RLHF scenarios with different reward structures
- Performance gains are primarily demonstrated on the MATH benchmark, raising questions about generalization to other domains
- The method's effectiveness depends on model confidence estimates, which may be poorly calibrated in practice

## Confidence
- **Theoretical equivalence between MLE and confidence-penalized policy gradient**: High confidence - rigorous mathematical derivation with clear step-by-step proofs
- **Empirical performance improvements on MATH benchmark**: Medium confidence - consistent improvements shown but limited to single benchmark
- **Efficiency gains from utilizing negative groups**: Medium confidence - theoretical arguments provided but lacks direct ablation studies on training time or sample efficiency

## Next Checks
1. Cross-domain generalization test: Evaluate LENS on at least three diverse reasoning or instruction-following benchmarks (e.g., GSM8K, HumanEval, BigBench) to assess whether performance gains extend beyond mathematical reasoning tasks
2. Calibration analysis: Conduct experiments measuring whether model's confidence estimates become better calibrated when using LENS compared to GRPO, and whether miscalibration affects effectiveness
3. Ablation on confidence weighting: Perform ablation study removing confidence weighting component while maintaining non-zero rewards for incorrect responses to isolate contribution of confidence-based penalties versus simply providing informative gradients for negative groups