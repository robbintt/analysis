---
ver: rpa2
title: 'RiddleBench: A New Generative Reasoning Benchmark for LLMs'
arxiv_id: '2510.24932'
source_url: https://arxiv.org/abs/2510.24932
tags:
- reasoning
- riddlebench
- language
- performance
- answer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces RiddleBench, a new benchmark of 1,737 challenging
  puzzles designed to assess flexible, multi-faceted reasoning abilities in LLMs,
  focusing on logical deduction, spatial reasoning, and constraint satisfaction. Unlike
  existing benchmarks that emphasize structured skills like quantitative problem-solving,
  RiddleBench targets the synthesis of multiple reasoning capabilities central to
  human intelligence.
---

# RiddleBench: A New Generative Reasoning Benchmark for LLMs

## Quick Facts
- arXiv ID: 2510.24932
- Source URL: https://arxiv.org/abs/2510.24932
- Authors: Deepon Halder; Alan Saji; Thanmay Jayakumar; Ratish Puduppully; Anoop Kunchukuttan; Raj Dabre
- Reference count: 14
- Key outcome: Introduces RiddleBench (1,737 puzzles) revealing significant LLM reasoning weaknesses: ~60% accuracy ceiling, hallucination cascades, self-confirmation bias, and constraint-order fragility.

## Executive Summary
RiddleBench is a new benchmark of 1,737 challenging puzzles designed to assess flexible, multi-faceted reasoning abilities in LLMs, focusing on logical deduction, spatial reasoning, and constraint satisfaction. Unlike existing benchmarks that emphasize structured skills like quantitative problem-solving, RiddleBench targets the synthesis of multiple reasoning capabilities central to human intelligence. The evaluation of leading proprietary and open-weight models on RiddleBench reveals significant weaknesses, with even top-tier models like Gemini 2.5 Pro, o3, and Claude 4 Sonnet achieving only around 60% accuracy. Further analysis uncovers critical failure modes, including hallucination cascades (uncritically accepting flawed reasoning from other models), strong self-confirmation bias hindering self-correction, and fragile reasoning that degrades with reordered constraints or irrelevant information. RiddleBench provides a valuable diagnostic tool and resource for guiding the development of more robust and reliable language models.

## Method Summary
RiddleBench contains 1,737 English puzzles across four categories: Sequential Reasoning (60%), Seating Arrangements (25%), Blood Relations (8%), and Coding-Decoding (7%), sourced from Indian competitive exam PDFs via OCR (Gemini 2.5 Flash) and human verification. Models are evaluated zero-shot with temperature 0.7 and 8192-token thinking budget, parsing final answers from \boxed{} format. The benchmark tests reasoning robustness through cross-model verification (hallucination cascade detection), self-correction assessment, and perturbation experiments (constraint shuffling, irrelevant information injection).

## Key Results
- Leading models achieve only ~60% accuracy on RiddleBench, with top performers like Gemini 2.5 Pro, o3, and Claude 4 Sonnet showing similar performance ceilings.
- Cross-model verification experiments show hallucination cascades: evaluators incorrectly validate flawed reasoning 45.2% of the time.
- Self-confirmation bias is severe: models successfully identify their own errors only 17.3% of the time versus 44.1% for peer evaluation.
- Constraint-order fragility is evident: accuracy drops 6.70 p.p. for Blood Relations and 3.69 p.p. for Seating Arrangements when constraints are shuffled.

## Why This Works (Mechanism)

### Mechanism 1: Hallucination Cascade in Cross-Model Verification
- Claim: When one model evaluates another model's flawed reasoning, it tends to uncritically accept the flawed premise rather than re-solving independently.
- Mechanism: The evaluator model (Qwen QwQ 32B) reads the generator's reasoning trace and validates the logical structure without reconstructing the solution from first principles. The paper documents the evaluator incorrectly validating flawed reasoning 45.2% of the time, with language like "The reasoning follows a logical step-by-step deduction..." even when the conclusion was wrong.
- Core assumption: Models prioritize surface-level coherence of reasoning traces over independent verification; they lack a "ground truth" construction process during evaluation.
- Evidence anchors:
  - [abstract] "hallucination cascades (uncritically accepting flawed reasoning from other models)"
  - [Section 5.2] "The evaluator's accuracy in identifying the flawed logic was only 44.1%, no better than a coin toss. Critically, it incorrectly validated the flawed reasoning as sound in 45.2% of cases."
  - [corpus] Weak direct evidence in neighbors; related work (ICPC-Eval, LINGOLY-TOO) examines reasoning evaluation but not specifically cross-model cascade dynamics.
- Break condition: If models are forced to solve independently before seeing another's reasoning, or if verification requires explicit re-derivation steps, this cascade should attenuate.

### Mechanism 2: Self-Confirmation Bias Blocking Self-Correction
- Claim: Models are significantly worse at detecting errors in their own reasoning than in others' reasoning, leading to error entrenchment rather than iterative refinement.
- Mechanism: When Qwen QwQ 32B judged its own flawed reasoning, it failed to identify errors in 67.7% of trials, succeeding only 17.3% of the time—drastically lower than the 44.1% success rate when evaluating a peer model. The paper terms this "self-confirmation bias," where models "are their own most potent deceivers."
- Core assumption: Models generate reasoning that appears self-consistent to themselves; there is no independent internal critic module that can step outside the generated context.
- Evidence anchors:
  - [abstract] "strong self-confirmation bias hindering self-correction"
  - [Section 5.3] "This success rate is drastically lower than the 44.1% achieved when evaluating a peer's reasoning, suggesting a powerful self-confirmation bias."
  - [corpus] No direct corpus corroboration; LINGOLY-TOO addresses knowledge-reasoning disentanglement but not self-correction dynamics specifically.
- Break condition: If an independent verification pass (separate model or independent context) is required before self-evaluation, bias may reduce.

### Mechanism 3: Constraint-Order and Noise Sensitivity in Reasoning
- Claim: LLM reasoning is fragile to superficial prompt perturbations—specifically constraint reordering and irrelevant information—that should not logically affect outcomes.
- Mechanism: When constraints were shuffled, performance dropped 6.70 percentage points (Blood Relations) and 3.69 p.p. (Seating Arrangements). With irrelevant "red herring" sentences inserted, most categories showed degradation. The paper interprets this as reliance on "brittle, sequential heuristics rather than robust comprehension."
- Core assumption: Models process constraints serially and build partial solutions incrementally, rather than constructing a complete mental model before solving.
- Evidence anchors:
  - [abstract] "fragile reasoning that degrades with reordered constraints or irrelevant information"
  - [Section 5.4] "For a system building a true holistic mental model, order should be irrelevant. However, performance... dropped significantly."
  - [corpus] GeoSense and VisualPuzzles examine reasoning robustness in multimodal contexts but do not directly address text-only constraint ordering effects.
- Break condition: If models are prompted to first list all constraints verbatim, then construct an explicit state representation before solving, fragility should decrease.

## Foundational Learning

- Concept: **Constraint Satisfaction Problems (CSP)**
  - Why needed here: RiddleBench puzzles (especially Seating Arrangements, Sequential Reasoning) require simultaneously satisfying multiple constraints; understanding CSP fundamentals helps diagnose where models fail to maintain holistic state.
  - Quick check question: Given constraints "A sits left of B" and "C sits right of B," can you enumerate valid orderings?

- Concept: **Spatial Representation and Mental Models**
  - Why needed here: Seating Arrangement puzzles (25% of benchmark) require constructing internal spatial layouts; the paper notes models fail to maintain "a holistic and mutable 'mental model' of the layout."
  - Quick check question: If five people sit in a row facing north, and X is second from the left, how many positions can Y occupy if Y must be adjacent to Z?

- Concept: **Reasoning Robustness and Adversarial Perturbation**
  - Why needed here: The benchmark explicitly tests fragility via constraint shuffling and irrelevant information injection; understanding robustness testing helps design better evaluation pipelines.
  - Quick check question: If a logical puzzle's constraints are reordered, should the answer change? Why or why not?

## Architecture Onboarding

- Component map: Dataset (1,737 puzzles) -> Zero-shot evaluation harness -> Cross-model verification module -> Self-correction probe -> Robustness perturbation tests
- Critical path: 1. Load puzzle → format with zero-shot prompt → call model API → extract \boxed{} answer → compare to ground truth. 2. For robustness tests: apply perturbation (shuffle constraints / insert red herring) → same evaluation path. 3. For cascade experiments: feed generator's reasoning trace to evaluator → parse YES/NO judgment.
- Design tradeoffs:
  - Zero-shot vs. few-shot: Paper uses zero-shot to probe intrinsic reasoning; few-shot may improve accuracy but masks underlying fragility.
  - 8192-token thinking budget: Balances computational cost against model "reasoning" capacity; 55% of cross-model verification tasks still timed out.
  - Single-language (English) dataset: Limits cross-linguistic generalization; paper acknowledges cultural bias from Indian exam sources.
- Failure signatures:
  - Hallucination cascade signature: Evaluator produces "The reasoning follows..." language without re-deriving the solution; validates incorrect reasoning as sound.
  - Self-confirmation bias signature: Model judges its own flawed output as correct at ~2.7x higher rate than peer evaluation failures.
  - Fragility signature: Accuracy drops >3 p.p. on shuffled constraints or noisy inputs, indicating procedural rather than principled reasoning.
- First 3 experiments:
  1. Baseline reproduction: Run zero-shot evaluation on a 100-puzzle subset across 2-3 model families; verify accuracy distribution matches paper's ~50-70% range.
  2. Constraint-order ablation: Take 50 Seating Arrangement puzzles; evaluate with original vs. shuffled constraint order; quantify delta.
  3. Self-correction probe: For 30 incorrectly solved puzzles, feed model its own reasoning trace back with "Is this correct?" prompt; measure reversal rate against paper's 17.3% baseline.

## Open Questions the Paper Calls Out

- Do the reasoning capabilities measured by RiddleBench transfer effectively across different languages?
  - Basis in paper: [explicit] Section 8 lists cross-lingual evaluation as a primary goal for future work, noting that "True reasoning capabilities should be language-agnostic."
  - Why unresolved: The current benchmark and evaluation are exclusively in English, leaving multilingual capabilities untested.
  - What evidence would resolve it: Evaluation of model performance on RiddleBench puzzles translated into diverse languages (e.g., major Indian languages) showing consistent accuracy.

- Can the "self-confirmation bias" that prevents LLMs from correcting their own errors be mitigated?
  - Basis in paper: [explicit] Section 5.3 identifies a "powerful self-confirmation bias" where models failed to identify their own errors in 67.7% of trials.
  - Why unresolved: The paper diagnoses the existence and severity of this bias but does not propose or test methods to overcome the "illusion of self-correction."
  - What evidence would resolve it: The development of training techniques or prompting strategies that significantly increase the success rate of self-correction beyond the reported 17.3%.

- Is the observed fragility to constraint ordering a fundamental limitation of current model architectures?
  - Basis in paper: [inferred] Section 5.4 shows performance drops when constraints are shuffled, suggesting models rely on "brittle, sequential heuristics" rather than building robust "mental models."
  - Why unresolved: The paper demonstrates the models' sensitivity to order but does not determine if this is an insurmountable architectural trait or a solvable training deficiency.
  - What evidence would resolve it: Demonstrating that a model can maintain statistically similar accuracy levels regardless of the random seed used to shuffle the order of constraints in the prompt.

## Limitations

- The cross-model verification and self-correction experiments lack precise sample size specifications, making exact replication difficult.
- The constraint reordering and noise perturbation protocols are underspecified, particularly regarding random seeds and sentence selection criteria.
- Cultural bias from Indian exam sources limits generalizability, and the single-language English dataset restricts cross-linguistic applicability.

## Confidence

- **High confidence**: The baseline accuracy findings (60% performance ceiling across top models) are well-supported by direct measurements and consistent with broader LLM reasoning literature.
- **Medium confidence**: The hallucination cascade mechanism is documented through specific evaluator failure rates (45.2% incorrect validation), but the underlying cause (surface coherence prioritization vs. other factors) requires further testing.
- **Medium confidence**: Self-confirmation bias findings (17.3% self-correction success vs. 44.1% peer evaluation) are robust within the experimental design, though the exact psychological mechanism remains unclear.
- **Medium confidence**: Constraint-order fragility is demonstrated empirically, but whether this reflects true reasoning deficiency or prompt engineering sensitivity requires additional controlled experiments.

## Next Checks

1. **Replication baseline**: Run zero-shot evaluation on 100-puzzle subset across 2-3 model families; verify accuracy distribution matches paper's ~50-70% range and category splits align with Table 1.
2. **Robustness quantification**: For 50 Seating Arrangement puzzles, evaluate with original vs. shuffled constraint order using documented random seed; measure delta against paper's 3.69 p.p. drop.
3. **Self-correction probe**: For 30 incorrectly solved puzzles, feed model its own reasoning trace back with "Is this correct?" prompt; measure reversal rate against paper's 17.3% baseline, controlling for thinking budget exhaustion.