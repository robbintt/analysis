---
ver: rpa2
title: Sequencing to Mitigate Catastrophic Forgetting in Continual Learning
arxiv_id: '2512.16871'
source_url: https://arxiv.org/abs/2512.16871
tags:
- learning
- training
- sequencing
- performance
- nwot
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper investigates catastrophic forgetting (CF) in continual
  learning (CL) and proposes a novel task-sequencing method based on zero-shot neural
  architecture search (NAS). The authors formulate CF mitigation as a stochastic optimization
  problem, then adapt the NAS Without Training (NWOT) algorithm to predict the next
  best task to learn, avoiding full retraining.
---

# Sequencing to Mitigate Catastrophic Forgetting in Continual Learning

## Quick Facts
- **arXiv ID:** 2512.16871
- **Source URL:** https://arxiv.org/abs/2512.16871
- **Reference count:** 40
- **Primary result:** Intelligent task sequencing using zero-shot NAS predictors (NWOT) reduces catastrophic forgetting compared to random ordering, especially when combined with EWC regularization.

## Executive Summary
This paper tackles catastrophic forgetting in continual learning by framing task sequencing as a stochastic optimization problem. The authors propose using a zero-shot neural architecture search (NAS) algorithm, specifically NWOT, to predict the next best task to learn based on activation diversity in an untrained network. Experiments on the DomainNet dataset show that intelligent sequencing reduces forgetting compared to random ordering, particularly when combined with traditional methods like EWC. A modified NWOT incorporating Activation Interval Dropout (AID) improves performance in non-IID scenarios by balancing task selection. The method achieves higher and more stable accuracy (up to ~50% vs. ~35-38%) in non-IID continual learning, demonstrating the effectiveness of sequencing guided by zero-shot NAS predictors.

## Method Summary
The paper formulates catastrophic forgetting mitigation as a stochastic optimization problem and proposes using zero-shot neural architecture search (NAS) scoring to sequence tasks optimally. The core method uses the NWOT algorithm, which computes a score based on the log-determinant of a kernel matrix derived from ReLU activation similarities, to predict the relative utility of a dataset for the current model state. For non-IID scenarios, the authors introduce Activation Interval Dropout (AID) to regularize the scoring mechanism and prevent domain-specific biases. The sequencing controller (MTRCE) orchestrates data gathering, scoring, selection, and training across distributed data nodes. The method is evaluated on DomainNet under both IID and non-IID settings, with and without EWC regularization.

## Key Results
- Intelligent task sequencing using NWOT reduces catastrophic forgetting compared to random ordering.
- NWOT-AID modification improves performance in non-IID scenarios by balancing task selection.
- Combining NWOT sequencing with EWC regularization enhances overall performance.
- The method achieves up to ~50% accuracy in non-IID continual learning, compared to ~35-38% for random selection.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** The diversity of activation patterns in an untrained network predicts the relative utility of a dataset for the current model state.
- **Mechanism:** The NWOT algorithm computes a score based on the log-determinant of a kernel matrix derived from ReLU activation similarities. A higher score indicates greater sample diversity, suggesting the dataset may provide more informative gradients for learning.
- **Core assumption:** Activation diversity in a forward pass correlates with the model's ability to learn useful features from that data without causing immediate interference.
- **Evidence anchors:**
  - [Abstract]: "...leveraging zero-shot scoring algorithms inspired by neural architecture search (NAS)... intelligent task sequencing can substantially reduce CF."
  - [Section VI-A]: "...higher score indicates greater diversity... which may suggest that it could provide more informative gradients for learning."
  - [Corpus]: Weak direct validation for sequencing specifically; neighbors focus on parameter selection (PPSEBM) or overparameterization analysis rather than NAS-based ordering.
- **Break condition:** Fails in non-IID settings where domain-specific activation variances differ significantly (e.g., "sketch" vs. "real" images), causing the score to conflate domain complexity with utility.

### Mechanism 2
- **Claim:** Activation Interval Dropout (AID) decouples the scoring mechanism from domain-specific activation biases in non-IID environments.
- **Mechanism:** AID applies stochastic masking to activation intervals (positive vs. negative) before computing the NWOT score. This prevents the kernel matrix from being dominated by neurons with high-magnitude activations specific to a certain domain, forcing the scorer to evaluate feature diversity more robustly.
- **Core assumption:** The bias in standard NWOT scores comes from saturated or high-variance neurons which can be muted via structured dropout to reveal "truer" representational diversity.
- **Evidence anchors:**
  - [Section VI-B]: "...AID-regularized activation matrix replaces the raw ReLU outputs... preventing the kernel computation from being biased toward sharp or domain-specific activation patterns."
  - [Figure 12]: Shows NWOT-AID achieving ~50% accuracy vs. ~35-38% for random selection.
  - [Corpus]: Not explicitly covered in neighbors; provides a novel angle compared to standard replay or regularization methods in the corpus.
- **Break condition:** If dropout probabilities are not tuned correctly, critical information for specific domains might be lost, leading to under-selection of complex tasks.

### Mechanism 3
- **Claim:** Intelligent sequencing reduces the "interference load" that regularization methods must handle, improving stability.
- **Mechanism:** By selecting the "optimal" next node (one that maximizes the performance gain $\Delta M$), the model takes smaller, more informed steps in the loss landscape. When combined with Elastic Weight Consolidation (EWC), this reduces the need for the regularizer to aggressively constrain weights, allowing better plasticity.
- **Core assumption:** The sequence of gradient updates matters as much as the regularization strength; an optimal order naturally minimizes the overlap between the current task's optimal weights and important weights for previous tasks.
- **Evidence anchors:**
  - [Abstract]: "...when combined with traditional continual learning strategies, sequencing offers enhanced performance..."
  - [Section VII-B]: "...coupling NWOT sequencing with traditional CL methods such as EWC enhances their performance."
  - [Corpus]: Supports the general difficulty of CF (e.g., "Analysis of Overparameterization"), but does not confirm this specific sequencing-regularization synergy.
- **Break condition:** If the NWOT score is noisy or misleading, the sequence might oscillate, potentially destabilizing the consolidation logic of methods like EWC.

## Foundational Learning

- **Concept:** **Catastrophic Forgetting (CF)**
  - **Why needed here:** This is the central failure mode the paper aims to solve. Understanding that CF is caused by weights overwriting previous knowledge during sequential training is essential.
  - **Quick check question:** Can you explain why training on a new domain might cause a model to fail on a previously mastered domain, even if the classes are distinct?

- **Concept:** **Zero-Shot Neural Architecture Search (NAS) / NWOT**
  - **Why needed here:** The core proxy used for decision making. You must understand that this method evaluates data/architecture fit *without* training (using only a forward pass) to see why it is computationally viable.
  - **Quick check question:** How does the NWOT algorithm estimate the potential accuracy of an architecture or the utility of a dataset without performing backpropagation?

- **Concept:** **Non-IID Data Distributions**
  - **Why needed here:** The paper highlights that the standard NWOT fails when data is not Independent and Identically Distributed (e.g., specific domains like sketches vs. photos). Recognizing this distribution shift is key to understanding the AID modification.
  - **Quick check question:** Why would a scoring method that works for shuffled data fail when data is split into distinct domains like "Clipart" vs. "Real"?

## Architecture Onboarding

- **Component map:** MTRCE (Controller) -> Data Nodes (DNs) -> Scorer (NWOT/NWOT-AID) -> Selector -> Learner (ResNet)
- **Critical path:**
  1. **Sample:** Gather minibatch from available DNs.
  2. **Score:** Compute NWOT score (forward pass only).
     - *If Non-IID:* Apply AID to activations before scoring.
  3. **Select:** Selector chooses DN with highest score (or rotates based on schedule).
  4. **Train:** Update model on selected DN's data (apply EWC if enabled).
  5. **Repeat:** Re-score with updated weights.
- **Design tradeoffs:**
  - **Speed vs. Robustness:** Standard NWOT is faster but biased in non-IID settings; AID adds computation but stabilizes selection.
  - **Scheduler vs. Greediness:** Purely greedy selection (highest score) might starve low-scoring domains; the paper enforces a scheduler to force visits, trading potential immediate gain for global coverage.
- **Failure signatures:**
  - **Domain Starvation:** Accuracy flattens or fluctuates wildly; logs show the model repeatedly visiting the same DN (e.g., Quickdraw) and ignoring others. *Fix:* Check scheduler/AID implementation.
  - **Biased Scoring:** "Sketch" domains score higher than "Real" domains consistently, but training on "Sketch" yields poor global accuracy. *Fix:* Enable AID modification.
  - **CF Collapse:** Accuracy drops despite sequencing. *Fix:* Verify EWC integration or broaden the "candidate set" $C_h$ in the selector logic.
- **First 3 experiments:**
  1. **IID Baseline:** Split DomainNet randomly into 6 DNs. Run Random Sequencing vs. Standard NWOT. *Expectation:* NWOT should converge faster and higher.
  2. **Non-IID Stress Test:** Split DomainNet by domain (Real, Sketch, etc.). Run Standard NWOT. *Expectation:* Observe failure mode (bias toward sparse domains like Quickdraw).
  3. **Full Stack Validation:** Non-IID setup + NWOT-AID + EWC. *Expectation:* Recovery of stability and highest accuracy (~50%), confirming the combined mechanism.

## Open Questions the Paper Calls Out
- Can Fisher-value-based prediction strategies outperform or complement the proposed zero-shot NWOT scoring method for task sequencing?
- How does the proposed sequencing method perform in task-based and domain-based continual learning scenarios?
- Is the NWOT-based sequencing strategy effective for non-CNN architectures, such as Transformers or Large Language Models (LLMs)?

## Limitations
- Hyperparameter sensitivity: Critical values (dropout rates, batch size, learning rate) are not specified, making direct replication challenging.
- Scheduler implementation: The "fair" node visit enforcement is described conceptually but lacks a precise algorithm.
- Generalization beyond DomainNet: Results are demonstrated only on one large-scale dataset; transfer to other benchmarks remains untested.

## Confidence
- **High:** The core claim that intelligent sequencing reduces catastrophic forgetting compared to random ordering, supported by quantitative comparisons and multiple experimental conditions.
- **Medium:** The specific performance numbers (~50% accuracy with NWOT-AID in non-IID) are credible but depend on undisclosed hyperparameters.
- **Low:** The theoretical claim that activation diversity in an untrained network predicts learning utility is plausible but lacks direct validation beyond the proxy score performance.

## Next Checks
1. **Replication with fixed hyperparameters:** Re-run the experiments using standardized learning rates, batch sizes, and AID dropout probabilities to verify reported accuracy gains.
2. **Scheduler ablation study:** Test the sequencing performance with and without the fair visit scheduler to isolate its contribution to overall stability.
3. **Cross-dataset evaluation:** Apply the NWOT-AID sequencing method to a different continual learning benchmark (e.g., CORe50) to assess generalizability.