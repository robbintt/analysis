---
ver: rpa2
title: How to Make Museums More Interactive? Case Study of Artistic Chatbot
arxiv_id: '2509.00572'
source_url: https://arxiv.org/abs/2509.00572
tags:
- chatbot
- system
- user
- exhibition
- questions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study presents Artistic Chatbot, a voice-to-voice RAG-powered
  system deployed at an art exhibition to enhance visitor engagement through interactive
  question answering. The system used 226 curated documents about the Faculty of Media
  Art, processed via a multilingual pipeline, and leveraged GPT-4o-mini for response
  generation.
---

# How to Make Museums More Interactive? Case Study of Artistic Chatbot

## Quick Facts
- arXiv ID: 2509.00572
- Source URL: https://arxiv.org/abs/2509.00572
- Reference count: 29
- Primary result: Voice-to-voice RAG-powered chatbot achieved 60% response relevance on 80% off-topic queries at an art exhibition

## Executive Summary
This study presents Artistic Chatbot, a voice-to-voice RAG-powered system deployed at an art exhibition to enhance visitor engagement through interactive question answering. The system used 226 curated documents about the Faculty of Media Art, processed via a multilingual pipeline, and leveraged GPT-4o-mini for response generation. Over a month, 727 spoken queries were logged, with 60% of responses judged relevant to the exhibition, despite only 20% of questions being strictly on-topic. This demonstrates the system's ability to ground answers in exhibition content even for unpredictable inputs. Key challenges included incomplete queries (32%), end-of-utterance detection issues, and moderate overall relevance (mean score: 2.66). The findings highlight the potential of voice-based chatbots in cultural heritage settings, while also pointing to areas for improvement in UX and response quality.

## Method Summary
The system implemented a voice-to-voice RAG pipeline where users triggered the chatbot with Polish phrases, spoke their queries, and received synthesized responses. The knowledge base consisted of 226 documents (159 PDFs + 67 biographies) translated to Polish and chunked into 11,596 segments. Retrieval used FAISS for fast similarity search followed by cross-encoder reranking to select top-3 context chunks. GPT-4o-mini generated responses based on the retrieved context and exhibition-specific persona templates, with ElevenLabs providing text-to-speech output. The system operated as a single-turn interaction without session memory, processing 727 queries over one month.

## Key Results
- 60% of generated responses were relevant to the exhibition content, even when 80% of user questions were off-topic
- Average relevance score was 2.66 out of 5, indicating room for improvement in response quality
- 32% of queries were incomplete due to silence-based end-of-utterance detection issues
- The system successfully grounded answers in exhibition context through RAG retrieval, despite user queries being predominantly unrelated to the exhibition

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Injecting retrieved domain context into the LLM prompt biases responses toward exhibition content, even when user queries are off-topic.
- Mechanism: RAG retrieval supplies top-k relevant chunks as context, constraining the LLM's generation space to the curated knowledge base rather than its pre-training distribution.
- Core assumption: The LLM will attend to and prioritize provided context over parametric knowledge when generating responses.
- Evidence anchors:
  - [abstract]: "60% of responses judged relevant to the exhibition, despite only 20% of questions being strictly on-topic"
  - [section 3]: "the system consistently supplied the LLM with exhibition-related context... many answers remained focused on the exhibition, even when the user's questions were unrelated"
  - [corpus]: Related museum chatbot work (SperlÃ­ 2020, Rachabatuni et al. 2024) uses similar RAG grounding but lacks quantitative relevance metrics; corpus evidence for this specific mechanism is limited.
- Break condition: If retrieval fails to surface relevant chunks (e.g., query-document semantic gap), the LLM may hallucinate or generate generic responses outside the domain.

### Mechanism 2
- Claim: Two-stage retrieval (dense search + cross-encoder reranking) improves context relevance while maintaining acceptable latency.
- Mechanism: FAISS performs fast approximate nearest-neighbor search to retrieve top-20 candidates; a CrossEncoder model then scores each candidate against the query more precisely, selecting top-3 for the final prompt.
- Core assumption: CrossEncoder attention-based scoring captures query-chunk relevance better than embedding cosine similarity alone, and the latency cost of reranking 20 candidates is acceptable.
- Evidence anchors:
  - [section 2.2]: "This initial retrieval prioritized speed over fine-grained relevance; thus, we needed a subsequent re-ranking step"
  - [section 2.2]: "Only the top 3 highest-scoring chunks, as determined by the re-ranker, were selected to form the contextual basis"
  - [corpus]: Corpus does not provide comparative studies of two-stage vs. single-stage retrieval in museum settings; evidence is primarily internal to this paper.
- Break condition: If the reranker is miscalibrated or chunks are poorly segmented, the top-3 may still be irrelevant, leading to low-quality responses.

### Mechanism 3
- Claim: Trigger-phrase gating reduces false activations from ambient noise in open physical spaces.
- Mechanism: The system idles in a low-sensitivity state until detecting one of four predefined Polish trigger phrases, then transitions to active query capture.
- Core assumption: Users will naturally use trigger phrases; background conversations will not match them closely enough to cause spurious activations.
- Evidence anchors:
  - [section 2.3]: "This mechanism ensured that the input was intentional, distinguishing user interaction from background noise"
  - [abstract]: No reported false-trigger issues; main UX challenge was end-of-utterance detection, not trigger recognition
  - [corpus]: Similar systems (Casillo et al. 2020) use button or touchscreen activation; trigger-based voice activation in cultural heritage is underdocumented.
- Break condition: In noisy or multilingual environments, trigger recognition may fail, or users may not discover the required phrases, reducing engagement.

## Foundational Learning

- Concept: **Retrieval-Augmented Generation (RAG)**
  - Why needed here: The entire system depends on RAG to ground responses in the exhibition's curated documents rather than relying on the LLM's parametric knowledge.
  - Quick check question: Can you explain why providing retrieved context in the prompt reduces hallucination compared to zero-shot LLM generation?

- Concept: **Semantic Search with Vector Embeddings**
  - Why needed here: Understanding how text is chunked, embedded, and indexed (FAISS) is essential for diagnosing retrieval failures and tuning chunk size/overlap.
  - Quick check question: What is the trade-off between larger chunk sizes (more context) and retrieval precision (more noise)?

- Concept: **End-of-Utterance (EoU) Detection in Voice Interfaces**
  - Why needed here: The paper identifies silence-based EoU as a key failure mode (32% incomplete queries); understanding VAD and timeout-based alternatives is critical for improvement.
  - Quick check question: Why does pure silence-based EoU detection fail with hesitant speakers or noisy environments?

## Architecture Onboarding

- Component map: Ceiling microphone -> Trigger-phrase detection -> Speech-to-Text -> Embedding -> FAISS search (top-20) -> CrossEncoder reranking (top-3) -> Prompt construction -> GPT-4o-mini -> Text-to-Speech -> Room speakers
- Critical path: Voice input -> Trigger detection -> STT -> Embedding -> FAISS retrieval -> CrossEncoder rerank -> Prompt construction -> LLM generation -> TTS -> Audio output
- Design tradeoffs:
  - Small curated KB (226 docs) limits knowledge breadth but improves retrieval latency and focus
  - Single-turn interaction (no session memory) simplifies state management but prevents follow-up questions
  - Translation-to-Polish pipeline (via GPT-4o) risks terminology errors vs. cross-lingual RAG
  - Silence-based EoU causes 32% incomplete queries; button-based activation would be more reliable but changes physical UX
- Failure signatures:
  - Incomplete queries (32%): Silence detection triggers too early, especially with hesitant speakers or noise
  - Low response relevance (mean 2.66): Retrieval fails to surface relevant chunks, or user intent is too vague
  - Off-topic questions (80%): Users ask questions outside the exhibition scope; system grounds responses anyway but quality varies
- First 3 experiments:
  1. Replace silence-based EoU with VAD + configurable timeout, or add a physical push-to-talk button; measure incomplete-query rate reduction
  2. Systematically vary chunk size (e.g., 2000, 5000, 8000 chars) and overlap (100, 200, 500 chars); evaluate retrieval precision on a held-out query set
  3. Compare translation-based RAG vs. cross-lingual RAG on multilingual documents; measure terminology accuracy and response quality

## Open Questions the Paper Calls Out

### Open Question 1
- Question: To what extent can sophisticated end-of-utterance (EoU) validation mechanisms reduce the rate of incomplete queries compared to silence-based detection in noisy public spaces?
- Basis in paper: [explicit] The authors note that 32% of queries were incomplete due to silence-based detection and suggest "enhancing the existing silence detection with a more sophisticated end-of-utterance (EoU) validation mechanism" as an alternative to physical buttons.
- Why unresolved: The current system relied on simple pause detection, which failed when users hesitated or background noise interfered, but the efficacy of advanced EoU models in this specific context was not tested.
- What evidence would resolve it: A comparative study logging query completeness rates using standard silence detection versus a validation model in a live exhibition setting.

### Open Question 2
- Question: Does fine-tuning the response generation model to express uncertainty ("I don't know") improve factual reliability without negatively impacting user engagement?
- Basis in paper: [explicit] The paper highlights the risk of hallucinations and explicitly suggests "fine-tuning the model to acknowledge uncertainty, that is, responding with 'I don't know' when needed" to increase reliability.
- Why unresolved: While RAG reduces hallucinations, the system still generated responses with moderate relevance (mean score 2.66), and it is unclear if refusal-to-answer strategies would frustrate visitors or improve trust.
- What evidence would resolve it: A/B testing different model configurations to measure the trade-off between hallucination reduction and user satisfaction/interaction duration.

### Open Question 3
- Question: Does a Cross-lingual RAG pipeline yield higher retrieval accuracy for multilingual art corpora than a fully translation-based pipeline?
- Basis in paper: [explicit] The authors identify translation-based preprocessing as a potential source of inaccuracies due to "domain terminology and cultural context shifts" and suggest "Cross-lingual RAG" as a proposed alternative.
- Why unresolved: The implemented system translated all documents (92% Polish, mixed other languages) into Polish before indexing; the performance gain of retrieving directly from original languages remains hypothetical.
- What evidence would resolve it: Benchmarking retrieval precision and response quality on the same dataset using monolingual translation versus cross-lingual embedding approaches.

## Limitations
- Evaluation was constrained to a single exhibition context without a control group, making it difficult to isolate the RAG architecture's impact from novelty effects
- The 32% incomplete-query rate was measured indirectly through manual review of transcriptions, not automated audio analysis
- The system's reliance on translation (via GPT-4o) for multilingual content introduces potential terminology drift that was not quantified

## Confidence
- **High confidence**: The mechanism by which RAG grounds responses in exhibition content, supported by direct quantitative evidence (60% relevance on 80% off-topic queries)
- **Medium confidence**: The two-stage retrieval pipeline improves relevance, based on internal observations but lacking comparative studies
- **Medium confidence**: Trigger-phrase gating reduces false activations, supported by absence of reported false-trigger issues but limited corpus precedent

## Next Checks
1. A/B test the current silence-based EoU detection against a button-based or VAD + timeout alternative, measuring both incomplete-query rate and user engagement
2. Conduct a controlled user study with a subset of queries (e.g., "on-topic" vs. "off-topic") to measure relevance differences and isolate the effect of RAG grounding
3. Compare the translation-based RAG pipeline against a cross-lingual RAG approach on a multilingual subset of the documents, measuring terminology accuracy and response quality