---
ver: rpa2
title: Knowledge-Guided Textual Reasoning for Explainable Video Anomaly Detection
  via LLMs
arxiv_id: '2511.07429'
source_url: https://arxiv.org/abs/2511.07429
tags:
- anomaly
- detection
- video
- textual
- knowledge
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces TbVAD, a text-based framework for video anomaly
  detection that performs anomaly detection and explanation entirely within the textual
  domain. The framework represents video semantics through language, enabling interpretable
  and knowledge-grounded reasoning.
---

# Knowledge-Guided Textual Reasoning for Explainable Video Anomaly Detection via LLMs

## Quick Facts
- **arXiv ID**: 2511.07429
- **Source URL**: https://arxiv.org/abs/2511.07429
- **Reference count**: 40
- **Primary result**: TbVAD achieves 85.42% AUC on UCF-Crime and 97.34% AP on XD-Violence using text-only reasoning with interpretable explanations

## Executive Summary
This paper introduces TbVAD, a text-based framework for video anomaly detection that performs anomaly detection and explanation entirely within the textual domain. The framework represents video semantics through language, enabling interpretable and knowledge-grounded reasoning. TbVAD operates in three stages: transforming video content into fine-grained captions using a vision-language model, constructing structured knowledge by organizing the captions into four semantic slots (action, object, context, environment), and generating slot-wise explanations that reveal which semantic factors contribute most to the anomaly decision. Evaluated on UCF-Crime and XD-Violence benchmarks, TbVAD demonstrates that textual knowledge reasoning provides interpretable and reliable anomaly detection for real-world surveillance scenarios, achieving competitive performance compared to visual-feature-based baselines while offering explainable reasoning through structured textual understanding.

## Method Summary
TbVAD is a three-branch text-based framework for weakly supervised video anomaly detection. The Structured Knowledge Branch uses VLMs (Molmo/BLIP2) to generate fine-grained captions, then LLM summarization to organize captions into four semantic slots (action, object, context, environment) using frozen Longformer encoders. The Text Understanding Branch encodes frame-level captions via BERT with trainable Transformer layers. The Explainable Reasoning Branch computes slot-wise importance through cross-attention and generates explanations using Qwen2.5-7B. Features from both branches are fused and classified, with explanations generated in parallel for interpretability.

## Key Results
- Achieves 85.42% AUC on UCF-Crime using object+environment slot combination
- Achieves 97.34% AP on XD-Violence with object+environment+context slots
- Demonstrates interpretable explanations that align with anomaly types (e.g., explosion emphasizes environment, riot emphasizes action/context)
- Shows cross-dataset generalization with 74.93% AP transferring from UCF-Crime to XD-Violence

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Structured semantic decomposition enables more discriminative anomaly reasoning than monolithic visual features
- Mechanism: The framework partitions video semantics into four orthogonal slots (action, object, context, environment) via LLM-based multi-aspect summarization. This forces the model to reason about anomalies along interpretable semantic axes rather than entangled visual feature dimensions.
- Core assumption: Anomalous events can be characterized by deviations along one or more interpretable semantic dimensions that are recoverable from textual descriptions.
- Evidence anchors:
  - [abstract] "constructing structured knowledge by organizing the captions into four semantic slots—action, object, context, and environment"
  - [Section 3.1.3] "To extract structured semantics, we design four tailored prompts targeting distinct aspects of each scene"
  - [Section 4.4, Table 3] Ablation shows action+object+context achieves 85.33% AUC while action+context alone drops to 76.98%, demonstrating slot complementarity
- Break condition: If anomalies cannot be distinguished along these four semantic dimensions (e.g., anomalies defined purely by motion patterns without semantic correlates), slot decomposition may not improve detection.

### Mechanism 2
- Claim: Text-only representation preserves sufficient anomaly signal while enabling interpretable reasoning chains
- Mechanism: VLM-generated captions serve as information-preserving projections from visual to linguistic space. Since anomalies in surveillance often involve unusual semantic compositions (e.g., "person holding weapon in convenience store"), language models can detect these compositional anomalies through structured textual comparison against normal/abnormal knowledge bases.
- Core assumption: VLMs can extract semantically meaningful descriptions from surveillance footage, including low-resolution or cluttered scenes.
- Evidence anchors:
  - [abstract] "TbVAD represents video semantics through language, enabling interpretable and knowledge-grounded reasoning"
  - [Section 4.3, Table 2] TbVAD achieves 97.34% AP on XD-Violence using only text, surpassing visual baselines; UCF-Crime achieves 85.42% AUC despite lower caption quality
  - [Section 4.3] "XD-Violence videos are typically clearer and captured at closer viewpoints, allowing vision-language models to generate highly descriptive and accurate captions"
  - [corpus] Related work "Unlocking VLMs for Video Anomaly Detection via Fine-Grained Prompting" (FMR=0.61) supports VLM prompting for anomaly detection
- Break condition: Caption quality degradation on extremely low-resolution or occluded footage may lose critical anomaly signals that visual features could preserve.

### Mechanism 3
- Claim: Cross-attention between instance descriptions and knowledge slots localizes which semantic factors drive anomaly predictions
- Mechanism: The framework computes attention alignment A between slot prototypes K_v and description tokens H_d, then estimates slot importance w_s through learned projections. This creates a differentiable path from input text to slot contributions, enabling both classification and explanation from the same computation.
- Core assumption: Attention weights correlate with semantic importance for anomaly decisions.
- Evidence anchors:
  - [Section 3.3.1] "A ∈ R^{S×T} represents the attention alignment between slots and tokens... w_s represents the normalized importance of slot s"
  - [Section 3.3.2] "For each slot, we identify the most semantically aligned knowledge sentence by measuring cosine similarity"
  - [Section 4.6, Figure 5] Visualization shows slot importance varies across anomaly types (Explosion emphasizes environment, Riot emphasizes action/context)
  - [corpus] Limited direct corpus support for attention-based explainability in VAD; mechanisms are framework-specific per paper
- Break condition: If attention fails to capture semantic relevance (e.g., attends to syntactic rather than semantic patterns), importance scores will not reflect true contributing factors.

## Foundational Learning

- **Vision-Language Models (VLMs) and caption generation**
  - Why needed here: TbVAD relies entirely on caption quality. Understanding how VLMs encode visual-linguistic alignments and their failure modes on surveillance footage is essential.
  - Quick check question: Given a cluttered surveillance frame with a small anomaly (e.g., distant fight), would a VLM caption likely capture the anomaly, the scene context, or neither?

- **Cross-attention mechanisms for alignment**
  - Why needed here: Slot-wise importance estimation uses cross-attention to align textual descriptions with knowledge slot prototypes. Misunderstanding attention computation leads to incorrect interpretation of importance scores.
  - Quick check question: If description embeddings H_d ∈ R^{T×d} and slot prototypes K_v ∈ R^{S×d}, what does each row of the attention matrix A = softmax(K_v H_d^T / √d) represent?

- **Weakly supervised learning with Multiple Instance Learning (MIL)**
  - Why needed here: The framework operates under weak supervision (video-level labels only). Understanding MIL's label ambiguity problem clarifies why textual reasoning helps separate normal/abnormal segments.
  - Quick check question: In a weakly supervised anomaly detection setting, why might the highest-scoring snippet in an abnormal-video bag still be normal?

## Architecture Onboarding

- **Component map:**
  ```
  Video Input → Frame Sampler → K evenly-spaced frames → VLM Captioner (Molmo/BLIP2) → Fine-grained captions F'_d
                                                      ↓
  ┌─────────────────────────────────────────────────────┐
  │                                                     │
  [Text Understanding Branch]              [Structured Knowledge Branch]
  BERT encoder on captions                 LLM summarization → 4 slots
  → P_d embedding                          Longformer encoder → P_V embedding
      │                                         │
      └──────────→ [Feature Fusion] ←───────────┘
                        ↓
              [Classification Head] → Anomaly score y
                        ↓
              [Explainable Reasoning Branch]
              Cross-attention: H_d ↔ K_v
              → Slot importance w_s
              → Evidence retrieval e*_v,s
              → [Qwen2.5-7B] → Explanation text
  ```

- **Critical path:** Frame sampling → VLM captioning → Dual encoding (fine-grained via BERT, structured via Longformer+LLM) → Feature fusion → Classification. The Explainable Reasoning Branch runs in parallel for interpretation but is not required for detection.

- **Design tradeoffs:**
  - VLM selection: Molmo produces richer captions (2602 avg length, TF-IDF 4.29) but is computationally infeasible for per-frame UCF-Crime evaluation. BLIP2 used for UCF instead.
  - Slot combinations: Table 3 shows object+environment (85.42%) outperforms all four slots (83.42%) on UCF; optimal slot subset is dataset-dependent.
  - Frozen vs. trainable: VLM, LLM for knowledge generation, and explanation generator are frozen; only BERT encoder and classification head are trainable.

- **Failure signatures:**
  - Low AP/AUC with high slot importance variance: Caption quality issue—VLM fails to capture anomaly-relevant semantics
  - High accuracy but uninformative explanations: Evidence retrieval threshold too permissive; retrieve top-1 instead of top-2
  - Cross-dataset performance collapse: Structured knowledge overfits to training distribution; Table 4 shows UCF→XD drops to 74.93% AP

- **First 3 experiments:**
  1. **Caption quality audit:** Generate captions for 50 sampled frames from each dataset using multiple VLMs (BLIP2, Molmo, LLaVA). Manually assess whether anomaly-relevant details are captured. This validates the foundational assumption before building downstream components.
  2. **Slot ablation sweep:** Train with each slot individually and all 15 combinations (2^4 - 1). Plot performance vs. slot combination to identify dataset-specific optimal configurations. Reproduces Table 3 and reveals whether all four slots are necessary.
  3. **Explanation alignment check:** For 30 detected anomalies, manually compare generated explanations against ground-truth anomaly types. Measure whether top-2 important slots align with human-identified causal factors. This validates the attention-to-importance mechanism before deployment.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the TbVAD pipeline be optimized for real-time analysis?
- Basis in paper: [explicit] The conclusion explicitly states future work will focus on extending TbVAD toward real-time analysis.
- Why unresolved: The authors note that using high-quality VLMs (like Molmo) for frame-level captioning on large datasets is currently "computationally infeasible," relying instead on frozen encoders or sparser sampling.
- What evidence would resolve it: Demonstration of a lightweight captioning mechanism or knowledge distillation technique that maintains detection accuracy (AUC/AP) while processing streams at real-time speeds (e.g., $\geq$ 25 FPS).

### Open Question 2
- Question: Under what specific conditions does the inclusion of "action" and "context" semantic slots introduce noise rather than improving detection accuracy?
- Basis in paper: [inferred] Table 3 reveals that using all four slots yields lower performance (83.42% AUC) than using only the "object" and "environment" pair (85.42% AUC).
- Why unresolved: The paper demonstrates that structured knowledge helps, but does not explain why the theoretically most comprehensive representation (all 4 slots) underperforms specific subsets, suggesting potential semantic conflicts or noise in certain slots.
- What evidence would resolve it: An ablation study analyzing the information gain vs. noise entropy for individual slots across different anomaly categories, or a dynamic slot-weighting mechanism that outperforms static combinations.

### Open Question 3
- Question: Does the framework's dependence on VLM-generated captions impose a hard ceiling on detection performance for low-resolution inputs compared to visual-feature-based models?
- Basis in paper: [inferred] The authors acknowledge that UCF-Crime's low resolution limits the precision of generated captions, whereas XD-Violence's clearer footage yields significantly better results (97.34% AP).
- Why unresolved: It is unclear if the text-based reasoning fails to recover the subtle visual cues (e.g., "minor object movements") that visual-feature models might capture, creating a modality-specific performance gap.
- What evidence would resolve it: A comparative analysis on extremely low-resolution subsets where visual features are down-sampled to test if textual abstraction is more robust than direct visual feature extraction.

## Limitations

- Computational infeasibility of high-quality VLM captioning for per-frame analysis on large datasets (Molmo used only for XD-Violence, BLIP2 for UCF-Crime)
- Performance degradation in cross-dataset transfer (UCF→XD drops to 74.93% AP), indicating overfitting to training distribution
- No validation of VLM caption quality on extremely low-resolution footage, leaving open whether text-based reasoning has inherent performance ceilings compared to visual features

## Confidence

- **Method implementation**: Medium - Core architecture described but key details like prompt templates, hyperparameters, and training procedures are unspecified
- **Reproducibility**: Medium - Code not provided, multiple unknown implementation details, but evaluation setup and metrics are clearly specified
- **Results interpretation**: High - Performance claims are directly supported by tables with clear comparisons to baselines and ablation studies
- **Generalizability**: Low - Limited to two datasets with specific characteristics; cross-dataset transfer shows significant performance drop

## Next Checks

1. **Caption quality verification**: Generate captions for 50 sampled frames from each dataset using multiple VLMs and manually assess whether anomaly-relevant details are captured to validate the foundational assumption

2. **Slot combination optimization**: Perform exhaustive ablation over all 15 slot combinations (2^4 - 1) to identify dataset-specific optimal configurations and understand when slot inclusion introduces noise

3. **Explanation quality validation**: For 30 detected anomalies, manually compare generated explanations against ground-truth anomaly types to verify whether top-2 important slots align with human-identified causal factors