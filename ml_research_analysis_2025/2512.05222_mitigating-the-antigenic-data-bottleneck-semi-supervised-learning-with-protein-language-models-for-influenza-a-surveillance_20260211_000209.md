---
ver: rpa2
title: 'Mitigating the Antigenic Data Bottleneck: Semi-supervised Learning with Protein
  Language Models for Influenza A Surveillance'
arxiv_id: '2512.05222'
source_url: https://arxiv.org/abs/2512.05222
tags:
- data
- antigenic
- learning
- influenza
- unlabelled
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study addresses the challenge of predicting influenza A virus
  antigenicity using machine learning, particularly when labeled data are scarce due
  to the labor-intensive nature of traditional assays. The authors propose combining
  Protein Language Models (PLMs) with Semi-Supervised Learning (SSL) to leverage abundant
  unlabeled genomic data.
---

# Mitigating the Antigenic Data Bottleneck: Semi-supervised Learning with Protein Language Models for Influenza A Surveillance

## Quick Facts
- arXiv ID: 2512.05222
- Source URL: https://arxiv.org/abs/2512.05222
- Reference count: 22
- Semi-supervised learning with protein language models improves influenza A virus antigenicity prediction under label scarcity

## Executive Summary
This study addresses the challenge of predicting influenza A virus antigenicity using machine learning when labeled data are scarce due to the labor-intensive nature of traditional assays. The authors propose combining Protein Language Models (PLMs) with Semi-Supervised Learning (SSL) to leverage abundant unlabeled genomic data. They evaluate four PLM embeddings (ESM-2, ProtVec, ProtT5, ProtBert) with two SSL strategies (Self-training, Label Spreading) across four IAV subtypes (H1N1, H3N2, H5N1, H9N2). Results show that SSL significantly improves predictive performance under low-label regimes, with Self-training using ProtVec yielding the largest relative gains. ESM-2 proved remarkably robust, maintaining F1 scores above 0.82 even with only 25% labeled data. While H1N1 and H9N2 were predicted with high accuracy, H3N2 remained challenging due to its hypervariability.

## Method Summary
The method combines Protein Language Models (ESM-2, ProtVec, ProtT5, ProtBert) with Semi-Supervised Learning strategies (Self-training, Label Spreading) to predict influenza A virus antigenicity from hemagglutinin (HA) sequences. The approach uses Archetti-Horsfall distances derived from HI titers to label virus-antiserum pairs as "Similar" or "Variant". A nested 5-fold cross-validation framework with label masking (25%, 50%, 75%, 100%) simulates label scarcity conditions. Mean-pooled PLM embeddings serve as features for Random Forest, SVM, CNN, and BiGRU classifiers, with SSL wrappers applied to incorporate unlabeled surveillance data.

## Key Results
- SSL consistently improved performance under label scarcity across all subtypes
- ESM-2 remained highly robust, achieving F1 scores above 0.82 with only 25% labeled data
- Self-training with ProtVec produced the largest relative gains, showing SSL can compensate for lower-resolution representations
- H3N2 remained challenging (F1 ~0.70-0.75) due to continuous antigenic drift weakening cluster assumptions

## Why This Works (Mechanism)

### Mechanism 1: SSL Compensates for Lower-Quality Embeddings
Semi-supervised learning compensates for lower-quality embeddings by leveraging manifold structure in unlabeled data. Self-training iteratively expands decision boundaries into low-density regions by pseudo-labeling high-confidence predictions, providing structural regularization most beneficial when initial feature representations are sparse or noisy. The cluster assumption holdsâ€”points close in embedding space share the same label. Evidence shows ProtVec, which relies on local k-mer contexts, performed poorly in supervised settings but saw the largest relative improvement when SSL was applied. This mechanism may fail when class boundaries are highly complex and overlapping (e.g., H3N2's continuous drift), where pseudo-labeling may introduce noise rather than signal.

### Mechanism 2: Evolutionary-Scale Pre-training Encodes Antigenic Determinants
ESM-2's masked language modeling on UniRef50 captures long-range dependencies and structural constraints unsupervised, providing an "evolutionary prior" that acts as a regularizer, making the embedding inherently informative for antigenicity even without task-specific fine-tuning. The core assumption is that antigenic determinants correlate with structural and evolutionary constraints learned by the PLM. ESM-2 remained highly robust, achieving F1 scores above 0.82 with only 25% labeled data, indicating that its embeddings inherently capture antigenic determinants. This mechanism may break if antigenic drift involves rare mutations not well-represented in UniRef50, where pre-training may not capture relevant features.

### Mechanism 3: Label Spreading Propagates Information via Graph-Based Similarity
Label spreading propagates information via graph-based similarity, effective when antigenic relationships form discernible clusters. A similarity graph is constructed where nodes are samples and edges represent embedding-space proximity, with labels propagating from known to unknown nodes based on smoothness assumptions. The core assumption is that antigenically similar sequences cluster in PLM embedding space. SSL consistently improved performance under label scarcity, with label spreading particularly effective for capturing the manifold structure of viral evolution. This mechanism may fail for hypervariable subtypes like H3N2 with "ladder-like" phylogeny, where distinct clusters are less defined, weakening the cluster assumption.

## Foundational Learning

- **Semi-supervised learning (SSL)**
  - Why needed here: HI assay labels are scarce; genomic data is abundant. SSL bridges this gap.
  - Quick check question: Can you explain why self-training might fail if the initial classifier is poorly calibrated?

- **Protein Language Models (PLMs)**
  - Why needed here: Raw sequences lack fixed-length representations; PLMs provide context-aware embeddings capturing evolutionary and structural information.
  - Quick check question: What is the difference between ProtVec (k-mer based) and ESM-2 (Transformer-based) in terms of what they capture?

- **The cluster assumption**
  - Why needed here: SSL methods like label spreading rely on the assumption that nearby points in embedding space share labels.
  - Quick check question: Why might H3N2 violate this assumption more than H1N1?

## Architecture Onboarding

- **Component map:** HA sequences (HA1 region) -> Embedding layer (ESM-2/ProtVec/ProtBert/ProtT5) -> Pooling (mean-pooling) -> Classifier (Random Forest/SVM/CNN/BiGRU) -> SSL wrapper (Self-training/Label Spreading) -> Evaluation (nested 5-fold CV with label masking)

- **Critical path:**
  1. Curate labeled pairs (Archetti-Horsfall distance from HI titers) + unlabeled sequences
  2. Generate PLM embeddings for all sequences
  3. Initialize classifier on labeled subset
  4. Apply SSL strategy to incorporate unlabeled data
  5. Evaluate via nested CV with F1 scoring

- **Design tradeoffs:**
  - ESM-2: Highest robustness, lowest variance, but requires more compute than ProtVec
  - ProtVec + SSL: Largest relative gains under scarcity, but lower ceiling than ESM-2
  - Self-training vs. Label Spreading: Self-training more effective for weak embeddings; Label Spreading better for capturing manifold structure but sensitive to graph construction

- **Failure signatures:**
  - H3N2 underperformance (F1 ~0.70-0.75): Continuous antigenic drift weakens cluster assumption
  - Performance degradation at high label ratios for SSL: Pseudo-labeling introduces noise when class boundaries are complex
  - High variance across folds: Embedding quality insufficient for stable decision boundaries

- **First 3 experiments:**
  1. Replicate ESM-2 + Self-training on H1N1 with 25% labels; confirm F1 > 0.82 baseline
  2. Ablate ProtVec + Self-training vs. supervised ProtVec to quantify SSL's "equalizer effect"
  3. Test H3N2 with increased `n_neighbors` in Label Spreading to probe whether denser graphs mitigate hypervariability challenges

## Open Questions the Paper Calls Out
None

## Limitations
- The specific distance thresholds used to binarize Archetti-Horsfall distances into "Similar" vs. "Variant" classes are not specified, potentially affecting reproducibility
- H3N2's continuous antigenic drift creates a fundamental limitation, with absolute performance ceiling remaining low (F1 ~0.70-0.75) regardless of method
- The generation logic for the 15,283 unlabeled pairs (random sampling vs. specific surveillance isolates) remains unclear

## Confidence
- **High confidence:** ESM-2's robustness and effectiveness in capturing antigenic determinants through evolutionary-scale pre-training
- **Medium confidence:** SSL's ability to compensate for weaker embeddings like ProtVec, though the magnitude of gains depends heavily on implementation details
- **Medium confidence:** The general trend that SSL improves performance under label scarcity, with some uncertainty about optimal SSL strategy selection for different PLM qualities

## Next Checks
1. Verify the exact distance threshold used for binarizing Archetti-Horsfall distances by examining the cited datasets' supplementary materials
2. Replicate the nested CV results for ESM-2 + Self-training on H1N1 with 25% labels to confirm F1 > 0.82
3. Conduct ablation studies comparing ProtVec performance with and without SSL to quantify the "equalizer effect" across different label ratios