---
ver: rpa2
title: 'X-VFL: A New Vertical Federated Learning Framework with Cross Completion and
  Decision Subspace Alignment'
arxiv_id: '2508.05568'
source_url: https://arxiv.org/abs/2508.05568
tags:
- missing
- features
- inference
- x-vfl
- clients
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'X-VFL is a new vertical federated learning framework that handles
  non-aligned data samples with partially missing features and supports locally independent
  inference at each client. It introduces two novel modules: Cross Completion (XCom),
  which reconstructs missing features by leveraging information from other clients,
  and Decision Subspace Alignment (DS-Align), which aligns local features with completed
  and global features to enable independent inference.'
---

# X-VFL: A New Vertical Federated Learning Framework with Cross Completion and Decision Subspace Alignment

## Quick Facts
- arXiv ID: 2508.05568
- Source URL: https://arxiv.org/abs/2508.05568
- Authors: Qinghua Yao; Xiangrui Xu; Zhize Li
- Reference count: 40
- X-VFL significantly outperforms existing methods, achieving up to 15% accuracy improvement on CIFAR-10 and 43% on MIMIC-III, while maintaining robust performance even under high feature missing rates (up to 90%).

## Executive Summary
X-VFL addresses the challenge of vertical federated learning with non-aligned data samples containing partially missing features. The framework introduces Cross Completion (XCom) to reconstruct missing features using embeddings from other clients, and Decision Subspace Alignment (DS-Align) to enable locally independent inference. By replacing concatenation with averaging for feature aggregation, X-VFL supports independent prediction at each client while maintaining collaborative performance. The framework demonstrates theoretical convergence guarantees (O(1/√T) for SGD-type algorithms and O(1/T) for PAGE-type algorithms) and significant empirical improvements over existing methods.

## Method Summary
X-VFL is a vertical federated learning framework that handles non-aligned data samples with partially missing features through two novel modules: Cross Completion (XCom) and Decision Subspace Alignment (DS-Align). XCom reconstructs missing features by leveraging information from other clients' embeddings, while DS-Align aligns local features with completed and global features to enable independent inference. The framework uses averaging (instead of concatenation) to aggregate client embeddings, ensuring dimensionally consistent inference between collaborative and independent modes. Experiments on CIFAR-10, TinyImageNet, UTKFace, MIMIC-III, Bank, and Avazu datasets show significant accuracy improvements, particularly under high missingness rates (up to 90%).

## Key Results
- Achieves up to 15% accuracy improvement on CIFAR-10 compared to existing VFL methods
- Demonstrates 43% accuracy improvement on MIMIC-III dataset
- Maintains robust performance with up to 90% feature missing rates
- Proves O(1/√T) convergence for SGD-type algorithms and O(1/T) for PAGE-type algorithms

## Why This Works (Mechanism)

### Mechanism 1: Cross-Client Feature Imputation via Embedding Mapping
XCom recovers utility from non-aligned data by conditionally reconstructing missing features using embeddings from aligned clients. A completer network maps Client B's embedding to a reconstruction of Client A's missing features, allowing data-poor clients to leverage the representational power of data-rich partners to fill local gaps.

### Mechanism 2: Dimensionality Preservation via Averaged Aggregation
X-VFL replaces standard concatenation with averaging to aggregate embeddings. This ensures the input dimension to the server's top model remains constant regardless of the number of active clients, allowing a single client's local embedding to be fed directly into the server model during independent inference.

### Mechanism 3: Decision Subspace Alignment for Knowledge Distillation
DS-Align enforces that the output of the top model using a single client's features mimics the output using averaged features. This acts as knowledge distillation where the global model (averaged inputs) trains the local model (single input) to maintain decision boundaries even when partners are missing.

## Foundational Learning

- **Split Neural Networks (SplitNN)**: X-VFL is a SplitNN variation where bottom models reside on clients and a top model resides on the server. Understanding where the cut layer occurs is essential for debugging communication bottlenecks.
  - Quick check: Can you identify which part of the model backpropagates gradients to the XCom module?

- **Knowledge Distillation**: The DS-Align mechanism effectively distills the "knowledge" of the collaborative model (teacher) into the local models (students) via MSE or similar losses on the decision logits.
  - Quick check: In Eq. (4), which term represents the teacher signal and which represents the student signal?

- **Stochastic Gradient Variance (SGD vs. PAGE)**: The paper theoretically proves convergence rates (O(1/√T) vs O(1/T)). Understanding variance reduction is key to selecting the right optimizer for high-missing-rate scenarios.
  - Quick check: Why does the PAGE algorithm theoretically require fewer communication rounds (T) to converge than standard SGD in this framework?

## Architecture Onboarding

- **Component map**: Local Data → (Masking) → Local Model → Embeddings → Server (Avg) → Logits
- **Critical path**: The DS-Align loss calculation is the most complex integration point, requiring buffering outputs of both local pass (h(Ei)) and collaborative pass (h(Eavg)) simultaneously to compute the alignment penalty.
- **Design tradeoffs**:
  - Averaging vs. Concatenation: Averaging enables independent inference but may reduce representational capacity for the collaborative model
  - XCom Complexity: A complex XCom generator improves data utilization but increases the risk of hallucinating features when inter-client correlations are weak
- **Failure signatures**:
  - Mode Collapse: If λ (alignment weight) is too high, local models may ignore actual features to output the "mean" embedding
  - XComp Divergence: If reconstruction loss fails to converge, the reconstructed features act as adversarial noise
  - Communication Deadlock: Non-aligned samples require specific activation of loss terms; incorrect masking logic can lead to NaN gradients
- **First 3 experiments**:
  1. Baseline Sanity Check: Run X-VFL with 0% missing features and verify if "Independent Inference" accuracy matches "Collaborative Inference"
  2. Ablation on XCom: Compare performance on 50% missing data with XCom disabled vs. enabled
  3. Hyperparameter Sensitivity: Sweep λ₁ and λ₂ on a validation set to find the equilibrium where reconstruction is accurate without overpowering the primary classification loss

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but several significant areas remain unexplored based on the current work.

## Limitations
- Architectural details of the XCom module (specifically layer structure and dimensions) are not explicitly specified, requiring assumptions for reproduction
- Weight hyperparameters (λ₁, λ₂) and optimizer configurations are not fully detailed in the experimental section
- The exact masking protocol for creating non-aligned samples across different datasets is not specified
- Experiments are limited to k=4 clients, leaving scalability to large-scale settings uncertain

## Confidence
- **High Confidence** in: The core architectural contribution (averaging vs. concatenation enabling independent inference), the convergence rate proofs, and the experimental methodology
- **Medium Confidence** in: The specific implementation details of XCom and DS-Align modules, the exact data partitioning protocols used across different datasets, and the hyperparameter sensitivity analysis
- **Low Confidence** in: The security analysis of XCom introducing privacy vulnerabilities, and the performance degradation under Missing Not at Random (MNAR) patterns

## Next Checks
1. **Architecture Verification**: Implement the XCom module with assumed MLP architecture and validate that feature reconstruction quality correlates with downstream accuracy improvements

2. **Averaging vs. Concatenation Baseline**: Run controlled experiments comparing averaging aggregation against concatenation to quantify the trade-off between independent inference capability and collaborative model capacity

3. **Convergence Rate Validation**: Replicate the convergence analysis using both SGD and PAGE optimizers on a synthetic dataset with controlled missingness rates to verify the theoretical O(1/√T) vs O(1/T) scaling claims