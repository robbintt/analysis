---
ver: rpa2
title: What's Taboo for You? - An Empirical Evaluation of LLMs Behavior Toward Sensitive
  Content
arxiv_id: '2507.23319'
source_url: https://arxiv.org/abs/2507.23319
tags:
- sensitivity
- llms
- sentences
- language
- content
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates whether Large Language Models (LLMs) like
  GPT-4o-mini implicitly sanitize sensitive content during paraphrasing, even without
  explicit detoxification instructions. The researchers collected sentences containing
  sensitive expressions, generated paraphrases using GPT-4o-mini, and annotated both
  original and paraphrased sentences with four sensitivity levels (Formal/Polite,
  Informal, Derogatory, Taboo) by human experts.
---

# What's Taboo for You? - An Empirical Evaluation of LLMs Behavior Toward Sensitive Content

## Quick Facts
- arXiv ID: 2507.23319
- Source URL: https://arxiv.org/abs/2507.23319
- Reference count: 11
- Key outcome: GPT-4o-mini systematically reduces sensitivity in paraphrased content, confirmed by statistical tests (χ²=138.255, p<0.0001).

## Executive Summary
This study investigates whether Large Language Models like GPT-4o-mini implicitly sanitize sensitive content during paraphrasing, even without explicit detoxification instructions. The researchers collected sentences containing sensitive expressions, generated paraphrases using GPT-4o-mini, and annotated both original and paraphrased sentences with four sensitivity levels by human experts. Results show GPT-4o-mini systematically reduces sensitivity, especially for Taboo and Derogatory content, while traditional classifiers outperformed open-source LLMs in replicating human judgments.

## Method Summary
The study collected 599 sensitive expressions from specialized dictionaries and retrieved 23,347 sentences from the English Web Corpus (enTenTen21). GPT-4o-mini generated three paraphrases per sentence (70,041 outputs total), which were then annotated by 10 expert annotators using a four-category sensitivity scale. The researchers compared human annotations with zero-shot classifications from open-source LLMs and traditional text classifiers, using Bowker's test and classification metrics to evaluate sensitivity shifts.

## Key Results
- GPT-4o-mini systematically moderates content toward less sensitive classes, with substantial reductions in derogatory and taboo language.
- Traditional classifiers (F1=0.53) outperformed open-source LLMs (best F1=0.47) in replicating human sensitivity judgments.
- Sensitivity reduction is asymmetric—Taboo content experienced a ∆ of -1.7 while Formal/Polite content saw a +0.16 shift.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GPT-4o-mini performs implicit content moderation during paraphrasing without explicit detoxification instructions.
- Mechanism: RLHF alignment training embeds harm-avoidance preferences into model behavior, causing automatic sanitization during text generation tasks.
- Core assumption: Alignment techniques produce generalized harm-avoidance that persists across paraphrasing tasks.
- Evidence anchors: [abstract] "GPT-4o-mini systematically moderates content toward less sensitive classes"; [section 4.1] Bowker's test χ² = 138.255, p < 0.0001.
- Break condition: If explicit detoxification instructions were inadvertently included in prompts.

### Mechanism 2
- Claim: Sensitivity reduction is asymmetric—higher-sensitivity content experiences larger downward shifts.
- Mechanism: Alignment training penalizes harmful outputs more heavily for content nearer to policy boundaries.
- Core assumption: The four-category ordinal scale maps to alignment penalty intensity during training.
- Evidence anchors: [section 4.1] Taboo: ∆ = -1.7; Derogatory: ∆ = -0.87; Informal: ∆ = -0.38; Formal/Polite: ∆ = +0.16.
- Break condition: If categories don't reflect alignment training's internal harm representation.

### Mechanism 3
- Claim: Traditional classifiers outperform open-source LLMs in replicating human sensitivity judgments.
- Mechanism: Domain-specific supervised training on in-domain annotations captures patterns that zero-shot LLMs miss.
- Core assumption: The annotated subset sufficiently represents sensitivity patterns for supervised learning.
- Evidence anchors: [section 4.2] Born Classifier: 0.53 F1 vs. best LLM (Gemma) at 0.47 F1.
- Break condition: With larger annotated datasets or fine-tuned LLMs, this gap may narrow.

## Foundational Learning

- Concept: RLHF (Reinforcement Learning from Human Feedback)
  - Why needed here: Explains why GPT-4o-mini sanitizes content implicitly—it's trained to avoid harm signals from human raters.
  - Quick check question: Can you explain how reward model training creates generalized harm-avoidance beyond explicit refusals?

- Concept: Confusion matrices for asymmetric transitions
  - Why needed here: Core evaluation tool showing directionality of sensitivity shifts (not just accuracy).
  - Quick check question: In a 4×4 sensitivity confusion matrix, what does concentration above vs. below the diagonal indicate?

- Concept: Zero-shot classification vs. supervised learning tradeoffs
  - Why needed here: Explains why traditional classifiers outperformed zero-shot LLMs despite LLMs' general capabilities.
  - Quick check question: Why might a Naïve Bayes classifier trained on 1,800 annotated sentences outperform a 14B-parameter LLM on the same task?

## Architecture Onboarding

- Component map: Expression Collection -> Sentence Retrieval -> Paraphrase Generation -> Annotation Layer -> Classification Benchmarks
- Critical path: Expression selection → Sentence collection → Paraphrase generation → Expert annotation → Automated classification evaluation
- Design tradeoffs: Single-source corpus limits domain diversity but ensures consistent methodology; three paraphrases per sentence increases variability but adds complexity.
- Failure signatures: Low inter-annotator agreement would undermine ground-truth validity; high refusal rate would create selection bias.
- First 3 experiments:
  1. Replicate sensitivity shift on new corpus (e.g., Reddit, news comments) to test generalization beyond enTenTen21 web text.
  2. Fine-tune an open-source LLM on the 1,800+ annotated sentences and compare against zero-shot and traditional classifiers.
  3. Test explicit vs. implicit instruction conditions by adding "preserve original tone" to paraphrase prompts.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do other proprietary and open-source LLMs exhibit similar implicit moderation behavior during paraphrasing?
- Basis in paper: [inferred] Only GPT-4o-mini's paraphrasing behavior was examined; open-source LLMs were only used as classifiers.
- Why unresolved: Only one model's paraphrasing behavior was examined; alignment techniques vary across providers.
- What evidence would resolve it: Systematic evaluation of multiple LLMs using identical paraphrasing prompts and annotation protocols.

### Open Question 2
- Question: To what extent does implicit sanitization preserve semantic meaning and intent when reducing sensitivity?
- Basis in paper: [inferred] The study measured sensitivity shifts but did not evaluate semantic preservation.
- Why unresolved: No semantic preservation metric was included.
- What evidence would resolve it: Add human or automated semantic similarity evaluations between original and paraphrased sentences.

### Open Question 3
- Question: How does implicit moderation behavior generalize across languages and cultural contexts beyond English?
- Basis in paper: [explicit] The authors state that relying solely on the English Web Corpus "could have limited the scope of our evaluation."
- Why unresolved: All data came from a single English corpus.
- What evidence would resolve it: Replicate the study with multilingual corpora and native speaker annotators.

### Open Question 4
- Question: Can fine-tuning or prompt engineering improve LLMs' alignment with human sensitivity judgments?
- Basis in paper: [inferred] Traditional classifiers outperformed zero-shot LLMs (best F1 = 0.53).
- Why unresolved: Only zero-shot prompting was tested.
- What evidence would resolve it: Compare fine-tuned and few-shot LLM classifiers against traditional baselines.

## Limitations
- Ground truth validity uncertain due to unreported inter-annotator agreement and annotation guidelines.
- Limited generalizability from exclusive use of English Web Corpus data.
- Zero-shot evaluation may systematically underestimate LLM capabilities.
- Four-category ordinal scale assumes linear relationships that may not reflect alignment training.

## Confidence

**High Confidence:** GPT-4o-mini systematically reduces sensitivity in paraphrased content.

**Medium Confidence:** Traditional classifiers outperform open-source LLMs on this specific task.

**Medium Confidence:** Alignment training causes implicit content moderation during paraphrasing.

**Low Confidence:** Sensitivity reduction is asymmetric with gradient pressure toward lower categories.

## Next Checks

1. Replicate sensitivity shift on new corpora using Reddit comments, news comments, and social media data to test generalization.

2. Fine-tune an open-source LLM on the 1,800+ annotated sentences and compare performance against both zero-shot LLMs and traditional classifiers.

3. Test explicit vs. implicit instruction conditions by adding "preserve original tone and sensitivity level" to paraphrase prompts.