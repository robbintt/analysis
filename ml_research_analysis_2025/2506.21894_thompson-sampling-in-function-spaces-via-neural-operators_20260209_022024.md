---
ver: rpa2
title: Thompson Sampling in Function Spaces via Neural Operators
arxiv_id: '2506.21894'
source_url: https://arxiv.org/abs/2506.21894
tags:
- neural
- operator
- where
- which
- kernel
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Neural operator Thompson sampling (NOTS) optimizes functionals
  of unknown operators between function spaces by integrating neural operator surrogates
  with Thompson sampling. The method avoids explicit uncertainty quantification by
  training randomly initialized neural operators to approximate posterior samples
  from a vector-valued Gaussian process, enabling scalable function-space Bayesian
  optimization.
---

# Thompson Sampling in Function Spaces via Neural Operators

## Quick Facts
- arXiv ID: 2506.21894
- Source URL: https://arxiv.org/abs/2506.21894
- Authors: Rafael Oliveira; Xuesong Wang; Kian Ming A. Chai; Edwin V. Bonilla
- Reference count: 40
- Primary result: Neural operator Thompson sampling (NOTS) achieves superior sample efficiency in function-space Bayesian optimization by avoiding explicit uncertainty quantification through random neural operator initialization.

## Executive Summary
NOTS introduces a scalable approach for optimizing functionals of unknown operators between function spaces by combining neural operator surrogates with Thompson sampling. The method avoids explicit uncertainty quantification by training randomly initialized neural operators to approximate posterior samples from a vector-valued Gaussian process. Theoretical results establish regret bounds and connect neural operators to Gaussian processes in infinite-dimensional settings. Experiments on Darcy flow and shallow water PDE benchmarks demonstrate NOTS achieving superior sample efficiency and performance compared to Bayesian optimization baselines, particularly in high-dimensional input settings where GP-based methods struggle.

## Method Summary
NOTS optimizes functionals of unknown operators G* between function spaces by training randomly initialized neural operator surrogates to approximate posterior samples. For each iteration t, the method reinitializes FNO weights θt,0∼N(0,Σ0), trains via regularized loss minimization (10 epochs mini-batch SGD with lr=10^-3 and cosine annealing), and selects the input a that maximizes the functional f(Gθt(a)). The method uses the implicit GP correspondence of neural operators with He initialization to avoid explicit uncertainty quantification. Theoretical analysis establishes sublinear regret bounds under assumptions of finite search space and bounded linear functionals.

## Key Results
- NOTS achieves cumulative regret of 0.37 on Darcy flow after 300 iterations versus 0.71 for GP-TS and 1.2 for BFO
- On shallow water equations with 32×64 grids, NOTS reaches average regret of 0.2 after 200 iterations while GP-based methods crash due to memory constraints
- NOTS demonstrates superior performance on high-dimensional input spaces where traditional GP methods fail due to computational complexity

## Why This Works (Mechanism)
NOTS bypasses explicit uncertainty quantification by leveraging the implicit GP correspondence of randomly initialized neural operators. Each reinitialization θt,0∼N(0,Σ0) generates a posterior sample approximation through training, enabling Thompson sampling without computing GP covariances. The infinite-width neural operator kernel provides a principled Bayesian prior over function spaces, while the training procedure efficiently explores the posterior distribution through stochastic optimization.

## Foundational Learning
- **Function spaces and operators**: Needed to understand optimization over infinite-dimensional input-output mappings; check by verifying understanding of how Darcy flow maps permeability to pressure
- **Neural operator theory**: Required for grasping the connection between random initialization and GP priors; check by confirming understanding of how FNOs approximate integral operators
- **Thompson sampling in infinite dimensions**: Essential for extending Bayesian optimization to function spaces; check by understanding why random initialization approximates posterior sampling
- **Regret analysis**: Necessary for theoretical guarantees; check by following the proof that cumulative regret grows sublinearly with T
- **Fourier neural operators**: Core architecture for learning operator mappings; check by understanding how Fourier layers enable resolution-invariant computation
- **Gaussian process infinite-width limits**: Provides theoretical foundation for implicit priors; check by verifying the NNGP kernel correspondence

## Architecture Onboarding

**Component Map**: Random initialization → FNO training → Functional maximization → Oracle query → Dataset update

**Critical Path**: The sequence of reinitializing weights, training on current dataset, computing argmax functional, querying oracle, and updating dataset forms the core loop that enables sample-efficient optimization.

**Design Tradeoffs**: NOTS trades explicit uncertainty quantification (computationally expensive for GPs) for implicit sampling through random initialization (scalable but requires careful initialization). The choice of 10 training epochs balances exploration (fresh initialization) with exploitation (learned posterior structure).

**Failure Signatures**: Poor exploration occurs when posterior variance is underestimated, leading to premature convergence. Memory crashes manifest when GP baselines attempt to handle high-dimensional inputs. Suboptimal performance indicates insufficient training or inappropriate functional choices.

**First Experiments**: 1) Verify FNO architecture scaling on Darcy flow with different layer depths and Fourier modes; 2) Test search space construction sensitivity using low-rank approximation methods; 3) Compare NOTS against noise-injected variants to calibrate uncertainty estimates.

## Open Questions the Paper Calls Out

**Open Question 1**: Can NOTS regret bounds be extended to continuous function domains rather than finite search spaces? The current analysis assumes finite |S| < ∞, limiting practical applicability. Resolution would require theoretical derivation for infinite function spaces with appropriate parameterizations.

**Open Question 2**: Can GP correspondence and regret guarantees extend to multi-layer neural operators? Current theory only covers single hidden layers, with multi-layer extensions facing potential bottleneck effects in infinite-dimensional intermediate spaces.

**Open Question 3**: How does NOTS perform under prior misspecification where the true operator differs from the assumed GP prior? Real-world operators may not align with the implicit prior defined by neural operator architecture and initialization.

**Open Question 4**: Can regret bounds be derived for nonlinear functionals beyond the bounded linear functionals assumed in Proposition 2? The quadratic potential energy functional and other nonlinear objectives used in experiments lack theoretical guarantees.

## Limitations
- Incomplete architectural specifications prevent precise reproduction of reported results
- Search space construction method and initial dataset size D₀ remain unspecified
- Theoretical analysis limited to bounded linear functionals, excluding many practical objectives
- Prior misspecification sensitivity not fully characterized for real-world operators

## Confidence
- **High Confidence**: Core theoretical framework connecting neural operators to Gaussian processes in function spaces
- **Medium Confidence**: Empirical performance claims against baselines, limited by incomplete architectural details
- **Low Confidence**: Precise reproduction of shallow water equation results due to missing FNO architecture specifications

## Next Checks
1. Verify FNO architecture scaling by testing different layer depths and Fourier mode counts on the Darcy flow benchmark
2. Implement search space S construction using the described low-rank approximation method and test initialization sensitivity
3. Run GP-based baseline implementations with input dimension reduction to confirm reported memory crashes on the shallow water problem