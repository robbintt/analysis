---
ver: rpa2
title: 'DayDreamer at CQs-Gen 2025: Generating Critical Questions through Argument
  Scheme Completion'
arxiv_id: '2505.15554'
source_url: https://arxiv.org/abs/2505.15554
tags:
- critical
- questions
- scheme
- argument
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper describes a critical question generation system that\
  \ uses large language models with chain-of-thought prompting to generate critical\
  \ questions guided by Walton\u2019s argumentation schemes. The approach extracts\
  \ arguments using scheme templates, generates relevant critical questions for each\
  \ scheme, and ranks them to select the top 3 most helpful ones."
---

# DayDreamer at CQs-Gen 2025: Generating Critical Questions through Argument Scheme Completion

## Quick Facts
- arXiv ID: 2505.15554
- Source URL: https://arxiv.org/abs/2505.15554
- Reference count: 3
- Primary result: 4th place out of 13 teams on CQs-Gen shared task with 60 helpful, 25 unhelpful, and 17 invalid questions

## Executive Summary
This paper describes DayDreamer, a critical question generation system that leverages large language models with chain-of-thought prompting to generate questions guided by Walton's argumentation schemes. The system extracts arguments using scheme templates, generates relevant critical questions for each scheme, and ranks them to select the top 3 most helpful ones. DayDreamer achieved competitive performance on the CQs-Gen shared task, placing 4th out of 13 teams, demonstrating its potential for fostering critical thinking and detecting missing or uninformed claims in argumentative text.

## Method Summary
DayDreamer employs a three-stage conversational LLM pipeline to generate critical questions from argumentative text. First, it extracts arguments by providing the LLM with relevant scheme templates and prompts it to instantiate these templates with text content. Second, for each identified scheme, the system generates critical questions using predefined templates from Walton et al. (2008). Finally, it ranks the generated questions and selects the top 3 most helpful ones. The pipeline uses GPT-4o-mini or LLaMa-3.1-8B-Instruct, implements a "sort scheme" technique to group identical scheme names before extraction, and employs fallback prompts when insufficient questions are generated.

## Key Results
- Achieved 4th place out of 13 teams in the CQs-Gen shared task
- Best run produced 60 helpful questions, 25 unhelpful questions, and 17 invalid questions
- Successfully generated scheme-specific critical questions that can foster critical thinking
- Demonstrated ability to detect missing or uninformed claims in argumentative text

## Why This Works (Mechanism)
DayDreamer works by structuring the critical question generation process around established argumentation schemes. By providing the LLM with specific scheme templates and definitions from Walton et al. (2008), the system grounds question generation in recognized argumentation patterns. The chain-of-thought prompting approach allows the model to first identify arguments within the text, then generate relevant critical questions for each identified scheme, and finally rank these questions based on their potential helpfulness. This systematic approach ensures that generated questions are both contextually relevant and aligned with established argumentation theory.

## Foundational Learning
- Argumentation schemes: Formal structures representing common patterns of reasoning (needed to ground question generation in established argumentation theory; quick check: verify all 26 schemes from Walton et al. are correctly implemented)
- Chain-of-thought prompting: Sequential reasoning approach where models break down complex tasks into intermediate steps (needed to structure the three-stage pipeline; quick check: ensure prompts maintain conversational context)
- Walton's critical questions: Specific questions designed to evaluate arguments within particular schemes (needed to generate scheme-specific questions; quick check: confirm CQ templates match those in Walton et al.)
- Scheme template instantiation: Process of filling abstract scheme patterns with concrete text content (needed for argument extraction; quick check: validate extracted arguments match input text)
- Conversational LLM pipelines: Multi-turn interactions with language models (needed to maintain context across extraction, generation, and ranking stages; quick check: verify chat history is preserved)
- Ranking mechanism: Process for selecting the most helpful questions from generated candidates (needed to reduce output to top 3 questions; quick check: confirm ranking criteria align with "helpfulness")

## Architecture Onboarding

Component map: Argument Extraction -> Critical Question Generation -> Ranking

Critical path: The three-stage pipeline forms the core workflow. Each stage builds on the previous one, with argument extraction providing the foundation for question generation, and ranking selecting the final output.

Design tradeoffs: Uses GPT-4o-mini for cost efficiency while maintaining quality, employs "sort scheme" technique to prevent hallucination, and implements fallback prompts to ensure minimum output quantity.

Failure signatures: Hallucination when >2 scheme templates are provided simultaneously; invalid CQs from "ER"-prefixed schemes due to missing definitions; insufficient CQs generated (<3) requiring fallback.

First experiments: 1) Test "sort scheme" technique on interventions with multiple scheme templates to confirm it prevents hallucination; 2) Run the fallback GENERAL_CQ_PROMPT when <3 CQs are generated to assess its effectiveness; 3) Compare outputs using GPT-4o-mini versus LLaMa-3.1-8B-Instruct to evaluate cost-quality tradeoff.

## Open Questions the Paper Calls Out
None

## Limitations
- Exact scheme definitions and CQ templates from Walton et al. (2008) are not fully specified in the paper, affecting reproducibility
- Dataset access and format details are not provided, limiting independent validation
- LLM parameters like temperature and max_tokens are unspecified, potentially causing output variability

## Confidence
- High: The three-stage pipeline architecture and its competitive performance on the CQs-Gen task
- Medium: The effectiveness of the "sort scheme" technique and fallback prompt for handling insufficient CQs
- Low: The generalizability of the approach to other argumentative domains beyond the CQs-Gen dataset

## Next Checks
1. Verify the exact scheme definitions and CQ templates from Walton et al. (2008) used in the prompts
2. Test the "sort scheme" technique on interventions with >2 scheme templates to confirm it prevents hallucination
3. Run the fallback GENERAL_CQ_PROMPT when <3 CQs are generated to assess its effectiveness in increasing output quantity