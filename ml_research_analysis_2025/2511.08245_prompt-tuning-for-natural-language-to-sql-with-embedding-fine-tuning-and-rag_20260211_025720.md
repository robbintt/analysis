---
ver: rpa2
title: Prompt Tuning for Natural Language to SQL with Embedding Fine-Tuning and RAG
arxiv_id: '2511.08245'
source_url: https://arxiv.org/abs/2511.08245
tags:
- error
- prompt
- language
- arxiv
- cases
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents an Error Correction through Prompt Tuning (ECPT)
  framework for NL-to-SQL translation, inspired by medical diagnosis processes. The
  approach combines error classification, retrieval-augmented generation (RAG), and
  embedding fine-tuning to correct SQL query errors.
---

# Prompt Tuning for Natural Language to SQL with Embedding Fine-Tuning and RAG

## Quick Facts
- arXiv ID: 2511.08245
- Source URL: https://arxiv.org/abs/2511.08245
- Reference count: 29
- Primary result: 12% accuracy improvement over baselines on Spider dev set using error correction framework

## Executive Summary
This paper introduces Error Correction through Prompt Tuning (ECPT), a three-step framework for NL-to-SQL error correction inspired by medical diagnosis processes. The approach combines error classification, retrieval-augmented generation (RAG), and embedding fine-tuning to correct SQL query errors. By integrating external knowledge bases and fine-tuning embedding models with custom error datasets, the framework achieves a 12% accuracy improvement over existing baselines on the Spider development set. Experiments demonstrate that embedding fine-tuning and RAG significantly enhance execution accuracy, while the size of correction cases has minimal impact on performance.

## Method Summary
The ECPT framework implements a medical-diagnosis-inspired pipeline that decomposes error correction into three sequential steps: Diagnose (error type classification), Write Prescription (retrieve similar correction cases and generate instructions), and Apply Treatment (execute the correction). The system uses a fine-tuned SentenceTransformer model with triplet loss to cluster error cases by type rather than surface similarity, enabling more relevant RAG retrieval. Correction cases are stored in a FAISS vector database and retrieved during the prescription phase as few-shot examples. The framework processes up to three correction attempts per query, measuring success by execution accuracy on the Spider development set.

## Key Results
- Achieves 88.08% execution accuracy on Spider dev set with GPT-4-turbo and full correction cases
- Embedding fine-tuning improves correction accuracy from 14.17% to 50.20%
- Using only 10% of available correction cases achieves 87.69% accuracy, nearly matching full dataset performance
- RAG-based retrieval with small example sets (10% of available cases) achieves comparable performance to larger sets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Decomposing error correction into discrete diagnostic-prescriptive-treatment phases improves LLM correction accuracy compared to single-step self-correction.
- Mechanism: The Diagnosis Prompt classifies errors into 13 predefined types (e.g., Schema-Linking, Join, Nested issues). Classified errors trigger retrieval of structurally similar correction cases, which provide context for generating targeted fix instructions. This separation reduces cognitive load on the LLM by isolating classification from repair reasoning.
- Core assumption: LLMs can reliably classify SQL errors when given structured error taxonomies, and similar errors benefit from similar correction patterns.
- Evidence anchors:
  - [abstract]: "integrating an error correction mechanism that diagnoses error types, identifies their causes, provides fixing instructions, and applies these corrections"
  - [section 4]: "decompose the error correction process into three steps... (1) Diagnose, (2) Write Prescription, and (3) Apply Treatment"
  - [corpus]: SQLens and Memo-SQL papers corroborate that decomposed error detection-correction pipelines outperform monolithic approaches; however, the specific medical-diagnosis metaphor is unique to ECPT.
- Break condition: If error types are misclassified (especially for ambiguous "Undesired Result" cases), the prescription step retrieves irrelevant cases, degrading correction quality.

### Mechanism 2
- Claim: Fine-tuning embedding models on error-type-labeled data improves RAG retrieval relevance for correction cases, even when training data is limited.
- Mechanism: A SentenceTransformer (MPNet-base) is fine-tuned using triplet loss with 14 labels (13 error types + success). This reshapes the embedding space so cases cluster by error type rather than surface-level token similarity. During inference, semantically similar error cases are retrieved more accurately.
- Core assumption: Error-type similarity is more useful for correction than token-level semantic similarity.
- Evidence anchors:
  - [abstract]: "fine-tuning embedding models with custom error datasets"
  - [section 4.2]: "embedding vectors for correction cases should be organized by case types rather than token meanings"
  - [section 5.2, Figure 4]: T-SNE visualizations show improved separation of success/error cases post-fine-tuning, though error subtypes remain intermingled
  - [corpus]: Weak corpus corroboration—neighbor papers mention embedding-based retrieval for NL2SQL but not embedding fine-tuning specifically for error taxonomy alignment.
- Break condition: If error types overlap significantly (e.g., "Wrong Cols" appearing in both Schema-Linking and Group-by categories), fine-tuning may not produce clean clusters.

### Mechanism 3
- Claim: RAG-based correction with small example sets (10% of available cases) achieves comparable performance to larger sets, suggesting error correction is pattern-driven rather than example-intensive.
- Mechanism: The system stores correction cases (table description, NL question, generated SQL, execution result, error type, correct SQL, reason, instruction) in a vector database (FAISS). During correction, only the most similar cases are retrieved as few-shot examples for the Prescription Prompt.
- Core assumption: SQL errors follow recurrent patterns that can be captured with limited exemplars; more examples don't proportionally improve correction.
- Evidence anchors:
  - [abstract]: "the size of correction cases has minimal impact on performance"
  - [section 5.3, Figure 5]: GPT4t-EFT(10%) achieves 87.69% vs GPT4t-EFT(100%) at 88.08%—a 0.39% gap despite 10× more data
  - [corpus]: Memo-SQL and LitE-SQL also use limited correction histories; however, they don't explicitly test scaling curves.
- Break condition: If error distribution shifts (e.g., new database schemas or query patterns not represented in correction cases), the small example pool may lack relevant patterns.

## Foundational Learning

- Concept: **Triplet Loss for Metric Learning**
  - Why needed here: Understanding how embedding fine-tuning reshapes vector space to cluster error-type-similar cases, which is central to the retrieval improvement.
  - Quick check question: Can you explain why triplet loss (anchor-positive-negative) would help separate "Schema-Linking:Wrong Cols" from "Group-by:Wrong Cols" in embedding space?

- Concept: **RAG (Retrieval-Augmented Generation)**
  - Why needed here: The framework uses RAG to inject relevant correction cases into LLM prompts; understanding retrieval-to-generation pipeline is prerequisite.
  - Quick check question: What happens to ECPT if the retrieval step returns cases with the same error type but different schema structures?

- Concept: **SQL Execution Accuracy vs. Syntactic Correctness**
  - Why needed here: The framework evaluates on execution accuracy (does the query return correct results?), not just whether SQL parses; errors include "Empty Table" and "Undesired Result" which require semantic understanding.
  - Quick check question: Why might a syntactically valid SQL query still be classified as an error in this framework?

## Architecture Onboarding

- Component map:
  - Diagnosis Prompt -> Vector Database (FAISS) -> Prescription Prompt -> Treatment Prompt -> Execution Engine

- Critical path: New error case → Encode to vector → Retrieve top-k similar cases → Generate instructions → Produce corrected SQL → Execute → If still failing, retry (max 3 attempts)

- Design tradeoffs:
  - Fine-tuned embeddings vs. off-the-shelf: +3% accuracy gain for GPT4-turbo, but requires labeled error dataset and training infrastructure
  - Small vs. large correction case pool: Minimal performance difference (0.39%), but smaller pool reduces storage/retrieval costs
  - Sequential 3-step vs. single-prompt correction: Higher interpretability and debugging visibility, but increased latency (3 LLM calls per correction attempt)
  - Option B (examples in diagnosis) and Option C (resolve all at once): Did NOT improve performance in ablation, suggesting error-type-only diagnosis is sufficient

- Failure signatures:
  - Low correction accuracy (<15%): Likely embedding model not fine-tuned or retrieval returning irrelevant cases
  - High token usage with low accuracy gain: Suggests Option B/C enabled; disable them per Table 2 results
  - Correction stuck on "Undesired Result": Requires ground-truth comparison; paper notes this is the hardest error type to self-correct without external feedback
  - Misclassification of error types: Check if error taxonomy covers observed error patterns; may need to extend Table 1 categories

- First 3 experiments:
  1. Baseline measurement: Run zero-shot NL-to-SQL on Spider dev set with GPT-3.5-turbo, record execution accuracy and error distribution across 13 types. Target: ~76% accuracy as per paper.
  2. Embedding fine-tuning ablation: Train SentenceTransformer on 10% of correction cases with triplet loss; compare retrieval relevance (hit rate) and downstream correction accuracy vs. untrained embeddings. Target: ~14% correction accuracy improvement.
  3. Full ECPT pipeline with GPT-4-turbo: Integrate diagnosis-prescription-treatment with fine-tuned embeddings; measure execution accuracy and cost-per-accuracy-gain. Target: ~88% execution accuracy with ~$20 cost per run.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the framework be adapted to identify and correct "Undesired Result" errors in real-world scenarios where ground-truth SQL queries are unavailable for execution verification?
- Basis in paper: [explicit] Section 6 states, "our approach... needs feedback based on ground-truth SQL queries, especially for execution result: 'Undesired Result'" and suggests integrating "Human-in-the-loop approaches to address this challenge."
- Why unresolved: The current ECPT framework relies on comparing execution results against ground-truth data to detect semantic errors (undesired results), a dependency that does not exist in production environments.
- What evidence would resolve it: A demonstrated method for validating semantic correctness (e.g., via user feedback or consistency checks) that maintains accuracy without access to the target SQL.

### Open Question 2
- Question: Can the manual initialization of error types be effectively automated using LLM agents without compromising the diagnostic precision of the correction process?
- Basis in paper: [explicit] Section 6 lists future work where "manual error type initialization can be automatic by utilizing LLM agents."
- Why unresolved: The current methodology relies on human experts to define and categorize the 13 distinct error types used for diagnosis and embedding fine-tuning.
- What evidence would resolve it: A comparative study showing that an LLM-agent can dynamically generate error taxonomies that perform as well as or better than the human-defined ones in Table 1.

### Open Question 3
- Question: Why does the size of the correction case dataset have minimal impact on performance, and does a saturation point exist for RAG-based SQL correction?
- Basis in paper: [explicit] Section 5.3 notes that "the size of the correction cases had minimal impact, suggesting that correcting SQL errors might require fewer examples," and explicitly states "This aspect needs more exploration in the future."
- Why unresolved: The results in Figure 5 show negligible differences between using 10%, 50%, and 100% of correction cases, but the underlying mechanism for this efficiency is not explained.
- What evidence would resolve it: An ablation study varying case complexity and quantity to identify the specific point where additional retrieval data ceases to provide utility.

## Limitations
- The 13 predefined error types may not generalize to new database schemas or query patterns
- Error type overlap and ambiguity (e.g., "Wrong Cols" appearing in multiple categories) complicates classification
- Difficulty correcting "Undesired Result" errors without ground-truth SQL queries for comparison
- Medical-diagnosis metaphor lacks strong corpus validation compared to alternative error-correction frameworks

## Confidence
- High Confidence: Embedding fine-tuning improves RAG retrieval relevance (14.17% to 50.20% correction accuracy)
- Medium Confidence: 3-step pipeline's superiority over single-step approaches is reasonably supported by ablation results
- Low Confidence: Generalizability of 13 error types across diverse database schemas remains uncertain

## Next Checks
1. **Cross-Schema Generalization Test**: Apply ECPT to a non-Spider dataset (e.g., WikiSQL or Academic) and measure error type classification accuracy and correction success rates. This validates whether the 13 error types and fine-tuned embeddings transfer beyond the original training distribution.

2. **Error Type Expansion Study**: Introduce 2-3 additional error types (e.g., "Temporal Logic Errors" or "Aggregation Context Errors") and retrain the embedding model. Measure whether correction accuracy improves for previously problematic error categories like "Undesired Result."

3. **Real-Time Correction Latency Analysis**: Profile the 3-step pipeline's execution time (diagnosis + retrieval + prescription + treatment) on 100 random Spider queries. Compare against single-prompt self-correction methods to quantify the latency cost of the medical-diagnosis approach.