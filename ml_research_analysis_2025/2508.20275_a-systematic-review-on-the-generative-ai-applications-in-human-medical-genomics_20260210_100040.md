---
ver: rpa2
title: A Systematic Review on the Generative AI Applications in Human Medical Genomics
arxiv_id: '2508.20275'
source_url: https://arxiv.org/abs/2508.20275
tags:
- data
- genetic
- clinical
- language
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This systematic review analyzed 172 studies on large language model
  (LLM) applications in human medical genomics. Automated keyword-based search was
  conducted across PubMed, bioRxiv, medRxiv, and arXiv, followed by deduplication
  and semantic filtering.
---

# A Systematic Review on the Generative AI Applications in Human Medical Genomics

## Quick Facts
- arXiv ID: 2508.20275
- Source URL: https://arxiv.org/abs/2508.20275
- Reference count: 40
- Key outcome: This systematic review analyzed 172 studies on large language model (LLM) applications in human medical genomics, finding that while transformer-based models significantly advance disease stratification, variant interpretation, and medical imaging analysis, major challenges persist in integrating multimodal data into clinically robust pipelines, facing limitations in generalizability and practical implementation in clinical settings.

## Executive Summary
This systematic review comprehensively analyzes 172 studies on transformer-based LLM applications in human medical genomics. The review covers automated keyword-based search across major databases, followed by semantic filtering and manual curation. The research focuses on three diagnostic pipeline stages: pre-analytical (knowledge navigation and risk stratification), analytical (medical imaging analysis, variant effect analysis, and clinical variant interpretation), and post-analytical (patient clustering, data aggregation, and clinical report generation). While transformer models show significant promise in advancing genetic diagnostics, the review identifies persistent challenges in multimodal data integration, generalizability across populations, and practical clinical implementation.

## Method Summary
The review employed automated keyword searches across PubMed, bioRxiv, medRxiv, and arXiv, retrieving 57,558 initial records. A systematic filtering pipeline was applied: deduplication, TF-IDF semantic filtering requiring matches across three semantic groups (LLM-related, Clinical, and Genetics-related), exclusion list application, and manual categorization. The final dataset consisted of 172 articles after manual review of approximately 550 records. The methodology focused on identifying transformer-based models and their applications in human medical genomics, excluding general AI/ML papers, animal models, and non-medical applications.

## Key Results
- Transformer-based models significantly advance disease and risk stratification, variant interpretation, and medical imaging analysis in genomics
- Major challenges persist in integrating multimodal data (genomic, imaging, clinical) into clinically robust pipelines
- Key limitations include generalizability issues, practical implementation barriers, and demographic/linguistic biases in current models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Transformer attention mechanisms enable modeling of long-range dependencies in genomic sequences and clinical texts, which is critical for variant interpretation and phenotype-genotype mapping.
- Mechanism: Self-attention allows the model to weigh relationships between distant positions in a sequence simultaneously, capturing non-local interactions that traditional models (CNNs, RNNs) miss. This is particularly valuable for DNA sequences where regulatory elements may be far from the genes they affect, and for clinical narratives where context spans multiple sentences.
- Core assumption: Genomic and clinical meaning depends on long-range contextual relationships, not just local patterns.
- Evidence anchors:
  - [abstract] "transformer-based models significantly advance disease and risk stratification, variant interpretation"
  - [section] "Unlike RNNs, transformers use an attention mechanism that allows the model to focus on different parts of the input data simultaneously, capturing long-range dependencies more effectively."
  - [corpus] Limited direct corpus support; neighboring papers focus on multimodal integration rather than attention-specific mechanisms.
- Break condition: When variant effects are primarily determined by local sequence motifs (e.g., splice sites, short motifs) rather than distal interactions, transformer advantages diminish.

### Mechanism 2
- Claim: Pre-training on large biomedical corpora creates foundation models that transfer generalizable patterns to specialized genetic diagnostics tasks with limited labeled data.
- Mechanism: Foundation models (e.g., Nucleotide Transformer, GENA-LM, BioBERT variants) learn statistical regularities from massive unsupervised datasets. These representations encode domain-specific priors that can be fine-tuned for downstream tasks like variant pathogenicity prediction, allowing few-shot learning and reducing dependence on large labeled clinical datasets.
- Core assumption: The statistical structure learned from pre-training data generalizes to clinical genetics tasks; labeled genetic data remains scarce.
- Evidence anchors:
  - [abstract] "LLMs demonstrated strong performance in tasks like variant identification, phenotype recognition"
  - [section] "Such foundation models are especially useful in low-data or low-resource settings, where fine-tuning benefits from pre-trained sequence-level general knowledge."
  - [corpus] Corpus neighbors emphasize multimodal foundation models but don't directly address pre-training mechanisms for genomics.
- Break condition: When downstream tasks require knowledge not present in pre-training data (e.g., rare syndromes with sparse literature), or when domain shift is substantial.

### Mechanism 3
- Claim: Multimodal fusion architectures integrate genomic, imaging, and clinical data to produce more robust diagnostic predictions than any single modality alone.
- Mechanism: Models like MGI, BioFusionNet, and Tri-COAT use cross-attention or contrastive learning to align representations from different modalities (DNA sequences, MRI/CT scans, EHRs). This enables the model to leverage complementary signals—e.g., imaging phenotypes that corroborate genetic predictions, or clinical history that contextualizes variant interpretation.
- Core assumption: Genetic diseases manifest across multiple observable modalities, and combining them reduces ambiguity and improves diagnostic precision.
- Evidence anchors:
  - [abstract] "challenges persist in multimodal data integration"
  - [section] "models such as BRLM... integrate BioBERT with deep classifiers for identifying mutation classes and gene-phenotype associations"
  - [corpus] "Multimodal Foundation Models for Early Disease Detection" and "Challenges and proposed solutions in modeling multimodal data" directly support this mechanism.
- Break condition: When modalities provide conflicting signals without clear weighting strategies, or when one modality dominates due to data imbalance, fusion may not improve over single-modality baselines.

## Foundational Learning

- Concept: Transformer architecture (encoder-decoder, self-attention)
  - Why needed here: All reviewed models inherit from this architecture; understanding attention is essential to interpret how they process sequences and why they outperform RNNs/CNNs on genetic tasks.
  - Quick check question: Can you explain why self-attention enables parallel processing and long-range dependency modeling?

- Concept: Foundation models and transfer learning
  - Why needed here: The paper emphasizes fine-tuning pre-trained models (Nucleotide Transformer, GENA-LM) rather than training from scratch. Understanding pre-training/transfer is critical for practical deployment.
  - Quick check question: What is the difference between mixed training and train-and-finetune strategies, and when would you choose each?

- Concept: Multimodal representation learning
  - Why needed here: Integration of genomics, imaging, and clinical records is identified as both a major opportunity and a key challenge.
  - Quick check question: How do cross-attention and contrastive learning differ in aligning multimodal representations?

## Architecture Onboarding

- Component map:
  - Pre-analytical: Knowledge extraction (NER, relation extraction, HPO normalization), risk stratification from EHR/family history
  - Analytical: Medical imaging analysis (ViTs for tumors, syndromic facial analysis), variant effect prediction (sequence → functional impact), clinical variant interpretation (prioritization, classification)
  - Post-analytical: Patient clustering/subtyping, data aggregation, report generation and decision support

- Critical path:
  1. Data ingestion (raw DNA sequences, imaging, clinical text) → preprocessing (tokenization, segmentation)
  2. Modality-specific encoding (transformer encoders for sequences, ViTs for images, BERT variants for text)
  3. Fusion layer (cross-attention or contrastive alignment) → task-specific heads (classification, regression, generation)
  4. Output integration (variant calls, risk scores, natural language reports)

- Design tradeoffs:
  - Encoder-only (BERT) vs. decoder-only (GPT) vs. encoder-decoder: Encoders excel at extraction/classification; decoders at generation; full transformers for interactive tools.
  - General-purpose vs. domain-specific pre-training: Domain-specific models (BioBERT, Nucleotide Transformer) may outperform general models on niche tasks but require curation investment.
  - Fine-tuning depth vs. generalization: Deep task-specific fine-tuning improves specialization but risks overfitting and losing transferability.

- Failure signatures:
  - Hallucinations in generative outputs (GPT models citing non-existent literature)
  - Performance degradation on rare diseases or underrepresented populations (bias)
  - Nondeterminism in clinical-grade workflows (same input → different classification)
  - Failure to generalize across institutions due to distribution shift in EHR formats or imaging protocols

- First 3 experiments:
  1. Baseline evaluation of a pre-trained genomic foundation model (e.g., Nucleotide Transformer) on a standard variant pathogenicity benchmark (e.g., ClinVar) to establish domain transfer performance.
  2. Ablation study of multimodal fusion vs. single-modality baselines on a paired imaging-genomics dataset (e.g., TCGA) to quantify fusion gains.
  3. Prompt engineering vs. fine-tuning comparison for a clinical NER task (e.g., HPO term extraction) to assess trade-offs between adaptation cost and accuracy.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can multimodal data (genomic sequences, imaging, and clinical records) be integrated into unified pipelines that maintain clinical robustness and generalizability?
- Basis in paper: [explicit] The Abstract and Conclusion state that "major challenges persist in integrating multimodal data... facing limitations in generalizability and practical implementation in clinical settings."
- Why unresolved: While contrastive learning and cross-attention mechanisms are being explored (Section 3.1.2), the authors note that current methods struggle to fuse diverse modalities consistently and scalably.
- What evidence would resolve it: The development and validation of fusion architectures that demonstrate high performance across heterogeneous external datasets and clinical environments.

### Open Question 2
- Question: Does domain-specific pretraining for genomic foundation models yield superior performance compared to mixed training strategies or random initialization?
- Basis in paper: [explicit] Discussion Section 4.4 notes that "recent findings suggest that domain-specific pretraining alone does not guarantee superior performance," as "randomly initialized models can match or exceed genomic foundation models."
- Why unresolved: There is ambiguity regarding whether the specific pretraining data or the training strategy itself (e.g., mixed training vs. train-and-finetune) drives downstream performance.
- What evidence would resolve it: Comparative ablation studies isolating the effects of pretraining data sources versus training strategies on identical downstream genomic tasks.

### Open Question 3
- Question: How can the field establish evaluation protocols that mitigate the risk of benchmark leakage in Large Language Models (LLMs) for genomics?
- Basis in paper: [explicit] Discussion Section 4.2 highlights the "threat of benchmark leakage, where models inadvertently see test data during pretraining," which inflates performance and undermines credibility.
- Why unresolved: Standard benchmarks may be contaminated by pretraining corpora, and current protocols lack transparency regarding training data composition.
- What evidence would resolve it: The adoption of "leakage-aware evaluation protocols" and strict hold-out datasets that are verified to be absent from pretraining corpora.

### Open Question 4
- Question: What methodologies are required to effectively reduce demographic and linguistic biases in LLMs applied to genetic diagnostics?
- Basis in paper: [explicit] Discussion Section 4.3 states that LLMs "often reflect biases present in their training data," including racial/demographic disparities and the limitation of being "English-centric."
- Why unresolved: Current fairness audits are inconsistent, and there is a lack of resources for non-English populations, limiting accessibility and accuracy.
- What evidence would resolve it: The creation of multi-lingual benchmarks and the demonstration of reduced performance variance across different demographic groups through algorithmic fairness interventions.

## Limitations
- Potential publication bias favoring positive results in transformer-based approaches
- Lack of comparative studies between transformer-based and traditional ML methods in most included papers
- Insufficient reporting of cross-validation and external validation results across studies

## Confidence
- High Confidence: The review's methodology for identifying and categorizing LLM applications in medical genomics is sound
- Medium Confidence: Claims about transformer-based models significantly advancing specific diagnostic tasks are supported by included studies, but generalizability remains uncertain
- Medium Confidence: Identification of multimodal integration as a major challenge is well-founded, though specific technical barriers may vary by application domain
- Low Confidence: Conclusions about clinical implementation barriers are primarily based on author observations rather than systematic evaluation of deployment studies

## Next Checks
1. **External Validation Assessment**: Systematically evaluate the proportion of included studies that performed external validation on independent datasets from different populations or institutions, and quantify the performance degradation when models encounter distribution shifts.

2. **Clinical Implementation Gap Analysis**: Identify and categorize the specific barriers preventing clinical deployment (e.g., regulatory approval, integration with EHR systems, clinician trust) by surveying practicing clinical geneticists about their experiences with LLM-based tools.

3. **Computational Cost-Benefit Analysis**: Measure and compare the computational resources required for transformer-based models versus traditional methods across different tasks (e.g., variant interpretation, imaging analysis) and calculate the trade-offs between performance gains and implementation costs.