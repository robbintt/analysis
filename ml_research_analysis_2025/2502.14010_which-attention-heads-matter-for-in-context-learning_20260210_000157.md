---
ver: rpa2
title: Which Attention Heads Matter for In-Context Learning?
arxiv_id: '2502.14010'
source_url: https://arxiv.org/abs/2502.14010
tags:
- heads
- induction
- pythia
- score
- ablation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper investigates which attention heads are crucial for
  in-context learning (ICL) in transformer models. Through systematic ablation studies
  across 12 models ranging from 70M to 7B parameters, the authors compare two proposed
  ICL mechanisms: induction heads that copy tokens, and function vector (FV) heads
  that encode task representations.'
---

# Which Attention Heads Matter for In-Context Learning?

## Quick Facts
- arXiv ID: 2502.14010
- Source URL: https://arxiv.org/abs/2502.14010
- Authors: Kayo Yin; Jacob Steinhardt
- Reference count: 40
- Key outcome: Function vector heads, not induction heads, are the primary drivers of few-shot ICL performance in transformer models.

## Executive Summary
This paper investigates which attention heads drive in-context learning (ICL) in transformer models through systematic ablation studies. The authors compare two proposed ICL mechanisms: induction heads that copy tokens, and function vector (FV) heads that encode task representations. Through experiments on 12 models ranging from 70M to 7B parameters, they discover that FV heads, not induction heads, are the primary drivers of few-shot ICL performance. The effect becomes more pronounced in larger models, challenging the prevailing view that induction heads are the key ICL mechanism.

## Method Summary
The authors employ ablation studies with exclusion to compare FV and induction head contributions to ICL. They compute induction scores using TransformerLens on repeated random sequences, and FV scores using causal mediation analysis on corrupted prompts. Top 2% heads by each score are identified, then mean ablated (replacing head output with dataset average) at 1-20% of heads. Ablation with exclusion controls for overlap between mechanisms. Training dynamics are analyzed across 8 Pythia checkpoints to track head evolution.

## Key Results
- FV head ablation significantly degrades ICL accuracy across all model sizes, with larger models showing more pronounced effects
- Induction head ablation has minimal impact beyond random ablation when controlling for overlap with FV heads
- Many FV heads evolve from induction heads during training, suggesting induction serves as a developmental precursor
- Results challenge the prevailing view that induction heads are the key ICL mechanism

## Why This Works (Mechanism)

### Mechanism 1: Function Vector Heads Encode Task Representations for ICL
- Claim: FV heads causally drive few-shot ICL performance by computing latent task encodings
- Core assumption: The ablation effect reflects causal importance rather than distributed redundancy
- Evidence anchors: Section 4.2 shows FV ablation causes significant ICL degradation; related work supports FV extraction
- Break condition: If FV ablation stops degrading ICL in models >10B parameters

### Mechanism 2: Induction Heads Serve as Developmental Precursors to FV Heads
- Claim: Many FV heads begin as induction heads during training before transitioning
- Core assumption: The correlation reflects developmental causation rather than independent convergence
- Evidence anchors: Section 5.2 documents unidirectional transition from induction to FV scores during training
- Break condition: If training interventions show induction ablation doesn't affect FV development

### Mechanism 3: Induction-FV Correlation Confounds Prior ICL Attribution
- Claim: Previous studies attributed ICL to induction heads partially due to overlap with FV heads
- Core assumption: The exclusion ablation method correctly isolates mechanism-specific contributions
- Evidence anchors: Section 3.2 shows FV heads rank at ~90-95th percentile of induction scores
- Break condition: If exclusion ablation introduces artifacts

## Foundational Learning

- Concept: **Causal ablation with exclusion**
  - Why needed here: Standard ablation conflates correlated mechanisms; exclusion isolates unique contributions
  - Quick check question: If you ablate all high-scoring induction heads, how do you know you're not also removing important FV heads?

- Concept: **Function vectors as task encodings**
  - Why needed here: Understanding FV as a compressed representation explains why it can substitute for demonstrations
  - Quick check question: How would you verify that an FV encodes a specific task rather than general language statistics?

- Concept: **Training dynamics and mechanism emergence**
  - Why needed here: The paper's developmental claim relies on checkpoint analysis
  - Quick check question: If induction heads emerge at step 1K and FV heads at step 16K, what does this temporal gap suggest?

## Architecture Onboarding

- Component map: Induction heads (early-to-middle layers) → FV heads (middle layers) → Task representations
- Critical path: 1) Identify top 2% heads by induction and FV scores; 2) Perform mean ablation with exclusion; 3) Measure ICL accuracy on held-out tasks
- Design tradeoffs: Mean ablation avoids OOD artifacts but may underestimate importance; exclusion preserves interpretability at cost of fewer heads ablated
- Failure signatures: No ablation effect suggests incorrect head identification; convergence with random ablation suggests redundant mechanisms
- First 3 experiments: 1) Replicate FV vs. induction ablation on Pythia 1.4B; 2) Extract FVs, inject into shuffled prompts, measure task recovery; 3) Analyze training checkpoints to track induction→FV transition

## Open Questions the Paper Calls Out

- What role do the remaining "pure" induction heads serve in fully trained models if they are not causally responsible for few-shot ICL?
- Does the induction head mechanism causally facilitate the learning of function vector heads during training?
- Is high attention head dimensionality a necessary architectural condition for the emergence of strong function vector mechanisms?

## Limitations

- Results may not generalize beyond Pythia models to other architectures or modalities
- Task distribution from 45 specific ICL tasks may not represent full diversity of in-context learning scenarios
- Alternative explanations exist for observed effects, including distributed redundancy or correlation without causation

## Confidence

- **High Confidence**: Relative importance comparison between FV and induction heads is well-supported by consistent results across multiple model scales
- **Medium Confidence**: Developmental precursor hypothesis is compelling but based on correlation in checkpoint analysis
- **Low Confidence**: Claim that FV heads compute "task representations" lacks direct behavioral evidence beyond performance improvements

## Next Checks

1. Replicate ablation experiments on GPT-2, LLaMA, or Mistral models to verify cross-architecture generalization
2. Extract FVs from FV heads, inject into corrupted prompts, and measure task recovery accuracy to validate FV encoding quality
3. Use head ablations during training to test whether removing induction heads affects FV head development or whether FV heads can form independently