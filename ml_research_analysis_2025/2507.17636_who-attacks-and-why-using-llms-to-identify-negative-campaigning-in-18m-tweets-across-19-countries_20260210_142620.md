---
ver: rpa2
title: Who Attacks, and Why? Using LLMs to Identify Negative Campaigning in 18M Tweets
  across 19 Countries
arxiv_id: '2507.17636'
source_url: https://arxiv.org/abs/2507.17636
tags:
- negative
- campaigning
- political
- parties
- across
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study introduces zero-shot large language models (LLMs) as
  a scalable method for classifying negative campaigning across languages and contexts.
  Using benchmark datasets in ten languages, LLMs matched or exceeded human coders'
  performance and outperformed supervised machine learning approaches.
---

# Who Attacks, and Why? Using LLMs to Identify Negative Campaigning in 18M Tweets across 19 Countries

## Quick Facts
- arXiv ID: 2507.17636
- Source URL: https://arxiv.org/abs/2507.17636
- Authors: Victor Hartman; Petter Törnberg
- Reference count: 32
- Key outcome: LLMs match or exceed human coders in cross-lingual negative campaigning classification, revealing that governing parties attack less while extreme and populist parties engage in significantly more negativity

## Executive Summary
This study introduces zero-shot large language models (LLMs) as a scalable method for classifying negative campaigning across languages and contexts. Using benchmark datasets in ten languages, LLMs matched or exceeded human coders' performance and outperformed supervised machine learning approaches. Applying this method to 18 million tweets from parliamentarians in 19 European countries, the analysis found that governing parties use less negative messaging, while ideologically extreme and populist parties—especially radical right parties—engage in significantly higher levels of negativity. The results demonstrate LLMs' potential for valid, transparent, and replicable cross-national political communication research.

## Method Summary
The study uses zero-shot prompting with GPT-4o-mini to classify 18 million tweets from 5,439 parliamentarians across 19 European countries (2017-2022) as negative campaigning or not. The classification prompt asks whether tweets contain "explicit attack or critique toward opponent party or candidate." The method was validated against two private benchmark datasets (Petkevic & Nai 2022; Klinger et al. 2023) across ten languages, achieving weighted F1 ≥ 0.86 and Brennan-Prediger ≥ 0.77. Individual tweet predictions were aggregated to party level and analyzed using OLS regression with country fixed effects, incorporating party characteristics from the Chapel Hill Expert Survey 2019.

## Key Results
- LLMs achieved weighted F1 ≥ 0.86 and Brennan-Prediger ≥ 0.77 across 11 countries, matching native-speaking human coders
- Governing parties use 6.57-7.40 percentage points less negative messaging than opposition parties
- Radical right parties exhibit the highest levels of negative campaigning among all ideological groups
- Smaller GPT-4o-mini model performs comparably to larger model at fraction of cost (~$156 for full corpus)

## Why This Works (Mechanism)

### Mechanism 1
Zero-shot LLMs can match or exceed human coder performance for cross-lingual negative campaigning classification without language-specific training data. Instruction-tuned LLMs leverage pre-trained multilingual representations to apply consistent coding rules specified in natural language prompts across languages, eliminating the need for per-language labeled datasets or separate models. The model's pre-training covers sufficient multilingual political discourse to transfer conceptual understanding across linguistic boundaries.

### Mechanism 2
Party-level strategic incentives predict negative campaigning frequency: governing parties attack less, while ideologically extreme and populist parties attack more. Parties weigh reputational costs against attention gains; governing parties risk coalition viability and credibility, while outsider parties face lower costs and benefit from differentiation through adversarial messaging. Aggregated social media output reflects coordinated party strategy rather than individual parliamentarian behavior.

### Mechanism 3
Smaller LLMs (GPT-4o-mini) can match larger model performance on structured classification tasks at fraction of cost. Well-defined classification tasks with explicit prompts reduce reasoning complexity, allowing distilled models to achieve parity with larger models where the bottleneck is instruction-following rather than deep inference. The classification task boundaries are sufficiently explicit in the prompt that model scale provides diminishing returns.

## Foundational Learning

- Concept: Zero-shot prompting with instruction-tuned models
  - Why needed here: The entire methodology rests on formulating classification rules in natural language that LLMs can apply without gradient updates or labeled examples
  - Quick check question: Can you write a prompt that defines "negative campaigning" precisely enough that a model could apply it consistently to ambiguous cases?

- Concept: Inter-rater reliability metrics (Krippendorff's α, Brennan-Prediger coefficient)
  - Why needed here: The paper benchmarks LLM performance against human coders using IRR rather than accuracy alone, requiring understanding of how these metrics handle class imbalance
  - Quick check question: Why might Brennan-Prediger be preferred over Cohen's kappa when evaluating agreement on imbalanced datasets like the Klinger et al. data (3.1% positive class)?

- Concept: Strategic incentives framework for party behavior in multiparty systems
  - Why needed here: The substantive findings depend on understanding coalition potential and reputational costs as mechanisms distinct from US-style incumbent-challenger dynamics
  - Quick check question: Why might a party with 15% vote share in a proportional system still face strong incentives to avoid negative campaigning?

## Architecture Onboarding

- Component map:
  - Data layer: Twitter ParlDab (van Vliet et al. 2020) → 18M tweets from 5,439 parliamentarians across 19 countries (2017-2022)
  - Classification layer: GPT-4o-mini with temperature=0, zero-shot prompt adapted from Petkevic & Nai (2022) codebook
  - Validation layer: Benchmark comparison against manually coded datasets (Petkevic & Nai 2022; Klinger et al. 2023)
  - Enrichment layer: Chapel Hill Expert Survey (CHES) 2019 for party-level covariates
  - Analysis layer: OLS regression with country fixed effects, clustered standard errors

- Critical path:
  1. Define classification prompt with explicit criteria for "explicit attack or critique toward opponent party or candidate"
  2. Validate against gold-standard human-coded datasets before full-scale deployment
  3. Process full corpus with validated prompt (batch API calls, temperature=0)
  4. Aggregate individual tweets to party-level proportions
  5. Merge with CHES party characteristics
  6. Estimate regression models with appropriate clustering

- Design tradeoffs:
  - Broader vs. stricter negativity definitions: Petkevic & Nai's broader definition yields higher baseline rates and easier classification; Klinger et al.'s stricter definition better captures true attacks but creates class imbalance challenges
  - Aggregation level: Party-level analysis enables structural inference but obscures temporal dynamics and individual variation
  - Cost/performance: GPT-4o-mini validated as sufficient for this task (~$156 for 18M tweets), but validation on a subset before full deployment is essential

- Failure signatures:
  - Inconsistent cross-lingual performance (check per-country metrics in Table 3)
  - Prompt drift when definitions are too abstract (compare "no context" vs. "system + user context" results in Table 1)
  - Temporal mismatch between outcome (2017-2022) and predictors (CHES 2019)—interpret government status as "governing experience" not instantaneous status

- First 3 experiments:
  1. Replicate validation on a held-out subset of your target corpus (at least 500 manually coded examples per language) before scaling
  2. Test prompt sensitivity: vary definition specificity and example inclusion to establish performance bounds
  3. Compare party-level vs. individual-level aggregation on a subset where both can be tested to validate the aggregation assumption

## Open Questions the Paper Calls Out

### Open Question 1
Do the patterns of negative campaigning identified in European democracies hold in non-democratic or hybrid regimes? The authors explicitly call for future research to "explore negativity beyond democratic contexts" to test the generalizability of the strategic incentives framework. The current study is limited to 19 European countries, restricting the variance in institutional settings to democratic multiparty systems.

### Open Question 2
How does negative campaigning fluctuate during acute crisis periods compared to routine political periods? The conclusion suggests future research should "investigate temporal dynamics in negativity during crisis periods." The study aggregates tweets over a five-year period (2017–2022), which abstracts away specific temporal variations and time-specific effects.

### Open Question 3
Can the LLM-based classification approach be effectively extended to visual and multimodal political content? The authors state that future research should "expand the analysis to include visual and multimodal political content." The current methodological contribution is restricted to textual data, whereas political communication increasingly relies on images and video.

### Open Question 4
Do the findings regarding party-level negativity generalize across different social media platforms? The study acknowledges Twitter's "unique" affordances for direct messaging and politician-to-politician interaction, yet analyzes only Twitter data. Platform-specific logics (e.g., Facebook's algorithmic curation or TikTok's short-form video focus) may alter the strategic incentives for negative campaigning.

## Limitations
- Aggregated party-level outcomes from individual parliamentarians' tweets may not fully capture coordinated party strategy versus individual behavior patterns
- Cross-sectional CHES 2019 data introduces potential temporal mismatch with 2017-2022 tweet corpus, particularly for government status variables
- Validation approach depends on private datasets that limit independent verification of reported performance metrics

## Confidence
- **High Confidence**: Zero-shot LLM methodology demonstrates robust performance with weighted F1 scores ≥ 0.86 and Brennan-Prediger coefficients ≥ 0.77 across all tested languages; finding that governing parties use less negative messaging shows consistent statistical significance (p < 0.001)
- **Medium Confidence**: Relationship between ideological extremism/populism and negative campaigning depends on self-reported CHES measures that may not capture short-term strategic shifts; finding that radical right parties engage in highest negativity levels could reflect measurement sensitivity to populist rhetoric
- **Low Confidence**: Cost-effectiveness claim for GPT-4o-mini assumes uniform task complexity across corpus; without testing on edge cases or ambiguous political discourse, performance boundaries remain uncertain; absence of qualitative validation means systematic biases across cultural contexts cannot be ruled out

## Next Checks
1. **Temporal Validation**: Re-run the regression analysis using rolling windows (2017-2019, 2019-2021, 2021-2022) to test whether negative campaigning patterns hold during periods of government formation/collapse versus stable governance

2. **Individual-Level Analysis**: Select 2-3 countries with sufficient data to disaggregate party-level results to individual parliamentarians, testing whether aggregate patterns reflect coordinated party behavior or individual variation

3. **Prompt Sensitivity Analysis**: Systematically vary the negativity definition in the prompt (broader vs. stricter criteria) and test performance on a manually coded validation subset to establish bounds on classification consistency across definitional changes