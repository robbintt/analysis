---
ver: rpa2
title: A Computational Approach to Improving Fairness in K-means Clustering
arxiv_id: '2505.22984'
source_url: https://arxiv.org/abs/2505.22984
tags:
- cluster
- points
- data
- clustering
- fairness
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper tackles the problem of fairness in K-means clustering,
  where certain clusters can be dominated by data points from a single subpopulation
  (e.g., gender or race), leading to potential bias. To address this, the authors
  propose a two-stage approach: first performing standard K-means clustering, then
  adjusting cluster memberships to improve fairness.'
---

# A Computational Approach to Improving Fairness in K-means Clustering

## Quick Facts
- **arXiv ID:** 2505.22984
- **Source URL:** https://arxiv.org/abs/2505.22984
- **Reference count:** 31
- **Primary result:** Proposed two-stage fairness algorithms improve fairness index F by up to 84% with minimal clustering quality degradation

## Executive Summary
This paper addresses fairness in K-means clustering, where clusters can be dominated by a single subpopulation (e.g., gender or race). The authors propose a two-stage approach: first performing standard K-means clustering, then adjusting cluster memberships to improve fairness. Two computationally efficient algorithms are introduced: a near-foreign heuristic that reassigns boundary points and a Gini index-based method that identifies highly "mixed" boundary points. Experiments on seven benchmark datasets show significant fairness improvements with minimal impact on clustering quality.

## Method Summary
The method follows a two-stage optimization framework. First, standard K-means clustering is performed to obtain initial cluster assignments. Then, cluster membership is adjusted by reassigning a small subset of points to improve fairness. Two algorithms are proposed: (1) fcNearForeign, which reassigns points far from their own cluster centroid but close to another cluster's centroid, and (2) fcGini, which uses Gini impurity in k-nearest neighborhoods to identify boundary points for reassignment. Both methods target boundary points to minimize impact on overall clustering quality while improving fairness representation.

## Key Results
- Both algorithms significantly improve fairness, reducing the fairness index by up to 84%
- Minimal impact on clustering quality, with cluster separation metric κ showing negligible degradation
- Gini index method is particularly robust, showing little sensitivity to neighborhood size choices (k ∈ {5, 10, 15})
- The proposed methods are broadly applicable and maintain clustering performance while enhancing fairness

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Switching cluster membership of points that are far from their own centroid but near another cluster's centroid improves fairness with minimal degradation to clustering quality.
- **Mechanism:** Boundary points contribute less to defining cluster structure than interior points. By targeting points with high within-cluster distance and low foreign-cluster distance, reassignment affects the overall partition minimally while enabling fairness rebalancing.
- **Core assumption:** Points near cluster boundaries have limited influence on the cluster quality metric (κ) compared to core/interior points.
- **Evidence anchors:**
  - [abstract] "near-foreign heuristic that reassigns points far from their own cluster centroid but close to another cluster's centroid"
  - [section 2.2] "reassigning them will not impact the overall cluster quality much since they do not contribute much in determining the class boundary"
  - [corpus] No direct corpus support for this specific boundary-point heuristic; related work on fair clustering (Fair Clustering via Alignment) addresses fairness constraints but uses different optimization approaches.
- **Break condition:** Complex non-convex geometries (e.g., Swiss-roll manifolds) where Euclidean distance fails to capture true cluster adjacency.

### Mechanism 2
- **Claim:** High Gini index in a k-nearest neighborhood indicates boundary points suitable for cluster reassignment.
- **Mechanism:** Boundary points have mixed local neighborhoods containing points from multiple clusters, yielding higher Gini impurity (G ≈ 0.5–0.66). Interior points have homogeneous neighborhoods (G → 0). Sorting by Gini identifies reassignment candidates.
- **Core assumption:** Local neighborhood composition reflects cluster boundary proximity; k-NN captures this adequately.
- **Evidence anchors:**
  - [abstract] "Gini index-based method that identifies highly 'mixed' boundary points by measuring impurity in local neighborhoods"
  - [section 2.3, Table 1] Demonstrates Gini values: Case 1 (mixed 1/3 each) → G=0.67; Case 5 (pure) → G=0
  - [corpus] Gini index usage in classification trees is well-established (Breiman 1984, referenced as [3]), but its application to boundary detection in clustering is novel.
- **Break condition:** Highly sparse data where k-NN neighborhoods don't capture meaningful local structure.

### Mechanism 3
- **Claim:** A two-stage optimization (cluster first, then adjust) achieves fairness improvements efficiently without solving the full constrained mixed-integer problem.
- **Mechanism:** Direct constrained K-means is NP-hard. By decoupling into (1) standard clustering and (2) targeted membership swaps on a small feasible set, computational complexity reduces to linear while still improving fairness.
- **Core assumption:** Only a small subset of points need reassignment to achieve meaningful fairness gains.
- **Evidence anchors:**
  - [abstract] "two-stage optimization formulation--clustering first and then adjust cluster membership of a small subset"
  - [section 2] "Directly solving the constrained optimization problem (⋆) can be very costly, as it is a mixed integer programming problem"
  - [corpus] Fair Clustering via Alignment (arXiv:2505.09131) similarly notes that existing fair clustering methods are "computationally expensive" and proposes alternative alignment-based approaches.
- **Break condition:** Cases where fairness constraints require substantial restructuring (many points must move), violating the "small subset" assumption.

## Foundational Learning

- **Concept: K-means objective (within-cluster sum of squares)**
  - **Why needed here:** The two-stage approach preserves κ (cluster separation) by minimizing disruption to the K-means solution. Understanding what κ measures is essential to interpret trade-offs.
  - **Quick check question:** If κ drops from 0.80 to 0.75 after fairness adjustment, what does this mean about cluster separation?

- **Concept: Gini impurity for categorical mixtures**
  - **Why needed here:** The fcGini algorithm repurposes Gini from decision trees to detect boundary points. Understanding how Gini responds to class proportion changes is critical.
  - **Quick check question:** A point has 10 neighbors: 5 from cluster A, 5 from cluster B. What is its Gini index?

- **Concept: Fairness as proportional representation**
  - **Why needed here:** The fairness index F measures deviation from population-level proportions within each cluster. Interpreting F=0.03 vs F=0.12 requires understanding this metric.
  - **Quick check question:** If subpopulation S1 is 40% of the total data, but cluster C1 is 80% S1, is F increasing or decreasing?

## Architecture Onboarding

- **Component map:** K-means module -> Cluster balance calculator (β) -> Boundary detector -> Swap controller

- **Critical path:**
  1. Run K-means → obtain initial partition
  2. Compute β for all clusters; identify max-β (over-represented) and min-β (under-represented) pair
  3. Rank candidate points via chosen boundary detector
  4. Swap top-ranked point; recompute β
  5. Repeat until clusters are "balanced enough" (β within tolerance of population β₀)
  6. (For K>2) Repeat for next most-imbalanced pair

- **Design tradeoffs:**
  - **Near-foreign (fcNearForeign):** O(n) per iteration; sensitive to distance metric; struggles with non-convex manifolds
  - **Gini (fcGini):** O(n log n) due to k-NN; more geometry-agnostic; empirically insensitive to k ∈ {5, 10, 15}
  - **Assumption:** Both assume convex or locally separable clusters; neither handles overlapping density structures natively.

- **Failure signatures:**
  - Near-foreign: High swap counts with minimal F improvement → suggests distance metric mismatch or non-convex geometry
  - Gini: κ drops sharply → k too small (noisy neighborhoods) or data too sparse
  - Both: β oscillates without converging → fairness target incompatible with cluster geometry

- **First 3 experiments:**
  1. **Synthetic 2-cluster test:** Generate two Gaussian clusters with known subpopulation imbalance; verify fcGini reduces F while κ remains stable (target: <5% κ drop)
  2. **k-sensitivity sweep:** On ILPD and Heart datasets, run fcGini with k ∈ {3, 5, 10, 15, 20}; plot F vs κ to confirm robustness claim
  3. **Boundary case inspection:** Visualize (via PCA if needed) the top-10 swapped points; confirm they lie near cluster boundaries, not in interiors

## Open Questions the Paper Calls Out

- **Open Question 1:** Can the near-foreign and Gini index heuristics be effectively combined into a single algorithm?
  - **Basis in paper:** [explicit] The conclusion states, "It might be interesting to see what happens if and how one may combine these two ideas."
  - **Why unresolved:** The paper evaluates the near-foreign and Gini index methods strictly in isolation and does not explore a hybrid approach.
  - **What evidence would resolve it:** An implementation that selects boundary points using both metrics and a comparison of its fairness-performance trade-off against the individual methods.

- **Open Question 2:** Can the two-stage optimization framework be extended to satisfy multiple fairness constraints simultaneously?
  - **Basis in paper:** [explicit] The authors identify future work to "extend our algorithms to account for multiple fairness measures simultaneously."
  - **Why unresolved:** The current formulation optimizes for a single fairness definition based on population proportions (Equation 3).
  - **What evidence would resolve it:** A modified algorithm that balances conflicting fairness metrics (e.g., balance vs. local fairness) and experimental results on datasets with multiple sensitive attributes.

- **Open Question 3:** Do the proposed post-processing methods maintain clustering quality when applied to non-centroid-based algorithms like spectral clustering?
  - **Basis in paper:** [explicit] The authors claim the method is "readily applicable to a broad class of clustering algorithms" like spectral clustering, but validate it only on K-means.
  - **Why unresolved:** The heuristics rely on centroid distances (near-foreign) or local neighborhoods (Gini), which may not align with the decision boundaries or graph-cut logic of spectral clustering.
  - **What evidence would resolve it:** Experiments applying fcGini and fcNearForeign to spectral clustering outputs to measure the impact on cluster quality (κ).

## Limitations
- The approach assumes boundary points have minimal impact on clustering quality, which may fail for non-convex or high-dimensional manifolds where Euclidean distance poorly captures cluster structure
- The "balance enough" threshold (β0) is not precisely specified, creating potential reproducibility issues
- The fairness metric F only considers binary sensitive attributes, limiting applicability to multi-category or continuous fairness scenarios

## Confidence
- **High Confidence:** The core mechanism of using Gini index to detect boundary points (Mechanism 2) is well-established from decision tree literature and properly applied here
- **Medium Confidence:** The computational efficiency claims hold for the tested datasets, but scalability to very large datasets with many clusters remains unverified
- **Low Confidence:** The claim that minimal κ degradation occurs across all dataset types needs broader validation, particularly for non-Gaussian cluster shapes

## Next Checks
1. **Manifold Sensitivity Test:** Apply fcGini to synthetic Swiss-roll data to verify it doesn't degrade κ while improving F
2. **Multi-class Extension:** Modify F to handle multi-category sensitive attributes and validate on a dataset with >2 subpopulations
3. **Parameter Sensitivity Analysis:** Systematically vary k in fcGini across a wider range (1-50) to precisely characterize the claimed robustness to neighborhood size