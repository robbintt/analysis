---
ver: rpa2
title: Pass@k Training for Adaptively Balancing Exploration and Exploitation of Large
  Reasoning Models
arxiv_id: '2508.10751'
source_url: https://arxiv.org/abs/2508.10751
tags:
- pass
- training
- group
- advantage
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Pass@k Training improves exploration in reinforcement learning
  with verifiable rewards (RLVR) by using Pass@k as the reward metric. This approach
  enhances the model's ability to generate diverse solutions without compromising
  Pass@1 performance.
---

# Pass@k Training for Adaptively Balancing Exploration and Exploitation of Large Reasoning Models

## Quick Facts
- arXiv ID: 2508.10751
- Source URL: https://arxiv.org/abs/2508.10751
- Reference count: 40
- Pass@k Training improves exploration in RLVR by using Pass@k as reward metric, enhancing diverse solutions without compromising Pass@1 performance

## Executive Summary
Pass@k Training introduces a novel approach to reinforcement learning with verifiable rewards (RLVR) that addresses the exploration-exploitation trade-off in large reasoning models. By using Pass@k as the reward metric instead of Pass@1, the method encourages diverse solution generation while maintaining strong performance on the primary metric. The approach employs analytical derivation for efficient advantage computation and demonstrates improved exploration capabilities across multiple reasoning tasks and model scales, with the added benefit that exploration gains can transfer to improved Pass@1 performance through continued training.

## Method Summary
The method implements Pass@k training by replacing Pass@1 reward with group-based rewards where at least one correct response in a group of k responses yields positive reward for the entire group. The key innovation is analytical derivation of advantage values using combinatorial formulas (Eqs. 14-15) rather than stochastic sampling, which reduces variance and improves training stability. The training uses DAPO/GRPO with token-level policy gradient loss, fixed positive/negative rewards (1/0), and analytical computation of group-level advantages based on the number of positive and negative responses in each rollout. The method includes a "Full Sampling" implementation for conceptual simplicity and an optimized "Analytical Derivation" that computes advantages directly from response statistics.

## Key Results
- Pass@k training consistently improves exploration metrics (entropy, diversity) compared to Pass@1
- Analytical derivation reduces variance and achieves similar performance to bootstrap sampling with 3-4x faster wall-clock time
- Transfer learning from Pass@k to Pass@1 training results in higher final Pass@1 scores than Pass@1-only training
- Benefits generalize from 7B to 32B parameter models across maze navigation and reasoning tasks

## Why This Works (Mechanism)

### Mechanism 1: Group-Level Reward Propagation
Pass@k training groups k responses and assigns the maximum reward of a group to all members, reducing penalties for exploration. This prevents convergence to conservative actions by sharing positive advantages with incorrect responses in successful groups, signaling that exploration directions were valuable even when individual responses were wrong.

### Mechanism 2: Hard-Problem Gradient Re-weighting
The Pass@k advantage function analytically shifts optimization focus from easy/medium problems to harder ones by peaking at lower accuracy levels (e.g., 25% vs 50% for Pass@1). This forces the model to allocate capacity to unsolved reasoning boundaries rather than overfitting to already-solved tasks.

### Mechanism 3: Variance Reduction via Analytical Derivation
Replacing stochastic group sampling with closed-form analytical advantage calculation stabilizes training by removing noise from advantage estimation. The combinatorial derivation provides cleaner gradient signals that allow the optimizer to follow more stable learning trajectories.

## Foundational Learning

**GRPO (Group Relative Policy Optimization)**: A variant of policy optimization where advantage is calculated relative to the mean reward of a group of samples for the same prompt. Why needed: The paper builds upon GRPO/DAPO, so understanding group-relative advantages is essential. Quick check: How does GRPO calculate the baseline for advantage estimation?

**Exploration-Exploitation Trade-off in RLVR**: The tension between trying diverse solutions (exploration) versus optimizing known successful approaches (exploitation). Why needed: Standard RLVR (Pass@1) exploits too early, causing entropy collapse. Quick check: Why does maximizing Pass@1 specifically lead to policy entropy collapse?

**Pass@k Metric**: Measures the probability of solving a problem given k attempts, rather than just one. Why needed: Repurposed as a training objective to encourage diverse solution generation. Quick check: If a model has Pass@1=20% and Pass@8=60%, what does that imply about its reasoning diversity?

## Architecture Onboarding

**Component map**: Rollout Engine -> Verifier -> Advantage Calculator (new component with analytical formulas) -> Optimizer (DAPO/GRPO)

**Critical path**: The implementation of the Advantage Calculator using Eq. 14 and 15 with correct combinatorial logic (C(N_neg,k)) to determine expected advantage. Failure here results in unstable gradients.

**Design tradeoffs**:
- Large k: Better exploration of hard problems but smaller gradient magnitude (requires higher learning rate)
- Full vs. Analytical: Analytical is faster and stable; Full Sampling is computationally expensive but conceptually simpler

**Failure signatures**:
- Entropy Collapse: Policy converges to repeating a single wrong answer (indicates k too low or learning rate too low)
- Training Instability: Sharp drops in reward (indicates bug in analytical derivation or learning rate too high)

**First 3 experiments**:
1. Sanity Check: Implement "Full Sampling" Pass@k training with k=4 on a small maze task to verify group-reward logic increases entropy compared to Pass@1
2. Efficiency Validation: Implement "Analytical Derivation" on the same task; verify similar reward curves but 3-4x faster wall-clock time
3. Transfer Test: Train with Pass@k, then switch to Pass@1 training; verify if final Pass@1 score exceeds Pass@1-only baseline

## Open Questions the Paper Calls Out

**Open Question 1**: How can the advantage function be adaptively adjusted based on the model's real-time state to balance Pass@1 and Pass@k optimization? The authors hypothesize that advantage computation could be state-adaptive but leave this for future work. Evidence would require a dynamic training loop that modulates advantage weights to maximize both metrics simultaneously.

**Open Question 2**: Do the identified properties of the sum of absolute advantage (Î·) curves generalize to non-synthetic tasks? Current evidence is derived mainly from synthetic Maze tasks, and more comprehensive experiments are needed on diverse benchmarks like code generation or scientific reasoning.

**Open Question 3**: Can implicit reward design be formalized to provide theoretical guarantees for convergence? While the paper demonstrates effectiveness of implicit reward transformations, it lacks a theoretical framework linking specific advantage function shapes to policy gradient convergence bounds.

## Limitations

- Generalization Across Domains: The Pass@k framework assumes binary rewards and verifiable outcomes, which may not extend to domains requiring continuous or subjective evaluation
- Scaling Behavior: Benefits on 7B and 32B models may not translate to frontier models (100B+) which may already have sufficient exploration capabilities
- Computational Overhead: Group-level advantage computation adds complexity compared to standard Pass@1 training

## Confidence

**High Confidence (8-10/10)**:
- Pass@k training consistently improves exploration metrics compared to Pass@1
- The analytical derivation method is mathematically sound and reduces variance
- Transfer learning from Pass@k to Pass@1 training is reproducible

**Medium Confidence (5-7/10)**:
- Pass@k training improves final task performance across all tested domains
- Benefits generalize from 7B to 32B parameter models
- The method outperforms other exploration enhancement techniques

**Low Confidence (1-4/10)**:
- Performance claims on extremely difficult problems (Pass@8 improvements)
- Comparison against all possible baselines in RLVR literature
- Long-term stability of policies trained with Pass@k rewards

## Next Checks

1. **Cross-Domain Validation**: Test Pass@k training on non-reasoning RLVR tasks (e.g., code generation, creative writing) to assess domain generality, measuring both exploration metrics and final task performance.

2. **Dynamic k Scheduling**: Implement a curriculum where k increases during training (starting at k=2, ending at k=16). Compare performance against fixed-k baselines to determine if adaptive k provides additional benefits.

3. **Ablation on Group Size**: Systematically vary N_rollout (group size) from 8 to 64 while keeping k constant. Analyze the relationship between group size, advantage signal quality, and final performance to identify optimal configurations.