---
ver: rpa2
title: 'Prompted Policy Search: Reinforcement Learning through Linguistic and Numerical
  Reasoning in LLMs'
arxiv_id: '2511.21928'
source_url: https://arxiv.org/abs/2511.21928
tags:
- policy
- params
- props
- reward
- should
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Prompted Policy Search (ProPS), a novel reinforcement
  learning method that leverages large language models (LLMs) to unify numerical reward
  feedback with natural language guidance. Unlike traditional RL, ProPS uses LLMs
  at the core of the policy optimization loop to propose updates based on both reward
  signals and linguistic context.
---

# Prompted Policy Search: Reinforcement Learning through Linguistic and Numerical Reasoning in LLMs

## Quick Facts
- arXiv ID: 2511.21928
- Source URL: https://arxiv.org/abs/2511.21928
- Reference count: 40
- Outperforms seven established RL algorithms in 8 out of 15 environments

## Executive Summary
Prompted Policy Search (ProPS) is a novel reinforcement learning method that leverages large language models as the core policy optimizer. Unlike traditional RL, ProPS uses LLMs to propose parameter updates based on both reward signals and natural language context. Evaluated on 15 diverse tasks, ProPS outperforms seven established RL algorithms in 8 out of 15 environments. Incorporating domain knowledge and expert hints further enhances performance, demonstrating that semantic and numerical reasoning together can lead to more efficient, transparent, and human-aligned learning.

## Method Summary
ProPS treats reinforcement learning as an in-context numerical optimization problem where an LLM proposes policy parameters based on a history of parameter-reward pairs. The method uses LLMs (primarily GPT-4o) to iteratively generate new policy parameters by reasoning over previous parameter configurations and their corresponding rewards. The LLM outputs both new parameters and textual justifications explaining the update rationale. ProPS+ extends this by incorporating natural language domain knowledge, environment descriptions, and expert hints into the prompt. The policy evaluation uses standard Gymnasium environments with linear or tabular parameterizations, and the optimization loop runs for 400 iterations with 20 episodes per evaluation.

## Key Results
- ProPS outperforms seven established RL baselines (PPO, SAC, TRPO, etc.) on 8 out of 15 tasks
- Incorporating semantic signals (ProPS+) further improves performance on 12 out of 15 tasks
- LLMs can perform numerical optimization in-context without explicit training for such tasks
- Textual justifications provide human-interpretable explanations of parameter update rationale

## Why This Works (Mechanism)

### Mechanism 1: In-Context Numerical Optimization via History Synthesis
LLMs perform iterative policy optimization by reasoning over a history of parameter-reward pairs without external optimizers. The LLM receives Γ = [θ₁:N, R₁:N] and proposes new θ through in-context learning, identifying trends in which parameter configurations yield higher rewards. This functions similarly to hill-climbing with implicit pattern recognition. Performance degrades when in-context history N is too small, parameter dimensionality exceeds ~100, or lightweight LLMs (<7B parameters) fail to follow instruction formats consistently.

### Mechanism 2: Semantic-Augmented Policy Search (ProPS+)
Incorporating natural language domain knowledge into prompts improves sample efficiency and final performance. The LLM uses this semantic context to constrain search space, avoid physically implausible policies, and initialize exploration in promising regions. Semantics help in 12/15 tasks but hurt in FrozenLake (misleading assumptions). Domain descriptions are beneficial when accurate but can backfire when they incorrectly assume deterministic dynamics in stochastic environments.

### Mechanism 3: Textual Gradients for Interpretability
LLM-generated justifications provide human-interpretable explanations of parameter update rationale. At each iteration, the LLM outputs both new parameters and a textual explanation ("textual gradient") describing observed reward-parameter relationships. This creates transparency unavailable in gradient-based methods. Interpretability breaks when LLMs produce plausible-sounding but incorrect rationales, or when high-dimensional policies make concise explanation impractical.

## Foundational Learning

- **Policy Search (vs. Value-Based RL)**: ProPS directly optimizes policy parameters θ without explicit value functions. Understanding that policy search treats RL as a black-box optimization problem (maximize E[R(τ)]) is essential.
  - Quick check: Can you explain why ProPS doesn't need a Q-function or value network?

- **In-Context Learning in LLMs**: The entire mechanism relies on LLMs learning from examples in the prompt (parameter-reward pairs) without weight updates. This is distinct from fine-tuning.
  - Quick check: If you clear the in-context history after 50 iterations, what would happen to optimization quality?

- **Linear vs. Tabular Policy Parameterization**: The paper uses different policy structures for different environments. Linear policies: π(s) = θᵀφ(s); Tabular: π(s) = θₛ. The LLM's output format changes accordingly.
  - Quick check: For a 4-state, 2-action environment, how many parameters would a tabular policy require?

## Architecture Onboarding

- **Component map:**
  System message (optimizer role) → Parameter definitions + constraints → [ProPS+] Environment description + hints → In-context history Γ (params → rewards) → LLM → new θ + textual justification → Policy Execution (Gymnasium env) → mean episodic reward R → History Buffer (deque with configurable size N) → Update: append (θ, R); maintain order for context

- **Critical path:**
  1. Prompt formatting → Must match expected LLM output format exactly (parse failure = crashed iteration)
  2. History management → N=1 caps reward at ~100; unbounded N reaches 200
  3. Parameter parsing → Extract floats from LLM response; handle malformed output

- **Design tradeoffs:**
  - History length N: Longer = better synthesis but higher token cost and context window pressure
  - Episodes per LLM call: Paper uses 20; fewer = more LLM calls (expensive); more = noisier reward estimates
  - Proprietary vs. open LLMs: GPT-4o works out-of-box; Qwen-14B requires GRPO fine-tuning for comparable performance
  - ProPS vs. ProPS+: Semantics help in 12/15 tasks but hurt in FrozenLake (misleading assumptions)

- **Failure signatures:**
  - FrozenLake pattern: ProPS+ underperforms ProPS when semantic context implies deterministic dynamics in stochastic environments
  - High-dimensional collapse: Performance degrades sharply above ~100 parameters
  - Small model failure: Qwen-7B repeatedly suggests identical parameters; instruction-following breaks down

- **First 3 experiments:**
  1. Baseline validation on MountainCar-Continuous: Run ProPS (numeric-only) with N=unbounded, 400 iterations, 10 trials. Expect: 87.21±29.28. Verify LLM outputs parseable parameters each iteration.
  2. Ablate history length: Compare N ∈ {1, 20, 40, 80, unbounded} on same task. Plot reward vs. N. Confirm near-linear relationship.
  3. Semantic injection test: Run ProPS+ vs. ProPS on FrozenLake. Document the failure case where ProPS+ assumes deterministic transitions. Compare success rates: ProPS (0.57±0.17) vs. ProPS+ (0.19±0.05).

## Open Questions the Paper Calls Out

### Open Question 1
Can Prompted Policy Search (ProPS) be effectively scaled to deep reinforcement learning scenarios involving high-dimensional neural network policies or end-to-end learning from image inputs? The authors explicitly state they "deliberately omitting Deep RL scenarios that involve high-dimensional neural network policies for end-to-end learning" and note that "more research and new methods... will be needed for Deep RL tasks." Current LLM context limits and numerical reasoning capabilities restrict ProPS to linear or tabular policies with moderate dimensionality (up to ~100 parameters).

### Open Question 2
What are the specific mechanisms within pre-trained LLMs that enable zero-shot numerical optimization capabilities for policy search? The authors ask, "How do LLMs exhibit zero-shot numerical optimization abilities without explicit training for such tasks?" and call for a "deeper analysis... to understand and explicitly promote the mechanisms." It is currently hypothesized that exposure to optimization-related corpora during pretraining creates implicit priors, but this remains unproven.

### Open Question 3
How can LLM-based policy search mitigate misleading inductive biases that arise when natural language descriptions incorrectly assume deterministic dynamics? The paper notes that ProPS+ failed on FrozenLake because the LLM assumed deterministic dynamics based on semantic context, whereas the base ProPS (numeric only) succeeded by relying on observed rewards. While the paper identifies that semantic context can be detrimental, it does not propose a mechanism to validate linguistic assumptions against the observed stochasticity of the environment.

## Limitations
- LLM hyperparameters (temperature, top_p, max_tokens) are unspecified, potentially affecting reproducibility
- Initial parameter generation mechanism for iteration 0 is unclear
- High-dimensional policy performance (>100 parameters) degrades significantly
- Context window limits may constrain unbounded history experiments

## Confidence
- **High confidence:** Core ProPS mechanism (LLM-as-optimizer with history), numerical optimization capabilities, basic implementation approach
- **Medium confidence:** Semantic augmentation effects (ProPS+), interpretability claims, ablation study interpretations
- **Low confidence:** Generalization to complex environments, scalability to high-dimensional policies, exact hyperparameter settings

## Next Checks
1. Implement history length ablation study (N ∈ {1, 20, 40, 80, unbounded}) on MountainCar-Continuous to verify near-linear reward improvement
2. Run ProPS vs. ProPS+ on FrozenLake to document the failure case where semantic context implies deterministic dynamics in stochastic environments
3. Test context window limits by running ProPS with 400 iterations and monitoring token usage to determine practical history bounds