---
ver: rpa2
title: 'CorIL: Towards Enriching Indian Language to Indian Language Parallel Corpora
  and Machine Translation Systems'
arxiv_id: '2509.19941'
source_url: https://arxiv.org/abs/2509.19941
tags:
- language
- languages
- translation
- nllb
- corpora
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CorIL, a high-quality parallel corpus for
  11 Indian languages across government, health, and general domains, totaling 772K
  sentence pairs. The dataset addresses the scarcity of domain-specific parallel corpora
  for low-resource Indian languages and aims to improve machine translation systems.
---

# CorIL: Towards Enriching Indian Language to Indian Language Parallel Corpora and Machine Translation Systems

## Quick Facts
- arXiv ID: 2509.19941
- Source URL: https://arxiv.org/abs/2509.19941
- Reference count: 40
- New high-quality parallel corpus for 11 Indian languages across government, health, and general domains

## Executive Summary
This paper introduces CorIL, a high-quality parallel corpus for 11 Indian languages across government, health, and general domains, totaling 772K sentence pairs. The dataset addresses the scarcity of domain-specific parallel corpora for low-resource Indian languages and aims to improve machine translation systems. To demonstrate utility, the authors fine-tune state-of-the-art NMT models (IndicTrans2, NLLB, BhashaVerse) and evaluate them across domains. Results show domain-specific fine-tuning improves performance on in-domain test sets but causes slight degradation on general benchmarks, indicating domain overfitting. Notably, multilingual models outperform bilingual ones on Perso-Arabic scripts (Urdu, Sindhi), while Indic scripts show varied results. Overall, CorIL is a valuable resource for advancing Indian language machine translation research and benchmarking.

## Method Summary
The authors constructed CorIL by collecting and aligning parallel text across 11 Indian languages in three domains: government, health, and general. They employed quality control measures including human validation and automated filtering to ensure corpus quality. The resulting dataset contains 772K sentence pairs with balanced representation across languages and domains. To validate the corpus utility, they fine-tuned three established neural machine translation models (IndicTrans2, NLLB, BhashaVerse) using the new data. Evaluation was conducted on both in-domain test sets and general MT benchmarks to assess domain adaptation effects and cross-script performance differences.

## Key Results
- CorIL dataset contains 772K parallel sentence pairs across 11 Indian languages
- Domain-specific fine-tuning improves in-domain translation performance but causes slight degradation on general benchmarks
- Multilingual models outperform bilingual models on Perso-Arabic scripts (Urdu, Sindhi), while Indic scripts show mixed results

## Why This Works (Mechanism)
Assumption: The dataset's effectiveness stems from providing sufficient domain-specific training data that captures specialized vocabulary and translation patterns unique to government, health, and general domains. The multilingual models' superior performance on Perso-Arabic scripts likely results from better handling of character-level variations and script-specific tokenization patterns through cross-language transfer learning. Domain overfitting occurs when models become too specialized to the fine-tuning data, reducing their ability to generalize to general language use.

## Foundational Learning
- **Neural Machine Translation (NMT)**: Sequence-to-sequence models using attention mechanisms to translate between languages; needed for modern translation systems; quick check: can translate simple sentence pairs
- **Parallel Corpora**: Aligned sentence pairs in source and target languages; essential training data for supervised MT; quick check: verify alignment quality and coverage
- **Domain Adaptation**: Fine-tuning models on specific domain data to improve performance; crucial for specialized vocabulary; quick check: measure performance difference between in-domain and out-of-domain
- **Multilingual Models**: Single models trained on multiple language pairs; leverage cross-language transfer; quick check: compare multilingual vs bilingual performance
- **BLEU Score**: Automated metric for MT quality based on n-gram overlap; widely used but limited for morphologically rich languages; quick check: understand BLEU score range and interpretation
- **Script Differences**: Perso-Arabic vs Indic scripts require different modeling approaches; affects character encoding and tokenization; quick check: identify script types in target languages

## Architecture Onboarding
**Component Map**: Data Collection -> Corpus Cleaning -> Model Fine-tuning -> Evaluation -> Benchmarking
**Critical Path**: Clean parallel corpus → Fine-tune NMT models → Evaluate on in-domain and general benchmarks
**Design Tradeoffs**: Domain-specific vs general-purpose models (performance vs versatility), multilingual vs bilingual models (transfer learning vs specialization)
**Failure Signatures**: Domain overfitting (good in-domain, poor general performance), script-specific limitations (better on Perso-Arabic with multilingual models)
**3 First Experiments**: 1) Evaluate BLEU scores across all 11 language pairs, 2) Compare multilingual vs bilingual model performance on Perso-Arabic scripts, 3) Test domain adaptation effects on government and health domains

## Open Questions the Paper Calls Out
Unknown: The paper does not explicitly identify specific open questions for future research. Based on the limitations section, potential areas for further investigation include developing more effective domain adaptation techniques that avoid overfitting, creating comprehensive evaluation frameworks for morphologically rich languages, and expanding the corpus to cover additional Indian languages and domains.

## Limitations
- Limited evaluation metrics (primarily BLEU) for morphologically rich Indian languages
- Observed domain overfitting when fine-tuning on in-domain data without clear mitigation strategies
- Selection of languages and domains may not fully represent Indian language diversity
- No systematic analysis of multilingual vs bilingual model performance differences beyond Perso-Arabic scripts

## Confidence
- Major claims: Medium
- Evaluation methodology: Low (reliance on BLEU scores)
- Corpus quality: Medium (human validation mentioned but not detailed)
- Domain adaptation findings: Medium (observed but not deeply analyzed)

## Next Checks
1) Evaluate model performance using additional metrics like chrF or COMET to complement BLEU scores
2) Conduct a more systematic analysis of the multilingual vs bilingual model performance differences across scripts
3) Test the trained models on additional, independently curated test sets to assess generalization beyond the in-domain data