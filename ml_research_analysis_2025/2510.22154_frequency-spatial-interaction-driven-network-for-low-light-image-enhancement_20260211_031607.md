---
ver: rpa2
title: Frequency-Spatial Interaction Driven Network for Low-Light Image Enhancement
arxiv_id: '2510.22154'
source_url: https://arxiv.org/abs/2510.22154
tags:
- image
- information
- enhancement
- frequency
- low-light
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a novel Frequency-Spatial Interaction Driven
  Network (FSIDNet) for low-light image enhancement. The method addresses the limitations
  of existing approaches that either ignore frequency domain information or fail to
  promote effective information propagation.
---

# Frequency-Spatial Interaction Driven Network for Low-Light Image Enhancement

## Quick Facts
- **arXiv ID:** 2510.22154
- **Source URL:** https://arxiv.org/abs/2510.22154
- **Reference count:** 40
- **Primary result:** Achieves SOTA PSNR of 22.65 and SSIM of 0.8524 on LOL-Real dataset

## Executive Summary
This paper introduces a novel two-stage network called FSIDNet for low-light image enhancement that addresses limitations in existing approaches by effectively utilizing both frequency and spatial domain information. The method employs a unique architecture where the first stage restores image amplitude for lightness enhancement, while the second stage restores phase information for fine-grained structural refinement. The core innovation lies in the frequency-spatial interaction blocks that mutually fuse global frequency and local spatial information, combined with an Information Exchange Module that incorporates cross-stage and cross-scale features to boost information propagation.

## Method Summary
FSIDNet employs a two-stage architecture where the first stage focuses on amplitude restoration to improve global lightness, and the second stage handles phase restoration to refine structural details. The model uses frequency-spatial interaction blocks (FSIA and FSIP) that process images in both spatial and frequency domains, allowing for complementary information fusion. An Information Exchange Module (IEM) connects the two stages by incorporating cross-stage and cross-scale features using dynamic filters. The network is trained on multiple benchmark datasets including LOL-Real, LOL-Synthetic, LSRW-Huawei, and LSRW-Nikon with 384×384 random crops and standard data augmentation techniques.

## Key Results
- Achieves state-of-the-art PSNR of 22.65 and SSIM of 0.8524 on LOL-Real dataset
- Outperforms competing methods in perceptual quality across multiple benchmark datasets
- Demonstrates effective decoupling of amplitude and phase information for improved enhancement quality

## Why This Works (Mechanism)

### Mechanism 1: Sequential Amplitude-Then-Phase Restoration
- **Claim:** Processing image enhancement in two sequential stages—first restoring Fourier amplitude for lightness, then restoring phase for structure—yields better fidelity than simultaneous restoration.
- **Mechanism:** The architecture decouples the enhancement task into an "Amplitude-guided brightness enhancement stage" and a "Phase-guided structure-refined stage." By supervising the first stage with a target derived from the ground truth amplitude but the input's phase ($F^{-1}(A(X_{gt}), P(X_{in}))$), the network focuses purely on global lightness adjustment before addressing fine-grained structural details in the second stage.
- **Core assumption:** The assumption is that amplitude and phase information are sufficiently independent that sequential optimization does not lead to conflicting gradient updates or irreversible information loss in the first stage.
- **Evidence anchors:** [abstract] "...first stage is designed to restore the amplitude... and the second stage devotes to restore phase information..."; [section III.B] "...uses $F^{-1}(A(Y_1), P(X_{in}))$ to ensure the retention of original phase information [for the second stage input]."

### Mechanism 2: Bidirectional Frequency-Spatial Interaction
- **Claim:** Bidirectional interaction between spatial and frequency features allows the model to correct global illumination errors (frequency) while maintaining local texture consistency (spatial).
- **Mechanism:** The Frequency-Spatial Interaction Blocks (FSIA/FSIP) operate two parallel branches. The frequency branch applies a learnable operation ($Op$) to the amplitude (or phase) in Fourier space. The output is then inverse-transformed and fused with the spatial branch via residual additions ($f'_{s1} = f_{s1} + W^1(f_{f1})$). This allows global frequency context to modulate local spatial features.
- **Core assumption:** Simple addition or convolution-based fusion ($W^1, W^2$) is sufficient to align the feature distributions between the Fourier domain (global statistics) and spatial domain (local pixels).
- **Evidence anchors:** [abstract] "...two frequency-spatial interaction blocks which mutually amalgamate the complementary spatial and frequency information..."; [section III.B] "In that case, both $f'_{s1}$ and $f'_{f1}$ get the complementary representation..."

### Mechanism 3: Cross-Stage Feature Propagation
- **Claim:** Propagating intermediate features across stages and scales prevents the loss of low-level details typically caused by deep network encoding.
- **Mechanism:** The Information Exchange Module (IEM) functions as a bypass system. It extracts features from the first stage's decoder ($\{D^1_{si}\}$) and encoder of the second stage, fuses them, and uses a Dynamic Filter Block (DFB) to generate content-adaptive convolution kernels. These kernels process the second stage's decoder features, explicitly injecting "cross-stage" and "cross-scale" context.
- **Core assumption:** The dynamic filters generated by the IEM can learn to select relevant context from the coarse first-stage output without reintroducing the noise or artifacts that the second stage is meant to remove.
- **Evidence anchors:** [abstract] "...Information Exchange Module (IEM) to associate two stages by adequately incorporating cross-stage and cross-scale features..."; [section III.C] "In the first stage's decoder and second stage's encoder, the IEM integrates information from all scales..."

## Foundational Learning

- **Concept: Discrete Fourier Transform (DFT) in Deep Learning**
  - **Why needed here:** The entire first stage and half the second stage rely on transforming images into the frequency domain using FFT. You must understand that DFT converts spatial signals into sine/cosine components (Amplitude/Phase) to understand *why* the authors separate lightness (Amplitude) and structure (Phase).
  - **Quick check question:** If you swap the amplitude of a low-light image with the amplitude of a well-lit image, does the structure of the resulting image change?

- **Concept: U-Net / Encoder-Decoder Architectures**
  - **Why needed here:** Both stages utilize an encoder-decoder-like format. Understanding how features are downsampled (compressed) and upsampled (reconstructed), and the role of skip connections (or the lack thereof, as noted in Stage 2), is essential for debugging the FSIA/FSIP blocks.
  - **Quick check question:** In a standard U-Net, why are skip connections critical, and what might happen to high-frequency details in Stage 2 of FSIDNet where standard skip connections are modified?

- **Concept: Dynamic Filtering**
  - **Why needed here:** The IEM uses a Dynamic Filter Block (DFB). Unlike standard CNNs with fixed weights after training, DFBs generate weights on the fly based on input content. This is key to how the model handles "in-the-wild" lighting variations.
  - **Quick check question:** How does a dynamic filter adapt to an input image compared to a standard 3x3 convolutional kernel?

## Architecture Onboarding

- **Component map:**
  - Input: Low-light image $X_{in}$
  - Stage 1 (Brightness): U-Net structure → Contains **FSIA Blocks** (Amplitude restoration) → Output $Y_1$ (Bright but potentially structural artifacts)
  - Intermediary: $X_{stage2\_input} = F^{-1}(A(Y_1), P(X_{in}))$ (Mixes Stage 1 lightness with Input structure)
  - Stage 2 (Structure): U-Net structure → Contains **FSIP Blocks** (Phase restoration) + **IEM** (Feature injection) → Output $Y_2$ (Final Enhanced)
  - Frequency Branches: Inside FSIA/FSIP → FFT → Process Amplitude/Phase → IFFT → Fuse with Spatial Branch

- **Critical path:**
  1. **FFT Operation:** The conversion to frequency domain in FSIA/FSIP must handle complex numbers correctly (mapping to real/imaginary channels)
  2. **IEM Fusion:** The cross-stage connection is where most "information gain" occurs. If this fails, Stage 2 is just a blind denoiser
  3. **Phase Preservation:** Ensure the transition from Stage 1 to Stage 2 strictly uses $P(X_{in})$, not $P(Y_1)$

- **Design tradeoffs:**
  - **Two-Stage vs. Complexity:** Achieves SOTA PSNR (22.65) but doubles the decoder depth compared to single-stage methods
  - **Frequency vs. Spatial:** Using FFT allows global lighting correction in one pass (efficient for global illumination) but adds computational overhead and complexity in handling complex-valued data

- **Failure signatures:**
  - **Color Artifacts:** If FSIA fails to handle amplitude correctly, spectral leakage in FFT may cause color banding
  - **Ghosting/Blur:** If IEM misaligns cross-scale features, texture details in Stage 2 may become blurry
  - **Over-exposure:** If Stage 1 amplitude prediction is too aggressive, Stage 2 might not recover highlight details

- **First 3 experiments:**
  1. **Ablation (Single Stage):** Run Stage 1 alone vs. Full Model to quantify how much the Phase/Structure stage contributes to SSIM (structural similarity)
  2. **Module Swap:** Replace the Dynamic Filter Block in IEM with a standard static convolution to verify the necessity of content-adaptive filtering
  3. **Frequency Visualization:** Extract $A(Y_1)$ and $P(Y_2)$ during inference and visualize the spectral maps to ensure the network is actually separating lightness and structure as claimed

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but the methodology and results raise several implicit research directions related to the sequential processing approach, the frequency-spatial interaction mechanism, and the impact on downstream vision tasks.

## Limitations
- The sequential dependency where phase restoration relies on amplitude-restored output may limit handling of degradation that couples lightness and structure
- The additive interaction mechanism (Eq. 7) is not proven to be optimal compared to attention-based fusion methods
- The frequency-domain manipulation may introduce high-frequency artifacts that could negatively impact downstream computer vision tasks

## Confidence
- **High confidence:** The two-stage architecture design and frequency-spatial interaction mechanism (FSIA/FSIP blocks) are well-specified and theoretically sound
- **Medium confidence:** The effectiveness of the IEM module and its contribution to the performance gain, as quantitative ablation results are limited
- **Medium confidence:** The claimed SOTA performance metrics, as the paper lacks detailed comparisons against all recent methods on multiple datasets

## Next Checks
1. **Ablation study:** Compare the full two-stage model against a single-stage variant where amplitude and phase are restored simultaneously to isolate the contribution of sequential processing
2. **Module isolation:** Replace the Dynamic Filter Block in IEM with a standard static convolution layer to verify the necessity of content-adaptive filtering
3. **Cross-dataset generalization:** Test the trained model on a held-out low-light dataset not seen during training to evaluate generalization beyond the reported benchmark datasets