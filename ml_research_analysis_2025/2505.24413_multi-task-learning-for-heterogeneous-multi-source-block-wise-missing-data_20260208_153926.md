---
ver: rpa2
title: Multi-task Learning for Heterogeneous Multi-source Block-Wise Missing Data
arxiv_id: '2505.24413'
source_url: https://arxiv.org/abs/2505.24413
tags:
- data
- learning
- tasks
- heterogeneity
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a two-step multi-task learning framework to
  address block-wise missing data and three types of heterogeneity (block-wise, distribution,
  and posterior) in multi-source settings. In the first step, a heterogeneous block-wise
  imputation (HBI) method is developed to impute missing blocks by extracting shared
  and task-specific representations from the anchoring source, which helps capture
  complex data structures and improve generalization.
---

# Multi-task Learning for Heterogeneous Multi-source Block-Wise Missing Data

## Quick Facts
- arXiv ID: 2505.24413
- Source URL: https://arxiv.org/abs/2505.24413
- Authors: Yang Sui; Qi Xu; Yang Bai; Annie Qu
- Reference count: 40
- Primary result: Proposes a two-step multi-task learning framework for block-wise missing data with three types of heterogeneity, outperforming single-task and transfer learning baselines on synthetic and ADNI data

## Executive Summary
This paper addresses the challenge of multi-task learning when data comes from multiple sources with block-wise missing patterns and three types of heterogeneity: block-wise, distribution, and posterior. The authors propose a two-step framework where the first step imputes missing blocks using shared and task-specific representations from an anchoring source, and the second step learns task-specific and shared response mappings. The method handles complex data structures by disentangling representations and uses regularizers to manage imputation uncertainty. Experiments on synthetic data and the ADNI database demonstrate improved prediction accuracy over competing methods.

## Method Summary
The framework operates in two steps: first, a Heterogeneous Block-wise Imputation (HBI) module extracts shared and task-specific representations from an anchoring source to impute missing blocks in task-specific sources. This uses encoder-decoder architectures with shared encoder Ec and task-specific encoders Ep. Second, a heterogeneous multi-task learning module disentangles the feature-response mapping into shared and task-specific components using encoders φc (on anchoring source only) and φp (on all features). The method integrates orthogonality and robustness regularizers to reduce redundancy and handle imputation errors, enabling effective information borrowing while adapting to task differences.

## Key Results
- The method outperforms single-task learning and transfer learning baselines, especially under high heterogeneity conditions
- Experiments on synthetic data show clear trends with varying levels of shared structure (α parameter)
- Real-data analysis on ADNI database demonstrates improved prediction accuracy over competing methods
- The model effectively captures both shared and task-specific representations as shown in t-SNE visualizations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Disentangled representations from the anchoring source enable cross-task imputation under distribution heterogeneity
- Mechanism: The HBI module learns shared encoder Ec (common across tasks) and task-specific encoders Ep (unique per task) from the anchoring source x0. The shared representation fc = Ec(x0) captures cross-task invariant features used by predictor G to impute missing blocks, while task-specific representations gp = Ep(x0) capture heterogeneity. Decoder D reconstructs x0 from both representations, ensuring they jointly explain the observed data
- Core assumption: The relationship between anchoring source x0 and task-specific sources xt can be partially transferred across tasks through shared latent structure
- Evidence anchors:
  - [abstract] "impute missing blocks by extracting shared and task-specific representations from the anchoring source, which helps capture complex data structures and improve generalization"
  - [section 3.1] "HBI extracts disentangled hidden representations from the anchoring source x0, including a shared representation across tasks and a task-specific representation for each task"
  - [corpus] Related work on domain separation networks (Bousmalis et al., 2016) shows similar disentanglement improves transfer

### Mechanism 2
- Claim: Two-pathway response mapping decouples shared signal from task-specific signal, enabling information borrowing while preserving heterogeneity
- Mechanism: The response yt = ψc(features) + ψtp(features) is modeled as sum of shared and task-specific components. Shared encoder φc extracts h from anchoring source only; task-specific encoder φp extracts k from all features including imputed blocks. These representations flow through L-layer networks with shared weights (for ψc) and task-specific weights (for ψtp), allowing gradient sharing where beneficial while isolating task-unique patterns
- Core assumption: Feature-response relationships contain both transferable (shared) and task-unique (specific) components that can be linearly decomposed
- Evidence anchors:
  - [abstract] "disentangle the mappings between input features and responses into a shared component and a task-specific component"
  - [section 3.2] "Equation (2) extends traditional meta-analysis, which often assumes a linear relationship... In contrast, we propose a flexible framework which accommodates non-linearities"
  - [corpus] Bica and van der Schaar (2022) uses similar shared/specific decomposition but assumes homogeneous anchoring source

### Mechanism 3
- Claim: Orthogonality and robustness regularizers prevent representation collapse and handle imputation uncertainty
- Mechanism: Three regularizers operate jointly: (1) Rorth = Σ‖H⊤K‖²F forces shared and task-specific representations to be orthogonal, preventing redundancy; (2) Rimp = Σ‖Θs,p,1‖²F penalizes first-layer weights on imputed features, downweighting uncertain inputs; (3) Rdr = Σ‖(Θc,l)⊤Θp,l‖²F enforces orthogonality between shared and task-specific layer weights
- Core assumption: Imputed features are less reliable than observed features and should receive lower effective weight
- Evidence anchors:
  - [section 3.2] "Since imputation can introduce errors, we downweight the imputed data... by applying a regularizer to the parameters of the first layer"
  - [section 3.2] "To further reduce redundancy between the shared and task-specific layers, we introduce an orthogonal regularizer"
  - [corpus] Orthogonal regularization for disentanglement is established in domain adaptation literature

## Foundational Learning

- Concept: Encoder-decoder architectures and representation disentanglement
  - Why needed here: The entire HBI module relies on understanding how encoders map inputs to latent spaces and decoders reconstruct from latents; disentanglement into shared/specific components is the core technical innovation
  - Quick check question: Can you explain why forcing two latent representations to be orthogonal helps them capture different aspects of the data?

- Concept: Multi-task learning with parameter sharing strategies
  - Why needed here: The framework must balance shared learning (for information transfer) against task-specific adaptation (for heterogeneity); understanding hard vs. soft parameter sharing is prerequisite
  - Quick check question: What would happen if all layers were shared across tasks (hard parameter sharing) when tasks have posterior heterogeneity?

- Concept: Missing data mechanisms (MCAR/MAR/MNAR) and imputation bias
  - Why needed here: Block-wise missing pattern is the core problem; the paper assumes a specific missing structure (anchoring source always observed, task-specific sources block-missing) that affects how imputation can work
  - Quick check question: Why does the paper assume the anchoring source is always observed? What would break if x0 were also block-wise missing?

## Architecture Onboarding

- Component map:
  ```
  Step 1 (HBI):
    Input: x0 (anchoring), xt (task-specific, observed only for task t)
    Ec (shared encoder) → fc (shared representation)
    Ep (task-specific encoder) → gp (task-specific representation)
    D (decoder) → reconstruction of x0
    G (predictor) → imputation of missing xt blocks
    
  Step 2 (MTL):
    Input: x0 (observed), all x1...xT (observed + imputed)
    φc (shared encoder on x0) → h
    φp (task-specific encoder on all features) → k
    L-layer network with shared/task-specific pathways → ŷ
  ```

- Critical path:
  1. Verify anchoring source quality (MMD test in Section 4.3 shows x0 may still be heterogeneous)
  2. Train HBI for each task-specific source in parallel (Algorithm 1, lines 3-21)
  3. Impute missing blocks using trained G∘Ec
  4. Train MTL module on observed + imputed data with all three regularizers
  5. Monitor validation loss with early stopping (patience=30 in experiments)

- Design tradeoffs:
  - Two-step vs. joint optimization: Paper uses two-step for stability; Section S.7 notes joint optimization is future work due to loss balancing challenges
  - Anchoring source selection: Must be available for all tasks; paper uses MRI in ADNI, but Section 4.3 shows even "shared" MRI has significant heterogeneity (p=1e-6)
  - Network depth: Deeper networks capture more complex relationships but risk overfitting on small samples (experiments use depths 1-5 with widths 8-128)

- Failure signatures:
  - Imputation produces near-constant values: Check if G is learning; Ec may have collapsed to trivial representation
  - Task-specific representation collapses to zero: Rorth weight γ may be too high
  - Performance worse than single-task learning: Distribution heterogeneity too extreme; check MMD between tasks
  - Training unstable: Check batch size (paper uses 8-32 for small datasets) and learning rate (0.001 standard)

- First 3 experiments:
  1. Sanity check on synthetic data with known ground truth: Generate data with controlled α (shared mapping proportion) and ρ (correlation); verify imputation accuracy and prediction RMSE match expected behavior (α→1 should favor MTL, α→0 should reduce to near-single-task)
  2. Ablation study per Section S.3: Compare (HBI + STL), (Naive imputation + Step 2), and full method to isolate contribution of each component; expect Step 2 to be more important than Step 1
  3. Real data validation on ADNI or similar: Replicate train/val/test split (60/20/20), verify improvement over STL and HTL baselines; check that t-SNE visualization shows distinct shared vs. task-specific clusters as in Figure 7

## Open Questions the Paper Calls Out

- Can the two-step HBI and MTL framework be unified into a single-step approach where shared and task-specific representations are learned jointly for both imputation and response prediction?
  - Basis in paper: The Discussion states: "A possible improvement would be to combine these two steps into one, unifying multiple tasks to learn the hidden representations for each task, which can then be used for both imputation and response learning."
  - Why unresolved: Balancing different loss functions to achieve both accurate imputation and prediction simultaneously poses computational challenges.
  - What evidence would resolve it: A unified algorithm demonstrating comparable or superior prediction accuracy with reduced computational cost on the same synthetic and ADNI benchmarks.

- How can the framework be extended to handle partially shared structures when only a subset of tasks share common information rather than assuming all tasks share a common mapping ψc?
  - Basis in paper: The Discussion notes: "In reality, when there is strong heterogeneity across multiple datasets, the shared structure is often only partial... an adaptive approach for MTL is needed, one that explores partially shared information among tasks."
  - Why unresolved: Current methods assume a common mapping exists across all tasks, which may not hold under strong heterogeneity.
  - What evidence would resolve it: Development of an adaptive MTL method that can identify and leverage partially shared structures while isolating highly heterogeneous tasks.

- How does the framework perform when the anchoring source assumption is violated or when multiple anchoring sources exist?
  - Basis in paper: The method explicitly requires "a common source, called the anchoring source, is observed" for all tasks. No experiments or discussion address scenarios where this assumption fails or where multiple potential anchoring sources exist.
  - Why unresolved: Real-world multi-source data may not have a single source guaranteed across all tasks.
  - What evidence would resolve it: Sensitivity analysis or methodological extension demonstrating robustness to anchoring source unavailability or methods for automatic anchoring source selection.

## Limitations

- The method assumes partial shared structure exists across tasks, but this is difficult to verify in practice and may break down under extreme heterogeneity
- Anchoring source selection is critical but not automated, requiring domain expertise to identify an appropriate source
- Regularization hyperparameters (γ, δ, κ) are shown to be "robust" but lack theoretical guidance for selection
- The two-step training procedure may not find globally optimal solutions compared to joint optimization

## Confidence

- **High Confidence**: The overall two-step framework design and the core intuition that disentangled representations enable better handling of heterogeneity. The synthetic data experiments showing clear trends with varying heterogeneity levels are convincing.
- **Medium Confidence**: The effectiveness of the specific regularizers (orthogonality and robustness) and the exact architecture choices. While the ablation studies support their importance, alternative designs might perform similarly.
- **Low Confidence**: The generalizability to real-world scenarios where anchoring source quality is uncertain or where multiple sources have complex missing patterns beyond the assumed block-wise structure.

## Next Checks

1. **Cross-task anchoring source validation**: Systematically test whether different anchoring sources (e.g., using Task 2 as anchor for Task 1) maintain performance, revealing whether the method is sensitive to anchoring source selection or truly captures transferable structure.

2. **Extreme heterogeneity stress test**: Generate synthetic data with varying degrees of distribution heterogeneity (measured via MMD) to identify the threshold where shared structure becomes insufficient, validating the paper's assumption about partial shared structure.

3. **Alternative regularization comparison**: Replace the orthogonality regularizer with alternative disentanglement methods (e.g., adversarial domain separation) to determine whether orthogonality is optimal or merely one effective choice for this problem.