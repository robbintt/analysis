---
ver: rpa2
title: Pre-Training Curriculum for Multi-Token Prediction in Language Models
arxiv_id: '2505.22757'
source_url: https://arxiv.org/abs/2505.22757
tags:
- curriculum
- reverse
- forward
- language
- heads
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper proposes a curriculum learning approach to improve
  the training of smaller language models using the multi-token prediction (MTP) objective.
  The authors introduce two variants: a forward curriculum that gradually increases
  prediction complexity from single-token to multi-token prediction, and a reverse
  curriculum that does the opposite.'
---

# Pre-Training Curriculum for Multi-Token Prediction in Language Models

## Quick Facts
- arXiv ID: 2505.22757
- Source URL: https://arxiv.org/abs/2505.22757
- Authors: Ansar Aynetdinov; Alan Akbik
- Reference count: 40
- Primary result: Forward curriculum enables smaller models to better leverage MTP benefits while maintaining self-speculative decoding advantages.

## Executive Summary
This paper proposes curriculum learning approaches to improve multi-token prediction (MTP) training for smaller language models (SLMs). The authors introduce forward and reverse curricula that gradually adjust the complexity of MTP training, enabling SLMs to benefit from MTP while maintaining inference speed advantages. The forward curriculum incrementally increases prediction heads from single-token to multi-token prediction, while the reverse curriculum starts with full MTP and gradually reduces complexity. Experiments demonstrate that forward curriculum MTP enables SLMs to achieve better downstream performance than static MTP while retaining self-speculative decoding benefits. Notably, byte-level models consistently outperform subword models across all configurations, suggesting MTP is more effective when modeling smaller semantic units.

## Method Summary
The paper proposes curriculum learning for multi-token prediction in language models, using two variants: forward curriculum (starting with single-token prediction and gradually increasing prediction heads) and reverse curriculum (starting with full MTP and gradually reducing complexity). The MTP objective predicts k future tokens simultaneously using a shared transformer backbone with k independent prediction heads (either linear layers or transformer layers). The curriculum scheduler adjusts the number of active heads based on training epoch using simple pre-defined schedules. Training uses a Llama-style decoder-only transformer on the MiniPile dataset with 1 epoch, batch size 1024, and standard optimization hyperparameters. The approach aims to enable smaller models to benefit from MTP while maintaining self-speculative decoding capabilities for faster inference.

## Key Results
- Forward curriculum enables SLMs to better leverage MTP benefits, improving downstream performance while maintaining self-speculative decoding advantages
- Reverse curriculum achieves stronger NTP performance and output quality but fails to provide inference speed benefits
- Byte-level models consistently outperform subword models across all metrics and model configurations
- Smaller models struggle with static MTP but succeed with curriculum-based approaches

## Why This Works (Mechanism)

### Mechanism 1
Gradual complexity increase via forward curriculum enables smaller models to learn MTP without capacity overload. Starting with single-token prediction and incrementally adding prediction heads allows the shared backbone to first establish fundamental token-level representations before being required to encode richer multi-token dependencies. The model builds compositional capacity progressively rather than simultaneously optimizing all heads.

### Mechanism 2
Reverse curriculum improves main-head NTP performance by annealing from representationally rich MTP to focused NTP. Training with full MTP forces the backbone to develop hidden states encoding longer-horizon dependencies. Gradually reducing k nudges the model toward NTP while retaining these richer representations in the main head, improving downstream single-token tasks.

### Mechanism 3
Byte-level tokenization reduces per-token semantic density, making multi-token prediction more tractable for SLMs. Bytes encode less morphological and semantic information per unit than subword tokens. Predicting 4 future bytes requires modeling simpler compositional patterns than predicting 4 future subwords, reducing the effective task complexity for capacity-constrained models.

## Foundational Learning

- Concept: **Curriculum Learning**
  - Why needed here: The paper's core contribution is structuring MTP training complexity over time rather than applying it uniformly
  - Quick check question: Can you explain why ordering training examples or objectives by difficulty might improve convergence?

- Concept: **Self-Speculative Decoding**
  - Why needed here: MTP enables faster inference by drafting multiple tokens per forward pass and verifying them, but this requires auxiliary heads to maintain meaningful predictions
  - Quick check question: How does drafting k tokens and verifying in one pass reduce total forward passes while maintaining output quality?

- Concept: **Shared Backbone with Multiple Heads**
  - Why needed here: The MTP architecture uses one transformer backbone feeding k prediction heads, trading off between linear layers (more params) and transformer layers (shared params but reduced backbone depth)
  - Quick check question: What is the memory overhead difference between adding a linear head vs. using a transformer layer as a head?

## Architecture Onboarding

- Component map:
  Shared Transformer Backbone -> k Prediction Heads (LL variant: linear layers OR TL variant: transformer layers) -> Final Output Layer

- Critical path:
  1. Initialize backbone + k_max heads
  2. Set k_current = 1 (forward) or k_max (reverse) at epoch 0
  3. Train for E/k_max epochs, then adjust k_current by Â±1
  4. Repeat until curriculum completes
  5. For inference: use all heads for self-speculative decoding (forward curriculum) or main head only (reverse curriculum)

- Design tradeoffs:
  - **LL vs. TL heads**: LL adds parameters but preserves full backbone depth; TL is parameter-free but weakens backbone
  - **Forward vs. Reverse**: Forward retains speculative decoding speedups (~2-3x per Figure 2); Reverse yields better NTP metrics but no speedup
  - **Byte vs. Subword**: Byte models require 4x context window but achieve better MTP performance with smaller head params

- Failure signatures:
  - Reverse curriculum auxiliary heads show near-zero acceptance rates in speculative decoding (Figure 3)
  - Subword SLMs with static MTP underperform NTP baselines on BLiMP (Table 1)
  - Forward curriculum with 8 LL byte heads shows degraded output quality vs. 4 LL (Table 2), suggesting head count saturation

- First 3 experiments:
  1. **Baseline replication**: Train 1.3B subword model with k=4 LL heads using static MTP; verify underperformance vs. NTP on LAMBADA/BLiMP
  2. **Forward curriculum ablation**: Same setup with forward curriculum; measure (a) NTP benchmark delta, (b) speculative decoding acceptance rate per head
  3. **Byte vs. subword comparison**: Train 3B byte-level model with k=4 LL heads + forward curriculum; compare BPB and inference speedup to subword equivalent

## Open Questions the Paper Calls Out

- Would adaptive curriculum schedules based on training dynamics outperform the pre-defined uniform schedules explored in this work?
- Can the benefits of MTP curricula transfer to non-transformer architectures such as Mamba, RWKV, or other state-space models?
- Why does the reverse curriculum fail to provide self-speculative decoding benefits while the forward curriculum retains them?
- Do the curriculum benefits scale to larger models (7B+) and longer training regimes beyond the 1.3B-3B models tested?

## Limitations

- The study focuses exclusively on small language models (1.3B and 3B parameters), leaving unclear whether curriculum benefits persist at larger scales where static MTP becomes effective
- The single-epoch training regime provides limited insight into whether curriculum advantages compound or decay over extended training
- The MiniPile dataset may not fully represent the diversity of larger pre-training corpora used in production settings

## Confidence

- **High Confidence**: The forward curriculum's effectiveness for enabling SLM-MTP benefits is well-supported across multiple model sizes, tokenization schemes, and evaluation benchmarks
- **Medium Confidence**: The reverse curriculum's superior NTP performance claims warrant moderate confidence due to the asymmetric value proposition without inference speed benefits
- **Low Confidence**: The byte-level advantage over subword tokenization is the least substantiated claim, lacking controlled ablations to isolate the causal mechanism

## Next Checks

1. Train 7B and 13B models using the same curriculum framework to determine whether the SLM-specific benefits persist, diminish, or reverse at larger scales
2. Compare the proposed epoch-based head increment against alternative scheduling strategies including linear interpolation, adaptive progression based on validation loss, and continuous head weighting
3. Train subword models with systematically reduced vocabulary sizes (8K, 16K, 32K) while holding model size and curriculum constant to disentangle whether byte-level gains derive from smaller semantic units versus simply reduced vocabulary complexity