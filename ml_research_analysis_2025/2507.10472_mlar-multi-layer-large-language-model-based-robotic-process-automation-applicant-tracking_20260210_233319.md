---
ver: rpa2
title: 'MLAR: Multi-layer Large Language Model-based Robotic Process Automation Applicant
  Tracking'
arxiv_id: '2507.10472'
source_url: https://arxiv.org/abs/2507.10472
tags:
- automation
- resume
- mlar
- recruitment
- system
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper presents MLAR, a novel multi-layer Large Language Model-based
  Applicant Tracking System designed to automate and enhance the recruitment process.
  The system employs three distinct layers: extracting job description attributes,
  parsing applicant resumes for key features, and performing semantic similarity matching
  between candidates and job requirements.'
---

# MLAR: Multi-layer Large Language Model-based Robotic Process Automation Applicant Tracking

## Quick Facts
- arXiv ID: 2507.10472
- Source URL: https://arxiv.org/abs/2507.10472
- Reference count: 22
- Primary result: Multi-layer LLM system for resume-job matching, achieving 5.4s processing time per resume, outperforming UiPath and Automation Anywhere by ~17%.

## Executive Summary
This paper presents MLAR, a novel multi-layer Large Language Model-based Applicant Tracking System designed to automate and enhance the recruitment process. The system employs three distinct layers: extracting job description attributes, parsing applicant resumes for key features, and performing semantic similarity matching between candidates and job requirements. Using Gemini LLM for parsing and matching, MLAR integrates seamlessly into RPA pipelines to automate resume processing, candidate ranking, and notifications. When benchmarked against UiPath and Automation Anywhere using a dataset of 2,400 resumes, MLAR achieved an average processing time of 5.4 seconds per resume, outperforming the other platforms by approximately 16.9–17.1%. The results demonstrate MLAR’s superior efficiency, accuracy, and scalability, positioning it as a transformative solution for modern high-volume recruitment workflows.

## Method Summary
MLAR implements a three-layer LLM pipeline using Gemini: (1) extract structured job attributes (title, skills, experience, education, preferences) from job descriptions, (2) parse resumes into structured features (name, contact, skills, experience, education, department), and (3) compute semantic similarity scores (0–100) between job-resume pairs using LLM-based matching. The system ingests 24 job descriptions and 2,400 PDF resumes, stores parsed data in MongoDB, ranks candidates, and automatically sends personalized notifications to top-3 matches. RPA orchestration handles file ingestion, database operations, and email notifications while continuously monitoring for new inputs and logging all operations.

## Key Results
- MLAR achieved an average processing time of 5.4 seconds per resume
- Performance improvement of approximately 16.9–17.1% over UiPath and Automation Anywhere
- Reported accuracy of 63.45% and precision of 74.24% in candidate matching
- Top-3 candidate selection capability for each job description

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Direct Python script execution without RPA orchestration middleware reduces per-resume processing latency.
- Mechanism: MLAR runs Gemini API calls and data operations natively in Python, bypassing the environment initialization, dependency management, and inter-process communication overhead that UiPath and Automation Anywhere introduce when orchestrating external scripts.
- Core assumption: The observed speed improvement is primarily attributable to architectural simplification rather than differences in underlying compute resources.
- Evidence anchors:
  - [abstract] "MLAR achieved an average processing time of 5.4 seconds per resume, reducing processing time by approximately 16.9% compared to Automation Anywhere and 17.1% compared to UiPath."
  - [section V] "MLAR bypasses these orchestration layers by running scripts directly in Python, resulting in faster initialization and execution."
  - [corpus] Weak direct support; neighbor papers discuss RPA integration with ML but do not isolate orchestration overhead as a variable.
- Break condition: If deployment moves to a distributed cloud environment where Python scripts incur their own orchestration (e.g., container orchestration, queueing), the latency advantage may diminish.

### Mechanism 2
- Claim: LLM-based semantic matching improves alignment detection between job requirements and candidate qualifications compared to keyword-based filtering.
- Mechanism: Gemini LLM extracts structured feature sets (education, skills, experience) from unstructured PDF resumes and job descriptions, then computes a semantic similarity score (0–100) rather than relying on exact keyword overlap.
- Core assumption: The LLM's contextual understanding captures relevant qualifications even when terminology differs between resume and job description.
- Evidence anchors:
  - [abstract] "These features are then matched through advanced semantic algorithms to identify the best candidates efficiently."
  - [section III-B] "S(j, r) = L(FJ(j), FR(r)) where L computes the semantic similarity."
  - [corpus] Related work [6] in paper shows BERT-based frameworks improve resume-job alignment over keyword methods; external corpus papers on LLM recruitment (e.g., MSLEF, Augmented Fine-Tuned LLMs) similarly leverage semantic matching but do not provide independent benchmark comparisons.
- Break condition: If resumes or job descriptions use highly domain-specific jargon or abbreviations not well-represented in the LLM's training data, extraction and matching quality may degrade.

### Mechanism 3
- Claim: Pre-classifying parsed resumes into department-specific database tables accelerates downstream similarity queries.
- Mechanism: After parsing, resumes are assigned a predicted department (e.g., "Engineering") and stored in corresponding tables; this limits the search space when matching against job descriptions for that department.
- Core assumption: Department prediction is sufficiently accurate that relevant candidates are not misclassified into wrong tables.
- Evidence anchors:
  - [section IV] "If LLM predicts that a resume belongs to the 'Engineering' category, it is stored in the 'Engineering' department table within the database. This classification ensures that resumes are grouped logically, improving the speed and accuracy of subsequent processes."
  - [corpus] No direct external validation of this specific database design pattern in neighbor papers.
- Break condition: If department prediction accuracy is low, candidates may be excluded from relevant matches, requiring cross-department queries that negate the efficiency gain.

## Foundational Learning

- Concept: **LLM Prompt Engineering for Structured Extraction**
  - Why needed here: You must design prompts that reliably extract fields (name, skills, education, experience) from unstructured PDF text into consistent schemas for database storage and matching.
  - Quick check question: Can you write a prompt that returns a JSON object with keys for "skills," "education," and "experience" from a free-text resume snippet?

- Concept: **Semantic Similarity Scoring**
  - Why needed here: Understanding how LLMs produce similarity scores (0–100) helps interpret match quality and set thresholds for candidate selection.
  - Quick check question: Given two short text profiles, would you expect a semantic model to assign a higher score to "shared skills but different titles" or "shared titles but different skills," and why?

- Concept: **RPA Orchestration vs. Native Script Execution**
  - Why needed here: The performance claim hinges on understanding the overhead introduced by RPA platforms when calling external scripts versus running them natively.
  - Quick check question: Name two sources of latency when an RPA platform (like UiPath) invokes a Python script versus running the same script directly from the command line.

## Architecture Onboarding

- Component map: Input Layer -> LLM Parsing Layer -> Database Layer -> Matching Layer -> Notification Layer
- Critical path: Job description parsing → Resume parsing & classification → Similarity scoring → Ranking → Notification. Latency is dominated by LLM API calls for parsing and matching.
- Design tradeoffs:
  - Speed vs. portability: Direct Python execution is faster but loses the no-code workflow management, logging, and governance features of UiPath/Automation Anywhere.
  - Department partitioning vs. retrieval completeness: Improves query speed but introduces risk of misclassification excluding valid candidates.
  - Single-LLM dependency vs. ensemble/fine-tuned models: Simplifies architecture but limits accuracy gains possible with specialized models.
- Failure signatures:
  - API rate limiting or outages from Gemini causing pipeline stalls
  - Misclassified resumes leading to zero matches for certain job descriptions
  - Email delivery failures if candidate contact extraction is incomplete or incorrect
  - Score clustering making top-3 selection arbitrary
- First 3 experiments:
  1. Baseline latency profiling: Instrument timing for each stage on a 100-resume subset to identify the true bottleneck
  2. Department classification accuracy check: Sample 50 resumes, compare LLM-predicted departments against ground truth
  3. Score distribution analysis: Plot similarity score distributions for matched vs. random resume-JD pairs

## Open Questions the Paper Calls Out

### Open Question 1
- Question: To what extent can the prediction accuracy of the MLAR system be improved by employing fine-tuned Large Language Models specifically trained on recruitment datasets compared to the general-purpose Gemini implementation?
- Basis in paper: [explicit] The authors state in the "Future Work" section that "the primary area for improvement involves the use of different fine-tuned LLMs specifically trained on recruitment datasets" to better understand the relationships between job descriptions and resumes.
- Why unresolved: The current implementation utilizes the general Gemini LLM without domain-specific fine-tuning, which may limit the model's ability to capture nuanced recruitment-specific semantic relationships.
- What evidence would resolve it: A comparative benchmark showing accuracy and precision metrics (improving on the reported 63.45% accuracy) between the base Gemini model and a recruitment-fine-tuned model using the same dataset.

### Open Question 2
- Question: How does the semantic matching performance of MLAR compare to traditional keyword-based systems in mitigating unconscious bias regarding gender, ethnicity, and resume formatting?
- Basis in paper: [inferred] The Introduction and Related Work sections explicitly identify "unconscious biases" and "equitable candidate selection" as major challenges in recruitment that LLMs could solve. However, the experimental results focus exclusively on processing time and computational efficiency, providing no metrics on fairness or bias reduction.
- Why unresolved: While the authors claim the system fosters "ethical hiring practices," they provide no empirical evaluation or statistical analysis demonstrating that the LLM-based matching is less biased than the manual or keyword-based methods criticized in the literature review.
- What evidence would resolve it: An audit of the ranking results using the 2,400-resume dataset to measure selection rates across different demographic groups or formatting styles compared to a ground truth of unbiased human evaluations.

### Open Question 3
- Question: Does the integration of complex, multi-step features such as automated interview scheduling negatively impact the processing latency and throughput advantages MLAR holds over standard RPA platforms?
- Basis in paper: [explicit] The Conclusion suggests that "future work may focus on... incorporating features like interview scheduling and advanced applicant tracking."
- Why unresolved: The current efficiency gains (5.25s per resume) are attributed to bypassing the orchestration layers of platforms like UiPath. It is unclear if adding complex, stateful workflow steps (like calendar integration) would reintroduce orchestration overhead or latency that would diminish these performance benefits.
- What evidence would resolve it: Performance benchmarking (time per end-to-end cycle) of an augmented MLAR system that includes bidirectional email communication and calendar API integrations for scheduling.

### Open Question 4
- Question: What are the specific failure modes or "hallucination" rates of the Gemini LLM when parsing unstructured resumes, and how do they impact the reliability of the semantic similarity scores?
- Basis in paper: [inferred] The paper reports an accuracy of 63.45% and precision of 74.24%, implying a significant error rate (approx. 25-35%) in the matching process. The text attributes errors to the lack of fine-tuning but does not analyze if errors stem from parsing failures (extracting wrong features) versus matching failures (scoring wrong features).
- Why unresolved: The methodology assumes the LLM successfully extracts education, experience, and skills (Equation 1) before matching occurs. Without error analysis on the extraction layer, it is unknown if the accuracy ceiling is limited by data extraction hallucinations or the matching algorithm itself.
- What evidence would resolve it: A manual review of a sample of parsed resumes (feature sets $F_R$) against the raw PDF text to quantify the extraction error rate before the matching step is performed.

## Limitations
- Moderate accuracy (63.45%) without external validation or cross-dataset testing
- LLM performance degradation with domain-specific jargon not in training data
- No empirical evaluation of bias reduction compared to traditional methods
- Sparse environmental details for performance claims (hardware, network conditions)

## Confidence

- Processing Speed Improvement: High
- Semantic Matching Accuracy: Medium
- Architectural Advantages: Medium

## Next Checks

1. **Latency Attribution Test**: Profile and compare per-stage execution times in both MLAR and a comparable RPA-based pipeline under identical hardware and network conditions to isolate the true source of latency reduction.

2. **Department Classification Accuracy Audit**: Manually validate LLM-predicted department assignments on a stratified sample of resumes to measure misclassification rate and its impact on match recall and overall system effectiveness.

3. **Cross-Dataset Generalization Study**: Evaluate MLAR on an independent resume and job description dataset (e.g., from a different industry or geographic region) to assess robustness and identify potential domain-specific failure modes.