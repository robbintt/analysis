---
ver: rpa2
title: 'Emergent World Beliefs: Exploring Transformers in Stochastic Games'
arxiv_id: '2512.23722'
source_url: https://arxiv.org/abs/2512.23722
tags:
- layer
- poker
- probe
- state
- world
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether transformer-based language models
  can develop internal representations of stochastic environments in incomplete-information
  games, specifically poker. The authors train a GPT-2 model on synthetically generated
  poker hand histories and probe its internal activations to determine if it learns
  both deterministic game features (hand ranks) and stochastic features (equity) without
  explicit instruction.
---

# Emergent World Beliefs: Exploring Transformers in Stochastic Games

## Quick Facts
- arXiv ID: 2512.23722
- Source URL: https://arxiv.org/abs/2512.23722
- Reference count: 20
- Authors: Adam Kamel; Tanish Rastogi; Michael Ma; Kailash Ranganathan; Kevin Zhu
- One-line result: Transformer language models trained on poker data develop internal representations encoding both deterministic game features (hand ranks) and stochastic features (equity) without explicit instruction.

## Executive Summary
This paper investigates whether transformer-based language models can develop internal representations of stochastic environments in incomplete-information games, specifically poker. The authors train a GPT-2 model on synthetically generated poker hand histories and probe its internal activations to determine if it learns both deterministic game features (hand ranks) and stochastic features (equity) without explicit instruction. Using linear and MLP probes, they demonstrate that the model encodes hand-rank information with up to 98% accuracy in early layers and learns representations of equity with correlation coefficients up to 0.59. Activation visualizations reveal distinct clusters organized by hand-rank and conceptual similarity, suggesting the model develops structured internal representations consistent with belief states in partially observable Markov decision processes.

## Method Summary
The study trains a GPT-2 base model on 2+ million synthetic 6-player No-Limit Texas Hold'em hands in PHH format. The model uses masked prediction with `<GAP>` and `<ANS>` tokens, where loss is computed only on tokens following `<ANS>`. Custom PreTrainedTokenizerFast vocabulary handles poker-specific notation. The training uses AdamW optimizer with specified hyperparameters for 13 epochs with early stopping. Probes (linear and 2-layer MLP) are trained per-layer on frozen activations from a held-out test set. Linear probes use argmax(Wx) and MLP probes use argmax(W1·ReLU(W2·x)), with data balanced at the 40th percentile per class to prevent imbalance issues.

## Key Results
- Linear probes achieve ~80% accuracy and MLP probes achieve ~98% accuracy for hand-rank classification
- Equity prediction shows correlation coefficient of 0.59 between probe outputs and simulation-based equity
- Early transformer layers (0-4) encode equity information most strongly, with R² decreasing from ~0.35 to near zero by layer 10
- t-SNE visualizations show distinct clusters organized by hand-rank and conceptual similarity

## Why This Works (Mechanism)

### Mechanism 1: Emergent Belief State Formation in POMDPs
- Claim: Transformers trained on sequential game data may develop internal representations corresponding to belief states over latent variables in partially observable environments.
- Mechanism: Next-token prediction on poker hand histories requires maintaining implicit probability distributions over hidden information (opponent holdings, deck composition). The model learns compressed representations that correlate with optimal Bayesian belief states.
- Core assumption: The training objective (next-token prediction) creates sufficient pressure to encode belief-relevant features rather than memorizing surface statistics.
- Evidence anchors: [abstract] "demonstrate that the model learns both deterministic structure... and stochastic features, such as equity, without explicit instruction"; [section 4.2] "correlation coefficient of 0.59 on our test dataset predictions" between probe outputs and simulation-based equity.

### Mechanism 2: Layer-wise Information Compression
- Claim: Game-relevant features are encoded most strongly in early transformer layers and diminish in deeper layers, consistent with information bottleneck compression.
- Mechanism: Early layers retain input-variable information useful for belief-state computation; deeper layers progressively discard information irrelevant to token prediction, focusing on output-relevant features.
- Core assumption: The observed R² decay across layers reflects genuine representational compression rather than probe training instability.
- Evidence anchors: [section 4.2] "understanding of equity is most strongly encoded in the early layers, becoming diluted after layers 0-4"; [section 4.2] Figure 2b shows R² decreasing from ~0.35 at layer 0 to near zero by layer 10.

### Mechanism 3: Nonlinear Structure in Learned Representations
- Claim: Internal representations of game states exhibit nonlinear structure requiring MLP probes for accurate extraction, beyond what linear probes can capture.
- Mechanism: Superposition or entangled encoding of multiple features may create representations where target variables are linearly accessible only after nonlinear transformation.
- Core assumption: The performance gap between linear and MLP probes reflects representational structure rather than probe overfitting.
- Evidence anchors: [section 4.1] "linear probe achieves ~80% accuracy... while the MLP reaches ~98% accuracy"; [section 4] "MLP probe achieved higher accuracy than the linear probe, consistent with the findings of Li et al. [2024]."

## Foundational Learning

- Concept: **Partially Observable Markov Decision Processes (POMDPs)**
  - Why needed here: Poker is framed as a canonical POMDP where the agent must maintain beliefs over hidden states (opponent cards) rather than observing true states directly.
  - Quick check question: Can you explain why poker requires maintaining a distribution over hidden states rather than a single state estimate?

- Concept: **Linear and Nonlinear Probing**
  - Why needed here: The methodology relies on training auxiliary classifiers on frozen activations to determine what information is encoded where.
  - Quick check question: What does it mean if a linear probe achieves 80% accuracy but an MLP probe achieves 98% on the same task?

- Concept: **Equity in Poker**
  - Why needed here: Equity (expected share of the pot given current hand and board) serves as the ground-truth stochastic feature the model is hypothesized to encode.
  - Quick check question: Why is equity a stochastic rather than deterministic feature in poker?

## Architecture Onboarding

- Component map: Synthetic poker data generator -> GPT-2 base model -> Activation extraction -> Probe training -> Visualization
- Critical path: 1) Generate synthetic poker games using equity-simulation-driven agents with randomized playing styles; 2) Pretrain GPT-2 on 2M+ hand histories with next-token prediction; 3) Extract activations at each layer for held-out test hands; 4) Train probes (linear/MLP) to predict hand-rank (deterministic) or equity (stochastic) from activations
- Design tradeoffs: Synthetic data enables scale (2M+ hands) but may not capture realistic opponent modeling or adaptation; GPT-2 base chosen for compute efficiency; unclear if findings scale to larger models; dataset balancing prevents majority-class dominance but reduces rare-hand samples
- Failure signatures: Probes achieving high accuracy only on frequent hand types suggests memorization, not structured representation; no layer-wise variation in probe performance suggests activations may not be task-differentiated; high variance across random seeds indicates unstable feature learning
- First 3 experiments:
  1. Reproduce linear vs. MLP probe gap: Train both probe types on layer 0 activations for hand-rank classification; verify ~80% vs. ~98% accuracy differential using the 40th percentile balanced dataset.
  2. Layer-wise equity probing: Train MLP probes at each layer to predict equity; plot R² across layers to confirm early-layer peak and subsequent decay pattern.
  3. Activation visualization sanity check: Apply t-SNE to layer 0-3 activations colored by hand-rank; verify distinct clustering by rank and conceptual similarity (pairs clustering together).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do the emergent world representations observed in poker transfer to novel stochastic domains or more complex latent variables?
- Basis in paper: [explicit] Section 5 states there are "no guarantees that current results... are able to be extended to analysis of new stochastic poker variables, or to novel domains."
- Why unresolved: The current study is limited to Texas Hold'em and specific features (hand rank, equity), leaving the generalizability of the POMDP representation mechanism untested.
- What evidence would resolve it: Replicating the probing methodology on different incomplete-information games (e.g., Hanabi) or successfully decoding higher-order poker concepts like "implied odds."

### Open Question 2
- Question: Does the triangular geometry found in activation plots causally correspond to a POMDP belief simplex?
- Basis in paper: [explicit] Appendix K notes the geometry resembles belief-state manifolds and asks if "vertices of the triangle may correspond to pure beliefs."
- Why unresolved: While PCA visualizations suggest a geometric structure, the paper does not verify if specific coordinate regions mathematically map to theoretical belief mixtures.
- What evidence would resolve it: Causal interventions that perturb activations along the identified geometric dimensions, resulting in predictable shifts in the model's predicted equity distributions.

### Open Question 3
- Question: Can Sparse Autoencoders (SAEs) disentangle the superposed features to reveal monosemantic game state variables?
- Basis in paper: [explicit] Section 6 states the intent to "better understand the structure of LLM predictive world representations through SAEs."
- Why unresolved: Current probing relies on MLPs which may capture entangled features; the individual "concept" directions (e.g., specific draws) remain unidentified.
- What evidence would resolve it: Extraction of interpretable, monosemantic features from the residual stream that activate exclusively for specific game concepts like "flush draw" or "pot odds."

## Limitations

- Synthetic data generation may not capture realistic opponent modeling dynamics found in human-played games
- Probing methodology cannot distinguish between genuine world-model learning versus sophisticated pattern matching
- Findings are limited to specific poker features (hand rank, equity) and may not generalize to other stochastic domains

## Confidence

- **High confidence**: The differential performance between linear (~80%) and MLP (~98%) probes for hand-rank classification is robust and well-supported by the results.
- **Medium confidence**: The equity correlation coefficient of 0.59 represents meaningful but imperfect encoding, with early-layer concentration plausible but specific values requiring careful interpretation.
- **Low confidence**: Claims about the model developing "belief states" consistent with optimal Bayesian reasoning extend beyond what the probing methodology can establish.

## Next Checks

1. **Cross-distribution probe transfer**: Train probes on activations from synthetic data but evaluate on a small set of real poker hand histories (where ground-truth equity can be estimated). Successful transfer would validate that learned representations generalize beyond the synthetic distribution.

2. **Layer-wise ablation study**: Systematically disable layers in the frozen model and retrain probes to identify which combinations of layers are actually necessary for the observed performance. This would test whether early-layer encoding is truly essential or if deeper layers also contribute meaningfully.

3. **Alternative synthetic data**: Generate a second synthetic dataset using different agent policies (e.g., rule-based vs. equity-driven, or with different levels of randomness) and retrain the model from scratch. Compare probe performance across both datasets to determine if the emergent representations are robust to changes in the training distribution.