---
ver: rpa2
title: 'WildFrame: Comparing Framing in Humans and LLMs on Naturally Occurring Texts'
arxiv_id: '2502.17091'
source_url: https://arxiv.org/abs/2502.17091
tags:
- sentiment
- framing
- human
- positive
- negative
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces WildFrame, a dataset designed to evaluate
  how Large Language Models (LLMs) respond to positive and negative framing in naturally
  occurring texts, comparing their behavior to humans. The dataset contains 1,000
  real-world statements from Amazon reviews, each reframed in both positive and negative
  lights, with human sentiment annotations.
---

# WildFrame: Comparing Framing in Humans and LLMs on Naturally Occurring Texts

## Quick Facts
- arXiv ID: 2502.17091
- Source URL: https://arxiv.org/abs/2502.17091
- Reference count: 21
- This paper introduces WildFrame, a dataset designed to evaluate how Large Language Models (LLMs) respond to positive and negative framing in naturally occurring texts, comparing their behavior to humans.

## Executive Summary
This paper introduces WildFrame, a dataset designed to evaluate how Large Language Models (LLMs) respond to positive and negative framing in naturally occurring texts, comparing their behavior to humans. The dataset contains 1,000 real-world statements from Amazon reviews, each reframed in both positive and negative lights, with human sentiment annotations. Eight state-of-the-art LLMs were evaluated on WildFrame, revealing that all models exhibit framing effects similar to humans (r≥0.57). Both humans and models were found to be more influenced by positive reframing than negative reframing, consistent with prior cognitive science findings. The study highlights that GPT models show the lowest correlation with human behavior among tested models, raising questions about LLM development goals.

## Method Summary
The authors created WildFrame by extracting 1,000 Amazon review statements (500 positive, 500 negative base statements), each reframed with opposite sentiment framing using GPT-4. Five human annotators per statement were recruited via MTurk to classify sentiment of reframed statements. Eleven LLMs were then prompted to classify sentiment on identical reframed statements, and correlation analysis was performed between human sentiment shift scores and model predictions. The sentiment shift metric measures the proportion of annotators/models labeling sentiment aligned with reframing rather than base statement.

## Key Results
- All models exhibit framing effects similar to humans with correlation scores r≥0.57
- Both humans and models are more influenced by positive reframing than negative reframing
- GPT models show the lowest correlation with human behavior among tested models

## Why This Works (Mechanism)

### Mechanism 1: Sentiment Shift Under Evaluative Framing
- Claim: When base statements with clear sentiment are reframed by appending/prepending opposite-sentiment clauses, both humans and LLMs shift their sentiment judgments in correlated ways.
- Mechanism: Framing introduces additional evaluative information that creates ambiguity in the original sentiment signal. Both human annotators and LLMs integrate this new clause, producing a "sentiment shift" score (0-5 annotators agreeing with the reframed sentiment) that correlates across populations.
- Core assumption: Sentiment shift patterns reflect genuine cognitive processing rather than surface-level pattern matching.
- Evidence anchors:
  - [abstract] "all models exhibit framing effects similar to humans (r≥0.57)"
  - [section 3.2] "LLMs are influenced by framing in a similar manner to human behavior, with all models exhibiting a strong correlation with human annotations (r≥0.52)"
  - [corpus] "When Wording Steers the Evaluation: Framing Bias in LLM judges" confirms LLMs produce varying responses depending on prompt phrasing, supporting sensitivity to framing.
- Break condition: If LLMs were simply performing token-level sentiment classification without integrating the reframing clause, we would see near-zero correlation with human sentiment shifts.

### Mechanism 2: Positive Framing Asymmetry
- Claim: Positive reframing applied to negative base statements produces more sentiment shifts than negative reframing applied to positive base statements.
- Mechanism: First-person perspective statements create "zero spatial distance" (per Tong et al., 2021), amplifying positive framing impact. Both humans and models weight positive evaluative information more heavily when integrating conflicting sentiment signals.
- Core assumption: The asymmetry reflects shared cognitive/processing mechanisms rather than dataset artifacts.
- Evidence anchors:
  - [abstract] "both humans and models were found to be more influenced by positive reframing than negative reframing, consistent with prior cognitive science findings"
  - [section 3.2] "all models except GPT-4o exhibit higher ratios of sentiment shifts for statements that were originally negative (red bars) compared to those that were originally positive (green bars)"
  - [corpus] Weak direct evidence for this specific asymmetry in corpus neighbors; primarily supported by in-paper citations to cognitive science literature.
- Break condition: If this were a dataset artifact specific to Amazon reviews, the asymmetry would not replicate across domains.

### Mechanism 3: Model Scale-Framing Sensitivity Relationship
- Claim: Larger models within the same family tend to exhibit higher correlation with human framing behavior, but this relationship does not hold across all architectures.
- Mechanism: Assumption: Increased parameter count may enable more nuanced integration of conflicting sentiment signals, better approximating human judgment patterns—but training procedures and alignment methods (e.g., RLHF) may counteract this effect.
- Evidence anchors:
  - [section 3.2] "for the Llama and Mistral model families, we observe a trend where larger models with more parameters exhibit higher correlation with human behavior... However, the Gemma family shows the opposite trend"
  - [section 3.2] "GPT-3.5 shows the lowest correlation with humans (r=0.52), followed by GPT-4o (r=0.57), while the most recent GPT-5-mini achieves the highest correlation (r=0.61)"
  - [corpus] No direct corpus evidence on scale-framing relationships.
- Break condition: If correlation were purely scale-driven, all larger models would outperform smaller ones regardless of family.

## Foundational Learning

- Concept: **Evaluative Framing (vs. Attribute Framing)**
  - Why needed here: The paper uses a broad definition where framing involves adding information that changes perspective, not just rewording. This differs from classic Tversky/Kahneman attribute framing.
  - Quick check question: Can you distinguish between "attribute framing" (changing how a fact is described) and "evaluative framing" (adding new contextual information)?

- Concept: **Sentiment Shift Scoring**
  - Why needed here: The paper's core metric measures how many annotators (0-5) shift their sentiment judgment to align with the reframing clause, not binary accuracy.
  - Quick check question: If 3 of 5 annotators label a reframed positive statement as "negative," what is the sentiment shift score?

- Concept: **Crowdsourced Annotation Validation**
  - Why needed here: The paper addresses concerns about LLM-contaminated crowdworker responses using three validation strategies—essential methodology for human-LLM comparison work.
  - Quick check question: Name two methods the paper uses to verify that crowdworker annotations reflect genuine human behavior.

## Architecture Onboarding

- Component map:
  - SPIKE extraction -> sentiment filtering -> GPT-4 reframing generation -> human annotation collection -> model evaluation

- Critical path:
  1. Statement extraction → sentiment filtering → reframing generation
  2. Human annotation collection with authenticity validation
  3. Model inference on identical reframed statements
  4. Correlation analysis between human sentiment shift scores and model predictions

- Design tradeoffs:
  - **Amazon Reviews vs. News**: Reviews chosen to avoid political bias confounds but limit domain generalizability
  - **5 annotations vs. more**: 5 annotators balances robustness with cost; sufficient for detecting ambiguity (2-3 scores)
  - **Soft scoring vs. binary**: Using continuous sentiment shift (0-5) enables fine-grained human-model correlation rather than accuracy alone

- Failure signatures:
  - **Model-floor effect**: If all models show near-identical framing sensitivity, the dataset cannot differentiate architectures
  - **Annotation contamination**: If crowdworkers use LLMs, human-model correlations become circular—validate via variance analysis and in-house baseline
  - **Reframing quality**: If reframing doesn't create genuine ambiguity, sentiment shift scores cluster at 0 or 5

- First 3 experiments:
  1. **Baseline correlation**: Run all 11 models on WildFrame, compute Pearson correlation with human sentiment shift scores per model family
  2. **Positive/negative asymmetry test**: Stratify analysis by base sentiment direction; verify positive reframing produces higher shift ratios
  3. **Robustness check**: Evaluate models on all 3 reframing variants; report instance-level prediction consistency (paper finds ~70% label stability)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the framing effect manifest in LLMs during decision-making tasks compared to the sentiment analysis tasks evaluated in this study?
- Basis in paper: [explicit] The authors state in the Limitations section that they "focus solely on sentiment analysis" and that "Other downstream tasks influenced by framing, such as question answering or decision-making, may exhibit different patterns of sensitivity."
- Why unresolved: The current study only measures shifts in sentiment labels, not the resulting decisions or actions the models might take in a real-world workflow.
- What evidence would resolve it: Applying the WildFrame evaluation framework to tasks requiring definitive decisions (e.g., hiring recommendations, medical triage) rather than affective classification.

### Open Question 2
- Question: Do the observed framing sensitivities generalize to high-stakes domains with different linguistic structures, such as political news or medical advice?
- Basis in paper: [explicit] The Limitations section notes the experiment was "conducted within a single domain – Amazon reviews," warning that findings may be "artifacts of this dataset rather than generalizable patterns" and that other domains introduce confounding factors.
- Why unresolved: Amazon reviews possess a specific first-person narrative style and sentiment distribution that may not map to the objective or persuasive language found in legal or political texts.
- What evidence would resolve it: Replicating the reframing and annotation methodology on datasets from domains like news media or clinical notes and comparing the correlation scores.

### Open Question 3
- Question: Why do different model families exhibit opposing relationships between model size and alignment with human framing behavior?
- Basis in paper: [explicit] The Analysis section observes that while larger Llama and Mistral models show higher correlation with humans, "the Gemma family shows the opposite trend," concluding that "This discrepancy highlights the need for further investigation into the relationship between model size, the framing effect, and its alignment."
- Why unresolved: The paper reports the correlation trends but lacks ablation studies to determine if the cause is architectural, data-dependent, or related to specific optimization objectives.
- What evidence would resolve it: Controlled experiments isolating model width, depth, and training data composition within these families to identify the driver of the divergence.

### Open Question 4
- Question: What specific alignment techniques cause GPT models to exhibit lower correlation with human framing behavior compared to other state-of-the-art models?
- Basis in paper: [explicit] The Results section highlights that "GPT models exhibit relatively low correlation with human behavior among all other tested models" and "raises questions about how architectural design choices and training procedures may influence a model’s susceptibility to framing effects."
- Why unresolved: The proprietary nature of GPT models prevents the authors from determining if RLHF or other alignment procedures specifically decouple the model from human cognitive biases.
- What evidence would resolve it: Evaluating open-source base models versus their instruction-tuned/aligned counterparts to see if alignment procedures systematically reduce correlation with human framing.

## Limitations

- The study is limited to Amazon reviews, which may not generalize to other domains with different linguistic structures and framing contexts.
- Human annotation contamination from LLM exposure cannot be completely ruled out despite validation attempts.
- The positive-negative framing asymmetry finding relies heavily on first-person perspective statements, which may not represent broader framing phenomena.

## Confidence

**High confidence** in the core finding that all tested models show framing effects correlated with human behavior (r≥0.57). This is supported by direct empirical evidence across multiple model families and validated through human-LLM correlation analysis.

**Medium confidence** in the positive-negative framing asymmetry claim. While the paper cites cognitive science literature supporting this asymmetry and observes it in their data, the corpus evidence is weak and the finding relies on first-person perspective statements, which may not generalize.

**Low confidence** in the model scale-framing sensitivity relationship. The evidence shows contradictory trends across different model families (Llama/Mistral show positive correlation with scale, Gemma shows negative), suggesting this relationship is not robust or universal.

## Next Checks

1. **Cross-domain validation**: Test WildFrame's framing sensitivity metrics on news articles or social media posts to verify that positive-negative asymmetry generalizes beyond Amazon reviews.

2. **Prompt sensitivity analysis**: Systematically vary the sentiment classification prompts across different phrasings and output formats to quantify how much correlation variance stems from prompt design rather than genuine model behavior.

3. **Temporal contamination test**: Re-run the human annotation collection with fresh crowdworkers who have not been exposed to LLMs, comparing sentiment shift distributions to the original dataset to quantify potential LLM contamination effects.