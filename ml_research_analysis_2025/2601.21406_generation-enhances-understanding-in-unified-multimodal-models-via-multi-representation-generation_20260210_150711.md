---
ver: rpa2
title: Generation Enhances Understanding in Unified Multimodal Models via Multi-Representation
  Generation
arxiv_id: '2601.21406'
source_url: https://arxiv.org/abs/2601.21406
tags:
- generation
- understanding
- depth
- umms
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces UniMRG, a post-training method that enhances\
  \ understanding capabilities in unified multimodal models (UMMs) by incorporating\
  \ auxiliary generation tasks. The approach trains UMMs to generate multiple intrinsic\
  \ image representations\u2014pixel reconstruction, depth maps (geometry), and segmentation\
  \ maps (structure)\u2014alongside standard visual understanding objectives."
---

# Generation Enhances Understanding in Unified Multimodal Models via Multi-Representation Generation

## Quick Facts
- arXiv ID: 2601.21406
- Source URL: https://arxiv.org/abs/2601.21406
- Reference count: 28
- Primary result: Post-training method that improves visual understanding in unified multimodal models by adding auxiliary generation tasks for depth maps and segmentation maps

## Executive Summary
UniMRG is a post-training approach that enhances visual understanding in unified multimodal models (UMMs) by incorporating auxiliary generation tasks for intrinsic image representations. The method trains UMMs to generate pixel reconstruction, depth maps, and segmentation maps alongside standard visual understanding objectives. By synthesizing these diverse representations, UMMs capture complementary information regarding appearance, spatial relations, and structural layout. The approach demonstrates consistent improvements across different UMM architectures while requiring minimal additional training time.

## Method Summary
UniMRG implements a multi-task training framework where UMMs simultaneously learn visual understanding and generation of intrinsic representations. The method freezes the visual encoder and applies four tasks per batch: understanding (cross-entropy loss), pixel reconstruction (cross-entropy or diffusion loss), depth generation (MSE loss), and segmentation generation (MSE loss). All losses are equally weighted (λ=1). The approach works with AR, AR+MAR, and AR+Diffusion architectures by adapting the generation loss accordingly. Depth maps are generated using Depth Anything V2, while segmentation maps use SAM with specified thresholds. The method requires 3-8 hours of training depending on model size.

## Key Results
- OpenUni-3.6B: MMVP improves from 71.67 to 74.67 (+3.00), HallusionBench from 60.88 to 64.56 (+3.68), VSR from 66.69 to 73.90 (+7.21)
- GenEval improves from 61.13 to 68.00, DPGBench from 79.41 to 81.78
- Architecture-agnostic: works on AR (Show-o), AR+MAR (Harmon), and AR+Diffusion (OpenUni) models
- Prevents generation capability collapse seen with understanding-only training

## Why This Works (Mechanism)

### Mechanism 1
Generating intrinsic visual representations (depth, segmentation) forces UMMs to internalize geometric and structural regularities that transfer to understanding tasks. The model learns to disentangle appearance from geometry/structure by producing depth maps (explicit spatial relations) and segmentation maps (object boundaries), providing dense supervision on factors weakly constrained by RGB reconstruction.

### Mechanism 2
Depth generation training specifically enhances spatial understanding by exposing relative distance ordering. The image-to-depth objective compels the model to learn scene geometry and relative distance, making distance comparisons in VQA more reliable through explicit depth encoding.

### Mechanism 3
Segmentation generation training reduces hallucinations by enforcing object-level grounding. The image-to-segmentation objective provides an object-centric structural prior that helps disentangle entities and reduces spurious attribute binding—a common hallucination source.

## Foundational Learning

- **VQ-VAE Tokenization**: AR-based UMMs encode images as discrete tokens; the codebook size directly limits multi-representation capacity. Quick check: Can you explain why a 4,096-token codebook might struggle to jointly represent pixel, depth, and segmentation information?

- **Diffusion Training Objectives (DDPM/Flow Matching)**: AR+Diffusion models use diffusion loss for generation; understanding when to apply cross-entropy vs. diffusion loss is critical. Quick check: Given equation (13) L_DDPM and equation (17) L_FM, which would you use for a continuous latent space and why?

- **Dense Prediction Tasks**: Depth estimation and segmentation are per-pixel prediction tasks; their dense supervision contrasts with sparse understanding losses. Quick check: How does dense per-pixel supervision differ from cross-entropy loss on discrete answer tokens in terms of gradient signal?

## Architecture Onboarding

- **Component map**: Visual Encoder -> Generation Decoder -> VQ-VAE -> Discrete Tokens; External Teacher Models -> Depth/Segmentation Targets
- **Critical path**: 1) Preprocess training images → generate depth maps and segmentation maps 2) For each batch, sample one of four tasks 3) Compute task-specific loss; all losses weighted equally 4) Update trainable components
- **Design tradeoffs**: Codebook size vs. multi-representation capacity; Training time vs. comprehensiveness; Pixel generation necessity for image quality
- **Failure signatures**: Understanding-only SFT causes generation collapse; Missing pixel generation leads to dark/textured-poor images; Insufficient codebook capacity causes depth/segmentation generation failure
- **First 3 experiments**: 1) Baseline reproduction: Train Harmon with SFT-only to confirm generation collapse 2) Single-representation ablation: Add only depth generation and measure VSR change 3) OOD depth generation test: Sample 100 images from MidjourneyV6, generate depth maps, compute 1-MAE vs. Depth Anything V2 targets

## Open Questions the Paper Calls Out
- Can UniMRG be adapted to video domains to enforce temporal consistency in intrinsic representations without prohibitive computational costs?
- Does inclusion of additional intrinsic representations (e.g., human pose or edge sketches) provide complementary gains in specific understanding domains?
- Is Show-o's performance limitation strictly due to VQ codebook size, or does the masked prediction strategy struggle with disparate representation densities?
- To what extent does UniMRG inherit systematic errors or biases of the external teacher models used for ground truth generation?

## Limitations
- Architecture-dependent effectiveness: Show-o's minimal gains highlight fundamental limitations with small codebooks
- Task sampling ambiguity: Exact ratios across four tasks are unspecified, affecting reproducibility
- Generalization concerns: Reliance on pretrained teacher models for depth/segmentation targets may introduce domain shift vulnerabilities

## Confidence
**High Confidence**: UniMRG consistently improves understanding metrics across architectures when capacity exists; multi-representation generation prevents understanding-only collapse; architecture-agnostic formulation works on AR, AR+MAR, and AR+Diffusion models

**Medium Confidence**: Depth generation enhances spatial understanding through distance learning; segmentation generation reduces hallucinations via object grounding; 3-8 hour training provides optimal tradeoff

**Low Confidence**: Show-o limitations are solely codebook-constrained; exact contribution of each representation type; long-term generalization benefits

## Next Checks
1. Test UniMRG on UMM with increased codebook size (8,192+ tokens) to isolate capacity constraints and measure generation quality vs. Harmon/OpenUni baselines

2. Perform systematic ablation studies adding each representation type individually to quantify marginal improvements and identify critical components for specific understanding tasks

3. Create comprehensive OOD test suite spanning diverse image distributions to measure understanding/generation performance degradation and analyze depth/segmentation generation robustness across distributions