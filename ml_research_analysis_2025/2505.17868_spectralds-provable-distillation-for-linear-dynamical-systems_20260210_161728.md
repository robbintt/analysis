---
ver: rpa2
title: 'SpectraLDS: Provable Distillation for Linear Dynamical Systems'
arxiv_id: '2505.17868'
source_url: https://arxiv.org/abs/2505.17868
tags:
- spectral
- filters
- state
- each
- dimension
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents the first provable method for distilling symmetric
  linear dynamical systems (LDS) from learned spectral filters, achieving constant-time
  inference independent of sequence length. The authors build on the Spectral Transform
  Unit (STU) framework, which uses fixed spectral filters to model LDS dynamics without
  explicitly learning the transition matrix.
---

# SpectraLDS: Provable Distillation for Linear Dynamical Systems

## Quick Facts
- arXiv ID: 2505.17868
- Source URL: https://arxiv.org/abs/2505.17868
- Reference count: 40
- Distills spectral filters into explicit LDS with constant-time inference

## Executive Summary
This paper presents the first provable method for distilling symmetric linear dynamical systems (LDS) from learned spectral filters, achieving constant-time inference independent of sequence length. The authors build on the Spectral Transform Unit (STU) framework, which uses fixed spectral filters to model LDS dynamics without explicitly learning the transition matrix. They introduce an algorithm that converts learned STU filters into explicit LDS parameters, enabling recurrent inference at O(1) time per token rather than O(L) for convolution operations.

## Method Summary
The method involves finding spectral representations of LDS impulse responses and constructing a transformation matrix that maps spectral filters to LDS dynamics. Theoretical analysis shows that with h ≥ k filters, the reconstructed LDS approximates the original spectral filters with error bounded by cλmax e^(-k/log L), where λmax is the largest eigenvalue of the spectral coefficients matrix. Experiments demonstrate that the distilled LDS achieves identical performance to the original STU on language modeling tasks while providing significant inference speed improvements.

## Key Results
- Distilled LDS achieves identical performance to original STU on language modeling benchmarks
- Inference speed improves by 2-4× for sequences longer than 65k tokens
- Error bounds show exponential decay with number of filters: cλmax e^(-k/log L)
- Reconstruction error remains below 10^-10 across tested configurations

## Why This Works (Mechanism)

### Mechanism 1: Spectral Representation of LDS Dynamics
The impulse response of a symmetric linear dynamical system (LDS) can be accurately represented as a linear combination of spectral filters derived from a fixed Hankel matrix basis. The paper leverages spectral filtering theory, showing that for a symmetric LDS with transition matrix A (||A|| ≤ 1), the impulse response µ(α) = (1, α, α², ...) can be approximated using the top k eigenvectors {ϕ₁, ..., ϕₖ} of the Hankel matrix Z = ∫ µ(α)µ(α)ᵀ dα. The approximation error decays exponentially as ~e^(-k/log L).

### Mechanism 2: Invertible Filter-to-System Mapping
A matrix transformation M can be constructed to map between spectral filter coefficients and explicit LDS parameters, enabling distillation with provable error bounds. Algorithm 2 samples h random LDS impulse responses (geometrically decaying filters), finds their spectral representations via convex optimization (Algorithm 1), and constructs a pseudo-inverse transformation matrix fM = M⁻¹. Theorem 1 guarantees that ||Φ₁:ₖ - fMµ_L(α₁,...,αₕ)|| ≤ c·λmax·h·e^(-k/log L).

### Mechanism 3: Constant-Time Recurrent Inference via Distilled LDS
The distilled explicit LDS parameters enable O(1) per-token inference independent of sequence length, replacing O(L log³L) FFT-based convolution. Once fM is computed offline, the STU convolutions U⁺_{t,j} = Σᵢ ϕⱼ(i)·u_{t-i+1} are replaced by recurrent updates: x_t = Ax_{t-1} + Bu_t, y_t = fM·Γ·x_t. This reduces inference from O(L log³L) to O(h) per token.

## Foundational Learning

- **Concept: Linear Dynamical Systems (LDS) and Impulse Response**
  - Why needed here: Understanding LDS formulation (x_t = Ax_{t-1} + Bu_t, y_t = Cx_t) and its convolutional expansion (y_t = Σ CAⁱB u_{t-i}) is essential to grasp how SpectraLDS replaces convolution with recurrence.
  - Quick check question: Can you derive the impulse response ψ[t] = CA^{t-1}B from the LDS equations and explain why symmetric A enables spectral decomposition?

- **Concept: Spectral Filtering and Hankel Matrix Eigenbasis**
  - Why needed here: The spectral filters {ϕⱼ} are eigenvectors of the Hankel matrix Z, and understanding their exponential approximation properties is critical to the distillation error bounds.
  - Quick check question: Why does the error ||µ(α) - Σ⟨µ(α), ϕⱼ⟩ϕⱼ|| decay as e^{-k/log L}, and what does this imply for the number of filters needed to approximate long-memory LDS?

- **Concept: Convex Optimization for Spectral Coefficients**
  - Why needed here: Algorithm 1 (FindSpectralRepresentation) uses convex optimization to find coefficients m that minimize ||m^T Φ₁:ₖ u - µ_L(α)^T u||, and this convexity ensures stable convergence.
  - Quick check question: Why is the optimization problem in Algorithm 1 convex, and how does this convexity contrast with non-convex direct learning of the transition matrix A?

## Architecture Onboarding

- **Component map:** STU Layer -> Distillation Module -> LDS Layer -> Inference Pipeline
- **Critical path:**
  1. Load pre-trained FlashSTU model (e.g., 340M parameters, Table 7)
  2. Extract spectral filters Φ₁:ₖ from STU layers
  3. Run Algorithm 3 with N = ~60k sampled 1D-LDS impulse responses (Appendix A.10)
  4. Greedily select subset I_sub of size h_start, expand to h = 80–160
  5. Gradient-refine fM to minimize ||Φ₁:ₖ - fM Ψ_sub||²_F
  6. Construct LDS parameters: A = diag(α₁,...,αₕ), B = (b₁,...,bₕ)^T, C = fM diag(c₁,...,cₕ)
  7. Deploy LDS layers with float64 precision, keeping other layers in bfloat16

- **Design tradeoffs:**
  - State dimension h: Larger h improves approximation (lower λmax·h) but increases per-token compute O(h). Empirically, h = 80–160 suffices for language tasks.
  - Precision: LDS layers require float64 to avoid numerical instability; other layers use bfloat16, creating memory/compute overhead.
  - Overparameterization: h > k (e.g., h = 80, k = 24) ensures well-conditioned M⁻¹, as shown in Appendix A.2.

- **Failure signatures:**
  - High reconstruction error: If ||Φ₁:ₖ - fM Ψ_sub||²_F > 10⁻¹⁰ (Figure 9), the LDS may not preserve STU accuracy. Check h sufficiency and gradient refinement convergence.
  - Inference instability: If generated tokens diverge, verify eigenvalues |αᵢ| ≤ 1 and consider increasing float64 precision or regularizing A.
  - OOM at long sequences: For L > 262k tokens, ensure filter length matches sequence length (Table 7) and cache size scales as O(h), not O(L).

- **First 3 experiments:**
  1. Synthetic LDS Reconstruction (Table 3): Train STU on random symmetric LDS (d_h = 1000, δ = 10⁻⁵), distill to LDS with h = 100, and measure MSE vs. baseline gradient descent. Target: MSE < 10⁻⁵.
  2. Filter Fitting Visualization (Figure 2): Plot spectral filters Φ₁:ₖ and their LDS approximations for h = 80. Target: visual alignment and reconstruction error < 10⁻¹².
  3. Inference Speed Benchmark (Figure 3, Table 4): Compare runtime of distilled LDS vs. naive convolution and FutureFill for L = 4k–1M tokens. Target: 2–4× speedup at L > 65k.

## Open Questions the Paper Calls Out

- **Open Question 1:** Can the distilled LDS layer maintain numerical stability and predictive accuracy using standard lower-precision formats (e.g., bfloat16) instead of float64? The current implementation requires float64 precision for LDS layers to function (Appendix A.1), which conflicts with the standard precision (bfloat16) used by the rest of the model and hardware accelerators.

- **Open Question 2:** Can the provable distillation method be extended to non-symmetric linear dynamical systems or systems with complex eigenvalues? The paper restricts the theoretical formulation to systems where the transition matrix $A$ is a "symmetric real matrix" (Section 4.1).

- **Open Question 3:** Can a rigorous theoretical upper bound be derived for the largest eigenvalue ($\lambda_{max}$) of the spectral coefficients matrix? Section 5.1 notes that "it is not immediate to upper bound $\lambda_{max}$" and that it can be exponentially large, relying only on empirical observations that it decreases with over-parameterization.

- **Open Question 4:** How do convolution, LDS, and attention layers interact at the hardware level to affect inference speed in hybrid architectures? The authors state that "investigation is warranted to better understand how convolution, LDS, and attention layers interact at the hardware level, and to optimize their coordination."

## Limitations

- Restriction to symmetric LDSs limits applicability to non-symmetric dynamical systems common in many domains
- Exponential decay error bound becomes loose when h is close to k due to Vandermonde conditioning, requiring significant overparameterization
- Method requires high-precision computation (float64) for LDS layers, creating memory and computational overhead compared to standard bfloat16 operations

## Confidence

- **High Confidence:** The mechanism of spectral representation of symmetric LDS impulse responses has strong theoretical grounding in Hankel matrix eigenbasis approximation, supported by explicit error bounds and validated through synthetic LDS reconstruction experiments.
- **Medium Confidence:** The invertible filter-to-system mapping is theoretically sound, but practical concerns about numerical conditioning when h≈k and the choice of sampling distributions for LDS parameters introduce uncertainty in real-world performance.
- **Medium Confidence:** The constant-time inference claim is well-supported by complexity analysis and benchmarks, though the float64 precision requirement and potential numerical instability during long-sequence inference reduce practical confidence.

## Next Checks

1. **Numerical Stability Analysis:** Systematically evaluate LDS inference stability across different float precision settings (bfloat16, float32, float64) and sequence lengths to quantify the precision trade-off and identify stability thresholds.

2. **Generalization to Non-Symmetric Systems:** Test the spectral approximation framework on non-symmetric LDSs by applying the method to diagonalizable matrices with complex eigenvalues, measuring approximation error degradation and identifying failure modes.

3. **Scaling Laws for h vs. k:** Conduct ablation studies varying the overparameterization ratio h/k across different memory regimes (short, medium, long sequences) to establish optimal h values that balance reconstruction accuracy against computational overhead.