---
ver: rpa2
title: 'MADTempo: An Interactive System for Multi-Event Temporal Video Retrieval with
  Query Augmentation'
arxiv_id: '2512.12929'
source_url: https://arxiv.org/abs/2512.12929
tags:
- retrieval
- video
- search
- temporal
- system
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MADTempo is a video retrieval system that enables temporal reasoning
  across multiple events in long videos. It combines a CLIP-based visual embedding
  pipeline with LLM-powered query decomposition to handle complex event queries, while
  also integrating a Google Image Search fallback for out-of-distribution visual concepts.
---

# MADTempo: An Interactive System for Multi-Event Temporal Video Retrieval with Query Augmentation

## Quick Facts
- arXiv ID: 2512.12929
- Source URL: https://arxiv.org/abs/2512.12929
- Reference count: 40
- Primary result: Achieved "very good" overall ranking in Ho Chi Minh City AI Challenge 2025 with excellent performance in multi-event retrieval tasks

## Executive Summary
MADTempo is a video retrieval system designed to handle complex multi-event queries across long video sequences. It combines CLIP-based visual embedding with LLM-powered query decomposition and a novel TRAKE algorithm that enforces temporal ordering and contextual coherence. The system also integrates Google Image Search as a fallback for out-of-distribution visual concepts, bridging gaps in pretrained vision-language models. Evaluated in a competitive AI challenge, MADTempo demonstrated strong performance in both temporal reasoning and semantic robustness.

## Method Summary
MADTempo processes raw videos through shot detection (TransNetV2), CLIP-Laion encoding, and metadata extraction (YOLOv8, OCR, ASR, captions) into Milvus and MongoDB stores. Queries undergo LLM decomposition into context and ordered events, followed by TRAKE's boundary matching, context scoring, and beam search. Final scores combine visual similarity (EventScore) and semantic coherence (ContextScore). For OOD concepts, Google Image Search provides user-selected images that supplement text queries via CLIP encoding.

## Key Results
- Achieved "very good" overall ranking in Ho Chi Minh City AI Challenge 2025
- Demonstrated excellent performance on multi-event retrieval tasks
- Showed strong visual-to-keyframe search capabilities

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Multi-event temporal retrieval can be decomposed into boundary localization, context validation, and intermediate event ordering.
- **Mechanism:** TRAKE pipeline operates in four stages: (1) LLM-based query decomposition into context C and ordered events {E1...En}; (2) boundary event matching identifies candidate start-end keyframe pairs with temporal constraints; (3) LLM evaluates semantic coherence between candidate segments and contextual sub-events; (4) beam search enforces sequential occurrence of intermediate events. Final ranking combines EventScore (visual similarity path) and ContextScore (LLM semantic coherence) via weighted fusion: FinalScore = α·EventScore + (1-α)·ContextScore.
- **Core assumption:** Events in natural language queries map to visually distinguishable keyframes with preserved temporal ordering, and CLIP embeddings capture sufficient semantic similarity for boundary matching.
- **Evidence anchors:**
  - [abstract]: "introduces a novel multi-event temporal retrieval algorithm (TRAKE) that enforces event ordering and contextual coherence"
  - [section 4.1]: "The overall objective is to identify a video segment Si within a large-scale corpus that maximizes its semantic and temporal consistency with all specified events"
  - [corpus]: "Multi-event Video-Text Retrieval" (arxiv:2308.11551) addresses similar temporal ordering challenges but lacks TRAKE's explicit boundary-to-intermediate decomposition
- **Break condition:** Fails when events lack visual grounding (e.g., "person thinks about dinner"), when temporal gaps exceed τ, or when LLM context scoring produces inconsistent rankings due to ambiguous event descriptions.

### Mechanism 2
- **Claim:** Hybrid retrieval combining CLIP-based vector search with multimodal metadata filtering improves precision over embedding-only approaches.
- **Mechanism:** Keyframes are encoded via CLIP-Laion into R^d vectors stored in Milvus for k-NN search. Parallel extraction populates MongoDB with YOLOv8 objects, Vintern OCR, Qwen2.5VL captions, and PhoWhisper ASR transcripts. Final results require: FinalSet = {k_j ∈ K | Filter(M(k_j), Q_m)}, joining Milvus candidates with MongoDB constraints before re-ranking.
- **Core assumption:** Visual semantics alone are insufficient; objects, text, and speech provide complementary disambiguation signals that improve retrieval specificity.
- **Evidence anchors:**
  - [abstract]: "combines a CLIP-based visual embedding pipeline with LLM-powered query decomposition"
  - [section 3.1]: "This repository supports structured filtering and full-text search, enabling fine-grained cross-modal retrieval"
  - [corpus]: "Enhanced Multimodal Video Retrieval System" (arxiv:2512.06334) demonstrates similar auxiliary modality integration for temporal retrieval
- **Break condition:** Metadata extraction failures (OCR on blurred text, ASR on background noise) or modality conflicts where visual similarity contradicts textual filters.

### Mechanism 3
- **Claim:** External web-scale image retrieval augments out-of-distribution (OOD) queries that pretrained CLIP embeddings cannot ground.
- **Mechanism:** For rare/ambiguous concepts, Google Image Search retrieves top-k images. User selects representative image I_selected, which CLIP-LAION image encoder processes: v_q = CLIP_image(I_selected). This visually-grounded embedding supplements the original text query, bridging gaps in CLIP's pretrained knowledge.
- **Core assumption:** OOD visual concepts exist in web image corpora with sufficient quality for CLIP to extract meaningful embeddings; user selection correctly disambiguates intent.
- **Evidence anchors:**
  - [abstract]: "integrating a Google Image Search fallback for out-of-distribution visual concepts"
  - [section 4.2]: "This validated combination of text and image allows the system to retrieve the semantically relevant video far more accurately, even for complex OOD queries"
  - [corpus]: "Smart Routing for Multimodal Video Retrieval" (arxiv:2507.13374) explores modality selection but not external knowledge grounding
- **Break condition:** API latency becomes prohibitive for real-time use; selected image introduces domain shift; Google results are noisy or culturally biased.

## Foundational Learning

- **Concept: Vision-Language Alignment (CLIP)**
  - **Why needed here:** Core to understanding how text queries map to visual keyframes without explicit training on the target video corpus.
  - **Quick check question:** Can you explain why CLIP enables zero-shot retrieval, and what types of visual concepts it typically fails to encode?

- **Concept: Temporal Segmentation and Shot Detection**
  - **Why needed here:** TRAKE assumes videos are pre-segmented into keyframes with timestamps; TransNetV2 provides this foundation.
  - **Quick check question:** How does shot boundary detection differ from semantic event segmentation, and why might shot-level granularity be insufficient for multi-event queries?

- **Concept: Late Fusion and Weighted Score Combination**
  - **Why needed here:** FinalScore combines EventScore and ContextScore; understanding when visual vs. semantic signals should dominate is critical for tuning α.
  - **Quick check question:** If ContextScore is noisy but EventScore is reliable, how should α be adjusted, and what failure modes might emerge?

## Architecture Onboarding

- **Component map:** Raw Video → TransNetV2 (shots) → CLIP-Laion (embeddings) → Milvus (vector DB) → MongoDB (metadata DB) → Query → GPT-5 (decomposition) → TRAKE pipeline → FinalScore fusion → Ranked segments → Google Image Search (OOD fallback)

- **Critical path:** Query decomposition → Boundary event retrieval → Context validation → Beam search for intermediate events → Score fusion. Latency is dominated by LLM calls (decomposition + context scoring) and beam search over large keyframe corpora.

- **Design tradeoffs:**
  - Google Image Search: Improves OOD robustness vs. introduces API latency and manual user selection
  - TRAKE beam width b: Higher b improves recall vs. exponential search cost
  - α weighting: Higher α prioritizes visual match vs. contextual coherence
  - Deduplication threshold (0.965 cosine): Aggressive filtering reduces storage vs. risks discarding near-duplicate but semantically distinct frames

- **Failure signatures:**
  - Empty retrieval: Boundary events too specific or τ too restrictive
  - Temporal incoherence: Beam search finds visually similar but temporally invalid paths
  - OOD fallback loop: Google results don't match user intent, requiring repeated selection
  - Context score inflation: LLM over-ranks segments with superficial keyword matches

- **First 3 experiments:**
  1. **Ablate TRAKE components:** Disable context scoring (α=1) vs. full TRAKE on held-out multi-event queries; measure mAP degradation to quantify ContextScore contribution.
  2. **OOD stress test:** Curate 50 queries referencing concepts unlikely in CLIP training (e.g., "Vietnamese locality-specific landmarks"); compare retrieval with/without Google Image Search fallback.
  3. **Latency profiling:** Instrument each pipeline stage (decomposition, Milvus queries, LLM context calls, beam search) on queries varying from 2 to 6 events; identify bottleneck for scaling.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the Google Image Search fallback be automated to eliminate the manual user selection step while maintaining retrieval accuracy for out-of-distribution queries?
- Basis in paper: [explicit] The authors state this approach "introduces a key limitation: variable latency... due to its reliance on two non-trivial overheads: the external API call to Google Search and the requisite step of manual user selection."
- Why unresolved: The current design requires human-in-the-loop selection, making it unsuitable for fully automated or real-time retrieval scenarios.
- What evidence would resolve it: A study comparing automated image selection strategies (e.g., clustering-based representative selection, relevance scoring) against manual selection on OOD query benchmarks.

### Open Question 2
- Question: What is the quantitative contribution of each system component (CLIP embeddings, LLM query decomposition, Google Image Search, TRAKE temporal algorithm) to overall retrieval performance?
- Basis in paper: [inferred] The paper reports overall rankings but does not include ablation studies isolating individual component contributions.
- Why unresolved: Without component-wise analysis, it remains unclear which elements are essential vs. marginal, limiting guidance for resource-constrained deployments.
- What evidence would resolve it: Ablation experiments on the Ho Chi Minh City AI Challenge tasks reporting performance with each component independently disabled.

### Open Question 3
- Question: How robust is the TRAKE algorithm when intermediate events share high visual similarity or have ambiguous temporal boundaries?
- Basis in paper: [inferred] TRAKE relies on sequential similarity scoring with beam search, but the paper does not analyze failure cases when events are visually confusable.
- Why unresolved: Real-world multi-event queries may contain semantically similar adjacent events that could confuse the temporal ordering constraint.
- What evidence would resolve it: Evaluation on synthetic or curated multi-event queries with controlled visual similarity between adjacent events.

### Open Question 4
- Question: How does substituting different open-source LLMs affect query decomposition quality and context scoring consistency?
- Basis in paper: [explicit] "Although our prototype employs GPT-5 for query enhancement, the module is fully LLM-agnostic. Any open-source model can be substituted without altering the architecture."
- Why unresolved: The claim lacks empirical validation; different LLMs may produce inconsistent decompositions or context scores, affecting retrieval reliability.
- What evidence would resolve it: Comparative evaluation using multiple LLMs (e.g., LLaMA, Mistral) on query parsing accuracy and context score correlation with ground-truth relevance.

## Limitations
- Missing specification of CLIP-Laion model variant and embedding dimension
- TRAKE hyperparameters (τ, b, top-M candidates, α) not provided
- GPT-5 prompt templates for decomposition and scoring unavailable
- Single competition evaluation limits generalizability claims
- Google Image Search fallback introduces latency and user-dependence

## Confidence
- **High confidence:** The hybrid retrieval approach combining CLIP embeddings with multimodal metadata filtering is well-established and the mechanism is clearly specified
- **Medium confidence:** The TRAKE algorithm's multi-stage pipeline (decomposition → boundary matching → context scoring → beam search) is logically sound, though exact parameter values are unknown
- **Medium confidence:** The OOD query handling via Google Image Search fallback is conceptually valid, but practical effectiveness depends on user selection quality and API reliability

## Next Checks
1. **Ablation study on TRAKE components:** Systematically disable context scoring (α=1) and beam search to quantify their individual contributions to retrieval accuracy on multi-event queries

2. **OOD robustness evaluation:** Create a test suite of 50 queries referencing concepts unlikely in CLIP training (e.g., "Vietnamese locality-specific landmarks") and compare retrieval performance with/without Google Image Search fallback

3. **Latency bottleneck analysis:** Instrument each pipeline stage (decomposition, Milvus queries, LLM context calls, beam search) on queries varying from 2 to 6 events to identify performance constraints and optimization opportunities