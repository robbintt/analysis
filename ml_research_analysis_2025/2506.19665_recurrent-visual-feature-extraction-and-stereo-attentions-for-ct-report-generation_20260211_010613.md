---
ver: rpa2
title: Recurrent Visual Feature Extraction and Stereo Attentions for CT Report Generation
arxiv_id: '2506.19665'
source_url: https://arxiv.org/abs/2506.19665
tags:
- slices
- visual
- report
- features
- attention
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of generating accurate radiology
  reports for computed tomography (CT) images. The proposed method uses a large language
  model (LLM) with a novel recurrent visual feature extraction (R-VFE) mechanism and
  stereo attentions to encode CT volumes and align visual features with textual content.
---

# Recurrent Visual Feature Extraction and Stereo Attentions for CT Report Generation

## Quick Facts
- **arXiv ID:** 2506.19665
- **Source URL:** https://arxiv.org/abs/2506.19665
- **Reference count:** 40
- **Primary result:** Achieves SOTA on M3D-Cap dataset with BLEU-1 score of 17.03, ROUGE score of 19.21, METEOR score of 14.63, and BERT-score of 88.67

## Executive Summary
This paper introduces a novel approach for generating accurate radiology reports from CT images using a large language model (LLM) enhanced with recurrent visual feature extraction (R-VFE) and stereo attentions. The method addresses the challenge of encoding 3D CT volumes into meaningful visual features that align with textual report content. By processing CT slices sequentially through a vision Transformer and applying hierarchical attention mechanisms, the system captures spatial correlations and emphasizes diagnostically relevant features. The approach outperforms strong baselines and achieves state-of-the-art results on the M3D-Cap dataset, demonstrating the effectiveness of combining recurrent encoding with multi-level attention for medical report generation.

## Method Summary
The method employs a Qwen2-VL-2B visual encoder with a novel recurrent visual feature extraction mechanism that processes CT slices sequentially using a vision Transformer. Each slice is divided into patches and encoded while incorporating information from previously processed slices through additive feature propagation. Three parallel stereo attention mechanisms then weight features at slice, window, and region levels to identify critical diagnostic information. These weighted features are projected and concatenated to form the final visual representation, which is fed to a Qwen2-VL-2B LLM to generate the radiology report. The entire system is trained end-to-end using cross-entropy loss between generated and gold-standard reports.

## Key Results
- Achieved BLEU-1 score of 17.03, ROUGE-1 score of 19.21, METEOR score of 14.63, and BERT-score of 88.67 on M3D-Cap test set
- Outperformed strong baselines including vision Transformer, graph convolutional network, and LLM-only approaches
- Demonstrated effectiveness of recurrent slice encoding and hierarchical attention mechanisms for CT report generation
- Validated on the M3D-Cap dataset with 116,092 training instances and 69.3 average slices per CT volume

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Recurrent slice encoding captures continuous spatial correlations across CT volumes better than independent or 3D-convolutional approaches
- Mechanism: A vision Transformer processes each slice sequentially, fusing the encoded patch features from position t-1 with the current patch embeddings via additive combination (Eq. 3: vt,n = pt,n + ht−1,n + posn). This propagates historical information forward through the volume.
- Core assumption: Consecutive CT slices exhibit strong spatial continuity, and lesions or anatomical structures transform predictably across positions.
- Evidence anchors:
  - [abstract]: "use a vision Transformer to recurrently process each slice in a CT volume"
  - [section 3.1]: "information of both historical and current slices are encoded and passed to the encoding process of the next slice"
  - [corpus]: Limited direct corpus validation for recurrent encoding specifically; neighbor papers (TomoGraphView, brat) explore graph-based and multi-view MRI approaches but not recurrent slice processing.
- Break condition: If CT slices have large spatial gaps, motion artifacts, or highly discontinuous pathology distributions, recurrent propagation may introduce noise rather than signal.

### Mechanism 2
- Claim: Hierarchical stereo attentions selectively emphasize diagnostically relevant features across slice, window, and region granularities
- Mechanism: Three parallel attention mechanisms compute softmax-normalized weights: (1) slice attention weights entire slice representations, (2) window attention weights fixed patch positions across all slices, (3) region attention weights patches within each slice. Weighted outputs are projected to align with LLM embedding space.
- Core assumption: Critical diagnostic information is sparsely distributed—specific slices, cross-slice anatomical positions, and local regions carry disproportionate importance.
- Evidence anchors:
  - [abstract]: "employ a set of attentions over the encoded slicesentions fromexemyology awardarrowarchive anly reportay
- Break condition: If diagnostic information is uniformly distributed across slices and regions, the attention mechanism may not provide significant advantage over flat encoding.

## Foundational Learning
- **Recurrent encoding in vision transformers**: Sequential processing of image slices while maintaining temporal context through feature propagation. Needed because CT volumes are inherently sequential and contain spatial continuity across slices. Quick check: Verify that the recurrent connection (vt,n = pt,n + ht−1,n + posn) is implemented correctly and propagates features as intended.
- **Multi-granularity attention mechanisms**: Parallel attention operations at different spatial scales (slice, window, region) to capture both global and local diagnostic features. Needed because critical information in CT scans may be distributed at varying levels of granularity. Quick check: Confirm that all three attention types (slice, window, region) are implemented and their outputs are properly projected and concatenated.
- **Cross-modal alignment for report generation**: Mapping visual features to LLM embedding space for coherent text generation. Needed because the visual encoder and LLM have different embedding dimensions and representations. Quick check: Verify that the projection layers (gs, gw, gr, g) correctly transform attention outputs to match LLM input dimensions.

## Architecture Onboarding

### Component Map
R-VFE encoder (ViT + recurrent connections) -> Stereo attentions (slice, window, region) -> Projection layers -> Concatenation -> LLM

### Critical Path
CT slices → Patch extraction → Recurrent ViT encoding → Stereo attentions → Feature projection → Concatenation → LLM input → Report generation

### Design Tradeoffs
- **Recurrent vs. parallel 3D processing**: Recurrent encoding captures temporal dependencies but may be slower than parallel 3D convolutions
- **Hierarchical attention vs. single attention**: Multi-level attention captures different granularities but increases model complexity
- **End-to-end training vs. frozen components**: Full fine-tuning allows adaptation but requires more computational resources

### Failure Signatures
- **Gradient vanishing in recurrent connections**: Poor performance on long CT sequences, especially for later slices
- **Attention collapse**: Uniform attention weights across slices/windows/regions, indicating failure to identify relevant features
- **Embedding misalignment**: Poor report quality due to mismatched visual and textual representations

### First Experiments
1. **Baseline comparison**: Implement a non-recurrent version (independent slice encoding) and single attention level to verify the contribution of each component
2. **Attention visualization**: Generate and analyze attention weight distributions to confirm they highlight diagnostically relevant features
3. **Sequence length sensitivity**: Test performance on CT volumes with varying slice counts to identify optimal sequence length for recurrent processing

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How does the proposed R-VFE method compare to 3D convolution or transformer baselines in terms of computational efficiency and inference latency?
- **Basis in paper:** [inferred] The R-VFE mechanism processes slices sequentially (recurrently) from $t=1$ to $T$, whereas standard 3D CNNs process volumes in parallel. The paper does not report runtime or complexity analysis (Page 3).
- **Why unresolved:** While the paper demonstrates performance gains (BLEU/ROUGE), it does not discuss the trade-off between the accuracy gained by recurrent modeling and the potential computational overhead of serialized processing.
- **What evidence would resolve it:** Comparative data on training duration, inference time (ms/slice), and GPU memory consumption against 3D-CNN baselines.

### Open Question 2
- **Question:** Does the unidirectional nature of the recurrence limit the model's ability to capture global context compared to a bidirectional approach?
- **Basis in paper:** [inferred] The R-VFE formulation (Eq. 3) incorporates features from the previous position ($h_{t-1}$) but not the subsequent position ($h_{t+1}$), implying a strictly forward flow of information (Page 3).
- **Why unresolved:** CT slices often require context from both preceding and succeeding slices to accurately identify boundaries of lesions. The paper does not ablate this design choice.
- **What evidence would resolve it:** An ablation study implementing a bidirectional recurrent connection (e.g., $h_{t} = f(h_{t-1}, h_{t+1})$) to see if backward context improves performance.

### Open Question 3
- **Question:** To what extent do the high NLG metric scores (e.g., BLEU, ROUGE) correlate with clinical factual accuracy and the absence of hallucinations?
- **Basis in paper:** [inferred] The evaluation relies exclusively on standard NLG metrics (BLEU, ROUGE, METEOR, BERT-score), which are known to correlate poorly with clinical validity in radiology reports (Page 4).
- **Why unresolved:** A generated report could score high on text overlap while containing factually incorrect clinical assertions (hallucinations) or missing critical findings.
- **What evidence would resolve it:** A quantitative evaluation using clinical efficacy metrics (e.g., precision/recall of specific labels) or a human evaluation by radiologists to grade factual correctness.

## Limitations
- **Architecture details unspecified**: Key hyperparameters such as patch size, attention projection dimensions, and position embedding configurations are not provided in the paper
- **Computational requirements unaddressed**: The method's computational cost for processing long CT sequences (avg 69.3 slices) is not discussed, raising potential GPU memory concerns
- **Dataset generalization unknown**: Results are reported only on the M3D-Cap dataset, with no validation on other CT datasets or pathologies

## Confidence

**High confidence:** The core mechanism of recurrent slice encoding and hierarchical stereo attentions is well-specified and theoretically sound. The sequential processing approach is appropriate for CT volumes.

**Medium confidence:** The reported performance metrics (BLEU-1 17.03, ROUGE 19.21, METEOR 14.63, BERT-score 88.67) are specific and verifiable, but the impact of unspecified architectural choices on these results is uncertain.

**Low confidence:** Claims about the method's superiority over specific baselines (vision Transformer, graph convolutional network, LLMs) are not fully supported by ablation studies or statistical significance tests.

## Next Checks
1. **Ablation study:** Implement and compare against simpler variants (no recurrence, single attention level) to verify the contribution of each mechanism to performance gains
2. **Cross-dataset evaluation:** Test the trained model on a different CT report generation dataset to assess generalization beyond M3D-Cap
3. **Attention visualization:** Generate and analyze slice, window, and region attention weight distributions to confirm they highlight diagnostically relevant features as claimed