---
ver: rpa2
title: Improving Estonian Text Simplification through Pretrained Language Models and
  Custom Datasets
arxiv_id: '2501.15624'
source_url: https://arxiv.org/abs/2501.15624
tags:
- simplification
- estonian
- llama
- sentence
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the scarcity of resources for Estonian text
  simplification by introducing a new dataset and benchmarking two neural approaches:
  OpenNMT and a fine-tuned LLaMA model. The Estonian Simplification Dataset was constructed
  by combining translated English corpora, GPT-4.0-generated simplifications, and
  manual corrections, totaling 50,416 sentence pairs.'
---

# Improving Estonian Text Simplification through Pretrained Language Models and Custom Datasets
## Quick Facts
- arXiv ID: 2501.15624
- Source URL: https://arxiv.org/abs/2501.15624
- Reference count: 13
- New Estonian Simplification Dataset (50,416 sentence pairs) enables low-resource text simplification research.

## Executive Summary
This paper addresses the scarcity of resources for Estonian text simplification by introducing a new dataset and benchmarking two neural approaches: OpenNMT and a fine-tuned LLaMA model. The Estonian Simplification Dataset was constructed by combining translated English corpora, GPT-4.0-generated simplifications, and manual corrections, totaling 50,416 sentence pairs. The LLaMA model was fine-tuned using this dataset with the Unsloth framework to optimize memory usage and training efficiency. Evaluation combined automatic metrics (BLEU, SARI, FKGL) with human ratings across grammaticality, readability, meaning preservation, and simplification effort. While OpenNMT achieved higher BLEU scores, LLaMA 3.1 outperformed it in SARI and human evaluations, demonstrating superior simplification quality and semantic retention. The results highlight the effectiveness of fine-tuned LLMs for low-resource languages and underscore the importance of human assessment in evaluating simplification systems. All resources, including the dataset, models, and code, are publicly released to support future research.

## Method Summary
The authors introduced a new Estonian Simplification Dataset by translating English corpora (Newsela, Wiki-Auto, TURK), generating initial simplifications using GPT-4.0, and manually correcting them. Two neural approaches were evaluated: OpenNMT, a traditional sequence-to-sequence model, and LLaMA 3.1, a large language model fine-tuned using the Unsloth framework for efficient training. The LLaMA model was optimized for memory usage and trained on the full dataset. Evaluation combined automatic metrics (BLEU, SARI, FKGL) with human assessments on grammaticality, readability, meaning preservation, and simplification effort. The LLaMA model demonstrated superior performance in human evaluations and SARI scores, while OpenNMT achieved higher BLEU scores.

## Key Results
- LLaMA 3.1 fine-tuned on the Estonian Simplification Dataset outperformed OpenNMT in SARI and human evaluations.
- Human raters rated LLaMA’s outputs higher on readability and meaning preservation than OpenNMT’s.
- The Estonian Simplification Dataset (50,416 pairs) enables low-resource text simplification research.

## Why This Works (Mechanism)
The approach leverages the strong semantic understanding of LLaMA 3.1, which, when fine-tuned on a custom Estonian simplification dataset, can better capture linguistic nuances than rule-based or smaller neural models. The Unsloth framework enables efficient fine-tuning by optimizing memory usage, allowing the model to learn from the full dataset without excessive computational cost. The combination of machine-generated simplifications and manual corrections ensures dataset quality while addressing the low-resource challenge for Estonian.

## Foundational Learning
- **Text Simplification**: Converting complex text into simpler, more readable forms while preserving meaning. Needed to address the lack of resources for Estonian simplification.
- **Sequence-to-Sequence Models**: Neural architectures that map input sequences to output sequences, commonly used in translation and simplification tasks. Required to generate simplified Estonian sentences.
- **Unsloth Framework**: A tool for efficient fine-tuning of large language models by optimizing memory and computational resources. Enables training LLaMA 3.1 on limited hardware.
- **Automatic Metrics (BLEU, SARI, FKGL)**: Quantitative measures for evaluating translation and simplification quality. Used for initial model comparison but limited for low-resource languages.
- **Human Evaluation**: Qualitative assessment of model outputs by native speakers on criteria like readability and meaning preservation. Critical for capturing nuances missed by automatic metrics.

## Architecture Onboarding
**Component Map**: Input Text -> Preprocessing -> LLaMA 3.1 Fine-tuning (Unsloth) -> Simplified Output -> Evaluation (Automatic + Human)
**Critical Path**: Dataset Construction → Model Fine-tuning → Evaluation
**Design Tradeoffs**: LLaMA 3.1 offers superior simplification quality but requires more resources; OpenNMT is faster but less accurate.
**Failure Signatures**: Low BLEU scores indicate poor fluency; poor SARI scores suggest inadequate simplification; low human ratings point to issues with readability or meaning preservation.
**First Experiments**: 1) Fine-tune LLaMA 3.1 on a subset of the dataset to verify training stability. 2) Compare model outputs using BLEU and SARI to establish baseline performance. 3) Conduct small-scale human evaluation to validate automatic metric alignment.

## Open Questions the Paper Calls Out
None

## Limitations
- Modest dataset size (50,416 pairs) relative to Estonian morphology may limit generalizability.
- GPT-4.0-generated simplifications introduce potential quality variance due to limited manual corrections.
- Small human evaluation pool (15 raters) without inter-rater reliability reporting raises consistency concerns.
- Automatic metrics (BLEU, SARI, FKGL) are primarily validated for English and may not capture Estonian nuances.
- Divergence between BLEU scores and human ratings suggests misalignment of automatic metrics for this language pair.

## Confidence
- LLaMA model’s superiority in human evaluations: High
- Dataset construction process: Medium
- Automatic metric comparisons: Low

## Next Checks
1. Conduct inter-rater reliability analysis on the human evaluation data to assess consistency across raters.
2. Expand the dataset with additional manually simplified Estonian sentences to test model performance at scale.
3. Apply alternative automatic metrics validated for morphologically rich languages or develop Estonian-specific evaluation measures.