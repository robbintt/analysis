---
ver: rpa2
title: 'SayNext-Bench: Why Do LLMs Struggle with Next-Utterance Prediction?'
arxiv_id: '2602.00327'
source_url: https://arxiv.org/abs/2602.00327
tags:
- priming
- prediction
- llms
- saynext-chat
- next-utterance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates why large language models (LLMs) struggle
  with next-utterance prediction in human dialogue, even when visual context is provided.
  The authors identify that humans rely on multimodal cues (gestures, gaze, emotional
  tone) for anticipatory processing, which current LLMs largely ignore due to their
  passive, text-only training paradigm.
---

# SayNext-Bench: Why Do LLMs Struggle with Next-Utterance Prediction?

## Quick Facts
- arXiv ID: 2602.00327
- Source URL: https://arxiv.org/abs/2602.00327
- Authors: Yueyi Yang; Haotian Liu; Fang Kang; Mengqi Zhang; Zheng Lian; Hao Tang; Haoyu Chen
- Reference count: 40
- Primary result: LLMs struggle with next-utterance prediction because they ignore multimodal cues (gestures, gaze, emotional tone) that humans use for anticipatory processing.

## Executive Summary
This paper investigates why large language models struggle with next-utterance prediction in human dialogue, even when visual context is provided. The authors identify that humans rely on multimodal cues (gestures, gaze, emotional tone) for anticipatory processing, which current LLMs largely ignore due to their passive, text-only training paradigm. To address this, they introduce SayNext-Bench, a benchmark for evaluating next-utterance prediction from multimodal cues, supported by SayNext-PC, a large-scale dataset of tennis post-match press conferences with rich visual and textual annotations. They also propose SayNext-Chat, a dual-route MLLM that incorporates cognitively inspired priming tokens to simulate anticipatory activation and align responses with contextual priors.

## Method Summary
SayNext-Chat is a dual-route MLLM that processes video frames through InternViT-300M and non-verbal features through a priming module, converging with text tokens for LLM generation. The model uses learnable priming tokens encoding contextual priors (20 cognitive-affective factors) to simulate human anticipatory processing. Training involves LoRA fine-tuning (r=16) on the SayNext-PC dataset with adaptive loss weighting between joint and priming objectives. The architecture separates fast perceptual mapping from deep prior inference, aiming to capture both immediate cue-response patterns and higher-level contextual understanding.

## Key Results
- SayNext-Chat outperforms state-of-the-art MLLMs across lexical overlap, semantic similarity, and emotion consistency metrics
- Priming tokens significantly enhance emotional alignment (+2.8% emotion valence gain in ablation studies)
- Cross-scenario and scalability evaluations demonstrate robust generalization beyond tennis press conferences
- Incorporating visual cues consistently improves next-utterance prediction performance

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Learnable priming tokens improve next-utterance prediction by encoding contextual prior beliefs before generation.
- **Mechanism:** The priming module extracts 20 cognitive-affective factors (e.g., Affect, Confidence, Perceived Pressure) from training data via LLM-assisted clustering, then learns to predict a priming vector from non-verbal features. This vector is injected as a dedicated token, biasing the LLM toward anticipatory activation.
- **Core assumption:** Semantic priming in humans—pre-activating contextually relevant representations—can be approximated via learned continuous embeddings.
- **Evidence anchors:** [abstract] "incorporates learnable priming tokens to simulate anticipatory activation"; [Section 4.1] "the target priming vector serves as an auxiliary representation that actively encodes contextual prior beliefs"
- **Break condition:** If priming factors are too domain-specific (e.g., tennis-only), transfer to other conversational contexts may fail.

### Mechanism 2
- **Claim:** Dual-route architecture separates fast perceptual mapping from deep prior inference, improving both fluency and alignment.
- **Mechanism:** The fast route (ViT → MLP → LLM) produces utterances directly from visual/text embeddings. The deep route processes non-verbal features through conv layers into a priming embedding, supervised by target priming vectors. Both routes converge at token level.
- **Core assumption:** Human predictive processing involves both automatic cue-response mapping and deliberative prior construction (Kahneman's System 1/2).
- **Evidence anchors:** [Section 4.2] "two complementary predictive routes: a fast route that directly maps low-level visual and textual cues... and a deep route that infers high-level priors"; [Section 5.2.3] Ablation shows priming tokens yield +2.8% emotion valence gain over fine-tuning alone
- **Break condition:** If visual backbone lacks temporal modeling (only 4 frames during training), dynamic cues like gaze shifts may be missed.

### Mechanism 3
- **Claim:** Multimodal non-verbal cues provide early signals of communicative intent that text-only models cannot access.
- **Mechanism:** Visual embeddings from InternViT capture facial expressions, gestures, and body language, which are fused with text tokens before LLM processing.
- **Core assumption:** Non-verbal cues causally predict verbal intent; humans exploit this but current MLLMs do not natively.
- **Evidence anchors:** [abstract] "overlooking non-verbal cues and lacking active predictive processing" identified as failure causes; [Section 5.2.1] "incorporating visual cues consistently improves next-utterance prediction performance"
- **Break condition:** If video quality is low (640×360 for 75% of PC19K) or camera shifts occur, cue extraction degrades.

## Foundational Learning

- **Concept:** Predictive Processing in Cognition
  - **Why needed here:** The paper explicitly frames next-utterance prediction as requiring "hierarchical predictions about forthcoming sensory and linguistic input" (Section 1).
  - **Quick check question:** Can you explain why statistical next-token prediction differs from active predictive processing?

- **Concept:** LoRA Fine-Tuning
  - **Why needed here:** Both visual and language backbones use LoRA (r=16) for efficient adaptation; understanding this is critical for reproduction.
  - **Quick check question:** What is the trade-off between LoRA rank and preservation of pre-trained knowledge?

- **Concept:** Semantic Priming
  - **Why needed here:** The priming vector design is directly inspired by cognitive psychology's semantic priming effect (Section 4.1).
  - **Quick check question:** How does semantic priming differ from simple attention mechanisms in transformers?

## Architecture Onboarding

- **Component map:** Video frames → InternViT-300M → visual tokens → concatenate with priming token → InternLM2.5 → predicted utterance. Parallel: non-verbal features → priming module → priming vector (supervised by target from codebook).

- **Critical path:** Video frames → InternViT → visual tokens → concatenate with priming token → InternLM2.5 → predicted utterance. Parallel: non-verbal features → priming module → priming vector (supervised by target from codebook).

- **Design tradeoffs:**
  - 4 training frames vs. 16 inference frames: reduces memory but may miss temporal dynamics
  - 20 priming factors: authors found k>20 degraded GPT-4.1 assignment stability
  - Adaptive loss weighting for L_joint vs. L_prim: EMA smoothing prevents priming loss dominance

- **Failure signatures:**
  - Low lexical overlap (~2-5% BLEU-4): expected; semantic/emotion metrics matter more
  - Pragmatic flattening: humor, sarcasm, metaphor often lost (Table 16 in appendix)
  - Subject-independent performance drops: individual patterns not captured

- **First 3 experiments:**
  1. Reproduce subject-dependent evaluation on SayNext-PC2K with and without priming module (ablation baseline).
  2. Test zero-shot transfer to IEMOCAP using checkpoint trained on PC2K (cross-scenario protocol).
  3. Vary number of input frames (4 vs. 8 vs. 16) to quantify temporal cue contribution.

## Open Questions the Paper Calls Out

- **Open Question 1:** Can MLLMs be adapted to successfully predict utterances containing non-literal pragmatic nuances, such as sarcasm, humor, and metaphor, which currently cause "pragmatically flattened" outputs?
  - **Basis in paper:** [explicit] Section 6 (Limitations & Future Works) and Appendix H.2 explicitly identify the failure to capture sarcasm and humor as a key limitation, noting models often miss "pragmatic layers of human communication."
  - **Why unresolved:** Current models tend to revert to literal or neutral explanations when faced with dry humor or metaphorical framing, indicating a lack of deeper pragmatic inference capabilities.
  - **What evidence would resolve it:** Successful prediction of ground-truth responses on a curated subset of SayNext-PC containing high rates of non-literal expressions, evaluated by human annotators for pragmatic appropriateness rather than just semantic overlap.

- **Open Question 2:** Does integrating fine-grained, model-level multi-turn history significantly improve next-utterance prediction accuracy and speaker personalization compared to prompt-level context injection?
  - **Basis in paper:** [explicit] Section 6 explicitly calls "fine-grained, model-level approaches to multi-turn utterance prediction" an "open research direction," while Appendix C.5 notes that prompt-level context only provides marginal gains.
  - **Why unresolved:** The paper's experiments were limited to prompt-level context augmentation; architectural mechanisms for maintaining speaker-specific habits and discourse dynamics over long horizons remain untested.
  - **What evidence would resolve it:** Comparison of a memory-augmented SayNext-Chat against the prompt-level baseline on the full SayNext-PC19K dataset, specifically measuring improvements in subject-dependent protocols.

- **Open Question 3:** Can the "priming factors" induced from specific domains (e.g., post-match interviews) function as a generalized cognitive inventory applicable to everyday casual dialogue?
  - **Basis in paper:** [inferred] While Section 5.2.4 suggests that scaling the dataset might support a "generalized inventory of priming vectors," the current codebook (Table 15) is heavily biased toward "Perceived Pressure," "Self-Efficacy," and sports-specific states.
  - **Why unresolved:** The universality of the specific 20 factors defined in the codebook is unknown; everyday conversation may rely on different cognitive priors (e.g., "Social Bonding," "Information Exchange") not dominant in press conferences.
  - **What evidence would resolve it:** Zero-shot transfer learning experiments where the SayNext-Chat priming module, trained on SayNext-PC, is applied to general conversation datasets (e.g., casual multi-party conversations) without retraining the priming codebook.

## Limitations
- The SayNext-PC dataset is specialized to tennis press conferences, limiting generalizability to other dialogue domains despite cross-scenario validation on IEMOCAP
- Dataset construction involves synthetic priming factor generation using GPT-4.1, introducing potential annotation biases
- Visual backbone processes only 4 frames during training and up to 16 during inference, potentially missing important temporal dynamics
- Subject-independent evaluation shows notable performance degradation, suggesting the model captures speaker-specific patterns rather than general conversational principles

## Confidence
**High Confidence:** The core empirical finding that multimodal integration improves next-utterance prediction performance, demonstrated through controlled ablation studies and quantitative metrics across lexical, semantic, and emotional dimensions.

**Medium Confidence:** The dual-route architecture design principle and its cognitive plausibility, as the paper provides theoretical motivation but limited direct validation of the fast-vs-deep route separation.

**Medium Confidence:** The priming token mechanism's effectiveness, though the specific design choices (20 factors, continuous embeddings) lack comparative analysis with alternative priming strategies or validation of their cognitive alignment.

**Low Confidence:** Claims about the model's ability to generalize beyond tennis press conferences to arbitrary conversational contexts, given the specialized nature of the training data and limited cross-domain evaluation.

## Next Checks
1. **Temporal Resolution Test:** Systematically vary the number of input frames (4, 8, 16, 32) during both training and inference to quantify the contribution of temporal dynamics to non-verbal cue interpretation and next-utterance prediction accuracy.

2. **Priming Factor Sensitivity Analysis:** Conduct ablation studies varying the number of priming factors (10, 15, 20, 25) and compare performance with alternative priming strategies such as discrete token injection or attention-based priming to validate the specific design choices.

3. **Cross-Domain Transfer Experiment:** Evaluate SayNext-Chat on dialogue datasets from different domains (customer service, casual conversation, debates) to assess generalization beyond tennis press conferences and identify domain-specific limitations.