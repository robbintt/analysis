---
ver: rpa2
title: iFlyBot-VLA Technical Report
arxiv_id: '2511.01914'
source_url: https://arxiv.org/abs/2511.01914
tags:
- action
- arxiv
- manipulation
- iflybot-vla
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper introduces iFlyBot-VLA, a large-scale Vision-Language-Action
  (VLA) model that addresses the challenge of precise, continuous robotic control
  by combining a pretrained Vision-Language Model (VLM) with a flow-matching diffusion
  expert. The core innovation is a dual-level action representation framework: latent
  actions learned from human and robot videos provide implicit high-level intentions,
  while structured discrete action tokens encode explicit low-level dynamics.'
---

# iFlyBot-VLA Technical Report

## Quick Facts
- arXiv ID: 2511.01914
- Source URL: https://arxiv.org/abs/2511.01914
- Authors: Yuan Zhang; Chenyu Xue; Wenjie Xu; Chao Ji; Jiajia wu; Jia Pan
- Reference count: 40
- Primary result: Achieves 93.8% average accuracy on LIBERO Franka benchmark, outperforming existing VLA models in precise continuous robotic control.

## Executive Summary
iFlyBot-VLA is a large-scale Vision-Language-Action model designed to address the challenge of precise, continuous robotic control. The core innovation is a dual-level action representation framework that combines latent actions learned from human and robot videos with structured discrete action tokens. This enables joint training of the VLM and action expert while preserving the VLM's perception and reasoning capabilities. The model incorporates a mixed training strategy blending robot trajectory data with general QA and spatial reasoning datasets, improving 3D perception and generalization.

## Method Summary
The model uses a three-stage training pipeline: (I) VQ-VAE latent action model pretrained on manipulation videos, (II) foundational pre-training with mixed robot and QA data using gradient truncation from action expert to VLM, and (III) task-specific fine-tuning with full gradients. The action expert employs flow-matching diffusion with KV cache reuse from the VLM's latent action tokens. Dual supervision uses latent actions for high-level intentions and FAST discrete tokens for explicit low-level dynamics, though FAST tokens are excluded from inference to avoid latency.

## Key Results
- Achieves 93.8% average accuracy on LIBERO Franka benchmark
- Demonstrates 96.25% accuracy in real-world pick-and-place tasks
- Shows strong performance on complex long-horizon manipulation and cloth folding with up to 96.25% accuracy

## Why This Works (Mechanism)

### Mechanism 1
The dual-level action representation enables joint training of the VLM and action expert while preserving the VLM's perceptual capabilities. Latent action tokens capture high-level intentions while FAST discrete tokens encode low-level dynamics. The VLM is supervised to predict both, aligning language, vision, and action spaces without degrading its reasoning ability. Core assumption: latent action space transfers to downstream control; FAST tokens provide sufficient explicit supervision. Break condition: if latent actions fail to generalize or FAST tokens cause overfitting.

### Mechanism 2
Flow-matching diffusion with KV cache reuse enables efficient, stable continuous action generation. The action expert receives KV caches from the VLM's latent action tokens, then denoises via Euler integration (5 steps, σ=0.2). KV cache is computed once per inference, regardless of integration steps. Core assumption: compressed latent action KV features contain sufficient planning information; bidirectional attention preserves temporal coherence. Break condition: if latent action compression discards critical dynamics information.

### Mechanism 3
Mixed training with spatial QA data preserves VLM reasoning while learning manipulation. Robot trajectory data is mixed with general VQA and spatial reasoning datasets at optimized ratios. For QA samples, action loss is zeroed and gradients to the action expert are blocked, preventing degradation of language/vision capabilities. Core assumption: spatial reasoning tasks share underlying representations with manipulation planning; gradient truncation prevents catastrophic forgetting. Break condition: if QA tasks are too semantically distant from manipulation.

## Foundational Learning

- Concept: VQ-VAE and latent action quantization
  - Why needed here: Understanding how continuous action dynamics are discretized into codebook entries for VLM prediction
  - Quick check question: Can you explain why NSVQ (noise substitution) prevents gradient collapse compared to straight-through estimation?

- Concept: Flow-matching diffusion vs. denoising diffusion
  - Why needed here: The action expert uses flow-matching (learning vector fields) rather than standard DDPM; this affects training stability and inference speed
  - Quick check question: What is the difference between learning to predict noise vs. learning a vector field in flow-matching?

- Concept: KV cache mechanics in transformer inference
  - Why needed here: The architecture relies on passing KV caches from VLM to action expert; understanding this is critical for debugging latency and memory
  - Quick check question: Why does KV cache reuse enable single-compute inference regardless of diffusion steps?

## Architecture Onboarding

- Component map: Qwen2.5-VL-3B backbone → latent action tokens + FAST tokens → VQ-VAE encoder-decoder → Diffusion Transformer action expert (receives KV cache from latent action tokens only)

- Critical path:
  1. Pre-train latent action model on manipulation videos → obtain codebook
  2. Train VLM + action expert jointly with dual supervision (latent + FAST), gradient truncation from expert to VLM in Stage II
  3. Enable full gradient flow in Stage III fine-tuning with multi-sample noise perturbations

- Design tradeoffs:
  - FAST tokens provide explicit supervision but are excluded from inference to avoid latency
  - Gradient truncation in pre-training preserves VLM capabilities but may slow action expert adaptation
  - 5-step Euler integration balances speed vs. action quality

- Failure signatures:
  - Gradient collapse in VQ-VAE → check NSVQ implementation
  - VLM reasoning degradation during end-to-end training → verify gradient truncation is active
  - Incoherent action sequences → inspect bidirectional attention mask and action window size
  - Slow inference → confirm FAST token features are excluded, KV cache is computed once

- First 3 experiments:
  1. Ablate latent action supervision (w/o LAM) on LIBERO-Spatial to isolate implicit planning contribution
  2. Ablate FAST tokens (w/o Fast) on LIBERO-Long to measure explicit dynamics impact
  3. Test gradient truncation vs. full backprop in Stage II on a held-out spatial QA task to verify VLM preservation

## Open Questions the Paper Calls Out

- Question: How can reinforcement learning (RL) be effectively integrated into the flow-matching framework to improve recovery from out-of-distribution states during deployment?
  - Basis in paper: The authors state in the Limitations section that the model struggles to "recover effectively when encountering out-of-distribution inputs" and explicitly propose integrating RL to surpass the "inherent limitations of imitation learning."
  - Why unresolved: The current architecture is purely trained via imitation learning and lacks a mechanism for online adaptation or reward-driven policy updates.
  - What evidence would resolve it: Demonstrating a training pipeline that incorporates RL fine-tuning, resulting in sustained performance under adversarial perturbations.

- Question: Does scaling model parameters and dataset size suffice to handle novel instructions involving unseen object concepts, or are architectural changes required?
  - Basis in paper: The authors acknowledge that iFlyBot-VLA "can still fail when following novel instructions involving unseen concepts" and identify "scaling up the model" and "expanding the training dataset" as future work.
  - Why unresolved: It is currently unclear if the failure to generalize to unseen concepts is a data scarcity issue or a fundamental limitation of the dual-level action representation's capacity to abstract new semantics.
  - What evidence would resolve it: A scaling law analysis showing a reduction in zero-shot error rates on semantically novel tasks as parameter count and data volume increase.

- Question: To what extent does the inclusion of human egocentric videos (e.g., Ego4D) improve the robot policy compared to using only robot demonstration data?
  - Basis in paper: The paper assumes that training the latent action model on large-scale human videos benefits downstream robot control, but provides no ablation isolating the contribution of human versus robot data.
  - Why unresolved: While the method claims cross-embodiment transfer benefits, the significant morphology gap between human hands and dual-arm robots introduces the risk of negative transfer or noisy latent spaces.
  - What evidence would resolve it: An ablation study comparing the performance of latent action models trained solely on robot data against those trained on the proposed human-robot mixture.

## Limitations
- The exact architecture of the Diffusion Transformer action expert remains unspecified
- The composition and mixing ratios of the spatial VQA dataset with robot trajectory data are not detailed
- Learning rate schedules and optimization hyperparameters across all three training stages are missing

## Confidence

- High Confidence: The dual-level action representation framework and its role in preserving VLM capabilities while enabling precise control is well-supported by ablation studies
- Medium Confidence: The effectiveness of mixed training with spatial QA data is supported by the 93.8% LIBERO accuracy, but lack of detailed mixing ratios introduces uncertainty
- Low Confidence: The flow-matching diffusion mechanism's specific implementation details and its advantages over standard denoising diffusion are not fully specified

## Next Checks
1. Conduct ablation studies on LIBERO-Long with varying latent action codebook sizes (16, 32, 64) to determine optimal latent action granularity
2. Implement and test gradient truncation vs. full backpropagation in Stage II on a held-out spatial reasoning benchmark
3. Evaluate the impact of different noise perturbation sampling strategies during Stage III fine-tuning on real-world task success rates