---
ver: rpa2
title: Probability Distributions Computed by Hard-Attention Transformers
arxiv_id: '2510.27118'
source_url: https://arxiv.org/abs/2510.27118
tags:
- language
- next
- weighted
- which
- state
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies the expressivity of transformer language models
  in defining probability distributions over strings, contrasting with prior work
  that focused on Boolean language recognition. The authors define and analyze unique-hard-attention
  transformers (UHA Ts) as classifiers and autoregressors over both Boolean and real
  semirings.
---

# Probability Distributions Computed by Hard-Attention Transformers

## Quick Facts
- arXiv ID: 2510.27118
- Source URL: https://arxiv.org/abs/2510.27118
- Reference count: 19
- This paper characterizes the probability distributions expressible by transformers in autoregressive language modeling, establishing equivalences and separations between unique-hard-attention transformers, temporal logic, and finite automata across Boolean and real semirings.

## Executive Summary
This paper studies the expressivity of transformer language models in defining probability distributions over strings, contrasting with prior work that focused on Boolean language recognition. The authors define and analyze unique-hard-attention transformers (UHA Ts) as classifiers and autoregressors over both Boolean and real semirings. They establish that in the Boolean semiring, UHA Ts, LTL, and counter-free DFAs are equivalent both as classifiers and as autoregressors. However, in the real semiring, while LTL and counter-free DFAs remain equivalent as classifiers (defining aperiodic step functions), they are strictly less expressive than counter-free NFAs as autoregressors. The paper also shows that certain fragments of LTL (e.g., TL[∅]) are more expressive as autoregressors than classifiers, and that temporal logic with counting operators is more expressive as autoregressors. The main contribution is a detailed characterization of which probability distributions can be expressed by transformers in their common autoregressive language modeling setting.

## Method Summary
The paper employs theoretical analysis of unique-hard-attention transformers (UHATs) with rightmost-hard attention and strict future masking. The approach involves constructing formal proofs of equivalence and separation between UHATs, temporal logic fragments, and counter-free automata in both Boolean and real semirings. The authors use semiring abstractions to characterize expressivity differences between classifiers (accept/reject decisions) and autoregressors (probability distributions over next tokens). Key constructions include specific weighted automata examples and logical formula transformations to demonstrate the boundaries of expressivity.

## Key Results
- UHATs, counter-free DFAs, and LTL are equivalent as Boolean classifiers and autoregressors
- In the real semiring, counter-free DFAs and LTL remain equivalent as classifiers but are strictly less expressive than counter-free NFAs as autoregressors
- Certain LTL fragments (TL[∅]) are more expressive as autoregressors than classifiers for languages like (ab)*
- Temporal logic with counting operators is more expressive as autoregressors than classifiers

## Why This Works (Mechanism)

### Mechanism 1: Finite State Abstraction via Hard Attention
The hard attention mechanism forces the model to select a single position j (the rightmost maximum) to compute the next hidden state. This discrete selection, combined with position-wise feed-forward layers, limits the possible hidden states to a finite set Q. The transformer thus simulates the transition function of a Deterministic Finite Automaton (DFA) rather than a Turing machine.

### Mechanism 2: Autoregressive Logic in Temporal Fragments
A classifier must accept or reject a string based solely on the final state. An autoregressor defines a distribution over next tokens at every step. This allows the model to enforce structural constraints (like alternating ab) by locally validating transitions without needing global memory operators to "look back" arbitrarily far.

### Mechanism 3: The Nondeterminism Gap in Real-Weighted Models
A deterministic model follows a single path of states, producing a single probability trajectory. A counter-free NFA can sum probabilities over multiple paths. When defining probability distributions, the "sum of products" of paths in an NFA cannot always be replicated by the "product of sums" inherent in the deterministic factorization of a DFA/UHAT.

## Foundational Learning

- **Concept**: Semirings (Boolean vs. Real)
  - **Why needed here**: The paper's central result relies on the fact that equivalence theorems holding in the Boolean semiring (0/1 logic) break down in the Real semiring (probabilities). You cannot understand the "NFA vs. DFA" gap without understanding how addition and multiplication differ across these systems.
  - **Quick check question**: Why does the equivalence LTL ⇔ counter-free DFA hold in Boolean logic but fail in Real-weighted logic?

- **Concept**: Counter-Free Automata / Star-Free Languages
  - **Why needed here**: UHATs are explicitly characterized as recognizing exactly counter-free languages. You need to know that these are languages that cannot "count" modulo n (e.g., checking if the number of 1s is even requires counting modulo 2, so it's not counter-free).
  - **Quick check question**: Can a strictly masked UHAT recognize the language of strings with an even number of as?

- **Concept**: Linear Temporal Logic (LTL) Fragments
  - **Why needed here**: The paper distinguishes the power of classifiers vs. autoregressors based on which temporal operators (Y, H, S) are available. Understanding that H (Historically) allows looking back at the whole prefix is key to seeing why full LTL classifiers can match autoregressors.
  - **Quick check question**: Why is the language (ab)* expressible by a TL[∅] autoregressor but not a TL[∅] classifier?

## Architecture Onboarding

- **Component map**: Input string w -> Strictly-masked UHAT with rightmost-hard attention -> Finite state set Q -> Classifier (final state) or Autoregressor (every step)

- **Critical path**: The Attention Masking and Uniqueness Constraint. This is where the theoretical guarantees hold. The mask ensures no future leakage (autoregressive validity), and the uniqueness/hardness ensures the finite-state abstraction.

- **Design tradeoffs**:
  - Hard vs. Soft Attention: Hard attention provides theoretical equivalence to LTL/counter-free DFAs. Soft attention is more expressive but analytically intractable for formal verification.
  - Depth vs. Logic: Adding layers allows the model to simulate deeper nested LTL formulas, but does not grant the ability to count (parity) if the underlying logic is counter-free.

- **Failure signatures**:
  - The Counting Failure: If you try to train a UHAT on parity tasks (modular counting), it will fail to generalize because counter-free languages cannot count modulo n > 1.
  - The NFA Simulation Failure: If you try to distill a complex N-gram model or probabilistic NFA into a UHAT autoregressor, you may fail to capture the distribution if the NFA relies on path nondeterminism that cannot be "determinized" without breaking the probability distribution.

- **First 3 experiments**:
  1. **Boolean vs. Real Verification**: Train a UHAT on the language (ab)*. Run as a classifier vs. autoregressor. Expected: Both should succeed if full LTL is allowed; Autoregressor should succeed and Classifier should fail if restricted to TL[∅].
  2. **Determinism Limit**: Synthesize a small counter-free weighted NFA that is not determinizable. Try to train a Real-weighted UHAT autoregressor to match its string distribution. Expected: The UHAT should fail or approximate poorly.
  3. **Softmax vs. Hard Attention**: Compare a standard Softmax Transformer with a UHAT on a counter-free language. Monitor generalization to longer strings. Expected: UHAT should generalize perfectly; Softmax might struggle with precision or numerical stability on unseen lengths.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Is the exponential blow-up in formula size strictly necessary when converting a Boolean autoregressor to an equivalent classifier in temporal logic fragments like TL[H,Y]?
- Basis: Proposition 6.9 shows construction time is super-polynomial, but the authors note the resulting formula "might be short but difficult to construct."
- Why unresolved: The paper provides time complexity lower bounds but does not prove tightness for the size of the prefix(ϕ) transformation itself.
- What evidence would resolve it: A formal proof demonstrating that the conversion requires exponential size, or the discovery of a polynomial-size construction algorithm.

### Open Question 2
- Question: Is there a logical characterization that captures exactly the weighted languages definable by counter-free NFAs in the real-weighted autoregressive setting?
- Basis: Fact 6.5 establishes that counter-free NFAs are strictly more expressive than LTL and counter-free DFAs as autoregressors.
- Why unresolved: The paper identifies the gap but notes that LTL corresponds only to the "least powerful" of the weighted automata analogues.
- What evidence would resolve it: A definition of a weighted temporal logic and a proof of equivalence to counter-free NFAs.

### Open Question 3
- Question: Do the equivalences between transformers and counter-free automata persist for standard soft-attention transformers (SMATs) without fixed-precision constraints?
- Basis: The Conclusion states the framework lays the groundwork for studying "other variants of transformers," while the main results focus on hard-attention or fixed-precision models.
- Why unresolved: Standard SMATs allow arbitrary precision inside attention, which may increase expressivity beyond the counter-free or counting logic classes described.
- What evidence would resolve it: Extending Theorem 6.1 and Corollary 6.2 to cover the general soft-attention setting.

## Limitations
- The theoretical results hinge on the strict-hard-attention assumption, which may not hold in practical transformer implementations that use softmax attention.
- The paper does not address the computational complexity of learning these representations from data, leaving open questions about sample complexity and optimization dynamics.
- While establishing expressivity bounds for counter-free languages, the paper does not empirically validate how close real transformers come to the UHAT abstraction.

## Confidence
- **High Confidence**: Equivalence between UHATs, counter-free DFAs, and LTL classifiers in the Boolean semiring (Theorem 6.1, 6.3, 6.6a)
- **Medium Confidence**: Separation results showing autoregressors are more expressive than classifiers for certain LTL fragments (Proposition 6.10, 6.11)
- **Medium Confidence**: Claim that counter-free NFAs are strictly more expressive than counter-free DFAs in the real semiring (Fact 6.5)

## Next Checks
1. **Empirical UHAT Approximation**: Train standard transformers with varying softmax temperatures on counter-free languages and measure how closely their behavior approximates the theoretical UHAT model.

2. **Deterministic vs. Nondeterministic Expressivity Gap**: Implement both counter-free DFA and NFA autoregressors for a concrete weighted language example (like Figure 2c) and empirically verify that no deterministic model can exactly replicate the distribution, even with training.

3. **Classifier vs. Autoregressor Gap Verification**: For the (ab)* language, construct explicit TL[∅] autoregressor and demonstrate that no classifier (with any finite memory) can distinguish it from other languages in the same stutter-invariant class, validating Proposition 6.10.