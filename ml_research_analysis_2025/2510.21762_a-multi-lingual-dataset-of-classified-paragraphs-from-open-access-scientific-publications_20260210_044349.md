---
ver: rpa2
title: A Multi-lingual Dataset of Classified Paragraphs from Open Access Scientific
  Publications
arxiv_id: '2510.21762'
source_url: https://arxiv.org/abs/2510.21762
tags:
- scientific
- mentions
- data
- dataset
- paragraphs
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents a multi-lingual dataset of 833k classified paragraphs
  from CC-BY licensed scientific publications, addressing the need for annotated training
  data to improve text classification and named entity recognition in scientific literature
  mining across multiple languages. The dataset was created by extracting paragraphs
  from the French Open Science Monitor corpus using GROBID, then semi-automatically
  classifying them into four categories (acknowledgments, data mentions, software
  mentions, clinical trial mentions) using specialized tools (GROBID, DataStet, Softcite).
---

# A Multi-lingual Dataset of Classified Paragraphs from Open Access Scientific Publications

## Quick Facts
- arXiv ID: 2510.21762
- Source URL: https://arxiv.org/abs/2510.21762
- Reference count: 2
- Dataset of 833k classified paragraphs from CC-BY scientific publications across 4 categories

## Executive Summary
This paper presents a multi-lingual dataset of 833k classified paragraphs extracted from open access scientific publications, aimed at improving text classification and named entity recognition in scientific literature mining across multiple languages. The dataset was created by processing PDFs from the French Open Science Monitor corpus using GROBID for XML/TEI extraction, then semi-automatically classifying paragraphs into acknowledgments, data mentions, software mentions, and clinical trial mentions using specialized tools. Each paragraph is annotated with language identification and scientific domain information, enabling training of classification models for non-English languages and NER models for extracting research data, software, clinical trial, and funding information.

## Method Summary
The dataset creation pipeline processes CC-BY licensed PDFs from the French Open Science Monitor corpus (2013+) through GROBID for structured extraction, then applies specialized tools (GROBID, DataStet, Softcite) with custom heuristics to classify paragraphs into four categories. Language detection uses fastText lid.176.bin, and scientific domains are assigned via OpenAlex primary topic fields. The output is a CSV with 833k rows containing license information, text content, DOI, classification type, detected language, publication year, binary flags for each category, and scientific field metadata. The classification approach combines tool-native recognition with custom heuristics, though specific implementation details for the heuristics are not fully documented.

## Key Results
- Dataset contains 833k classified paragraphs from CC-BY licensed publications
- Extreme language imbalance: 98.4% English, 1.5% French content
- Category distribution: 570k data mentions, 203k software mentions, 108k acknowledgments, 8.7k clinical trial mentions
- Enables training of multi-lingual text classification and NER models for scientific literature

## Why This Works (Mechanism)
The dataset addresses a critical gap in scientific text mining by providing large-scale, multi-lingual training data for paragraph classification and named entity recognition. By leveraging existing specialized tools (GROBID, DataStet, Softcite) and extending them with custom heuristics, the authors create a scalable pipeline for extracting and classifying relevant scientific information. The use of CC-BY licensed content ensures legal compliance for model training, while the multi-lingual focus (primarily English and French) enables development of models that work beyond English-language scientific literature.

## Foundational Learning
- **GROBID for PDF extraction**: Converts scientific PDFs to structured XML/TEI format - needed for reliable text extraction from scientific publications; quick check: verify XML structure contains proper paragraph segmentation
- **Specialized classification tools**: GROBID (acknowledgments), DataStet (data mentions), Softcite (software mentions) - needed to leverage domain-specific recognition capabilities; quick check: validate tool outputs match expected patterns
- **fastText language detection**: Uses pre-trained lid.176.bin model - needed for accurate language identification at paragraph level; quick check: confirm ~98.4% English, ~1.5% French distribution
- **Publication-level data splitting**: Group by DOI to prevent information leakage - needed for valid model evaluation; quick check: ensure no shared DOIs between train/test sets
- **OpenAlex domain assignment**: Uses primary topic field for scientific field classification - needed for domain-specific analysis; quick check: verify field_name and field_id mappings are consistent
- **Semi-automated classification pipeline**: Combines tool outputs with custom heuristics - needed to achieve scale while maintaining reasonable accuracy; quick check: sample paragraphs across categories for quality assessment

## Architecture Onboarding
- **Component map**: PDFs -> GROBID extraction -> Tool classification (GROBID/DataStet/Softcite) -> Heuristics application -> fastText language detection -> OpenAlex domain assignment -> CSV output
- **Critical path**: PDF ingestion through GROBID extraction is the foundation; all downstream classification depends on quality of this structured output
- **Design tradeoffs**: Scale vs. accuracy - semi-automated approach enables 833k paragraphs but introduces uncertainty in classification quality due to unspecified heuristics
- **Failure signatures**: Extreme class imbalance (8.7k clinical trials vs 570k data mentions) indicates potential issues with minority class recognition; unknown heuristics create reproducibility challenges
- **First experiments**: 1) Verify CSV schema matches specifications, 2) Assess classification quality through random sampling, 3) Test publication-level splitting impact on model performance

## Open Questions the Paper Calls Out
None

## Limitations
- Classification quality depends on unspecified "extra heuristics" applied to tool outputs, making exact replication impossible
- Clinical trial pattern matching rules are entirely unspecified, creating ambiguity about what constitutes valid mentions
- Extreme class imbalance (570k data mentions vs 8.7k clinical trials) limits utility for training minority class models
- Dataset is primarily English (98.4%), severely limiting multi-lingual utility despite paper's claims

## Confidence
- **High confidence**: Dataset creation methodology and overall statistics (833k paragraphs, 4 categories, language distribution, CSV format)
- **Medium confidence**: Classification pipeline description (tool usage), language detection via fastText, domain assignment via OpenAlex
- **Low confidence**: Classification accuracy, quality of heuristic extensions, clinical trial pattern matching rules, suitability for training minority class models

## Next Checks
1. Download the dataset and verify the exact CSV schema matches specifications, including all column names and data types
2. Examine random samples of paragraphs across all four categories to assess classification quality and identify potential false positives/negatives
3. Test the impact of publication-level (DOI) splitting on classification performance to quantify data leakage effects and establish proper validation procedures