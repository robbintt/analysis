---
ver: rpa2
title: Scientific Data Compression and Super-Resolution Sampling
arxiv_id: '2511.13675'
source_url: https://arxiv.org/abs/2511.13675
tags:
- data
- compression
- samples
- qois
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a framework for scientific data compression
  and super-resolution sampling that preserves key physical quantities of interest
  (QoIs) while enabling efficient data reduction. The method learns exponential family
  distributions from data using GRISE (for discrete data) or score matching (for continuous
  data), with QoIs as sufficient statistics, enabling compact representation.
---

# Scientific Data Compression and Super-Resolution Sampling

## Quick Facts
- arXiv ID: 2511.13675
- Source URL: https://arxiv.org/abs/2511.13675
- Authors: Minh Vu; Andrey Lokhov
- Reference count: 32
- Primary result: Achieves up to 90% compression while preserving physical quantities of interest within 0.05 error tolerance using 10-50 MCMC correction steps

## Executive Summary
This paper introduces a framework for compressing scientific data while preserving key physical quantities of interest (QoIs), enabling efficient storage and super-resolution sampling. The method learns exponential family distributions with QoIs as sufficient statistics, compresses data using DCT, and reconstructs via MCMC sampling initialized from decompressed data. Experiments demonstrate high compression ratios (up to 90%) with QoI accuracy maintained within 0.05 error tolerance using only 10-50 MCMC correction steps, compared to 250+ steps needed for random initialization. The approach scales polynomially with system size and outperforms standard and regularized autoencoders in QoI preservation.

## Method Summary
The method learns exponential family distributions from scientific data using GRISE for discrete data or score matching for continuous data, with QoIs as sufficient statistics. Compressed data is stored using lossy DCT compression by retaining coefficients capturing a threshold of total energy. Super-resolution is achieved by initializing MCMC sampling from decompressed data, which efficiently corrects distributions while preserving QoIs. The framework works on both synthetic and real scientific datasets, maintaining QoI accuracy while achieving high compression ratios through the combination of learned exponential family representation and DCT-based compression.

## Key Results
- Achieves compression ratios up to 90% while maintaining QoI accuracy within 0.05 error tolerance
- Requires only 10-50 MCMC correction steps versus 250+ steps needed for random initialization
- Scales polynomially with system size, outperforming standard and regularized autoencoders in QoI preservation
- Successfully applied to Ising models, quantum computer data, multivariate normals, lattice field theory, and molecular dynamics

## Why This Works (Mechanism)

### Mechanism 1: QoI-Preserving Exponential Family Representation
Encoding quantities of interest as sufficient statistics in an exponential family distribution guarantees that samples from the learned model statistically preserve those QoIs. The energy function E(x⃗) = θ⃗·Q(x⃗) parameterizes a maximum-entropy distribution where Q(x⃗) are the sufficient statistics, ensuring the learned distribution conserves the QoIs on expectation.

### Mechanism 2: Decompressed Data Initialization Reduces MCMC Mixing Time
Initializing MCMC chains from decompressed samples rather than random states dramatically reduces the steps needed to converge to the target distribution. Original training samples lie near the learned distribution manifold, and decompressed samples initialized from these stay within the same basin of attraction, requiring only local exploration rather than traversing the full energy landscape.

### Mechanism 3: DCT Coefficient Thresholding for Tunable Compression-Quality Tradeoff
DCT-based compression with energy preservation threshold enables controllable trade-off between storage reduction and MCMC correction burden. DCT concentrates signal energy in low-frequency coefficients, and storing only coefficients capturing E_presv fraction of total energy yields compression ratio C = 1 - E_presv. Lower E_presv increases C but requires more MCMC steps to correct.

## Foundational Learning

- Concept: Exponential family distributions and sufficient statistics
  - Why needed here: The entire compression framework relies on parameterizing distributions as P(x) ∝ exp(θ·Q(x)) where Q(x) are QoIs; without understanding sufficient statistics, you cannot reason about what information is preserved.
  - Quick check question: If Q(x) = {⟨x_i⟩, ⟨x_i x_j⟩}, what distribution family does this define for continuous and discrete variables respectively?

- Concept: Score matching for continuous distributions
  - Why needed here: Used to learn continuous exponential family parameters without computing the intractable partition function Z; the gradient-based objective J(θ) = ½∫p_d(x)||ψ_θ(x) - ψ_d(x)||² dx bypasses normalization.
  - Quick check question: Why does score matching avoid the partition function that makes maximum likelihood intractable?

- Concept: MCMC mixing time and burn-in
  - Why needed here: Understanding why random initialization requires 250+ steps while warm-started chains need only 10-50 requires grasping how MCMC explores energy landscapes and what "convergence" means.
  - Quick check question: What property of the energy landscape could cause mixing time to scale exponentially with system size?

## Architecture Onboarding

- Component map: Original Data → [GRISE/Score Matching] → Learned Parameters θ → [DCT Compression] → Compressed Coefficients → [DCT Decompression] → Perturbed Samples → [MCMC w/ θ] → Super-Resolved Samples
- Critical path: The MCMC correction loop is the performance bottleneck. Start with learning (verify QoI reconstruction), then tune compression level against MCMC steps.
- Design tradeoffs:
  - Higher compression (lower E_presv) → less storage, more MCMC steps
  - More QoIs as sufficient statistics → better physical fidelity, larger parameter space to learn
  - Smaller MCMC step size → more stable convergence, more steps needed
- Failure signatures:
  - QoI error plateaus above tolerance after 100+ MCMC steps → compression too aggressive or model misspecified
  - MCMC diverges (unbounded values for continuous case) → step size too large or score function unstable
  - Learned parameters near zero → insufficient data or over-regularization
- First 3 experiments:
  1. Replicate the synthetic Ising model experiment (p=16 nodes, M=10^6 samples). Verify that 10 correction steps achieve <0.05 QoI error across compression levels 0.1-0.9.
  2. Ablation: Compare random initialization vs. decompressed initialization on the same data, plotting QoI error vs. MCMC steps. Quantify the warm-start benefit claimed (250 vs. 10-50 steps).
  3. Stress test: On the multivariate Normal experiment, systematically vary condition number of Σ (10, 50, 100, 200) and observe whether correction steps or QoI accuracy degrade.

## Open Questions the Paper Calls Out

### Open Question 1
Can theoretical guarantees on QoI preservation and mixing time improvements be formally established for the super-resolution sampling procedure? The paper relies primarily on empirical validation, noting that "non-trivial energy-based models in high dimensions are difficult to sample from, especially using Markov Chain Monte Carlo (MCMC) methods that may exhibit exponentially large mixing times." While it cites theoretical work on data-based initialization (Koehler et al., 2024), formal guarantees for this specific compression-recovery pipeline remain unstated.

### Open Question 2
How does the framework perform on systems where the true underlying distribution contains higher-order interactions beyond the chosen sufficient statistics? For D-Wave experiments, the authors note "the actual couplings and fields on the chip remain unknown" and "these QoIs no longer fully characterize the underlying distribution" because the true model "containing higher-order moments as sufficient statistics."

### Open Question 3
What is the optimal lossy compression scheme for initializing MCMC correction, beyond the DCT-based approach demonstrated? The authors state "An exact lossy compression and recovery scheme used is not important, but we discuss a popular choice that we adopt in this work next," acknowledging DCT is one option among many.

### Open Question 4
How does the number of required MCMC correction steps scale with system size for distributions beyond the tested multivariate normals and Ising models? The scalability study shows polynomial scaling up to N=500, but the authors caution that "for non-trivial distributions, the correction sampling from random initializations can take time growing exponentially with the system size."

## Limitations

- The method's effectiveness critically depends on the assumption that chosen QoIs are sufficient statistics for the scientific phenomena of interest, potentially leading to scientifically misleading reconstructions despite accurate QoI preservation
- DCT compression assumes spectral concentration of scientific data, which may not hold for systems with important high-frequency features like shock fronts
- The framework requires careful tuning of the compression level against MCMC correction steps, with no universal optimal setting

## Confidence

- **High confidence**: The core mechanism of using exponential family distributions with QoIs as sufficient statistics to guarantee QoI preservation on expectation is mathematically sound and well-established
- **Medium confidence**: The dramatic reduction in MCMC steps (250→10-50) from decompressed initialization is supported by experiments but relies on the assumption that compression artifacts remain within the same basin of attraction as original data
- **Medium confidence**: The polynomial scaling claim is theoretically plausible given the learning and sampling procedures but lacks empirical validation across varying system sizes

## Next Checks

1. **QoI sufficiency stress test**: Systematically vary the set of QoIs used (e.g., moments up to order 2, 3, 4) on the Ising model and measure impact on QoI accuracy vs. compression ratio to determine the minimal sufficient set
2. **Extreme compression robustness**: Apply compression levels up to 99% (E_presv=0.01) on the multivariate Normal data and quantify degradation in QoI accuracy and required MCMC steps to identify the practical compression limit
3. **High-frequency feature preservation**: Modify the Ising model to include structured noise at high frequencies and assess whether DCT compression discards physically relevant information before MCMC can recover it