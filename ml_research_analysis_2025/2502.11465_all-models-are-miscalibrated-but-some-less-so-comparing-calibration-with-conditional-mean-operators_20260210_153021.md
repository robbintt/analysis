---
ver: rpa2
title: 'All Models Are Miscalibrated, But Some Less So: Comparing Calibration with
  Conditional Mean Operators'
arxiv_id: '2502.11465'
source_url: https://arxiv.org/abs/2502.11465
tags:
- calibration
- kernel
- conditional
- ckce
- mean
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of measuring calibration error
  in probabilistic models, which is crucial for high-risk settings where model reliability
  is essential. The authors propose the conditional kernel calibration error (CKCE)
  as a more robust metric compared to existing methods like ECE and JKCE.
---

# All Models Are Miscalibrated, But Some Less So: Comparing Calibration with Conditional Mean Operators

## Quick Facts
- arXiv ID: 2502.11465
- Source URL: https://arxiv.org/abs/2502.11465
- Authors: Peter Moskvichev; Dino Sejdinovic
- Reference count: 32
- Primary result: CKCE provides more robust model calibration comparisons by measuring conditional distribution discrepancy without sensitivity to marginal prediction distribution.

## Executive Summary
This paper addresses the problem of measuring calibration error in probabilistic models, which is crucial for high-risk settings where model reliability is essential. The authors propose the conditional kernel calibration error (CKCE) as a more robust metric compared to existing methods like ECE and JKCE. The core idea is to measure calibration error as the discrepancy between conditional distributions using conditional mean operators in reproducing kernel Hilbert spaces (RKHS). This approach directly compares the conditional distributions P(Y|Q_X) and P(Z_X|Q_X) without being influenced by the marginal distribution of model predictions.

## Method Summary
CKCE measures calibration error as the Hilbert-Schmidt norm of the difference between conditional mean operators (CMOs) for true labels and predicted probabilities. The method uses a combined linear-Gaussian kernel on the probability simplex and Kronecker kernel on labels, with empirical CMO estimation via regularized linear inversion. A scalable random Fourier features (RFF) approximation reduces computational complexity from O(n³) to O((m+2D)³). The approach targets strong calibration (all class probabilities calibrated) rather than just confidence calibration.

## Key Results
- CKCE remains stable under covariate shift while ECE and JKCE vary significantly, as demonstrated on synthetic data with varying input distributions
- On ImageNet, CKCE indicates ViT has lower calibration error than ResNet, while ECE shows the opposite due to sensitivity to prediction confidence distribution
- The combined linear-Gaussian kernel is essential for CKCE stability under distribution shift; using only linear or only Gaussian kernels shows noticeable degradation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: CKCE provides more reliable relative calibration comparisons by removing sensitivity to the marginal distribution of model predictions
- Mechanism: By measuring calibration as the discrepancy between conditional distributions P(Y|Q_X) and P(Z_X|Q_X) using conditional mean operators, CKCE isolates the calibration property from how frequently the model makes predictions at different confidence levels
- Core assumption: The conditional distributions P(Y|Q_X) and P(Z_X|Q_X) can be adequately estimated from finite samples using regularized CMO estimators
- Evidence anchors: ImageNet experiment shows ResNet preferred by ECE due to more predictions in well-calibrated regions, while CKCE correctly identifies ViT as better calibrated

### Mechanism 2
- Claim: The Hilbert-Schmidt norm of the CMO difference captures strong calibration violations across all classes simultaneously
- Mechanism: Conditional mean operators encode how predicted probabilities map to conditional distributions over labels, providing a single scalar measuring deviation from calibration
- Core assumption: The chosen kernels are universal/characteristic, ensuring zero CKCE implies equal conditional distributions
- Evidence anchors: Mathematical formulation shows CKCE = 0 implies strong calibration holds almost everywhere

### Mechanism 3
- Claim: The combined linear-Gaussian kernel ensures both linear maps and smooth non-linear functions are representable in the RKHS
- Mechanism: The kernel k(p,q) = p^T q + exp(-||p-q||²/2γ²) combines linear (identity map) and Gaussian (smooth deviations) components, satisfying CMO assumptions
- Core assumption: Median heuristic bandwidth selection is appropriate across prediction distributions
- Evidence anchors: Figure 4 shows linear-only or Gaussian-only kernels degrade under covariate shift while combined kernel remains stable

## Foundational Learning

- Concept: **Reproducing Kernel Hilbert Spaces (RKHS) and Kernel Mean Embeddings**
  - Why needed here: The entire CKCE framework relies on representing distributions as points in an RKHS via mean embeddings, and computing distances via MMD
  - Quick check question: Given a kernel k and distribution P, write the expression for the kernel mean embedding μ_P and explain what property makes MMD a valid distance metric

- Concept: **Conditional Mean Operators (CMO)**
  - Why needed here: CKCE is defined as the Hilbert-Schmidt norm of the difference between two CMOs
  - Quick check question: What is the empirical estimator for C_{Y|X} and what role does the regularization parameter λ play?

- Concept: **Strong vs. Confidence Calibration**
  - Why needed here: The paper targets strong calibration (all class probabilities calibrated), not just confidence calibration (predicted class only)
  - Quick check question: For a 3-class classifier, give an example where confidence calibration holds but strong calibration fails

## Architecture Onboarding

- Component map: Input samples -> Predictions and labels -> Gram matrix computation -> CMO estimators -> CKCE output
- Critical path:
  1. Collect calibration set {(x_i, y_i)}
  2. Extract predictions q_i = f(x_i) for each sample
  3. Compute Gram matrix K_{QQ} using combined kernel
  4. Build Ψ_Y (label embeddings) and M_Z (mean embeddings of predictive distributions)
  5. Solve regularized linear system (K_{QQ} + λnI)^{-1}
  6. Compute Frobenius norm of CMO difference

- Design tradeoffs:
  - Regularization λ: Smaller λ reduces bias but increases variance; paper uses λ_n = n^{-1/4} as default schedule
  - RFF dimension D: Higher D improves kernel approximation but increases compute; paper uses D=100
  - Kernel bandwidth γ: Median heuristic is automatic but may not be optimal for all prediction distributions
  - Sample size n: CKCE requires sufficient coverage of the probability simplex; sparse regions yield unreliable estimates

- Failure signatures:
  - CKCE = 0 but model clearly miscalibrated: Kernel may not be characteristic; check kernel choice
  - High variance across random seeds: Insufficient samples or poor regularization; increase n or adjust λ
  - Inconsistent rankings under small distribution shifts: Suggests CMO estimation instability; verify kernel bandwidth and regularization
  - Computational bottleneck at matrix inversion: Switch to RFF implementation for n > 5000

- First 3 experiments:
  1. Compute CKCE on a perfectly calibrated synthetic model (sample labels from predicted probabilities); verify CKCE ≈ 0
  2. Take a fixed model, reweight the input distribution to shift Q_X marginals, confirm CKCE remains stable while ECE/JKCE vary
  3. Compare linear-only, Gaussian-only, and combined kernels under covariate shift (varying input distribution location parameter); replicate Figure 4 pattern

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the Conditional Kernel Calibration Error (CKCE) be effectively utilized as a regularization term during model training to improve calibration?
- Basis in paper: The conclusion states that "future research could explore using CKCE as a regularisation term in model training."
- Why unresolved: The current work focuses solely on CKCE as a post-hoc evaluation metric for relative model comparison, not as an optimization objective
- What evidence would resolve it: Empirical results from models trained with a CKCE-based loss term, demonstrating improved calibration metrics without significant degradation in accuracy

### Open Question 2
- Question: How can the null distribution for CKCE be estimated to enable its use as a statistic in calibration hypothesis testing?
- Basis in paper: Page 2 notes that "it might be challenging to use CKCE as a test statistic for a calibration test because of the difficulty in estimating the null distribution."
- Why unresolved: The paper focuses on relative comparisons rather than hypothesis testing, leaving the derivation of the null distribution properties as an open problem
- What evidence would resolve it: A proposed method (e.g., permutation testing or analytical bounds) that reliably approximates the CKCE null distribution and validates it via type I error control

### Open Question 3
- Question: Is the fixed regularization schedule λ_n = n^{-1/4} optimal for Conditional Mean Operator (CMO) estimation in the context of calibration?
- Basis in paper: Page 5 states the authors chose the schedule "for simplicity" and notes that cross-validation is an alternative, but does not compare the sensitivity of CKCE rankings to this choice
- Why unresolved: The paper provides no ablation study on the regularization parameter λ, leaving the robustness of the derived rankings relative to this hyperparameter unverified
- What evidence would resolve it: Experiments comparing the consistency of model rankings when using cross-validated λ versus the fixed theoretical schedule

## Limitations
- CMO estimation requires sufficient sample size to cover the probability simplex, particularly problematic for high-dimensional predictions
- Median heuristic for bandwidth selection lacks guarantees of optimality across different prediction distributions
- Behavior under label shift and underparameterized regimes remains unexplored

## Confidence
- **High confidence**: The mathematical formulation of CKCE and its relationship to strong calibration is sound
- **Medium confidence**: Empirical demonstrations showing CKCE stability under covariate shift are convincing but limited in scope
- **Low confidence**: Claims about CKCE being "more effective" for model comparison require more extensive benchmarking across diverse architectures

## Next Checks
1. Systematically test whether the combined kernel is indeed characteristic on the probability simplex by checking if CKCE = 0 implies identical conditional distributions across various synthetic scenarios
2. Extend the covariate shift experiments to include label shift and combined shift scenarios, measuring CKCE sensitivity to different types of distribution changes
3. Evaluate the RFF approximation's accuracy degradation as dimensionality increases and sample size varies, establishing practical limits for CKCE application in large-scale settings