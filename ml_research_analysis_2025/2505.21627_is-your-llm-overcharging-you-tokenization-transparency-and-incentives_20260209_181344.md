---
ver: rpa2
title: Is Your LLM Overcharging You? Tokenization, Transparency, and Incentives
arxiv_id: '2505.21627'
source_url: https://arxiv.org/abs/2505.21627
tags:
- provider
- token
- output
- sequence
- pricing
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper demonstrates that the widely used pay-per-token pricing\
  \ mechanism for LLM services creates a strong incentive for providers to overcharge\
  \ users by misreporting the tokenization of generated outputs. Under this mechanism,\
  \ a provider\u2019s revenue is directly tied to the length of the reported token\
  \ sequence, encouraging longer tokenizations regardless of plausibility."
---

# Is Your LLM Overcharging You? Tokenization, Transparency, and Incentives

## Quick Facts
- arXiv ID: 2505.21627
- Source URL: https://arxiv.org/abs/2505.21627
- Reference count: 40
- Primary result: Pay-per-token pricing incentivizes LLM providers to overcharge by misreporting tokenizations; only pay-per-character pricing is incentive-compatible.

## Executive Summary
This paper demonstrates that the widely used pay-per-token pricing mechanism for LLM services creates a strong incentive for providers to overcharge users by misreporting the tokenization of generated outputs. Under this mechanism, a provider's revenue is directly tied to the length of the reported token sequence, encouraging longer tokenizations regardless of plausibility. Even when providers are transparent about the generative process and implement plausibility checks, the authors show that finding the longest plausible tokenization is computationally hard, yet efficient heuristic algorithms can still yield significant overcharging (up to 11.2% more tokens) without raising suspicion. The financial gain from such misreporting often exceeds the computational cost of plausibility verification, making it a profitable strategy. To eliminate this vulnerability, the authors prove that only a pay-per-character pricing mechanism is incentive-compatible—charging users proportionally to the character count of outputs rather than token count.

## Method Summary
The paper employs both theoretical analysis and empirical experiments to evaluate tokenization misreporting incentives. The authors develop Algorithm 1 (random splitting) and Algorithm 2 (greedy splitting by highest token index with plausibility verification) to find longer plausible tokenizations. They use the LMSYS-Chat-1M dataset, filtering for 600 prompts in English, Spanish, Russian, and Chinese with output lengths of 20-100 characters. Experiments are conducted across multiple models (Llama-3.2-1B-3B, Gemma-3-1B-4B, Ministral-8B) using temperature 1.3 and top-p sampling with p ∈ {0.90, 0.95, 0.99}. The study measures percentage of overcharged tokens, fraction of outputs where Algorithm 2 finds longer plausible tokenizations, and relative increase in provider utility from misreporting.

## Key Results
- Pay-per-token pricing creates a strong incentive for providers to overcharge users by misreporting tokenizations.
- Efficient heuristic algorithms can find longer plausible tokenizations, yielding up to 11.2% more tokens without raising suspicion.
- Only pay-per-character pricing is theoretically incentive-compatible, eliminating the incentive to misreport token counts.
- The proposed pricing mechanism requires setting price per character as token price multiplied by average token-to-character ratio to maintain provider profit margins.

## Why This Works (Mechanism)

### Mechanism 1: Tokenization Misreporting via Information Asymmetry
Under pay-per-token pricing, a provider's utility function $U_\pi(\tilde{t}, t) = r(\tilde{t}) - c_{gen}(t) - c_\pi(t)$ increases with the length of the reported sequence $\tilde{t}$ because revenue $r(\tilde{t}) = r_o \cdot \text{len}(\tilde{t})$ is linear. Since tokenization is non-unique (e.g., "Damascus" vs. "Da|ma|s|cus"), the provider can increase $\text{len}(\tilde{t})$ without altering the string $str(\tilde{t})$ perceived by the user. Users derive value from the output string rather than the specific token sequence and cannot observe the internal generative process. The break condition occurs if users can audit the generation process or if pricing depends on string length rather than token count.

### Mechanism 2: Plausibility-Constrained Heuristic Overcharging
Finding the longest plausible tokenization is computationally hard (NP-Hard), but Algorithm 2 efficiently finds longer plausible tokenizations by iteratively splitting tokens with high vocabulary indices and verifying plausibility through forward passes. The provider's cost of verifying plausibility is lower than the additional revenue gained from extra tokens, making this strategy profitable. The break condition occurs when verification cost exceeds expected revenue increase or when transparency requirements force use of the optimal (NP-Hard) solution.

### Mechanism 3: Incentive-Compatible Pay-Per-Character Pricing
Charging users based on character count of the output string eliminates the incentive to misreport token counts. By setting price $r(t) = |str(t)| \cdot r_c$, revenue depends only on the string, which the provider cannot change without altering user-perceived value. The break condition occurs if the average token-to-character ratio prescription fails to maintain profit margins due to high variance in output languages.

## Foundational Learning

- **Concept: Principal-Agent Problem (Moral Hazard)**
  - Why needed: To understand that the vulnerability is an economic inevitability when one party's actions are unobservable to the other.
  - Quick check: If a provider's utility were based on user satisfaction rather than token count, would the incentive to split tokens persist?

- **Concept: Byte-Pair Encoding (BPE) and Tokenization Non-Uniqueness**
  - Why needed: To grasp that "Damascus" ≠ "Dam|ascus" is a fundamental property enabling the splitting strategy.
  - Quick check: Why does a greedy tokenizer typically produce the shortest token sequence, and how does Algorithm 2 exploit this?

- **Concept: Top-p (Nucleus) Sampling**
  - Why needed: To understand the constraint under which the provider must operate—only claiming sequences with cumulative probability exceeding p.
  - Quick check: How does increasing temperature or p value affect the feasibility of finding a "plausible" longer tokenization?

## Architecture Onboarding

- **Component map:** Generator -> Reporting Policy ($\pi$) -> Pricing Engine
- **Critical path:** The Verification Step in Algorithm 2. The system must perform a forward pass to check if split tokens fall within the top-p probability mass. Skipping this check allows user detection via plausibility analysis.
- **Design tradeoffs:**
  - Pay-per-Token: High user vulnerability, stable provider margins
  - Pay-per-Character: Incentive-compatible, but unstable provider margins across languages
- **Failure signatures:**
  - Negative Margins: Under pay-per-character, outputs with high token-to-character ratios may cost more to generate than charged price
  - Detection: Under pay-per-token, too many single-character tokens reveal systematic manipulation
- **First 3 experiments:**
  1. Implement Algorithm 1 (random splitting) on local Llama model to measure theoretical maximum token inflation without plausibility checks
  2. Implement Algorithm 2 and measure "Overcharged tokens (%)" against p values (0.9 vs 0.99) to validate greediness vs detection risk tradeoff
  3. Simulate pay-per-character mechanism on multilingual outputs to calculate profit margin standard deviation and identify break-even points

## Open Questions the Paper Calls Out

### Open Question 1
How can a pricing mechanism be designed to simultaneously incentivize faithful token reporting and high output quality? The current work focuses exclusively on aligning incentives for correct tokenization counts relative to costs, without addressing the semantic quality or utility of the generated text.

### Open Question 2
Can statistical methods effectively audit provider tokenizations to detect systematic misreporting across multiple outputs? The paper demonstrates vulnerability in single interactions but does not explore whether aggregate data analysis could reveal patterns of inflation invisible in individual outputs.

### Open Question 3
How do different pricing mechanisms influence competitive dynamics and user choice in markets with multiple providers? The paper models the interaction as a single principal-agent problem and does not analyze how a shift to pay-per-character pricing would affect market share or competition.

## Limitations

- Conclusions rely heavily on theoretical modeling and controlled experiments rather than real-world deployment evidence.
- The actual prevalence of tokenization misreporting in commercial LLM services remains unknown.
- The paper uses relatively small models (1B-8B parameters) which may not reflect production API architectures and optimizations.

## Confidence

- **High Confidence:** Theoretical proof that pay-per-token pricing creates misreporting incentives, and that pay-per-character pricing eliminates this incentive.
- **Medium Confidence:** Empirical effectiveness of Algorithm 2 in finding longer plausible tokenizations under controlled conditions with temperature > 1.0.
- **Low Confidence:** Practical impact and prevalence of this vulnerability in real-world LLM services, and effectiveness of the pay-per-character transition prescription.

## Next Checks

1. **Real-world API Audit:** Deploy Algorithm 2 against actual commercial LLM APIs to measure actual prevalence and magnitude of tokenization misreporting in production services.

2. **Margin Stability Analysis:** Implement the pay-per-character mechanism with average ratio prescription across diverse multilingual datasets to empirically validate whether the proposed pricing strategy maintains consistent profit margins.

3. **Economic Equilibrium Simulation:** Model long-term market dynamics if pay-per-character pricing were adopted, including potential provider responses such as optimizing tokenizers for character efficiency or developing new business models.