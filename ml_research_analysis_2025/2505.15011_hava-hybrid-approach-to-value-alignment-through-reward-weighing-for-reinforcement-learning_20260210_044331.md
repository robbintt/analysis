---
ver: rpa2
title: 'HAVA: Hybrid Approach to Value-Alignment through Reward Weighing for Reinforcement
  Learning'
arxiv_id: '2505.15011'
source_url: https://arxiv.org/abs/2505.15011
tags:
- norms
- agent
- value
- social
- hava
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the problem of aligning reinforcement learning
  agents with societal values, particularly when norms are both explicitly defined
  (legal/safety) and implicitly learned (social). The proposed method, HAVA (Hybrid
  Approach to Value-Alignment through Reward Weighing), integrates rule-based and
  data-driven norms into a single framework.
---

# HAVA: Hybrid Approach to Value-Alignment through Reward Weighing for Reinforcement Learning

## Quick Facts
- **arXiv ID:** 2505.15011
- **Source URL:** https://arxiv.org/abs/2505.15011
- **Reference count:** 38
- **Primary result:** HAVA successfully aligns RL agents with societal values by integrating rule-based and data-driven norms, achieving statistically indistinguishable human-like behavior in traffic simulations.

## Executive Summary
HAVA addresses the challenge of aligning reinforcement learning agents with societal values by integrating rule-based (legal/safety) and data-driven (social) norms through a reputation-based reward weighing mechanism. The approach uses a dynamic reputation score that scales task rewards based on norm compliance, encouraging agents to balance task efficiency with social appropriateness. Tested in both grid-world and traffic simulation scenarios, HAVA demonstrates that it can produce value-aligned policies that are statistically indistinguishable from human behavior while preventing both safety violations and anti-social conduct.

## Method Summary
HAVA implements a hybrid approach where rule-based norms act as hard constraints (shielding) and data-driven norms provide soft guidance through reputation scoring. The agent receives environment rewards scaled by a dynamic reputation score that decreases when violating either type of norm. Rule-based norms (safety/legal) override agent actions if they would cause violations, while data-driven norms (social preferences) influence behavior through reputation penalties. The reputation score recovers exponentially over time according to a forgiveness parameter, allowing the agent to learn socially acceptable policies while maintaining task performance. The framework was implemented using Deep Q-learning with double, dueling, and noisy networks, trained on both grid-world and SUMO traffic junction scenarios.

## Key Results
- HAVA policies achieved statistical indistinguishability from human behavior (p-value = 0.42) in traffic simulations
- Using only rule-based norms resulted in unsocial, abrupt behavior
- Using only data-driven norms led to safety violations due to inadequate norm severity understanding
- Optimal performance achieved with forgiveness parameter α = 0.1, balancing compliance and task completion

## Why This Works (Mechanism)

### Mechanism 1: Reputation-Weighted Reward Scaling
Scaling the environment reward by a dynamic reputation score forces the agent to trade off task efficiency against norm compliance. HAVA computes reputation w_t ∈ [0,1] based on adherence to norms, scaling the original task reward r_t by w_{t+1}. If the agent violates norms, w_t drops, immediately reducing task reward value. To maximize cumulative reward, the agent must maintain high reputation.

### Mechanism 2: Asymmetric Norm Enforcement (Shielding vs. Weighing)
Separating norms into "mandatory" (rule-based) and "tentative" (data-driven) with different enforcement mechanisms prevents catastrophic safety failures while retaining social flexibility. Rule-based norms act as a hard shield, overwriting forbidden actions with safe alternatives. Data-driven norms act as a soft constraint, lowering reputation when violated. This enforces a hierarchy where safety is guaranteed but social behavior is incentivized.

### Mechanism 3: Non-Linear Reputation Recovery (Forgiveness)
A non-linear recovery function for reputation allows the system to penalize recent violations heavily while eventually forgiving past misbehavior, stabilizing long-term learning. Reputation recovery follows an exponential curve governed by α, where recovery is slow immediately after a violation but speeds up as reputation rises. This prevents "gaming" the system through rapid sin-and-repent cycles while avoiding permanent zero-reward states.

## Foundational Learning

- **Concept: Markov Decision Processes (MDPs) & Reward Shaping**
  - **Why needed here:** HAVA modifies the reward function without changing environment dynamics, requiring understanding that the agent optimizes the modified return.
  - **Quick check question:** If an agent receives r_t=100 but has w_{t+1}=0.1, what is the effective reward? (Answer: 10)

- **Concept: Constrained Reinforcement Learning (Shielding)**
  - **Why needed here:** Rule-based norms use action masking where the agent's choice is overridden if unsafe.
  - **Quick check question:** Does the agent learn Q-values for intended or executed actions? (Answer: Executed actions)

- **Concept: Supervised Norm Approximation**
  - **Why needed here:** Data-driven component is trained via supervised learning to predict acceptable actions, acting as a critic for reputation calculation.
  - **Quick check question:** How does D_D inform the RL agent without being part of the environment? (Answer: Through distance calculation from predicted acceptable actions)

## Architecture Onboarding

- **Component map:** Action Selection → R_B Filter (Override if needed) → Env Step → Calculate Distance to D_D → Update Reputation → Weigh Reward → Update Agent

- **Critical path:** The agent selects an action, which is filtered through rule-based constraints. The environment executes the action, and the system calculates distance from data-driven norm predictions to update reputation. The reputation score then scales the received reward before updating the agent's policy.

- **Design tradeoffs:**
  - **τ (Tolerance):** High τ tolerates large deviations from social norms; low τ creates strict reputation drops for minor errors
  - **α (Forgiveness):** High α recovers reputation quickly (risking repeat violations); low α enforces strict compliance (risking sparse reward signals)

- **Failure signatures:**
  - "The Zeno Trap": Agent moves infinitesimally slowly to avoid reputation drops
  - "Anti-Social Safety": Agent follows rules perfectly but ignores social norms, creating abrupt movements
  - "Catastrophic Forgetting": Agent ignores reputation fluctuations if penalties aren't significant enough

- **First 3 experiments:**
  1. Grid World Sanity Check: Implement toy example and verify policy selection matches Table 1 as α decreases
  2. Junction Ablation (RB Only): Train agent using only rule-based shield to confirm unsocial behavior
  3. Norm Conflict Test: Construct scenario where R_B permits actions D_D hates to verify reputation penalties

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can automatic tuning methods be developed to optimally set τ and α hyperparameters without manual experimentation?
- Basis in paper: The authors state automatic tuning methods for finding τ, α hyperparameter values could be another direction for future work.
- Why unresolved: Currently set via heuristics and trial-and-error requiring human designer intervention.
- What evidence would resolve it: An algorithm capable of dynamically adjusting τ and α during training to achieve statistical indistinguishability without prior manual tuning.

### Open Question 2
- Question: How can HAVA be extended to distinguish antisocial but legal behaviors from social legal behaviors?
- Basis in paper: The authors identify HAVA's inability to distinguish antisocial but legal/safe behaviors from social legal/safe behaviors.
- Why unresolved: The current data-driven component may learn norms that are compliant with rules but statistically under-represented or considered undesirable.
- What evidence would resolve it: A modification incorporating behavior frequency to penalize rare but technically legal actions, validated against broader social acceptability metrics.

### Open Question 3
- Question: Is HAVA robust to noise and misaligned data present in real-world human datasets?
- Basis in paper: The paper notes data-driven approaches are sensitive to misaligned data and the experiments used simulated rather than noisy real-world logs.
- Why unresolved: Experiments relied on controlled simulation where humans followed specific models; it's unclear if reputation mechanism would collapse with sparse, noisy real data.
- What evidence would resolve it: Successful replication using real-world human driving trajectories containing inherent noise and outliers.

## Limitations
- Neural network architecture details (DQN and DD) are not fully specified, affecting reproducibility
- State space representation for SUMO is not completely detailed
- Reputation mechanism's robustness to noisy norm data from D_D is not thoroughly tested
- Evaluation focuses primarily on trajectory similarity without assessing long-term safety or robustness to environmental changes

## Confidence
- **High Confidence:** Core mechanism of reputation-weighted rewards effectively balancing rule-based and data-driven norms
- **Medium Confidence:** Claim that HAVA produces policies statistically indistinguishable from human behavior (limited to one traffic scenario)
- **Medium Confidence:** Assertion that ablations lead to either safety violations or unsocial behavior (based on controlled experiments)
- **Low Confidence:** Generalization to other domains and complex multi-agent scenarios is not demonstrated

## Next Checks
1. **Noise Sensitivity Test:** Evaluate HAVA's performance when D_D is trained on noisy or inconsistent human data to assess reputation mechanism robustness
2. **Long-Term Safety Validation:** Run extended simulations (10x longer than current) to check if HAVA maintains safety compliance over time and doesn't develop emergent violations
3. **Cross-Domain Transfer:** Apply HAVA to a different domain (e.g., robotic navigation with different norm types) to validate framework's generalizability beyond traffic scenarios