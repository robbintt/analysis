---
ver: rpa2
title: 'Table Understanding and (Multimodal) LLMs: A Cross-Domain Case Study on Scientific
  vs. Non-Scientific Data'
arxiv_id: '2507.00152'
source_url: https://arxiv.org/abs/2507.00152
tags:
- table
- united
- data
- tables
- html
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study evaluates the performance of large language models
  (LLMs) and multimodal LLMs (MLLMs) on table understanding (TU) tasks across scientific
  and non-scientific domains using diverse table formats. The authors introduce TableEval,
  a benchmark of 3,017 tables from scholarly publications, Wikipedia, and financial
  reports, represented in five formats: Image, Dictionary, HTML, XML, and LATEX.'
---

# Table Understanding and (Multimodal) LLMs: A Cross-Domain Case Study on Scientific vs. Non-Scientific Data

## Quick Facts
- arXiv ID: 2507.00152
- Source URL: https://arxiv.org/abs/2507.00152
- Reference count: 40
- Primary result: Scientific tables degrade LLM/MLLM performance by 17-34% compared to non-scientific tables across all modalities

## Executive Summary
This study evaluates large language models and multimodal large language models on table understanding tasks across scientific and non-scientific domains using five table formats. The authors introduce TableEval, a benchmark of 3,017 tables from scholarly publications, Wikipedia, and financial reports, represented in Image, Dictionary, HTML, XML, and LATEX formats. Experiments on six datasets reveal that while models remain robust across table modalities, their performance significantly declines on scientific tables compared to non-scientific ones. Interpretability analysis using gradient-based saliency maps provides insights into model decision-making, highlighting differences in attention to table content and tokenization effects.

## Method Summary
The study uses zero-shot evaluation on TableEval, a benchmark containing 3,017 tables from 6 source datasets (ComTQA, numericNLG, SciGen, LogicNLG, Logic2Text) across 5 formats. Four MLLMs (Gemini-2.0-Flash, LLaVA-NeXT, Qwen2.5-VL, Idefics3) and four LLMs (Gemini, Mistral-Nemo, Qwen2.5, Llama-3) are evaluated using 9 metrics including BertScore, ROUGE, and BLEURT. Interpretability is performed using Inseq's Input×Gradient method on failure cases. The evaluation pipeline and dataset are available on GitHub and Hugging Face.

## Key Results
- Model performance degrades by 17-34% on scientific tables compared to non-scientific tables
- Input format (Image, HTML, XML, LaTeX, Dict) has minimal impact (<4% variance) on model performance
- Tokenization granularity significantly affects numerical reasoning accuracy, with digit-level tokenization preserving numerical differences better than multi-digit tokenization

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Scientific tables degrade model performance by 17-34% compared to non-scientific tables.
- Mechanism: Scientific tables require domain-specific knowledge, complex arithmetic reasoning, and synthesis of entire table contents, whereas non-scientific tables often involve single-statement logic over subsets of rows.
- Core assumption: The performance gap stems from both task complexity and sparsity of scientific tabular data in pretraining corpora.
- Evidence anchors:
  - [abstract] "their performance significantly declines on scientific tables compared to non-scientific ones"
  - [page 6] SciGen requires summarizing "the entire table content and involves multiple types of reasoning," whereas LogicNLG "often focuses on a subset of table rows and is associated with a single type of reasoning"
  - [corpus] Yang et al. (2025) confirm "scientific tables present challenges to multimodal LLMs"

### Mechanism 2
- Claim: Input format has minimal impact on LLM performance, with maximum ~4% variation across text formats.
- Mechanism: LLMs encounter these serialization formats during large-scale pretraining, making them invariant to surface-level structural differences.
- Core assumption: Pretraining exposure normalizes format-specific token patterns and structural encodings.
- Evidence anchors:
  - [page 5] "we do not observe a large variation in results across LLMs and the four text formats, with the maximum gap equal to about 4%"
  - [page 5] "Such outcomes may be attributed to LLMs' exposure to data encoded in the given formats during pretraining"
  - [corpus] No strong corpus evidence contradicts this; Deng et al. (2024) report similar format robustness for Gemini/GPT-4

### Mechanism 3
- Claim: Tokenization granularity influences numerical reasoning accuracy.
- Mechanism: Digit-level tokenization (e.g., Mistral-Nemo) preserves marginal numerical differences better than multi-digit tokenization (e.g., Llama3), which can truncate year values across token boundaries.
- Core assumption: Token boundary alignment with numerical semantics affects a model's ability to compare values.
- Evidence anchors:
  - [page 8] "Mistral-Nemo splits all numbers into single digits, Llama3 often uses three-digit tokens where the fourth digit of a year is cut off. We assume that this makes it harder for Llama3 to process the marginal differences correctly"
  - [page 8] Saliency maps show Llama3 places high attribution on prompt boundaries and row labels, while Mistral-Nemo focuses on relevant year columns
  - [corpus] No corpus papers directly address tokenization effects on tables; this is an original contribution

## Foundational Learning

- Concept: **Table Understanding (TU) Tasks**
  - Why needed here: The paper evaluates QA and table-to-text generation across domains; understanding task definitions is prerequisite to interpreting performance metrics.
  - Quick check question: Can you distinguish a QA task (ComTQA) from a T2T task (SciGen) based on their output requirements?

- Concept: **Feature Attribution via Gradient-Based Saliency**
  - Why needed here: The paper uses Inseq's Input×Gradient method to visualize token importance; understanding this interpretability technique is essential for diagnosing model failures.
  - Quick check question: What does a high saliency score on a table cell indicate about the model's reasoning path?

- Concept: **Zero-Shot Evaluation**
  - Why needed here: All experiments use zero-shot prompts with default hyperparameters; this design choice affects generalization claims.
  - Quick check question: Why might zero-shot evaluation understate a model's true TU capabilities compared to few-shot?

## Architecture Onboarding

- Component map:
  Data Layer (TableEval corpus) -> Model Layer (4 MLLMs + 4 LLMs) -> Evaluation Layer (9 metrics) -> Interpretability Layer (Inseq with Input×Gradient)

- Critical path: Format conversion (PDF→Image/HTML/XML/LaTeX/Dict) → Prompt construction (format-agnostic) → Zero-shot inference → Metric computation → Saliency visualization on failure cases

- Design tradeoffs:
  - Format-agnostic prompts: Excludes format headers for capability assessment but may reduce performance; alternative is format-aware prompting
  - No output postprocessing: Preserves raw model behavior but hurts n-gram overlap metrics; alternative is constrained decoding via Jsonformer
  - Instance-level interpretability: Reveals fine-grained failures but limits statistical generalization; dataset-level aggregation flattens nuances

- Failure signatures:
  - Scientific tables: 20-30% BLEU/ROUGE drop vs. non-scientific; dense numerical columns with high attribution sparsity
  - Llama3 on years: High attribution on prompt boundaries, low on numeric columns; tokenization splits multi-digit years
  - Long-form T2T: Low BLEURT scores (−0.79 on SciGen) with high variance across formats

- First 3 experiments:
  1. Format sensitivity baseline: Run Qwen2.5-7B on all 5 formats for SciGen; expect <5% variance if mechanism holds
  2. Tokenization ablation: Compare Mistral-Nemo vs. Llama-3 on ComTQA year-comparison questions; quantify digit-token vs. multi-digit-token error rates
  3. Scientific domain probe: Evaluate Gemini-2.0-Flash on scientific vs. non-scientific tables with identical schema complexity; if gap persists, domain knowledge—not structure—is the bottleneck

## Open Questions the Paper Calls Out

- To what extent does the performance decline on scientific tables stem from data complexity versus the lack of scientific data in pre-training corpora?
- How does enforcing structured outputs or using model-specific prompts affect the accuracy of open-source models on table understanding tasks?
- How do multimodal interpretability methods like CC-SHAP quantify the importance of the visual modality compared to text for MLLMs processing tables?
- Do the cross-domain robustness and performance gaps observed in English generalize to multilingual table understanding?

## Limitations

- Hyperparameter specification is unclear, with only "default hyperparameters" mentioned for zero-shot evaluation
- Interpretability method details lack specific configuration parameters (layer selection, aggregation method)
- Scientific vs. non-scientific domain generalization claims lack quantification of actual pretraining exposure to scientific tabular data
- The benchmark is limited to English, preventing assessment of multilingual table understanding capabilities

## Confidence

- **High Confidence**: Format robustness claims - The finding that input format has minimal impact (<4% variance) is well-supported by experimental results across multiple models and metrics
- **Medium Confidence**: Scientific table performance degradation - The 17-34% performance drop is clearly demonstrated, but the attribution to domain complexity vs. pretraining data sparsity is inferred rather than empirically validated
- **Low Confidence**: Tokenization effects on numerical reasoning - This is an original contribution with limited empirical support; the attribution to tokenization granularity is hypothesized based on observed saliency patterns rather than controlled experiments

## Next Checks

1. Controlled format sensitivity test: Run Qwen2.5-7B on all 5 formats for a scientific subset (SciGen) with fixed hyperparameters to confirm <5% variance

2. Tokenization ablation study: Systematically compare Mistral-Nemo vs. Llama3 on ComTQA year-comparison questions with controlled numerical inputs, quantifying digit-token vs. multi-digit-token error rates

3. Pretraining data analysis: Extract and analyze the scientific tabular content proportion in the pretraining corpora of evaluated models to empirically validate whether pretraining data sparsity explains the scientific vs. non-scientific performance gap