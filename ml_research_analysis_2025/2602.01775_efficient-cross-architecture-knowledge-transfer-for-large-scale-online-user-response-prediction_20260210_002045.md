---
ver: rpa2
title: Efficient Cross-Architecture Knowledge Transfer for Large-Scale Online User
  Response Prediction
arxiv_id: '2602.01775'
source_url: https://arxiv.org/abs/2602.01775
tags:
- uni00000013
- uni00000011
- knowledge
- teacher
- online
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses high model switching costs in large-scale
  user response prediction systems, where deploying new architectures requires expensive
  retraining and suffers performance degradation due to data retention constraints.
  The proposed CrossAdapt framework solves this through a two-stage approach: offline
  cross-architecture transfer using dimension-adaptive embedding projections and progressive
  network distillation, combined with strategic sampling to reduce computational cost;
  and online adaptive co-distillation with asymmetric teacher-student updates and
  distribution-aware historical knowledge preservation.'
---

# Efficient Cross-Architecture Knowledge Transfer for Large-Scale Online User Response Prediction

## Quick Facts
- **arXiv ID:** 2602.01775
- **Source URL:** https://arxiv.org/abs/2602.01775
- **Reference count:** 40
- **Primary result:** 0.27-0.43% AUC improvements while reducing training time by 43-71% through cross-architecture transfer framework

## Executive Summary
This paper addresses the high computational and performance costs of switching between model architectures in large-scale user response prediction systems. The proposed CrossAdapt framework enables rapid deployment of new architectures through dimension-adaptive embedding projections and progressive distillation, combined with online adaptive co-distillation. The approach preserves embedding relationships through mathematical projection (PCA/QR) and uses asymmetric teacher-student updates with distribution-aware historical knowledge preservation. Large-scale deployment on Tencent WeChat Channels demonstrates significant reductions in AUC degradation, LogLoss increase, and prediction bias compared to standard distillation baselines.

## Method Summary
CrossAdapt uses a two-stage approach: offline cross-architecture transfer and online adaptive co-distillation. The offline stage employs dimension-adaptive embedding projections (PCA for dimension reduction, QR decomposition for expansion) to transfer embedding tables without iterative training, followed by progressive network distillation with a two-phase training strategy that freezes embeddings initially to stabilize the randomly-initialized interaction network. Strategic sampling (10% of training data) reduces computational cost. The online stage implements asymmetric teacher-student updates where the student updates every step while the teacher updates every τ steps, combined with distribution shift detection via JS divergence to dynamically adjust historical sample mixing ratios for knowledge preservation.

## Key Results
- 0.27-0.43% AUC improvements on three public datasets (Criteo, Avazu, KDD12)
- 43-71% reduction in training time through strategic sampling
- Significant reductions in AUC degradation, LogLoss increase, and prediction bias in large-scale deployment on Tencent WeChat Channels (10M daily samples)
- Progressive distillation and dimension-adaptive projection each contribute 0.02-0.15% AUC gains

## Why This Works (Mechanism)

### Mechanism 1: Dimension-Adaptive Embedding Table Projection
If embedding dimensions differ between teacher and student, mathematical projection (PCA or orthogonal expansion) may enable near-lossless transfer of pairwise feature relationships without iterative training. Embedding semantics are preserved through inner products (similarity/affinity). PCA projects to principal components minimizing inner-product distortion; orthogonal expansion preserves all inner products exactly. This bypasses costly iterative embedding distillation. Core assumption: embeddings' value lies primarily in relational structure (inner products capturing feature affinities), which can be preserved via linear projection. Break condition: If student architecture fundamentally changes embedding semantics (e.g., different interaction mechanisms that invalidate inner-product relationships), projection-based transfer may degrade.

### Mechanism 2: Progressive Interaction Network Distillation
A two-phase training strategy (freeze embeddings → train interaction network → unfreeze all) may stabilize transfer by preventing noisy interaction-network gradients from corrupting projected embeddings. Phase 1 freezes projected embeddings, allowing the randomly-initialized interaction network to align with teacher outputs under stable feature representations. Phase 2 unfreezes for joint fine-tuning. This decouples optimization to avoid gradient interference. Core assumption: randomly initialized interaction networks produce high-variance gradients that can destabilize transferred embeddings if trained jointly from the start. Break condition: If student interaction network is very shallow or well-initialized, the benefit of progressive training may diminish.

### Mechanism 3: Online Asymmetric Co-Distillation with Distribution-Aware Adaptation
If online data exhibits distribution shift, a mechanism where the student updates continuously but the teacher updates infrequently (with historical sample injection based on shift detection) may balance rapid adaptation with knowledge preservation. Student updates at every step (captures new patterns); teacher updates every τ steps (maintains stable supervision). Distribution shift detection (e.g., JS divergence) dynamically adjusts historical sample ratio in batches: increase during stability, decrease during shifts. Core assumption: frequent teacher updates destabilize distillation targets; distribution shift can be reliably detected via feature-level divergence metrics. Break condition: If distribution shift is abrupt and not captured by windowed divergence metrics, the adaptation may lag.

## Foundational Learning

- **Knowledge Distillation (KD)**
  - Why needed here: Core technique for transferring knowledge from teacher to student. CrossAdapt extends KD with architecture-specific adaptations.
  - Quick check question: Can you explain the standard KD loss (L_KD) and why it alone may be insufficient for cross-architecture transfer?

- **Online Learning in Recommendation Systems**
  - Why needed here: The system operates on streaming data; online continual adaptation is required to maintain performance.
  - Quick check question: What are the primary challenges of online learning in user response prediction (e.g., data distribution shift, catastrophic forgetting)?

- **Embedding Tables in CTR/CVR Models**
  - Why needed here: Embeddings are the dominant parameter component; their efficient transfer is the main efficiency bottleneck addressed.
  - Quick check question: Why might direct parameter copying fail when embedding dimensions differ between teacher and student?

## Architecture Onboarding

- **Component map:** Dimension-Adaptive Embedding Projection → Strategic Sampling → Progressive Distillation (Phase 1 freeze, Phase 2 joint) → Distribution Shift Detector → Adaptive Historical Sampler → Asymmetric Teacher-Student Updater
- **Critical path:** The projection module is foundational—errors here propagate through both stages. Start by validating projection (inner product preservation) before full framework integration.
- **Design tradeoffs:**
  - Sampling ratio (r): Higher r improves offline performance but increases training time (Fig 6a). r=0.1 offers a practical balance.
  - Teacher update interval (τ): Larger τ increases stability but may slow teacher adaptation to genuine shifts. τ=10 is paper default.
  - Historical enhancement ratio (r_enh): Higher r_enh improves stability but may delay adaptation during shifts. r_enh up to 0.5 shows benefit (Fig 6e).
- **Failure signatures:**
  1. Projection failure: Sudden AUC drop on cold-start items (new IDs) if PCA discards important minor components.
  2. Stabilization failure: Training divergence or embedding collapse during offline progressive distillation if Phase 1 is skipped.
  3. Adaptation failure: Stagnant or declining online AUC if shift detection thresholds (θ_low, θ_high) are miscalibrated, causing excessive historical anchoring or insufficient.
- **First 3 experiments:**
  1. Validate embedding projection: Project teacher embeddings to student dimension and measure cosine similarity/pairwise inner-product distortion on a held-out feature subset. Compare to random projection baseline.
  2. Ablate progressive distillation: Run offline transfer with and without the two-phase freezing. Compare convergence speed and final AUC.
  3. Sensitivity analysis for online shift detection: Vary θ_low and θ_high on a dataset with known synthetic distribution shifts. Plot AUC over time vs. enhancement ratio behavior.

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions in a dedicated section. However, several limitations and areas for future work are implied:

1. Infrastructure limitations for deploying the adaptive historical knowledge preservation module in production systems
2. Theoretical guarantees for cross-architecture transfer when interaction mechanisms differ fundamentally
3. Automatic calibration of distribution shift detection thresholds for generalization across datasets

## Limitations

- Real-world deployment constraints may limit the moderate AUC improvements (0.27-0.43%) shown in controlled settings
- Reliance on JS divergence for shift detection may miss subtle concept drifts in rapidly changing distributions
- 10% sampling strategy could exclude important tail patterns in long-tailed feature distributions
- Large-scale deployment claims primarily demonstrated on single platform without independent verification

## Confidence

- **High Confidence:** Dimension-adaptive embedding projection mechanism (theoretically grounded with formal proof for inner-product preservation)
- **Medium Confidence:** Progressive interaction network distillation (empirically validated but theoretically heuristic)
- **Medium Confidence:** Online asymmetric co-distillation (practically effective but sensitive to parameter tuning)
- **Low Confidence:** Large-scale deployment claims (primarily demonstrated on single platform without independent verification)

## Next Checks

1. **Distribution Shift Robustness:** Apply CrossAdapt to datasets with known concept drift (e.g., varying class proportions over time) and measure AUC degradation when shift detection parameters are miscalibrated.

2. **Teacher-Student Gap Scaling:** Systematically increase architectural differences between teacher and student (embedding dimension ratios, interaction network depth) to identify when projection-based transfer breaks down.

3. **Ablation on Sampling Strategy:** Compare the 10% strategic sampling approach against alternative sampling methods (importance sampling, stratified sampling) to quantify the marginal benefit of the proposed sampling ratio.