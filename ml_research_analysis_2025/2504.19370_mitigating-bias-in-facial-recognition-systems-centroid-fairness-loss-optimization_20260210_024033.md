---
ver: rpa2
title: 'Mitigating Bias in Facial Recognition Systems: Centroid Fairness Loss Optimization'
arxiv_id: '2504.19370'
source_url: https://arxiv.org/abs/2504.19370
tags:
- fairness
- pre-trained
- training
- bias
- metrics
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses fairness bias in facial recognition systems
  across different demographic groups, which can compromise deployment despite high
  global accuracy. The authors propose a post-processing approach called Centroid
  Fairness that reduces bias without retraining the entire model.
---

# Mitigating Bias in Facial Recognition Systems: Centroid Fairness Loss Optimization

## Quick Facts
- **arXiv ID**: 2504.19370
- **Source URL**: https://arxiv.org/abs/2504.19370
- **Reference count**: 23
- **Primary result**: A post-processing method called Centroid Fairness that reduces demographic bias in facial recognition systems without retraining the original model

## Executive Summary
The paper addresses fairness bias in facial recognition systems across different demographic groups, which can compromise deployment despite high global accuracy. The authors propose a post-processing approach called Centroid Fairness that reduces bias without retraining the entire model. The method works by aligning intra-group performance curves (FRR and FAR) across subgroups using pseudo-score transformations based on centroid-based scores. A small neural network called the Fairness Module is trained to transform embeddings so that performance metrics align with a reference group.

## Method Summary
The Centroid Fairness approach is a post-processing method that reduces demographic bias in facial recognition systems without requiring retraining of the original model. It works by aligning intra-group performance curves (FRR and FAR) across subgroups using pseudo-score transformations based on centroid-based scores. The method trains a small neural network called the Fairness Module to transform embeddings so that performance metrics align with a reference group. This transformation is achieved by minimizing a centroid fairness loss that measures the distance between transformed subgroup scores and reference group scores.

## Key Results
- Reduces fairness metrics BFAR and BFRR while maintaining high recognition accuracy (ROC scores)
- Outperforms existing methods like PASS-s which sacrifice performance for fairness
- Demonstrates efficiency by requiring no retraining of the original model
- Shows robustness across different architectures and training sets

## Why This Works (Mechanism)
The Centroid Fairness method works by post-processing embeddings from a trained facial recognition model to align performance across demographic subgroups. It uses centroid-based scores to measure the average similarity of embeddings within each subgroup to their respective class centroids. The Fairness Module learns a transformation function that maps these scores to align with a reference group's performance curve. By minimizing the distance between transformed subgroup scores and reference group scores through the centroid fairness loss, the method ensures that false acceptance rates (FAR) and false rejection rates (FRR) are balanced across all demographic groups.

## Foundational Learning
- **Centroid-based scores**: The average similarity of embeddings to class centroids within subgroups; needed for measuring intra-group performance; quick check: verify centroid computation for each subgroup
- **Fairness Module**: A small neural network that transforms embeddings; needed to learn the score alignment function; quick check: confirm module architecture is minimal compared to base model
- **Centroid Fairness Loss**: Measures distance between transformed subgroup scores and reference group scores; needed to guide the alignment process; quick check: validate loss decreases during training
- **FRR/FAR curves**: False Rejection Rate and False Acceptance Rate as functions of score thresholds; needed to quantify performance across subgroups; quick check: plot curves for all subgroups before and after transformation
- **Reference group selection**: Choosing one demographic group as the target for alignment; needed to establish the performance baseline; quick check: test with different reference groups to assess sensitivity

## Architecture Onboarding

**Component Map:**
Facial Recognition Model -> Fairness Module -> Centroid Fairness Loss -> Optimized Fairness Module

**Critical Path:**
1. Extract embeddings from facial recognition model
2. Compute centroid-based scores for each subgroup
3. Apply Fairness Module transformation
4. Calculate centroid fairness loss
5. Update Fairness Module parameters

**Design Tradeoffs:**
- Post-processing vs. retraining: Post-processing preserves original model performance while reducing bias, but may be less optimal than joint training
- Reference group dependency: Aligning to one group may introduce bias toward that group's characteristics
- Privacy considerations: Requires subgroup membership information for centroid computation

**Failure Signatures:**
- Poor fairness metrics indicate insufficient transformation learning
- Significant drop in overall accuracy suggests over-regularization of the Fairness Module
- Inconsistent performance across different subgroups may indicate reference group bias

**3 First Experiments:**
1. Apply the method to a baseline ArcFace model on the RFW dataset and measure changes in BFAR and BFRR
2. Test different reference group selections to evaluate sensitivity to this choice
3. Measure the computational overhead of the Fairness Module compared to the base model

## Open Questions the Paper Calls Out
None

## Limitations
- Limited validation to only two specific facial recognition architectures (ArcFace, CurricularFace)
- Only tested on three datasets focusing on demographic-based subgrouping
- Does not address privacy concerns with centroid-based score computation requiring subgroup-specific data
- Reliance on reference group for alignment may introduce bias toward that group's characteristics

## Confidence

**Confidence Labels:**
- **High**: The core algorithmic approach and its theoretical foundation are sound
- **Medium**: Performance claims on tested datasets and architectures
- **Medium**: Claims about computational efficiency and deployment feasibility
- **Low**: Claims about robustness to different types of bias beyond demographic subgroups

## Next Checks
1. Test the method on additional facial recognition architectures (e.g., CosFace, AdaFace) and datasets with different demographic distributions to verify claimed robustness
2. Evaluate performance when subgroup membership information is partially or fully unavailable, testing privacy-preserving variants
3. Assess the method's effectiveness on non-demographic biases (age, pose, expression, illumination variations) that commonly affect facial recognition systems