---
ver: rpa2
title: 'Clustering Discourses: Racial Biases in Short Stories about Women Generated
  by Large Language Models'
arxiv_id: '2509.02834'
source_url: https://arxiv.org/abs/2509.02834
tags:
- stories
- women
- black
- white
- about
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study combines computational clustering with qualitative discourse
  analysis to examine racial and gender biases in Portuguese short stories generated
  by LLaMA 3.2-3B. The researchers used feature embeddings to cluster 2,100 stories
  and manually analyzed 18 representative texts.
---

# Clustering Discourses: Racial Biases in Short Stories about Women Generated by Large Language Models

## Quick Facts
- arXiv ID: 2509.02834
- Source URL: https://arxiv.org/abs/2509.02834
- Reference count: 3
- Primary result: SVM classifiers achieved 94.89-100% accuracy in predicting protagonist race from Portuguese short stories generated by LLaMA 3.2-3B

## Executive Summary
This study combines computational clustering with qualitative discourse analysis to examine racial and gender biases in Portuguese short stories generated by LLaMA 3.2-3B. The researchers used feature embeddings to cluster 2,100 stories and manually analyzed 18 representative texts. Three narrative patterns emerged: overcoming social barriers, ancestral mythification, and subjective self-realization. Black women were often portrayed as strong, resilient figures facing adversity, while white women were more often depicted in introspective, self-discovery narratives.

## Method Summary
The researchers generated 2,100 short stories using LLaMA 3.2-3B with prompts varying by protagonist race and name. They transformed stories into 1024-dimensional vectors using BGE M3 embeddings, then applied DBSCAN clustering and validated results with SVM classifiers and VRC metrics. Eighteen stories were manually analyzed through Discursive Sequences of Reference (DSR) methodology to identify thematic patterns. Word clouds and lexical analysis complemented the quantitative findings.

## Key Results
- SVM classifiers predicted character race with up to 100% accuracy from story embeddings
- DBSCAN yielded three interpretable clusters with VRC score of 0.70
- Cluster 0 (Social Overcoming) and Cluster 1 (Ancestral Mythification) were predominantly Black stories, while Cluster 2 (Self-Realization) was 98% white
- Distinct lexical patterns emerged: "community/force" for Black women vs. "soul/discovery" for white women

## Why This Works (Mechanism)

### Mechanism 1: Semantic Separability of Racialized Prompts
- **Claim:** Vector embeddings of LLM-generated narratives contain sufficient signal to linearly separate stories by the protagonist's race.
- **Mechanism:** When an LLM generates text conditioned on racial markers (e.g., "Black woman" vs. "white woman"), it draws from semantically distinct regions of its training data. These differences manifest in the vector space, allowing a Support Vector Machine (SVM) to distinguish between them with high accuracy.
- **Core assumption:** The embedding model (BGE M3) captures the semantic nuance of bias (lexical choices, themes) and not just surface-level grammar.
- **Evidence anchors:**
  - [abstract] Mentions identifying "grammatically coherent, seemingly neutral texts" that materialize specific framings.
  - [section 4.1] Reports classifiers averaged 94.89% accuracy, peaking at 100%, indicating the features are highly predictive of the prompt's racial category.
  - [corpus] Related work (e.g., "Yet another algorithmic bias...") supports the premise that LLMs reinforce dominant discourses, though this specific high-accuracy separability result is specific to this paper's dataset.
- **Break condition:** If the classifier accuracy dropped to near-chance (50%), it would imply the narratives are semantically indistinguishable regarding race, breaking the link between prompt condition and narrative space.

### Mechanism 2: Clustering as Topical Segregation
- **Claim:** Unsupervised clustering algorithms naturally partition the story corpus into archetypes that align with racialized narrative tropes (resistance vs. introspection).
- **Mechanism:** By grouping stories based on embedding proximity, the algorithm reveals that the model maps "Black woman" prompts to neighborhoods of social struggle or mysticism, and "white woman" prompts to neighborhoods of individual emotional growth.
- **Core assumption:** Proximity in the vector embedding space corresponds to thematic and narrative similarity (semantic cohesion).
- **Evidence anchors:**
  - [abstract] States the method "identifies three narrative clusters: social overcoming, ancestral mythification, and subjective self-realization" and their correlation with character race.
  - [section 4.2.1] Details how Cluster 0 (Social Overcoming) and Cluster 1 (Ancestral Mythification) are predominantly Black stories, while Cluster 2 (Self-Realization) is 98% white.
  - [corpus] External papers like "Examining the Robustness of Homogeneity Bias" corroborate the tendency of models to represent groups homogeneously, supporting the clustering outcome.
- **Break condition:** If clusters contained a roughly 50/50 mix of Black and white protagonist stories, the mechanism of topical segregation by race would be invalid.

### Mechanism 3: Lexical-Adjective Anchoring
- **Claim:** Distinct adjectives and lexical fields act as primary features for both the clustering and the human-perceived bias, serving as linguistic markers of inequality.
- **Mechanism:** The LLM associates specific adjectival sets with racial groups (e.g., "strong/resilient" vs. "anxious/creative"). These lexical choices serve as high-weight features in the vector representation, driving the computational separation and qualitative interpretation.
- **Core assumption:** Word frequency and adjective usage are reliable proxies for underlying ideological framing or "colonial" discourses.
- **Evidence anchors:**
  - [section 4.2.2] Word clouds show "community/force" for Black women vs. "soul/discovery" for white women; adjectives like "magical/wise" vs. "anxious/sad" reinforce the split.
  - [abstract] Notes the framing of the "female body" and "crystallized" inequality.
  - [corpus] Assumption supported by general NLP bias literature, though specific adjective lists are unique to this study's Portuguese corpus.
- **Break condition:** If lexical analysis showed overlapping vocabulary with no distinct adjectival patterns between groups, the discursive hypothesis would lack linguistic grounding.

## Foundational Learning

- **Concept: DBSCAN (Density-Based Spatial Clustering of Applications with Noise)**
  - **Why needed here:** Unlike K-Means, DBSCAN does not require pre-specifying the number of clusters and can identify outliers. This was crucial for finding the natural narrative structure (3 clusters) without forcing the data into arbitrary groups.
  - **Quick check question:** How does DBSCAN handle the "outliers" mentioned in Section 4.1, and why might these stories be analytically important or irrelevant?

- **Concept: Discursive Sequences of Reference (DSR)**
  - **Why needed here:** This is the qualitative unit of analysis used to bridge computational clustering and human interpretation. Understanding DSRs is necessary to see how the authors moved from vector math to claims about "colonial framing."
  - **Quick check question:** How does extracting a DSR from a cluster centroid differ from simply reading random stories in that cluster?

- **Concept: UMAP (Uniform Manifold Approximation and Projection)**
  - **Why needed here:** Used to reduce 1024-dimension vectors to 2D for visualization (Figure 2). It preserves the local structure of data better than PCA, making the visual separation of racial clusters apparent.
  - **Quick check question:** Why is UMAP preferred over t-SNE or PCA for visualizing the semantic relationships between the story clusters in this context?

## Architecture Onboarding

- **Component map:** Generator (LLaMA 3.2-3B + Prompts) -> Encoder (BGE M3) -> Clusterer (DBSCAN) -> Validator (SVM + VRC) -> Visualizer (UMAP) -> Analyst (Human DSR review)

- **Critical path:** The validity of the qualitative claim rests on the **SVM Classification Accuracy**. If the vectors were not separable (i.e., SVM failed), the subsequent clustering would likely show no racial stratification, rendering the discourse analysis a study of random noise rather than systemic bias.

- **Design tradeoffs:**
  - **Scale vs. Depth:** The authors analyzed 2,100 stories computationally but only 18 stories qualitatively (top/bottom of clusters). This risks missing nuance in the "outlier" group or average stories not at the extremes.
  - **Language Specificity:** Focusing on Portuguese captures local colonial context but limits direct applicability to English-centric bias benchmarks.

- **Failure signatures:**
  - **High VRC but Low Purity:** Clusters are mathematically distinct but racially mixed (e.g., Cluster 0 is 50% Black/50% White). This would indicate thematic variety is independent of race.
  - **SVM Overfitting:** If the SVM only memorized specific names rather than semantic content, the high accuracy would be a metric artifact.

- **First 3 experiments:**
  1. **Ablation on Prompts:** Run the pipeline with gender-only prompts (removing race) to see if the "Social Overcoming" vs. "Self-Realization" clusters persist or collapse.
  2. **Encoder Sensitivity:** Swap BGE M3 for a monolingual Portuguese encoder to test if the clustering holds or if the bias signal is encoder-dependent.
  3. **Centroid Analysis:** Instead of manual selection, extract the top 10 keywords (TF-IDF) from each cluster to computationally verify the "Community" vs. "Self" lexical divide.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do the identified discursive clusters (social overcoming vs. self-realization) persist when generating stories in English compared to Portuguese?
- Basis in paper: [explicit] The authors explicitly propose "a comparative analysis between stories generated in English and Portuguese" in the conclusion.
- Why unresolved: The current study restricted its corpus generation and analysis solely to the Portuguese language.
- What evidence would resolve it: Running the identical prompt templates in English and applying the same clustering and qualitative analysis pipeline to compare resulting narrative themes.

### Open Question 2
- Question: To what extent do character names introduce a distinct "nationality bias" independent of the racial markers (Black/white) analyzed?
- Basis in paper: [explicit] The authors call for a "deeper investigation into the influence of character names â€” particularly about activating a third type of bias: nationality."
- Why unresolved: While names were standardized inputs, the specific interaction between the name list (derived from 105 countries) and the narrative output was not isolated from the racial variable in this study.
- What evidence would resolve it: Ablation studies controlling for name origin (e.g., using names strongly associated with specific countries) while holding the race prompt constant to observe shifts in narrative context.

### Open Question 3
- Question: Are the specific narrative clusters identified (mythification and social overcoming for Black women; self-realization for white women) consistent across different LLM architectures?
- Basis in paper: [inferred] The methodology relies exclusively on LLaMA 3.2-3B.
- Why unresolved: It is unclear if the observed discursive segregation is an artifact of LLaMA's specific training data or a systemic feature of large language models in general.
- What evidence would resolve it: Replicating the exact methodology using distinct model families (e.g., GPT, PaLM) and comparing the resulting cluster compositions and discursive patterns.

## Limitations

- The study's findings are based on a single LLM (LLaMA 3.2-3B) and may not generalize to other models or languages
- The manual qualitative analysis covers only 18 stories (0.9% of corpus), potentially missing patterns in the broader dataset
- The study focuses on Portuguese narratives, limiting direct comparison with existing English-language bias research
- No analysis of how different prompting strategies might influence bias manifestations

## Confidence

- **High Confidence:** SVM classification accuracy (94.89-100%) and DBSCAN clustering validity (VRC = 0.70) demonstrate robust computational separation
- **Medium Confidence:** The three identified narrative clusters align with racial groups, though the small qualitative sample size introduces uncertainty
- **Medium Confidence:** Lexical patterns show distinct adjective usage between groups, but the interpretation of these as "essentialized representations" requires careful contextual consideration

## Next Checks

1. **Cross-model validation:** Run the same pipeline with different LLMs (GPT, Claude, Mistral) to test if the racial narrative patterns persist across architectures
2. **Prompt variation study:** Systematically vary prompt wording while holding race constant to determine how sensitive the bias patterns are to prompt engineering
3. **Temporal stability test:** Generate new story sets at different time points to assess whether these bias patterns remain stable or evolve as models are updated