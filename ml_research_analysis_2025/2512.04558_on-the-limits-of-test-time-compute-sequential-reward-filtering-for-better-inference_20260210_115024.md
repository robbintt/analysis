---
ver: rpa2
title: 'On the Limits of Test-Time Compute: Sequential Reward Filtering for Better
  Inference'
arxiv_id: '2512.04558'
source_url: https://arxiv.org/abs/2512.04558
tags:
- arxiv
- reward
- have
- algorithm
- rf-seqbon
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper establishes the fundamental limits of test-time compute
  (TTC) for large language models (LLMs) and demonstrates that standard best-of-n
  (BoN) sampling is inherently suboptimal. The authors analyze a mixture-of-reference-policies
  model where the LLM pretraining data consists of trajectories from multiple policies.
---

# On the Limits of Test-Time Compute: Sequential Reward Filtering for Better Inference

## Quick Facts
- arXiv ID: 2512.04558
- Source URL: https://arxiv.org/abs/2512.04558
- Authors: Yue Yu; Qiwei Di; Quanquan Gu; Dongruo Zhou
- Reference count: 9
- Key outcome: This paper establishes the fundamental limits of test-time compute (TTC) for large language models (LLMs) and demonstrates that standard best-of-n (BoN) sampling is inherently suboptimal.

## Executive Summary
This paper establishes the fundamental limits of test-time compute (TTC) for large language models (LLMs) and demonstrates that standard best-of-n (BoN) sampling is inherently suboptimal. The authors analyze a mixture-of-reference-policies model where the LLM pretraining data consists of trajectories from multiple policies. They prove that any sequential sample-and-evaluate algorithm requires fewer samples than BoN to achieve near-optimal performance, establishing a separation between parallel and sequential TTC. To address this gap, the authors propose Reward-Filtered Sequential Best-of-n (RF-SeqBoN), which selectively incorporates only high-reward generations into the context. Under mild assumptions on the reward model, RF-SeqBoN provably achieves strictly better sample complexity than BoN. Extensive experiments across diverse benchmarks (MATH500, GPQA-Diamond, AMC'23, AIME'24/25) using Qwen3-4B-Instruct and Qwen3-0.6B-Thinking models demonstrate consistent improvements over vanilla BoN and pure sequential baselines, validating the theoretical advantages in practice.

## Method Summary
The paper analyzes test-time compute (TTC) for LLMs using a mixture-of-reference-policies framework. The authors prove that standard best-of-n (BoN) sampling is statistically suboptimal compared to sequential methods, which can leverage information from previous samples to reshape the sampling distribution. They propose Reward-Filtered Sequential Best-of-n (RF-SeqBoN), which selectively appends only high-reward generations to the context, shifting the model's distribution toward the optimal reference policy more efficiently. Under mild assumptions on the reward model, RF-SeqBoN provably achieves strictly better sample complexity than BoN. The method is validated through extensive experiments on diverse benchmarks using Qwen3 models.

## Key Results
- RF-SeqBoN achieves strictly better sample complexity than vanilla BoN under mild assumptions on the reward model.
- Extensive experiments across MATH500, GPQA-Diamond, AMC'23, and AIME'24/25 benchmarks demonstrate consistent improvements over vanilla BoN and pure sequential baselines.
- PRM (Process Reward Model) outperforms ORM (Outcome Reward Model) for the filtering mechanism.
- Optimal filtering threshold γ requires tuning (e.g., 0.90-0.97 for MATH); too high or too low degrades performance.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Selectively appending high-reward generations to the context shifts the model's sampling distribution toward the optimal reference policy more efficiently than parallel sampling.
- **Mechanism:** The paper models the LLM as a mixture of reference policies. When the context contains only actions that exceed a reward threshold γ, the posterior probability of the optimal reference policy τ* increases (Bayesian update). This reduces the effective "coverage gap" (EM-divergence) for subsequent samples, requiring fewer total samples to find the optimal action than if sampling from the base mixture distribution.
- **Core assumption:** Assumption 5.4 (High-reward actions are statistically distinguishable and more likely under the optimal reference policy τ*) and Assumption 3.5 (Realizability/LLM as a Bayesian aggregator).
- **Evidence anchors:** Section 5 states "This procedure progressively refines the generation distribution towards that of the optimal policy... π_LLM(·|h) → π_ref^τ*(·|x)." Theorem 5.5 establishes the sample complexity bound showing strict improvement over parallel TTC under the filtering regime.

### Mechanism 2
- **Claim:** Parallel Best-of-N (BoN) is statistically suboptimal compared to sequential methods because it cannot leverage the information gained from previous samples to reshape the sampling distribution.
- **Mechanism:** BoN draws n independent samples from the base policy π_LLM. Theoretical analysis via Theorem 4.3 shows that the lower bound for sequential algorithms (which can condition on history) is strictly smaller than the lower bound for parallel algorithms (M_τ* ≤ M_LLM). Sequential feedback allows the model to "switch" to a better reference policy embedded in its pretraining.
- **Core assumption:** The pretraining data is a mixture of policies (some optimal, some not), allowing the LLM to recognize and emulate the optimal one given the right context (In-Context Learning).
- **Evidence anchors:** Abstract states "...proving that standard BoN is inherently suboptimal." Section 4's Theorem 4.3 proves a strict separation in sample complexity between parallel and sequential sample-and-evaluate frameworks.

### Mechanism 3
- **Claim:** Pure sequential revision (keeping all history) degrades performance due to noise accumulation; reward filtering acts as a "noise gate" to maintain context quality.
- **Mechanism:** Unfiltered sequential inference (PureSeq) forces the model to condition on low-quality previous attempts, potentially reinforcing errors (degeneration). RF-SeqBoN uses a threshold γ to block low-reward samples from the context window, ensuring the "context stream" consists only of potential solutions, thereby stabilizing the inference path.
- **Core assumption:** The reward signal is a reliable proxy for solution quality (verifiable rewards).
- **Evidence anchors:** Section 6 states "PureSeq... accumulates both useful and noisy answers, which makes it less stable than RF-SeqBoN." Figure 3a's ablation on γ shows that removing the filter (γ → 0) or over-filtering (γ → 1) degrades results.

## Foundational Learning

- **Concept: In-Context Learning (ICL) as Bayesian Inference**
  - **Why needed here:** The paper's theoretical backbone relies on the model being a mixture of reference policies (Section 3). To understand why high-reward history helps, you must grasp that providing examples in the context updates the model's "posterior" over which policy (e.g., "expert mathematician" vs. "casual writer") to emulate.
  - **Quick check question:** Can you explain how a prompt with specific examples shifts a model's output distribution without weight updates?

- **Concept: Best-of-N (BoN) Sampling & Reward Models**
  - **Why needed here:** This is the baseline being improved upon. You need to understand the cost (N samples) vs. benefit (selecting the max reward) trade-off to appreciate why RF-SeqBoN offers better "budget efficiency."
  - **Quick check question:** How does the performance of BoN scale with the rarity of the correct answer in the base model's distribution?

- **Concept: EM-Divergence (Exponentiated Mixture Divergence)**
  - **Why needed here:** The paper uses EM-divergence (M_ε) to quantify the "hardness" of a query. Theoretical claims about "strict improvement" are expressed in terms of reducing this divergence via sequential filtering.
  - **Quick check question:** If a policy has high EM-divergence relative to the optimal policy, does it require more or fewer samples to find the optimal action?

## Architecture Onboarding

- **Component map:** Prompt Wrapper -> Generator (LLM) -> Verifier (RM) -> Controller
- **Critical path:**
  1. Initialize history h̄ ← ⟨x⟩.
  2. Sample: Generate a_i ~ π_LLM(· | h̄).
  3. Evaluate: Compute score r(a_i, x).
  4. Filter: If r ≥ γ, append a_i to h̄ (subject to token limits).
  5. Select: After N steps, return the single highest-reward answer found (standard BoN selection over the trajectory).

- **Design tradeoffs:**
  - Threshold γ: Requires tuning (e.g., 0.90-0.97 for MATH). Too high → no history updates (degrades to BoN). Too low → noise in history (degrades to PureSeq).
  - History Budget: The paper uses a sliding window (e.g., history_budget=3) to manage context length, trading off long-term memory for token limits.
  - Verifier Choice: PRM (Process Reward Model) outperforms ORM (Outcome Reward Model) because it provides denser signal for intermediate reasoning steps, making the filter γ more effective.

- **Failure signatures:**
  - Reward Hacking: The model generates outputs that maximize the proxy reward r but are incorrect. The feedback loop amplifies this (Appendix D).
  - Context Drift: If the sliding window drops the "correct" context too early, subsequent samples may revert to the base distribution.
  - Stagnation: If γ is too aggressive, the history never updates, and the system burns compute equivalent to standard BoN but with higher latency.

- **First 3 experiments:**
  1. Threshold Sweep: Implement Algorithm 3 on a validation set (e.g., MATH500) and sweep γ ∈ [0.85, 0.99] to identify the optimal filtering pressure.
  2. Scaling Comparison: Plot accuracy vs. generation budget N (e.g., N=2 to 128) for RF-SeqBoN vs. Vanilla BoN vs. PureSeq to visualize the "budget efficiency" gap.
  3. Ablation on Verifier: Swap the PRM for a standard ORM to measure the sensitivity of the filtering mechanism to reward signal quality (per Appendix C.5).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the theoretical sample complexity guarantees for RF-SeqBoN be extended to general comparator policies π* and larger values of ε without requiring the high-reward support condition?
- Basis in paper: [explicit] Remark 5.7 states, "We look forward to extending our result to general ε and π* as future work."
- Why unresolved: The current proofs (Theorem 5.5) rely on the restrictive assumption that ε is sufficiently small and the comparator policy π* only distributes mass on the support set where rewards are large.
- What evidence would resolve it: A theoretical derivation of sample complexity bounds that holds for any ε ∈ (0,1) and arbitrary π*, removing the condition supp(π*(·|x)) ⊆ {r(a,x) ≥ γ*(x)}.

### Open Question 2
- Question: How can the sequential reward-filtering framework be modified to systematically prevent inference-time reward hacking when the reward model is mis-specified or adversarial?
- Basis in paper: [explicit] Appendix D discusses limitations regarding mis-specified rewards, noting that "Developing a more systematic understanding of... sequential TTC under adversarial or mis-specified rewards, remains an important direction for future work."
- Why unresolved: The current mechanism preferentially retains high-reward actions; if the reward model is flawed, this feedback loop amplifies biases, leading to "inference-time reward hacking" rather than improved performance.
- What evidence would resolve it: Empirical or theoretical results demonstrating a modified algorithm (e.g., using lower confidence bounds or robust thresholds) that maintains sample efficiency while strictly bounding performance degradation under reward model corruption.

### Open Question 3
- Question: Does the empirical advantage of RF-SeqBoN over parallel BoN persist at significantly larger model scales (e.g., 70B+ parameters)?
- Basis in paper: [inferred] The experimental section is restricted to Qwen3-0.6B and Qwen3-4B. As base model capabilities scale, the "coverage" and "error" rates of the base policy change, potentially narrowing the gap between parallel and sequential TTC.
- Why unresolved: It is unclear if the improvements observed in smaller models translate to frontier models where the base policy is already highly competent, or if the theoretical separation is only practically relevant for "weaker" base policies.
- What evidence would resolve it: Benchmark results (e.g., MATH500) on larger models (e.g., Llama-3-70B) plotting the accuracy-compute tradeoff curves for both BoN and RF-SeqBoN to compare the magnitude of the performance gap.

## Limitations
- The theoretical claims rely heavily on idealized assumptions about the reward model's accuracy and the pretraining mixture's composition.
- The analysis focuses on single-step decision problems; extension to multi-step reasoning tasks would require further theoretical development.
- The paper does not address the computational overhead of sequential sampling compared to parallel methods.

## Confidence
- **High Confidence:** The empirical improvements over vanilla BoN and PureSeq on MATH500, GPQA-Diamond, AMC'23, and AIME'25 benchmarks using Qwen3-4B-Instruct and Qwen3-0.6B-Thinking models are well-demonstrated and reproducible. The observation that PRM outperforms ORM is also strongly supported.
- **Medium Confidence:** The theoretical separation between parallel and sequential TTC (Theorem 4.3) is sound within the paper's model, but its practical significance depends on the realizability of the mixture-of-policies assumption in actual LLM pretraining.
- **Medium Confidence:** The claim that RF-SeqBoN "provably achieves strictly better sample complexity than BoN" is valid under the stated assumptions (Assumption 5.4, 3.5), but translating this to real-world sample efficiency gains requires careful consideration of reward model quality and the choice of the filtering threshold γ.

## Next Checks
1. **Reward Model Robustness:** Systematically evaluate RF-SeqBoN with reward models of varying quality (e.g., different PRM training sets, or injecting noise) to quantify the sensitivity of the theoretical gains to reward signal reliability.
2. **Multi-Step Extension:** Apply the RF-SeqBoN framework to a multi-step reasoning task (e.g., a complex proof or a chain-of-thought problem) and analyze if the sample complexity benefits scale or degrade compared to the single-step case.
3. **Overhead Analysis:** Measure the wall-clock time and energy consumption of RF-SeqBoN versus parallel BoN across different budget sizes (N=8, 32, 128) to determine if the statistical gains justify the sequential computation cost in a production setting.