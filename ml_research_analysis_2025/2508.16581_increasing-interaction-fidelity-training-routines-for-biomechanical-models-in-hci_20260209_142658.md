---
ver: rpa2
title: 'Increasing Interaction Fidelity: Training Routines for Biomechanical Models
  in HCI'
arxiv_id: '2508.16581'
source_url: https://arxiv.org/abs/2508.16581
tags:
- learning
- training
- biomechanical
- interaction
- routines
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the challenge of training biomechanical models
  for precise touchscreen interactions using reinforcement learning. The authors propose
  practical training routines including action masking, multi-stage curriculum learning,
  dynamic reward shaping, adaptive target sampling, larger network configurations,
  and simulation tuning.
---

# Increasing Interaction Fidelity: Training Routines for Biomechanical Models in HCI

## Quick Facts
- arXiv ID: 2508.16581
- Source URL: https://arxiv.org/abs/2508.16581
- Reference count: 30
- Primary result: Best configuration achieves 100% success rate on 1.5 mm targets with 8.07 avg errors per episode and 1.40s completion time

## Executive Summary
This work addresses the challenge of training biomechanical models for precise touchscreen interactions using reinforcement learning. The authors propose practical training routines including action masking, multi-stage curriculum learning, dynamic reward shaping, adaptive target sampling, larger network configurations, and simulation tuning. These methods enable fine motor control with higher interaction fidelity than previous approaches. The best-performing configuration achieved perfect accuracy on small 1.5 mm targets while maintaining reasonable error rates and interaction times.

## Method Summary
The authors train musculoskeletal models using PPO with a 512×512 policy network, learning rate of 6×10⁻⁴ with linear decay, and clip range of 0.2 (also linearly decayed). Key innovations include action masking to restrict control to the index finger only, a 4-stage curriculum learning approach with progressive reward shaping, and adaptive target sampling. The curriculum progresses from 3D volumetric targets to 2D surfaces, increases reward complexity from simple distance-based to including penalties for wrong touches and jerk, adapts sampling to emphasize difficult locations, and trains sequential movements. The training runs for 200M+ timesteps with frameskip set to 3 for finer control granularity.

## Key Results
- Best configuration achieves 100% success rate on 1.5 mm targets
- Average of 8.07 unintended touches per successful episode
- Average completion time of 1.40 seconds per successful interaction
- Action masking improves learning significantly compared to full-finger control
- Multi-stage curriculum with dynamic reward shaping enables fine motor control

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Action masking accelerates learning by constraining exploration to task-relevant degrees of freedom
- Mechanism: Disabling all fingers except the index finger reduces the action space dimensionality, eliminating unnecessary exploration of irrelevant muscle activations. This focuses the policy gradient on the subset of actions that directly contribute to touchscreen pointing.
- Core assumption: The touchscreen pointing task only requires index finger control; multi-finger coordination is not needed.
- Evidence anchors:
  - [abstract]: "action masking...can significantly improve the agent's ability to learn accurate touch behavior"
  - [section]: Table 1 shows that removing action masking (row 3: 256×256, 800M timesteps, no masking) drops success rate to 5%, while the same configuration with masking achieves substantially higher performance.
  - [corpus]: Stolz et al. (2024) in NeurIPS demonstrate that "excluding the irrelevant" through continuous action masking improves sample efficiency in RL.
- Break condition: Tasks requiring multi-finger gestures (e.g., pinch-to-zoom) would require unmasking additional actuators.

### Mechanism 2
- Claim: Multi-stage curriculum learning with dynamic reward shaping enables fine motor control that single-stage training fails to achieve
- Mechanism: The curriculum progresses through four stages: (1) 3D volumetric targets flatten to 2D surfaces, (2) reward complexity increases from simple success/failure to penalizing jerk and muscle effort, (3) target sampling adapts to emphasize low-performing locations, (4) sequential movements train from arbitrary starting states. Early-stage rewards are sparse; later stages add penalties that shape naturalistic movement.
- Core assumption: Motor skill acquisition benefits from graduated difficulty, similar to human motor learning.
- Evidence anchors:
  - [abstract]: "curriculum learning...can significantly improve the agent's ability to learn"
  - [section]: "Without this step-wise shaping, the agent fails to learn meaningful behavior."
  - [corpus]: Selder et al. (2025) at CHI examine reward function design in biomechanical user simulation, though systematic curriculum structures remain underexplored.
- Break condition: Misaligned curriculum stages (e.g., advancing too early) may cause catastrophic forgetting or unstable convergence.

### Mechanism 3
- Claim: Larger policy networks (512×512) with low frameskip enable sub-centimeter precision by capturing fine motor complexity
- Mechanism: Fine motor control requires representing high-dimensional muscle coordination patterns. Smaller networks (128×128, 256×256) lack capacity to encode precise fingertip trajectories. Lower frameskip (3 vs. 10) provides finer-grained action frequency, preventing overshooting from prolonged muscle activations.
- Core assumption: Network capacity and control frequency—not algorithm choice—were the primary bottlenecks for fine motor precision.
- Evidence anchors:
  - [abstract]: "more complex network configurations...can significantly improve the agent's ability to learn accurate touch behavior"
  - [section]: Table 1 shows 512×512 achieves 100% success rate on 1.5mm targets with 8.07 errors per episode, compared to 82–99% for smaller networks with more errors.
  - [corpus]: Corpus evidence is limited on network scaling specifically for biomechanical HCI tasks; this remains an empirical contribution.
- Break condition: Computational budget constraints that prevent training larger networks for sufficient timesteps.

## Foundational Learning

- **Concept: Proximal Policy Optimization (PPO)**
  - Why needed here: The paper uses PPO as the core RL algorithm; understanding its clip range and learning rate decay is essential for reproducing results.
  - Quick check question: Can you explain why PPO's clipped objective prevents destructively large policy updates?

- **Concept: MuJoCo Physics Simulation**
  - Why needed here: The biomechanical model runs in MuJoCo; frameskip, observation frequency, and muscle actuation are MuJoCo-specific parameters.
  - Quick check question: What is the difference between MuJoCo's muscle actuators and joint torque actuators?

- **Concept: Reward Shaping in RL**
  - Why needed here: Dynamic reward shaping (Stage 2) is critical; the paper shows that reward complexity must increase gradually to avoid learning failure.
  - Quick check question: How can a poorly designed reward function cause an agent to exploit loopholes rather than achieve the intended behavior?

## Architecture Onboarding

- **Component map:**
  - PPO policy network (512×512) → muscle activation outputs → MuJoCo physics simulation → fingertip position → reward calculator
  - Curriculum controller manages stage transitions based on performance thresholds
  - Adaptive target sampler adjusts target location distribution based on per-location success rates
  - Action mask layer zeroes out non-index-finger muscle activations before environment execution

- **Critical path:**
  1. Configure biomechanical model with action masking (disable non-index fingers)
  2. Implement 4-stage curriculum with reward functions: r1 (distance-based) → r2 (wrong touch penalty) → r3 (jerk penalty)
  3. Set frameskip=3, learning rate=6×10⁻⁴ with linear decay, clip range=0.2
  4. Train for 200M timesteps minimum

- **Design tradeoffs:**
  - Larger networks (512×512) improve precision but require 200M+ timesteps; smaller networks train faster but cap at ~99% success with higher error rates
  - Low frameskip improves control fidelity but increases compute per episode
  - Complex curriculum improves final performance but requires tuning stage transition thresholds

- **Failure signatures:**
  - Agent overshoots targets consistently → frameskip too high; reduce to 3
  - Agent fails to learn any meaningful behavior → reward shaping too complex initially; start with simple distance-based reward
  - High variance in success across target locations → enable adaptive target sampling (Stage 3)
  - 0% success despite training → action masking missing or curriculum stages skipped

- **First 3 experiments:**
  1. **Baseline validation:** Train 128×128 network with action masking only on 6mm targets; verify >50% success rate
  2. **Ablation test:** Add Stage 1-2 curriculum (task complexity + dynamic reward); compare error rates against baseline
  3. **Full configuration:** Enable all stages with 512×512 network, frameskip=3, adaptive sampling; target 1.5mm buttons

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the proposed training routines generalize to more complex, multi-finger interactions beyond single-finger pointing tasks?
- Basis in paper: [inferred] The paper applies action masking by disabling all fingers except the index finger, and the abstract states current approaches "do not generalize well." The training routines are evaluated only on a single touchscreen pointing task.
- Why unresolved: The action masking technique fundamentally restricts the action space to reduce complexity, but real mobile interactions require coordinated multi-finger gestures (pinching, typing, swiping).
- What evidence would resolve it: Successful training of agents performing multi-touch gestures (e.g., pinch-to-zoom) using the same training routines without disabling finger degrees of freedom.

### Open Question 2
- Question: How can the error rate (8.07 unintended touches per successful episode) be further reduced while maintaining the 100% success rate?
- Basis in paper: [explicit] Table 1 reports the best configuration achieves 100% success rate but still averages 8.07 errors per successful episode, which the paper describes as "indicating suboptimal control and a lack of fine motor skills."
- Why unresolved: The paper demonstrates improved accuracy over prior work but does not achieve human-level precision, suggesting the fine motor control remains imperfect despite the training improvements.
- What evidence would resolve it: Training configurations that reduce errors per successful episode to near-zero while maintaining high success rates, potentially through additional curriculum stages or refined reward penalties.

### Open Question 3
- Question: How well do movements generated by these trained agents match empirical human kinematic data for the same touchscreen tasks?
- Basis in paper: [inferred] The paper claims to enable "human-like movements" and "interaction fidelity" but evaluates only task success metrics (accuracy, errors, time), not kinematic similarity to actual human movement data.
- Why unresolved: Task success alone does not guarantee movement patterns resemble human biomechanics; agents could achieve goals through non-human-like strategies.
- What evidence would resolve it: Quantitative comparison of joint trajectories, velocity profiles, and movement patterns between trained agents and motion capture data from human participants performing identical tasks.

## Limitations
- Computational demands of 512×512 network configuration (200M+ timesteps) create resource barriers for verification
- Single biomechanical model assumption (index finger only) limits generalizability to multi-finger gestures
- Limited empirical validation of network scaling benefits specifically for biomechanical HCI tasks

## Confidence
- **High confidence**: Action masking effectiveness (supported by quantitative comparison showing 5% vs 100% success rates with/without masking)
- **Medium confidence**: Multi-stage curriculum learning mechanism (strong theoretical grounding, but limited ablation studies on individual stage contributions)
- **Medium confidence**: Network scaling and frameskip benefits (empirical results show improvement, but lack systematic comparison across different algorithm variants)

## Next Checks
1. **Reward weight sensitivity analysis**: Systematically vary w1-w5 reward coefficients across multiple orders of magnitude to identify stability boundaries and optimal ranges
2. **Curriculum stage ablation study**: Train separate models with individual curriculum stages removed to quantify each stage's contribution to final performance
3. **Cross-model generalizability test**: Apply training routines to alternative musculoskeletal models with different finger configurations to validate architecture independence