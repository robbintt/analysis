---
ver: rpa2
title: Can LLMs Detect Their Own Hallucinations?
arxiv_id: '2511.11087'
source_url: https://arxiv.org/abs/2511.11087
tags:
- llms
- hallucinations
- sentence
- sentences
- knowledge
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether large language models (LLMs) can
  detect their own hallucinations. The authors formulate hallucination detection as
  a sentence classification task and propose a framework using Chain-of-Thought (CoT)
  reasoning to extract knowledge from model parameters.
---

# Can LLMs Detect Their Own Hallucinations?

## Quick Facts
- arXiv ID: 2511.11087
- Source URL: https://arxiv.org/abs/2511.11087
- Authors: Sora Kadotani; Kosuke Nishida; Kyosuke Nishida
- Reference count: 13
- Key outcome: GPT-3.5-Turbo with CoT detected 58.2% of its own hallucinations vs 21.9% without CoT

## Executive Summary
This paper investigates whether large language models can detect their own hallucinations by formulating hallucination detection as a binary sentence classification task. The authors propose a framework using Chain-of-Thought (CoT) reasoning to extract knowledge from model parameters, generating true and false sentences from the T-REx dataset triples. Experimental results demonstrate that GPT-3.5-Turbo with CoT achieves 58.2% recall in detecting hallucinations compared to 21.9% without CoT reasoning, with a positive correlation between recall and knowledge amount in the model.

## Method Summary
The authors develop a three-stage framework for hallucination detection using Chain-of-Thought reasoning. First, true sentences are generated by paraphrasing T-REx facts while including object keywords. Second, false sentences are created by rewriting only the object keyword to something plausible but incorrect. Third, sentences are classified as true or false using either direct yes/no responses or CoT prompting ("Let's think step by step"). The framework employs 10-shot in-context learning with manually created examples for each of the 41 relation labels in T-REx. Models tested include GPT-3.5-Turbo (main), GPT-4-Turbo, and Llama 3.1 70B, with primary metric being Recall since missing hallucinations is considered more critical than false positives.

## Key Results
- GPT-3.5-Turbo with CoT achieved 58.2% recall versus 21.9% without CoT in detecting hallucinations
- False sentences generated in the framework reproduced actual LLM hallucinations with 3.17% match rate
- Positive correlation found between recall and knowledge amount (measured via Wikipedia page views) with Spearman's ρ = 0.487 (p = 0.001)
- Precision remained similar at 64.2% with CoT versus 65.5% without CoT

## Why This Works (Mechanism)
The mechanism works because CoT reasoning enables LLMs to access and apply stored parametric knowledge during classification. By prompting the model to "think step by step," it can retrieve relevant facts from its pretraining data and use this knowledge to evaluate whether a sentence is factually correct. This process leverages the model's ability to extract implicit knowledge from its parameters rather than relying solely on pattern matching from training data.

## Foundational Learning
- **Chain-of-Thought Prompting**: Sequential reasoning technique that guides models through multi-step problem solving. Needed to access deeper parametric knowledge. Quick check: Does adding "Let's think step by step" improve reasoning on simple arithmetic tasks?
- **In-Context Learning**: Few-shot learning approach where examples are provided in the prompt itself. Needed to adapt the model to the hallucination detection task without fine-tuning. Quick check: Does classification accuracy improve with more than 10 examples?
- **Sentence Classification**: Binary classification task distinguishing true from false statements. Needed as the fundamental task structure for hallucination detection. Quick check: Can the model classify simple factual statements correctly?
- **T-REx Dataset**: Knowledge graph triples from Wikipedia used to generate evaluation sentences. Needed as a reliable source of true facts for generating both true and false sentences. Quick check: Are all generated sentences grammatically correct?
- **Knowledge Correlation Analysis**: Statistical analysis linking model performance to knowledge content. Needed to validate that performance improvements stem from knowledge access. Quick check: Does correlation hold across different knowledge domains?

## Architecture Onboarding

**Component Map**: T-REx Dataset -> True Sentence Generator -> False Sentence Generator -> Classifier (with/without CoT) -> Evaluation Metrics

**Critical Path**: Data preprocessing → In-context example creation → Sentence generation → Classification → Performance evaluation

**Design Tradeoffs**: The framework prioritizes recall over precision since missing hallucinations is more problematic than false positives. This means accepting lower precision (64.2%) in exchange for higher recall (58.2%). The use of synthetic false sentences rather than naturally occurring hallucinations allows controlled experimentation but may not capture all hallucination types.

**Failure Signatures**: 
- Low recall indicates the model cannot access sufficient knowledge for certain relation types
- Precision drops when CoT occasionally extracts incorrect knowledge from pretraining data
- Non-"yes"/"no" outputs occur in ~1% of cases when model is uncertain

**First Experiments**:
1. Test direct classification (no CoT) on 5-10 relation labels to establish baseline performance
2. Add CoT prompting to same subset to verify recall improvement
3. Manually inspect CoT reasoning chains to identify patterns in correct versus incorrect knowledge extraction

## Open Questions the Paper Calls Out
1. **Training/Inference Hallucinations**: The framework focuses on data-related hallucinations and does not cover hallucinations from training and inference. It's unclear if CoT can identify errors from architectural flaws or decoding probabilities without grounding in specific facts.

2. **Active Hallucination Reduction**: While the framework demonstrates detection capability, it doesn't propose a mechanism for the model to revise or suppress hallucinated content during generation. The paper lists detecting and reducing hallucinations as future work.

3. **Synthetic vs Natural Hallucinations**: The false sentences generated via keyword rewriting may not fully represent the semantic diversity of naturally occurring hallucinations. While showing 3.17% match rate (higher than chance), this low absolute percentage suggests the framework may miss more complex hallucination types.

## Limitations
- Core methodology depends on manually created in-context learning examples (41 relation-specific prompts) that are not publicly available
- Does not specify API hyperparameters like temperature and top_p for main experiments
- Synthetic false sentences may not capture full diversity of naturally occurring hallucinations
- Precision remains similar with or without CoT, suggesting limited improvement in overall detection quality

## Confidence
**High Confidence**: The experimental observation that GPT-3.5-Turbo with CoT achieves 58.2% recall versus 21.9% without CoT is well-supported. The positive correlation between recall and knowledge amount is statistically demonstrated.

**Medium Confidence**: The claim that LLMs can detect their own hallucinations "if sufficient knowledge is contained in their parameters" is supported but has important caveats about knowledge thresholds. The finding that synthetic false sentences reproduce actual hallucinations is based on qualitative observation.

**Low Confidence**: The minimal precision improvement (64.2% vs 65.5%) suggests CoT may not significantly enhance overall detection quality. The paper acknowledges CoT can occasionally extract incorrect knowledge causing false positives but doesn't thoroughly analyze the frequency.

## Next Checks
1. Replicate the framework using a small subset of 5-10 relation labels with publicly available T-REx data to verify the CoT recall improvement holds across different relation types.
2. Systematically analyze the CoT reasoning chains to quantify how often incorrect knowledge extraction occurs and whether this pattern correlates with specific relation types or sentence structures.
3. Test the framework with a different LLM (e.g., Claude, Gemini) to determine if the CoT benefit generalizes beyond OpenAI models or is specific to their pretraining approaches.