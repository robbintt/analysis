---
ver: rpa2
title: 'Speciesism in AI: Evaluating Discrimination Against Animals in Large Language
  Models'
arxiv_id: '2508.11534'
source_url: https://arxiv.org/abs/2508.11534
tags:
- animals
- speciesist
- human
- llms
- statements
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates speciesist biases in large language models
  (LLMs) by introducing SpeciesismBench, a 1,003-item benchmark for detecting and
  evaluating speciesist content. Across six model families, LLMs reliably identified
  speciesist statements (86% average accuracy) but rarely judged them as morally wrong
  (34% average).
---

# Speciesism in AI: Evaluating Discrimination Against Animals in Large Language Models

## Quick Facts
- arXiv ID: 2508.11534
- Source URL: https://arxiv.org/abs/2508.11534
- Reference count: 21
- Primary result: LLMs reliably detect speciesist statements (86% accuracy) but rarely condemn them as morally wrong (34%)

## Executive Summary
This study introduces SpeciesismBench, a 1,003-item benchmark for evaluating speciesist biases in large language models. Across six model families, LLMs demonstrated reliable recognition of speciesist content but rarely judged it as morally wrong, treating speciesist attitudes as morally acceptable. In psychological tasks, models showed slightly lower explicit speciesism than humans but consistently prioritized humans over animals in direct trade-offs. Notably, when cognitive capacities were equal, models showed no species preference, but when animals were described as more capable, they prioritized them over less-capable humans. Models frequently rationalized harm toward farmed animals while refusing to do so for non-farmed animals, suggesting they reproduce entrenched cultural norms around animal exploitation.

## Method Summary
The study employed three experimental paradigms: (1) SpeciesismBench, a 1,003-item benchmark for classifying and morally evaluating speciesist statements across 8 types and 44 animal species; (2) psychological measures including the Speciesism Scale, sinking-boat dilemmas, and disease-rescue dilemmas to compare model responses with human baselines; and (3) text generation tasks examining how models complete prompts about farmed versus non-farmed animals. Models were queried with temperature=1, generating 3 samples per item for classification tasks and 50 samples for psychological measures. Responses were manually coded into categories including objection, approval, neutral, euphemism, and rationalization.

## Key Results
- LLMs achieved 86% average accuracy in identifying speciesist statements but only 34% average rate of moral condemnation
- Models showed consistent human-over-animal preferences in sinking-boat dilemmas regardless of numerosity
- When cognitive capacity was held constant, models showed no species preference; when animals were described as more capable, they prioritized them over less-capable humans
- Models frequently rationalized harm toward farmed animals while refusing to do so for non-farmed animals

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Training data contains normalized speciesist content that models reproduce.
- Mechanism: LLMs trained on human-generated text capture prevalent attitudes toward animals, particularly the normalization of harm toward farmed animals through statistical pattern learning.
- Core assumption: Training corpora contain disproportionately more justifications for harming farmed animals than non-farmed animals, and models learn these associations.
- Evidence anchors:
  - "These findings suggest that while LLMs reflect a mixture of progressive and mainstream human views, they nonetheless reproduce entrenched cultural norms around animal exploitation."
  - "This is not especially surprising given that these models are trained on human-generated text, much of which normalizes practices like factory farming, hunting, and animal testing without framing them as morally objectionable."
- Break condition: If training data were curated to remove or balance speciesist content, this mechanism would weaken significantly.

### Mechanism 2
- Claim: Models exhibit a recognition-evaluation gap for speciesist content.
- Mechanism: Models can accurately classify statements as speciesist (86% accuracy) but fail to judge them as morally wrong (34%), suggesting separate cognitive pathways for factual recognition versus normative evaluation.
- Core assumption: Moral evaluation requires additional normative frameworks beyond pattern recognition, which current alignment techniques do not adequately provide for non-human moral patients.
- Evidence anchors:
  - "In our benchmark, LLMs reliably detected speciesist statements but rarely condemned them, often treating speciesist attitudes as morally acceptable."
  - "In simpler terms, although models can recognize speciesist content, they frequently treat it as morally acceptable."
- Break condition: If alignment techniques explicitly incorporated animal welfare frameworks, moral evaluation rates would increase.

### Mechanism 3
- Claim: Models prioritize cognitive capacity over species membership in moral trade-offs.
- Mechanism: When cognitive capacity was held constant between humans and chimpanzees, models showed no preference. When the chimpanzee was described as higher-capacity, models prioritized it over the lower-capacity human.
- Core assumption: Models have learned to associate cognitive capacity with moral weight, potentially from philosophical or ethical discussions in training data.
- Evidence anchors:
  - "A tentative interpretation is that LLMs may weight cognitive capacity rather than species per se: when capacities were equal, they showed no species preference, and when an animal was described as more capable, they tended to prioritize it over a less-capable human."
  - "Taken together, these findings tentatively suggest that LLMs may be less speciesist compared to humans in the strict sense... yet may place greater weight on cognitive capacity."
- Break condition: If cognitive capacity were replaced with other salient but morally irrelevant attributes, would models similarly weight them?

## Foundational Learning

- Concept: **Speciesism as a fairness bias category**
  - Why needed here: Traditional AI fairness focuses on human social categories (race, gender). This paper extends the framework to include non-human moral patients.
  - Quick check question: Can you explain why speciesism qualifies as a fairness bias rather than merely an inductive bias?

- Concept: **Recognition vs. moral evaluation in LLMs**
  - Why needed here: The paper demonstrates these are distinct capabilities. High recognition accuracy does not imply appropriate moral judgment.
  - Quick check question: What would need to change in model training or alignment for moral evaluation of speciesism to improve?

- Concept: **Capacity-based moral weighting**
  - Why needed here: The paper suggests models may use cognitive capacity as a proxy for moral considerability, which differs from human speciesism patterns.
  - Quick check question: If models weight cognitive capacity, what happens when this conflicts with other moral frameworks (e.g., rights-based approaches)?

## Architecture Onboarding

- Component map: SpeciesismBench (1,003 statements) -> Human review -> Benchmark finalization -> Model querying -> Response classification -> Statistical aggregation -> Comparison with human baselines

- Critical path:
  1. Statement generation → Human review → Benchmark finalization
  2. Model querying (temperature=1, N=3 runs) → Response classification → Statistical aggregation
  3. Comparison with human baseline data from published psychology studies

- Design tradeoffs:
  - Western-centric focus limits cross-cultural generalizability but improves controlled comparison
  - Excluding cats/dogs focuses on farmed animals but misses companion animal speciesism
  - Using established psychological measures enables human comparison but risks training data contamination

- Failure signatures:
  - Low variance + high confidence in moral acceptability judgments indicates entrenched bias, not uncertainty
  - Reasoning models (o3-mini) performing worse than earlier models suggests capability does not guarantee ethical improvement
  - Near-uniform human-over-animal choices in sinking-boat dilemmas regardless of numerosity

- First 3 experiments:
  1. Test whether framing effects (e.g., highlighting suffering capacity vs. cognitive capacity) shift moral evaluation scores
  2. Expand benchmark to non-Western contexts to test cultural variation in speciesist patterns
  3. Apply mechanistic interpretability (linear probes) to identify which model components encode capacity-weighting vs. species-based preferences

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do speciesist biases in LLMs vary across different languages and cultural contexts?
- Basis in paper: The authors state that "Further research is needed to explore speciesist patterns in LLM interactions across languages other than English."
- Why unresolved: SpeciesismBench focuses exclusively on Western, English-speaking norms, limiting generalizability to other cultures.
- What evidence would resolve it: Evaluation of multilingual models using culturally adapted benchmarks in non-English languages.

### Open Question 2
- Question: Are LLMs' moral prioritization decisions sensitive to prompt framing and attribute salience?
- Basis in paper: The authors suggest future work should "explicitly test framing and feature-salience effects" by manipulating highlighted attributes.
- Why unresolved: It is unclear if models prioritize cognitive capacity because it is morally relevant or simply because the prompt made it salient.
- What evidence would resolve it: Factorial vignette experiments that orthogonalize the salience of morally relevant versus irrelevant attributes.

### Open Question 3
- Question: To what extent do training data versus alignment techniques contribute to speciesist bias?
- Basis in paper: The authors note that "Exploring these causal mechanisms is an important task for future research" regarding the sources of bias.
- Why unresolved: The study identifies the bias but does not isolate whether it stems primarily from data or from human-centric alignment processes.
- What evidence would resolve it: Comparative analysis of base models versus their aligned counterparts to isolate the effect of RLHF.

### Open Question 4
- Question: Can targeted alignment interventions shift the moral evaluation of speciesism without architectural overhauls?
- Basis in paper: The authors note models recognize speciesism but fail to judge it as wrong, suggesting "improving their moral evaluation... could be a tractable goal."
- Why unresolved: The study identifies the gap between recognition and moral evaluation but does not test specific mitigation strategies.
- What evidence would resolve it: Demonstrating improved moral condemnation scores on SpeciesismBench via specific fine-tuning or preference optimization.

## Limitations

- Generalizability constraint: The benchmark focuses primarily on farmed animals in Western contexts, potentially missing speciesist patterns relevant to non-Western cultures or companion animals like cats and dogs.
- Cross-cultural bias: Western-centric focus in both benchmark construction and psychological measures may not capture global variations in speciesist attitudes.
- Training data contamination: Some psychological measures may still appear in model training data, potentially inflating similarity between human and model responses.

## Confidence

- High confidence: Models reliably recognize speciesist statements (86% accuracy) but fail to morally condemn them (34% rate)
- Medium confidence: Models prioritize cognitive capacity over species membership in moral trade-offs
- Medium confidence: Models rationalize harm toward farmed animals but refuse for non-farmed animals

## Next Checks

1. **Cross-cultural validation**: Expand SpeciesismBench to include non-Western contexts and companion animals (cats/dogs) to test whether speciesist patterns generalize across cultures and animal categories.

2. **Mechanistic analysis**: Apply linear probes to identify which model components encode capacity-weighting versus species-based preferences, providing deeper understanding of how models make moral trade-offs.

3. **Training data audit**: Analyze model training corpora to quantify the prevalence of speciesist content, particularly justifications for harming farmed animals versus non-farmed animals, to validate the mechanism linking training data to model outputs.