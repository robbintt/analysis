---
ver: rpa2
title: 'LibriVAD: A Scalable Open Dataset with Deep Learning Benchmarks for Voice
  Activity Detection'
arxiv_id: '2512.17281'
source_url: https://arxiv.org/abs/2512.17281
tags:
- libriv
- dataset
- speech
- noise
- ad-concat
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces LibriVAD, a large-scale, open-source dataset
  for voice activity detection (VAD) research, addressing the lack of publicly available,
  systematically controlled datasets for training and evaluating VAD models. LibriVAD
  is derived from the LibriSpeech corpus and augmented with diverse real-world and
  synthetic noise sources, enabling systematic control over speech-to-noise ratio,
  silence-to-speech ratio, and noise diversity.
---

# LibriVAD: A Scalable Open Dataset with Deep Learning Benchmarks for Voice Activity Detection

## Quick Facts
- **arXiv ID:** 2512.17281
- **Source URL:** https://arxiv.org/abs/2512.17281
- **Reference count:** 40
- **Primary result:** LibriVAD is a large-scale, open-source dataset for voice activity detection (VAD) research, enabling systematic control over speech-to-noise ratio, silence-to-speech ratio, and noise diversity, with benchmarking showing ViT with MFCC features outperforms established models across seen, unseen, and out-of-distribution conditions.

## Executive Summary
This paper introduces LibriVAD, a large-scale, open-source dataset for voice activity detection (VAD) research, addressing the lack of publicly available, systematically controlled datasets for training and evaluating VAD models. LibriVAD is derived from the LibriSpeech corpus and augmented with diverse real-world and synthetic noise sources, enabling systematic control over speech-to-noise ratio, silence-to-speech ratio, and noise diversity. The dataset is released in three sizes (15 GB, 150 GB, and 1.5 TB) with two variants to support different experimental setups. The authors benchmark multiple feature-model combinations, including waveform, Mel-Frequency Cepstral Coefficients (MFCC), and Gammatone filter bank cepstral coefficients, and introduce the Vision Transformer (ViT) architecture for VAD. Experiments show that ViT with MFCC features consistently outperforms established VAD models such as boosted deep neural network and convolutional long short-term memory deep neural network across seen, unseen, and out-of-distribution (OOD) conditions, including evaluation on the real-world VOiCES dataset. The study further demonstrates that scaling up dataset size and balancing silence-to-speech ratio noticeably and consistently enhance VAD performance under OOD conditions.

## Method Summary
LibriVAD is constructed by augmenting the LibriSpeech corpus with diverse real-world and synthetic noise sources from Audioset, DEMAND, Freesound, and QUT datasets. The dataset is systematically controlled for speech-to-noise ratio, silence-to-speech ratio, and noise diversity, and released in three sizes (15 GB, 150 GB, and 1.5 TB) with two variants to support different experimental setups. The authors benchmark multiple feature-model combinations, including waveform, Mel-Frequency Cepstral Coefficients (MFCC), and Gammatone filter bank cepstral coefficients, and introduce the Vision Transformer (ViT) architecture for VAD. Experiments evaluate models on seen, unseen, and out-of-distribution (OOD) conditions, including the real-world VOiCES dataset.

## Key Results
- ViT with MFCC features consistently outperforms established VAD models (BDNN, CLDNN) across seen, unseen, and OOD conditions.
- Scaling up dataset size and balancing silence-to-speech ratio noticeably and consistently enhance VAD performance under OOD conditions.
- The dataset augmentation process and benchmarking framework are well-documented and reproducible, enabling systematic control over noise and silence characteristics for VAD research.

## Why This Works (Mechanism)
None provided.

## Foundational Learning
- **Voice Activity Detection (VAD):** A binary classification task that detects the presence or absence of human speech in an audio signal. Essential for downstream speech processing tasks like speech recognition and speaker identification.
- **Mel-Frequency Cepstral Coefficients (MFCC):** A popular audio feature representation that captures the spectral envelope of speech. Widely used in speech processing due to its robustness to noise and ability to represent speaker characteristics.
- **Vision Transformer (ViT):** A transformer-based architecture originally designed for image classification, adapted here for audio processing by treating spectrograms as images. Enables long-range temporal dependencies in VAD.
- **Speech-to-Noise Ratio (SNR):** The ratio of speech signal power to background noise power. Critical for VAD performance, as low SNR conditions are challenging for speech detection.
- **Out-of-Distribution (OOD) Evaluation:** Testing model generalization on data that differs from the training distribution, such as real-world recordings with unseen noise types or recording conditions.

## Architecture Onboarding

**Component Map:** Raw audio -> Feature extraction (MFCC/Gammatone/Waveform) -> ViT encoder -> Classification head -> VAD output

**Critical Path:** Audio input → Feature extraction → ViT patch embedding → Multi-head self-attention → Feed-forward network → Classification

**Design Tradeoffs:** ViT offers strong performance on VAD but requires significant computational resources; MFCC features provide robustness to noise but may lose fine-grained temporal information; dataset scaling improves OOD performance but increases training time.

**Failure Signatures:** Poor performance on low SNR or unseen noise types; overfitting to synthetic noise patterns; failure to generalize to real-world recording conditions.

**First Experiments:**
1. Evaluate baseline models (BDNN, CLDNN) on LibriVAD with varying SNR and silence-to-speech ratios.
2. Train ViT with MFCC features on LibriVAD and test on seen, unseen, and OOD conditions.
3. Analyze the impact of dataset scaling and silence-to-speech ratio balancing on OOD performance.

## Open Questions the Paper Calls Out
None provided.

## Limitations
- The dataset augmentation process relies on simulated noise mixtures, which may not fully capture the complexity and variability of real-world acoustic environments.
- The analysis of feature-model combinations is comprehensive but may not exhaust all possible architectures or feature representations relevant to VAD.
- The computational requirements for training large-scale ViT models may limit practical deployment in resource-constrained applications.

## Confidence
- **High confidence:** The dataset creation methodology and benchmarking framework are well-documented and reproducible. The performance improvements of ViT over baseline models on controlled experiments are statistically robust and clearly demonstrated.
- **Medium confidence:** The generalization claims to out-of-distribution conditions, while supported by VOiCES evaluation, would benefit from broader real-world testing across more diverse acoustic scenarios and recording devices.
- **Low confidence:** The practical deployment implications of scaling dataset size and balancing silence-to-speech ratio for real-time, edge-based VAD applications are not fully explored.

## Next Checks
1. Evaluate the trained models on additional real-world VAD datasets with diverse recording conditions, including mobile device recordings and far-field microphone arrays.
2. Conduct ablation studies to quantify the individual contributions of dataset scaling, noise diversity, and silence-to-speech ratio balancing to overall performance improvements.
3. Assess the computational efficiency and memory requirements of the ViT-based approach for real-time, on-device VAD applications and compare with lightweight baseline models.