---
ver: rpa2
title: Combining Large Language Models with Static Analyzers for Code Review Generation
arxiv_id: '2502.06633'
source_url: https://arxiv.org/abs/2502.06633
tags:
- code
- review
- static
- reviews
- software
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the challenge of improving automated code
  review generation by combining the precision of static analysis tools with the contextual
  understanding of large language models (LLMs). The authors propose three hybrid
  strategies to integrate knowledge-based systems (KBS) with learning-based systems
  (LBS): Data-Augmented Training (DAT), Retrieval-Augmented Generation (RAG), and
  Naive Concatenation of Outputs (NCO).'
---

# Combining Large Language Models with Static Analyzers for Code Review Generation

## Quick Facts
- arXiv ID: 2502.06633
- Source URL: https://arxiv.org/abs/2502.06633
- Reference count: 40
- Key outcome: Hybrid approaches combining static analyzers with LLMs improve code review generation accuracy and coverage

## Executive Summary
This paper addresses the challenge of improving automated code review generation by combining the precision of static analysis tools with the contextual understanding of large language models (LLMs). The authors propose three hybrid strategies to integrate knowledge-based systems (KBS) with learning-based systems (LBS): Data-Augmented Training (DAT), Retrieval-Augmented Generation (RAG), and Naive Concatenation of Outputs (NCO). DAT involves fine-tuning the LLM on an augmented dataset that includes both static analysis and LLM-generated reviews. RAG dynamically retrieves static analysis results during inference to guide the LLM. NCO simply combines the outputs of both systems after generation.

The evaluation demonstrates that the RAG approach significantly improves accuracy compared to using the LLM alone, while DAT achieves the highest coverage by exposing the model to diverse issues during training. NCO provides moderate improvements. The hybrid approaches effectively bridge the gap between the precision of static analysis tools and the comprehensiveness of LLMs, resulting in more effective and human-like code reviews. The study highlights the potential of combining static analysis with LLMs to enhance automated code review generation.

## Method Summary
The paper proposes three hybrid strategies for combining static analyzers with LLMs: Data-Augmented Training (DAT), Retrieval-Augmented Generation (RAG), and Naive Concatenation of Outputs (NCO). DAT involves fine-tuning the LLM on an augmented dataset that includes both static analysis and LLM-generated reviews. RAG dynamically retrieves static analysis results during inference to guide the LLM. NCO simply combines the outputs of both systems after generation. The evaluation demonstrates that RAG significantly improves accuracy, DAT achieves the highest coverage, and NCO provides moderate improvements, effectively bridging the gap between the precision of static analysis tools and the comprehensiveness of LLMs.

## Key Results
- RAG approach significantly improves accuracy compared to using the LLM alone
- DAT achieves the highest coverage by exposing the model to diverse issues during training
- NCO provides moderate improvements in code review quality

## Why This Works (Mechanism)
The hybrid approaches work by leveraging the complementary strengths of static analyzers and LLMs. Static analyzers provide precise detection of specific code issues but lack contextual understanding and human-like explanations. LLMs offer comprehensive contextual understanding and natural language generation but may miss specific technical issues. By combining these systems, the hybrid approaches achieve both precision and comprehensiveness in code review generation.

## Foundational Learning
- **Static Analysis Tools**: Automated tools that detect specific code issues and violations - needed for precise issue detection, check by running on sample code
- **Large Language Models**: AI models trained on vast text data for contextual understanding and generation - needed for natural language explanations, check by generating sample text
- **Hybrid Integration Strategies**: Methods to combine KBS and LBS outputs - needed for optimal performance, check by comparing different integration approaches
- **Retrieval-Augmented Generation**: Dynamic retrieval of external information during LLM inference - needed for context-aware generation, check by implementing basic RAG pipeline
- **Fine-tuning**: Adapting pre-trained models to specific tasks using augmented datasets - needed for task-specific performance, check by training on domain-specific data

## Architecture Onboarding

Component Map: Static Analyzer -> Hybrid Strategy -> LLM -> Code Review Output

Critical Path: Code Input → Static Analysis Detection → Hybrid Strategy Processing → LLM Generation → Final Review Output

Design Tradeoffs: RAG offers highest accuracy but requires runtime retrieval overhead, DAT provides best coverage but needs expensive fine-tuning, NCO is simplest but least effective

Failure Signatures: RAG may fail when retrieval is slow or inaccurate, DAT may overfit to training data patterns, NCO may produce redundant or conflicting suggestions

First Experiments:
1. Run static analyzer on sample code to verify issue detection accuracy
2. Implement basic RAG pipeline to test retrieval effectiveness
3. Compare NCO output quality against individual system outputs

## Open Questions the Paper Calls Out
None

## Limitations
- Does not address computational overhead or latency introduced by hybrid approaches in real-world development environments
- Limited exploration of performance across different programming languages or codebases beyond specific datasets
- Evaluation metrics focus on technical precision and coverage but do not assess developer satisfaction or practical utility in actual code review workflows

## Confidence

| Claim | Confidence |
|---|---|
| RAG significantly improves accuracy compared to LLM alone | High |
| DAT achieves highest coverage through exposure to diverse issues | Medium |
| Hybrid approaches effectively bridge precision-comprehensiveness gap | Medium |

## Next Checks
1. Measure computational overhead and inference latency of RAG and DAT approaches compared to standalone LLM or static analyzer usage in continuous integration pipelines
2. Conduct user studies with developers to evaluate satisfaction and practical utility of hybrid-generated reviews versus human-written reviews
3. Test hybrid approaches across multiple programming languages and diverse codebases to assess generalizability beyond the initial evaluation datasets