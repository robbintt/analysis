---
ver: rpa2
title: 'SHA256 at SemEval-2025 Task 4: Selective Amnesia -- Constrained Unlearning
  for Large Language Models via Knowledge Isolation'
arxiv_id: '2504.12996'
source_url: https://arxiv.org/abs/2504.12996
tags:
- unlearning
- layers
- task
- loss
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the problem of selectively removing sensitive
  information from large language models without degrading overall capabilities. The
  authors develop a two-stage approach combining causal mediation analysis with layer-specific
  optimization, identifying early transformer layers (0-5) as critical for storing
  subject-attribute associations.
---

# SHA256 at SemEval-2025 Task 4: Selective Amnesia -- Constrained Unlearning for Large Language Models via Knowledge Isolation
## Quick Facts
- arXiv ID: 2504.12996
- Source URL: https://arxiv.org/abs/2504.12996
- Reference count: 8
- Achieved 2nd place in 1B model track with 0.652 final score

## Executive Summary
This work addresses selective unlearning in large language models by removing sensitive information while preserving general capabilities. The authors develop a two-stage approach that first identifies knowledge-critical layers through causal mediation analysis, then applies targeted parameter updates with a joint loss function. The method achieved strong performance in the SemEval-2025 Task 4 competition, particularly for the 1B parameter model where it maintained 88% of baseline MMLU accuracy while achieving high task performance.

## Method Summary
The approach combines causal mediation analysis (CMA) with constrained gradient updates to selectively unlearn sensitive information. In Stage 1, CMA identifies early transformer layers (0-5) as critical for storing subject-attribute associations using synthetic QA datasets. Stage 2 applies a joint loss function that maximizes forget set loss while adaptively minimizing retain set deviation through output token cross-entropy penalties. The method freezes MLP parameters in identified layers while allowing attention parameters to remain trainable, creating a knowledge isolation mechanism that preserves general capabilities.

## Key Results
- 1B model achieved 2nd place in competition with 0.652 final score and 0.973 task aggregate
- Successfully removed forget set information while maintaining 88% of baseline MMLU accuracy
- 7B variant showed comparable forget set removal (0.964 task score) but experienced 46% MMLU degradation

## Why This Works (Mechanism)
The method exploits the distributed nature of transformer knowledge storage by targeting early layers where subject-attribute associations are concentrated. By freezing MLP parameters in these critical layers while allowing attention mechanisms to adapt, the approach creates a knowledge isolation effect that prevents catastrophic forgetting. The adaptive cross-entropy penalty ensures retain set preservation during optimization, while the joint loss function enables selective removal of sensitive information through gradient-based updates.

## Foundational Learning
- **Causal Mediation Analysis**: Identifies which model components causally influence specific outputs by comparing predictions with and without interventions on individual layers. Needed to locate knowledge-critical layers without full retraining. Quick check: Verify intervention effects are statistically significant across multiple trials.
- **Transformer Layer Architecture**: Understanding MLP vs attention parameter roles in knowledge representation. Needed to design selective freezing strategies. Quick check: Confirm MLP attention parameter ratio affects unlearning efficacy.
- **Cross-Entropy Loss with Adaptive Regularization**: Joint loss combining forget set maximization with retain set preservation through dynamic penalty terms. Needed to balance selective forgetting with capability retention. Quick check: Monitor retain set accuracy during training to ensure adaptive penalty effectiveness.

## Architecture Onboarding
- **Component Map**: Input -> Embedding Layer -> Layer 0-5 (Frozen MLP) -> Layers 6+ (Trainable) -> Output Layer
- **Critical Path**: Causal tracing identifies layers 0-5 as critical for subject-attribute associations; these layers have frozen MLP parameters while attention parameters remain trainable
- **Design Tradeoffs**: Freezing MLP parameters provides knowledge isolation but limits adaptation capacity; attention parameter freedom enables task-specific optimization while preserving general capabilities
- **Failure Signatures**: Catastrophic MMLU degradation (46% drop in 7B model) indicates over-aggressive freezing or insufficient retain set preservation
- **First Experiments**: 1) Layer-wise CMA ablation to identify minimum critical layers; 2) Retain set accuracy monitoring during joint loss optimization; 3) Comparative MMLU performance across different freezing thresholds

## Open Questions the Paper Calls Out
### Open Question 1
- Question: Can constrained gradient updates using KL divergence preserve model utility better than the current cross-entropy-based joint loss?
- Basis in paper: The authors state "Future work should investigate constrained gradient updates through KL divergence to reduce the drop in model utility after unlearning."
- Why unresolved: The current method shows significant MMLU degradation (46% for 7B model), suggesting the optimization landscape allows destructive parameter drift.
- What evidence would resolve it: Comparative experiments replacing the adaptive cross-entropy regularization with KL-based constraints, measuring MMLU retention and task aggregate scores.

### Open Question 2
- Question: How should the layer freezing threshold scale with model depth and parameter count to maintain utility across different model sizes?
- Basis in paper: The paper concludes "layer freezing thresholds must scale non-linearly with model depth to maintain utility" after observing that layers 0-5 work well for 1B but cause 46% MMLU drop in 7B.
- Why unresolved: The same absolute layer range (0-5) was applied to both models despite different total layers and capacities, leaving the scaling relationship unknown.
- What evidence would resolve it: Systematic ablation varying frozen layer proportions across model sizes (e.g., 1B, 3B, 7B, 13B) to identify the functional relationship between model depth and optimal freezing thresholds.

### Open Question 3
- Question: Does the finding that early MLP layers (0-5) store factual associations generalize across transformer architectures beyond OLMo?
- Basis in paper: Causal mediation analysis was conducted only on OLMo 1B and 7B models, but the architectures of LLaMA, GPT, and Mistral may distribute knowledge differently.
- Why unresolved: Different architectures use varying MLP configurations, layer normalization placements, and attention mechanisms that could alter where factual associations are stored.
- What evidence would resolve it: Replicating the causal tracing experiments on at least two other architectures (e.g., LLaMA-style and GPT-style) using equivalent synthetic QA datasets.

### Open Question 4
- Question: Is 125 samples sufficient for causal mediation analysis to reliably identify knowledge-critical layers, or does layer importance vary across knowledge domains?
- Basis in paper: The CMA experiments used only 125 samples from Subtask 2 (personal information), but the forget set spans creative documents, PII, and real documents across three subtasks.
- Why unresolved: Different knowledge types (narrative text, structured PII, factual content) may be stored in different layer patterns, and the small sample may not capture this variation.
- What evidence would resolve it: Stratified CMA across all subtask types with sample size ablation to determine minimum samples needed for stable layer identification.

## Limitations
- The approach shows degraded performance on larger models (7B variant experiencing 46% MMLU decrease), suggesting scalability limitations
- Causal mediation analysis assumes linear relationships between layers and associations, which may not capture complex nonlinear dependencies
- The adaptive cross-entropy penalty could lead to over-regularization, potentially masking underlying representational drift
- Evaluation focuses primarily on task-specific metrics rather than comprehensive probing of knowledge retention across diverse domains

## Confidence
- **High**: 1B model track results (2nd place, 0.652 final score) and task performance metrics (0.973 aggregate) are well-supported by competition evaluation
- **Medium**: Identification of early layers (0-5) as critical for subject-attribute associations requires further validation across different model architectures
- **Medium**: Comparative performance against baseline unlearning methods needs more rigorous ablation studies to isolate specific component contributions

## Next Checks
1. Conduct layer-wise ablation studies on the 7B variant to identify specific capacity bottlenecks and develop targeted layer-specific optimization strategies
2. Implement comprehensive knowledge probing across multiple domains to verify that MMLU preservation correlates with broader capability retention
3. Test the approach on transformer architectures with different attention mechanisms (sparse attention, local attention) to evaluate generalizability beyond standard full-attention models