---
ver: rpa2
title: 'Mind the Gap: Data Rewriting for Stable Off-Policy Supervised Fine-Tuning'
arxiv_id: '2509.15157'
source_url: https://arxiv.org/abs/2509.15157
tags:
- data
- policy
- rewriting
- training
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper addresses the instability in supervised fine-tuning
  (SFT) of large language models due to the distribution mismatch between expert demonstrations
  (behavior policy) and the evolving target model policy, which leads to high variance
  in importance sampling. The proposed method proactively reduces this policy gap
  by rewriting the training data through a three-stage alignment hierarchy: self-alignment
  (retaining correct model-generated responses), guided-alignment (re-solving problems
  with reference solutions in own words), and fallback to expert data when needed.'
---

# Mind the Gap: Data Rewriting for Stable Off-Policy Supervised Fine-Tuning
## Quick Facts
- arXiv ID: 2509.15157
- Source URL: https://arxiv.org/abs/2509.15157
- Reference count: 13
- Primary result: Data rewriting reduces policy gap in SFT, improving mathematical reasoning from 23.23% to 30.33% accuracy

## Executive Summary
This paper addresses a fundamental instability in supervised fine-tuning (SFT) of large language models: the distribution mismatch between expert demonstrations and the evolving target model policy leads to high variance in importance sampling and training instability. The authors propose a data rewriting approach that proactively reduces this policy gap through a three-stage alignment hierarchy—self-alignment, guided-alignment, and fallback to expert data—combined with importance sampling during training. The method shows consistent improvements on five mathematical reasoning benchmarks, particularly benefiting base models and demonstrating that data-level alignment is a powerful complement to optimization-level techniques.

## Method Summary
The proposed method tackles the distribution mismatch in SFT by rewriting training data to better align with the target model's policy. The approach operates in three stages: self-alignment retains model-generated responses that match expert answers, guided-alignment re-solves problems using reference solutions in the model's own words, and fallback uses expert data when needed. During training, importance sampling is used to handle residual distribution mismatch. The three-stage hierarchy progressively reduces the policy gap, while importance sampling manages any remaining discrepancy. This combination of proactive data alignment and adaptive training stabilizes the fine-tuning process and improves performance.

## Key Results
- Qwen2.5-Math-7B accuracy improved from 23.23% to 30.33% over standard SFT
- Outperformed state-of-the-art Dynamic Fine-Tuning approach, improving accuracy from 36.61% to 42.03%
- Demonstrated particular effectiveness on base models, showing data-level alignment complements optimization techniques

## Why This Works (Mechanism)
The instability in SFT arises from the distribution mismatch between the behavior policy (expert demonstrations) and the target policy (evolving model). This mismatch causes high variance in importance sampling weights, leading to unstable training. By rewriting the data to reduce this gap before training, the method decreases the variance in IS weights, making training more stable. The three-stage hierarchy ensures progressively better alignment: self-alignment captures the model's correct reasoning patterns, guided-alignment helps the model learn from expert solutions in its own terms, and fallback maintains quality when the model struggles. This proactive reduction of the policy gap addresses the root cause of instability rather than just mitigating its symptoms.

## Foundational Learning
**Importance Sampling (IS)**: A technique to correct for distribution mismatch by weighting samples according to the ratio of target and behavior policy probabilities. Why needed: Without IS, training on mismatched data leads to biased gradients and poor convergence. Quick check: Verify that IS weights have reasonable variance and don't explode.

**Behavior vs Target Policy Gap**: The discrepancy between the distribution of expert demonstrations and the model's own outputs. Why needed: This gap is the primary source of instability in off-policy SFT. Quick check: Measure KL divergence between behavior and target policies.

**Data Rewriting**: The process of modifying training data to better match the target model's policy. Why needed: Proactive alignment reduces the need for aggressive importance sampling. Quick check: Ensure rewritten data maintains semantic correctness.

## Architecture Onboarding
**Component Map**: Data Rewriting Pipeline -> Importance Sampling Module -> Training Loop -> Model
**Critical Path**: Rewriting (Self-Alignment → Guided-Alignment → Fallback) → IS Weight Calculation → Gradient Update
**Design Tradeoffs**: Rewriting quality vs computational cost, IS weight clipping vs variance reduction, alignment hierarchy depth vs effectiveness
**Failure Signatures**: High IS weight variance (indicates poor alignment), rewritten data quality degradation (indicates model weakness), training instability (indicates residual mismatch)
**First Experiments**: 1) Measure policy gap before/after rewriting, 2) Compare IS weight distributions with/without rewriting, 3) Ablation study of three rewriting stages

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, focusing instead on demonstrating the effectiveness of the proposed approach for mathematical reasoning tasks.

## Limitations
- Effectiveness may be limited to domains where correctness can be objectively verified, such as mathematical reasoning
- Reliance on model's self-alignment capability assumes sufficient initial reasoning ability, which may not hold for weaker base models
- Computational overhead of generating rewritten data at each training step could be prohibitive for very large-scale applications

## Confidence
**High confidence**: The core observation about distribution mismatch causing instability in SFT is well-established in the literature, and the experimental improvements over baselines are statistically significant and reproducible
**Medium confidence**: The effectiveness of the three-stage rewriting hierarchy is demonstrated but relies on synthetic data generation quality, which may vary across domains and model scales
**Medium confidence**: The superiority over Dynamic Fine-Tuning is shown on specific benchmarks but may not generalize to all reasoning tasks or model architectures

## Next Checks
1. Test the method on non-mathematical domains (e.g., creative writing, code generation) where correctness is more subjective and the model's self-alignment capability may be limited
2. Evaluate performance scaling on larger model sizes (70B+ parameters) to assess computational feasibility and effectiveness at frontier scales
3. Conduct ablation studies isolating the contribution of each rewriting stage and importance sampling to quantify their individual impacts on stability and performance