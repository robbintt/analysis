---
ver: rpa2
title: 'Code Generation with Small Language Models: A Codeforces-Based Study'
arxiv_id: '2504.07343'
source_url: https://arxiv.org/abs/2504.07343
tags:
- code
- phi-4-14b
- problems
- pass
- problem
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study benchmarks three small language models (LLAMA-3.2-3B,
  GEMMA-3-12B, and PHI-4-14B) on 280 Codeforces programming problems, finding that
  PHI-4-14B achieves the highest performance with a pass@3 of 63.6%, nearly matching
  the proprietary o3-mini-high (86.8%). The model shows strong results across 36 topics,
  particularly in Implementation (83.5%), Sorting (76.3%), and Strings (75.9%), and
  demonstrates semantic consistency of 77.5%.
---

# Code Generation with Small Language Models: A Codeforces-Based Study

## Quick Facts
- arXiv ID: 2504.07343
- Source URL: https://arxiv.org/abs/2504.07343
- Reference count: 25
- Small language models achieve up to 63.6% pass@3 on Codeforces problems, approaching proprietary model performance

## Executive Summary
This study benchmarks three small language models (PHI-4-14B, GEMMA-3-12B, and LLAMA-3.2-3B) on 280 Codeforces competitive programming problems, finding that PHI-4-14B achieves the highest performance with 63.6% pass@3, nearly matching the proprietary o3-mini-high (86.8%). The model demonstrates strong results across 36 topics, particularly in Implementation (83.5%), Sorting (76.3%), and Strings (75.9%), with 77.5% semantic consistency. Multi-language generation combining Python and C++ outputs improves performance to 73.6% pass@6. Qualitative analysis reveals most failures stem from minor implementation issues rather than fundamental reasoning flaws, indicating practical potential for small models as efficient, accessible alternatives to larger proprietary systems.

## Method Summary
The study generates Python solutions for 280 Codeforces problems (20 per rating level from 800-2100 Elo) using three small language models with Q4_K_M quantization. Models were configured with different temperatures (PHI-4-14B=0.4, GEMMA-3-12B=1.0, LLAMA-3.2-3B=0.8) and generated 3 solutions per problem via Ollama + LangChain API. Code was extracted using regex patterns and submitted to Codeforces for automated evaluation. Performance was measured using pass@k metrics, semantic consistency, and qualitative error analysis of failed submissions.

## Key Results
- PHI-4-14B achieves 63.6% pass@3, outperforming GEMMA-3-12B (19.6% pass@3) and LLAMA-3.2-3B (9.6% pass@3)
- Multi-language generation improves PHI-4-14B to 73.6% pass@6 by combining Python and C++ outputs
- Performance varies by topic: Implementation (83.5%), Sorting (76.3%), Strings (75.9%) show highest success rates
- Semantic consistency highest for GEMMA-3-12B (85.4%) despite lower accuracy, indicating different failure modes

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multi-language code generation expands problem coverage by leveraging language-specific computational advantages.
- Mechanism: Python and C++ solutions show approximately 20% non-overlap in problems solved. C++ excels on time-critical problems (25 of 28 uniquely solved C++ problems rated above 1500 Elo), while Python handles different problem subsets. Union of both languages yields higher aggregate pass rates (73.6% pass@6 vs 63.6% pass@3 for Python alone).
- Core assumption: Assumes developers can deploy multi-language pipelines or choose optimal language per problem type.
- Evidence anchors: [abstract] "Combining Python and C++ outputs further improves PHI-4-14B's pass@6 to 73.6%"; [Section III-F] "The sets of problems solved in each language differ around 20%."

### Mechanism 2
- Claim: Temperature tuning significantly affects SLM code generation quality, with lower temperatures improving accuracy for capable models.
- Mechanism: Default temperature (0.8) from LangChain Ollama API produced suboptimal results. Exploratory testing showed temperature 0.4 improved PHI-4-14B performance, suggesting code generation benefits from more deterministic sampling rather than exploratory diversity.
- Core assumption: Assumes temperature is the primary sampling parameter affecting code quality.
- Evidence anchors: [Section II, Models Setup] "An exploratory test with PHI-4-14B showed that temperature 0.4 performed better"; [Section III] PHI-4-14B achieved 48.3% pass@1, 63.6% pass@3 with tuned settings vs GEMMA-3-12B at 16.9%/19.6% with defaults.

### Mechanism 3
- Claim: SLM failures concentrate in minor implementation details rather than fundamental reasoning, enabling targeted repair strategies.
- Mechanism: Qualitative analysis of 21 failed PHI-4-14B submissions revealed 8 near-correct solutions requiring only small fixes (variable initialization, edge cases, output formatting), 3 time-limit issues solvable by language switch, and only 5 fundamentally flawed solutions.
- Core assumption: Assumes manual categorization by single expert (competitive programmer with 2245 Elo) generalizes.
- Evidence anchors: [abstract] "Qualitative analysis reveals many failures stem from minor implementation issues rather than reasoning flaws"; [Section III-I] "In five of the reviewed problems, PHI-4-14B produced almost correct code, needing simple fixes such as correcting variable initialization."

## Foundational Learning

- Concept: **pass@k metric**
  - Why needed here: Core evaluation measure throughout paper; estimates probability that at least one of k generated solutions is correct.
  - Quick check question: If a model generates 3 solutions per problem and solves 178 of 280 problems with at least one correct solution, what is its pass@3? (Answer: 63.6%)

- Concept: **Semantic Consistency (SC)**
  - Why needed here: Complements pass@k by measuring reliability across repeated attempts. GEMMA-3-12B shows highest SC (85.4%) despite lower accuracy.
  - Quick check question: Why might a model have high semantic consistency but low pass@k? (Answer: Consistently produces the same incorrect solution)

- Concept: **Quantization (Q4 K M)**
  - Why needed here: All evaluated models used 4-bit quantization, affecting inference quality and memory footprint.
  - Quick check question: What tradeoff does Q4 quantization introduce compared to full-precision inference? (Answer: Reduced memory/latency vs potential accuracy degradation)

## Architecture Onboarding

- Component map: Codeforces API -> Prompt generation -> Ollama + LangChain -> Model inference -> Code extraction (regex) -> Codeforces submission -> Verdict classification
- Critical path: Start with PHI-4-14B (Python) at temperature 0.4 for problems rated 800-1500 Elo → For failures, attempt C++ generation → For remaining failures, check for minor implementation issues before escalating to proprietary models
- Design tradeoffs: PHI-4-14B accuracy (63.6%) vs GEMMA-3-12B consistency (85.4%) → Choose based on whether reliable failure modes or higher success rate matters more; Python only (simpler pipeline) vs Python+C++ (higher coverage) → +10% pass rate vs doubled inference cost
- Failure signatures: Wrong Answer (33.9-58.5%) → Check edge cases, output formatting, variable initialization; Runtime Error (27% for LLAMA-3.2-3B) → Invalid memory access, uninitialized variables—consider model upgrade; Time Limit Exceeded (39.3% for GEMMA) → Algorithmic inefficiency—try C++ or prompt for optimization
- First 3 experiments: 1) Reproduce baseline: Run PHI-4-14B (temperature 0.4, Q4 quantization) on 20 problems each at Elo 800, 1200, 1600; measure pass@1 and pass@3; 2) Temperature sweep: Test PHI-4-14B at temperatures [0.2, 0.4, 0.6, 0.8, 1.0] on held-out 40-problem subset; 3) Multi-language delta: Generate both Python and C++ for 50 problems rated 1500+; quantify unique solves per language

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of SLMs change when evaluated on efficiency-oriented metrics such as inference latency, energy consumption, and cost-per-accepted solution?
- Basis in paper: [explicit] The authors explicitly state in the Future Work section: "we intend to include efficiency-oriented metrics such as inference time, latency, energy consumption, and cost-per-accepted solutions, which are critical for practical deployment but not assessed here."
- Why unresolved: The current study focuses solely on correctness (pass@k) and semantic consistency, neglecting the computational overhead required to achieve these results.
- What evidence would resolve it: A comparative benchmark measuring kilowatt-hours consumed and time taken per successful submission for SLMs versus proprietary models.

### Open Question 2
- Question: To what extent can automated self-repair mechanisms or agentic approaches correct the "near-correct" implementation errors identified in PHI-4-14B?
- Basis in paper: [inferred] The qualitative analysis reveals that many failures stem from "minor implementation issues" rather than reasoning flaws, and the authors suggest that "addressing such cases systematically could bring the model's performance even closer."
- Why unresolved: While the paper identifies the types of errors (e.g., edge cases, variable initialization), it does not test whether an automated feedback loop can fix them without human intervention.
- What evidence would resolve it: A follow-up experiment where the SLM is provided with the compilation error or failed test case as feedback to generate a corrected solution.

### Open Question 3
- Question: How sensitive are the reported performance gaps to specific hyperparameter configurations, such as temperature settings and quantization levels?
- Basis in paper: [explicit] The Threats to Validity and Future Work sections note that "results may vary with alternative configurations" and call for "ablation studies (e.g., temperature, seeds, quantization) to clarify configuration sensitivity."
- Why unresolved: The study relied on fixed settings (e.g., temperature 0.8 for most, quantization Q4_K_M) and an isolated finding that temperature 0.4 worked better for PHI-4, suggesting the current results may not represent optimal performance.
- What evidence would resolve it: A grid search analysis across different temperature and quantization settings showing the variance in pass@k rates for all three models.

## Limitations
- Reliance on manual expert evaluation for qualitative error analysis (21 problems reviewed by single expert) introduces subjectivity and limited generalizability
- Temperature optimization was exploratory rather than systematic, potentially missing optimal configurations for different problem types
- Codeforces verdicts may systematically bias against resource-intensive but conceptually sound solutions due to platform constraints

## Confidence
- High Confidence: PHI-4-14B achieves superior performance (63.6% pass@3) compared to other small models, and multi-language generation provides measurable accuracy gains
- Medium Confidence: Failures concentrate in minor implementation details rather than fundamental reasoning, based on limited qualitative sample
- Low Confidence: Temperature sensitivity being the primary driver of performance differences without systematic parameter sweeps

## Next Checks
1. Independent Error Categorization: Replicate qualitative analysis with 3-5 independent reviewers categorizing the same 21 failed PHI-4-14B solutions to establish inter-rater reliability
2. Systematic Temperature Sweep: Conduct comprehensive temperature sensitivity study across all three models testing temperatures [0.2, 0.4, 0.6, 0.8, 1.0] on stratified 100-problem sample
3. Cross-Platform Validation: Test same models and prompt strategies on alternative competitive programming platforms (LeetCode, AtCoder) to verify generalizability beyond Codeforces-specific constraints