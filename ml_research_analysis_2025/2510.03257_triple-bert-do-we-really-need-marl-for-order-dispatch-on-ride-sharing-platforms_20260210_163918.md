---
ver: rpa2
title: 'Triple-BERT: Do We Really Need MARL for Order Dispatch on Ride-Sharing Platforms?'
arxiv_id: '2510.03257'
source_url: https://arxiv.org/abs/2510.03257
tags:
- order
- time
- methods
- orders
- each
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes Triple-BERT, a centralized single-agent reinforcement
  learning method for large-scale order dispatch in ride-sharing platforms. The method
  addresses the challenges of vast action and observation spaces by decomposing joint
  action probabilities into individual driver action probabilities and introducing
  a novel BERT-based neural network with parameter reuse.
---

# Triple-BERT: Do We Really Need MARL for Order Dispatch on Ride-Sharing Platforms?

## Quick Facts
- arXiv ID: 2510.03257
- Source URL: https://arxiv.org/abs/2510.03257
- Reference count: 40
- Primary result: 11.95% improvement over state-of-the-art methods with 4.26% increase in served orders

## Executive Summary
This paper proposes Triple-BERT, a centralized single-agent reinforcement learning (SARL) method for large-scale order dispatch in ride-sharing platforms. The method addresses the challenges of vast action and observation spaces by decomposing joint action probabilities into individual driver action probabilities and introducing a novel BERT-based neural network with parameter reuse. A two-stage training strategy is employed to tackle sample scarcity, using pre-trained feature extractors from a MARL approach. Experimental results on real-world Manhattan ride-hailing data show that Triple-BERT achieves a 11.95% improvement over state-of-the-art methods, with a 4.26% increase in served orders and a 22.25% reduction in pickup times.

## Method Summary
Triple-BERT uses a two-stage training approach to solve large-scale order dispatch. In Stage 1, encoders are pre-trained using Independent DDQN (MARL) to learn general feature representations. In Stage 2, a centralized TD3 algorithm fine-tunes the entire network. The method decomposes the joint action space by outputting utility logits for all driver-order pairs, then uses an ILP solver to find the optimal matching. A BERT-based architecture with parameter reuse and positive normalization processes the state information. The action selection is performed via ILP on log-probabilities, making the method scalable to thousands of drivers and orders.

## Key Results
- 11.95% improvement in overall reward compared to state-of-the-art MARL methods
- 4.26% increase in served orders and 22.25% reduction in pickup times
- Strong generalization across different datasets and driver volumes
- Outperforms baselines in terms of delivery, detour, pickup, and confirmation times

## Why This Works (Mechanism)

### Mechanism 1: Action Decomposition via Logit Model
The method decomposes the exponentially large joint action space into manageable individual driver selection probabilities. Instead of directly sampling massive joint action vectors, the actor network outputs a utility matrix for all driver-order pairs. The probability of a joint action is modeled as the product of individual selection probabilities, with an ILP solver selecting the optimal matching. This assumes complex inter-driver dependencies are captured implicitly by global state encoding rather than explicit joint action sampling.

### Mechanism 2: Parameter Reuse & Positive Normalization
A BERT-based architecture with shared parameters processes driver and order sequences. The QK-Attention module outputs utility values, while positive normalization (forcing non-negative attention weights with L2 norm of 1) stabilizes training. The model omits positional embeddings, treating relationships as permutation invariant. This normalization solves parameter redundancy issues that caused instability in earlier versions.

### Mechanism 3: Two-Stage Training (Warm-Start)
To address sample scarcity in SARL, the method uses MARL (IDDQN) in Stage 1 to pre-train feature encoders, then switches to centralized TD3 in Stage 2. This assumes feature representations learned from local rewards transfer effectively to global reward optimization. Without Stage 1 pre-training, the model fails to converge and exhibits significant fluctuations.

## Foundational Learning

- **Markov Decision Process (MDP) vs. POMDP**: The dispatch problem is formulated as a centralized MDP with complete state information. Quick check: Does the agent see the location of every single driver and order at every step?

- **The "Curse of Dimensionality"**: CTDE MARL fails due to exponential growth in joint action space (approximately 10^30 for 1000 drivers/10 orders). Quick check: Why can't standard DQN handle the action space of 1000 drivers directly?

- **Attention Mechanisms (Self-Attention)**: The BERT backbone uses self-attention to aggregate information across variable-length sequences. Quick check: How does self-attention allow the model to handle a variable number of inputs without changing network parameters?

## Architecture Onboarding

- **Component map**: Worker Sequence & Order Sequence -> Encoder (BiLSTM + MLP) -> Actor-BERT -> QK-Attention (Logits) -> ILP -> Action

- **Critical path**: State -> Encoder -> Actor-BERT -> QK-Attention (Logits) -> ILP -> Action

- **Design tradeoffs**: Centralized approach is more sensitive to single points of failure compared to distributed MARL. QK-Attention reduces complexity from O(|F|路n路m) to O(|f|路n + |g|路m), trading depth for breadth.

- **Failure signatures**: Missing positive normalization causes training instability and poor performance. Skipping Stage 1 leads to non-convergence and reward collapse.

- **First 3 experiments**: 
  1. Compare random matching vs. distance-based greedy matching as baseline
  2. Train Triple-BERT from scratch (skipping Stage 1 IDDQN) to verify sample scarcity hypothesis
  3. Toggle positive normalization in QK-Attention to test its impact on stability

## Open Questions the Paper Calls Out

### Open Question 1
Can offline reinforcement learning effectively replace the online MARL pre-training stage (Stage 1) to initialize the feature extractors? The paper suggests investigating offline training as a future direction, but it's unknown if static datasets can provide sufficient state coverage without MARL exploration.

### Open Question 2
How can the centralized Triple-BERT framework be made robust to single points of failure or missing global state information? The model is more sensitive to comprehensive information loss, but the paper doesn't explore how attention mechanisms degrade with partial state corruption.

### Open Question 3
Does applying importance sampling to the actor optimization improve the stability or convergence speed of policy gradient updates? The current method uses specific noise injection, but it's unclear if importance sampling would yield more efficient gradient estimates.

## Limitations
- The joint action decomposition assumption may break down with strong driver-driver interactions or complex constraints
- Two-stage training relies heavily on transferability of MARL feature representations to SARL
- Permutation-invariant assumption may limit performance in geographically structured markets
- Centralized approach is more sensitive to single points of failure compared to distributed MARL

## Confidence

- **High Confidence**: Experimental results showing 11.95% improvement and technical implementation of ILP-based action selection
- **Medium Confidence**: Mechanism explanations for action decomposition and parameter reuse, as these rely on theoretical assumptions
- **Low Confidence**: Claim that centralized SARL is more sensitive to single points of failure - requires empirical validation

## Next Checks

1. **Robustness Test**: Evaluate Triple-BERT performance when 10-20% of drivers become unavailable mid-episode to validate the single-point-of-failure claim

2. **Cross-City Generalization**: Test the model on taxi data from cities with different geographic structures (grid vs. radial layouts) to assess permutation-invariant assumption validity

3. **Constraint Violation Analysis**: Systematically introduce hard constraints between specific driver pairs and measure how much the product assumption degrades performance compared to explicit joint action sampling