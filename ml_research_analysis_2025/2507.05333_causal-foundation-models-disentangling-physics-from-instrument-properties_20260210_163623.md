---
ver: rpa2
title: 'Causal Foundation Models: Disentangling Physics from Instrument Properties'
arxiv_id: '2507.05333'
source_url: https://arxiv.org/abs/2507.05333
tags:
- latent
- foundation
- stellar
- learning
- instrument
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces a causally-motivated foundation model for
  structured time series data that explicitly disentangles physical phenomena from
  instrument-specific distortions. The method uses a dual-encoder architecture trained
  with structured contrastive learning, leveraging naturally occurring observational
  triplets where the same target is measured under varying conditions and distinct
  targets are measured under shared conditions.
---

# Causal Foundation Models: Disentangling Physics from Instrument Properties

## Quick Facts
- **arXiv ID:** 2507.05333
- **Source URL:** https://arxiv.org/abs/2507.05333
- **Reference count:** 11
- **Primary result:** Dual-encoder foundation model trained with structured contrastive learning achieves superior few-shot prediction of stellar parameters using ten times less data than single-latent space baselines.

## Executive Summary
This work introduces a causally-motivated foundation model for structured time series that explicitly disentangles physical phenomena from instrument-specific distortions. The method uses a dual-encoder architecture trained with structured contrastive learning, leveraging naturally occurring observational triplets where the same target is measured under varying conditions and distinct targets are measured under shared conditions. The model learns separate latent representations for underlying physical signals and instrument effects, enabling few-shot generalization and efficient adaptation. Evaluated on simulated astronomical time series resembling variable stars observed by NASA's TESS mission, the method significantly outperforms traditional single-latent space foundation models on downstream prediction tasks, particularly in low-data regimes.

## Method Summary
The method employs a dual-encoder architecture where one encoder learns stellar representations and another learns instrument representations. Both encoders process identical time series inputs but learn separate 20-dimensional latent spaces through structured contrastive learning. The training objective combines reconstruction loss with two InfoNCE contrastive losses: one that clusters same-star observations across instruments in the stellar space, and another that clusters same-instrument observations across stars in the instrument space. The decoder reconstructs observations using element-wise multiplication of projected latent representations. The method is trained on simulated light curves generated with multiplicative instrument distortions and additive noise, then evaluated on few-shot downstream tasks predicting stellar parameters.

## Key Results
- Achieves superior R² scores for predicting stellar parameters using ten times less training data compared to baseline approaches
- Successfully disentangles physical and instrumental factors in latent space, with stellar space capturing physical properties while minimizing instrument clustering
- Instrument space reveals strong sector structure with minimal physical information leakage
- Method outperforms single-latent space foundation models across all data regimes tested

## Why This Works (Mechanism)

### Mechanism 1: Structured Triplet Contrastive Learning for Disentanglement
The InfoNCE loss creates attractive forces between embeddings that share a common factor (stellar identity or instrument identity) while pushing apart embeddings that differ. By applying separate contrastive objectives to each latent space with appropriately structured positives/negatives, the model learns to encode only the relevant factor in each space. This works because stellar properties are independent of instrument configuration given a star, and instrument effects are independent of stellar properties given an instrument.

### Mechanism 2: Multiplicative Decoder Matching Generative Structure
The element-wise multiplicative combination of stellar and instrumental latent projections aligns with the causal generative model of observations. The simulator generates observations as F_observed = S(t) × F_true(t) + O(t), where S is a scale factor and O is an offset. The decoder's Hadamard product mirrors this multiplicative structure, enabling the reconstruction loss to provide factor-specific gradients that reinforce separation.

### Mechanism 3: Dual Encoder Competition for Information
Competing contrastive objectives across two encoders create pressure for each to ignore the other's target factor. The stellar encoder receives gradients to cluster same-star observations while the instrument encoder clusters same-instrument observations. Since both encoders receive identical inputs, each must learn to discard information relevant to the competing objective. The reconstruction loss ensures neither encoder collapses to trivial solutions.

## Foundational Learning

- **Concept: InfoNCE / Contrastive Learning**
  - **Why needed here:** The core training signal comes from structured contrastive losses that enforce invariance properties. Without understanding how positives/negatives shape the embedding space, you cannot debug disentanglement failures.
  - **Quick check question:** Can you explain why multiple positives per anchor (vs. single positive in SimCLR) changes the gradient structure?

- **Concept: Causal Representation Learning / Disentanglement**
  - **Why needed here:** The method is explicitly motivated by causal structure—understanding what "disentanglement" means formally (independence of latent factors given appropriate conditioning) is required to evaluate whether the model succeeds.
  - **Quick check question:** What does it mean for z_star to be "invariant" to instrument changes, and how would you measure this?

- **Concept: Foundation Model Transfer Paradigm**
  - **Why needed here:** The model is evaluated on few-shot downstream tasks. Understanding why pretrained representations should generalize with limited data (and when they won't) is essential for interpreting results.
  - **Quick check question:** Why does disentanglement specifically help in low-data regimes compared to entangled representations?

## Architecture Onboarding

- **Component map:**
  - Time series data → Dual Conformer encoders (E_star, E_instr) → 20-dim latent spaces (z_star, z_instr) → Projection heads → Contrastive embeddings → InfoNCE losses → Decoder (D) → Hadamard product (g_star ⊙ g_instr) → Reconstructed time series → Combined loss

- **Critical path:**
  1. Data pipeline must construct valid triplets (anchor, same-star, same-instrument) from metadata
  2. Both encoders forward-pass the same input; projection heads produce contrastive embeddings
  3. InfoNCE losses computed per latent space with structured positives
  4. Decoder reconstructs from combined latents; reconstruction loss backpropagates to both encoders
  5. At inference, discard projection heads; downstream tasks use raw latents z_star or z_instr

- **Design tradeoffs:**
  - Encoder complexity: Paper notes simpler MLP encoders achieve similar disentanglement—the contrastive objective drives separation, not architecture sophistication
  - Latent dimensionality: 20-dim per space; insufficient capacity may force information into wrong space
  - Temperature τ in InfoNCE: Controls hardness of negatives; not specified in paper but critical for convergence

- **Failure signatures:**
  - Information leakage: If z_instr correlates with stellar parameters, contrastive structure may be insufficient or triplet quality poor
  - Collapsed latents: If one encoder produces near-constant embeddings, its contrastive loss may be too weak or reconstruction possible from single latent
  - No generalization gain: If downstream performance matches baseline, disentanglement may be superficial—verify latent space structure before blaming downstream architecture

- **First 3 experiments:**
  1. **Triplet ablation:** Train with only same-star pairs (no instrument contrastive loss) and compare latent space clustering to confirm structured triplets are necessary
  2. **Decoder perturbation:** Replace Hadamard product with concatenation + MLP to test whether multiplicative structure contributes to disentanglement or is incidental
  3. **Leakage quantification:** Train a classifier to predict stellar parameters from z_instr; if accuracy is high, investigate whether triplet construction or loss weighting is insufficient

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can minimizing mutual information between the stellar and instrument latent spaces eliminate the observed leakage of physical information into z_instr?
- **Basis in paper:** [explicit] "However, we also observe a moderate correlation between θs,0 and the instrument latent space, indicating some leakage of physical information into z_instr... In future work, we will explore minimizing the mutual information between the latent spaces to improve the separations."
- **Why unresolved:** The current contrastive objective alone does not fully disentangle the factors; physical properties partially correlate with instrument embeddings, suggesting an information bottleneck or objective gap.
- **What evidence would resolve it:** A modified loss incorporating mutual information regularization that achieves lower correlation between θs,0 and z_instr while maintaining downstream task performance.

### Open Question 2
- **Question:** Does the triplet-based contrastive learning framework generalize to domains where observational triplets are sparse, noisy, or must be inferred rather than explicitly available in metadata?
- **Basis in paper:** [inferred] The method relies on "naturally occurring observational triplets" with clear structure. Real-world datasets may have incomplete overlap, missing metadata, or weaker instrument/physics separation than astronomical surveys.
- **Why unresolved:** The evaluation uses simulated data with clean triplet structure; robustness to imperfect or constructed triplets remains untested.
- **What evidence would resolve it:** Experiments on datasets with synthetic triplet corruption (missing anchors, label noise) or domains requiring inferred triplet relationships (e.g., biomedical sensors without explicit calibration records).

### Open Question 3
- **Question:** Can the dual-encoder disentanglement approach extend effectively to multimodal settings where different modalities carry distinct physical and instrumental signatures?
- **Basis in paper:** [explicit] "Future work will explore... extensions to multimodal and cross-domain foundation models."
- **Why unresolved:** The current architecture and loss are designed for single-modality time series; multimodal integration introduces new challenges in aligning latent spaces across modalities while maintaining disentanglement.
- **What evidence would resolve it:** A multimodal variant evaluated on tasks requiring cross-modal transfer (e.g., light curves paired with spectra), demonstrating that disentangled representations improve over fused single-latent baselines.

### Open Question 4
- **Question:** How does performance scale when deployed on large mission-scale real datasets compared to the 40,000-curve simulated setting?
- **Basis in paper:** [explicit] "Future work will explore... deployment on large mission-scale datasets" and "Our preliminary experiments on NASA TESS light curves demonstrate the method's potential for real-world application."
- **Why unresolved:** Systematic evaluation on real TESS or similar mission data at scale is not presented; simulated-to-real transfer and computational scalability remain open.
- **What evidence would resolve it:** Benchmarks on full TESS sectors or similar surveys showing comparable disentanglement quality and few-shot generalization to the simulated results.

## Limitations

- The method relies heavily on naturally occurring observational triplets, which may not exist in many domains beyond astronomy
- The simulated data uses multiplicative distortions with additive noise, which may not capture the full complexity of real instrument effects
- The decoder's multiplicative structure is assumed to match the generative process, but real instrument distortions may require more complex (nonlinear) combinations

## Confidence

- **High confidence**: The core mechanism of using structured contrastive learning with dual encoders to enforce factor-specific invariance
- **Medium confidence**: The claim that disentanglement specifically improves few-shot generalization
- **Low confidence**: The generalizability claim to "any structured time series data" with observational triplets

## Next Checks

1. **Triplet quality analysis**: Systematically vary the number and quality of natural triplets in the training data and measure the impact on disentanglement quality and downstream performance
2. **Real data validation**: Apply the method to real astronomical light curves from TESS and compare the learned stellar representations against known stellar catalogs to verify physical signal recovery
3. **Generalization stress test**: Test the model on time series with instrument distortions that violate the multiplicative assumption (e.g., saturation, clipping beyond the simulated clip[-1,1] operation) to assess robustness