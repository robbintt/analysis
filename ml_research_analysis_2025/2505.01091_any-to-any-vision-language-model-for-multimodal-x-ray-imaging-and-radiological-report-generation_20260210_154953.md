---
ver: rpa2
title: Any-to-Any Vision-Language Model for Multimodal X-ray Imaging and Radiological
  Report Generation
arxiv_id: '2505.01091'
source_url: https://arxiv.org/abs/2505.01091
tags:
- data
- codi
- generation
- medical
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of generating high-quality multimodal
  medical data, specifically chest X-rays and their corresponding clinical reports,
  to overcome data scarcity and privacy constraints in healthcare. The authors propose
  CoDi XR, an adaptation of the Composable Diffusion (CoDi) framework, which leverages
  latent diffusion models and contrastive learning to align and generate coherent
  multimodal outputs.
---

# Any-to-Any Vision-Language Model for Multimodal X-ray Imaging and Radiological Report Generation

## Quick Facts
- arXiv ID: 2505.01091
- Source URL: https://arxiv.org/abs/2505.01091
- Reference count: 40
- Primary result: Domain-adapted CoDi framework generates multi-view chest X-rays and radiology reports with improved FID and BLEU scores over baseline, achieving comparable or better performance than real data on downstream disease classification.

## Executive Summary
This paper addresses the challenge of generating high-quality multimodal medical data by proposing CoDi XR, an adaptation of the Composable Diffusion framework for chest X-ray imaging and radiology report generation. The framework leverages latent diffusion models and contrastive learning to align and generate coherent multimodal outputs from the MIMIC-CXR dataset. Through three-stage training on frontal and lateral X-rays with their associated reports, CoDi XR demonstrates significant improvements in both image and text generation quality compared to the baseline model, while maintaining or exceeding real data performance on downstream pathology classification tasks.

## Method Summary
CoDi XR adapts the Composable Diffusion framework for medical imaging by implementing a three-stage training process. First, a shared latent space is established using CLIP with ViT-base image encoders and Transformer text encoders trained with InfoNCE loss. Second, separate latent diffusion models are trained for each modality - X-ray LDMs using Stable Diffusion 1.5 architecture with frozen AutoKL VAE and fine-tuned UNet, and report LDMs using Optimus VAE with FCResBlock UNet. Third, cross-modal alignment is achieved by training cross-attention layers and encoders between modality pairs. The model is trained on preprocessed MIMIC-CXR data with 154,721 X-rays from 78,584 studies, evaluating performance using FID, BLEU scores, and downstream disease classification metrics.

## Key Results
- CoDi XR achieves significantly lower FID scores compared to baseline CoDi (e.g., 0.86 vs higher values for text-to-front X-ray generation)
- BLEU-1/2/3/4 scores show substantial improvement in generated report quality over baseline
- Downstream pathology classification performance (AUROC/F1) matches or exceeds real data performance on 10 disease categories

## Why This Works (Mechanism)
The framework's success stems from effective alignment of multimodal representations through contrastive learning, enabling coherent generation across X-ray images and clinical reports. The three-stage training approach allows for progressive refinement: first establishing shared latent space, then optimizing individual modalities, and finally enabling cross-modal generation through learned attention mechanisms.

## Foundational Learning
- **Latent Diffusion Models (LDMs)**: Denoising diffusion models operating in compressed latent space rather than pixel space; needed for efficient high-resolution image generation; quick check: verify VAE reconstruction quality on medical images.
- **Contrastive Learning (InfoNCE loss)**: Learning representations by maximizing agreement between related samples while minimizing agreement between unrelated samples; needed for aligning image and text embeddings; quick check: confirm alignment by computing nearest neighbors in shared space.
- **Cross-Attention Mechanisms**: Enabling one modality to condition generation of another through learned attention weights; needed for any-to-any generation capability; quick check: visualize attention maps during generation.
- **CLIP Architecture**: Joint image-text embedding model using separate encoders; needed for establishing shared latent space; quick check: measure alignment quality using cosine similarity.
- **Stable Diffusion Architecture**: Latent diffusion framework with U-Net denoiser and VAE; needed for efficient X-ray generation; quick check: evaluate reconstruction quality with Fréchet Inception Distance.
- **CheXpert Labeler**: Automated radiology report classification tool; needed for downstream evaluation; quick check: validate labeler performance on MIMIC-CXR subset.

## Architecture Onboarding
**Component Map**: MIMIC-CXR Data -> CLIP Pre-training -> X-ray LDM Training -> Report LDM Training -> Cross-Modal Alignment -> Evaluation

**Critical Path**: Data preprocessing → CLIP contrastive learning → Modality-specific LDM training → Cross-attention fine-tuning → Joint generation

**Design Tradeoffs**: 
- Separate LDMs per modality vs. single unified model (chosen for better specialization)
- Fine-tuning vs. training from scratch (chosen for efficiency given 6.77B parameters)
- Impression-only reports vs. full reports (chosen for consistency and length control)

**Failure Signatures**:
- Poor FID scores (>500) indicate failure in image generation pipeline
- BLEU scores near zero suggest text generation collapse
- Degradation when conditioning on X-ray input indicates insufficient cross-modal training

**3 First Experiments**:
1. Validate CLIP alignment by measuring cosine similarity between paired X-ray-report embeddings
2. Test individual modality LDMs for reconstruction quality before cross-modal training
3. Evaluate zero-shot generation capability (text-to-X-ray) before full any-to-any training

## Open Questions the Paper Calls Out
None

## Limitations
- Limited to chest X-rays without addressing other modalities like CT or MRI
- Ethical implications of synthetic data use and potential biases not thoroughly addressed
- Critical hyperparameter details for cross-modal alignment and VAE pre-training are unspecified

## Confidence
- **High Confidence**: Core methodology and quantitative improvements over baseline are clearly specified and supported
- **Medium Confidence**: Claims about matching real data performance on classification tasks are supported but lack explicit statistical validation
- **Low Confidence**: Qualitative claims about clinical utility enhancements are not rigorously validated beyond the MIMIC-CXR dataset

## Next Checks
1. Reproduce cross-modal alignment stage with explicit hyperparameter tuning to confirm reported FID and BLEU score improvements
2. Analyze model outputs for potential biases using external datasets or subgroup evaluations
3. Evaluate generalization on independent chest X-ray datasets (e.g., CheXpert or PadChest) to assess robustness beyond MIMIC-CXR