---
ver: rpa2
title: Multi-Task Vehicle Routing Solver via Mixture of Specialized Experts under
  State-Decomposable MDP
arxiv_id: '2510.21453'
source_url: https://arxiv.org/abs/2510.21453
tags:
- basis
- moses
- time
- cada
- lora
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a new framework for multi-task vehicle routing
  problems (VRPs) that leverages the compositional structure of VRP variants by reusing
  basis solvers specialized for each component. The authors introduce a State-Decomposable
  MDP (SDMDP) framework to reformulate VRPs, enabling the optimal unified policy to
  be recovered from optimal basis policies via a mixture function in the latent space.
---

# Multi-Task Vehicle Routing Solver via Mixture of Specialized Experts under State-Decomposable MDP

## Quick Facts
- arXiv ID: 2510.21453
- Source URL: https://arxiv.org/abs/2510.21453
- Reference count: 40
- Key outcome: MoSES achieves 25.6% better average optimality gaps than state-of-the-art unified solvers across 16 VRP variants

## Executive Summary
This paper introduces a new framework for solving multiple Vehicle Routing Problem (VRP) variants using a single neural solver. The key insight is that VRP variants can be viewed as compositions of shared constraints, allowing for policy reuse across tasks. The authors propose a State-Decomposable MDP (SDMDP) framework that enables the optimal unified policy to be recovered from optimal basis policies via a mixture function in latent space. They implement this as a Mixture-of-Specialized-Experts Solver (MoSES) using LoRA experts and an adaptive gating mechanism, demonstrating superior performance over prior methods with strong out-of-distribution generalization.

## Method Summary
The method builds on a pretrained backbone model (RF-TE or CaDA) trained on CVRP, then adds 4 LoRA-based expert modules specialized for different VRP constraints (Open Route, Backhaul, Duration Limit, Time Window). These frozen experts are combined with a trainable residual LoRA using an adaptive gating network that determines expert contributions. The gating mechanism uses either softmax (convex combination) or sigmoid (independent selection) activation. The model is trained on mixed batches of all 16 VRP variants with dense routing strategy, where all experts contribute to each prediction.

## Key Results
- MoSES reduces average optimality gaps by 25.6% compared to state-of-the-art unified solvers
- Achieves strong out-of-distribution generalization across 16 VRP variants
- Dense routing strategy outperforms variant-aware routing, suggesting transferable skills between experts
- Mild computational overhead from fine-grained aggregation, but maintains efficiency

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Reformulating VRPs as State-Decomposable MDPs enables a single solver to handle combinatorial variants by perceiving them as compositions of shared constraints.
- **Mechanism:** The framework decomposes complex VRP states into conditionally independent basis states, allowing the model to reuse optimal policies for basis constraints across any variant containing those constraints.
- **Core assumption:** VRP constraints are modular and can evolve conditionally independently given an action.
- **Evidence anchors:** Section 4.1 defines SDMDP with state space as Cartesian product of basis state spaces and factorized transition probabilities.
- **Break condition:** If VRP constraints interact non-linearly such that transition dynamics of one constraint fundamentally alter feasibility or transition probabilities of another in an unfactorizable way.

### Mechanism 2
- **Claim:** The optimal unified policy can be recovered by mixing the latent embeddings of optimal basis policies via a learnable function.
- **Mechanism:** LS-SDMDP extracts embeddings from optimal basis policies and uses a mixture function to map these basis embeddings to a unified state embedding, allowing the unified solver to leverage specialized knowledge.
- **Core assumption:** A deterministic bijective mixture function exists that can map basis embeddings to unified state embedding, and the optimal unified policy aligns with this mixture.
- **Evidence anchors:** Section 4.2 introduces LS-SDMDP with mixture function $z_t = f_\phi(z^{(b_0)}_t, \dots, z^{(b_m)}_t; s_t)$ and Theorem 2 claims recoverability.
- **Break condition:** If the mixture function cannot effectively resolve conflicts between basis embeddings or the bijection requirement fails.

### Mechanism 3
- **Claim:** Implementing basis policies as LoRA experts with dynamic gating mechanism provides parameter-efficient method to switch between or combine constraint-specific heuristics.
- **Mechanism:** MoSES uses frozen backbone with fine-tuned LoRA adapters as experts, where an adaptive gating network calculates coefficients to weight each expert's contribution during inference.
- **Core assumption:** Knowledge for basis VRP variants can be compressed into low-rank matrix updates, and gating signal is sufficient to identify relevant experts.
- **Evidence anchors:** Section 4.3 describes forward pass with weighted sum of backbone and expert outputs; ablation studies show dense routing performs best.
- **Break condition:** If LoRA ranks are too low to capture basis constraint complexity or gating network suffers from collapse (always selecting same expert).

## Foundational Learning

- **Concept:** Markov Decision Processes (MDPs) in Combinatorial Optimization
  - **Why needed here:** The paper reformulates VRPs using custom SDMDP definition; understanding standard MDPs is required to grasp state space decomposition.
  - **Quick check question:** Can you explain how "transition probability" in standard VRP differs from "factorized transition probability" in SDMDP?

- **Concept:** Parameter-Efficient Fine-Tuning (PEFT) / LoRA
  - **Why needed here:** Practical implementation relies entirely on LoRA to serve as "experts"; understanding LoRA injection of trainable low-rank matrices is crucial.
  - **Quick check question:** If linear layer has weights $W_0$ and LoRA adds $BA$, how does inference cost change compared to full fine-tuning?

- **Concept:** Mixture of Experts (MoE) Routing
  - **Why needed here:** Core of solver is adaptive gating mechanism; distinguishing "hard" routing (selecting one expert) from "soft" routing (weighted average) is essential.
  - **Quick check question:** Why might "sigmoid" activation in gating function behave differently than "softmax" activation in terms of expert selection?

## Architecture Onboarding

- **Component map:** Input Instance → Encoder Layers → Frozen Backbone + All LoRA Experts (parallel) → Gating Network computes coefficients → Weighted sum of outputs + Residual LoRA → Decoder constructs solution

- **Critical path:** 1) Input passes through frozen backbone and all LoRA experts in parallel 2) Gating network inspects input and computes coefficients $\alpha$ 3) Layer output = backbone output + weighted sum of LoRA expert outputs + residual output 4) Decoder uses aggregated embeddings to construct solution

- **Design tradeoffs:**
  - **Routing Strategy:** Dense Routing (use all experts) vs. Variant-Aware Routing (use only relevant experts). Paper finds Dense Routing works better, suggesting experts offer transferable skills but increases compute.
  - **Activation Functions:** Softmax (convex combination) vs. Sigmoid (independent selection). Softmax enforces competition; Sigmoid allows expert to be "on" even if others are also fully "on."
  - **LoRA Rank:** High rank captures more basis information but increases parameters. Paper suggests trainable residual LoRA is more critical for larger problem scales ($N=100$) than frozen experts.

- **Failure signatures:**
  - **Expert Collapse:** Gating network outputs uniform weights or zero weights for specific experts, resulting in performance identical to backbone.
  - **OOD Degradation:** Model fails to generalize if combination of constraints in test set was never seen in gating training distribution.
  - **Constraint Violation:** Gating network fails to sufficiently weight expert responsible for specific constraint (e.g., Time Windows), resulting in infeasible solutions.

- **First 3 experiments:**
  1. **Sanity Check - Basis Expert Validation:** Train individual LoRA adapters for each basis VRP (OVRP, VRPB, etc.) on frozen backbone. Verify they outperform raw backbone on specific tasks to ensure "experts" are actually expert.
  2. **Gating Ablation:** Train unified MoSES model with different routing strategies (Dense vs. Variant-Aware) on small subset of VRP variants. Plot weights assigned by gating network to verify correlation with input instance constraints.
  3. **OOD Generalization Test:** Train model on subset of VRP variants (e.g., only those with Capacity and Time Windows) and test on held-out combination (e.g., Capacity, Time Windows, AND Backhauls) to test if compositionality handles unseen combinations.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can more efficient aggregation techniques be developed to reduce computational overhead of MoSES's fine-grained routing while maintaining solution quality?
- **Basis in paper:** Section 6 states "This limitation suggests designing more efficient aggregation techniques which preserve decent performance as future research directions."
- **Why unresolved:** Current method incurs mild overhead due to layer-wise and token-wise aggregation mechanisms compared to baselines.
- **Evidence:** Routing algorithm achieving similar optimality gaps with inference speeds comparable to backbone models (RF-TE or CaDA).

### Open Question 2
- **Question:** How can State-Decomposable MDP framework be effectively extended to broader decision-making settings beyond multi-task VRPs?
- **Basis in paper:** Section 6 notes "Extending this framework to broader and general decision-making settings is also appealing."
- **Why unresolved:** State decomposition relies on specific compositional structure of VRP variants; unclear if "basis task" concept transfers to general RL.
- **Evidence:** Successful application of LS-SDMDP formulation to standard multi-task RL benchmarks (e.g., Meta-World or MuJoCo tasks) demonstrating policy reuse.

### Open Question 3
- **Question:** Does framework's reliance on conditionally independent basis states limit applicability to VRP variants with highly coupled or interacting constraints?
- **Basis in paper:** Section 4.1 assumes basis states evolve "conditionally independently," which may not strictly hold for complex, interacting real-world constraints.
- **Why unresolved:** Experiments cover 16 variants derived from 5 basis constraints but do not test variants where basis state transitions are statistically dependent.
- **Evidence:** Empirical results on VRP variants with dependent constraints (e.g., specific inventory-routing or pickup-and-delivery pairs) showing if performance degrades compared to independent variants.

## Limitations

- Core SDMDP framework assumes conditional independence of constraint dynamics, which may not hold for all VRP variants where constraints interact non-linearly
- Adaptive gating mechanism creates computational overhead that scales with number of basis variants
- Theoretical claims about state-decomposability and mixture function recoverability lack rigorous guarantees

## Confidence

- **High confidence** in empirical results showing MoSES outperforms baselines on in-distribution test sets (25.6% reduction in optimality gap)
- **Medium confidence** in generalization claims across 16 VRP variants
- **Low confidence** in theoretical claims about state-decomposability and conditional independence assumptions

## Next Checks

1. **Constraint Interaction Analysis:** Systematically test VRP variants where constraints have known interactions (e.g., capacity and time windows creating tight coupling) to validate whether conditional independence assumption holds in practice.

2. **Gating Network Stress Test:** Create synthetic instances with contradictory or conflicting constraint signals to test whether gating network can appropriately resolve expert conflicts rather than simply averaging them.

3. **Computational Efficiency Benchmark:** Measure wall-clock inference time for MoSES compared to baseline unified solvers across different instance sizes to quantify claimed computational overhead and determine if it becomes prohibitive at scale.