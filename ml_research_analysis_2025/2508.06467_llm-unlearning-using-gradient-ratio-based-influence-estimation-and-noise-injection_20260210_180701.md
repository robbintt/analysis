---
ver: rpa2
title: LLM Unlearning using Gradient Ratio-Based Influence Estimation and Noise Injection
arxiv_id: '2508.06467'
source_url: https://arxiv.org/abs/2508.06467
tags:
- unlearning
- dataset
- forget
- utility
- retain
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: GRIN introduces a gradient-ratio-based metric to identify influential
  parameters for unlearning in LLMs, combined with selective noise injection to enhance
  forgetting while preserving utility. Experiments on TOFU, WMDP, and SafePKU benchmarks
  show that GRIN achieves stronger forgetting (e.g., lower ROUGE-L Recall and Keyword
  Accuracy on forget sets) and better utility retention compared to baselines, while
  remaining modular and computationally efficient.
---

# LLM Unlearning using Gradient Ratio-Based Influence Estimation and Noise Injection

## Quick Facts
- arXiv ID: 2508.06467
- Source URL: https://arxiv.org/abs/2508.06467
- Reference count: 27
- Key outcome: GRIN achieves stronger forgetting (e.g., lower ROUGE-L Recall and Keyword Accuracy on forget sets) and better utility retention compared to baselines, while remaining modular and computationally efficient.

## Executive Summary
GRIN introduces a gradient-ratio-based metric to identify influential parameters for unlearning in LLMs, combined with selective noise injection to enhance forgetting while preserving utility. Experiments on TOFU, WMDP, and SafePKU benchmarks show that GRIN achieves stronger forgetting (e.g., lower ROUGE-L Recall and Keyword Accuracy on forget sets) and better utility retention compared to baselines, while remaining modular and computationally efficient.

## Method Summary
GRIN computes gradients of forget and retain loss functions to identify parameters most responsible for memorizing forget data. It then selects the top p% of parameters based on a gradient-ratio influence score and injects Gaussian noise into them before fine-tuning. The method is modular and integrates with various unlearning algorithms like PO, NPO, or Grad-Diff, allowing flexible adaptation to different unlearning objectives while maintaining computational efficiency.

## Key Results
- GRIN reduces forget ROUGE-L Recall from 0.084 to 0.072 compared to Full FT without noise
- Optimal weight selection percentage varies (0.2-0.4) across tasks, requiring manual tuning
- Feedforward layers are preferentially selected for factual unlearning, aligning with prior findings about knowledge storage

## Why This Works (Mechanism)

### Mechanism 1
Parameters with high forget-gradient magnitude relative to retain-gradient magnitude are more influential for memorizing the forget data and should be prioritized for modification. The influence score `|G_f[θ_i]| / (|G_r[θ_i]| + ε)` ranks weights by their relative contribution to forget-set loss versus retain-set loss. A high ratio indicates a parameter that strongly encodes forget data while being minimally important for retained knowledge.

### Mechanism 2
Injecting small Gaussian noise into selected parameters before fine-tuning destabilizes memorized patterns and improves forgetting effectiveness. Pre-trained models have already converged on both forget and retain data, yielding small gradients (vanishing gradient problem). Adding noise `θ_i ← θ_i + N(0, σ²)` perturbs the local minimum, increasing gradient magnitudes and enabling more effective updates during subsequent fine-tuning.

### Mechanism 3
Targeting feedforward layers rather than attention layers yields better forgetting because factual memorization is more concentrated in feedforward pathways. The influence score naturally selects more weights from feedforward layers when applied to TOFU unlearning tasks, as these layers encode key-value memories for factual associations.

## Foundational Learning

- Concept: Gradient-based optimization and loss landscapes
  - Why needed here: GRIN relies on interpreting gradient magnitudes as proxies for parameter influence. Understanding how gradients flow and why vanishing gradients occur in converged models is essential for grasping why noise injection helps.
  - Quick check question: Why would a model that has already converged on its training data have small gradients, and how does adding noise change this?

- Concept: Transformer architecture (attention vs. feedforward layers)
  - Why needed here: The paper's analysis shows feedforward layers are preferentially selected for unlearning factual knowledge. Understanding the functional roles of these components helps interpret where knowledge is stored.
  - Quick check question: In a transformer block, what is the role of the feedforward network versus the self-attention mechanism?

- Concept: Preference optimization (PO / NPO / RLHF-style objectives)
  - Why needed here: GRIN is modular and integrates with PO, NPO, or Grad-Diff as the base unlearning algorithm. Understanding how preference modeling frames unlearning (e.g., preferring "I don't know" over leaking sensitive info) is necessary to interpret the results.
  - Quick check question: How does Negative Preference Optimization differ from standard gradient ascent for unlearning?

## Architecture Onboarding

- Component map: Gradient computation -> Influence scoring -> Mask generation -> Noise injection -> Fine-tuning
- Critical path: Prepare forget/retain sets -> Compute gradients -> Generate mask and inject noise -> Fine-tune with masked gradients -> Evaluate on forget/retain metrics
- Design tradeoffs:
  - p% of weights selected: Too low → incomplete forgetting; too high → collateral damage
  - Noise variance σ²: Too high → utility collapse; too low → minimal effect
  - Base unlearning algorithm: NPO is most aggressive (best forgetting, some utility loss); PO is more conservative (better utility, less complete forgetting)
- Failure signatures:
  - Incomplete forgetting: ROUGE-L Recall or Keyword Accuracy on forget set remains high (>0.1)
  - Collateral utility loss: Retain set metrics drop significantly
  - Instability during training: Loss oscillates or diverges
- First 3 experiments:
  1. Ablation on weight selection percentage: Fix σ²=0.001, use PO, and vary p ∈ {0.2, 0.4, 0.6, 0.8} on TOFU 10% forget set
  2. Ablation on noise variance: Fix p=0.4, use PO, and vary σ² ∈ {0, 0.0001, 0.001, 0.01}
  3. Cross-dataset generalization: Apply best hyperparameters from TOFU to WMDP-Cyber using NPO

## Open Questions the Paper Calls Out

- Can the optimal fraction of model parameters selected for fine-tuning be determined automatically rather than via greedy search?
- How can the influence estimation metric be refined to better account for parameter interdependence?
- Does selective noise injection primarily aid unlearning by mitigating vanishing gradients or by destabilizing specific memorized patterns?
- What evaluation metrics can more comprehensively capture unlearning success in open-ended LLM generation?

## Limitations

- Core mechanism assumptions about gradient-ratio metric and noise injection lack rigorous theoretical validation
- Parameter selection claim about feedforward layers may not generalize to non-factual unlearning tasks
- Reproducibility gaps in batch sizes and exact loss function implementations

## Confidence

- High Confidence: Empirical results showing improved forgetting and utility retention
- Medium Confidence: Mechanisms explaining why gradient ratios and noise injection work
- Low Confidence: Assumptions about noise injection selectively disrupting memorized patterns and gradient-ratio metric accuracy

## Next Checks

1. Test GRIN on non-factual unlearning tasks to verify if feedforward layer preference holds
2. Conduct ablation studies to quantify how noise injection specifically disrupts memorized patterns versus generalizable knowledge
3. Compare gradient-ratio metric against alternative influence estimation methods on a validation set