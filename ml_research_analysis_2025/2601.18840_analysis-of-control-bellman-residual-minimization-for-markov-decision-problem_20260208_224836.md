---
ver: rpa2
title: Analysis of Control Bellman Residual Minimization for Markov Decision Problem
arxiv_id: '2601.18840'
source_url: https://arxiv.org/abs/2601.18840
tags:
- lemma
- point
- proof
- bellman
- function
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper establishes foundational results for control Bellman
  residual minimization in Markov decision problems, extending prior work from policy
  evaluation to policy optimization. It analyzes both standard (CBR) and soft (SCBR)
  Bellman residual objectives under linear function approximation, showing that while
  CBR is non-convex and non-differentiable, it admits piecewise quadratic structure
  and a Clarke subdifferential enabling generalized gradient methods.
---

# Analysis of Control Bellman Residual Minimization for Markov Decision Problem

## Quick Facts
- arXiv ID: 2601.18840
- Source URL: https://arxiv.org/abs/2601.18840
- Reference count: 40
- Primary result: Establishes theoretical foundations for minimizing Bellman residuals to optimize policies in MDPs, extending prior work from policy evaluation to policy optimization

## Executive Summary
This paper establishes foundational results for control Bellman residual minimization in Markov decision problems, extending prior work from policy evaluation to policy optimization. It analyzes both standard (CBR) and soft (SCBR) Bellman residual objectives under linear function approximation, showing that while CBR is non-convex and non-differentiable, it admits piecewise quadratic structure and a Clarke subdifferential enabling generalized gradient methods. SCBR is differentiable, allowing standard gradient descent. The authors prove convergence to stationary points satisfying oblique projected Bellman equations, and derive error bounds relating solutions to optimal Q-functions. In the tabular case, CBR stationary points are unique and coincide with optimal Q*, while SCBR exhibits linear convergence. Experiments demonstrate SCBR's advantages over projected value iteration, particularly in avoiding divergence, though a deep RL variant generally underperforms DQN.

## Method Summary
The paper proposes minimizing the squared Bellman residual as an alternative to standard value iteration for policy optimization. For CBR, it uses generalized gradient descent with a Clarke subdifferential to handle the non-smooth max operator. For SCBR, it replaces the hard max with a log-sum-exp (softmax) operator, making the objective differentiable and enabling standard gradient descent. Both methods find stationary points of oblique projected Bellman equations. In the tabular case, CBR converges to the optimal Q*, while SCBR converges to a "soft" optimal Q*. Under linear function approximation, both find stationary points that approximate the optimal Q-function with bounded error.

## Key Results
- CBR has piecewise quadratic structure with polyhedral partition of parameter space, enabling generalized gradient descent
- SCBR is infinitely differentiable, allowing standard gradient descent with proven linear convergence in tabular case
- Stationary points of both objectives satisfy oblique projected Bellman equations, explaining their stability compared to standard value iteration
- SCBR converges to a unique stationary point in the tabular case; LFA case has error bounds but non-uniqueness
- Deep RL implementation of SCBR underperforms DQN due to double sampling requirements

## Why This Works (Mechanism)

### Mechanism 1: Piecewise Quadratic Geometry & Subdifferential Descent
- The parameter space is divided into polyhedral cones where a specific greedy policy is optimal. Inside each cone, the objective reduces to a strongly convex quadratic function. The Clarke subdifferential provides valid descent directions even at boundaries between cones.
- **Core assumption:** Feature matrix Φ is full column rank; problem is locally Lipschitz.
- **Evidence anchors:** Proposition 1 states f is piecewise quadratic; Theorem 1 derives explicit subdifferential; neighbor work suggests renewed interest in BRM.
- **Break condition:** Step-size not satisfying Armijo rule may fail to find stationary point.

### Mechanism 2: Smoothing via Softmax (SCBR)
- Replacing hard max with log-sum-exp renders objective infinitely differentiable, enabling standard gradient descent. The gradient depends on Boltzmann policy, allowing efficient first-order optimizers.
- **Core assumption:** Temperature λ > 0; gradient is locally Lipschitz.
- **Evidence anchors:** Theorem 6 derives exact gradient; Theorem 10 proves linear convergence in tabular case.
- **Break condition:** Deep RL requires double sampling; failing to decorrelate samples introduces bias.

### Mechanism 3: Oblique Projection Stability
- Stationary points satisfy Oblique Projected Control Bellman Equation (OP-CBE), guaranteeing solution existence even where standard DP diverges. Standard P-VI projects orthogonally and may diverge if composite operator isn't a contraction.
- **Core assumption:** Stationary point exists; feature matrix Φ is full rank.
- **Evidence anchors:** Eq (5) defines OP-CBE; Figure 2 shows P-VI diverging while SCBR converges.
- **Break condition:** Solution exists but isn't unique in LFA case; different stationary points may yield different policy qualities.

## Foundational Learning

- **Concept: Clarke Subdifferential**
  - **Why needed here:** To optimize CBR, one must understand how to take "gradients" on non-smooth functions like max. The subdifferential provides set of all valid subgradients at a "kink."
  - **Quick check question:** If f(x) = max(x, 0), what is the subdifferential at x=0? (Answer: interval [0, 1]).

- **Concept: Bellman Residual Minimization vs. Fixed-Point Iteration**
  - **Why needed here:** Paper contrasts minimizing squared residual ||TQ - Q||² with standard approach of iterating Q_{k+1} = TQ_k. Understanding this distinction explains why residual methods are more stable but computationally distinct.
  - **Quick check question:** Does minimizing Bellman residual always yield optimal Q* in tabular case? (Answer: Yes, per Theorem 4).

- **Concept: Linear Function Approximation (LFA) Geometry**
  - **Why needed here:** Paper relies heavily on projections onto subspace spanned by features Φ. Visualizing "oblique projection" vs "orthogonal projection" is key to understanding error bounds.
  - **Quick check question:** Why does paper claim P-VI might diverge with LFA? (Answer: Composite operator ΓΦ|Φ∘T may lose contraction property).

## Architecture Onboarding

- **Component map:**
  - States/Rewards/Transitions -> Q-function approximator -> Bellman target calculator -> Loss computation -> Optimizer (GD or subgradient) -> Policy extraction

- **Critical path:**
  1. Compute Q_θ(s,a) via linear approximator Φθ or neural net
  2. Calculate target: y = R + γ max/softmax Q(s')
  3. Compute Loss: 0.5 ||y - Q_θ||²
  4. Backprop: Crucial step—ensure target gradient is handled (standard in SCBR, complex in Deep CBR)

- **Design tradeoffs:**
  - **CBR (Hard):** Theoretical guarantees in tabular settings (converges to Q*); piecewise quadratic structure. **Con:** Non-smooth optimization is harder; generalized gradient descent is slow.
  - **SCBR (Soft):** Smooth, differentiable, standard GD works; faster convergence rate (linear). **Con:** Converges to "soft" optimal Q*_λ, not hard Q*; bias issues in Deep RL without double sampling.
  - **Deep RL:** Deep RL variant underperforms DQN. **Assumption:** Likely due to difficulty of unbiased gradient estimation without double sampling.

- **Failure signatures:**
  - Divergence in P-VI: Oscillating or exploding Q-values (observed in Figure 2)
  - Deep SCBR Stagnation: Learning curves flatten or degrade compared to DQN (observed in Figure 14) due to gradient bias
  - Non-uniqueness: In LFA, algorithm converges to a stationary point, but not necessarily global minimizer of value error

- **First 3 experiments:**
  1. Implement SCBR gradient descent on small grid (e.g., FrozenLake) to verify convergence to Q*_λ and compare speed against Value Iteration
  2. Reproduce Figure 2 by comparing SCBR against Projected Value Iteration (P-VI) with random features to confirm SCBR prevents divergence
  3. Implement Deep RL SCBR with and without "double sampling" trick (Eq. in Appendix A.7) to measure impact of gradient bias on simple control tasks (e.g., CartPole)

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can theoretical stability advantages of SCBR be translated into superior performance over DQN in deep RL settings?
- **Basis in paper:** Section A.7 notes deep RL variant "generally underperforms DQN" despite theoretical convergence guarantees, stating "Further investigation and experiments remain important directions for future work."
- **Why unresolved:** Analysis relies on LFA; non-convexity in deep nonlinear approximation creates optimization landscapes where DQN's semi-gradient approach may empirically outperform full residual gradient.
- **What evidence would resolve it:** Empirical benchmarks showing SCBR matching or exceeding DQN sample efficiency and final returns in standard deep RL suites (e.g., Atari), potentially requiring architectural modifications to handle double-sampling requirement effectively.

### Open Question 2
- **Question:** Can generalized gradient descent method for non-smooth CBR objective be accelerated to achieve convergence rates comparable to dynamic programming approaches like value iteration?
- **Basis in paper:** Section 4.3 states proposed algorithm is "slower than value iteration, which... enjoys exponential (geometric) convergence," concluding "this approach remains appealing and worthy of further investigation."
- **Why unresolved:** CBR objective is non-convex and non-smooth, limiting applicability of standard acceleration techniques (like Nesterov momentum) which rely on smoothness or convexity assumptions.
- **What evidence would resolve it:** Modified optimization algorithm for CBR with proven convergence rate rivaling exponential rate of value iteration, or empirical evidence showing significantly faster wall-clock convergence than standard subgradient method.

## Limitations

- Theoretical guarantees for CBR are primarily established in tabular setting; extension to deep RL shows inferior performance to DQN
- Analysis relies on full column rank assumption for feature matrix Φ, which may not hold in practice with deep neural networks
- Convergence analysis doesn't provide explicit rates for non-smooth CBR case
- Stationary points obtained may not be globally optimal in function approximation setting
- Deep RL implementation requires double sampling, making online implementation difficult

## Confidence

- **High confidence:** Piecewise quadratic structure of CBR and existence of Clarke subdifferential (Mechanism 1) are mathematically rigorous and well-supported by proofs
- **Medium confidence:** Convergence guarantees for SCBR in linear function approximation case are strong, but extension to deep RL shows performance degradation suggesting implementation or theoretical gaps
- **Low confidence:** Uniqueness of stationary points in linear approximation case is claimed only for tabular setting; behavior in high-dimensional LFA remains open question

## Next Checks

1. **Non-full rank features:** Systematically test CBR/SCBR convergence when Φ has rank deficiency (e.g., duplicate features or m > |S×A|) to validate full-rank assumption's necessity

2. **Tabular vs. LFA comparison:** Quantify suboptimality gap ||Q* - Q̂||∞ between true optimal Q* and LFA solution Q̂ = Φθ̂ across varying feature dimensions to measure approximation error

3. **Deep RL double sampling ablation:** Implement SCBR with and without double sampling on stochastic control task (e.g., CartPole with stochastic transitions) to measure empirical impact of gradient bias on learning stability and final performance