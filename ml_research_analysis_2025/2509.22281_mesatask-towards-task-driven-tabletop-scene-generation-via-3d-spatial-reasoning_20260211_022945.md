---
ver: rpa2
title: 'MesaTask: Towards Task-Driven Tabletop Scene Generation via 3D Spatial Reasoning'
arxiv_id: '2509.22281'
source_url: https://arxiv.org/abs/2509.22281
tags:
- scene
- object
- task
- objects
- tabletop
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MesaTask, a novel framework for generating
  task-oriented 3D tabletop scenes directly from high-level human instructions. To
  support this task, the authors created MesaTask-10K, a large-scale dataset of 10,700
  synthetic scenes with manually crafted layouts and over 12,000 3D assets.
---

# MesaTask: Towards Task-Driven Tabletop Scene Generation via 3D Spatial Reasoning

## Quick Facts
- arXiv ID: 2509.22281
- Source URL: https://arxiv.org/abs/2509.22281
- Authors: Jinkun Hao; Naifu Liang; Zhen Luo; Xudong Xu; Weipeng Zhong; Ran Yi; Yichen Jin; Zhaoyang Lyu; Feng Zheng; Lizhuang Ma; Jiangmiao Pang
- Reference count: 40
- Key outcome: MesaTask generates task-oriented 3D tabletop scenes from high-level instructions, outperforming baselines in task alignment, realism, and physical plausibility with higher GPT scores and user study ratings.

## Executive Summary
MesaTask introduces a novel framework for generating task-oriented 3D tabletop scenes directly from high-level human instructions. The system employs a Spatial Reasoning Chain that decomposes generation into object inference, spatial interrelation reasoning, and scene graph construction. Trained on MesaTask-10K, a large-scale dataset of 10,700 synthetic scenes with manually crafted layouts and over 12,000 3D assets, MesaTask uses supervised fine-tuning followed by Direct Preference Optimization to ensure physically plausible layouts aligned with task descriptions. Evaluations demonstrate superior performance across diverse task types and complexity levels.

## Method Summary
MesaTask uses a Qwen3-8B base LLM fine-tuned with supervised learning (SFT) on 50,000 task-scene pairs, followed by Direct Preference Optimization (DPO) on 10,000 preference pairs. The Spatial Reasoning Chain processes task instructions through four stages: object list completion, inter-object spatial relation inference, scene graph construction with coarse positions/orientations, and final 3D layout generation. Asset retrieval uses text similarity (SBERT all-mpnet-base-v2) and size similarity (cosine), weighted at α=0.9 and β=0.1. Physics validation employs Drake geometry engine to ensure collision-free placement.

## Key Results
- MesaTask achieves higher GPT-Score averages (8.25 vs. 8.23) and user study ratings compared to baselines across CwT, OSR, PPI, LCR, OV dimensions
- The framework shows strong performance across all four task difficulty levels with varying FID scores (59.3 for Level 1 to 43.9 for Level 4)
- Physics validation confirms 0% collision rates in generated scenes after DPO training

## Why This Works (Mechanism)

### Mechanism 1: Structured Spatial Reasoning Chain
Decomposing task-to-scene generation into a structured reasoning chain improves layout quality and reduces the instruction-to-3D gap. The Spatial Reasoning Chain sequentially performs object list completion, inter-object spatial relation inference, scene graph construction, and final 3D layout generation. This decomposition helps LLMs handle complex spatial reasoning more effectively than end-to-end generation.

### Mechanism 2: High-Quality Synthetic Training Data
Training on large-scale, manually refined 3D scene layouts with complex inter-object relations enables physically plausible generation. MesaTask-10K provides 10,700 scenes with human-crafted layouts, depth estimation, coarse layout generation, manual refinement, and physics simulation validation. This captures stacking, containment, and realistic object co-occurrence patterns.

### Mechanism 3: DPO for Physical Plausibility
DPO on paired positive/negative layouts reduces collisions and improves task alignment beyond SFT alone. Negative samples are created through position perturbation for collisions, scene graph corruption for relation errors, and object removal for missing task-relevant items. The model learns preference for collision-free, task-aligned layouts.

## Foundational Learning

- **Scene Graph Representation**: Intermediate representation between task instructions and 3D layouts; encodes spatial relations (left/right/above/in), coarse 9-grid positions, and 8-direction orientations. Quick check: How is the "In" (containment) relationship defined in the scene graph extraction rules?

- **Chain-of-Thought Reasoning in LLMs**: Spatial Reasoning Chain is a structured CoT approach; understanding CoT helps explain decomposition effectiveness. Quick check: What are the three sequential stages of the Spatial Reasoning Chain and their outputs?

- **Direct Preference Optimization (DPO)**: Second training phase aligns outputs with physical plausibility; critical for debugging alignment issues. Quick check: How are the three negative sample types constructed for DPO training?

## Architecture Onboarding

- **Component map**: Task instruction → GPT-4o expansion → MesaTask inference → Reasoning chain + Layout → Asset retrieval → Scene placement → Physics check
- **Critical path**: Task instruction → GPT-4o expansion → MesaTask inference → Reasoning chain + Layout → Asset retrieval → Scene placement → Physics check
- **Design tradeoffs**: 8B model for efficiency vs larger models for reasoning capacity; retrieval from 12K asset library vs generative 3D (limits diversity, ensures quality); synthetic training data vs real images (potential domain gap); DPO corruption strategy may not cover all edge-case failures
- **Failure signatures**: Object collisions → DPO insufficient or negative samples don't cover case; missing task objects → Reasoning chain object completion failing or task expansion incomplete; floating/implausible relations → Scene graph construction or asset size mismatch; low text-to-asset matching → Retrieval weight adjustment needed; failure on unseen table types → Objects/relations outside training distribution
- **First 3 experiments**: Ablation on reasoning chain stages: Measure each stage's contribution by running partial chains; DPO negative sampling analysis: Vary corruption strategies (collision-only vs all three) and measure collision rate/task alignment impact; Asset retrieval sensitivity: Test different α/β weights for text vs size similarity on visual coherence

## Open Questions the Paper Calls Out

### Open Question 1: Sim-to-Real Transfer
Can MesaTask's task-driven scene generation transfer effectively to real-world robotic manipulation pipelines, or does the synthetic data pipeline create a sim-to-real gap? The dataset is entirely synthetic with no real-world validation discussed.

### Open Question 2: Task Difficulty Imbalance
How does the severe task difficulty imbalance in MesaTask-10K (83.8% Level 4 tasks) affect model performance on simpler, lower-level tasks? The training distribution heavily favors abstract outcome-based tasks.

### Open Question 3: Object Diversity Limitations
To what extent does MesaTask's object diversity limitation—constrained to the 12,000 assets in the retrieval database—impact generalization to novel or user-specified objects? The framework cannot create objects outside the database.

### Open Question 4: DPO Coverage Completeness
Does the current DPO negative sampling strategy adequately capture all failure modes, or are there systematic errors that remain unaddressed? The three corruption types may not cover all real model failure patterns.

## Limitations
- Synthetic data pipeline may create sim-to-real gap for robotic applications
- Severe task difficulty imbalance (83.8% Level 4 tasks) may underrepresent simpler scenarios
- Object diversity limited to 12,000 assets in retrieval database, constraining generalization to novel objects

## Confidence

- **Spatial Reasoning Chain Effectiveness**: High confidence, supported by ablation studies and qualitative examples
- **DPO Impact on Physical Plausibility**: Medium confidence, as negative sample strategies may not cover all failure modes
- **Generalization to Novel Tasks**: Medium confidence, with partial but not complete coverage of unseen table types

## Next Checks

1. **Ablation on Reasoning Chain Stages**: Measure each stage's contribution by running partial chains and analyzing performance drops in object presence, relation accuracy, and layout plausibility.

2. **DPO Negative Sampling Robustness**: Vary corruption strategies (collision-only vs all three) and measure collision rate/task alignment impact to ensure DPO addresses target problems.

3. **Asset Retrieval Sensitivity**: Test different α/β weights for text vs size similarity on visual coherence and task alignment in generated scenes.