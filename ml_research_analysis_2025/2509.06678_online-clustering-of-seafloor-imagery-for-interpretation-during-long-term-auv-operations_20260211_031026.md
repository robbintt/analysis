---
ver: rpa2
title: Online Clustering of Seafloor Imagery for Interpretation during Long-Term AUV
  Operations
arxiv_id: '2509.06678'
source_url: https://arxiv.org/abs/2509.06678
tags:
- data
- clustering
- cluster
- clusters
- online
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces an Online Clustering Framework (OCF) for
  real-time interpretation of seafloor imagery during long-term AUV operations. The
  key innovation is a method that maintains a bounded, representative subset of images
  to enable continuous clustering without reprocessing the full growing dataset.
---

# Online Clustering of Seafloor Imagery for Interpretation during Long-Term AUV Operations

## Quick Facts
- arXiv ID: 2509.06678
- Source URL: https://arxiv.org/abs/2509.06678
- Reference count: 40
- Primary result: Online Clustering Framework achieves average F1 score of 0.68 across three diverse seafloor datasets while maintaining bounded computational time.

## Executive Summary
This paper introduces an Online Clustering Framework (OCF) for real-time interpretation of seafloor imagery during long-term AUV operations. The key innovation is a method that maintains a bounded, representative subset of images to enable continuous clustering without reprocessing the full growing dataset. The framework employs dynamic cluster merging and splitting based on statistical criteria, using a density-based sampling strategy to select representatives that capture the evolving feature distribution. This approach enables the system to adapt to new patterns while preserving computational efficiency.

## Method Summary
The framework processes seafloor imagery in batches of 1,000 images, using a pretrained frozen encoder to extract 16-dimensional latent vectors. It maintains a global cluster set S with bounded memory through a density-stratified representative subset (4,000 images). New batches are initially clustered via K-means, then merged with existing clusters using Mahalanobis distance and covariance volume criteria. Recursive cluster splitting occurs when 2-component GMMs on representatives yield better AIC/BIC scores than single-component models. The backbone uses Dirichlet Process GMM for robustness to noise and class imbalance. The system updates continuously, adapting to new seafloor patterns while preserving computational efficiency.

## Key Results
- Achieves average F1 score of 0.68 across three diverse seafloor datasets
- Maintains consistently lower and bounded computational time as data volume increases
- Outperforms other online clustering approaches in clustering capability and robustness to trajectory variation
- Demonstrates superior performance for generating survey data summaries and supporting informative path planning

## Why This Works (Mechanism)

### Mechanism 1: Bounded Historical Representation
The framework maintains a fixed-size, density-stratified representative subset that approximates the full historical data distribution for cluster splitting without unbounded memory growth. Local density is calculated using Manhattan distance to nearest neighbors, and representatives are selected by stratified sampling across density rankings. This subset (fixed at 4,000) substitutes the full history during recursive cluster splitting operations. The statistical distribution of the representative subset correlates sufficiently with the full dataset to define valid cluster boundaries for Gaussian Mixture Models.

### Mechanism 2: Dynamic Cluster Topology (Merge-Split Reservoir)
The framework combines conservative merging with information-criterion-driven splitting to stabilize cluster count and adapt to new data modes. Clusters merge if Mahalanobis distance is below threshold AND the merged covariance volume is approximately the sum of individual volumes. Clusters split if fitting a 2-component GMM on representatives yields lower AIC/BIC than a 1-component model. Latent features of seafloor imagery adhere sufficiently to Gaussian distributions for these criteria to be valid proxies for semantic coherence.

### Mechanism 3: Bayesian Nonparametric Backbone (DPGMM)
Using Dirichlet Process GMM as the inference backbone provides robustness to noise and class imbalance compared to standard GMMs. Unlike standard GMMs using Maximum Likelihood Estimation, DPGMM employs a Dirichlet Process prior that regularizes component usage and penalizes unlikely assignments. This yields more conservative posteriors in sparse/ambiguous regions. The "rich-get-richer" property of the Dirichlet Process aligns with the expected distribution of seafloor features.

## Foundational Learning

- **Mahalanobis Distance & Covariance Ellipsoids**: Required to implement merge criteria. You cannot determine if two clusters overlap in multi-dimensional latent space without understanding covariance-aware distance. Quick check: If two clusters have the same center but one is a tall ellipse and the other a flat disk, should they merge? (Answer depends on volume criteria, not just center distance).

- **Information Criteria (AIC vs. BIC)**: Required to implement split criteria. The system decides if one cluster is "actually two" by checking if the complexity penalty of adding a second cluster is outweighed by the likelihood gain. Quick check: As dataset size N grows, which criterion becomes more conservative (harder to split): AIC or BIC? (BIC, due to the k · log(N) term).

- **Data Distillation / Coresets**: This is the "Bounded Representative Subset". Understanding how a small subset can statistically stand in for a large dataset is crucial for the "constant time" claim. Quick check: Why does random sampling fail here? (It misses low-density regions/minority classes which are critical for defining cluster boundaries).

## Architecture Onboarding

- **Component map**: Encoder (Frozen) -> Buffer -> Batch Clusterer -> Merger -> Representative Manager -> Splitter -> Backbone (DPGMM)
- **Critical path**: The Representative Manager -> Splitter loop. If the representatives fail to capture the history, the splitter makes decisions on garbage data, degrading the Global Set S.
- **Design tradeoffs**: Representative Count (4,000 chosen for CPU limits - lowering speeds up splitting but risks missing minority classes). Merge Thresholds (conservative - looser values reduce cluster count but increase semantic mixing). Backbone (DPGMM chosen over GMM for robustness +3.7% F1 but adds complexity).
- **Failure signatures**: Cluster Explosion (merge criteria too strict or split criteria too loose), Stagnation (merge criteria too loose - distinct habitats fused), Drift (representative set not updated correctly - splitter relies on stale history).
- **First 3 experiments**: 1) Ablation on Representatives (Random vs Density-based Sampling on imbalanced dataset). 2) Merge/Split Isolation (Only Merging vs Only Splitting vs Full OCF on revisiting trajectory). 3) Latency Scaling (Plot inference time vs Total Image Count to verify "constant time" claim).

## Open Questions the Paper Calls Out

- How can the Online Clustering Framework's output be formally integrated into an informative path planning (IPP) algorithm to dynamically adjust AUV trajectories in real-time? The abstract states the framework supports IPP, but the paper only validates the perception module without demonstrating downstream adaptive decision-making.

- To what extent are classification errors in habitat transition zones caused by the dimensionality of the feature encoder versus the limitations of the online clustering split/merge criteria? The paper attributes errors to "inter-class similarity" but doesn't ablate whether higher-dimensional feature vectors would improve separability before clustering.

- How sensitive are the merging thresholds (ε_D, ε_V) to extreme domain shifts, and can they be adaptively tuned without manual calibration? The method works on specific benthic datasets but it's unclear if static statistical criteria hold for environments with drastically different noise profiles without re-calibration.

## Limitations

- Encoder specifications remain unspecified, requiring assumption of a suitable 16-dimensional embedding extractor
- Hyperparameter details for DPGMM (concentration, base distribution) are not fully disclosed
- Dataset provenance relies on external references for exact image-class mappings
- Computational environment (hardware, libraries) is not documented

## Confidence

- **High Confidence**: The bounded-memory design via density-based representatives; the merge-split cycle based on Mahalanobis and GMM criteria
- **Medium Confidence**: The robustness gains from DPGMM over standard GMM (supported by ablation but not extensively tested across all datasets)
- **Low Confidence**: The exact quantitative thresholds for merge/split decisions in all edge cases; the impact of trajectory variation on long-term stability

## Next Checks

1. **Representative Coverage Test**: Run the framework with both random and density-based sampling on an imbalanced dataset (e.g., SH) and verify whether minority class F1 scores improve under density selection.

2. **Merge-Split Ablation**: Isolate the effects of merging and splitting by running "only merging" and "only splitting" variants on a revisiting trajectory to assess their individual contributions to cluster stability and consistency.

3. **Time Complexity Verification**: Measure and plot per-update time versus total images processed to confirm the claimed constant-time behavior after the initial buffer fill.