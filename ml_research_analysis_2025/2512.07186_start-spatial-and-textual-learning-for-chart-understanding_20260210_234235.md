---
ver: rpa2
title: 'START: Spatial and Textual Learning for Chart Understanding'
arxiv_id: '2512.07186'
source_url: https://arxiv.org/abs/2512.07186
tags:
- chart
- question
- code
- answer
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of chart understanding for multimodal
  large language models (MLLMs) by introducing spatial and textual learning for chart
  understanding (START). The method combines chart-element grounding to improve spatial
  understanding with chart-to-code generation to enhance textual understanding.
---

# START: Spatial and Textual Learning for Chart Understanding

## Quick Facts
- **arXiv ID:** 2512.07186
- **Source URL:** https://arxiv.org/abs/2512.07186
- **Reference count:** 40
- **Primary result:** START-RL-7B outperforms previous state-of-the-art by clear margins on CharXiv, ChartQAPro, ChartMimic, and CS-Bench

## Executive Summary
This paper introduces START (Spatial and Textual Learning for Chart Understanding), a method that enhances MLLM chart understanding through joint training on chart-element grounding and chart-to-code generation. The approach uses a novel dataset construction pipeline that converts real chart images to Python code and evolves it to extract precise element locations. START achieves substantial improvements over base models across multiple chart understanding benchmarks, with START-RL-7B outperforming previous state-of-the-art models by clear margins.

## Method Summary
START trains MLLMs on three tasks: chart question answering, chart element grounding (predicting bounding boxes), and chart-to-code generation. The dataset construction pipeline converts real charts to Python code using a proprietary model, then evolves this code with an LLM to extract precise element locations. Training proceeds in two phases: supervised fine-tuning (SFT) followed by reinforcement learning using GRPO with task-specific rewards (accuracy for QA/code, IoU for grounding). A new benchmark, CS-Bench, evaluates spatial reasoning capabilities.

## Key Results
- START-RL-7B achieves 84.45% accuracy on CharXiv reasoning, 86.35% on ChartQAPro, and 76.17 on ChartMimic
- Significant improvement on spatial reasoning benchmark CS-Bench (55.62% recall@0.3 IoU vs 36.42% baseline)
- The "think-before-answer" reinforcement learning approach improves performance even on low-level spatial tasks
- START consistently outperforms previous state-of-the-art models across all major chart understanding benchmarks

## Why This Works (Mechanism)

### Mechanism 1: Spatial-Textual Duality
Jointly training on chart-element grounding and chart-to-code generation improves understanding more than QA alone. Grounding forces explicit learning of chart layout while code generation forces recovery of underlying data and textual details, providing a more robust representation of the chart's fundamental properties.

### Mechanism 2: Code-Evolution for Precise Grounding
A dataset pipeline converts real charts to executable Python code, then uses an LLM to evolve the code to extract element locations. This creates higher-quality spatial grounding data than off-the-shelf models because locations are programmatically generated from rendering code, ensuring precision and consistency.

### Mechanism 3: Think-Before-Answer Reinforcement
Applying a "think-before-answer" format during reinforcement learning improves performance even on low-level visual tasks like grounding. The thinking block provides richer context and intermediate steps, helping the MLLM deconstruct complex charts before committing to final answers.

## Foundational Learning

**Multimodal Large Language Models (MLLMs)**
- Why needed: START builds upon MLLMs (e.g., Qwen2.5-VL) - understanding visual encoder to LLM token alignment is essential for incorporating additional tasks
- Quick check: Can you explain how a visual encoder processes an image into tokens that an LLM can understand?

**Reinforcement Learning from Verifiable Rewards (GRPO)**
- Why needed: Core contribution uses GRPO for training - reward design (accuracy for code/QA, IoU for grounding) is central to learning spatial and textual tasks
- Quick check: How does GRPO differ from standard RL algorithms like PPO in how it calculates advantages?

**Bounding Box Regression**
- Why needed: Chart-element grounding is framed as predicting bounding boxes in JSON format with IoU-based rewards
- Quick check: If a predicted bounding box perfectly contains a ground-truth box but is twice as large, will the IoU be high or low?

## Architecture Onboarding

**Component map:** Dataset Construction Pipeline -> SFT Training -> RL Training -> Evaluation on Benchmarks

**Critical path:** The dataset construction pipeline is most critical - if chart-to-code conversion or code evolution is flawed, training data is compromised

**Design tradeoffs:** Proprietary model chosen for chart-to-code conversion (less reproducible but likely more capable); real-world charts converted to code preserve visual complexity but may introduce reconstruction errors

**Failure signatures:** Code evolution failure manifests as completely off bounding boxes; SFT failure looks like syntactically correct JSON with values outside image coordinates

**First 3 experiments:**
1. Validate the Data Pipeline: Run on small diverse charts, manually inspect (a) fidelity of reproduced chart vs original and (b) accuracy of location.json by visualizing bounding boxes
2. SFT Ablation on Small Scale: Train 3B model on three subsets: (1) QA only, (2) QA + Code, (3) QA + Code + Grounding; compare on CS-Bench and ChartQA
3. Full Training Run & RL Analysis: Execute full SFT -> RL pipeline; monitor learning curves for iou reward for grounding and code accuracy score; stagnating iou indicates spatial learning problems

## Open Questions the Paper Calls Out

**Open Question 1:** Does the "think-before-answer" mechanism provide measurable benefits for low-level spatial grounding tasks compared to direct coordinate prediction? The paper demonstrates improvement but doesn't isolate the specific cognitive mechanism.

**Open Question 2:** How does the framework perform on real-world charts that cannot be faithfully reverse-engineered into executable Python code? The method assumes visual layout can be captured by code evolution, but charts with non-standard elements may break the pipeline.

**Open Question 3:** Is the reinforcement learning reward (IoU) sufficient to correct "hallucinated" spatial locations derived from imperfect code evolution? While RL improves performance, it's unclear if the model learns to ground based on visual features or simply mimics potentially noisy code-generated labels.

## Limitations
- Proprietary model used for chart-to-code conversion limits reproducibility of dataset construction
- Limited ablation studies on task importance (only three models compared) reduce confidence in relative contributions
- No analysis of performance degradation on charts with unusual layouts or non-standard elements

## Confidence
- **High Confidence:** START-RL-7B outperforms previous SOTA on multiple benchmarks by clear margins
- **Medium Confidence:** Combining grounding and code generation tasks is more effective than either alone (based on limited ablation evidence)
- **Medium Confidence:** Dataset construction pipeline generates high-quality spatial grounding data (though not independently verified)

## Next Checks
1. **Independent Dataset Validation:** Reconstruct a small subset of START dataset using open-source models to verify quality of generated code and grounding labels
2. **Robustness Testing:** Evaluate START on charts with unconventional layouts, mixed chart types, or corrupted visual elements to assess generalization limits
3. **Task Ablation Study:** Train and evaluate models with each individual task (QA only, grounding only, code only) to quantify relative contributions beyond current three-model comparison