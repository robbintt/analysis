---
ver: rpa2
title: Pretraining Strategies using Monolingual and Parallel Data for Low-Resource
  Machine Translation
arxiv_id: '2510.25116'
source_url: https://arxiv.org/abs/2510.25116
tags:
- languages
- data
- pretraining
- translation
- monolingual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates pretraining strategies for machine translation
  models tailored to low-resource languages, focusing on Lingala, an under-resourced
  African language. Building on Reid and Artetxe (2021), the research explores different
  pretraining methodologies, including the integration of multiple languages and the
  use of both monolingual and parallel data during pretraining.
---

# Pretraining Strategies using Monolingual and Parallel Data for Low-Resource Machine Translation

## Quick Facts
- arXiv ID: 2510.25116
- Source URL: https://arxiv.org/abs/2510.25116
- Reference count: 3
- This study investigates pretraining strategies for machine translation models tailored to low-resource languages, focusing on Lingala, an under-resourced African language.

## Executive Summary
This study investigates pretraining strategies for machine translation models tailored to low-resource languages, focusing on Lingala, an under-resourced African language. Building on Reid and Artetxe (2021), the research explores different pretraining methodologies, including the integration of multiple languages and the use of both monolingual and parallel data during pretraining. Experiments show that pretraining on multiple languages and leveraging both monolingual and parallel data significantly enhance translation quality. Specifically, the study finds that pretraining on multiple African languages, including the target low-resource language, improves performance compared to pretraining on the source and target languages alone. BLEU scores improved from 21.02 to 27.38, and chrF scores improved from 48.92 to 53.24, when incorporating multiple languages. The findings contribute to developing more inclusive and accurate NLP models for marginalized communities and underrepresented populations.

## Method Summary
The research employs a sequence-to-sequence Transformer architecture (6 encoder and 6 decoder layers, hidden dimension 768, feed-forward dimension 3072, ~162M parameters) following the mBART framework. The approach combines denoising autoencoder pretraining on monolingual data with translation objectives on parallel data. Training uses a shared SentencePiece vocabulary (80K subwords) across all languages. The pretraining phase involves 100K iterations with a batch size of 1024 and sequence length of 1024, followed by fine-tuning on the specific low-resource language pair. The study compares multiple pretraining configurations: monolingual-only (Lingala), bilingual (English-Lingala), and multilingual (including related African languages: Afrikaans, Swahili, Zulu) with varying amounts of English monolingual data.

## Key Results
- BLEU scores improved from 21.02 to 27.38 when incorporating multiple African languages in pretraining
- chrF scores improved from 48.92 to 53.24 with multilingual pretraining strategy
- Increasing English monolingual data from 10MB to 112MB yielded a 3.38 BLEU point improvement
- Combined denoising and translation objectives during pretraining outperformed monolingual-only pretraining

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Pretraining on multiple related African languages improves low-resource MT more than training on source-target pair alone.
- Mechanism: Multilingual pretraining creates shared cross-lingual representations; related languages (Afrikaans, Swahili, Zulu) provide morphological and syntactic signals that transfer to Lingala through common encoder-decoder weights.
- Core assumption: Related languages share learnable structural patterns that benefit a shared model.
- Evidence anchors:
  - [abstract] "pretraining on multiple African languages, including the target low-resource language, improves performance compared to pretraining on the source and target languages alone"
  - [section 7] Experiment 4 (English-Lingala only) achieved BLEU 21.02/chrF 48.92, while Experiment 2 (multiple African languages) achieved BLEU 27.38/chrF 53.24
  - [corpus] Related work (Nguyen et al., "An Efficient Approach for Machine Translation on Low-resource Languages") similarly leverages multilingual pretraining for low-resource pairs, suggesting cross-lingual transfer is a consistent signal.
- Break condition: If target language is linguistically isolated (no related languages in pretraining data), multilingual benefits likely diminish.

### Mechanism 2
- Claim: Combining monolingual denoising and parallel translation objectives during pretraining outperforms monolingual-only pretraining.
- Mechanism: Denoising on monolingual data learns general representations; parallel data with translation objective explicitly teaches cross-lingual alignment. Joint training yields complementary signal types.
- Core assumption: The model can jointly optimize both objectives without interference; parallel data quality is sufficient.
- Evidence anchors:
  - [abstract] "leveraging both monolingual and parallel data significantly enhance translation quality"
  - [section 6.1.2] Experiment 2 combined denoising (monolingual) and translation (parallel) tasks, yielding best results (BLEU 27.38)
  - [corpus] Weak direct corpus evidence on joint objective effectiveness for low-resource; most neighbors focus on backtranslation rather than joint pretraining objectives.
- Break condition: If parallel data is noisy or misaligned, translation objective may introduce harmful signal.

### Mechanism 3
- Claim: Source language (English) monolingual data scale critically affects pretraining quality for the translation pair.
- Mechanism: Small English samples (10MB) create representation imbalance or insufficient source-language coverage; larger samples (112MB) restore balance and improve encoder quality for source-side understanding.
- Core assumption: Encoder needs adequate source-language exposure; the problem is under-representation, not interference.
- Evidence anchors:
  - [section 6.1.3] 10MB English sample yielded BLEU 21.8; increasing to 112MB improved to BLEU 25.18 (+3.38 points)
  - [section 7] "when we increased the size of the English monolingual data, we observed a notable improvement"
  - [corpus] No direct corpus confirmation of this scale threshold effect; remains paper-specific finding.
- Break condition: Excessive source-language data could dominate multilingual representation space, potentially harming low-resource target performance.

## Foundational Learning

- Concept: Sequence-to-sequence Transformer architecture (encoder-decoder attention)
  - Why needed here: The entire pretraining approach builds on mBART-style seq2seq; understanding cross-attention between encoder and decoder is essential for debugging alignment issues.
  - Quick check question: Can you explain why encoder-decoder architectures are preferred for translation over encoder-only (BERT-style) models?

- Concept: Denoising autoencoder pretraining objectives (masked token/span prediction)
  - Why needed here: The monolingual pretraining phase uses mBART-style denoising; you must understand how corruption and reconstruction teach language representations.
  - Quick check question: What happens if masking rate is too high or too low for a morphologically rich language like Lingala?

- Concept: Subword tokenization (SentencePiece/BPE) for multilingual vocabularies
  - Why needed here: 80K shared vocabulary across 5+ languages; vocabulary design affects how well low-resource languages are represented versus high-resource.
  - Quick check question: How would you diagnose if Lingala is under-represented in a shared vocabulary dominated by English?

## Architecture Onboarding

- Component map: Monolingual corpora + Parallel corpora -> SentencePiece vocabulary (80K) -> mBART-style Transformer (6L/6L, 768/3072) -> Pretraining (denoising + translation) -> Fine-tuning (English-Lingala) -> Evaluation (BLEU/chrF)

- Critical path:
  1. Prepare monolingual corpora (5 languages) + parallel corpora (4 language pairs)
  2. Train shared SentencePiece vocabulary
  3. Pretrain with joint denoising + translation objectives (100K iterations, ~4-5 days on T4 GPU)
  4. Fine-tune on English-Lingala parallel data
  5. Evaluate with SacreBLEU (detokenized BLEU + chrF)

- Design tradeoffs:
  - Vocabulary size (80K): Larger improves low-resource coverage but increases computation; smaller speeds training but risks OOV
  - Language selection: Including more African languages helps multilingual transfer but dilutes per-language capacity
  - English data scale: Too little hurts (10MB); sufficient helps (112MB), but optimal ratio to target-language data is unstated

- Failure signatures:
  - BLEU < 22 with monolingual-only pretraining: Expected without parallel data in pretraining
  - BLEU drops when adding small English data (~10MB): Under-representation of source language
  - chrF significantly lower than BLEU relative to baseline: Possible tokenization issues with morphologically rich target
  - AfroBART outperforms your model: Check if Lingala was included in pretraining; verify parallel data quality

- First 3 experiments:
  1. Replicate Experiment 1 (monolingual Lingala only, 100K iterations) to establish baseline and verify training pipeline
  2. Add one related African language (e.g., Swahili) to pretraining; compare BLEU/chrF to quantify marginal benefit
  3. Introduce parallel data with translation objective alongside denoising; confirm joint training improves over denoising-only

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the integration of explicit linguistic knowledge into the pretraining phase significantly improve translation quality for morphologically rich, low-resource languages compared to data-driven methods alone?
- Basis in paper: [explicit] The conclusion states that future research "can explore different pretraining techniques and incorporate more linguistic knowledge."
- Why unresolved: The current study relied on statistical learning from monolingual and parallel corpora without explicitly modeling linguistic rules or structures specific to Lingala.
- What evidence would resolve it: A comparative study showing performance gains when linguistic features (e.g., morphology, syntax) are explicitly injected into the model during pretraining.

### Open Question 2
- Question: To what extent do the reported BLEU and chrF scores for Lingala translation correlate with human evaluations of fluency and adequacy?
- Basis in paper: [explicit] The Limitations section notes the reliance on automatic metrics and states that "Future work should explore... evaluation methods, such as human evaluation."
- Why unresolved: The study relied entirely on automatic metrics (BLEU, chrF), which the authors acknowledge may not fully capture translation quality for morphologically rich languages.
- What evidence would resolve it: Human evaluation scores (e.g., Direct Assessment or error analysis) for the model outputs that show a strong statistical correlation with the reported automatic metric improvements.

### Open Question 3
- Question: Can the finding that multilingual pretraining on African languages outperforms bilingual pretraining be generalized to other low-resource language families outside of the African context?
- Basis in paper: [inferred] The Limitations section mentions that "findings may not generalize to all low-resource languages due to linguistic differences," and the Conclusion calls for exploring "different pretraining techniques."
- Why unresolved: The experiments were restricted to Lingala and a small set of African languages (Afrikaans, Swahili, Zulu), leaving the efficacy of this specific data mix untested for non-African language families.
- What evidence would resolve it: Results from replicating the exact pretraining strategy (monolingual + parallel data mix) on a distinct set of low-resource languages (e.g., indigenous American or Asian languages).

## Limitations
- The study focuses on a single low-resource language pair (English-Lingala) without broader generalization across language families
- Key hyperparameters for denoising objectives, learning rate schedules, and dropout rates are unspecified, making exact reproduction challenging
- Data augmentation and oversampling techniques are described but implementation details are absent

## Confidence

- **High confidence**: The general finding that multilingual pretraining improves low-resource translation quality is well-supported by both this study and the broader literature on cross-lingual transfer learning.

- **Medium confidence**: The specific performance metrics (BLEU 27.38, chrF 53.24) are credible given the methodology, but the exact reproducibility is uncertain due to unspecified hyperparameters and potential data availability issues.

- **Low confidence**: The precise threshold effect of English data scale (10MB vs 112MB) appears to be an empirical finding specific to this experimental setup without broader testing.

## Next Checks
1. Replicate the English data scale experiment: Systematically vary the amount of English monolingual data used in pretraining (e.g., 10MB, 50MB, 100MB, 112MB) while keeping all other factors constant to verify the reported threshold effect.

2. Conduct an ablation study on related language inclusion: Pretrain on English-Lingala only, then incrementally add one related African language at a time to quantify the marginal benefit of each additional language.

3. Compare denoising vs translation objective contributions: Train models using only the denoising objective, only the translation objective, and the combined objective to directly measure whether the reported improvement from joint training is additive, synergistic, or potentially subject to interference effects.