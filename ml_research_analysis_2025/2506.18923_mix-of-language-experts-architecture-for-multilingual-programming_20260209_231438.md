---
ver: rpa2
title: Mix-of-Language-Experts Architecture for Multilingual Programming
arxiv_id: '2506.18923'
source_url: https://arxiv.org/abs/2506.18923
tags:
- programming
- language
- adapter
- languages
- code
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of efficiently supporting multilingual
  programming across multiple programming languages using large language models. The
  authors propose Mix-of-Language-Experts (MOLE), a novel architecture that balances
  efficiency and specialization by using a shared LoRA adapter for common programming
  knowledge, expert LoRA adapters for language-specific syntax/semantics, and an NL
  adapter for natural language processing.
---

# Mix-of-Language-Experts Architecture for Multilingual Programming

## Quick Facts
- **arXiv ID**: 2506.18923
- **Source URL**: https://arxiv.org/abs/2506.18923
- **Reference count**: 40
- **Primary result**: MOLE achieves close to full finetuning performance on multilingual programming tasks while being more parameter-efficient

## Executive Summary
This paper addresses the challenge of efficiently supporting multilingual programming across multiple programming languages using large language models. The authors propose Mix-of-Language-Experts (MOLE), a novel architecture that balances efficiency and specialization by using a shared LoRA adapter for common programming knowledge, expert LoRA adapters for language-specific syntax/semantics, and an NL adapter for natural language processing. During inference, MOLE automatically routes to the appropriate expert adapter based on the token's language. The approach was evaluated on eight programming languages across three tasks (code summarization, synthesis, and translation) using the HumanEvalPack benchmark. MOLE consistently outperformed both all-language finetuning and per-language finetuning baselines, achieving Pass@1 scores of 23.75% on summarization, 31.34% on synthesis, and 60.62% on translation.

## Method Summary
The Mix-of-Language-Experts (MOLE) architecture employs a multi-adapter approach where a shared LoRA adapter handles common programming knowledge, while individual expert LoRA adapters manage language-specific syntax and semantics for each programming language. An additional NL adapter processes natural language components. During inference, the system performs token-level language identification to route each token to the appropriate adapter, allowing the model to leverage specialized knowledge when needed while maintaining efficiency through shared components. The architecture was trained jointly on all languages rather than independently finetuning separate models, enabling better knowledge organization and transfer between languages.

## Key Results
- MOLE achieved Pass@1 scores of 23.75% on code summarization, 31.34% on synthesis, and 60.62% on translation tasks
- Performance was close to full finetuning while being more parameter-efficient than training separate models per language
- Demonstrated particular effectiveness for low-resource programming languages compared to baseline approaches

## Why This Works (Mechanism)
The MOLE architecture works by intelligently combining shared and specialized knowledge through a routing mechanism that identifies the language of each token during inference. This allows the model to apply general programming principles universally while invoking specialized adapters for language-specific syntax and semantics when necessary. The joint training approach enables the model to learn common programming patterns across languages while maintaining the ability to handle unique language features. The NL adapter provides additional capability for handling natural language components in code comments and documentation. This hybrid approach balances the efficiency of shared parameters with the performance benefits of specialization, addressing the limitations of both full finetuning (high computational cost) and per-language finetuning (poor knowledge sharing).

## Foundational Learning

**LoRA adapters** - Low-Rank Adaptation technique that freezes original model weights and applies small trainable matrices for efficient finetuning. *Why needed*: Enables parameter-efficient specialization without full model retraining. *Quick check*: Verify adapter dimensions and rank settings are appropriate for target model size.

**Token-level language identification** - Process of determining which programming language each token belongs to in mixed-language contexts. *Why needed*: Enables precise routing to appropriate expert adapters during inference. *Quick check*: Test language detection accuracy on code with mixed-language tokens.

**Parameter sharing vs. specialization tradeoff** - Fundamental balance between computational efficiency and task-specific performance. *Why needed*: Determines how to allocate model capacity across common vs. language-specific knowledge. *Quick check*: Compare performance degradation when removing shared vs. expert components.

**Joint vs. independent training** - Approaches to training multiple language models either together or separately. *Why needed*: Affects knowledge transfer and model generalization across languages. *Quick check*: Measure performance on low-resource languages to assess transfer benefits.

## Architecture Onboarding

**Component map**: Input tokens -> Language ID Router -> Shared LoRA Adapter + (Expert LoRA Adapter OR NL Adapter) -> Output tokens

**Critical path**: During inference, each token flows through language identification, then routing to either the shared adapter alone or combined with the appropriate expert adapter, producing the final output.

**Design tradeoffs**: The architecture balances between computational efficiency (shared parameters) and specialization (expert adapters), accepting the routing overhead to achieve better performance on low-resource languages. The choice of LoRA rank and adapter dimensions represents a tradeoff between adaptation capacity and parameter efficiency.

**Failure signatures**: Performance degradation on unseen languages, routing errors for ambiguous tokens, and context loss when switching between languages. The model may struggle with mixed-language code or tokens that belong to multiple languages.

**First experiments**: 1) Benchmark routing accuracy on code with known language labels, 2) Ablation study removing individual expert adapters to measure their contribution, 3) Test generalization to programming languages not included in training.

## Open Questions the Paper Calls Out
None

## Limitations
- Performance may degrade on unseen programming languages not included in training
- Evaluation focuses on synthetic benchmarks rather than real-world programming scenarios
- Computational overhead from routing decisions and multiple adapter management could impact deployment

## Confidence
- "Close to full finetuning performance": Medium confidence (improvements over baselines but still lags on some tasks)
- "Particularly effective for low-resource languages": High confidence (clear performance gains demonstrated)
- "Better knowledge organization and sharing": Medium confidence (supported by results but needs more detailed analysis)

## Next Checks
1. Evaluate MOLE on real-world programming tasks involving large codebases with mixed-language files to test routing robustness in practical scenarios
2. Conduct ablation studies removing individual expert adapters to quantify their specific contributions and validate the routing mechanism's decision-making
3. Test the architecture's generalization to additional programming languages beyond the eight evaluated to assess scalability and cross-language transfer capabilities