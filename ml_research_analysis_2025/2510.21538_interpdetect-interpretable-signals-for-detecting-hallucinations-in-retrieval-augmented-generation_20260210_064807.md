---
ver: rpa2
title: 'InterpDetect: Interpretable Signals for Detecting Hallucinations in Retrieval-Augmented
  Generation'
arxiv_id: '2510.21538'
source_url: https://arxiv.org/abs/2510.21538
tags:
- hallucination
- detection
- context
- knowledge
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a mechanistic interpretability-based approach
  for detecting hallucinations in retrieval-augmented generation (RAG) systems. The
  method computes external context scores (ECS) and parametric knowledge scores (PKS)
  across model layers and attention heads to quantify reliance on retrieved information
  versus internal knowledge.
---

# InterpDetect: Interpretable Signals for Detecting Hallucinations in Retrieval-Augmented Generation

## Quick Facts
- **arXiv ID**: 2510.21538
- **Source URL**: https://arxiv.org/abs/2510.21538
- **Reference count**: 32
- **Primary result**: Proposes ECS/PKS-based mechanistic interpretability for RAG hallucination detection, achieving 75.36% F1 via proxy-model evaluation

## Executive Summary
This paper introduces a mechanistic interpretability approach for detecting hallucinations in retrieval-augmented generation systems. The method computes External Context Scores (ECS) and Parametric Knowledge Scores (PKS) across model layers and attention heads to quantify reliance on retrieved information versus internal knowledge. Using Qwen3-0.6b, these scores are computed at the span level and used as features to train regression-based classifiers for hallucination detection. The approach achieves F1 scores of 74.68% in self-evaluation and 75.36% in proxy-based evaluation, outperforming several commercial detection systems. Notably, classifiers trained on the 0.6B-parameter Qwen3-0.6b generalize effectively to detect hallucinations in GPT-4.1-mini outputs, demonstrating the potential of proxy-model evaluation for economical large-scale deployment.

## Method Summary
The method computes ECS by measuring semantic alignment between response chunks and context chunks most attended to by each attention head, using cosine similarity via BAAI/bge-base-en-v1.5 embeddings. PKS measures Jensen-Shannon divergence between vocabulary distributions before and after FFN application to quantify parametric knowledge injection. These features are computed at the span level across all layers and heads, then reduced from 476 to 341 features through correlation filtering. Classifiers (SVC, Logistic Regression, Random Forest, XGBoost) are trained on labeled FinQA data with responses from Qwen3-0.6b. Span-level predictions are aggregated to response-level predictions for final hallucination detection.

## Key Results
- Achieves 74.68% F1 score in self-evaluation (training and testing on Qwen3-0.6b signals)
- Achieves 75.36% F1 score in proxy-based evaluation (training on Qwen3-0.6b, testing on GPT-4.1-mini)
- Outperforms commercial baselines (TruLens, RefChecker) and smaller open-source models
- Demonstrates effective cross-model generalization from 0.6B proxy to larger model outputs

## Why This Works (Mechanism)

### Mechanism 1
Lower External Context Score (ECS) correlates with higher hallucination likelihood in RAG outputs. ECS quantifies semantic alignment between response chunks and the context chunks most attended to by each attention head. When models fail to adequately exploit retrieved context, attention heads show weaker alignment between attended context and generated output, yielding lower ECS values. Core assumption: Attention head patterns meaningfully reflect information sourcing; semantic similarity via embeddings captures grounding quality. Evidence: All attention heads exhibit negative correlations with hallucinations, confirming higher ECS values associate with lower hallucination likelihood. Break condition: If attention patterns are decorrelated from actual information flow, ECS becomes noisy proxy.

### Mechanism 2
Elevated Parametric Knowledge Score (PKS) in later-layer FFNs correlates with hallucination occurrence. PKS measures JSD between vocabulary distributions before and after FFN application. Later-layer FFNs injecting more parametric knowledge shift output distributions away from context-grounded predictions. During hallucinations, these modules over-contribute internal knowledge relative to external grounding. Core assumption: The residual stream-to-vocabulary projection meaningfully captures "what the FFN is suggesting"; JSD distance reflects knowledge injection magnitude. Evidence: Later-layer FFNs are positively correlated with hallucinations. Break condition: If FFN contributions are confounded by non-knowledge operations, PKS may capture noise rather than parametric knowledge injection.

### Mechanism 3
Classifiers trained on small-proxy-model mechanistic signals transfer to detecting hallucinations in larger-model outputs. ECS/PKS patterns reflect a model-agnostic principle—grounded responses should show higher context utilization and lower parametric injection. A 0.6B proxy model's internal signals can assess this balance for any response-context pair, regardless of which model generated the response. Core assumption: The relationship between grounding quality and ECS/PKS signatures generalizes across model scales and architectures. Evidence: Classifiers trained on Qwen3-0.6b signals generalize to GPT-4.1-mini responses with 75.36% F1. Break condition: If different architectures encode grounding differently, proxy transfer degrades.

## Foundational Learning

- **Residual stream architecture in transformers**: PKS computation requires understanding how FFNs write to and modify the residual stream. The "before FFN" vs. "after FFN" comparison only makes sense if you grasp residual connections. Quick check: Can you explain why comparing residual stream states before and after an FFN layer reveals what that layer contributed?

- **Mechanistic interpretability via activation analysis**: The entire method depends on extracting and interpreting internal activations (attention patterns, FFN outputs) rather than treating the model as black-box. Quick check: What's the difference between probing a model's outputs vs. probing its internal activations?

- **Attention head function decomposition**: ECS relies on identifying which context chunks each attention head attends to. Understanding that different heads serve different computational roles clarifies why per-head scoring matters. Quick check: Why compute ECS per attention head rather than aggregating attention across all heads?

## Architecture Onboarding

- **Component map**: RAG pipeline (query → retrieval → response generation) → TransformerLens hook-based activation extraction → ECS computation (attention weights → identify max-attended context chunk → cosine similarity with response chunk) → PKS computation (pre-FFN residual → project to vocab distribution → post-FFN residual → project to vocab distribution → JSD) → Feature engineering (476 raw features → correlation-filtered to 341) → Classifier (SVC selected via validation F1)

- **Critical path**: Response-context pairs must be chunked consistently (sentence-level); Activations must be captured at correct layer positions (pre/post FFN); Vocabulary projection must use same unembedding matrix for valid JSD comparison; Feature selection before classification to reduce redundancy

- **Design tradeoffs**: Chunk-level vs. token-level chosen for computational efficiency; Full layer/head coverage used due to weak copying-head correlation in Qwen3-0.6b; SVC preferred over XGBoost due to overfitting with limited data

- **Failure signatures**: High recall / lower precision suggests false-positive span labels from labeling pipeline; GPU memory overflow during PKS computation requires float16 and explicit tensor cleanup; Cross-model transfer gaps if source model's attention patterns differ substantially from target

- **First 3 experiments**: 1) Reproduce ECS/PKS computation on single response-context pair to verify implementation; 2) Ablate feature sets using only late-layer features vs. full set to test late-layer sufficiency hypothesis; 3) Cross-architecture proxy test training on Qwen3-0.6b, evaluating on LLaMA variant to characterize generalization bounds

## Open Questions the Paper Calls Out

- Can predictive performance be maintained or improved by computing mechanistic metrics only on a pruned subset of late layers rather than all layers and heads? The authors note that computing scores across all layers is computationally intensive and suggest that retaining only late-layer signals may suffice.

- Can mechanistic signals (ECS/PKS) be effectively utilized for real-time response intervention to prevent hallucinations during generation? The current framework is insufficient as it lacks the ability to manipulate layer-level activations in real time.

- Does augmenting mechanistic features with uncertainty-based measures or representation-level features significantly improve detection accuracy? The authors state that expanding the feature set to include uncertainty-based measures and representation-level features could enrich the representation and improve detection accuracy.

## Limitations
- Cross-model proxy evaluation (training on Qwen3-0.6b, evaluating on GPT-4.1-mini) lacks extensive empirical validation across multiple architecture pairs
- Reliance on proxy-model mechanistic signals may not capture all hallucination types, particularly those from prompt ambiguity or retrieval quality issues
- Performance still falls short of the largest proprietary models (e.g., GPT-5), suggesting the signal is incomplete

## Confidence

- **High confidence**: ECS and PKS computations are mechanistically sound; correlation patterns between lower ECS/higher PKS and hallucinations are well-established within the Qwen3-0.6b model
- **Medium confidence**: Classifier performance claims are reproducible given specified data and methods, though exact hyperparameters remain unspecified
- **Low confidence**: Cross-model transfer claim - while results are promising, the generalization across architectures requires more systematic validation

## Next Checks
1. **Ablation study on model architectures**: Train classifiers on ECS/PKS from multiple different architectures (e.g., LLaMA, Mistral) and test transfer performance across all pairs to quantify generalization bounds
2. **Feature importance analysis**: Conduct SHAP or permutation importance analysis to verify that late-layer features dominate predictions as hypothesized, and that specific attention heads contribute meaningfully to ECS
3. **Robustness to retrieval quality**: Test classifier performance across varying retrieval relevance scores to determine whether ECS/PKS signals remain predictive when retrieved context quality degrades