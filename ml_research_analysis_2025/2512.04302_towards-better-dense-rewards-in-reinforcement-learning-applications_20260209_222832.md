---
ver: rpa2
title: Towards better dense rewards in Reinforcement Learning Applications
arxiv_id: '2512.04302'
source_url: https://arxiv.org/abs/2512.04302
tags:
- reward
- graph
- learning
- state
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of creating meaningful and accurate
  dense rewards in reinforcement learning, particularly for goal-conditioned hierarchical
  RL and text generation via RLHF. The core idea is to leverage graph structures to
  better represent state transitions and spatial relationships, improving reward signals
  at both the high and low levels of hierarchy.
---

# Towards better dense rewards in Reinforcement Learning Applications

## Quick Facts
- arXiv ID: 2512.04302
- Source URL: https://arxiv.org/abs/2512.04302
- Authors: Shuyuan Zhang
- Reference count: 40
- One-line primary result: Graph-encoded dense rewards (G4RL) and Shapley-based reward shaping (SCAR) significantly improve learning efficiency in both goal-conditioned hierarchical RL and RLHF text generation tasks.

## Executive Summary
This paper addresses the challenge of creating meaningful and accurate dense rewards in reinforcement learning, particularly for goal-conditioned hierarchical RL and text generation via RLHF. The core idea is to leverage graph structures to better represent state transitions and spatial relationships, improving reward signals at both the high and low levels of hierarchy. For GCHRL, a graph encoder-decoder (G4RL) is introduced to encode subgoal representations that respect true transition dynamics, leading to more accurate intermediate rewards. For RLHF, Shapley Credit Assignment Rewards (SCAR) redistribute sparse terminal rewards across sequence steps based on game-theoretic Shapley values, enabling denser and more efficient credit assignment. Experiments show G4RL significantly improves performance across navigation tasks (e.g., AntMaze, AntPush) and image-based states, while SCAR accelerates learning and improves final reward scores in sentiment control, text summarization, and instruction tuning tasks, with LLM-as-a-judge win rates up to 61.2% over baselines.

## Method Summary
The paper introduces two complementary approaches for dense reward generation. G4RL uses graph neural networks to encode state transition dynamics, replacing Euclidean distance with graph-encoded transition distance for subgoal reward computation in hierarchical RL. It maintains an adaptive state graph that captures reachability rather than spatial proximity. SCAR applies Shapley value theory to redistribute sparse terminal rewards across text generation steps, providing denser credit assignment signals while preserving policy optimality through the efficiency axiom. Both methods are designed to be plug-in replacements for existing RL pipelines.

## Key Results
- G4RL achieves up to 15% higher success rates in AntMaze navigation tasks compared to baseline hierarchical RL methods
- SCAR improves final reward scores by 12-15% across three text generation tasks (IMDB sentiment, TL;DR summarization, HH instruction tuning)
- LLM-as-a-judge win rates reach 61.2% for SCAR vs RLHF baselines on summarization tasks
- G4RL shows effective generalization to image-based state representations with comparable performance gains
- Dual-level intrinsic rewards in G4RL (high-level subgoal shaping + low-level progress shaping) accelerate convergence more than either alone

## Why This Works (Mechanism)

### Mechanism 1: Graph-Encoded Transition Geometry Corrects Euclidean Distance Bias
- Claim: Replacing Euclidean state distance with graph-encoded transition distance produces more accurate intermediate rewards in hierarchical RL, *conditional on* the environment having predominantly symmetric and reversible dynamics.
- Mechanism: A graph encoder-decoder (G4RL) learns to embed states such that dot-product similarity between embeddings approximates normalized adjacency weights from a state transition graph. This encodes reachability rather than spatial proximity.
- Core assumption: The dot product of embeddings can capture neighborhood overlap and connectivity patterns. Undirected graphs suffice because environments are primarily reversible.
- Evidence anchors:
  - [abstract]: "leverage graph structures to better represent state transitions and spatial relationships, improving reward signals at both the high and low levels of hierarchy"
  - [Section 3.1]: "The graph is constantly updated during exploration... edges in the graph represent connectivity between states"
  - [Section 3.4, Eq. 10]: Low-level reward uses D(E(ϕ(st+1)), E(gt)) to compute distance in subgoal space rather than original state space
  - [corpus]: Neighbor papers on dense reward shaping (e.g., "From Sparse to Dense") address similar problems but via different mechanisms; no direct corpus validation of graph-based approach specifically.
- Break condition: Fails in highly asymmetric environments where irreversible transitions dominate (acknowledged in Section 3.1 and AntPush/AntFall experiments show degraded but non-zero gains).

### Mechanism 2: Shapley Values Provide Policy-Invariant Reward Redistribution
- Claim: Redistributing sparse terminal rewards via Shapley values across text units produces denser learning signals while preserving optimal policy, *conditional on* the reward model providing meaningful scores for partial sequences.
- Mechanism: Treats each text unit (token/span/sentence) as a "player" in a cooperative game where the characteristic function v(S) = r_ϕ(x, y_S) evaluates partial sequences. Shapley values compute marginal contributions over all coalitions, ensuring efficiency (total credit equals original reward).
- Core assumption: The reward model generalizes to partial/out-of-distribution sequences without catastrophic degradation.
- Evidence anchors:
  - [abstract]: "SCAR redistribute sparse terminal rewards across sequence steps based on game-theoretic Shapley values"
  - [Section 4.3, Theorem 1]: Proves policy invariance—optimal policies under shaped and original MDPs coincide
  - [Section 4.5, Table 1]: SCAR achieves highest test rewards across all three tasks (IMDB: 9.27, TL;DR: 4.35, HH: 7.31)
  - [corpus]: Weak direct validation; corpus emphasizes VLM/LLM-based reward engineering but not Shapley specifically.
- Break condition: Rule-based reward models (e.g., math correctness checkers) that cannot score incomplete sequences.

### Mechanism 3: Dual-Level Intrinsic Rewards Align Hierarchical Exploration
- Claim: Combining intrinsic rewards at both hierarchy levels accelerates convergence more than either alone, *conditional on* appropriate α_h and α_l hyperparameter tuning.
- Mechanism: High-level intrinsic (Eq. 9) penalizes proposing unreachable subgoals via graph distance D(E(φ(st)), E(gt)). Low-level intrinsic (Eq. 10) rewards progress toward subgoals in the learned representation space. These operate complementarily—high-level shapes *what* to pursue; low-level shapes *how* to pursue it.
- Core assumption: Graph structure is sufficiently mature when intrinsic rewards begin to guide exploration meaningfully.
- Evidence anchors:
  - [Section 3.6.1, Figures 4-5]: "The combination of high-level and low-level intrinsic rewards results in the highest success rates and fastest convergence" across HESS and HLPS variants
  - [Section 3.6.1]: "High-level intrinsic-only variant outperforms low-level intrinsic-only variant, especially in sparse reward tasks"
  - [corpus]: No direct corpus comparison; intrinsic reward design is an active area but this dual-level coordination is paper-specific.
- Break condition: Incorrect hyperparameter balances (α too high) may override extrinsic task signal, causing reward hacking.

## Foundational Learning

- **Concept: Goal-Conditioned Hierarchical RL (GCHRL)**
  - Why needed here: G4RL is a plug-in augmentation for existing GCHRL methods (HIRO, HRAC, HESS, HLPS). Understanding the high-level/low-level policy split and subgoal-based training is prerequisite.
  - Quick check question: Can you explain why Euclidean subgoal distance fails in a maze with walls?

- **Concept: Shapley Values in Cooperative Game Theory**
  - Why needed here: SCAR's theoretical foundation. The efficiency axiom (Σ SV = v(P)) ensures dense rewards sum to the original sparse reward without introducing bias.
  - Quick check question: Why does computing exact Shapley values require O(2^N) evaluations, and what does Owen value approximation sacrifice to reduce this?

- **Concept: Reward Shaping and Policy Invariance**
  - Why needed here: Critical for understanding *why* SCAR's redistribution is safe. Potential-based shaping guarantees policy invariance; SCAR achieves this via Shapley efficiency property.
  - Quick check question: What would happen to policy optimality if dense rewards summed to more than the terminal reward?

## Architecture Onboarding

- **Component map:**
  ```
  [G4RL Pipeline]
  Environment → State φ(s) → State Graph (V, E, A) → Graph Encoder E(·) → Subgoal Embedding g(s)
                                                                     ↓
  Low-Level Policy π_l ← Intrinsic Reward r_l (Eq. 10) ← Decoder D(·,·)
  
  [SCAR Pipeline]
  Prompt x → Policy π_θ → Sequence y → Segmentation → Units {u_i}
                                               ↓
  Reward Model r_φ → Characteristic Function v(S) → Shapley Approximation → Dense R_shap_t
  ```

- **Critical path:**
  1. For G4RL: Graph construction (Section 3.1.1) must run online during exploration; encoder-decoder training (Section 3.2) must adapt as graph evolves.
  2. For SCAR: Segmentation granularity choice (token/span/sentence) directly controls N and approximation quality.

- **Design tradeoffs:**
  - G4RL: Larger N (graph capacity) improves coverage but quadratic memory growth; sampling interval t_c trades graph fidelity for speed (Section 3.5).
  - SCAR: Finer segmentation (token-level) maximizes credit granularity but explodes N; Owen value approximation reduces O(2^N) → O(N^2) but relies on hierarchical partition quality.
  - α interpolation (Eq. 16): α=1 fully trusts Shapley; α=0 reverts to sparse. Intermediate values hedge against Shapley noise.

- **Failure signatures:**
  - G4RL: High variance in early episodes (Section 3.3), degraded performance in asymmetric environments, overfitting if encoder-decoder trains on stale graph data.
  - SCAR: Negative rewards assigned to tokens that are actually beneficial (reward model noise), catastrophic OOD evaluation if partial sequences confuse r_φ.
  - Both: Hyperparameter sensitivity (ε_d, α_h, α_l, α) requiring per-environment tuning.

- **First 3 experiments:**
  1. **Validate G4RL graph quality:** Run AntMaze with graph logging. Visualize learned embeddings via t-SNE; verify that spatially proximate but transitionally distant states (across walls) have low dot-product similarity.
  2. **SCAR ablation on segmentation:** On TL;DR summarization, compare token-level vs. span-level vs. sentence-level units. Measure convergence speed and final reward; expect span-level to offer best tradeoff.
  3. **Hyperparameter sweep on α_l/α_h:** Grid search α_h ∈ {0.0, 0.5, 1.0} × α_l ∈ {0.0, 0.5, 1.0} in AntMaze-Sparse. Replicate Figure 4 curves to validate dual-intrinsic synergy.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can methods be developed to automatically select G4RL hyperparameters (e.g., $\epsilon_d, \alpha_l, \alpha_h$) based on environmental dynamics?
- Basis in paper: [explicit] The authors state in Section 3.7 that future work aims to develop methods for automatic hyperparameter selection to reduce manual tuning.
- Why unresolved: Currently, G4RL's effectiveness relies significantly on the careful manual tuning of these parameters across different environments.
- What evidence would resolve it: An adaptive mechanism that sets these parameters dynamically without human intervention while maintaining performance parity.

### Open Question 2
- Question: How robust is the G4RL framework in environments with predominantly asymmetric or irreversible state transitions?
- Basis in paper: [explicit] Section 3.7 notes that the method assumes symmetric dynamics and explicitly calls for "more evidence on asymmetric environments" to demonstrate robustness.
- Why unresolved: The current graph structure is undirected to ensure efficiency and method compatibility, which may fail to accurately model irreversible changes.
- What evidence would resolve it: Successful performance benchmarks in complex environments specifically designed to feature irreversible state dynamics.

### Open Question 3
- Question: Can more efficient approximation techniques be developed to reduce the computational overhead of Shapley value calculations in SCAR?
- Basis in paper: [explicit] Section 4.6 lists "computational overhead of Shapley approximations" as a limitation and targets efficient approximation techniques for future work.
- Why unresolved: While Owen values reduce complexity from exponential to quadratic, the overhead remains non-trivial for large-scale application.
- What evidence would resolve it: A modified approximation algorithm that lowers computational cost (time/memory) without degrading the quality of credit assignment.

### Open Question 4
- Question: Can structural similarities identified via graph Laplacians be effectively used to generate intrinsic rewards for continual reinforcement learning?
- Basis in paper: [explicit] Section 5 proposes using graph spectral comparisons to identify "invariant knowledge" for transfer, listing the identification of test environments and baselines as ongoing work.
- Why unresolved: The approach is currently a proposal; the theoretical mapping from spectral similarity to effective intrinsic rewards remains unvalidated.
- What evidence would resolve it: Experiments showing accelerated learning curves in a second task derived from the spectral analysis of a prior task's state graph.

## Limitations
- Numerical hyperparameters (ε_d, α_h, α_l, N, β, t_c, α) are unspecified, blocking exact reproduction
- Performance claims in asymmetric environments (AntPush, AntFall) show degraded but non-zero gains, indicating limited robustness
- Generalization to image-based states is minimally validated with only one experimental condition (AntMaze-Image)

## Confidence
- **High confidence**: The theoretical foundations (Shapley efficiency property ensuring policy invariance, graph encoding of transition geometry) are mathematically sound and well-proven
- **Medium confidence**: Experimental results show consistent improvements across tasks, but hyperparameter sensitivity and limited ablation studies reduce confidence in robustness claims
- **Low confidence**: Generalization claims to image-based states and asymmetric environments are minimally validated, with only single-task demonstrations

## Next Checks
1. **Validate G4RL graph quality:** Run AntMaze with graph logging. Visualize learned embeddings via t-SNE; verify that spatially proximate but transitionally distant states (across walls) have low dot-product similarity.
2. **SCAR ablation on segmentation:** On TL;DR summarization, compare token-level vs. span-level vs. sentence-level units. Measure convergence speed and final reward; expect span-level to offer best tradeoff.
3. **Hyperparameter sweep on α_l/α_h:** Grid search α_h ∈ {0.0, 0.5, 1.0} × α_l ∈ {0.0, 0.5, 1.0} in AntMaze-Sparse. Replicate Figure 4 curves to validate dual-intrinsic synergy.