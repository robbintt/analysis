---
ver: rpa2
title: Hardware/Software Co-Design of RISC-V Extensions for Accelerating Sparse DNNs
  on FPGAs
arxiv_id: '2504.19659'
source_url: https://arxiv.org/abs/2504.19659
tags:
- sparsity
- weights
- pruning
- unstructured
- design
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper presents a hardware/software co-design approach to
  accelerate sparse deep neural networks (DNNs) on FPGAs by extending the RISC-V instruction
  set. The authors introduce three designs: one for unstructured sparsity using a
  variable-cycle sequential MAC unit, one for semi-structured sparsity with a lookahead
  encoding scheme that embeds sparsity information in DNN weights, and a combined
  design supporting both types.'
---

# Hardware/Software Co-Design of RISC-V Extensions for Accelerating Sparse DNNs on FPGAs

## Quick Facts
- arXiv ID: 2504.19659
- Source URL: https://arxiv.org/abs/2504.19659
- Authors: Muhammad Sabih; Abrarul Karim; Jakob Wittmann; Frank Hannig; Jürgen Teich
- Reference count: 37
- Primary result: 5× speedup on TinyML models with <4% FPGA LUT overhead via RISC-V CFU extensions for sparse DNNs

## Executive Summary
This paper introduces a hardware/software co-design approach to accelerate sparse deep neural network inference on FPGAs by extending the RISC-V instruction set. The authors present three designs: one for unstructured sparsity using variable-cycle sequential MAC units, one for semi-structured sparsity with lookahead encoding that embeds skip information in weight LSBs, and a combined design supporting both. The approach achieves up to 5× speedup on standard TinyML applications while maintaining model accuracy when reducing INT8 weights to INT7 for semi-structured sparsity.

## Method Summary
The authors extend RISC-V with custom functional units (CFUs) to accelerate sparse DNN inference through hardware/software co-design. Three designs are presented: Unstructured Sparsity Support Architecture (USSA) uses variable-cycle sequential MAC units with zero-detection to skip computations; Semi-Structured Sparsity Support Architecture (SSSA) employs lookahead encoding where skip information is embedded in weight LSBs using 4-bit skip counts for 4-weight blocks; and Combined Sparsity Support Architecture (CSA) integrates both approaches. Models are pruned using iterative explainable-AI-based ranking, then preprocessed to embed skip information for SSSA/CSA. The CFUs are implemented using VexRiscv in CFU Playground and synthesized with SymbiFlow/Vivado.

## Key Results
- 5× speedup on TinyML applications (keyword spotting, image classification, person detection) compared to baseline SIMD MAC
- Less than 4% additional LUT overhead and 6% more flip-flops for FPGA implementation
- No accuracy loss when reducing precision from INT8 to INT7 for semi-structured sparsity approach
- Support for high sparsity ratios (50-90%) without accuracy degradation

## Why This Works (Mechanism)
The approach works by combining algorithmic sparsity patterns with specialized hardware acceleration. Unstructured sparsity is handled by detecting zero weights and skipping computations entirely, while semi-structured sparsity uses lookahead encoding to compress skip information directly into weight representations. This encoding allows the hardware to quickly determine how many computations to skip without additional metadata storage, enabling efficient block skipping during computation.

## Foundational Learning
- RISC-V Custom Functional Units (CFUs): Why needed - enables custom instruction extensions for domain-specific acceleration; Quick check - verify CFU integration with VexRiscv in CFU Playground
- Variable-cycle sequential MAC: Why needed - handles varying numbers of non-zero elements in unstructured sparsity; Quick check - confirm 1-cycle pass for all-zero blocks
- Lookahead encoding scheme: Why needed - embeds sparsity information in weights to avoid separate metadata; Quick check - validate 4-bit skip count encoding in weight LSBs
- Iterative explainable-AI-based pruning: Why needed - creates structured sparsity patterns for hardware acceleration; Quick check - verify per-layer sparsity ratios match target distributions
- INT7 quantization constraint: Why needed - reserves LSB for skip information in SSSA/CSA; Quick check - ensure weight distribution allows clipping to [-64,63] without accuracy loss

## Architecture Onboarding
**Component map:** Pruned model -> Lookahead encoding (SSSA/CSA) or direct processing (USSA) -> CFU hardware -> Modified kernel loops -> FPGA execution

**Critical path:** Custom instruction decode -> Skip detection/weight extraction -> MAC computation -> Induction variable update

**Design tradeoffs:** INT7 precision limitation vs. metadata-free encoding, variable latency vs. fixed throughput, hardware complexity vs. sparsity pattern flexibility

**Failure signatures:** Accuracy degradation indicates insufficient weight magnitude headroom for INT7 clipping; no speedup suggests CFU not on critical path or skip logic not exercised; timing violations indicate control logic complexity issues

**First experiments:** 1) Implement USSA variable-cycle MAC with zero-detection on Arty A7-35T; 2) Test SSSA lookahead encoding preprocessing script on VGG16 weights; 3) Validate combined design with CSA custom instructions on ResNet-56 layer

## Open Questions the Paper Calls Out
None

## Limitations
- INT7 weight constraint may not generalize to larger models with higher-magnitude weights
- Lookahead encoding efficiency depends heavily on specific 4-weight block structure
- Lack of publicly available RTL implementation details for USSA control logic
- Pruning methodology references iterative explainable-AI-based ranking without specifying exact hyperparameters

## Confidence
- **High Confidence**: FPGA resource overhead measurements (<4% LUTs, <6% FFs) and cycle count speedups for TinyML models are directly measured and reproducible
- **Medium Confidence**: 5× speedup and minimal accuracy loss when reducing to INT7 are specific to evaluated pruned models and sparsity patterns
- **Low Confidence**: Lookahead encoding efficiency claims depend on specific 4-weight block structure and may not extend to other block sizes

## Next Checks
1. Implement and verify USSA variable-cycle MAC control logic on Arty A7-35T with pruned VGG16 layer, measuring actual cycle counts vs. theoretical speedup
2. Test INT7 encoding approach on larger model (e.g., ResNet-50 on ImageNet) to assess accuracy loss beyond TinyML scale
3. Profile CFU timing closure and resource utilization across different sparsity ratios (50-90%) to confirm sub-4% LUT overhead holds