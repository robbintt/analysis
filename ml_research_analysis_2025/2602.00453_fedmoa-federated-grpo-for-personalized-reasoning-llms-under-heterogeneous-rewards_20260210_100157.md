---
ver: rpa2
title: 'FedMOA: Federated GRPO for Personalized Reasoning LLMs under Heterogeneous
  Rewards'
arxiv_id: '2602.00453'
source_url: https://arxiv.org/abs/2602.00453
tags:
- reward
- federated
- grpo
- fedmoa
- accuracy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes FedMOA, a federated GRPO algorithm for multi-objective
  alignment under heterogeneous rewards. FedMOA addresses the challenges of heterogeneous
  rewards, multi-objective alignment, and high computational costs in federated GRPO
  by introducing adaptive objective weighting via hypergradient descent and progress-aware
  aggregation based on locally adapted reward weights.
---

# FedMOA: Federated GRPO for Personalized Reasoning LLMs under Heterogeneous Rewards

## Quick Facts
- **arXiv ID:** 2602.00453
- **Source URL:** https://arxiv.org/abs/2602.00453
- **Reference count:** 10
- **Primary result:** FedMOA achieves up to 2.2% accuracy gain on MATH and 2.0% on GSM8K over federated baselines while balancing multi-objective rewards under heterogeneous data-reward settings.

## Executive Summary
FedMOA introduces a federated GRPO algorithm for personalized reasoning LLM alignment under heterogeneous rewards. The key innovation is a two-stage hierarchical aggregation: clients adaptively reweight objectives using hypergradient descent, and the server performs task-aware intra-cluster aggregation weighted by accuracy progress, followed by cross-cluster FedAvg. Experiments on MATH, GSM8K, and MBPP show consistent improvements in both global accuracy and multi-objective reward balance compared to standard federated baselines.

## Method Summary
FedMOA combines client-side adaptive objective weighting via hypergradient descent with server-side task- and accuracy-aware aggregation. During local training, clients compute per-objective gradients on an intermediate layer and update reward weights based on gradient alignment signals. These weights are used to prioritize primary reasoning objectives as auxiliary objectives saturate. At the server, clients are clustered by task, and within each cluster, aggregation weights are computed as the inverse of the accuracy weight (proxy for convergence progress), normalized by softmax. The global model is then aggregated across clusters using data proportion weighting.

## Key Results
- **Global accuracy improvements:** FedMOA improves global accuracy from 62.6% to 64.8% on MATH and from 75.0% to 77.0% on GSM8K in heterogeneous settings.
- **Multi-objective reward balance:** FedMOA achieves better aggregated reward scores (0.775 → 0.827 on MATH) by preventing auxiliary objectives from dominating optimization.
- **Per-client performance:** Individual client accuracy improves by up to 1.6% on MATH and 2.0% on GSM8K, demonstrating effective personalization under heterogeneous rewards.

## Why This Works (Mechanism)

### Mechanism 1: Client-Side Hypergradient-Based Objective Reweighting
FedMOA stabilizes local GRPO training through online adaptive weighting via hypergradient descent. At each iteration, clients compute per-objective gradients on an intermediate layer and use the inner product of consecutive gradients (Δ_k = g_k^T g_k^{t-1}) to measure progress. Positive alignment increases the weight (objective still improving), negative alignment decreases it (near convergence or oscillation). This prevents auxiliary objectives like format or tag count from overwhelming the primary accuracy objective.

### Mechanism 2: Accuracy-Weighted Intra-Cluster Aggregation
The server uses the adapted accuracy weight w_0^(m) as an inverse proxy for local progress. Clients are clustered by task, and within each cluster, aggregation weights are computed as s_m = 1/(w_0^(m)+ε), then softmax-normalized. Clients with smaller w_0 (accuracy closer to convergence, thus de-emphasized locally) receive larger aggregation weights, prioritizing those that have made more accuracy gains.

### Mechanism 3: Two-Stage Hierarchical Aggregation
FedMOA separates task-aware intra-cluster aggregation from inter-cluster FedAvg-style aggregation. Stage 1: Within task clusters, accuracy-weighted aggregation produces specialized cluster models. Stage 2: Across clusters, models are aggregated by data proportion (N_c / ΣN_c'). This preserves task-specific specialization while enabling cross-task knowledge sharing.

## Foundational Learning

- **Concept: Group Relative Policy Optimization (GRPO)**
  - **Why needed here:** FedMOA builds directly on GRPO as its base RL algorithm; understanding how GRPO computes group-relative advantages without a critic network is essential to grasp why it's suitable for federated on-device training.
  - **Quick check question:** How does GRPO compute advantages without a separate critic network, and why does this reduce memory overhead compared to PPO?

- **Concept: Federated Averaging (FedAvg)**
  - **Why needed here:** FedMOA's server-side aggregation extends FedAvg with task- and accuracy-aware weighting; understanding the baseline aggregation scheme is necessary to see what FedMOA modifies.
  - **Quick check question:** In FedAvg, how are client updates aggregated, and what assumption does this make about client data distributions?

- **Concept: Hypergradient Descent**
  - **Why needed here:** The adaptive objective weighting mechanism uses hypergradient descent to update reward weights online; this is the core innovation on the client side.
  - **Quick check question:** What signal does hypergradient descent use to adapt a hyperparameter, and how does Eq. 6 approximate this signal for learning rates?

## Architecture Onboarding

- **Component map:** Client GRPO trainer -> per-objective gradient extractor -> hypergradient weight updater -> simplex projector -> parameter sender. Server reward-name clusterer -> accuracy-weight inverse scorer -> softmax aggregator -> FedAvg aggregator -> model broadcaster.

- **Critical path:**
  1. Initialize shared model and reward weights per task cluster.
  2. Clients run local GRPO with hypergradient weight adaptation for T steps.
  3. Clients transmit parameters and adapted weights to server.
  4. Server clusters by task, computes accuracy-weighted aggregation per cluster.
  5. Server aggregates across clusters by data proportion.
  6. Server broadcasts global model and cluster-specific reward weights.
  7. Repeat for R communication rounds.

- **Design tradeoffs:**
  - **Gradient extraction layer choice:** Computing g_k on a single intermediate layer reduces cost but may lose signal; deeper layers capture more task-specific information but increase memory.
  - **Hypergradient step size λ:** Larger λ enables faster adaptation but risks instability; paper uses λ = 0.01.
  - **Aggregation weight ε:** Small ε (paper: 10^{-6}) prevents division by zero but may amplify noise if w_0^(m) is near-zero.

- **Failure signatures:**
  - Accuracy weights collapse to near-zero early: indicates hypergradient descent is too aggressive; reduce λ.
  - Global model underperforms local models significantly: suggests inter-cluster aggregation is destructive; verify task clustering correctness.
  - Auxiliary rewards dominate final model: indicates weight adaptation is not effective; check gradient extraction layer and hypergradient computation.
  - High variance in aggregated reward weights across rounds: may indicate unstable local training; increase local steps or reduce learning rate.

- **First 3 experiments:**
  1. **Reproduce Homo+Homo baseline:** Train FedGRPO (without FedMOA adaptations) on GSM8K with 10 clients for 3 rounds; verify global accuracy ~73.3% and stable reward curves. This establishes baseline behavior.
  2. **Ablate adaptive weighting:** Run FedMOA with λ = 0 (disable hypergradient) but keep accuracy-aware aggregation; measure accuracy and aggregated reward on MATH Heter+Heter. Expect modest improvement over FedGRPO (paper: 62.6% → 64.0% accuracy, reward unchanged).
  3. **Full FedMOA on Heter+Heter:** Enable both adaptations, train across MATH, GSM8K, and MBPP with 5 clients each; compare global and per-client accuracy against Table 1. Verify ~2% accuracy gains and improved multi-objective balance.

## Open Questions the Paper Calls Out

- **Scaling to larger models and complex objectives:** The authors explicitly state results "motivate future work on scaling to larger models and more complex objective structures," but current experiments are limited to Qwen2.5-1.5B-Instruct with 2-3 reward components.

- **Compatibility with other FL algorithms:** The paper excludes comparisons with FedProx, SCAFFOLD, or FedNova, noting they are "orthogonal to our focus on multi-objective alignment." It remains unclear if FedMOA's aggregation interferes with regularization or control variates used by these methods.

- **Robustness to clustering errors:** The method relies on "reward names" to group clients into "coarse task clusters" (e.g., math, coding), assuming distinct and accurate labels. In realistic scenarios with overlapping or semantically similar rewards, mis-grouping could distort global updates.

## Limitations

- **Single-layer gradient approximation:** The hypergradient mechanism uses gradient alignment on one intermediate layer as a proxy for optimization progress, which may not generalize well across diverse reward structures or network architectures.

- **Accuracy-weight convergence assumption:** The server-side aggregation assumes the accuracy weight w_0^(m) strongly correlates with actual optimization progress, but noisy gradients or early auxiliary reward saturation could break this assumption.

- **Task clustering dependency:** The two-stage aggregation requires meaningful task clusters with comparable accuracy objectives; poorly defined clusters or incomparable metrics could lead to destructive interference and degraded performance.

## Confidence

- **High confidence:** Experimental results showing accuracy improvements (up to 2.2% on MATH, 2.0% on GSM8K) and improved multi-objective reward balance are well-supported by provided tables and ablation studies.

- **Medium confidence:** Mechanism descriptions are clear, but the effectiveness of the single-layer gradient alignment approximation and accuracy-weight convergence proxy depends on factors not fully explored in the paper.

- **Low confidence:** Impact of unknown implementation details (exact reward formulations, layer selection, data partitioning) on reproducibility and generalizability to other model sizes or task domains.

## Next Checks

1. **Gradient alignment sensitivity analysis:** Systematically vary the intermediate layer depth used for hypergradient computation and measure the impact on adaptive weight stability and final accuracy gains.

2. **Accuracy-weight correlation validation:** Conduct controlled experiments where clients have known convergence states and verify that w_0^(m) accurately predicts optimization progress.

3. **Task clustering robustness test:** Run experiments with deliberately mis-clustered tasks and measure the degradation in both global and per-client performance to quantify the importance of correct task grouping.