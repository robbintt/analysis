---
ver: rpa2
title: Hierarchical LoRA MoE for Efficient CTR Model Scaling
arxiv_id: '2510.10432'
source_url: https://arxiv.org/abs/2510.10432
tags:
- expert
- experts
- scaling
- arxiv
- conference
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of scaling click-through rate
  (CTR) prediction models by combining the complementary strengths of vertical scaling
  through layer stacking and horizontal scaling through Mixture of Experts (MoE).
  The authors propose HiLoMoE, a hierarchical LoRA MoE framework that employs lightweight
  rank-1 experts for efficient horizontal scaling and stacks multiple MoE layers with
  hierarchical routing for combinatorially diverse expert compositions.
---

# Hierarchical LoRA MoE for Efficient CTR Model Scaling

## Quick Facts
- **arXiv ID:** 2510.10432
- **Source URL:** https://arxiv.org/abs/2510.10432
- **Reference count:** 40
- **Primary result:** HiLoMoE achieves 0.20% average AUC improvement and 18.5% FLOP reduction vs non-MoE baseline on four CTR datasets

## Executive Summary
This paper addresses the challenge of scaling click-through rate (CTR) prediction models by combining the complementary strengths of vertical scaling through layer stacking and horizontal scaling through Mixture of Experts (MoE). The authors propose HiLoMoE, a hierarchical LoRA MoE framework that employs lightweight rank-1 experts for efficient horizontal scaling and stacks multiple MoE layers with hierarchical routing for combinatorially diverse expert compositions. The routing mechanism allows parallel execution across layers, and a three-stage training framework with auxiliary losses ensures stable optimization and expert diversity. Experiments on four public datasets show that HiLoMoE achieves an average AUC improvement of 0.20% and an 18.5% reduction in FLOPs compared to the non-MoE baseline, demonstrating superior performance-efficiency tradeoffs.

## Method Summary
HiLoMoE introduces a hierarchical architecture that injects $L$ layers of $K$ rank-1 LoRA experts into Transformer backbones for CTR prediction. The framework employs hierarchical routing where each layer computes scores via softmax over query-key inner products, with query updates accumulating routing decisions across layers. A three-stage training procedure begins with backbone warmup, followed by sequential expert layer unfreezing with zero-initialized expert weights, and concludes with full fine-tuning. Auxiliary load-balancing and z-losses stabilize training by ensuring expert diversity and preventing routing collapse. The design enables parallel expert execution across layers while maintaining parameter efficiency through rank-1 adaptations.

## Key Results
- Achieves 0.20% average AUC improvement across four public CTR datasets
- Reduces FLOPs by 18.5% compared to non-MoE baseline models
- Maintains competitive parameter counts while enabling scalable model depth
- Demonstrates effectiveness across multiple backbone architectures (DIN, DIEN, BST, TransAct, InterFormer)

## Why This Works (Mechanism)
HiLoMoE addresses the fundamental tension between model capacity and efficiency in CTR prediction by leveraging hierarchical routing to distribute computation across parallel expert layers while maintaining low parameter overhead through rank-1 LoRA adaptations. The three-stage training procedure incrementally builds model capacity, starting from a stable backbone before introducing expert layers sequentially, which prevents optimization instability common in large MoE models. The auxiliary losses explicitly encourage expert diversity and prevent routing collapse, ensuring that the horizontal scaling through experts complements rather than duplicates the vertical scaling from layer stacking.

## Foundational Learning

**CTR Prediction**: Predicting the probability that a user clicks on a recommended item based on historical interaction data. *Why needed:* Core task that HiLoMoE optimizes for. *Quick check:* Model outputs probabilities in [0,1] and uses cross-entropy loss.

**Mixture of Experts (MoE)**: Architecture that routes inputs to specialized expert networks, activating only a subset per example. *Why needed:* Enables horizontal scaling by specializing computation. *Quick check:* Each input activates only top-1 expert per layer.

**LoRA (Low-Rank Adaptation)**: Technique that approximates weight updates using low-rank matrices to reduce parameters. *Why needed:* Keeps expert parameters minimal for efficiency. *Quick check:* Expert weights computed as outer product of two vectors.

**Hierarchical Routing**: Multi-layer routing where query representations accumulate across layers. *Why needed:* Enables complex routing decisions while maintaining parallel execution. *Quick check:* Query update formula shows accumulation of routing decisions.

**Three-Stage Training**: Progressive training schedule with backbone warmup, expert warmup, and full fine-tuning. *Why needed:* Stabilizes training of deep MoE models. *Quick check:* Training proceeds through three distinct phases with increasing model capacity.

## Architecture Onboarding

**Component Map**: Input -> Backbone Transformer -> $L$ MoE Layers -> Output. Each MoE layer: Query Generator -> $K$ Rank-1 Experts + Router -> Output Aggregation.

**Critical Path**: Query computation → Expert selection (top-1) → Expert execution → Output aggregation. This path determines routing decisions and computational flow.

**Design Tradeoffs**: Rank-1 experts minimize parameters but may limit individual expert capacity; hierarchical routing enables parallel execution but requires careful query accumulation to prevent routing uncertainty.

**Failure Signatures**: Expert collapse (only one expert used), routing instability (fluctuating expert selection), or optimization divergence during expert warmup phase.

**Three First Experiments**:
1. Verify expert utilization by monitoring routing score entropy across layers
2. Test baseline vs HiLoMoE on single dataset to confirm AUC improvement
3. Measure FLOPs reduction by comparing computational graphs with/without experts

## Open Questions the Paper Calls Out

**Open Question 1**: How can the hierarchical routing strategy be refined to mitigate routing uncertainty and redundancy, thereby improving the efficacy of vertical scaling? *Basis:* Section 5.3.2 notes that vertical scaling yields limited gains because stacking more layers increases dependency depth and routing uncertainty, leading to redundant transformations. *Why unresolved:* The current hierarchical design, while parallelizable, struggles with optimization stability and expressiveness as depth increases, limiting returns from vertical model growth. *What evidence would resolve it:* Improved AUC/logloss metrics when scaling depth ($L > 3$), or analysis showing reduced redundancy in expert paths across layers.

**Open Question 2**: Does the strict rank-1 constraint on LoRA experts limit their ability to model complex feature interactions compared to higher-rank adaptations? *Basis:* Section 4.2 adopts rank-1 experts specifically for parameter efficiency, but the paper does not ablate the impact of higher ranks on expert capacity or feature capture. *Why unresolved:* While efficient, rank-1 matrices may constitute a bottleneck that forces the router to combine many experts to approximate simple concepts, potentially complicating optimization. *What evidence would resolve it:* Ablation studies comparing model performance and expert interpretability across varying LoRA ranks (e.g., rank 1, 4, 8).

**Open Question 3**: To what extent do the learned hierarchical experts align with the semantic hierarchies inherent in recommendation data (e.g., Category → Product)? *Basis:* Figure 1 motivates the design with semantic hierarchy, but experiments focus on AUC/efficiency tradeoffs without verifying if experts capture intended semantic levels. *Why unresolved:* The routing mechanism is learned end-to-end via task loss, so it's unclear if experts capture coarse-to-fine semantic structure or simply statistical correlations. *What evidence would resolve it:* Qualitative analysis or probing tasks correlating specific expert activations with ground-truth hierarchical metadata (categories, brands).

## Limitations

- Auxiliary loss weights (load-balancing and z-loss coefficients) are unspecified, making exact reproduction difficult
- Learning rate schedule details beyond optimizer type are omitted, potentially affecting convergence
- Implementation assumption about stop-gradient placement in routing query updates lacks clarification
- Performance gains are modest (0.20% AUC) and may not justify complexity in all scenarios

## Confidence

**High Confidence**: The overall architectural framework (rank-1 LoRA experts, hierarchical routing, 3-stage training) is clearly specified and reproducible.

**Medium Confidence**: The claimed performance improvements (0.20% AUC, 18.5% FLOP reduction) are plausible given the design, but exact reproduction depends on resolving auxiliary loss weights and LR schedule.

**Low Confidence**: The exact mechanism of routing gradient flow (stop-gradient placement) remains ambiguous and could critically affect expert utilization.

## Next Checks

1. **Auxiliary Loss Sensitivity Analysis**: Systematically vary the load-balancing loss weight α_lb (e.g., {0.01, 0.1, 1.0}) and measure expert utilization entropy and AUC. This identifies the weight achieving both diversity and accuracy.

2. **Routing Gradient Verification**: Implement both with and without stop gradients on the routing path. Compare expert activation entropy and training stability across datasets. The configuration yielding sustained multi-expert utilization is the likely correct implementation.

3. **LR Schedule Impact Test**: Replace the unspecified scheduler with cosine decay with linear warmup. Train HiLoMoE across all four datasets and measure AUC/FLOPs against reported gains. Significant deviation would indicate the original scheduler was a critical unreported detail.