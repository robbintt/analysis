---
ver: rpa2
title: Scaling Reasoning without Attention
arxiv_id: '2505.22425'
source_url: https://arxiv.org/abs/2505.22425
tags:
- reasoning
- arxiv
- aime
- prompt
- preprint
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces PROMPT COT-MAMBA, the first attention-free
  language model that matches and exceeds the performance of strong Transformer and
  hybrid baselines on challenging math and code tasks. Built on Mamba-2's structured
  state space architecture, it delivers constant-time, fixed-memory inference, addressing
  key inefficiencies of attention-based models.
---

# Scaling Reasoning without Attention

## Quick Facts
- arXiv ID: 2505.22425
- Source URL: https://arxiv.org/abs/2505.22425
- Authors: Xueliang Zhao; Wei Wu; Lingpeng Kong
- Reference count: 7
- Key outcome: First attention-free model (PROMPT COT-MAMBA) matches and exceeds strong Transformer and hybrid baselines on math and code tasks, achieving state-of-the-art results on AIME 24 (35.2%), AIME 25 (24.6%), and Livecodebench-v5 (29.9%), with up to 3.66× higher throughput on 24GB memory.

## Executive Summary
PROMPT COT-MAMBA is the first attention-free language model to match and exceed strong Transformer and hybrid baselines on challenging math and code reasoning tasks. Built on Mamba-2's structured state space architecture, it delivers constant-time, fixed-memory inference, addressing key inefficiencies of attention-based models. The model employs a two-phase curriculum fine-tuning pipeline based on the PromptCoT paradigm, enabling it to learn complex reasoning skills from challenging data. PROMPT COT-MAMBA-7B outperforms comparably sized models and even surpasses larger Transformers like Gemma3-27B, achieving new state-of-the-art results on AIME 24 (35.2%), AIME 25 (24.6%), and Livecodebench-v5 (29.9%). Additionally, it offers up to 3.66× higher throughput on 24GB memory and 1.69× on 72GB memory compared to strong Transformer baselines, confirming its suitability for efficient long-context inference.

## Method Summary
PROMPT COT-MAMBA uses Mamba-2's structured state space architecture with SSD recurrence replacing attention layers. The model employs a two-phase curriculum fine-tuning pipeline: Phase 1 trains on 1.88M examples from OpenCodeReasoning and OpenThoughts2 for foundational reasoning, while Phase 2 adds 256k PromptCoT-synthesized examples with explicit pedagogical structure. The model is initialized from Mamba Codestral weights and fine-tuned with AdamW optimizer (lr=5e-6, β1=0.9, β2=0.95, weight_decay=0.01), 100-step warmup, batch_size=64, max_len=16,384 tokens in Phase 1 and 20,480 tokens in Phase 2. Evaluation uses generation lengths up to 65,536 tokens across seven benchmarks including MATH-500, AIME 24/25, OlympiadBench, HumanEval, HumanEval+, and LiveCodeBench-v5.

## Key Results
- PROMPT COT-MAMBA-7B achieves state-of-the-art results: AIME 24 (35.2%), AIME 25 (24.6%), Livecodebench-v5 (29.9%)
- Outperforms larger Transformers: surpasses Gemma3-27B on AIME benchmarks despite being 7B vs 27B parameters
- Delivers 1.69× to 3.66× higher throughput than Transformer baselines, especially at longer generation lengths
- Ablation shows severe performance drops without PromptCoT (-23.5% AIME 24) and without OpenThoughts data (-23.8% AIME 24)

## Why This Works (Mechanism)

### Mechanism 1: State Space Dual (SSD) Recurrence Replaces Attention
The SSD layer achieves constant-time, fixed-memory inference by replacing attention's pairwise token comparisons with a recurrent hidden state update. At each position t, the model computes decay factor a_t ∈ [0,1], input/output kernels b_t, c_t, and projected feature u_t via a shared linear projection. The hidden state H_t updates as H_t = a_t · H_{t-1} + u_t ⊗ b_t (low-rank outer product), and output y_t = c_t^T H_t. This replaces the O(T²) attention matrix with O(TNP) structured contractions during training and O(NP) per step at inference.

### Mechanism 2: Two-Phase Curriculum Fine-Tuning Scaffolds Reasoning
Performance gains depend critically on the staged curriculum: foundational reasoning first, then PromptCoT-synthesized advanced problems. Phase 1 uses 1.88M examples from OpenCodeReasoning and OpenThoughts2 (synthesized by DeepSeek-R1, QwQ) for foundational skills. Phase 2 adds 256k PromptCoT examples with explicit pedagogical structure: concept selection → rationale generation → problem synthesis. This progresses from simpler to higher-complexity objectives.

### Mechanism 3: Domain Specialization Trades Breadth for Depth
Narrowing training distribution to math-only data improves mathematical