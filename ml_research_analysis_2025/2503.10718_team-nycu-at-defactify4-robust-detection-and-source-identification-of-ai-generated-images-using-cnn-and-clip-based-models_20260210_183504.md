---
ver: rpa2
title: 'Team NYCU at Defactify4: Robust Detection and Source Identification of AI-Generated
  Images Using CNN and CLIP-Based Models'
arxiv_id: '2503.10718'
source_url: https://arxiv.org/abs/2503.10718
tags:
- image
- images
- clip-vit
- source
- ai-generated
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of detecting AI-generated images
  and identifying their source models. The authors propose using CNN and CLIP-ViT
  classifiers, with the CNN leveraging EfficientNet-B0 and additional features like
  frequency information and reconstruction errors, while the CLIP-ViT uses a pretrained
  CLIP image encoder with SVM for classification.
---

# Team NYCU at Defactify4: Robust Detection and Source Identification of AI-Generated Images Using CNN and CLIP-Based Models

## Quick Facts
- arXiv ID: 2503.10718
- Source URL: https://arxiv.org/abs/2503.10718
- Reference count: 18
- Detects and identifies source models of AI-generated images using CNN and CLIP-based approaches

## Executive Summary
This paper addresses the challenge of detecting AI-generated images and identifying their source models. The authors propose using CNN and CLIP-ViT classifiers, with the CNN leveraging EfficientNet-B0 and additional features like frequency information and reconstruction errors, while the CLIP-ViT uses a pretrained CLIP image encoder with SVM for classification. Evaluated on the Defactify 4 dataset, the methods demonstrate strong performance, with CLIP-ViT showing superior robustness to image perturbations. The approach achieves competitive results compared to baselines like AEROBLADE and OCC-CLIP, ranking Top-3 overall in the competition. Data augmentation significantly improves model generalization, underscoring the importance of robust model design for practical applications.

## Method Summary
The method employs two complementary approaches for AI-generated image detection and source identification. The first uses EfficientNet-B0 with a 5-channel input combining RGB, frequency domain features (via FFT), and VAE reconstruction errors from Stable Diffusion. The second approach leverages CLIP-ViT for feature extraction followed by SVM classification with RBF kernel. Both methods utilize extensive data augmentation including JPEG compression, Gaussian blur, noise, and brightness reduction to improve robustness. The system is evaluated on the Defactify 4 dataset containing images from six source models (real, SD2.1, SDxl, SD3, DALL·E, MidJourney) with balanced training, validation, and test sets.

## Key Results
- CLIP-ViT demonstrates superior robustness to image perturbations compared to CNN-based methods
- Multi-channel feature fusion (RGB + frequency + reconstruction error) provides complementary signals for detection
- Data augmentation significantly improves model generalization to real-world image degradations
- Achieves competitive performance ranking Top-3 overall in the Defactify4 competition
- CLIP-ViT maintains high accuracy across noise, JPEG compression, brightness reduction, and blur while EfficientNet degrades

## Why This Works (Mechanism)

### Mechanism 1
Multi-channel feature fusion (RGB + frequency + reconstruction error) provides complementary signals for distinguishing AI-generated from real images. AI-generated images exhibit different frequency domain patterns and higher reconstruction fidelity when processed by Stable Diffusion's VAE. Concatenating these as additional input channels allows EfficientNet-B0 to learn joint representations.

### Mechanism 2
CLIP's pre-trained image encoder captures semantic and stylistic fingerprints that generalize across generative models without task-specific training. CLIP-ViT maps images to a high-dimensional space aligned with text during pre-training. An SVM with RBF kernel then separates classes based on these frozen features.

### Mechanism 3
Perturbation-based data augmentation during training improves generalization to real-world image degradations. Exposing the model to JPEG compression, Gaussian blur, noise, and brightness reduction during training creates invariance to these transformations at inference time.

## Foundational Learning

- **Vision Transformer (ViT) patch embeddings**: Why needed - CLIP-ViT divides images into patches and processes them through self-attention. Quick check - Can you explain why a ViT might handle spatially localized noise better than a CNN with fixed receptive fields?

- **Fourier Transform for image frequency analysis**: Why needed - The frequency feature is computed via 2D FFT on grayscale images. Quick check - Given an image, how would you interpret high vs. low frequency components in the log-magnitude spectrum?

- **Support Vector Machine with RBF kernel**: Why needed - CLIP features are classified via SVM rather than a neural head. Quick check - What does the hyperparameter γ control in an RBF kernel, and how does it affect the decision boundary?

## Architecture Onboarding

- **Component map**:
```
Input Image (512×512×3)
    ├── EfficientNet Path → RGB + FFT(Grayscale) + |Reconstruction - Original|
    │                        └── Concat → (512×512×5) → EfficientNet-B0 → Softmax
    │
    └── CLIP-ViT Path → CLIP Encoder (frozen) → 512-dim embedding → SVM(RBF) → Prediction
```

- **Critical path**: For production deployment, CLIP-ViT is preferred due to robustness. The critical components are: (1) image preprocessing to match CLIP's expected input, (2) feature extraction via the frozen encoder, (3) SVM inference with pre-computed support vectors.

- **Design tradeoffs**:
  - EfficientNet-B0: Higher accuracy on clean data (99.51% vs 93.77%) but brittle under perturbation
  - CLIP-ViT: Lower peak accuracy but stable across degradations; no training required for feature extractor
  - Reconstruction error computation requires running a VAE forward pass—adds inference latency

- **Failure signatures**:
  - EfficientNet accuracy collapse under noise >0.1 std
  - CLIP-ViT struggles with Task B (source identification) relative to EfficientNet on clean data
  - AEROBLADE baseline shows overlapping LPIPS2 distributions, making threshold-based binary classification unreliable

- **First 3 experiments**:
  1. Baseline reproduction: Run both architectures on validation set with provided hyperparameters. Verify Task A and Task B metrics match Table 2 within ±1%.
  2. Perturbation sweep: Apply each degradation type at multiple intensities to validation images. Plot accuracy curves to confirm CLIP-ViT robustness.
  3. Ablation on feature channels: Train EfficientNet with RGB-only, RGB+frequency, and RGB+reconstruction to quantify each feature's contribution.

## Open Questions the Paper Calls Out

### Open Question 1
How can feature interpretability be enhanced to better explain source model attribution decisions? The conclusion states future work will focus on enhancing feature interpretability for source model attribution. The current study focuses on optimizing classification metrics without deeply analyzing specific visual artifacts or latent features that distinguish one model from another.

### Open Question 2
Can the high standard accuracy of EfficientNet be combined with the superior robustness of CLIP-ViT? Table 2 shows EfficientNet achieves higher validation accuracy, while Figure 3 demonstrates CLIP-ViT significantly outperforms EfficientNet under perturbations. The paper evaluates these architectures separately, leaving the potential performance of a fused or ensemble approach unexplored.

### Open Question 3
Does the reconstruction error feature generalize to non-diffusion-based generative models? The EfficientNet method uses a Stable Diffusion VAE to calculate reconstruction error, which assumes the input may share latent characteristics with diffusion models. It is unclear if this feature remains discriminative when detecting images from fundamentally different architectures like GANs or autoregressive models.

## Limitations
- Reliance on frozen CLIP features for source identification presents a fundamental tradeoff between robustness and fine-grained model-specific signals
- Multi-channel CNN fusion depends on artifacts (frequency patterns, VAE reconstruction errors) that may diminish as generative models optimize against them
- Reconstruction error computation adds inference latency through VAE forward passes
- Exact data augmentation sampling strategy remains unspecified

## Confidence
- **High confidence**: CLIP-ViT's superior robustness to perturbations (supported by Figure 3 and consistent with "DINO-Detect" and "RAID" findings)
- **Medium confidence**: Multi-channel feature fusion effectiveness (neighbor papers focus on ViT-based detection, not CNN feature fusion; limited direct validation)
- **Medium confidence**: Data augmentation benefits (strong empirical support in Figure 4, but real-world perturbation types may differ)

## Next Checks
1. **Cross-model transferability**: Evaluate both methods on images from generative models not in the training set (e.g., Google's Imagen, Midjourney v6) to test generalization beyond Defactify 4.

2. **Artifact persistence analysis**: Generate images with state-of-the-art models (SD3.5, DALL·E 3) and measure whether frequency and reconstruction error signals remain discriminative compared to the training set.

3. **Latency-accuracy tradeoff**: Profile end-to-end inference time for both pipelines (including VAE forward pass for EfficientNet) and assess whether CLIP-ViT's speed advantage compensates for its lower Task B accuracy in production scenarios.