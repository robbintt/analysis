---
ver: rpa2
title: 'DafnyPro: LLM-Assisted Automated Verification for Dafny Programs'
arxiv_id: '2601.05385'
source_url: https://arxiv.org/abs/2601.05385
tags:
- dafnypro
- dafny
- verification
- length
- invariants
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: DafnyPro is an inference-time framework that enhances LLMs for
  generating verification annotations in Dafny by preventing code modifications, pruning
  unnecessary invariants, and incorporating problem-independent proof strategies.
  It achieves state-of-the-art performance with 86% correct proofs on DafnyBench using
  Claude 3.5 Sonnet, a 16 percentage point improvement over prior work.
---

# DafnyPro: LLM-Assisted Automated Verification for Dafny Programs

## Quick Facts
- arXiv ID: 2601.05385
- Source URL: https://arxiv.org/abs/2601.05385
- Reference count: 39
- Primary result: DafnyPro achieves 86% correct proofs on DafnyBench using Claude 3.5 Sonnet, a 16 percentage point improvement over prior work.

## Executive Summary
DafnyPro is an inference-time framework that enhances large language models (LLMs) for generating verification annotations in Dafny programs. The framework addresses three key challenges: preventing code modifications that undermine verification reliability, removing unnecessary invariants that waste iteration budget, and incorporating reusable proof strategies through problem-independent hints. By combining these three mechanisms, DafnyPro achieves state-of-the-art performance on DafnyBench with 86% correct proofs using Claude 3.5 Sonnet, demonstrating significant improvements over previous LLM-based approaches.

## Method Summary
DafnyPro employs an iterative generation pipeline with three core components: (1) a diff-checker that uses Dafny's parser to verify the regenerated code, stripped of annotations, is byte-identical to the original base code, preventing "cheating" modifications; (2) a greedy pruner that removes non-inductive invariant clauses identified by the Dafny verifier to reduce wasted feedback iterations; and (3) a hint-augmentation system with eight manually crafted, problem-independent proof strategies that capture structural properties of data types. The framework operates within a maximum of 10 attempts, providing failed verification attempts and relevant hints to the LLM for each subsequent generation. Fine-tuning experiments use LoRA on 31k examples from verification attempts, achieving 68% and 70% correct proofs with 7B and 14B Qwen models respectively.

## Key Results
- DafnyPro achieves 86% correct proofs on DafnyBench using Claude 3.5 Sonnet, a 16 percentage point improvement over prior work.
- The three components provide complementary gains: diff-checking (+11.78% to +20.85%), pruning (+3.06% to +5.66%), and hints (+9.97% to +33.64%).
- Fine-tuned Qwen2.5-7B and Qwen3-14B models achieve 68% and 70% correct proofs respectively, demonstrating efficient deployment while maintaining high verification accuracy.

## Why This Works (Mechanism)

### Mechanism 1: Diff-Checking for Soundness Enforcement
- Claim: Preventing LLMs from modifying base code logic improves verification reliability by eliminating "reward hacking" behaviors.
- Mechanism: The diff-checker uses Dafny's parser to extract annotations from LLM-generated programs and verifies that the regenerated code, stripped of annotations, is byte-identical to the original base code. This principled approach catches subtle modifications that ad hoc checkers miss.
- Core assumption: LLMs, when regenerating entire programs with annotations, will sometimes alter logic to pass verification rather than producing correct annotations.
- Evidence anchors: The abstract mentions "a diff-checker that prevents modifications to base program logic," and section 3.1 states that such modifications are not reliably detected by ad hoc checkers used in prior work.

### Mechanism 2: Greedy Invariant Pruning
- Claim: Removing non-inductive, unnecessary invariant clauses reduces wasted feedback iterations and improves verification success rates.
- Mechanism: For each generated annotated program, the pruner identifies non-inductive invariant clauses using the Dafny verifier and removes them. It then re-attempts verification with remaining clauses.
- Core assumption: LLMs frequently generate invariant clauses that are not required for proving the post-condition and cannot be proven inductively, causing verification to fail on irrelevant clauses.
- Evidence anchors: Table 1b shows pruning contributes +3.06% to +5.66% improvement depending on model/benchmark, and section 3.2 explains how unnecessary invariants waste attempts fixing such clauses.

### Mechanism 3: Problem-Independent Hint Augmentation
- Claim: A small set of reusable proof strategies (hints) can be retrieved and applied across problems to resolve verifier impasses.
- Mechanism: Eight manually crafted hints capture structural properties of data types (e.g., array slicing equivalence). During inference, failed attempts and hints are provided to the LLM, which retrieves relevant portions for subsequent generation.
- Core assumption: Many verification failures stem not from incorrect invariants but from missing explicit assertions that help the verifier connect loop-final states to postconditions.
- Evidence anchors: Table 1b shows hint augmentation contributes +9.97% to +33.64% improvement—the largest contributor across components, and section 3.3 describes these hints as independent of base program logic while capturing structural properties.

## Foundational Learning

- Concept: **Loop Invariants**
  - Why needed here: The entire framework targets generating invariants that (1) hold before loop entry, (2) are preserved by each iteration, and (3) combine with loop exit conditions to prove postconditions. Understanding why invariants must be inductive is essential for interpreting pruning logic.
  - Quick check question: Given a loop `while i < n`, does the invariant `i <= n` hold at entry when `i = 0, n = 5`? Does it survive one iteration if the body executes `i := i + 1`?

- Concept: **Dafny Verification Architecture**
  - Why needed here: DafnyPro relies on Dafny's verifier (based on Boogie/Z3) to check correctness. The framework's iterative feedback loop depends on understanding what verifier error messages indicate (non-inductive invariants, assertion failures, postcondition violations).
  - Quick check question: If Dafny reports "this invariant could not be proved," does it mean the invariant is false or that the verifier couldn't establish it from available facts?

- Concept: **LLM In-Context Learning with Feedback**
  - Why needed here: DafnyPro's iterative approach feeds failed verification attempts back to the LLM. Understanding how models use context windows to incorporate verifier feedback is critical for debugging generation failures.
  - Quick check question: Why might providing the full error message be less effective than "informalizing" it into natural language guidance?

## Architecture Onboarding

- Component map:
Input: Base Dafny program (no annotations, with specs)
         ↓
    [LLM Generation] ←─── [Hint Retrieval] ←── [Tactic Library (8 hints)]
         ↓
    [Diff-Checker] ──→ fail → retry
         ↓ pass
    [Dafny Verifier] ──→ success → Output
         ↓ fail (non-inductive)
    [Pruner] ──→ [Verifier retry]
         ↓ fail
    [Store attempt + retrieve hints]
         ↓
    Next iteration (max N=10)

- Critical path: The diff-checker → verifier → pruner sequence must complete within the attempt budget. Hint augmentation provides the largest gains (Table 1b), so ensuring hint relevance during retrieval is performance-critical.

- Design tradeoffs:
  - **Soundness vs. coverage**: Strict diff-checking guarantees no base-code modification but rejects outputs that could otherwise verify correctly with minor formatting changes.
  - **Pruning aggressiveness**: Greedy removal may discard clauses that are non-inductive due to missing auxiliary assertions rather than being truly unnecessary.
  - **Hint manual vs. automated**: Eight manual hints work well (large gains), but scalability to new domains requires automated hint extraction (Appendix A.5 shows LLMs can do this).

- Failure signatures:
  - **All 10 attempts fail diff-check**: LLM consistently hallucinates base-code changes; check prompt formatting or model capability.
  - **Verifier timeouts**: Complex invariants may trigger SMT solver explosions; consider simplifying or adding intermediate assertions.
  - **Pruning empties all invariants**: LLM generates only non-inductive clauses; may indicate prompt misunderstanding or insufficient model capability.

- First 3 experiments:
  1. **Baseline replication**: Run DafnyPro with diff-checker only (no pruning, no hints) on 50 DafnyBench samples to establish baseline; verify ~70% first-attempt diff-check pass rate per paper implications.
  2. **Ablation by component**: Add pruning alone, then hints alone, then both; measure per-component contribution on same 50 samples to validate Table 1b proportions.
  3. **Hint boundary testing**: Identify 10 DafnyBench problems where hints provide largest gains (likely array-slicing patterns) vs. 10 where hints don't help; analyze what distinguishes them to understand retrieval effectiveness.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can DafnyPro's inference-time framework generalize effectively to foundation models beyond Claude 3.5/3.7 Sonnet (e.g., GPT-4, Gemini, open-source models)?
- Basis in paper: The authors state: "our comparison among foundation models includes only Claude 3.5/3.7 Sonnet. Evaluating a broader range of recent models would provide a more comprehensive assessment of DafnyPro's generalizability."
- Why unresolved: Only Claude models were evaluated; component contributions may interact differently with models having distinct reasoning patterns.
- What evidence would resolve it: Systematic evaluation of DafnyPro on at least 3-5 additional foundation models across the same four benchmarks.

### Open Question 2
- Question: Can reinforcement learning-based fine-tuning (e.g., as in PREFACE) improve upon the supervised fine-tuning approach for smaller models using DafnyPro?
- Basis in paper: The authors state: "incorporating reinforcement learning-based fine-tuning techniques, as explored in prior work [3], could further improve the model's ability to utilize verifier feedback and strengthen iterative reasoning."
- Why unresolved: Fine-tuning experiments used only supervised learning with LoRA; RL-based approaches that directly optimize for verifier rewards remain unexplored.
- What evidence would resolve it: Comparative study of RL-fine-tuned vs. SFT-fine-tuned models on DafnyBench with DafnyPro.

### Open Question 3
- Question: What structural or semantic characteristics distinguish the ~14% of DafnyBench programs that remain unverifiable even with DafnyPro?
- Basis in paper: DafnyPro achieves 86% on DafnyBench, leaving 14% unsolved. The paper provides no analysis of failure modes or whether these require fundamentally different proof strategies beyond the 8 hints used.
- Why unresolved: No failure analysis is provided; it is unclear whether remaining failures stem from insufficient hints, model reasoning limits, or problem complexity.
- What evidence would resolve it: Categorization of failed problems by invariant complexity, proof structure, or hint applicability; ablation testing with expanded hint sets.

### Open Question 4
- Question: Can DafnyPro's diff-checking, pruning, and hint-augmentation techniques transfer effectively to other verification languages (e.g., F*, Coq, Lean)?
- Basis in paper: The framework is specific to Dafny's syntax and verifier; problem-independent hints are Dafny-specific. No discussion of cross-language applicability is provided.
- Why unresolved: Different verification systems have distinct annotation syntaxes, proof strategies, and verifier behaviors that may require different pruning logic or hint formulations.
- What evidence would resolve it: Porting DafnyPro components to at least one other verification system and measuring performance on equivalent benchmarks.

## Limitations

- **Critical component details missing**: The paper lacks detailed specification for prompt templates for LLM annotation generation and the hint retrieval mechanism ("RetrieveTactics"), preventing faithful reproduction despite providing source code.
- **Model dependency concerns**: The 86% success rate relies heavily on Claude 3.5 Sonnet, raising questions about whether smaller models can achieve comparable results without extensive fine-tuning.
- **Manual hint effort scalability**: The eight manually crafted proof hints work well but may not scale efficiently to new domains, though the paper suggests LLMs could automate this process.

## Confidence

- **High Confidence**: The diff-checking mechanism is well-specified and demonstrably effective at preventing base-code modifications, with clear parser-based implementation. The pruning strategy using Dafny's verifier to identify non-inductive clauses is technically sound and well-supported by ablation results.
- **Medium Confidence**: The overall performance claims (86% correct proofs) are credible given the ablation studies showing significant contributions from all three components. However, exact prompt engineering and hint retrieval details are missing.
- **Low Confidence**: The fine-tuning results for 7B and 14B models (68% and 70% respectively) are promising but insufficiently detailed, with unclear training data composition and hyperparameter specifics beyond LoRA configuration.

## Next Checks

1. **Component Ablation Replication**: Implement DafnyPro with only diff-checking (no pruning, no hints) and measure first-attempt diff-check pass rate on 50 DafnyBench samples to verify the paper's implied ~70% baseline.

2. **Hint Boundary Analysis**: Systematically identify 10 DafnyBench problems where hints provide maximum gains (likely array-slicing patterns) versus 10 where hints don't help. Analyze distinguishing features to understand retrieval effectiveness and hint relevance criteria.

3. **Small Model Capability Test**: Using the provided fine-tuning setup, train a 7B model on the 31k examples with LoRA (r=512, alpha=512, dropout=0.05, q_proj/v_proj targets) and evaluate on the held-out 197-program test split. Compare results to the paper's 68% claim under identical conditions.