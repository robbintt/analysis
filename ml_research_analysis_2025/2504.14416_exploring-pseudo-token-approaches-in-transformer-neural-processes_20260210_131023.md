---
ver: rpa2
title: Exploring Pseudo-Token Approaches in Transformer Neural Processes
arxiv_id: '2504.14416'
source_url: https://arxiv.org/abs/2504.14416
tags:
- isanp
- context
- isanp-2
- lbanp
- latent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces Induced Set Attentive Neural Processes (ISANPs),
  a new pseudo-token-based approach to Transformer Neural Processes that addresses
  computational complexity issues while maintaining competitive performance. ISANPs
  use latent vectors to encode context information and employ cross-attention mechanisms
  for efficient querying.
---

# Exploring Pseudo-Token Approaches in Transformer Neural Processes

## Quick Facts
- arXiv ID: 2504.14416
- Source URL: https://arxiv.org/abs/2504.14416
- Authors: Jose Lara-Rangel; Nanze Chen; Fengzhe Zhang
- Reference count: 40
- Primary result: ISANPs achieve competitive performance with traditional TNPs while scaling better to larger datasets through efficient latent-mediated querying

## Executive Summary
This paper introduces Induced Set Attentive Neural Processes (ISANPs), a novel pseudo-token-based approach to Transformer Neural Processes that addresses computational complexity issues while maintaining competitive performance. ISANPs use latent vectors to encode context information and employ cross-attention mechanisms for efficient querying, offering tunable trade-offs between performance and efficiency through the number of latent vectors. The method introduces two variants: ISANP with O(M L) query complexity and ISANP-2 with O(N M) query complexity. Experimental results across 1D regression, image completion, contextual bandits, and Bayesian optimization show that ISANPs outperform existing pseudo-token approaches like LBANPs and achieve competitive results with traditional TNPs while scaling better to larger datasets.

## Method Summary
ISANPs encode context information using learnable latent vectors that function as sufficient statistics for the context set. The conditioning phase employs bidirectional cross-attention between context embeddings and latent vectors, creating a refined shared representation at O(2NL) cost. The query phase offers two variants: ISANP attends to latent vectors (O(ML) complexity), while ISANP-2 attends to context embeddings (O(NM) complexity). Both variants avoid recomputing context-target joint attention, unlike TNP's O((N+M)²) requirement. The number of latent vectors L controls the bottleneck capacity, with the paper demonstrating that 8 latent vectors can match or exceed LBANP performance with 128 latent vectors in most tasks.

## Key Results
- ISANPs with 8 latent vectors matched or exceeded LBANP performance with 128 latent vectors in most tasks
- ISANP achieves O(ML) query complexity compared to TNP's O((N+M)²), enabling better scalability
- Bidirectional attention in conditioning phase extracts more information per latent than LBANP's architecture
- Performance degrades with more latent vectors on CelebA32 (overfitting) and shows non-monotonic improvement patterns

## Why This Works (Mechanism)

### Mechanism 1: Bidirectional Context-Latent Information Flow
The conditioning phase efficiency comes from iterative bidirectional cross-attention between context embeddings and latent vectors, not unidirectional compression. The model alternates: `LEMBi = CA(LEMBi-1, CEMBi-1)` then `CEMBi = CA(CEMBi-1, LEMBi)`. Latents query context to extract relevant information, then context queries latents to enrich its own representations with compressed global patterns. This creates a refined shared representation at O(2NL) cost. The core assumption is that latent vectors can function as sufficient statistics for context when bidirectional flow is allowed.

### Mechanism 2: Query Complexity Reduction via Latent-Mediated Retrieval
Decoupling query complexity from context size (N) enables scalability to large datasets where TNP fails. ISANP query phase attends only to latent vectors (O(ML)), while ISANP-2 attends to context embeddings (O(NM)). Both avoid recomputing context-target joint attention, unlike TNP's O((N+M)²) requirement. Pre-computed context embeddings are reused across multiple queries. The core assumption is that for many tasks, latent-compressed context contains sufficient predictive information.

### Mechanism 3: Capacity-Efficiency Tuning via Latent Vector Cardinality
Performance scales with latent vector count L, but with diminishing returns and task-dependent optimal points. L controls bottleneck capacity. More latents = richer context representation but higher O(2NL) conditioning cost. ISANP(8) matches LBANP(128) on 1D regression, suggesting bidirectional attention extracts more information per latent than LBANP's architecture. The core assumption is that information density per latent can be improved through architectural choices (attention patterns), not just quantity.

## Foundational Learning

- **Concept: Neural Process meta-learning paradigm** - Why needed: ISANPs inherit the NP framework (encoder → aggregator → decoder) but modify aggregation via latent attention. Understanding the base pattern reveals what's being improved. Quick check: Given context set {(xᵢ, yᵢ)}ᴺ and target {xⱼ}ᴹ, can you trace how information flows through encoder, aggregator, and decoder in a standard CNP?

- **Concept: Cross-attention vs. self-attention complexity** - Why needed: The paper's efficiency claims rest on understanding that cross-attention with L latents costs O(NL) vs. self-attention's O(N²). Without this, the complexity table is opaque. Quick check: For N=1000 context points and L=8 latents, why is CA(embeddings, latents) cheaper than SA(embeddings)?

- **Concept: Set Transformer architecture (Lee et al., 2019)** - Why needed: ISANPs build directly on Induced Set Attention from Set Transformers. The multi-head attention over induced points is the core primitive being adapted. Quick check: What role do "induced points" play in reducing Set Transformer complexity, and how does this differ from standard attention?

## Architecture Onboarding

- **Component map:** Context {(xᵢ, yᵢ)} → Encoder → CEMB₀; Latents LEMB₀ (learnable, L×D_L); Conditioning (K layers): LEMBᵢ = CrossAttn(Q=LEMB, K,V=CEMB), CEMBᵢ = CrossAttn(Q=CEMB, K,V=LEMB); Query (ISANP): Target {xⱼ} → QEMB → CrossAttn(Q=QEMB, K,V=LEMB) → Predictor; Query (ISANP-2): Target {xⱼ} → QEMB → CrossAttn(Q=QEMB, K,V=CEMB) → Predictor

- **Critical path:** Latent initialization quality → conditioning convergence → query attention pattern determines if latents suffice (ISANP) or direct context access needed (ISANP-2)

- **Design tradeoffs:** ISANP: Faster queries O(ML), better for simple functions, may lose fine context detail; ISANP-2: Slower queries O(NM), better for structured data (images), scales linearly with context; Latent count L: More = higher capacity but diminishing returns and potential overfitting

- **Failure signatures:** High regret on rare-event tasks (δ→0.99 in bandits): latent compression discards critical boundary information; Non-monotonic improvement with more latents (Figure 9a): potential overfitting or optimization instability; Memory failures on 64×64 images with 128 latents: conditioning phase still O(2NL) which grows with context

- **First 3 experiments:** 1) Reproduce 1D regression (Table 2) with L=8 vs L=128: Verify ISANP(8) ≈ LBANP(128) claim on RBF kernel. Expected log-likelihood: ~1.26. If gap > 0.1, check bidirectional attention implementation; 2) Ablate conditioning depth K: Run with K=1, 2, 4 layers on CelebA32. Plot log-likelihood vs. K to find where bidirectional flow saturates. Paper uses default K but doesn't ablate this; 3) Profile memory at scale: Measure peak memory for N∈{500, 1000, 2000} context points on fixed target M=50. Confirm ISANP constant memory, ISANP-2 linear growth, TNP quadratic growth (Figure 9c replication)

## Open Questions the Paper Calls Out

### Open Question 1
Does ISANP performance consistently improve with additional latent vectors (beyond 128) on large-scale tasks, and how does it compare to TNP-D at equivalent computational budgets? The paper states "due to limitations in computational resources, these improvements could not be thoroughly verified" and "Testing with more latent vectors and complex tasks is essential for further comparison with other NPs." The ablation study (Figure 9a) shows an unexpected performance drop for ISANP at 128 latent vectors, and experiments on CelebA64/128 could not be completed.

### Open Question 2
How do ISANPs perform on multi-dimensional Bayesian optimization tasks compared to TNPs and GPs? The paper notes "We also planned to run the experiments in a multi-dimensional setting. However, due to the limitation of computing power, we will set it as future work." BO experiments were limited to 1D objective functions; scaling behavior in higher dimensions remains unknown.

### Open Question 3
Can a strategy-distance metric better capture NPs' adaptation ability in contextual bandits than cumulative regret? The authors propose: distance(optimal strategy – eventual strategy) – distance(optimal strategy – initial strategy). Current experiments show NPs update strategies only at the very beginning and fail to adapt to subsequent context, yet cumulative regret doesn't account for initial strategy biases.

### Open Question 4
Why does ISANP-2 not benefit from increased latent vectors, and can this overfitting be mitigated? Figure 9a shows ISANP-2 performance degrades with more latents. Authors hypothesize: "merely increasing the number of latent vectors does not enrich the context dataset with more useful information for predictions."

## Limitations

- The conditioning phase complexity of O(2NL) still scales linearly with context size N, which may become prohibitive for very large datasets despite improvements over TNP's quadratic scaling
- Performance claims based on single runs without variance estimates, making it difficult to assess statistical significance
- Dramatic degradation at δ=0.99 in contextual bandits suggests latent compression may fundamentally fail for tasks requiring precise discrimination in rare-event regions

## Confidence

**High confidence:** The theoretical complexity analysis (O(ML) vs O(NM) vs O((N+M)²)) is sound and well-supported by the architecture description. The bidirectional attention mechanism in the conditioning phase is clearly specified and implementable.

**Medium confidence:** The empirical performance claims on 1D regression and image completion are reasonable given the results shown, but the lack of multiple random seeds and variance reporting limits confidence in the magnitude of improvements.

**Low confidence:** The contextual bandits results, particularly the dramatic degradation at δ=0.99, suggest latent compression may fundamentally fail for tasks requiring precise discrimination in rare-event regions. However, the paper acknowledges NP inconsistency and doesn't provide sufficient analysis of why this occurs or how to mitigate it.

## Next Checks

1. **Statistical significance testing:** Run ISANP(8) vs LBANP(128) comparison with 10 random seeds on 1D regression tasks. Report mean ± std for log-likelihood to determine if claimed performance parity is statistically significant.

2. **Latent capacity ablation study:** Systematically vary L from 4 to 256 on CelebA32 while monitoring both performance and memory usage. Identify the point of diminishing returns and test whether ISANP(8)'s performance advantage over LBANP(128) persists across the full range.

3. **Rare-event task analysis:** Design a controlled experiment where context points are sampled from two regions with dramatically different densities (e.g., 95% from [0,0.5], 5% from [0.9,1.0]). Measure ISANP performance degradation as the rare region's importance increases, quantifying the break condition for latent compression.