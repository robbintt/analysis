---
ver: rpa2
title: 'LENS: LLM-Enabled Narrative Synthesis for Mental Health by Aligning Multimodal
  Sensing with Language Models'
arxiv_id: '2512.23025'
source_url: https://arxiv.org/abs/2512.23025
tags:
- lens
- data
- narrative
- user
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: LENS generates clinically grounded mental health narratives by
  aligning raw multimodal sensing data with large language models. It uses a patch-level
  time-series encoder to project sensor signals into LLM representation space, trained
  on a dataset of over 100,000 sensor-text pairs derived from EMA responses.
---

# LENS: LLM-Enabled Narrative Synthesis for Mental Health by Aligning Multimodal Sensing with Language Models

## Quick Facts
- arXiv ID: 2512.23025
- Source URL: https://arxiv.org/abs/2512.23025
- Reference count: 40
- LENS achieves ROUGE-L of 0.409 and BERTScore of 0.775 for narrative generation, with superior symptom coverage (0.801) and alignment (0.601)

## Executive Summary
LENS is a system that generates clinically grounded mental health narratives by aligning raw multimodal sensing data with large language models. The system uses a patch-level time-series encoder to project smartphone sensor signals into LLM representation space, trained on a dataset of over 100,000 sensor-text pairs derived from Ecological Momentary Assessment (EMA) responses. LENS outperforms baseline approaches on linguistic and clinical metrics, demonstrating the potential for passive sensing data to enhance mental health assessment and monitoring.

## Method Summary
LENS employs a novel patch-level time-series encoding approach that transforms continuous smartphone sensing data into discrete patches, which are then projected into LLM representation space. The system was trained on a large dataset of over 100,000 sensor-text pairs collected from EMA responses, enabling the alignment of multimodal sensing patterns with clinically relevant narratives. The architecture leverages vision-language models while maintaining computational efficiency through its specialized encoding scheme.

## Key Results
- LENS achieves ROUGE-L of 0.409 and BERTScore of 0.775 for narrative generation
- Superior symptom coverage (0.801) and alignment (0.601) compared to baselines
- Mental health experts rated LENS narratives as more comprehensive, accurate, and clinically useful than text-based baselines, with effect sizes ranging from dz = 0.39 to dz = 1.17
- LENS matches performance of 2.2× larger vision-language models while reducing token consumption by 94% compared to text serialization

## Why This Works (Mechanism)
LENS works by creating a learned alignment between passive sensing data and clinical narratives through patch-level encoding. The system transforms continuous sensor streams into discrete patches that can be mapped to LLM representations, effectively bridging the modality gap between time-series data and language. This alignment is learned from real-world EMA data where users self-reported their mental states alongside their sensor data, creating a ground truth mapping between behavioral patterns and clinical descriptions.

## Foundational Learning
- **Patch-level time-series encoding**: Needed to convert continuous sensor data into discrete units that can be processed by LLMs; quick check: verify that patch boundaries preserve temporal context
- **Multimodal alignment**: Required to map sensor representations to language model space; quick check: measure alignment loss during training
- **EMA-based ground truth**: Essential for supervised learning of symptom-narratives; quick check: validate EMA response quality and completeness
- **Vision-language model adaptation**: Needed to handle non-visual multimodal data; quick check: confirm model performance on held-out vision tasks
- **Clinical metric validation**: Critical for ensuring medical utility; quick check: involve domain experts in evaluation design

## Architecture Onboarding

**Component Map**
Smartphone Sensors -> Patch Encoder -> Representation Space -> LLM Alignment -> Clinical Narrative

**Critical Path**
Sensor data acquisition → Patch-level encoding → Representation projection → LLM-based generation → Clinical narrative output

**Design Tradeoffs**
The patch-level encoding approach trades temporal resolution for computational efficiency and modality compatibility. While this may lose some fine-grained temporal dependencies, it enables the system to process continuous sensor streams efficiently and align them with language models.

**Failure Signatures**
- Poor narrative quality when sensor data is sparse or noisy
- Loss of temporal context between related sensor events
- Misalignment between sensing patterns and narrative content when EMA data is inconsistent
- Overfitting to specific sensing patterns in the training data

**3 First Experiments**
1. Test narrative generation with synthetic sensor patterns to validate encoding robustness
2. Evaluate sensitivity to different patch sizes and sampling rates
3. Compare performance across different combinations of sensor modalities

## Open Questions the Paper Calls Out
None

## Limitations
- Reliance on self-reported EMA data may introduce reporting biases and recall inaccuracies
- Patch-level encoding may lose important temporal dependencies between sensor events
- Mental health expert evaluation involved a relatively small sample of 12 clinicians
- Focus on depression and anxiety symptoms may limit applicability to other mental health conditions

## Confidence

**High confidence in:**
- Technical feasibility of aligning multimodal sensing data with LLM representations through patch-level encoding
- Computational efficiency gains achieved through the proposed architecture
- Basic premise that multimodal sensing can enhance mental health narrative generation

**Medium confidence in:**
- Clinical utility of the generated narratives as assessed by mental health experts
- Comparative performance metrics against baseline models
- Generalizability of findings across diverse populations and mental health conditions

**Low confidence in:**
- Long-term reliability of narratives generated from passive sensing data
- Ability to capture complex mental health states without additional contextual information
- System's performance in real-world clinical settings beyond controlled evaluation conditions

## Next Checks
1. Conduct a longitudinal study with diverse clinical populations to assess narrative stability and accuracy over extended time periods and varying contexts
2. Implement a randomized controlled trial comparing clinician decision-making and patient outcomes when using LENS-generated narratives versus traditional assessment methods
3. Perform ablation studies to determine the relative contribution of different sensor modalities and explore optimal sampling frequencies for various mental health indicators