---
ver: rpa2
title: 'H-Neurons: On the Existence, Impact, and Origin of Hallucination-Associated
  Neurons in LLMs'
arxiv_id: '2512.01797'
source_url: https://arxiv.org/abs/2512.01797
tags:
- neurons
- h-neurons
- hallucination
- these
- hallucinations
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the microscopic mechanisms of hallucinations
  in large language models (LLMs) by identifying and analyzing hallucination-associated
  neurons (H-Neurons). The researchers developed a systematic methodology to identify
  a sparse subset of neurons (less than 0.1% of total neurons) that can reliably predict
  hallucination occurrences across diverse scenarios.
---

# H-Neurons: On the Existence, Impact, and Origin of Hallucination-Associated Neurons in LLMs
## Quick Facts
- arXiv ID: 2512.01797
- Source URL: https://arxiv.org/abs/2512.01797
- Reference count: 25
- Primary result: Identified hallucination-associated neurons (H-Neurons) that are causally linked to over-compliance behaviors in LLMs

## Executive Summary
This paper investigates the microscopic mechanisms underlying hallucinations in large language models by identifying and analyzing hallucination-associated neurons (H-Neurons). The researchers developed a systematic methodology to identify a sparse subset of neurons (less than 0.1% of total neurons) that can reliably predict hallucination occurrences across diverse scenarios. Through controlled interventions, they demonstrated that these neurons are causally linked to over-compliance behaviors, where models prioritize conversational compliance over factual integrity. Tracing the origins of these neurons revealed they emerge during the pre-training phase rather than post-training alignment, as evidenced by cross-model transfer experiments showing the neurons retain predictive ability in base models. The findings bridge macroscopic behavioral patterns with microscopic neural mechanisms, offering insights for developing more reliable LLMs by targeting specific neurons for detection and intervention strategies.

## Method Summary
The researchers developed a systematic methodology to identify hallucination-associated neurons (H-Neurons) in large language models. Their approach involved analyzing neuron activation patterns across diverse hallucination scenarios, using controlled interventions to establish causal relationships between specific neurons and hallucination behaviors, and conducting cross-model transfer experiments to trace the origins of these neurons. The methodology included evaluating neuron responses to various hallucination types, measuring predictive accuracy for hallucination occurrences, and performing ablation studies to confirm causal links. The identification process leveraged statistical analysis of activation patterns combined with intervention experiments to distinguish between correlation and causation.

## Key Results
- Identified a sparse subset of neurons (<0.1% of total) that can reliably predict hallucination occurrences across diverse scenarios
- Demonstrated causal link between H-Neurons and over-compliance behaviors through controlled interventions
- Traced H-Neuron origins to the pre-training phase rather than post-training alignment via cross-model transfer experiments

## Why This Works (Mechanism)
The paper's mechanism centers on how specific neurons in LLMs become specialized for hallucination behaviors through the training process. H-Neurons emerge as the model learns to balance between generating coherent responses and adhering to factual constraints. During pre-training, these neurons develop activation patterns that correlate with scenarios where the model must choose between conversational compliance and factual accuracy. The over-compliance mechanism occurs when these neurons become overly activated, causing the model to prioritize response generation that maintains conversational flow over factual integrity. This mechanism explains why hallucinations often occur in contexts where the model faces pressure to provide answers despite lacking sufficient factual grounding.

## Foundational Learning
- Neuron activation patterns: Understanding how individual neurons respond to different inputs is crucial for identifying H-Neurons and their predictive capabilities (quick check: verify activation distributions across hallucination vs non-hallucination scenarios)
- Causal inference in neural networks: Establishing that H-Neurons cause over-compliance rather than merely correlating with it requires rigorous intervention experiments (quick check: compare pre- and post-intervention hallucination rates)
- Cross-model transfer learning: Demonstrating that H-Neurons persist across different model versions helps trace their origins to pre-training (quick check: measure predictive accuracy of H-Neurons in base vs aligned models)
- Over-compliance behavior: Recognizing this specific failure mode is essential for understanding why hallucinations occur despite model training (quick check: identify scenarios where compliance conflicts with factual accuracy)
- Sparse neural coding: The finding that <0.1% of neurons are responsible for hallucinations suggests highly specialized neural representations (quick check: analyze neuron sparsity patterns across different hallucination types)

## Architecture Onboarding
- Component map: Input tokens -> Embedding layer -> Transformer blocks -> H-Neurons in attention heads/MLP layers -> Output generation
- Critical path: Token embedding → Multiple transformer layers → H-Neuron activation → Attention mechanism → Response generation
- Design tradeoffs: Model capacity vs hallucination control, computational efficiency vs interpretability, pre-training objectives vs alignment safety
- Failure signatures: Excessive H-Neuron activation, over-reliance on conversational patterns over factual retrieval, confidence in incorrect responses
- First experiments: 1) Map H-Neuron activation patterns across different hallucination types, 2) Test intervention effectiveness on hallucination reduction, 3) Verify pre-training origin through base model transfer

## Open Questions the Paper Calls Out
None

## Limitations
- Methodology may not capture the full diversity of hallucination types in real-world applications
- Causal relationship findings require validation across broader model architectures and tasks
- Cross-model transfer experiments are based on a limited set of models, limiting generalizability

## Confidence
- High confidence: The existence and predictive capability of H-Neurons within the specific experimental framework
- Medium confidence: The causal relationship between H-Neurons and over-compliance behaviors, and their emergence during pre-training phase
- Low confidence: Generalizability of findings to all LLM architectures, training methods, and real-world scenarios

## Next Checks
1. Conduct experiments across a more diverse set of LLM architectures (different model sizes, training objectives, and datasets) to assess the generalizability of H-Neuron identification and their predictive power for hallucinations.
2. Perform ablation studies where identified H-Neurons are selectively removed or modified in various models to quantify their impact on hallucination rates across multiple task types and domains.
3. Investigate the temporal dynamics of H-Neuron emergence during training by analyzing model checkpoints throughout the training process to pinpoint when these neurons first appear and how their activation patterns evolve.