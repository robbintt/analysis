---
ver: rpa2
title: 'CLI-RAG: A Retrieval-Augmented Framework for Clinically Structured and Context
  Aware Text Generation with LLMs'
arxiv_id: '2507.06715'
source_url: https://arxiv.org/abs/2507.06715
tags:
- clinical
- notes
- note
- retrieval
- generation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'CLI-RAG introduces a dual-stage retrieval-augmented generation
  framework that synthesizes structured progress notes from fragmented clinical data.
  It combines hierarchical chunking that preserves clinical section structure with
  a two-phase retrieval: global retrieval identifies relevant note types using task-specific
  questions, and local retrieval extracts detailed evidence within those types.'
---

# CLI-RAG: A Retrieval-Augmented Framework for Clinically Structured and Context Aware Text Generation with LLMs

## Quick Facts
- **arXiv ID:** 2507.06715
- **Source URL:** https://arxiv.org/abs/2507.06715
- **Authors:** Garapati Keerthana; Manik Gupta
- **Reference count:** 15
- **Primary result:** Dual-stage retrieval-augmented generation achieves 87.7% temporal alignment and 96% LLM-preference for structured clinical notes

## Executive Summary
CLI-RAG is a dual-stage retrieval-augmented generation framework that synthesizes structured SOAP-format progress notes from fragmented clinical data. The system combines hierarchical chunking that preserves clinical section structure with a two-phase retrieval approach: global retrieval identifies relevant note types using task-specific questions, while local retrieval extracts detailed evidence within those types. Evaluated on MIMIC-III, CLI-RAG demonstrates superior temporal coherence and note quality compared to real clinician notes, achieving 87.7% temporal alignment versus 80.7% baseline.

## Method Summary
The method employs hierarchical chunking that first segments clinical notes by section headers (e.g., "History of Present Illness," "Assessment"), then recursively divides into sub-chunks with overlap. A dual-stage retrieval pipeline uses task-driven clinical questions to first identify relevant note types (global), then extract fine-grained content within those types (local). The system incorporates prior visit summaries for longitudinal consistency and uses hybrid scoring (BM25 + cosine) with symptom-based reranking. Generated notes are evaluated for SOAP completeness, semantic similarity, temporal alignment, and LLM preference.

## Key Results
- Achieves 87.7% temporal alignment compared to 80.7% for real clinician notes
- Consistently produces complete SOAP notes (4/4 sections)
- LLM preference shows 96% preference for CLI-RAG notes over clinician-authored notes
- Semantic similarity scores range from 0.74-0.75 across different model sizes

## Why This Works (Mechanism)

### Mechanism 1
Hierarchical chunking that respects clinical section boundaries preserves semantic coherence better than fixed-length chunking. Notes are first segmented by clinical section headers, then recursively divided into sub-chunks based on character thresholds with overlap. Each chunk carries metadata including note_type, header_name, chunk_id_in_header, and temporal tags. This approach preserves intra-document cohesion and reduces boundary-related semantic leakage during encoding.

### Mechanism 2
Dual-stage retrieval (global → local) improves evidence coverage by first identifying relevant note types, then extracting fine-grained content within them. Global retrieval uses task-driven clinical questions to search across all 15 note types without note_type constraints, identifying which document categories contain relevant information. Local retrieval then restricts search to specific note types with curated sub-queries, enabling section-aware extraction tuned to each document's documentation patterns.

### Mechanism 3
Incorporating prior visit summaries as context improves longitudinal temporal coherence in generated notes. For visits beyond the first, the system summarizes the previous visit's generated note and includes it in the prompt, conditioning the model on temporal evolution rather than isolated snapshots. This approach helps distinguish new findings from ongoing conditions and maintains continuity across visits.

## Foundational Learning

### Dense Retrieval & Hybrid Scoring
Why needed: CLI-RAG uses sentence embeddings combined with BM25 lexical scoring for reranking. Understanding how semantic similarity and keyword matching complement each other explains why both are needed. Quick check: Given a query about "hypoxemia symptoms," would pure embedding retrieval capture a chunk mentioning "oxygen saturation dropping into 80s" without the word "hypoxemia"?

### SOAP Note Structure
Why needed: The target output format is SOAP (Subjective, Objective, Assessment, Plan). The evaluation explicitly checks for all four sections. Understanding what belongs in each section is necessary to assess output quality. Quick check: In a SOAP note, would "patient reports chest pain" belong in Subjective or Objective?

### Hierarchical Document Chunking
Why needed: Unlike flat chunking (fixed token windows), hierarchical chunking preserves section boundaries first, then splits within sections. This affects how retrieval operates and what metadata is available. Quick check: If a 2000-token "Assessment" section is chunked hierarchically vs. flat, which approach keeps all assessment-related content retrievable as a coherent unit?

## Architecture Onboarding

### Component map:
Raw Notes (15 types) → Preprocessing (denoising, header normalization) → Hierarchical Chunking (section-first, then length-based) → Vector Store (ChromaDB, all-mpnet-base-v2 embeddings) → Global Retrieval (task queries, no note_type filter) → Local Retrieval (note-type-specific queries, filtered search) → Deduplication + Hybrid Reranking (BM25 + cosine) → Prompt Construction (sorted chunks, metadata brackets, prior summary) → LLM Generation (SOAP output)

### Critical path:
The dual-stage retrieval (global → local) is the core innovation. If either stage fails, evidence quality degrades. Global retrieval without proper queries misses relevant note types; local retrieval without note_type scoping returns unfocused content.

### Design tradeoffs:
Two-stage retrieval increases latency but improves evidence precision (not quantified in paper for inference time). Hierarchical chunking requires consistent section headers; fallback "Unlabeled" chunks preserve content but lose section-aware retrieval benefits. Prior visit summaries improve temporal coherence but risk error propagation if earlier generated notes contain hallucinations.

### Failure signatures:
Missing SOAP sections in output → retrieval failed to surface relevant evidence for that section. Low semantic similarity despite high lexical overlap → model copying rather than synthesizing. Temporal inconsistency across visits → prior summaries not incorporated or not influencing generation.

### First 3 experiments:
1. **Single-stage ablation:** Run retrieval with only global or only local stage; compare temporal alignment scores to dual-stage baseline.
2. **Chunking strategy comparison:** Flat fixed-length vs. hierarchical chunking; measure retrieval precision using held-out relevance judgments.
3. **Longitudinal conditioning test:** Generate notes with and without prior visit summaries; compute temporal alignment across adjacent visits to isolate contribution.

## Open Questions the Paper Calls Out

### Open Question 1
How can factuality calibration be systematically integrated into clinical RAG pipelines to prevent hallucinations in high-stakes medical text generation? The authors state in the Discussion: "Future extensions should explore factuality calibration... Embedding human-in-the-loop evaluation will be essential to ensure safety, trustworthiness, and integration into real-world workflows." This is unresolved because the current framework relies on retrieval grounding and prompt instructions to discourage hallucination but lacks explicit mechanisms to verify factual accuracy of generated content against source evidence.

### Open Question 2
Does LLM-based preference voting (96% preference for generated notes) correlate with actual clinician judgment and patient care outcomes? The evaluation uses Mistral-7B for blinded quality checks, achieving 96% preference for generated notes over clinician-authored ones. No human clinician evaluation is reported, raising concerns about whether LLM preferences align with clinical utility. The unusually high preference score may reflect LLM aesthetic biases rather than genuine clinical quality.

### Open Question 3
How well does CLI-RAG generalize to underrepresented specialties and diverse healthcare systems beyond the MIMIC-III ICU cohort? The Discussion explicitly calls for "broader generalization to underrepresented specialties" as a future direction. The current evaluation uses only 56 patients from MIMIC-III, a single-institution ICU database. Documentation practices, note structures, and clinical workflows vary substantially across specialties and healthcare systems, which may affect both hierarchical chunking effectiveness and retrieval relevance.

### Open Question 4
Can multimodal evidence (imaging, waveforms, structured lab data) be effectively integrated into the dual-stage retrieval framework without degrading text-based retrieval performance? The authors identify "incorporation of multimodal evidence" as a future extension in the Discussion. The current framework operates solely on textual notes; clinical decision-making often requires synthesizing information across modalities, but cross-modal retrieval alignment and unified prompting strategies remain unexplored.

## Limitations
The experimental design lacks direct comparison against simpler retrieval-augmented baselines (single-stage retrieval, flat chunking) on the same data, making it unclear whether the dual-stage approach provides measurable advantages. LLM-preference evaluation methodology lacks transparency regarding the specific model used, prompt structure, and scoring rubric. The paper reports strong performance metrics but provides limited qualitative analysis of failure cases or edge conditions where the system might produce clinically unsafe outputs.

## Confidence

**High confidence:** SOAP completeness (4/4 sections consistently achieved) and semantic similarity scores (~0.74-0.75) - these are directly measurable from the output.

**Medium confidence:** Temporal alignment (87.7%) and LLM-preference (96%) - these depend on evaluation methodology details not fully specified.

**Low confidence:** Causal attribution of performance gains to specific mechanisms (hierarchical chunking vs. dual-stage retrieval vs. longitudinal conditioning) without proper ablation studies.

## Next Checks

1. **Ablation study on retrieval stages:** Run CLI-RAG with only global retrieval, only local retrieval, and both stages disabled (pure LLM generation) to isolate the contribution of each retrieval component to the final performance metrics.

2. **Chunking strategy comparison:** Implement and evaluate a flat chunking baseline (fixed token windows without section awareness) using identical retrieval and generation pipeline, then compare SOAP completeness, semantic similarity, and temporal alignment scores.

3. **External validation on unseen patients:** Apply the trained CLI-RAG system to a held-out test set of patients not seen during development to verify that the 87.7% temporal alignment and 96% LLM-preference scores generalize beyond the reported cohort.