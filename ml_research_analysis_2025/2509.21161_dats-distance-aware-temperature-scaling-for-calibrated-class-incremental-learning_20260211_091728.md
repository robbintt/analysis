---
ver: rpa2
title: 'DATS: Distance-Aware Temperature Scaling for Calibrated Class-Incremental
  Learning'
arxiv_id: '2509.21161'
source_url: https://arxiv.org/abs/2509.21161
tags:
- calibration
- task
- temperature
- classes
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the problem of post-hoc uncertainty calibration
  in continual learning, where models learn incrementally from sequences of new classes
  and suffer from catastrophic forgetting. Existing calibration methods rely on a
  single shared temperature across tasks, ignoring task-specific variability and leading
  to unstable calibration errors.
---

# DATS: Distance-Aware Temperature Scaling for Calibrated Class-Incremental Learning
## Quick Facts
- arXiv ID: 2509.21161
- Source URL: https://arxiv.org/abs/2509.21161
- Reference count: 23
- Primary result: DATS reduces calibration error across tasks without requiring task labels at test time, outperforming single-temperature baselines in continual learning.

## Executive Summary
This paper addresses the challenge of uncertainty calibration in continual learning, where models sequentially learn new classes and suffer from catastrophic forgetting. Traditional post-hoc calibration methods apply a single global temperature to all predictions, which leads to unstable calibration errors across tasks. The authors propose Distance-Aware Temperature Scaling (DATS), a method that infers task proximity using prototype-based distance estimation from a calibration buffer, then applies adaptive temperatures during calibration without requiring task labels at test time. Empirical results on CIFAR-10, CIFAR-100, TinyImageNet, and imbalanced biomedical datasets (BloodCell, SkinLesions) show DATS consistently reduces miscalibration and provides more stable calibration performance across tasks compared to state-of-the-art approaches.

## Method Summary
DATS calibrates continual learning models by first maintaining a calibration buffer and computing class prototypes. For each test sample, it estimates distance to prototypes across tasks, then assigns an adaptive temperature based on proximity rather than using a single global temperature. This approach addresses the instability of traditional temperature scaling in class-incremental learning by adapting calibration to task-specific uncertainty patterns. The method operates without task labels at test time, making it practical for real-world deployment.

## Key Results
- DATS consistently reduces calibration error across tasks compared to single-temperature baselines
- Achieves more stable calibration performance across tasks, avoiding large fluctuations in calibration error
- Demonstrates effectiveness on both standard vision benchmarks and imbalanced biomedical datasets

## Why This Works (Mechanism)
DATS leverages prototype-based distance estimation to infer task proximity, allowing adaptive temperature assignment that reflects task-specific uncertainty patterns. By avoiding a single global temperature, it prevents the over- or under-calibration that occurs when task-specific variability is ignored. The calibration buffer provides representative samples for computing prototypes, while the distance-based mechanism enables task-aware calibration without requiring explicit task labels during inference.

## Foundational Learning
- **Continual Learning (CL)**: Sequential learning of new classes without catastrophic forgetting; needed because models must adapt to new tasks while retaining old knowledge; quick check: does the method maintain performance on previous tasks?
- **Post-hoc Calibration**: Adjusting model confidence after training to match true accuracy; needed because continual learning models often become overconfident on new classes; quick check: does ECE decrease after calibration?
- **Temperature Scaling**: Scaling logits by a temperature parameter to adjust confidence; needed as a simple calibration method but insufficient alone for CL; quick check: does single-temperature scaling produce unstable results?
- **Prototype-based Distance Estimation**: Computing class prototypes and measuring sample distances; needed to infer task proximity without labels; quick check: are prototypes stable across training epochs?
- **Calibration Buffer**: A small set of representative samples maintained for calibration; needed to compute prototypes and validate calibration quality; quick check: does buffer size affect calibration performance?
- **Expected Calibration Error (ECE)**: A metric measuring the gap between predicted confidence and actual accuracy; needed to quantify miscalibration; quick check: does ECE consistently decrease with DATS?

## Architecture Onboarding
**Component Map**: Calibration Buffer -> Prototype Computation -> Distance Estimation -> Temperature Assignment -> Calibrated Prediction

**Critical Path**: The core innovation is the adaptive temperature assignment based on prototype distance, which requires stable prototype computation and accurate distance estimation from the buffer.

**Design Tradeoffs**: DATS trades buffer storage and prototype computation overhead for improved calibration stability. The method avoids task labels at test time but requires a buffer that may grow with tasks. The distance-based approach is more complex than single-temperature scaling but provides task-specific adaptation.

**Failure Signatures**: Calibration may fail if the buffer is too small or unrepresentative, leading to unstable prototypes. Large domain shifts between tasks may break the distance estimation. If prototypes become noisy due to forgetting, temperature assignment will be unreliable.

**3 First Experiments**:
1. Verify ECE reduction on CIFAR-10 with sequential class addition
2. Test calibration stability across tasks with varying buffer sizes
3. Compare DATS calibration performance against single-temperature baselines under imbalanced data conditions

## Open Questions the Paper Calls Out
None specified in the provided content.

## Limitations
- Calibration effectiveness depends heavily on prototype distance estimation quality, which may be unstable in low-data or imbalanced scenarios
- Reliance on a fixed calibration buffer introduces sensitivity to buffer size and sampling strategy
- Does not explicitly address catastrophic forgetting, which may still affect predictive accuracy
- Empirical scope limited to standard vision datasets, not exploring large domain shifts or highly non-stationary distributions

## Confidence
- Calibration stability improvements on benchmark datasets: High
- Generalization to imbalanced or biomedical data: Medium
- Robustness under extreme forgetting or large domain shifts: Low

## Next Checks
1. Test DATS calibration stability when buffer size is reduced or when using more complex, non-i.i.d. buffer sampling
2. Integrate DATS into a full CL pipeline that explicitly mitigates forgetting (e.g., rehearsal or regularization) and assess joint effects on accuracy and calibration
3. Evaluate DATS under large domain shifts or task sequences with high imbalance to stress-test prototype-based distance estimation