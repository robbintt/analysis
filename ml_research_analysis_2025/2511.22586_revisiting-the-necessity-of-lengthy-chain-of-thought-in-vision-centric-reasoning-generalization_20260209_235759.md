---
ver: rpa2
title: Revisiting the Necessity of Lengthy Chain-of-Thought in Vision-centric Reasoning
  Generalization
arxiv_id: '2511.22586'
source_url: https://arxiv.org/abs/2511.22586
tags:
- reasoning
- visual
- arxiv
- maze
- path
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper investigates how different Chain-of-Thought (CoT) formats
  impact the acquisition of generalizable visual reasoning in vision-language models
  (VLMs). Through a controlled maze-solving benchmark and a standard SFT-then-RL training
  pipeline, the study compares three CoT strategies: Language CoT, Grounding CoT,
  and Visual CoT.'
---

# Revisiting the Necessity of Lengthy Chain-of-Thought in Vision-centric Reasoning Generalization

## Quick Facts
- arXiv ID: 2511.22586
- Source URL: https://arxiv.org/abs/2511.22586
- Authors: Yifan Du; Kun Zhou; Yingqian Min; Yue Ling; Wayne Xin Zhao; Youbin Wu
- Reference count: 40
- Primary result: Concise CoT formats (particularly minimal grounding) achieve superior generalization across maze sizes compared to verbose visual CoT, despite faster convergence from longer traces.

## Executive Summary
This paper investigates how different Chain-of-Thought (CoT) formats impact generalizable visual reasoning in vision-language models. Through controlled experiments on a maze-solving benchmark with Qwen2.5-VL-7B, the study compares Language CoT, Grounding CoT, and Visual CoT formats using SFT-then-RL training. The key finding is that while visual and longer CoT traces accelerate training convergence, they do not improve final performance ceilings. Surprisingly, concise CoT formats—particularly those retaining only minimal grounding results—achieve superior generalization across maze sizes, demonstrating a "short is long" effect that suggests well-grounded but concise supervision better promotes reusable reasoning patterns.

## Method Summary
The method uses a controlled maze-solving benchmark with a standard SFT-then-RL pipeline on Qwen2.5-VL-7B. SFT stage involves 8K synthesized CoT trajectories per format (Language, Grounding, Visual, Grounding-least) using Gemini-2.5-Pro prompting, trained for 3 epochs with lr=1e-5. RL stage applies GRPO on 20K maze samples with reward combining accuracy (0.1) and format compliance (0.9). The study evaluates generalization by training on smaller mazes (4×4–6×6) and testing on larger unseen sizes (7×7, 10×10), comparing how different CoT formats affect both convergence speed and generalization performance.

## Key Results
- Visual CoT yields fastest RL convergence (roughly half the training steps of language CoT) but does not lift final performance ceiling
- Concise grounding-based CoT (G-CoT-least) generalizes best across different maze sizes, starting from higher performance and converging faster than explicit grounding CoT
- Grounding-based implicit reasoning encourages models to internalize scale-invariant, local navigation rules while visual CoT is more prone to overfitting to specific visual layouts

## Why This Works (Mechanism)

### Mechanism 1
Visual CoT accelerates convergence by providing spatially-structured supervision that aligns with task geometry. Visual CoT interleaves image manipulations (drawing, cropping) with reasoning steps, creating tighter coupling between reasoning tokens and underlying spatial structure. This reduces search space for policy optimization during RL. Core assumption: visual operations provide information gain proportional to computational cost. Evidence: visual CoT converges fastest (section 4.2.1) but doesn't improve final ceiling. Break condition: if visual manipulation doesn't correspond to task-relevant features.

### Mechanism 2
Grounding ability, once aligned, enables implicit reasoning without explicit coordinate generation. The model internalizes spatial representations during grounding-focused SFT, then operates over latent spatial representations directly, bypassing need to externalize intermediate coordinates. Core assumption: base VLM has sufficient spatial representation capacity from pre-training; SFT primarily aligns this capacity to task format. Evidence: G-CoT-least converges faster and starts higher than explicit grounding CoT (section 4.2.2). Break condition: if base model lacks sufficient spatial pre-training, implicit reasoning may fail.

### Mechanism 3
Concise grounding promotes scale-invariant reasoning patterns that generalize better than verbose traces. Long CoT traces over-specify reasoning path, encouraging model to memorize sequence-specific patterns rather than abstract navigation rules. Concise grounding provides only essential signal, forcing model to learn transferable rules. Core assumption: task has underlying rules invariant across scale; verbose supervision adds noise that distracts from learning these rules. Evidence: CoT retaining minimal grounding results generalizes best across maze sizes (abstract). Break condition: if task requires inherently scale-dependent reasoning, concise grounding may under-specify.

## Foundational Learning

- **Grounding alignment**
  - Why needed: Paper's core finding is that models can perform implicit reasoning after grounding is "aligned" to visual environment, but this alignment is pre-condition
  - Quick check: Can your base VLM accurately localize points/regions on images from target domain before any task-specific training?

- **SFT as policy shaping for RL**
  - Why needed: Paper shows RL from scratch collapses; SFT provides shaped policy space that stabilizes RL exploration
  - Quick check: Does your SFT data cover action space sufficiently to provide reasonable initial policy, or are there large unexplored regions?

- **Generalization vs. convergence trade-off**
  - Why needed: Paper explicitly separates convergence speed (training efficiency) from generalization (test performance on unseen scales), showing these can be anti-correlated
  - Quick check: Are you optimizing for fast convergence on training data, or for stable performance on distributionally shifted test data?

## Architecture Onboarding

- **Component map**: Base model (Qwen2.5-VL-7B) -> SFT stage (8K CoT samples, 3 epochs) -> RL stage (GRPO on 20K mazes) -> Evaluation (generalization across maze sizes)

- **Critical path**: 
  1. Generate synthetic CoT data for each format using rule-based + Gemini-2.5-Pro prompting
  2. Run SFT to obtain format-specific policy models
  3. Apply GRPO until convergence (monitor training accuracy)
  4. Evaluate on held-out scales (train on 4×4–6×6, test on 7×7)

- **Design tradeoffs**:
  - Visual CoT: Faster convergence but no ceiling improvement; higher inference cost due to image manipulation
  - Grounding-least CoT: Best generalization with minimal supervision; requires good grounding alignment from base model
  - Long CoT: May overfit to specific patterns; computationally expensive during both training and inference

- **Failure signatures**:
  - RL collapse (accuracy drops to zero): Missing or insufficient SFT cold-start
  - No generalization to larger scales: Over-specified CoT traces (try grounding-least)
  - Slow convergence with visual CoT: Visual operations may not be task-relevant; verify alignment between operations and task structure

- **First 3 experiments**:
  1. Sanity check: Replicate maze experiment with grounding-least vs. visual CoT on small scale (4×4 → 5×5) to validate "short is long" effect
  2. Ablation on grounding alignment: Test grounding-least on base model with weaker spatial pre-training to identify if implicit reasoning depends on pre-trained grounding capacity
  3. Transfer to new domain: Apply grounding-least CoT to different vision-centric task (e.g., visual search, puzzle assembly) to test mechanism generalizability

## Open Questions the Paper Calls Out

### Open Question 1
Does the "short is long" effect observed in maze navigation generalize across different VLM architectures with varying visual encoders and pre-training objectives? Basis: authors plan to extend analysis to richer task families beyond mazes and VLMs. Why unresolved: all experiments use only Qwen2.5-VL-7B; findings may depend on its specific visual grounding capabilities. What evidence would resolve it: replicating same SFT-then-RL protocol with other VLMs on identical maze benchmarks.

### Open Question 2
What is the mechanistic explanation for why concise grounding-only CoT promotes better generalization than verbose step-by-step traces? Basis: paper demonstrates effect empirically but states grounding-based implicit reasoning "encourages the model to internalize scale-invariant, local navigation rules" without explaining underlying learning dynamics. Why unresolved: paper identifies phenomenon but doesn't analyze internal representations or attention patterns to explain why longer traces may introduce overfitting or unnecessary noise. What evidence would resolve it: probing experiments analyzing latent spatial representations or attention maps in models trained with different CoT lengths.

### Open Question 3
How does task structure interact with optimal CoT design—specifically, do structured spatial tasks (mazes, puzzles) favor concise CoT while open-ended reasoning tasks may still benefit from longer traces? Basis: authors distinguish between "vision-centric reasoning" (spatial, structured) and "language-dominant reasoning" tasks, validating findings on mazes, FrozenLake, and Jigsaw but noting "vision-centric reasoning" as focus. Why unresolved: paper doesn't test whether longer CoT might still be beneficial for tasks requiring more abstract or semantic reasoning rather than pure spatial manipulation. What evidence would resolve it: comparative experiments on both structured spatial tasks and open-ended visual QA tasks using same CoT variants.

## Limitations

- Claims about generalization rest on single synthetic benchmark (maze navigation) with specific VLM (Qwen2.5-VL-7B)
- Mechanism explanations for why concise grounding generalizes better are largely theoretical with limited empirical dissection of learned patterns
- Reward shaping in RL (format compliance weighted 0.9) may artificially inflate performance differences between CoT formats
- Visual CoT's computational overhead during inference is acknowledged but not quantified, leaving practical deployment considerations underexplored

## Confidence

- **High confidence**: Empirical observation that visual and longer CoT traces accelerate convergence without improving final performance ceilings
- **Medium confidence**: Claim that concise grounding (G-CoT-least) achieves superior generalization across maze sizes
- **Low confidence**: Proposed mechanisms explaining why concise grounding generalizes better (e.g., avoiding over-specification, promoting scale-invariant reasoning)

## Next Checks

1. Sanity check replication: Replicate maze experiment with grounding-least vs. visual CoT on small scale (4×4 → 5×5) to validate "short is long" effect in your setup
2. Ablation on grounding alignment: Test grounding-least on base model with weaker spatial pre-training (e.g., smaller VLM) to identify if implicit reasoning depends on pre-trained grounding capacity
3. Transfer to new domain: Apply grounding-least CoT to different vision-centric task (e.g., visual search, puzzle assembly) to test whether mechanism generalizes beyond mazes