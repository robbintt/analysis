---
ver: rpa2
title: 'Multilinguality in LLM-Designed Reward Functions for Restless Bandits: Effects
  on Task Performance and Fairness'
arxiv_id: '2501.13120'
source_url: https://arxiv.org/abs/2501.13120
tags:
- prompt
- prompts
- more
- allocation
- reward
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the effects of language diversity on LLM-designed
  reward functions for Restless Multi-Armed Bandits (RMABs), a framework for resource
  allocation problems in public health. The authors study how non-English prompts
  affect both task performance and fairness when using the DLM algorithm, a recent
  method for designing reward functions via LLMs.
---

# Multilinguality in LLM-Designed Reward Functions for Restless Bandits: Effects on Task Performance and Fairness

## Quick Facts
- arXiv ID: 2501.13120
- Source URL: https://arxiv.org/abs/2501.13120
- Reference count: 4
- Key outcome: English prompts significantly outperform other languages in proposing acceptable reward functions and achieving better task performance for LLM-designed reward functions in Restless Multi-Armed Bandits.

## Executive Summary
This paper investigates how language diversity affects LLM-designed reward functions for Restless Multi-Armed Bandits (RMABs), a framework for resource allocation problems in public health. The authors study the DLM algorithm's performance when prompted in English, Hindi, Tamil, and Tulu across varying prompt complexities. Results show English prompts yield significantly higher rates of acceptable reward functions and better task performance, with greater robustness to increasing complexity. Additionally, low-resource languages and complex prompts are more likely to introduce fairness violations along unintended dimensions. The study highlights the importance of considering linguistic diversity in LLM applications for equitable resource allocation.

## Method Summary
The study uses the DLM algorithm with Gemini 1.0 Pro to generate Python reward functions from natural language prompts for Restless Multi-Armed Bandits. Researchers created a synthetic environment with 6 features (Age, Income, Language Spoken, Education Level, Phone Ownership, Times To Be Called) using structural equations with controlled correlation strength (α ∈ {0.2, 0.8}). They translated 8 goal prompts into four languages and ran 20 independent experiments per configuration, measuring acceptable reward function rates, task success rates, and DP variance on unintended features. The Whittle Index-based RL solver was used for allocation decisions.

## Key Results
- English prompts show 0.65 ± 0.15 acceptable rate for Prompt 1 vs. 0.45 ± 0.15 for Tulu
- English prompts maintain advantage but still degrade with complexity; all languages fail at ≥4 features
- Low-resource languages and complex prompts show higher DP variance in unintended features

## Why This Works (Mechanism)

### Mechanism 1: Training Distribution Alignment Affects Reward Function Acceptability
- Claim: English prompts yield higher acceptable reward functions because the underlying LLM has stronger grounding in English-language instruction following and code generation patterns.
- Mechanism: The DLM algorithm uses an LLM to generate Python reward functions from natural language. When prompts are in English, the LLM more reliably maps prompt features to correct feature indices and produces syntactically valid, semantically aligned reward expressions.
- Core assumption: The LLM's training corpus is English-dominant, creating differential capability across languages.
- Evidence anchors: [abstract] "Our results show that the LLM-proposed reward functions are significantly better when prompted in English compared to other languages." [Table 3] English shows 0.65 ± 0.15 acceptable rate for Prompt 1 vs. 0.45 ± 0.15 for Tulu.

### Mechanism 2: Prompt Complexity Degrades Feature-to-Index Mapping
- Claim: As prompt complexity increases, the LLM's ability to correctly identify and weight all intended features declines, especially for non-English prompts.
- Mechanism: Complex prompts require the LLM to track multiple feature-index mappings simultaneously. Non-English prompts introduce additional translation ambiguity, compounding the mapping error.
- Core assumption: Translation does not preserve precise feature-value boundaries.
- Evidence anchors: [Section 4.3] "Beyond a certain threshold of complexity (three feature allocation), performance collapses regardless of language." [Figure 6, 7] Success rate declines monotonically with complexity.

### Mechanism 3: Correlated Feature Structure Propagates Allocation Bias
- Claim: When environment features are highly correlated (α = 0.8), intended allocation bias propagates to unintended features via the graphical model structure, creating fairness violations.
- Mechanism: The synthetic environment has structural correlations (Age → Income → Phone Ownership). If the reward function prioritizes Age, allocations skew by Income too, because Income is causally downstream of Age.
- Core assumption: The Whittle Index policy does not decorrelate features when allocating resources.
- Evidence anchors: [Section 5.1] "This is not as pronounced in α = 0.8 since the high DP variance in the intended feature propagates to the unintended features due to high conditional correlation." [Figure 8, 11] Low-resource languages and complex prompts show higher DP variance in unintended features.

## Foundational Learning

- Concept: **Restless Multi-Armed Bandits (RMABs)**
  - Why needed here: RMABs model sequential resource allocation where arms continue to evolve when not acted upon. The DLM algorithm designs reward functions that guide allocation decisions.
  - Quick check question: Can you explain why a Whittle Index approximation is needed for RMABs instead of standard Thompson Sampling?

- Concept: **Demographic Parity and DP Variance**
  - Why needed here: Fairness is measured via DP variance—variance in allocation probability across demographic groups. Lower variance indicates fairer allocations.
  - Quick check question: If Age is the intended feature and DP variance is high on Language Spoken (unintended), what does this indicate?

- Concept: **LLM-to-Reward Translation Pipeline**
  - Why needed here: The system uses chain-of-thought prompting to generate Python reward functions. Understanding this pipeline is critical for debugging unacceptable reward proposals.
  - Quick check question: What happens if the LLM outputs a reward function referencing `feats[34]` when the array has length 34?

## Architecture Onboarding

- Component map: User Goal Prompt -> Prompt Translator -> LLM Reward Generator -> Reflection Module -> RMAB Environment -> Whittle Index Policy -> Fairness Evaluator
- Critical path: User provides goal prompt in local language → LLM generates candidate reward functions → Reflection LLM selects final reward function → Whittle Index policy allocates resources → Fairness evaluator checks DP variance
- Design tradeoffs:
  - Explicit phrasing vs. natural expression: Explicit prompts perform better but require user training
  - English-only vs. multilingual interface: English prompts yield 1.3-1.5x higher acceptable rates but exclude non-English speakers
  - Complexity ceiling: Prompts with >3 features have near-zero success rates; recommend capping complexity
- Failure signatures:
  - Zero acceptable rate: LLM fails to map ordered categorical features to correct indices
  - High DP variance on unintended features: Indicates correlated feature structure or unintended preferences
  - Syntactically invalid reward functions: LLM outputs non-Python or references out-of-bounds indices
- First 3 experiments:
  1. Baseline language comparison: Run all 8 prompts in English, Hindi, Tamil, Tulu with α = 0.2; measure acceptable rate and DP variance
  2. Explicit vs. vague phrasing: Compare Prompt 1 vs. Prompt 7 across languages; expect explicit phrasing to improve allocation alignment by 10-25%
  3. Complexity ablation: Progressively add features to prompts (1 → 2 → 3 → 4 features) and measure success rate collapse point

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do ordered categorical features require explicit range specifications in prompts to achieve acceptable task performance across languages?
- Basis in paper: [explicit] The authors note poor performance for Prompts 2 and 3 involving Income and Times To Be Called features, stating "we hypothesize that these types of features require explicit expression of the ranges, but further analysis is required for a definitive stance."
- Why unresolved: The paper only hypothesizes the cause; no controlled experiments isolate ordered categorical features vs. other feature types.
- What evidence would resolve it: Ablation studies comparing prompts with explicit range specifications versus implicit descriptions for ordered categorical features, across all tested languages.

### Open Question 2
- Question: Would a prompt rewriting component that standardizes phrasing and makes feature values explicit significantly improve task performance for low-resource languages?
- Basis in paper: [explicit] The authors propose this direction, noting "our results indicate that both of these could potentially improve task performance" and that phrasing differences cause noticeable allocation disparities.
- Why unresolved: No prompt rewriting system was implemented or evaluated in the study.
- What evidence would resolve it: Implementing a multilingual prompt rewriting module and measuring changes in acceptable reward function rates and task success rates across languages.

### Open Question 3
- Question: How well do these findings generalize to real-world public health datasets compared to the synthetic environment used?
- Basis in paper: [explicit] The authors state they "intend to use more realistic distributions based on real-world data in future work" and acknowledge they assumed uniform distributions due to lack of prior information.
- Why unresolved: The study uses entirely synthetic data with assumed structural equations; real-world feature correlations and transition probabilities may differ.
- What evidence would resolve it: Replicating experiments on actual public health program data (e.g., maternal health call programs) and comparing performance and fairness metrics to synthetic results.

## Limitations
- Reliance on synthetic environment with fixed structural correlations that may not capture real-world complexity
- Unverified translation quality for non-English prompts—automated translation may introduce semantic drift
- Single LLM (Gemini 1.0 Pro) without comparison to other models or multilingual variants

## Confidence
- **High confidence**: English prompts consistently yield higher acceptable reward rates and task performance due to LLM training data bias
- **Medium confidence**: Prompt complexity directly degrades feature mapping accuracy, especially for non-English prompts
- **Medium confidence**: Correlated feature structure propagates allocation bias to unintended dimensions

## Next Checks
1. Back-translation verification: Have independent native speakers translate non-English prompts back to English and measure semantic drift from originals
2. Real-world environment validation: Test the DLM algorithm on a real public health RMAB dataset (e.g., TB screening) to verify synthetic findings transfer
3. LLM ablation study: Compare performance across multiple LLMs (including multilingual instruction-tuned models) to isolate whether English advantage stems from model choice vs. language bias