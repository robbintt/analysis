---
ver: rpa2
title: 'HalluClean: A Unified Framework to Combat Hallucinations in LLMs'
arxiv_id: '2511.08916'
source_url: https://arxiv.org/abs/2511.08916
tags:
- task
- arxiv
- detection
- reasoning
- hallucination
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: HalluClean is a lightweight, task-agnostic framework that detects
  and corrects hallucinations in LLM-generated text using structured reasoning. It
  decomposes the process into planning, execution, and revision stages, guided by
  minimal task-specific prompts.
---

# HalluClean: A Unified Framework to Combat Hallucinations in LLMs

## Quick Facts
- **arXiv ID:** 2511.08916
- **Source URL:** https://arxiv.org/abs/2511.08916
- **Reference count:** 34
- **Primary result:** HalluClean achieves up to 87% F1 score and 89% accuracy in hallucination detection, reducing hallucinations by over 70% in revisions across five tasks.

## Executive Summary
HalluClean is a lightweight, task-agnostic framework that detects and corrects hallucinations in LLM-generated text using structured reasoning. It decomposes the process into planning, execution, and revision stages, guided by minimal task-specific prompts. HalluClean operates in zero-shot settings without external knowledge or supervised detectors. Experiments on five tasks—question answering, dialogue, summarization, math word problems, and contradiction detection—show it significantly improves factual consistency, achieving up to 87% F1 score and 89% accuracy in hallucination detection, and reducing hallucinations by over 70% in revisions. It generalizes across tasks and domains, including medicine and finance, and supports open-source LLM deployment.

## Method Summary
HalluClean uses a 4-step structural reasoning pipeline: (1) Task-oriented Planning, (2) Plan-guided Reasoning, (3) Final Judgment (Yes/No), and (4) Content Refinement. The framework is zero-shot, prompt-based, and does not rely on external knowledge or supervised detectors. It was tested on GPT-3.5-turbo, GPT-4o-mini, Llama-3-70B (4-bit), DeepSeek-V3, and DeepSeek-R1 across five tasks using datasets HaluEval, UMWPs, ChatProtect, and HaluBench. Detection is evaluated using F1 score and accuracy, while revision success is measured by hallucination reduction rate and BERTScore threshold (0.85).

## Key Results
- HalluClean achieves up to 87% F1 score and 89% accuracy in hallucination detection.
- The framework reduces hallucinations by over 70% in revised outputs.
- It generalizes across five tasks and domains including medicine and finance.

## Why This Works (Mechanism)
HalluClean’s effectiveness stems from its structured reasoning approach, which breaks down the hallucination detection and correction process into manageable, interpretable steps. By guiding the model through explicit planning and reasoning, it reduces the likelihood of oversight and ensures thorough evaluation of each claim. The task-agnostic design allows it to adapt to diverse inputs without retraining, and the zero-shot nature means it can be applied immediately without domain-specific data.

## Foundational Learning
- **Structured reasoning:** Why needed: Enables systematic evaluation of LLM outputs. Quick check: Verify model outputs follow the required sections.
- **Zero-shot detection:** Why needed: Avoids need for labeled training data. Quick check: Test on unseen tasks without adaptation.
- **BERTScore thresholding:** Why needed: Quantifies semantic similarity for revision quality. Quick check: Compute score against reference texts.
- **4-bit quantization:** Why needed: Enables efficient open-source LLM deployment. Quick check: Confirm model loads and runs within memory constraints.
- **Chained prompt inference:** Why needed: Ensures logical flow between detection and correction. Quick check: Validate each step’s output feeds correctly into the next.
- **Task-agnostic prompts:** Why needed: Supports broad applicability without retraining. Quick check: Apply same prompts across diverse datasets.

## Architecture Onboarding

**Component map:** Input -> Planning -> Reasoning -> Judgment -> (Optional) Revision -> Output

**Critical path:** The 4-step pipeline is sequential and must be executed in order. Each step’s output becomes the input for the next, with revision only triggered if hallucination is detected.

**Design tradeoffs:** Zero-shot operation maximizes flexibility but may underperform specialized detectors on domain-specific tasks. Lightweight prompts reduce computational cost but rely heavily on model reasoning capacity.

**Failure signatures:** Model ignores structured format and outputs free-form text; revision introduces new hallucinations; high false positives due to language misunderstanding or missing knowledge.

**First experiments:** 1) Run the 4-step pipeline on a single QA sample and inspect each output. 2) Measure BERTScore for a revised summary against its reference. 3) Test detection accuracy on a balanced sample from HaluEval.

## Open Questions the Paper Calls Out
- Can fine-tuning smaller language models with distilled reasoning traces from larger models effectively preserve HalluClean’s detection performance in low-resource environments?
- Does integrating retrieval-augmented generation (RAG) into the reasoning process significantly improve the correction of hallucinations caused by knowledge gaps?
- How susceptible is the structural reasoning mechanism to “reasoning hallucinations,” where the model generates a flawed but plausible logic trace?

## Limitations
- Unknown implementation details (API parameters, prompt formatting) may affect reproducibility.
- Single-pass revision strategy may leave residual hallucinations uncorrected.
- Limited model coverage; performance on smaller or non-transformer LLMs not explored.

## Confidence
- **High Confidence:** Detection performance metrics (F1 scores up to 87%, accuracy up to 89%) and reduction rates (>70% hallucination reduction) are directly reported and verifiable.
- **Medium Confidence:** Generalization claims across tasks and domains are supported by experimental results but rely on limited sample sizes per domain.
- **Low Confidence:** Claims about open-source LLM deployment feasibility are theoretical given limited testing on only 4-bit quantized Llama-3-70B.

## Next Checks
1. Implement exact prompt templates and test with minimal parameter tuning to isolate implementation sensitivity.
2. Measure second-pass error rates by re-running detection on corrected outputs to quantify hallucination reintroduction.
3. Benchmark on out-of-domain samples to evaluate true task-agnostic generalization.