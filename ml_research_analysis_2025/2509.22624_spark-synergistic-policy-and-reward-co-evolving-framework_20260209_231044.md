---
ver: rpa2
title: 'SPARK: Synergistic Policy And Reward Co-Evolving Framework'
arxiv_id: '2509.22624'
source_url: https://arxiv.org/abs/2509.22624
tags:
- reward
- answer
- reasoning
- arxiv
- spark
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "SPARK is a synergistic policy and reward co-evolving framework\
  \ that recycles rollouts and correctness signals from reinforcement learning with\
  \ verifiable rewards (RLVR) to simultaneously train the model itself as a generative\
  \ reward model. By co-evolving policy and reward within a single unified model using\
  \ a mix of objectives\u2014including pointwise scoring, pairwise comparison, and\
  \ reflection-based correction\u2014SPARK eliminates the need for separate reward\
  \ models and costly human preference data."
---

# SPARK: Synergistic Policy And Reward Co-Evolving Framework

## Quick Facts
- arXiv ID: 2509.22624
- Source URL: https://arxiv.org/abs/2509.22624
- Reference count: 40
- Primary result: SPARK achieves 9.7% average gains on reasoning benchmarks, 12.1% on reward benchmarks, and 1.5% on general benchmarks

## Executive Summary
SPARK introduces a novel framework that co-evolves policy and reward models within a single unified architecture, eliminating the need for separate reward models and human preference data. By leveraging verifiable rewards from reinforcement learning with verifiable rewards (RLVR), SPARK trains the model itself as a generative reward model while simultaneously improving the policy through a mix of objectives including pointwise scoring, pairwise comparison, and reflection-based correction. This synergistic approach reduces training costs and improves stability while achieving significant performance gains across multiple benchmarks.

## Method Summary
SPARK operates through a co-evolving process where the same model serves dual roles as both policy and reward model. The framework recycles rollouts and correctness signals from RLVR to train the reward model while using the reward model to guide policy improvements. Training involves three key components: pointwise scoring to evaluate individual responses, pairwise comparison to rank alternatives, and reflection-based correction to refine judgments. The unified model architecture allows for parameter-efficient training and enables test-time scaling through self-reflection without requiring external reward models. This approach addresses the computational burden and potential misalignment issues associated with separate reward models.

## Key Results
- SPARK-VL-7B achieves 9.7% average gains on seven reasoning benchmarks
- 12.1% improvement on two reward benchmarks
- 1.5% gains on eight general benchmarks over baseline approaches

## Why This Works (Mechanism)
The co-evolutionary mechanism works by creating a feedback loop where policy improvements generate better rollouts, which in turn provide higher-quality training data for the reward model. The unified architecture ensures that both components share representations and parameters, leading to better alignment between policy and reward objectives. The mix of training objectives (pointwise, pairwise, and reflection) provides diverse supervision signals that help the model learn robust reward judgments while simultaneously improving its ability to generate high-quality responses.

## Foundational Learning
- **Reinforcement Learning with Verifiable Rewards (RLVR)**: Needed because it provides the verifiable correctness signals that SPARK uses for co-evolution; quick check: verify that RLVR tasks have clear ground truth or automated evaluation metrics
- **Dual-role modeling**: Understanding how a single model can serve as both policy and reward model; quick check: verify parameter sharing and training stability during co-evolution
- **Multi-objective training**: Knowledge of how to combine pointwise scoring, pairwise comparison, and reflection objectives; quick check: confirm gradient flow and objective balancing during training
- **Test-time scaling**: How self-reflection can be used for inference-time improvements; quick check: measure performance gains from reflection steps without additional training
- **Reward model alignment**: Understanding the importance of reward model quality for policy learning; quick check: evaluate reward model accuracy against human judgments
- **Computational efficiency**: How unified architectures reduce training overhead; quick check: compare FLOPs and memory usage against separate model approaches

## Architecture Onboarding

**Component Map:**
Policy Model <-> Reward Model <-> Training Loop -> Reflection Module

**Critical Path:**
Input -> Policy Generation -> Rollout Collection -> Reward Evaluation -> Policy Update -> Repeat

**Design Tradeoffs:**
The unified architecture trades off potential specialization for parameter efficiency and alignment. Separate reward models might achieve higher accuracy but at the cost of additional parameters and potential misalignment. The co-evolutionary approach sacrifices some reward model quality for improved policy performance and reduced computational overhead.

**Failure Signatures:**
- Reward collapse: The reward model becomes too lenient or too strict, failing to provide useful gradients
- Policy-reward misalignment: The policy exploits weaknesses in the reward model rather than learning genuine improvements
- Training instability: Oscillations or divergence when the two models interfere with each other's learning
- Reflection loops getting stuck: Self-reflection fails to escape local optima or produces diminishing returns

**3 First Experiments:**
1. Run co-evolution for 100 steps and measure policy and reward model performance separately
2. Disable reflection module and compare performance to full model
3. Train separate policy and reward models and compare to unified approach

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but implicit limitations include how the framework performs on non-verifiable tasks and the long-term stability of the co-evolutionary process.

## Limitations
- Restricted to verifiable reward tasks, limiting applicability to subjective domains
- Ablation studies don't explore full parameter space of co-evolutionary training
- Self-reflection may get stuck in local optima without external reward signals

## Confidence
- Performance improvements: High
- Methodological soundness: Medium
- Generalizability: Medium
- Computational efficiency claims: Medium

## Next Checks
1. Test SPARK on non-verifiable tasks such as creative writing, summarization, or open-ended dialogue
2. Conduct extended training runs beyond 2000 steps to evaluate long-term stability
3. Implement independent evaluation of reward model accuracy against human annotators on complex reasoning tasks