---
ver: rpa2
title: 'PANTHER: Generative Pretraining Beyond Language for Sequential User Behavior
  Modeling'
arxiv_id: '2510.10102'
source_url: https://arxiv.org/abs/2510.10102
tags:
- user
- panther
- behavior
- transaction
- fraud
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper extends generative pretraining to sequential user behavior
  modeling for large-scale payment platforms. PANTHER introduces structured tokenization,
  a pattern-aware convolutional cross-attention module, and contrastive user profile
  embeddings to model high-cardinality, sparse transaction sequences.
---

# PANTHER: Generative Pretraining Beyond Language for Sequential User Behavior Modeling

## Quick Facts
- **arXiv ID**: 2510.10102
- **Source URL**: https://arxiv.org/abs/2510.10102
- **Reference count**: 40
- **Key outcome**: 25.6% relative improvement in next-transaction prediction and 38.6% improvement in fraud detection recall over baselines on WeChat Pay, with cross-domain generalization

## Executive Summary
PANTHER introduces a generative pretraining framework for sequential user behavior modeling in large-scale payment platforms. It extends language modeling techniques to transaction sequences through structured tokenization and introduces a pattern-aware convolutional cross-attention module (SPRM) to capture complex transaction patterns. The framework achieves significant improvements in next-transaction prediction and fraud detection, demonstrating strong performance across multiple benchmarks including WeChat Pay and public datasets like CCT and MovieLens.

## Method Summary
PANTHER employs a two-stage approach: first, generative pretraining for next-transaction prediction using structured tokenization that creates Cartesian products of contextual and counterparty features with frequency-based compression (retaining top 60K tokens). Second, hybrid inference for fraud detection that fuses pretrained embeddings with behavior deviation scores. The core innovation is the SPRM module, which combines dilated convolutions with prototype cross-attention to capture both local and global transaction patterns. User profiles are embedded using contrastive learning with demographic-based positive pairs, enabling efficient real-time inference through offline caching of user embeddings.

## Key Results
- Achieves 25.6% relative improvement in next-transaction prediction (HR@10) over baselines on WeChat Pay
- Delivers 38.6% improvement in fraud detection recall (Recall@Top-10%) compared to state-of-the-art models
- Demonstrates strong cross-domain generalization with consistent improvements across CCT, MBD-mini, MovieLens-1M, and Yelp benchmarks

## Why This Works (Mechanism)
The framework's effectiveness stems from treating transaction sequences as structured language, where each transaction is tokenized through feature interactions rather than raw values. The SPRM module's dilated convolutions capture multi-scale temporal patterns while prototype cross-attention learns semantic clusters of similar transaction behaviors. Contrastive user profile embeddings enable efficient real-time inference by representing users in a shared behavioral space, allowing quick similarity comparisons for fraud detection.

## Foundational Learning

**Structured Tokenization**: Converting transactions into discrete tokens via Cartesian products of features with frequency compression
- Why needed: Raw transaction data is high-cardinality and sparse; tokenization enables effective sequence modeling
- Quick check: Verify token frequency distribution covers >96% of transactions after compression

**Dilated Convolutions in SPRM**: Multi-scale receptive fields that capture both local and global transaction patterns
- Why needed: Transaction sequences have varying temporal dependencies requiring different kernel sizes
- Quick check: Test different dilation rates and kernel sizes to optimize pattern capture

**Prototype Cross-Attention**: Learnable clusters that represent semantic groups of transaction behaviors
- Why needed: Enables grouping similar transactions for efficient pattern matching and anomaly detection
- Quick check: Visualize prototype clusters to ensure meaningful behavioral groupings

## Architecture Onboarding

**Component Map**: Structured Tokenization -> SPRM-Enhanced Transformer -> Contrastive User Embeddings -> Hybrid Inference

**Critical Path**: Tokenization → SPRM → User Profile Embedding → Real-time Fusion for fraud detection

**Design Tradeoffs**: The 60K token limit balances vocabulary coverage with computational efficiency, while prototype cross-attention trades model complexity for better pattern generalization. Caching user embeddings enables sub-100ms inference at the cost of potential staleness.

**Failure Signatures**: Poor cold-start performance indicates insufficient contrastive pretraining; vocabulary explosion suggests improper frequency-based compression; latency spikes reveal inefficient cache lookup or fusion computation.

**First Experiments**:
1. Implement structured tokenization on WeChat Pay sample data, measuring compression ratio and coverage
2. Test SPRM with different dilation configurations (2x2, 3x3, 4x4) on CCT benchmark
3. Evaluate user profile embedding quality through t-SNE visualization of positive vs. negative pairs

## Open Questions the Paper Calls Out
- **Interpretability**: The complex representations may hinder transparency in high-stakes applications. Future work will focus on improving explainability for broader applicability in regulated domains.
- **Long-tail Pattern Detection**: The 4% of transactions mapped to [UNK] token may include sophisticated fraud patterns exploiting rare feature combinations.
- **Embedding Staleness**: Cached user profile embeddings may become stale, affecting fraud detection performance, but optimal refresh strategies are not specified.
- **Demographic Bias**: The contrastive learning assumption that demographically similar users share behavioral patterns may introduce or amplify biases in fraud detection.

## Limitations
- Structured tokenization requires manual feature engineering and may not generalize to domains with different feature distributions
- SPRM hyperparameters (dilation rates, kernel sizes) are not fully specified, affecting reproducibility
- The contrastive learning approach may propagate demographic biases if historical fraud patterns reflect systemic rather than individual factors

## Confidence
- **High**: The two-stage framework design (generative pretraining → hybrid inference) is feasible and well-motivated for sequential user behavior modeling
- **Medium**: The reported 25.6% and 38.6% improvements are promising, though exact reproduction requires additional architectural details
- **Low**: Exact replication of results is uncertain without full hyperparameter configuration and WeChat Pay tokenization schema

## Next Checks
1. Implement structured tokenization on WeChat Pay subset with 60K vocab limit, validating frequency distribution and compression effectiveness
2. Conduct SPRM ablation studies testing different dilation rate configurations and prototype counts (16, 32, 64)
3. Validate contrastive embedding quality by visualizing user profile embeddings using t-SNE on held-out demographics, checking separation between positive pairs versus random pairs