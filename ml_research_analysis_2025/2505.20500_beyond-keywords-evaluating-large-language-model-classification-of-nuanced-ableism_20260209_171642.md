---
ver: rpa2
title: 'Beyond Keywords: Evaluating Large Language Model Classification of Nuanced
  Ableism'
arxiv_id: '2505.20500'
source_url: https://arxiv.org/abs/2505.20500
tags:
- llms
- autistic
- autism
- human
- ableist
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates how large language models (LLMs) classify
  nuanced ableism against autistic individuals, revealing a significant gap between
  their ability to recognize autism-related terminology and their capacity to identify
  harmful or offensive connotations in context. The research employs a mixed-method
  approach, combining human annotations using established psychometric instruments
  (SATA, AQ, IAT) with LLM evaluations using in-context learning examples and persona
  prompting.
---

# Beyond Keywords: Evaluating Large Language Model Classification of Nuanced Ableism

## Quick Facts
- arXiv ID: 2505.20500
- Source URL: https://arxiv.org/abs/2505.20500
- Reference count: 19
- LLMs fail to identify nuanced ableism against autistic individuals, relying on keyword matching rather than contextual reasoning

## Executive Summary
This study reveals that large language models significantly underperform human annotators in detecting nuanced ableism against autistic individuals. The research demonstrates that LLMs systematically misclassify reclaimed intra-community language as harmful while failing to identify medicalized deficit-based language as ableist. Using psychometric instruments (SATA, AQ, IAT) to stratify human perspectives, the study establishes that neither persona prompting nor in-context learning effectively aligns LLMs with autistic community viewpoints. The findings highlight fundamental limitations in current LLM architectures' ability to reason about speaker identity, intent, and reclaimed language contexts.

## Method Summary
The study employs a mixed-method approach combining human annotations using established psychometric instruments with LLM evaluations. Human annotators were stratified by SATA (explicit attitudes), AQ (autistic traits), and IAT (implicit bias) scores, then used to create ground truth classifications for 283 sentences. Four LLMs (Gemma-2 9B, Mistral 7B, Llama-3 8B, DeepSeek 7B) were evaluated using binary classification prompts with variations including persona instructions and in-context learning examples. Agreement between LLM classifications and human ground truth was measured using Fleiss's Kappa, with qualitative error analysis comparing LLM justifications to human rationales.

## Key Results
- LLMs misclassify reclaimed intra-community language (e.g., "aspie," "autie") as ableist while failing to identify medicalized deficit language as harmful
- Fleiss's Kappa agreement between LLMs and human ground truth consistently falls below 0.2 across all prompt conditions
- Models more effectively replicate anti-autistic biases than community perspectives, even when explicitly instructed otherwise through ICL examples
- No evaluated LLM scored high enough on the Autism Spectrum Quotient to be considered "autistic," suggesting models distance themselves from autism despite prevalent stereotypes

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs classify ableism through surface-level keyword association rather than contextual discourse reasoning
- Mechanism: Classification decisions are driven by statistical co-occurrence patterns between specific terms and "harmful" labels in training data, without modeling speaker identity, intent, or reclamation contexts
- Core assumption: The failure stems from architectural limitations in discourse-level reasoning rather than prompt design
- Evidence anchors:
  - [abstract]: "LLMs tend to rely on surface-level keyword matching, leading to context misinterpretations"
  - [section 4.2]: "sentences containing explicit slurs were almost always labeled ableist by LLMs regardless of context"
  - [corpus]: Related content moderation work (arxiv:2512.20061) shows LLM promise for policy-grounded moderation but doesn't examine nuanced bias detection

### Mechanism 2
- Claim: Training data distribution skews LLMs toward reproducing anti-autistic biases over community perspectives
- Mechanism: Deficit-based, medicalized autism narratives are overrepresented in training corpora, creating stronger learned associations that override prompt-level signals
- Core assumption: The distributional imbalance exists at a scale and embedding depth that prompt engineering cannot effectively counter
- Evidence anchors:
  - [abstract]: "LLMs more effectively replicate anti-autistic biases than community perspectives"
  - [section 4.1]: "LLMs more effectively replicated labeling patterns associated with human biases than those aligned with autism acceptance"
  - [corpus: arxiv:2507.16130]: Related audit of ableism in Western and Indic LLMs supports cross-cultural variation in disability bias

### Mechanism 3
- Claim: Persona prompting and in-context learning are structurally insufficient for aligning LLMs with marginalized community perspectives on nuanced harm
- Mechanism: These techniques modify surface-level behavior patterns but don't alter deeply embedded associations between linguistic patterns and classification decisions formed during pretraining
- Core assumption: Bias in ableism detection operates at a representational level that post-hoc prompting cannot reach
- Evidence anchors:
  - [abstract]: "neither personas nor in-context learning examples significantly improve LLM alignment with autistic viewpoints"
  - [section 4.4]: "modifying prompts through ICL or persona design is insufficient to correct their systematic issues in detecting anti-autistic ableist speech"
  - [corpus]: Work on structured personality control (arxiv:2601.10025) explores persona-based approaches but doesn't validate effectiveness for bias mitigation

## Foundational Learning

- Concept: Keyword-based vs. context-aware classification
  - Why needed here: The core finding is that LLMs fail at nuanced ableism detection because they rely on surface pattern matching rather than discourse-level reasoning
  - Quick check question: Can you explain why the same term (e.g., "aspie") should be classified differently when used for self-identification versus as an external label?

- Concept: Reclaimed language and intra-community discourse
  - Why needed here: A critical failure mode is misclassifying community self-reference as hate speech; understanding linguistic reclamation is essential for interpreting the results
  - Quick check question: Why might terms like "autie" be acceptable within the autistic community but harmful when used by neurotypical outsiders?

- Concept: Medical model vs. neurodiversity paradigm
  - Why needed here: LLMs associate medicalized language (deficit, illness) with neutrality, reflecting dominant training narratives; this paradigm distinction explains a systematic bias direction
  - Quick check question: How does framing autism as a "deficit" differ from framing it as a neurotype, and why might the former be considered ableist?

## Architecture Onboarding

- Component map:
  - AUTALIC dataset (2,121 sentences) -> Human annotations (SATA, AQ, IAT) -> Ground truth set (283 sentences) -> LLM classification conditions (8 total) -> Fleiss's Kappa agreement scores

- Critical path:
  1. Collect human annotations with psychometric instruments → segment by perspective type
  2. Establish ground truth via autistic annotator consensus (inter-rater agreement required)
  3. Generate LLM classifications under 8 prompt conditions (persona × ICL combinations)
  4. Measure LLM-human agreement using Fleiss's Kappa across conditions
  5. Qualitative error analysis: Compare LLM justifications to human rationales for 100 samples

- Design tradeoffs:
  - Binary vs. ternary classification: Binary reduces noise (LLMs conflate -1 and 0 labels); ternary allows uncertainty expression but introduces inconsistency
  - ICL example count: 58 examples proved insufficient; scaling behavior unknown
  - Model size: 7-9B parameter models due to computational constraints; larger models may differ
  - Psychometric instruments: SATA/AQ/IAT have known limitations (medical model framing, single-administration reliability issues)

- Failure signatures:
  - Keyword-triggered false positives: Terms like "aspie" or "autie" classified as ableist regardless of self-identification context
  - Contextual false negatives: Medicalized deficit language ("autism as illness") classified as non-ableist despite explicit ICL guidance
  - Reasoning-label inconsistency: LLMs occasionally provide justifications contradicting their classification
  - Label invention: Models creating undocumented categories (0.5, 8) when ternary scheme is confusing
  - Persona insensitivity: Agreement scores remain flat across persona conditions

- First 3 experiments:
  1. Baseline measurement: Run all 4 models on ground truth set (283 sentences) with base prompt only; compute Fleiss's Kappa against autistic annotator consensus
  2. Classification scheme ablation: Compare binary (0/1) vs. ternary (-1/0/1) across all models; quantify label conflation rate and self-consistency via shuffled re-run
  3. ICL scaling test: For the worst-performing perspective condition, incrementally increase ICL examples (10 → 30 → 58 → 100) to identify if a threshold exists where agreement improves

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can LLMs be modified to prioritize speaker intent and impact over surface-level keyword matching when classifying nuanced ableism?
- Basis in paper: [explicit] The authors state in Section 5 that "LLMs must improve their ability to go beyond superficial keyword detection and instead assess the broader context of a sentence, including its impact, intent, and the identity of its speaker."
- Why unresolved: Current models rely heavily on statistical associations with specific terms rather than understanding the context of reclaimed language or medicalized framing, leading to high misclassification rates.
- What evidence would resolve it: A model architecture or training methodology that successfully distinguishes between intra-community reclaimed usage and external derogatory usage with high agreement with autistic human annotators.

### Open Question 2
- Question: How does LLM performance in detecting anti-autistic ableism generalize to multilingual or non-Western cultural contexts?
- Basis in paper: [explicit] The authors note in Section 7 that their findings are limited to Western, English-speaking viewpoints and "encourage researchers to expand upon this work to assess performance and implications in more diverse settings."
- Why unresolved: The study utilized datasets and annotators primarily within a Western context, and the nuance of ableist language varies significantly across cultures and languages.
- What evidence would resolve it: A replication of the study's methodology using non-English datasets and a diverse set of annotators from various cultural backgrounds to compare alignment scores.

### Open Question 3
- Question: What alternative alignment techniques, beyond in-context learning (ICL) and persona prompting, are required to effectively replicate autistic community perspectives?
- Basis in paper: [inferred] The paper concludes in Section 4.4 and Section 5 that current ICL and persona methods are "insufficient" and "ineffective" for aligning LLMs with autistic viewpoints, implying the need for new methods.
- Why unresolved: The study demonstrated that current popular alignment techniques fail to overcome the models' default "neuronormative" biases or their tendency to replicate anti-autistic stigma found in training data.
- What evidence would resolve it: Identifying a fine-tuning or reinforcement learning strategy that results in Fleiss's Kappa agreement scores between LLMs and autistic annotators that are significantly higher than those achieved via ICL.

## Limitations

- Computational scope limited to 7-9B parameter models, preventing generalization to frontier models
- Binary classification scheme potentially masks nuanced harm detection capabilities
- AUTALIC dataset construction introduces potential cultural and linguistic bias through English-language focus and Western psychometric instruments

## Confidence

**High Confidence**: The finding that LLMs systematically misclassify reclaimed intra-community language as ableist is strongly supported by direct evidence from section 4.2. The quantitative agreement gap (Fleiss's Kappa consistently below 0.2) is robust across multiple model architectures.

**Medium Confidence**: The claim that training data distribution drives anti-autistic bias over community perspectives rests on distributional inference rather than direct corpus analysis. While section 4.1 shows persistent mislabeling of deficit-based language despite ICL examples, the mechanism remains speculative.

**Low Confidence**: The assertion that no evaluated LLM scored "autistic" on AQ requires careful interpretation—AQ is designed as a screening tool for human autism traits, not a valid metric for language model evaluation.

## Next Checks

1. **Corpus Attribution Analysis**: Trace classification decisions for 100 false positive (reclaimed language) and false negative (medicalized deficit language) cases back to their training corpus sources using model activation analysis.

2. **Cross-Cultural Replication**: Replicate the study using non-English datasets with culturally specific autism discourse and local psychometric instruments.

3. **Architecture Scaling Experiment**: Compare 7-9B parameter models against frontier models (e.g., GPT-4, Claude) using identical prompts and evaluation sets.