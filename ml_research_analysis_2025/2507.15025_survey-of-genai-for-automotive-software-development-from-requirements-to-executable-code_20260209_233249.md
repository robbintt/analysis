---
ver: rpa2
title: 'Survey of GenAI for Automotive Software Development: From Requirements to
  Executable Code'
arxiv_id: '2507.15025'
source_url: https://arxiv.org/abs/2507.15025
tags:
- code
- automotive
- generation
- language
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper surveys Generative AI adoption in automotive software
  development, covering requirements handling, compliance, code generation, and optimization.
  It identifies Large Language Models (LLMs), Retrieval Augmented Generation (RAG),
  Vision-Language Models (VLMs), and prompting techniques as core methodologies.
---

# Survey of GenAI for Automotive Software Development: From Requirements to Executable Code

## Quick Facts
- **arXiv ID:** 2507.15025
- **Source URL:** https://arxiv.org/abs/2507.15025
- **Reference count:** 40
- **Primary result:** GenAI adoption in automotive development is near-universal, with LLMs dominating code generation and RAG improving compliance accuracy, though requirements handling lags due to data privacy constraints.

## Executive Summary
This survey examines how Generative AI, particularly Large Language Models, is transforming automotive software development from requirements gathering through code generation. The authors analyze current methodologies including LLMs, RAG, VLMs, and prompting techniques, finding that while GPT-based models excel at code generation, locally deployable fine-tuned models like Llama3 are preferred for requirements processing due to intellectual property concerns. Industry survey results show 100% adoption of GenAI tools among participants, with reported productivity improvements, though requirements handling remains underutilized due to the lack of domain-specific datasets and data protection requirements.

## Method Summary
The study employs a comprehensive literature review combined with an industry survey of 20 automotive software practitioners to assess GenAI adoption patterns. The authors systematically categorize AI methodologies across the development lifecycle, identifying dominant techniques for each phase. The survey methodology captures both technical capabilities and organizational adoption patterns, while the literature review synthesizes current research trends and identifies gaps in the automotive AI ecosystem.

## Key Results
- GPT-based models dominate code generation tasks while locally fine-tuned models like Llama3 are preferred for requirements processing
- 100% of surveyed industry participants report using GenAI tools, with most noting productivity improvements
- Requirements handling remains underutilized due to IP protection concerns and lack of domain-specific datasets
- RAG and multi-agent consensus strategies show promise for improving compliance and reducing hallucinations
- Vision-Language Models face challenges with diagram interpretation accuracy

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Intermediate formal representations (e.g., metamodels, Simulink) may act as a grounding layer to improve the verifiability of generated automotive code compared to direct text-to-code translation.
- **Mechanism:** LLMs convert unstructured requirements into structured model instances (e.g., Ecore, XMI). These models are validated against constraints before code generation, reducing the propagation of hallucinations.
- **Core assumption:** The LLM can reliably map natural language semantics to the strict syntax of a target metamodel or formal specification language.
- **Evidence anchors:**
  - [abstract] Identifies the workflow covering requirements handling, compliance, and code generation with human feedback loops.
  - [section IV] States that "LLMs are used to summarize the extracted requirements with respect to some formal template or metamodel" to enable design-time checks.
  - [corpus] The neighbor paper "Automating Automotive Software Development" further supports the synergy between GenAI and Model-Based Methods.
- **Break condition:** If the LLM fails to generate a syntactically valid model instance, the subsequent formal verification step cannot execute, breaking the automation pipeline.

### Mechanism 2
- **Claim:** Retrieval-Augmented Generation (RAG) likely enables domain-specific accuracy for compliance tasks by anchoring generation in external regulatory texts.
- **Mechanism:** Regulatory documents (e.g., UN157) are indexed into vector stores. During generation, relevant chunks are retrieved and injected into the prompt context, focusing the LLM on specific legal constraints rather than general training data.
- **Core assumption:** The retrieval mechanism successfully identifies and extracts the specific clauses relevant to the query from the ingested regulatory documents.
- **Evidence anchors:**
  - [section V] Notes that RAG is identified as crucial for handling requirements and compliance checking.
  - [section VI] Describes using RAG to extract design-related constraints and assess compliance dynamically.
  - [corpus] Evidence in neighbors is weak regarding specific RAG architectures for automotive; this is primarily derived from the source text.
- **Break condition:** If the retrieval step fetches irrelevant or contradictory context chunks, the LLM may produce confused or non-compliant outputs (retrieval noise).

### Mechanism 3
- **Claim:** Multi-agent consensus strategies (e.g., debate, voting) may reduce reasoning errors compared to single-pass generation.
- **Mechanism:** Multiple LLM instances generate solutions independently. A "judge" or clustering algorithm compares these outputs to filter out inconsistent or hallucinated results.
- **Core assumption:** Errors in reasoning are uncorrelated across independent instances, meaning the majority or consensus view is more likely to be correct.
- **Evidence anchors:**
  - [section VIII] Details frameworks like RECSIP and Multiagent Debate (MAD) which use discourse between agents to improve precision.
  - [corpus] The neighbor "Generating Automotive Code" highlights the need for verification in safety-critical systems, supporting the need for such error-checking mechanisms.
- **Break condition:** If the agents share a common logical blind spot or bias, the consensus mechanism will reinforce the error rather than correct it.

## Foundational Learning

- **Concept:** **Model-Driven Engineering (MDE) & Metamodeling**
  - **Why needed here:** The survey highlights that direct code generation is risky; the preferred architecture uses LLMs to generate *models* (e.g., Ecore, SysML) first. Understanding how text maps to abstract syntax trees or object models is critical.
  - **Quick check question:** Can you explain the difference between generating a Python script directly versus generating a JSON or Ecore model instance that *describes* the script?

- **Concept:** **Prompt Engineering (Chain of Thought & Specification-Driven)**
  - **Why needed here:** The paper identifies Specification-Driven and Chain of Thought (CoT) prompting as dominant techniques for complex automotive logic. Engineers must know how to structure prompts to elicit reasoning steps, not just final code.
  - **Quick check question:** How would you modify a prompt asking for "Cruise Control logic" to force the model to expose its reasoning steps before outputting the code block?

- **Concept:** **Context Window & Chunking Strategies**
  - **Why needed here:** Automotive requirements often span hundreds of thousands of items (Page 1). Understanding how to split large documents without losing semantic context is essential for the RAG ingestion phase.
  - **Quick check question:** If a safety requirement spans two separate document chunks during ingestion, what risk does this pose to the RAG system's retrieval accuracy?

## Architecture Onboarding

- **Component map:** Ingestion Layer -> RAG Pipeline -> Processing Layer -> Verification Layer -> Output
- **Critical path:** The transformation from **Unstructured Requirement** -> **Structured Model Instance** (Page 4). This step determines whether the final code is traceable to a specific requirement or a hallucination.
- **Design tradeoffs:**
  - **IP Protection vs. Performance:** The survey notes a preference for locally deployable models (e.g., Llama 3) for requirements to protect IP, despite commercial models (GPT-4) performing better on code generation tasks.
  - **Direct vs. Indirect Generation:** "Direct" generation is faster but prone to errors; "Indirect" (via models) is slower and more complex but supports verification.
- **Failure signatures:**
  - **Syntactic Hallucination:** Generated code fails to compile or violates MISRA C guidelines.
  - **Logical Drift:** In multi-turn conversations, the LLM loses track of initial constraints (Context Drift).
  - **Visual Misinterpretation:** VLMs failing to read diagram arrows or flowchart orders correctly (Section X).
- **First 3 experiments:**
  1. **VLM Extraction Test:** Provide a UML diagram from a legacy document and prompt a VLM (e.g., GPT-4o or InternVL) to extract class attributes into a JSON format. Verify accuracy against the original image.
  2. **RAG Compliance Check:** Ingest a small set of ISO 26262 requirements. Ask the system to verify a dummy code snippet against these requirements.
  3. **Local vs. Cloud Comparison:** Run a specific requirement-to-code prompt on both a local fine-tuned Llama instance and a commercial GPT instance to compare security (data leakage risks) vs. code correctness.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the bottleneck of limited public data be overcome to enable effective fine-tuning of locally deployable LLMs for requirements handling?
- Basis in paper: [explicit] The Conclusion states that while local models are preferred for IP protection, the "main bottleneck is lack of publicly available specialized automotive datasets which can be used for fine tuning."
- Why unresolved: Automotive requirements are protected by NDAs and data protection policies, preventing the creation of the large-scale datasets needed to train or fine-tune local models to the performance level of commercial alternatives.
- What evidence would resolve it: The creation of synthetic datasets, privacy-preserving data sharing frameworks, or the release of open-source automotive requirement corpora that allow local models to achieve performance parity with commercial models like GPT-4.

### Open Question 2
- Question: What methods are required to effectively detect and mitigate "visual hallucinations" in Vision-Language Models (VLMs) within automotive workflows?
- Basis in paper: [inferred] Section VIII.B contains a header titled "Visual hallucinations" but provides no content, implying a gap in current literature or solutions. Section X further notes that understanding diagrams often requires "additional refinements" to ensure accuracy.
- Why unresolved: The paper successfully surveys textual hallucination mitigation (e.g., multi-agent debate), but the section on visual hallucinations is empty, suggesting a lack of established mitigation strategies for VLMs processing automotive diagrams.
- What evidence would resolve it: Research proposing validation techniques or model architectures that reduce hallucinations when extracting requirements or parameters from UML diagrams and technical schematics.

### Open Question 3
- Question: Can LLMs effectively automate hardware abstraction tasks, specifically interface and signal vehicle mapping?
- Basis in paper: [explicit] The Conclusion explicitly lists "hardware abstraction-related works aiming automotive systems, including interface and signal vehicle mapping with respect to given catalog using LLMs" as a specific area for future work.
- Why unresolved: Current surveyed works focus heavily on software code generation and requirements, but the application of GenAI to the hardware-software interface (e.g., mapping signals to VSS catalogs) remains unexplored in the review.
- What evidence would resolve it: Studies demonstrating LLM-based workflows that can accurately map abstract software signals to specific hardware interface catalogs or generate hardware abstraction layer code.

## Limitations
- Survey relies on self-reported adoption rates from a small industry sample (20 participants), potentially overstating actual productivity gains
- Lacks empirical benchmarks comparing different LLM architectures or prompting strategies on automotive-specific tasks
- Security analysis focuses on data leakage but does not address adversarial prompt injection or model extraction risks in safety-critical contexts

## Confidence
- **High Confidence**: Survey findings on adoption patterns (100% tool usage) and preference for local models for requirements processing are well-supported by direct survey results
- **Medium Confidence**: Theoretical advantages of intermediate model representations and multi-agent consensus are plausible but lack empirical validation in automotive contexts
- **Low Confidence**: Claims about productivity improvements lack control groups or baseline comparisons; VLM effectiveness is based on limited examples without quantitative accuracy metrics

## Next Checks
1. **Benchmark local vs. cloud models**: Implement a controlled experiment comparing GPT-4 and fine-tuned Llama3 on identical automotive requirements-to-code tasks, measuring both accuracy and data leakage risks
2. **Multi-agent consensus validation**: Design a test suite of automotive logic problems where single LLM outputs and multi-agent consensus outputs can be objectively compared for error rates
3. **RAG compliance verification**: Create a test corpus of automotive code snippets with known compliance violations and measure RAG system accuracy in identifying regulatory mismatches