---
ver: rpa2
title: Geometric Scaling of Bayesian Inference in LLMs
arxiv_id: '2512.23752'
source_url: https://arxiv.org/abs/2512.23752
tags:
- bayesian
- geometric
- value
- attention
- entropy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper investigates whether the geometric substrate for Bayesian\
  \ inference\u2014low-dimensional value manifolds, orthogonal key frames, and progressive\
  \ attention focusing\u2014persists in production-scale language models. Across Pythia,\
  \ Phi-2, Llama-3, and Mistral families, the authors find that last-layer value representations\
  \ organize along a dominant entropy-aligned axis, with domain restriction collapsing\
  \ this structure into the same low-dimensional manifolds observed in synthetic settings."
---

# Geometric Scaling of Bayesian Inference in LLMs

## Quick Facts
- arXiv ID: 2512.23752
- Source URL: https://arxiv.org/abs/2512.23752
- Authors: Naman Agarwal; Siddhartha R. Dalal; Vishal Misra
- Reference count: 15
- Primary result: Production-scale transformers preserve low-dimensional value manifolds, orthogonal key frames, and progressive attention focusing—geometric signatures of Bayesian inference—even without ground-truth posteriors.

## Executive Summary
This paper investigates whether the geometric substrate for Bayesian inference—low-dimensional value manifolds, orthogonal key frames, and progressive attention focusing—persists in production-scale language models. Across Pythia, Phi-2, Llama-3, and Mistral families, the authors find that last-layer value representations organize along a dominant entropy-aligned axis, with domain restriction collapsing this structure into the same low-dimensional manifolds observed in synthetic settings. Targeted interventions on this axis disrupt local uncertainty geometry while leaving Bayesian-like calibration largely intact, indicating the geometry is a privileged uncertainty readout rather than a singular computational bottleneck. These results show that modern transformers preserve the geometric substrate underlying Bayesian inference primitives and organize their approximate updates along this substrate, even when ground-truth posteriors are unavailable.

## Method Summary
The paper extracts final-layer value vectors from multiple transformer architectures, concatenates head-wise representations, and applies PCA to measure variance explained along dominant axes. Key projection matrices are analyzed for orthogonality via mean off-diagonal cosine similarity. Per-head attention entropy at the final token position is computed to track layerwise focusing dynamics. Stratified entropy sampling ensures geometric results aren't prompt-dependent. Domain-restriction experiments compare mixed versus single-domain prompts to test manifold collapse predictions. SULA in-context learning tasks validate inference-time Bayesian updating along the identified manifolds.

## Key Results
- Last-layer value representations organize along a dominant axis whose position correlates with predictive entropy, with domain-restricted prompts collapsing this structure into low-dimensional manifolds (PC1+PC2 ≈ 85-95%)
- Key projection matrices develop near-orthogonal column structure during training (mean off-diagonal cosine drops to 0.034-0.18 from initialization baselines of 0.35-0.45)
- Progressive attention focusing shows architecture-dependence: full-sequence MHA achieves 82-86% entropy reduction versus 20-31% in GQA/sliding-window variants
- Targeted ablation of the entropy axis disrupts local uncertainty geometry while preserving Bayesian-like calibration, indicating the geometry is a readout rather than computational bottleneck

## Why This Works (Mechanism)

### Mechanism 1: Entropy-Ordered Value Manifolds
- Claim: Last-layer value vectors organize along a dominant principal component whose position correlates with predictive entropy, providing a geometric encoding of posterior uncertainty.
- Mechanism: Training sculpts value space such that prompts with similar next-token entropy occupy nearby regions on a low-dimensional manifold. When domain-restricted prompts reduce task heterogeneity, this manifold collapses toward one dimension (PC1+PC2 ≈ 85-95%), matching wind-tunnel geometry.
- Core assumption: Predictive entropy is a meaningful proxy for posterior uncertainty in natural language; the manifold structure reflects inference rather than memorization.
- Evidence anchors:
  - [abstract] "last-layer value representations organize along a single dominant axis whose position strongly correlates with predictive entropy, and domain-restricted prompts collapse this structure into the same low-dimensional manifolds observed in synthetic settings"
  - [Section 5.1] Llama-3.2-1B shows PC1+PC2 increasing from 51.4% (mixed-domain) to 73.6% (mathematics-only), a 22-percentage-point domain-restriction effect
  - [corpus] Neighbor paper "The Bayesian Geometry of Transformer Attention" establishes that small transformers in wind-tunnel settings achieve near-exact Bayesian inference with similar low-dimensional value manifolds; this provides theoretical grounding but limited direct evidence for production-scale transfer
- Break condition: If domain restriction fails to reduce dimensionality across diverse prompt distributions, or if PC1-entropy correlations vanish under bootstrap resampling, the manifold may be an artifact of token statistics rather than uncertainty encoding.

### Mechanism 2: Orthogonal Hypothesis Frames via Key Structuring
- Claim: Key projection matrices develop near-orthogonal column structure during training, forming separable hypothesis-frame directions that support content-based retrieval.
- Mechanism: Cross-entropy gradients encourage specialization of key directions to minimize interference between competing hypotheses. Mean off-diagonal cosine similarity drops from initialization baselines (0.35-0.45) to trained values (0.034-0.18), indicating training-sculpted structure.
- Core assumption: Key orthogonality enables random-access binding (retrieving hypotheses by content); this is necessary but not sufficient for Bayesian inference.
- Evidence anchors:
  - [abstract] "orthogonal key frames...persists in production-scale language models"
  - [Section 4.2] "Trained models consistently achieve mean off-diagonal cosines between 0.034 and 0.18 across most layers, representing a 2-10× improvement relative to the initialization baseline"
  - [corpus] Weak direct corpus support for orthogonality specifically; "The Bayesian Geometry of Transformer Attention" mentions key structure but empirical validation at scale is limited to this paper's findings
- Break condition: If key orthogonality remains high (>0.25 mean off-diagonal cosine) after training, or if orthogonality correlates negatively with task performance, the hypothesis-frame mechanism is not operating as theorized.

### Mechanism 3: Progressive Attention Focusing (Architecture-Dependent)
- Claim: Attention entropy decreases layerwise when global routing capacity exists, implementing evidence integration; this dynamic is attenuated under architectural constraints.
- Mechanism: Early layers bind context; mid layers eliminate low-probability hypotheses; late layers refine the posterior. Entropy reduction of 30-86% observed in full-sequence MHA, but only 20-31% in GQA/sliding-window variants.
- Core assumption: Progressive focusing is a sufficient (not necessary) mechanism for Bayesian refinement; architectures with constrained routing may compute posterials through alternative pathways.
- Evidence anchors:
  - [Section 5.7] "Dynamic focusing is attenuated...attention entropy decreases only modestly (typically 20%-30%) and often non-monotonically across layers" in Mistral
  - [Section 6.1] "The 50+ percentage point gap between MHA models (82-86%) and GQA/sliding-window models (20-31%) indicates that architectural constraints can substantially limit dynamic Bayesian refinement"
  - [corpus] "Neural networks leverage nominally quantum and post-quantum representations" suggests transformers discover belief-state representations over latent variables, compatible with attention-mediated inference but not directly validating the focusing mechanism
- Break condition: If entropy reduction is non-monotonic even in full-sequence MHA, or if calibration correlates more strongly with static geometry than dynamic focusing, the focusing mechanism may be epiphenomenal.

## Foundational Learning

- **Concept: Principal Component Analysis (PCA) and Explained Variance**
  - Why needed here: The paper quantifies manifold collapse via PC1/PC1+PC2 explained variance ratios; understanding that PC1=80% means one direction captures most variance is essential for interpreting the geometry.
  - Quick check question: If PC1=30% and PC2=25%, how many effective dimensions does the manifold have?

- **Concept: Predictive Entropy and Uncertainty Quantification**
  - Why needed here: The core hypothesis links manifold position to predictive entropy (H = -Σ p(x)log p(x)); you must understand entropy as a scalar measure of distributional uncertainty.
  - Quick check question: For a distribution [0.5, 0.5] vs. [0.9, 0.1], which has higher entropy?

- **Concept: Key-Value Attention and Hypothesis Frames**
  - Why needed here: The paper attributes orthogonality to key projection matrices (W_K) and uncertainty encoding to value vectors (W_V outputs); distinguishing these roles is prerequisite for understanding the geometry.
  - Quick check question: In attention, do keys determine what is retrieved or how uncertain the retrieval is?

## Architecture Onboarding

- **Component map**:
  Value extraction -> standardization -> PCA -> PC1/PC1+PC2 and entropy correlation
  Key extraction -> normalization -> mean off-diagonal cosine computation
  Attention extraction -> per-head entropy computation -> layerwise averaging

- **Critical path**:
  1. Stratified entropy sampling (5 domains, 5 quintiles, 15 prompts each) -> ensures geometric results don't depend on hand-picked prompts
  2. Domain-restriction experiment (mixed vs. single-domain) -> tests manifold collapse prediction
  3. SULA in-context learning task -> validates inference-time Bayesian updating along the manifold

- **Design tradeoffs**:
  - MHA vs. GQA: Full MHA yields clearer geometry (orthogonality 0.034-0.13, focusing 82-86%) but 4× KV cache; GQA maintains qualitative structure with weaker signatures (orthogonality 0.15-0.18, focusing 31%)
  - Curated vs. web-scale training: Phi-2 (curated) shows sharpest geometry; Llama (web-scale) shows weaker but detectable structure
  - Depth vs. interpretability: Deeper models develop richer (2D, multi-lobed) manifolds under mixed prompts, harder to interpret but potentially encoding multimodal uncertainty

- **Failure signatures**:
  - Pythia-410M outlier: Near-complete collapse (PC1+PC2 ≈ 99.7%) under mixed prompts, suggesting intrinsic low-dimensionality rather than domain-sensitive geometry
  - Mistral focusing attenuation: Non-monotone entropy reduction, indicating architectural constraints on dynamic refinement
  - Causal dissociation: Ablating entropy axis destroys geometry but not calibration -> geometry is readout, not bottleneck

- **First 3 experiments**:
  1. **Domain-restriction replication**: Run mixed-domain vs. mathematics-only prompts on your target model; expect PC1+PC2 increase of 15-25 points if wind-tunnel correspondence holds
  2. **Entropy-axis correlation**: Compute PC1 of final-layer values across 200 prompts; correlate with next-token entropy; |ρ| > 0.3 indicates meaningful alignment
  3. **Orthogonality baseline comparison**: Extract W_K from layer 8; compute mean off-diagonal cosine; verify it's < 0.2 (trained) vs. ~0.4 (initialization baseline)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the entropy-aligned manifold play a causal role in Bayesian computation, or is it merely a privileged readout of distributed uncertainty processing?
- Basis in paper: [explicit] "Establishing stronger causal claims will require multi-layer or multi-axis ablations, activation patching, or training-time interventions, which we leave for future work."
- Why unresolved: Single-layer axis ablations destroyed entropy geometry but left calibration intact; uncertainty information appears distributed across multiple dimensions and layers.
- What evidence would resolve it: Multi-layer simultaneous ablations of entropy-aligned axes; training-time interventions that prevent manifold formation; activation patching between high- and low-entropy states.

### Open Question 2
- Question: Do Bayesian geometric signatures persist or transform at frontier scale (70B-400B parameters)?
- Basis in paper: [explicit] "validation at 70B-400B scale is necessary to determine whether new geometric effects emerge or whether multi-lobed manifolds continue to dominate mixed-domain settings."
- Why unresolved: Largest dense model tested was 12B; geometric phenomena may change qualitatively at frontier scale.
- What evidence would resolve it: Applying the same geometric extraction protocol to Llama-3-70B, Mixtral-8x22B, and similar frontier checkpoints.

### Open Question 3
- Question: What computational role do 2D and multi-lobed manifolds in deeper models serve?
- Basis in paper: [explicit] "The emergence of 2D or multi-lobed manifolds in deeper or larger models is not yet theoretically understood. These structures may encode multimodal uncertainty, semantic clustering, training-set heterogeneity, or task-mixture effects."
- Why unresolved: Deeper models (Phi-2, Llama) show 2D manifolds while Pythia-410M shows near-1D collapse; the additional dimensions could reflect multiple distinct phenomena.
- What evidence would resolve it: Correlating secondary manifold dimensions with controlled variations in uncertainty modality, semantic category, or task mixture; probing whether different manifold lobes correspond to distinct inference modes.

### Open Question 4
- Question: Do entropy-ordered manifolds emerge in selective state-space architectures (Mamba) and hybrid transformer-SSM designs?
- Basis in paper: [explicit] "Extending the geometric extraction methods developed here to Mamba and hybrid transformer-SSM designs—and verifying whether the same entropy-ordered manifolds emerge—is an important direction for unifying the geometric theory across architectures with content-based routing."
- Why unresolved: Paper I established Mamba implements Bayesian inference via different routing mechanics; whether the same geometric substrate emerges is untested.
- What evidence would resolve it: Extracting value-space geometry from Mamba checkpoints; comparing manifold structure between attention-based and SSM-based routing.

## Limitations
- Core geometric findings hinge on interpreting value-space alignment with entropy as evidence for Bayesian inference rather than memorization or token-level correlations
- Orthogonality findings are robust but limited direct corpus validation for key structure's role in enabling Bayesian-like computation
- Progressive attention focusing shows strong architecture-dependence, suggesting the focusing mechanism is neither necessary nor sufficient for Bayesian inference geometry

## Confidence
- **High confidence**: Value manifold entropy correlation and domain-restriction effects (Llama-3.2-1B shows 22 percentage point increase in PC1+PC2 with mathematics-only prompts; Spearman correlations |ρ| > 0.3 consistently observed)
- **Medium confidence**: Key orthogonality as trained feature (2-10× improvement over initialization baselines; consistent across architectures but limited direct corpus validation)
- **Low confidence**: Progressive attention focusing as Bayesian refinement mechanism (strong architecture-dependence suggests alternative computational pathways; unclear whether focusing causes or reflects uncertainty reduction)

## Next Checks
1. **Alternative uncertainty measures**: Replace predictive entropy with mutual information or epistemic uncertainty estimates; verify value-manifold alignment persists under different uncertainty quantification frameworks
2. **Cross-architecture ablation study**: Systematically compare full MHA, GQA, and sliding-window models of identical scale; determine whether architecture-dependent focusing differences reflect fundamental computational constraints or implementation details
3. **Out-of-distribution generalization**: Test value-manifold geometry on prompts from unseen domains (e.g., legal documents, scientific literature); evaluate whether entropy-aligned structure generalizes beyond training distribution or reflects memorization of token-level patterns