---
ver: rpa2
title: Learning Appearance and Motion Cues for Panoptic Tracking
arxiv_id: '2503.09191'
source_url: https://arxiv.org/abs/2503.09191
tags:
- tracking
- segmentation
- panoptic
- motion
- features
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces MAPT, a novel panoptic tracking method that
  leverages both appearance and motion cues to improve object tracking in dynamic
  environments. Unlike existing approaches that rely primarily on appearance features
  or external optical flow, MAPT integrates a motion-aware tracking head that propagates
  object masks using semantic feature changes and a dedicated appearance-aware tracking
  head that learns motion-enhanced embeddings.
---

# Learning Appearance and Motion Cues for Panoptic Tracking

## Quick Facts
- arXiv ID: 2503.09191
- Source URL: https://arxiv.org/abs/2503.09191
- Reference count: 40
- Key outcome: MAPT achieves 2.04% improvement in PAT score on KITTI-STEP validation set

## Executive Summary
MAPT introduces a panoptic tracking method that leverages both appearance and motion cues to improve object tracking in dynamic environments. Unlike existing approaches that rely primarily on appearance features or external optical flow, MAPT integrates a motion-aware tracking head that propagates object masks using semantic feature changes and a dedicated appearance-aware tracking head that learns motion-enhanced embeddings. The method employs multi-scale deformable convolutions to capture motion offsets and semantic context, and a two-step fusion module that refines instance associations using both motion and appearance information. Extensive evaluations on KITTI-STEP and MOTChallenge-STEP datasets demonstrate that MAPT achieves state-of-the-art performance in panoptic tracking accuracy, outperforming prior methods in maintaining object identities over time.

## Method Summary
MAPT is a panoptic tracking architecture that processes video frames in pairs to maintain consistent object identities across time. The method builds on a RegNetY-8.0GF backbone with 2-way FPN and includes four heads: Semantic (with DPC module), Instance (modified Mask R-CNN), Motion (MAPT_Motion), and Appearance (MAPT_Appearance). The Motion head computes semantic feature differences between frames to predict motion offsets, which are used to propagate instance masks. The Appearance head learns motion-enhanced embeddings by fusing static appearance features with propagated motion features. A two-step fusion module first matches instances by motion proximity, then refines associations using appearance embeddings. The method is trained end-to-end with a combination of instance, semantic, motion, and appearance losses using SGD with momentum 0.9.

## Key Results
- MAPT achieves 2.04% improvement in PAT score on KITTI-STEP validation set
- Motion head alone achieves 61.61% PAT, appearance head alone achieves 59.84% PAT, combined achieves 62.65% PAT
- MAPT is approximately 284x more computationally efficient than FlowNet2 (3.50 GFLOPs vs 993.13 GFLOPs) while achieving comparable PAT scores

## Why This Works (Mechanism)

### Mechanism 1: Motion-Aware Mask Propagation via Semantic Feature Differencing
- Claim: Computing semantic feature differences between frames enables learning motion offsets that propagate instance masks across time.
- Mechanism: MAPT_Motion computes St-1 - St (semantic feature difference), passes through dilated convolutions to predict 4 motion offsets with dilations [1, 3, 6, 12]. These offsets guide multi-scale deformable convolutions that warp previous instance masks toward current frame positions.
- Core assumption: Semantic features capture sufficient scene context to infer object displacement even when appearance changes; objects follow coherent motion patterns between adjacent frames.

### Mechanism 2: Motion-Enhanced Appearance Embeddings via Attention-Weighted Fusion
- Claim: Fusing static appearance features with propagated motion features creates embeddings more robust to visual ambiguity.
- Mechanism: MAPT_Appearance combines RoIAlign features (masked by instance segmentation) with propagated features from MAPT_Motion using learned attention weights Wa and Wp. This produces motion-enhanced appearance features processed through FC layers to generate tracking embeddings.
- Core assumption: Propagated features carry temporal information complementary to static appearance; attention can adaptively weight motion vs. appearance based on scene conditions.

### Mechanism 3: Two-Step Hierarchical Instance Association
- Claim: Sequential matching—first by motion proximity, then by appearance similarity—reduces identity switches compared to single-stage association.
- Mechanism: Step 1 assigns IDs based on propagated mask proximity to current detections. Step 2 resolves unmatched cases using cosine similarity of embeddings when similarity exceeds threshold AND semantic class matches. This provides fallback when motion is ambiguous.
- Core assumption: Motion-based matching works for well-tracked objects; appearance provides backup when masks lack spatial overlap; class consistency is a hard constraint.

## Foundational Learning

- **Multi-Scale Deformable Convolutions**: Used to learn spatial offsets that warp features across frames without explicit optical flow. Quick check: Can you explain how deformable convolutions differ from standard convolutions in handling spatial transformations?

- **Instance Segmentation with Mask R-CNN Architecture**: MAPT builds on Mask R-CNN-style heads for bounding boxes, class predictions, and mask logits. Quick check: How does mask pooling extract instance-specific features from RoIAlign outputs?

- **Siamese/Triplet-Style Embedding Learning**: Appearance head learns embeddings trained with cosine similarity and cross-entropy loss over track pairs. Quick check: How does the cosine similarity loss encourage same-track embeddings to cluster while separating different tracks?

## Architecture Onboarding

- **Component map**: Backbone (RegNetY-8.0GF + 2-way FPN) -> Semantic Head -> Instance Head -> MAPT_Motion -> MAPT_Appearance -> Fusion Module

- **Critical path**: Frame pair through shared backbone → FPN features → Semantic head produces St-1, St → Instance head produces masks Mt-1, Mt → MAPT_Motion: (St-1 - St) + Mt-1 → propagated features → Mt-1→t → MAPT_Appearance: RoI features + Mt + propagated features → embeddings → Fusion: Match Mt with Mt-1→t via motion, refine unmatched via appearance embeddings

- **Design tradeoffs**:
  - External flow vs. internal motion: Paper claims 3.50 GFLOPs (internal) vs. 993.13 GFLOPs (FlowNet2) with comparable PAT—motion head is ~284x more efficient
  - STQ vs. PAT optimization: MAPT optimizes PAT (instance-focused, penalizes ID switches) at some cost to STQ (pixel-level segmentation); choose based on whether identity coherence or pixel accuracy is priority
  - Temporal interval selection: Paper notes training interval significantly affects performance; shorter intervals capture fine motion but may miss long-range associations

- **Failure signatures**:
  - Occlusion-induced ID switches: When object fully occluded, neither motion propagation nor appearance matching recovers track
  - Similar-instance confusion: Multiple visually similar objects may swap IDs if motion cues are ambiguous
  - Motion blur / fast movement: Exceeds deformable convolution receptive field, causing propagation failure

- **First 3 experiments**:
  1. Ablate motion vs. appearance heads: Run MAPT_Motion only, MAPT_Appearance only, and combined on KITTI-STEP validation. Expect combined to outperform either alone.
  2. Compare fusion strategies: Test motion-first vs. appearance-first vs. simultaneous matching on sequences with high occlusion rates. Measure ID switch count and PAT.
  3. Stress test on rapid motion: Subsample video frames to simulate faster object movement. Identify motion head break point where deformable convolutions fail to capture displacement.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can adaptive temporal interval selection be integrated into the training pipeline to optimize mask propagation across varying object velocities?
- Basis in paper: The authors state that the choice of temporal interval significantly affects tracking and propose "adaptive interval selection" as future work.
- Why unresolved: Fixed intervals may fail for fast-moving objects or low frame rates, but a dynamic mechanism has not yet been developed.
- What evidence would resolve it: A dynamic scheduler that adjusts intervals based on motion magnitude, yielding higher PAT scores.

### Open Question 2
- Question: Why does MAPT underperform on dense pedestrian datasets (MOTChallenge-STEP) compared to automotive benchmarks?
- Basis in paper: Despite SOTA results on KITTI-STEP, MAPT trails state-of-the-art methods on MOTChallenge-STEP.
- Why unresolved: The two-step fusion module may struggle with the high occlusion rates and small object sizes inherent in crowded pedestrian scenes.
- What evidence would resolve it: Ablation studies isolating the fusion module's performance in high-occlusion scenarios or specific tuning for crowded classes.

### Open Question 3
- Question: Can the inverse relationship between tracking consistency (PAT) and segmentation quality (STQ) be mitigated to surpass state-of-the-art in both simultaneously?
- Basis in paper: MAPT achieves higher PAT but lower STQ than Video-kMaX, suggesting a trade-off in the current optimization.
- Why unresolved: The current fusion strategy prioritizes instance coherence, potentially at the expense of pixel-level segmentation boundaries.
- What evidence would resolve it: A unified loss function or architecture modification that closes the STQ gap without reducing PAT.

## Limitations
- Temporal interval sensitivity: The paper acknowledges that temporal interval τ significantly impacts performance, but specific values for training and inference are not provided.
- Limited cross-dataset generalization: MAPT achieves SOTA on KITTI-STEP but trails on MOTChallenge-STEP, suggesting dataset-specific limitations.
- External validation constraints: Claims about computational efficiency are based on FLOPs comparison without real-world latency measurements.

## Confidence
- **High Confidence**: Core architectural design combining motion-aware and appearance-aware tracking heads is technically sound and well-documented.
- **Medium Confidence**: 2.04% PAT improvement on KITTI-STEP validation is significant but cannot be fully validated against current SOTA without test set access.
- **Low Confidence**: Claims about computational efficiency (284x faster than FlowNet2) are based on FLOPs comparison without considering real-world inference latency or memory usage.

## Next Checks
1. **Temporal Interval Sweep**: Systematically evaluate MAPT performance across different temporal intervals (τ = 1, 2, 3, 5) on KITTI-STEP validation to identify optimal interval and quantify sensitivity.
2. **Failure Case Analysis**: Create a benchmark of challenging sequences featuring rapid motion, severe occlusion, and similar object confusion. Measure ID switches and tracking fragmentation to identify architectural limitations.
3. **Cross-Dataset Generalization**: Test MAPT pre-trained on KITTI-STEP on other tracking datasets (e.g., BDD100K, Waymo Open Dataset) without fine-tuning to assess domain generalization capabilities.