---
ver: rpa2
title: 'Efficiently Integrate Large Language Models with Visual Perception: A Survey
  from the Training Paradigm Perspective'
arxiv_id: '2502.01524'
source_url: https://arxiv.org/abs/2502.01524
tags:
- tuning
- visual
- language
- training
- vision
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This survey systematically categorizes and reviews 34 Vision Large
  Language Models (VLLMs) based on their training paradigms and parameter-efficient
  strategies. The three primary paradigms are: Single-stage Tuning (integrating vision
  with frozen LLMs), Two-stage Tuning (adding instruction tuning to improve generalization),
  and Direct Adaptation (fine-tuning only on downstream tasks).'
---

# Efficiently Integrate Large Language Models with Visual Perception: A Survey from the Training Paradigm Perspective

## Quick Facts
- **arXiv ID:** 2502.01524
- **Source URL:** https://arxiv.org/abs/2502.01524
- **Reference count:** 40
- **Primary result:** Systematic survey categorizing 34 VLLMs by training paradigms (Single-stage, Two-stage, Direct Adaptation) and evaluating parameter-efficient strategies

## Executive Summary
This survey provides a comprehensive analysis of Vision Large Language Models (VLLMs) through the lens of training paradigms and parameter-efficient adaptation. The authors systematically categorize 34 VLLMs into three training paradigms based on how they integrate visual modalities with frozen or partially tuned LLMs. The study reveals that Two-stage Tuning consistently outperforms other approaches across standard benchmarks, while highlighting the critical role of Modality Integrators in balancing efficiency and visual information preservation. The work offers both theoretical insights and practical guidance for researchers developing VLLMs, particularly regarding the trade-offs between performance, parameter efficiency, and architectural design choices.

## Method Summary
The authors conducted a systematic survey of 34 VLLMs, categorizing them by training paradigm and parameter-efficient adaptation strategy. For empirical validation, they replicated results for Direct Adaptation models (VL-Adapter, LST, VL-PET, LaVIN, MemVP) using T5-base (220M) as the primary LLM. Experiments were single-run outcomes on A100 GPUs, utilizing official codebases for LaVIN, MemVP, and VL-PET, and the LST repository for others. The survey evaluated performance across standard vision-language benchmarks (VQAv2, GQA, COCO Captions) and analyzed efficiency metrics including trainable parameters and peak memory usage.

## Key Results
- Two-stage Tuning achieves superior performance by separating modality alignment from capability enhancement
- Instruction tuning proves highly effective for unlocking zero-shot transfer and user-intention understanding
- Parameter-efficient adaptation (LoRA, adapters, prefix tuning) can match or exceed full fine-tuning while training only modality-relevant components

## Why This Works (Mechanism)

### Mechanism 1
Two-stage Tuning achieves superior VLLM performance by explicitly separating modality alignment from capability enhancement. Stage 1 pretrains the Modality Integrator on image-text pairs to align visual embeddings with LLM semantic space. Stage 2 applies instruction tuning or multi-task learning to unlock zero-shot transfer and user-intention understanding. The separation allows the MI to first establish cross-modal grounding, then refine task behavior without destabilizing the alignment. Core assumption: Frozen pretrained LLMs already encode sufficient reasoning capability; the bottleneck is modality grounding and instruction following.

### Mechanism 2
The Modality Integrator architecture determines the tradeoff between parameter efficiency and visual information preservation. Out-of-block Integrators (e.g., Q-Former, Resampler, Convolution-based Abstractors) transform visual embeddings before LLM input, controlling token length and dimensionality. Attention-based abstractors compress visual tokens but may lose spatial context; linear/dense projectors preserve context but lack output flexibility; convolution-based abstractors balance both by retaining local structure while enabling downsampling. Core assumption: Visual embeddings from the encoder contain redundant information that can be compressed without critical semantic loss.

### Mechanism 3
Parameter-efficient adaptation can match or exceed full fine-tuning by isolating trainable parameters to modality-relevant components. Instead of updating all LLM weights, PEA methods (LoRA, adapters, prefix tuning) constrain updates to low-rank decompositions, bottleneck modules, or soft prompts. In Direct Adaptation, only the MI is trained on downstream tasks via multi-task learning, leveraging shared visual representations. This prevents catastrophic forgetting of LLM knowledge while adapting to VL tasks. Core assumption: The frozen LLM backbone provides sufficiently general language reasoning; adaptation only requires injecting modality-specific transformations.

## Foundational Learning

- **Concept:** Transformer attention mechanisms and residual connections
  - Why needed here: The entire VLLM architecture builds on transformer blocks; understanding how queries, keys, values interact is essential for grasping both Out-of-block (Resampler, Q-Former) and In-block (Attention-based Adapter) integrators.
  - Quick check question: Can you explain how cross-attention differs from self-attention in processing visual and textual tokens?

- **Concept:** Parameter-efficient fine-tuning (LoRA, Adapters, Prefix Tuning)
  - Why needed here: The survey categorizes VLLMs by their PEA strategies; without this foundation, you cannot evaluate tradeoffs between tuning only MI vs. adding LoRA vs. full fine-tuning.
  - Quick check question: Given a model with dimension d=4096, rank r=8, how many trainable parameters does LoRA add per weight matrix compared to full fine-tuning?

- **Concept:** Vision encoders (ViT vs. ResNet) and CLIP pretraining
  - Why needed here: The choice of vision encoder determines the input embedding format (patches vs. features) and whether the encoder already has multimodal alignment (CLIP) or is unimodal, directly affecting MI design requirements.
  - Quick check question: Why might a CLIP-pretrained vision encoder reduce the training burden on the MI compared to a unimodal ImageNet-pretrained encoder?

## Architecture Onboarding

- **Component map:**
  ```
  [Image] → [Vision Encoder (CLIP ViT/ResNet)] → X_v (visual embeddings)
                                                ↓
                                        [Modality Integrator]
                                        ├── Out-of-block: Q-Former/Resampler/Linear/Conv
                                        └── In-block: Adapter/Prefix/LoRA (optional)
                                                ↓
                                        P_v (aligned visual tokens)
                                                ↓
  [Text] → [Tokenizer] → X_t ──────────→ [Concat: P_v; X_t] → [Frozen/Partially Tuned LLM] → Output
  ```

- **Critical path:**
  1. Select LLM backbone based on paradigm: GPT/Chinchilla for Single-stage (historical), LLaMA/Vicuna 7B-13B for Two-stage, T5/BART or small LLaMA for Direct Adaptation.
  2. Select vision encoder: CLIP ViT-L/14 is dominant; consider EVA-CLIP for larger scale or ResNet for efficiency.
  3. Design MI: Start with Linear Projector (simplest), evaluate Resampler if token compression needed, consider Conv-based (MobileVLM) for efficiency.
  4. Choose training paradigm based on constraints: Two-stage for performance (requires instruction data), Direct Adaptation for efficiency (multi-task only), Single-stage for rapid prototyping.

- **Design tradeoffs:**
  - Performance vs. Parameters: Two-stage with LoRA achieves best benchmarks but requires 4-7% of LLM parameters; Direct Adaptation with adapter-only achieves 95%+ of performance with <1% parameters.
  - Token Compression vs. Spatial Fidelity: Resamplers reduce visual tokens from O(256) to O(64) but may lose small objects; Linear projectors preserve all tokens, increasing inference cost.
  - Frozen vs. Tuned LLM: Frozen LLM preserves generalization but limits task-specific adaptation; LoRA provides middle ground with rank-r updates.

- **Failure signatures:**
  - Hallucination on unseen image types: MI undertrained or instruction data lacks diversity (Section 5.3 notes RLHF helps reduce hallucinations).
  - Poor OCR performance: Attention-based abstractors compressing away text regions; switch to Linear or add text-specific instruction data.
  - Catastrophic forgetting: Full LLM fine-tuning on small VL datasets; switch to LoRA or adapter-only.
  - Memory overflow during training: Backpropagating through frozen LLM; use LST side-network or QLoRA quantization.

- **First 3 experiments:**
  1. **Baseline MI comparison:** Implement Linear Projector vs. Q-Former on LLaVA-style architecture with frozen Vicuna-7B; evaluate on VQAv2 and GQA to quantify compression vs. preservation tradeoff.
  2. **Paradigm ablation:** Train same MI architecture under Single-stage (pretrain only), Two-stage (pretrain + 158K instruction tuning), and Direct Adaptation (multi-task on VQAv2/GQA/COCO); compare parameter counts and benchmark scores.
  3. **PEA method comparison:** For Two-stage setup, compare LoRA (r=64), DoRA, and adapter-only tuning on LLaMA-7B; measure peak memory, trainable parameters, and performance delta on MMBench and MME to validate efficiency claims from Table 7.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can Convolution-based Abstractors function effectively as the sole tunable component in Single-stage Tuning to balance parameter efficiency and spatial context preservation?
- **Basis in paper:** The authors note that while Convolution-based Abstractors are efficient and adaptable, there is a distinct "lack of initiatives to tune only the Convolution-based Abstractors" specifically within the Single-stage Tuning paradigm (Section 5.2).
- **Why unresolved:** Current Single-stage methods primarily use Attention-based Abstractors (Resamplers) or Dense Projectors. The potential for convolution-based designs to capture local context more effectively than attention-based methods (which may ignore insignificant objects) while offering more flexibility than Dense Projectors remains unexplored in this specific training context.
- **What evidence would resolve it:** Comparative experiments showing a VLLM using only a tunable Convolution-based Abstractor achieving competitive performance against Resampler-based models on standard benchmarks (e.g., COCO, VQAv2) with lower parameter counts.

### Open Question 2
- **Question:** How can efficient Modality Integrator (MI) designs be adapted for billion-parameter LLMs (e.g., LLaMA) within the Direct Adaptation paradigm without requiring the extensive pretraining of Two-stage Tuning?
- **Basis in paper:** The conclusion states that "Applying efficient MI designs to larger LLMs is becoming increasingly necessary." Additionally, Section 5.4 highlights that while Direct Adaptation typically uses smaller LLMs (T5/BART), there is a "trend... to adapt LLaMA models" which requires resolving the trade-off between model scale and the parameter efficiency of the integrator.
- **Why unresolved:** Efficient tuning techniques developed for smaller encoder-decoder models (like T5) may not scale linearly or effectively to larger decoder-only models without pretraining, and the efficiency of "Direct Adaptation" is challenged by the sheer size of modern foundational LLMs.
- **What evidence would resolve it:** A study demonstrating that parameter-efficient integrators (like Bottleneck Adapters or LST) applied to 7B+ models in a Direct Adaptation setting can achieve performance parity with Two-stage models while maintaining significantly lower training costs.

### Open Question 3
- **Question:** To what extent can Instruction Tuning be successfully incorporated into the Direct Adaptation paradigm to match the zero-shot generalization of Two-stage Tuning?
- **Basis in paper:** The survey concludes that while Instruction Tuning is standard in Two-stage Tuning, "there are new attempts to incorporate it into Direct Adaptation," citing LaVIN as a pioneer.
- **Why unresolved:** It is currently unclear if the data efficiency and generalization benefits of Instruction Tuning can be fully realized without the pre-training alignment phase characteristic of Two-stage Tuning, or if the "Direct Adaptation" approach limits the model's ability to follow complex instructions effectively.
- **What evidence would resolve it:** Evaluations of Direct Adaptation models utilizing Instruction Tuning on advanced reasoning benchmarks (e.g., MMBench, SEED-I) to see if they can close the performance gap with Two-stage models like LLaVA 1.5.

## Limitations

- **Empirical validation constraints:** The survey's performance claims rely on single-run replications with T5-base as the primary LLM, lacking variance analysis across multiple seeds.
- **Dataset coverage limitations:** Benchmark evaluations focus on standard datasets (VQAv2, GQA, COCO) that may not represent real-world deployment scenarios or edge cases.
- **Practical deployment gaps:** The analysis emphasizes parameter efficiency but does not address inference latency, memory constraints, or edge deployment requirements that determine real-world adoption.

## Confidence

- **High Confidence:** The categorization framework itself (Single-stage, Two-stage, Direct Adaptation) and the identification of Modality Integrator as a critical architectural component are well-supported by the corpus and design patterns observed across 34 VLLMs.
- **Medium Confidence:** Claims about specific performance rankings (Two-stage > Single-stage > Direct Adaptation) and the effectiveness of instruction tuning are supported by benchmark data but limited by single-run replications and narrow dataset coverage.
- **Low Confidence:** Assertions about generalizability to downstream applications (VLA, embodied AI) are speculative, as the survey does not empirically validate cross-task transfer or deployment performance in embodied settings.

## Next Checks

1. **Multi-run benchmarking:** Replicate the Direct Adaptation evaluation across at least 3 random seeds to establish variance bounds and validate the single-run performance claims in Table 7/11.

2. **Cross-dataset generalization:** Test the best-performing Two-stage models on non-standard VL datasets (e.g., NLVR2, OK-VQA) to assess whether instruction tuning's effectiveness generalizes beyond the survey's benchmark suite.

3. **Edge deployment analysis:** Measure inference latency and memory usage for top-performing VLLMs on mobile/embedded hardware to validate the practical relevance of parameter efficiency claims for real-world applications.