---
ver: rpa2
title: 'Technologies on Effectiveness and Efficiency: A Survey of State Spaces Models'
arxiv_id: '2503.11224'
source_url: https://arxiv.org/abs/2503.11224
tags:
- mamba
- state
- arxiv
- ssms
- space
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper provides a comprehensive survey of State Space Models
  (SSMs), tracing their evolution from the original SSM through structured SSMs like
  S4 to selective SSMs exemplified by Mamba. It details key techniques that address
  effectiveness (e.g., HiPPO for long-range memory, selectivity for input-dependent
  parameterization) and efficiency (e.g., DPLR structure, semiseparable matrices,
  hardware-aware state expansion).
---

# Technologies on Effectiveness and Efficiency: A Survey of State Spaces Models

## Quick Facts
- arXiv ID: 2503.11224
- Source URL: https://arxiv.org/abs/2503.11224
- Reference count: 40
- Primary result: Comprehensive survey of State Space Models evolution from original SSM through S4 to Mamba, detailing effectiveness techniques (HiPPO, selectivity) and efficiency techniques (DPLR, semiseparable matrices)

## Executive Summary
This survey provides a comprehensive overview of State Space Models (SSMs), tracing their evolution from the original continuous-time SSM formulation through structured variants like S4 to selective models like Mamba. The paper details key techniques that address effectiveness (e.g., HiPPO for long-range memory, selectivity for input-dependent parameterization) and efficiency (e.g., DPLR structure, semiseparable matrices, hardware-aware state expansion). It covers mathematical foundations, architectural developments, and relationships with other models like RNNs, CNNs, and Transformers, while presenting diverse real-world applications across domains such as video understanding, speech processing, molecular modeling, 3D signal analysis, time series forecasting, and structured data modeling.

## Method Summary
The survey synthesizes theoretical developments in SSMs through three evolutionary stages: (1) Original SSM with discretization methods (Euler, ZOH, Bilinear), (2) S4 with HiPPO initialization using LegS matrices and DPLR structure enabling FFT-based convolution for O(N + L) complexity, and (3) Mamba with selective SSM featuring input-dependent parameterization via linear layers, hardware-aware state expansion in GPU SRAM, and semiseparable matrices in Mamba2. The paper covers mathematical foundations including continuous-to-discrete transformation, convolutional-recurrent duality, and low-rank matrix identities, while examining applications across multiple domains without presenting original experimental results.

## Key Results
- SSMs evolved from basic recurrence to structured forms (S4) enabling parallel training via convolutional reformulation
- HiPPO initialization provides theoretically grounded state matrix structures for long-range dependency modeling
- Selective SSMs (Mamba) achieve content-aware filtering through input-dependent parameterization while maintaining computational efficiency
- Hybrid architectures (Jamba, Samba) balance SSM efficiency with Transformer recall capabilities for improved performance
- SSMs demonstrate broad applicability across video, speech, molecular, 3D, time series, and structured data domains

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Structured parameter initialization via HiPPO may improve long-range dependency capture in SSMs.
- Mechanism: The HiPPO framework derives optimal state matrix structures (particularly LegS) that project input history onto orthogonal polynomial bases. This provides theoretically grounded initialization for the A and B matrices, potentially mitigating exponential memory decay common in recurrent systems.
- Core assumption: The optimal polynomial approximation of continuous signal history transfers effectively to discrete sequence modeling tasks.
- Evidence anchors:
  - [abstract]: "HiPPO for long-range memory" listed as key technique for effectiveness
  - [section 3.2]: "HiPPO can be conceptualized as a system: given a function f(t) and a measure μ(t), it outputs the optimal representation coefficients c(t) of f(t) under μ(t)"
  - [corpus]: "The Effect of Depth on the Expressivity of Deep Linear State-Space Models" examines expressivity but does not directly validate HiPPO optimality claims
- Break condition: If trained parameters deviate significantly from HiPPO initialization, performance may degrade. Tasks requiring memory beyond the polynomial approximation capacity may fail.

### Mechanism 2
- Claim: DPLR structure combined with frequency-domain computation enables parallel training with reduced complexity.
- Mechanism: Constraining the state matrix A to Diagonal Plus Low-Rank form (A = Λ − PQ*) allows the convolutional kernel K to be computed via FFT. The Woodbury identity and Cauchy kernel reduce matrix inversion to diagonal operations, enabling O(L + N) complexity instead of O(LN²).
- Core assumption: The DPLR constraint does not severely limit the expressiveness needed for practical sequence tasks.
- Evidence anchors:
  - [abstract]: "DPLR structure" cited as efficiency technique
  - [section 3.3]: "S4 constrains the parameter matrices to adhere to the DPLR structure... achieves parallel computation and reduces training complexity to O(N(Ñ + L̃))"
  - [corpus]: Limited corpus validation; QS4D paper discusses quantization for structured SSMs but does not independently verify DPLR efficiency claims
- Break condition: Very long sequences may still face numerical stability issues. Non-NPLR-compatible matrices cannot use this approach directly.

### Mechanism 3
- Claim: Input-dependent parameterization in selective SSMs enables content-aware filtering but invalidates convolutional efficiency.
- Mechanism: By making Δ, B, and C functions of the input x, selective SSMs can weight important tokens more heavily while suppressing noise. However, this breaks time-invariance, preventing the convolutional reformulation. Mamba addresses this through hardware-aware computation (SRAM-based recurrence) and Mamba2 through semiseparable matrix decomposition.
- Core assumption: GPU SRAM is sufficiently large for typical sequence segments, and recomputation overhead is acceptable.
- Evidence anchors:
  - [abstract]: "selectivity for input-dependent parameterization" cited as effectiveness technique
  - [section 4.2-4.3]: "By correlating model parameters with the input, the selective SSM has the ability to assign higher parameter weights to key inputs"
  - [corpus]: "Understanding and Enhancing Mamba-Transformer Hybrids" analyzes hybrid architectures, providing indirect support for selective mechanisms' utility in recall tasks
- Break condition: Extremely long sequences exceeding SRAM capacity require segmentation, potentially losing cross-segment dependencies. Tasks requiring exact token copying may underperform compared to attention (per corpus: "Transformers are better than state space models at copying").

## Foundational Learning

- Concept: **Continuous-to-Discrete Transformation via ZOH/Bilinear Methods**
  - Why needed here: SSMs originate from continuous ODEs; understanding discretization (Equations 9-11) is essential for grasping how SSMs process token sequences.
  - Quick check question: Can you explain why ZOH discretization yields Ā = exp(ΔA) and how Δ affects the temporal resolution?

- Concept: **Convolutional-Recurrent Duality**
  - Why needed here: The same SSM can be expressed as recurrence (Equation 7) or convolution (Equation 13). This duality underpins efficiency gains in structured SSMs.
  - Quick check question: Given K = (CB, CAB, CA²B, ...), how would you compute the output y for input x using convolution?

- Concept: **Low-Rank Matrix Identities (Woodbury)**
  - Why needed here: DPLR efficiency hinges on the Woodbury identity (Equation 20) for inverting structured matrices.
  - Quick check question: For Λ diagonal and P, Q rank-1 vectors, how does (Λ + PQ*)⁻¹ simplify?

## Architecture Onboarding

- Component map:
  - Input Projection -> SSM Core -> Discretization Layer -> Output Projection
  - (with optional Skip Connection from Input to Output)

- Critical path:
  1. Initialize A with HiPPO-LegS (Section 3.2, Equation 17)
  2. Apply DPLR constraint: A = Λ − PQ* (Section 3.3)
  3. For selective SSM: derive Δ, B, C from input via linear layers (Section 4.2)
  4. Compute discretized Ā, B̄ using ZOH (Equation 10)
  5. For inference: sequential recurrence; for training: convolutional mode or hardware-aware scan

- Design tradeoffs:
  - S4 vs. Mamba: S4 enables convolutional parallelism but lacks content-awareness; Mamba adds selectivity but requires recurrent computation
  - Diagonal vs. DPLR: DSS/S4D simplify to diagonal (faster) but may lose expressiveness
  - Hybrid vs. Pure: Mamba-Transformer hybrids (Jamba, Section 4.5) balance efficiency and recall capability

- Failure signatures:
  - Training instability: Real parts of Λ becoming positive (Section 3.4 suggests constraints)
  - Long-segment overflow: SRAM limits exceeded during Mamba's hardware-aware expansion
  - Poor copy performance: Selective SSMs may struggle with exact token repetition tasks

- First 3 experiments:
  1. Baseline S4 on Long Range Arena (LRA): Implement S4 with HiPPO initialization and DPLR structure; verify ~80% accuracy benchmark cited in Section 3.4
  2. Selective vs. Non-Selective Ablation: Compare Mamba (input-dependent Δ, B, C) against time-invariant variant on a text classification task; measure both accuracy and inference latency
  3. Memory Profile Analysis: Profile GPU HBM vs. SRAM usage for Mamba across sequence lengths (512, 2048, 8192 tokens) to validate the claimed IO complexity reduction from O(BLDN) to O(BLD)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can Selective SSMs be theoretically or architecturally modified to overcome the limitation of fixed-size latent states when performing tasks that require copying exponential-length strings?
- Basis in paper: [explicit] Section 4.6 notes that while Transformers can copy strings of exponential length, "SSMs are constrained by their fixed-size latent states," causing them to underperform in efficiency and generalization on such tasks.
- Why unresolved: The recurrent nature of SSMs compresses history into a fixed state vector $h_k$, creating an information bottleneck for specific algorithmic tasks like exact string copying that Transformers handle via growing KV caches.
- What evidence would resolve it: A theoretical proof or empirical demonstration of an SSM variant that achieves parity with Transformers on the "Repeat-after-me" copying tasks described in Jelassi et al. (2024) without increasing state size linearly with sequence length.

### Open Question 2
- Question: What are the optimal design patterns for integrating "n-gram heads" into Selective SSMs to improve in-context learning (ICL) capabilities?
- Basis in paper: [explicit] Section 4.6 cites Akyürek et al. (2024), identifying "n-gram heads" as a key factor for Transformer success in ICL. The paper notes that incorporating these heads into SSMs improves performance, implying the standard mechanism is currently lacking.
- Why unresolved: While the paper notes that incorporation helps, it does not define the standard mechanism for doing so within the linear recurrence structure of SSMs, nor does it analyze the computational overhead this introduces.
- What evidence would resolve it: Comparative benchmarks showing that SSMs with integrated n-gram heads match or exceed Transformers on sparse parity learning and regression tasks while retaining the linear inference complexity characteristic of SSMs.

### Open Question 3
- Question: What are the theoretical guidelines for determining the optimal ratio and placement of SSM layers versus Attention layers in hybrid architectures (e.g., Jamba)?
- Basis in paper: [inferred] Section 4.5 reviews hybrid models like Jamba, Samba, and MambaFormer which mix Mamba and Transformer layers, but the survey presents these as experimental architectures without establishing a unified theory for their optimal configuration.
- Why unresolved: The survey describes various hybrid setups (e.g., replacing attention in GAU, mixing layers with MoE) but leaves open the question of how to systematically balance the quadratic complexity of attention against the state-compression of SSMs for specific data modalities.
- What evidence would resolve it: An ablation study across diverse modalities (text, video, time-series) that correlates the ratio of SSM-to-Attention layers with performance metrics on long-range dependency tasks versus computational cost.

## Limitations
- Limited empirical validation of efficiency claims (O(N + L) complexity) and effectiveness benchmarks
- Proprietary implementation details for Mamba's hardware-aware computation prevent independent verification
- Does not address domain-specific limitations or failure modes beyond basic numerical instability
- Relies heavily on claims from original papers without independent verification of performance metrics

## Confidence
- High confidence: Mathematical foundations (continuous-to-discrete transformation, convolutional-recurrent duality, low-rank matrix identities)
- Medium confidence: Structured SSM techniques (HiPPO initialization, DPLR structure, FFT-based convolution)
- Low confidence: Selective SSM mechanisms and hardware-aware optimization

## Next Checks
1. **Complexity validation**: Implement S4 with DPLR structure and measure actual runtime complexity on sequences of varying lengths (10³ to 10⁶ tokens) to verify claimed O(N + L) scaling
2. **Memory usage profiling**: Instrument Mamba implementation to track SRAM vs HBM usage across sequence lengths, confirming the claimed transition from O(BLDN) to O(BLD) complexity through hardware-aware computation
3. **Cross-domain Generalization test**: Evaluate S4/Mamba on both LRA benchmark and a structured data task (e.g., molecular modeling) to validate the survey's claim of broad applicability across domains