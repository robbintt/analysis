---
ver: rpa2
title: 'Kairos: Towards Adaptive and Generalizable Time Series Foundation Models'
arxiv_id: '2509.25826'
source_url: https://arxiv.org/abs/2509.25826
tags:
- time
- series
- patch
- information
- each
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes Kairos, a time series foundation model designed
  to address the limitations of existing approaches in handling heterogeneous time
  series data with varying information densities and temporal dynamics. The core innovations
  include Mixture-of-Size Dynamic Patching (MoS-DP), which adaptively tokenizes time
  series using multiple patch sizes based on local information density, and Instance-Adaptive
  Rotary Position Embedding (IARoPE), which modulates positional encodings based on
  spectral features extracted from each time series instance.
---

# Kairos: Towards Adaptive and Generalizable Time Series Foundation Models

## Quick Facts
- arXiv ID: 2509.25826
- Source URL: https://arxiv.org/abs/2509.25826
- Reference count: 40
- Kairos base (50M) achieves best MASE and second-best CRPS on GIFT-Eval zero-shot forecasting benchmark

## Executive Summary
Kairos is a time series foundation model designed to address limitations of existing approaches in handling heterogeneous time series data with varying information densities and temporal dynamics. The model introduces two key innovations: Mixture-of-Size Dynamic Patching (MoS-DP) for adaptive tokenization based on local information density, and Instance-Adaptive Rotary Position Embedding (IARoPE) for instance-specific temporal relationship modeling. Trained on a large-scale Predictability-Stratified Time Series corpus, Kairos achieves superior zero-shot forecasting performance on GIFT-Eval and Time-Series-Library benchmarks while maintaining a compact parameter footprint.

## Method Summary
Kairos tokenizes time series using MoS-DP, which adaptively selects from predefined patch sizes (32, 64, 128) based on local information density through a router mechanism. The resulting embeddings are processed by a Transformer encoder with IARoPE, which modulates positional encodings using spectral features extracted from each time series instance via FFT. The model employs a multi-patch prediction strategy during inference, simultaneously forecasting multiple patches to reduce cumulative autoregressive error. Kairos is trained on a large-scale Predictability-Stratified Time Series corpus using weighted quantile loss, achieving strong performance on GIFT-Eval and Time-Series-Library benchmarks with significantly fewer parameters than competing approaches.

## Key Results
- Kairos base (50M) achieves best MASE and second-best CRPS on GIFT-Eval zero-shot forecasting benchmark
- Kairos mini (10M) surpasses recent advanced time series foundation models and full-shot deep learning models on TSLib
- Outperforms established methods with significantly fewer parameters while maintaining strong generalization across heterogeneous time series data

## Why This Works (Mechanism)

### Mechanism 1: Mixture-of-Size Dynamic Patching (MoS-DP)
- Claim: Adaptive patch sizing based on local information density improves tokenization of heterogeneous time series
- Mechanism: Router selects from predefined patch sizes {32, 64, 128} per coarsest patch segment; activated experts (FFNs) extract features at assigned granularity; final embeddings are weighted fusions aggregating information from finest selected patch and ancestor patches
- Core assumption: Information density varies meaningfully within and across time series; smaller patches capture abrupt changes better while larger patches suffice for stable regions
- Evidence anchors: Visualization shows smaller weighted average patch sizes correlate with abrupt changes; ablation studies demonstrate superiority over fixed patching

### Mechanism 2: Instance-Adaptive Rotary Position Embedding (IARoPE)
- Claim: Modulating RoPE frequencies per instance via spectral features improves temporal relationship modeling
- Mechanism: Extract low-frequency FFT magnitudes from input series; MLP predicts per-layer modulation parameters applied in log-space to base frequencies; adapted frequencies define rotation angles in RoPE
- Core assumption: Low-frequency spectral features capture dominant periodicity/trend structure relevant to positional encoding; instance-specific modulation generalizes better than fixed or dataset-learned θ
- Evidence anchors: Causal analysis shows IARoPE outperforms Intra-Dataset Shuffle, Inter-Dataset Shuffle, and Fixed RoPE baselines

### Mechanism 3: Multi-Patch Prediction
- Claim: Simultaneously forecasting multiple patches reduces cumulative autoregressive error
- Mechanism: J learnable forecast tokens are decoded in parallel, each predicting a patch of horizon H/J; outputs are concatenated
- Core assumption: Predicting shorter patches in parallel is easier than predicting long horizon autoregressively; trade-off between task difficulty and autoregression steps has sweet spot
- Evidence anchors: Ablation shows J=2 offers best overall performance across short/medium/long horizons

## Foundational Learning

- Concept: Rotary Position Embedding (RoPE)
  - Why needed here: IARoPE builds directly on RoPE's rotation-based relative position encoding; understanding base mechanism is prerequisite to grasping adaptive modulation
  - Quick check question: Can you explain how RoPE encodes relative position via rotation matrices and why dot products after rotation depend only on relative distance?

- Concept: Mixture-of-Experts (MoE) routing
  - Why needed here: MoS-DP adapts MoE routing concepts to patch-level tokenization; top-K selection, gating values, and load balancing are directly borrowed
  - Quick check question: Given affinity scores s_{n,i} for a patch across S experts, how would you compute normalized gating values and select top-K?

- Concept: Spectral analysis (FFT) for time series
  - Why needed here: IARoPE extracts frequency-domain features to modulate positional encodings; interpreting low-frequency magnitudes is essential
  - Quick check question: Given a time series, what does the magnitude of its first few FFT coefficients typically represent, and why might low frequencies be preferred for capturing periodicity?

## Architecture Onboarding

- Component map: Input → MoS-DP (coarsest partition → Dynamic Patch Router → Dynamic Patch Fusion) → fused embeddings E → Transformer encoder with IARoPE → Encoder output → Transformer decoder with J learnable forecast tokens → Decoder output → shared ResidualFFN head → J patch predictions concatenated

- Critical path:
  1. Correct computation of ancestor function A(x_{n,m}^{p_k}, p_i) is essential for fusion
  2. IARoPE log-space modulation must match dimension pairs (D_h/2) and use layer-specific (γ, β)
  3. Load balancing bias updates must run per batch to maintain target distribution τ

- Design tradeoffs:
  - Patch sizes {32, 64, 128}: Smaller sizes increase tokens and compute; larger sizes may miss fine details
  - Number of forecast patches J: J=2 trades off autoregression reduction vs. prediction difficulty
  - Null experts Z: Increase flexibility but require careful load balancing to avoid under-training real experts
  - Predictability stratification in PreSTS: Higher-quality supervision vs. reduced coverage of edge cases

- Failure signatures:
  - Router collapse: All patches routed to one patch size; check bias update (η_b) and target distribution τ
  - IARoPE instability: Exploding/vanishing θ'; check log-space implementation and LayerNorm on FFT features
  - Degraded long-horizon performance: May indicate J too small or cumulative error; increase J or inspect autoregressive fallback logic
  - Slow convergence: May indicate overly aggressive predictability filtering or insufficient synthetic data diversity

- First 3 experiments:
  1. Ablate MoS-DP vs. fixed patching (size 64) on heterogeneous dataset; measure MASE/CRPS and inspect patch size distribution
  2. Ablate IARoPE vs. standard RoPE on datasets with varying periodicity; check if performance gap widens with greater frequency diversity
  3. Sweep forecast patch count J ∈ {1, 2, 4, 6} on short/medium/long horizons; reproduce Appendix A curve to confirm J=2 optimum

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but the methodology raises several important unresolved issues regarding task generalization, scalability, optimal parameter choices, and architectural alternatives.

## Limitations
- MoS-DP effectiveness depends on meaningful variation in information density; on uniform-density datasets, dynamic routing overhead may provide marginal benefit
- IARoPE performance relies on spectral features capturing relevant temporal dynamics; datasets with low intra-series frequency variability may see diminished returns
- Multi-patch prediction strategy's optimal J=2 finding may not generalize across all forecasting horizons and domains

## Confidence
- High confidence in MoS-DP's core mechanism and its ability to adapt patch sizing based on local features, supported by visualization evidence
- Medium confidence in IARoPE's generalization benefits, as causal analysis demonstrates superiority over ablation baselines but external validation is limited
- Medium confidence in multi-patch prediction improvements, given ablation results but lack of independent verification of J=2 optimization

## Next Checks
1. Conduct ablation studies on datasets with known uniform information density to verify whether MoS-DP provides measurable benefits over fixed patching in such scenarios
2. Test IARoPE on synthetic time series with controlled frequency profiles to determine the relationship between spectral variability and performance gains
3. Perform horizon-specific analysis of multi-patch prediction across diverse domains to identify whether J=2 remains optimal or if domain-specific tuning is required