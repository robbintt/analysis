---
ver: rpa2
title: ASR Error Correction in Low-Resource Burmese with Alignment-Enhanced Transformers
  using Phonetic Features
arxiv_id: '2511.21088'
source_url: https://arxiv.org/abs/2511.21088
tags:
- error
- correction
- speech
- data
- whisper
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents the first study on automatic speech recognition
  (ASR) error correction for Burmese, a low-resource language. The authors investigate
  sequence-to-sequence Transformer models enhanced with International Phonetic Alphabet
  (IPA) features and alignment information for post-ASR correction.
---

# ASR Error Correction in Low-Resource Burmese with Alignment-Enhanced Transformers using Phonetic Features

## Quick Facts
- **arXiv ID**: 2511.21088
- **Source URL**: https://arxiv.org/abs/2511.21088
- **Reference count**: 34
- **Primary result**: First study on ASR error correction for Burmese, achieving 39.82 WER (vs 51.56 baseline) using IPA and alignment-enhanced Transformers

## Executive Summary
This paper presents the first study on automatic speech recognition (ASR) error correction for Burmese, a low-resource language. The authors investigate sequence-to-sequence Transformer models enhanced with International Phonetic Alphabet (IPA) features and alignment information for post-ASR correction. Five different ASR backbones were evaluated, and the proposed error correction models consistently improved both word-level and character-level accuracy over baseline ASR outputs. The best-performing model, combining IPA and alignment features, reduced average word error rate (WER) from 51.56 to 39.82 (before data augmentation) and improved character F-score (chrF++) from 0.5864 to 0.627.

## Method Summary
The authors formulate ASR error correction as a sequence-to-sequence translation problem, using Transformers to map noisy ASR outputs to clean text. They fine-tune five Whisper variants and MMS-1B on Burmese speech (OpenSLR80 + FLEURS), then generate parallel corpora of ASR errors and ground-truth transcripts. IPA features are extracted using a CRF-based Grapheme-to-IPA model (F1=0.98), and word alignments are computed with fast_align. The S2S Transformer incorporates IPA embeddings via MLP fusion and uses alignment supervision on decoder attention. Data augmentation is applied to 10% of training audio using various techniques.

## Key Results
- Best model (IPA+alignment) reduced average WER from 51.56 to 39.82 on clean test data
- Character F-score improved from 0.5864 to 0.627 with IPA+alignment features
- Data augmentation increased WER across all models (51.56→43.59) but sometimes improved chrF++ for smaller models
- All five ASR backbones showed consistent improvement with error correction
- Alignment features reduced hallucinations and improved word-level correction accuracy

## Why This Works (Mechanism)

### Mechanism 1: Alignment-Regularized Attention
External word alignments constrain the Transformer decoder, reducing hallucinations and improving word-level correction accuracy. The authors use fast_align to extract source–target alignment links from noisy-to-clean sentence pairs. These alignments are injected as supervision signals on the decoder's cross-attention distributions during training, penalizing attention to irrelevant source positions and encouraging faithful source–target correspondences. Core assumption: The quality of fast_align alignments is sufficiently high on noisy Burmese text, and alignment errors do not systematically mislead the model more than the regularization benefits. Evidence: Table V shows consistent WER reductions with alignment features across multiple ASR backbones.

### Mechanism 2: Phonologically-Grounded Feature Fusion
Incorporating IPA features improves character-level fidelity, especially for tonal languages like Burmese where ASR errors are often phonetically confusable. IPA sequences are generated via a CRF-based Grapheme-to-IPA model (F1=0.98). These IPA sequences are embedded and fused with word embeddings through an MLP, allowing the Transformer encoder to jointly represent orthographic and phonetic information. Core assumption: The G2IPA CRF model generalizes well to ASR outputs, and the IPA representations capture phonetic distinctions that correlate with ASR error patterns. Evidence: Table V shows chrF++ improvements with IPA for MMS (0.6126→0.6923 with +AEC+IPA).

### Mechanism 3: Distribution-Mismatched Data Augmentation
Acoustic data augmentation can improve character-level similarity for smaller models but often increases WER on clean test data due to distribution shift. The authors augment 10% of training audio using pitch shifting, speed perturbation, noise addition, VTLP, and time/frequency masking. Synthetic ASR outputs are generated from augmented audio, expanding training diversity but creating a mismatch with the clean test distribution. Core assumption: The clean test set is representative of deployment; augmentation should ideally match deployment conditions. Evidence: Section V.C documents increased WER for all models after augmentation, while chrF++ improved for Whisper Tiny (0.5483→0.6245 with +AEC).

## Foundational Learning

- **Concept: Sequence-to-Sequence Error Correction**
  - Why needed here: AEC is formulated as S2S translation from noisy ASR output to corrected text, requiring understanding of encoder-decoder architectures and attention.
  - Quick check question: Why is ASR error correction modeled as a sequence-to-sequence problem rather than a token-level classification problem?

- **Concept: Word Alignment Models (IBM Model 2 / fast_align)**
  - Why needed here: Alignment extraction is core to the attention regularization mechanism; understanding its assumptions helps diagnose failure modes.
  - Quick check question: What limitation of IBM Model 1 does Model 2 (and fast_align) attempt to address, and what assumptions does it still make?

- **Concept: Data Augmentation for Speech**
  - Why needed here: Interpreting the trade-offs between WER and chrF++ under augmentation requires understanding how acoustic variability affects training distribution.
  - Quick check question: Why might acoustic augmentation improve robustness but simultaneously increase WER on a clean test set?

## Architecture Onboarding

- **Component map**: ASR Backbone → Noisy Transcription → Text Cleaning → Syllable Segmentation → Parallel Corpus Construction → G2IPA Extraction → IPA Embedding → MLP Fusion → Transformer Encoder → fast_align → Alignment Links → Attention Supervision → Transformer Decoder → Evaluation

- **Critical path**:
  1. Fine-tune Whisper models on Burmese speech (OpenSLR80 + FLEURS)
  2. Generate parallel corpus: ASR outputs ↔ ground-truth transcripts (original + augmented)
  3. Train CRF-based G2IPA model for IPA feature extraction
  4. Run fast_align on parallel corpus to extract alignments
  5. Train S2S Transformer with IPA fusion and alignment supervision
  6. Evaluate on held-out test set using WER and chrF++

- **Design tradeoffs**:
  - IPA feature quality vs. G2IPA complexity (CRF chosen for high F1, but may not generalize to all error types)
  - Alignment supervision strength vs. model flexibility (over-constraining may limit correction of complex errors)
  - Augmentation diversity vs. distribution mismatch (helps small models on chrF++, hurts WER on clean test)

- **Failure signatures**:
  - WER increases after correction: check alignment quality; model may be over-generating or hallucinating
  - chrF++ drops with IPA features: G2IPA may be introducing noise on specific ASR outputs; verify G2IPA accuracy on error patterns
  - Augmentation improves chrF++ but not WER for small models: indicates surface-level character improvements without lexical accuracy gains

- **First 3 experiments**:
  1. Establish baseline: Train AEC without IPA or alignment on original data; verify WER/chrF++ improvement over raw ASR outputs
  2. Ablate features: Train +AEC+IPA and +AEC+Align separately to isolate individual contributions of phonetic and alignment features
  3. Test augmentation impact: Train on original vs. augmented data for Whisper Tiny and Whisper Large to confirm scale-dependent effects on WER and chrF++

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How can data augmentation strategies be designed to reduce distribution mismatch and prevent WER degradation while maintaining character-level improvements?
- **Basis in paper**: The authors state: "Future work will explore augmentation strategies better matched to test conditions." They also note augmentation "consistently increased WER for all ASR models" due to distribution shift.
- **Why unresolved**: Current augmentation improves chrF++ for smaller models but degrades word-level accuracy across all models. The trade-off between diversity and distribution alignment remains unoptimized.
- **What evidence would resolve it**: Experiments with augmentation techniques that preserve linguistic properties of Burmese while matching test distribution; evaluation on both clean and augmented test sets to measure generalization.

### Open Question 2
- **Question**: Would incorporating acoustic features alongside ASR hypotheses improve correction accuracy for Burmese AEC?
- **Basis in paper**: The conclusion states: "Future work will explore... extending AEC to multimodal scenarios." The introduction references cross-modal AEC studies [7]-[9] but the current work uses only text-based features.
- **Why unresolved**: Text-only AEC cannot recover information lost during ASR decoding; acoustic features might help disambiguate phonetically similar errors common in tonal Burmese.
- **What evidence would resolve it**: Comparative experiments adding acoustic embeddings from ASR encoder states to the current IPA+alignment model, measuring WER/chrF++ improvements.

### Open Question 3
- **Question**: Why do IPA features improve chrF++ for MMS but increase WER for Whisper models?
- **Basis in paper**: Table V shows inconsistent IPA effects: MMS improves (WER 30.70→31.40, chrF++ 0.6461→0.6923) while Whisper Tiny degrades. The error analysis notes MMS produces "unstable decoding" that "could hurt G2IPA conversion."
- **Why unresolved**: The interaction between ASR backbone characteristics and phonetic feature utility is unclear; possibly related to how different models distribute errors across syllables versus words.
- **What evidence would resolve it**: Fine-grained error analysis correlating G2IPA accuracy with AEC gains per model; ablation studies isolating phonetic error types each model produces.

## Limitations
- The study is limited to Burmese, a tonal language, making generalization to non-tonal languages uncertain
- Data augmentation strategy increases WER due to distribution mismatch with clean test data
- Implementation details for alignment supervision mechanism are not fully specified
- Limited training data (~31.7k sentences) may affect model robustness and generalization

## Confidence

**High Confidence**: The effectiveness of sequence-to-sequence Transformers for ASR error correction in Burmese is well-supported by consistent WER reductions (51.56→39.82) and chrF++ improvements (0.5864→0.627) across all five ASR backbones. The ablation studies clearly demonstrate the individual contributions of IPA features and alignment information.

**Medium Confidence**: The claim that alignment features reduce hallucinations is supported by qualitative descriptions and consistent performance improvements, but the exact mechanism of attention regularization is underspecified. The data augmentation benefits for smaller models (Whisper Tiny) are demonstrated but may be specific to this dataset scale and Burmese orthography.

**Low Confidence**: Claims about IPA features being particularly effective for tonal languages are partially supported by the Burmese results, but the study lacks comparison with non-tonal low-resource languages. The generalization of findings to real-world deployment scenarios is uncertain given the clean test set evaluation.

## Next Checks
1. **Reproduce alignment supervision mechanism**: Implement and test the exact method for integrating fast_align outputs as decoder attention constraints. Compare models with and without proper alignment supervision to isolate this effect.

2. **Test augmentation distribution alignment**: Train models on subsets of augmented data that more closely match the test set distribution. Measure the trade-off between acoustic diversity and distribution mismatch across different augmentation rates (5%, 15%, 20%).

3. **Cross-lingual validation**: Apply the same AEC framework to another low-resource language with different phonological properties (e.g., Vietnamese for tonal comparison, or Khmer for orthographic similarity). Compare feature effectiveness patterns across languages.