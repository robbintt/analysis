---
ver: rpa2
title: Combining Reinforcement Learning and Behavior Trees for NPCs in Video Games
  with AMD Schola
arxiv_id: '2510.14154'
source_url: https://arxiv.org/abs/2510.14154
tags:
- agent
- learning
- game
- player
- npcs
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of creating multi-task, intelligent
  NPCs in video games by combining reinforcement learning (RL) with behavior trees
  (BTs). The authors propose a hybrid approach where RL models handle specific skills
  (Combat, Flee, Search, Hide, Collect) while BTs provide the overall strategy and
  structure.
---

# Combining Reinforcement Learning and Behavior Trees for NPCs in Video Games with AMD Schola

## Quick Facts
- **arXiv ID**: 2510.14154
- **Source URL**: https://arxiv.org/abs/2510.14154
- **Reference count**: 22
- **Primary result**: Hybrid BT+RL approach achieves 53% win rate vs aggressive NPCs, outperforming pure RL curriculum learning (41%) but trailing pure BT (59%).

## Executive Summary
This paper proposes a hybrid approach combining reinforcement learning (RL) with behavior trees (BTs) for creating multi-task intelligent NPCs in video games. The authors train individual RL models for specific skills (Combat, Flee, Search, Hide, Collect) while using BTs to provide overall strategic structure and control. Using AMD Schola in Unreal Engine, they evaluate the hybrid approach against both static and aggressive opponents, showing it balances RL's adaptability with BT's interpretability and developer control. The hybrid approach achieves competitive performance while maintaining modular design that enables skill reuse and simpler reward shaping.

## Method Summary
The authors implement a hybrid BT+RL architecture using AMD Schola in Unreal Engine. They train five independent RL models (Combat, Flee, Search, Hide, Collect) using PPO with skill-specific observations and rewards. The BT structure routes to appropriate skills based on conditions like health, ammo, and distance to target. Each skill trains in isolation using step counts ranging from 2M to 12M steps, then integrates into the BT framework for evaluation. The approach is compared against pure BT and pure RL curriculum learning baselines in a third-person shooter environment with 4000-unit square maps and static obstacles.

## Key Results
- Hybrid model achieves 53% win rate against aggressive NPCs (vs 59% for pure BT, 41% for pure RL curriculum)
- Damage dealt: 149.86 (vs 170.48 for BT, 137.80 for RL)
- FPS with 1 agent: 211.9 (vs 261.9 for BT, 186.4 for RL)
- Episode timeout occurs ~10% of time with >10,000 steps

## Why This Works (Mechanism)

### Mechanism 1
- Decomposing multi-task NPC behavior into modular RL skills governed by a BT enables simpler reward shaping and skill reuse
- The BT enforces strategic transitions, constraining when each RL policy is active and avoiding negative task transfer
- Core assumption: Skill boundaries are cleanly separable; BT logic correctly routes to appropriate skills
- Evidence: Each skill uses distinct reward structures and termination conditions in training environments

### Mechanism 2
- BT-provided structure reduces RL search space, enabling faster convergence with smaller networks
- BT handles conditional logic and state transitions, so each RL model only learns action selection within fixed tactical context
- Core assumption: BT decision logic is correctly designed; errors propagate to all subordinate RL skills
- Evidence: Models use small MLPs (Depth 2, Width 64-128) with attention only for curriculum baseline

### Mechanism 3
- Hybrid preserves developer interpretability and control while gaining RL's adaptability within skills
- BT nodes remain human-readable and modifiable; RL models are encapsulated as leaf nodes
- Core assumption: Developers accept mixture of hand-authored logic and learned behaviors
- Evidence: Authors highlight BT+RL as capturing RL's enhanced abilities with BT's interpretability

## Foundational Learning

- **Behavior Trees (BTs)**: Selector nodes (?) and Sequence nodes (â†’) execute based on conditions; why needed for understanding hybrid architecture routing logic
  - Quick check: Given a BT with Selector at root and two Sequence children, which executes if first Sequence's condition fails?

- **Proximal Policy Optimization (PPO)**: RL algorithm with clipping and advantage estimation; why needed for understanding training stability and policy updates
  - Quick check: What happens to policy updates if KL divergence between old and new policies grows too large?

- **Curriculum Learning**: Phased training with task progression; why needed for understanding why hybrid approach avoids curriculum complexity
  - Quick check: In 5-phase curriculum where Phase 4 introduces death penalties, what failure occurs if Phase 3 undertrained?

## Architecture Onboarding

- **Component map**: AMD Schola Plugin -> BT Controller -> RL Skill Models -> Training Environments -> Evaluation Environment
- **Critical path**: 1) Set up AMD Schola in Unreal Engine 2) Implement BT structure matching Figure 1 3) Create training environments for each skill 4) Define observation/action spaces per Table 2 5) Train skills to convergence 6) Integrate into BT and evaluate
- **Design tradeoffs**: Hybrid vs. Pure BT: BT wins on win rate and FPS; hybrid offers more varied trajectories. Hybrid vs. Curriculum RL: Hybrid outperforms curriculum on win rate and damage. Model batching not implemented but enabled by modular design.
- **Failure signatures**: Skill never activates (BT condition logic incorrect), Agent stuck in loop (RL policy oscillates), FPS degradation with multiple agents, Episode timeout (>10,000 steps ~10% of time)
- **First 3 experiments**: 1) Validate BT routing with logging stubs 2) Single-skill ablation (Combat vs Static NPC) 3) Scalability benchmark (1, 5, 10 agents) to identify bottlenecks

## Open Questions the Paper Calls Out

- **Open Question 1**: Can batching optimizations significantly improve real-time performance (FPS) of hybrid BT+RL approach to match or exceed pure RL baselines? The authors note skipping model optimizations like batching as future work, with hybrid currently showing lower FPS (211.9) than pure BT (261.9).

- **Open Question 2**: To what extent can integrating curriculum learning into individual RL skills within hybrid framework improve overall win rate? Authors note hybrid could benefit from techniques like curriculum learning, but pure RL baseline used it while hybrid skills were trained individually.

- **Open Question 3**: Does hybrid BT+RL approach generalize effectively to complex, open-world map geometries compared to enclosed square arena used in study? Environment is described as "4000 units enclosed square," but authors cite "The Last of Us" (complex stealth/navigation) as inspiration.

## Limitations
- Performance gap remains vs pure BT baseline on win rate (53% vs 59%) and damage
- FPS degradation with multiple agents (10 agents: ~110 FPS) not fully addressed
- Lack of ablation studies isolating BT structural benefits from RL skill improvements

## Confidence
- **Win rate claims**: Medium confidence (lacks ablation studies)
- **Interpretability argument**: High confidence (clear architectural separation demonstrated)
- **Performance metrics**: Medium confidence (shows hybrid outperforms pure RL but trails pure BT)

## Next Checks
1. Ablation study comparing hybrid vs. monolithic RL with identical skill-specific rewards but no BT gating
2. Analysis of skill activation frequency to verify BT conditions correctly trigger RL policies
3. Scalability testing with 10+ agents to validate claimed FPS degradation patterns and identify bottlenecks