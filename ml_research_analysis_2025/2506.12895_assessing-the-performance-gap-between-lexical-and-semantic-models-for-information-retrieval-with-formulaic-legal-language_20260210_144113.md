---
ver: rpa2
title: Assessing the Performance Gap Between Lexical and Semantic Models for Information
  Retrieval With Formulaic Legal Language
arxiv_id: '2506.12895'
source_url: https://arxiv.org/abs/2506.12895
tags:
- bm25
- recall
- legal
- paragraph
- cited
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates legal passage retrieval in the highly structured
  and formulaic language of the Court of Justice of the European Union (CJEU), focusing
  on when lexical methods like BM25 outperform semantic dense retrieval models. By
  analyzing citation pairs where the citing paragraph is often a verbatim quote or
  close paraphrase of the cited paragraph, the authors compare lexical (BM25, TF-IDF)
  and dense models (zero-shot and fine-tuned SBERT variants).
---

# Assessing the Performance Gap Between Lexical and Semantic Models for Information Retrieval With Formulaic Legal Language

## Quick Facts
- arXiv ID: 2506.12895
- Source URL: https://arxiv.org/abs/2506.12895
- Reference count: 40
- Primary result: BM25 outperforms most zero-shot dense models in formulaic legal retrieval; fine-tuned LegalSBERT-ft surpasses BM25 on most metrics.

## Executive Summary
This study investigates legal passage retrieval in the highly structured and formulaic language of the Court of Justice of the European Union (CJEU), focusing on when lexical methods like BM25 outperform semantic dense retrieval models. By analyzing citation pairs where the citing paragraph is often a verbatim quote or close paraphrase of the cited paragraph, the authors compare lexical (BM25, TF-IDF) and dense models (zero-shot and fine-tuned SBERT variants). Results show BM25 is a strong baseline, outperforming most zero-shot dense models on multiple metrics. Fine-tuning dense models on domain-specific data significantly improves performance, with LegalSBERT-ft surpassing BM25 in most metrics. Qualitative and quantitative analyses reveal that both model types perform well in highly repetitive language scenarios, but BM25 excels in nuanced cases with less verbatim overlap and longer queries. The study also examines how fine-tuning data volume affects performance and temporal robustness, finding improvements plateau after certain data thresholds.

## Method Summary
The authors evaluate legal passage retrieval on a CJEU judgment dataset with temporal splits (1979-2016 for training, 2017-2018 for validation, 2019-2021 for testing). They compare BM25 and TF-IDF as lexical baselines against zero-shot dense models (SBERT, SimCSE, Nomic, Ada-v2, Emb-3-large) and fine-tuned variants (SBERT-ft, LegalSBERT-ft). The dense models are trained using Multiple Negatives Ranking Loss on citation pairs. Evaluation uses Recall@k, nDCG@10, MAP, and MRR metrics. The study includes qualitative analysis of retrieval failures and examines the impact of training data volume and temporal robustness on model performance.

## Key Results
- BM25 outperforms most zero-shot dense models on multiple retrieval metrics in formulaic legal language.
- Fine-tuning LegalSBERT on domain-specific data significantly improves performance, surpassing BM25 on most metrics.
- BM25 shows particular strength on longer queries and cases with less verbatim overlap between query and target passages.
- Performance improvements from fine-tuning plateau after training on approximately 50% of available data.

## Why This Works (Mechanism)

### Mechanism 1: High Lexical Overlap Amplifies Term Frequency Advantages
In formulaic legal corpora with extensive verbatim quoting, lexical methods like BM25 leverage exact term overlap more reliably than zero-shot dense embeddings. The CJEU dataset exhibits "highly structured and formulaic" language with citation pairs often being "verbatim quote or close paraphrase." BM25's term frequency weighting captures this repetitive vocabulary directly, while dense models (without domain adaptation) may not align their semantic embeddings to these specific lexical patterns.

### Mechanism 2: Noise from Contextual Expansion Degrades Dense Retrieval in Longer Queries
Dense models underperform relative to BM25 when queries contain substantial non-overlapping context surrounding the relevant citation. Dense models encode the entire query into a single embedding, potentially diluting the signal from the relevant quoted portion with noise from surrounding legal discussion. BM25's sparse representation isolates key terms regardless of query length.

### Mechanism 3: Domain-Specific Fine-Tuning Aligns Semantic Space to Legal Formulaic Patterns
Fine-tuning dense models on in-domain legal citation data closes the performance gap with BM25 by adapting embeddings to the specific vocabulary and citation patterns. LegalSBERT-ft, fine-tuned on CJEU data using contrastive learning, outperforms BM25 on most metrics. This suggests the model learns to weight domain-relevant terms and contextual patterns specific to EU legal language, overcoming the zero-shot misalignment.

## Foundational Learning

- **Concept: BM25 Ranking Function**
  - Why needed here: BM25 serves as the primary lexical baseline against which dense models are compared. Understanding its IDF and term frequency components is essential to interpret why it excels with verbatim overlap.
  - Quick check question: In BM25, does a rare term appearing in both query and document contribute more or less to the score than a common term?

- **Concept: Dense Retrieval with Sentence Embeddings**
  - Why needed here: The dense models (SBERT, SimCSE, LegalSBERT-ft) operate by encoding queries and documents into fixed-dimensional vectors and computing cosine similarity. Knowing how these embeddings are trained helps understand their failure modes with out-of-domain text.
  - Quick check question: What is the primary difference between zero-shot dense retrieval and fine-tuned dense retrieval in terms of training data exposure?

- **Concept: Temporal Split in Evaluation**
  - Why needed here: The dataset uses a temporal split (training 1979-2016, test 2019-2021) to simulate real-world legal precedent retrieval. This impacts model robustness and must be considered when interpreting fine-tuning benefits.
  - Quick check question: Why might a model trained on older legal texts perform differently on newer cases in a temporally split evaluation?

## Architecture Onboarding

- **Component map:** Data Preprocessing -> Lexical Retrievers (BM25, TF-IDF) -> Dense Embedders (Zero-shot and Fine-tuned) -> Scoring (Cosine similarity, BM25) -> Evaluation (Recall@k, nDCG@10, MAP, MRR)

- **Critical path:**
  1. Load preprocessed CJEU dataset
  2. Build BM25 index over training+validation paragraphs
  3. Load or train dense embedding models
  4. For each query in test set, retrieve top-k candidates using each method
  5. Compare ranked results against ground-truth citations using evaluation metrics
  6. Perform qualitative analysis on subsets where BM25 outperforms dense or vice versa

- **Design tradeoffs:**
  - Zero-shot vs Fine-tuned: Zero-shot dense models require no domain data but underperform BM25; fine-tuned models surpass BM25 but require labeled pairs and compute
  - Lexical vs Semantic: BM25 excels on verbatim/longer queries with clear term overlap; dense models capture synonyms but may be distracted by context noise
  - Temporal Robustness: Training on older data may limit generalization to newer legal language; fine-tuning benefits plateau after moderate data volume

- **Failure signatures:**
  - Dense model retrieves top candidates discussing a different legal topic than the cited passage
  - BM25 fails when query uses synonyms or semantic role shifts not present in target
  - Fine-tuned model performance plateaus or drops when trained on >50% of data, suggesting potential overfitting

- **First 3 experiments:**
  1. Replicate baseline comparison: Run BM25 and zero-shot dense models on the CJEU test split, confirm BM25 outperforms most on Recall@k metrics
  2. Fine-tune ablation: Train SBERT-ft on 10%, 30%, 50%, 80% of training data, measure performance plateau trend
  3. Qualitative error analysis: Sample citation pairs where BM25 succeeds and dense fails (and vice versa), compute edit distance, N-gram overlap, and query length to validate proposed mechanisms

## Open Questions the Paper Calls Out

- **Question:** Does a two-stage re-ranking pipeline, utilizing BM25 for coarse retrieval followed by a fine-tuned dense model for precision ranking, outperform single-stage approaches in formulaic legal retrieval?
- **Question:** How does the "formulaicity" of legal language in other judicial bodies (e.g., US courts, ECtHR) impact the performance gap between lexical and semantic models compared to the CJEU?
- **Question:** Can a formal typology of cases be developed to predict when lexical methods will outperform dense models based on specific linguistic or structural features?
- **Question:** Do retrieval models trained on citation pairs generalize effectively to practical legal research tasks where queries are not derived from verbatim quotes or close paraphrases?

## Limitations

- The study's conclusions are drawn from a highly specialized legal corpus with extensive verbatim citation patterns, limiting generalizability to domains with less formulaic language.
- The zero-shot dense models used are based on general-purpose embeddings without legal-specific pretraining, creating an imbalanced comparison where domain-adapted models naturally outperform.
- The qualitative analysis samples are small (20 cases for each mechanism), which may not represent the full distribution of retrieval scenarios.

## Confidence

- **BM25 baseline superiority in formulaic legal language:** High confidence
- **Fine-tuning effectiveness:** Medium confidence
- **Query length and noise effects:** Medium confidence

## Next Checks

1. **Cross-domain robustness test:** Evaluate the same BM25 vs fine-tuned dense model comparison on a non-legal corpus (e.g., scientific citations or news articles) to determine if the formulaic language advantage holds outside the CJEU domain.

2. **Ablation of query context:** Systematically mask or remove contextual text surrounding verbatim quotes in longer queries to measure the performance drop for dense models, isolating the noise effect from genuine semantic understanding.

3. **Temporal generalization stress test:** Extend the evaluation to include 2022-2023 data (if available) to measure how well the 2019-2021 fine-tuned models handle legal language evolution and emerging precedents beyond the training period.