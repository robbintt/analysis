---
ver: rpa2
title: 'Automated Risk-of-Bias Assessment of Randomized Controlled Trials: A First
  Look at a GEPA-trained Programmatic Prompting Framework'
arxiv_id: '2512.01452'
source_url: https://arxiv.org/abs/2512.01452
tags:
- risk
- were
- assessment
- prompts
- gepa
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study introduces a programmatic approach to automating risk-of-bias
  (RoB) assessment in randomized controlled trials using large language models. Instead
  of relying on manual prompt engineering, it leverages the GEPA module within DSPy
  to optimize prompts through Pareto-guided search, producing transparent, reproducible,
  and interpretable reasoning traces.
---

# Automated Risk-of-Bias Assessment of Randomized Controlled Trials: A First Look at a GEPA-trained Programmatic Prompting Framework

## Quick Facts
- arXiv ID: 2512.01452
- Source URL: https://arxiv.org/abs/2512.01452
- Reference count: 40
- Primary result: GEPA-generated prompts achieved up to 40% improvement in key RoB domains compared to manual prompts

## Executive Summary
This study introduces a programmatic approach to automating risk-of-bias (RoB) assessment in randomized controlled trials using large language models. Instead of relying on manual prompt engineering, it leverages the GEPA module within DSPy to optimize prompts through Pareto-guided search, producing transparent, reproducible, and interpretable reasoning traces. Evaluated on 100 RCTs across seven RoB domains, the GEPA-generated prompts achieved high accuracy and consistency, particularly in domains with clearer methodological reporting such as random sequence generation. When compared to three manually crafted prompts, GEPA demonstrated superior performance with up to 40% improvement in key domains and maintained competitive alignment overall. The approach also proved computationally efficient and cost-effective across both open-weight and commercial models. These findings demonstrate that GEPA can provide a structured, standardized, and sustainable foundation for integrating LLMs into evidence synthesis workflows, reducing reliance on manual prompt design while maintaining alignment with expert human judgments.

## Method Summary
The study used DSPy v3.0.4b1 with GEPA to optimize prompts for Cochrane RoB 1 tool assessment across 7 domains. It employed a two-stage reasoning approach with paired questions (evidence extraction followed by adequacy assessment) per domain. The optimization used Pareto-guided search with auto="light" configuration (~428 metric calls) on datasets of 18 training + 12 validation examples per domain. Three model configurations were tested: Mistral Small 3.1 + GPT-oss-20b, GPT-5 Nano + GPT-5 Mini, and Mistral Small 3.1 + Claude-3.5-Sonnet. Five optimization runs per domain with three inference runs per prompt were conducted using fixed seed=42 and temperature=0.0.

## Key Results
- GEPA prompts achieved median accuracy of 0.59 across all domains, outperforming manual prompts (0.38-0.49)
- In D1 (random sequence generation), GEPA achieved accuracy of 0.84 with κ=0.73, a 40% improvement over manual prompts
- GEPA demonstrated 2-4% higher accuracy than manual prompts on commercial models while maintaining computational efficiency
- The approach showed systematic conservatism bias, downgrading judgments to "unclear" when explicit evidence was missing, aligning with normative RoB standards

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Pareto-guided prompt search produces more robust RoB assessment prompts than manual engineering.
- Mechanism: GEPA searches a space of candidate prompts using multi-objective optimization (accuracy, faithfulness, conciseness), selecting prompts on the Pareto frontier rather than relying on single-metric hill-climbing or human intuition.
- Core assumption: The RoB assessment task has stable decision boundaries that can be captured by optimizing against annotated examples, and these boundaries generalize to unseen RCTs.
- Evidence anchors: [abstract] "GEPA refines LLM reasoning through Pareto-guided search and produces inspectable execution traces"; [section 3.2] GEPA achieved median accuracy 0.59 across domains vs. 0.38-0.49 for manual prompts; [corpus] Related work reports inconsistent LLM performance with handcrafted prompts.

### Mechanism 2
- Claim: Paired-question reasoning structures enforce evidence grounding before judgment.
- Mechanism: Each RoB domain uses two sequential questions—one extracting relevant text spans, one assessing adequacy. This forces the model to anchor decisions in explicit evidence rather than pattern-matching to conclusions.
- Core assumption: RCT methodological reporting is sufficiently explicit that correct judgments can be derived from textual evidence alone.
- Evidence anchors: [section 2.2.1] "The system employs sequential reasoning guided by paired questions...one focusing on identifying information...the other assessing adequacy"; [section 3.4] Disagreement analysis shows LLMs downgrade judgments when procedural details are missing.

### Mechanism 3
- Claim: Conservatism bias from GEPA prompts aligns better with RoB tool intent than typical human practice.
- Mechanism: GEPA learns from aggregated annotations that encode methodological caution. Optimized prompts enforce explicit procedural detail requirements, producing "unclear" judgments when evidence is incomplete—matching normative RoB standards.
- Core assumption: Published meta-analyses' RoB judgments represent a reasonable approximation of correct assessment, despite known inter-rater variability.
- Evidence anchors: [section 3.4.1] "Models were systematically more cautious than the reference, most often downgrading low/high risk ratings to unclear"; [section 4.3] "GEPA tends to adopt a more conservative and criteria-driven stance...which corresponds to the normative intent of RoB tools."

## Foundational Learning

- Concept: **Cochrane Risk of Bias 1 (RoB 1) domains**
  - Why needed here: The entire system is structured around seven RoB domains (D1-D7), each with distinct criteria for Low/Unclear/High judgments. Without understanding these criteria, you cannot interpret outputs or debug failures.
  - Quick check question: Can you explain why "random sequence generation" and "allocation concealment" are assessed separately?

- Concept: **DSPy signatures and modules**
  - Why needed here: GEPA operates within DSPy's declarative framework. Understanding how signatures define input/output contracts and how modules compose them is prerequisite to modifying the pipeline.
  - Quick check question: What is the difference between a DSPy Signature and a manually crafted prompt string?

- Concept: **Pareto optimization for multi-objective search**
  - Why needed here: GEPA uses Pareto fronts to balance competing objectives (accuracy, conciseness, faithfulness). Understanding trade-offs helps interpret why certain prompts are selected.
  - Quick check question: If two prompts have equal accuracy but one is more concise, which appears on the Pareto front?

## Architecture Onboarding

- Component map: MinerU PDF-to-text conversion -> domain-specific text excerpts -> GEPA optimizer -> paired-question reasoning -> structured outputs (reasoning, risk_level, justification, confidence)

- Critical path: 1. Curate balanced training data with annotated evidence spans and justifications; 2. Run GEPA optimization (auto="light", ~428 metric calls) with fixed seed; 3. Validate optimized prompts on held-out test set; 4. Inspect execution traces for reproducibility

- Design tradeoffs:
  - Open-weight vs. commercial models: Open-weight (Mistral + GPT-oss-20b) costs ~$0.04/domain to train; commercial (GPT-5 Nano/Mini) achieves 2-4% higher accuracy but costs ~$0.55-$0.91/domain
  - Light vs. full optimization: auto="light" balances exploration with efficiency; deeper search may yield marginal gains at higher cost
  - Text-only input: Faster and cheaper but loses information from figures/flow diagrams, particularly affecting D5 (incomplete outcome data)

- Failure signatures:
  - Domain imbalance: D6 (selective reporting) had 89% "low risk" labels, suppressing κ despite reasonable accuracy
  - Conservative downgrade cascade: Models assign "unclear" when humans infer adequacy from context—check if domain criteria are too strict
  - Missing multimodal data: D5 underperforms because CONSORT flow diagrams aren't processed

- First 3 experiments:
  1. Baseline replication: Re-run GEPA on a single domain (e.g., D1) with identical settings (seed=42, temp=0.0) to verify reproducibility of prompt outputs and performance metrics
  2. Ablation on training size: Reduce training examples from 18 to 9 per domain and measure accuracy degradation to understand data efficiency
  3. Cross-domain transfer: Apply D1-optimized prompt to D2 without retraining to test whether learned reasoning patterns transfer across related methodological domains

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the GEPA framework be extended to accommodate the multi-step reasoning chains and conditional decision rules required by the RoB 2 tool?
- Basis in paper: [explicit] The authors state in the Conclusion that the current framework relies on RoB 1 structure and that "Extending GEPA to accommodate multi-step reasoning chains and conditional decision rules would enhance its applicability and alignment with current methodological standards."
- Why unresolved: The RoB 2 tool uses a signaling-question approach rather than direct domain-level judgments, requiring a different algorithmic architecture than the one tested in this study.
- What evidence would resolve it: Empirical validation of a modified GEPA pipeline on a dataset of trials assessed with the RoB 2 framework, demonstrating equivalent or superior alignment with human experts.

### Open Question 2
- Question: Does integrating multimodal inputs (such as figures and CONSORT flow diagrams) improve accuracy in domains where textual reporting is often non-verbal, such as "Incomplete Outcome Data"?
- Basis in paper: [inferred] The paper notes in the Limitations section that "preprocessing relied on PDF-to-text conversion, resulting in the loss of figures... that may contain relevant methodological information," specifically citing that human reviewers often extracted this data from visual elements where the LLM could not.
- Why unresolved: It is unclear if the lower performance in domains like incomplete outcome data is a failure of the reasoning model or simply a failure of data ingestion (missing the flow diagrams).
- What evidence would resolve it: A comparative evaluation running the GEPA pipeline on text-only inputs versus multimodal inputs (including images) for the same set of RCTs.

### Open Question 3
- Question: How does the size and inter-rater reliability of the training dataset impact the stability and generalizability of the optimized prompts?
- Basis in paper: [explicit] The Conclusion notes that "the optimal scope and characteristics of such datasets remain unclear" and suggests future work examine "how larger, more diverse, and more consistently annotated datasets could improve stability."
- Why unresolved: This study used a specific training setup (18 training/12 validation examples), and it is unknown if the observed "conservatism bias" or domain variability is a result of data constraints.
- What evidence would resolve it: Ablation studies varying the volume and consistency (Cohen's Kappa) of the training data to measure the effect on prompt optimization convergence.

### Open Question 4
- Question: To what extent does the "conservatism bias" (tendency to assign "Unclear" risk) observed in GEPA models reflect a stricter, more valid application of RoB criteria compared to human reviewers?
- Basis in paper: [inferred] The Disagreement Analysis section notes that models were "systematically more cautious" and that "this strictness corresponds to the normative intent of RoB tools," suggesting the model may be correctly identifying vague reporting that humans overlook.
- Why unresolved: The paper measures alignment against existing human "Gold Labels," but does not resolve whether divergence represents model error or superior model consistency.
- What evidence would resolve it: An expert re-evaluation of trials where the model disagreed with the original gold standard, specifically assessing if the model's demand for explicit textual evidence was methodologically justified.

## Limitations

- The study relies on text-only processing, excluding critical information from figures and CONSORT flow diagrams that human reviewers use
- Conservative "unclear" bias in subjective domains (D3-D7) may be overly cautious for practical use
- Performance generalization to RCTs from different medical specialties and publication eras remains untested

## Confidence

- **High Confidence**: GEPA's ability to generate reproducible, inspectable prompts (consistent outputs with fixed seeds, transparent reasoning traces)
- **Medium Confidence**: Domain-specific performance improvements over manual prompts (valid within tested domains but unknown for new RCT types)
- **Medium Confidence**: Cost-effectiveness claims (based on specific API pricing; may vary with usage patterns)
- **Low Confidence**: Generalization to new RCT populations and medical fields (not tested beyond cardiovascular, metabolic, women's health, and pediatrics)

## Next Checks

1. Cross-domain generalization test: Apply GEPA-optimized prompts to RCTs from different medical specialties (e.g., oncology, neurology) and compare accuracy degradation rates
2. Multimodal input validation: Integrate figure/table extraction for CONSORT flow diagrams and measure D5 performance improvement
3. Human-AI agreement calibration: Conduct blind comparison of GEPA assessments against fresh human expert reviews on a held-out RCT set