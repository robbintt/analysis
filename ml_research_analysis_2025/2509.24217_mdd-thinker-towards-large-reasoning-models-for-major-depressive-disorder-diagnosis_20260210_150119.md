---
ver: rpa2
title: 'MDD-Thinker: Towards Large Reasoning Models for Major Depressive Disorder
  Diagnosis'
arxiv_id: '2509.24217'
source_url: https://arxiv.org/abs/2509.24217
tags:
- reasoning
- data
- diagnostic
- accuracy
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study introduces MDD-Thinker, a reasoning-enhanced large language
  model framework for diagnosing major depressive disorder. The approach integrates
  supervised fine-tuning and reinforcement learning to strengthen reasoning ability
  and interpretability.
---

# MDD-Thinker: Towards Large Reasoning Models for Major Depressive Disorder Diagnosis

## Quick Facts
- **arXiv ID**: 2509.24217
- **Source URL**: https://arxiv.org/abs/2509.24217
- **Reference count**: 0
- **Primary result**: 0.8268 accuracy and 0.8081 F1-score on MDD diagnosis using reasoning-enhanced LLM

## Executive Summary
This study introduces MDD-Thinker, a reasoning-enhanced large language model framework for diagnosing major depressive disorder. The approach integrates supervised fine-tuning and reinforcement learning to strengthen reasoning ability and interpretability. Using the UK Biobank dataset, 40,000 reasoning samples were generated, supplemented with 10,000 mental health-related entries. MDD-Thinker achieved an accuracy of 0.8268 and F1-score of 0.8081, outperforming traditional baselines and general-purpose LLMs. The combination of supervised fine-tuning and reinforcement learning yielded relative gains of 29.0% in accuracy, 38.1% in F1-score, and 34.8% in AUC. The model also demonstrated robust reasoning comparable to much larger LLMs while maintaining computational efficiency, highlighting its potential as a scalable and clinically reliable tool for MDD diagnostics.

## Method Summary
MDD-Thinker uses a two-stage training approach on Qwen2.5-7B. First, supervised fine-tuning (SFT) trains the model on 50,000 reasoning samples generated from UK Biobank data with Chain-of-Thought prompting. Second, reinforcement learning with Group Relative Policy Optimization (GRPO) refines the model's reasoning policy using accuracy and format rewards. The dataset includes 208,406 participants after filtering, with 22 selected features from demographic, clinical, and biochemical domains. The model employs multi-model consensus filtering (GPT-4o, Gemini 2.5, DeepSeek-R1) to validate reasoning samples before training.

## Key Results
- Achieved 0.8268 accuracy and 0.8081 F1-score on MDD diagnosis
- SFT+RL combination yielded 29.0% accuracy improvement over baselines
- 38.1% F1-score and 34.8% AUC relative gains with combined approach
- Complex CoT prompting achieved 0.8268 accuracy vs 0.6409 for Direct Response
- Performance comparable to larger LLMs while maintaining computational efficiency

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multi-step Chain-of-Thought (CoT) reasoning improves diagnostic accuracy and interpretability over direct classification.
- Mechanism: CoT prompting decomposes patient data into sequential logical steps (comprehension → feature analysis → intermediate rationale → conclusion), producing explicit reasoning chains that guide the model toward accurate diagnoses while exposing the decision path.
- Core assumption: Reasoning paths that match ground truth labels contain clinically meaningful logic, not merely post-hoc rationalizations.
- Evidence anchors: [abstract] "MDD-Thinker...integrates supervised fine-tuning with reinforcement learning to strengthen reasoning ability and interpretability"; [section 3.5] Complex CoT achieved 0.8268 accuracy vs. 0.6409 for Direct Response; token count increased from 36 to 358

### Mechanism 2
- Claim: Combining SFT and RL yields synergistic gains beyond either method alone.
- Mechanism: SFT embeds domain-specific medical knowledge and establishes baseline reasoning patterns; RL (via GRPO) subsequently refines policy by rewarding accurate, logically coherent outputs while regularizing against divergence from the reference model.
- Core assumption: The reward function combining accuracy and format constraints meaningfully captures diagnostic quality.
- Evidence anchors: [abstract] "Incorporating both SFT and RL yielded relative gains of 29.0% in accuracy, 38.1% in F1-score"; [section 3.4] SFT alone improved accuracy by 22.1%; SFT+RL achieved best results; RL alone showed moderate improvement

### Mechanism 3
- Claim: Multi-model consensus filtering of training data improves corpus quality.
- Mechanism: Three LLMs (GPT-4o, Gemini 2.5, DeepSeek-R1) verify diagnostic consistency; samples misclassified by all three are excluded, reducing noise from misdiagnoses or ambiguous cases.
- Core assumption: Consensus among advanced LLMs approximates ground truth validity for filtering purposes.
- Evidence anchors: [section 2.2] "Cases that were not correctly classified by all three models were excluded"; [section 2.2] Final dataset: 208,406 participants after filtering (9,755 MDD, 198,651 controls)

## Foundational Learning

- Concept: **Chain-of-Thought Prompting**
  - Why needed here: Enables the model to generate explicit intermediate reasoning steps rather than direct predictions, which is essential for clinical interpretability requirements.
  - Quick check question: Can you explain how CoT differs from standard prompting in terms of output structure and computational cost?

- Concept: **Group Relative Policy Optimization (GRPO)**
  - Why needed here: The RL stage uses GRPO to optimize the model's policy by generating multiple candidate outputs per query and comparing their relative advantages.
  - Quick check question: How does GRPO differ from standard PPO in terms of advantage estimation and reward signal construction?

- Concept: **Supervised Fine-Tuning Objectives for Autoregressive Models**
  - Why needed here: The SFT stage minimizes negative log-likelihood over target sequences, requiring understanding of next-token prediction training.
  - Quick check question: What does the loss function ℒ_SFT(θ) = −Σ log P_θ(y_t | y_{1:t-1}, x) optimize, and how does full-parameter fine-tuning differ from LoRA?

## Architecture Onboarding

- Component map: Base Model -> SFT Training -> RL Refinement -> Diagnostic Output
- Critical path:
  1. Feature selection (22 variables from demographic, clinical, biochemical domains)
  2. Tabular-to-text conversion following predefined rules
  3. GPT-4o generates reasoning paths with CoT prompting
  4. Validation: generated answer must match ground truth (up to T attempts)
  5. SFT training on validated reasoning corpus
  6. RL refinement using GRPO with accuracy + format rewards

- Design tradeoffs:
  - 7B model chosen for computational efficiency vs. larger models (70B+)—trade accuracy on general medical benchmarks for MDD specialization
  - Complex CoT increases token count ~10x vs. direct response—trade inference cost for interpretability
  - Multi-model filtering reduces dataset size—trade data volume for quality

- Failure signatures:
  - RL stage collapses to short, low-reasoning outputs if format reward dominates
  - Reasoning paths become repetitive or fail to ground in specific patient features
  - Performance degrades significantly on populations outside UK Biobank demographics (age 40-69, UK-based)

- First 3 experiments:
  1. Ablation: Train with SFT-only vs. RL-only vs. SFT+RL on held-out UK Biobank subset to verify synergistic gains
  2. CoT complexity: Compare Direct Response, Simple CoT, and Complex CoT on accuracy, F1, and average token length
  3. Generalization test: Evaluate on public benchmarks (MedQA, MedMCQA, PubMedQA) to assess domain boundaries and identify where specialized MDD training does not transfer

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can MDD-Thinker maintain diagnostic accuracy and reasoning quality across multi-center, multi-ethnic, and international cohorts?
- Basis in paper: [Explicit] The authors state the model was primarily trained on the UK Biobank, which lacks diversity in race, age, and cultural background, limiting generalizability.
- Why unresolved: The homogeneity of the training data (UK residents aged 40–69) creates uncertainty regarding performance for underrepresented demographic groups.
- What evidence would resolve it: Evaluation results from validation studies conducted on diverse, international datasets distinct from the UK Biobank population.

### Open Question 2
- Question: Can Retrieval-Augmented Generation (RAG) be effectively integrated to enable dynamic knowledge updating without requiring full model retraining?
- Basis in paper: [Explicit] The authors identify continual learning as a challenge due to high computational costs and propose RAG as a future approach to allow dynamic knowledge updating.
- Why unresolved: Current LLM architectures typically require resource-intensive retraining to incorporate new medical data or guidelines.
- What evidence would resolve it: A comparative study showing that an RAG-enhanced version of MDD-Thinker can integrate new clinical data efficiently while maintaining performance.

### Open Question 3
- Question: How does the integration of MDD-Thinker into clinical workflows affect decision safety and the efficacy of human-AI collaboration?
- Basis in paper: [Explicit] The authors list "further evaluation of model interpretability, decision safety, and collaboration with clinicians" as essential for advancing practical applications.
- Why unresolved: While the model is accurate in isolation, its safety margins and impact on clinician judgment in real-world settings have not been quantified.
- What evidence would resolve it: Results from clinical user studies or trials measuring diagnostic error rates and clinician efficiency when assisted by the model.

## Limitations

- The model was trained primarily on UK Biobank data (age 40-69, UK-based), limiting generalizability to diverse populations and age groups
- Reasoning validity is verified by LLMs rather than clinical experts, raising questions about clinical validity of generated reasoning paths
- Multi-model consensus filtering may introduce selection bias by excluding ambiguous cases that could be clinically relevant

## Confidence

- **High Confidence**: The technical implementation of the two-stage training approach (SFT followed by GRPO) is well-documented and follows established methodologies in the literature
- **Medium Confidence**: The reported performance improvements (29.0% accuracy gain, 38.1% F1-score improvement) are internally consistent within the study's framework but depend on the quality and representativeness of the UK Biobank dataset
- **Low Confidence**: The clinical validity of generated reasoning chains and their interpretability for actual clinical decision-making remains uncertain without independent expert validation

## Next Checks

1. **External Validation**: Test MDD-Thinker on diverse, independent clinical datasets from different healthcare systems and demographic populations to assess generalization beyond UK Biobank
2. **Clinical Expert Review**: Have psychiatrists and mental health professionals evaluate a sample of generated reasoning chains for clinical validity, logical coherence, and practical interpretability
3. **Real-World Deployment Study**: Conduct a prospective study where MDD-Thinker assists clinicians in actual diagnostic workflows, measuring impact on diagnostic accuracy, efficiency, and clinical outcomes compared to standard practice