---
ver: rpa2
title: 'Symbolically Scaffolded Play: Designing Role-Sensitive Prompts for Generative
  NPC Dialogue'
arxiv_id: '2510.25820'
source_url: https://arxiv.org/abs/2510.25820
tags:
- play
- dialogue
- prompt
- prompts
- games
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates whether prompt complexity improves player
  experience in LLM-powered NPC dialogue. Through a within-subjects usability study
  (N=10) comparing high- and low-constraint prompts in a voice-based detective game,
  researchers found no reliable experiential differences between conditions.
---

# Symbolically Scaffolded Play: Designing Role-Sensitive Prompts for Generative NPC Dialogue

## Quick Facts
- **arXiv ID:** 2510.25820
- **Source URL:** https://arxiv.org/abs/2510.25820
- **Reference count:** 40
- **Key outcome:** Role-dependent scaffolding improves quest-giver consistency but reduces suspect improvisational believability.

## Executive Summary
This paper investigates whether complex prompt architectures improve player experience in LLM-powered NPC dialogue within a detective game. Through a within-subjects usability study (N=10), researchers found no reliable experiential differences between high- and low-constraint prompts beyond sensitivity to technical breakdowns. Guided by these findings, they redesigned the high-constraint prompt into a hybrid JSON+RAG scaffold and conducted synthetic evaluation with an LLM judge. The results revealed that scaffolding effects are role-dependent: JSON+RAG improved the interviewer's consistency while reducing suspects' improvisational believability. This work introduces Symbolically Scaffolded Play, a framework formalizing prompts as fuzzy boundaries that stabilize coherence where needed while preserving improvisation where surprise sustains engagement.

## Method Summary
The study employed a within-subjects usability test with 10 participants comparing high-constraint (HCP) and low-constraint (LCP) prompt conditions in a voice-based detective game. Participants interacted with NPCs using speech-to-text transcription via Azure Cognitive Services. The team also conducted synthetic evaluation using GPT-4o as a judge to score 60 scripted input prompts across variation, relevance, and hallucination metrics. Based on findings, they developed a hybrid JSON+RAG prompt architecture combining structured memory schemas with retrieval-augmented generation, applying role-specific constraints where needed.

## Key Results
- Usability study revealed no reliable experiential differences between HCP and LCP conditions beyond technical breakdown sensitivity.
- Synthetic evaluation showed JSON+RAG scaffolds improved interviewer (quest-giver NPC) consistency but reduced suspects' improvisational believability.
- Role-dependent trade-offs emerged: high-constraint scaffolds stabilized functional roles requiring accuracy while degrading perceived believability for improvisational roles relying on social nuance.
- Fuzzy numerical parameters (0.0-1.0) stored in shared memory schemas may sustain narrative continuity better than static natural language descriptors.

## Why This Works (Mechanism)

### Mechanism 1: Role-Dependent Constraint Trade-offs
If prompt constraints are applied uniformly across all NPCs, they improve reliability for functional roles (quest-givers) but may degrade perceived believability for improvisational roles (suspects). High-constraint scaffolds likely suppress the "surprise" factor essential for roles relying on deception or social nuance, while simultaneously stabilizing roles requiring factual accuracy and narrative progression. Players evaluate "quest-giver" NPCs based on coherence and "suspect" NPCs based on variability and emotional responsiveness.

### Mechanism 2: Perceptibility Threshold of Backend Refinements
Players likely cannot distinguish between high- and low-constraint prompt architectures during short-term play unless surface-level breakdowns (latency, errors) occur. Usability is dominated by interaction friction rather than the semantic nuance of underlying prompt logic; "hidden refinements" yield diminishing returns. First-time players prioritize interaction flow over narrative subtlety.

### Mechanism 3: Fuzzy State Persistence for Narrative Continuity
Modulating NPC behavior via continuous numerical parameters stored in a shared memory schema may sustain dialogue continuity better than static natural language descriptors. Numeric values (e.g., "evasiveness": 0.0-1.0) persist across turns, allowing the LLM to maintain consistent character arcs that transient prompts cannot enforce.

## Foundational Learning

- **Concept: Fuzzy Logic Boundaries**
  - **Why needed here:** The framework relies on "fuzzy" numerical ranges (0.0 to 1.0) rather than binary states to control NPC behavior, enabling gradual shifts in traits like "evasiveness" or "guidance intensity."
  - **Quick check question:** How does a fuzzy logic range (0.0-1.0) differ from a discrete emotional state (e.g., "angry") in terms of prompt modulation?

- **Concept: Retrieval-Augmented Generation (RAG)**
  - **Why needed here:** Used to ground the "Interviewer" NPC in game lore without bloating the prompt, ensuring factual consistency while allowing the generative model to handle dialogue flow.
  - **Quick check question:** In this architecture, does RAG retrieve player dialogue history, game lore, or both?

- **Concept: Synthetic Evaluation (LLM-as-Judge)**
  - **Why needed here:** The study uses GPT-4o to evaluate dialogue variation and hallucination to complement small-sample human testing.
  - **Quick check question:** What is the primary risk of using an LLM judge to evaluate "creativity" or "believability"?

## Architecture Onboarding

- **Component map:** Unity Client -> Azure Cognitive Services (STT) -> Prompt Manager (JSON schemas + lore) -> Shared JSON Short-term Memory (npc_state) -> GPT-4o (RAG + State) -> OpenAI TTS (Streaming)

- **Critical path:**
  1. Player Speech -> STT Transcription
  2. Query Rewrite -> RAG Search (Lore + History) + Character Engine (JSON State lookup)
  3. Construct Prompt (Template + RAG Context + Fuzzy State)
  4. LLM Generation -> Stream to TTS
  5. Update Shared Memory (e.g., increase disclosure_prob if evidence presented)

- **Design tradeoffs:**
  - **HCP vs. LCP:** High Constraint ensures lore accuracy but adds latency (~3s) and reduces "liveliness"; Low Constraint is faster but risks hallucination.
  - **Resolution:** Role-specific scaffolding. Use JSON+RAG for Interviewers; use softer constraints for Suspects.

- **Failure signatures:**
  - **"On-rails" dialogue:** Over-scaffolding (JSON too rigid)
  - **Hallucination:** Under-scaffolding or RAG retrieval failure
  - **Latency kill:** Non-streaming TTS/LLM generation > 3 seconds
  - **State amnesia:** Shared JSON memory not updating or being pruned too aggressively

- **First 3 experiments:**
  1. **Role Sensitivity Test:** A/B test a JSON-strict prompt vs. a loose prompt on the Suspect role specifically; measure player ratings of "surprise" vs. "coherence."
  2. **Fuzzy State Validation:** Log the npc_state JSON changes over a session to verify if evasiveness actually decreases when evidence_count rises.
  3. **Latency Threshold:** Implement the streaming TTS pipeline and measure if perceived lag drops below the critical 3-second threshold.

## Open Questions the Paper Calls Out

### Open Question 1
At what point in repeated play sessions does improvisational variety shift from potentially confusing to genuinely valued by players? The usability study tested only single ~30-minute sessions per condition, capturing early-stage learning dynamics rather than long-run preferences. A longitudinal study tracking the same players across multiple sessions (e.g., 5–10 play sessions over weeks) would measure when preference shifts from consistency toward variety.

### Open Question 2
How do role-dependent scaffolding trade-offs manifest across different game genres beyond the detective/mystery format? The Interview prototype tested only one genre (detective game) with high-stakes interrogation dialogue; different genres may require different balance points between stability and improvisation. Replication studies applying the framework to contrasting genres (e.g., social simulation, RPG companions, educational games) would compare role-specific trade-off patterns.

### Open Question 3
How reliably do LLM judges align with human player perceptions when evaluating NPC dialogue quality across scaffold conditions? Synthetic evaluation used scripted prompts and a single LLM (GPT-4o) as judge; no human validation of the judge's ratings was conducted. A mixed-methods study comparing LLM judge scores against human player ratings on the same dialogue outputs would measure correlation and identify systematic divergences.

### Open Question 4
What is the optimal calibration of fuzzy parameter ranges (0.0–1.0) for different NPC role archetypes, and how sensitive is player experience to parameter tuning? Parameter values were designed a priori based on theoretical framing; no ablation or tuning experiments were conducted. Systematic parameter sweeps varying fuzzy ranges for each role type, with player experience metrics collected, would identify optimal settings and sensitivity thresholds.

## Limitations
- Small sample size (N=10) in usability study limits generalizability of experiential findings.
- Synthetic evaluation relies on GPT-4o as judge, which may inherently favor structured JSON+RAG outputs due to training bias.
- Fuzzy state mechanism lacks external validation—evidence comes from internal implementation rather than independent testing.

## Confidence
- **High confidence:** Role-dependent trade-offs (JSON+RAG improves Interviewer coherence while reducing Suspect believability) are supported by both human and synthetic evaluation data.
- **Medium confidence:** Players cannot perceptibly distinguish high- vs. low-constraint prompts in short sessions—supported by usability data but limited by small sample size.
- **Low confidence:** Fuzzy state persistence mechanism works as intended—described in architecture but lacks external validation.

## Next Checks
1. **Role Sensitivity Validation:** Conduct an A/B test specifically on Suspect NPCs comparing JSON-strict vs. loose prompts, measuring player ratings of "surprise" and "believability" to confirm the trade-off direction.
2. **Fuzzy State Logging:** Instrument the system to log all npc_state JSON changes during sessions, verifying that parameters like evasiveness actually decrease when evidence_count increases as intended.
3. **Judge Bias Mitigation:** Randomize synthetic evaluation prompts and neutralize labels (HCP vs. JSON+RAG) to test whether GPT-4o's scoring systematically favors structured outputs, potentially inflating JSON+RAG performance.