---
ver: rpa2
title: 'SEED: A Structural Encoder for Embedding-Driven Decoding in Time Series Prediction
  with LLMs'
arxiv_id: '2506.20167'
source_url: https://arxiv.org/abs/2506.20167
tags:
- seed
- time
- series
- structural
- semantic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes SEED, a novel framework that bridges the gap
  between structural encoding and semantic reasoning in multivariate time series forecasting
  by integrating a token-aware structural encoder with a frozen LLM-based decoder.
  The method uses patch projection and semantic reprogramming to map numerical time
  series into semantically grounded token sequences interpretable by LLMs.
---

# SEED: A Structural Encoder for Embedding-Driven Decoding in Time Series Prediction with LLMs

## Quick Facts
- arXiv ID: 2506.20167
- Source URL: https://arxiv.org/abs/2506.20167
- Reference count: 22
- Primary result: SEED achieves consistent improvements over strong baselines including Time-LLM, iTransformer, Autoformer, Informer, and Reformer, with significant reductions in MSE (e.g., 10x improvement on ETTh2 vs. Informer).

## Executive Summary
SEED introduces a novel framework that bridges structural encoding and semantic reasoning in multivariate time series forecasting. The method integrates a token-aware structural encoder with a frozen LLM-based decoder, using patch projection and semantic reprogramming to map numerical time series into semantically grounded token sequences interpretable by LLMs. Evaluated on eight benchmark datasets, SEED demonstrates consistent improvements over strong baselines while enabling efficient, zero-shot forecasting without LLM fine-tuning.

## Method Summary
SEED processes multivariate time series by first transposing the input to treat each variable as a distinct token, then applying variable-wise self-attention to capture structural dependencies. The encoded representations are reshaped into patches, linearly projected to match LLM embedding dimensions, and enhanced with learnable positional embeddings. Semantic reprogramming maps these patches to convex combinations of learnable textual prototypes, grounding numerical patterns in semantic space. Task prompts are prepended before feeding to a frozen LLM decoder, which generates predictions through a linear output head. The framework maintains the LLM frozen throughout training, updating only the encoder, projection, and prototype parameters.

## Key Results
- SEED achieves 10x improvement in MSE on ETTh2 compared to Informer
- Significant performance gains across eight benchmark datasets including ETTh1, ETTh2, ETTm1, ETTm2, Weather, Electricity, Traffic, and Solar-Energy
- Consistent improvements over strong baselines including Time-LLM, iTransformer, Autoformer, Informer, and Reformer
- Enables zero-shot forecasting without requiring LLM fine-tuning

## Why This Works (Mechanism)

### Mechanism 1: Variable-wise Tokenization with Inverted Attention
SEED treats each multivariate time series variable as a distinct token rather than time steps, applying self-attention over the variable dimension. This approach computes relationships between variables using multi-head attention derived from temporal projections of each variable's trajectory. The core assumption is that inter-variable structural dependencies are more critical to model explicitly than temporal token relationships for downstream LLM alignment. This works because structural relationships between variables provide a stable foundation for semantic alignment before temporal reasoning occurs.

### Mechanism 2: Patch Projection into LLM Embedding Space
The method projects fixed-length patches of encoded representations into the LLM's expected embedding dimension through linear transformation. Encoded output is reshaped, partitioned into non-overlapping patches, flattened, and linearly projected while adding learnable positional embeddings. This assumes that local temporal context within patches plus positional encoding suffices to preserve task-relevant structure. The projection matrix learns a general mapping without requiring LLM fine-tuning, enabling efficient adaptation to new domains.

### Mechanism 3: Prototype-Guided Semantic Reprogramming
SEED maps numerical patch embeddings to convex combinations of learnable textual prototypes, grounding them in semantic space for frozen LLM processing. Given K learnable prototypes, attention scores weight each prototype to create semantically grounded tokens. Task prompts are prepended before LLM decoding. This assumes prototypes can capture domain-relevant temporal patterns and that frozen LLMs can generalize from these semantic anchors without parameter updates, enabling zero-shot forecasting capabilities.

## Foundational Learning

- **Concept: Self-Attention over Arbitrary Dimensions**
  - Why needed here: SEED applies attention over the variable dimension (N) rather than sequence length; understanding that attention is permutation-invariant and dimension-agnostic clarifies why inverted attention works.
  - Quick check question: Given input (L=96 timesteps, N=7 variables), what shape are Q, K, V after transposition and projection?

- **Concept: Patch-based Tokenization in Time Series**
  - Why needed here: Converting continuous sequences into discrete patches is central to bridging numerical time series with token-based LLMs.
  - Quick check question: With L=96 and patch size P=16, how many patch tokens are produced per variable-grouped embedding?

- **Concept: Prompt Tuning / Input Reprogramming**
  - Why needed here: SEED keeps the LLM frozen; all adaptation occurs via prototype-based reprogramming and task prompts. Understanding prompt tuning clarifies why this avoids fine-tuning.
  - Quick check question: What parameters are updated during training vs. which remain frozen in SEED?

## Architecture Onboarding

- **Component map:** Input X → transpose → temporal projection φ → variable-attention layers → reshape to patches → W_p projection + positional embeddings → prototype attention → prepend task prompts → frozen LLM → linear head → prediction ŷ

- **Critical path:** Input X ∈ R^{L×N} → transpose → temporal projection φ → variable-attention layers → reshape to patches → W_p projection + positional embeddings → prototype attention (Equation 5-6) → prepend task prompts → frozen LLM → linear head → prediction ŷ

- **Design tradeoffs:**
  - Patch size P: Larger P captures more temporal context per token but reduces sequence length; default P=16 may not suit all periodicities
  - Number of prototypes K: More prototypes increase semantic coverage but risk overfitting; default K=8
  - LLM choice: Larger models (LLaMA-2) may offer better reasoning but increase inference cost; GPT-2 is lighter
  - Frozen vs. fine-tuned: Freezing enables zero-shot generalization but may underperform on datasets with extreme outliers (observed on Traffic)

- **Failure signatures:**
  - High MSE with low MAE: Model captures central tendency but misses extremes (seen on Traffic vs. Time-LLM)
  - Training instability: Check layer normalization and residual connections in encoder stack
  - Semantic misalignment: If prototype initialization is poor, attention weights collapse—monitor α distribution

- **First 3 experiments:**
  1. Reproduce ETTh2 baseline with default P=16, K=8, LLaMA-2 decoder; confirm MSE ~0.33 per Table I
  2. Ablation: vary patch size P ∈ {8, 16, 32} on Weather dataset; track MSE/MAE to identify optimal temporal granularity
  3. Ablation: vary prototype count K ∈ {4, 8, 16} on ECL dataset; analyze attention entropy to detect prototype utilization

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the SEED framework be adapted to better handle extreme outliers and long-tail error distributions without requiring fine-tuning of the frozen LLM decoder?
- Basis in paper: The authors explicitly note that SEED underperforms on Traffic and ETTh1, attributing this to "extreme outliers and long-tail error distributions" and the model's reduced effectiveness in capturing "subtle signal distortions" when the LLM is frozen.
- Why unresolved: The current semantic reprogramming mechanism relies on learnable prototypes which may smooth over or fail to adequately represent sparse, extreme values, creating a performance bottleneck on volatile datasets.
- What evidence would resolve it: Demonstrating a modified reprogramming technique (e.g., quantile-based prototypes or robust loss functions) that closes the performance gap with baselines like Time-LLM on the Traffic and ETTh1 datasets while keeping the LLM frozen.

### Open Question 2
- Question: Is the optimal number of textual prototypes (K=8) dependent on the complexity or dimensionality of the specific time series domain?
- Basis in paper: The paper specifies a default of K=8 learnable vectors "unless stated otherwise," but does not provide an ablation study or theoretical justification for why this fixed cardinality suffices for heterogeneous domains like Weather (low variance) vs. Traffic (high variance).
- Why unresolved: A fixed number of prototypes may act as an information bottleneck for high-dimensional data or introduce noise for simpler data, yet the sensitivity of the model to this hyperparameter remains unexplored.
- What evidence would resolve it: Results from an ablation study showing the impact of varying K (e.g., K ∈ {4, 8, 16, 32}) on the reconstruction fidelity of the semantic reprogramming layer and final forecasting MSE across different dataset complexities.

### Open Question 3
- Question: To what extent does the performance of SEED rely on the specific choice of the backbone LLM (e.g., GPT-2 vs. LLaMA-2) relative to the structural encoder?
- Basis in paper: While the paper mentions the framework is agnostic to the LLM, it relies on a frozen decoder for "semantic reasoning." It is unresolved whether the improvements stem primarily from the structural encoder or the LLM's inherent ability to interpret the reprogrammed embeddings.
- Why unresolved: Without comparing multiple LLM backbones within the identical SEED architecture, it is difficult to determine if the "semantic reasoning" capacity is robust across different language model architectures or overfitted to the specific characteristics of the one used.
- What evidence would resolve it: A comparative analysis of SEED's performance using different frozen decoders (e.g., GPT-2, LLaMA-2, Mistral) on the same benchmark suites to isolate the contribution of the LLM's pre-training from the encoder's structural modeling.

## Limitations
- Performance degradation on datasets with extreme outliers and long-tail error distributions, particularly Traffic and ETTh1
- Fixed hyperparameters (patch size P=16, prototype count K=8) may not be optimal across all domains and lack sensitivity analysis
- Reliance on structural dependencies between variables may not generalize to domains where temporal patterns dominate

## Confidence
- **High confidence (9/10):** The core architectural framework (structural encoder + patch projection + semantic reprogramming + frozen LLM) is well-specified and the reported improvements over strong baselines are substantial and consistent across multiple datasets.
- **Medium confidence (6/10):** The mechanism explanations are reasonable but rely on architectural intuition rather than ablation studies that isolate each component's contribution. The assumption that variable-wise attention is universally beneficial needs validation.
- **Low confidence (3/10):** The paper lacks sensitivity analysis for critical hyperparameters (patch size, prototype count, encoder depth) and does not provide error analysis showing where the model succeeds or fails on different pattern types.

## Next Checks
1. **Ablation study on structural encoding:** Compare SEED against a variant using temporal attention (standard self-attention over time steps) rather than variable-wise attention on datasets with varying inter-variable correlation structures to validate the inverted attention assumption.

2. **Hyperparameter sensitivity analysis:** Systematically vary patch size P ∈ {8, 16, 32} and prototype count K ∈ {4, 8, 16} across all eight datasets to identify optimal configurations and determine whether the default values are universally applicable.

3. **Error pattern analysis:** For datasets where SEED underperforms baselines (e.g., Traffic with high MSE), analyze prediction errors to determine whether failures stem from structural encoding limitations, semantic misalignment, or inability to capture extreme values.