---
ver: rpa2
title: Adapting Large Language Models for Multi-Domain Retrieval-Augmented-Generation
arxiv_id: '2504.02411'
source_url: https://arxiv.org/abs/2504.02411
tags:
- context
- llmeval
- multi-domain
- recall
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces a diverse multi-domain RAG benchmark spanning
  13 domains and 8 data sources to study out-of-domain generalization challenges.
  The authors systematically evaluate standard fine-tuning versus sequence-level knowledge
  distillation (SKD) with teacher-generated labels for RAG adaptation.
---

# Adapting Large Language Models for Multi-Domain Retrieval-Augmented-Generation

## Quick Facts
- **arXiv ID:** 2504.02411
- **Source URL:** https://arxiv.org/abs/2504.02411
- **Reference count:** 40
- **Primary result:** Sequence-level knowledge distillation with teacher-generated labels improves out-of-domain RAG performance compared to standard fine-tuning or vanilla RAG

## Executive Summary
This paper addresses the challenge of adapting large language models for retrieval-augmented generation (RAG) across multiple domains. The authors introduce a comprehensive multi-domain benchmark spanning biomedical, web-search, context-critical, and long-form domains to study out-of-domain generalization. They systematically compare standard fine-tuning against sequence-level knowledge distillation (SKD) with teacher-generated labels, finding that SKD significantly improves cross-domain performance while standard fine-tuning degrades on context-critical and long-form tasks due to distributional mismatch with short answer formats.

## Method Summary
The approach uses SPLADE-v3 for sparse retrieval and DeBERTa-v3 for reranking to obtain top-5 documents per query. These are combined with the query as input to a generator (Llama-3.2-1B/8B or Mistral-7B). Fine-tuning employs LoRA with different parameter subsets: Full-LoRA (all layers), LoRA-QKAtt (query/key matrices only), LoRA-Att (attention layers), and LoRA-MLP (MLP layers). The key innovation is sequence-level knowledge distillation where a teacher model (Mistral-7B) generates verbose labels that are used for training instead of short gold answers. Training uses LoRA with learning rate 5e-4, 1 epoch, AdamW optimizer, warmup 0.05, dropout 0.1, and alpha 32.

## Key Results
- Standard fine-tuning improves performance on general and web-search domains but degrades on context-critical and long-form datasets
- Sequence-level knowledge distillation with teacher-generated labels achieves 0.58 average LLMEval score versus 0.51 for vanilla RAG and 0.50 for standard fine-tuning
- LoRA-QK attention modification shows consistent improvements over Full-LoRA on oracle document utilization
- Small models (1B parameters) are particularly sensitive to noisy context, degrading with distractor documents

## Why This Works (Mechanism)

### Mechanism 1: Sequence-Level Knowledge Distillation Improves Cross-Domain Generalization
Training student LLMs on teacher-generated labels rather than short gold labels improves out-of-domain RAG performance. Teacher-generated labels are more verbose and coherent, containing explanations and reasoning from the retrieved context. This reduces distributional mismatch between training data and the model's natural generation distribution, allowing the model to focus on context exploitation rather than format mimicry. The assumption is that the teacher model generates sufficiently high-quality labels containing reasoning traces beneficial for student learning.

### Mechanism 2: LoRA-QK Attention Modification Enhances Context Exploitation
Fine-tuning only the Query (Q) and Key (K) matrices in attention layers improves the model's ability to identify relevant information in retrieved documents under domain shift. Q and K matrices determine attention patterns. By isolating updates to these components, the model learns to attend differently to context without disrupting learned representations in Value matrices or MLP layers. The assumption is that attention pattern modification is the primary bottleneck for cross-domain context exploitation.

### Mechanism 3: Short-Label Fine-Tuning Degrades Long-Form and Context-Critical Performance
Standard RAG fine-tuning on short answer datasets improves general and web-search domains but degrades performance on context-critical and long-form tasks. Short gold labels create stylistic mismatch with verbose model outputs. Models overfit to terse answer formats, losing ability to synthesize information across multiple retrieved documents for complex answers. The assumption is that the degradation is primarily due to answer-format mismatch rather than other training dynamics.

## Foundational Learning

- **Concept: Retrieval-Augmented Generation (RAG)**
  - Why needed here: The entire paper evaluates LLM adaptation specifically for RAG settings where models must exploit retrieved context
  - Quick check question: Can you explain why RAG helps with out-of-domain queries compared to relying on parametric knowledge alone?

- **Concept: Knowledge Distillation**
  - Why needed here: Sequence-level distillation from teacher to student is the primary intervention that improves cross-domain generalization
  - Quick check question: What is the difference between token-level and sequence-level distillation, and why might sequence-level be preferred for generation tasks?

- **Concept: Low-Rank Adaptation (LoRA)**
  - Why needed here: All fine-tuning experiments use LoRA variants; understanding parameter-efficient fine-tuning is necessary to interpret the adaptation strategies
  - Quick check question: Why does LoRA enable faster fine-tuning while preserving the pre-trained model's knowledge?

## Architecture Onboarding

- **Component map:** SPLADE-v3 (retriever) -> DeBERTa-v3 (reranker) -> Generator (Llama-3.2-1B/Mistral-7B) -> LLMEval (evaluation)

- **Critical path:** Query → SPLADE-v3 retrieves top-k documents → DeBERTa-v3 reranks documents → Top-5 documents + query formatted as prompt → Generator produces answer → LLMEval judges correctness against ground truth

- **Design tradeoffs:** Top-k selection: More documents provide coverage but introduce noise (smaller models degrade with noise); LoRA variant selection: Full-LoRA maximizes capacity but may overfit; LoRA-QKAtt targets attention specifically; Distillation teacher choice: Stronger teachers may not be better teachers

- **Failure signatures:** Short outputs on long-form tasks → model overfit to short-label training distribution; High recall but low faithfulness → model generates claims not grounded in retrieved context; Performance gap between oracle and retrieved documents → retriever or context-extraction bottleneck

- **First 3 experiments:** 1) Reproduce baseline comparison: Run vanilla RAG vs. Full-LoRA fine-tuning on MultiQA, evaluate on BioASQ and FiQA to confirm distillation benefit; 2) Ablate teacher model: Test whether a weaker teacher produces similar distillation gains to isolate whether quality or stylistic match matters more; 3) Probe attention patterns: Extract attention weights from LoRA-QKAtt vs. Full-LoRA on oracle documents to visualize how attention differs when context is gold-standard

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions, but based on the limitations section and methodology, several questions arise regarding the generalization of SKD improvements, the role of reasoning chains versus stylistic coherence, and the dependence on retrieval quality.

## Limitations

- The paper relies on teacher-generated labels without human evaluation or hallucination detection, risking propagation of errors
- LLMEval using SOLAR-10.7B as judge may favor verbose responses, potentially conflating judge preference with true capability
- Domain boundaries may not be orthogonal, with web-search domains overlapping significantly with training data

## Confidence

**High confidence:**
- Standard fine-tuning on short gold labels improves in-domain performance but degrades out-of-domain performance
- Sequence-level distillation with teacher-generated labels improves average performance across all domain clusters
- LoRA-QKAtt variant shows consistent improvements over Full-LoRA on oracle document utilization

**Medium confidence:**
- The mechanism that teacher-generated labels reduce distributional mismatch is plausible but not directly validated
- The degradation from short-label fine-tuning is primarily due to format mismatch rather than other training dynamics
- Attention modification is the primary bottleneck for cross-domain context exploitation

**Low confidence:**
- The specific LoRA-QK attention modification is optimal for RAG adaptation
- The observed improvements will transfer to models larger than 1B parameters
- The teacher model choice (Mistral-7B) is optimal or near-optimal for distillation

## Next Checks

1. **Teacher quality validation:** Evaluate the same sequence-level distillation using multiple teacher models (Mistral-7B, Llama-3-8B, GPT-4) and compare both performance and label quality metrics

2. **Metric triangulation:** Re-run the primary experiments using human evaluation on a subset of domains and alternative automated metrics (e.g., AnswerFactuality, RAGAS) to confirm that LLMEval scores reflect true answer quality

3. **Attention pattern analysis:** Extract and compare attention weights from LoRA-QKAtt, LoRA-Att, and Full-LoRA models when processing oracle vs. retrieved documents to empirically validate whether QK modification specifically improves context exploitation