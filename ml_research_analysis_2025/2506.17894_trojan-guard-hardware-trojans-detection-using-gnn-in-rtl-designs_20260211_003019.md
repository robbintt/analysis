---
ver: rpa2
title: 'TROJAN-GUARD: Hardware Trojans Detection Using GNN in RTL Designs'
arxiv_id: '2506.17894'
source_url: https://arxiv.org/abs/2506.17894
tags:
- graph
- hardware
- trojan
- designs
- detection
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Hardware Trojan (HT) detection in chip designs is challenging due
  to complex and large-scale RTL netlists and the covert nature of HTs. This paper
  presents a GNN-based framework that generates graph embeddings for large designs
  (e.g., RISC-V) and incorporates various GNN models (GCN, GAT, GIN) tailored for
  HT detection.
---

# TROJAN-GUARD: Hardware Trojans Detection Using GNN in RTL Designs

## Quick Facts
- arXiv ID: 2506.17894
- Source URL: https://arxiv.org/abs/2506.17894
- Reference count: 36
- Hardware trojan detection using GNN achieves 98.66% precision and 92.30% recall on RTL designs

## Executive Summary
Hardware Trojan (HT) detection in chip designs is challenging due to complex and large-scale RTL netlists and the covert nature of HTs. This paper presents a GNN-based framework that generates graph embeddings for large designs (e.g., RISC-V) and incorporates various GNN models (GCN, GAT, GIN) tailored for HT detection. It introduces 4-bit post-training quantization to reduce model size by 8× and speed up inference without significantly affecting detection accuracy. The method is evaluated on a custom dataset covering diverse RTL designs and multiple HT types. Results show a precision of 98.66% and recall of 92.30%, demonstrating effectiveness in detecting HTs in large-scale chip designs.

## Method Summary
The framework converts RTL Verilog designs into directed data flow graphs using Pyverilog parser, where nodes represent signals, constants, and operations with dependency edges. Multiple GNN architectures (GCN, GAT, GIN) are evaluated for graph-level binary classification, with 2-layer GCN achieving the best performance. The method incorporates 4-bit post-training quantization to reduce model size by 8×. The system is trained and evaluated on a custom dataset of 51 RTL designs containing both Trojan and clean versions, using 5-fold cross-validation to address class imbalance.

## Key Results
- GCN-2 achieves highest precision (98.66%), accuracy (92.4%), recall (92.30%), and F1-score (94.6%)
- 4-bit quantization reduces model size by 8× with only 6% precision drop (from 98.66% to 92.2%)
- GAT-5 shows severe overfitting with precision dropping to 31.2% on the custom dataset
- GCN-2 has lowest memory usage (1,688 MiB) among evaluated models

## Why This Works (Mechanism)

### Mechanism 1: RTL-to-Graph Structural Representation
Converting RTL netlists to directed data flow graphs preserves trojan-relevant structural patterns that GNNs can learn to distinguish. Pyverilog parses Verilog into an abstract syntax tree, then builds signal-level data flow graphs where nodes represent signals, constants, and operations (xor, and, concatenation, branch), and edges represent dependencies. These graphs are merged and pruned to create a unified representation G = (V, E). Trojan trigger and payload logic exhibit distinct structural signatures (e.g., rare conditional paths, unusual node connectivity) that differ from benign logic.

### Mechanism 2: Shallow GNN Architecture for Graph-Level Classification
Two-layer GNN architectures (particularly GCN) outperform deeper variants for HT detection, likely due to better generalization on limited training data. The GNN performs aggregation of neighbor features followed by node representation updates. Graph-level pooling produces a single embedding for classification. The 2-layer GCN achieved highest precision (98.66%) and recall (92.30%), while 5-layer models showed degraded performance (GCN-5: 84.8% precision, 70.7% recall).

### Mechanism 3: 4-bit Post-Training Quantization for Efficient Inference
Reducing weight precision from 32-bit float to 4-bit integers preserves detection accuracy while enabling 8× model compression. Post-training quantization maps full-precision weights W to quantized values Ŵ = round(W/S) × S using a scaling factor S. The 4-bit representation limits weights to 16 discrete levels. For the 2-layer GCN, precision dropped only from 98.66% to 92.2% after quantization.

## Foundational Learning

- **GNN Message Passing (Aggregation + Update)**: Understanding how GCN, GAT, and GIN differ in their aggregation strategies is essential for selecting the right architecture and debugging poor performance.
  - Quick check question: Can you explain why GAT uses attention coefficients (α_k_vu) while GCN uses a fixed normalized adjacency matrix?

- **RTL Design and Hardware Trojans**: The graph construction depends on understanding RTL constructs (modules, signals, always blocks) and trojan anatomy (trigger conditions, payload effects).
  - Quick check question: In the paper's trojan example, what condition causes the trigger to activate, and what does the payload do?

- **Quantization Fundamentals**: Understanding the trade-off between precision loss and memory/compute savings is critical for deployment decisions.
  - Quick check question: If a model has 1M parameters, how much memory does it require in 32-bit float vs. 4-bit quantized representation?

## Architecture Onboarding

- **Component map**: RTL Verilog -> Pyverilog parser -> Abstract Syntax Tree -> Data Flow Graph -> Merged/Pruned Graph with node features -> 2-layer GCN/GAT/GIN (200 hidden units, 0.5 dropout) -> Graph pooling (0.8 ratio) -> Fully connected layers -> Binary prediction (Trojan/No-Trojan) -> Optional 4-bit post-training quantization

- **Critical path**: RTL flattening and syntax fixing (multi-module designs) -> Graph construction quality (disconnected sub-graphs must be trimmed) -> GNN layer selection (use GCN-2 as baseline per paper results) -> Class imbalance handling (5-fold cross-validation recommended)

- **Design tradeoffs**: GCN vs. GAT vs. GIN: GCN-2 has best accuracy (92.4%) and lowest memory (1,688 MiB); GAT-5 has highest memory (5,296 MiB) with worse performance; Layer depth: 2-layer models consistently outperform 3-layer and 5-layer variants across all GNN types; Quantization: ~6% precision drop for 8× memory reduction; acceptable for resource-constrained deployment

- **Failure signatures**: Deep networks (5+ layers) showing >15% precision drop indicate overfitting; GAT-5 precision dropping to 31.2% suggests attention mechanism overfitting on small dataset; PIC design showing 100% metrics across methods indicates dataset too small for reliable evaluation

- **First 3 experiments**: 1) Baseline reproduction: Train GCN-2 on the TrustHub-derived dataset with 70/10/20 split, verify ~98% precision and ~92% recall using 5-fold cross-validation; 2) Ablation on graph features: Remove node type features (signal vs. operation) and measure performance degradation to understand feature importance; 3) Quantization sweep: Compare 4-bit, 8-bit, and 16-bit quantization on GCN-2 and GIN-5 to identify the optimal accuracy-efficiency tradeoff for your deployment constraints

## Open Questions the Paper Calls Out

- Can Quantization-Aware Training (QAT) recover the detection accuracy lost during post-training quantization?
- Can architectural modifications enable deeper GNNs to outperform shallow 2-layer models?
- How does the framework generalize to hardware trojans employing structural obfuscation or analog triggers?

## Limitations
- Limited dataset size (51 RTL designs) constrains generalization and may inflate reported metrics
- Custom dataset construction details are sparse, making independent validation challenging
- Node feature encoding scheme remains underspecified, potentially affecting reproducibility
- 4-bit quantization evaluation lacks comparison with quantization-aware training approaches

## Confidence
- **High Confidence**: The core claim that GNNs can detect hardware trojans from RTL graph representations is supported by systematic ablation studies across three GNN architectures
- **Medium Confidence**: The superiority of shallow (2-layer) architectures over deeper ones is well-demonstrated, though the mechanism requires further investigation
- **Medium Confidence**: The 8× model compression through 4-bit quantization is validated, but the ~6% accuracy drop and lack of comparison with alternative quantization methods limit confidence in deployment recommendations

## Next Checks
1. Apply the trained GCN-2 model to an external, independently-sourced RTL benchmark suite (e.g., OpenCores) to verify performance beyond the TrustHub-derived dataset
2. Conduct ablation studies systematically removing different node features to quantify their individual contributions to detection accuracy
3. Implement quantization-aware training (QAT) for the GCN-2 model and compare final precision/recall against the reported post-training quantization results