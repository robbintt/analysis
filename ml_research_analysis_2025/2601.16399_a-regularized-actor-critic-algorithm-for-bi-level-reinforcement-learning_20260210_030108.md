---
ver: rpa2
title: A Regularized Actor-Critic Algorithm for Bi-Level Reinforcement Learning
arxiv_id: '2601.16399'
source_url: https://arxiv.org/abs/2601.16399
tags:
- lemma
- algorithm
- bi-level
- where
- inequality
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a single-loop, first-order actor-critic algorithm
  for bi-level reinforcement learning, where the upper-level variable parameterizes
  the reward of a lower-level MDP. The key innovation is introducing an attenuating
  entropy regularization to the lower-level objective, enabling asymptotically unbiased
  hyper-gradient estimation without requiring exact solution of the unregularized
  RL problem.
---

# A Regularized Actor-Critic Algorithm for Bi-Level Reinforcement Learning

## Quick Facts
- **arXiv ID**: 2601.16399
- **Source URL**: https://arxiv.org/abs/2601.16399
- **Reference count**: 40
- **Key outcome**: Proposes a single-loop, first-order actor-critic algorithm for bi-level RL with sample complexity O(ϵ⁻¹⁰), improving over existing methods by introducing attenuating entropy regularization to enable asymptotically unbiased hyper-gradient estimation.

## Executive Summary
This paper addresses bi-level reinforcement learning, where an upper-level variable parameterizes the reward of a lower-level Markov decision process. The proposed algorithm introduces attenuating entropy regularization to the lower-level objective, enabling asymptotically unbiased hyper-gradient estimation without requiring exact solution of the unregularized RL problem. This allows a fully first-order method to converge to a stationary point of the original bi-level objective with improved sample complexity. The method is validated on both a GridWorld goal placement task and a language model fine-tuning task for happy tweet generation.

## Method Summary
The algorithm optimizes a bi-level RL problem via a penalty-based reformulation that avoids second-order information. The upper-level variable (e.g., reward parameters) is updated using a hyper-gradient estimate computed from trajectories of two policies: the main policy and an auxiliary policy that optimizes the penalty-augmented Lagrangian. Entropy regularization is added to the lower-level objective with a decaying weight τ_k, which ensures the regularized solution converges to the unregularized optimum. A multi-time-scale update structure (with step sizes ζ_k ≪ α_k ≪ β_k) approximates nested-loop optimization in a single loop. The algorithm supports both decaying τ_k for unbiased convergence (O(ϵ⁻¹⁰) complexity) and constant τ for faster convergence to a regularized objective (O(ϵ⁻³) complexity).

## Key Results
- Achieves O(ϵ⁻¹⁰) sample complexity for convergence to a stationary point of the original bi-level RL objective, improving over existing methods
- Demonstrates faster convergence and better performance than comparable baselines on both GridWorld goal placement and language model fine-tuning for happy tweet generation
- Supports constant regularization for faster O(ϵ⁻³) convergence to the regularized objective when exact recovery of the original problem is not required
- Validates the effectiveness of the penalty-based first-order reformulation and attenuating entropy regularization through empirical results

## Why This Works (Mechanism)

### Mechanism 1: Attenuating Entropy Regularization
The paper introduces entropy regularization to the lower-level RL objective with a decaying weight τ_k. This creates a time-varying optimization landscape that satisfies a regularization-dependent PL condition, ensuring tractable convergence. As τ_k → 0, the regularized optimum converges to the unique entropy-maximizing optimum of the original problem. The algorithm tracks this moving target, recovering the unregularized solution asymptotically. Core assumptions include ergodic MDP dynamics and initial state coverage, plus a PL condition dependent on the regularization parameter.

### Mechanism 2: Penalty-Based First-Order Reformulation
Instead of using implicit function theorem-based hyper-gradients requiring second-order information, the paper reformulates the bi-level problem via a penalty term. This creates a Lagrangian-like objective where the hyper-gradient can be expressed using only first-order terms, bypassing Hessian estimation. The penalty weight w_k is decayed to asymptotically enforce the lower-level optimality constraint. The approach requires the Lagrangian to satisfy a penalty-weight dependent PL condition and assumes unique minimizers exist for all positive w and τ.

### Mechanism 3: Multi-Time-Scale Single-Loop Actor-Critic
The algorithm updates five components (upper variable, two policies, two critics) in a single loop with carefully ordered step sizes. The critic learns fastest (β_k), followed by the actor (α_k), then the upper-level variable (ζ_k). This separation approximates nested-loop optimization where the inner loop fully converges before outer updates, while sharing samples for efficiency. Specific decay schedules (c_ζ=9/10, c_α=1/2, c_β=1/2) must satisfy inequality constraints balancing the decay rates of w_k and τ_k with update rates.

## Foundational Learning

### Concept: Bi-Level Optimization
**Why needed here**: The entire problem formulation is bi-level: an upper-level variable (e.g., reward parameter) controls a lower-level RL problem, and the upper objective depends on the lower-level's optimal solution.
**Quick check question**: Can you explain the difference between a standard single-level optimization problem and a bi-level one where the upper-level constraint is that the lower-level variable must be optimal?

### Concept: Entropy Regularization in Reinforcement Learning
**Why needed here**: The core innovation uses entropy regularization (adding -τ * Σ π(a|s) log π(a|s) to the reward) to make the lower-level objective satisfy a strong PL condition, which is key for provable convergence.
**Quick check question**: What effect does adding an entropy bonus to the policy's objective have on the learned policy's behavior, and why might this improve optimization landscape properties?

### Concept: Actor-Critic Algorithms
**Why needed here**: The proposed algorithm is a specific instantiation of the actor-critic framework, where an "actor" updates the policy and a "critic" estimates the value function to reduce variance in gradient estimates.
**Quick check question**: In an actor-critic method, what is the role of the critic, and how does its estimate contribute to the actor's policy update?

## Architecture Onboarding

### Component map
Upper variable x → Penalty weight w_k → Regularization τ_k → Lower policy θ → Auxiliary policy θ^L → Critic V̂ → Auxiliary critic V̂^L

### Critical path
1. Sample two trajectories from policies θ_k and θ^L_k
2. Compute stochastic gradients for x_k, θ_k, θ^L_k, and TD errors for V̂_k, V̂^L_k
3. Update x_k using the penalty-based hyper-gradient estimate
4. Update θ_k and θ^L_k via entropy-regularized policy gradients
5. Update V̂_k and V̂^L_k via TD learning
6. Decay τ_k and w_k

### Design tradeoffs
- *Decaying vs. Constant τ*: Decaying τ targets the original unregularized objective (O(ϵ⁻¹⁰) complexity) but is more complex to tune. Constant τ targets the regularized objective (O(ϵ⁻³) complexity) with simpler analysis but yields a biased solution to the original problem.
- *Single-loop vs. Nested-loop*: Single-loop is more practical and sample-efficient but requires careful multi-time-scale analysis. Nested-loop offers clearer theoretical analysis but is less practical due to expensive inner-loop solves.

### Failure signatures
1. **Divergence/Violent oscillations**: Likely due to improper time-scale separation (ζ_k too large relative to α_k) or step-size schedules that violate the required inequality constraints.
2. **Convergence to sub-optimal policy (when using decaying τ)**: Likely τ_k decays too slowly, so algorithm remains in highly regularized regime.
3. **High variance in gradient estimates**: Critic (V̂) learning rate β_k too small or bootstrap target too noisy.

### First 3 experiments
1. **Implement core algorithm in a tabular GridWorld** (as in paper's Section 5). Verify that Φ(x_k) decreases and compare convergence speed against the "Partial SGD" baseline. This tests the basic single-loop update.
2. **Ablation on regularization schedules**: Test three variants: (a) Proposed decaying τ_k, (b) Large constant τ, (c) Small constant τ. Measure final Φ(x) value to confirm decaying τ recovers the better solution. This validates Mechanism 1.
3. **Sensitivity analysis on time-scale ratios**: Fix α_k, β_k schedules and vary the initial ζ_0 to intentionally break the ζ_k ≪ α_k condition. Plot the stability boundary to empirically find the required ratio. This tests Mechanism 3.

## Open Questions the Paper Calls Out

### Open Question 1
**Question**: Can the convergence rate of the proposed algorithm be improved by incorporating variance reduction and/or momentum?
**Basis in paper**: The authors state in Section 6: "A future direction is to investigate whether the convergence rate can be improved by incorporating variance reduction and/or momentum... Momentum has been shown to improve the convergence rate of penalty-based bi-level optimization algorithms under lower-level strong convexity, giving promise that it may also lead to convergence acceleration under our weaker version of PL condition."
**Why unresolved**: The current theoretical analysis is for the basic stochastic gradient update without variance reduction or momentum techniques, which are known to accelerate convergence in related optimization problems.
**What evidence would resolve it**: A formal finite-time analysis of the algorithm with variance reduction (e.g., STORM, SPIDER) or momentum (e.g., Adam, heavy-ball), or extensive empirical demonstration of improved sample complexity on the same benchmark tasks.

### Open Question 2
**Question**: Can the algorithm and its convergence guarantees be extended to the setting where the transition kernel depends on the upper-level variable?
**Basis in paper**: The authors note in Remark 2: "If the transition kernel P were a function of x, the penalty-based method discussed prior to (14) would still remain valid, but the gradient ∇ₓJτ(x, π) would become more challenging to derive and estimate using samples. Extending the algorithm and analysis to the setting of x-dependent transition kernel can be an important future work."
**Why unresolved**: The current formulation and proof techniques rely on the simplifying assumption that the transition dynamics are fixed and independent of the upper-level parameter x, which restricts the scope of applicability.
**What evidence would resolve it**: A modified algorithm and its sample complexity analysis that handles the more general case where the MDP dynamics are also parameterized by the upper-level variable.

### Open Question 3
**Question**: Do the theoretical convergence guarantees hold for general function approximation beyond tabular softmax policies?
**Basis in paper**: The theoretical analysis in Section 4 and Appendix C is explicitly developed for tabular softmax parameterization. The authors note in a footnote that "the algorithm is compatible with any function approximation in practical implementations," but no finite-sample guarantees are provided for neural network or other function approximators.
**Why unresolved**: Extending the analysis requires handling the approximation error in policy representation and ensuring that the regularization-dependent PL condition (Assumption 5) holds or is appropriately adapted in function space.
**What evidence would resolve it**: A convergence analysis that incorporates function approximation error bounds, or empirical validation demonstrating that the algorithm achieves the same or comparable convergence rates with deep neural network policies on large-scale problems.

## Limitations
- The theoretical analysis relies heavily on specific decay schedules for τ_k and w_k that are not specified numerically, limiting practical applicability
- The O(ϵ⁻¹⁰) sample complexity bound, while better than existing methods, is still quite high and may require many samples in practice
- Experiments are limited to a small tabular environment and a simulated preference learning task, leaving performance on large-scale continuous control or real-world RLHF applications uncertain

## Confidence

### High Confidence
- The convergence proofs for the constant regularization variant (O(ϵ⁻³) complexity) are robust, as they rely on standard PL condition assumptions without the added complexity of tracking moving targets

### Medium Confidence
- The theoretical claims for the decaying regularization variant are sound within the paper's assumptions, but the practical performance depends heavily on correct scheduling of τ_k and w_k, which is not specified beyond the theorem's requirements
- The experimental results demonstrate the algorithm's effectiveness on the presented tasks, but the limited scope and lack of comparison to a wider range of baselines reduce generalizability

## Next Checks

1. **Cross-Environment Generalization**: Implement the algorithm on a different bi-level RL task, such as a continuous control problem (e.g., goal placement in MuJoCo environments) or a different reward shaping scenario. This will test the method's applicability beyond the specific GridWorld setup.

2. **Schedule Sensitivity Analysis**: Systematically vary the decay rates of τ_k and w_k (e.g., c_τ ∈ [0.01, 0.1], c_w ∈ [0.1, 0.3]) and measure the impact on convergence speed and final solution quality. This will provide empirical guidance on schedule tuning.

3. **Large-Scale RLHF Benchmark**: Evaluate the method on a real-world RLHF task, such as fine-tuning a larger language model (e.g., 1B+ parameters) on actual human preference data (e.g., from the Anthropic Helpful and Harmless dataset). This will test scalability and real-world performance.