---
ver: rpa2
title: 'GroupDebate: Enhancing the Efficiency of Multi-Agent Debate Using Group Discussion'
arxiv_id: '2409.14051'
source_url: https://arxiv.org/abs/2409.14051
tags:
- uni00000013
- agents
- token
- cost
- debate
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the scalability issue of multi-agent debate
  systems, where increasing the number of agents and debate rounds leads to rapidly
  escalating token costs that limit broader application. The proposed GroupDebate
  method divides agents into multiple debate groups that conduct internal debates
  and share summaries between groups, enabling more efficient collaboration while
  preserving viewpoint diversity.
---

# GroupDebate: Enhancing the Efficiency of Multi-Agent Debate Using Group Discussion

## Quick Facts
- arXiv ID: 2409.14051
- Source URL: https://arxiv.org/abs/2409.14051
- Authors: Tongxuan Liu, Xingyu Wang, Weizhe Huang, Wenjiang Xu, Yuting Zeng, Lei Jiang, Hailong Yang, Jing Li
- Reference count: 40
- Reduces multi-agent debate token costs by up to 46.9% while maintaining or improving accuracy across five reasoning datasets

## Executive Summary
This paper addresses the scalability challenge in multi-agent debate systems where increasing agent count and debate rounds leads to exponential token cost growth. The proposed GroupDebate method partitions agents into groups that conduct internal debates and share compressed summaries between groups, achieving significant computational efficiency while preserving reasoning performance. Theoretical analysis demonstrates reduced token complexity from O(M²T + MT²) to O(MTQ + (M²T/N + MSN)C), and extensive experiments across five datasets show token reductions of 20.4-46.9% with accuracy improvements of up to 21.9% on challenging reasoning tasks.

## Method Summary
GroupDebate divides M agents into N groups of K=M/N agents each, organizing T total debate rounds into S stages with R rounds per stage (T=S×R). Each stage begins with intra-group debate where agents share full outputs within their group. At stage boundaries, each group generates a compressed summary of their deliberation which is shared with all other groups. Agents in subsequent stages receive only the latest summaries from other groups rather than full conversation histories, implementing a "forgetfulness" mechanism. The process concludes with majority voting across all agent outputs to determine the final answer.

## Key Results
- Achieves 20.4-46.9% token cost reduction across Arithmetic, GSM8K, MMLU, MATH, and GPQA datasets
- Maintains or improves accuracy compared to standard multi-agent debate, with up to 21.9% accuracy improvement on GPQA
- Optimal configuration found at R=2 intra-group rounds, with accuracy declining when rounds exceed 4
- Best accuracy achieved with balanced group partitions (e.g., 3+3 for 6 agents vs 4+2)

## Why This Works (Mechanism)

### Mechanism 1
Dividing agents into groups reduces token complexity from O(M²T + MT²) to O(M²T/N + MSN). Standard MAD requires each agent to receive inputs from all other agents in every round, creating quadratic scaling with agent count M. By partitioning M agents into N groups of size K=M/N, intra-group communication scales with K² per group rather than M² globally. Inter-group communication occurs only S times through compressed summaries. Core assumption: Agents within a group can converge toward useful intermediate conclusions before cross-group synchronization is necessary. Break condition: If groups are too small (K→1), intra-group debate becomes trivial; if groups are too large (N→1), the method degenerates to standard MAD with no efficiency gain.

### Mechanism 2
Sharing summaries between groups at stage boundaries preserves viewpoint diversity while preventing unbounded context growth. Rather than accumulating full conversation histories across all agents, GroupDebate compresses each group's deliberation into a summary of bounded length (≤80 words per prompt in experiments). Each agent receives only the latest stage's summaries from other groups, not the full debate history—this is the "forgetfulness" mechanism adapted from prior work. Core assumption: Critical information survives compression into summaries; historical context beyond the most recent stage provides diminishing returns. Break condition: If summaries lose critical reasoning steps or group conclusions are incorrect, error propagation across stages may compound rather than self-correct.

### Mechanism 3
Brief intra-group debate rounds (R=2 optimal in experiments) achieve accuracy gains while minimizing overhead. Each stage contains R rounds of intra-group debate. The experiments show R=2 produces highest accuracy—sufficient for groups to refine initial responses through peer critique, but not so long that token costs escalate or groupthink emerges. More rounds reduce the number of stages S for fixed total rounds T, trading inter-group diversity for intra-group convergence. Core assumption: A small number of rounds allows productive disagreement without premature convergence to incorrect consensus. Break condition: If tasks require deeper multi-step reasoning within groups before cross-group comparison, R=2 may be insufficient.

## Foundational Learning

- **Concept: Multi-Agent Debate (MAD) fundamentals**
  - Why needed here: GroupDebate is a modification of MAD; understanding the baseline clarifies what problem is being solved.
  - Quick check question: In standard MAD with M agents and T rounds, why does token cost grow quadratically with M?

- **Concept: Big-O token complexity analysis**
  - Why needed here: The paper's primary contribution is quantified through complexity reduction; reading the formulas requires this literacy.
  - Quick check question: What is the difference in the dominant term between O(M²T) and O(M²T/N) when N=M/2?

- **Concept: Hierarchical communication in distributed systems**
  - Why needed here: GroupDebate's two-level structure (intra-group, inter-group) mirrors hierarchical consensus protocols.
  - Quick check question: Why might a system designer choose to synchronize groups less frequently than individuals within a group?

## Architecture Onboarding

- **Component map:**
  1. Agent pool (M agents, randomly assigned)
  2. Group partitioner (N groups of ~K=M/N agents each)
  3. Stage controller (S stages, each with R rounds; T=S×R)
  4. Intra-group debate loop (agents share full outputs within group)
  5. Summary generator (LLM compresses group outputs to ≤80 words)
  6. Summary pool (shared storage for inter-group communication)
  7. Majority voter (final aggregation across all agent outputs)

- **Critical path:**
  1. Initialize agents with question Q (Round 1, Stage 1)
  2. For each stage s: run R rounds of intra-group debate per group
  3. At stage end (except final stage): generate and pool summaries
  4. At stage start (except first stage): inject pooled summaries as context
  5. After S stages: collect final outputs and vote

- **Design tradeoffs:**
  - N (number of groups): Higher N → lower token cost but fewer agents per group → potentially weaker intra-group reasoning
  - R (intra-group rounds): Higher R → deeper local refinement but fewer stages → less inter-group diversity exposure
  - S (stages): Higher S → more inter-group synthesis but increased summary overhead (MSN term)
  - Group assignment: Random vs. capability-stratified; paper uses random but optimal assignment is unexplored

- **Failure signatures:**
  1. Context overflow: MAD baseline hits token limits at M≥6 on MATH dataset; GroupDebate should handle this but verify prompt lengths at scale
  2. Accuracy degradation with excess rounds: Paper notes accuracy declines when rounds >4 (Figure 7), likely due to over-thinking or drift
  3. Unbalanced group sizes: If M is not divisible by N, uneven groups may bias voting

- **First 3 experiments:**
  1. Reproduce token reduction claim: Run MAD(5,3) vs. GD(5,3) on GSM8K with GPT-3.5-turbo; expect ~45% token reduction per Table 1
  2. Ablate group structure: Compare GD(6,4) with groupings [3+3], [2+2+2], [4+2] on MMLU; verify 3+3 achieves highest accuracy per Table 2
  3. Stress-test context limits: Run MAD(8,4) and GD(8,4) on MATH; confirm MAD fails while GD completes, measuring token headroom

## Open Questions the Paper Calls Out

- **Question 1:** What are the theoretically optimal values for the number of groups (N) and stages (S) to maximize reasoning accuracy under a specific token budget?
  - Basis: Section 7 states optimal N and S settings require more evaluations and experiments.
  - Why unresolved: While token cost complexity is derived, the accuracy-maximizing configuration remains unexplored.
  - Evidence needed: Theoretical model or comprehensive empirical analysis mapping N, S, and accuracy-cost trade-off.

- **Question 2:** What is the theoretical mechanism that causes GroupDebate to improve accuracy compared to standard multi-agent debate?
  - Basis: Section 6 proposes to explore the theorem of how group discussion improves accuracy.
  - Why unresolved: Authors hypothesize diversity preservation drives accuracy but lack formal proof.
  - Evidence needed: Formal proofs or specific ablation studies quantifying the relationship between group structures, viewpoint diversity, and reasoning correctness.

- **Question 3:** What are the scaling laws governing the relationship between token cost, agent count, and reasoning capabilities in multi-agent debate frameworks?
  - Basis: Section 4.4 notes it's an intriguing research point to explore scaling laws about accuracy and efficiency.
  - Why unresolved: Paper observes accuracy "sharply increasing points" but does not formalize the mathematical relationship.
  - Evidence needed: Formal scaling laws predicting performance saturation and emergent capability thresholds based on computational investment.

## Limitations

- Optimal configuration parameters (N, R, S) are highly dataset-dependent without clear theoretical guidance for selection
- Summary compression mechanism assumes critical reasoning survives 80-word summaries without validation across diverse problem types
- Random group assignment strategy may not optimize for heterogeneous agent capabilities or task-specific reasoning patterns
- Majority voting mechanism's robustness to correlated errors across groups is not thoroughly examined

## Confidence

- **High confidence:** Token complexity reduction (O(M²T/N + MSN) vs O(M²T + MT²)) - direct mathematical derivation from algorithm structure
- **Medium confidence:** Accuracy improvements across datasets - experimental results are consistent but limited to specific model versions and datasets
- **Medium confidence:** Optimal R=2 configuration - empirical finding supported by ablation studies but lacks theoretical justification
- **Low confidence:** Generalization to arbitrary LLMs and task domains - current validation covers only specific reasoning tasks with particular model families

## Next Checks

1. **Summary compression validation:** Implement a blinded evaluation where human judges assess whether 80-word summaries preserve the essential reasoning steps from full group outputs across 50 diverse problems from GSM8K and MATH datasets.

2. **Configuration optimization study:** Systematically vary N (group count) from 2 to M/2 while holding M constant, and measure accuracy-token tradeoff curves on MMLU to derive dataset-specific configuration recommendations.

3. **Error correlation analysis:** Track answer agreement patterns across groups and stages on GPQA to determine whether GroupDebate's grouping structure creates new failure modes through error propagation between stages.