---
ver: rpa2
title: Agentic Design of Compositional Machines
arxiv_id: '2510.14980'
source_url: https://arxiv.org/abs/2510.14980
tags:
- block
- machine
- design
- blocks
- orientation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces compositional machine design as a task where
  standardized mechanical parts are assembled to achieve functional goals like locomotion
  or manipulation. The authors develop BesiegeField, an interactive environment based
  on the game Besiege, enabling part-based construction, physical simulation, and
  reward-driven evaluation.
---

# Agentic Design of Compositional Machines

## Quick Facts
- arXiv ID: 2510.14980
- Source URL: https://arxiv.org/abs/2510.14980
- Reference count: 40
- Primary result: Benchmarks LLMs on compositional machine design, showing iterative multi-agent refinement with simulation feedback outperforms single-agent generation.

## Executive Summary
This paper introduces compositional machine design as a benchmark task where agents must assemble standardized mechanical parts to achieve functional goals like locomotion or manipulation. The authors develop BesiegeField, an interactive environment based on the game Besiege, enabling part-based construction, physical simulation, and reward-driven evaluation. They systematically evaluate state-of-the-art LLMs using agentic workflows, finding that spatial reasoning, strategic assembly, and instruction-following are key capabilities required for success. Since open-source models fall short, the paper explores reinforcement learning as a path to improvement, curating a cold-start dataset, conducting RL finetuning experiments, and highlighting open challenges at the intersection of language, machine design, and physical reasoning.

## Method Summary
The paper develops BesiegeField, an interactive environment for compositional machine design based on the game Besiege, enabling part-based construction with ~80 standardized block types, physical simulation, and reward-driven evaluation. The method employs agentic workflows with multiple specialized roles (designer, inspector, refiner, querier) that iteratively improve machine designs through environment feedback. The workflow uses hierarchical blueprint decomposition via a meta-designer agent that produces high-level functional blueprints guiding subsequent block-by-block construction. For model improvement, the authors curate a cold-start dataset of 9,984 machine-CoT pairs and conduct reinforcement learning finetuning using GRPO with reward shaping that combines validity flags and task-specific performance metrics.

## Key Results
- Agentic systems with iterative multi-agent refinement and simulation feedback significantly outperform single-agent generation in compositional machine design tasks.
- Current open-source LLMs show limitations in spatial reasoning, strategic assembly, and instruction-following required for successful machine design.
- Reinforcement learning with simulation-derived rewards can improve validity rates and maximum performance, but requires careful reward shaping and cold-start data to avoid collapse to narrow strategies.

## Why This Works (Mechanism)

### Mechanism 1: Iterative Multi-Agent Refinement with Simulation Feedback
Agentic systems with multiple specialized roles (designer, inspector, refiner, querier) coupled with environment feedback from physics simulation can iteratively improve compositional machine designs beyond single-agent generation. The workflow decomposes the design task into role-specific reasoning: the designer generates an initial plan, the inspector evaluates abstract structural flaws, the refiner proposes modifications, and the environment querier selectively extracts targeted feedback (e.g., stress points, velocities) from simulated runs. This cycle allows models to correct orientation errors, attachment failures, and instability issues that single-pass generation cannot catch. Core assumption: LLMs can interpret and act on structured simulation feedback to revise spatial reasoning errors; the feedback-to-modification mapping is learnable in-context.

### Mechanism 2: Reinforcement Learning with Verifiable Rewards (RLVR) for Design Strategy Consolidation
RL finetuning with simulation-derived rewards can improve the validity rate and maximum performance of generated machines by consolidating successful design patterns into model weights. A cold-start dataset aligns the model's reasoning to expert-like chain-of-thought. GRPO with pass@k advantage estimation then optimizes a reward combining validity flags (parse success, collision-free, constraint satisfaction) and task-specific performance (e.g., throw distance × height). KL regularization prevents collapse to narrow strategies. Core assumption: Valid design trajectories can be differentiated from invalid ones via reward shaping; the simulator provides reliable, differentiable-enough signals for policy improvement.

### Mechanism 3: Hierarchical Blueprint Decomposition via Meta-Designer
A meta-designer agent that first produces a high-level functional blueprint can improve design consistency and reduce variance by guiding subsequent block-by-block construction. The meta-designer analyzes requirements and outputs a JSON blueprint decomposing the machine into functional blocks (e.g., "Base Frame," "Powered Throwing Arm") with recommended block types and guidance. Builder agents then autoregressively construct each block, with valid designs passed to the next stage. Core assumption: LLMs can reliably decompose functional requirements into macro-level mechanical components, and lower-level agents can translate these blueprints into geometrically valid structures.

## Foundational Learning

- **Compositional Reasoning**
  - Why needed here: The core task requires understanding how discrete parts combine into functional assemblies with emergent behavior—this is not just pattern matching but reasoning about part-part and part-whole relationships under physics.
  - Quick check question: Given three blocks (wheel, hinge, chassis), can you predict whether they form a functional steering mechanism when connected in a specific topology?

- **Physical Simulation Literacy**
  - Why needed here: Designs must satisfy real-world constraints (gravity, torque, collision, material strength); without intuitive physics, LLMs may produce visually plausible but dynamically invalid machines.
  - Quick check question: If a lever arm has a heavy counterweight at one end and is mounted on an unbraced tower, will the tower collapse when the arm swings? Why?

- **Multi-Agent Coordination**
  - Why needed here: The agentic workflow requires agents to share structured state (JSON blueprints, simulation logs, defect reports) and maintain consistency across modification rounds.
  - Quick check question: How should an environment querier decide which block's velocity data to request after observing a structural failure at 0.4s?

## Architecture Onboarding

- **Component map**: BesiegeField Environment -> LLM Agent Pool -> Simulation-to-Reward Pipeline -> RL Finetuning Infrastructure -> Cold-Start Dataset
- **Critical path**:
  1. User specifies functional goal (e.g., "throw boulder max distance")
  2. Meta-designer decomposes into functional blocks (optional but recommended)
  3. Designer generates initial JSON construction tree
  4. Inspector evaluates abstract flaws; refiner proposes edits
  5. Environment querier runs simulation, extracts targeted feedback
  6. Refiner revises based on feedback; cycle repeats for N rounds
  7. For RL: Rollout designs → compute reward → update policy

- **Design tradeoffs**:
  - Position-based vs. Construction-tree representation: Construction trees explicitly encode parent-child face attachments, improving LLM understanding at the cost of parsing complexity. Paper shows construction trees outperform position-only representations (Table 7).
  - Pass@1 vs. Pass@k advantage estimators: Pass@k better explores diverse designs but requires more rollouts per step.
  - Detailed vs. high-level meta-designer guidance: More detailed step-by-step instructions help stronger models but may overwhelm weaker ones (Table 10).

- **Failure signatures**:
  - Incorrect part orientations: Wheels face wrong direction, hinges swing into collisions
  - Structural collapse under load: Unbraced towers buckle when torque applied
  - CoT-machine misalignment: High-level reasoning describes a trebuchet, but output JSON attaches the rotating block incorrectly
  - Reward hacking: Boulder dropped behind walls instead of thrown (mitigated by adding height thresholds)

- **First 3 experiments**:
  1. Baseline agent capability audit: Run single-agent generation for car and catapult tasks across 3-5 LLMs; measure file validity, spatial validity, and max score.
  2. Feedback ablation: Compare full environment feedback (block-level position/velocity) vs. reward-only vs. no feedback in the iterative editing loop.
  3. Cold-start + RL minimal viable test: Fine-tune Qwen-2.5-14B on a 500-example subset of the cold-start dataset, then run 100 GRPO steps with pass@8.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can compositional machine design frameworks effectively model the interplay between physical structure, control policy, and compositional constraints?
- Basis in paper: [explicit] The paper states, "A key open problem is therefore how to account for the interplay among physics, control policy, and compositional structure in machine design."
- Why unresolved: The current BesiegeField environment and experiments use a shared control policy to isolate structural composition, deliberately excluding this complex coupling.
- What evidence would resolve it: An agent capable of co-designing a machine's structure and its specific control policy to maximize a functional reward, rather than relying on a fixed operational policy.

### Open Question 2
- Question: Can reinforcement learning finetuning methods be modified to maintain diverse design strategies and prevent models from collapsing into narrow structural solutions?
- Basis in paper: [explicit] The authors highlight the need for "RL finetuning methods that prevent models from collapsing into a narrow set of strategies and structures."
- Why unresolved: The authors observed that standard RL (GRPO) leads to rapid entropy collapse, where models refine details but fail to explore novel high-level strategies.
- What evidence would resolve it: A modified RLVR algorithm in BesiegeField that maintains high output entropy (design diversity) while achieving superior or equivalent task performance compared to standard GRPO.

### Open Question 3
- Question: Does integrating visual feedback or 3D spatial grounding significantly enhance LLM performance in compositional machine design tasks compared to pure text-based reasoning?
- Basis in paper: [inferred] The paper notes that "Multimodal reasoning is also important because effective machine design typically relies on integrating textual descriptions with visual or schematic representations," but limits the study to text-only agents.
- Why unresolved: The current benchmark relies on pure LLM-based reasoning to isolate capabilities, despite the authors observing that providing parsed 3D information improves performance.
- What evidence would resolve it: A comparative benchmark in BesiegeField where multimodal agents (vision-language models) outperform text-only baselines in spatial validity and functional performance.

## Limitations

- The RLVR mechanism's success depends on maintaining gradient flow from sparse validity rewards, which may not scale well to more complex machine assemblies.
- The small cold-start dataset (9,984 examples from a single game community) may limit the RL model's generalization to novel design challenges.
- The effectiveness of environment querier feedback in correcting spatial reasoning errors remains uncertain, especially given the limited diversity in the curated dataset.

## Confidence

- **High confidence**: The environment infrastructure (BesiegeField) is well-defined with clear validity metrics and simulation capabilities.
- **Medium confidence**: The iterative agent workflow and reward design principles are theoretically sound, but empirical validation across diverse machine types is limited.
- **Medium confidence**: The RL finetuning approach with cold-start dataset is methodologically appropriate, but the small dataset size and potential reward sparsity raise concerns about scalability.

## Next Checks

1. **Spatial Reasoning Validation**: Systematically evaluate how well LLMs interpret block-level simulation feedback (position, velocity, stress) by introducing controlled design errors and measuring correction rates across multiple refinement cycles.
2. **Reward Signal Analysis**: Conduct ablation studies on reward components (validity-only vs. performance-weighted) and measure how different feedback granularities (block-level vs. aggregate metrics) affect learning convergence in RL experiments.
3. **Dataset Diversity Audit**: Analyze the cold-start dataset's coverage across machine types, complexity levels, and failure modes to quantify potential biases that could limit the RL model's generalization to novel design challenges.