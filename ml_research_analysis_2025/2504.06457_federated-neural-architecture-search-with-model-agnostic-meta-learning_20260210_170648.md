---
ver: rpa2
title: Federated Neural Architecture Search with Model-Agnostic Meta Learning
arxiv_id: '2504.06457'
source_url: https://arxiv.org/abs/2504.06457
tags:
- search
- architecture
- learning
- federated
- fedmetanas
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes FedMetaNAS, a federated neural architecture
  search framework that integrates model-agnostic meta-learning (MAML) to address
  data heterogeneity in federated learning. The approach uses Gumbel-Softmax reparameterization
  to relax search spaces and enable soft pruning, eliminating the need for retraining.
---

# Federated Neural Architecture Search with Model-Agnostic Meta Learning

## Quick Facts
- **arXiv ID**: 2504.06457
- **Source URL**: https://arxiv.org/abs/2504.06457
- **Reference count**: 9
- **One-line primary result**: FedMetaNAS achieves up to 81.65% accuracy on CIFAR10 and 64.75% on CIFAR100 while reducing search time by over 50% compared to FedNAS

## Executive Summary
This paper proposes FedMetaNAS, a federated neural architecture search framework that integrates model-agnostic meta-learning (MAML) to address data heterogeneity in federated learning. The approach uses Gumbel-Softmax reparameterization to relax search spaces and enable soft pruning, eliminating the need for retraining. It employs a dual-learner system with task-specific and meta-learners to jointly optimize both architecture parameters and model weights. Experiments show FedMetaNAS outperforms baseline methods, achieving up to 81.65% accuracy on CIFAR10 and 64.75% on CIFAR100 while reducing search time by over 50% compared to FedNAS. The method demonstrates robustness across various non-IID data distributions and personalization scenarios.

## Method Summary
FedMetaNAS combines federated learning with neural architecture search through a MAML framework. The method operates on a DARTS-based search space with Gumbel-Softmax reparameterization for continuous relaxation of operations and input nodes. During each round, clients optimize both model weights and architecture parameters using an inner-loop task learner on support data, followed by an outer-loop meta learner that updates global parameters using query data gradients. The architecture parameters are aggregated using FedAvg alongside model weights. The Gumbel-Softmax approach enables soft pruning, allowing the method to produce a discrete architecture without requiring a separate retraining stage. First-order gradient approximation is used to reduce memory overhead during the MAML optimization.

## Key Results
- Achieves 81.65% accuracy on CIFAR10 and 64.75% on CIFAR100 under non-IID data distributions
- Reduces search time by over 50% compared to FedNAS baseline
- Demonstrates robustness across Dirichlet ($\alpha=0.1, 0.3$) and Label Skew ($\tau=4, 8$) non-IID settings
- Eliminates need for separate retraining stage through Gumbel-Softmax soft pruning

## Why This Works (Mechanism)
FedMetaNAS leverages MAML's meta-learning capability to adapt to heterogeneous data distributions in federated settings. By optimizing architecture parameters alongside model weights in a dual-learner framework, the method can discover architectures that are both globally optimal and locally adaptable. The Gumbel-Softmax relaxation enables gradient-based optimization of discrete architecture choices while maintaining differentiability, and the soft pruning mechanism allows for immediate deployment of the discovered architecture without retraining. This combination addresses the fundamental challenge of data heterogeneity in federated NAS while maintaining computational efficiency.

## Foundational Learning

**Gumbel-Softmax Reparameterization**: Softens discrete architecture choices into continuous distributions for gradient-based optimization. Needed to make architecture search differentiable. Quick check: Verify temperature annealing schedule is properly implemented.

**Model-Agnostic Meta Learning (MAML)**: Enables adaptation to heterogeneous client data through inner-loop task optimization and outer-loop meta update. Needed to handle non-IID distributions. Quick check: Confirm first-order gradient approximation is correctly implemented.

**Federated Averaging (FedAvg)**: Aggregates model weights and architecture parameters from multiple clients. Needed for distributed optimization. Quick check: Verify weighted averaging accounts for client data sizes.

**DARTS Cell Structure**: Defines the search space as normal and reduction cells with mixed operations. Needed to constrain the architecture search space. Quick check: Ensure all candidate operations are properly initialized.

**Soft Pruning**: Eliminates low-probability operations during optimization rather than after. Needed to avoid separate retraining stage. Quick check: Monitor architecture sparsity during training.

## Architecture Onboarding

**Component Map**: Data Distribution -> Client Local Training (Task Learner) -> Server Aggregation (FedAvg) -> Architecture Pruning -> Final Model

**Critical Path**: The dual-learner MAML loop (Task Learner -> Meta Learner) represents the critical path where architecture and weight optimization occur simultaneously.

**Design Tradeoffs**: First-order MAML approximation reduces memory overhead but may limit gradient quality compared to second-order methods. Gumbel-Softmax relaxation enables gradient-based optimization but requires careful temperature scheduling.

**Failure Signatures**: Performance degradation after pruning indicates improper Gumbel-Softmax temperature or pruning threshold. Convergence instability in high heterogeneity suggests meta learning rate issues.

**First Experiments**: 1) Validate Gumbel-Softmax sampling produces correct architecture probabilities. 2) Test FedAvg aggregation with synthetic architecture parameters. 3) Verify MAML inner-loop optimization converges on local data.

## Open Questions the Paper Calls Out

**Open Question 1**: Can the gradient-based FedMetaNAS framework be adapted to handle Vision Transformer (ViT) search spaces that typically rely on evolutionary algorithms? The paper identifies this as future work, noting that FedMetaNAS relies on Gumbel-Softmax relaxation while standard ViT search methods often employ discrete evolutionary strategies.

**Open Question 2**: What are the communication overheads and bandwidth requirements of transmitting both architecture parameters and weights in resource-constrained cross-device environments? The authors note this was not explicitly evaluated despite being critical for FL scenarios.

**Open Question 3**: Does the computational complexity of the dual-learner MAML system exceed the hardware capabilities of typical edge devices? The paper acknowledges computational overhead was not evaluated, particularly regarding the multiple gradient steps required by MAML.

## Limitations

- Missing critical hyperparameters including learning rates, temperature schedules, and batch configurations
- First-order MAML approximation may limit gradient quality in highly heterogeneous settings
- No independent validation of the claimed >50% search time reduction versus FedNAS
- Communication efficiency and edge device compatibility not evaluated

## Confidence

- **High confidence**: The core methodology combining federated NAS with MAML and Gumbel-Softmax relaxation is technically sound and well-justified
- **Medium confidence**: The reported accuracy improvements and search time reduction are plausible but depend heavily on undisclosed hyperparameters
- **Low confidence**: The claim of eliminating retraining through soft pruning is theoretically reasonable but lacks empirical validation

## Next Checks

1. Implement and test the algorithm with systematically varied learning rates (task and meta) to identify optimal configurations and assess sensitivity
2. Compare FedMetaNAS performance against FedNAS using identical search budgets and data distributions to independently verify the claimed >50% time reduction
3. Conduct ablation studies removing the MAML component to quantify its contribution to performance gains versus standard federated optimization approaches