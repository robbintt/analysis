---
ver: rpa2
title: 'TopoLoRA-SAM: Topology-Aware Parameter-Efficient Adaptation of Foundation
  Segmenters for Thin-Structure and Cross-Domain Binary Semantic Segmentation'
arxiv_id: '2601.02273'
source_url: https://arxiv.org/abs/2601.02273
tags:
- segmentation
- dice
- adaptation
- while
- topolora-sam
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: TopoLoRA-SAM is a topology-aware, parameter-efficient adaptation
  framework for foundation segmentation models, specifically designed for binary semantic
  segmentation tasks involving thin structures (e.g., retinal vessels) and cross-domain
  visual data (e.g., SAR imagery). The method freezes the SAM ViT-B encoder and applies
  Low-Rank Adaptation (LoRA) modules to the feed-forward layers, augmented with a
  lightweight spatial convolutional adapter for high-resolution feature refinement.
---

# TopoLoRA-SAM: Topology-Aware Parameter-Efficient Adaptation of Foundation Segmenters for Thin-Structure and Cross-Domain Binary Semantic Segmentation

## Quick Facts
- **arXiv ID:** 2601.02273
- **Source URL:** https://arxiv.org/abs/2601.02273
- **Reference count:** 40
- **One-line primary result:** Achieves best retina-average Dice (0.595) and overall average Dice (0.735) while training only 5.2% (~4.9M) parameters.

## Executive Summary
TopoLoRA-SAM adapts the Segment Anything Model (SAM) ViT-B encoder for binary semantic segmentation of thin structures and cross-domain visual data through parameter-efficient fine-tuning. By freezing the SAM encoder and applying Low-Rank Adaptation (LoRA) modules to feed-forward layers, combined with a lightweight spatial convolutional adapter and optional topology-aware clDice loss, the method achieves state-of-the-art performance on retinal vessel segmentation, polyp segmentation, and SAR imagery while training only 4.9M parameters (5.2% of the total model). The approach demonstrates strong topology preservation, robust cross-dataset generalization, and favorable calibration with minimal computational overhead.

## Method Summary
TopoLoRA-SAM freezes the SAM ViT-B encoder (93.7M parameters) and applies LoRA modules to the feed-forward network layers of each transformer block, with rank r=16 adding approximately 2.4M trainable parameters. A lightweight spatial convolutional adapter with depthwise-separable convolutions provides local spatial refinement for high-resolution boundary delineation. The mask decoder is trained from scratch along with the LoRA modules and adapter. Optional topology-aware supervision via differentiable clDice loss encourages connectivity preservation in thin structures. The method is trained end-to-end for 50 epochs using AdamW with learning rate 1e-4 and cosine decay, batch size 1 with gradient accumulation, and a combined loss of BCE, Dice, and clDice with weights 1.0, 1.0, and 0.5 respectively.

## Key Results
- Achieves best retina-average Dice score of 0.595 and overall average Dice of 0.735 across five diverse datasets
- Improves CHASE_DB1 Dice score by up to 8.4 points compared to baseline methods
- Trains only 5.2% (4.9M) of model parameters while maintaining or improving performance
- Demonstrates strong topology preservation with improved clDice and BFScore metrics
- Shows favorable calibration with low expected calibration error (ECE) across datasets

## Why This Works (Mechanism)

### Mechanism 1: Low-Rank Adaptation of Frozen Encoder FFN Layers
Injecting LoRA modules into frozen SAM ViT-B encoder FFN layers enables domain-specific feature transformation while preserving pretrained representations. For a pretrained linear layer W₀, LoRA reparameterizes the forward pass as h = W₀x + BAx, where A and B are low-rank trainable factors. This targets FFN layers specifically, assuming attention weights encode more transferable relational patterns.

### Mechanism 2: Lightweight Spatial Adapter for High-Resolution Boundary Refinement
A depthwise-separable convolutional adapter operating on SAM's 16× downsampled embeddings provides local spatial reasoning necessary for thin-structure boundary delineation. The adapter applies z' = z + Conv1×1(ReLU(DepthwiseConv3×3(z))) with a residual connection, adding only ~66K parameters while enabling local spatial refinement.

### Mechanism 3: clDice Topology-Aware Supervision for Connectivity Preservation
Differentiable soft-skeletonization loss (clDice) explicitly optimizes skeleton overlap and encourages connected centerlines in tubular structures. The loss computes Dice on soft-skeletonized predictions, addressing the insensitivity of standard region-based losses to topological errors that disconnect branches in thin structures.

## Foundational Learning

- **Vision Transformers (ViT) and patch-based processing:** SAM's ViT-B encoder divides images into 16×16 patches, processes them through self-attention, and reconstructs embeddings. Understanding this is essential for grasping where LoRA modules are injected and why 16× downsampling matters for thin structures.
  - *Quick check:* If a 1024×1024 retinal image is input to SAM ViT-B, what are the dimensions of the output embedding tensor z, and why does this resolution challenge thin-vessel segmentation?

- **Low-Rank Matrix Factorization (LoRA origins in NLP):** The core technique transfers from language model PEFT. Understanding the rank constraint (r ≪ d), scaling factor (α/r), and initialization strategy (B=0, A=Kaiming) is necessary for hyperparameter tuning and debugging convergence issues.
  - *Quick check:* Why does zero-initializing B while using Kaiming initialization for A ensure that training begins from the pretrained model's behavior?

- **Topological Connectivity vs. Region Overlap in Segmentation:** Understanding why perfect Dice score can coexist with topologically broken predictions (disconnected vessel branches) motivates the clDice loss. You need to grasp skeletonization, T_prec/T_sens metrics, and the gradient signal they provide.
  - *Quick check:* Given two predictions with identical Dice=0.80 on retinal vessels, what additional metric would reveal which prediction preserves vascular connectivity for downstream clinical analysis?

## Architecture Onboarding

- **Component map:** Input image → Frozen SAM ViT-B encoder with LoRA in FFN layers → Spatial adapter → Trainable mask decoder → Binary mask output
- **Critical path:** LoRA injection in FFN layers (rank r=16) provides primary driver (+4.2 Dice over decoder-only baseline), spatial adapter on 16× downsampled embeddings offers secondary refinement (+1.2 BFScore), clDice regularization (λ_cl=0.5) provides tertiary connectivity improvement (+0.8 clDice)
- **Design tradeoffs:** LoRA rank r=16 is Pareto-optimal; r=32 doubles parameters (4.8M) with diminishing returns (Dice drops from 0.690 to 0.648 on DRIVE); λ_cl=0.5 balances topology and region metrics; excessive weighting (λ_cl≥2.0) causes training instability
- **Failure signatures:** High variance across seeds (e.g., Mask2Former on CHASE_DB1: 0.285±0.293) indicates adaptation instability; TopoLoRA-SAM shows 0.569±0.016; fragmented vessel predictions with good Dice but poor clDice suggest missing topology regularization; miscalibration correlates with variance issues
- **First 3 experiments:** 1) Decoder-only baseline: Train with frozen encoder (no LoRA, no adapter, λ_cl=0) to quantify adaptation gap; 2) LoRA rank ablation on DRIVE: Test r∈{4,8,16,32} to reproduce Pareto-optimal finding; 3) clDice weight sweep on CHASE_DB1: Vary λ_cl∈{0.0,0.25,0.5,1.0,2.0} to identify optimal regularization strength

## Open Questions the Paper Calls Out

- **Open Question 1:** Can TopoLoRA-SAM be effectively extended to multi-class semantic segmentation while preserving its parameter efficiency and topology-aware benefits? The current framework and all experiments are restricted to binary segmentation; the decoder architecture and clDice formulation do not directly generalize to multi-class scenarios.
- **Open Question 2:** Can domain-adaptive or learnable clDice weighting schemes improve generalization across datasets with varying thin-structure characteristics? Fixed λcl = 0.5 is optimal on average but not universally; performance degrades at higher weights due to gradient conflicts, suggesting a need for adaptive strategies.
- **Open Question 3:** Can the memory and inference overhead of the frozen SAM ViT-B backbone be reduced while maintaining segmentation quality for resource-constrained deployment? Although only 5.2% of parameters are trained, the full 93.7M backbone must still be loaded and executed at inference time.

## Limitations

- **Memory and inference costs:** The frozen SAM backbone still incurs non-trivial memory and inference costs, which may limit deployment in resource-constrained settings despite parameter efficiency
- **Limited hyperparameter exploration:** The hyperparameter search space for the regularization weight λ_cl is not fully explored, with only limited values tested
- **Unspecified implementation details:** The LoRA scaling factor α and clDice implementation details (skeletonization kernel size and iteration count) are unspecified, potentially impacting reproducibility

## Confidence

- **High Confidence:** Parameter efficiency claims (4.9M trainable parameters, 5.2% of total) and their validation across five diverse datasets
- **Medium Confidence:** Performance improvements on thin-structure datasets, particularly the +8.4 Dice improvement on CHASE_DB1, though the variance reduction claim needs more statistical validation
- **Medium Confidence:** Topology preservation claims via clDice, though the ablation study shows diminishing returns with higher regularization weights

## Next Checks

1. Run a systematic LoRA rank ablation (r∈{4,8,16,32}) on DRIVE to confirm the claimed Pareto-optimal point and identify diminishing returns
2. Perform a clDice weight sweep (λ_cl∈{0.0,0.25,0.5,1.0,2.0}) on CHASE_DB1 to verify the stability threshold and identify optimal regularization strength
3. Conduct a three-seed ablation study comparing LoRA-only vs. full TopoLoRA-SAM on STARE to isolate the spatial adapter's contribution and validate the modest BFScore improvement claim