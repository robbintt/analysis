---
ver: rpa2
title: Model-Document Protocol for AI Search
arxiv_id: '2510.25160'
source_url: https://arxiv.org/abs/2510.25160
tags:
- knowledge
- mdp-agent
- reasoning
- retrieval
- evidence
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the Model-Document Protocol (MDP), a framework
  that bridges unstructured documents to LLMs by transforming raw text into compact,
  structured knowledge. The authors formalize retrieval as a multi-stage process involving
  agentic reasoning, memory grounding, and structured encoding.
---

# Model-Document Protocol for AI Search

## Quick Facts
- arXiv ID: 2510.25160
- Source URL: https://arxiv.org/abs/2510.25160
- Authors: Hongjin Qian; Zheng Liu
- Reference count: 6
- Key outcome: MDP-Agent achieves up to 53.1% accuracy on GAIA and WebWalkerQA benchmarks, outperforming RAG, ReAct, and WebThinker on complex, multi-step reasoning tasks.

## Executive Summary
This paper introduces the Model-Document Protocol (MDP), a framework that bridges unstructured documents to LLMs by transforming raw text into compact, structured knowledge. The authors formalize retrieval as a multi-stage process involving agentic reasoning, memory grounding, and structured encoding. MDP-Agent, their instantiation, uses document-level gist memories, diffusion-based exploration, and parallel synthesis to create LLM-ready contexts. Evaluated on GAIA and WebWalkerQA benchmarks, MDP-Agent consistently outperforms strong baselines including RAG, ReAct, and WebThinker, achieving up to 53.1% accuracy on GAIA and 53.1% on WebWalkerQA, with significant gains on complex, multi-step reasoning tasks. Ablation studies confirm the importance of reasoning models and diffusion depth. The work demonstrates MDP's effectiveness in converting data chaos into structured knowledge, enabling scalable and reliable LLM reasoning over large-scale external sources.

## Method Summary
MDP operates in two stages: data indexing and agentic knowledge discovery. For indexing, gist memories are generated for each document using a long-context LLM, then encoded with BGE-M3 for dense retrieval and indexed with BM25 for sparse retrieval. These are combined via hybrid scoring. For agentic discovery, queries are decomposed into atomic sub-queries, expanded through diffusion search (max depth 5), and processed via memory-guided parallel synthesis (filter→extract→reduce operators). The final context assembles as a knowledge chain combining initial context with sequentially retrieved subspaces.

## Key Results
- MDP-Agent achieves 53.1% accuracy on GAIA benchmark vs 13.6% for direct reasoning baseline
- On WebWalkerQA, MDP-Agent achieves 53.1% accuracy vs 26.2% for RAG baseline
- Ablation shows performance plateaus after diffusion depth 5 with diminishing returns

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Document-level gist memories enable efficient global-semantic retrieval and aggressive pre-filtering of irrelevant content.
- Mechanism: For each document D, a lightweight long-context model produces a textual abstraction D̄ that verbalizes high-level topics and structural cues. This gist is encoded as a dense vector for retrieval, while raw text is indexed sparsely. Relevance combines both: Rel(q, D) = α·sim_dense(q, z_D) + (1-α)·sim_sparse(q, D). During synthesis, gist memories allow filtering operator F to discard documents without reading full text.
- Core assumption: Long-context models can produce abstractions that preserve task-relevant signals while discarding noise.
- Evidence anchors:
  - [abstract] "constructing document-level gist memories for global coverage"
  - [section 3.1] "This richer abstraction allows retrieval methods not only to locate broadly relevant documents but also to filter and prioritize them more effectively"
  - [corpus] Related work on agentic search (Search-o1, Laser) addresses knowledge insufficiency but does not formalize gist-based pre-filtering; corpus evidence for this specific mechanism is weak.
- Break condition: Gist abstractions become too compressed to distinguish relevant from irrelevant documents for niche queries.

### Mechanism 2
- Claim: Diffusion-based query expansion improves coverage of intent-relevant knowledge by iteratively refining the search frontier.
- Mechanism: After initial queries retrieve atomic knowledge spaces {S_i,1, ..., S_i,m_i}, the agent evaluates sufficiency. If insufficient, it generates additional queries conditioned on past results: {q_i,1, ..., q_i,m_i} → {S_i,1, ..., S_i,m_i} → [K_i if sufficient, else {q_i,m_i+1, ...}]. This recursive expansion continues until the subspace captures required knowledge.
- Core assumption: Intent descriptions are often biased or incomplete, requiring iterative correction through evidence-conditioned query generation.
- Evidence anchors:
  - [abstract] "performing diffusion-based exploration with vertical exploitation to uncover layered dependencies"
  - [section 3.2] "After executing the initial queries and obtaining atomic spaces, the agent evaluates whether the accumulated evidence suffices for I_i. If not, it expands the search frontier"
  - [corpus] Laser (arXiv:2512.20458) similarly addresses long-horizon agentic search but focuses on structured protocols rather than diffusion-based coverage expansion.
- Break condition: Diffusion depth exceeds information saturation point; ablation shows performance plateaus after depth 5 with diminishing returns.

### Mechanism 3
- Claim: Memory-guided parallel synthesis scales evidence processing by decoupling heavy reasoning (central model) from bulk data handling (auxiliary model).
- Mechanism: Three operators—Filter (F), Extract (E), Synthesize (R)—process documents in map-reduce fashion. F uses gist memories for lightweight relevance checks; E processes surviving documents in parallel; R integrates into subspace K_i. Ablation shows filtering removes ~90% of pages while reasoning consumes only 8.9K tokens vs 227K for data processing.
- Core assumption: Task-relevant signals can be identified from gist-level representations without full-text analysis.
- Evidence anchors:
  - [section 3.2] "All operators F, E, R are powered by an auxiliary lightweight LLM, enabling MDP-Agent to filter aggressively, extract in parallel, and synthesize compactly"
  - [section 4.3] "it filters out nearly 90% of irrelevant pages using gist memory and processes the remainder in a map–reduce manner"
  - [corpus] Weak corpus evidence; related systems (WebThinker, Search-o1) do not explicitly formalize this decoupling pattern.
- Break condition: Filtering threshold is too aggressive and discards marginally relevant documents containing critical evidence.

## Foundational Learning

- Concept: Knowledge chain decomposition (K = K_1 → K_2 → ... → K_t)
  - Why needed here: MDP formalizes complex tasks as sequential knowledge gaps, each requiring atomic retrieval operations. Understanding this decomposition is prerequisite to implementing intent planning and subspace construction.
  - Quick check question: Given a multi-hop query ("What animals mentioned in both Author A and Author B's papers on species X were also in the 2021 study cited on Wikipedia?"), can you decompose it into sequential knowledge dependencies?

- Concept: Hybrid retrieval (dense + sparse)
  - Why needed here: The gist-memory hybrid index combines semantic coherence (dense) with detail sensitivity (sparse). Implementers must understand when to weight each component via α parameter.
  - Quick check question: For a query requiring exact entity matching versus conceptual relevance, how would you adjust α?

- Concept: Map-reduce evidence synthesis
  - Why needed here: Scalability depends on parallel processing of filtered documents. Understanding the three-operator pipeline (F→E→R) is essential for debugging retrieval quality.
  - Quick check question: If synthesis produces incomplete evidence, which operator (F, E, or R) should be instrumented first?

## Architecture Onboarding

- Component map:
  Offline Pipeline: Raw documents → Gist memory generation (long-context LLM) → Hybrid indexing (BGE-M3 dense + BM25 sparse via ElasticSearch)
  Online Pipeline: Query → Intent planner (reasoning LLM) → Diffusive query expansion → Retrieval → Filter (auxiliary LLM + gist) → Extract (parallel) → Synthesize (reduce) → Task-specific context → Answer generation (any LLM)

- Critical path:
  1. Gist memory quality determines filtering precision
  2. Diffusion depth (max 5) balances coverage vs. noise
  3. Reasoning model strength (QwQ-32B outperforms Qwen3-32B) dominates final accuracy

- Design tradeoffs:
  - Deeper diffusion increases retrieved pages (~367 at depth 9) but also processing tokens; ablation shows optimal depth ~5
  - Stronger central reasoning models improve accuracy but increase latency; auxiliary models handle bulk processing
  - Online search is "slow and unstable"; local index with pre-computed gist memories is recommended for evaluation

- Failure signatures:
  - Low accuracy on Level-3 tasks (33.3% vs 61.5% on Level-1) indicates diffusion insufficient for deep reasoning chains
  - High retrieved pages but low browsed pages suggests over-aggressive filtering
  - Reasoning token ratio >20% of total suggests insufficient parallelization

- First 3 experiments:
  1. **Baseline sanity check**: Run vanilla RAG and ReAct baselines on GAIA Level-1 subset to confirm expected performance gaps (~13.6% direct reasoning → ~50% MDP-Agent).
  2. **Ablation on diffusion depth**: Sweep depth 1→9 on held-out queries; plot accuracy vs. retrieved pages to identify saturation point.
  3. **Gist quality test**: Replace gist memories with first-1024-token truncation; measure filtering precision drop to quantify gist contribution.

## Open Questions the Paper Calls Out
None

## Limitations
- Several key components lack full specification in the paper text, creating uncertainty about exact reproduction
- The ablation studies provide strong evidence for specific design choices but the underlying mechanism explaining why these values work optimally remains implicit
- The generalizability of the diffusion-based exploration mechanism is unclear, as performance plateaus after depth 5 may be corpus-dependent

## Confidence
- **High Confidence** in overall framework effectiveness and benchmark performance claims
- **Medium Confidence** in specific mechanism claims due to missing implementation details
- **Low Confidence** in generalizability of diffusion-based exploration mechanism

## Next Checks
1. **Prompt fidelity validation**: Implement the three operators (Filter, Extract, Synthesize) using the exact prompts from the repository and measure accuracy degradation compared to the paper's reported results.
2. **Corpus dependency analysis**: Test MDP-Agent on a held-out corpus with different domain characteristics to validate whether the diffusion depth 5 and 90% filtering rate are universal or corpus-dependent.
3. **Reasoning model sensitivity**: Systematically vary the central reasoning model to establish whether the accuracy gains are primarily due to the MDP framework or the specific reasoning model choice.