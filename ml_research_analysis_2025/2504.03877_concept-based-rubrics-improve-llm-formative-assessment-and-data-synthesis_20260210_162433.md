---
ver: rpa2
title: Concept-based Rubrics Improve LLM Formative Assessment and Data Synthesis
arxiv_id: '2504.03877'
source_url: https://arxiv.org/abs/2504.03877
tags:
- data
- student
- assessment
- responses
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study demonstrates that concept-based rubrics significantly
  enhance the performance of large language models (LLMs) in formative assessment
  tasks, narrowing the gap between LLMs and supervised methods. While supervised PLMs
  achieve high accuracy in student response assessment, LLMs typically underperform
  without training data.
---

# Concept-based Rubrics Improve LLM Formative Assessment and Data Synthesis

## Quick Facts
- **arXiv ID:** 2504.03877
- **Source URL:** https://arxiv.org/abs/2504.03877
- **Reference count:** 40
- **Key outcome:** Concept-based rubrics dramatically improve LLM zero-shot performance in formative assessment and enable effective synthetic data generation for PLMs.

## Executive Summary
This study demonstrates that concept-based rubrics significantly enhance the performance of large language models (LLMs) in formative assessment tasks, narrowing the gap between LLMs and supervised methods. While supervised PLMs achieve high accuracy in student response assessment, LLMs typically underperform without training data. However, using concept-based rubrics as prompts for GPT-4o-mini improves LLM performance dramatically, outperforming example-based prompts and matching supervised models on certain datasets. The same rubrics also enable LLMs to generate high-quality synthetic data for training lightweight PLMs, achieving comparable accuracy to the original LLM without requiring human-labeled data. Additionally, the study explores the potential for LLMs to provide explanatory feedback, with most rubric-aligned explanations justifying assessment decisions effectively. These findings highlight the value of concept-based rubrics in enhancing both LLM assessment capabilities and synthetic data generation for automated formative assessment in STEM education.

## Method Summary
The study evaluates LLM performance on automated formative assessment using five datasets (Beetle, SciEntsBank, ASAP-SAS, ISTUDIO, CLASSIFIES) with three-way classification (Correct, Partially Correct, Incorrect). GPT-4o-mini performs zero-shot and few-shot inference using concept-based rubrics as prompts, with strict output formatting `[[score]]`. For synthetic data generation, a "DiversityEnhanced" method prompts LLMs to generate case statements from rubrics, create responses of varying lengths (5-128 words), then relabel generated responses for consistency. The synthetic data trains RoBERTa-large or Longformer-large PLMs for 10 epochs with learning rate 2e-5. Performance is measured using classification accuracy and macro F1 score.

## Key Results
- Concept-based rubrics significantly improve LLM zero-shot accuracy compared to example-only prompts (RQ1).
- LLM-generated synthetic data enables lightweight PLMs to achieve accuracy comparable to the original LLM without human-labeled data (RQ2).
- Most rubric-aligned explanations provided by LLMs are valid and sufficient for justifying assessment decisions (RQ3).

## Why This Works (Mechanism)
Concept-based rubrics provide structured, concept-focused guidance that helps LLMs align their reasoning with assessment criteria. This structured approach reduces ambiguity and improves consistency in scoring decisions. The same rubrics enable effective synthetic data generation by providing clear criteria for both response generation and relabeling, ensuring the synthetic data maintains high quality and consistency with human-labeled data.

## Foundational Learning
- **Concept-based rubrics**: Structured assessment criteria focusing on specific concepts rather than holistic evaluation. Needed to provide clear, actionable guidance for LLMs in STEM domains.
- **Zero-shot vs few-shot inference**: Zero-shot uses only rubric-based prompts, while few-shot adds examples. Quick check: Compare performance gaps between these conditions.
- **Synthetic data relabeling**: Generating responses then having LLMs re-label them for consistency. Needed to maintain data quality when training PLMs on synthetic data.
- **Diversity enhancement**: Generating varied response lengths and cases from rubrics. Quick check: Measure diversity metrics across synthetic datasets.

## Architecture Onboarding

**Component Map:** Rubric Template -> LLM Inference -> Score Extraction -> Synthetic Data Pipeline -> PLM Training -> Evaluation

**Critical Path:** Concept-based rubric input → LLM assessment → score extraction → (optional) synthetic data generation → PLM fine-tuning → accuracy evaluation

**Design Tradeoffs:** Rubric specificity vs. generality; synthetic data volume vs. quality; PLM architecture choice (RoBERTa vs Longformer) based on input length

**Failure Signatures:** Syntactic parsing errors in score extraction; misalignment between synthetic and original data distributions; PLM overfitting to synthetic data

**First Experiments:** 1) Run baseline LLM inference with rubric-only prompts vs example-only prompts. 2) Generate synthetic dataset using DiversityEnhanced method and verify relabeling consistency. 3) Train PLM on synthetic data and compare accuracy to original LLM.

## Open Questions the Paper Calls Out

**Open Question 1:** Can holistic or non-concept-based rubrics be restructured to provide the same performance benefits for LLMs as concept-based rubrics? The study shows concept-based rubrics work well in STEM but general rubrics in ASAP did not improve performance, suggesting structure is critical.

**Open Question 2:** How can prompting strategies be refined to prevent LLMs from defaulting to subjective stylistic critiques when their assessment disagrees with human labels? Current prompts were insufficient to enforce rubric-based reasoning in ambiguous cases.

**Open Question 3:** Does the prevalence of AI-assisted student responses in a dataset improve the efficacy of LLM-based synthetic data generation? The study observed better performance with "Labels&Responses" synthesis on datasets with high AI-generation rates, but this correlation remains untested causally.

## Limitations
- Relies on proprietary GPT-4o-mini API and specific rubric formulations not fully reproducible
- Two datasets (ISTUDIO, CLASSIFIES) are not publicly accessible at time of writing
- Performance gains measured only in zero-shot/few-shot settings; may not generalize to other LLM families or finetuning regimes

## Confidence
- **High confidence**: Concept-based rubrics improve LLM accuracy in zero-shot assessment compared to example-only prompts (RQ1).
- **Medium confidence**: Rubric-enhanced LLMs can generate synthetic training data that enables lightweight PLMs to match original LLM accuracy (RQ2).
- **Medium confidence**: Rubric-aligned explanations are mostly valid and sufficient for justifying rubric-based scores (RQ3).

## Next Checks
1. Reproduce synthetic data pipeline: Generate 4,000 samples using the "DiversityEnhanced" strategy and verify consistency in relabeling step.
2. Test rubric generalization: Apply same rubric-prompt to GPT-3.5/4 (non-mini) and measure accuracy drop.
3. Ablation on rubric specificity: Compare performance when rubric contains only concepts vs. detailed scoring criteria.