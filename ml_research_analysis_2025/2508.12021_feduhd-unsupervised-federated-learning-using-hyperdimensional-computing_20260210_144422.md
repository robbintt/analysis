---
ver: rpa2
title: 'FedUHD: Unsupervised Federated Learning using Hyperdimensional Computing'
arxiv_id: '2508.12021'
source_url: https://arxiv.org/abs/2508.12021
tags:
- feduhd
- cluster
- local
- learning
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: FedUHD introduces the first unsupervised federated learning framework
  using Hyperdimensional Computing (HDC). It addresses challenges in UFL including
  non-iid data distribution, high computational and communication costs, and vulnerability
  to communication noise.
---

# FedUHD: Unsupervised Federated Learning using Hyperdimensional Computing

## Quick Facts
- **arXiv ID**: 2508.12021
- **Source URL**: https://arxiv.org/abs/2508.12021
- **Reference count**: 40
- **Key outcome**: FedUHD achieves up to 173.6× speedup, 612.7× energy efficiency improvements, 271× lower communication costs, and 15.50% higher accuracy compared to neural network-based UFL methods.

## Executive Summary
FedUHD introduces the first unsupervised federated learning framework using Hyperdimensional Computing (HDC) to address challenges in non-IID data distribution, high computational and communication costs, and vulnerability to communication noise. The method employs a kNN-based cluster hypervector removal technique on clients to eliminate outliers from non-IID data, and a weighted HDC aggregation approach on the server to balance data distribution across clients. Experiments demonstrate significant improvements in training speed, energy efficiency, communication cost, and accuracy compared to state-of-the-art neural network-based UFL methods, while showing superior robustness to communication noise.

## Method Summary
FedUHD uses Hyperdimensional Computing to perform unsupervised federated learning by encoding data into high-dimensional hypervectors, clustering them locally, and aggregating results across clients. Clients use a kNN-based filter to remove irrelevant global clusters before local clustering, while the server performs weighted aggregation of local cluster hypervectors based on sample sizes. The framework uses frozen pretrained feature extractors (ResNet18 for images) followed by random projection to create hypervectors, with clustering performed in this high-dimensional space without requiring gradient updates or backpropagation.

## Key Results
- Achieves up to 173.6× speedup and 612.7× energy efficiency improvements in training
- Reduces communication costs by 271× compared to neural network-based methods
- Demonstrates 15.50% higher accuracy on average across benchmark datasets
- Shows up to 49.27 percentage points less accuracy degradation under communication noise

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Filtering out irrelevant global information improves local model stability when data is non-IID and unlabeled.
- **Mechanism**: The client performs a kNN-based check on incoming global cluster hypervectors. If a global cluster HV has no local neighbors sharing its cluster ID, it is flagged as an "outlier" and removed before local clustering begins.
- **Core assumption**: Non-IID data distributions mean that some global clusters are irrelevant to specific clients, and forcing alignment with these irrelevant clusters degrades local representation.
- **Evidence anchors**: [abstract] "kNN-based cluster hypervector removal method... eliminating detrimental outliers." [section III-B] "FedUHD treats the global cluster HV as an 'outlier' and the corresponding global and local cluster HV... are removed."
- **Break condition**: If the local dataset is extremely small or sparse, the k-nearest neighbor check may validly fail for all global clusters, leaving the client with no useful prior information to initialize clustering.

### Mechanism 2
- **Claim**: Weighting cluster aggregation by sample size balances the influence of clients in a label-free environment.
- **Mechanism**: During server aggregation, local cluster HVs are weighted by the number of local samples assigned to that cluster. The weight normalizes the contribution so that clusters with more data evidence have higher influence on the resulting global cluster HV.
- **Core assumption**: The size of a cluster is a proxy for its importance or validity, even without ground truth labels.
- **Evidence anchors**: [abstract] "weighted HDC aggregation technique balances the non-iid data distribution across clients." [section III-C] "FedUHD assigns weights to local cluster hypervectors... $g_j = \sum W_{ij} \cdot l_{ij}$."
- **Break condition**: If a client has corrupted or mislabeled data (in terms of clustering), this mechanism amplifies the error by giving higher weight to larger (but incorrect) clusters.

### Mechanism 3
- **Claim**: High-dimensional holographic representations preserve similarity while resisting communication noise.
- **Mechanism**: Data is encoded into high-dimensional vectors (e.g., 10,000 dimensions). Because information is stored holographically (distributed across all bits), random bit flips during transmission affect the vector marginally. Similarity checks (cosine similarity) remain reliable despite noise.
- **Core assumption**: The dimensionality is high enough that random noise averages out and does not destroy the semantic direction of the vector.
- **Evidence anchors**: [abstract] "HDC is a brain-inspired computing scheme... robustness to communication noise." [section I] "...robustness, which arises from the holistic representation (i.e., if an element of an HV flipped, it does not dramatically affect a specific input feature)."
- **Break condition**: If the signal-to-noise ratio drops significantly or the hypervector dimension is reduced too aggressively, the orthogonality between class clusters collapses.

## Foundational Learning

- **Concept**: Hyperdimensional Computing (HDC) Operations
  - **Why needed here**: You cannot understand the training or aggregation steps without grasping "bundling" (addition) and "binding" (multiplication) of vectors. The paper replaces standard weight updates with these algebraic operations.
  - **Quick check question**: If I add two hypervectors A + B, does the result look more like A or B, or both? (Answer: It is similar to both).

- **Concept**: Unsupervised Clustering in FL (Non-IID)
  - **Why needed here**: The core difficulty is aligning local clusters (which might be biased) into a global structure without labels. You need to understand why standard FedAvg fails here (requires labels/weights to align).
  - **Quick check question**: Why can't we just average the cluster centers from all clients? (Answer: Cluster 1 on Client A might represent "Cats" while Cluster 1 on Client B represents "Dogs" due to Non-IID data).

- **Concept**: Random Projection / Feature Extraction
  - **Why needed here**: The paper uses a frozen pre-trained feature extractor or random projection to map raw data to the HDC space. This is the "encoding" layer that determines the quality of the hypervectors.
  - **Quick check question**: Does the model update the weights of the feature extractor (ResNet18) during training? (Answer: No, it is frozen).

## Architecture Onboarding

- **Component map**: Input Data -> FE (Feature Extractor) -> RP (Random Projection) -> Encoded HVs -> kNN Filter -> Local k-Means Clustering
- **Critical path**: The kNN-based removal step on the client is the most sensitive logic. It connects the global "prior" (clusters from the server) to the local "posterior" (local clustering). If this fails, the local model ignores the global model, or the global model distorts the local model.
- **Design tradeoffs**:
  - **Dimension (D)**: Higher D increases accuracy and robustness but linearly increases communication cost and memory.
  - **Cluster Count (J)**: Must be > K (true classes). If J is too low, distinct classes are forced together; if too high, computation rises but accuracy may improve via finer granularity.
  - **Feature Extractor**: Using a frozen ResNet is efficient but limits the system to the features ResNet can see. It cannot learn new features from the federated data.
- **Failure signatures**:
  - **Accuracy Collapse**: If the kNN filter is too aggressive (low k_n), clients may consistently drop global clusters, preventing convergence.
  - **Stagnation**: If J is too small for the complexity of the data, the clustering accuracy plateaus early.
  - **Noise Sensitivity**: If dimensions are packed too densely (too many clusters for the dimension D), the "holographic" property degrades, and noise robustness drops.
- **First 3 experiments**:
  1. **Baseline Validation**: Run FedUHD on the HAR dataset (tabular data) with 10 clients. Verify that the "cluster accuracy" metric is calculated correctly without label supervision.
  2. **Ablation (kNN Filter)**: Disable the kNN-based removal step (accept all global clusters). Compare convergence speed and final accuracy against the full model to quantify the impact of the non-IID handling mechanism.
  3. **Robustness Test**: Inject Gaussian noise into the global cluster HVs before they are sent to clients in a simulation. Plot accuracy degradation vs. noise level to validate the holographic robustness claim.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: Can the number of global clusters J be determined automatically during training rather than requiring validation-based tuning?
- **Basis in paper**: Section IV-B states: "The number of clusters J is a key design choice in cluster-based UFL such as FedUHD and Orchestra, when the number of ground-truth classes is not provided." Figure 6 shows J significantly impacts accuracy.
- **Why unresolved**: The paper requires validation experiments to set J without proposing an adaptive selection mechanism, which may be impractical in truly unsupervised settings without labeled validation data.
- **What evidence would resolve it**: An adaptive algorithm that adjusts J dynamically based on clustering quality metrics, achieving comparable accuracy without manual tuning.

### Open Question 2
- **Question**: How does FedUHD perform when the pretrained feature extractor exhibits significant domain shift from the federated learning task?
- **Basis in paper**: Section III-A notes the feature extractor is "frozen during both training and inference" and "pretrained on a different dataset (e.g., ImageNet)." The paper assumes good transfer but does not analyze performance degradation under domain mismatch.
- **Why unresolved**: Experiments only use ImageNet-pretrained ResNet18 for CIFAR datasets—both natural image domains with substantial overlap in visual features.
- **What evidence would resolve it**: Experiments using feature extractors pretrained on dissimilar domains (e.g., medical imaging, satellite data) applied to different FL tasks.

### Open Question 3
- **Question**: What are the theoretical convergence guarantees for FedUHD under varying degrees of non-IID data distribution?
- **Basis in paper**: The paper provides empirical convergence curves but lacks formal analysis of convergence conditions or rates relative to HDC dimensionality, client count, and non-IID severity.
- **Why unresolved**: No theoretical framework is established for understanding when and how quickly FedUHD converges.
- **What evidence would resolve it**: Mathematical bounds on convergence rate as a function of non-IID parameter α, hypervector dimension D, and number of clients I.

### Open Question 4
- **Question**: How does FedUHD handle dynamic client participation patterns such as stragglers, dropouts, or late-joining clients?
- **Basis in paper**: The experimental setup assumes fixed client participation (P=1.0 for all cases). Real-world FL systems face heterogeneous client availability that may affect the kNN-based removal and weighted aggregation mechanisms.
- **Why unresolved**: No experiments or discussion address partial client participation or temporal dynamics in client availability.
- **What evidence would resolve it**: Experiments with varying participation ratios, straggler simulation, and analysis of how weighted aggregation adapts to inconsistent client contributions.

## Limitations

- The framework's performance depends heavily on the quality of the frozen feature extractor, which may not generalize well to domains with significant domain shift from the pretraining data.
- The kNN-based filtering mechanism may become ineffective when client datasets are extremely small or sparse, potentially preventing useful global model information from reaching local clients.
- The need to manually tune the number of clusters J through validation experiments limits applicability in truly unsupervised settings without labeled validation data.

## Confidence

- **High Confidence**: Claims about HDC's inherent robustness to communication noise and the computational efficiency benefits of using hypervectors instead of neural network parameters.
- **Medium Confidence**: Claims about the effectiveness of the kNN-based cluster hypervector removal technique for handling non-IID data, as this mechanism is novel and not externally validated.
- **Low Confidence**: Claims about the weighted HDC aggregation approach's ability to balance data distribution across clients, as this specific weighted bundling technique for unsupervised learning lacks external validation.

## Next Checks

1. **Cross-Dataset Validation**: Test FedUHD on a third dataset (e.g., Fashion-MNIST or STL-10) to verify the claimed accuracy improvements generalize beyond CIFAR and HAR.
2. **Extreme Non-IID Testing**: Systematically vary the Dirichlet parameter α to create increasingly severe non-IID partitions and measure the kNN filtering mechanism's performance degradation point.
3. **Noise Robustness Quantification**: Measure the exact bit-flip tolerance threshold where HDC's holographic property breaks down by systematically corrupting transmitted hypervectors with varying noise levels.