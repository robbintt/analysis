---
ver: rpa2
title: 'Actions as Language: Fine-Tuning VLMs into VLAs Without Catastrophic Forgetting'
arxiv_id: '2509.22195'
source_url: https://arxiv.org/abs/2509.22195
tags:
- robot
- move
- action
- reasoning
- vlm2vla
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of catastrophic forgetting in
  vision-language-action (VLA) models when fine-tuning vision-language models (VLMs)
  for robotic control. The core method introduces VLM2VLA, which resolves the distribution
  mismatch between VLM pretraining data and robotics data by representing low-level
  robot actions as natural language descriptions.
---

# Actions as Language: Fine-Tuning VLMs into VLAs Without Catastrophic Forgetting
## Quick Facts
- arXiv ID: 2509.22195
- Source URL: https://arxiv.org/abs/2509.22195
- Authors: Asher J. Hancock; Xindi Wu; Lihan Zha; Olga Russakovsky; Anirudha Majumdar
- Reference count: 40
- Primary result: Over 85% VQA performance retention while achieving superior zero-shot generalization in robotic manipulation tasks

## Executive Summary
This paper introduces VLM2VLA, a method for converting vision-language models (VLMs) into vision-language-action models (VLAs) without catastrophic forgetting. The core innovation is representing low-level robot actions as natural language descriptions, which aligns the distribution of robotics data with VLM pretraining data. This allows fine-tuning with LoRA adapters alone, minimizing modifications to the VLM backbone while preserving semantic reasoning capabilities.

The approach addresses a fundamental challenge in robotics: the distribution mismatch between web-scale image-text pairs used in VLM pretraining and the limited, task-specific robotics datasets used for control. By framing actions as language, VLM2VLA enables the model to leverage its existing linguistic and visual reasoning abilities when learning new robotic tasks, resulting in better generalization to novel instructions and improved performance on out-of-distribution scenarios.

## Method Summary
VLM2VLA resolves catastrophic forgetting by representing robot actions as natural language tokens instead of discrete action indices. This design choice ensures that robotics fine-tuning data follows the same distribution as VLM pretraining data, allowing the use of LoRA adapters without modifying the VLM backbone. The method involves minimal architectural changes: a small number of additional LoRA parameters are introduced to adapt the VLM for action prediction while preserving the original language and vision capabilities. The action-as-language formulation enables zero-shot generalization to novel tasks, including multilingual instructions and semantic reasoning scenarios, by leveraging the VLM's existing understanding of language and vision.

## Key Results
- VQA performance retention exceeds 85% after fine-tuning, demonstrating preservation of semantic reasoning.
- Superior zero-shot generalization to novel tasks, including multilingual and semantically complex instructions.
- Outperforms baseline VLAs in out-of-distribution robotic manipulation tasks.

## Why This Works (Mechanism)
The action-as-language formulation bridges the distribution gap between VLM pretraining data and robotics data. By representing actions as natural language tokens, the model can leverage its existing linguistic and visual reasoning capabilities during fine-tuning. This alignment allows the use of LoRA adapters alone, minimizing changes to the VLM backbone and preserving semantic knowledge. The approach enables the model to generalize to novel tasks by applying its understanding of language and vision in new contexts.

## Foundational Learning
- **Catastrophic forgetting**: The phenomenon where models lose previously learned knowledge when trained on new tasks. Needed to understand why traditional fine-tuning methods fail for VLMs. Quick check: Compare performance on VQA before and after fine-tuning.
- **Vision-Language-Action (VLA) models**: Models that combine visual perception, language understanding, and action prediction for robotics. Needed to contextualize the problem of converting VLMs to VLAs. Quick check: Verify the model can predict actions from visual inputs.
- **LoRA (Low-Rank Adaptation)**: A parameter-efficient fine-tuning method that adds small trainable matrices to existing model layers. Needed to understand how VLM2VLA minimizes modifications to the VLM backbone. Quick check: Confirm that only LoRA parameters are updated during fine-tuning.
- **Action representation**: The way robot actions are encoded for model prediction. Needed to understand why representing actions as language is crucial for this method. Quick check: Compare performance using action-as-language versus discrete action indices.
- **Distribution alignment**: Ensuring that fine-tuning data follows the same distribution as pretraining data. Needed to understand how VLM2VLA prevents catastrophic forgetting. Quick check: Analyze the similarity between VLM pretraining data and robotics data with action-as-language.

## Architecture Onboarding
- **Component map**: VLM backbone -> LoRA adapters -> Action prediction head
- **Critical path**: Input image and instruction → VLM encoding → LoRA-adapted representation → Action language tokens → Robot execution
- **Design tradeoffs**: Minimal modifications to VLM (preserves reasoning) vs. potential limitations in action expressiveness; LoRA-only fine-tuning (parameter efficiency) vs. possible constraints on learning capacity.
- **Failure signatures**: Degradation in VQA performance indicates catastrophic forgetting; poor generalization to novel tasks suggests insufficient action representation; limited action vocabulary restricts task diversity.
- **First experiments**: 1) Compare VQA performance before and after fine-tuning; 2) Test zero-shot generalization on novel instructions; 3) Evaluate action prediction accuracy on held-out robotics tasks.

## Open Questions the Paper Calls Out
None

## Limitations
- Lack of quantitative comparisons with prior VL-to-VLA transfer methods makes it difficult to isolate the contribution of the action-as-language formulation.
- Dataset bias toward pick-and-place tasks may limit generalizability to more diverse or dexterous manipulation skills.
- Evaluation of zero-shot generalization is primarily qualitative, with limited ablation on the role of LoRA versus full fine-tuning.

## Confidence
- Catastrophic forgetting mitigation: **High** - Clear experimental setup and quantitative retention metrics support the claims.
- Action-as-language representation effectiveness: **Medium** - Supported by controlled comparisons but lacking head-to-head benchmarks with alternative representations.
- Generalization to novel tasks: **Low to Medium** - Results rely on small-scale out-of-distribution tests without extensive cross-dataset validation.

## Next Checks
1. Benchmark VLM2VLA against established VL-to-VLA transfer methods (e.g., RT-2, ALFRED-based approaches) on a common robotic manipulation suite.
2. Evaluate VLM2VLA on a broader set of vision-language tasks (e.g., GQA, NLVR2) to quantify cross-domain reasoning retention.
3. Conduct ablation studies comparing LoRA-only fine-tuning versus partial/full fine-tuning under identical action representation schemes.