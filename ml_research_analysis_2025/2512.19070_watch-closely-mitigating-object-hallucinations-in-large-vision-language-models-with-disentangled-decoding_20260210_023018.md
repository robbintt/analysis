---
ver: rpa2
title: 'Watch Closely: Mitigating Object Hallucinations in Large Vision-Language Models
  with Disentangled Decoding'
arxiv_id: '2512.19070'
source_url: https://arxiv.org/abs/2512.19070
tags:
- image
- hallucinations
- decoding
- visual
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of object hallucinations in Large
  Vision-Language Models (LVLMs), where models often fail to accurately identify objects
  in images, generating text that appears fluent but does not correspond to the visual
  content. The core method, Hallucination Disentangled Decoding (HDD), requires no
  training and enhances the original image by segmenting it and selecting images that
  augment the original, while also utilizing a blank image to eliminate language prior
  hallucinations in both the original and segmented images.
---

# Watch Closely: Mitigating Object Hallucinations in Large Vision-Language Models with Disentangled Decoding

## Quick Facts
- arXiv ID: 2512.19070
- Source URL: https://arxiv.org/abs/2512.19070
- Authors: Ruiqi Ma; Yu Yan; Chunhong Zhang; Minghao Yin; XinChao Liu; Zhihong Jin; Zheng Hu
- Reference count: 23
- Primary result: HDD achieves up to 8.1% absolute accuracy gain on POPE benchmarks by combining segmentation-based visual enhancement with blank-image contrastive decoding.

## Executive Summary
This paper introduces Hallucination Disentangled Decoding (HDD), a training-free method to reduce object hallucinations in Large Vision-Language Models (LVLMs). The approach enhances visual detail by segmenting images and selecting task-relevant segments via Jensen-Shannon divergence (JSD), then applies contrastive decoding with a blank image to suppress language priors. HDD demonstrates superior perceptual capabilities across multiple hallucination benchmarks without requiring model retraining.

## Method Summary
HDD operates by first segmenting the input image into two complementary views using the largest N masks from a segmentation model like SAM. It then generates a blank image and runs the LVLM on the original image, both segmented views, and the blank image to obtain logits. The method selects the most informative segment based on JSD versus the blank image, computes an adaptive weight δ from the JSD difference, and blends the original and selected segment logits. Finally, it applies contrastive decoding by subtracting α-weighted blank image logits to eliminate language prior hallucinations, producing the final HDD-adjusted logits for auto-regressive decoding.

## Key Results
- HDD achieves up to 8.1% absolute accuracy gain on POPE (binary object existence) benchmarks.
- Outperforms recent hallucination mitigation methods (VCD, SDCD, HALC, OPERA) on both POPE and CHAIR caption hallucination metrics.
- Demonstrates consistent improvements across LLaVA-1.5, InstructBLIP, and LLaVA-NeXT models with different hyperparameter settings for α.

## Why This Works (Mechanism)

### Mechanism 1: Visual Detail Enhancement via Segmentation
- Segmented images amplify local details that the visual encoder otherwise underweights.
- A segmentation model partitions the original image into entity-level masks. The LVLM produces separate output distributions for each segmented view; Jensen-Shannon divergence (JSD) vs. a blank image ranks which segment carries more task-relevant signal. The selected segment's logits are combined with the original image's logits.
- Core assumption: The visual encoder's sensitivity correlates with object area; cropping/masking objects increases their effective proportion in the input and improves logit prominence.
- Evidence: Experiments show logits for small entities scale with area; segmentation plus JSD-based selection enhances local details (Equations 5–8).
- Break condition: If segmentation masks are noisy, highly overlapping, or misaligned with the queried entity, the JSD-based selection may choose an unhelpful segment, failing to improve—and potentially degrading—accuracy.

### Mechanism 2: Language Prior Elimination via Contrastive Decoding
- Subtracting the output distribution from a blank image reduces language-prior–driven hallucinations.
- A blank image forces the LVLM into a language-only mode that reveals pure linguistic priors. Contrastive decoding subtracts the blank-image distribution from the image-conditioned distribution (Equation 9), attenuating tokens that arise from text co-occurrence rather than visual evidence.
- Core assumption: Tokens strongly activated by a blank image reflect statistical language priors not grounded in visual content.
- Evidence: Introduces "Decoding Inertia" via prompts like "On the road," which increase "car" logits even with a blank image; contrastive decoding with α-weighted subtraction is proposed.
- Break condition: If the blank-image distribution misattributes visually grounded tokens to language priors, contrastive subtraction may suppress correct visual outputs, especially for common objects strongly associated with typical prompts.

### Mechanism 3: Adaptive Weight Control
- An adaptive weight (δ) based on JSD differences automatically controls how much segmentation-derived signal is combined with the original.
- δ = |Div1 − Div2| measures relative informativeness between two segmented views vs. blank. Final logits are (1−δ)·logit(V) + δ·logit(vi*), blending original and selected-segment logits (Equations 7–8, 10).
- Core assumption: Larger JSD indicates more task-relevant visual content in a segment; the larger the JSD gap, the stronger the appropriate enhancement.
- Evidence: Equations 7–10 define the adaptive soft adjustment and the final HDD logits.
- Break condition: If JSD does not reliably correlate with task-relevant content (e.g., when distractor textures yield high divergence), the adaptive weight may over- or under-enhance.

## Foundational Learning

- **Auto-regressive decoding in LVLMs**: The method intervenes at each step on the next-token logit distribution; understanding yt = Mθ(v, x, y<t) and how priors accumulate is essential. Quick check: Can you write the token-by-token sampling loop and explain where logit modifications would be injected?
- **Contrastive decoding**: HDD uses (1+α)·logit(vin) − α·logit(vn) with a blank image; knowing how subtracting distributions changes the sampling probabilities is critical. Quick check: If a token has logit 4.0 with the real image and 6.0 with a blank image (α=0.5), what is the adjusted logit?
- **Semantic segmentation basics and mask application**: The pipeline segments V into v1, v2 via masks; understanding mask–image multiplication and "largest N masks" selection is necessary for implementation. Quick check: Given an image and a binary mask, how do you apply the mask to extract a segmented region?

## Architecture Onboarding

- **Component map**: Segmentation module (e.g., SAM) → produces N entity masks → Mask combiner → splits masks into two complementary views (v1, v2) plus retains original V → Blank-image generator → creates vn → LVLM forward passes → runs 4 inputs (V, v1, v2, vn) through the model to obtain logits → JSD selector → computes Div1, Div2, picks vi* with larger JSD vs. blank → Adaptive weight calculator → computes δ and performs logit blending (Equations 7–10) → Contrastive subtractor → applies α-weighted blank-image subtraction (Equation 9) → Sample the next token from the final HDD-adjusted logits.
- **Critical path**: 1. Segment the image and create v1, v2, vn. 2. Run LVLM forward for each view to get per-step logits. 3. Compute JSD between each segment distribution and the blank distribution; select best segment. 4. Compute δ and blend original + selected-segment logits. 5. Perform contrastive decoding with the blank distribution. 6. Sample the next token from the final HDD-adjusted logits; repeat auto-regressively.
- **Design tradeoffs**: Extra segmentation and 4 forward passes per generation step (or per query if cached) increase latency vs. vanilla decoding; the paper reports latency comparable to VCD and better than HALC/OPERA. Choice of segmentation model (SAM, Mask2Former, Mask R-CNN) had minimal performance variance in ablations (≈0.5% accuracy), suggesting robustness. The hyperparameter α must be tuned per model; ranges differ (e.g., 0.1–0.6 for LLaVA-1.5 vs. 1.0–1.6 for LLaVA-NeXT).
- **Failure signatures**: Persistent hallucinations on small, densely packed objects despite segmentation. Over-suppression of common but correct object names due to high blank-image logits. Noisy or oversegmented masks leading to unstable JSD rankings and inconsistent enhancements.
- **First 3 experiments**: 1. Replicate the "small vs. large entity" logit sensitivity check on a minimal LVLM; visualize logits vs. object area to validate the size-sensitivity hypothesis. 2. Implement blank-image contrastive decoding alone (no segmentation enhancement) and measure POPE accuracy/F1 to isolate the language-prior reduction effect. 3. Add the segmentation + JSD-based enhancement on top of the contrastive decoding; compare against step 2 and vanilla decoding to quantify the visual enhancement contribution.

## Open Questions the Paper Calls Out
The authors explicitly state in the Limitations section that their research focuses on image-text alignment and has "not extending to other modalities such as video or 3D point clouds," leaving this exploration for future work.

## Limitations
- The method requires 4 forward passes per generation step (original, 2 segments, blank), increasing latency compared to vanilla decoding.
- The segmentation-based enhancement may fail for small objects if they are not captured in the largest N masks, as the method prioritizes large areas.
- The optimal hyperparameter α varies significantly across models (0.1–0.6 for LLaVA-1.5 vs. 1.0–1.6 for LLaVA-NeXT), requiring per-model tuning.

## Confidence

- **High Confidence**: The overall approach of combining segmentation-based visual enhancement with blank-image contrastive decoding is methodologically sound and builds on established techniques. The ablation showing InstructBLIP's superior POPE performance on small entities provides direct support for the core hypothesis that enhancing visual detail mitigates hallucinations.
- **Medium Confidence**: The specific HDD implementation details (JSD ranking, adaptive δ blending, and the exact blank image construction) are internally consistent with the proposed mechanism, but the paper lacks exhaustive ablation studies isolating each component's contribution.
- **Low Confidence**: The assumption that JSD differences between segments and blank images directly correlate with task-relevant informativeness is plausible but under-tested. The paper does not provide qualitative examples where JSD ranking succeeds or fails, nor does it explore alternative selection criteria.

## Next Checks

1. **JSD Selection Ablation**: Run HDD with random segment selection (instead of JSD ranking) on POPE and CHAIR benchmarks. If random selection performs comparably, the JSD heuristic's contribution is questionable. If it drops significantly, this validates JSD as a useful ranking signal.

2. **Blank Image Construction Sensitivity**: Test HDD with variations of the blank image (e.g., all-zero pixels, uniform gray, or a learned "no-visual" embedding). Measure whether performance is sensitive to blank image choice, which would indicate whether the contrastive subtraction is robust or overfit to a specific blank image design.

3. **Common Object Hallucination Test**: Create a targeted evaluation set with common objects (e.g., cars, dogs) where language priors are strong. Measure if HDD's contrastive decoding over-suppresses correct outputs for these objects compared to rare objects, isolating the risk of spurious prior elimination.