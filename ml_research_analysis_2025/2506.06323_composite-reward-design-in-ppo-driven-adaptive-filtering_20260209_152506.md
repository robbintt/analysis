---
ver: rpa2
title: Composite Reward Design in PPO-Driven Adaptive Filtering
arxiv_id: '2506.06323'
source_url: https://arxiv.org/abs/2506.06323
tags:
- noise
- signal
- filter
- adaptive
- filtering
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the problem of adaptive signal denoising in
  dynamic, non-stationary environments where traditional filters like LMS, RLS, and
  Kalman filters struggle due to rigid assumptions about noise statistics and stationarity.
  The proposed method uses Proximal Policy Optimization (PPO) with a composite reward
  function that balances SNR improvement, MSE reduction, and residual smoothness to
  learn an adaptive filtering policy.
---

# Composite Reward Design in PPO-Driven Adaptive Filtering

## Quick Facts
- arXiv ID: 2506.06323
- Source URL: https://arxiv.org/abs/2506.06323
- Reference count: 11
- Primary result: RL-based adaptive filtering with PPO achieves superior SNR performance on non-stationary noise, generalizing from Gaussian-only training to multiple noise types

## Executive Summary
This work introduces a PPO-based adaptive filtering approach that learns to update FIR filter coefficients in real-time for signal denoising in dynamic environments. Traditional filters struggle with non-stationary noise due to rigid statistical assumptions, while the proposed RL method uses a composite reward function balancing SNR improvement, MSE reduction, and residual smoothness. The agent demonstrates strong generalization, maintaining high SNR performance across noise types unseen during training, and achieves real-time inference speeds under 1 ms per time step.

## Method Summary
The method frames adaptive filtering as a sequential decision problem where an LSTM-based PPO agent updates FIR filter coefficients based on a sliding window of recent signal samples. The agent receives a composite reward combining SNR improvement, MSE reduction, and total variation (smoothness) of residuals. Training uses only Gaussian noise, yet evaluation shows generalization to Laplacian, impulse, pink, brown, and uniform noise. The architecture includes an LSTM actor producing Gaussian-distributed coefficient updates, a two-layer MLP critic, and an FIR filter block that processes the noisy input to generate denoised output.

## Key Results
- PPO-based filter achieves 21.3 dB SNR on test signals, outperforming classical baselines (LMS, RLS, Kalman, Wiener) across all noise types
- Generalizes from Gaussian-only training to impulse, Laplacian, pink, brown, and uniform noise with minimal performance degradation
- Maintains real-time inference speeds under 1 ms per time step while preserving smooth residual outputs

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The composite reward formulation enables stable training and generalization by balancing competing objectives.
- Mechanism: The reward R(t) = α·ΔSNR(t) − β·MSE(t) − γ·TV(r(t)) simultaneously encourages noise reduction (SNR/MSE terms) while penalizing erratic output fluctuations (TV smoothness term). This prevents the agent from achieving low error through high-frequency "jitter" corrections that would fail on new noise distributions.
- Core assumption: The three reward components capture the essential tradeoffs for good filtering; no critical objective is missing.
- Evidence anchors:
  - [abstract] "composite reward that balances SNR improvement, MSE reduction, and residual smoothness"
  - [section VII-E, Table I] Full reward achieves 21.3 dB SNR with stable generalization; removing TV causes divergence (<0 dB); removing MSE reduces to 12.8 dB with borderline stability
  - [corpus] Weak direct evidence—neighbor papers address PPO stability (PPO-BR, AM-PPO) but not composite reward design for signal processing
- Break condition: If reward weights (α, β, γ) are poorly calibrated for a new domain, training may diverge or converge to a suboptimal policy that over-emphasizes one component.

### Mechanism 2
- Claim: PPO's clipped surrogate objective provides stable policy updates under nonstationary noise conditions.
- Mechanism: The clipping constraint clip(r_t(θ), 1−ε, 1+ε) limits policy deviation per update, preventing destructive large updates when noise statistics shift suddenly. This conservatism allows the agent to adapt gradually without catastrophic forgetting.
- Core assumption: The noise nonstationarity occurs at a rate slower than the policy's effective adaptation window.
- Evidence anchors:
  - [section IV] "By clipping large policy ratio deviations, PPO ensures that each policy update is conservative, which is crucial when the reward landscape shifts due to changing noise conditions"
  - [section VII-B] PPO maintains high SNR under impulsive noise where classical methods degrade significantly
  - [corpus] PPO-BR and AM-PPO papers corroborate that PPO's trust region mechanism benefits from adaptive modulation, suggesting the base mechanism is sound but can be improved
- Break condition: If noise statistics change faster than the effective learning rate permits adaptation, the policy may lag perpetually behind optimal behavior.

### Mechanism 3
- Claim: Training only on Gaussian noise generalizes to unseen noise distributions because the learned policy captures structural signal properties rather than noise-specific patterns.
- Mechanism: The sliding window observation (64 samples + residual) combined with LSTM memory allows the agent to learn temporal signal structure. Since the composite reward penalizes residual roughness universally, the policy learns to suppress any deviation from smooth signal structure regardless of noise origin.
- Core assumption: The clean signal has consistent temporal structure (sinusoidal in experiments) that differs characteristically from noise; real-world signals may have more complex structure.
- Evidence anchors:
  - [section VI] Agent trained only on Gaussian noise; evaluated on Laplacian, impulse, pink, brown, and uniform
  - [section VII-B, Fig. 1] PPO retains high SNR across all six noise types despite single-noise training
  - [corpus] No direct corpus validation of Gaussian-to-diverse-noise generalization in RL filtering
- Break condition: If deployed on signals with fundamentally different temporal structure (e.g., chaotic, speech with plosives), the smoothness prior may over-regularize and distort legitimate signal features.

## Foundational Learning

- Concept: **Markov Decision Processes (MDP) for signal processing**
  - Why needed here: The paper frames filtering as sequential decision-making where state = signal window, action = filter coefficient updates, reward = denoising quality. Understanding this formulation is prerequisite to grasping why RL applies to filtering at all.
  - Quick check question: Can you explain why adaptive filtering can be written as an MDP with the reward defined as negative MSE?

- Concept: **FIR filter fundamentals**
  - Why needed here: The PPO agent directly manipulates FIR coefficients h_k. Without understanding how FIR filters convolve samples, you cannot interpret what the agent is actually learning to control.
  - Quick check question: Given an FIR filter with coefficients [0.5, 0.3, 0.2], what is the output at time t for input samples [1.0, 2.0, 3.0]?

- Concept: **Policy gradient basics and the advantage function**
  - Why needed here: PPO is a policy gradient method using advantage estimates A_t = R_t − V(s_t). The paper uses baseline-subtracted returns without GAE. Understanding advantage is necessary to debug training dynamics.
  - Quick check question: Why does subtracting a value baseline from returns reduce variance in policy gradient estimates?

## Architecture Onboarding

- Component map:
  - Input processor -> LSTM actor -> MLP critic -> FIR filter block -> Reward computer
  - FIR filter block -> Output
  - Reward computer -> PPO update

- Critical path:
  1. Receive noisy sample y(t), append to sliding window buffer
  2. Concatenate window + residual → state s_t
  3. Actor samples action a_t ~ π(·|s_t) → FIR coefficient update
  4. Apply FIR filter to produce x̂(t)
  5. Compute reward R(t) using reference signal (training) or self-consistency metrics (inference)
  6. Store transition (s_t, a_t, R_t); periodically update policy via PPO clipped objective

- Design tradeoffs:
  - **LSTM vs. MLP for temporal modeling**: Paper uses LSTM for memory; transformer could capture longer dependencies but adds latency
  - **GAE vs. simple baseline subtraction**: Paper omits GAE for simplicity; may increase variance but improved stability in early training
  - **Direct coefficient control vs. step-size tuning**: Paper directly updates coefficients; alternative (tuning μ for LMS-like updates) may be more interpretable but less expressive

- Failure signatures:
  - **Reward divergence (SNR < 0 dB)**: Likely missing TV smoothness term—verify reward weights
  - **Policy collapse to constant output**: Check if reward scaling dominates gradient signal; reduce α or increase β
  - **Slow adaptation to noise shifts**: Increase learning rate or reduce PPO clipping ε conservatively
  - **Residual shows high-frequency artifacts**: Increase γ weight on TV penalty

- First 3 experiments:
  1. **Reproduce ablation**: Train with full reward, then remove TV term—confirm divergence matches Table I. This validates your implementation before exploring new domains.
  2. **Noise-type sensitivity**: Train separate agents on Gaussian vs. impulse noise; compare generalization profiles. This reveals whether training distribution matters or if Gaussian is sufficient.
  3. **Latency budget test**: Profile inference time with different LSTM hidden sizes (64, 128, 256) to find the smallest model meeting your <1 ms constraint on target hardware.

## Open Questions the Paper Calls Out

- **Can the proposed PPO-based filter maintain its performance advantage when deployed on resource-constrained embedded hardware?**
  - Basis: The authors note that for "power- or resource-constrained devices, further optimization or simplification of the learned policy model might be necessary."
  - Why unresolved: While inference is under 1 ms on an M2 chip, the computational complexity relative to simple LMS filters may be prohibitive for low-power IoT devices.
  - What evidence would resolve it: Successful deployment and latency benchmarks on embedded microcontrollers or FPGA implementations.

- **Does the training methodology generalize effectively to real-world signals with unmodeled disturbances and non-linearity?**
  - Basis: The paper states "real-world validation remains outstanding" and that real signals "may reveal additional challenges such as unmodeled disturbances."
  - Why unresolved: All experiments were conducted on synthetic signals constructed from sums of sinusoids, which may not capture the complexity of live wireless or biomedical data.
  - What evidence would resolve it: Evaluation results on Over-The-Air (OTA) radio signals or physiological datasets (e.g., EEG) showing comparable SNR improvements.

- **Can the single-channel PPO framework be extended to handle multichannel or MIMO filtering scenarios?**
  - Basis: The authors list "multichannel or MIMO filtering scenarios" as a specific avenue for future research.
  - Why unresolved: The current MDP and network architecture are defined for a single stream of observed samples and scalar filter updates.
  - What evidence would resolve it: A modified agent architecture capable of managing cross-channel correlations and a comparative study against multichannel baselines.

## Limitations

- **Real-world validation gap**: All experiments use synthetic signals; performance on real-world non-stationary signals with unmodeled disturbances remains untested
- **Hardware deployment constraints**: While inference is fast on M2 hardware, computational requirements may be prohibitive for resource-constrained embedded devices
- **Single-channel architecture**: The framework is limited to single-input single-output filtering and cannot handle multichannel or MIMO scenarios without significant architectural modifications

## Confidence

- **High confidence**: The PPO-based filter achieves superior SNR performance on synthetic signals across multiple noise types compared to classical baselines, validating the core RL approach.
- **Medium confidence**: The composite reward mechanism (SNR/MSE/TV) prevents training divergence and enables generalization, though exact weight tuning remains domain-specific.
- **Low confidence**: Generalization to real-world non-stationary signals (speech, biomedical, seismic) without retraining, due to reliance on smoothness priors that may not match complex signal structures.

## Next Checks

1. **Cross-domain stress test**: Deploy the pre-trained Gaussian agent on real-world non-stationary signals (speech with plosives, ECG with arrhythmias). Measure SNR vs baseline and document any structural distortions from smoothness regularization.
2. **Latency hardware profiling**: Implement the PPO agent on target DSP/FPGA hardware. Profile inference latency and memory usage with varying LSTM sizes (64, 128, 256) to identify minimum viable model meeting real-time constraints.
3. **Noise-level envelope analysis**: Systematically vary training SNR from -5 dB to 30 dB. Plot performance curves for each noise type to identify the operational SNR range where PPO maintains advantage over classical filters.