---
ver: rpa2
title: 'From Kicking to Causality: Simulating Infant Agency Detection with a Robust
  Intrinsic Reward'
arxiv_id: '2507.15106'
source_url: https://arxiv.org/abs/2507.15106
tags:
- agent
- reward
- cais
- action
- causal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the challenge of robust agency detection in
  noisy environments, where standard reinforcement learning methods fail due to their
  reliance on correlational rewards. To overcome this, the authors introduce the Causal
  Action Influence Score (CAIS), a novel intrinsic reward rooted in causal inference
  that quantifies an action's influence by measuring the 1-Wasserstein distance between
  the learned distribution of sensory outcomes conditional on that action and the
  baseline outcome distribution.
---

# From Kicking to Causality: Simulating Infant Agency Detection with a Robust Intrinsic Reward

## Quick Facts
- **arXiv ID**: 2507.15106
- **Source URL**: https://arxiv.org/abs/2507.15106
- **Reference count**: 33
- **Primary result**: CAIS enables robust agency detection in noisy environments where standard RL methods fail, and reproduces psychological phenomena like extinction burst.

## Executive Summary
This work introduces the Causal Action Influence Score (CAIS), a novel intrinsic reward mechanism for agency detection that leverages causal inference rather than correlational rewards. Tested in a simulated infant-mobile environment, CAIS enables agents to discover their causal efficacy even in the presence of significant environmental noise. The method quantifies an action's causal influence by measuring the 1-Wasserstein distance between the distribution of sensory outcomes conditional on that action and a baseline outcome distribution. This approach successfully overcomes the fundamental limitation of standard reinforcement learning methods that fail when environmental noise obscures true causal relationships.

## Method Summary
The method uses quantile regression to model the distribution of sensory outcomes (h) for each action and a baseline distribution. The 1-Wasserstein distance between these distributions serves as the CAIS reward. A pre-trained frozen visual encoder (CLTT-trained U-Net) processes 128×128 RGB observations into 64-dimensional latents, which are concatenated with 47-dimensional proprioceptive states. A SARSA policy network maps the combined state to Q-values for 24 discrete actions (2 actions × 12 joints). The method optionally combines CAIS with a surprise signal (prediction error from the CAIS predictive model) to reproduce extinction burst behavior.

## Key Results
- CAIS successfully discovers causal efficacy in noisy environments where correlation-based rewards (RTL/MTL) fail completely
- The predictive model learned for CAIS, when augmented with surprise, reproduces the "extinction burst" phenomenon
- CAIS demonstrates psychological plausibility through its ability to model developmental agency detection mechanisms

## Why This Works (Mechanism)

### Mechanism 1: Causal Influence via Distributional Divergence
Measuring the Wasserstein distance between action-conditional and baseline outcome distributions isolates true causal influence from environmental noise. CAIS computes W₁(p(h|a), p(h)) where p(h|a) is the distribution of sensory outcomes given action a, and p(h) is the baseline outcome distribution. If an action has no causal effect, distributions match; if it causes systematic change, they diverge. Core assumption: The baseline distribution p(h) adequately captures environmental noise, and causal actions produce detectable distributional shifts.

### Mechanism 2: Quantile Regression for Distribution Modeling
Non-parametric quantile regression captures arbitrary outcome distributions and enables efficient Wasserstein computation. Model 49 quantiles (τ ∈ {0.02, 0.04, ..., 0.98}) for both conditional and baseline distributions using asymmetric quantile Huber loss. Wasserstein distance computed via W₁ = ∫₀¹ |q₁(τ) − q₂(τ)|dτ. Core assumption: Outcome distributions can be adequately approximated by marginal quantiles without modeling temporal dependencies explicitly.

### Mechanism 3: Surprise-Augmented Reward for Extinction Burst
Prediction error from the CAIS predictive model, when combined with CAIS reward, reproduces extinction burst behavior. Surprise = distance between expected outcome E[h|a] and actual h_{t+1}. When contingency is removed, persistent expectation violation generates reward spikes that drive intensified exploratory behavior. Core assumption: The predictive model provides sufficiently accurate expectations; extinction burst can be driven by reward modulation rather than separate behavioral mechanisms.

## Foundational Learning

- **Distributional Reinforcement Learning & Quantile Regression**
  - Why needed here: CAIS requires modeling full outcome distributions, not just expected values. Quantile regression provides non-parametric distribution approximation.
  - Quick check question: Why does modeling 49 quantiles capture more information than modeling only the mean and variance?

- **Wasserstein Distance (Optimal Transport)**
  - Why needed here: Chosen specifically over KL divergence because it provides meaningful distance even for non-overlapping distributions—critical when comparing noisy vs causal outcome distributions.
  - Quick check question: When would KL divergence fail or produce infinite values that Wasserstein distance handles gracefully?

- **Causal Inference: Interventional vs Observational Distributions**
  - Why needed here: The core distinction between p(h|a) (what happens when I act) versus p(h) (what happens naturally) mirrors do-calculus intervention concepts.
  - Quick check question: What confounding factors could make p(h|a) ≠ p(h) even if action a has no true causal effect on h?

## Architecture Onboarding

- **Component map**: Visual observation → Visual Encoder (frozen) → h_t → Concatenate with proprioception → SARSA Policy Network → Action selection → Environment → New observation → Update quantile models → Compute Wasserstein distance → CAIS reward → SARSA update

- **Critical path**: Environment step → (visual observation, proprioception) → Visual encoder (frozen) → latent h_t → Concatenate with proprioception → SARSA network → softmax action selection → Action applied for 200ms, then 400ms rest → environment updates → New observation → update quantile models with outcome → Compute Wasserstein distance between updated conditional and baseline quantiles → CAIS reward → Optional: compute surprise from prediction error → SARSA Q-value update with combined reward

- **Design tradeoffs**:
  - Discrete vs continuous actions: Binary choice per joint (MOVE/NO_TORQUE) simplifies exploration but limits motor expressiveness
  - Frozen encoder vs end-to-end: Pre-trained visual encoder provides stable representations but cannot adapt to task-specific features
  - 49 quantiles: Balance between distributional precision and computational/memory cost
  - Expected SARSA vs Q-learning: On-policy for psychological plausibility vs off-policy for sample efficiency

- **Failure signatures**:
  - Noisy condition with RTL/MTL rewards: Reward signal high and erratic across all phases, no Q-value separation between attached/unattached limbs
  - Surprise without robust base model: No clear extinction burst spike; surprise signal drowned by noisy base reward
  - Insufficient baseline sampling: CAIS rewards unstable early in training before baseline distribution converges

- **First 3 experiments**:
  1. Free-Mobile with RTL reward: Verify baseline functionality—should show clear reward increase during attached phase, Q-value separation for contingent limb
  2. Noisy-Mobile with CAIS: Critical test—CAIS reward for attached limb should separate from unattached limbs despite 40N random force, unlike RTL which fails
  3. CAIS+Surprise extinction phase: Verify extinction burst—combined reward should spike at extinction onset (step 2001), producing temporary increase in MOVE probability before return to baseline

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the CAIS framework be extended to computationally model the developmental "contingency switch," where an infant's preference shifts from perfect self-contingency to high-but-imperfect social contingency?
- Basis in paper: The Conclusion states that "next steps include computationally modeling the 'contingency switch' itself, likely by introducing probabilistic action-outcome links."
- Why unresolved: The current model successfully simulates the pre-switch phase (discovering self-efficacy) but does not implement the mechanism for shifting preference to social agents.
- What evidence would resolve it: A modification of the model where the agent eventually prefers an imperfectly contingent social partner over a perfectly contingent object after the initial phase of self-discovery.

### Open Question 2
- Question: Can the CAIS mechanism be integrated into an agent that learns continuous actions and visual representations end-to-end, rather than relying on discrete actions and pre-trained encoders?
- Basis in paper: The Conclusion identifies that "Integrating the CAIS mechanism into agents that learn representations and continuous actions end-to-end remains a key long-term objective."
- Why unresolved: The current implementation relies on a frozen visual encoder (trained via CLTT) and a discrete action space (binary torque) to simplify the learning problem.
- What evidence would resolve it: A learning curve demonstrating that an agent using CAIS can acquire continuous motor control and visual features simultaneously in a noisy environment without pre-training.

### Open Question 3
- Question: Does the robustness of the CAIS reward generalize effectively from the simulated MuJoCo environment to physical robotic platforms?
- Basis in paper: The Conclusion lists "validating the agent’s robustness by transferring it to a physical robotic platform" as a necessary future step.
- Why unresolved: The results are currently limited to a simulation environment which may not capture the full complexity of sensor noise and physical dynamics found in the real world.
- What evidence would resolve it: Successful deployment of the CAIS-based policy on a physical robot discovering its causal efficacy in a noisy real-world setting.

## Limitations

- The method relies on sufficient baseline sampling to capture environmental noise, which may be challenging in highly dynamic environments
- The approach assumes that causal actions produce detectable distributional shifts, which may not hold for weak causal signals
- The psychological plausibility claims regarding extinction burst reproduction require more rigorous validation beyond qualitative observation

## Confidence

- **Mechanism 1 (Causal Influence via Distributional Divergence)**: Medium - Theoretically sound but practical limitations exist in complex scenarios
- **Mechanism 2 (Quantile Regression for Distribution Modeling)**: Medium - Innovative but may have limitations with complex temporal dependencies or multimodal distributions
- **Mechanism 3 (Surprise-Augmented Reward for Extinction Burst)**: Low - Interesting but requires more rigorous validation beyond qualitative observation

## Next Checks

1. **Cross-Environment Generalization**: Test CAIS in environments with multiple potential causal factors to verify it can distinguish true causal relationships from spurious correlations in more complex scenarios

2. **Quantile Sensitivity Analysis**: Systematically vary the number and placement of quantiles (e.g., 25, 49, 97 quantiles) to determine the optimal configuration for different types of outcome distributions

3. **Alternative Distribution Metrics**: Compare CAIS performance against alternative distributional distance measures (e.g., KL divergence, Maximum Mean Discrepancy) and maximum likelihood estimation methods to validate that Wasserstein distance is indeed the optimal choice