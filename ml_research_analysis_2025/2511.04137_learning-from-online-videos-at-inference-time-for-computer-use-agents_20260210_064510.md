---
ver: rpa2
title: Learning from Online Videos at Inference Time for Computer-Use Agents
arxiv_id: '2511.04137'
source_url: https://arxiv.org/abs/2511.04137
tags:
- video
- task
- actions
- videos
- action
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Computer-use agents still struggle with domain-specific procedural
  knowledge, despite rapid progress. Humans overcome this gap by watching online video
  tutorials, searching, skimming, and selectively imitating short segments that match
  their current subgoal.
---

# Learning from Online Videos at Inference Time for Computer-Use Agents

## Quick Facts
- arXiv ID: 2511.04137
- Source URL: https://arxiv.org/abs/2511.04137
- Reference count: 32
- Outperforms state-of-the-art Jedi framework by 2.1 points on OSWorld-Verified and AgentOccam by 4.2 points on WebArena

## Executive Summary
Computer-use agents struggle with domain-specific procedural knowledge that humans overcome by watching online video tutorials. This paper proposes a framework that retrieves tutorial videos at inference time, converts them into structured demonstration trajectories with explicit objectives and action sequences, and dynamically selects relevant trajectories as in-context guidance during execution. Using a vision-language model to infer UI actions and segment videos into short subsequences, the framework achieves consistent improvements over strong base agents on both desktop (OSWorld) and web (WebArena) benchmarks.

## Method Summary
The framework implements a three-stage pipeline: (1) Video retrieval using YouTube API with coarse LLM filtering and VLM content verification, (2) Video processing with Qwen2.5-VL-32B-Instruct for action labeling and trajectory construction including objective relabeling, and (3) Two-stage trajectory selection at inference time that first ranks by textual objectives then selects a single trajectory based on full action sequences. The method converts raw videos into structured guidance units that match the agent's decision-making granularity, enabling dynamic in-context learning without parameter updates.

## Key Results
- Outperforms state-of-the-art Jedi framework by 2.1 points on OSWorld-Verified
- Outperforms AgentOccam by 4.2 points on WebArena
- Benefits from more relevant videos, indicating potential for further performance scale-up
- Visual information in trajectories contributes 4.7 points to performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Converting raw videos into structured demonstration trajectories with explicit objectives and action sequences provides more actionable guidance than unstructured text or transcripts.
- Mechanism: The VLM extracts UI actions from frame transitions, segments long videos into short subsequences, and generates textual objectives for each subsequence via "hindsight relabeling."
- Core assumption: VLMs can accurately infer UI actions from pixel-level screen transitions without access to underlying action labels.
- Evidence anchors: Performance drops 4.7 points without trajectory segmentation (Table 4); VideoAgentTrek uses similar VLM-based action extraction but for offline pretraining rather than inference-time guidance.
- Break condition: If VLM action inference accuracy is low (<70% correct actions), trajectory guidance may mislead rather than help.

### Mechanism 2
- Claim: Dynamic two-stage trajectory selection at each inference step focuses the agent on locally relevant guidance, preventing distraction from irrelevant video content.
- Mechanism: Stage 1 does coarse ranking using only textual objectives (efficient). Stage 2 inspects full action sequences and initial screenshots to select a single trajectory.
- Core assumption: The agent's current subgoal can be inferred from the task description and current observation, enabling relevant trajectory matching.
- Evidence anchors: 4.5-point drop without selection (Table 4); ViVa uses video-trained value functions for guidance but requires offline training rather than inference-time retrieval.
- Break condition: If trajectory objectives are poorly generated or task-observation context is insufficient, selection becomes random.

### Mechanism 3
- Claim: Visual information in trajectories (screenshots) provides grounding that text-only summaries cannot capture.
- Mechanism: Each trajectory includes not just the objective and action descriptions but also screenshots at each step.
- Core assumption: Visual similarity between trajectory screenshots and agent's current observation aids decision-making.
- Evidence anchors: 4.7-point performance drop when visual information removed (Table 4); VideoAgentTrek also emphasizes visual grounding from videos for computer-use agents.
- Break condition: If visual appearance differs significantly between tutorial and task environments (different themes, versions), visual grounding may fail.

## Foundational Learning

- Concept: **Imitation Learning from Observations**
  - Why needed here: The core approach treats videos as state-only demonstrations without explicit action labels, requiring understanding of how to infer actions from state transitions.
  - Quick check question: Can you explain why inverse dynamics models help convert observations into actionable trajectories?

- Concept: **In-Context Learning**
  - Why needed here: The framework relies on providing demonstration trajectories in the agent's context window at inference time, without parameter updates.
  - Quick check question: How does providing demonstrations in-context differ from fine-tuning on demonstration data?

- Concept: **Hindsight Experience Relabeling**
  - Why needed here: The paper adapts this RL technique to generate objectives for partial video subsequences that don't complete the original video's task.
  - Quick check question: Given a sequence of actions that doesn't achieve the original goal, how would you assign it a new achievable objective?

## Architecture Onboarding

- Component map: Video Retrieval -> Video Processing -> Video Application
- Critical path: Video processing quality directly determines downstream performance. If VLM action inference or objective generation fails, no amount of selection optimization helps.
- Design tradeoffs:
  - Shorter clips (20 frames) vs. longer: Shorter clips reduce computational cost but may miss multi-step action context
  - Single trajectory vs. multiple per step: Single trajectory reduces context window usage but limits guidance diversity
  - Strict filtering vs. loose: Strict filtering improves precision but reduces coverage (only 211/361 OSWorld tasks had relevant videos)
- Failure signatures:
  - High token costs from processing multiple videos per task
  - Context window overflow if trajectories are too long
  - Performance degradation on tasks without relevant YouTube videos (baseline behavior on 150 OSWorld tasks)
  - Mismatch between tutorial UI version and task environment
- First 3 experiments:
  1. Baseline comparison: Run framework on 211 OSWorld tasks with videos, compare success rate against Jedi baseline and transcript-only variant (expect +3.5 and +3.6 improvement respectively per Table 1)
  2. Ablation on trajectory selection: Disable two-stage selection, always use longest trajectory per video; expect ~4.5 point drop per Table 4
  3. Video quantity scaling: Vary available videos per task (1 vs. 3.6) to verify performance scales with video availability; expect ~3.4 point improvement with more videos per Table 3

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the performance improvement from video guidance scale predictably with the volume and diversity of retrieved tutorials?
- Basis in paper: The conclusion states "our method benefits from more relevant videos, indicating the potential for further performance scale-up," and Section 4.3 analyzes performance with varying video counts.
- Why unresolved: The current experiments were limited to an average of 3.6 videos per task; the saturation point or scaling laws for this data source remain unknown.
- What evidence would resolve it: Experiments varying video counts from 0 to 100+ across diverse benchmarks to plot performance scaling curves.

### Open Question 2
- Question: Can the framework's accuracy be improved by integrating specialized inverse dynamics models (IDMs) for action labeling instead of relying on off-the-shelf VLMs?
- Basis in paper: Section 2 notes that concurrent works train specialized models and suggests "our method can potentially be further enhanced if equipped with the specialized inverse dynamics model trained from their works."
- Why unresolved: The current framework relies on prompting general-purpose VLMs for action inference, which introduces noise requiring heuristic filtering (Algorithm 1).
- What evidence would resolve it: Ablation studies replacing the VLM-based action labeling component with a trained IDM to measure error reduction.

### Open Question 3
- Question: How can the framework maintain robustness for tasks where the video retrieval step fails to find relevant tutorials?
- Basis in paper: Section 4.2 reports that videos were retrieved for only 211 of 361 tasks, and performance on the remaining 150 tasks defaults to the base agent without adaptive fallback mechanisms.
- Why unresolved: The method currently relies on the existence of specific external content; retrieval failure results in zero gain over the baseline.
- What evidence would resolve it: Analysis of fallback strategies, such as query relaxation or synthesizing guidance from internal knowledge when retrieval yields null results.

### Open Question 4
- Question: Is the selection of a single trajectory sufficient, or can agents benefit from synthesizing guidance across multiple trajectories simultaneously?
- Basis in paper: Section 3.4 states the mechanism "dynamically chooses a single trajectory to add in context," likely to manage context window constraints, but this may exclude complementary information from other relevant videos.
- Why unresolved: The paper does not explore whether combining sub-trajectories from different videos could solve complex tasks requiring multi-faceted procedural knowledge.
- What evidence would resolve it: Experiments comparing single-trajectory selection against a fusion approach that injects segmented steps from multiple top-ranked videos.

## Limitations
- 41.6% of OSWorld tasks (150/361) had no relevant YouTube videos, suggesting limited real-world applicability when video coverage is sparse
- Reliance on gpt-5-mini-2025-08-07 creates significant reproduction barrier as this model is not publicly available
- Performance depends on visual similarity between tutorial and target environments, which may fail with different UI themes or versions

## Confidence

**High**: The core mechanism of converting videos to structured trajectories with objectives improves over text-only baselines (4.7 point drop without trajectory segmentation, Table 4).

**Medium**: The two-stage selection mechanism's effectiveness (4.5 point drop without selection) depends on VLM accuracy in inferring both objectives and actions.

**Medium**: Visual information contribution (4.7 point drop without visuals) assumes sufficient similarity between tutorial and target environments.

## Next Checks

1. Test framework performance with publicly available VLMs (GPT-4o, Claude 3.5 Sonnet) to establish baseline substitution impact.
2. Measure action inference accuracy of the VLM on a held-out video dataset to validate the core assumption about state-to-action conversion quality.
3. Evaluate performance on tasks with zero retrieved videos to confirm the framework degrades gracefully to base agent behavior as claimed.