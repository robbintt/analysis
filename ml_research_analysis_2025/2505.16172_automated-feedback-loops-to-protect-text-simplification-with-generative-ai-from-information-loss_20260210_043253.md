---
ver: rpa2
title: Automated Feedback Loops to Protect Text Simplification with Generative AI
  from Information Loss
arxiv_id: '2505.16172'
source_url: https://arxiv.org/abs/2505.16172
tags:
- text
- simplified
- missing
- information
- entities
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study addresses the challenge of information loss during automated
  text simplification, particularly in health-related content, where missing critical
  information can impact comprehension. The authors propose an automated feedback
  loop using generative AI (ChatGPT) to detect and reinsert missing elements in simplified
  text.
---

# Automated Feedback Loops to Protect Text Simplification with Generative AI from Information Loss

## Quick Facts
- arXiv ID: 2505.16172
- Source URL: https://arxiv.org/abs/2505.16172
- Reference count: 31
- Adding all missing entities provides best document-level semantic alignment, while adding all missing words excels at summary level

## Executive Summary
This study addresses information loss during automated text simplification, particularly in health-related content where missing critical information can impact comprehension. The authors propose an automated feedback loop using generative AI (ChatGPT) to detect and reinsert missing elements in simplified text. By identifying missing words through frequency analysis and missing named entities through biomedical NER, then regenerating text with these elements reinserted, the approach significantly improves semantic alignment with the original text. The method achieves cosine similarity improvements of up to 0.916 at the document level and ROUGE-1 scores of 0.656, demonstrating that comprehensive reinsertion outperforms selective or random approaches.

## Method Summary
The method involves four main steps: (1) simplify original health texts using GPT-4-0613, (2) detect missing information through two parallel paths—frequency analysis of words appearing ≥2 times in original but <2 times in simplified, and biomedical named entity recognition to find entities present in original but missing in simplified, (3) regenerate text using five different approaches (adding all missing entities, adding all missing words, adding top-3 ranked entities by GPT-4, adding 3 random entities, or adding a random number of entities equal to missing words), and (4) evaluate results using cosine similarity on sentence embeddings and ROUGE-1 scores at both document and summary levels. The study used 50 BMJ rheumatology articles as source material.

## Key Results
- Comprehensive entity reinsertion (approach A1) achieved highest document-level cosine similarity of 0.9162
- Adding all missing words (approach A2) was most effective at preserving summary-level semantic content with ROUGE-1 score of 0.6555
- GPT-4's entity ranking mechanism (approach A3) performed worse than random selection, with mean cosine similarity of 0.8758
- Approaches involving selective entity addition (A3 and A4) showed significantly less improvement than comprehensive approaches

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Comprehensive entity reinsertion improves document-level semantic alignment more effectively than selective or random insertion.
- Mechanism: Named entities (extracted via scispacy) represent domain-critical concepts. By identifying all entities present in original but missing in simplified text (set difference: SE_missing = SE_original ∖ SE_simplified), then instructing an LLM to reinsert them with context, the regenerated text recovers semantic content that simplification typically strips.
- Core assumption: Named entities serve as proxies for critical information; their absence signals meaningful information loss rather than benign simplification.
- Evidence anchors:
  - [abstract]: "Adding all the missing entities resulted in better text regeneration, which was better than adding the top-ranked entities or words, or random words."
  - [section 5]: "A1 achieves the highest mean cosine similarity of 0.9162... A1 still obtains the best mean ROUGE-1 score of 0.6555."
  - [corpus]: Neighbor paper "LLM-based Text Simplification" reports similar findings on "minimally lossy text simplification" using self-refinement, suggesting convergence around feedback-based approaches.
- Break condition: If entities are misextracted (scispacy errors) or if entity count is extremely high (making reinsertion impractical without compromising readability), the mechanism may over-correct and reduce fluency.

### Mechanism 2
- Claim: Frequency-based word detection captures fine-grained content that entity-centric approaches miss, improving summary-level alignment.
- Mechanism: Words appearing ≥2 times in original but <2 times in simplified (W_missing = {w ∈ W_original | f(w, T_original) ≥ 2 and f(w, T_simplified) < 2}) are flagged as missing. This captures repeated non-entity terms (e.g., "severe," "chronic") that shape meaning but aren't named entities. Reinserting these improves lexical overlap with original summaries.
- Core assumption: Frequency correlates with importance; words repeated in original text carry disproportionate semantic weight.
- Evidence anchors:
  - [section 3.1]: Formal definition of W_missing using frequency thresholds.
  - [section 5]: "A2 is the best at preserving the meaning and the content for the summarized representations... A2 outperforms A1" at summary level.
  - [corpus]: Weak direct evidence; neighbor papers focus on entity-level or syntactic simplification rather than frequency-based word recovery.
- Break condition: If original text contains repetitive but non-informative words (boilerplate, redundancy), this mechanism may prioritize noise.

### Mechanism 3
- Claim: GPT-4's ranking of entity importance does not reliably outperform random selection for selective reinsertion.
- Mechanism: The authors prompted GPT-4 to rank missing entities by importance and selected top-3 (A3). This underperformed comprehensive approaches and showed no clear advantage over random selection (A4), suggesting current LLMs lack reliable intrinsic importance calibration for this task.
- Core assumption: Assumption: GPT-4 can meaningfully rank entity importance based solely on context, without external importance ground truth.
- Evidence anchors:
  - [section 5]: "A3 and A4 are the worst, with a mean cosine similarity of 0.8758 and 0.8632, respectively."
  - [section 6]: "The ranking mechanism of the gpt-4-0613 model does not provide a significant advantage over selecting the same number of entities randomly."
  - [corpus]: No direct neighbor evidence on LLM ranking reliability for entity importance.
- Break condition: If a domain-specific importance scorer or human-labeled importance rankings were introduced, this mechanism could be re-evaluated.

## Foundational Learning

- Concept: **Named Entity Recognition (NER) for Biomedical Text**
  - Why needed here: The approach depends on accurately extracting biomedical entities (diseases, treatments, anatomical terms) from original and simplified texts. Errors here propagate through the entire feedback loop.
  - Quick check question: Can you explain why general-purpose NER (e.g., spaCy default models) might underperform on biomedical text compared to scispacy models trained on biomedical corpora?

- Concept: **Cosine Similarity on Sentence Embeddings**
  - Why needed here: Primary evaluation metric for semantic alignment. Understanding what embeddings capture (semantic vs. lexical similarity) is essential for interpreting results correctly.
  - Quick check question: Why might cosine similarity increase even when specific factual details remain missing from regenerated text?

- Concept: **ROUGE-1 F1 Score**
  - Why needed here: Complements cosine similarity by measuring lexical overlap. Understanding precision-recall tradeoffs helps diagnose whether regeneration adds content (recall) or preserves brevity (precision).
  - Quick check question: If ROUGE-1 recall increases but precision decreases substantially after regeneration, what does this suggest about the regenerated text's quality?

## Architecture Onboarding

- Component map:
  - Original health text -> GPT-4 simplification -> Missing element detection (NER + frequency analysis) -> GPT-4 regeneration -> Evaluation (cosine similarity + ROUGE-1)

- Critical path: Original text -> GPT-4 simplification -> missing entity/word detection -> GPT-4 regeneration -> evaluation. The detection accuracy and prompt quality for regeneration are the two highest-leverage points.

- Design tradeoffs:
  - **A1 vs. A2**: A1 (all entities) optimizes document-level semantic alignment; A2 (all words) optimizes summary-level detail. Hybrid approaches (future work) may capture both.
  - **Comprehensive vs. selective insertion**: Comprehensive (A1/A2) outperforms selective (A3) but risks verbosity. Threshold tuning (future work) could balance this.
  - **Automation vs. human validation**: Fully automated but no human ground truth for importance; relies on proxy metrics.

- Failure signatures:
  - Low cosine similarity improvement despite entity reinsertion -> likely prompt issue (LLM not integrating entities naturally)
  - ROUGE-1 increasing but cosine decreasing -> regenerated text may be adding words without semantic coherence
  - A5 (random k-entities) outperforming A1 -> entity extraction may be capturing noise; review NER output

- First 3 experiments:
  1. **Reproduce the A1 vs. A2 comparison on a small sample (5-10 documents)**: Verify that entity-based reinsertion improves document-level metrics and word-based improves summary-level. Check regeneration quality manually.
  2. **Ablation on detection accuracy**: Manually annotate missing entities in 5 simplified texts and compare to scispacy output. Quantify false positives/negatives in detection.
  3. **Prompt sensitivity test**: Vary the regeneration prompt (Fig. 1) with different phrasing for "maintain simplicity" and measure impact on cosine/ROUGE and qualitative fluency.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Would a hybrid approach combining missing entities (A1) and missing words (A2) outperform individual approaches at both document and summary levels?
- Basis in paper: [explicit] "Our future improvements focus on exploring hybrid approaches that combine the strengths of A1 and A2, such as adding both missing entities and words to better capture broader semantic concepts as well as fine-grained details."
- Why unresolved: A1 excelled at document-level alignment while A2 excelled at summary-level; no combined approach was tested.
- What evidence would resolve it: Comparing A1+A2 hybrid against individual approaches using cosine similarity and ROUGE-1 scores.

### Open Question 2
- Question: Does reinserting missing information compromise text readability and simplicity for the intended lay audience?
- Basis in paper: [inferred] The paper assumes higher similarity to original text indicates better quality, but does not evaluate whether regenerated text remains simple or comprehensible to non-experts.
- Why unresolved: Only automated metrics were used; no human evaluation of readability, comprehension, or perceived difficulty.
- What evidence would resolve it: Human subject evaluation measuring readability scores and comprehension of regenerated simplified text compared to original simplified text.

### Open Question 3
- Question: Why does GPT-4's ranking mechanism fail to identify the most important entities for reinsertion?
- Basis in paper: [explicit] "Current tools can identify these entities, but are not valuable in ranking them."
- Why unresolved: A3 (top-3 ranked entities) underperformed significantly, but the paper does not investigate why the LLM's importance ranking is ineffective.
- What evidence would resolve it: Comparative analysis of GPT-4's entity rankings versus human expert rankings to identify systematic ranking errors.

### Open Question 4
- Question: Can alternative ranking mechanisms or dynamic thresholds optimize entity selection better than the current approaches?
- Basis in paper: [explicit] Future work will "focus on alternative ranking mechanisms to pick the crucial entities and experiment with dynamic thresholds for selecting missing words or entities."
- Why unresolved: Fixed approaches (all entities, all words, top-3) were tested, but adaptive selection strategies were not explored.
- What evidence would resolve it: Testing importance-weighted or context-aware selection methods against the current five approaches.

## Limitations

- The study relies on proxy metrics (cosine similarity and ROUGE-1) rather than direct human comprehension assessment, leaving uncertainty about whether semantic alignment translates to improved reader understanding.
- The approach only tested on rheumatology texts from BMJ, which may not represent the diversity of health information needs across different medical domains.
- The optimal balance between comprehensive reinsertion and maintaining simplicity remains unclear, as comprehensive approaches may reintroduce complexity that undermines the simplification goal.

## Confidence

- **High confidence**: Comprehensive entity reinsertion (A1) improves document-level semantic alignment compared to selective approaches; frequency-based word detection effectively captures summary-level detail
- **Medium confidence**: GPT-4 ranking of entity importance provides no advantage over random selection; the approach generalizes across different health text domains
- **Low confidence**: The regenerated texts maintain appropriate readability and simplicity; proxy metrics accurately reflect improved comprehension for lay readers

## Next Checks

1. **Human evaluation study**: Conduct a small-scale comprehension test with lay readers comparing original, simplified, and regenerated texts to validate whether proxy metric improvements translate to better understanding

2. **Cross-domain testing**: Apply the approach to health texts from different medical specialties (e.g., cardiology, oncology) and evaluate whether detection and regeneration accuracy remains consistent

3. **Readability assessment**: Use established readability formulas (e.g., Flesch-Kincaid) to measure whether comprehensive reinsertion (A1/A2) maintains appropriate reading levels for the target audience