---
ver: rpa2
title: 'Disentangling Granularity: An Implicit Inductive Bias in Factorized VAEs'
arxiv_id: '2505.24684'
source_url: https://arxiv.org/abs/2505.24684
tags:
- disentangling
- latent
- granularity
- parameter
- capacity
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This work uncovers disentangling granularity as an implicit inductive\
  \ bias in factorized VAEs, revealing that fixed granularity in conventional models\
  \ leads to disentangling low-complexity features. By decomposing total correlation,\
  \ the authors propose \u03B2-STCVAE, which allows explicit tuning of granularity\
  \ to improve disentanglement performance."
---

# Disentangling Granularity: An Implicit Inductive Bias in Factorized VAEs

## Quick Facts
- **arXiv ID**: 2505.24684
- **Source URL**: https://arxiv.org/abs/2505.24684
- **Reference count**: 40
- **Primary result**: Œ≤-STCVAE achieves up to 20% higher MIG scores compared to Œ≤-TCVAE by tuning disentangling granularity, validated across 100K+ experiments.

## Executive Summary
This work uncovers disentangling granularity as an implicit inductive bias in factorized VAEs, revealing that fixed granularity in conventional models leads to disentangling low-complexity features. By decomposing total correlation into iteratively refined joint distribution constraints, the authors propose Œ≤-STCVAE, which allows explicit tuning of granularity to improve disentanglement performance. The paper demonstrates a "V"-shaped optimal ELBO trajectory in parameter space, showing that optimal granularity decreases then increases as network capacity grows. Over 100K experiments show Œ≤-STCVAE consistently outperforms Œ≤-TCVAE, achieving up to 20% higher MIG scores on dSprites and cars3D datasets.

## Method Summary
The method builds on Œ≤-TCVAE by replacing the standard total correlation term with TC_joint_ùëèÃÇ(z), where bÃÇ is a grouping factor divisor of latent dimension n. Disentangling granularity g = bÃÇ/m (normalized, m = max divisor) controls the model's preference for disentangling low vs. high-complexity features. The loss function becomes: L = E[log p(x|z)] - I(z;x) - Œ≤¬∑TC_joint_ùëèÃÇ(z) - Œ£ D_KL(q(z_i)||p(z_i)). The iterative decomposition of total correlation allows explicit control over which feature complexities the model disentangles. The authors validate their approach through extensive experiments across multiple datasets, showing that optimal granularity follows a V-shaped trajectory relative to network capacity.

## Key Results
- Œ≤-STCVAE achieves up to 20% higher MIG scores compared to Œ≤-TCVAE on dSprites and cars3D datasets
- A "V"-shaped optimal ELBO trajectory emerges consistently across all datasets when varying capacity and granularity
- Over 100K experiments validate that Œ≤-STCVAE consistently outperforms Œ≤-TCVAE across different hyperparameter settings
- Disentangling granularity enables the model to capture features of varying complexity, addressing limitations of fixed granularity in conventional factorized VAEs

## Why This Works (Mechanism)

### Mechanism 1: Hierarchical Total Correlation Decomposition
The paper decomposes TC(z) = D_KL(q(z)||‚àèq(z_k)) into a sum of mutual information terms within joint distributions (MU_joint) plus a residual joint distribution TC term. By iteratively applying this decomposition, they can selectively remove independence constraints at specific grouping levels, allowing latent variable groups to jointly model higher-complexity features. The decomposition formula TC(z) = Œ£ MU_joint_b(z) + I(z_f1; z_f2) enables explicit control over which feature complexities the model disentangles.

### Mechanism 2: Disentangling Granularity as Implicit Inductive Bias Control
The grouping factor bÃÇ controls the model's preference for disentangling low vs. high-complexity features. Smaller g enforces stronger marginal independence (Œ≤-TCVAE behavior), pushing each latent to capture simple, low-complexity features. Larger g relaxes independence within groups, allowing joint distributions to encode higher-complexity features. This creates a "V"-shaped optimal ELBO trajectory where optimal g decreases then increases as network capacity grows.

### Mechanism 3: Capacity-Granularity Co-optimization via ELBO
Optimal disentanglement requires matching disentangling granularity to network parameter capacity. At low capacity, single latents cannot capture complete features ‚Üí relax constraints (higher g). As capacity increases, single latents become sufficient ‚Üí lower g. At very high capacity, latents learn complex features requiring joint distributions ‚Üí higher g again. This creates the V-shaped optimal ELBO trajectory where ELBO serves as a reliable proxy for disentanglement quality.

## Foundational Learning

- **Concept: Total Correlation (TC)**
  - Why needed here: TC measures statistical dependence among all latent variables. Understanding that TC = 0 means complete independence is essential to grasp why the decomposition enables granularity control.
  - Quick check question: If TC(z) = 0, what does this imply about the relationship between latent variables?

- **Concept: Evidence Lower Bound (ELBO)**
  - Why needed here: The paper uses ELBO as the optimization objective and shows how granularity affects ELBO trajectory. You must understand ELBO's role as a tractable bound on log-likelihood.
  - Quick check question: Why is maximizing ELBO equivalent to minimizing reconstruction error plus KL divergence?

- **Concept: Inductive Bias in Unsupervised Learning**
  - Why needed here: The paper builds on the ICML 2019 result that unsupervised disentanglement is impossible without implicit inductive biases. Disentangling granularity is identified as one such bias.
  - Quick check question: Why can't a model learn disentangled representations from data alone without any assumptions about the data or model structure?

## Architecture Onboarding

- **Component map**: Encoder -> Œº, log œÉ¬≤ parameters -> Latent sampling (reparameterization) -> TC_joint computation (grouped KL) -> Decoder reconstruction -> Loss computation

- **Critical path**: 1) Forward pass: x ‚Üí encoder ‚Üí (Œº, log œÉ¬≤) ‚Üí sample z via reparameterization; 2) Group latents according to bÃÇ: z_grouped = reshape(z, n/bÃÇ, bÃÇ); 3) Compute TC_joint_ùëèÃÇ(z) using grouped joint distributions; 4) Compute reconstruction loss + TC penalty + index-code KL; 5) Backward pass and optimize

- **Design tradeoffs**: Higher Œ≤ increases disentangling pressure; higher g relaxes independence within groups. Must tune jointly. Follow V-shaped guidance ‚Äî low capacity needs higher g, medium capacity needs lower g, very high capacity needs higher g again. bÃÇ must divide n evenly.

- **Failure signatures**: Omniscient latent variable (single latent with near-zero entropy), fuzzy/low-quality samples (underfitting), no disentanglement improvement over Œ≤-TCVAE, high variance in MIG scores.

- **First 3 experiments**: 1) Replicate V-shape on single dataset by fixing Œ≤=5, varying network capacity and granularity, plotting optimal ELBO vs capacity; 2) Compare Œ≤-STCVAE vs Œ≤-TCVAE on MIG by fixing n=8, Œ≤=5, training both models, reporting MIG scores; 3) Ablation on granularity choice by training Œ≤-STCVAE with g=0.33, 0.5, 1.0 on cars3D, inspecting latent traversals qualitatively.

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions in the text provided. However, several areas warrant further investigation based on the limitations discussed, including the theoretical derivation of the V-shaped ELBO trajectory, formal prevention of the omniscient latent phenomenon, and methods to determine optimal granularity a priori without extensive grid search.

## Limitations

- The observed V-shaped optimal ELBO trajectory, while empirically validated across 100K+ experiments, may be dataset-specific and not generalizable to more complex real-world datasets
- The assumption that feature complexity follows a predictable relationship with network capacity and granularity may not hold for datasets with uniformly complex features
- The method relies on extensive grid search to find optimal granularity, requiring significant computational resources and lacking a theoretically grounded method for a priori determination

## Confidence

- **High confidence**: The TC decomposition mechanism (Œ≤-STCVAE formulation) is mathematically sound and correctly implemented
- **Medium confidence**: The V-shaped ELBO trajectory as a general principle across datasets and capacities
- **Medium confidence**: The 20% MIG improvement claim, given that it depends on finding the correct granularity for each capacity

## Next Checks

1. **Replication on single dataset**: Fix Œ≤=5, vary network capacity and granularity on cars3D. Plot optimal ELBO vs. capacity to confirm V-shape pattern.

2. **Ablation study**: Train Œ≤-STCVAE with g=0.33, 0.5, 1.0 on dSprites. Report MIG scores and inspect latent traversals for qualitative disentanglement differences.

3. **Capacity mismatch test**: Intentionally mismatch g and capacity (e.g., high g with low capacity) to observe failure modes like omniscient latents or underfitting.