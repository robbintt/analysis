---
ver: rpa2
title: Auditing Language Model Unlearning via Information Decomposition
arxiv_id: '2601.15111'
source_url: https://arxiv.org/abs/2601.15111
tags:
- information
- unlearning
- forget
- knowledge
- residual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the gap between apparent and actual effectiveness
  of machine unlearning in LLMs, revealing that even when unlearning algorithms achieve
  high forget quality, significant residual information about forgotten data remains
  linearly decodable from model representations. The authors introduce an information-theoretic
  auditing framework based on Partial Information Decomposition (PID), which quantifies
  "unlearned knowledge" (information uniquely removed) and "residual knowledge" (information
  still shared between base and unlearned models).
---

# Auditing Language Model Unlearning via Information Decomposition

## Quick Facts
- arXiv ID: 2601.15111
- Source URL: https://arxiv.org/abs/2601.15111
- Authors: Anmol Goel; Alan Ritter; Iryna Gurevych
- Reference count: 40
- Primary result: Framework quantifies "unlearned knowledge" and "residual knowledge" after unlearning via Partial Information Decomposition (PID), revealing that even successful-looking unlearning leaves linearly decodable membership information.

## Executive Summary
This paper introduces an information-theoretic auditing framework for evaluating machine unlearning in large language models. The core insight is that approximate unlearning algorithms often exhibit "shallow unlearning" - achieving high behavioral forget quality while significant residual information about forgotten data remains encoded in model representations. Using Partial Information Decomposition (PID), the framework quantifies how much information was truly erased versus persists, revealing that residual knowledge strongly correlates with adversarial vulnerability and can power a privacy-enhancing abstention mechanism.

The methodology combines representation extraction from base and unlearned models with a novel RINE estimator that trains agreement-constrained decoders to estimate redundant information. Empirically, the framework demonstrates that gradient-based methods like RMU outperform representation-based approaches in both unlearning efficacy and residual information removal. The audit provides actionable metrics for safer LLM deployment and regulatory compliance by moving beyond output-based metrics to white-box analysis of internal representations.

## Method Summary
The framework decomposes mutual information between forgotten data and model representations into unique (unlearned) and redundant (residual) components using Partial Information Decomposition. Two logistic regression probes are trained on final-layer representations from base and unlearned models to predict binary membership, with an agreement constraint D(f1,f2)→0 enforced via Lagrangian optimization. The RINE estimator computes redundancy I_∩ from the constrained loss, then derives unlearned knowledge I_B^uniq = I(Y;B) - I_∩. The approach is validated on TOFU and MUSE benchmarks across multiple LLM architectures, comparing unlearning algorithms including GA, RMU, NPO, and SimNPO.

## Key Results
- Linear probes achieve 50-90% AUROC across layers 0-14 for membership prediction, demonstrating shallow unlearning across all major algorithms
- Residual knowledge I_∩ strongly correlates with adversarial vulnerability (r=0.60-0.71, p<0.05) and outperforms MIA baselines
- RMU outperforms gradient-based methods in both forget quality and residual information removal
- Risk score abstention improves Forget Quality from 0.72 to 0.83 (+0.11) versus predictive entropy (+0.05)

## Why This Works (Mechanism)

### Mechanism 1: Partial Information Decomposition for Unlearning Auditing
- Claim: Decomposing mutual information into unique and redundant components quantifies how much information was truly erased versus persists after unlearning.
- Mechanism: Given base model representations B, unlearned representations U, and membership label Y, PID decomposes I(Y; B, U) into four terms: unique information in B (I_B^uniq = unlearned knowledge), unique information in U (I_U^uniq), redundant information I_∩ (residual knowledge), and synergistic information. The RINE estimator trains two decoders f1, f2 under an agreement constraint D(f1, f2) → 0, minimizing cross-entropy on Y from both representations.
- Core assumption: Assumption: Information extractable by similar decoders from both representations reflects residual memorization that unlearning failed to remove.
- Evidence anchors:
  - [abstract]: "By comparing model representations before and after unlearning, we decompose the mutual information with the forgotten data into distinct components, formalizing the notions of unlearned and residual knowledge."
  - [Section 5, Definition 5.1-5.2]: Formalizes unlearned knowledge as I_B^uniq and residual knowledge as I_∩.
  - [corpus]: Related work "Verifying Robust Unlearning" (FMR=0.63) similarly probes residual knowledge, suggesting convergent validation of the shallow unlearning problem.
- Break condition: If representations B and U become informationally independent with respect to Y (I_∩ → 0), the audit certifies successful unlearning. If I_B^uniq → 0 while I_∩ remains high, unlearning has failed despite behavioral changes.

### Mechanism 2: Linear Decodability of Membership from Residual Stream
- Claim: Membership information about forgotten data remains linearly decodable from intermediate representations even when output-based metrics suggest successful unlearning.
- Mechanism: Train logistic regression probes h(z) to predict binary membership Y from representations z = f_θ(x) at each layer. Use AUROC to measure probe performance. Fano's inequality bounds the minimum achievable error: P(h(Z)≠Y) ≥ (H(Y) - I(Y;Z) - log 2) / log 2, implying high probe accuracy necessitates non-trivial I(Y;Z).
- Core assumption: Assumption: If a linear probe can extract membership information, then the information is robustly encoded and not merely an artifact of the probe training.
- Evidence anchors:
  - [Section 3, Figure 2]: Shows probe AUROC scores of 50-90% across layers 0-14 for various unlearning algorithms on llama for the TOFU and MUSE unlearning benchmarks, demonstrating linearly decodable residual information.
  - [Section 3]: "Surprisingly, we find evidence of shallow unlearning... across all major unlearning algorithms on llama for the TOFU and MUSE unlearning benchmarks."
  - [corpus]: REMIND (FMR=0.50) similarly finds residual memorization detectable via input loss landscapes, supporting the persistence phenomenon.
- Break condition: If probe AUROC drops to random baseline (~50%) across all layers, membership information is no longer linearly decodable, indicating potential successful unlearning.

### Mechanism 3: Residual Knowledge Predicts Adversarial Vulnerability
- Claim: Residual knowledge (I_∩) strongly correlates with susceptibility to adversarial reconstruction attacks and can power an inference-time abstention mechanism.
- Mechanism: Fit regression models using attack success rates (ASR) from finetuning and orthogonalization attacks to predict residual knowledge. Derive a risk score: RiskScore(x) = 0.5(p1 + p2) × (1 - |p1 - p2|), where p1, p2 are probe probabilities from base and unlearned models. High risk → abstain.
- Core assumption: Assumption: The correlation between residual knowledge and attack success is causal, not merely correlational—reducing residual knowledge would reduce vulnerability.
- Evidence anchors:
  - [Section 6.1, Table 2]: Correlation coefficients of 0.60-0.71 (p<0.01-0.05) between residual knowledge and attack success, outperforming MIA baselines (0.38-0.51).
  - [Section 6.2, Table 5]: Risk score abstention improves Forget Quality from 0.72 to 0.83 (+0.11) versus predictive entropy (+0.05), with comparable utility cost.
  - [corpus]: Corpus evidence for the abstention mechanism is limited; no directly comparable work found.
- Break condition: If residual knowledge → 0 but attacks still succeed, the correlation is spurious or attacks exploit non-redundant pathways. If risk score triggers excessive false positives on retain samples, the agreement term is miscalibrated.

## Foundational Learning

- Concept: **Mutual Information I(X;Y)**
  - Why needed here: PID extends mutual information to decompose how multiple sources jointly inform a target. Without grasping I(X;Y) = H(Y) - H(Y|X), the redundancy/unique/synergy decomposition is opaque.
  - Quick check question: If I(X;Y) = 0.5 bits and H(Y) = 1 bit, what is H(Y|X)?

- Concept: **Linear Probing**
  - Why needed here: The paper uses linear probes both to demonstrate shallow unlearning and as decoders in RINE. Understanding why linear separability implies robust encoding is essential.
  - Quick check question: Why might a linear probe be preferred over a deep MLP for auditing representations, even if the latter achieves higher accuracy?

- Concept: **Exact vs. Approximate Unlearning**
  - Why needed here: The paper critiques approximate methods for "shallow unlearning." Understanding that exact unlearning requires retraining from scratch clarifies why the audit problem exists at all.
  - Quick check question: Why is exact unlearning infeasible for LLMs, and what tradeoff do approximate methods make?

## Architecture Onboarding

- Component map:
  - Base Model (f_θb) -> Representation Extractor -> Base Representations B
  - Unlearned Model (f_θu) -> Representation Extractor -> Unlearned Representations U
  - (B, Y) -> Membership Decoders (f1, f2) with agreement constraint
  - (U, Y) -> Membership Decoders (f1, f2) with agreement constraint
  - RINE Optimizer -> PID Calculator -> I_∩ and I_B^uniq
  - Risk Scorer -> Abstention Decision

- Critical path:
  1. Receive unlearning request → apply unlearning algorithm (RMU, NPO, etc.) → obtain f_θu
  2. Extract representations B, U from forget set samples at target layer
  3. Train f1, f2 on (B, Y), (U, Y) with Lagrangian optimization enforcing D(f1,f2)→0
  4. Compute I_∩ = H(Y) - L_V, then I_B^uniq = I(Y;B) - I_∩
  5. Report unlearned knowledge (high I_B^uniq = good) and residual knowledge (low I_∩ = good)
  6. (Optional) Deploy risk scorer for inference-time abstention

- Design tradeoffs:
  - **Layer selection**: Earlier layers may encode more syntactic information; later layers more semantic. Paper uses final layer—consider layer-wise analysis for localization (acknowledged as future work).
  - **Decoder family V**: Linear probes are interpretable and fast but may underestimate true redundancy. Neural decoders are more expressive but risk overfitting.
  - **β parameter**: Controls constraint strength. Low β allows decoder divergence (underestimates redundancy); high β forces agreement but may prevent convergence.
  - **White-box vs. black-box**: Framework requires representation access. Paper argues this is necessary for meaningful audit, but limits applicability to API-only deployments.

- Failure signatures:
  - **I_∩ near H(Y)**: Decoders converged to predicting the majority class; agreement achieved trivially, audit fails.
  - **I_B^uniq ≈ 0 and I_∩ ≈ 0**: Both models contain no decodable membership information—either perfect unlearning or probe training failed. Check probe AUROC on base model.
  - **Risk score high on retain samples**: Threshold τ is too low or probes overfit to forget set characteristics not specific to membership.
  - **High I_∩ but low attack correlation**: Residual knowledge may be non-extractable by tested attacks; consider alternative attack vectors before certifying safety.

- First 3 experiments:
  1. **Probe baseline**: Train independent logistic probes on base and unlearned representations for membership prediction. Report AUROC per layer to confirm shallow unlearning exists in your setup.
  2. **RINE redundancy estimation**: Implement the constrained optimization (Equation 12) with β sweep [0.1, 1.0, 10.0]. Compare I_∩ values across unlearning algorithms (GA, RMU, NPO) on TOFU forget/retain split.
  3. **Correlation validation**: For each (model, algorithm) pair, compute residual knowledge I_∩ and attack success rate under finetuning attack. Fit linear regression; confirm r > 0.5 before relying on I_∩ as vulnerability proxy.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the PID-based auditing framework be successfully extended to quantify unlearning in pre-training or instruction-tuning phases, rather than just fine-tuning?
- **Basis in paper:** [explicit] The authors state their setting is restricted to unlearning finetuned LLMs, but add that "the core principles remain valid for... unlearning from instruction-tuned models, or unlearning from pre training data."
- **Why unresolved:** The empirical validation in the study is limited to fine-tuning scenarios; pre-training involves vastly different data distributions and model states which may affect the estimation of residual knowledge.
- **What evidence would resolve it:** Successful application of the RINE estimator and residual knowledge metrics to models where specific pre-training data subsets have been targeted for removal.

### Open Question 2
- **Question:** How does the quantification of unlearned versus residual knowledge vary when performing a fine-grained localization analysis (layer-by-layer or head-by-head)?
- **Basis in paper:** [explicit] The authors note that a "localization study would be out of scope of our current work" and that their framework "lays the essential groundwork for future work to perform such fine-grained localization."
- **Why unresolved:** The current study aggregates results at the representation level (specifically the final hidden layer), leaving the internal distribution of residual information across the model architecture unmapped.
- **What evidence would resolve it:** Layer-wise or head-wise decomposition of mutual information to identify specific architectural components that retain the highest residual knowledge post-unlearning.

### Open Question 3
- **Question:** Can this auditing framework effectively detect the persistence of specific sensitive attributes (e.g., PII) rather than just binary membership status?
- **Basis in paper:** [explicit] The authors list as a limitation: "We investigate the residual information with respect to data membership, future works should look into specific sensitive attritbutes relevant for deployers."
- **Why unresolved:** The current methodology relies on binary membership labels ($Y \in \{0,1\}$); it is unconfirmed if the probes can isolate specific sensitive concepts amidst the high-dimensional noise of general memorization.
- **What evidence would resolve it:** Adapting the target variable $Y$ to represent specific attributes and testing if the Residual Knowledge score correlates with the extractability of those specific attributes.

## Limitations
- Framework requires white-box access to intermediate model representations, limiting applicability to closed APIs or deployed systems
- RINE estimator assumes agreement-constrained decoders accurately capture residual information, untested against alternative decompositions
- Residual knowledge estimates depend on probe family choice; deeper architectures might extract more information than linear probes

## Confidence
- **High confidence**: Linear decodability of membership information from residual streams (supported by probe AUROC scores 50-90% across layers), correlation between residual knowledge and adversarial vulnerability (r=0.60-0.71, p<0.05)
- **Medium confidence**: RINE estimator's ability to accurately quantify residual knowledge through agreement-constrained probes, and the causal relationship between reducing residual knowledge and improving unlearning robustness
- **Low confidence**: Generalizability of the abstention mechanism to diverse real-world scenarios, and whether the current probe family V captures all extractable residual information

## Next Checks
1. **Probe Architecture Sweep**: Compare residual knowledge estimates I_∩ using linear probes versus small MLPs (1-2 hidden layers) on the same representation data to establish sensitivity to probe family choice
2. **Cross-Dataset Vulnerability**: Apply the risk score abstention mechanism to an out-of-distribution dataset (e.g., different domain or sequence length) and measure degradation in both false positive rates and actual attack resistance
3. **Information-Theoretic Baseline**: Implement an alternative PID estimator (e.g., IB-PI method) on the same (B, U, Y) triples to verify that RINE produces consistent I_∩ estimates and is not an artifact of the specific optimization approach