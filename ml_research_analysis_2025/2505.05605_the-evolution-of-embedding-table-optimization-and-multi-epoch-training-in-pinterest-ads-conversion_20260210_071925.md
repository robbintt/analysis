---
ver: rpa2
title: The Evolution of Embedding Table Optimization and Multi-Epoch Training in Pinterest
  Ads Conversion
arxiv_id: '2505.05605'
source_url: https://arxiv.org/abs/2505.05605
tags:
- embedding
- training
- click
- overfitting
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses two key challenges in training large embedding
  tables for Pinterest Ads Conversion models: slow convergence due to gradient sparsity
  and multi-epoch overfitting caused by label sparsity. To tackle these issues, the
  authors introduce a Sparse Optimizer that applies a higher layer-specific learning
  rate to embedding tables, significantly speeding up convergence.'
---

# The Evolution of Embedding Table Optimization and Multi-Epoch Training in Pinterest Ads Conversion

## Quick Facts
- **arXiv ID**: 2505.05605
- **Source URL**: https://arxiv.org/abs/2505.05605
- **Reference count**: 40
- **Primary result**: Sparse Optimizer improves convergence; FAL reduces multi-epoch overfitting

## Executive Summary
This paper addresses two key challenges in training large embedding tables for Pinterest Ads Conversion models: slow convergence due to gradient sparsity and multi-epoch overfitting caused by label sparsity. The authors introduce a Sparse Optimizer that applies a higher layer-specific learning rate to embedding tables, significantly speeding up convergence. Additionally, they propose a Frequency-Adaptive Learning Rate (FAL) method that selectively reduces the learning rate for infrequent rows to mitigate overfitting while preserving performance for frequently accessed rows. Offline experiments on a large-scale industrial dataset demonstrate that the Sparse Optimizer improves convergence and achieves better performance compared to the production model. FAL reduces multi-epoch overfitting effectively, outperforming embedding re-initialization (MEDA) in most cases. However, the performance gains from both methods diminish after several days of continual training, suggesting that overfitting may not be a critical issue if fresh data is available. The Sparse Optimizer has already been deployed in production models.

## Method Summary
The authors tackle slow convergence and multi-epoch overfitting in Pinterest's ads conversion models by introducing two novel optimization techniques. First, they implement a Sparse Optimizer that applies a 50× higher learning rate specifically to embedding tables while keeping dense layer learning rates unchanged. This addresses the gradient sparsity problem where only a small subset of embedding rows receive updates per batch. Second, they propose Frequency-Adaptive Learning Rate (FAL), which scales each embedding row's learning rate by the logarithm of its access frequency, slowing down updates for infrequent rows to prevent overfitting. Both methods are evaluated on a multi-task model with six objectives (click, good click, add-to-cart | click, add-to-cart | view, checkout | click, checkout | view) using a 110-day shuffled dataset. The Sparse Optimizer achieves faster convergence with at least 0.41% lower training loss and 0.10% cumulative AUC gain compared to the baseline. FAL reduces multi-epoch overfitting on most objectives, outperforming MEDA in most cases, though it underperforms on the sparsest objective.

## Key Results
- Sparse Optimizer achieves at least 0.41% lower training loss and 0.10% cumulative AUC gain compared to production model
- FAL outperforms baseline methods on most objectives with cumulative AUC gains of 0.07–0.20%
- Multi-epoch overfitting severity correlates with label density, with sparse objectives showing larger performance degradation
- Performance gains from both methods diminish after several days of continual training on fresh data

## Why This Works (Mechanism)

### Mechanism 1
- Claim: A higher layer-specific learning rate for embedding tables accelerates convergence without harming final performance.
- Mechanism: Only a small subset of embedding rows receives gradient updates per batch due to sparse feature activations. By multiplying the base learning rate for all embedding parameters (while keeping dense-layer rates fixed), each row makes larger updates during its fewer update opportunities, reaching effective representations faster.
- Core assumption: Embedding rows trained on similar data distributions benefit from faster convergence; large learning rates do not destabilize training when dense layers remain at lower rates.
- Evidence anchors:
  - [abstract] "our Sparse Optimizer speeds up convergence"
  - [section 3.1] "we assign a higher layer-specific learning rate for embedding tables on top of our Adam optimizer"
  - [section 5.1] "Sparse Optimizer achieves at least 0.41% lower training loss" at all points after 10,000 iterations; 0.10% cumulative AUC gain vs. retrain baseline
  - [corpus] Weak direct comparison—neighbor papers focus on embedding architectures and graph representations, not optimizer tuning.
- Break condition: If training becomes unstable (loss divergence) or online AUC degrades, the multiplier is too aggressive; reduce to 10–20× instead of 50×.

### Mechanism 2
- Claim: Scaling down learning rates for low-frequency embedding rows reduces multi-epoch overfitting while preserving performance on frequent rows.
- Mechanism: Multi-epoch overfitting is driven by infrequent rows overfitting to limited positive examples, then propagating bias downstream. FAL scales each row's learning rate by log(frequency + 1) / max_log_frequency, so rare rows update slowly across epochs, limiting memorization.
- Core assumption: The frequency distribution observed during training approximates inference-time frequency; log-scaling prevents highly frequent rows from dominating the learning-rate distribution.
- Evidence anchors:
  - [abstract] FAL "outperforms baseline methods on most objectives, with cumulative AUC gains of 0.07–0.20%"
  - [section 3.2] Formula and Algorithm 1 detail the log-scaled row-wise learning rate
  - [section 5.2.1] FAL reduces loss jumps at epoch boundaries for sparse objectives (e.g., p(add-to-cart | click) from +1.19% to +0.23%)
  - [corpus] Neighbor paper "Adaptive Regularization for Large-Scale Sparse Feature Embedding Models" addresses one-epoch overfitting via regularization—supports frequency-aware approaches but not FAL directly.
- Break condition: If the sparsest objectives (e.g., p(checkout | click)) still show overfitting or underperform MEDA, FAL alone may be insufficient; consider combining with re-initialization.

### Mechanism 3
- Claim: Multi-epoch overfitting severity correlates with label density—sparser objectives overfit more.
- Mechanism: Objectives with fewer positive labels (e.g., checkout | click) have smaller effective training sets, amplifying memorization risk for rare embedding rows during subsequent epochs. Denser objectives (e.g., click) exhibit minimal overfitting.
- Core assumption: Label density is the primary driver of overfitting variance across objectives; data volume increases can also reduce severity.
- Evidence anchors:
  - [abstract] "multi-epoch overfitting varies in severity between different objectives in a multi-task model depending on label sparsity"
  - [section 5.2] p(add-to-cart | click) shows +1.19% loss jump vs. p(add-to-cart | view, no click) with -0.37%; Table 2 shows relative label densities
  - [section 5.3] Larger Aug–Dec dataset (25% larger) shows less overfitting than May–Aug dataset
  - [corpus] "Taming the One-Epoch Phenomenon" confirms one-epoch overfitting tied to long-tail distributions but does not compare multi-task objectives.
- Break condition: If adding more training data or increasing label density is infeasible, mitigation techniques (FAL, MEDA) are required only for sparse objectives.

## Foundational Learning

- Concept: Gradient sparsity in embedding tables
  - Why needed here: Understanding why embedding rows converge slowly (few updates per batch) motivates the Sparse Optimizer design.
  - Quick check question: In a batch of 2,000 examples with 60 categorical features, what fraction of unique embedding rows receive updates?

- Concept: Multi-epoch overfitting vs. one-epoch phenomenon
  - Why needed here: The paper positions FAL and MEDA as solutions to performance drops at epoch boundaries; distinguishing this from general overfitting clarifies when interventions apply.
  - Quick check question: If validation loss increases sharply at epoch 2 but not during epoch 1, is this classical overfitting or multi-epoch overfitting?

- Concept: Log-scaling for frequency normalization
  - Why needed here: FAL uses log(frequency + 1) to prevent highly frequent rows from receiving disproportionately high learning rates.
  - Quick check question: Why would linear scaling of learning rates by frequency harm rare-row learning?

## Architecture Onboarding

- Component map: Categorical feature → hash → embedding lookup → FAL scaling (row-wise LR) → sparse optimizer update (50× layer LR) → ensemble backbone → task-specific MLP → sigmoid output

- Critical path: Categorical feature → hash → embedding lookup → FAL scaling (row-wise LR) → sparse optimizer update (50× layer LR) → ensemble backbone → task-specific MLP → sigmoid output

- Design tradeoffs:
  - Sparse Optimizer: Faster convergence but requires tuning multiplier (50× worked here; may differ elsewhere)
  - FAL: Reduces overfitting on most objectives but underperforms MEDA on sparsest (p(checkout | click)); adds ~3% memory overhead for frequency tensors
  - MEDA: Better on sparsest objective but treats all rows equally, losing frequency-awareness; may complicate mid-epoch early stopping

- Failure signatures:
  - Sparse Optimizer too aggressive: Training loss divergence or oscillation; reduce multiplier
  - FAL ineffective: Sparsest objectives still show epoch-boundary loss spikes; combine with MEDA or increase data volume
  - Multi-epoch overfitting mild: Gains from FAL/MEDA vanish after continual training on fresh data—intervention may be unnecessary

- First 3 experiments:
  1. Ablate Sparse Optimizer multiplier: Compare 10×, 25×, 50× embedding LR on training loss convergence and cumulative AUC over 10 days of continual training
  2. Compare FAL vs. MEDA per objective: Measure epoch-boundary loss jumps and end-of-epoch test loss for each of the six objectives, with emphasis on sparsest (p(checkout | click))
  3. Continual-training convergence test: After 2-epoch batch training, run 10+ days of daily continual training to verify whether FAL/MEDA gains persist or baseline catches up

## Open Questions the Paper Calls Out

- **Open Question 1**: Can a frequency-aware extension of MEDA (embedding re-initialization) that reduces emphasis on frequent rows outperform both standalone FAL and MEDA?
  - Basis in paper: [explicit] "We believe that a frequency-aware extension of MEDA or regularization, which reduces emphasis on frequent rows may achieve the best of both worlds."
  - Why unresolved: FAL underperforms MEDA on the sparsest objective (p(checkout|click)), suggesting current frequency-adaptive approaches have limitations for extremely sparse labels.
  - What evidence would resolve it: An experiment combining MEDA with frequency-weighted re-initialization, showing improved AUC over both baselines across all label densities.

- **Open Question 2**: Can combining Sparse Optimizer (higher learning rates) with FAL (frequency-adaptive reduction) simultaneously optimize convergence for frequent rows while preventing overfitting of infrequent rows?
  - Basis in paper: [explicit] "We believe that a combination of our Sparse Optimizer... and FAL, which punishes convergence speed for especially low-frequency IDs, may be a promising direction forward."
  - Why unresolved: The paper evaluates Sparse Optimizer and FAL separately but never jointly; potential interactions between accelerated convergence and adaptive regularization remain unexplored.
  - What evidence would resolve it: A/B tests comparing joint Sparse Optimizer + FAL against individual methods on cumulative AUC over multi-epoch training.

- **Open Question 3**: Can layer-specific learning rates improve convergence for non-embedding components (e.g., Transformer encoders, MLPs) that receive fewer informative updates due to positive label sparsity in multi-task learning?
  - Basis in paper: [explicit] "We believe that extending layer-specific learning rates can extend beyond embedding table training... certain layers may receive fewer informative parameter updates due to positive label sparsity."
  - Why unresolved: Sparse Optimizer was only applied to embedding tables; other modules with gradient sparsity were not tested.
  - What evidence would resolve it: Offline experiments applying per-module learning rates to Transformer and MLP components, measuring convergence speed and final AUC.

## Limitations

- Performance gains from both methods diminish after several days of continual training, suggesting overfitting may not be critical if fresh data is available
- FAL underperforms MEDA on the sparsest objective (p(checkout | click)), indicating neither method fully solves multi-epoch overfitting across all tasks
- The paper lacks online A/B test results for the proposed methods, particularly Sparse Optimizer and FAL

## Confidence

- Sparse Optimizer convergence claims: **High** (clear offline metrics, deployed in production)
- FAL overfitting mitigation claims: **Medium** (effective on most objectives but fails on sparsest)
- Multi-epoch overfitting severity claims: **High** (consistent across datasets and objectives)
- Production impact claims: **Medium** (Sparse Optimizer deployed but no online metrics reported)

## Next Checks

1. **Online A/B test**: Deploy Sparse Optimizer and FAL in production A/B test to measure actual CTR/CTR improvement vs offline gains
2. **Long-term continual training**: Track performance over 30+ days of daily continual training to verify if convergence advantages persist
3. **Hybrid method evaluation**: Test combined FAL + MEDA approach on sparsest objective to determine if hybrid outperforms either method alone