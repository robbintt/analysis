---
ver: rpa2
title: 'MulFeRL: Enhancing Reinforcement Learning with Verbal Feedback in a Multi-turn
  Loop'
arxiv_id: '2601.22900'
source_url: https://arxiv.org/abs/2601.22900
tags:
- feedback
- mulferl
- training
- learning
- prompt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the problem of sparse and uninformative scalar
  rewards in Reinforcement Learning with Verifiable Rewards (RLVR), particularly on
  failed samples where feedback merely indicates failure without explaining why. The
  core method, MulFeRL, introduces a multi-turn feedback-guided RL framework that
  uses verbal feedback to guide regeneration on failed samples, converting feedback
  into stable learning signals.
---

# MulFeRL: Enhancing Reinforcement Learning with Verbal Feedback in a Multi-turn Loop

## Quick Facts
- **arXiv ID:** 2601.22900
- **Source URL:** https://arxiv.org/abs/2601.22900
- **Reference count:** 40
- **Primary result:** MulFeRL improves RLVR sample efficiency by converting failed samples into learnable signals via multi-turn verbal feedback, outperforming baselines on 8 math benchmarks.

## Executive Summary
This paper addresses the problem of sparse and uninformative scalar rewards in Reinforcement Learning with Verifiable Rewards (RLVR), particularly on failed samples where feedback merely indicates failure without explaining why. The core method, MulFeRL, introduces a multi-turn feedback-guided RL framework that uses verbal feedback to guide regeneration on failed samples, converting feedback into stable learning signals. It employs dynamic multi-turn regeneration, two complementary learning signals (within-turn GRPO and cross-turn DPO), and structured feedback injection. Trained on sampled OpenR1-Math, MulFeRL outperforms supervised fine-tuning and RLVR baselines in-domain and generalizes well out-of-domain.

## Method Summary
MulFeRL is a multi-turn RL framework that triggers verbal feedback only on failed samples (ALLFAIL groups) to guide regeneration in subsequent turns. The method uses group relative policy optimization (GRPO) within each turn when there's mixed success/failure, and direct preference optimization (DPO) across turns when regeneration produces uniformly correct responses. Feedback is structured injected via fixed `<feedback>` tags within reasoning tokens, and feedback tokens are masked from loss. The framework uses group size K=8, max 2 regeneration turns, and trains on 4k sampled instances from OpenR1-Math-220k using backbones Qwen2.5-7B-Base or Qwen3-4B-Inst.

## Key Results
- MulFeRL achieves 3-8% absolute improvements over supervised fine-tuning and RLVR baselines on in-domain benchmarks (AMC23, AIME24, OlympiadBench, MATH-500, Minerva-Math)
- The method generalizes to out-of-domain benchmarks (MMLU-Pro, GPQA-Diamond, TheoremQA) with competitive performance
- Ablation studies show structured feedback injection improves performance by ~2-3% compared to prompt-appending approaches

## Why This Works (Mechanism)

### Mechanism 1: Feedback-Triggered Regeneration on Failed Samples
Converting failed samples from zero-signal to learnable signal via verbal feedback improves sample efficiency. When all K rollouts fail, verbal feedback identifies root errors and provides repair suggestions; the model then regenerates under this conditioning. The reward improvement from turn t to turn t+1 creates a measurable learning signal where none existed. Core assumption: The feedback simulator produces actionable corrections that reliably shift the policy toward correct solutions.

### Mechanism 2: Complementary GRPO (Within-Turn) and DPO (Cross-Turn) Objectives
Using two complementary optimization objectives prevents signal collapse when regenerated groups become uniformly correct. GRPO exploits within-group contrast when MIXED (some success, some failure). When regeneration produces ALLPOS, GRPO advantages collapse, so cross-turn DPO contrasts the current turn's responses against the previous turn's failed responses under the same feedback-conditioned context. Core assumption: Cross-turn preference pairs capture meaningful improvement direction rather than noise.

### Mechanism 3: Structured Feedback Injection via Fixed Slots
Injecting feedback into a dedicated structural slot improves conditioning and feedback utilization compared to plain prompting. Feedback is inserted into a fixed `<feedback>` tag within `<thinking>`. External feedback tokens are masked from loss. This ensures the model's reasoning always conditions on feedback at a predictable location, tighter integration than prompt-appending. Core assumption: Structural injection improves feedback grounding compared to free-form prompting.

## Foundational Learning

- **GRPO (Group Relative Policy Optimization)**: MulFeRL builds on GRPO's within-group advantage computation; understanding baseline GRPO is prerequisite. *Quick check:* Can you explain how GRPO computes advantages when some rollouts succeed and others fail?
- **DPO (Direct Preference Optimization)**: Cross-turn DPO is the secondary learning signal when GRPO collapses; understanding log-ratio preferences is essential. *Quick check:* How does DPO learn preferences without an explicit reward model?
- **Multi-turn RL rollouts**: MulFeRL's core loop involves turn-based state transitions and cross-turn comparisons. *Quick check:* What distinguishes on-policy multi-turn RL from single-turn RLVR?

## Architecture Onboarding

- **Component map**: Verifier V -> Feedback simulator ψ -> Policy π_θ -> Reference π_ref -> Turn controller
- **Critical path**: 1. Sample K rollouts for prompt x 2. Verify rewards → determine group state (ALLFAIL / MIXED / ALLPOS) 3. If ALLFAIL: query ψ → aggregate feedback → regenerate 4. Apply GRPO (MIXED) or DPO (cross-turn ALLPOS) or continue loop 5. Mask injected feedback tokens from loss
- **Design tradeoffs**: Higher training cost vs. denser learning signal (Fig 4a: ~35 GPU-hours vs. ~22 for GRPO), stricter output format vs. improved feedback grounding, turn budget vs. diminishing returns (Fig 5: gains saturate after ~2-3 turns)
- **Failure signatures**: ALLFAIL at max turns: No learning signal; sample skipped (L(x)=0), ALLPOS at turn 0: No GRPO contrast, no DPO baseline; skipped, Low feedback quality: Regeneration fails to improve, DPO pairs encode noise
- **First 3 experiments**: 1. Baseline comparison: Run GRPO vs. MulFeRL on a small math subset (500 samples) with turn budget 2; compare convergence speed and final accuracy 2. Simulator ablation: Train with self-feedback (backbone as ψ) vs. GPT-4o; measure gap to quantify simulator quality impact 3. Component ablation: Disable DPO, then disable structured injection; isolate each mechanism's contribution per Table 2

## Open Questions the Paper Calls Out
- How does MulFeRL performance degrade when the feedback simulator provides noisy, biased, or inconsistent feedback, and can specific filtering or weighting mechanisms mitigate this?
- Can MulFeRL be effectively extended to partially verifiable or open-ended domains where strict binary outcome rewards are unavailable?
- Does adaptive, confidence-based triggering of regeneration preserve performance while significantly reducing the computational overhead compared to the current fixed-turn budget?
- Do more sophisticated pairing strategies for the cross-turn DPO loss (e.g., best-of-N vs. worst-of-N) yield better learning signals than the current index-matching approach?

## Limitations
- The feedback simulator's quality is critical to MulFeRL's success, yet the paper doesn't quantify simulator error rates or show ablation studies with degraded feedback quality
- The structured feedback injection mechanism shows modest gains (2-3%) in ablation studies, but the evaluation doesn't test alternative injection strategies to confirm this is optimal
- Claims about "generalizing well out-of-domain" rest on limited OOD benchmarks (3 tests) without showing performance on non-mathematical reasoning tasks

## Confidence
- **High confidence**: Core architectural design (feedback-triggered regeneration, dual-objective optimization) is clearly specified and logically sound
- **Medium confidence**: Performance claims are supported by 8 benchmarks with 5 independent runs each, providing reasonable statistical robustness
- **Low confidence**: Claims about feedback quality requirements and simulator independence are largely untested

## Next Checks
1. **Simulator Quality Stress Test**: Train MulFeRL with progressively degraded feedback quality (controlled noise injection in Issue+Fix steps) to identify the minimum feedback quality threshold for effective learning
2. **Cross-Domain Generalization**: Evaluate MulFeRL on non-mathematical reasoning tasks (e.g., commonsense reasoning, code generation) to test whether the feedback mechanism generalizes beyond the mathematical domain where it was developed
3. **Cost-Benefit Analysis**: Conduct head-to-head runtime comparisons showing wall-clock time and GPU-hours per training epoch between MulFeRL and single-turn RLVR baselines, correlating with performance gains to determine if the improvement justifies the computational overhead