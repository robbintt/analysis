---
ver: rpa2
title: 'Integral Transformer: Denoising Attention, Not Too Much Not Too Little'
arxiv_id: '2508.18387'
source_url: https://arxiv.org/abs/2508.18387
tags:
- attention
- intg
- transformer
- tokens
- vanilla
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the Integral Transformer, a novel self-attention
  mechanism that denoises attention by integrating signals sampled from the distribution
  of attention logits. Unlike recent methods that introduce negative attention scores
  (Cog Attention, Differential Transformer), which risk discarding useful information,
  the Integral Transformer mitigates noise while preserving contributions of special
  tokens critical for model performance.
---

# Integral Transformer: Denoising Attention, Not Too Much Not Too Little

## Quick Facts
- arXiv ID: 2508.18387
- Source URL: https://arxiv.org/abs/2508.18387
- Authors: Ivan Kobyzev; Abbas Ghaddar; Dingtao Hu; Boxing Chen
- Reference count: 30
- Primary result: Integral Transformer outperforms vanilla, Cog, and Differential attention on 8 reasoning/knowledge benchmarks

## Executive Summary
This paper introduces the Integral Transformer, a novel self-attention mechanism that denoises attention by integrating signals sampled from the distribution of attention logits. Unlike recent methods that introduce negative attention scores (Cog Attention, Differential Transformer), which risk discarding useful information, the Integral Transformer mitigates noise while preserving contributions of special tokens critical for model performance. Comprehensive pretraining experiments from scratch demonstrate that the Integral Transformer outperforms vanilla, Cog, and Differential attention variants on 8 well-established knowledge and reasoning language benchmarks. The analysis reveals that employing vanilla self-attention in lower Transformer layers enhances performance, and the Integral Transformer effectively balances attention distributions and reduces rank collapse in upper layers.

## Method Summary
The Integral Transformer replaces standard multi-head self-attention with a denoising variant that averages S independent query-key projections before applying softmax. For each layer, S sets of W^Q and W^K matrices are learned, each producing a logit matrix Q_sK_s^T. These are averaged and passed through a single softmax, with the head dimension effectively divided by S. The key innovation is applying this Integral attention only to the top 50% of Transformer layers, while using vanilla attention in the bottom layers. This hierarchical approach balances denoising benefits with the need for feature extraction in early layers.

## Key Results
- Integral Transformer outperforms vanilla, Cog, and Differential attention on 8 reasoning/knowledge benchmarks
- Using vanilla self-attention in lower Transformer layers enhances overall performance
- Integral attention effectively balances attention distributions and reduces rank collapse in upper layers
- Pretraining from scratch on Cosmopedia v2 and FineWeb-Edu validates the approach across 125M and 1.2B parameter scales

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Averaging multiple logit signals before softmax reduces zero-mean noise fluctuations in attention.
- Mechanism: Treats logits Z = QKᵀ as signals from a latent distribution with noise as zero-mean fluctuations. By averaging S independent samples (1/S ∑ QₛKₛᵀ), noise variance is reduced before the softmax nonlinearity, inspired by spatial antenna diversity.
- Core assumption: Noise in attention logits manifests as zero-mean fluctuations that are reduced by averaging independent samples.
- Evidence anchors:
  - [abstract] "denoises attention by integrating signals sampled from the distribution of logits"
  - [Section 3.2] "we reinterpret noise as zero-mean fluctuations in the logit signals. Then, the natural way to denoise the attention map is to integrate it"
  - [corpus] Weak/missing: Related papers don't directly validate this specific denoising approach.
- Break condition: If noise is not zero-mean or samples are highly correlated, averaging may smooth important features.

### Mechanism 2
- Claim: Preserving attention to special tokens improves performance by maintaining their role as "attention sinks."
- Mechanism: Unlike Differential (subtraction) or Cog (signed softmax), averaging logits before softmax is more outlier-resistant, preventing aggressive suppression of special tokens while still reducing noise.
- Core assumption: Special tokens serve a functional role that must be preserved.
- Evidence anchors:
  - [abstract] "mitigates noise while preserving the contributions of special tokens"
  - [Section 3.1] "dropping such tokens... detrimentally affects LLM performance"; "50% of COG and 41% of DIFF attention weights to [BOS] are negative"
  - [corpus] "What are you sinking?" suggests attention sinks are a fundamental geometric property.
- Break condition: If special token importance is task-specific, preserving their attention may not generalize.

### Mechanism 3
- Claim: Applying Integral attention only to upper Transformer layers improves performance.
- Mechanism: Lower layers focus on local patterns and may not benefit from denoising. Upper layers, handling global integration, benefit from balanced attention and reduced rank collapse.
- Core assumption: Attention noise differs qualitatively across layers.
- Evidence anchors:
  - [abstract] "employing vanilla self-attention in lower Transformer layers enhances performance"
  - [Section 5.3] "INTG reduces rank collapse in Transformer upper layers"
  - [Section 4.3] "applying the INTG Transformer to the bottom 50% of layers leads to significant underperformance"
  - [corpus] Weak/missing: No direct evidence in corpus for this hierarchical strategy.
- Break condition: Effectiveness may vary with model depth or domain-specific noise patterns.

## Foundational Learning

- **Concept: Softmax Attention and Attention Noise**
  - Why needed here: The paper identifies that softmax attention disproportionately weights uninformative tokens (special tokens, punctuation) as "noise."
  - Quick check question: Why might a model attend heavily to a [BOS] token, and when is this beneficial versus detrimental?

- **Concept: Differential and Cog Attention**
  - Why needed here: These are the baselines the Integral Transformer is compared against. They address noise by allowing negative attention scores.
  - Quick check question: What is the primary risk of allowing negative attention weights, as identified in the paper?

- **Concept: Rank Collapse in Transformers**
  - Why needed here: The paper analyzes rank collapse in attention matrices and claims the Integral Transformer mitigates it more effectively.
  - Quick check question: How does rank collapse in attention matrices relate to overall model performance?

## Architecture Onboarding

- **Component map**: Input X -> S independent linear projections (W^Q_s, W^K_s) -> S logit matrices (Q_s*K_s^T) -> Average of logits -> Softmax -> Value aggregation. Apply this only to layers in the top half of the model.

- **Critical path**: `Input X` → `S independent linear projections` (W^Q_s, W^K_s) → `S logit matrices` (Q_s*K_s^T) → `Average of logits` → `Softmax` → `Value aggregation`. Apply this only to layers in the top half of the model.

- **Design tradeoffs**: More signals (higher S) improve denoising but reduce per-signal dimensionality. Using too many heads or signals (e.g., 16 heads, 8 signals in a 125M model) leads to very small effective head sizes (e.g., d_h=6) and performance degradation.

- **Failure signatures**: Applying Integral attention to lower layers causes underperformance compared to vanilla. Using too many signals with a fixed head count degrades performance. Aggressive denoising (all layers) underperforms partial (top 50%) application.

- **First 3 experiments**:
  1. Ablation on layer placement: Compare top 25%, 50%, 75%, and 100% Integral layer application against a vanilla baseline to identify the optimal denoising depth.
  2. Ablation on signal count: For a fixed model size and head count, test S = {2, 4, 8} to find the point of diminishing returns before the effective head size becomes too small.
  3. Analysis of attention distribution: Visualize attention scores to special tokens (e.g., [BOS]) across layers for vanilla, Differential, and Integral models to confirm noise mitigation without total suppression.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the Integral Transformer maintain its performance advantage over Differential and Vanilla attention at scales exceeding two billion parameters?
- Basis in paper: [explicit] The authors state in the Limitations section that they "could not properly investigate the scaling law" due to resource constraints limiting models to 1.2B parameters.
- Why unresolved: Training dynamics and the signal-to-noise ratio in logits often change non-linearly with model scale, making small-scale success an unreliable predictor of large-scale behavior.
- What evidence would resolve it: Pretraining experiments at standard large scales (e.g., 7B or 70B parameters) comparing INTG against baselines using identical data curves.

### Open Question 2
- Question: Why does applying denoising attention to lower Transformer layers cause performance degradation, and what is the "nature" of this layer-specific noise?
- Basis in paper: [explicit] Section 4.3 notes that applying INTG to the bottom layers hurts performance, concluding "deeper and more theoretical studies are needed to fully understand this phenomenon."
- Why unresolved: The paper empirically observes that noise is not uniform across layers but lacks a theoretical framework explaining why early attention noise might be beneficial or necessary for feature extraction.
- What evidence would resolve it: A mechanistic interpretability study analyzing rank collapse and gradient flow in early layers when denoising is forcibly applied versus withheld.

### Open Question 3
- Question: Does the Integral mechanism generalize to structured reasoning domains like coding, where the definition of "noise" tokens differs from natural language?
- Basis in paper: [explicit] The Limitations section highlights that experiments were restricted to specific NLP benchmarks and suggests "additional evaluation on more diverse domains—such as coding" is necessary.
- Why unresolved: The method optimizes the retention of special tokens (e.g., [BOS]) and punctuation based on NLP heuristics; it is unclear if this bias helps or harms performance in syntax-heavy domains like programming.
- What evidence would resolve it: Evaluation on code generation benchmarks (e.g., HumanEval) to measure if the retention of structural tokens improves syntactic validity.

## Limitations

- Empirical scope limited to Llama2-like architectures with Cosmopedia v2 and FineWeb-Edu pretraining; no results on non-LM tasks or different model families.
- Architectural coupling requires vanilla attention in bottom 50% of layers, limiting its generality as a drop-in replacement for self-attention.
- Noise model specificity relies on zero-mean fluctuations assumption that is theoretically motivated but not empirically validated.

## Confidence

**High Confidence**: The claim that the Integral Transformer outperforms vanilla attention on the studied benchmarks when applied to the top 50% of layers. This is directly supported by pretraining results on two model scales.

**Medium Confidence**: The claim that the Integral Transformer effectively balances attention distributions and reduces rank collapse in upper layers. While the analysis supports this, the link between rank collapse and downstream performance is not rigorously established.

**Medium Confidence**: The claim that preserving attention to special tokens improves performance by maintaining their role as "attention sinks." The paper provides ablation evidence, but the broader importance of this mechanism across tasks is not fully validated.

**Low Confidence**: The claim that the denoising mechanism works by averaging independent signals from a zero-mean noise distribution. This is theoretically motivated but not empirically validated; the noise model is assumed rather than measured.

## Next Checks

1. **Noise model validation**: Measure the empirical distribution of attention logits (e.g., mean, variance, correlation across signals) in both vanilla and Integral Transformer models to verify the zero-mean noise assumption and assess the actual denoising effect.

2. **Layer-wise generalization**: Evaluate the Integral Transformer on a non-LM task (e.g., image classification with a Vision Transformer or masked language modeling with an encoder) to test if the layer-wise denoising strategy (vanilla in bottom, Integral in top) generalizes beyond causal language modeling.

3. **Signal count sensitivity**: Conduct a systematic ablation study on the number of signals S across a wider range (e.g., S = 1, 2, 4, 8, 16) for different model sizes to identify the precise point of diminishing returns and the minimum viable effective head dimension before performance degrades.