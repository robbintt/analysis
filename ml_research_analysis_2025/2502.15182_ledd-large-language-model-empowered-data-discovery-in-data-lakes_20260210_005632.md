---
ver: rpa2
title: 'LEDD: Large Language Model-Empowered Data Discovery in Data Lakes'
arxiv_id: '2502.15182'
source_url: https://arxiv.org/abs/2502.15182
tags:
- data
- ledd
- semantic
- search
- hierarchical
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: LEDD is a system that uses large language models (LLMs) to create
  semantic catalogs and enable semantic search in data lakes. It organizes millions
  of tables and columns into hierarchical categories with meaningful labels, allowing
  users to find related data using natural language queries.
---

# LEDD: Large Language Model-Empowered Data Discovery in Data Lakes

## Quick Facts
- arXiv ID: 2502.15182
- Source URL: https://arxiv.org/abs/2502.15182
- Reference count: 14
- Creates hierarchical catalogs and enables semantic search in data lakes using LLM summarization

## Executive Summary
LEDD is a system that uses large language models (LLMs) to create semantic catalogs and enable semantic search in data lakes. It organizes millions of tables and columns into hierarchical categories with meaningful labels, allowing users to find related data using natural language queries. LEDD integrates with existing data lake infrastructure through Python UDFs, making it easy to extend or replace algorithms. By combining semantic embeddings and LLM-based summarization, LEDD offers a comprehensive solution for data discovery that was previously unsupported in similar systems.

## Method Summary
LEDD extracts six facets of column metadata (data source info, source-table-column path, sibling columns, value characteristics, related tasks, and description) and uses LLM summarization to create unified textual descriptions. These descriptions are embedded and stored in Milvus for similarity search. The system performs iterative hierarchical clustering, generating category labels at each level using LLMs. For semantic search, user queries are rephrased by an LLM, embedded, and matched against the Milvus index to retrieve semantically related tables and columns.

## Key Results
- Creates hierarchical catalogs from millions of tables and columns using LLM summarization
- Enables semantic search through natural language queries by rephrasing and embedding
- Provides real-time relation analysis to show how data entities connect

## Why This Works (Mechanism)

### Mechanism 1: LLM-Based Semantic Fusion for Column Representation
LEDD combines six facets of column metadata through LLM summarization rather than vector fusion, producing embeddings that better align with human semantic understanding. The LLM creates a unified textual description from all metadata facets, which is then embedded into a single vector per column.

### Mechanism 2: Iterative Hierarchical Clustering with LLM Category Summarization
The system performs bottom-up clustering where each column embedding starts as a graph node. LEDD iteratively clusters nodes into K groups and prompts the LLM to generate descriptive phrases for each cluster, repeating until reaching the top-level node count K.

### Mechanism 3: Vector-Based Semantic Search with LLM Query Rephrasing
User queries are rephrased via LLM before embedding to improve retrieval recall. The rephrased query is embedded and searched against pre-computed embeddings in Milvus, returning the n closest vectors for hierarchical graph visualization.

## Foundational Learning

- **Vector embeddings and similarity search**: LEDD stores all semantic representations as vectors in Milvus; understanding embedding spaces and approximate nearest neighbor search is essential for debugging retrieval quality.
- **Hierarchical clustering algorithms**: LEDD's catalog generation relies on iterative agglomerative-style clustering; understanding cluster quality metrics helps evaluate and tune the K parameter.
- **LLM prompting strategies**: LEDD uses LLMs for summarization, category labeling, query rephrasing, and relation analysis; prompt design directly affects output quality and consistency.

## Architecture Onboarding

- **Component map**: IGinX (data lake) → Python UDFs (LEDD logic) → OpenAI API (LLM calls) → Milvus (vector storage) → Apache Zeppelin (visualization + UI)
- **Critical path**: Schema extraction from IGinX → LLM summarization → embedding → Milvus storage → iterative clustering → query rephrasing → Milvus search → graph highlight
- **Design tradeoffs**: LLM summarization vs. composite embedding (semantic alignment vs. latency/cost); Milvus vs. LSH/HNSW (managed search vs. pure similarity); fixed K vs. adaptive clustering (UI suitability vs. natural hierarchy)
- **Failure signatures**: Empty/generic category labels (LLM under-specified prompts), search returns irrelevant tables (embedding model mismatch), slow UI response (LLM API latency), hierarchy too deep/shallow (K parameter misconfigured)
- **First 3 experiments**: 1) Validate embedding quality by computing pairwise cosine similarities and verifying semantic relationships, 2) Test LLM summarization consistency by running prompts multiple times on same column, 3) Evaluate retrieval precision by measuring recall@n and precision@n with ground-truth relevance judgments

## Open Questions the Paper Calls Out

### Open Question 1
How does LEDD manage computational latency and monetary costs associated with invoking LLMs for millions of columns? The paper demonstrates processing large schemas but doesn't address scalability bottlenecks or API costs at scale.

### Open Question 2
To what extent do LLM-generated hierarchical category labels align with human mental models compared to traditional metadata clustering techniques? The authors claim better alignment with human cognition but provide no experimental validation or user studies.

### Open Question 3
How accurate is LLM-based real-time relation analysis in identifying structural relationships compared to deterministic data discovery algorithms? The system claims to identify joinable/unionable relations but offers no precision/recall metrics against established benchmarks.

## Limitations
- No quantitative metrics for catalog quality or search precision/recall
- Unspecified LLM prompts and hyperparameters
- Reliance on OpenAI APIs without cost/complexity analysis at scale
- No validation against simpler vector fusion baselines

## Confidence
- **Medium**: The LLM+embedding pipeline is plausible given recent advances, but effectiveness depends heavily on prompt quality and embedding model choice
- **Low**: No empirical validation of six-facet summarization vs. composite embeddings
- **Medium**: Architectural claims are reasonable but unverified at production scale

## Next Checks
1. Benchmark retrieval quality on SchemaPile with ground-truth relevance judgments
2. Compare six-facet summarization vs. composite embeddings using human evaluators
3. Stress-test system latency/cost at 10M+ columns with realistic metadata sparsity