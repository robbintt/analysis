---
ver: rpa2
title: 'Hierarchical Retrieval: The Geometry and a Pretrain-Finetune Recipe'
arxiv_id: '2509.16411'
source_url: https://arxiv.org/abs/2509.16411
tags:
- retrieval
- pairs
- query
- embeddings
- pretrain-finetune
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of learning dual encoder (DE)
  models for hierarchical retrieval (HR), where documents form a hierarchy and queries
  need to retrieve all ancestors. The authors prove that DEs are theoretically feasible
  for HR with embedding dimension scaling linearly in hierarchy depth and logarithmically
  in document count.
---

# Hierarchical Retrieval: The Geometry and a Pretrain-Finetune Recipe

## Quick Facts
- arXiv ID: 2509.16411
- Source URL: https://arxiv.org/abs/2509.16411
- Authors: Chong You; Rajesh Jayaram; Ananda Theertha Suresh; Robin Nittka; Felix Yu; Sanjiv Kumar
- Reference count: 26
- Primary result: Pretrain-finetune recipe improves long-distance HR recall from 19% to 76% while maintaining short-distance performance

## Executive Summary
This paper tackles hierarchical retrieval (HR), where queries need to retrieve all ancestor documents in a hierarchy. The authors prove that dual encoder models can theoretically solve HR with embedding dimension scaling linearly in hierarchy depth and logarithmically in document count. However, they discover a "lost-in-the-long-distance" phenomenon where standard training produces embeddings that excel at retrieving nearby documents but catastrophically fail at distant ancestors. To address this, they propose a pretrain-finetune recipe: first pretrain on regular data, then finetune exclusively on long-distance pairs. Experiments on WordNet show this improves long-distance recall from 19% to 76%, while maintaining short-distance performance. On the ESCI shopping dataset, the method outperforms joint training for retrieving both exact and substitute products.

## Method Summary
The method addresses hierarchical retrieval where relevant documents for a query are all ancestors in a Directed Acyclic Graph (DAG). Dual encoders map queries and documents to a shared Euclidean embedding space where similarity is computed via dot products. The pretrain-finetune recipe involves two stages: (1) pretrain the dual encoder on regular-sampled data using standard softmax loss, and (2) finetune exclusively on long-distance pairs with reduced learning rate (1000× smaller) and increased temperature (T=500). This approach specifically targets the "lost-in-the-long-distance" phenomenon where standard training fails to learn representations for distant hierarchical relationships.

## Key Results
- Theoretical proof that dual encoders can solve HR with embedding dimension scaling as O(s log m)
- "Lost-in-the-long-distance" phenomenon: standard training achieves 100% recall for distance 0 pairs but fails for distances 1+
- Pretrain-finetune recipe improves long-distance recall from 19% to 76% on WordNet while maintaining short-distance performance
- Outperforms joint training and reweighting schemes on ESCI shopping dataset for substitute product retrieval

## Why This Works (Mechanism)

### Mechanism 1: Existence of Asymmetric Euclidean Embeddings
Dual encoders can theoretically solve HR despite Euclidean geometry constraints, provided the embedding dimension scales as O(s log m). This works through a constructive proof using normalized Gaussian random vectors for documents, where a query's embedding is the normalized sum of its relevant document vectors. This exploits the near-orthogonality of random high-dimensional vectors to ensure that relevant query-document inner products exceed a universal threshold while irrelevant pairs fall below it.

### Mechanism 2: Lost-in-the-Long-Distance Phenomenon
Standard DE training produces embeddings that achieve near-perfect recall for short-distance query-document pairs but fail catastrophically for long-distance hierarchical relationships. The softmax loss optimization is dominated by the more frequent short-distance training pairs, causing the model to converge to a local optimum that captures local hierarchical structure but fails to learn abstract representations needed for distant ancestor relationships.

### Mechanism 3: Pretrain-Finetune Recipe
A two-stage training approach—pretraining on standard data followed by finetuning exclusively on long-distance pairs—achieves high recall across all distances where joint training and rebalancing fail. Pretraining establishes foundational representations across the entire hierarchy, while finetuning on long-distance pairs specifically adapts decision boundaries for underrepresented distant relationships without catastrophic forgetting of short-distance patterns, provided learning rate is reduced and temperature is increased.

## Foundational Learning

- **Concept: Dual Encoder (DE) Architecture**
  - **Why needed here:** Core architecture under investigation; understanding how queries and documents map to a shared embedding space via dot products is essential.
  - **Quick check question:** How does a DE compute similarity between a query and document?

- **Concept: Softmax Loss (InfoNCE)**
  - **Why needed here:** The training objective whose optimization dynamics create the "lost-in-the-long-distance" problem.
  - **Quick check question:** What does the softmax loss maximize for a matching query-document pair relative to negative samples?

- **Concept: Directed Acyclic Graph (DAG) for Hierarchies**
  - **Why needed here:** Formal structure representing document relationships; the paper proves these can be embedded in Euclidean space under dimension constraints.
  - **Quick check question:** In a DAG representing HR, what nodes must be retrieved for a query with exact match node u?

## Architecture Onboarding

- **Component map:**
  - Query/Document Encoders (f_q, f_x) -> Embedding Space (ℝ^d) -> Softmax Cross-Entropy Loss
  - Synthetic/Real data -> Training Pipeline -> Dual Encoder with lookup table/Transformer

- **Critical path:**
  1. Pretrain DE on full (biased) dataset with standard learning rate and temperature (T=20)
  2. Finetune on long-distance-only subset with reduced learning rate (1000× smaller) and increased temperature (T=500)
  3. Apply early stopping based on validation set to prevent short-distance degradation

- **Design tradeoffs:**
  - Dimension vs. Capacity: Higher d supports deeper hierarchies but increases latency/memory
  - Training Strategy: Standard (simple, fails long-distance) vs. Rebalanced (improves long-distance, destroys short-distance) vs. Pretrain-finetune (solves both, adds complexity)
  - Finetuning Duration: Must stop before short-distance recall collapses

- **Failure signatures:**
  - Short-distance recall ≈100%, long-distance recall ≈0: Classic lost-in-the-long-distance
  - Long-distance recall high, short-distance ≈0: Catastrophic forgetting from over-finetuning

- **First 3 experiments:**
  1. Reproduce baseline failure on synthetic H=4, W=5 tree with d=3; plot recall by distance
  2. Test rebalanced sampling (p=0.03); demonstrate short-distance recall collapse
  3. Implement pretrain-finetune; verify high recall across all distances with early stopping

## Open Questions the Paper Calls Out

- **Open Question 1:** Is there a theoretical characterization of the optimization dynamics that explains why standard training causes the "lost-in-the-long-distance" phenomenon? The paper empirically identifies this failure mode but lacks a formal theoretical explanation for why gradient descent fails to find solutions for long-distance pairs.

- **Open Question 2:** Can the pretrain-finetune recipe be effectively combined with non-Euclidean geometries, such as hyperbolic embeddings, to further improve hierarchical retrieval? While hyperbolic embeddings outperform standard Euclidean ones, they still underperform compared to the pretrain-finetune recipe.

- **Open Question 3:** How can the specific "long-distance" data required for finetuning be generated or discovered in fully unsupervised real-world applications where the hierarchy is unknown? The current method assumes access to a partition of data (e.g., "Substitute" vs "Exact" in ESCI) that may not exist in generic retrieval datasets.

- **Open Question 4:** Does the pretrain-finetune recipe effectively lower the theoretical upper bound on the embedding dimension required to solve hierarchical retrieval? The paper demonstrates improved retrieval quality empirically but doesn't prove that the algorithm changes the minimum dimensionality needed to guarantee a solution.

## Limitations

- Theoretical analysis relies on specific Gaussian random vector construction and doesn't fully characterize the optimization landscape causing the "lost-in-the-long-distance" phenomenon
- Empirical validation limited to two datasets (WordNet and ESCI), with only one deep hierarchy (WordNet's depth-8 structure)
- Method assumes access to hierarchy information or proxy distance metrics, limiting applicability to unsupervised real-world scenarios

## Confidence

**High Confidence**: Theoretical feasibility proof for DEs in HR tasks and empirical demonstration that standard training fails on long-distance pairs while pretrain-finetune succeeds.

**Medium Confidence**: Universality of the "lost-in-the-long-distance" phenomenon across different dataset types and robustness of the pretrain-finetune recipe across different hyperparameters.

**Low Confidence**: Optimality of specific pretrain-finetune hyperparameters and whether alternative approaches could achieve similar or better results.

## Next Checks

1. **Hyperparameter Sensitivity Analysis**: Systematically vary the finetuning learning rate reduction factor (currently 1000×) and temperature increase (currently from 20 to 500) to identify optimal ranges and test robustness.

2. **Alternative Deep Hierarchy**: Evaluate the pretrain-finetune approach on a deeper, more complex hierarchy than WordNet (e.g., MeSH medical ontology or patent classification systems) to test scalability limits.

3. **Alternative Optimization Approaches**: Compare pretrain-finetune against curriculum learning strategies that gradually introduce longer-distance pairs during training, and against reweighting schemes with adaptive temperature scheduling.