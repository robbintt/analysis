---
ver: rpa2
title: Algorithmic Guarantees for Distilling Supervised and Offline RL Datasets
arxiv_id: '2512.00536'
source_url: https://arxiv.org/abs/2512.00536
tags:
- dataset
- train
- dsup
- synthetic
- lerr
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper develops theoretical guarantees for dataset distillation\
  \ in supervised regression and offline reinforcement learning. For supervised regression\
  \ in d dimensions, the authors prove that optimizing a synthetic dataset to match\
  \ MSE losses with respect to O(d\xB2) randomly sampled linear regressors is sufficient\
  \ to guarantee that any bounded linear model has approximately the same loss on\
  \ the training and synthetic datasets."
---

# Algorithmic Guarantees for Distilling Supervised and Offline RL Datasets

## Quick Facts
- arXiv ID: 2512.00536
- Source URL: https://arxiv.org/abs/2512.00536
- Reference count: 40
- One-line primary result: Theoretical guarantees for dataset distillation in supervised regression and offline RL, with O(d²) complexity for linear models and decomposable features, matching Ω(d²) lower bound.

## Executive Summary
This paper develops theoretical guarantees for dataset distillation in supervised regression and offline reinforcement learning. For supervised regression in d dimensions, the authors prove that optimizing a synthetic dataset to match MSE losses with respect to O(d²) randomly sampled linear regressors is sufficient to guarantee that any bounded linear model has approximately the same loss on the training and synthetic datasets. They also prove a matching Ω(d²) lower bound showing this is tight. For offline RL, they extend this approach by matching Bellman losses using sampled Q-value predictors, requiring exponentially many (exp(O(d log d))) predictors in the general case but only O(d²) when the feature map is decomposable. The algorithms generate synthetic datasets without model training, and experiments show performance gains over baselines including leverage score subsampling, with the synthetic datasets achieving similar or better results than full training data while being much smaller.

## Method Summary
The method samples random linear regressors (supervised) or Q-value predictors (offline RL) from Gaussian distributions, then optimizes a synthetic dataset to minimize the sum of squared loss differences across these samples. For supervised regression, this involves minimizing MSE loss mismatches, while for offline RL it minimizes Bellman loss mismatches. The approach handles terminated and non-terminated states separately in RL. The optimization is convex for supervised regression and decomposable offline RL, but may require gradient-based methods for general offline RL. The synthetic dataset is parameterized to stay within bounded domains, and for decomposable features, a linear moment constraint is enforced.

## Key Results
- Matching upper and lower bounds of Θ(d²) for supervised dataset distillation in d dimensions
- Exponential (exp(O(d log d))) sample complexity for general offline RL distillation, reducible to Õ(d²) for decomposable feature maps
- Experimental validation showing synthetic datasets achieve similar or better performance than full training data while being much smaller
- Superior performance over leverage score subsampling baseline in supervised regression tasks

## Why This Works (Mechanism)

### Mechanism 1: Random Regressor Loss Matching
Optimizing a synthetic dataset to match MSE losses with respect to Õ(d²) randomly sampled linear regressors guarantees that any bounded linear model has approximately the same loss on training and synthetic datasets. The squared loss difference is a degree-4 polynomial in Gaussian variables, and by Carbery-Wright anti-concentration, if any regressor has loss gap > Δ, then with probability ≥ 1/3, a randomly sampled regressor will also detect this gap (scaled by 1/d²). Taking k = O(d² log terms) samples and union bounding over all possible regressor/synthetic dataset pairs provides the guarantee.

### Mechanism 2: ε-Net Coverage for Uniform Guarantees
A union bound over fine-grained nets of regressors and synthetic datasets converts a probabilistic lower bound for a fixed pair into a uniform guarantee for all bounded linear models. Construct an ε-net over the unit ball of regressors and a net over size-d subsets of synthetic data points. The product has size exp(O(d² log terms)). Each net point approximately preserves the loss difference within O(ε(B+b)⁴). With k samples, Chernoff bounds guarantee that if any net point has gap > Δ, enough samples will detect it with high probability. Union bound over all net points gives the uniform guarantee.

### Mechanism 3: Decomposable Feature Map Reduction for Offline RL
When the state-action feature map ϕ is decomposable (ϕ(s,a) = ϕ₁(s) + ϕ₂(a)), the offline RL distillation problem reduces to supervised regression, requiring only Õ(d²) sampled Q-predictors instead of exp(O(d log d)). In the Bellman loss, max_a′ f(s′,a′) = v^T ϕ₁(s′) + max_a′ v^T ϕ₂(a′). The second term depends only on v, not on the dataset. When computing the loss difference between D_train and D_syn, terms involving α(v) cancel, leaving only a linear regression objective in the transformed features. This admits the same anti-concentration and net-based argument as supervised regression, but additionally requires the linear constraint E_Dsyn[ϕ₁(s), ϕ₂(a), r, ϕ₁(s′)] = E_Dtrain[ϕ₁(s), ϕ₂(a), r, ϕ₁(s′)].

## Foundational Learning

- **ε-Nets and Covering Numbers**: Used to approximate all possible regressors and synthetic datasets with finite nets, then taking a union bound. The covering number (1+2r/ε)^d quantifies how many points suffice.
  - Quick check: For a unit ball in R¹⁰, what is the minimum size of a 0.1-cover under the ℓ₂ metric?

- **Pseudo-Dimension of Function Classes**: The offline RL Bellman loss has a max operation that makes it non-polynomial; pseudo-dimension bounds the covering number of the loss class, controlling how many samples are needed for generalization.
  - Quick check: What is the pseudo-dimension of the class of linear functions on R^d?

- **Anti-Concentration for Gaussian Polynomials**: The core probabilistic argument uses Carbery-Wright to show that if a degree-4 polynomial has non-trivial expectation, it is unlikely to be near zero. This ensures random regressors detect loss gaps.
  - Quick check: Why is anti-concentration different from concentration, and why does a degree-4 polynomial require a different approach than a linear function?

## Architecture Onboarding

- **Component map**: Sampler -> Optimizer -> Constraint checker (for decomposable RL) -> Evaluator
- **Critical path**:
  - Supervised: Initialize D_syn randomly → run convex optimizer → output D_syn. Path is straightforward; optimization is tractable.
  - Offline RL (general): Same path, but the optimization is not guaranteed convex due to the Bellman max term. May require gradient-based methods with no convergence guarantee.
  - Offline RL (decomposable): Add linear moment constraints; optimization remains convex if ϕ₁, ϕ₂ are linear.
- **Design tradeoffs**:
  - k (number of sampled regressors): Higher k improves guarantee tightness but increases optimization cost. Experiments show k=20–100 works; theory requires O(d² log factors).
  - m (synthetic dataset size): Lower bound is d+1 (Lemma C.2); larger m may improve performance but increases storage.
  - Feature map choice: Decomposable maps give Õ(d²) complexity but may reduce expressivity. Non-decomposable maps are more expressive but require exp(O(d log d)) samples in theory.
- **Failure signatures**:
  - Too few regressors (k << d²): Synthetic dataset may match losses on sampled regressors but fail on held-out models. Verify by evaluating on a separate regressor set.
  - Non-decomposable feature map with k polynomial in d: Theory only guarantees exp(O(d log d)) samples; empirical performance may degrade unpredictably.
  - Constraint violation in decomposable offline RL: If the moment constraint cannot be satisfied (e.g., D_train has feature drift not representable by bounded D_syn), the guarantee voids.
- **First 3 experiments**:
  1. **Supervised regression toy task**: Generate D_train from a known linear model in d=10 with n=1000 points. Run Algorithm 1 with k=100, m=50. Plot test MSE of model trained on D_syn vs. D_train as k varies from 10 to 500.
  2. **Offline RL with decomposable features**: Implement in Cartpole with one-hot action encoding (ϕ₂(a) = one-hot, ϕ₁(s) = linear projection). Run Algorithm 2 with k=20–100, Nsyn=50–200. Compare returns to D_train and D_rand. Verify linear moment constraint is satisfiable.
  3. **Ablation on feature map decomposability**: Construct a non-decomposable ϕ (e.g., MLP features) and compare required k for stable performance vs. decomposable baseline. Expect higher k needed, consistent with theory.

## Open Questions the Paper Calls Out

- **Open Question 1**: Can the theoretical guarantees for supervised dataset distillation be extended to non-linear neural networks?
  - Basis in paper: [explicit] The conclusion states, "An interesting yet challenging future direction in to extend our theoretical results in the supervised setting to neural networks."
  - Why unresolved: The current proofs rely on polynomial anti-concentration arguments specific to linear regressors, which do not directly transfer to non-linear architectures.
  - What evidence would resolve it: A proof showing that optimizing synthetic datasets against randomly sampled neural networks preserves loss bounds for all networks in the class.

- **Open Question 2**: Can the sample complexity for offline RL distillation be reduced from exponential to polynomial for general feature maps?
  - Basis in paper: [explicit] The authors identify the goal of obtaining "provably efficient offline RL dataset distillation algorithms for more general classes of state-action embeddings."
  - Why unresolved: Theorem 4.3 provides an exponential bound $exp(O(d \log d))$ for general embeddings due to the "max" term in the Bellman loss, whereas the decomposable case allows $\tilde{O}(d^2)$.
  - What evidence would resolve it: An algorithm that achieves polynomial sample complexity for general embeddings, or a lower bound proof demonstrating that exponential complexity is unavoidable.

- **Open Question 3**: Can specific geometric properties of MDPs (other than decomposability) be leveraged to ensure efficient offline distillation?
  - Basis in paper: [explicit] The conclusion suggests "leveraging specific geometric properties of the associated MDPs" as a method to improve efficiency.
  - Why unresolved: The current polynomial guarantees (Theorem 4.4) rely strictly on the decomposability of the feature map, leaving other structural properties unexplored.
  - What evidence would resolve it: A theoretical analysis identifying MDP geometric properties (e.g., smoothness, reachability) that enable efficient distillation without requiring decomposable feature maps.

## Limitations

- The exponential sample complexity bound for general offline RL features suggests the approach may scale poorly for complex state-action representations, though experiments with simple environments show feasibility.
- The decomposability assumption for improved sample complexity is a strong structural requirement that may not hold in many practical RL settings.
- The convex optimization formulation for supervised regression and decomposable offline RL is tractable, but the general offline RL case requires non-convex optimization without convergence guarantees.

## Confidence

- **Mechanism 1 (Random Regressor Loss Matching)**: High - Based on rigorous mathematical proofs using anti-concentration arguments
- **Mechanism 2 (ε-Net Coverage)**: High - Well-established covering number techniques with standard proofs
- **Mechanism 3 (Decomposable Feature Map Reduction)**: Medium - Theoretical proof is sound but relies on restrictive assumption and lacks extensive empirical validation

## Next Checks

1. **Scaling with Dimensionality**: Systematically evaluate the impact of feature dimension d on required sample complexity k for both supervised and offline RL tasks, comparing against the theoretical O(d²) and exp(O(d log d)) bounds.

2. **Non-Decomposable Feature Robustness**: Test the algorithm with various non-decomposable feature maps (MLP, RBF kernels) to empirically verify the predicted exponential scaling in k and identify practical workarounds.

3. **Constraint Feasibility Analysis**: For decomposable offline RL, analyze the feasibility of satisfying the linear moment constraint across different environment dynamics and data distributions, quantifying when the guarantee remains valid.