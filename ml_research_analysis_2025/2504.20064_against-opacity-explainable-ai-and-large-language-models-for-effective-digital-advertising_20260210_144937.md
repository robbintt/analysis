---
ver: rpa2
title: 'Against Opacity: Explainable AI and Large Language Models for Effective Digital
  Advertising'
arxiv_id: '2504.20064'
source_url: https://arxiv.org/abs/2504.20064
tags:
- advertising
- data
- content
- prediction
- such
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the lack of transparency in digital advertising
  platforms, where opaque algorithms control targeting and pricing, leaving marketers
  without actionable insights. The authors propose SODA, a system that combines large
  language models (LLMs) with explainable AI to analyze advertising content and provide
  interpretable insights.
---

# Against Opacity: Explainable AI and Large Language Models for Effective Digital Advertising

## Quick Facts
- arXiv ID: 2504.20064
- Source URL: https://arxiv.org/abs/2504.20064
- Reference count: 40
- This paper addresses opacity in digital advertising platforms and proposes SODA, a system combining LLMs with explainable AI to provide interpretable marketing insights.

## Executive Summary
This paper tackles the critical issue of opacity in digital advertising platforms where complex algorithms control targeting and pricing without providing marketers actionable insights. The authors propose SODA (System for Optimized Digital Advertising), which integrates large language models with explainable AI to analyze advertising content and generate interpretable insights. SODA employs an improved CTR prediction model with attention visualization to highlight key image elements, then uses LLMs to extract standardized features and generate actionable marketing insights including user personas and campaign comparisons. A case study with 12 marketing professionals demonstrated that SODA provides useful, standardized, and tangible insights that could significantly enhance marketing decision-making and efficiency.

## Method Summary
SODA combines large language models with explainable AI to analyze advertising content and provide interpretable insights for digital marketing. The system uses an improved CTR prediction model (SoWide-v2) with attention-based visualization to identify key image elements influencing predictions. LLMs then extract standardized features from these visual insights and generate actionable marketing insights such as user personas and campaign comparisons. The approach aims to bridge the gap between opaque AI predictions and actionable human understanding in advertising by providing marketers with clear, interpretable explanations of why certain content performs well.

## Key Results
- SODA was tested with 12 marketing professionals who found it provided useful, standardized, and tangible insights
- The system combines improved CTR prediction with attention-based visualization to highlight key image elements
- LLM integration enables extraction of standardized features and generation of actionable marketing insights

## Why This Works (Mechanism)
The system works by first using an attention-based CTR prediction model to identify which visual elements in advertisements are most influential for user engagement. These visual insights are then processed by LLMs to extract standardized features and generate human-interpretable marketing insights. This two-stage approach leverages the pattern recognition capabilities of deep learning models while using LLMs to translate complex visual patterns into actionable business insights that marketers can understand and act upon.

## Foundational Learning
- Explainable AI: Techniques that make AI model decisions interpretable to humans; needed because opaque algorithms prevent marketers from understanding why campaigns succeed or fail; quick check: can you explain the model's decision in plain language?
- Attention mechanisms: Neural network components that highlight important features in input data; needed to identify which visual elements drive CTR predictions; quick check: does the attention visualization align with human intuition about ad effectiveness?
- Large Language Models for feature extraction: Using LLMs to process and standardize insights from visual data; needed to bridge the gap between visual patterns and marketing terminology; quick check: are the generated insights consistent and actionable for marketing professionals?

## Architecture Onboarding

Component Map: Input Ads -> SoWide-v2 CTR Prediction -> Attention Visualization -> LLM Feature Extraction -> Insight Generation -> Marketing Insights

Critical Path: The critical path flows from input advertisements through CTR prediction and attention visualization to LLM processing and final insight generation. Each stage must complete successfully for the system to produce actionable insights.

Design Tradeoffs: The system trades computational efficiency for interpretability by using attention visualization and LLM processing, which adds latency but provides human-understandable insights. The choice of LLMs for feature extraction trades precision for broader applicability across different advertising contexts.

Failure Signatures: System failures may manifest as irrelevant attention highlights, generic LLM-generated insights, or insights that don't align with actual campaign performance. Attention visualization failures might highlight unimportant elements, while LLM failures might produce insights too abstract to be actionable.

First 3 Experiments:
1. Test attention visualization on a diverse set of advertisements to verify it highlights intuitive visual elements
2. Validate LLM-generated insights against expert marketing knowledge for consistency and actionability
3. Measure correlation between SODA's predicted important elements and actual user engagement metrics

## Open Questions the Paper Calls Out
None identified in the provided content.

## Limitations
- Small sample size of 12 marketing professionals may not represent the broader advertising industry
- Case study appears to be a usability evaluation rather than rigorous experimental validation
- Lacks quantitative metrics demonstrating improvements in CTR prediction accuracy or marketing performance

## Confidence

High confidence:
- The problem statement regarding opacity in digital advertising platforms is well-established

Medium confidence:
- The technical approach combining LLMs with explainable AI is plausible but lacks quantitative validation

Low confidence:
- The claim that SODA "significantly enhances marketing decision-making and efficiency" is based on qualitative feedback from a small user study without controlled experimental evidence

## Next Checks
1. Conduct a controlled experiment with a larger sample size (minimum 50-100 marketing professionals) comparing SODA's insights against traditional advertising analytics tools, measuring actual improvements in campaign performance metrics like CTR, conversion rates, and ROI.

2. Perform a comprehensive bias audit of the LLM-based feature extraction and insight generation components, testing for systematic biases across different product categories, demographic groups, and cultural contexts.

3. Implement A/B testing framework where marketing teams use SODA-generated insights for campaign optimization versus teams using standard analytics, tracking long-term performance differences across multiple advertising campaigns.