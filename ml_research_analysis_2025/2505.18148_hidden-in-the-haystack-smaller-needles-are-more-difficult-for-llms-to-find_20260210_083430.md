---
ver: rpa2
title: 'Hidden in the Haystack: Smaller Needles are More Difficult for LLMs to Find'
arxiv_id: '2505.18148'
source_url: https://arxiv.org/abs/2505.18148
tags:
- gold
- context
- size
- answer
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Large language models (LLMs) perform worse when relevant information
  is contained in smaller documents, even when controlling for position, answer repetition,
  gold-to-distractor ratio, distractor volume, and domain specificity. This study
  presents the first systematic analysis of gold context size in long-context question
  answering, spanning three diverse benchmarks (general knowledge, biomedical reasoning,
  and mathematical reasoning), eleven state-of-the-art LLMs (including recent reasoning
  models), and more than 150K controlled runs.
---

# Hidden in the Haystack: Smaller Needles are More Difficult for LLMs to Find

## Quick Facts
- **arXiv ID:** 2505.18148
- **Source URL:** https://arxiv.org/abs/2505.18148
- **Reference count:** 40
- **Primary result:** Smaller gold contexts consistently degrade LLM performance in long-context question answering across diverse domains and models

## Executive Summary
This study systematically investigates how the size of relevant information (gold context) affects large language model performance in long-context question answering. Through controlled experiments spanning three diverse benchmarks (general knowledge, biomedical reasoning, and mathematical reasoning), eleven state-of-the-art LLMs, and over 150K runs, the research demonstrates that smaller gold contexts consistently degrade performance even when controlling for position, answer repetition, gold-to-distractor ratio, distractor volume, and domain specificity. The findings reveal a fundamental fragility in current LLMs when processing scattered, fine-grained information of varying lengths, with implications for agentic systems that must integrate heterogeneous document sources.

The research establishes that this size effect is not attributable to confounding factors but represents a genuine performance bottleneck. Larger gold contexts yield significant improvements independent of other variables, while smaller contexts amplify positional sensitivity and make relevant information harder to extract. The effect persists across all tested conditions and model families, including recent reasoning models, suggesting a fundamental limitation in how LLMs process and integrate information from documents of different sizes.

## Method Summary
The study employs a controlled experimental design using three diverse benchmarks: general knowledge (HotpotQA), biomedical reasoning (BioASQ), and mathematical reasoning (MATH). Researchers conducted over 150K controlled runs across eleven state-of-the-art LLMs, systematically varying gold context sizes while holding other factors constant. The experiments manipulate document lengths, answer positions, repetition patterns, and domain characteristics to isolate the effect of gold context size on model performance. Performance is measured through standard question-answering metrics, with careful attention to controlling for potential confounders that might explain performance differences.

## Key Results
- Smaller gold contexts consistently degrade LLM performance across all three benchmark domains (general knowledge, biomedical reasoning, mathematical reasoning)
- The size effect persists even when controlling for position, answer repetition, gold-to-distractor ratio, distractor volume, and domain specificity
- Larger gold contexts yield significant performance improvements independent of confounding factors
- Smaller contexts amplify positional sensitivity, making relevant information harder to extract

## Why This Works (Mechanism)
The study reveals that LLMs struggle with information integration when relevant content is fragmented across smaller documents. The mechanism appears to involve both attention allocation challenges and contextual integration difficulties when processing scattered, fine-grained information. Smaller contexts may fail to provide sufficient surrounding context for the model to establish relevance and relationships, while also being more vulnerable to positional effects in the attention mechanism. This suggests fundamental limitations in how current transformer architectures process and integrate information from documents of varying sizes, particularly when the relevant information is compressed into minimal contexts.

## Foundational Learning

**Long-context question answering** - Understanding how LLMs process and reason over extended document sequences is essential because this study examines performance across varying context lengths and document sizes. Quick check: Can you explain the difference between short-context and long-context QA approaches?

**Attention mechanisms** - Knowledge of how transformer attention works is needed because the study implicates positional sensitivity and attention allocation as key factors in the size effect. Quick check: How does positional encoding affect attention weights in transformer models?

**Context window management** - Understanding how models handle context windows of different sizes is crucial since the research directly manipulates gold context length. Quick check: What strategies do LLMs use to manage information within fixed context windows?

**Information integration** - The ability to synthesize information from multiple sources is central to the study's findings about scattered, fine-grained information. Quick check: How do LLMs typically integrate information from multiple documents or passages?

**Benchmark design** - Knowledge of how question-answering benchmarks are constructed helps understand the controlled experimental conditions. Quick check: What are the key differences between HotpotQA, BioASQ, and MATH benchmarks?

## Architecture Onboarding

**Component map:** Document Retrieval -> Context Aggregation -> Answer Generation -> Performance Evaluation

**Critical path:** The study focuses on how context aggregation (combining documents of varying sizes) affects the answer generation stage, with performance evaluation measuring the downstream impact of gold context size on final outputs.

**Design tradeoffs:** Models must balance attention allocation across varying context sizes while maintaining performance, creating tension between processing efficiency and accuracy when relevant information is scattered across documents of different lengths.

**Failure signatures:** Performance degradation when gold contexts are small, increased sensitivity to position within the context window, and difficulty extracting relevant information from fine-grained, scattered sources.

**First experiments:**
1. Replicate the controlled experiments with a new benchmark domain to test generalizability
2. Test the effect on open-domain retrieval-augmented systems with dynamic document retrieval
3. Evaluate whether adaptive context processing strategies can mitigate the size-related performance degradation

## Open Questions the Paper Calls Out
None

## Limitations
- The study uses controlled experimental conditions that may not fully capture real-world document processing scenarios
- Results are based on curated benchmarks rather than dynamic, real-world information retrieval contexts
- The research does not explore optimal balancing ratios for document length aggregation or implementation costs
- Alternative mitigation strategies beyond document length balancing are not fully explored

## Confidence

**High confidence:**
- Smaller gold contexts consistently degrade LLM performance across diverse benchmarks and model families
- The effect persists across different conditions and reasoning models
- Larger gold contexts yield significant performance improvements independent of confounders

**Medium confidence:**
- Practical implications for agentic systems that must integrate heterogeneous document sources
- Document length balancing as an effective mitigation strategy
- Real-world impact on multi-agent systems with varying document preprocessing pipelines

## Next Checks

1. Test the gold context size effect on open-domain retrieval-augmented systems where documents are dynamically retrieved rather than pre-curated
2. Evaluate whether adaptive context processing (varying attention mechanisms based on gold context length) can mitigate the size-related performance degradation
3. Replicate the findings with continuously updated LLMs and newer architectures beyond the April 2025 timeframe to assess temporal stability of the effect