---
ver: rpa2
title: Automated Interpretation of Non-Destructive Evaluation Contour Maps Using Large
  Language Models for Bridge Condition Assessment
arxiv_id: '2507.14107'
source_url: https://arxiv.org/abs/2507.14107
tags:
- bridge
- image
- llms
- captioning
- areas
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This pilot study evaluated Large Language Models (LLMs) for interpreting
  bridge Non-Destructive Evaluation (NDE) contour maps. Five bridge inspection datasets
  were processed by nine LLM models using specialized prompts.
---

# Automated Interpretation of Non-Destructive Evaluation Contour Maps Using Large Language Models for Bridge Condition Assessment

## Quick Facts
- **arXiv ID:** 2507.14107
- **Source URL:** https://arxiv.org/abs/2507.14107
- **Reference count:** 39
- **Primary result:** Four LLM models achieved highest ratings for defect identification and coverage, with ChatGPT-4 and Claude 3.5 Sonnet producing the most comprehensive summaries for bridge condition assessment.

## Executive Summary
This pilot study demonstrates that Large Language Models can effectively interpret bridge Non-Destructive Evaluation contour maps, identifying structural defects and generating actionable maintenance recommendations. Using a multi-model approach with nine different LLMs, the researchers processed five NDE images from a single bridge, achieving high ratings for technical specificity and coverage. The methodology shows promise for accelerating bridge inspection workflows by combining parallel image captioning with hierarchical summarization, though the reliance on qualitative expert ratings and single-case analysis limits generalizability.

## Method Summary
The researchers fed five NDE contour map images (GPR cover depth, GPR attenuation, Electrical Resistivity, Impact-Echo frequency, USW modulus) from one bridge to nine multimodal LLMs using a standardized structural engineer prompt. Four models (ChatGPT-4, Claude 3.5 Sonnet, CogVLM2, ShareGPT4V) were identified as top performers and their outputs were synthesized by five summarization models. All models were evaluated by researchers and a domain expert using qualitative ratings for relevance, usefulness, coverage, and specificity.

## Key Results
- Four models (ChatGPT-4, Claude 3.5 Sonnet, CogVLM2, ShareGPT4V) achieved highest ratings (4-5/5) for defect identification and coverage
- ChatGPT-4 and Claude 3.5 Sonnet produced the most comprehensive summaries (4.67-5/5 overall)
- Multi-model parallel processing improved reliability through cross-referencing diverse interpretations
- The approach successfully generated actionable maintenance recommendations while maintaining technical accuracy

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Parallel multi-model image captioning improves reliability through cross-referencing diverse model outputs.
- **Mechanism:** Different vision-language models capture different aspects of NDE contour maps. By processing the same image through multiple architectures (proprietary and open-source), the system reduces single-point-of-failure errors and surfaces consistent findings across models.
- **Core assumption:** Cross-model agreement correlates with interpretation correctness; models make uncorrelated errors.
- **Evidence anchors:**
  - [abstract] "several state-of-the-art LLMs are explored with prompts specifically designed to enhance the quality of image descriptions"
  - [section III.B] "This parallel processing approach facilitates cross-referencing of findings across different models, leading to more reliable data interpretation"
  - [corpus] Weak direct evidence—neighbor papers focus on single-model defect detection (e.g., GPR image recognition, Impact-Echo neural networks), not multi-model ensembles.
- **Break condition:** If all models share systematic biases from common training data, cross-referencing fails to catch shared errors.

### Mechanism 2
- **Claim:** Role-assigned, context-rich prompts elicit technically grounded outputs from general-purpose multimodal LLMs.
- **Mechanism:** The prompt explicitly assigns the model a "structural engineer" role, provides coordinate context (axes in feet), and requests specific outputs (condition description, repair needs, problem area identification). This constrains the output space toward domain-relevant responses.
- **Core assumption:** LLMs can transfer general visual reasoning to technical contour map interpretation when properly scaffolded by context.
- **Evidence anchors:**
  - [section IV.C] "The original prompt was intended to provide a simple description, but it underwent several iterative refinement steps to ultimately produce complete and relevant responses"
  - [appendix B] Prompt explicitly: "You are a structural engineer analyzing a bridge... determine if it needs any repairs, and identify which areas of the bridge are in poor condition"
  - [corpus] No direct corpus evidence on prompt engineering for NDE imagery; related work addresses CNN-based captioning for visible damage (Li et al., Chun et al.), not LLM prompting strategies.
- **Break condition:** When contour map conventions differ significantly from natural images in pre-training distributions, even optimized prompts may fail to extract meaningful technical content.

### Mechanism 3
- **Claim:** Hierarchical summarization consolidates multi-model outputs into actionable assessments while filtering noise.
- **Mechanism:** Four top-performing image captioning models generate parallel interpretations; a second-stage LLM (ChatGPT-4, Claude 3.5 Sonnet) synthesizes these into unified condition reports with prioritized findings and recommendations.
- **Core assumption:** Summarization models can identify and weight correct information across conflicting model outputs without ground truth.
- **Evidence anchors:**
  - [section III.C] "The summarization model integrates the interpretations from different models' outputs, prioritizing critical information about the bridge condition"
  - [table II] ChatGPT-4 achieved perfect scores (5/5) on Completeness, In-depth Coverage, and Formatting & Presentation
  - [corpus] LUMIR paper shows LLMs reasoning over spectroscopy data, suggesting transferability to technical signal interpretation—but not directly validating hierarchical summarization.
- **Break condition:** If input models produce plausible but incorrect interpretations at high rates, the summarizer may amplify confident errors rather than filter them.

## Foundational Learning

- **Concept:** NDE contour map interpretation
  - **Why needed here:** LLMs must understand what color gradients, axes, and spatial patterns represent in each NDE modality (GPR depth, ER corrosion risk, IE delamination, USW modulus). Without this, outputs are surface-level descriptions.
  - **Quick check question:** Can you explain why lower electrical resistivity (blue regions) correlates with higher corrosion risk in reinforced concrete?

- **Concept:** Vision-language model capabilities and limits
  - **Why needed here:** Not all multimodal LLMs handle technical graphics equally. The paper shows 4/9 models scored 4+, meaning model selection critically affects output quality.
  - **Quick check question:** What distinguishes a model trained on natural images from one capable of interpreting abstract data visualizations like contour maps?

- **Concept:** Evaluation without ground truth
  - **Why needed here:** The study uses expert qualitative ratings (Yes/No across four metrics) rather than automated metrics, reflecting the challenge of validating NDE interpretations.
  - **Quick check question:** Why would BLEU or other captioning metrics be inappropriate for evaluating technical NDE descriptions?

## Architecture Onboarding

- **Component map:** Input layer (5 NDE contour maps) -> Processing layer (9 parallel image captioning LLMs) -> Filtering layer (top 4 models) -> Synthesis layer (5 summarization LLMs) -> Output layer (condition assessment)

- **Critical path:**
  1. Prompt engineering for technical interpretation (determines extraction quality)
  2. Model selection for image captioning (4/9 models viable)
  3. Summarization model selection (ChatGPT-4, Claude 3.5 Sonnet outperform open-source options)

- **Design tradeoffs:**
  - Proprietary models (ChatGPT-4, Claude 3.5 Sonnet) outperform open-source but introduce cost, data privacy, and availability dependencies
  - Multi-model redundancy increases robustness but adds latency and complexity
  - Expert evaluation ensures quality but doesn't scale; automated metrics remain unsolved

- **Failure signatures:**
  - Models describing visual elements (colors, shapes) without technical meaning → prompt/context insufficient
  - High variance across models on same image → contour conventions outside training distribution
  - Summarizer producing generic advice → captioning outputs too shallow or contradictory

- **First 3 experiments:**
  1. **Baseline calibration:** Run all 9 captioning models on a single NDE map with the standardized prompt; score outputs using the 4-metric rubric to replicate the paper's model ranking.
  2. **Prompt ablation:** Test the same model with vs. without role assignment ("structural engineer") and coordinate context; measure impact on technical specificity scores.
  3. **Summarization stress test:** Feed the summarizer deliberately contradictory captioning outputs (e.g., one model says "good condition," another says "severe delamination"); assess whether the summary flags uncertainty or produces confident errors.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How does LLM interpretation performance generalize across multiple bridges with varying structural conditions, ages, and NDE data quality?
- **Basis in paper:** [explicit] The study analyzed only one bridge ("For this pilot study, we looked at only one bridge located in Mississippi") from 38 available in the FHWA database.
- **Why unresolved:** The pilot study scope was intentionally narrow, testing only five NDE contour maps from a single structure. Whether findings transfer to bridges with different deterioration patterns, construction types, or data characteristics remains unknown.
- **What evidence would resolve it:** A multi-bridge study evaluating the same LLMs across diverse structures, comparing performance metrics (Relevance, Usefulness, Coverage, Specificity) to determine if model rankings remain consistent.

### Open Question 2
- **Question:** What is the quantitative accuracy of LLM-generated interpretations compared to ground truth expert assessments and actual bridge conditions?
- **Basis in paper:** [inferred] The evaluation used qualitative ratings from "researchers and one domain expert" with binary assessments, but did not validate LLM outputs against verified defect locations or established inspection reports.
- **Why unresolved:** The paper establishes that LLMs produce coherent, plausible-sounding analyses but does not confirm whether identified defects actually exist or whether recommendations align with established engineering assessments.
- **What evidence would resolve it:** Comparison of LLM-identified defect locations and severity against verified ground truth data (e.g., confirmed delamination, corrosion, or cover depth issues from destructive testing or multi-expert consensus reports).

### Open Question 3
- **Question:** Can domain-specific fine-tuning of vision-language models improve NDE contour map interpretation compared to general-purpose LLMs?
- **Basis in paper:** [inferred] The study tested only general-purpose models (ChatGPT-4, Claude 3.5 Sonnet, CogVLM2, etc.) without NDE-specific training, yet noted performance disparities even among top performers (ShareGPT4V scored lower on Specificity).
- **Why unresolved:** It remains unclear whether the superior performance of proprietary models stems from general capability or could be replicated through targeted fine-tuning of open-source models on NDE datasets.
- **What evidence would resolve it:** Comparative study fine-tuning open-source models (e.g., CogVLM2, Florence-2) on curated NDE contour maps with expert annotations, measuring improvements in Coverage and Specificity metrics.

## Limitations
- Single bridge case study limits generalizability across different structural conditions and NDE modalities
- Reliance on qualitative expert ratings rather than automated metrics introduces subjectivity
- Proprietary models (ChatGPT-4, Claude 3.5 Sonnet) outperform open-source alternatives, creating cost and accessibility barriers

## Confidence
- **High Confidence:** The multi-model parallel processing approach (Mechanism 1) is technically sound and the methodology for generating structured outputs is well-defined
- **Medium Confidence:** The prompt engineering strategy (Mechanism 2) shows promise but lacks systematic ablation testing to isolate effective components
- **Low Confidence:** The hierarchical summarization approach (Mechanism 3) lacks validation against ground truth data, making it difficult to assess whether amplified findings are correct

## Next Checks
1. **Ground truth validation:** Test the pipeline on synthetic NDE maps with known defects to measure accuracy against actual conditions
2. **Cross-bridge generalization:** Apply the methodology to 3-5 additional bridges with varying structural issues to assess robustness across different defect patterns
3. **Open-source performance gap:** Conduct controlled experiments comparing proprietary vs. open-source models on identical prompts to quantify the exact performance differential and identify specific failure modes