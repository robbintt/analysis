---
ver: rpa2
title: 'FactGuard: Event-Centric and Commonsense-Guided Fake News Detection'
arxiv_id: '2511.10281'
source_url: https://arxiv.org/abs/2511.10281
tags:
- news
- fake
- detection
- topic-content
- real
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: FACTGUARD introduces an LLM-driven framework for fake news detection
  that extracts event-centric content to reduce writing-style bias and incorporates
  a dynamic usability module to adaptively assess LLM advice reliability. By combining
  commonsense reasoning with style-debiased topic extraction, it outperforms existing
  methods on Weibo21 and GossipCop datasets, achieving the best macro-F1, accuracy,
  and F1 scores for both real and fake news.
---

# FactGuard: Event-Centric and Commonsense-Guided Fake News Detection

## Quick Facts
- arXiv ID: 2511.10281
- Source URL: https://arxiv.org/abs/2511.10281
- Authors: Jing He; Han Zhang; Yuanhui Xiao; Wei Guo; Shaowen Yao; Renyang Liu
- Reference count: 40
- Key outcome: Introduces FACTGUARD framework combining event-centric extraction and commonsense reasoning for fake news detection, achieving state-of-the-art performance on Weibo21 and GossipCop datasets while maintaining interpretability and efficiency through distillation.

## Executive Summary
FACTGUARD addresses fake news detection by extracting event-centric content to reduce writing-style bias and incorporating a dynamic usability module to adaptively assess LLM advice reliability. The framework combines commonsense reasoning with style-debiased topic extraction, outperforming existing methods on Weibo21 and GossipCop datasets. A distilled lightweight variant, FACTGUARD-D, enables efficient deployment in resource-constrained scenarios while maintaining strong performance.

## Method Summary
The FACTGUARD framework processes news through three stages: original news text, LLM-extracted topic-content (removing stylistic noise), and LLM-generated commonsense rationales. It employs dual cross-attention mechanisms to fuse content and rationale representations, with a rationale usability evaluator dynamically weighting LLM advice based on internal contradictions. The model is trained using a multi-task loss combining classification, usability, and text reconstruction objectives. For deployment efficiency, FACTGUARD-D distills the teacher model's reasoning into a lightweight student network using feature simulation.

## Key Results
- Achieves best macro-F1, accuracy, and F1 scores for both real and fake news on Weibo21 and GossipCop datasets
- FACTGUARD-D variant provides 10x+ speedup with only ~1.3% accuracy drop on Weibo21
- Dynamic usability mechanism improves reliability by identifying and filtering unreliable LLM advice
- Event-centric extraction successfully reduces writing-style bias vulnerability

## Why This Works (Mechanism)

### Mechanism 1: Style Debiasing through Event-Centric Extraction
The framework extracts event-centric content to reduce dependence on writing style, mitigating bias where adversaries imitate authentic reporting styles. An LLM strips stylistic noise from raw news text, isolating the core topic and content for comparison against commonsense rationales rather than stylometric features. This approach assumes the LLM can successfully disentangle event core from stylistic presentation without hallucination.

### Mechanism 2: Dynamic Usability Evaluation
A dual-branch usability module improves reliability by dynamically weighting LLM advice based on internal contradictions or confidence levels. One MLP branch reduces influence when LLM's direct detection capability is weak, while another increases influence when commonsense reasoning identifies specific contradictions. This adaptively gates the fusion of features, assuming LLMs provide valuable signals when detecting contradictions even if their binary classification is unreliable.

### Mechanism 3: Knowledge Distillation for Efficient Deployment
The distilled variant FACTGUARD-D internalizes teacher model's reasoning into a lightweight student through feature simulation. A Transformer encoder in the student model is trained to mimic the feature representations of full FACTGUARD using MSE loss, eliminating the need for real-time LLM invocation during inference. This assumes the student network has sufficient capacity to approximate complex reasoning mappings learned by the teacher.

## Foundational Learning

- **Cross-Attention Mechanisms**: Needed to fuse distinct modalities (Topic-Content and Commonsense Rationale) by allowing content to weigh the importance of rationale tokens. Quick check: In CA(Q,K,V), which modality serves as Query when content attends to rationale?

- **Knowledge Distillation (Teacher-Student)**: Required to transfer "dark knowledge" from resource-rich FACTGUARD to efficient FACTGUARD-D. Quick check: Why does MSE between teacher and student features preserve performance better than training student only on ground truth labels?

- **Prompt Engineering for Semantic Extraction**: Essential for quality of topic-content extraction, as results depend entirely on LLM's ability to follow instructions separating topic and style. Quick check: How does cosine similarity > 0.9 constraint function as guardrail against hallucination during extraction?

## Architecture Onboarding

- **Component map**: Original News -> BERT/RoBERTa Encoder -> Dual Cross-Attention (Topic-Content â†” Rationale) -> Rationale Usability Evaluator (Dual-branch MLP) -> Feature Fusion -> MLP Classifier
- **Critical path**: The Rationale Usability Evaluator is the critical control point. If weights collapse to zero, system defaults to standard classifier; if saturated, over-relies on potentially noisy LLM rationales.
- **Design tradeoffs**: Accuracy vs. Cost (FACTGUARD higher Macro-F1 but incurs LLM inference cost; FACTGUARD-D sacrifices ~1.3% accuracy for 10x+ speedup). Dual vs. Single Branch (dual branch captures nuance of when LLM is wrong vs. provides useful contradiction signals).
- **Failure signatures**: Low similarity scores (<0.8/0.9) indicating hallucination, high entropy drop suggesting critical nuance stripped, weight saturation indicating broken adaptive mechanism.
- **First 3 experiments**: 1) Extraction Quality Audit: plot cosine similarity and Shannon entropy distributions to ensure style debiasing safety. 2) Ablation on Usability: compare FACTGUARD vs. FACTGUARD w/o llm-usability to quantify dynamic weighting gain. 3) Distillation Gap Analysis: compare teacher and student confusion matrices on hard cases to verify conflict resolution internalization.

## Open Questions the Paper Calls Out

- Enhancing interpretability of usability evaluation module to improve transparency and credibility for end-users and fact-checkers.
- Considering benchmark data contamination of employed LLMs, as DeepSeek-R1-Distill-Llama-8B and SOLAR-10.7B may have been pre-trained on GossipCop and Weibo21 data.
- Extending cross-domain adaptation across emerging platforms and multimodal signals, as current framework operates exclusively on text.
- Evaluating robustness across diverse LLM architectures with varying reasoning capabilities and hallucination rates, as only two specific LLMs were tested.

## Limitations

- Framework's effectiveness depends on LLM's ability to reliably separate event facts from stylistic cues, with no reported failure rate for quality threshold checks.
- Reliance on external commonsense rationales from ARG (Hu et al. 2024) introduces dependency without full specification of prompt engineering and quality control.
- Distilled variant sacrifices ~1.3% accuracy on Weibo21, which may be significant in high-stakes applications despite enabling faster inference.

## Confidence

- **High Confidence**: Dual cross-attention for fusing content and rationale, and distillation approach for creating FACTGUARD-D are well-established techniques with reproducible implementation details.
- **Medium Confidence**: Effectiveness of Rationale Usability Evaluator for dynamically weighting LLM advice is supported by ablation results but specific conditions for dual-branch superiority not fully explored.
- **Medium Confidence**: Style-debiasing claim is theoretically sound but lacks direct ablation showing performance drop when using raw news versus extracted topic-content.

## Next Checks

1. **Extraction Quality Audit**: Run LLM extraction prompts on validation set and plot cosine similarity and Shannon entropy distributions to ensure style debiasing safety and quantify quality threshold failure rate.

2. **Ablation on Usability**: Compare FACTGUARD vs. FACTGUARD w/o llm-usability to quantify specific performance gain provided by dynamic weighting mechanism and understand its contribution to overall accuracy.

3. **Distillation Gap Analysis**: Compare teacher and student confusion matrices on hard cases (where LLM judgment conflicts with ground truth) to verify student effectively internalized conflict resolution logic and identify systematic errors introduced by distillation.