---
ver: rpa2
title: Evaluating Large Language Models for Line-Level Vulnerability Localization
arxiv_id: '2404.00287'
source_url: https://arxiv.org/abs/2404.00287
tags:
- llms
- vulnerability
- code
- vulnerable
- software
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents the first comprehensive evaluation of large
  language models (LLMs) for line-level vulnerability localization (AVL). The authors
  assess 19 state-of-the-art LLMs across three training paradigms (prompting, discriminative
  fine-tuning, and generative fine-tuning) on C/C++ and smart contract vulnerability
  datasets.
---

# Evaluating Large Language Models for Line-Level Vulnerability Localization

## Quick Facts
- arXiv ID: 2404.00287
- Source URL: https://arxiv.org/abs/2404.00287
- Reference count: 40
- Primary result: Discriminative fine-tuning achieves 63.8% F1-score for line-level vulnerability localization

## Executive Summary
This paper presents the first comprehensive evaluation of large language models (LLMs) for line-level vulnerability localization (AVL). The authors assess 19 state-of-the-art LLMs across three training paradigms (prompting, discriminative fine-tuning, and generative fine-tuning) on C/C++ and smart contract vulnerability datasets. Their results show that discriminative fine-tuning substantially outperforms existing learning-based AVL methods, achieving up to 63.8% F1-score. The study identifies critical challenges with input length limitations and unidirectional context in LLMs, proposing sliding window and right-forward embedding strategies that improve performance by up to 29.7% F1-score. The authors also reveal that while fine-tuned LLMs generalize well across common vulnerability types, they struggle significantly with newly discovered vulnerabilities containing unfamiliar patterns.

## Method Summary
The authors systematically evaluated 19 state-of-the-art LLMs using three training paradigms: prompt-based zero-shot learning, discriminative fine-tuning, and generative fine-tuning. They constructed evaluation datasets from C/C++ vulnerabilities in GitHub repositories and smart contract vulnerabilities from Etherscan. The study implemented innovative strategies to address LLM limitations including sliding window processing to handle input length constraints and right-forward embedding to mitigate unidirectional context issues. Models were assessed on their ability to identify vulnerable lines within individual functions, with performance measured using precision, recall, and F1-score metrics. The evaluation included both cross-dataset testing for generalization assessment and cross-vulnerability-type testing to measure performance on different CWE categories.

## Key Results
- Discriminative fine-tuning achieved up to 63.8% F1-score, outperforming existing learning-based AVL methods
- Sliding window and right-forward embedding strategies improved performance by up to 29.7% F1-score
- Models showed strong generalization across common vulnerability types but struggled significantly with newly discovered vulnerabilities containing unfamiliar patterns
- Input length limitations and unidirectional context were identified as critical bottlenecks, with performance degrading when input exceeded model context windows

## Why This Works (Mechanism)
The effectiveness of discriminative fine-tuning stems from its ability to leverage the rich semantic understanding of LLMs while optimizing specifically for vulnerability classification tasks. Unlike prompt-based approaches that rely on zero-shot reasoning, discriminative fine-tuning trains models to explicitly distinguish between vulnerable and non-vulnerable code patterns. The sliding window strategy addresses the fundamental constraint of fixed context windows in transformers by processing code in overlapping segments, while right-forward embedding preserves positional information that would otherwise be lost in bidirectional attention mechanisms. These architectural modifications enable LLMs to maintain semantic coherence across code boundaries and capture long-range dependencies critical for vulnerability detection.

## Foundational Learning
- **Vulnerability classification vs. generation**: LLMs must be trained to classify code as vulnerable rather than generate fixes, requiring different optimization objectives
- **Context window limitations**: Transformer-based models have fixed input lengths that constrain analysis of large functions or files
- **Bidirectional vs. unidirectional attention**: Standard transformers use bidirectional attention, but vulnerability analysis often benefits from maintaining directional flow of program execution
- **Fine-tuning paradigms**: Different training approaches (prompting, discriminative, generative) yield substantially different performance characteristics for security tasks
- **Cross-dataset generalization**: Models trained on one vulnerability corpus may not perform well on previously unseen vulnerability patterns
- **Code vs. natural language processing**: Source code has unique structural properties requiring specialized tokenization and embedding strategies

## Architecture Onboarding

Component map: Input code function -> Tokenizer -> Sliding window processor -> Right-forward embedding -> LLM backbone -> Classification head -> Vulnerability score

Critical path: The model processes code through sliding windows to handle length constraints, applies right-forward embeddings to preserve execution context, then uses the fine-tuned LLM to classify each line as vulnerable or non-vulnerable. The classification head outputs probability scores that are aggregated to identify the most likely vulnerable lines.

Design tradeoffs: The sliding window approach introduces redundancy and computational overhead but enables processing of arbitrarily long functions. Right-forward embedding sacrifices some bidirectional context understanding to maintain execution flow directionality. The choice between discriminative and generative fine-tuning involves a tradeoff between precise classification accuracy and the ability to generate explanatory content.

Failure signatures: Models struggle with vulnerabilities requiring cross-function analysis, boundary condition errors, and recently discovered vulnerability patterns. Performance degrades significantly when input exceeds context window sizes, and models exhibit high false positive rates on code containing similar but non-vulnerable patterns.

First experiments:
1. Compare discriminative fine-tuning performance against prompt-based zero-shot learning on a held-out test set
2. Evaluate sliding window performance with different overlap percentages (25%, 50%, 75%) to optimize accuracy vs. computational cost
3. Test cross-dataset generalization by training on C/C++ vulnerabilities and evaluating on smart contract vulnerabilities

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How does incorporating interprocedural or cross-file context via retrieval mechanisms impact the performance of LLM-based vulnerability localization?
- **Basis in paper:** [explicit] The authors state in Section V.A.2 that operating on isolated functions restricts available program information and that "Future AVL systems therefore require mechanisms that can supply LLMs with richer structural and semantic information."
- **Why unresolved:** The study limited inputs to isolated functions to benchmark against existing methods (e.g., LineVD, IVDetect) which rely on local scope. Consequently, dependencies like allocationâ€“deallocation mismatches across multiple functions were unobservable.
- **What evidence would resolve it:** A comparative evaluation where models are augmented with a retrieval component (e.g., call-graph expansion) to process wider context windows, specifically targeting vulnerabilities requiring cross-function data flow analysis.

### Open Question 2
- **Question:** Can dynamic analysis techniques (e.g., taint tracking or symbolic execution) generate ground-truth labels that improve model training accuracy over static patch-based heuristics?
- **Basis in paper:** [explicit] In Section V.B.1, the authors suggest that "More robust ground-truth construction may be achieved by augmenting patch-based heuristics with dynamic taint tracking... or automated test generation."
- **Why unresolved:** The current ground truth relies on static dependency propagation (Rule 2), which cannot fully capture "deeper causal mechanisms" or implicit error conditions, leading to potential label noise.
- **What evidence would resolve it:** A study comparing models trained on static labels versus those trained on labels refined by dynamic analysis, specifically measuring the reduction in false positive rates during training.

### Open Question 3
- **Question:** What is the efficacy of Chain-of-Thought (CoT) prompting or counterfactual patch reasoning in improving LLM detection of semantic vulnerabilities, such as boundary condition errors?
- **Basis in paper:** [explicit] Section V.B.5 notes that failures in boundary-condition reasoning (CWE-119, CWE-125) suggest models treat localization as token classification rather than semantic reasoning. The authors propose that "Techniques such as Chain-of-Thought prompting... offer promising direction."
- **Why unresolved:** The current discriminative fine-tuning approach optimizes for classification labels but does not force the model to articulate the causal chain or "explain why a line may cause unsafe behavior."
- **What evidence would resolve it:** An evaluation of fine-tuned models utilizing reasoning-enhanced prompting strategies, specifically analyzing performance improvements on the top-10 CWEs identified as difficult in Section IV.C.

## Limitations
- Evaluation limited to C/C++ and smart contract (Solidity) vulnerabilities, with uncertain generalizability to other programming languages
- Datasets may not fully represent real-world vulnerability diversity, particularly for zero-day vulnerabilities with novel patterns
- Sliding window and right-forward embedding strategies introduce computational overhead that was not thoroughly characterized
- Focus on individual line-level localization does not address practical needs for understanding vulnerability context across multiple lines or functions

## Confidence
- **High confidence**: Discriminative fine-tuning performance advantages (63.8% F1-score) are well-supported by experimental results across multiple models and datasets
- **Medium confidence**: Generalization claims across common vulnerability types, as limited by dataset scope and potential selection bias
- **Medium confidence**: Sliding window and right-forward embedding effectiveness, though computational cost analysis is incomplete

## Next Checks
1. Evaluate the proposed approaches on additional programming languages (Java, Python, JavaScript) and vulnerability datasets to assess cross-language generalizability
2. Conduct ablation studies to quantify the computational overhead introduced by sliding window and right-forward embedding strategies under various window sizes
3. Test model performance on actively maintained open-source projects with known vulnerability histories to validate real-world applicability and assess false positive rates in production scenarios