---
ver: rpa2
title: 'Exploring Adversarial Watermarking in Transformer-Based Models: Transferability
  and Robustness Against Defense Mechanism for Medical Images'
arxiv_id: '2506.06389'
source_url: https://arxiv.org/abs/2506.06389
tags:
- adversarial
- image
- watermarking
- medical
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates adversarial watermarking vulnerabilities
  in Vision Transformers (ViTs) for medical image classification, focusing on skin
  disease diagnosis. The study employs Projected Gradient Descent (PGD) to generate
  adversarial perturbations and examines transferability to CNNs, as well as robustness
  under adversarial training.
---

# Exploring Adversarial Watermarking in Transformer-Based Models: Transferability and Robustness Against Defense Mechanism for Medical Images

## Quick Facts
- arXiv ID: 2506.06389
- Source URL: https://arxiv.org/abs/2506.06389
- Reference count: 25
- Primary result: ViTs are significantly more vulnerable to adversarial attacks than CNNs, with accuracy dropping from 94.4% to 27.6% under attack

## Executive Summary
This paper investigates adversarial watermarking vulnerabilities in Vision Transformers (ViTs) for medical image classification, focusing on skin disease diagnosis. The study employs Projected Gradient Descent (PGD) to generate adversarial perturbations and examines transferability to CNNs, as well as robustness under adversarial training. Experiments reveal that ViTs are significantly more vulnerable to adversarial attacks than traditional CNNs, with accuracy dropping from 94.4% to 27.6% under attack. In contrast, ResNet-50 and VGG16 show greater resilience with 70.0% and 54.4% accuracy, respectively. Adversarial training substantially improves ViT robustness, recovering accuracy to 90.0%, while ResNet-50 and VGG16 improve to 81.6% and 82.6%. These findings highlight the need for robust defense mechanisms in medical imaging applications to ensure reliable diagnoses and mitigate security risks. Future work will explore dynamic adversarial training and evaluate ViT performance on larger, more diverse datasets.

## Method Summary
The study trains Vision Transformer, ResNet-50, and VGG16 models on a 5-class skin disease dataset (Atopic dermatitis, Eczema, Herpes, Nevus, Melanoma) with images resized to 224×224 and augmented with random cropping and flipping. Baseline models are trained for 20 epochs using Adam optimizer (lr=0.0001, batch size 32). PGD is used to generate adversarial examples by iteratively maximizing classification loss within bounded perturbations. The study evaluates model vulnerability by measuring accuracy degradation on adversarial examples and tests adversarial training effectiveness by retraining models with PGD-generated samples mixed into the training data. Transferability of attacks between ViTs and CNNs is also examined.

## Key Results
- ViT accuracy drops from 94.4% to 27.6% under PGD attack, while ResNet-50 and VGG16 maintain 70.0% and 54.4% accuracy respectively
- Adversarial training recovers ViT robustness to 90.0% accuracy, a +62.4% improvement
- ResNet-50 and VGG16 also benefit from adversarial training, improving to 81.6% and 82.6% accuracy
- PGD-generated adversarial examples successfully transfer between ViTs and CNNs, demonstrating cross-model vulnerability

## Why This Works (Mechanism)

### Mechanism 1
PGD generates effective adversarial perturbations by iteratively maximizing the model's loss function while constraining perturbations within an imperceptible bound. The update rule computes the gradient of the loss with respect to the input, identifies the perturbation direction that most increases classification error, applies a small step in that direction, and projects the result back into an allowed bound. This process repeats to create inputs visually near-originals that cause misclassification.

### Mechanism 2
ViTs are more vulnerable to adversarial attacks than CNNs due to their global self-attention mechanism, which processes all image patches simultaneously without hierarchical feature extraction. Unlike CNNs that build hierarchical features through local convolutions, ViTs process patches in parallel. This global attention makes them sensitive to small distributed perturbations that alter inter-patch relationships, causing attention-map misrepresentations.

### Mechanism 3
Adversarial training substantially improves ViT robustness by exposing the model to adversarial examples during training, enabling it to learn more robust representations. Incorporating PGD-generated adversarial examples into training forces the model to classify both clean and perturbed images correctly, shifting decision boundaries to be less sensitive to small input changes. ViTs exhibited faster loss convergence during adversarial training compared to CNNs, suggesting rapid adaptation capacity.

## Foundational Learning

- **Concept**: Projected Gradient Descent (PGD) for Adversarial Attacks
  - **Why needed here**: PGD is the core attack-generation method used throughout the paper. Understanding how iterative gradient-based perturbations work is essential to interpret attack results and defense effectiveness.
  - **Quick check question**: Given a model with loss J(θ, x, y) and input x, in which direction should x be perturbed to maximize misclassification probability?

- **Concept**: Self-Attention in Vision Transformers
  - **Why needed here**: The paper's central vulnerability hypothesis centers on how ViT's global self-attention differs from CNNs' local convolutions. Understanding global vs. local feature processing explains the robustness gap.
  - **Quick check question**: How does processing image patches globally rather than hierarchically affect a model's sensitivity to distributed perturbations?

- **Concept**: Adversarial Training as Defense
  - **Why needed here**: The paper's primary defense mechanism is adversarial training. Understanding how exposure to attacks during training builds robustness is crucial for practical implementation.
  - **Quick check question**: Why might including adversarial examples in training data change a model's decision boundaries compared to training on clean data only?

## Architecture Onboarding

- **Component map**: Input preprocessing -> ViT patch embedding -> Transformer blocks (self-attention + feed-forward) -> Classification head -> PGD attack module -> Adversarial training pipeline

- **Critical path**: 1) Train baseline models on clean images → 2) Generate adversarial examples using PGD → 3) Evaluate vulnerability via accuracy drop → 4) Apply adversarial training → 5) Measure robustness recovery

- **Design tradeoffs**: ViT vs. CNN baseline: ViT achieves highest clean accuracy (94.4%) but worst adversarial robustness (27.6%); CNNs trade some clean performance for stronger inherent resistance. Attack strength vs. imperceptibility: Larger ε increases attack potency but risks visible artifacts; α controls convergence speed. Adversarial training intensity: More adversarial epochs can improve robustness but may reduce clean accuracy or overfit to attack patterns. Dataset scope: Experiments use a small dataset (5 skin disease classes); generalization to larger, more diverse datasets is noted as future work.

- **Failure signatures**: Sudden accuracy collapse (e.g., ViT from 94.4% to 27.6%) signals successful attack penetration through global attention vulnerability. Slow or unstable adversarial training loss convergence may indicate hyperparameter issues. Overfitting to adversarial patterns: if validation robustness improves but test performance degrades, adjust regularization or training mix. Visible perturbation artifacts indicate overly large ε bounds.

- **First 3 experiments**:
  1. Baseline accuracy comparison: Train ViT, ResNet-50, VGG16 on clean images (5 classes) for 20 epochs (Adam, lr=0.0001, batch=32); report validation accuracy.
  2. Adversarial vulnerability assessment: Generate PGD adversarial examples from each trained model; evaluate accuracy degradation with fixed perturbation parameters across architectures.
  3. Adversarial training effectiveness: Retrain all models with augmented data (clean + adversarial examples); measure robustness recovery (target ViT ~90%) and compare improvement deltas across architectures.

## Open Questions the Paper Calls Out

### Open Question 1
How do dynamic adversarial training strategies compare to static methods in maintaining Vision Transformer (ViT) robustness on medical images? The current study only evaluated robustness using a standard adversarial training setup, leaving the potential benefits of dynamic training regimes unexplored. A comparative study measuring the classification accuracy of ViTs on adversarial medical images when trained using dynamic strategies versus the static PGD-based method would resolve this.

### Open Question 2
Does the observed high robustness of ViTs following adversarial training generalize to larger and more diverse medical imaging datasets? The 90% recovery rate reported may be specific to the 5-class dataset used and might not scale to real-world clinical diversity containing rare conditions or higher variability. Replicating the adversarial training experiments on a large-scale benchmark would verify if the 62.4% accuracy improvement is maintained.

### Open Question 3
Is the ViT robustness observed against Projected Gradient Descent (PGD) effective against other adversarial attack algorithms? The current study only tested against PGD attacks, and defense mechanisms often overfit to specific attack types. Stress-testing the adversarially trained ViT model against a suite of different white-box and black-box attack algorithms would assess general defense capability.

## Limitations
- The study uses a relatively small dataset with only 5 skin disease classes, limiting generalizability to broader clinical applications
- Specific PGD hyperparameters (ε, α, iteration count) were not fully disclosed, limiting reproducibility
- Computational requirements and training times for adversarial training were not reported, which could be substantial for medical imaging applications

## Confidence
- **High confidence**: ViT demonstrates significantly higher vulnerability to adversarial attacks compared to CNNs (accuracy drops from 94.4% to 27.6% vs. ResNet-50/VGG16 drops to 70.0%/54.4%). Adversarial training effectively recovers ViT robustness to 90.0%.
- **Medium confidence**: The hypothesis that global self-attention makes ViTs more vulnerable than hierarchical CNNs is supported but not definitively proven—other architectural differences could contribute.
- **Medium confidence**: The claim that ViTs exhibit faster adversarial training convergence suggests efficient adaptation, though direct CNN convergence comparisons are limited.

## Next Checks
1. **Cross-dataset validation**: Test the same attack-defense pipeline on larger, more diverse medical imaging datasets (e.g., HAM10000, DermNet) to assess generalization beyond the 5-class study.
2. **Transferability quantification**: Systematically vary PGD perturbation strength (ε, α) and measure the point at which CNN robustness matches ViT vulnerability to determine if attention mechanisms are the sole vulnerability factor.
3. **Dynamic defense evaluation**: Implement dynamic adversarial training with varying attack patterns per epoch and measure whether this improves generalization compared to static adversarial training.