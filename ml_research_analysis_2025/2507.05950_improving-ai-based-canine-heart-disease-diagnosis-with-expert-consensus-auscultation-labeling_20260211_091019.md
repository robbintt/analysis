---
ver: rpa2
title: Improving AI-Based Canine Heart Disease Diagnosis with Expert-Consensus Auscultation
  Labeling
arxiv_id: '2507.05950'
source_url: https://arxiv.org/abs/2507.05950
tags:
- heart
- classification
- were
- label
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study addresses label noise in veterinary auscultation data
  by leveraging expert consensus to refine heart murmur classifications in canine
  MMVD. Using five independent assessments per heart sound recording, the authors
  applied a multi-step selection process to reduce label noise and improve dataset
  quality.
---

# Improving AI-Based Canine Heart Disease Diagnosis with Expert-Consensus Auscultation Labeling

## Quick Facts
- arXiv ID: 2507.05950
- Source URL: https://arxiv.org/abs/2507.05950
- Reference count: 29
- Expert consensus labeling reduced label noise and improved MMVD classification sensitivity from 37.71% to 90.98% for mild murmurs.

## Executive Summary
This study addresses label noise in veterinary auscultation data by leveraging expert consensus to refine heart murmur classifications in canine MMVD. Using five independent assessments per heart sound recording, the authors applied a multi-step selection process to reduce label noise and improve dataset quality. The refined dataset of 70 high-quality recordings was used to train AdaBoost, Random Forest, and XGBoost classifiers. XGBoost achieved the strongest performance, with sensitivity for mild murmur detection rising from 37.71% to 90.98%, moderate from 30.23% to 55.81%, and loud/thrilling from 58.28% to 95.09%. Specificity improvements were similarly substantial, highlighting the value of expert-consensus labeling for accurate MMVD classification.

## Method Summary
The study used 140 heart sound recordings from small-breed dogs, each annotated by five veterinary cardiologists. A four-step filtering cascade removed low-quality recordings, healthy samples, borderline cases, and samples with high label disagreement. The remaining 70 recordings underwent majority voting to establish final labels. Heart cycles were segmented using Hilbert envelope analysis and bandpass filtering (50–500 Hz). Time and frequency domain features were extracted for each cycle. The dataset was split by recording ID to prevent leakage, and XGBoost, Random Forest, and AdaBoost classifiers were trained and evaluated.

## Key Results
- XGBoost achieved the highest sensitivity: mild murmurs 90.98%, moderate 55.81%, loud/thrilling 95.09%.
- Specificity improved substantially: mild 98.03%, moderate 95.59%, loud/thrilling 95.70%.
- Label noise reduction via expert consensus improved classification performance across all metrics.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Multiple independent expert assessments systematically reduce label noise in subjective medical annotations.
- **Mechanism:** Each heart sound recording receives 5 independent evaluations from veterinary cardiologists. A 4-step filtering cascade removes: (1) bad quality recordings, (2) healthy samples, (3) borderline cases where two classes each appear ≥2 times, (4) cases with ≥3 different labels. Surviving samples receive majority-voted labels.
- **Core assumption:** Agreement among multiple experts correlates with ground truth; divergent assessments indicate labeling ambiguity rather than valid class overlap.
- **Evidence anchors:**
  - [abstract]: "Using five independent assessments per heart sound recording, the authors applied a multi-step selection process to reduce label noise."
  - [section II.B]: "Upon completion of sample selection Step 4... only those HSR remained for which all experts (or all but one) agreed on the presence of exactly one class of murmur intensity."
  - [corpus]: Weak corpus support—no directly comparable multi-expert veterinary auscultation studies found; neighboring papers focus on single-annotation approaches.
- **Break condition:** If experts share systematic biases (e.g., training from same institution), consensus amplifies error rather than reducing it.

### Mechanism 2
- **Claim:** Heart cycle segmentation expands training data while preserving murmur-intensity labels.
- **Mechanism:** 10-second recordings are bandpass-filtered (50–500 Hz), then segmented into individual heart cycles via S1 peak detection using Hilbert envelope analysis. Each cycle inherits the parent recording's label. Feature vectors (time-domain statistics, frequency-domain spectral features, MFCCs) are extracted per cycle.
- **Core assumption:** Murmur intensity is consistent across all heart cycles within a single recording.
- **Evidence anchors:**
  - [section II.C.1]: "By increasing the number of available samples while reducing their dimensionality, the performance and robustness of the trained classifiers can be enhanced."
  - [section II.C.1]: "Each extracted heart cycle was transformed into a feature vector containing time- and frequency-domain characteristics."
  - [corpus]: Paper ID 148900 (Heart Sound Segmentation Using Deep Learning Techniques) corroborates cycle-based analysis as standard preprocessing.
- **Break condition:** If arrhythmias or recording artifacts cause cycle-to-cycle intensity variation, inherited labels become noisy at the cycle level.

### Mechanism 3
- **Claim:** XGBoost gains disproportionate performance improvements from label noise reduction compared to AdaBoost and Random Forest.
- **Mechanism:** Gradient boosting's sequential error-correction structure makes it sensitive to label noise—each tree fits residuals from previous iterations, so mislabeled samples propagate errors. With noise-reduced labels, this same sensitivity becomes an advantage, allowing precise discrimination.
- **Core assumption:** Label noise (not feature noise or class imbalance) is the primary bottleneck for moderate-murmur classification.
- **Evidence anchors:**
  - [abstract]: "XGBoost achieved the strongest performance... sensitivity for mild murmur detection rising from 37.71% to 90.98%, moderate from 30.23% to 55.81%."
  - [section III.D.2, Table III]: MCC for moderate murmurs improved from -0.037 (random-level) to 0.6029 with noise-reduced labels.
  - [corpus]: Paper ID 14307 (CycleGuardian) reports similar boosting-sensitive behavior in respiratory sound classification.
- **Break condition:** If moderate-murmur sensitivity ceiling is due to acoustic similarity to mild/loud classes rather than labels, further label refinement yields diminishing returns.

## Foundational Learning

- **Concept:** Label noise taxonomy (systematic vs. random, class-conditional vs. instance-dependent)
  - **Why needed here:** The 4-step selection process removes specific noise types (borderline cases = class-conditional ambiguity; bad quality = instance-dependent noise). Understanding which type you're targeting determines whether filtering or relabeling is appropriate.
  - **Quick check question:** If you observe high inter-rater disagreement concentrated in one class, is filtering or relabeling more appropriate?

- **Concept:** Agreement metrics beyond accuracy (Krippendorff's α, Cohen's κ)
  - **Why needed here:** The paper uses Krippendorff's α because it handles multiple raters and ordinal data. Raw agreement percentages would conflate chance agreement with true consensus.
  - **Quick check question:** Why does α increase from 0.46 → 0.68 after filtering? What does this tell you about the removed samples?

- **Concept:** Data leakage via grouped samples
  - **Why needed here:** Heart cycles from the same recording must stay entirely in train OR test—never both. Random splitting at the cycle level inflates performance because adjacent cycles are acoustically similar.
  - **Quick check question:** Your dataset has 328 test cycles from 70 recordings. How do you verify no patient-level overlap exists?

## Architecture Onboarding

- **Component map:**
Raw HSR (10s) -> Bandpass Filter -> S1 Peak Detection -> Cycle Segmentation
      ↓
Feature Extraction (time + frequency) -> Feature Vectors
      ↓
Expert Annotation (×5) -> 4-Step Filtering -> Majority Vote -> Final Labels
      ↓
Train/Test Split (recording-level grouping) -> XGBoost/AdaBoost/RandomForest

- **Critical path:** The label refinement pipeline (annotation -> filtering -> majority vote) is the high-leverage component. A 50% data reduction (140->70 recordings) produced 2–3× sensitivity gains. Do not optimize classifiers before stabilizing labels.

- **Design tradeoffs:**
  - Dataset size vs. label quality: 50% of data discarded to achieve consensus. For rare classes (moderate murmurs: only 10 samples remain), this creates imbalance.
  - Rater redundancy vs. annotation cost: 5 assessments per sample is expensive. The paper's agreement analysis (αA=0.86 vs. αB=0.72) suggests 3 raters may suffice for consistent experts.

- **Failure signatures:**
  - Moderate-murmur sensitivity stuck at 55.81% despite noise reduction -> likely class overlap or feature insufficiency, not a label problem.
  - Validation not performed due to data scarcity -> results may not generalize; treat as pilot study.

- **First 3 experiments:**
  1. **Baseline replication:** Reproduce the 4-step filtering on the raw label matrix. Verify you get 70 HQ samples with matching class distribution (25 mild, 10 moderate, 35 loud).
  2. **Ablation on consensus threshold:** Test what happens if you require 4-of-5 agreement (stricter) vs. 3-of-5 (looser). Measure α and classifier performance at each threshold.
  3. **Class-balanced evaluation:** The moderate class has only 43 test cycles vs. 122–163 for others. Compute per-class metrics and MCC; report whether performance differences are statistically significant or could be explained by sample variance.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can data augmentation or stratified sampling techniques resolve the low sensitivity (55.81%) for "moderate" murmurs caused by class imbalance and labeling subjectivity?
- **Basis in paper:** [explicit] The Discussion notes that the sample selection process exacerbated class imbalance for moderate cases and suggests future studies use data augmentation or collect more high-quality data.
- **Why unresolved:** The rigorous filtering reduced the training dataset to include only 10 moderate samples, significantly limiting the classifier's ability to generalize for this intensity level compared to mild and loud classes.
- **What evidence would resolve it:** Results from a larger, balanced dataset where the sensitivity for moderate murmurs approaches the >90% levels seen for mild and loud classes.

### Open Question 2
- **Question:** Would deep learning models (CNNs or RNNs) outperform the current XGBoost approach by automatically extracting relevant features from raw audio?
- **Basis in paper:** [explicit] The authors suggest that alternative models like CNNs or RNNs might identify hidden patterns better than the manually selected time- and frequency-domain features used in this study.
- **Why unresolved:** The study relied exclusively on handcrafted features fed into ensemble classifiers, leaving the potential of end-to-end deep learning feature extraction unexplored.
- **What evidence would resolve it:** A comparative analysis showing deep learning architectures achieving higher sensitivity and specificity metrics on the same heart sound recordings without manual feature engineering.

### Open Question 3
- **Question:** Are the reported performance gains generalizable, or are they artifacts of the single train-test split used in this pilot study?
- **Basis in paper:** [explicit] The authors state that "this paper did not include a cross-validation step due to limited data availability" and classify the work as a pilot study requiring future validation.
- **Why unresolved:** The small size of the high-quality dataset (70 recordings) and the reliance on a single data split increase the risk of variance in the reported performance metrics.
- **What evidence would resolve it:** A follow-up study utilizing k-fold cross-validation or an external validation dataset that reproduces the high classification metrics (e.g., >90% sensitivity for mild murmurs).

## Limitations

- Small dataset size after filtering (70 recordings) creates significant class imbalance, particularly for moderate murmurs.
- Single-center validation limits generalizability across different stethoscope equipment and patient populations.
- Lack of external validation raises concerns about whether performance gains will hold in broader clinical settings.

## Confidence

- **High confidence:** The label noise reduction methodology and its impact on classifier performance are well-supported by the experimental design and results.
- **Medium confidence:** The specific performance gains (e.g., mild sensitivity rising from 37.71% to 90.98%) are impressive but may be influenced by class imbalance and limited test data.
- **Low confidence:** Claims about the general applicability of this approach to other veterinary auscultation tasks due to the constrained dataset and single-center validation.

## Next Checks

1. Perform external validation on a separate dataset collected from different veterinary centers using varied stethoscope equipment.
2. Conduct ablation studies testing the impact of different consensus thresholds (3-of-5 vs 4-of-5 expert agreement) on dataset size and classifier performance.
3. Apply class-balancing techniques (SMOTE, class weights, or undersampling) to evaluate whether the moderate-murmur performance improvements persist under balanced conditions.