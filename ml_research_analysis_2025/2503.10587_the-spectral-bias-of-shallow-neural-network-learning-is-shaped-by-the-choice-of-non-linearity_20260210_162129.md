---
ver: rpa2
title: The Spectral Bias of Shallow Neural Network Learning is Shaped by the Choice
  of Non-linearity
arxiv_id: '2503.10587'
source_url: https://arxiv.org/abs/2503.10587
tags:
- function
- activation
- training
- functions
- relu
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates how the choice of non-linear activation
  function shapes the implicit bias in neural network learning, focusing on the generalization
  capabilities of overparameterized shallow networks. The authors introduce a reparameterization
  that simplifies the relationship between shallow networks and the Radon transform,
  leading to a new interpretation of the implicit bias as a spectral seminorm that
  penalizes high-frequency components in a manner dependent on the activation function.
---

# The Spectral Bias of Shallow Neural Network Learning is Shaped by the Choice of Non-linearity

## Quick Facts
- arXiv ID: 2503.10587
- Source URL: https://arxiv.org/abs/2503.10587
- Reference count: 40
- Primary result: A reparameterization of shallow neural networks into "Radon Spline" coordinates allows explicit calculation of the implicit spectral bias imposed by any activation function.

## Executive Summary
This paper investigates how the choice of non-linear activation function shapes the implicit bias in neural network learning, focusing on the generalization capabilities of overparameterized shallow networks. The authors introduce a reparameterization that simplifies the relationship between shallow networks and the Radon transform, leading to a new interpretation of the implicit bias as a spectral seminorm that penalizes high-frequency components in a manner dependent on the activation function. They derive explicit formulas for the implicit bias induced by a broad class of activation functions, including the design of activation functions that impose desired Fourier-space penalties. In the adaptive regime, they demonstrate the existence of local dynamical attractors that facilitate the formation of clusters of hyperplanes where the input to a neuron's activation function is zero, yielding alignment between many neurons' response functions. The theoretical results are confirmed with simulations, providing insights into the mechanisms underlying the generalization capabilities of overparameterized neural networks and offering potential pathways for designing more efficient and robust models.

## Method Summary
The authors reparameterize shallow neural networks into "Radon Spline" coordinates, where each neuron is characterized by orientation $\xi$, signed distance $\gamma$, scale $\mu$, and rescaling parameter $\omega$. This allows them to leverage the Radon transform and Central Slice Theorem to map the network's implicit bias into the frequency domain. They analyze two training regimes: (1) Kernel regime where input weights are frozen and only output weights are trained, and (2) Adaptive regime where all parameters are trained. The spectral penalty $\rho(k)$ is shown to be the product of an architectural factor ($k^{D-1}$) and an activation factor ($1/|F[\phi](k)|^2$). They derive formulas for the spectral penalty induced by various activation functions and demonstrate how to design custom activations with desired spectral properties.

## Key Results
- The implicit bias of shallow networks in the kernel regime can be interpreted as minimizing a spectral seminorm that penalizes high-frequency components based on the activation function's Fourier transform.
- The reparameterization into Radon Spline coordinates reveals that gradient descent in the adaptive regime drives neurons to cluster their hyperplanes along orientations determined by data correlations.
- Custom activation functions can be designed to impose specific Fourier-space penalties by inverting the relationship between activation and spectral penalty.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** The implicit bias of a shallow network in the kernel regime acts as a frequency-domain regularizer, penalizing high-frequency components based on the activation function's Fourier transform.
- **Mechanism:** By reparameterizing the network into "Radon Spline" coordinates (orientation $\xi$, offset $\gamma$, scale $\mu$), the authors show the learning problem is equivalent to minimizing a "Radon seminorm." Using the Central Slice Theorem, this norm translates into Fourier space. The total spectral penalty $\rho(k)$ is the product of an architectural factor ($k^{D-1}$) and an activation factor ($1/|F[\phi](k)|^2$). This creates a "spectral bias" where frequencies heavily penalized by $\rho(k)$ are suppressed in the learned function.
- **Core assumption:** The network operates in the **kernel regime** (weights $\xi, \gamma$ fixed at initialization; only $\mu$ is trained) or infinite-width limit.
- **Evidence anchors:**
  - [Abstract] "...interpret the kernel regime's inductive bias as minimizing a spectral seminorm that penalizes high-frequency components..."
  - [Section III-A] Equation (8) explicitly defines the objective as integrating the product of the architecture's and activation's spectral penalty factors against the target's spectrum.
  - [Corpus] Related work on "Spectral Bottleneck" (arXiv:2509.09719) supports the sensitivity of frequency fitting to architecture and initialization.
- **Break condition:** This mechanism fails if the network enters the **adaptive regime** where $\xi$ and $\gamma$ change significantly, or if the target function's primary frequency content lies exactly where the penalty $\rho(k)$ is smallest (which is rare for typical smooth priors but possible for "noisy" targets).

### Mechanism 2
- **Claim:** In the adaptive regime, gradient descent drives neurons to cluster their hyperplanes (breakplanes) along specific orientations determined by data correlations.
- **Mechanism:** The reparameterized dynamics reveal that neuron hyperplanes follow a "velocity flow" vector field $v_t$. This field has local attractors (sinks) at orientations $\xi^*$ that maximize correlation with the residual error. Neurons initialized near these basins flow toward the attractors, causing their hyperplanes to align and form clusters. This alignment effectively increases the "representational cost" (amplitude $\mu$) along those orientations to fit the data.
- **Core assumption:** The learning rate is sufficiently small to track the gradient flow, and the "rich" adaptive regime is active (breakplanes are not frozen).
- **Evidence anchors:**
  - [Section IV] "...breakplanes flow towards the moving target... leading to breakplanes forming 'soft' or 'smeared-out' clusters..."
  - [Figure 5] Visualizes the vector field $v_t$ and the resulting clustering of parameters in $(\xi, \gamma)$ space.
- **Break condition:** Clustering may fail or result in "soft" rather than "hard" clusters if the attractors $\xi^*$ move too rapidly during training (high residual variance) or if the initialization is extremely diffuse relative to the basin width.

### Mechanism 3
- **Claim:** The activation function can be reverse-engineered to impose a specific desired spectral penalty profile in Fourier space.
- **Mechanism:** Since the penalty factor is inversely proportional to the squared magnitude of the activation's Fourier transform ($\rho_\phi(k) \propto 1/|F[\phi](k)|^2$), the authors invert this relationship. They demonstrate that one can define a desired penalty $\rho(k)$ (e.g., an exponential decay to force smoothness) and derive the corresponding activation $\phi(z)$ via the inverse Fourier transform.
- **Core assumption:** The derived activation function must be mathematically valid and stable (e.g., bounded, differentiable if required) for practical implementation.
- **Evidence anchors:**
  - [Section III-B] Equation (10) provides the formula $\phi_\rho(z) \triangleq F^{-1}[1/\sqrt{\rho(k)}](z)$.
  - [Table I] lists specific activation functions (e.g., G-function, Cauchy, Gaussian) alongside their derived spectral penalties, confirming the mapping.
- **Break condition:** Design fails if the target penalty $\rho(k)$ does not correspond to a valid inverse Fourier transform or results in an activation $\phi$ that is numerically unstable (e.g., unbounded gradients).

## Foundational Learning

- **Concept: The Radon Transform & Central Slice Theorem**
  - **Why needed here:** The entire theoretical framework relies on mapping the neural network function to the Radon domain (integrals over hyperplanes) to separate the effects of architecture and activation.
  - **Quick check question:** Can you explain how a 1D slice of a 2D Fourier transform corresponds to a specific projection (Radon transform) of the original 2D image?

- **Concept: Kernel vs. Adaptive Regimes**
  - **Why needed here:** The paper derives different mechanisms for these two distinct phases of training. Confusing them leads to misapplying the spectral bias theory (which assumes fixed features) to dynamics where features move.
  - **Quick check question:** In which regime do the "breakplanes" (hyperplanes where input sums to zero) remain stationary relative to the input space?

- **Concept: Implicit Regularization (Spectral Bias)**
  - **Why needed here:** Understanding *why* overparameterized networks generalize (they prefer low-frequency solutions) is the core problem this paper addresses mathematically.
  - **Quick check question:** If a network learns low frequencies first, what happens when we try to fit a "bed of nails" (spiky) function with a standard ReLU network in the kernel regime?

## Architecture Onboarding

- **Component map:**
  - Input ($x$) -> Reparameterized Layer: Instead of standard weights $w$, bias $b$, view the transformation as defining a **Hyperplane** characterized by Orientation ($\xi$) and Signed Distance ($\gamma$).
  - Radon Spline: The "Radon Spline" parameterization ($\mu, \xi, \gamma, \omega$) replaces $(v, w, b)$. Here $\mu$ is the output scale, and $\omega$ handles the rescaling symmetry ($\alpha$-degeneracy).
  - Activation ($\phi$) -> Spectral Filter: The non-linearity acts as a filter in Fourier space. The choice of $\phi$ directly dictates the penalty $\rho_\phi(k)$ applied to different frequencies $k$.

- **Critical path:** To implement or debug this architecture, one must first select the activation $\phi$, compute its spectral penalty $\rho_\phi(k)$ (using Table I or Eq. 10), and verify it matches the frequency profile of the target data. During training, monitor $\theta_{RS}$ (specifically the distribution of $\xi$) to observe the expected clustering (adaptive regime) or stability (kernel regime).

- **Design tradeoffs:**
  - **ReLU:** Simple, but strong penalty ($\propto k^{-4}$) effectively suppresses very high frequencies; good for piecewise linear targets.
  - **Sigmoid/Logistic:** Penalizes frequencies via hyperbolic sine; strong bias towards smoothness.
  - **Custom (e.g., G-function):** Offers precise control over spectral decay (e.g., exponential), but adds computational complexity and potential numerical instability.

- **Failure signatures:**
  - **Oscillatory artifacts:** Fitting high-frequency data with an activation that penalizes those frequencies too heavily (e.g., using Gaussian activation for "Bumps" data, as seen in Experiment 2).
  - **Instability:** Using activations with extreme growth (e.g., ReLU3.5) which cause small parameter changes to explode the output, breaking gradient descent.
  - **Memorization (Bed of Nails):** If the penalty $\rho(k)$ does not grow fast enough as $k \to \infty$, the network may fit noise rather than signal (Section III-C, curse of dimensionality discussion).

- **First 3 experiments:**
  1. **Convex Fit Verification:** Implement the convex optimization solution (Equation 4) for a simple 1D regression task using ReLU activation and compare the result to a standard gradient-descent trained network in the kernel regime.
  2. **Spectral Penalty Visualization:** Calculate the Fourier transform of the learned function for different activations (ReLU vs. SoftPlus vs. Wavepacket) on the "Ring Fourier" or "Bumps" dataset and compare the decay rates to the theoretical predictions in Table I.
  3. **Cluster Tracking:** Train a 2D ReLU network on MNIST or a synthetic dataset, track the $(\xi, \gamma)$ parameters over epochs, and visualize the formation of clusters in the Radon parameter space to confirm the "attractor" mechanism.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the spectral interpretation of implicit bias be generalized to the adaptive regime to characterize solutions where zero-planes are free to move?
- Basis in paper: [explicit] The Discussion states that this work focused on the kernel regime and that "future work may be able to leverage this machinery to calculate similar results in the adaptive regime."
- Why unresolved: The adaptive regime involves non-linear dynamics and complex clustering of hyperplanes (breakplanes) that the current linear spectral penalty framework does not capture.
- What evidence would resolve it: A derivation of a time-dependent functional norm or spectral penalty that accounts for the changing density of zero-planes ($\eta_t(\xi, \gamma)$) during training.

### Open Question 2
- Question: Do activation functions designed to impose specific Fourier-space penalties effectively mitigate the curse of dimensionality on high-dimensional data?
- Basis in paper: [explicit] Section III-C discusses "implications... for generalization and potential challenges posed by the curse of dimensionality" when using tailored activation functions.
- Why unresolved: The paper notes that while the architecture helps avoid the curse of dimensionality, standard spectral penalties may still prefer non-generalizing functions ("bed of nails") in certain contexts.
- What evidence would resolve it: Theoretical bounds or empirical results on high-dimensional datasets demonstrating that designed activations (e.g., G-functions) generalize better than standard activations like ReLU without requiring exponentially large widths.

### Open Question 3
- Question: Can the Radon Spline reparameterization framework be extended to derive explicit implicit bias characterizations for deep neural networks?
- Basis in paper: [inferred] The paper restricts its theoretical derivations to "shallow" networks. The Introduction contrasts this with prior work on deep networks, implying the current Radon transform relationship does not readily extend to compositions of layers.
- Why unresolved: The reliance on the Radon transform and the specific summation form of shallow networks makes it difficult to analyze the compositional structure of deep networks.
- What evidence would resolve it: A formulation connecting layer-wise Radon Spline parameters across depth, or a proof showing that the spectral bias of deep networks can be decomposed using this parameterization.

## Limitations
- The theoretical framework assumes exact kernel or adaptive regimes, but real networks may transition between these phases during training, potentially blurring the sharp spectral biases described.
- The "frequency domain" analysis relies on assuming well-behaved Fourier transforms of both the activation and the learned function, which may break down for non-smooth activations or high-dimensional inputs (curse of dimensionality).
- The clustering mechanism in the adaptive regime is described as "soft," and the paper does not provide rigorous bounds on the size or stability of these clusters under different data distributions or initialization schemes.

## Confidence
- **High**: The spectral bias formulas (Equations 7-8) and their connection to the Radon transform are mathematically sound and well-supported by derivations.
- **Medium**: The existence and role of local attractors in the adaptive regime are plausible given the gradient flow analysis, but the "soft" nature of clustering and its dependence on data geometry is less precisely characterized.
- **Low**: The design of custom activations to achieve specific spectral penalties (Section III-B) is theoretically valid, but the practical implications (e.g., numerical stability, training dynamics) are not thoroughly explored.

## Next Checks
1. **Spectral Bias Verification**: For a simple 1D regression task, train networks with different activations (ReLU, Sigmoid, Custom) in the kernel regime and measure the frequency content of the learned function via FFT. Verify the predicted decay rates match Table I.
2. **Clustering Robustness**: Train a 2D shallow network on a synthetic dataset with known orientation structure. Track the $(\xi, \gamma)$ parameters over epochs and quantify clustering strength under different learning rates and initialization spreads.
3. **Adaptive Regime Transition**: Implement a "switch" from adaptive to kernel regime during training (as in Experiment 4). Monitor the evolution of the zero-plane similarity matrix to see if clustering persists after freezing the input weights.