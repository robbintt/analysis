---
ver: rpa2
title: Learning Local Causal World Models with State Space Models and Attention
arxiv_id: '2505.02074'
source_url: https://arxiv.org/abs/2505.02074
tags:
- causal
- world
- learning
- environment
- each
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the potential of State-Space Models (SSMs)
  for learning causal world models, aiming to address the limitations of existing
  methods in capturing causal relationships within environments. The authors propose
  Sparse Slot State Space Model (S2-SSM), a novel architecture that extends SlotSSM
  by incorporating sparsity regularization on its attention layers to learn local
  causal interactions between objects in a scene.
---

# Learning Local Causal World Models with State Space Models and Attention

## Quick Facts
- arXiv ID: 2505.02074
- Source URL: https://arxiv.org/abs/2505.02074
- Authors: Francesco Petri; Luigi Asprino; Aldo Gangemi
- Reference count: 23
- Primary result: S2-SSM learns sparse causal graphs with SSM layers matching Transformer performance on synthetic Interventional Pong environment

## Executive Summary
This paper investigates State-Space Models (SSMs) for learning causal world models, proposing Sparse Slot State Space Model (S2-SSM) to capture local causal interactions between objects. S2-SSM extends SlotSSM by incorporating sparsity regularization on attention layers, enabling the model to learn minimal causal graphs while maintaining prediction accuracy. Experiments on Interventional Pong demonstrate that S2-SSM outperforms or matches Transformer-based baselines in both reconstruction error and structural Hamming distance for causal graph accuracy. An ablation study confirms the importance of sparsity regularization for learning meaningful causal structures.

## Method Summary
The method encodes each object into a D-dimensional slot using a VAE encoder, then processes these slots through multiple SlotSSM blocks. Each block contains an SSM layer (Mamba) to predict independent object evolution followed by cross-attention to model inter-object interactions. The model learns a causal graph by counting paths across attention layers, with sparsity regularization encouraging minimal causal structures. Training minimizes a composite loss combining reconstruction error (MSE) and a sparsity penalty on the total number of causal paths, with dynamic weight scheduling based on prediction accuracy.

## Key Results
- S2-SSM achieves MSE < 3×10⁻⁴ and SHD ≈ 0 on Neutral environment (no interventions)
- Ablation study shows dense-SSM (λ=0) produces SHD ≈ 16-17 (fully connected graph)
- S2-SSM successfully generalizes to composite environments with only 10% fine-tuning data

## Why This Works (Mechanism)

### Mechanism 1
Sparsity regularization on attention layers enables learning of minimal causal graphs rather than fully-connected default structures. The loss function penalizes the total number of causal paths (|Ā|) with dynamically adjusted weight λ. When reconstruction error drops below a target threshold τ, λ increases via λ ← −e^(MSE−τ) · λ, forcing the model to prune spurious edges while maintaining prediction accuracy. Core assumption: Attention weights between object/environment slots correlate with causal relationships between the corresponding entities.

### Mechanism 2
State Space Model layers capture temporal dynamics of individual objects more efficiently than Transformers while maintaining causal discovery capability. SSM layers (specifically Mamba) model the "free evolution" of each object independently, followed by cross-attention layers that capture inter-object interactions. This separation allows efficient temporal modeling with linear complexity rather than quadratic attention cost. Core assumption: Object dynamics can be decomposed into independent temporal evolution plus sparse inter-object interactions.

### Mechanism 3
Path counting across multiple attention layers aggregates indirect causal relationships into a unified structural causal model. The final causal graph is computed as Ā = (A_L + I)...(A_2 + I)(A_1 + I), where each A_i is an adjacency matrix from layer i. This matrix multiplication counts all paths between variable pairs, capturing multi-hop causal dependencies that single-layer attention cannot represent. Core assumption: Multi-layer attention structure corresponds to causal depth; indirect effects require multiple computational steps.

## Foundational Learning

- Concept: **Structural Causal Models (SCMs) and Structural Hamming Distance**
  - Why needed here: The paper evaluates learned causal graphs using SHD, which counts edge additions, deletions, and reversals compared to ground truth. Understanding SCMs is essential to interpret what the adjacency matrices represent.
  - Quick check question: Given two directed graphs G1 with edges A→B→C and G2 with edges A→B, A→C, what is the SHD between them?

- Concept: **State Space Models (SSMs) and the Mamba Architecture**
  - Why needed here: The core contribution replaces Transformer layers with SSM (Mamba) layers. SSMs discretize continuous-time differential equations to model sequences with O(n) inference complexity.
  - Quick check question: How does an SSM's memory mechanism differ from a Transformer's attention-based memory for a sequence of length 1000?

- Concept: **Object-Centric Representations via Slot Attention**
  - Why needed here: The model operates on "slots"—learned representations of individual objects—rather than raw pixels. This abstraction is prerequisite to modeling inter-object causal relationships.
  - Quick check question: Why might slot-based representations fail in scenes with overlapping or heavily occluded objects?

## Architecture Onboarding

- Component map: Input Frame (xt) → Object Masks → VAE Encoder → Object Slots (s^1...s^O) → [SlotSSM Blocks] → Next State Prediction, with Codebook → Environment Slot (s_env) as parallel path

- Critical path: 1) Object isolation via masks, 2) VAE encoding to D-dimensional slots, 3) Environment slot selection, 4) Sequential SlotSSM blocks with sparsity-regularized attention, 5) Loss computation: MSE reconstruction + λ-weighted path count

- Design tradeoffs: λ scheduling (too high prevents learning, too low yields dense graphs), number of attention layers L (more enable deeper causal chains but increase complexity), slot dimension D (higher captures more features but increases computational cost)

- Failure signatures: SHD ≈ 16-17 indicates dense graph (sparsity not working), high MSE with low SHD indicates poor object representations, composite environment performance collapse indicates environment slot inference failure

- First 3 experiments: 1) Baseline replication on Neutral environment (verify MSE < 3×10⁻⁴ and SHD ≈ 0), 2) Sparsity ablation (compare dense-SSM λ=0 vs S2-SSM, confirm SHD increases to ~16), 3) Composite generalization (train on 7 simple, test on 4 composite with 10% fine-tuning)

## Open Questions the Paper Calls Out

1. Can S2-SSM effectively model objects that are temporarily occluded or out of view by leveraging the long-range memory capabilities of SSMs? The authors plan to study this by adding an occlusion feature, as current experiments maintain full visibility.

2. Can the architecture identify causal relationships when interventions are determined by historical observations rather than explicit environment slots? The authors want to experiment with environments where entity behavior is determined by previous observations instead of given intervention slots.

3. Is the causal discovery capability robust when object representations must be learned unsupervised rather than provided as ground-truth masks? The authors defer object-centric learning as not the primary focus, though real-world applicability requires joint learning of object representations and dynamics.

## Limitations

- Results demonstrated only on synthetic Interventional Pong environment with known ground truth causal graphs
- Exact architectural hyperparameters and training details underspecified, making faithful reproduction challenging
- The assumption that attention weights directly encode causal relationships is untested

## Confidence

- High Confidence: Sparsity-regularized attention for causal discovery is well-supported by ablation study and methodology
- Medium Confidence: Path-counting approach for aggregating multi-layer causal relationships is plausible but lacks direct validation
- Low Confidence: Claims about SSMs being "promising alternatives to Transformers" based on single synthetic environment may not generalize

## Next Checks

1. Conduct architectural sensitivity analysis by systematically varying key hyperparameters (slot dimension, number of layers, λ scheduling parameters) to determine robustness of causal discovery performance

2. Evaluate cross-environment transfer by training on 7 simple environments and testing on 4 composite environments without fine-tuning to assess generalization of both prediction accuracy and causal structure

3. Validate attention-to-causality correspondence by visualizing attention weights and computing ground truth intervention effects for a subset of predictions, quantifying correlation between attention strength and actual causal influence