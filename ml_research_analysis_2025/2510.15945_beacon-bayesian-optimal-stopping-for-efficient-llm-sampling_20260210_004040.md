---
ver: rpa2
title: 'BEACON: Bayesian Optimal Stopping for Efficient LLM Sampling'
arxiv_id: '2510.15945'
source_url: https://arxiv.org/abs/2510.15945
tags:
- beacon
- reward
- sampling
- samples
- stopping
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: BEACON is a Bayesian optimal stopping framework for adaptive LLM
  sampling that reformulates the task as sequential search with online Bayesian learning.
  It uses conjugate priors to update reward distribution beliefs in real time and
  applies the Universal Index Policy to determine when to stop sampling based on marginal
  utility versus computational cost.
---

# BEACON: Bayesian Optimal Stopping for Efficient LLM Sampling

## Quick Facts
- arXiv ID: 2510.15945
- Source URL: https://arxiv.org/abs/2510.15945
- Reference count: 40
- Primary result: Achieves up to 80% reduction in average samples while maintaining response quality through Bayesian optimal stopping

## Executive Summary
BEACON is a Bayesian optimal stopping framework for adaptive LLM sampling that reformulates the task as sequential search with online Bayesian learning. It uses conjugate priors to update reward distribution beliefs in real time and applies the Universal Index Policy to determine when to stop sampling based on marginal utility versus computational cost. BEACON achieves up to 80% reduction in average samples while maintaining response quality, as demonstrated across reasoning and alignment benchmarks. It provides theoretical optimality guarantees, practical extensions for robust updating and batch parallelism, and shows utility for efficient preference data generation in iterative training pipelines.

## Method Summary
BEACON determines the optimal stopping point K for LLM sampling by balancing expected reward gains against computational cost. It maintains a Normal-Inverse-Gamma conjugate prior over reward distribution parameters, updating sufficient statistics (mean, variance, best reward) after each sample. A pre-computed h-index table provides optimal stopping thresholds based on the standardized best reward, and sampling continues if the expected marginal gain exceeds the cost threshold. The framework includes robust updating to handle outliers and supports batch parallelism for improved wall-clock efficiency.

## Key Results
- Up to 80% reduction in average samples compared to fixed sampling budgets
- Maintains or improves response quality (accuracy, win rate) across MATH500, AIME24, AMC23, and AlpacaEval 2.0 benchmarks
- Outperforms heuristic baselines (Top-K, Confidence-based) in sample efficiency
- Achieves 4x speedup in wall-clock time through batch parallelism without quality degradation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: BEACON optimally balances expected reward gains against cumulative sampling cost.
- Mechanism: The policy uses a pre-computed h-index table (Universal Index Policy) to evaluate whether the expected marginal gain from another sample justifies its cost. The agent continues sampling iff $h_{n,k}(\hat{z}_k) > c / \sigma_k$, stopping once the standardized best reward $\hat{z}_k$ crosses the cost-adjusted index threshold.
- Core assumption: Reward distribution is approximately Normal; the cost parameter $c$ accurately reflects user tolerance for computation.
- Evidence anchors:
  - [abstract]: "BEACON ... determines when to stop by weighing expected gains against computational cost."
  - [section]: Theorem 1 formalizes the optimal stopping condition (Eq. 3) and its Bellman derivation (Eq. 1–2).
  - [corpus]: Related work ("Optimal Stopping vs Best-of-N") discusses similar cost-quality trade-offs but does not validate BEACON's specific index policy.
- Break condition: If $c$ is set far from true operational cost, or if reward model scores are systematically miscalibrated, the stopping rule will over- or under-sample relative to true optimality.

### Mechanism 2
- Claim: Real-time Bayesian updates with conjugate priors enable zero-shot adaptive sampling without offline training.
- Mechanism: Starting from Jeffreys' non-informative prior, after each observed reward the posterior over $(\mu, \sigma^2)$ is updated in closed form. The triple $(z_k, \mu_k, \sigma_k)$ is sufficient for the stopping decision.
- Core assumption: Conjugate Normal-Inverse-Gamma prior is appropriate; rewards are i.i.d. conditioned on the query.
- Evidence anchors:
  - [abstract]: "... updates posterior belief over reward distributions in real time without further training ..."
  - [section]: Section 2.2 describes NIG conjugate prior updates; Algorithm 1 implements online updating after each sample.
  - [corpus]: Weak direct evidence; corpus focuses on Bayesian optimization and bandit early stopping rather than LLM-specific conjugate updating.
- Break condition: If rewards are strongly autocorrelated or distribution is multimodal (not Normal), posterior updates may misrepresent uncertainty, leading to suboptimal stopping.

### Mechanism 3
- Claim: Robust updating mitigates negative skewness and outlier influence while preserving right-tail reward information.
- Mechanism: Replace rewards below the 1% posterior-predictive quantile with the posterior mean (one-sided winsorization), then update sufficient statistics. This prevents extreme low scores from inflating $\sigma_k$ or depressing $\mu_k$.
- Core assumption: Outliers are concentrated in the left tail; the right tail contains the informative high-quality signals.
- Evidence anchors:
  - [section]: Section 3.2 and Appendix E.4 describe the robust update rule (Eq. 15–18) and show improved stopping point accuracy in Fig. 9.
  - [corpus]: No direct external validation; general robust Bayesian updating is standard but not specifically tested for LLM reward distributions.
- Break condition: If outliers are on the right tail (e.g., reward model artifacts producing spuriously high scores), robust updates will not protect against overestimation.

## Foundational Learning

- Concept: **Conjugate Priors and Bayesian Updating**
  - Why needed here: Enables tractable real-time posterior updates over reward distribution parameters without storing full history.
  - Quick check question: Can you derive the posterior parameters after observing one new reward given a Normal-Inverse-Gamma prior?

- Concept: **Optimal Stopping Theory and Bellman Equations**
  - Why needed here: Provides the theoretical basis for deciding when marginal expected gain no longer justifies sampling cost.
  - Quick check question: What is the relationship between the value function $V_{n,k}$ and the h-index threshold in the stopping rule?

- Concept: **Sufficient Statistics**
  - Why needed here: Reduces state dimensionality from storing all past rewards to tracking just $(z_k, \mu_k, \sigma_k)$.
  - Quick check question: Why are $(z_k, \mu_k, \sigma_k)$ sufficient for the optimal stopping decision under Normal rewards?

## Architecture Onboarding

- Component map: Policy LLM -> Reward Model -> Bayesian Updater -> h-Index Table -> Stopping Decision Module -> (Robust Filter optional) -> Policy LLM

- Critical path:
  1. Pre-compute h-index table for chosen horizon $n$ and grid resolution $G$.
  2. For each query: generate $k_0=3$ initial responses, score with RM, initialize posterior.
  3. Enter adaptive loop: lookup h-index, compare to cost threshold, decide stop/continue.
  4. If continue: generate next response, apply robust filter, update sufficient statistics, repeat.
  5. On stop: return best response and its reward.

- Design tradeoffs:
  - **Horizon $n$**: Larger $n$ increases patience but raises pre-computation cost and memory.
  - **Cost $c$**: Higher $c$ prioritizes efficiency (earlier stopping); lower $c$ prioritizes quality (later stopping). Default $c=0.1$ is a balanced starting point.
  - **Batch size $b$**: Parallel batch sampling reduces wall-clock time but increases memory and may reduce adaptivity (minimum exploration depth per batch).
  - **Robust update threshold**: 1% winsorization is default; can be tuned if reward distributions show heavier left tails.

- Failure signatures:
  - **Premature stopping**: Consistently early $K$ with low accuracy; likely $c$ too high or RM miscalibrated low.
  - **Excessive sampling**: $K$ approaching $n$ on easy queries; likely $c$ too low or RM high-variance/noisy.
  - **Posterior collapse**: $\sigma_k \to 0$ too quickly; check for insufficient initial samples or overly informative prior.
  - **Index lookup errors**: NaNs or out-of-range $\hat{z}_k$; ensure grid covers expected standardized reward range.

- First 3 experiments:
  1. **Calibration sweep on $c$**: Run BEACON on 50–100 held-out queries with $c \in \{0.01, 0.05, 0.1, 0.2, 0.3\}$; plot accuracy vs. average $K$ to identify knee point.
  2. **Ablation of robust updating**: Compare BEACON with and without robust filter on a dataset with known negatively-skewed rewards; measure difference in stopping point optimality and value scores.
  3. **Batch parallelism trade-off**: Test batch sizes $b \in \{1, 2, 4, 8\}$ on a latency-constrained workload; measure wall-clock speedup, memory overhead, and accuracy change.

## Open Questions the Paper Calls Out

- How can ensemble-based reward model uncertainty quantification be systematically integrated into BEACON's Bayesian posterior updates to enhance robustness against reward model miscalibration?
- How can structured priors be designed to capture and exploit correlations between generated LLM responses (e.g., shared reasoning patterns) to improve sample efficiency?
- Can the sampling cost parameter c be dynamically tuned based on query characteristics to enable fully automated adaptation to varying computational budgets without manual calibration?
- How does BEACON's performance and optimal stopping behavior change across different reward model architectures and training paradigms (e.g., process-based vs. outcome-based reward models)?

## Limitations

- The framework assumes reward independence across samples, missing potential efficiency gains from exploiting response correlations
- The sampling cost parameter c requires manual calibration per domain and reward model, limiting fully automated deployment
- Performance depends heavily on the Normal reward distribution assumption, which may not hold for all tasks

## Confidence

- **High Confidence**: The optimal stopping theory (Theorem 1), conjugate prior update mechanics, and sufficient statistics sufficiency
- **Medium Confidence**: The cost-benefit trade-off formulation and h-index policy lookup, robust updating methodology
- **Low Confidence**: The transferability of cost parameter c across different reward models and tasks without recalibration, and the behavior under non-Normal reward distributions

## Next Checks

1. **Cross-Domain Cost Calibration**: Systematically vary c across different reward model scales and tasks (reasoning vs alignment vs open-ended generation) to map the relationship between cost setting and accuracy drop-off.

2. **Reward Distribution Validation**: Test BEACON's performance when reward distributions are deliberately non-Normal (e.g., bimodal, heavy-tailed, or highly skewed) to quantify robustness to distributional assumptions.

3. **Right-Tail Outlier Stress Test**: Evaluate BEACON with artificially injected high-reward outliers to measure vulnerability to spuriously optimistic reward signals that could mislead the stopping decision.