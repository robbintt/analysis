---
ver: rpa2
title: 'InCo-DPO: Balancing Distribution Shift and Data Quality for Enhanced Preference
  Optimization'
arxiv_id: '2503.15880'
source_url: https://arxiv.org/abs/2503.15880
tags:
- data
- on-policy
- reward
- off-policy
- inco-dpo
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper identifies two key factors in preference data for direct
  preference optimization: distribution consistency and data quality. While on-policy
  data ensures consistency, its quality is limited by the current model''s capabilities.'
---

# InCo-DPO: Balancing Distribution Shift and Data Quality for Enhanced Preference Optimization

## Quick Facts
- arXiv ID: 2503.15880
- Source URL: https://arxiv.org/abs/2503.15880
- Reference count: 13
- Primary result: Achieves 60.8% win rate on Arena-Hard with Gemma-2 using vanilla DPO

## Executive Summary
This paper addresses a fundamental trade-off in direct preference optimization (DPO): on-policy data ensures distribution consistency but suffers from limited quality due to model constraints, while off-policy data from stronger models offers higher quality but introduces distribution shifts. The authors propose InCo-DPO, which synthesizes preference data by continuing strong model prefixes with the policy model at lower temperatures. This approach dynamically balances reward improvement and consistency maintenance. Experiments across multiple datasets, models, and DPO variants show InCo-DPO consistently outperforms both on-policy and off-policy data, achieving state-of-the-art performance even on small models.

## Method Summary
InCo-DPO generates preference pairs by sampling instruction prefixes from a strong external model (e.g., Qwen2.5-72B) and continuing them with the target policy model at reduced temperature (0.6). This creates high-quality responses that maintain better distribution consistency than pure off-policy data. The method uses a reward model (ArmoRM) to score completions and construct preference pairs, which are then used to train the policy via DPO. The approach allows dynamic adjustment of prefix length (2-8 tokens optimal) to balance quality and consistency, effectively bridging the gap between on-policy stability and off-policy quality.

## Key Results
- Achieves 60.8% win rate on Arena-Hard with Gemma-2 using vanilla DPO
- Outperforms both on-policy and off-policy baselines across multiple datasets
- Demonstrates superior generalization and effectiveness even on tiny models
- Maintains higher consistency weight (~0.9) compared to pure off-policy approaches

## Why This Works (Mechanism)

### Mechanism 1: Correlated Reward Transfer via Partial Observations
The method leverages the observed statistical link (correlation coefficient 0.81) between the reward of a response prefix and the complete response. By initializing generation with a high-reward prefix from a capable external model, the policy model is biased toward completing the trajectory in a manner that maintains this elevated reward profile, effectively lifting the quality ceiling inherent to its own raw capabilities.

### Mechanism 2: Consistency-Preserving Temperature Scaling
Reducing sampling temperature during the continuation phase mitigates the distribution shift caused by off-policy prefixes, maintaining a higher "consistency weight." By lowering the sampling temperature (e.g., from 0.8 to 0.6), the model is forced to select tokens with higher probability mass in its own distribution, partially recovering the consistency weight lost by the off-policy prefix.

### Mechanism 3: Dynamic Quality-Distribution Trade-off via Prefix Length
The trade-off between data quality (reward) and distribution shift (consistency) can be tuned by adjusting the prefix token count. A minimal prefix (2–10 tokens) is sufficient to anchor the response in a high-reward region without occupying enough sequence length to severely degrade the overall consistency weight.

## Foundational Learning

- **Direct Preference Optimization (DPO)**: DPO relies on a closed-form mapping between reward functions and policy probabilities to grasp why "consistency weight" (probability under the policy) matters for training stability. *Quick check: How does DPO eliminate the need for a separate reward model during the policy optimization phase?*

- **Distribution Shift in RL**: Training on data different from the model's current output distribution often leads to divergence or instability. *Quick check: Why does training a policy on trajectories generated by a different (expert) policy often lead to divergence or instability?*

- **Reward Modeling (ArmoRM)**: The paper quantifies "quality" explicitly via reward model scores (ArmoRM). Understanding that these scores are the ground truth for "quality" is essential, as the mechanism relies on maximizing this scalar. *Quick check: In the context of this paper, what is the specific statistical relationship observed between the ArmoRM score of a partial response and its full completion?*

## Architecture Onboarding

- **Component map**: External Model (Teacher) -> Policy Model (Student) -> Reward Model (Judge) -> DPO Loss

- **Critical path**: Instruction -> External Model (Generates Prefix) -> Policy Model (Continues with Temp=0.6) -> Reward Model (Scores) -> Pair Selection (Max vs Min Reward) -> DPO Training

- **Design tradeoffs**: 
  - Prefix Length (2-8 tokens): Shorter prefixes optimize the quality/consistency balance
  - Sampling Temperature (0.6): Lower temp stabilizes consistency; higher temp increases diversity
  - Data Source: InCo-DPO hybrid midpoint between on-policy safety and off-policy quality

- **Failure signatures**:
  - Catastrophic Forgetting: Hybrid approaches degraded performance with WPO on Llama-3
  - Mode Collapse (Low Temp): Too similar responses result in low "reward margin"

- **First 3 experiments**:
  1. Prefix Length Sweep: Generate data with |y_p| ∈ {0, 2, 4, 8, 16} and plot both "Consistency Weight" and "Average Reward"
  2. Off-Policy Baseline Failure: Train pure DPO on Qwen-2.5 outputs using Gemma-2 as policy model
  3. Temperature Ablation: Run InCo-DPO with Temp ∈ {0.5, 0.6, 0.7, 0.8} to confirm 0.6 provides optimal balance

## Open Questions the Paper Calls Out

- Does InCo-DPO maintain its efficacy in safety alignment and multi-turn instruction scenarios? The current study restricted its scope to single-turn helpfulness, truthfulness, and honesty using UltraFeedback, leaving safety and conversational context retention unverified.

- How should the choice between continuation and rewriting strategies be adapted to different preference optimization objectives? While Continuation is superior for vanilla DPO, Rewriting achieves higher AlpacaEval scores when combined with WPO, suggesting the optimal synthesis method depends on the objective function.

- Is the positive correlation between partial and full response rewards robust for complex reasoning tasks where the logical structure is developed mid-response? The hypothesis relies on data where style and early content are strong indicators of quality; this assumption may fail for mathematical or logical problems where the critical steps occur after the initial tokens.

## Limitations

- The observed 0.81 correlation coefficient between partial and full response rewards may not generalize across different instruction domains, model architectures, and reward model implementations
- The temperature scaling mechanism lacks rigorous theoretical justification for why the specific value (0.6) works optimally
- The prefix length optimization of 2-8 tokens may not be universal across instruction complexities and domain specificities

## Confidence

**High Confidence (8/10)**: Core experimental results showing InCo-DPO outperforming both on-policy and off-policy baselines across multiple datasets and model combinations.

**Medium Confidence (6/10)**: Theoretical mechanism explaining why prefix continuation with temperature scaling balances distribution shift and quality.

**Low Confidence (4/10)**: Claim that InCo-DPO achieves state-of-the-art performance with "vanilla DPO" on Arena-Hard, as the comparison framework doesn't adequately control for differences in data quality versus optimization strategy.

## Next Checks

1. **Correlation Robustness Test**: Generate prefix-continuation pairs across three diverse instruction domains (technical, creative, conversational) and measure the correlation coefficient between partial and full rewards for each domain.

2. **Temperature Sensitivity Analysis**: Systematically vary the continuation temperature from 0.4 to 1.0 in increments of 0.1, measuring both consistency weight and final win rates.

3. **Prefix Length Scaling Study**: Test prefix lengths beyond the proposed 2-8 token range (0, 1, 2, 4, 8, 16, 32 tokens) across instructions of varying complexity (simple factoid vs. multi-step reasoning).