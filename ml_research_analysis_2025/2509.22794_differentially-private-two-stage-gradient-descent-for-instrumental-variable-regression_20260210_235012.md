---
ver: rpa2
title: Differentially Private Two-Stage Gradient Descent for Instrumental Variable
  Regression
arxiv_id: '2509.22794'
source_url: https://arxiv.org/abs/2509.22794
tags:
- have
- privacy
- lemma
- algorithm
- probability
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces DP-2S-GD, the first differentially private\
  \ algorithm for instrumental variable regression. The method uses noisy gradient\
  \ descent with per-sample clipping and Gaussian noise injection, ensuring \u03C1\
  -zero-concentrated differential privacy."
---

# Differentially Private Two-Stage Gradient Descent for Instrumental Variable Regression

## Quick Facts
- arXiv ID: 2509.22794
- Source URL: https://arxiv.org/abs/2509.22794
- Reference count: 40
- This paper introduces DP-2S-GD, the first differentially private algorithm for instrumental variable regression, with provable finite-sample convergence rates.

## Executive Summary
This paper addresses the challenge of performing causal inference with instrumental variables while preserving differential privacy. The authors propose DP-2S-GD, a two-stage gradient descent algorithm that alternately updates projection matrices and regression parameters with carefully calibrated Gaussian noise injection. The method provides theoretical guarantees for both privacy (ρ-zero-concentrated differential privacy) and accuracy, establishing explicit trade-offs between privacy budget, sample size, and estimation error. Experiments on synthetic and real datasets validate the theoretical predictions about the privacy-accuracy trade-off.

## Method Summary
DP-2S-GD solves the instrumental variable regression problem using a two-stage approach where Stage 1 regresses endogenous variables onto instruments, and Stage 2 estimates the causal effect on the outcome. Each stage uses gradient descent with per-sample gradient clipping and Gaussian noise injection calibrated to achieve ρ-zCDP. The algorithm alternates between updating the projection matrix Θ (mapping instruments to endogenous variables) and the causal parameter β, with privacy budgets split between stages. The method guarantees finite-sample convergence with error bounds that explicitly characterize the trade-off between privacy, sample size, and accuracy.

## Key Results
- Introduces the first differentially private algorithm for IVaR with theoretical privacy and accuracy guarantees
- Proves finite-sample convergence rates with explicit characterization of privacy-accuracy trade-offs
- Establishes error bounds decreasing as O(κ(τ)/T + √p(√q+√τ)³/(n√ρ√T) + √pq(τ+log(pq))/√n)
- Demonstrates practical privacy-utility trade-offs on synthetic and real datasets

## Why This Works (Mechanism)

### Mechanism 1: Gradient-based Privacy via Clipping and Noise Injection
If per-sample gradients are clipped to a threshold γ and Gaussian noise scaled to γ is added, the iterative update satisfies ρ-zero-concentrated differential privacy (zCDP). The mechanism bounds the sensitivity of the gradient sum (via clipping) and masks individual contributions (via noise). Because zCDP composes additively (ρ_total = Σρ_t), the noise scale can be calibrated precisely to the total iteration count T. The clipping threshold γ must be set high enough that "true" gradients are rarely clipped but low enough to limit sensitivity.

### Mechanism 2: Two-Stage Alternating Descent for Endogeneity
Updating the projection matrix Θ (Stage 1) and the causal parameter β (Stage 2) in alternating steps allows the algorithm to solve the bi-level IVaR optimization problem while isolating privacy costs. Stage 1 regresses endogenous variables X onto instruments Z to purge confounding. Stage 2 uses the predicted X̂ to estimate the causal effect on Y. By treating these as separate Gaussian mechanisms, the system isolates privacy leakage per stage (ρ₁ vs ρ₂).

### Mechanism 3: Finite-Sample Convergence via Contraction-Dominated Iteration
The estimation error decreases exponentially (contraction) initially, but eventually the cumulative privacy noise (growing with √T) dominates. The algorithm relies on a contraction rate κ(τ) < 1 to reduce error per step. However, noise variance sums over T steps. The theoretical bound proves a "sweet spot" for T exists before noise accumulation overcomes the benefit of further descent.

## Foundational Learning

- **Concept: Instrumental Variable Regression (IVaR)**
  - Why needed here: The entire algorithm is designed to solve the "endogeneity" problem where standard regression fails. You cannot interpret the algorithm's structure without understanding that it is trying to isolate causal variation from confounders using instruments.
  - Quick check question: Can you explain why Ordinary Least Squares (OLS) produces biased estimates when there is an unobserved confounder affecting both the regressor X and outcome Y?

- **Concept: Zero-Concentrated Differential Privacy (zCDP)**
  - Why needed here: The paper uses zCDP rather than standard (ε, δ)-DP. Understanding zCDP is required to interpret the privacy guarantees and why the noise scales the way it does (additive composition).
  - Quick check question: Why is zCDP preferred over (ε, δ)-DP for analyzing iterative algorithms like gradient descent?

- **Concept: Sensitivity and Gradient Clipping**
  - Why needed here: Clipping is not just a regularization trick here; it is a mathematical requirement to bound "sensitivity" so that differential privacy can be applied. The setting of the threshold γ is a critical hyperparameter discussed extensively in the proof (Lemma D.1).
  - Quick check question: If you double the clipping threshold γ, how does the required Gaussian noise scale change to maintain the same privacy level?

## Architecture Onboarding

- **Component map:** Inputs (Z, X, Y, ρ₁, ρ₂, T) -> Stage 1 (compute Θ gradients) -> Clip (γ₁) -> Noise (Ξ) -> Update Θ -> Stage 2 (compute β gradients) -> Clip (γ₂) -> Noise (ν) -> Update β -> Output (β^(T))

- **Critical path:** The Stage 1 → Stage 2 dependency. Errors in the projection matrix Θ directly distort the gradients in Stage 2. The algorithm is coupled; you cannot parallelize these stages.

- **Design tradeoffs:**
  - **Privacy Allocation:** You must decide how to split the total budget ρ between ρ₁ (first stage) and ρ₂ (second stage). If you only care about β, you can set ρ₁ = ∞ (no noise in stage 1) to tighten bounds.
  - **Iteration Count (T):** Setting T is non-trivial. Too small = under-converged; too large = noise explosion. The "optimal" T is sub-linear in n.

- **Failure signatures:**
  - **Divergence at High T:** If you see error increasing after an initial drop, you have likely entered the noise-dominated regime (Condition 5 violated).
  - **Stagnation:** If the clipping threshold γ is too low, gradients are systematically zeroed out, and the model learns nothing.

- **First 3 experiments:**
  1. **Validation of T-Curve (Synthetic):** Run DP-2S-GD with fixed n and varying T. Plot the error curve to verify it follows the predicted "convergence → plateau → explosion" shape.
  2. **Privacy-Utility Trade-off (Real Data):** Apply to the Angrist dataset. Vary ρ (e.g., ρ=1 vs ρ=100) and observe the variance of the causal estimate β.
  3. **Weak Instrument Test:** Generate synthetic data with low correlation between Z and X. Observe if the convergence slows down (increased iteration requirement) and if this pushes the system into the noise-dominated regime faster.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can a differentially private algorithm for IVaR achieve convergence rates that exactly match the non-private 2SLS estimator, eliminating the additional √p factor in the error bound?
- Basis in paper: [explicit] Remark 3.7 states: "the convergence of the two-stage gradient descent estimator to β̂ is slower by a √p compared to the convergence of β̂ to the true parameter β... a fundamentally different modification of the algorithm may be required to algorithmically match the rate."
- Why unresolved: The current DP-2S-GD algorithm inherits this suboptimality from the base 2S-GD method, suggesting a fundamental algorithmic limitation rather than merely a privacy cost.
- What evidence would resolve it: A new algorithm achieving error rate O(√q(τ+log(q))/√n) under privacy, or a formal lower bound proving the √p penalty is unavoidable.

### Open Question 2
- Question: What are the information-theoretic lower bounds on the privacy-accuracy trade-off for instrumental variable regression?
- Basis in paper: [explicit] The conclusion explicitly identifies "establishing lower-bounds for privacy-accuracy tradeoffs for the IVaR problem" as an interesting future direction.
- Why unresolved: The paper only provides upper bounds; without lower bounds, it remains unknown whether DP-2S-GD achieves optimal privacy-utility trade-offs.
- What evidence would resolve it: A minimax lower bound showing that any ρ-zCDP algorithm must incur estimation error at least Ω(f(n,p,q,ρ)), matching or gap-existing with the upper bounds.

### Open Question 3
- Question: Can the differentially private approach be extended to nonlinear instrumental variable regression settings such as kernel IV or DeepIV?
- Basis in paper: [inferred] The introduction mentions extensions to nonlinear IV including "kernel-based methods Singh et al. [2019] and DeepIV Hartford et al. [2017]," but the paper's scope is limited to linear models. The Related Work section notes prior nonlinear IV methods "assume unrestricted access to the data and does not provide end-to-end differential privacy guarantees."
- Why unresolved: Nonlinear IV involves more complex optimization landscapes and potentially unbounded sensitivity, making direct extension non-trivial.
- What evidence would resolve it: A privacy-preserving algorithm for kernel IV or neural network-based IV with provable convergence guarantees and privacy bounds.

## Limitations
- Theoretical privacy analysis relies heavily on sub-Gaussian assumptions for data-generating process, which may not hold in real-world datasets
- Clipping thresholds γ₁, γ₂ involve an unspecified constant c₀, making exact reproduction challenging without additional calibration guidance
- Optimal iteration count T requires solving a delicate balance between contraction speed and noise accumulation, with only asymptotic guidance provided

## Confidence
- **High confidence**: The privacy mechanism (clipping + Gaussian noise injection) and the zCDP composition framework are well-established in the DP literature
- **Medium confidence**: The finite-sample convergence bounds appear mathematically sound but rely on strong statistical assumptions about data distributions
- **Medium confidence**: The empirical results demonstrate the predicted privacy-utility trade-off, though the synthetic data setup is somewhat idealized

## Next Checks
1. **Noise sensitivity analysis**: Systematically vary the privacy parameters ρ₁, ρ₂ and measure the variance inflation in the causal estimates β^(T) across multiple synthetic datasets
2. **Weak instrument stress test**: Generate datasets with decreasing instrument strength (lower correlation between Z and X) and measure how quickly the contraction rate κ(τ) degrades
3. **Clipping threshold calibration**: Implement the suggested threshold formula γ = c₀(√q + √τ + √log(nT))² with various c₀ values to empirically determine the minimal threshold that avoids systematic gradient clipping