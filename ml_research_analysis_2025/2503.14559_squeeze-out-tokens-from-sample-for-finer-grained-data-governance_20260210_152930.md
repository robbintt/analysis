---
ver: rpa2
title: Squeeze Out Tokens from Sample for Finer-Grained Data Governance
arxiv_id: '2503.14559'
source_url: https://arxiv.org/abs/2503.14559
tags:
- data
- vision
- datajuicer
- image
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper addresses the challenge of improving data governance\
  \ in vision-language pretraining by proposing a finer-grained approach called DataJuicer.\
  \ Unlike existing methods that rely on coarse-grained sample selection based on\
  \ heuristic scores, DataJuicer operates at the token level\u2014using vision foundation\
  \ models to retain salient image patches and language models to enhance captions\
  \ with visual semantics."
---

# Squeeze Out Tokens from Sample for Finer-Grained Data Governance

## Quick Facts
- arXiv ID: 2503.14559
- Source URL: https://arxiv.org/abs/2503.14559
- Reference count: 40
- Key result: DataJuicer achieves state-of-the-art performance on image-text retrieval, classification, and dense visual reasoning tasks while reducing token usage and inference time

## Executive Summary
This paper addresses the challenge of improving data governance in vision-language pretraining by proposing a finer-grained approach called DataJuicer. Unlike existing methods that rely on coarse-grained sample selection based on heuristic scores, DataJuicer operates at the token level—using vision foundation models to retain salient image patches and language models to enhance captions with visual semantics. The method reduces redundancy in images and improves cross-modal alignment by integrating high-confidence object classes into textual descriptions. Extensive experiments show that DataJuicer significantly outperforms existing data sieve methods on tasks like image-text retrieval, classification, and dense visual reasoning, while also offering faster inference due to reduced token usage. It scales effectively with data, model size, and training schedule, and generalizes across datasets and model architectures.

## Method Summary
DataJuicer implements a dual-branch pipeline for vision-language pretraining data governance. The vision branch uses a pretrained DINO model to compute attention-based contribution scores for each image patch relative to the [CLS] token, retaining only the top-k patches. It also predicts object classes with confidence scores, filtering those above threshold ε=0.7. The text branch takes the original caption and filtered object classes, using an LLM (LLaMA/Qwen) to rewrite the caption, incorporating visual evidence while correcting grammar and vagueness. The curated dataset is then used to train CLIP-style models from scratch using standard contrastive learning objectives. The method operates at 50% patch retention ratio for main experiments and demonstrates improved efficiency and performance across multiple vision-language tasks.

## Key Results
- Outperforms existing data sieve methods (e.g., DataSieve) on image-text retrieval (MSCOCO, Flickr30k), classification (ImageNet-1K/R/A), and MLLM tasks (OKVQA, MMBench)
- Achieves faster inference due to reduced token usage while maintaining or improving accuracy
- Scales effectively with data (3M→12M), model size (ViT-S/B/L), and training schedule
- Generalizes across datasets (CC3M, CC12M, LAION40M) and model architectures

## Why This Works (Mechanism)

### Mechanism 1: Attention-Based Patch Contribution Scoring
Discarding image patches with low contribution to global semantics reduces redundancy without harming downstream performance. A Vision Foundation Model (DINO) computes contribution scores for each patch by measuring attention similarity between the patch token and the [CLS] token. Only top-k patches are retained; low-scoring patches are masked out. The [CLS] token encapsulates global image semantics, and patches with low attention to it are semantically redundant.

### Mechanism 2: Cross-Modal Caption Enhancement via Visual Evidence
Incorporating high-confidence object classes into captions improves image-text alignment and reduces noise. The vision branch predicts object classes with confidence scores; classes above threshold ε are fed to an LLM as prompts. The LLM rewrites the original caption to integrate these visual semantics, correcting grammar and vagueness. LLMs leverage world knowledge to infer semantic connections between predicted visual objects and textual descriptions.

### Mechanism 3: Model-Derived vs Heuristic-Based Governance
Token-level, model-derived contribution estimation outperforms sample-level heuristic selection, especially at scale. Instead of heuristic metrics (CLIP-score, cluster distance), DataJuicer uses attention and prediction outputs of pretrained Foundation Models to estimate token/patch value. This avoids heuristic-induced biases and generalizes better. Foundation Model attention and classification outputs correlate with genuine semantic importance for VLP tasks.

## Foundational Learning

Concept: Vision Transformer (ViT) Attention Mechanisms
- Why needed here: Understanding how [CLS] tokens and patch tokens interact via self-attention is essential for grasping the patch contribution scoring mechanism.
- Quick check question: Can you explain why the [CLS] token is used as a global semantic aggregator in ViTs?

Concept: Cross-Modal Contrastive Learning (e.g., CLIP)
- Why needed here: The downstream VLP training uses Image-Text Contrastive (ITC) loss; understanding contrastive alignment is critical.
- Quick check question: How does contrastive loss align image and text embeddings in a shared space?

Concept: Foundation Model Transfer and Zero-Shot Evaluation
- Why needed here: The method relies on pretrained Vision and Language FMs for data governance, not from-scratch training of these components.
- Quick check question: What does "zero-shot" evaluation mean in the context of CLIP-style models?

## Architecture Onboarding

Component map: Vision Branch (DINO-S) -> Patch Scoring -> Top-k Selection -> Object Class Prediction -> Confidence Thresholding -> Text Branch (LLM) -> Caption Enhancement -> VLP Training (CLIP)

Critical path: 1. Patch scoring via attention → 2. Top-k selection → 3. Object class prediction → 4. Confidence thresholding → 5. LLM caption rewrite → 6. VLP contrastive training

Design tradeoffs:
- Patch retention ratio (k/m): Higher = more information, slower inference; lower = faster, potential loss of fine-grained details
- Confidence threshold ε: Higher = more reliable visual evidence, fewer enhancements; lower = more integration, risk of noise
- LLM choice: Larger models (LLaMA) offer better rewriting but higher overhead; smaller models trade quality for speed

Failure signatures:
- Over-aggressive patch removal: Images with uniformly distributed semantics lose critical information
- High-confidence misclassification: Wrong object classes hallucinate unrelated content into captions
- LLM hallucination: Rewrites introduce entities not present in the image
- Train/inference gap: Models trained on reduced patches may underperform on full-image inference for detail-heavy tasks

First 3 experiments:
1. Baseline comparison: Train CLIP on raw vs. DataJuicer-governed CC3M at 50% compression; evaluate zero-shot ImageNet-1K and retrieval
2. Ablation study: Remove vision-only (no patch filtering) or text-only (no caption enhancement) branches; measure performance drop
3. Scaling analysis: Compare DataJuicer vs. DataSieve across data scale (3M→12M), model size (ViT-S/B/L), and training schedule; plot performance vs. compute

## Open Questions the Paper Calls Out
None explicitly stated in the provided content.

## Limitations
- Method relies heavily on pretrained Foundation Models (DINO, LLMs) whose outputs may not generalize across domains or specialized terminology
- Attention-based patch scoring assumes uniform attention patterns, which may not hold for images with distributed semantics or complex scenes
- LLM caption enhancement introduces potential for hallucination or loss of domain-specific context
- Training/inference gap exists: models trained on reduced patch sets may underperform on full-resolution images
- Significant compute required for Foundation Model inference during data curation, potentially limiting scalability

## Confidence
High confidence: Core claim that token-level data governance outperforms sample-level heuristic methods is well-supported by extensive experiments across multiple datasets, model sizes, and tasks.

Medium confidence: Claim about cross-modal caption enhancement improving alignment depends on LLM generalization and may be dataset-dependent. Attention-based patch scoring assumes DINO's attention patterns reliably capture semantic importance.

Low confidence: Assertion that method "generalizes across datasets and model architectures" is based on CLIP-style models on web-scale data; performance on other architectures or specialized domains remains untested.

## Next Checks
1. Train/inference gap validation: Train DataJuicer on reduced patches, then evaluate on full-resolution images across all downstream tasks. Compare against baseline models trained on full images to quantify performance degradation.

2. Domain robustness test: Apply DataJuicer to specialized datasets (medical imaging, satellite imagery, or fine art) where Foundation Model pretraining may be less effective. Evaluate whether attention-based patch scoring and LLM caption enhancement maintain performance.

3. Foundation Model dependency analysis: Replace DINO with alternative vision models (CLIP, DINOv2, or supervised ViT) for patch scoring and object prediction. Similarly, test different LLM sizes and architectures for caption enhancement. Measure sensitivity of downstream performance to these component choices.