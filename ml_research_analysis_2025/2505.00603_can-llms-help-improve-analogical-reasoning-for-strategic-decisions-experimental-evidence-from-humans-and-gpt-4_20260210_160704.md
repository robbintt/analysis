---
ver: rpa2
title: Can LLMs Help Improve Analogical Reasoning For Strategic Decisions? Experimental
  Evidence from Humans and GPT-4
arxiv_id: '2505.00603'
source_url: https://arxiv.org/abs/2505.00603
tags:
- analogical
- human
- reasoning
- problem
- analogy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study compared human (n=199) and GPT-4 (n=60) analogical reasoning
  in strategic decision-making tasks. Humans demonstrated high precision but low recall
  in identifying correct analogies, while GPT-4 achieved high recall but low precision.
---

# Can LLMs Help Improve Analogical Reasoning For Strategic Decisions? Experimental Evidence from Humans and GPT-4

## Quick Facts
- arXiv ID: 2505.00603
- Source URL: https://arxiv.org/abs/2505.00603
- Reference count: 7
- Primary result: Humans showed high precision but low recall in analogical reasoning, while GPT-4 achieved high recall but low precision; suggested division of labor where AI generates candidates and humans evaluate structural relevance.

## Executive Summary
This study experimentally compared human (n=199) and GPT-4 (n=60) analogical reasoning performance on strategic decision-making tasks involving source-to-target matching. The research revealed a fundamental trade-off: humans achieved high precision but low recall in identifying correct analogies, while GPT-4 demonstrated the opposite pattern with high recall but low precision. Without hints, humans achieved 0.57 accuracy versus GPT-4's 0.50, which improved to 0.63 and 0.53 respectively when hints were provided. Error analysis showed GPT-4 relied on surface-level similarities while human errors stemmed from structural misinterpretations.

## Method Summary
The experiment used source-to-target analogical matching tasks with 2 source stories (Radiation, Dolphins) and 2 target problems (City Factory, HR). Participants read and summarized both stories (3 min each), then solved assigned problems either with or without explicit hints about using the stories. GPT-4 followed the same chronology but with no time limits, running 15 independent trials per condition (60 total). Responses were manually coded against pre-defined DAGs specifying correct analogical transfers, measuring precision, recall, F1, and accuracy.

## Key Results
- Humans: High precision (0.67/0.70) but low recall (0.65/0.69) in identifying correct analogies
- GPT-4: Low precision (0.25/0.25) but high recall (0.78/0.80) across conditions
- Without hints: Human accuracy 0.57 vs GPT-4 0.50; with hints: 0.63 vs 0.53
- GPT-4 errors primarily involved surface-level similarities; human errors involved structural misinterpretations

## Why This Works (Mechanism)
The study demonstrates complementary cognitive profiles between humans and LLMs in analogical reasoning tasks. Humans excel at precision by carefully evaluating structural correspondences but miss many valid analogies (low recall), while LLMs cast a wider net by identifying more potential matches but with lower accuracy in judging structural relevance. This trade-off suggests that combining both approaches could yield superior decision-making outcomes.

## Foundational Learning
- Analogical reasoning trade-offs: Understanding the precision-recall tension in human vs. AI cognition is crucial for designing collaborative workflows. Quick check: Compare confusion matrices to visualize error patterns.
- Source-to-target mapping: The radiation and dolphin stories map to factory and HR problems respectively through causal structural similarities. Quick check: Verify mappings align with DAG specifications.
- Hint effects: Explicit prompts improve human accuracy but have minimal impact on GPT-4 performance. Quick check: Analyze accuracy gap between hint/no-hint conditions.

## Architecture Onboarding
- Component map: Human evaluators <-[evaluate] AI candidate analogies -> AI model
- Critical path: (1) AI generates analogy candidates, (2) Humans evaluate structural relevance, (3) Decision made based on human judgment
- Design tradeoffs: Precision (human strength) vs Recall (AI strength); Static prompts (current) vs Iterative dialogue (future)
- Failure signatures: GPT-4 produces surface-level matches (false positives); Humans miss valid structural matches (false negatives)
- First experiments: (1) Test prompt variations to reduce GPT-4 surface-level errors, (2) Validate DAG coding criteria with independent raters, (3) Compare single-shot vs iterative prompting accuracy

## Open Questions the Paper Calls Out
- Does the high-recall/low-precision profile observed in GPT-4 generalize to other large language model families, sizes, and training objectives? The study used a specific GPT-4 version, leaving broader LLM performance characteristics unknown.
- Can iterative prompting, where the model queries constraints or receives feedback, reduce false positives and improve precision in analogical matching? Current static prompts limited the model's ability to refine structural mapping through dialogue.
- Does the proposed "division of labor" workflow, where LLMs generate candidates and humans evaluate them, improve decision accuracy compared to humans working alone? The experiment tested humans and AI in isolation, not the integrated workflow.

## Limitations
- Unknown prompt specifications for GPT-4 and incomplete visibility of DAG figures that define correct analogical transfer
- Small sample size for GPT-4 (n=60) and use of single model version limits generalizability
- Time constraints for humans but not GPT-4 may bias comparisons

## Confidence
- Precision-recall trade-off findings: Medium confidence (qualitative error analysis plausible but not fully validated)
- Division-of-labor hypothesis: Medium confidence (inferred from performance gaps rather than directly tested)
- Absolute accuracy figures: Low confidence (due to unknown prompt and coding details)

## Next Checks
1. Reconstruct and test the exact prompt sequence with GPT-4 to verify reproducibility of reported accuracy and error patterns
2. Reconstruct DAG figures and coding criteria, then manually code a subset of responses to confirm inter-rater reliability
3. Conduct follow-up experiment with multiple LLM variants and larger sample sizes, explicitly testing the division-of-labor hypothesis