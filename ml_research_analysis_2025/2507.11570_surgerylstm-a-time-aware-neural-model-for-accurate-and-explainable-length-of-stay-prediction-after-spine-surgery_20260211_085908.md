---
ver: rpa2
title: 'SurgeryLSTM: A Time-Aware Neural Model for Accurate and Explainable Length
  of Stay Prediction After Spine Surgery'
arxiv_id: '2507.11570'
source_url: https://arxiv.org/abs/2507.11570
tags:
- were
- prediction
- data
- patient
- clinical
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study developed SurgeryLSTM, a time-aware neural model for
  predicting length of stay (LOS) after elective spine surgery. It uses a masked bidirectional
  LSTM with attention mechanism to capture sequential patterns in preoperative electronic
  health record data.
---

# SurgeryLSTM: A Time-Aware Neural Model for Accurate and Explainable Length of Stay Prediction After Spine Surgery
arXiv ID: 2507.11570
Source URL: https://arxiv.org/abs/2507.11570
Reference count: 22
Key outcome: Developed SurgeryLSTM, a time-aware neural model achieving R²=0.86 for LOS prediction after elective spine surgery, outperforming traditional ML methods

## Executive Summary
SurgeryLSTM is a time-aware neural model designed to predict length of stay (LOS) after elective spine surgery using preoperative electronic health record data. The model employs a masked bidirectional LSTM with attention mechanism to capture sequential patterns in patient data. When compared against traditional machine learning methods including XGBoost, random forest, SVM, and linear regression, SurgeryLSTM achieved the highest accuracy with an R² of 0.86, marginally outperforming XGBoost at R²=0.85. The attention mechanism provides interpretability by highlighting the most influential time points and clinical features in the prediction process.

## Method Summary
The study developed SurgeryLSTM using a masked bidirectional LSTM architecture with attention mechanism to process sequential preoperative EHR data for LOS prediction. The model was trained and evaluated on data from elective spine surgery patients, with performance compared against traditional ML baselines including XGBoost, random forest, SVM, and linear regression. Key predictors identified included bone disorder, chronic kidney disease, and lumbar fusion procedures. The attention mechanism enabled interpretable predictions by highlighting influential time points and clinical features throughout the patient's preoperative timeline.

## Key Results
- SurgeryLSTM achieved R²=0.86 for LOS prediction, outperforming XGBoost (R²=0.85) and other baseline methods
- Attention mechanism successfully highlighted influential clinical features including bone disorders, chronic kidney disease, and lumbar fusion
- Model demonstrated strong performance in capturing sequential patterns in preoperative EHR data for discharge planning applications

## Why This Works (Mechanism)
The time-aware neural architecture captures temporal dependencies in preoperative patient data through sequential processing, allowing the model to identify patterns that influence post-surgical recovery duration. The masked bidirectional LSTM processes patient history in both forward and backward directions while accounting for missing data through masking, enabling comprehensive temporal feature extraction. The attention mechanism provides interpretability by weighting the importance of different time points and clinical features, allowing clinicians to understand which aspects of the preoperative timeline most strongly influence predicted LOS.

## Foundational Learning
- **Masked Bidirectional LSTM**: Why needed - handles missing temporal data while capturing bidirectional temporal dependencies; Quick check - verify masking implementation preserves temporal sequence integrity
- **Attention Mechanisms in Healthcare**: Why needed - provides interpretability for clinical decision support; Quick check - confirm attention weights align with clinical expertise
- **Time-aware Neural Architectures**: Why needed - captures temporal patterns in sequential medical data; Quick check - validate temporal feature extraction against clinical knowledge
- **Explainable AI in Clinical Settings**: Why needed - ensures model predictions are interpretable for clinical validation; Quick check - assess clinician acceptance of attention-based explanations
- **Preoperative Risk Stratification**: Why needed - identifies patients requiring extended monitoring or resources; Quick check - correlate predictions with actual clinical outcomes
- **Sequential EHR Data Processing**: Why needed - captures longitudinal patient trajectories; Quick check - validate temporal ordering of clinical events

## Architecture Onboarding
Component Map: Preoperative EHR Data -> Masked Bidirectional LSTM -> Attention Mechanism -> LOS Prediction
Critical Path: Sequential EHR input → LSTM processing with masking → Bidirectional temporal feature extraction → Attention-weighted feature combination → LOS output
Design Tradeoffs: Bidirectional processing captures retrospective context but increases computational complexity; attention mechanism adds interpretability but requires careful calibration
Failure Signatures: Overfitting to single-center data patterns; attention weights not aligning with clinical expectations; poor performance on unseen patient demographics
First 3 Experiments: 1) Validate temporal feature extraction against known clinical risk factors 2) Test model performance across different patient subgroups 3) Evaluate attention mechanism outputs with clinical experts

## Open Questions the Paper Calls Out
None

## Limitations
- Single-center dataset limits generalizability across different clinical settings and patient populations
- Performance metrics not validated on external datasets or through temporal validation splits
- Attention mechanism interpretability requires clinical expert validation to assess practical utility

## Confidence
- Model architecture and technical implementation: High
- Performance comparison with baseline methods: Medium (limited to single dataset)
- Clinical interpretability of attention weights: Low (requires expert validation)
- Generalizability to other surgical contexts: Low

## Next Checks
1. External validation on multi-center datasets with diverse patient populations
2. Prospective clinical validation comparing model predictions with actual discharge planning outcomes
3. Expert clinician review of attention mechanism outputs to assess clinical relevance and decision support utility