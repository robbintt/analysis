---
ver: rpa2
title: 'FIFA: Unified Faithfulness Evaluation Framework for Text-to-Video and Video-to-Text
  Generation'
arxiv_id: '2507.06523'
source_url: https://arxiv.org/abs/2507.06523
tags:
- entity
- whole
- relation
- spatial
- attribute
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces FIFA, a unified faithfulness evaluation framework
  for both Text-to-Video and Video-to-Text tasks. FIFA addresses the challenge of
  hallucinations in video multimodal models by extracting comprehensive descriptive
  facts, modeling their semantic dependencies via a Spatio-Temporal Semantic Dependency
  Graph, and verifying them using VideoQA models.
---

# FIFA: Unified Faithfulness Evaluation Framework for Text-to-Video and Video-to-Text Generation

## Quick Facts
- arXiv ID: 2507.06523
- Source URL: https://arxiv.org/abs/2507.06523
- Reference count: 40
- Primary result: Introduces FIFA framework that achieves highest correlation with human judgment for evaluating faithfulness in video multimodal generation tasks

## Executive Summary
This paper introduces FIFA (Faithfulness Evaluation Framework for multimodal Video content and texts), a unified framework for evaluating faithfulness in both text-to-video and video-to-text generation tasks. FIFA addresses the critical challenge of hallucinations in video multimodal models by extracting comprehensive descriptive facts from video content, modeling their semantic dependencies through a Spatio-Temporal Semantic Dependency Graph, and verifying them using VideoQA models. The framework introduces event-level facts to capture composite semantics that atomic facts miss, and explicitly models dependencies between facts to improve reliability. Experimental results demonstrate that FIFA achieves the highest correlation with human judgment compared to existing evaluation methods, while also proposing a Post-Correction framework that leverages FIFA's intermediate results to effectively mitigate hallucinations in both generated text and video outputs.

## Method Summary
FIFA operates through a multi-stage pipeline that first extracts atomic facts from videos using vision-language models, then clusters these into event-level facts to capture composite semantics. The framework constructs a Spatio-Temporal Semantic Dependency Graph to model relationships between facts, and employs multiple VideoQA models to verify fact consistency with video content. For evaluation, FIFA computes faithfulness scores based on the verification results and dependency graph structure. The Post-Correction framework uses FIFA's intermediate fact extraction and dependency modeling to identify and correct hallucinations in generated outputs, improving both text and video quality. The unified design allows FIFA to evaluate both text-to-video and video-to-text generation tasks using the same underlying architecture.

## Key Results
- FIFA achieves the highest correlation with human judgment compared to existing evaluation methods
- Event-level fact extraction improves faithfulness detection by capturing composite semantics missed by atomic facts
- The Post-Correction framework effectively reduces hallucinations in both generated text and video outputs
- FIFA's unified approach works for both text-to-video and video-to-text generation tasks

## Why This Works (Mechanism)
FIFA's effectiveness stems from its comprehensive approach to faithfulness evaluation that addresses the fundamental limitations of existing methods. By extracting both atomic and event-level facts, FIFA captures the full semantic spectrum of video content, from individual objects and actions to complex composite events. The Spatio-Temporal Semantic Dependency Graph models the relationships between facts, providing context that helps VideoQA models verify facts more accurately. Using multiple VideoQA models for verification reduces the impact of individual model limitations and provides more robust results. The Post-Correction framework leverages FIFA's detailed analysis to identify and fix hallucinations at the fact level, making the correction process more precise and effective than holistic approaches.

## Foundational Learning

**Spatio-Temporal Semantic Dependency Graph**
- Why needed: Captures relationships between facts across both spatial and temporal dimensions in video content
- Quick check: Verify that the graph correctly represents dependencies between facts in simple video sequences

**Event-level Fact Extraction**
- Why needed: Atomic facts alone miss composite semantics and complex relationships in video content
- Quick check: Confirm that event-level facts capture multi-object interactions and temporal sequences

**VideoQA-based Fact Verification**
- Why needed: Leverages advanced multimodal reasoning to verify facts against video content
- Quick check: Test verification accuracy on videos with clear ground truth facts

## Architecture Onboarding

**Component Map**
Video Content -> Atomic Fact Extraction -> Event-level Fact Clustering -> Spatio-Temporal Dependency Graph Construction -> VideoQA Verification -> Faithfulness Score Computation -> Post-Correction (optional)

**Critical Path**
Video Content -> Atomic Fact Extraction -> Event-level Fact Clustering -> VideoQA Verification -> Faithfulness Score Computation

**Design Tradeoffs**
- Multiple VideoQA models improve reliability but increase computational cost
- Event-level fact extraction adds semantic coverage but requires more complex clustering algorithms
- Dependency graph modeling improves verification accuracy but adds architectural complexity

**Failure Signatures**
- Low faithfulness scores across multiple VideoQA models may indicate fundamental content mismatch
- High variance in VideoQA model scores suggests ambiguous or poorly defined facts
- Poor event-level clustering results in missed composite semantics

**3 First Experiments**
1. Test atomic fact extraction accuracy on videos with known ground truth facts
2. Evaluate event-level clustering performance on videos with clear composite events
3. Measure VideoQA verification accuracy on benchmark video question-answering datasets

## Open Questions the Paper Calls Out
None

## Limitations
- Reliance on VideoQA models inherits limitations of current multimodal models including reasoning errors and context misunderstanding
- Event-level fact extraction may still miss nuanced semantic relationships requiring deeper semantic understanding
- Evaluation of only five VideoQA models provides limited coverage of available multimodal reasoning systems
- Correlation with human judgment needs validation across diverse video domains and longer-form content

## Confidence

**High**: Technical architecture and experimental methodology are well-documented and reproducible

**Medium**: Effectiveness of event-level fact extraction requires broader validation across different video types and generation tasks

**Low**: Generalization of correlation results to real-world deployment scenarios, particularly for videos outside curated datasets

## Next Checks
1. Test FIFA's performance on videos from diverse domains (e.g., sports, medical, educational) and longer duration content to assess robustness across semantic complexity

2. Compare FIFA's fact extraction and verification accuracy against human-annotated ground truth on a held-out test set

3. Evaluate the post-correction framework's effectiveness on state-of-the-art text-to-video models across multiple generations to measure practical hallucination reduction