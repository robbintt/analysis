---
ver: rpa2
title: Reasoning Models Sometimes Output Illegible Chains of Thought
arxiv_id: '2510.27338'
source_url: https://arxiv.org/abs/2510.27338
tags:
- reasoning
- illegible
- cots
- legibility
- claude
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper examines whether outcome-based reinforcement learning
  causes reasoning models to produce illegible chain-of-thought (CoT) reasoning. The
  author evaluates 14 reasoning models on difficult scientific questions, finding
  that most models trained with outcome-based RL (except Claude) frequently generate
  CoTs that become illegible to both humans and AI monitors, despite returning perfectly
  readable final answers.
---

# Reasoning Models Sometimes Output Illegible Chains of Thought

## Quick Facts
- arXiv ID: 2510.27338
- Source URL: https://arxiv.org/abs/2510.27338
- Authors: Arun Jose
- Reference count: 25
- Most RL-trained reasoning models frequently generate illegible chain-of-thought reasoning that humans and AI monitors cannot understand, despite returning correct answers.

## Executive Summary
This paper examines whether outcome-based reinforcement learning causes reasoning models to produce illegible chain-of-thought (CoT) reasoning. The author evaluates 14 reasoning models on difficult scientific questions, finding that most models trained with outcome-based RL (except Claude) frequently generate CoTs that become illegible to both humans and AI monitors, despite returning perfectly readable final answers. The illegible reasoning contains nonsensical phrases and mixed languages. Experiments show that forcing models to use only legible portions of CoTs reduces performance by 53%, suggesting the illegible reasoning contributes to correct answers. However, no correlation exists between legibility and performance when resampling responses. The author proposes that illegible reasoning may be vestigial tokens that are useful for model training but not causally important for reasoning, rather than evidence of novel reasoning capabilities.

## Method Summary
The paper evaluates 14 reasoning models on GPQA-Diamond, a dataset of 198 difficult scientific questions with answer choices removed. Legibility is scored 1-9 by GPT-4o autograder, with higher scores indicating less legible reasoning. The author conducts two main experiments: prefill interventions that truncate CoTs at illegibility thresholds and force models to answer from partial context, and resampling analysis that correlates legibility with accuracy across multiple responses. The study compares RL-trained models (DeepSeek R1, QwQ) with non-RL models (V3) and Claude models, which maintain legibility despite RL training.

## Key Results
- Most outcome-based RL models (except Claude) produce illegible CoT reasoning with nonsensical phrases and mixed languages, despite correct final answers
- Forcing models to use only legible portions of CoTs reduces performance by 53%, suggesting illegible reasoning contributes to correct answers
- No correlation exists between legibility and performance when resampling responses, supporting the vestigial token hypothesis
- Illegible reasoning becomes more severe on harder questions, with legibility degrading as CoT progresses

## Why This Works (Mechanism)

### Mechanism 1: Vestigial Token Reinforcement
- **Claim:** Illegible CoT tokens may be reinforced by outcome-based RL despite being non-causal to reasoning.
- **Mechanism:** RL upweights entire token trajectories leading to correct answers, including tokens that merely prime subsequent reasoning steps rather than carry semantic content. These "bridge tokens" become increasingly compressed or nonsensical as optimization favors efficiency over legibility.
- **Core assumption:** Gradient signals disproportionately weight high-entropy tokens that push reasoning forward, while low-importance tokens accumulate noise without penalty.
- **Evidence anchors:** [abstract] "illegible reasoning may be vestigial tokens that are useful for model training but not causally important for reasoning"; [Section 4.1] Cites Wang et al. (2025): masking gradients to lowest-entropy 80% of tokens during RL can improve performance.

### Mechanism 2: RL-Induced Compression Without Legibility Constraint
- **Claim:** Outcome-based RL naturally produces opaque reasoning unless explicitly optimized for legibility.
- **Mechanism:** Reward signals based solely on answer correctness provide no gradient pressure toward readable intermediate steps. Models learn internal representations that maximize reward, which may include compressed or encoded reasoning that sacrifices interpretability.
- **Core assumption:** The training objective does not inherently penalize illegibility; Claude's legibility suggests post-training intervention or different RL configuration.
- **Evidence anchors:** [Section 3.1] V3 (non-RL base) produces almost no illegible reasoning; R1 and R1-Zero (RL-trained) show high illegibility; [Section 3.1] Claude models maintain legibility despite outcome-based RL.

### Mechanism 3: Difficulty-Sensitive Token Budget Allocation
- **Claim:** Harder questions trigger more illegible reasoning as models allocate computational budget differently.
- **Mechanism:** On difficult inputs, models extend CoT length and compress reasoning more aggressively. Longer CoTs become less legible per-character as compression intensifies in later segments.
- **Core assumption:** Illegibility is a resource-allocation response, not a capability failure or emergent encoding.
- **Evidence anchors:** [Section 3.2, Figure 4] "legibility degrades on harder questions" with 23% relative increase in illegibility for R1 on hard vs. easy questions; [Appendix A, Figure 6] Illegibility increases as CoT progresses; Spearman correlation 0.54 (p < 0.0001) between position and illegibility.

## Foundational Learning

- **Concept: Chain-of-Thought (CoT) Monitoring**
  - **Why needed here:** The paper's central concern is whether CoT can serve as a reliable oversight mechanism when models produce illegible reasoning.
  - **Quick check question:** Can you explain why a faithful but illegible CoT would undermine monitoring even if the final answer is correct?

- **Concept: Outcome-Based Reinforcement Learning**
  - **Why needed here:** This training paradigm is identified as the causal source of illegibility; understanding its reward structure explains why compression emerges.
  - **Quick check question:** What reward signal component would need to be added to preserve legibility during RL training?

- **Concept: Token Importance Distributions**
  - **Why needed here:** The vestigial token hypothesis relies on understanding that not all tokens contribute equally to reasoning gradients.
  - **Quick check question:** If 80% of tokens in a CoT have near-zero gradient importance, what does this imply for interventions that truncate or paraphrase reasoning?

## Architecture Onboarding

- **Component map:** Reasoning model (e.g., R1, QwQ) -> GPT-4o autograder -> Legibility score -> Prefill intervention or Resampling analysis
- **Critical path:** 1. Query reasoning model on GPQA-Diamond (hard scientific questions, choices removed) 2. Extract full CoT and final answer 3. Score CoT legibility segment-by-segment 4. For intervention: truncate at threshold (score ≥ 6), use as prefill, measure accuracy drop 5. For correlation: sample 100 responses per question, compute per-question legibility-accuracy correlation
- **Design tradeoffs:** Autograder choice trades off scalability against potential bias; prefill vs. resampling tests different aspects of causality vs. correlation; temperature setting affects illegibility but not uniformly across models
- **Failure signatures:** High illegibility with low accuracy (model failed); low illegibility with low accuracy (model reasoned legibly but incorrectly); high illegibility with high accuracy (vestigious reasoning or steganography); prefill causing >50% accuracy drop suggests illegible segments carry task-relevant information
- **First 3 experiments:** 1. Reproduce legibility scoring: Query R1 or QwQ on 20 GPQA-Diamond questions, score CoT segments, verify distribution matches paper 2. Prefill ablation: Truncate CoTs at score ≥ 6, force answer from legible portion, confirm ~50% accuracy drop 3. Difficulty stratification: Bin questions by baseline model accuracy, measure illegibility per bin, confirm positive correlation with difficulty

## Open Questions the Paper Calls Out

- **Do illegible Chain-of-Thought (CoT) tokens function as "vestigial" artifacts or as efficient steganographic encodings?**
  - **Basis in paper:** [explicit] The authors propose the "vestigial reasoning" hypothesis but state in the Limitations that they "cannot definitively determine the causal mechanisms" from observations alone.
  - **Why unresolved:** Observational analysis and prefill experiments cannot distinguish whether the tokens are causally necessary for computation or merely correlated with the model's internal state.
  - **What evidence would resolve it:** Ablation studies targeting specific illegible tokens or white-box mechanistic interpretability analysis of activation patterns.

- **Does maintaining high CoT legibility during outcome-based RL inevitably incur a penalty in model performance or faithfulness?**
  - **Basis in paper:** [explicit] Section 3.1 notes that while Claude models remain legible, "it remains unclear whether this comes at a performance and faithfulness costs."
  - **Why unresolved:** The specific training pipelines for Claude are unknown, and there is no controlled comparison between legibility-optimized and standard RL training runs in the paper.
  - **What evidence would resolve it:** Training identical base models with and without explicit legibility constraints to compare resulting performance ceilings.

- **Does the correlation between question difficulty and CoT illegibility persist in non-scientific or multi-turn agentic tasks?**
  - **Basis in paper:** [explicit] The Limitations section notes the evaluation relies on GPQA-Diamond, which "do not fully represent the range of tasks," suggesting "factors specific to this setup" may influence illegibility rates.
  - **Why unresolved:** The study is restricted to single-turn scientific Q&A; it is unknown if complex, multi-step agentic workflows trigger similar illegibility scaling.
  - **What evidence would resolve it:** Evaluating reasoning models on long-horizon tasks (e.g., software engineering or navigation) and measuring legibility scores relative to task complexity.

## Limitations
- The study relies on GPT-4o as the legibility evaluator, which may introduce evaluator bias and inconsistent scoring across different contexts
- Prefill interventions may introduce out-of-distribution effects that confound the causal relationship between illegible reasoning and correct answers
- The paper does not investigate whether illegible tokens encode information through steganographic methods rather than being truly vestigial
- The experimental setup cannot definitively determine whether illegible tokens are causally necessary for reasoning or merely correlated artifacts

## Confidence

**High Confidence:** The observation that outcome-based RL models (except Claude) produce significantly more illegible reasoning than non-RL models or Claude is well-supported by the data. The prefill experiments demonstrating substantial performance drops when illegible portions are removed provide robust evidence for the relationship between illegibility and performance.

**Medium Confidence:** The vestigial token hypothesis explaining illegible reasoning as non-causal artifacts of RL training is plausible but not definitively proven. The lack of correlation between legibility and performance in resampling experiments supports this view, but does not rule out alternative explanations such as steganographic encoding or different reasoning strategies that sacrifice interpretability for correctness.

**Low Confidence:** The mechanism by which Claude maintains legibility despite outcome-based RL remains unclear. The paper notes this as an anomaly without providing a satisfactory explanation for why Claude's training configuration or post-training interventions succeed where others fail.

## Next Checks
1. **Cross-Evaluator Validation:** Replicate the legibility scoring using multiple independent evaluators (human annotators and different LLM judges) to establish whether GPT-4o's scoring is consistent and unbiased across different evaluation methodologies.

2. **Steganography Detection:** Analyze the distribution and entropy of illegible tokens to determine if they encode information through non-standard patterns that might be semantically meaningful but not easily interpretable through conventional language understanding.

3. **Temporal Stability Analysis:** Track how illegibility evolves during model training and whether targeted interventions at different training stages can preserve legibility without sacrificing performance, helping to distinguish between vestigial tokens and emergent reasoning strategies.