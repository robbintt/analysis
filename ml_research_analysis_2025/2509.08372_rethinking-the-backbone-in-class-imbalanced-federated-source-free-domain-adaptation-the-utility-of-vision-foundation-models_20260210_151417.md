---
ver: rpa2
title: 'Rethinking the Backbone in Class Imbalanced Federated Source Free Domain Adaptation:
  The Utility of Vision Foundation Models'
arxiv_id: '2509.08372'
source_url: https://arxiv.org/abs/2509.08372
tags:
- domain
- source
- target
- adaptation
- vit-s
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study addresses the challenges of Federated Source-Free Domain
  Adaptation (FFREEDA) under severe class imbalance, introducing Class Imbalanced
  FFREEDA (CI-FFREEDA) that combines domain gaps, class imbalance in both source and
  target, and label shifts among clients. The authors propose replacing traditional
  CNN backbones with frozen vision foundation models (VFMs), demonstrating that VFMs
  mitigate domain gaps, class imbalances, and non-IID-ness while reducing computational
  and communication costs.
---

# Rethinking the Backbone in Class Imbalanced Federated Source Free Domain Adaptation: The Utility of Vision Foundation Models

## Quick Facts
- **arXiv ID**: 2509.08372
- **Source URL**: https://arxiv.org/abs/2509.08372
- **Reference count**: 0
- **Primary result**: Replacing CNN backbones with frozen vision foundation models improves CI-FFREEDA accuracy by 10.5–16.6% over ResNet-50

## Executive Summary
This paper addresses Federated Source-Free Domain Adaptation (FFREEDA) under severe class imbalance, introducing Class Imbalanced FFREEDA (CI-FFREEDA) that combines domain gaps, class imbalance in both source and target, and label shifts among clients. The authors propose replacing traditional CNN backbones with frozen vision foundation models (VFMs), demonstrating that VFMs mitigate domain gaps, class imbalances, and non-IID-ness while reducing computational and communication costs. Experiments on Office-Home and VisDA datasets show that VFMs improve accuracy by 10.5–16.6% over ResNet-50, outperforming existing SFDA methods. VFMs also maintain performance under label shifts among clients without requiring complex federated learning methods, suggesting that strong feature extractors are key to real-world federated learning success.

## Method Summary
The paper introduces CI-FFREEDA, a federated source-free domain adaptation framework for class-imbalanced data. The method uses frozen vision foundation models (DINOv2 ViT-S/14 or ViT-B/14) as backbones, with only a bottleneck (256-dim) and classifier trained during adaptation. Source models are pre-trained with balanced sampling on 60% of imbalanced data. Target data is distributed across clients using Dirichlet(α=0.5) sampling, creating non-IID partitions. Federated adaptation uses FedAvg with 10 communication rounds and 5 local epochs per round. SFDA methods (SHOT, NRC, AaD, ISFDA, ICPR) adapt the bottleneck+classifier. The best global model is selected based on client validation scores. Key innovations include freezing the VFM backbone to reduce costs and using VFMs to improve robustness to domain gaps and class imbalance.

## Key Results
- VFMs improve MAR by 10.5–16.6% over ResNet-50 in CI-FFREEDA settings
- Frozen VFMs reduce communication costs by >99% (transmitting <1MB vs 94MB) and FLOPs by ~50%
- Balanced sampling during source training recovers 3-5% MAR for VFMs under severe imbalance
- ViT-B outperforms ViT-S by 6.1% on Office-Home but shows marginal gains on VisDA (12 classes)
- Complex federated learning methods (FedProx, FedETF) provide no consistent benefit over FedAvg when using VFMs

## Why This Works (Mechanism)

### Mechanism 1
Frozen VFMs extract domain-invariant features that reduce accuracy degradation during target domain adaptation. VFMs pre-trained on 142M+ images (DINOv2) produce feature representations that transfer across domains with smaller performance gaps than ImageNet-pretrained CNNs. The pre-training scale and diversity of VFMs yield features that generalize across domains without fine-tuning. Evidence shows ViT-B has 7.3% source-to-target accuracy degradation vs. 17.6% for ResNet-50. This mechanism may fail if target domain differs radically from VFM pre-training distribution (e.g., specialized medical imaging).

### Mechanism 2
Freezing the backbone reduces federated communication and computation while maintaining or improving accuracy. Only bottleneck and classifier parameters (<1 MB) are transmitted between server and clients; backbone forward passes can be pre-computed into feature banks. The bottleneck+classifier have sufficient capacity to adapt to target distributions without backbone updates. Evidence shows ViT-S requires 11.0 G FLOPs vs. 24.6 G for ResNet-50; transmitted model size <1 MB vs. 94 MB. This mechanism may fail if label shift requires learning fundamentally new feature combinations.

### Mechanism 3
VFMs reduce sensitivity to class imbalance during source training and target adaptation. Strong pre-trained features yield more balanced per-class representations, reducing classifier bias toward majority classes. VFM pre-training includes sufficient class diversity that minority-class features are already well-represented. Evidence shows accuracy degradation from balanced to imbalanced is 2.9% for ViT-B vs. 4.4% for ResNet-50. Balanced sampling during source training further closes the gap. This mechanism may fail under extreme imbalance ratios (e.g., 1:1000+).

## Foundational Learning

- **Concept: Federated Source-Free Domain Adaptation (FFREEDA)**
  - Why needed: The problem setting assumes server has labeled source data only during pre-training; clients have unlabeled target data. Understanding this constraint is essential for interpreting why backbone quality matters more than complex adaptation algorithms.
  - Quick check question: Can you explain why conventional domain adaptation (requiring source data access during adaptation) is infeasible in this setting?

- **Concept: Non-IID data distributions in FL**
  - Why needed: Clients have different label distributions (inter-client label shift) plus imbalance within each client. The paper shows VFMs reduce sensitivity to this heterogeneity.
  - Quick check question: How does Dirichlet sampling (α=0.5 used here) create non-IID partitions across clients?

- **Concept: Macro-Averaged Recall (MAR) for imbalanced evaluation**
  - Why needed: The paper uses MAR rather than accuracy because standard accuracy is dominated by majority classes and can be misleading under severe imbalance.
  - Quick check question: Why would standard accuracy be a poor metric if 90% of test samples belong to a single class?

## Architecture Onboarding

- **Component map**: Frozen VFM (DINOv2 ViT-S/14 or ViT-B/14) → bottleneck (256-dim) → classifier (trainable)
- **Critical path**: 1) Pre-train bottleneck+classifier on labeled source data with balanced sampling (frozen VFM backbone) 2) Distribute frozen VFM once to all clients (no repeated transmission) 3) Clients perform local SFDA adaptation (e.g., SHOT) using pseudo-labels 4) Aggregate only bottleneck+classifier via FedAvg 5) Server selects best global model based on client validation scores
- **Design tradeoffs**: ViT-S vs. ViT-B: ViT-B yields +6.1% accuracy on Office-Home but 4× parameters; marginal gain on VisDA suggests smaller models suffice for fewer classes. Frozen vs. fine-tuned backbone: Frozen reduces costs dramatically; fine-tuning shows marginal accuracy gains but requires full model transmission. Lightweight models (TinyViT-5M, LightViT-Tiny): Lower accuracy but viable for edge deployment.
- **Failure signatures**: Source model trained on imbalanced data without balanced sampling shows 5-10% lower MAR. ResNet-50 backbones show high variance across runs in VisDA, suggesting instability under severe imbalance. Non-IID FL methods (FedProx, FedETF) provide no consistent benefit over FedAvg when using VFMs.
- **First 3 experiments**: 1) Reproduce backbone comparison: Train source model on Office-Home Clipart with balanced sampling; adapt to Product using SHOT+FedAvg; compare ResNet-50 vs. ViT-S vs. ViT-B. Expected: ViT-B > ViT-S > ResNet-50 by 10-15% MAR. 2) Ablate balanced sampling: Train source models with and without balanced sampling on imbalanced source data. Expected: Balanced sampling recovers 3-5% MAR for VFMs. 3) Test communication efficiency: Measure FLOPs and transmission size for ViT-S (frozen) vs. ResNet-50 (fine-tuned) over 10 communication rounds. Expected: ViT-S reduces FLOPs by ~50% and transmission by >99%.

## Open Questions the Paper Calls Out

### Open Question 1
Does the frozen VFM approach maintain its performance advantage in multi-source or multi-target federated domain adaptation scenarios? The authors explicitly state they excluded methods like LADD and FedWCA because they are designed for multi-target adaptation, whereas "we focus on single-target adaptation in this paper." The robustness of VFMs to label shifts and domain gaps was only tested in a single-source-single-target setup, leaving interactions between multiple diverse domains unexamined.

### Open Question 2
What specific fine-tuning strategies are required for lightweight VFMs (e.g., TinyViT) to prevent the performance degradation observed with standard fine-tuning in CI-FFREEDA? Appendix C notes that fine-tuning lightweight models resulted in decreased accuracy, suggesting "more sensitive adjustment may be necessary," while frozen backbones remained robust. The paper demonstrates that freezing is better than standard fine-tuning for light models but does not explore adaptive learning rates or adapter layers that might close the performance gap with larger VFMs.

### Open Question 3
To what extent does the domain invariance of frozen VFMs render complex data augmentation pipelines redundant or detrimental in class-imbalanced SFDA? The authors note that ICPR (a heavy augmentation method) showed an advantage with ResNet-101 that "diminishes when using VFMs," suggesting VFM features mitigate augmentation-induced variations. The paper compares methods "as-is" but does not isolate the specific contribution of the augmentation component versus the loss function when using VFM backbones.

## Limitations
- Generalizability to domains vastly different from VFM pre-training (e.g., medical imaging) remains uncertain
- Extreme imbalance ratios (e.g., 1:1000+) may overwhelm VFM robustness, particularly for rare classes absent from pre-training data
- Claims about communication efficiency are supported by theoretical arguments but lack direct empirical validation

## Confidence

- VFM accuracy improvements over CNNs: **High**
- Communication/computation cost reductions: **Medium** (theoretical support, limited empirical data)
- VFM robustness to class imbalance and label shifts: **Medium-High** (supported but with edge cases)
- Generalizability to domains outside pre-training distribution: **Low-Medium**

## Next Checks

1. Test VFM performance on specialized domains (medical imaging, satellite imagery) with known domain gaps to validate cross-domain generalization claims
2. Systematically vary class imbalance ratios (1:10, 1:50, 1:100) to identify breaking points for VFM robustness
3. Compare frozen VFM performance against fine-tuned smaller models (MobileNet, EfficientNet) to assess whether strong features or model scale drives improvements