---
ver: rpa2
title: 'ACCESS : A Benchmark for Abstract Causal Event Discovery and Reasoning'
arxiv_id: '2502.08148'
source_url: https://arxiv.org/abs/2502.08148
tags:
- causal
- event
- person
- events
- abstract
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ACCESS, a benchmark for abstract causal event
  discovery and reasoning in natural language processing. The benchmark provides a
  graphical modeling of causal relations among 725 abstract events, extracted from
  9,513 stories in the GLUCOSE dataset.
---

# ACCESS : A Benchmark for Abstract Causal Event Discovery and Reasoning

## Quick Facts
- **arXiv ID:** 2502.08148
- **Source URL:** https://arxiv.org/abs/2502.08148
- **Reference count:** 40
- **Primary result:** ACCESS improves LLM QA reasoning performance by up to 20% through abstract causal knowledge injection

## Executive Summary
This paper introduces ACCESS, a benchmark for abstract causal event discovery and reasoning in natural language processing. The benchmark provides a graphical modeling of causal relations among 725 abstract events, extracted from 9,513 stories in the GLUCOSE dataset. The authors propose a pipeline combining automatic clustering and human annotation to identify abstract events and their causal relationships. Experiments show that while LLMs and statistical methods struggle with event abstraction and causal discovery, incorporating abstract causal knowledge from ACCESS significantly improves QA reasoning performance in LLMs by up to 20%. The benchmark addresses the challenge of evaluating causal reasoning beyond lexical cues and provides a foundation for future research in abstract causal event understanding.

## Method Summary
ACCESS is built by first extracting event mentions from GLUCOSE stories, then clustering these mentions into abstract event generalizations using PIVOT clustering and Sentence Transformers. Human annotators refine these clusters and identify causal relationships between them. The resulting graph contains 725 abstract events and 1,400+ causal pairs. For evaluation, LLMs are tested on both pairwise causal discovery (classifying whether event pairs are causally related) and QA reasoning tasks where the ACCESS causal graph is injected as context to improve performance.

## Key Results
- LLMs show significant improvement in QA reasoning (up to 20% gain) when provided with ACCESS causal context
- Automatic statistical methods (PC, NOTEARS) fail to recover causal structures from binary co-occurrence data
- Abstraction pipeline achieves 82.3% accuracy on ground-truth GLUCOSE generalizations
- Binary representation of causal relationships proves insufficient for statistical structure learning

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Event abstraction functions as a semantic noise-reduction filter, enabling causal discovery algorithms to operate on normalized variables rather than sparse lexical mentions.
- **Mechanism:** The pipeline aggregates specific event mentions into abstract events, mapping high-dimensional, noisy text into lower-dimensional random variables. By treating these clusters as nodes in a graph, the system reduces the complexity of the co-occurrence matrix, allowing statistical solvers to identify conditional independence patterns.
- **Core assumption:** Semantic equivalence implies causal equivalence within the context of the story corpus.
- **Evidence anchors:** Abstract claims 1,400 causal pairs extracted from abstractions; section states abstraction leverages causality theories without lexical cues.
- **Break condition:** If clustering over-generalizes (e.g., mapping distinct events to "A person does something"), causal specificity is lost.

### Mechanism 2
- **Claim:** Injecting explicit abstract causal knowledge into the prompt context offloads the "reasoning" burden from the LLM, converting it into a recognition and mapping task.
- **Mechanism:** LLMs struggle with non-contextual causal intuition. ACCESS bypasses this by retrieving relevant causal rules and appending them to the prompt. The LLM maps abstract rules to specific story entities rather than inferring causality.
- **Core assumption:** The LLM possesses sufficient "concretization" capability to map abstract rules back to specific story entities.
- **Evidence anchors:** Abstract reports 20% improvement in QA reasoning; section notes large improvements with causal graph context.
- **Break condition:** If the story contains specific logical qualifiers (e.g., "needs money but is too lazy to work"), the abstract rule may mislead the model.

### Mechanism 3
- **Claim:** Statistical structure learning fails on narrative data when relying solely on binary co-occurrence due to the "semantics-gap" in representation.
- **Mechanism:** Algorithms like PC and NOTEARS fail on binary presence/absence matrices because binary data cannot capture the strength, polarity, or temporal ordering of causal links, only correlation.
- **Core assumption:** The "true" causal graph is recoverable purely from observational story statistics without interventional data.
- **Evidence anchors:** Abstract notes difficulties due to limited scale and reliance on lexical cues; section highlights challenges for statistical algorithms.
- **Break condition:** If representation were enriched (e.g., using embeddings) or corpus size vastly increased, this failure might be mitigated.

## Foundational Learning

**Concept: Counterfactual Theory of Causation**
- **Why needed here:** ACCESS defines ground truth using Lewis's theory ("y would not occur if x did not occur"). Understanding this distinguishes true causality from mere correlation.
- **Quick check question:** If "The sun rises" and "The cock crows" always co-occur in stories, would ACCESS mark them as causal?

**Concept: Constraint-Based Causal Discovery (PC Algorithm)**
- **Why needed here:** The authors use the PC algorithm as a baseline for automatic discovery. Understanding conditional independence tests is needed to diagnose algorithm failure.
- **Quick check question:** Why does a sparse co-occurrence matrix make determining conditional independence unreliable?

**Concept: Event Abstraction / Conceptualization**
- **Why needed here:** The core contribution is the hierarchy: Mention → Generalization → Abstraction. Evaluating the system requires distinguishing valid abstraction from over-generalization.
- **Quick check question:** Is "A person moves" a valid abstraction for "A person drives to work," or is it too coarse?

## Architecture Onboarding

**Component map:** Source (GLUCOSE Stories) → Abstraction Engine (PIVOT Clustering + Sentence Transformers + Human Sub-clustering) → Graph Constructor (PC Algorithm + GLUCOSE Annotations + Human Validation) → Inference Interface (LLM + ACCESS Causal Context Injection)

**Critical path:** The Abstraction Engine. If clustering fails to produce causally consistent clusters, subsequent graph construction will be meaningless noise.

**Design tradeoffs:**
- **Automation vs. Quality:** Heavy reliance on human annotation for sub-clustering and causal validation. Fully automated pipeline showed 40% degradation.
- **Binary vs. Dense Representation:** Binary variables for PC algorithm resulted in poor performance, trading semantic nuance for statistical simplicity.

**Failure signatures:**
- **Over-generalization:** LLMs producing "A person feels an emotion" for "A person is scared," destroying causal specificity.
- **Temporal Bias:** LLMs assuming A→B simply because A appears before B in the story.

**First 3 experiments:**
1. **Abstraction Validation:** Run PIVOT clustering on GLUCOSE subset and measure Rand Index against human-annotated clusters to calibrate similarity threshold (0.7).
2. **Pairwise Discovery Baseline:** Prompt GPT-4o-mini with 100 ACCESS pairs and 100 random pairs to establish "non-contextual" F1 baseline.
3. **Context Injection Ablation:** Run QA task with and without causal graph context to replicate 20% performance gain.

## Open Questions the Paper Calls Out

**Open Question 1:** How can abstract events be accurately identified from real-world corpora when ground-truth annotations are unavailable?
- **Basis:** Authors explicitly ask this in Section 4.1
- **Why unresolved:** LLMs over-generalize (e.g., "A person do something") and automatic clustering fails to align with human judgments
- **What evidence would resolve it:** A method achieving high cluster purity and semantic consistency without manual annotation

**Open Question 2:** What representation learning methods can capture causal knowledge more effectively than simple co-occurrence matrices?
- **Basis:** Appendix D notes statistical algorithms performed poorly, motivating research into abstract causal representation learning
- **Why unresolved:** Standard statistical algorithms failed to recover graph edges, implying input features lacked necessary semantic expressiveness
- **What evidence would resolve it:** New representation technique significantly improving F1 score and SHD in causal graph recovery

**Open Question 3:** Can abstract causal graphs be leveraged for interventional and counterfactual reasoning?
- **Basis:** Limitations section states future work could leverage the graph for causal inference according to Pearl's engine
- **Why unresolved:** Current work focuses on structure discovery (Rung 1) and reasoning via QA, but hasn't validated utility for complex causal effects or counterfactuals
- **What evidence would resolve it:** Successful execution of do-calculus queries or counterfactual predictions using ACCESS graph

## Limitations
- ACCESS is built from GLUCOSE, which may limit generalizability to other domains
- Heavy reliance on human annotation makes the pipeline difficult to scale
- Binary representation of events proves insufficient for statistical structure learning
- Abstraction process may lose important semantic details needed for fine-grained causal reasoning

## Confidence
- **Medium:** 20% performance gain measured on GLUCOSE-QA constructed from same corpus used to build ACCESS (potential data leakage)
- **Low:** No ablation studies isolating contribution of different abstraction levels; PIVOT threshold empirically chosen without sensitivity analysis
- **Medium:** Statistical structure learning failure demonstrated on binary co-occurrence, but richer representations not explored

## Next Checks
1. **Cross-corpus generalization:** Test ACCESS integration on QA tasks from datasets independent of GLUCOSE (e.g., MCTest, NarrativeQA)
2. **Abstraction fidelity analysis:** Measure semantic drift between GLUCOSE generalizations and ACCESS abstractions using SBERT similarity distributions
3. **Temporal structure recovery:** Augment co-occurrence matrix with temporal ordering information and re-run structure learning algorithms