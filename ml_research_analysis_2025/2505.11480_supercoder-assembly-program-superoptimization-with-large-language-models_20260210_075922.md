---
ver: rpa2
title: 'SuperCoder: Assembly Program Superoptimization with Large Language Models'
arxiv_id: '2505.11480'
source_url: https://arxiv.org/abs/2505.11480
tags:
- code
- arxiv
- assembly
- program
- speedup
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether large language models can act as
  superoptimizers, generating assembly programs that outperform code already optimized
  by industry-standard compilers. The authors construct a large-scale benchmark of
  8,072 assembly programs (averaging 130 lines) and evaluate 23 LLMs on this task.
---

# SuperCoder: Assembly Program Superoptimization with Large Language Models

## Quick Facts
- arXiv ID: 2505.11480
- Source URL: https://arxiv.org/abs/2505.11480
- Reference count: 26
- This paper investigates whether large language models can act as superoptimizers, generating assembly programs that outperform code already optimized by industry-standard compilers.

## Executive Summary
This paper explores the capability of large language models to function as superoptimizers for assembly programs, aiming to generate code that surpasses the performance of industry-standard compiler optimizations. The authors construct a comprehensive benchmark of 8,072 assembly programs and evaluate 23 different LLMs on this task. Through reinforcement learning fine-tuning with a reward function that balances correctness and performance, they develop SuperCoder, which achieves 95.0% correctness and a 1.46x average speedup over gcc -O3 optimization. The work establishes a foundational approach for using LLMs in program performance optimization beyond traditional compiler heuristics.

## Method Summary
The authors construct a large-scale benchmark of 8,072 assembly programs averaging 130 lines in length. They evaluate 23 different LLMs on this superoptimization task, with Claude-opus-4 serving as the strongest baseline at 51.5% test-passing rate and 1.43x average speedup. To improve performance, they employ reinforcement learning fine-tuning using a reward function that integrates both correctness and performance metrics. The resulting SuperCoder model achieves 95.0% correctness and 1.46x average speedup. Additional improvements come from Best-of-N sampling and iterative refinement techniques.

## Key Results
- SuperCoder achieves 95.0% correctness rate on the assembly superoptimization benchmark
- SuperCoder provides 1.46x average speedup over gcc -O3, compared to Claude-opus-4's 1.43x
- This represents the first demonstration of LLMs successfully applied as superoptimizers for assembly programs

## Why This Works (Mechanism)
Large language models can leverage their understanding of code patterns and optimization techniques to generate assembly programs that outperform compiler-generated code. Through reinforcement learning fine-tuning with carefully designed reward functions that balance correctness and performance, the models learn to identify and apply optimization opportunities that compilers might miss. The iterative refinement and Best-of-N sampling strategies help the model explore multiple optimization paths and select the most effective solutions.

## Foundational Learning
- **Assembly language fundamentals**: Understanding low-level programming constructs and instruction sets is essential for evaluating and generating assembly code optimizations.
- **Compiler optimization techniques**: Knowledge of how compilers optimize code (like gcc -O3) provides the baseline for measuring improvements and understanding what optimizations might be missed.
- **Reinforcement learning for code generation**: The RL fine-tuning process requires understanding how reward functions can guide language models toward generating correct and performant code.
- **Performance benchmarking**: Measuring and comparing execution speed requires familiarity with benchmarking methodologies and metrics.
- **Large language model fine-tuning**: Understanding how to adapt pre-trained models for specific tasks like code optimization is crucial for implementing the SuperCoder approach.
- **Assembly correctness verification**: Ensuring generated assembly programs produce correct outputs is critical, requiring knowledge of testing methodologies for low-level code.

## Architecture Onboarding

**Component Map**: Dataset -> LLM Evaluation -> Reward Function -> RL Fine-tuning -> SuperCoder -> Best-of-N + Iterative Refinement -> Final Output

**Critical Path**: The most time-consuming component is the RL fine-tuning process, which requires multiple iterations of generating assembly code, executing it, measuring performance, and updating model weights based on the reward function.

**Design Tradeoffs**: The authors balance between correctness (ensuring generated code produces correct outputs) and performance (maximizing speed improvements). They choose reinforcement learning over supervised learning because it can optimize for both objectives simultaneously through the reward function.

**Failure Signatures**: Common failure modes include generating incorrect assembly code that fails to produce correct outputs, creating code that is slower than the baseline, or producing code that doesn't generalize beyond the training distribution.

**3 First Experiments**:
1. Evaluate SuperCoder on a small subset of the benchmark to verify basic functionality and measure initial performance gains
2. Test the impact of different reward function weights on the balance between correctness and performance
3. Compare Best-of-N sampling with different values of N to find the optimal tradeoff between computation cost and performance improvement

## Open Questions the Paper Calls Out
None

## Limitations
- The evaluation benchmark of 8,072 programs may not fully represent the diversity of real-world assembly code scenarios
- Performance comparisons are limited to gcc -O3, with no evaluation against other compilers like Clang or ICC
- The generalization capability of SuperCoder to unseen code patterns or different instruction sets remains unclear
- The RL fine-tuning process and reward function design could introduce biases not fully explored

## Confidence

- **High Confidence**: The demonstration that LLMs can be fine-tuned to improve assembly program performance with measurable speedups and correctness
- **Medium Confidence**: The relative performance comparison between SuperCoder and Claude-opus-4 baseline, given the benchmark size and scope
- **Medium Confidence**: The effectiveness of Best-of-N sampling and iterative refinement techniques, as results may vary with different program characteristics

## Next Checks

1. Evaluate SuperCoder on additional benchmarks including programs optimized by Clang and ICC to establish broader compiler comparison
2. Test SuperCoder's generalization by evaluating on assembly programs from different domains (e.g., cryptographic code, numerical computing) not represented in the original benchmark
3. Conduct ablation studies to isolate the contribution of RL fine-tuning versus architectural improvements or training data quality