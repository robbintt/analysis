---
ver: rpa2
title: 'Morality is Contextual: Learning Interpretable Moral Contexts from Human Data
  with Probabilistic Clustering and Large Language Models'
arxiv_id: '2512.21439'
source_url: https://arxiv.org/abs/2512.21439
tags:
- moral
- scenarios
- context
- each
- scenario
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: COMETH introduces a probabilistic context learner combined with
  LLM-based preprocessing and feature generalization to model context-sensitive moral
  judgments. It clusters scenarios into interpretable moral contexts using ternary
  human evaluations and learns feature weights for transparent predictions.
---

# Morality is Contextual: Learning Interpretable Moral Contexts from Human Data with Probabilistic Clustering and Large Language Models

## Quick Facts
- **arXiv ID**: 2512.21439
- **Source URL**: https://arxiv.org/abs/2512.21439
- **Reference count**: 40
- **Primary result**: COMETH improves moral judgment alignment with humans from ~30% to ~60% compared to end-to-end LLMs.

## Executive Summary
COMETH is a framework for modeling context-sensitive moral judgments by clustering scenarios into interpretable moral contexts using human evaluation distributions. It combines LLM-based preprocessing, probabilistic clustering, and interpretable feature learning to achieve significantly better alignment with human moral judgments than black-box approaches. The method produces transparent, context-aware moral evaluations that can explain why similar actions receive different moral ratings in different contexts.

## Method Summary
COMETH processes moral scenarios through a pipeline: first, scenarios are standardized by extracting core actions via LLMs and clustering them using sentence embeddings; then, scenarios sharing the same core action are grouped by their human judgment distributions using probabilistic clustering; finally, interpretable binary features are extracted for each context and used to predict moral judgments with learned feature weights. The approach uses ternary human evaluations (Blame/Neutral/Support) and aims to capture how moral context changes judgment for the same action.

## Key Results
- COMETH achieves ~60% alignment with human majority judgments versus ~30% for end-to-end LLM prompting
- Pre-processing produces robust, reproducible core-action clusters (ARI up to 0.90 for Qwen-80B)
- Probabilistic clustering yields stable, fine-grained contexts that improve prediction accuracy
- The feature-based model provides interpretable explanations and outperforms black-box classifiers (82% accuracy vs ≤54%)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Clustering scenarios based on human moral judgment distributions improves alignment with human majority judgments compared to end-to-end LLM prompting.
- Mechanism: The Probabilistic Context Learner groups scenarios sharing the same core action into "moral contexts" by comparing ternary judgment distributions via KL divergence and merging similar clusters via semi-weighted Jensen-Shannon divergence. This yields context-specific reward distributions that capture normative variability in how humans judge similar actions in different contexts.
- Core assumption: Human moral disagreement for the same action is structured and can be captured by discrete probabilistic clusters rather than a single monolithic judgment.

### Mechanism 2
- Claim: LLM-extracted binary contextual features with learned weights enable interpretable prediction of moral judgments and improve generalization.
- Mechanism: The Generalization module uses an LLM to extract non-evaluative, binary features per cluster, encodes scenarios as feature vectors, and learns feature importance weights by minimizing negative log-likelihood of cluster assignments. Predictions are made via softmax over similarity scores.
- Core assumption: Moral context can be approximated by a small set of semantically meaningful, non-evaluative features that humans would recognize as contextually relevant.

### Mechanism 3
- Claim: LLM-based preprocessing with embedding clustering produces robust, semantically coherent core-action groupings that enable action-specific context learning.
- Mechanism: Scenarios are filtered by LLMs into a standardized "to + verb + complement" format, embedded with all-MiniLM-L6-v2, and clustered via K-means into core actions. This ensures the context learner groups only scenarios with semantically equivalent actions.
- Core assumption: Moral evaluation is action-specific, and conflation of semantically distinct actions would corrupt context learning.

## Foundational Learning

- Concept: **Probabilistic Clustering of Human Distributions**
  - Why needed here: COMETH's core novelty is grouping moral scenarios by the distribution of human ternary judgments, not by textual similarity. Understanding divergence measures (KL, Jensen-Shannon) and online clustering is essential.
  - Quick check question: Given two scenarios with judgment distributions {0.2, 0.5, 0.3} and {0.8, 0.1, 0.1}, would a low KL divergence indicate they belong in the same moral context? Why or why not?

- Concept: **LLM-based Feature Extraction for Interpretability**
  - Why needed here: The generalization module relies on LLMs to produce binary, non-evaluative features. Engineers must understand prompt design to avoid evaluative language and ensure features are contextually discriminative.
  - Quick check question: If an LLM returns the feature "cruel intent," how would this violate the non-evaluative constraint, and what risk does it pose to interpretability?

- Concept: **Embedding-based Action Standardization**
  - Why needed here: Preprocessing clusters scenarios by core action using sentence embeddings. Familiarity with sentence transformers and K-means is necessary to debug or extend this pipeline.
  - Quick check question: Why is it important to standardize actions (e.g., "inject lethal drugs" vs. "perform euthanasia") before clustering judgment distributions?

## Architecture Onboarding

- Component map:
  Human Judgment Collection -> Pre-processing (LLM filter → embedding → K-means) -> Probabilistic Context Learner (KL divergence → merging) -> Generalization (LLM feature extraction → NLL training → softmax prediction)

- Critical path:
  Data quality (judgment distribution noise) → preprocessing accuracy (core action clustering) → context granularity (threshold tuning Δa, Δm) → feature relevance (LLM prompt quality) → alignment rate. The paper's reported ~60% alignment depends on all stages functioning correctly.

- Design tradeoffs:
  - Cluster granularity vs. stability: Lower Δm merges more clusters, increasing stability but potentially losing fine-grained contexts; higher Δm preserves detail but risks instability.
  - LLM size vs. reproducibility: Larger LLMs yield higher pre-processing ARI but may not be deployable in resource-constrained settings.
  - Interpretability vs. alignment: The feature-based model is interpretable but may underfit compared to a black-box classifier.

- Failure signatures:
  - Low alignment rate: Check preprocessing ARI, judgment distribution noise, or feature quality
  - Cluster instability across runs: Likely Δm too high or insufficient scenarios per action
  - Poor feature interpretability: LLM prompt may be under-specified

- First 3 experiments:
  1. Parameter sensitivity analysis: Vary Δa and Δm around the reported optimum to understand robustness
  2. Feature ablation: Replace LLM-extracted features with random features to quantify semantic feature contribution
  3. Baseline comparison: Replicate the end-to-end LLM baseline to validate the ~30% vs. ~60% gap

## Open Questions the Paper Calls Out

- **Scalability to diverse moral actions**: Can COMETH maintain interpretability and alignment performance when scaled to significantly larger datasets with more diverse moral actions beyond the six tested?
- **Active learning integration**: Does incorporating active learning to query humans in regions of high uncertainty improve the efficiency of moral context learning compared to random sampling?
- **Uncertainty-aware predictions**: Does replacing the majority-label decision rule with calibrated uncertainty estimates or abstention mechanisms better reflect the inherent ambiguity of moral judgments?

## Limitations

- Dataset dependency: Performance highly dependent on quality and representativeness of the 300 curated scenarios and human judgments
- Threshold sensitivity: Clustering thresholds are tuned on synthetic distributions, not actual human data
- Resource intensity: Requires multiple large LLMs and extensive human annotation, raising scalability concerns
- Limited generalizability: Current study limited to six core actions and may not generalize to broader populations or cultural contexts

## Confidence

- **High Confidence**: The core mechanism of clustering scenarios by judgment distributions and learning context-specific features is well-supported by results
- **Medium Confidence**: The alignment improvement (~60% vs. ~30%) is demonstrated but conditions and robustness are not fully characterized
- **Low Confidence**: Claims about advancing AI alignment are aspirational without evidence for real AI system deployment

## Next Checks

1. **Threshold robustness test**: Vary Δa and Δm systematically on the full dataset to reveal if reported thresholds are brittle or robust
2. **Feature quality audit**: Manually inspect LLM-extracted features for evaluative or non-discriminative content and measure impact on performance
3. **Cross-dataset generalization**: Apply COMETH to a different moral judgment dataset to test generalizability beyond the authors' curated scenarios