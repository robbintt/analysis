---
ver: rpa2
title: 'MODE: Mixture of Document Experts for RAG'
arxiv_id: '2509.00100'
source_url: https://arxiv.org/abs/2509.00100
tags:
- mode
- retrieval
- cluster
- hotpotqa
- squad
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MODE presents a lightweight RAG framework that replaces vector
  database and re-ranking steps with a cluster-and-route mechanism. Documents are
  embedded, grouped into semantically coherent clusters using HDBSCAN and KMeans,
  and represented by cached centroids.
---

# MODE: Mixture of Document Experts for RAG

## Quick Facts
- arXiv ID: 2509.00100
- Source URL: https://arxiv.org/abs/2509.00100
- Reference count: 20
- Key result: Achieves 0.70-0.89 GPT-4o accuracy and 0.74-0.95 BERT F1 on HotpotQA/SQuAD while reducing latency by over an order of magnitude compared to traditional RAG

## Executive Summary
MODE presents a lightweight RAG framework that replaces vector database and re-ranking steps with a cluster-and-route mechanism. Documents are embedded, grouped into semantically coherent clusters using HDBSCAN and KMeans, and represented by cached centroids. At query time, routing to the top centroid(s) and retrieving context within those clusters enables faster retrieval without external infrastructure. On HotpotQA and SQuAD with 100-500 chunks, MODE achieves GPT-4o accuracy of 0.70-0.89 and BERT F1-scores of 0.74-0.95, matching or exceeding a traditional RAG baseline that uses FAISS and a cross-encoder re-ranker. Latency is reduced by over an order of magnitude, while ablation studies confirm that cluster granularity and multi-cluster routing control the recall/precision trade-off.

## Method Summary
MODE eliminates external vector database and re-ranking infrastructure by clustering documents and routing queries to thematically coherent clusters. The ingestion pipeline chunks documents into 300-token windows with 15% overlap, embeds them using GIST-large, then applies HDBSCAN followed by KMeans for large clusters to create semantically coherent groups. Each cluster is represented by a cached centroid. At inference, query embeddings are compared against centroids (O(Md) where M is cluster count), the top-m clusters are selected, and context is retrieved from within those clusters only. This approach trades fine-grained nearest-neighbor search for faster, infrastructure-free retrieval while maintaining accuracy through thematic coherence.

## Key Results
- On HotpotQA and SQuAD with 100-500 chunks, MODE achieves GPT-4o accuracy of 0.70-0.89 and BERT F1-scores of 0.74-0.95
- Latency reduced by over an order of magnitude compared to traditional RAG with FAISS and cross-encoder re-ranker
- Matches or exceeds traditional RAG baseline performance while eliminating external vector database and re-ranking infrastructure
- Ablation studies show cluster granularity and multi-cluster routing (m=1 vs m=2) control the recall/precision trade-off

## Why This Works (Mechanism)

### Mechanism 1: Centroid Routing as Efficient NN Proxy
Replacing vector database ANN search with centroid comparison reduces retrieval complexity from corpus-dependent to cluster-count-dependent. Query embedding is compared against M cached centroids (O(M·d)) rather than all N chunks. Top-m clusters are selected, and retrieval occurs only within those clusters. The core assumption is that cluster tightness ensures centroid distance approximates member distances. This mechanism fails when clusters have high intra-cluster variance, causing centroid distance to poorly approximate member relevance.

### Mechanism 2: Cluster Hypothesis for Thematic Coherence
Retrieving diverse chunks from a thematically coherent cluster yields better signal-to-noise than top-k chunks across entire corpus. Clusters formed via HDBSCAN+KMeans group semantically related documents. Query routes to best-matching topic, retrieving multiple perspectives within that theme rather than potentially redundant near-duplicates from global search. The core assumption is that documents clustering together are relevant to the same information needs. This mechanism fails for queries requiring cross-topic synthesis or multi-hop reasoning across distant themes.

### Mechanism 3: Re-ranking Elimination via Cluster Pre-selection
Cluster-based pre-filtering removes need for cross-encoder re-ranking while maintaining retrieval quality. Traditional RAG retrieves top-k then re-ranks with expensive cross-encoder. MODE restricts candidate pool to pre-filtered cluster(s), making simple embedding similarity sufficient for final selection. The core assumption is that tight clusters reduce intra-cluster variance enough that embedding similarity is discriminative without re-ranking. This mechanism fails when fine-grained distinctions within clusters require re-ranking to resolve.

## Foundational Learning

- **Concept: Cluster Hypothesis (IR)**
  - Why needed here: Foundational justification for MODE's design—understanding why clustering before retrieval is theoretically sound
  - Quick check question: Given a query about "neural network optimization," would documents about "SGD" and "Adam" likely cluster together?

- **Concept: Embedding Space Geometry**
  - Why needed here: MODE relies on centroid-to-query distance as retrieval proxy; understanding embedding similarity is essential
  - Quick check question: If two documents have cosine similarity 0.95, what does their centroid approximately represent?

- **Concept: HDBSCAN Density-Based Clustering**
  - Why needed here: MODE uses HDBSCAN for initial cluster discovery; understanding its outlier handling and variable cluster sizes is critical for tuning
  - Quick check question: How does HDBSCAN handle points that don't belong to any dense region?

## Architecture Onboarding

- **Component map**: Chunker → Embedder → HDBSCAN → KMeans → Centroid Cache → Query Embedder → Centroid Matcher → Intra-cluster Retriever → LLM

- **Critical path**: 
  1. Cluster quality determines everything—validate intra-cluster similarity during ingestion
  2. Centroid matching is single point of failure—test boundary queries
  3. Intra-cluster retrieval depth (p) controls context window usage

- **Design tradeoffs**:
  - Cluster granularity (min_cluster_size): Fewer clusters = faster routing but broader topics; more clusters = precision but risk routing errors
  - Multi-cluster routing (m=1 vs m=2): Single cluster maximizes speed; dual clusters provides robustness for ambiguous queries
  - Intra-cluster retrieval (p): More chunks = richer context but longer LLM inference

- **Failure signatures**:
  - Low BERTScore with high GPT Accuracy: Retrieved context is topically wrong but LLM hallucinates correct answer
  - Accuracy drops with corpus size: Clusters becoming too broad; need to increase cluster count
  - Query latency spikes: Centroid cache not loaded; falling back to re-computation

- **First 3 experiments**:
  1. Baseline validation: Replicate HotpotQA results with 100 chunks; compare BERT F1 against paper's reported 0.8154 (m=1) and 0.7493 (m=2)
  2. Cluster tightness analysis: Measure intra-cluster cosine similarity; correlate with retrieval hit rate to validate centroid-proxy assumption
  3. Boundary query testing: Construct queries semantically between clusters; measure m=1 vs m=2 accuracy difference to calibrate robustness needs

## Open Questions the Paper Calls Out
None specified in the paper.

## Limitations
- Lacks critical hyperparameters for HDBSCAN clustering (min_cluster_size, minimum samples) and KMeans threshold for splitting large clusters, making exact reproduction difficult
- Ablation studies only vary m and corpus size, not fundamental clustering parameters, leaving questions about robustness to different document distributions
- Uses GPT-4o as both retriever and judge, potentially introducing circular validation where the same model benefits from and evaluates MODE's design

## Confidence
- **High Confidence**: Latency improvements and computational complexity claims (O(Md) vs O(Nd)) are well-supported by the algorithmic design and ablation studies
- **Medium Confidence**: GPT-4o accuracy and BERT F1 scores are reported across multiple corpus sizes, but lack of hyperparameter details and single LLM judge reduces reproducibility confidence
- **Low Confidence**: Theoretical justification for eliminating re-ranking through cluster pre-filtering is primarily based on internal metrics rather than direct comparison of re-ranking necessity

## Next Checks
1. **Hyperparameter Sensitivity Analysis**: Reproduce HotpotQA results while systematically varying HDBSCAN min_cluster_size (e.g., 3, 5, 10) and KMeans split threshold. Measure how these affect intra-cluster similarity, retrieval recall, and final accuracy to establish robustness bounds.

2. **Cross-Judge Validation**: Re-run the 100-question evaluation using both GPT-4o and a different LLM judge (e.g., Claude 3.5 or GPT-4 Turbo) on the same retrieved contexts. Compare accuracy scores to assess whether reported gains are judge-specific or generalize across evaluators.

3. **Re-ranking Necessity Test**: Take the top-5 chunks retrieved by MODE from the same clusters used in main evaluation, then apply the cross-encoder re-ranker from baseline to these chunks. Compare re-ranked results against MODE's original retrieval to quantify actual benefit of eliminating re-ranking in this clustered setting.