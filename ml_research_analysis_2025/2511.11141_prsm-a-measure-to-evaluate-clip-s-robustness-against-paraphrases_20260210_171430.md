---
ver: rpa2
title: 'PRSM: A Measure to Evaluate CLIP''s Robustness Against Paraphrases'
arxiv_id: '2511.11141'
source_url: https://arxiv.org/abs/2511.11141
tags:
- robustness
- paraphrases
- paraphrase
- stability
- clip
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces PRSM, a novel metric to evaluate the robustness
  of CLIP against paraphrases. The authors highlight the importance of paraphrase
  robustness in vision-language models, particularly for fairness in socially sensitive
  applications.
---

# PRSM: A Measure to Evaluate CLIP's Robustness Against Paraphrases

## Quick Facts
- arXiv ID: 2511.11141
- Source URL: https://arxiv.org/abs/2511.11141
- Reference count: 19
- CLIP exhibits low global stability (Spearman < 0.04) but moderate local stability (top-100 overlap ~0.85) against paraphrases

## Executive Summary
This paper introduces PRSM, a novel metric to evaluate the robustness of CLIP against paraphrases. The authors highlight the importance of paraphrase robustness in vision-language models, particularly for fairness in socially sensitive applications. They propose PRSM, which measures both global ranking stability (via Spearman correlation) and local retrieval consistency (via top-k overlap) under paraphrased queries. Using the Social Counterfactuals dataset, they evaluate CLIP's performance across three types of paraphrases: LLM-generated, prefix-based, and attribute-based. Results show that CLIP exhibits low global stability but moderate local stability, with subtle but consistent gender differences. The study demonstrates that while retrieval outcomes may appear stable, the underlying rankings are highly sensitive to linguistic variations, posing risks for fairness and reliability in multimodal systems.

## Method Summary
The authors propose PRSM to measure CLIP's robustness against paraphrases by computing both global ranking stability (Spearman correlation across full ranked lists) and local retrieval consistency (top-k overlap). They use the Social Counterfactuals dataset with controlled counterfactual pairs and generate three paraphrase types: LLM-generated full rewrites (P1), strategic prefix substitutions (P2), and attribute synonym replacements (P3). The metric aggregates pairwise comparisons across all paraphrase sets, stratifying results by demographic attributes to reveal differential robustness patterns.

## Key Results
- CLIP shows extremely low global stability with Spearman correlations below 0.04 across all paraphrase types
- Local stability is moderate to high, with top-100 overlap reaching ~0.89 for prefix changes but only ~0.64 for full LLM rewrites
- Subtle but consistent gender differences emerge: female-associated queries show slightly higher local overlap (0.897-0.900) than male queries (0.847-0.900) in prefix paraphrases
- Attribute-level paraphrases show differential sensitivity, with "young/adolescent" synonyms causing more ranking disruption than "adult/elderly" pairs

## Why This Works (Mechanism)

### Mechanism 1: Dual-Component Stability Measurement
- Claim: PRSM captures both global and local retrieval stability to provide comprehensive robustness evaluation that single-metric approaches miss.
- Mechanism: The metric combines Spearman correlation across full ranked lists (global stability) with top-k overlap fraction (local stability), aggregating across all pairwise paraphrase comparisons using the formula PRSM = 2/(m(m-1)) × Σ ρ(R(qi), R(qj)).
- Core assumption: Global and local stability are independent dimensions; a model can have stable top results while having unstable global rankings, masking systemic fragility.
- Evidence anchors:
  - [abstract]: "PRSM, which measures both global ranking stability (via Spearman correlation) and local retrieval consistency (via top-k overlap)"
  - [section 4]: "global stability is extremely low: Spearman correlations remain below 0.04" while "P2 achieves substantially higher consistency (top-100 ∼0.89)"
  - [corpus]: Weak corpus evidence—related paraphrase benchmarks (PADBen, SPARTA) focus on detection/segmentation tasks, not dual-component retrieval metrics
- Break condition: If global and local stability are strongly correlated in practice, the dual-component approach provides minimal additional diagnostic value.

### Mechanism 2: Controlled Paraphrase Generation via Linguistic Component Targeting
- Claim: Strategic paraphrasing of specific linguistic components reveals differential sensitivity patterns in CLIP embeddings that natural paraphrase distributions obscure.
- Mechanism: Three paraphrase types isolate linguistic contributions—P1 (LLM-generated full rewrites via Llama-3.1-8B-Instruct), P2 (prefix substitutions among four variants), P3 (attribute synonym replacements via GPT-5)—enabling attribution of instability to specific text components.
- Core assumption: Paraphrase types represent distinct robustness challenges; the prefix "a photo of" and attribute synonyms "young/adolescent" affect embeddings through different mechanisms.
- Evidence anchors:
  - [section 3]: "One random LLM-generated set (P1), one strategic set that paraphrases the prefix (P2), and one strategic set that paraphrases the demographic attributes (P3)"
  - [section 4, Table 1]: P2 achieves top-100 overlap ~0.89, P1 achieves ~0.64, P3 ranges 0.63–0.74—demonstrating prefix changes are less disruptive than attribute or full-sentence paraphrases
  - [corpus]: SPARTA paper similarly uses adversarial paraphrasing to probe multimodal robustness, confirming paraphrase-based stress testing as a valid methodology
- Break condition: If paraphrase type doesn't differentially impact embeddings, component-targeted generation wouldn't reveal distinct failure modes.

### Mechanism 3: Demographic-Attribute Interaction with Paraphrase Sensitivity
- Claim: Paraphrase robustness varies systematically with demographic attributes, potentially amplifying fairness concerns in socially sensitive deployments.
- Mechanism: Gendered queries expose differential embedding fragility through the Social Counterfactuals dataset's controlled counterfactual pairs—female-associated queries show slightly higher local overlap in P2/P3, while male-associated queries show marginally higher Spearman correlations.
- Core assumption: Small but consistent PRSM differences (0.01–0.05) accumulate into meaningful fairness disparities at deployment scale.
- Evidence anchors:
  - [abstract]: "subtle yet consistent differences observed between male- and female-associated queries"
  - [section 4, Table 1]: Female queries achieve top-100 overlap 0.897–0.900 in P2 (p1-p2), male queries achieve 0.847–0.900; male queries show Spearman 0.035–0.037 vs. female 0.033–0.034 in P2
  - [corpus]: Weak corpus evidence—neighbor papers don't specifically examine demographic-paraphrase interactions in multimodal settings
- Break condition: If demographic-paraphrase interaction is dataset-specific rather than model-inherent, findings wouldn't generalize to other corpora or demographic attributes.

## Foundational Learning

### Concept: Contrastive Language-Image Pre-training (CLIP)
- Why needed here: The paper evaluates CLIP specifically; understanding its text-image alignment mechanism explains why paraphrase sensitivity directly impacts retrieval consistency.
- Quick check question: How does CLIP create joint embeddings for text and images, and why would semantically equivalent paraphrases produce different image rankings?

### Concept: Spearman Rank Correlation
- Why needed here: PRSM uses Spearman correlation as its global stability measure; interpreting results requires understanding what ρ < 0.05 means for ranking similarity.
- Quick check question: If two retrieval rankings have Spearman correlation of 0.03, what does that indicate about their relationship? What about 0.89?

### Concept: Top-k Retrieval Evaluation
- Why needed here: PRSM's local stability measure uses top-k overlap; this reflects how end users experience retrieval systems in practice.
- Quick check question: Why might global ranking instability (ρ < 0.05) matter less than top-10 inconsistency for a user searching for images? When would global instability become critical?

## Architecture Onboarding

### Component Map:
Paraphrase Generator Module -> CLIP Text Encoder -> CLIP Image Encoder -> Similarity Computer -> Ranking Extractor -> PRSM Calculator -> Demographic Analyzer

### Critical Path:
Original caption → Paraphrase generation (3 parallel streams) → CLIP text encoding → Similarity computation across image corpus → Ranking extraction → Pairwise PRSM calculation (Spearman + top-k) → Demographic stratification → Result aggregation

### Design Tradeoffs:
- **Paraphrase naturalness vs. control**: LLM-generated paraphrases (P1) offer realistic variation but introduce uncontrolled lexical changes; strategic paraphrases (P2/P3) isolate components but may not reflect real user rephrasings
- **Global vs. local diagnostic value**: Global measures reveal systemic embedding fragility; local measures reflect user-facing behavior—the paper shows these can diverge significantly
- **Dataset scope vs. demographic breadth**: Social Counterfactuals enables controlled gender analysis but limits demographic categories (only male/female); extensions to race, religion, disability require additional datasets
- **Paraphrase count vs. computational cost**: Two paraphrases per caption (P1) vs. four (P2) vs. three (P3) affects statistical reliability and evaluation time

### Failure Signatures:
- **Low Spearman (<0.05) with high top-k (>0.85)**: Global instability masked by local consistency—dangerous for fairness-sensitive applications where downstream systems use full rankings
- **Large P1 vs. P2 performance gap**: Model robust to superficial prefix changes but fragile under semantic reformulation—indicates shallow rather than semantic text understanding
- **Consistent gender-split PRSM differences (>0.02)**: Systematic demographic bias requiring targeted mitigation; male/female queries embedding into meaningfully different regions of space
- **High variance across paraphrase pairs**: PRSM instability across o-c1 vs. o-c2 indicates sensitivity to specific lexical choices rather than systematic behavior

### First 3 Experiments:
1. **Baseline PRSM profiling on your CLIP variant**: Calculate PRSM across all three paraphrase types on a 10% held-out subset of Social Counterfactuals (n≈17k query pairs) to establish your model's robustness profile and identify weakest paraphrase types
2. **Paraphrase-type ablation study**: Run PRSM with each paraphrase type independently (P1-only, P2-only, P3-only) to identify which linguistic components your model handles most poorly; prioritize attribute-level robustness if P3 scores lowest
3. **Demographic extension pilot**: Select one additional demographic attribute relevant to your deployment context (e.g., age, profession) and extend the P3 methodology with 5–10 synonym pairs per attribute value to test generalization of gender-based findings

## Open Questions the Paper Calls Out
- **Question**: Does PRSM reveal similar robustness patterns in autoregressive vision-language models (e.g., BLIP) compared to encoder-only models like CLIP?
  - **Basis in paper**: [explicit] The authors state: "another important direction for future work is the extension to autoregressive vision-language models (BLIP) and the mitigation of possible biases in them."
  - **Why unresolved**: This study only evaluated CLIP; autoregressive architectures may exhibit different embedding behaviors due to their generative training objectives.
  - **What evidence would resolve it**: Applying PRSM to BLIP and other autoregressive VLMs on the same Social Counterfactuals dataset, comparing global and local stability scores.

- **Question**: How does paraphrase robustness interact with demographic attributes beyond gender, such as race, religion, or disability status?
  - **Basis in paper**: [explicit] The conclusion notes: "we only report results for one dataset and separate the findings by only one demographic attribute. Further research would expand to include more demographic or other social attributes."
  - **Why unresolved**: The Social Counterfactuals dataset only provides male/female categories, limiting intersectional bias analysis.
  - **What evidence would resolve it**: Constructing or using datasets with additional demographic attributes and measuring PRSM across these groups.

- **Question**: Can paraphrase-invariant training methods effectively mitigate the global ranking instability observed with PRSM?
  - **Basis in paper**: [explicit] The authors underscore "the need for paraphrase-invariant training methods or post-hoc methods for mitigation."
  - **Why unresolved**: This paper only proposes a measurement tool, not mitigation strategies.
  - **What evidence would resolve it**: Training CLIP with paraphrase augmentation or contrastive objectives and measuring PRSM improvement.

## Limitations
- The generalizability of PRSM findings beyond CLIP to other multimodal architectures remains untested
- The Social Counterfactuals dataset's controlled demographic structure may not capture real-world query distributions
- GPT-5's availability limits reproducibility of the attribute-paraphrase generation pipeline

## Confidence
- **High confidence**: The PRSM metric formulation is mathematically sound and the computational methodology is reproducible
- **Medium confidence**: The claim that CLIP exhibits low global but moderate local stability is well-supported by empirical data
- **Medium confidence**: The observed gender differences in PRSM scores are statistically consistent but their practical significance requires further investigation
- **Low confidence**: The assertion that systematic demographic-paraphrase interactions will generalize to other protected attributes beyond gender

## Next Checks
1. **Cross-architecture validation**: Compute PRSM on alternative multimodal models (e.g., BLIP, Florence) using the same Social Counterfactuals dataset to test generalizability of CLIP-specific findings
2. **Real-world query simulation**: Generate natural paraphrase distributions from search logs and compare PRSM scores to controlled paraphrase types to validate ecological validity
3. **Downstream impact assessment**: Measure whether low PRSM scores correlate with measurable fairness violations in deployed retrieval systems using A/B testing with perturbed queries