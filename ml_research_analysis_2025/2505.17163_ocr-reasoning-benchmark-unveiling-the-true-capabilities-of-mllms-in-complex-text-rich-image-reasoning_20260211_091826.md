---
ver: rpa2
title: 'OCR-Reasoning Benchmark: Unveiling the True Capabilities of MLLMs in Complex
  Text-Rich Image Reasoning'
arxiv_id: '2505.17163'
source_url: https://arxiv.org/abs/2505.17163
tags:
- reasoning
- answer
- arxiv
- image
- text-rich
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces OCR-Reasoning, a benchmark designed to evaluate
  Multimodal Large Language Models (MLLMs) on text-rich image reasoning tasks. The
  benchmark comprises 1,069 human-annotated examples spanning six core reasoning abilities
  and 18 practical reasoning tasks.
---

# OCR-Reasoning Benchmark: Unveiling the True Capabilities of MLLMs in Complex Text-Rich Image Reasoning

## Quick Facts
- arXiv ID: 2505.17163
- Source URL: https://arxiv.org/abs/2505.17163
- Reference count: 40
- None of the evaluated MLLMs achieve accuracy above 50% on text-rich image reasoning tasks

## Executive Summary
This paper introduces OCR-Reasoning, a comprehensive benchmark designed to evaluate Multimodal Large Language Models (MLLMs) on text-rich image reasoning tasks. Unlike existing benchmarks that only measure final answers, OCR-Reasoning provides detailed annotations of reasoning processes, making it particularly valuable for assessing how models approach complex visual-textual problems. The benchmark includes 1,069 human-annotated examples across six core reasoning abilities and 18 practical reasoning tasks, representing a significant advancement in multimodal evaluation methodology.

The evaluation reveals that even state-of-the-art models struggle with text-rich image reasoning, with the best-performing model (DouBao-1.5-Vision-Pro) achieving only 46.8% accuracy and models like OpenAI-o1 and Gemini-2.0-Flash scoring below 45%. This performance ceiling highlights the substantial gap between current MLLM capabilities and the demands of real-world text-rich visual reasoning applications. The benchmark and evaluation scripts are publicly available at https://github.com/SCUT-DLVCLab/OCR-Reasoning.

## Method Summary
The OCR-Reasoning benchmark was constructed through a systematic process involving the collection of text-rich images from diverse real-world scenarios, followed by human annotation of both final answers and detailed reasoning processes. The benchmark covers six core reasoning abilities and 18 practical reasoning tasks, with examples carefully designed to test various aspects of multimodal understanding and reasoning. The evaluation methodology employs both traditional accuracy metrics and LLM-as-Judge approaches to assess reasoning trajectories, providing a more comprehensive evaluation framework than previous benchmarks.

## Key Results
- No evaluated MLLM achieves accuracy above 50% on the OCR-Reasoning benchmark
- Best-performing model (DouBao-1.5-Vision-Pro) reaches 46.8% accuracy
- OpenAI-o1 and Gemini-2.0-Flash score below 45% accuracy
- Comprehensive evaluation reveals significant challenges in text-rich image reasoning across all tested models
- Benchmark provides detailed reasoning process annotations, not just final answers

## Why This Works (Mechanism)
The benchmark's effectiveness stems from its focus on real-world text-rich images rather than simplified mathematical problems, forcing models to demonstrate genuine multimodal reasoning capabilities. By requiring models to both extract text from images and reason about the extracted information in context, the benchmark captures the complexity of practical applications where visual and textual information must be integrated.

## Foundational Learning
1. Multimodal Large Language Models (MLLMs) - Understanding how these models integrate visual and textual information
   - Why needed: Core technology being evaluated
   - Quick check: Review existing MLLM architectures and their capabilities

2. Text-rich image processing - Techniques for extracting and understanding text within visual contexts
   - Why needed: Fundamental challenge addressed by the benchmark
   - Quick check: Examine OCR accuracy on the benchmark's image types

3. Complex reasoning evaluation - Methods for assessing multi-step reasoning processes
   - Why needed: Benchmark's unique contribution to evaluation methodology
   - Quick check: Review LLM-as-Judge reliability studies

4. Human annotation for multimodal tasks - Best practices for creating reliable benchmark data
   - Why needed: Foundation of benchmark's validity
   - Quick check: Analyze inter-annotator agreement statistics

## Architecture Onboarding
Component map: Image Input -> OCR Extraction -> Text Understanding -> Reasoning Engine -> Answer Generation -> Reasoning Validation
Critical path: The bottleneck appears to be the integration of OCR-extracted text with visual context for complex reasoning
Design tradeoffs: Balancing between comprehensive task coverage and manageable benchmark size
Failure signatures: Models consistently struggle with multi-step reasoning requiring integration of text and visual cues
First experiments:
1. Test individual reasoning abilities in isolation to identify specific weaknesses
2. Evaluate model performance on text-only vs. image-only versions of tasks
3. Analyze reasoning trajectories to identify common failure patterns

## Open Questions the Paper Calls Out
### Open Question 1
- Question: How can reinforcement learning reward functions be specifically designed to improve performance on text-rich image reasoning tasks?
- Basis in paper: [explicit] The authors state in Section 4.2 that most existing reward functions are tailored for mathematical reasoning, and designing one for text-rich image reasoning is a "highly worthwhile research direction."
- Why unresolved: Current reinforcement learning methods perform poorly on the benchmark compared to baselines, largely due to the mismatch between existing reward functions and the diverse requirements of text-rich reasoning.
- What evidence would resolve it: The development of an RL-trained model using a novel reward function that achieves significantly higher accuracy on OCR-Reasoning compared to current RL baselines.

### Open Question 2
- Question: What training data curation strategies are most effective for bridging the gap between mathematical reasoning and general text-rich reasoning?
- Basis in paper: [explicit] Section 4.2 notes a "notable discrepancy" between training data (mostly printed mathematical problems) and the benchmark's diverse scenarios, asking how to select training data to improve performance.
- Why unresolved: Models trained primarily on mathematical data fail to generalize to the 18 practical tasks (e.g., financial analysis, spatial reasoning) defined in the benchmark.
- What evidence would resolve it: An ablation study showing that models trained on a curated, diverse dataset of non-mathematical text-rich images outperform those trained solely on standard mathematical reasoning datasets.

### Open Question 3
- Question: How can the evaluation of reasoning trajectories be automated without the biases and weaknesses inherent in current LLM-as-Judge methods?
- Basis in paper: [explicit] The Limitations section acknowledges that while they used LLMs to evaluate reasoning processes, issues like "biases in LLMs-as-Judges, adversarial attacks, and inherent weaknesses" affect accuracy.
- Why unresolved: There is currently no robust, scalable alternative to using LLMs for grading the complex, multi-step reasoning chains required by the benchmark.
- What evidence would resolve it: The proposal of a new evaluation framework or metric that demonstrates higher correlation with human expert evaluation scores than current LLM-based judging methods.

## Limitations
- Relatively small dataset size (1,069 examples) may limit statistical power and generalizability
- Evaluation methodology relies heavily on accuracy metrics, which may not capture nuanced performance differences
- Benchmark coverage, while comprehensive, may still miss edge cases or rare reasoning patterns
- Performance ceiling of ~50% accuracy raises questions about benchmark difficulty calibration

## Confidence
- Main claims: Medium
- Performance metrics: Medium
- Methodology: Medium
- Dataset quality: Medium

## Next Checks
1. Independent replication of the benchmark evaluation using the released scripts and dataset to verify the reported accuracy scores across all tested models
2. Analysis of model failure modes by examining specific examples where models perform poorly to determine if errors stem from reasoning limitations or other factors
3. Expansion of the benchmark with additional examples and tasks to test whether the ~50% accuracy ceiling persists with larger sample sizes and more diverse scenarios