---
ver: rpa2
title: 'Toward Responsible Federated Large Language Models: Leveraging a Safety Filter
  and Constitutional AI'
arxiv_id: '2502.16691'
source_url: https://arxiv.org/abs/2502.16691
tags:
- safety
- responses
- fedllm
- data
- filter
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the problem of training safe large language
  models (LLMs) in federated learning environments, where client data may contain
  harmful content. The authors propose incorporating two responsible AI methods into
  federated LLM training: a safety filter to remove harmful data at the client level,
  and constitutional AI (CAI) to improve safety of the global model.'
---

# Toward Responsible Federated Large Language Models: Leveraging a Safety Filter and Constitutional AI

## Quick Facts
- arXiv ID: 2502.16691
- Source URL: https://arxiv.org/abs/2502.16691
- Reference count: 40
- Primary result: Safety filter + cost-efficient CAI improves AdvBench safety score by >20% in federated LLM training

## Executive Summary
This paper addresses the challenge of training safe large language models (LLMs) in federated learning environments where client data may contain harmful content. The authors propose integrating two responsible AI methods into federated LLM training: a safety filter to remove harmful data at the client level, and constitutional AI (CAI) to improve safety of the global model. A key innovation is a cost-efficient CAI approach that reduces computational overhead by 96% while maintaining effectiveness. Experiments using the SQuARe dataset demonstrate that both methods significantly enhance safety performance compared to baseline federated training.

## Method Summary
The proposed approach combines a safety filter at the client level with constitutional AI applied to the global model. A Llama Guard 3 safety filter is finetuned on a custom dataset (S-LG20K) to classify query-response pairs as safe or unsafe, removing harmful data before local training. Constitutional AI is applied server-side after each round using a cost-efficient approach with 50 training iterations (96% reduction from full epoch). The CAI process uses self-critique and preference learning to generate training pairs that teach the model to recognize and revise harmful responses. Experiments use Llama3.1-8B-Instruct with LoRA adapters in a federated setting with 20 clients over 50 rounds.

## Key Results
- Safety filter + CAI combination achieves >20% improvement on AdvBench safety benchmark
- Cost-efficient CAI reduces training time per round from 80 minutes to 3.2 minutes (96% reduction)
- Safety filter slightly reduces MT-Bench scores (-0.3 to -0.2), while CAI improves them (+3.0 to +3.1)
- Combined approach maintains or improves both safety and helpfulness metrics

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Filtering harmful training data at the client level prevents harmful patterns from being learned and aggregated into the global model.
- Mechanism: Llama Guard 3 classifies query-response pairs as safe or unsafe before local training. Unsafe pairs are excluded, so local LoRA weights are trained only on filtered data, preventing harmful gradient updates from propagating to the global LoRA during aggregation.
- Core assumption: Harmful outputs are primarily learned from harmful training examples; removing these examples at the source prevents the model from acquiring harmful behavioral patterns.
- Evidence anchors:
  - [abstract] "safety filter to screen out harmful local data"
  - [section 4.1] "the safety filter takes the query and the LLM's response as input and classifies them as safe or unsafe. Unsafe data is excluded from training."
  - [corpus] Related work "Safe-FedLLM" addresses federated LLM safety but focuses on malicious client defenses rather than data filtering; corpus evidence for this specific filtering mechanism is limited.
- Break condition: If harmful behavioral patterns already exist in the pretrained model weights or emerge from seemingly benign data combinations, the safety filter cannot prevent those outputs.

### Mechanism 2
- Claim: Constitutional AI applied to the global model after aggregation corrects accumulated harmful tendencies through self-critique and preference learning.
- Mechanism: CAI uses a three-turn process (red response → self-critique → self-revised response) to generate training pairs. SFT trains on revised responses as targets; DPO trains the model to prefer revised responses over red ones. This is applied server-side after each round.
- Core assumption: Models can learn to recognize and self-correct harmful outputs when explicitly trained on critique-revision patterns, and this capability generalizes to novel prompts.
- Evidence anchors:
  - [abstract] "Constitutional AI (CAI) to the global model to ensure safe responses"
  - [section 4.2] "CAI defines constitutions... and prompts the LLM to self-critique and revise its responses accordingly. The self-revised responses are then used to continually refine the LLM"
  - [corpus] Constitutional AI (Bai et al. 2022) is a recognized technique; corpus shows no direct evidence of its application to FedLLM, which the paper claims as novel.
- Break condition: If constitutional principles don't cover edge-case harmful prompts, or if the model lacks capacity for reliable self-critique across domains, safety guarantees degrade.

### Mechanism 3
- Claim: Cost-efficient CAI (50 iterations vs. full epoch) maintains meaningful safety improvements while reducing per-round computation by 96%.
- Mechanism: Instead of training CAI for a full epoch (~80 minutes on 4× A100), the approach uses only 50 iterations (~3.2 minutes) per round. This lightweight touch is applied every round, accumulating safety alignment across the training process.
- Core assumption: Incremental, frequent CAI application compensates for reduced per-round training depth; safety gains compound across rounds.
- Evidence anchors:
  - [abstract] "cost-efficient CAI approach reduces training iterations by 96%"
  - [section 4.2] "shortens the CAI training time per round from 80 minutes to 3.2 minutes, resulting in a 96% reduction. Although applying CAI for one epoch per round is expected to yield better performance, our experiments demonstrate that our cost-efficient approach still achieves sufficiently effective results."
  - [corpus] No corpus evidence for this efficiency optimization; appears to be a novel empirical finding of this work.
- Break condition: If harmful drift accumulates faster than lightweight CAI can correct (e.g., in high-harm scenarios or very heterogeneous client data), safety may degrade over many rounds.

## Foundational Learning

- Concept: Low-Rank Adaptation (LoRA) for LLM Finetuning
  - Why needed here: The entire FedLLM architecture depends on LoRA to make training and communication tractable. You must understand that W_P (pretrained weights) stays frozen while W_L (low-rank adapters) are the only trainable parameters exchanged between clients and server.
  - Quick check question: If a client has W_G with rank-8 LoRA adapters and trains locally for 25 iterations, what exactly gets transmitted back to the server?

- Concept: Direct Preference Optimization (DPO)
  - Why needed here: CAI's second stage uses DPO to train the model to prefer self-revised responses over red responses. Understanding how DPO works (and how it differs from RLHF) is essential for debugging CAI failures.
  - Quick check question: In DPO, how does the loss function encode the preference for "chosen" over "rejected" responses, and what happens if both responses are similar?

- Concept: Federated Averaging (FedAvg) vs. SCAFFOLD
  - Why needed here: The paper evaluates both FL algorithms. Understanding their differences (FedAvg's data heterogeneity issues vs. SCAFFOLD's control variates) helps interpret why safety improvements vary between them.
  - Quick check question: Why might SCAFFOLD show different safety improvement patterns than FedAvg when clients have heterogeneous harmful data distributions?

## Architecture Onboarding

- Component map:
  - Server-side (pre-training): Pretrained LLM W_P (frozen) → Global LoRA W_G (trainable)
  - Server-side (per-round): Aggregate W_L from clients → Apply cost-efficient CAI → Distribute updated W_G
  - Client-side: Receive W_G → Filter local data with LG3 → Train local W_L → Transmit W_L
  - Safety Filter: LG3 finetuned on S-LG20K (server) → deployed to clients for inference-only filtering

- Critical path:
  1. One-time setup: Finetune LG3 on S-LG20K → Generate S-CAI20K via self-critique pipeline
  2. Per-round flow: Server distributes W_G → Clients filter data with LG3 → Clients train W_L on filtered data → Server aggregates W_L via FedAvg/SCAFFOLD → Server applies 50-iteration CAI → Updated W_G ready for next round

- Design tradeoffs:
  - Safety filter precision vs. recall: Table 2 shows finetuned LG3 has 56.7% precision (many filtered samples were safe) but 73.7% recall (~26% of harmful data passes). Tuning this threshold affects both safety and data utility.
  - CAI iteration depth vs. round latency: 50 iterations provides "sufficient" results but likely underperforms full-epoch CAI. The paper doesn't quantify this gap.
  - Safety vs. helpfulness: Safety filter slightly hurts MT-Bench (-0.3 to -0.2), while CAI improves it (+3.0 to +3.1). Combined approach yields net positive helpfulness.

- Failure signatures:
  - High false-negative rate in safety filter: If LG3 misses harmful data (Table 2: 26.3% false negative rate), contaminated W_L propagates to global model
  - Over-aggressive filtering: Low precision (56.7%) means substantial safe data is discarded, potentially degrading model quality on legitimate tasks
  - CAI distribution shift: If S-CAI20K doesn't represent the distribution of harmful prompts clients encounter, CAI provides limited protection
  - Heterogeneous harm distribution: If some clients have high harm concentrations while others have none, aggregation may dilute but not eliminate harmful behaviors

- First 3 experiments:
  1. Replicate baseline degradation: Run FedLLM on SQuARe20K without safety measures. Verify AdvBench drops from ~99.6% (pretrained) to ~72.5% (Table 1, #2), confirming RQ1.
  2. Isolate safety filter impact: Run FedLLM with finetuned LG3 only (no CAI). Measure AdvBench improvement (target: +6-8%) and MT-Bench impact (expect slight decrease). Analyze false positives/negatives on held-out data.
  3. Isolate CAI impact: Run FedLLM with cost-efficient CAI only (no safety filter). Measure AdvBench improvement (target: +24%) and verify MT-Bench gains. Compare 50-iteration vs. full-epoch CAI to quantify the efficiency-performance tradeoff.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the safety filter and Constitutional AI frameworks be effectively adapted and validated for multimodal FedLLM?
- Basis in paper: [explicit] The conclusion explicitly states the authors plan to "extend our approach to multimodal FedLLM by applying RAI methods to multimodal data" as future work.
- Why unresolved: The current study strictly validates the proposed methods on text-based LLMs using the SQuARe dataset and AdvBench benchmark.
- What evidence would resolve it: Experimental results applying these RAI methods to multimodal models (e.g., image-text) within a federated learning environment.

### Open Question 2
- Question: To what extent does the safety filter's removal of benign data (false positives) degrade the global model's helpfulness and general knowledge over long-term training?
- Basis in paper: [inferred] Table 1 shows the safety filter reduces MT-Bench (helpfulness) scores, and Table 2 reports 56.7% precision, implying a significant volume of safe data is incorrectly filtered out.
- Why unresolved: The paper acknowledges the helpfulness drop but does not explore if the cumulative loss of diverse safe data permanently degrades model utility.
- What evidence would resolve it: Long-run federated experiments analyzing the trade-off between safety scores and general task performance as filtering errors accumulate.

### Open Question 3
- Question: Do the proposed RAI methods remain robust when the proportion of harmful data significantly exceeds the 30% ratio tested in the experiments?
- Basis in paper: [inferred] The authors designed the experiment assuming a "relatively low" proportion of harmful data (fixed at 30%), leaving high-toxicity scenarios unexplored.
- Why unresolved: It is unclear if the safety filter and cost-efficient CAI can effectively realign the model if client data is dominated by malicious or harmful content.
- What evidence would resolve it: Ablation studies testing model safety performance across varying ratios of harmful client data (e.g., 50%, 70%, 90%).

## Limitations

- The experiments use synthetic SQuARe datasets rather than real-world federated data, limiting generalizability to actual deployment scenarios.
- The safety filter's low precision (56.7%) suggests substantial false positives, potentially discarding valuable training data and degrading model quality.
- The 96% computational reduction in CAI is achieved through reduced training iterations, but the paper doesn't quantify the performance gap between this approach and full-epoch CAI.

## Confidence

- Safety improvement claims (High confidence): The AdvBench improvements (>20%) are well-supported by direct experimental results showing clear performance gains over baseline FedLLM with red data.
- Computational efficiency claims (Medium confidence): The 96% reduction figure is based on iteration counts, but without benchmarking full-epoch CAI performance, the actual efficiency gain relative to effectiveness remains uncertain.
- Generalizability claims (Low confidence): Claims about effectiveness across diverse federated environments are weakly supported, as all experiments use controlled synthetic data distributions rather than real-world federated scenarios.

## Next Checks

1. **Heterogeneous harm distribution test**: Run FedLLM with clients having drastically different proportions of harmful data (0%, 25%, 50%, 75%, 100%) to evaluate whether the safety filter and CAI maintain effectiveness when harm is highly concentrated in specific clients.

2. **False positive/negative analysis**: Conduct detailed error analysis on a held-out validation set to quantify the safety filter's precision-recall tradeoff across different harm categories, and measure whether CAI compensates for safety filter failures.

3. **Real-world data validation**: Apply the proposed methods to a federated learning setup using actual user-generated content from multiple sources (e.g., forum posts, customer service logs) to assess performance on naturally occurring harmful content rather than synthetic red teaming prompts.