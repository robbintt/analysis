---
ver: rpa2
title: 'CorrectHDL: Agentic HDL Design with LLMs Leveraging High-Level Synthesis as
  Reference'
arxiv_id: '2511.16395'
source_url: https://arxiv.org/abs/2511.16395
tags:
- design
- functional
- designs
- reference
- llm-generated
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces CorrectHDL, a framework that uses high-level
  synthesis (HLS) results as a functional reference to guide large language models
  (LLMs) in generating syntactically and functionally correct hardware description
  language (HDL) designs. It decomposes complex C/C++ programs into smaller submodules,
  repairs syntax errors with a retrieval-augmented generation mechanism, and iteratively
  corrects functional errors by comparing LLM-generated HDL with HLS-generated golden
  references.
---

# CorrectHDL: Agentic HDL Design with LLMs Leveraging High-Level Synthesis as Reference

## Quick Facts
- **arXiv ID:** 2511.16395
- **Source URL:** https://arxiv.org/abs/2511.16395
- **Reference count:** 40
- **Key outcome:** Framework improves functional HDL simulation pass rates by up to 38.54%, reduces area by 24.83%, and lowers power by 26.98% compared to HLS-generated designs.

## Executive Summary
CorrectHDL addresses the challenge of generating functionally correct hardware description language (HDL) from high-level C/C++ specifications using large language models (LLMs). The framework uses High-Level Synthesis (HLS) results as a functional reference to guide LLM-generated HDL designs through iterative refinement. By decomposing complex programs into submodules and employing a differential verification loop, CorrectHDL achieves higher functional correctness and better PPA metrics than both pure HLS outputs and manual designs, while producing more readable and maintainable code.

## Method Summary
The method uses HLS as a golden reference to iteratively repair LLM-generated HDL. It starts by decomposing C/C++ source code into smaller submodules with explicit I/O definitions to improve LLM context handling. The LLM generates Verilog for each submodule, with syntax errors repaired using a retrieval-augmented generation (RAG) mechanism that queries a library of HDL rules. The system then synthesizes a golden reference using HLS and compares simulation outputs through differential verification. Mismatched results are fed back to the LLM with specific input vectors for functional repair, iterating up to three times per submodule before integration into the final design.

## Key Results
- Functional simulation pass rate improved by up to 38.54% compared to direct LLM generation
- Area overhead reduced by average of 24.83% compared to HLS-generated designs
- Power consumption lowered by 26.98% compared to HLS outputs
- Syntax compilation pass rate increased by average of 15.49% using RAG repair mechanism

## Why This Works (Mechanism)

### Mechanism 1: HLS as a Functional Oracle for Differential Verification
The framework improves functional correctness by using HLS outputs as a golden reference to detect and repair behavioral discrepancies in LLM-generated HDL. It synthesizes the input C/C++ into verified but inefficient HDL using standard HLS tools, then simulates both the LLM-generated design and HLS-generated design with identical test stimuli. When outputs diverge, the system extracts specific input vectors and mismatched output values, feeding this behavioral delta back to the LLM for iterative repair. This mechanism assumes HLS generates functionally correct HDL relative to the source C/C++, and that the LLM can map simulation mismatches back to logic faults.

### Mechanism 2: Context Management via C/C++ Decomposition
Decomposing complex C/C++ programs into smaller submodules improves LLM generation quality by mitigating attention dilution. The framework splits source code along function boundaries with explicit I/O definitions, forcing the LLM to focus on hardware semantics of single modules rather than attempting to hold entire system context in working memory. This assumes functionality can be cleanly partitioned into function-level blocks without losing critical timing context or creating complex inter-module dependencies.

### Mechanism 3: Retrieval-Augmented Syntax Repair
Automating syntax correction via RAG is more effective for compilation success than relying solely on the LLM's internal knowledge. When LLM-generated HDL fails compilation, the system embeds the error log and queries a knowledge base of HDL rule templates, retrieving the specific rule violated and injecting this explicit guidance into the LLM prompt for the next iteration. This assumes syntax errors follow recognizable patterns that can be mapped to a static library of rules.

## Foundational Learning

- **High-Level Synthesis (HLS):** HLS tools compile C++ to Verilog, producing functionally correct but inefficient and unreadable code. Understanding HLS is critical because it serves as the golden reference for verification. *Quick check:* Can you explain why HLS-generated code is functionally reliable but undesirable for final implementation?

- **RTL Simulation & Testbenches:** The entire repair loop depends on comparing simulation outputs. You need to understand how test vectors stimulate a design and how to dump signal values to compare DUT vs. Golden outputs. *Quick check:* How does the framework verify that the LLM-generated module matches the HLS-generated module?

- **Agentic Design Flows:** This is an iterative loop where an agent executes tools (simulator, compiler), reads output, and decides the next action (repair/integrate). *Quick check:* What triggers the agent to stop iterating and move to the integration phase?

## Architecture Onboarding

- **Component map:** C/C++ Source + Testbench -> Decomposer -> Generator -> Syntax Fixer -> HLS Oracle -> Differential Verifier -> Integrator -> Final HDL
- **Critical path:** The Differential Verification Loop, where latency lies due to expensive simulations and LLM inferences for convergence
- **Design tradeoffs:** Speed vs. Quality (HLS adds overhead as functional checker) and Readability vs. Optimization (prioritizes human-readable LLM code over optimized HLS code)
- **Failure signatures:** Infinite Repair Loop (LLM oscillates between incorrect implementations), Integration Chaos (submodules pass individually but Top-Level fails due to misaligned handshakes or port wiring)
- **First 3 experiments:**
  1. Baseline Functional Check: Generate HDL directly with standard LLM and measure functional pass rate against HLS reference without using CorrectHDL
  2. RAG Effectiveness: Disable RAG syntax repair module and measure compilation pass rate to quantify the 15.49% gain
  3. PPA Comparison: Synthesize final CorrectHDL output and HLS output for same task to verify claimed ~24% area reduction

## Open Questions the Paper Calls Out

- **Open Question 1:** How can the framework be extended to generate C/C++ specifications directly from natural language inputs? The conclusion states future work will "integrate C/C++ program generation from natural language to establish a complete LLM-assisted workflow from natural language to HDL design." This is unresolved because the current implementation requires C/C++ programs as the starting point.

- **Open Question 2:** Can LLM-generated circuits be optimized to consistently surpass human-engineered designs in area and power efficiency? The conclusion notes the need to "explore their potential in matching and even surpassing human-engineered designs," while results currently show the framework only "approaches the quality" of manual designs.

- **Open Question 3:** How can dependency on human verification for LLM-adapted HDL testbenches be fully eliminated? The methodology notes that "Limited human effort is still required during this process to ensure the quality of the resulting testbench," creating a bottleneck for full autonomy.

## Limitations
- Performance heavily depends on HLS tool correctness for complex C++ constructs like dynamic memory and pointer aliasing
- Critical dependence on "GPT-5" model access, which is limited and may affect reasoning capability claims
- HDL rule library construction for RAG syntax repair is not fully specified in the paper

## Confidence
- **High Confidence:** HLS as functional oracle mechanism is sound and well-supported by implementation
- **Medium Confidence:** Decomposition strategy effectiveness may vary with benchmark complexity and inter-module dependencies
- **Low Confidence:** PPA improvement claims (24.83% area, 26.98% power) require validation with exact synthesis setup and HLS tool version

## Next Checks
1. Reproduce the RAG syntax repair gain by disabling the RAG module and measuring functional pass rate on a subset of benchmarks
2. Validate PPA claims by synthesizing final CorrectHDL output and direct HLS output using identical tools and PDKs
3. Test complex C++ constructs with dynamic memory or pointer-heavy code to identify failure modes and assess HLS oracle limits