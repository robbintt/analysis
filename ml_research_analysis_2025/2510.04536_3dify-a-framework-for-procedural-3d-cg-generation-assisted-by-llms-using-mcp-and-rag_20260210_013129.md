---
ver: rpa2
title: '3Dify: a Framework for Procedural 3D-CG Generation Assisted by LLMs Using
  MCP and RAG'
arxiv_id: '2510.04536'
source_url: https://arxiv.org/abs/2510.04536
tags:
- dify
- generation
- d-cg
- tools
- procedural
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 3Dify is a procedural 3D computer graphics generation framework
  that enables users to create 3D models using only natural language instructions.
  The framework leverages Large Language Models (LLMs) and integrates state-of-the-art
  technologies such as the Model Context Protocol (MCP) and Retrieval-Augmented Generation
  (RAG).
---

# 3Dify: a Framework for Procedural 3D-CG Generation Assisted by LLMs Using MCP and RAG

## Quick Facts
- **arXiv ID:** 2510.04536
- **Source URL:** https://arxiv.org/abs/2510.04536
- **Reference count:** 20
- **Primary result:** Procedural 3D-CG generation from natural language using LLM-controlled DCC tools via MCP and RAG

## Executive Summary
3Dify is a procedural 3D computer graphics generation framework that enables users to create 3D models using only natural language instructions. The framework leverages Large Language Models (LLMs) and integrates state-of-the-art technologies such as the Model Context Protocol (MCP) and Retrieval-Augmented Generation (RAG). Built upon the open-source Dify platform, 3Dify automates operations of Digital Content Creation (DCC) tools like Blender, Unreal Engine, and Unity, either through MCP or Computer-Using Agent (CUA) methods. The framework includes an interactive feedback loop that refines image generation quality by allowing users to select preferred images from multiple candidates, enabling the LLM to learn variable patterns for subsequent generations.

## Method Summary
3Dify uses a three-agent LLM system (Visualizer→Planner→Manager) built on Dify v1.3 to automate DCC tools via MCP servers. The framework integrates RAG for documentation retrieval and employs an iterative feedback loop where users select preferred images from multiple candidates to refine subsequent generations. MCP handles tool automation for supported DCC tools, while CUA (UI-TARS) provides GUI operation fallback when MCP is unavailable. The system supports local LLM deployment for cost and security benefits, and includes conversation variables for multi-agent control across the generation workflow.

## Key Results
- Successfully generated a desktop gaming PC 3D model from a single natural language prompt
- Demonstrated automation of Blender, Unreal Engine, and Unity using MCP integration
- Showed effective use of RAG for retrieving DCC tool documentation to assist LLM decision-making
- Implemented iterative image selection feedback loop to improve generation quality

## Why This Works (Mechanism)
The framework works by decomposing the 3D generation task into specialized LLM agents that handle visualization, planning, and execution. MCP provides standardized tool integration, allowing LLMs to control DCC applications programmatically. RAG enhances the LLM's domain knowledge by retrieving relevant documentation, while the feedback loop enables continuous learning from user preferences. The modular architecture allows swapping between MCP and CUA automation methods based on tool availability.

## Foundational Learning
- **Model Context Protocol (MCP):** Standardized protocol for LLM-tool integration; needed for consistent automation across DCC tools; quick check: verify MCP server registration and tool discovery
- **Retrieval-Augmented Generation (RAG):** Technique for enhancing LLM responses with external document retrieval; needed for accessing DCC tool documentation; quick check: test RAG retrieval accuracy with sample queries
- **Computer-Using Agents (CUA):** GUI automation approach using visual feedback; needed as MCP fallback for tools without server support; quick check: verify UI-TARS can locate and interact with target UI elements
- **Multi-agent orchestration:** Coordination of specialized LLM agents for complex tasks; needed for task decomposition and workflow management; quick check: validate agent communication and variable passing
- **Iterative feedback refinement:** User-driven selection of preferred outputs to improve subsequent generations; needed for quality control and learning; quick check: measure consistency across multiple feedback iterations

## Architecture Onboarding

**Component Map:** User Prompt -> Visualizer Agent -> Planner Agent -> Manager Agent -> MCP/CUA -> DCC Tool -> Image Generation -> User Feedback Loop

**Critical Path:** Natural language input → LLM visualization → Planning with RAG documentation → MCP/CUA tool execution → Image generation → User selection → Updated context for next iteration

**Design Tradeoffs:** MCP provides programmatic control but requires server support; CUA offers broader compatibility but may be slower and less precise; RAG improves accuracy but adds retrieval latency; local LLMs reduce costs but may have lower capability than cloud models

**Failure Signatures:** Spatial coherence loss across iterative instructions; MCP tool context bloat consuming LLM context; incorrect tool selection due to incomplete documentation; feedback loop divergence from original intent

**First Experiments:**
1. Test MCP integration with Blender using basic mesh creation commands
2. Verify RAG retrieval accuracy with DCC tool documentation queries
3. Validate multi-agent coordination with a simple object placement task

## Open Questions the Paper Calls Out

### Open Question 1
**Question:** How can the framework maintain spatial coherence and accurate positional relationships among dozens of objects during multi-turn iterative generation tasks?
**Basis in paper:** Section V notes that during the desktop PC demonstration, "the instruction to move the entire PC upward did not function correctly—some parts protruded from the case," attributing this to the LLM struggling to track spatial relationships.
**Why unresolved:** As additional tasks accumulate, the text-based LLM context appears insufficient to track the precise coordinates and dependency relationships of previously generated components.
**What evidence would resolve it:** A quantitative evaluation of complex scene modifications showing a significant reduction in object intersection errors or protrusions without manual intervention.

### Open Question 2
**Question:** Does incorporating visual feedback via Computer-Using Agents (CUA) significantly improve spatial accuracy compared to the MCP-only approach used in the demonstration?
**Basis in paper:** Section V states that the demonstration used only MCP and "did not incorporate any visual information," identifying CUA as a "promising direction for improvement" for spatial issues.
**Why unresolved:** The current reliance on text-based command generation (via Python scripts) lacks a visual verification loop to confirm that executed results match the user's intent for spatial adjustments.
**What evidence would resolve it:** An ablation study comparing the spatial error rates of scenes generated via MCP-only versus MCP-plus-CUA with visual grounding.

### Open Question 3
**Question:** Can procedural generation knowledge (e.g., placement constraints) retrieved via RAG be effectively shared and reused across distinct DCC tools (e.g., applying Infinigen logic to Unreal Engine)?
**Basis in paper:** Section IV-C mentions the authors "envision cases where knowledge and solvers... from Infinigen (originally developed for Blender) could be reused" in other tools, but notes 3Dify "does not readily adopt this implementation."
**Why unresolved:** Translating abstract procedural logic or "solvers" from one tool's specific API or node structure to another's (e.g., Blender Python to Unreal PCG) remains an implementation gap.
**What evidence would resolve it:** A demonstration where a user successfully generates a complex scene in Unreal Engine using natural language prompts that leverage RAG-retrieved logic originally documented for Blender.

## Limitations
- Iterative feedback loop introduces variability and potential inconsistency across users
- Missing quantitative metrics beyond user satisfaction for performance evaluation
- Limited comparison between MCP and CUA automation effectiveness
- RAG system impact on LLM accuracy not empirically validated

## Confidence

**High confidence:** Core framework architecture and integration claims are well-documented and demonstrated
**Medium confidence:** Practical utility claims based on single example demonstration
**Low confidence:** Reproducibility due to missing critical implementation details

## Next Checks
1. Test the iterative feedback loop with multiple users using the same initial prompt to assess consistency in final outputs and identify variability patterns
2. Benchmark MCP tool automation versus CUA methods on identical tasks to quantify performance differences and context usage efficiency
3. Evaluate RAG retrieval accuracy by measuring how well documentation search improves LLM task completion rates across different DCC tools