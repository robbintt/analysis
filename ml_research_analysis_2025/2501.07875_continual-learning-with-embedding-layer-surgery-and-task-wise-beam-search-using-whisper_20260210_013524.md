---
ver: rpa2
title: Continual Learning with Embedding Layer Surgery and Task-wise Beam Search using
  Whisper
arxiv_id: '2501.07875'
source_url: https://arxiv.org/abs/2501.07875
tags:
- languages
- language
- arxiv
- token
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses catastrophic forgetting in multilingual ASR
  by focusing on the token embedding lookup table, which previous continual learning
  methods overlooked. The authors propose Embedding Layer Surgery, which creates separate
  copies of token embeddings for each new language, and Task-wise Beam Search, which
  maintains multiple language hypotheses during decoding to improve LID and ASR accuracy.
---

# Continual Learning with Embedding Layer Surgery and Task-wise Beam Search using Whisper

## Quick Facts
- arXiv ID: 2501.07875
- Source URL: https://arxiv.org/abs/2501.07875
- Authors: Chin Yuen Kwok; Jia Qi Yip; Eng Siong Chng
- Reference count: 0
- One-line primary result: Reduces pre-trained language WER from 14.2% to 11.9% using embedding separation and task-wise beam search

## Executive Summary
This paper addresses catastrophic forgetting in multilingual ASR by focusing on the token embedding lookup table, which previous continual learning methods overlooked. The authors propose Embedding Layer Surgery, which creates separate copies of token embeddings for each new language, and Task-wise Beam Search, which maintains multiple language hypotheses during decoding to improve LID and ASR accuracy. Evaluated on Whisper adapted to 10 unseen languages from Common Voice, their method reduces the average WER of pre-trained languages from 14.2% to 11.9% compared to Experience Replay, without compromising performance on new languages.

## Method Summary
The method involves two key components: (1) Embedding Layer Surgery, which isolates token embeddings by creating separate copies for each new language while freezing original embeddings for pre-trained languages, and (2) Task-wise Beam Search, which maintains multiple language hypotheses during decoding to correct early LID mistakes. The approach uses a language-agnostic system where the model identifies the language itself without manual prompts. The model is trained sequentially on new languages using Experience Replay with 1 hour of replay data per old language, while only updating the decoder weights with the encoder frozen.

## Key Results
- Reduces pre-trained language WER from 14.2% to 11.9% compared to Experience Replay
- Maintains performance on new languages while mitigating catastrophic forgetting
- Achieves improved LID accuracy through task-wise beam search with N=2 candidates

## Why This Works (Mechanism)

### Mechanism 1
Isolating token embeddings by language mitigates semantic overwriting during fine-tuning. The method copies the original token embedding table, then freezes the original (for pre-trained languages) and updates only a new language-specific table during adaptation. This prevents the embeddings of shared tokens from drifting toward the semantics of the new language.

### Mechanism 2
Decoupling language-agnostic identification from language-specific transcription allows a more robust LID process. By splitting the token embedding table into two parts—a shared "Special Token" (ST) part for control and LID, and a separate vocabulary part for each language—the model first uses the stable ST embeddings to identify the language, then routes to the appropriate language-specific vocabulary for transcription.

### Mechanism 3
Task-wise beam search reduces error propagation from early LID mistakes. Instead of committing to a single LID decision, this decoding method retains multiple hypotheses in different languages (top-N). It then scores the full transcriptions from these different language paths and selects the one with the highest score, allowing the model to "self-correct" if a richer or more coherent transcription exists in a different language than the one initially ranked highest.

## Foundational Learning

- **Catastrophic Forgetting in Multimodal Models**: The entire paper is framed around mitigating catastrophic forgetting (CF) when adapting a large, pre-trained multilingual ASR model (Whisper) to new languages. Quick check: Can you explain why a model that has learned to transcribe English perfectly would suddenly fail at it after being trained on a new language like Esperanto?

- **Subword Tokenization and Embedding Tables**: The core technical contribution ("Embedding Layer Surgery") operates directly on the token embedding lookup table. Understanding how a subword tokenizer (like Whisper's) works and what an embedding represents is essential. Quick check: Given a tokenizer with 50,000 tokens, how does the model convert a spoken sound into a sequence of these tokens? What is the role of the embedding table in this process?

- **Language Agnostic Decoding**: The proposed solution is designed for a language-agnostic system where the model must identify the language itself, rather than being prompted. The problems this paper solves (LID error propagation) are specific to this paradigm. Quick check: How does the decoding process differ when a model is language-agnostic versus when it is explicitly told which language to transcribe?

## Architecture Onboarding

- **Component map**: Audio input -> WhisperEncoder (Frozen) -> WhisperDecoder (Trainable) with modular embeddings (Shared_ST_Embeddings, Old_Lang_Embeddings, New_Lang_Embeddings_Table) -> Beam Search Router

- **Critical path**: 1. Audio input -> WhisperEncoder (Frozen). 2. Decoder starts with a special token (e.g., <startoftranscript>). 3. Decoder attends to encoder output using embeddings from Shared_ST_Embeddings. 4. Model predicts a language ID token (e.g., <es>). 5. Beam Search Router takes the top-N language predictions. 6. For each of the N languages, the router swaps in the corresponding language-specific vocabulary embeddings (Old or New) into the decoder. 7. Each language path generates a transcription hypothesis. 8. Hypotheses are scored and the best one is selected.

- **Design tradeoffs**: Memory vs. Stability: The separate embeddings increase memory usage linearly with the number of new languages, but are crucial for stability. Complexity vs. Performance: Task-wise beam search adds significant inference-time complexity but improves performance by correcting LID errors. Heuristics: The method relies on heuristics like M_len and M_overlap to ensure stability, which may require tuning and could fail on edge cases.

- **Failure signatures**: LID Collapse: The model defaults to always predicting the most recently trained language. Uncalibrated Scores: If scores aren't comparable, the final output will always be from a single dominant language, regardless of the input. Memory Overflow: Training will crash if the number of new language embedding tables exceeds available GPU memory.

- **First 3 experiments**: 1. Reproduce Baseline: Fine-tune a pre-trained Whisper model on a new language (e.g., Esperanto) using standard fine-tuning. Measure the Word Error Rate (WER) drop on a held-out pre-trained language (e.g., English). 2. Ablation: Separate Embeddings: Implement only the Embedding Layer Surgery. Train on the new language with the new embeddings, while keeping the old embeddings frozen. Measure WER on both the new and old language. 3. Ablation: Task-wise Beam Search: Implement only the Task-wise Beam Search on top of a standard fine-tuned model. Measure if there is any performance gain. Then, combine both Embedding Layer Surgery and Task-wise Beam Search to confirm their synergistic effect.

## Open Questions the Paper Calls Out
No specific open questions were called out in the paper.

## Limitations
- The effectiveness of embedding isolation for mitigating catastrophic forgetting is supported by experimental results, but the lack of per-language baseline comparisons and the single dataset (Common Voice) limit generalizability.
- The claim that task-wise beam search meaningfully improves LID and ASR accuracy beyond what embedding isolation achieves alone is weakly supported.
- The paper assumes the ST embeddings remain stable for language identification, but doesn't verify this empirically.

## Confidence
- High Confidence: The technical implementation of separate embedding tables (Mechanism 1) is straightforward and the memory/memory trade-off is clearly stated.
- Medium Confidence: The effectiveness of embedding isolation for mitigating catastrophic forgetting is supported by experimental results, but limited by single dataset and lack of per-language comparisons.
- Low Confidence: The claim that task-wise beam search meaningfully improves LID and ASR accuracy beyond what embedding isolation achieves alone is weakly supported.

## Next Checks
1. **Per-language forgetting analysis**: Run the experiment tracking WER degradation for each pre-trained language individually as new languages are added sequentially. Plot WER trajectories to identify which language families are most vulnerable to forgetting, and test whether the proposed method prevents catastrophic forgetting for all language pairs or only specific combinations.

2. **Score calibration verification**: Implement a cross-language calibration test where the model transcribes mixed-language audio containing segments from multiple languages. Measure whether the beam search selection is language-biased by analyzing the distribution of selected languages across different input conditions, and verify that the ASR scores are comparable across languages using techniques like temperature scaling or Platt scaling.

3. **Incremental LID stability test**: Add languages in different orders (e.g., starting with typologically similar vs. dissimilar languages) and track LID accuracy for all languages after each addition. This would reveal whether the ST embeddings maintain discriminative power throughout the continual learning process or degrade over successive adaptations, potentially exposing hidden failure modes in the routing mechanism.