---
ver: rpa2
title: 'Addition is almost all you need: Compressing neural networks with double binary
  factorization'
arxiv_id: '2505.11076'
source_url: https://arxiv.org/abs/2505.11076
tags:
- compression
- weight
- binary
- quantization
- factorization
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Double Binary Factorization (DBF) compresses LLM weight matrices
  by factorizing them into two binary matrices with scaling vectors, achieving compression
  rates of 1-2 bits per weight. DBF provides 2-3.5x speedup during inference by replacing
  multiplications with additions.
---

# Addition is almost all you need: Compressing neural networks with double binary factorization

## Quick Facts
- arXiv ID: 2505.11076
- Source URL: https://arxiv.org/abs/2505.11076
- Reference count: 6
- Primary result: DBF achieves 1-2 bits per weight compression with 2-3.5x inference speedup using only additions

## Executive Summary
Double Binary Factorization (DBF) presents a novel approach to neural network compression by factorizing weight matrices into two binary matrices with scaling vectors. The method achieves compression rates of 1-2 bits per weight while replacing multiplications with additions, enabling 2-3.5x faster inference. DBF uniquely combines the benefits of binary quantization with factorization, allowing fine-grained control over compression ratios and incorporating weight importance during the factorization process.

The method demonstrates competitive performance compared to state-of-the-art quantization techniques like QuIP# and QTIP, achieving better perplexity and zero-shot accuracy than existing binary quantization approaches. With Llama2-7B, DBF reaches 26.45 perplexity at 1 bit/weight and 37.03 perplexity at 2 bits/weight, while providing significant inference speedups through addition-only computations.

## Method Summary
Double Binary Factorization works by decomposing each weight matrix W into two binary matrices P and Q with scaling vectors s_P and s_Q, such that W ≈ s_P ⊙ P ⊙ s_Q ⊙ Q. During inference, this factorization replaces expensive matrix multiplications with binary operations and additions. The method incorporates a loss function that considers both reconstruction error and weight importance, enabling better preservation of critical weights during compression. Unlike traditional quantization that uses fixed bit-width representations, DBF provides continuous control over the compression ratio by adjusting the factorization rank, making it more flexible than existing binary quantization methods.

## Key Results
- Achieves 1-2 bits per weight compression on Llama2-7B models
- Provides 2-3.5x inference speedup by replacing multiplications with additions
- Outperforms OneBit quantization and matches state-of-the-art methods like QuIP# and QTIP in perplexity and accuracy

## Why This Works (Mechanism)
DBF works by leveraging the sparsity and redundancy in neural network weight matrices through factorization. By decomposing weights into binary matrices, the method exploits the fact that many neural network computations can be approximated using binary operations with minimal accuracy loss. The scaling vectors allow fine-grained control over weight magnitudes, while the factorization rank provides a continuous knob to trade off between compression ratio and accuracy. The incorporation of weight importance during factorization ensures that critical weights are preserved, preventing catastrophic accuracy degradation that often occurs with aggressive quantization.

## Foundational Learning

**Binary Matrix Factorization**
- Why needed: Forms the mathematical foundation for decomposing weight matrices into binary components
- Quick check: Verify that P and Q matrices contain only +1/-1 values

**Weight Importance Scoring**
- Why needed: Ensures critical weights are preserved during aggressive compression
- Quick check: Confirm importance scores correlate with weight magnitudes and gradients

**Scaling Vector Optimization**
- Why needed: Provides continuous control over weight magnitudes beyond binary representation
- Quick check: Verify scaling vectors capture the dynamic range of original weights

## Architecture Onboarding

**Component Map**
Input -> Binary Matrix P -> Scaling Vector s_P -> Binary Matrix Q -> Scaling Vector s_Q -> Output

**Critical Path**
The critical path involves the matrix multiplication W·x being replaced by (s_P ⊙ P) · ((s_Q ⊙ Q) · x), where ⊙ denotes element-wise multiplication. This decomposition enables the use of binary operations and additions instead of floating-point multiplications.

**Design Tradeoffs**
DBF trades off reconstruction accuracy for compression ratio through the factorization rank. Lower ranks provide higher compression but may require more extensive fine-tuning. The method also balances between binary precision and the overhead of storing scaling vectors, which adds memory cost but enables better accuracy preservation.

**Failure Signatures**
- Excessive reconstruction error when factorization rank is too low
- Poor accuracy recovery despite fine-tuning when weight importance is not properly incorporated
- Memory overhead becomes prohibitive when scaling vectors are stored in full precision

**3 First Experiments**
1. Compare perplexity of compressed vs uncompressed Llama2-7B at different factorization ranks
2. Measure actual inference speedup on GPU/CPU for different batch sizes
3. Evaluate zero-shot accuracy on multiple benchmarks to verify generalization

## Open Questions the Paper Calls Out
None

## Limitations
- Limited evaluation to Llama2-7B model family only
- No exploration of larger models (70B+) or different architectures
- Fine-tuning procedures not fully detailed for reproducibility
- Does not address memory overhead from scaling vector storage

## Confidence

**High Confidence**: Technical methodology is sound with clear mathematical formulation. Claims about 1-2 bits/weight compression and addition-only operations are well-supported.

**Medium Confidence**: Reported 2-3.5x speedup and perplexity scores are reasonable but limited to single model family. Comparison with existing methods is fair but lacks extensive ablation studies.

**Low Confidence**: Generalization claims to other architectures and assertion of "outperforming" all methods lack sufficient cross-model validation evidence.

## Next Checks

1. Test DBF on multiple model architectures (GPT, Mistral, OPT) and sizes (Llama2-13B, 34B) to assess generalizability

2. Conduct systematic ablation studies varying factorization rank, scaling vector precision, and fine-tuning duration

3. Implement DBF on actual inference hardware to verify claimed 2-3.5x speedup and measure memory overhead from scaling vectors