---
ver: rpa2
title: 'STROKEVISION-BENCH: A Multimodal Video And 2D Pose Benchmark For Tracking
  Stroke Recovery'
arxiv_id: '2509.07994'
source_url: https://arxiv.org/abs/2509.07994
tags:
- block
- video
- stroke
- action
- transport
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces StrokeVision-Bench, the first dedicated dataset
  of 1,000 videos capturing stroke patients performing the Box and Block Test. The
  dataset includes both raw RGB frames and 2D skeletal keypoints for each video, annotated
  into four clinically meaningful action classes.
---

# STROKEVISION-BENCH: A Multimodal Video And 2D Pose Benchmark For Tracking Stroke Recovery

## Quick Facts
- arXiv ID: 2509.07994
- Source URL: https://arxiv.org/abs/2509.07994
- Reference count: 0
- Primary result: 87.68% accuracy on 4-class action classification using R3D CNN

## Executive Summary
This paper introduces StrokeVision-Bench, the first dedicated dataset of 1,000 videos capturing stroke patients performing the Box and Block Test. The dataset includes both raw RGB frames and 2D skeletal keypoints for each video, annotated into four clinically meaningful action classes. The authors benchmark seven state-of-the-art models, including four video-based (R3D, R2Plus1D, Video MViT, Video Swin Transformer) and three skeleton-based (MotionBERT, PoseConv3D, MS-G3D). Among these, R3D achieves the highest overall accuracy at 87.68%, while MotionBERT leads among skeleton-based methods with 84.29%. The results show that CNN-based models generally outperform transformer-based ones on this small, challenging dataset, and that skeleton-based methods offer comparable accuracy with additional privacy benefits and detailed mobility insights. The study highlights the potential of video action classification for objective stroke rehabilitation assessment.

## Method Summary
The StrokeVision-Bench dataset consists of 1,000 one-second videos of stroke patients performing the Box and Block Test, annotated into four action classes: Grasping, Non-task Movement, Transport with Object, and Transport without Object. The dataset provides both raw RGB frames and 2D skeletal keypoints (17 joints via Sapiens pose estimation). Seven models were benchmarked: four video-based (R3D, R2Plus1D, Video MViT, Video Swin Transformer) pretrained on Kinetics-400, and three skeleton-based (MotionBERT, PoseConv3D pretrained on NTU RGB+D, and MS-G3D from scratch). Models were trained on a NVIDIA V100 GPU using PyTorch, with video input size V ∈ R^(T×C×H×W) and skeleton input V_J ∈ R^(T×J×2).

## Key Results
- R3D achieves the highest overall accuracy at 87.68% on the 4-class classification task
- MotionBERT leads skeleton-based methods with 84.29% accuracy, demonstrating comparable performance to frame-based models
- CNN-based models (R3D, R2Plus1D) outperform transformer-based models (Video MViT, Video Swin) on this small dataset
- Distinguishing "Transport with block" from "Transport without block" remains the most challenging classification case, with accuracy dropping to 57.1% for Video MViT

## Why This Works (Mechanism)

### Mechanism 1
- Claim: CNN-based 3D architectures outperform vision transformers on small, clinically-derived video datasets for stroke rehabilitation action classification.
- Mechanism: R3D and R(2+1)D leverage 3D convolutions that decompose spatiotemporal processing into local patterns, requiring fewer samples to converge than global attention mechanisms in transformers. The inductive bias of convolution (locality, translation equivariance) matches the constrained, repetitive nature of BBT actions.
- Core assumption: The BBT action vocabulary (4 classes: Grasping, Non-task Movement, Transport with/without Object) exhibits consistent local motion patterns that generalize from Kinetics-400 pretraining.
- Evidence anchors:
  - [abstract] "CNN-based models generally outperform transformer-based ones on this small, challenging dataset"
  - [section 5] "accuracy falls from 87% for R3D to 74% for the Video Swin Transformer, reflecting the transformers' greater data requirements"
  - [corpus] Related work (MMeViT, ARAT automation) uses transformer ensembles but on larger/multi-view datasets—no direct contradiction, but suggests transformer viability scales with data.
- Break condition: If actions require modeling long-range temporal dependencies beyond 1-second clips, or if dataset expands significantly, transformer gap may narrow.

### Mechanism 2
- Claim: 2D skeleton-based methods achieve comparable accuracy to frame-based models while providing joint-level interpretability and privacy preservation.
- Mechanism: Pose extraction (via Sapiens) reduces raw video to 17 keypoints × 30 frames, focusing model capacity on body kinematics. MotionBERT's transformer encoder captures temporal dynamics of joint trajectories without appearance confounders.
- Core assumption: Pose estimation quality remains robust for stroke-impaired movements with atypical kinematics; Sapiens generalizes to clinical populations.
- Evidence anchors:
  - [abstract] "skeleton-based methods offer comparable accuracy with additional privacy benefits and detailed mobility insights"
  - [section 1] "2D skeleton serves two purposes: (1) rich information about mobility... (2) does not reveal privacy attributes"
  - [corpus] ST_GCN attention for stroke (arxiv 2510.00049) validates graph-convolutional skeleton modeling for rehabilitation—consistent finding.
- Break condition: If pose estimation degrades on severely impaired patients (occlusion, atypical postures), skeleton accuracy may drop below frame-based methods.

### Mechanism 3
- Claim: Transfer learning from general action datasets provides effective initialization despite domain shift to clinical stroke movements.
- Mechanism: Kinetics-400 pretraining for video models and NTU RGB+D for skeleton models inject low-level spatiotemporal features (edge motion, joint trajectories) that transfer even when source actions differ from BBT. Fine-tuning adapts domain-specific patterns.
- Core assumption: The learned representations capture transferable "movement primitives" (reaching, grasping) present in both general and clinical actions.
- Evidence anchors:
  - [section 4] "initialized all frame-based networks with weights pretrained on Kinetics-400"
  - [table 1] MS-G3D without pretraining achieves 78.39% vs MotionBERT's 84.29% with pretraining—suggesting pretraining helps but isn't decisive
  - [corpus] No direct corpus evidence on transfer learning effectiveness for stroke; assumption remains unvalidated externally.
- Break condition: If stroke movements differ fundamentally (e.g., spasticity patterns not in source data), pretraining may provide limited benefit or negative transfer.

## Foundational Learning

- Concept: 3D Convolution Decomposition (R3D vs R(2+1)D)
  - Why needed here: Understanding why R3D and R(2+1)D achieve top performance (87.68%, 86.96%) requires grasping how 3D kernels factorize temporal vs spatial processing.
  - Quick check question: Can you explain why (2+1)D convolutions might regularize learning compared to full 3D kernels?

- Concept: Skeleton Sequence Modeling (Graph vs Transformer)
  - Why needed here: The benchmark includes three skeleton architectures (MotionBERT, PoseConv3D, MS-G3D) with different inductive biases; selecting among them requires understanding graph convolutions vs temporal transformers.
  - Quick check question: What structural prior does MS-G3D's graph convolution encode that MotionBERT's transformer does not?

- Concept: Clinical Assessment Granularity
  - Why needed here: The four action classes derive from BBT semantics; designing or extending this taxonomy requires understanding clinical reasoning behind "Transport with/without Object" distinctions.
  - Quick check question: Why might "Transport without Object" be the most challenging class to classify (71.4% MS-G3D, 57.1% Video MViT)?

## Architecture Onboarding

- Component map: Raw RGB video (T×C×H×W) OR 2D keypoints (T×17×2) via Sapiens pose estimator -> Encoder: Video pathway (R3D/R2Plus1D/MViT/Video Swin) OR Skeleton pathway (MotionBERT/PoseConv3D/MS-G3D) -> Classification head: 4-class softmax (Grasping, Non-task, Transport+Object, Transport-Object)

- Critical path:
  1. Pose extraction must complete before skeleton models can train—Sapiens inference is a blocking dependency.
  2. Train-test split enforces patient separation (no leakage); verify before any training run.
  3. Class imbalance (Non-task/Grasping overrepresented) requires weighted loss or oversampling.

- Design tradeoffs:
  - CNN vs Transformer: CNNs for data efficiency (current 1K videos); transformers if dataset grows 5-10×
  - Frame vs Skeleton: Frames for maximum accuracy (87.68% R3D); skeleton for privacy/joint analytics (84.29% MotionBERT)
  - Pretrained vs Scratch: Pretraining helps (MotionBERT +5.9% vs MS-G3D) but MS-G3D's scratch performance suggests architectural choices matter equally

- Failure signatures:
  - Low "Transport without Object" accuracy across all models → signal that this class needs more samples or distinctive features
  - Transformer underperformance (MViT 77.14%) → dataset too small for global attention
  - PoseConv3D collapse (68.93%) → 3D convolution on sparse keypoints may overfit; verify keypoint density

- First 3 experiments:
  1. Establish R3D baseline with Kinetics-400 pretraining; log per-class accuracy to identify "Transport without Object" gap.
  2. Run MotionBERT on skeleton data; extract attention maps to validate joint-level interpretability claims.
  3. Ablate pretraining by training R3D and MotionBERT from scratch; quantify transfer learning contribution on this domain shift.

## Open Questions the Paper Calls Out

- **Question:** Does combining raw RGB frames with 2D skeletal keypoints via multimodal fusion improve classification accuracy over the best unimodal baselines?
- **Basis in paper:** [inferred] The paper presents StrokeVision-Bench as a "Multimodal" dataset but evaluates video and skeleton-based methods strictly in isolation.
- **Why unresolved:** The current study only establishes independent baselines for each modality without testing integration or fusion strategies that could leverage the strengths of both CNNs and skeleton networks.
- **What evidence would resolve it:** Accuracy metrics comparing fused architectures (e.g., late fusion or attention-based merging) against the standalone R3D (87.68%) and MotionBERT (84.29%) models.

- **Question:** How can models be improved to better distinguish "Transport with block" from "Transport without block," which is the primary source of classification error?
- **Basis in paper:** [inferred] The confusion matrix analysis explicitly identifies distinguishing the two transport variants as a "complex case" where even top-performing architectures like Video MViT and MS-G3D suffer significant performance drops (e.g., 57.1% accuracy).
- **Why unresolved:** Current state-of-the-art models struggle to capture the subtle visual or kinematic cues differentiating these two specific temporal phases of the action.
- **What evidence would resolve it:** Improved recall or significantly reduced confusion rates between the "Transport with/without block" classes in future benchmark iterations.

- **Question:** Can the features learned from StrokeVision-Bench generalize to other standardized clinical assessments beyond the Box and Block Test (BBT)?
- **Basis in paper:** [explicit] The authors explicitly state in the Limitations section that the dataset "includes only the Box and Block Test, leaving other clinical assessments of patient mobility unexamined."
- **Why unresolved:** The dataset is currently task-specific, and it is unknown if the learned representations of upper extremity function transfer to other rehabilitation exercises or scoring systems.
- **What evidence would resolve it:** Zero-shot or fine-tuned transfer learning performance on datasets containing distinct clinical tasks (e.g., Action Research Arm Test items).

## Limitations
- Dataset access not publicly available—no download link provided
- Limited to Box and Block Test only, not covering other clinical assessments
- Key hyperparameters (learning rates, augmentation, batch sizes) unspecified
- Pose estimation quality on stroke-impaired movements remains untested

## Confidence

- High: CNN-based models (R3D) outperforming transformers on this small dataset
- Medium: Skeleton methods achieving comparable accuracy while preserving privacy
- Medium: Transfer learning from Kinetics-400 and NTU RGB+D providing effective initialization
- Low: Clinical generalizability beyond the specific patient cohort and Box and Block Test

## Next Checks

1. Obtain or simulate the StrokeVision-Bench dataset and verify class distributions match reported splits
2. Run ablation studies with/without Kinetics-400 pretraining to quantify transfer learning contribution
3. Test pose estimation robustness on stroke-impaired movements using external clinical video samples