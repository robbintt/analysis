---
ver: rpa2
title: 'PreLoRA: Hybrid Pre-training of Vision Transformers with Full Training and
  Low-Rank Adapters'
arxiv_id: '2509.21619'
source_url: https://arxiv.org/abs/2509.21619
tags:
- training
- lora
- weight
- layer
- convergence
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces PreLoRA, a framework for integrating Low-Rank
  Adaptation (LoRA) into the pretraining of large-scale models, specifically Vision
  Transformers (ViT-Large). The approach addresses the computational and memory bottlenecks
  associated with training large models by dynamically switching from full-parameter
  training to LoRA-based training during the pretraining phase.
---

# PreLoRA: Hybrid Pre-training of Vision Transformers with Full Training and Low-Rank Adapters

## Quick Facts
- arXiv ID: 2509.21619
- Source URL: https://arxiv.org/abs/2509.21619
- Authors: Krishu K Thapa, Reet Barik, Krishna Teja Chitty-Venkata, Murali Emani, Venkatram Vishwanath
- Reference count: 31
- Key outcome: Framework that integrates LoRA into pretraining, reducing trainable parameters to 10% while maintaining accuracy and improving throughput 3x

## Executive Summary
PreLoRA introduces a novel framework that dynamically transitions Vision Transformer pretraining from full-parameter training to LoRA-based training when weight norms stabilize. The method employs a partial convergence test to identify the optimal switching point and uses dynamic rank assignment to allocate model capacity efficiently across layers. Experiments demonstrate that PreLoRA preserves model accuracy while achieving 3x throughput improvement, 1.5x faster training time per epoch, and 20% reduction in GPU memory consumption.

## Method Summary
PreLoRA works by first training a ViT-Large model with full parameters until a partial convergence test indicates weight norm stabilization. At this point, the method switches to LoRA training where only low-rank adapter matrices are updated. The framework includes a dynamic rank assignment algorithm that allocates different ranks to different layers based on their convergence rates, and a warmup period where both full and LoRA parameters are trained together before freezing the base model. The implementation uses PyTorch v2.7.1, HuggingFace v0.33.0, and PEFT v0.15.2 on 64 NVIDIA A100 GPUs.

## Key Results
- Preserves ImageNet-1k top-1 accuracy while reducing trainable parameters to 10% of original size
- Achieves 3x improvement in throughput during LoRA-only training phase
- Reduces average training time per epoch by 1.5x compared to full-parameter training
- Decreases GPU memory consumption by 20% through parameter freezing

## Why This Works (Mechanism)

### Mechanism 1: Weight Norm Stabilization Enables Low-Rank Learning
As pretraining progresses, weight updates during later stages stabilize, indicating the optimization trajectory has shifted to a low-intrinsic-rank subspace. PreLoRA exploits this by switching to LoRA, assuming remaining learning can be captured by low-rank matrices rather than full-rank updates. The core assumption is that the remaining optimization path resides in a low-dimensional parameter manifold.

### Mechanism 2: Partial Convergence Test Prevents Premature Switching
PreLoRA monitors weight norm changes and loss changes over sliding windows, switching only when changes fall below thresholds. This ensures the "high learning" phase is completed in full-parameter mode before transitioning to LoRA. The convergence of weight norms serves as a proxy for completing the high-capacity learning phase.

### Mechanism 3: Dynamic Rank Assignment Optimizes Parameter Efficiency
Different layers converge at different rates, so PreLoRA assigns higher ranks to layers with larger weight changes and lower ranks to stable layers. This maximizes the utility of the limited trainable parameter budget by matching layer capacity to layer-specific convergence rates.

## Foundational Learning

- **Concept: Low-Rank Adaptation (LoRA)**
  - Why needed: PreLoRA relies on LoRA to freeze the main model and inject trainable rank decomposition matrices
  - Quick check: Can you explain why LoRA reduces memory overhead during backpropagation compared to full fine-tuning?

- **Concept: Training Dynamics (Loss vs. Weight Norms)**
  - Why needed: The core switching logic relies on distinguishing "learning" (loss decrease) from "stabilization" (weight norm decrease)
  - Quick check: If loss is decreasing but weight norms are constant, is the model still learning?

- **Concept: Vision Transformer (ViT) Architecture**
  - Why needed: The method targets specific modules (Query, Key, Value, Dense) within the attention mechanism
  - Quick check: Which modules in a Transformer block typically handle the projection of input tokens into the attention space?

## Architecture Onboarding

- **Component map:** Full Model Trainer -> Convergence Monitor -> Switch Manager -> LoRA Trainer
- **Critical path:** Train full model -> Trigger partial convergence test -> If passed, compute dynamic ranks -> Inject LoRA + Warmup -> Freeze Full Model -> Continue LoRA training
- **Design tradeoffs:**
  - Strictness (τ, ζ): Strict thresholds preserve accuracy but reduce time savings
  - Warmup Window (w): Longer warmup stabilizes LoRA initialization but delays benefits
  - Rank Range: Min=8, Max=64; setting r_min too low might prune necessary features
- **Failure signatures:**
  - Early Switch: Training loss plateaus prematurely at higher value than baseline
  - Insufficient Warmup: Sudden spike in loss immediately after freezing base model
  - Uniform Rank Misapplication: Applying high ranks to all layers negates parameter savings
- **First 3 experiments:**
  1. Baseline Calibration: Train full ViT-Large and log weight norms/loss per epoch to visualize stabilization
  2. Threshold Ablation: Run PreLoRA with relaxed vs. strict settings to measure accuracy-vs-speed trade-off
  3. Warmup Tuning: Set fixed switch point and vary warmup windows to identify minimum required

## Open Questions the Paper Calls Out
1. Can online hyperparameter optimization techniques replace user-defined thresholds to automate the partial convergence test?
2. Does the correlation between weight norm stabilization and intrinsic rank hold for Large Language Models as it does for Vision Transformers?
3. Is the min-max normalization strategy for rank assignment optimal compared to learned or gradient-based allocation methods?

## Limitations
- Results are based on ViT-Large architecture and may not generalize to other vision or language models
- Performance on ImageNet-1k may not transfer to other domains with different feature complexities
- 300 epochs total training time may not be feasible for all applications

## Confidence
- High confidence: The core mechanisms (partial convergence testing, dynamic rank assignment) are logically sound and supported by experimental results
- Medium confidence: The claim of 3x throughput improvement depends heavily on hardware configuration and implementation details
- Medium confidence: The generalization potential to other architectures is plausible but not empirically validated in this work

## Next Checks
1. Apply PreLoRA to BERT-base or ResNet-50 and compare convergence behavior, accuracy retention, and efficiency gains against full training baselines
2. Implement automated logging of weight norm stabilization curves to validate the proposed partial convergence thresholds across different model scales
3. Profile GPU memory consumption during warmup phase to verify the claimed 20% reduction holds consistently across different batch sizes and gradient accumulation strategies