---
ver: rpa2
title: Adaptive Keyframe Sampling for Long Video Understanding
arxiv_id: '2502.21271'
source_url: https://arxiv.org/abs/2502.21271
tags:
- video
- arxiv
- frames
- understanding
- keyframes
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents Adaptive Keyframe Sampling (AKS), a plug-and-play
  module designed to improve long video understanding by selecting informative keyframes
  for multimodal large language models (MLLMs). AKS addresses the challenge that the
  vast number of video tokens exceeds MLLM capacity, leading to information loss when
  using uniform sampling.
---

# Adaptive Keyframe Sampling for Long Video Understanding

## Quick Facts
- arXiv ID: 2502.21271
- Source URL: https://arxiv.org/abs/2502.21271
- Reference count: 40
- Improves long video understanding by selecting informative keyframes for multimodal large language models

## Executive Summary
This paper addresses the challenge of long video understanding with multimodal large language models (MLLMs), which struggle with the high dimensionality of video data. The authors propose Adaptive Keyframe Sampling (AKS), a plug-and-play module that selects informative keyframes to maximize useful information within the fixed token budget of MLLMs. AKS formulates keyframe selection as an optimization problem balancing relevance between frames and prompts, and coverage of the video timeline. Experiments on LongVideoBench and VideoMME benchmarks show consistent improvements across three baseline MLLMs, with the strongest improvement being 3.8% on LongVideoBench and 0.9% on VideoMME when integrated with LLaVA-Video-7B.

## Method Summary
AKS is a plug-and-play module that preprocesses long videos by selecting informative keyframes before feeding them to MLLMs. The method uses a Vision-Language model (BLIP or CLIP) to score the relevance of each candidate frame to the text prompt. An adaptive algorithm (ADA) recursively partitions the video timeline, balancing between concentrating on highly relevant frames (TOP mode) and ensuring temporal coverage (BIN mode). The algorithm approximates the optimal tradeoff between relevance maximization and coverage by computing score differences at each partition level. AKS operates as a pre-filtering stage that replaces uniform sampling, maintaining the MLLM's token budget while improving the information density of the input frames.

## Key Results
- AKS consistently improves video QA accuracy across three baseline MLLMs (Qwen2VL, LLaVA-OV, LLaVA-Video)
- Strongest improvement: 3.8% on LongVideoBench and 0.9% on VideoMME when integrated with LLaVA-Video-7B
- AKS selects keyframes more relevant to the question, leading to better MLLM performance
- Different VL scorers perform better on different benchmarks (BLIP for LongVideoBench, CLIP for VideoMME)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Query-conditioned relevance scoring identifies frames most likely to contain answer-critical information
- Mechanism: A lightweight vision-language model (BLIP or CLIP) computes an image-text matching score s(Q, Ft) between each candidate frame and the text prompt. This score approximates how relevant frame t is to answering question Q, enabling selective filtering before expensive MLLM processing.
- Core assumption: Pre-trained VL models can approximate semantic relevance between visual content and text queries without task-specific training.
- Evidence anchors:
  - [abstract] "AKS formulates keyframe selection as an optimization problem balancing relevance between keyframes and prompts"
  - [section 3.2] "We propose a heuristic method... (1) The relevance between each frame and the prompt is high, i.e., the visual data is useful for question answering"
  - [corpus] FOCUS (2510.27280) similarly applies retrieval-style scoring for keyframe selection, suggesting convergent validation of relevance-based approaches
- Break condition: Questions requiring complex temporal reasoning (e.g., "What happened after X?") may not be well-captured by per-frame relevance scores alone.

### Mechanism 2
- Claim: Recursive adaptive partitioning balances concentrated relevance peaks against temporal coverage requirements
- Mechanism: The ADA algorithm recursively partitions the video timeline. At each level, it computes stop (mean of top-M scores) and sall (mean of all scores). If stop - sall exceeds threshold sthr, it samples top-M frames directly (concentrated mode). Otherwise, it splits the bin and recurses (distributed mode). This approximates the optimal tradeoff between relevance maximization and coverage.
- Core assumption: Score distribution variance signals whether relevant information is temporally concentrated (few peaks favor TOP) or distributed (even spread favors BIN).
- Evidence anchors:
  - [abstract] "The adaptive algorithm approximates the best solution by recursively partitioning the video and selecting informative keyframes"
  - [section 3.3] "ADA is a compromise between the special cases, TOP and BIN... absorbs the advantages of TOP and BIN strategies and adaptively allocates keyframes"
  - [corpus] Episodic Memory Representation (2508.09486) notes that keyframe retrieval methods simplify temporal dependencies—relevant to understanding coverage limitations
- Break condition: Threshold sthr miscalibration causes mode collapse—over-concentrating on one region or over-spreading across irrelevant frames.

### Mechanism 3
- Claim: Pre-filtering module architecture achieves gains without modifying the base MLLM
- Mechanism: AKS operates as a plug-and-play preprocessing stage that replaces uniform sampling upstream of the MLLM's vision encoder. The MLLM receives exactly M frames (unchanged token budget), but with higher per-frame information density. No retraining or architectural changes to the MLLM are required.
- Core assumption: MLLM performance on long videos is bottlenecked by input quality (frame relevance), not by reasoning capacity or context window size alone.
- Evidence anchors:
  - [abstract] "introduces a plug-and-play keyframe selection module that maximizes useful information with a fixed number of video tokens"
  - [section 3.1] "We insert a plug-and-play module, Adaptive Keyframe Sampling (AKS, marked in green frames) into the MLLM"
  - [corpus] VSI (2508.06869) similarly treats sparse frame sampling as a necessary preprocessing step, reinforcing the pre-filtering paradigm
- Break condition: When critical information is inherently distributed across more frames than the token budget allows, any selection method will lose signal.

## Foundational Learning

- Concept: Vision-Language Contrastive Learning (CLIP-style)
  - Why needed here: Understanding how BLIP/CLIP produce image-text matching scores is essential for interpreting s(Q, Ft) and selecting appropriate VL scorers.
  - Quick check question: Why does CLIP enable zero-shot image-text matching without task-specific fine-tuning?

- Concept: Submodular Optimization and Coverage Functions
  - Why needed here: The coverage term c(I) relates to facility location and maximum coverage problems—knowing this helps understand why recursive partitioning approximates an intractable optimization.
  - Quick check question: Why is greedy selection of top-K frames insufficient when you need coverage over a temporal dimension?

- Concept: Transformer Context Window Constraints
  - Why needed here: The entire motivation for keyframe selection stems from MLLMs having finite visual token capacity (32-64 frames typically).
  - Quick check question: If each frame produces 256 visual tokens and an MLLM has an 8K token context window, how many frames can fit alongside a typical text prompt?

## Architecture Onboarding

- Component map:
  - Raw video V ∈ R^(T×W×H×C) -> Candidate Extractor (1 fps default) -> VL Scorer (BLIP/CLIP) -> ADA Core (recursive partitioner) -> Frame Selector -> Downstream MLLM (unchanged)

- Critical path:
  1. Video -> candidate frame extraction (1 fps default)
  2. Each candidate + prompt -> VL scorer -> relevance score list
  3. Score list -> ADA recursive partitioning -> M selected indices
  4. Selected frames -> MLLM vision encoder -> visual tokens
  5. Visual tokens + prompt -> LLM -> generated answer

- Design tradeoffs:
  - VL scorer choice: BLIP favors object-level questions (LongVideoBench); CLIP favors global perception (VideoMME)
  - Sampling frequency: 1 fps maximizes accuracy; 0.1–0.25 fps reduces compute with moderate accuracy loss
  - sthr threshold: Lower -> TOP-like behavior (concentrated); Higher -> BIN-like behavior (distributed)
  - Max depth L: Higher L enables finer-grained partitioning but increases overhead

- Failure signatures:
  - All keyframes cluster in narrow time window -> sthr too low; increase threshold
  - No improvement over uniform baseline -> VL scores have low variance; check scorer quality
  - Large accuracy variance across question types -> may need benchmark-specific hyperparameter tuning
  - Excessive latency -> reduce candidate sampling fps or use lighter VL scorer

- First 3 experiments:
  1. VL scorer validation: Compare BLIP vs CLIP scoring on your target benchmark's validation split to determine optimal scorer.
  2. Hyperparameter sweep: Grid search sthr ∈ {0.2, 0.4, 0.6, 0.8} and L ∈ {3, 4, 5} on validation data; paper shows different benchmarks favor different settings.
  3. Efficiency frontier: Measure accuracy vs. total latency at fps ∈ {1.0, 0.5, 0.25, 0.1} to identify acceptable operating point for your latency budget.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can the pre-filtering algorithm be optimized to achieve a better tradeoff between selection accuracy and computational efficiency, particularly for videos exceeding typical benchmark lengths?
- **Basis in paper:** [explicit] The authors state in Section 4.4, "It is worth exploring more efficient pre-filtering algorithms towards a better tradeoff between accuracy and efficiency," noting that reducing sampling frequency (fps) is currently required to manage costs.
- **Why unresolved:** The current implementation relies on scoring every candidate frame (e.g., at 1 fps) using a Vision-Language model, which adds significant latency before the MLLM even processes the input.
- **What evidence would resolve it:** Development of a sparse scoring mechanism or a lightweight selector that maintains QA accuracy while reducing the pre-processing FLOPs and latency by a significant margin.

### Open Question 2
- **Question:** Can the keyframe selection criteria be learned end-to-end rather than relying on fixed, off-the-shelf Vision-Language models (like BLIP or CLIP) for relevance scoring?
- **Basis in paper:** [inferred] The paper uses frozen VL models to approximate the "usefulness" of frames (Eq. 1). However, Section 4.4 shows that different VL models (BLIP vs. CLIP) perform better on different benchmarks, suggesting a mismatch between the proxy score and the actual MLLM's needs.
- **Why unresolved:** Using a fixed, non-trainable scorer assumes that the similarity score from a model like CLIP perfectly aligns with the MLLM's probability of answering correctly, which is a heuristic approximation.
- **What evidence would resolve it:** An ablation study showing that a trainable selection module, jointly optimized with the downstream MLLM, outperforms the current plug-and-play approach using frozen scorers.

### Open Question 3
- **Question:** Is it possible to dynamically determine the optimal hyperparameters, specifically the recursion depth ($L$) and score threshold ($s_{thr}$), based on the content complexity of the specific video?
- **Basis in paper:** [inferred] Table 5 demonstrates that the optimal parameters vary significantly between datasets (LongVideoBench vs. VideoMME). The current method requires manual tuning or a fixed setting, implying the "one-size-fits-all" configuration is suboptimal.
- **Why unresolved:** The algorithm currently treats $L$ and $s_{thr}$ as global hyperparameters, failing to account for videos that might be information-dense (requiring different splitting) versus those that are sparse.
- **What evidence would resolve it:** A mechanism that adaptively sets these parameters per video and demonstrates consistent performance improvements without requiring dataset-specific manual tuning.

## Limitations
- Performance sensitive to threshold hyperparameter sthr requiring benchmark-specific tuning
- Reliance on pre-trained VL models without task-specific fine-tuning may limit domain adaptability
- Candidate sampling rate (1 fps default) may miss critical events in high-frame-rate content

## Confidence
- **High confidence**: The core mechanism of using VL model-based relevance scoring for keyframe selection is well-established in the literature (evidenced by convergent approaches like FOCUS and VSI). The plug-and-play architecture that avoids MLLM modification is clearly demonstrated and practically valuable.
- **Medium confidence**: The specific adaptive partitioning algorithm (ADA) shows consistent improvements across benchmarks, but the optimal hyperparameters (sthr, L) vary by task, suggesting the algorithm may not be universally optimal without tuning. The claim that AKS "maximizes useful information" is supported by empirical results but lacks theoretical guarantees about optimality.
- **Low confidence**: The paper's assertion that AKS is broadly applicable across different MLLM architectures (Qwen2VL, LLaVA-OV, LLaVA-Video) is based on limited testing. The qualitative analysis showing AKS selects "more relevant keyframes" is subjective and lacks quantitative metrics for keyframe quality assessment.

## Next Checks
1. Cross-domain robustness test: Evaluate AKS on specialized video domains (e.g., surveillance footage, medical procedure videos, sports analytics) where the pre-trained BLIP/CLIP models may have limited visual-semantic coverage. Compare performance against domain-specific fine-tuned VL models for relevance scoring.
2. Temporal reasoning capability assessment: Design targeted experiments with questions requiring complex temporal reasoning ("What happened 5 minutes after event X?") to quantify AKS's limitations in handling non-local temporal dependencies. Measure whether adaptive sampling can be extended to explicitly model temporal gaps.
3. Scalability analysis: Test AKS on videos exceeding 10 minutes with varying frame rates (30 fps, 60 fps) to assess whether the 1 fps candidate sampling rate remains adequate. Measure the trade-off between sampling frequency, computational cost, and accuracy degradation as video length increases.