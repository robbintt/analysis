---
ver: rpa2
title: 'MCAQ-YOLO: Morphological Complexity-Aware Quantization for Efficient Object
  Detection with Curriculum Learning'
arxiv_id: '2511.12976'
source_url: https://arxiv.org/abs/2511.12976
tags:
- quantization
- complexity
- tile
- morphological
- spatial
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces MCAQ-YOLO, a framework for tile-wise spatial\
  \ mixed-precision quantization in real-time object detectors. The key insight is\
  \ that morphological complexity\u2014measured through fractal dimension, texture\
  \ entropy, gradient variance, edge density, and contour complexity\u2014serves as\
  \ a predictor of spatial quantization sensitivity."
---

# MCAQ-YOLO: Morphological Complexity-Aware Quantization for Efficient Object Detection with Curriculum Learning

## Quick Facts
- **arXiv ID:** 2511.12976
- **Source URL:** https://arxiv.org/abs/2511.12976
- **Reference count:** 34
- **Primary result:** 85.6% mAP@0.5 with 4.2 bits average precision and 151 FPS throughput

## Executive Summary
This paper introduces MCAQ-YOLO, a framework for tile-wise spatial mixed-precision quantization in real-time object detectors. The key insight is that morphological complexity—measured through fractal dimension, texture entropy, gradient variance, edge density, and contour complexity—serves as a predictor of spatial quantization sensitivity. The approach allocates different bit-widths to different spatial regions based on their complexity, with a calibration-time analysis design that adds only 0.3ms inference overhead while achieving 151 FPS throughput.

On a construction safety equipment dataset, MCAQ-YOLO achieves 85.6% mAP@0.5 with an average bit-width of 4.2 bits and a 7.6× compression ratio, outperforming uniform 4-bit quantization by 3.5 percentage points. The method also incorporates curriculum learning to stabilize optimization and accelerate convergence, yielding 2.5× faster convergence and approximately 60% lower gradient variance. Cross-dataset evaluation on COCO 2017 (+2.9%) and Pascal VOC 2012 (+2.3%) demonstrates consistent improvements, with performance gains correlating with within-image complexity variation.

## Method Summary
MCAQ-YOLO implements tile-wise spatial mixed-precision quantization for YOLOv8 object detection by allocating 2-8 bits per spatial tile based on morphological complexity. The method uses five complementary metrics—fractal dimension, texture entropy, gradient variance, edge density, and contour complexity—aggregated through a learned MLP with monotonicity constraints. A curriculum learning schedule progressively increases training sample complexity and quantization difficulty, with three stages: warm-up (epochs 0-20, low complexity only, FP16), transition (epochs 20-50, mixed complexity), and full MCAQ (epochs 50-300). The framework includes a custom CUDA kernel for efficient inference-time tile-wise quantization, achieving 151 FPS with only 0.3ms overhead through calibration-time analysis and temporal caching for video streams.

## Key Results
- Achieves 85.6% mAP@0.5 with 4.2 bits average precision and 7.6× compression ratio on construction safety equipment dataset
- Outperforms uniform 4-bit quantization by 3.5 percentage points while maintaining 151 FPS throughput
- Demonstrates 2.5× faster convergence and 60% lower gradient variance through curriculum learning
- Shows consistent cross-dataset improvements (+2.9% on COCO 2017, +2.3% on Pascal VOC 2012) with gains correlating to complexity variation

## Why This Works (Mechanism)

### Mechanism 1: Morphological Complexity as Spatial Quantization Sensitivity Predictor
The framework proposes that regions with higher morphological complexity (irregular boundaries, rich textures) require higher bit precision while simple regions tolerate aggressive compression. Five complementary metrics—fractal dimension (D_f), texture entropy (LBP-based), gradient variance, edge density, and contour complexity—are aggregated into a unified score C(Ω) via learned weights. An MLP maps C → bit-width with monotonicity constraints. This replaces network-centric sensitivity metrics with signal-centric metrics operating at tile granularity. The empirical correlation between morphological complexity and quantization-induced mAP degradation (ρ = 0.73, tile-level, n = 5,000) generalizes across deployment domains.

### Mechanism 2: Calibration-Time Analysis with Inference-Time Lookup
Spatial mixed-precision can be deployed with minimal inference overhead by precomputing bit-maps during calibration. Morphological analysis runs once on calibration images (~1,000 samples) to generate spatial bit-maps (~0.5KB per image for 8×8 grid). At inference, a custom CUDA kernel retrieves tile bit-widths and applies per-element quantization with spatially varying precision. For video streams, temporal caching (85% hit rate on similar consecutive frames) reduces amortized overhead.

### Mechanism 3: Curriculum Learning for Quantization-Aware Training Stability
Gradually increasing training sample complexity and quantization difficulty accelerates convergence and reduces gradient variance. Three-stage curriculum: (1) Warm-up (epochs 0–20): low-complexity samples only (C ≤ 0.2), FP16 precision; (2) Transition (epochs 20–50): mixed complexity, dynamic bit allocation with temperature annealing (α: 10 → 1); (3) Full MCAQ (epochs 50–300): all samples, aggressive quantization. The complexity threshold τ_t increases linearly during warm-up.

## Foundational Learning

- **Concept: Rate–Distortion Theory Basics**
  - **Why needed here:** The paper motivates bit allocation via R(D) = ½log₂(σ²/D), explaining why high-variance (complex) regions need more bits. Understanding this connects morphological complexity to information-theoretic coding requirements.
  - **Quick check question:** Given two tiles with variances σ₁² = 100 and σ₂² = 400, which requires more bits to achieve the same distortion D, and by approximately what factor?

- **Concept: Straight-Through Estimator (STE) for Quantization Gradients**
  - **Why needed here:** Gradients cannot flow through discrete quantization Q(x). STE approximates ∂Q/∂x ≈ 1 within clipping range. The mapping network and quantization module rely on this for end-to-end training.
  - **Quick check question:** If quantization function is Q(x) = round(x/s)·s, what gradient does STE assign to the rounding operation?

- **Concept: Box-Counting Fractal Dimension**
  - **Why needed here:** D_f is the primary geometric complexity metric (O(n log n) optimized). Understanding how edge patterns translate to dimension values (1.0 = line, 2.0 = area-filling) helps interpret complexity heatmaps.
  - **Quick check question:** An edge map requires N(ε) boxes at scale ε. If N doubles when ε halves, what is the fractal dimension?

## Architecture Onboarding

- **Component map:** Input Image → YOLOv8 Backbone → Feature Maps → Morphological Complexity Analyzer (5 metrics + MLP) → Complexity Map C → Complexity-to-Bit Mapping Network (MLP + monotonicity) → Bit Allocation Map → Spatial Adaptive Quantization (Custom CUDA Kernel) → FPN Neck → Detection Head

- **Critical path:** 1. Feature extraction (YOLOv8 backbone, 4.2ms) 2. Morphological analysis on downsampled features (1.8ms calibration / cached at inference) 3. Bit-map generation via MLP (0.4ms / precomputed) 4. Tile-wise quantization with smooth mask blending (0.3ms) 5. Detection head forward pass (1.5ms)

- **Design tradeoffs:**
  | Decision | Choice Made | Alternative | Tradeoff |
  |----------|-------------|-------------|----------|
  | Tile size | 8×8 default (H/8) | 16×16 or 32×32 | Finer grids: +0.3–0.4pp mAP but 2–7× higher overhead |
  | Bit range | [2, 8] | [3, 5] or [4, 6] | Wider range: better compression but higher kernel complexity |
  | Deployment mode | Static (precomputed) | Online (per-frame) | Static: 151 FPS; Online: 127 FPS |
  | Complexity metrics | 5 metrics + interactions | Single metric (entropy) | Multi-metric: +2.2pp vs entropy-only |

- **Failure signatures:**
  - **Bit budget explosion:** Avg bits > 5.0 indicates λ₁(t) annealing too slow; increase initial λ₁ or check MLP monotonicity constraint
  - **Tile boundary artifacts:** Visible quantization discontinuities suggest smoothness loss weight λ₂ too low; verify bilateral filter on C map (σₛ=2, σᵣ=0.1)
  - **Convergence stall at ~82% mAP:** Curriculum not progressing; check τ_t schedule reaching 1.0 by epoch 20
  - **Small object AP collapse:** APS significantly lower than baseline; verify tile size ≤ H/16 for dense object scenes
  - **Cache miss rate > 30% (video):** Hash function too restrictive; increase LSH bins or check feature similarity threshold

- **First 3 experiments:**
  1. **Complexity-sensitivity correlation validation:** Compute 5 morphological metrics on 500 calibration images, quantize at uniform 4-bit, measure per-tile mAP drop. Verify ρ ≥ 0.6 between C and degradation. If ρ < 0.5, metric weights need domain-specific retuning.
  2. **Tile size sweep:** Compare 8×8, 16×16, 32×32 grids on validation set measuring mAP@0.5, FPS, and memory. Target: identify sweet spot within 1pp of best mAP while maintaining ≥100 FPS.
  3. **Curriculum ablation:** Train three variants—(a) full curriculum, (b) no curriculum (all samples from epoch 0), (c) temperature-only (no complexity filtering). Measure iterations to 80% final mAP and final accuracy gap. Expect: (a) 20k iterations, 85.6% mAP; (b) 50k iterations, 82.4% mAP; (c) intermediate.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the empirical relationship between morphological complexity and quantization sensitivity be formally grounded in information theory or learning theory?
- **Basis in paper:** [explicit] "The complexity–sensitivity relationship is empirically established rather than theoretically grounded; formal justification remains an open problem" (Section I-C). The authors note that rate–distortion theory applies to signal compression, not network parameter quantization.
- **Why unresolved:** The paper provides empirical correlation (ρ = 0.73) and heuristic hypotheses (Eq. 6-7), but lacks formal proof connecting input morphology to network sensitivity.
- **What evidence would resolve it:** A theoretical framework deriving the complexity–sensitivity relationship from first principles (e.g., generalization bounds, Fisher information), with predictive power across architectures without empirical fitting.

### Open Question 2
- **Question:** Does spatial mixed-precision quantization transfer effectively to Transformer-based object detectors with attention mechanisms?
- **Basis in paper:** [explicit] "Validation is limited to CNN-based YOLO detectors; extensions to Transformer-based architectures (e.g., DETR, RT-DETR) require additional investigation" (Section I-C, reiterated in Conclusion).
- **Why unresolved:** Transformer attention maps have different spatial structure than CNN feature maps; whether morphological complexity predicts attention-head quantization sensitivity is unknown.
- **What evidence would resolve it:** Cross-architecture experiments on DETR/RT-DETR showing comparable mAP gains and complexity–sensitivity correlations, or identification of architecture-specific adaptations required.

### Open Question 3
- **Question:** Can complexity metrics be learned end-to-end rather than hand-crafted, and would this improve quantization performance?
- **Basis in paper:** [explicit] Future directions include "learning complexity metrics end-to-end rather than relying on hand-crafted descriptors" (Section VI-C).
- **Why unresolved:** The current five metrics (fractal dimension, texture entropy, gradient variance, edge density, contour complexity) are designed based on domain knowledge; whether learned representations could capture additional relevant signal properties is unexplored.
- **What evidence would resolve it:** Ablation comparing learned complexity encoders against hand-crafted metrics, demonstrating improved correlation with quantization sensitivity or higher mAP under equivalent bit budgets.

## Limitations
- **Domain dependency:** Performance gains are highly dependent on input domain having significant morphological complexity variation, with modest cross-dataset improvements (+2.9% on COCO, +2.3% on VOC) compared to primary dataset results.
- **Calibration overhead:** The 0.3ms inference overhead assumes calibration phase feasibility; single-image deployment requires full online analysis (1.8ms), reducing FPS from 151 to ~127.
- **Curriculum evidence:** The curriculum learning contribution (2.5× faster convergence) lacks strong foundational support from related literature, though empirical ablation shows 2.5pp mAP drop without curriculum.

## Confidence

- **High Confidence:** The correlation between morphological complexity and quantization sensitivity (ρ = 0.73) is empirically validated at tile level with statistical significance (p < 0.001). The calibration-time analysis design with inference-time lookup is technically sound and performance claims (151 FPS, 0.3ms overhead) are internally consistent.
- **Medium Confidence:** The morphological complexity aggregation (5 metrics + MLP) and its effectiveness across domains is supported by cross-dataset evaluation but may require domain-specific metric weighting for optimal performance.
- **Low Confidence:** The curriculum learning mechanism's contribution (2.5× faster convergence) lacks strong foundational support from related literature, though the ablation study (2.5pp mAP drop without curriculum) provides empirical validation.

## Next Checks

1. **Domain Transfer Test:** Apply MCAQ-YOLO to a dataset with uniformly low complexity (e.g., synthetic/simple backgrounds) to verify performance degrades gracefully and doesn't exceed uniform quantization in mAP.

2. **Online Calibration Overhead:** Implement the full online morphological analysis pipeline (1.8ms) and measure actual FPS impact on video sequences with varying temporal coherence to validate cache effectiveness claims (85% hit rate).

3. **Curriculum Ablation in High-Complexity Domains:** Train MCAQ-YOLO on a dataset with uniformly high morphological complexity (no "easy" samples) to determine if the curriculum schedule adapts or if convergence degrades without low-complexity warm-up samples.