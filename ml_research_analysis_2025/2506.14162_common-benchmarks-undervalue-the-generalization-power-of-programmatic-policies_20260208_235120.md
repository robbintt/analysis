---
ver: rpa2
title: Common Benchmarks Undervalue the Generalization Power of Programmatic Policies
arxiv_id: '2506.14162'
source_url: https://arxiv.org/abs/2506.14162
tags:
- policies
- agent
- neural
- programmatic
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper challenges the widely held belief that programmatic policies
  outperform neural policies in out-of-distribution (OOD) generalization. By revisiting
  experiments from four recent studies across domains like TORCS, KAREL, and PARKING,
  the authors show that neural policies can achieve comparable OOD generalization
  when using simpler architectures, sparse observations, and safer reward functions.
---

# Common Benchmarks Undervalue the Generalization Power of Programmatic Policies

## Quick Facts
- **arXiv ID:** 2506.14162
- **Source URL:** https://arxiv.org/abs/2506.14162
- **Reference count:** 25
- **Primary result:** Neural policies can match programmatic policies' out-of-distribution generalization when using sparse observations, simpler architectures, and safer reward functions.

## Executive Summary
This paper challenges the prevailing view that programmatic policies inherently outperform neural policies in out-of-distribution (OOD) generalization. Through revisiting experiments from four recent studies across TORCS, KAREL, and PARKING domains, the authors demonstrate that neural policies can achieve comparable OOD generalization when experimental design choices are optimized. Key modifications include using sparse observations, simpler architectures (Fully Connected Networks), and safer reward functions that penalize high speeds. However, the authors note that PARKING remains difficult for both representations, and they propose creating new benchmarks requiring algorithmic constructs like stacks to better highlight programmatic policies' strengths.

## Method Summary
The paper revisits four previous studies comparing neural and programmatic policies on OOD generalization tasks. For TORCS, DDPG with a cautious reward function (β=0.5) was used to penalize high speeds. For KAREL, PPO with a simple Fully Connected network was implemented, augmenting observations with the previous action to handle partial observability. For PARKING, DQN with a shaped reward was used. The key modification was changing the observation space from dense (full grid or raw pixels) to sparse (local sensors only), matching the observation type used by programmatic policies. The authors evaluated generalization by testing policies on environments larger or different from training conditions.

## Key Results
- In KAREL, PPO with sparse observations and last action augmentation achieved perfect generalization (1.00 return) matching programmatic policies on 100x100 grids.
- In TORCS, reducing the velocity reward multiplier from β=1.0 to β=0.5 enabled neural policies to complete OOD tracks where they previously crashed.
- In the proposed SparseMaze benchmark, neural policies with GRU failed to learn (Return 0.09), while programmatic synthesis (FunSearch) succeeded (Return 1.00), demonstrating domains where programmatic approaches remain necessary.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Neural policies utilizing sparse observations and simpler architectures achieve significant OOD generalization improvements, potentially matching programmatic policies.
- **Mechanism:** Reducing observation space to task-relevant features prevents overfitting to environmental distractions, while simpler architectures (FCNs) are easier to optimize effectively with reduced features compared to complex architectures like ConvNets or LSTMs.
- **Core assumption:** Task logic can be expressed reactively or with minimal memory (specifically the last action) rather than requiring complex sequential memory structures.
- **Evidence anchors:** KAREL experiments show PPO with sparse obs + FCN achieving perfect generalization (1.00) where ConvNet and LSTM versions failed (0.00) on larger grids.
- **Break condition:** Fails when tasks require processing global spatial relationships (where ConvNets excel) or complex temporal dependencies beyond last action capture.

### Mechanism 2
- **Claim:** Modifying reward functions to encourage conservative behaviors improves transfer to novel environments by decoupling policies from specific training geometry.
- **Mechanism:** Standard reward functions that maximize speed exploit specific training environment features; penalizing high speeds teaches more robust control strategies that don't rely on the absence of sharp turns.
- **Core assumption:** OOD failure is primarily driven by dynamic constraints (inability to make sharp turns at high speed) rather than logical or perceptual failures.
- **Evidence anchors:** TORCS experiments show reducing velocity reward multiplier (β) from 1.0 to 0.5 allowed DRL agents to complete OOD tracks where they previously crashed.
- **Break condition:** Does not aid generalization if OOD failure is caused by novel obstacles, rules, or observation spaces not penalized by "safe" reward.

### Mechanism 3
- **Claim:** Tasks requiring algorithmic constructs like stacks are solved by programmatic synthesis but fail for standard neural architectures.
- **Mechanism:** Pathfinding in sparse mazes requires systematic state tracking (BFS using a queue) to avoid infinite loops; neural policies learn reactive heuristics that fail in sparse environments while programmatic synthesis generates required algorithmic code.
- **Core assumption:** Problem requires strict algorithmic correctness rather than maximizing shaped reward through local heuristics.
- **Evidence anchors:** SparseMaze benchmark shows PPO with GRU failed training (Return 0.09) while FunSearch synthesized BFS policy (Return 1.00).
- **Break condition:** If neural network is explicitly augmented with external memory or algorithmic inductive biases, it may overcome this limitation.

## Foundational Learning

- **Concept:** **Markov Decision Processes (MDPs)**
  - **Why needed here:** Paper defines sequential decision-making using MDPs ($S, A, p, r, \mu, \gamma$); understanding transition model $p$ vs policy $\pi$ is necessary to grasp what is being "generalized."
  - **Quick check question:** In TORCS, does changing the "track" change the MDP's transition function $p$, reward function $r$, or just the state sequence?

- **Concept:** **Context-Free Grammars (CFGs)**
  - **Why needed here:** Programmatic policies are strings generated by CFGs; understanding how grammar constrains policy space explains why programmatic policies are "sparse" or "structured."
  - **Quick check question:** If grammar restricts agent to only observe `frontIsClear`, how does that inherently limit policy complexity compared to neural network taking raw pixel image?

- **Concept:** **Imitation Learning (specifically DAGGER)**
  - **Why needed here:** Baseline programmatic methods (NDPS, PROPEL) use neural "oracle" to teach program; representation gap between neural teacher and programmatic student is key variable in OOD performance.
  - **Quick check question:** Why might neural policy "too optimized" for specific track be poor teacher for programmatic policy needing to generalize to new track?

## Architecture Onboarding

- **Component map:** Sparse observation vector + Last Action -> Fully Connected Network -> PPO/DDPG optimizer -> Shaped reward signal

- **Critical path:**
  1. Select minimal set of task-relevant features matching programmatic DSL
  2. Augment state by concatenating previous action $a_{t-1}$ to observation
  3. Tune reward to penalize environment-specific overfitting (e.g., lower speed incentives)
  4. Train using standard RL algorithm with simple FCN architecture

- **Design tradeoffs:**
  - **ConvNet vs. FCN:** ConvNets process full grids but overfit to spatial patterns; FCNs with sparse obs generalize better but cannot process raw images
  - **LSTM vs. Last-Action:** LSTMs are theoretically more powerful but "hard to train" and often fail to converge; $a_{t-1}$ provides sufficient short-term memory for reactive tasks
  - **Optimality vs. Safety:** Maximizing return leads to specialization (crashing on OOD tracks); intentionally capping return preserves robustness

- **Failure signatures:**
  - **Observation Overfit:** Perfect behavior on 12x12 grid but random on 100x100; fix: reduce observation density or use $a_{t-1}$
  - **Dynamics Overfit:** Perfect on training track but crashes on first turn of test track; fix: increase penalty for high speed/aggressive maneuvers
  - **Algorithmic Failure:** Loops infinitely or moves randomly in open spaces despite training; diagnosis: task requires explicit algorithmic memory (Stack/Queue)

- **First 3 experiments:**
  1. **KAREL Generalization Test:** Train PPO with FCN on 12x12 grid using local sensors + $a_{t-1}$; evaluate on 100x100 to verify 1.00 return
  2. **TORCS Speed Penalty:** Train DDPG on G-Track-1 with β=1.0 vs β=0.5; measure lap time vs completion rate on G-Track-2
  3. **Sparse Maze Baseline:** Train PPO with GRU on proposed SparseMaze; verify failure (Return ~0.09) confirming need for programmatic constructs

## Open Questions the Paper Calls Out
- What specific benchmark environments effectively force agents to utilize complex data structures (e.g., stacks, queues) to achieve OOD generalization?
- How can the inductive biases of programmatic and neural representations be effectively combined to create hybrid models?
- What architectural or training modifications are necessary for neural networks to reliably generalize in continuous control tasks involving repetitive behaviors like PARKING?

## Limitations
- The proposed mechanisms (sparse observations, safer rewards) are effective in domains with simple, reactive task logic but their applicability to complex, real-world environments remains untested.
- PARKING domain proved resistant to both neural and programmatic approaches despite modifications, suggesting fundamental limitations exist.
- The paper focuses on comparing representations in isolation rather than integrating them, leaving hybrid approaches unexplored.

## Confidence
- **High:** Experimental results showing neural policies' improved OOD generalization with sparse observations + last-action augmentation in KAREL (1.00 return matching programmatic policies)
- **Medium:** Claim that TORCS generalization gaps can be eliminated through reward shaping (results show improvement but not complete elimination)
- **Medium:** Assertion that neural policies' OOD failures stem primarily from experimental design rather than inherent limitations (some domains like PARKING remained challenging)

## Next Checks
1. Test the proposed SparseMaze benchmark with both neural policies (with augmented memory) and programmatic synthesis to verify if neural approaches with enhanced architectures can match programmatic performance.
2. Evaluate whether the sparse observation + last-action mechanism generalizes to domains requiring more complex temporal reasoning beyond simple reactive tasks.
3. Investigate PARKING's resistance to both neural and programmatic approaches to determine if this domain requires fundamentally different solution strategies or reveals inherent limitations of both representations.