---
ver: rpa2
title: 'COM-BOM: Bayesian Exemplar Search for Efficiently Exploring the Accuracy-Calibration
  Pareto Frontier'
arxiv_id: '2510.01178'
source_url: https://arxiv.org/abs/2510.01178
tags:
- accuracy
- exemplar
- optimization
- exemplars
- search
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces COM-BOM, a Bayesian optimization algorithm
  for exemplar selection in in-context learning that jointly optimizes predictive
  accuracy and calibration error. Unlike prior work that focuses solely on accuracy,
  COM-BOM treats exemplar selection as a multi-objective problem, seeking Pareto-optimal
  solutions that balance correctness with reliable confidence estimates.
---

# COM-BOM: Bayesian Exemplar Search for Efficiently Exploring the Accuracy-Calibration Pareto Frontier

## Quick Facts
- arXiv ID: 2510.01178
- Source URL: https://arxiv.org/abs/2510.01178
- Reference count: 19
- COM-BOM optimizes both predictive accuracy and calibration error for in-context learning

## Executive Summary
This paper introduces COM-BOM, a Bayesian optimization algorithm that treats exemplar selection in in-context learning as a multi-objective problem. Unlike previous approaches that focus solely on maximizing predictive accuracy, COM-BOM jointly optimizes for both accuracy and calibration error, seeking Pareto-optimal solutions that balance correctness with reliable confidence estimates. The method uses Gaussian process surrogates with exponentiated Hamming kernels to model the relationship between exemplar selection and performance metrics.

The algorithm employs hypervolume-based acquisition functions to efficiently explore the combinatorial search space of possible exemplar combinations. Through experiments on MMLU-Pro using Qwen3-8B and LLaMA-3.3-70B models, COM-BOM demonstrates superior accuracy-calibration trade-offs compared to baselines including random search, genetic algorithms, and retrieval-based methods. The approach requires fewer LLM API calls while achieving better overall performance across both metrics.

## Method Summary
COM-BOM addresses the challenge of exemplar selection in in-context learning by formulating it as a multi-objective optimization problem. The method uses Gaussian process surrogates with exponentiated Hamming kernels to model both accuracy and calibration error as functions of exemplar selection. An acquisition function based on hypervolume improvement guides the search toward Pareto-optimal solutions. The algorithm efficiently explores the combinatorial space of exemplar combinations, balancing exploration and exploitation to find exemplars that achieve good trade-offs between predictive accuracy and well-calibrated confidence estimates. This approach differs from traditional single-objective methods by explicitly considering calibration quality alongside accuracy.

## Key Results
- COM-BOM outperforms random search, genetic algorithms, and retrieval-based methods on accuracy-calibration trade-offs
- Achieves better performance with fewer LLM API calls compared to baseline approaches
- Successfully identifies Pareto-optimal exemplar sets that balance correctness and reliable confidence estimates
- Demonstrates effectiveness on MMLU-Pro benchmark with both Qwen3-8B and LLaMA-3.3-70B models

## Why This Works (Mechanism)
COM-BOM works by treating exemplar selection as a multi-objective optimization problem rather than a single-objective one. The Gaussian process surrogate with exponentiated Hamming kernel effectively captures the complex relationship between exemplar selection and both accuracy and calibration metrics. The hypervolume-based acquisition function naturally handles the multi-objective nature of the problem, guiding the search toward regions that offer improvements across both metrics rather than optimizing for one at the expense of the other. This balanced approach ensures that selected exemplars not only achieve high accuracy but also produce well-calibrated confidence estimates, which is crucial for trustworthy LLM deployment.

## Foundational Learning

**Gaussian Process Regression**
- Why needed: Provides probabilistic modeling of the accuracy-calibration relationship across exemplar selections
- Quick check: Verify GP hyperparameters through cross-validation on a small subset of exemplars

**Exponentiated Hamming Kernel**
- Why needed: Captures similarity between exemplar sets in the combinatorial search space
- Quick check: Test kernel performance on synthetic exemplar selection problems

**Multi-objective Optimization**
- Why needed: Enables simultaneous optimization of accuracy and calibration error
- Quick check: Compare hypervolume-based acquisition against single-objective variants

**Hypervolume Improvement**
- Why needed: Quantifies progress toward Pareto-optimal solutions in objective space
- Quick check: Validate hypervolume calculations on known Pareto fronts

**Pareto Frontier Analysis**
- Why needed: Characterizes the optimal trade-off between accuracy and calibration
- Quick check: Verify Pareto optimality through dominance testing

## Architecture Onboarding

Component map: Data → Gaussian Process Surrogate → Acquisition Function → Selection → LLM Evaluation → Update Surrogate

Critical path: Exemplar pool initialization → GP surrogate training → Hypervolume acquisition → Exemplar selection → LLM evaluation → Surrogate update → Repeat until convergence

Design tradeoffs: Accuracy-calibration balance vs. computational overhead of Bayesian optimization

Failure signatures: Poor convergence when exemplar pool is too small or too diverse; acquisition function collapse to exploitation-only mode

First experiments: 1) Test on synthetic binary classification with known Pareto front, 2) Compare acquisition functions on small exemplar sets, 3) Validate calibration metrics on calibrated vs. miscalibrated models

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation limited to MMLU-Pro with only two specific model configurations (Qwen3-8B and LLaMA-3.3-70B)
- Computational overhead of Gaussian process surrogates may become prohibitive for larger exemplar pools
- Limited consideration of exemplar diversity factors beyond accuracy-calibration trade-off
- Potential sensitivity of hypervolume-based acquisition function to hyperparameter choices

## Confidence
- High Confidence: Multi-objective formulation of exemplar selection as Pareto optimization is well-justified
- Medium Confidence: Empirical improvements are convincing for specific experimental setup but need broader validation
- Low Confidence: Computational efficiency claims require more rigorous analysis of cost-benefit trade-offs

## Next Checks
1. Generalize evaluation to at least five additional LLM architectures and three distinct task domains
2. Analyze scalability by varying exemplar pool sizes from 50 to 1000 examples
3. Conduct ablation studies removing components of the acquisition function to isolate individual contributions