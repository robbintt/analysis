---
ver: rpa2
title: Feasibility with Language Models for Open-World Compositional Zero-Shot Learning
arxiv_id: '2505.11181'
source_url: https://arxiv.org/abs/2505.11181
tags:
- feasibility
- classes
- pairs
- feasible
- seen
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes Feasibility with Language Model (FLM), a method
  that uses Large Language Models (LLMs) to determine the feasibility of state-object
  pairs in Open-World Compositional Zero-Shot Learning (OW-CZSL). FLM queries LLMs
  with prompts containing related seen classes and retrieves the logit for the positive
  answer as a feasibility score.
---

# Feasibility with Language Models for Open-World Compositional Zero-Shot Learning

## Quick Facts
- arXiv ID: 2505.11181
- Source URL: https://arxiv.org/abs/2505.11181
- Authors: Jae Myung Kim; Stephan Alaniz; Cordelia Schmid; Zeynep Akata
- Reference count: 40
- Primary result: FLM consistently outperforms previous methods on MIT-States, UT-Zappos, and C-GQA benchmarks for open-world compositional zero-shot learning

## Executive Summary
This paper introduces Feasibility with Language Model (FLM), a method that leverages Large Language Models (LLMs) to determine the feasibility of state-object pairs in Open-World Compositional Zero-Shot Learning (OW-CZSL). The key insight is that LLMs can be guided with in-context examples from the training set to correctly assess whether rare or unusual compositions are valid within the dataset's semantic conventions. By querying LLMs with prompts containing related seen classes and retrieving the logit for the positive answer as a feasibility score, FLM improves the accuracy of filtering out implausible compositions. The approach consistently outperforms previous methods across three benchmarks when integrated with Vision-Language Models like CLIP, CoOp, and CSP.

## Method Summary
FLM uses LLMs (Vicuna-13B recommended) to assess feasibility of state-object pairs through in-context learning. For each query pair, the method retrieves related seen pairs sharing either the state or object, constructs a prompt with these examples, and extracts the logit for "Yes" token as a feasibility score. A threshold τ filters out infeasible pairs from the candidate label space before classification by a Vision-Language Model. The method requires local LLM access for logit extraction (Vicuna outperforms API-only ChatGPT), uses list format over QA format for prompts, and tunes τ via validation accuracy. The approach integrates with VLMs including CLIP, CoOp, and CSP, with CSP trained using standard hyperparameters (batch size 64, LR 5e-4, weight decay 1e-5, 20 epochs, attribute dropout 0.3).

## Key Results
- FLM achieves higher harmonic mean and AUC scores across all three datasets (MIT-States, UT-Zappos, C-GQA) compared to previous methods
- Vicuna with logit access outperforms ChatGPT with binary output, though authors speculate GPT-4 with logits would perform even better
- Using related seen pairs as guidance is critical - random pairs degrade performance from 17.4% to 14.9% harmonic mean on MIT-States
- List format prompts outperform QA format, which biases toward "Yes" responses when only positive examples are available

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** In-context guidance with dataset-specific examples enables LLMs to correctly assess feasibility of rare but valid compositions.
- **Mechanism:** By providing related seen pairs (sharing state or object with query) in the prompt, the LLM learns the dataset's semantic conventions (e.g., "dark" can describe lighting conditions, not just brightness of objects). This overrides the LLM's default priors which may reject rare combinations like "dark fire."
- **Core assumption:** The training set contains semantically representative pairs that define what "feasible" means in the dataset context.
- **Evidence anchors:**
  - [abstract]: "To mitigate potential misguidance of the LLM given that many of the state-object compositions are rare or completely infeasible, we observe that the in-context learning ability of LLMs is essential."
  - [section 1]: "To teach the LLM about the relevant context for image classification, we can inform the LLM of semantically similar and feasible compositions from the training set, such as 'dark lightning'. As a result, the LLM can correctly infer in-context that the state 'dark' can also be associated with 'fire'."
  - [corpus]: Neighbor papers on compositional generalization (e.g., "What Drives Compositional Generalization in Visual Generative Models?") suggest compositional reasoning benefits from relational understanding between primitives—consistent with FLM's use of related pairs.
- **Break condition:** If training pairs are biased (e.g., only texture states, no color states), FLM may misclassify queries with unrepresented semantics (see Appendix A).

### Mechanism 2
- **Claim:** Access to LLM output logits for the "Yes" token provides more informative feasibility scores than binary responses.
- **Mechanism:** The logit value reflects the LLM's confidence in feasibility, enabling fine-grained threshold tuning per dataset. Binary API responses collapse this signal, reducing discrimination between marginally feasible and clearly infeasible pairs.
- **Core assumption:** Logit values monotonically correlate with ground-truth feasibility.
- **Evidence anchors:**
  - [section 3.2]: "The probability or the logit of the word 'Yes' in the next-token output distribution indicates the LLM's confidence in the feasibility... More formally, our feasibility score function is g(s, o) = log pLLM(t = 'Yes'|f(s, o; c))"
  - [section 4.3, Figure 2]: "both Vicuna and LLaMa-2 show lower performance with a binary answer compared to using logits... ChatGPT with a binary answer consistently outperforms Vicuna with a binary answer and oftentimes even Vicuna using logits."
  - [corpus]: No direct corpus evidence on logit vs. binary comparison for CZSL—this appears to be a novel contribution of FLM.
- **Break condition:** For proprietary LLMs without logit access, performance may degrade; authors speculate logit access to advanced models (e.g., GPT-4) would outperform current results.

### Mechanism 3
- **Claim:** Pruning infeasible pairs from the candidate label space reduces false positives in open-world classification.
- **Mechanism:** OW-CZSL considers all possible state-object combinations (e.g., 278,362 pairs in C-GQA). Removing infeasible pairs via threshold τ concentrates VLM probability mass on plausible classes, improving unseen-class accuracy.
- **Core assumption:** The feasibility function has higher precision than the VLM's raw classification for distinguishing infeasible pairs.
- **Evidence anchors:**
  - [section 3.1]: "classes with scores below a threshold τ are deemed infeasible and consequently removed from the test label space"
  - [section 4.2, Table 1]: FLM with CSP achieves AUC of 5.76% on MIT-States vs. 5.12% for GloVe, demonstrating improved feasibility filtering.
  - [corpus]: KGSP (neighbor paper) similarly uses external knowledge (ConceptNet) for feasibility, but FLM outperforms it across benchmarks (Table 1), suggesting LLM-based feasibility is more effective.
- **Break condition:** Over-aggressive thresholding may remove ground-truth classes; under-aggressive thresholding leaves too many false candidates.

## Foundational Learning

- **Concept: Open-World vs. Closed-World Zero-Shot Learning**
  - **Why needed here:** In closed-world CZSL, test classes are known in advance. In open-world, all combinations are candidates, making feasibility filtering critical.
  - **Quick check question:** If you have 50 states and 100 objects, how many candidate classes exist in open-world CZSL?

- **Concept: In-Context Learning in LLMs**
  - **Why needed here:** FLM relies on LLMs adapting to the dataset's feasibility semantics via few-shot examples, without gradient updates.
  - **Quick check question:** What happens if you provide no guidance pairs in the FLM prompt?

- **Concept: Vision-Language Model (VLM) Calibration**
  - **Why needed here:** VLMs trained on seen classes are biased toward predicting seen compositions; feasibility filtering mitigates this by reducing the candidate space.
  - **Quick check question:** Why does the evaluation protocol subtract a calibration bias from seen-class outputs?

## Architecture Onboarding

- **Component map:** Training Set (Y_seen) -> LLM (Vicuna-13B) -> Threshold τ -> VLM (CLIP/CoOp/CSP)

- **Critical path:**
  1. For each query pair (s, o), retrieve related seen pairs Y_pos = {(s_i, o_i) | s_i = s OR o_i = o}
  2. Construct prompt with persona, instruction, guidance (list of Y_pos), and query
  3. Extract logit for "Yes" token -> feasibility score g(s, o)
  4. Apply threshold τ (tuned on validation set) to determine feasibility
  5. Pass feasible pairs to VLM for final classification

- **Design tradeoffs:**
  - **LLM choice:** Vicuna provides logit access; ChatGPT has stronger reasoning but API-only (binary output). Authors find Vicuna with logits competitive.
  - **Number of guidance pairs:** More pairs improve performance up to ~50 for MIT-States; beyond this, gains plateau. Random pairs perform worse than related pairs (Table 3).
  - **Prompt format:** List format outperforms QA format (which biases toward "Yes" since only positive examples are available).

- **Failure signatures:**
  - **High false negative rate:** LLM rejects feasible classes (e.g., canonical prompt without guidance: "dark fire" -> "No")
  - **High false positive rate:** Overly permissive threshold retains infeasible classes, confusing VLM
  - **Biased training set:** If Y_seen lacks diversity (e.g., no color states), FLM may misclassify queries with unrepresented semantics

- **First 3 experiments:**
  1. **Sanity check:** Run FLM with canonical prompt (no guidance) on MIT-States validation set. Expect harmonic mean ~15.4% (Table 3, "Canonical"). Compare with in-context prompt to confirm guidance effect.
  2. **Threshold sweep:** Vary τ on validation set and plot seen/unseen accuracy curve. Identify optimal τ that maximizes harmonic mean (should match Table 1 values for each dataset).
  3. **LLM comparison:** Query both Vicuna (logit) and ChatGPT (binary) on UT-Zappos. Compare feasibility accuracy (Table 2 metrics) to understand logit vs. binary tradeoff.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How much would advanced proprietary LLMs (e.g., ChatGPT, GPT-4) with logit access improve OW-CZSL performance compared to open-source models like Vicuna?
- **Basis in paper:** [explicit] The authors state: "We speculate that ChatGPT with logit access would likely surpass Vicuna with logit considerably, implying that more advanced LLMs with logit access would yield improved feasibility scores."
- **Why unresolved:** Current API constraints for proprietary models only expose binary outputs, not probability distributions or logits.
- **What evidence would resolve it:** API modifications exposing logit access, or experiments with proprietary models deployed locally.

### Open Question 2
- **Question:** How can FLM's dependence on biased seen classes be mitigated when the training set covers only partial semantic domains?
- **Basis in paper:** [explicit] In the limitations: "If the seen classes represent only part of the semantics, e.g. only the texture-related attributes... while age-related states exist in the test set, our method may struggle to predict the true feasible pairs accurately."
- **Why unresolved:** FLM relies entirely on seen pairs for in-context guidance; no mechanism handles semantic gaps between training and test distributions.
- **What evidence would resolve it:** Methods that augment in-context examples with external knowledge or debiasing techniques evaluated on controlled semantic splits.

### Open Question 3
- **Question:** Can the threshold τ be set adaptively rather than requiring validation-based grid search?
- **Basis in paper:** [inferred] The paper tunes τ via grid search to maximize unseen validation accuracy, but performance is sensitive to this hyperparameter—different thresholds directly control which classes remain in the candidate label space.
- **Why unresolved:** No theoretical or data-driven method proposed for automatic threshold selection; current approach depends on validation data availability.
- **What evidence would resolve it:** An adaptive thresholding method achieving comparable or superior performance without validation-based tuning.

## Limitations
- **Semantic bias:** FLM's performance depends on training pairs being semantically representative; biased training sets (e.g., only texture states) can lead to misclassification of queries with unrepresented semantics.
- **LLM access constraints:** The method requires local LLM access for logit extraction, limiting use of more powerful proprietary models that only provide binary API outputs.
- **Hyperparameter sensitivity:** The threshold τ must be carefully tuned per dataset, with aggressive pruning potentially removing ground-truth classes and under-aggressive pruning leaving too many false candidates.

## Confidence
- **High confidence:** The general mechanism of using in-context learning with related seen pairs to improve feasibility predictions is well-supported by empirical results across all three benchmarks.
- **Medium confidence:** The claim that logit values provide more informative feasibility scores than binary responses is supported by ablation studies, but the comparison is somewhat confounded by using different LLMs.
- **Low confidence:** The assumption that training pairs are semantically representative enough to define dataset-specific feasibility is reasonable but not empirically validated.

## Next Checks
1. **Bias sensitivity test:** Intentionally create a biased training set (e.g., only texture states, no color states) and evaluate whether FLM misclassifies queries with unrepresented semantics, as suggested in Appendix A.

2. **Logit vs binary comparison:** Run both Vicuna (with logit access) and ChatGPT (binary API) on the same queries using identical prompts to isolate the effect of output format from model capabilities.

3. **Threshold sensitivity analysis:** Systematically vary τ across a wider range than reported and measure the precision-recall tradeoff to identify whether the method is over-pruning or under-pruning feasible classes in different datasets.