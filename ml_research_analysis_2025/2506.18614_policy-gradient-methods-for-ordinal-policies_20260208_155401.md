---
ver: rpa2
title: Policy gradient methods for ordinal policies
arxiv_id: '2506.18614'
source_url: https://arxiv.org/abs/2506.18614
tags:
- actions
- politique
- nous
- pour
- ordinale
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a policy gradient method for ordinal policies
  in reinforcement learning, addressing the limitation of standard softmax parametrization
  that fails to capture order relationships between discrete actions. The proposed
  method uses ordinal regression models adapted to the RL setting, where actions are
  ordered and a latent score function combined with ordered thresholds determines
  action probabilities.
---

# Policy gradient methods for ordinal policies

## Quick Facts
- arXiv ID: 2506.18614
- Source URL: https://arxiv.org/abs/2506.18614
- Reference count: 0
- One-line primary result: Ordinal policy parametrization improves reinforcement learning performance for naturally ordered actions while providing competitive results for discretized continuous control

## Executive Summary
This paper introduces ordinal policies for reinforcement learning, addressing the limitation of standard softmax parametrization that fails to capture order relationships between discrete actions. The method uses ordinal regression models adapted to RL, where actions are ordered and a latent score function combined with ordered thresholds determines action probabilities. The approach was evaluated in two contexts: a simulated eyeglass tinting control problem with four ordered tint levels, and continuous control tasks where action spaces were discretized into ordered categories.

The results demonstrate that ordinal policies outperform standard softmax policies in convergence speed and solution quality for naturally ordered actions, while achieving competitive or slightly better performance compared to continuous action policies across multiple benchmark environments. The ordinal policy formulation ensures structural robustness by maintaining action hierarchies during updates, leading to more stable learning. The method shows that incorporating ordinal structure through appropriate parametrization can improve RL performance in applications with naturally ordered actions, while also providing a practical approach for discretizing continuous action spaces without significant performance loss.

## Method Summary
The proposed method uses an ordinal policy parametrization where action probabilities are computed through cumulative sigmoid differences. The policy has two sets of parameters: a score function g_ω(s) that produces a scalar value from the state, and K-1 ordered thresholds τ₁ < τ₂ < ... < τ_{K-1}. The probability of taking action a is P(A=a|s) = σ(τ_a - g_ω(s)) - σ(τ_{a-1} - g_ω(s)), where σ is the sigmoid function. This formulation ensures that action probabilities maintain their natural ordering during learning. The method was integrated with standard policy gradient algorithms including REINFORCE, Natural Policy Gradient, TRPO, and PPO. For continuous control tasks, the continuous action space was discretized into K ordered bins per dimension, with each dimension treated independently using its own ordinal policy.

## Key Results
- In eyeglass tinting control with 4 ordered actions, ordinal policies converged faster and to better solutions than multinomial softmax policies
- For continuous control tasks discretized into 17 classes per dimension, ordinal PPO achieved competitive or slightly better performance than continuous PPO on Ant, HalfCheetah, and Humanoid environments
- The ordinal policy formulation maintained action hierarchies during updates, leading to more stable learning compared to standard approaches

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Ordinal parametrization enforces action hierarchy preservation during gradient updates.
- Mechanism: The policy uses a latent score function g_ω(s) and ordered thresholds τ₁ < τ₂ < ... < τ_{K-1}. Action probabilities are computed as π(a|s) = σ(τ_a - g_ω(s)) - σ(τ_{a-1} - g_ω(s)), where σ is the sigmoid function. This cumulative formulation structurally guarantees that adjacent actions remain ordered regardless of parameter updates.
- Core assumption: Actions have a natural total ordering that should be preserved throughout learning.
- Evidence anchors:
  - [abstract] "maintaining action hierarchies during updates, leading to more stable learning"
  - [Section 3] Defines the parametrization with ordered thresholds and cumulative probability structure
  - [corpus] Weak direct corpus support for this specific mechanism; related work on alternative parametrizations (MAD) addresses stability but through different means
- Break condition: If actions lack meaningful ordinal relationships, the inductive bias becomes a constraint rather than an aid.

### Mechanism 2
- Claim: Reduced parameter space dimensionality improves sample efficiency for ordered actions.
- Mechanism: For K actions, softmax requires K independent logits per state, while ordinal policy uses 1 latent score plus K-1 thresholds. The score function g_ω(s) produces a single value, shifting the entire probability distribution along the action spectrum rather than independently adjusting each action's probability.
- Core assumption: The optimal policy can be represented by a monotonic relationship between state-derived score and action preference.
- Evidence anchors:
  - [Section 3] "la fonction g_ω est une forme de score, plus elle est élevée plus la politique choisira actions élevées"
  - [Section 4 Results] Ordinal policies "convergent plus rapidement et vers de meilleures solutions que les politiques multinomiales"
  - [corpus] Categorical Policies paper explores multimodal distributions, suggesting unimodal ordinal assumptions may not hold universally
- Break condition: Tasks requiring non-monotonic action preferences (e.g., "medium" actions preferred for extreme states) may not fit this parametrization well.

### Mechanism 3
- Claim: Discretizing continuous action spaces with ordinal policies preserves competitive performance.
- Mechanism: Continuous action space [m, M] is discretized into K ordered bins. Each dimension is treated independently with its own ordinal policy. The threshold structure provides smooth interpolation between discrete actions, approximating continuous behavior while maintaining computational benefits of discrete policies.
- Core assumption: A fixed discretization granularity (K=17 per dimension in experiments) is sufficient for the task.
- Evidence anchors:
  - [Section 4] "la politique ordinale atteint des performances équivalentes, voire légèrement supérieures, à celles de la politique continue"
  - [Figure 2] Learning curves show ordinal PPO matching or exceeding continuous PPO on Ant, HalfCheetah, Humanoid
  - [corpus] PPO in Fisher-Rao geometry paper examines PPO convergence properties but doesn't address discretization
- Break condition: High-precision tasks may require finer discretization than computationally feasible, or adaptive discretization (noted as future work in Section 5).

## Foundational Learning

- Concept: **Cumulative probability models / Ordinal regression**
  - Why needed here: The paper adapts statistical ordinal regression (cumulative logit models) to RL. Understanding P(A ≤ j|S) formulation is essential to grasp why thresholds create natural ordering.
  - Quick check question: Can you explain why P(A = j) = P(A ≤ j) - P(A ≤ j-1) produces a valid probability distribution?

- Concept: **Policy gradient theorem and compatible function approximation**
  - Why needed here: The method integrates with REINFORCE, NPG, TRPO, and PPO. Understanding how gradients flow through the ordinal parametrization is critical for implementation.
  - Quick check question: What is the gradient of J(θ) with respect to the score function parameters ω versus threshold parameters τ?

- Concept: **Action space discretization trade-offs**
  - Why needed here: The paper's continuous control results rely on discretizing action spaces. Understanding precision vs. computational cost trade-offs informs design choices.
  - Quick check question: For a 6-DOF robot arm discretized into 17 bins per dimension, how many discrete joint configurations exist, and what does this imply for exploration?

## Architecture Onboarding

- Component map:
  State s → Neural network g_ω(s) → Scalar score
                                         ↓
            Learned thresholds [τ₁, ..., τ_{K-1}] → Sigmoid differences
                                         ↓
                              Action probabilities π(a|s)

- Critical path:
  1. Initialize thresholds with equal spacing on real line (e.g., linspace(-2, 2, K-1))
  2. Ensure threshold ordering constraint during optimization (project or parameterize as monotonic)
  3. Compute action probabilities via cumulative sigmoid differences
  4. Integrate with standard PPO/TRPO loss; gradients flow through both ω and τ

- Design tradeoffs:
  - **K (number of discrete actions)**: Higher K → finer control but slower computation and larger threshold vector. Paper uses K=17 empirically.
  - **Threshold initialization**: Uniform spacing vs. task-informed priors. Paper doesn't specify details.
  - **Multi-dimensional actions**: Independent ordinal policies per dimension vs. joint modeling. Paper uses independent per-dimension discretization.

- Failure signatures:
  - Threshold collapse: Two or more thresholds converge to same value → degenerate probability for intervening action. Monitor threshold spacing.
  - Score explosion: g_ω(s) values far outside threshold range → near-deterministic policy, exploration collapse.
  - Ordering violation: If optimization doesn't enforce τ ordering, probability computation fails.

- First 3 experiments:
  1. **Sanity check on simple bandit**: Implement ordinal policy for 5-armed ordered bandit. Verify that increasing reward for higher actions shifts score function upward while thresholds remain stable.
  2. **Eyeglass tinting replication**: 4 ordered actions, simulated user response. Compare convergence curves of ordinal vs. softmax using REINFORCE. Expected: ordinal reaches higher reward with lower variance.
  3. **Single-dimension continuous control**: Apply to HalfCheetah (6 dimensions) with K=17 bins per dimension. Start with one dimension frozen to isolate behavior before scaling to full 6D.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can an adaptive discretization scheme dynamically add action classes during training while preserving learned structure?
- Basis in paper: [explicit] The conclusion states: "the discretization of continuous actions remains an open challenge... new classes could be added dynamically by keeping the learned predictor gω and introducing new thresholds, optimizing the granularity of the policy."
- Why unresolved: The current approach fixes K=17 classes arbitrarily; no mechanism exists for adaptive granularity adjustment during learning.
- What evidence would resolve it: An algorithm that incrementally adds thresholds τ_k during training, with empirical validation showing improved sample efficiency or asymptotic performance compared to fixed discretization.

### Open Question 2
- Question: How does the ordinal policy scale to high-dimensional action spaces where per-dimension discretization leads to combinatorial explosion?
- Basis in paper: [inferred] The paper tests environments with |A| up to 17 (Humanoid), applying discretization separately to each dimension, but does not analyze computational or statistical scaling properties.
- Why unresolved: Per-dimension ordinal policies may become impractical when action dimensionality grows, as joint dependencies across dimensions are not modeled.
- What evidence would resolve it: Experiments on tasks with >20 action dimensions, analyzing training time, memory, and performance degradation relative to continuous baselines.

### Open Question 3
- Question: How sensitive is ordinal policy performance to the initialization and parameterization of ordered thresholds τ?
- Basis in paper: [inferred] The paper specifies that τ₁ < τ₂ < ... < τ_{K-1} must be ordered but provides no analysis of how threshold initialization affects convergence speed or final performance.
- Why unresolved: Poor threshold initialization could lead to slow learning if initial action distributions poorly match the task structure.
- What evidence would resolve it: Ablation studies varying threshold initialization strategies (uniform, data-driven, random) and measuring impact on learning curves across multiple environments.

## Limitations

- The method assumes actions have meaningful ordinal relationships, which may not hold in many RL domains where action relationships are categorical or unstructured
- Empirical validation is limited to two specific contexts (eyeglass tinting with 4 actions and discretized continuous control with fixed K=17 per dimension)
- The paper doesn't address how ordinal policies perform relative to more sophisticated continuous control approaches beyond PPO baseline

## Confidence

- **High confidence**: The ordinal parametrization formulation and its mathematical properties (cumulative probability structure, gradient derivations)
- **Medium confidence**: Empirical results showing ordinal policies outperforming softmax in eyeglass task and matching continuous control performance
- **Low confidence**: Claims about stability/robustness from maintaining action hierarchies during updates - while theoretically sound, the paper provides limited empirical evidence of this specific benefit

## Next Checks

1. Test ordinal policies on tasks where action ordering is ambiguous or non-monotonic to quantify the cost of the inductive bias
2. Compare ordinal PPO against state-of-the-art continuous control methods (e.g., SAC, TD3) beyond the PPO baseline used
3. Evaluate performance sensitivity to K (number of discrete actions) and threshold initialization strategies across multiple problem scales