---
ver: rpa2
title: 'Exploring Agentic Artificial Intelligence Systems: Towards a Typological Framework'
arxiv_id: '2508.00844'
source_url: https://arxiv.org/abs/2508.00844
tags:
- systems
- agentic
- typology
- agency
- agents
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a typological framework to characterize agentic
  AI systems, addressing the challenge of classifying these evolving technologies.
  The framework defines eight dimensions that capture cognitive and environmental
  agency, ranging from non-agentic to general intelligence levels.
---

# Exploring Agentic Artificial Intelligence Systems: Towards a Typological Framework

## Quick Facts
- arXiv ID: 2508.00844
- Source URL: https://arxiv.org/abs/2508.00844
- Reference count: 14
- Primary result: Introduces a typological framework with eight dimensions to systematically classify agentic AI systems from non-agentic tools to general intelligence levels.

## Executive Summary
This paper addresses the challenge of classifying evolving agentic AI systems by developing a typological framework that captures cognitive and environmental agency across eight dimensions. The framework defines four ordinal levels (0-3) ranging from non-agentic to general intelligence, enabling systematic comparison of disparate AI systems. Through a six-phase methodology including literature review, concept matrix extraction, and refinement with empirical examples, the authors created a structured approach to evaluate agentic capabilities. The framework was evaluated using a human-AI hybrid method with 43 real-world AI systems, resulting in a tool that supports both research and practical decision-making for organizations deploying AI technologies.

## Method Summary
The research employed a six-phase approach to develop the typological framework. Phase 1 established construct validity through a comprehensive literature review of agentic AI systems. Phase 2 constructed an ideal type using concept matrix extraction from 12 key papers. Phase 3 consolidated dimensions into eight key characteristics. Phase 4 refined these into ordinal levels (0-3) for each dimension. Phase 5 employed a human-AI hybrid evaluation using OpenAI Deep Research to validate the framework against 43 real-world AI systems. Phase 6 reduced the eight dimensions into two high-level axes (Cognitive and Environmental Agency) to create four constructed types (Simple, Task, Research, and Complex agents).

## Key Results
- Framework defines eight dimensions (Knowledge Scope, Perception, Reasoning, Interactivity, Operation, Contextualization, Self-Improvement, Normative Alignment) with four ordinal levels each
- Reduction to two high-level axes enables classification into four constructed types: Simple, Task, Research, and Complex agents
- Human-AI hybrid evaluation validated the framework's ability to distinguish between different levels of agentic capabilities
- Framework enables systematic comparison of AI systems and supports strategic decision-making for organizations

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Decomposing "agency" into eight observable dimensions enables systematic comparison of disparate AI systems.
- **Mechanism:** Operationalizes agency into granular functional capabilities (e.g., Knowledge Scope, Reasoning, Interactivity) converting subjective assessment into structured capability profiles.
- **Core assumption:** Agency can be accurately captured by observable functional behaviors rather than internal mental states or intent.
- **Evidence anchors:**
  - [Page 3]: "We adopt a more restrictive, instrumentalist viewpoint that does not require mental representations... allowing us to assess the functionality of artificial agents."
  - [Page 11, Table 5]: Defines eight specific dimensions used for classification.
  - [Corpus]: "The STAR-XAI Protocol" emphasizes difficulty verifying agency in black box models, supporting need for structured protocols.

### Mechanism 2
- **Claim:** Structuring dimensions with ordinal levels (0-3) allows organizations to benchmark current systems and quantify capability gaps.
- **Mechanism:** Assigning scores (0=Non-agentic to 3=General Intelligence) for each dimension transforms qualitative features into quantitative profiles for gap analysis.
- **Core assumption:** Agency progresses linearly or predictably within these specific dimensions, and "General Intelligence" is a valid extension of current capabilities.
- **Evidence anchors:**
  - [Abstract]: Mentions "eight dimensions that define their cognitive and environmental agency in an ordinal structure."
  - [Page 9]: Describes refinement phase creating ordinal progression with baseline "non-agentic" level.
  - [Corpus]: "Alignment, Agency and Autonomy in Frontier AI" notes lack of universal definitions for agency, reinforcing value of self-contained ordinal scale.

### Mechanism 3
- **Claim:** Reducing 8-dimension profile into two high-level axes (Cognitive and Environmental Agency) simplifies strategic decision-making and vendor communication.
- **Mechanism:** Grouping technical details into "how the AI thinks" (Cognitive) and "how the AI acts" (Environmental) lowers cognitive load for non-technical stakeholders.
- **Core assumption:** Aggregation of dimensions accurately reflects operational reality without losing critical nuance.
- **Evidence anchors:**
  - [Page 10]: "We recognized a grouping into two overarching dimensions: Cognitive agency... [and] Environmental agency."
  - [Page 12, Table 6]: Demonstrates application by classifying real systems like "Deep Research" and "Copilot" into constructed types.

## Foundational Learning

- **Concept: Typology vs. Taxonomy**
  - **Why needed here:** To understand that this framework is theory-driven and forward-looking (predicting future agent types) rather than just categorization of existing technologies.
  - **Quick check question:** "Does the framework classify only existing tools like ChatGPT, or does it provide a structure for future systems like AGI?" (Answer: It includes "General Intelligence" level 3 for future anticipation).

- **Concept: The "Agenticness" Spectrum**
  - **Why needed here:** To avoid binary thinking (System is/is not an agent) and instead evaluate systems on a continuum of capability.
  - **Quick check question:** "According to the paper, is a standard supervised learning model considered completely non-agentic?" (Answer: No, it may exhibit limited autonomy or interactivity, but sits at the lower end of the spectrum).

- **Concept: Constructed Types (Archetypes)**
  - **Why needed here:** To understand how authors move from 8 complex dimensions to 4 usable categories (Simple, Task, Research, Complex) for practical application.
  - **Quick check question:** "What differentiates a 'Task Agent' from a 'Research Agent' in the reduced framework?" (Answer: Task Agents focus on execution/interaction [Environmental], while Research Agents focus on high reasoning/reflection [Cognitive]).

## Architecture Onboarding

- **Component map:**
  - Core Dimensions (8): Knowledge Scope, Perception, Reasoning, Interactivity, Operation, Contextualization, Self-Improvement, Normative Alignment
  - Superset Axes (2): Cognitive Agency (Reasoning, Knowledge, Improvement, Alignment) vs. Environmental Agency (Perception, Operation, Interactivity, Contextualization)
  - Levels: 0 (Non-agentic), 1 (Basic), 2 (Sophisticated), 3 (General Intelligence)

- **Critical path:**
  1. Define Scope: Select specific AI system or use case to evaluate
  2. Assess Dimensions: Score system (0-3) across all 8 dimensions using definitions in Table 5
  3. Aggregate: Sum scores to position system on Cognitive vs. Environmental matrix
  4. Classify: Identify "Constructed Type" (e.g., Simple vs. Complex Agent) to guide procurement or development

- **Design tradeoffs:**
  - Precision vs. Usability: Full 8-dimension model is precise but complex; reduced 2-axis model is easier for decision-making but loses nuance
  - Theoretical vs. Empirical: Framework is theory-driven (typology), anticipating systems (Level 3) that may not yet exist empirically, trading current accuracy for future relevance

- **Failure signatures:**
  - Flat Profile: System scoring 0 or 1 across all dimensions is likely a passive tool, not an agent; attempting to deploy as "Complex Agent" will fail
  - Misaligned Autonomy: System with high "Cognitive Agency" (Reasoning: 2) but low "Normative Alignment" (Level 0 or 1) poses high risk of unforeseen consequences or misalignment

- **First 3 experiments:**
  1. Baseline Assessment: Select 3 current tools in your stack (e.g., chatbot, automation script, predictive model) and score them using 8 dimensions to establish comparative baseline
  2. Target Profiling: Define "ideal" scores for new project (e.g., "We need Reasoning level of 2 and Interactivity of 2") and use profile to evaluate vendor claims
  3. Risk Mapping: Identify one high-agency system in your organization and specifically evaluate its "Normative Alignment" and "Self-Improvement" scores to assess governance risk

## Open Questions the Paper Calls Out

- **Open Question 1:** How does the typology perform when tested and refined through expert interviews and practical case studies?
  - Basis in paper: [explicit] Authors state "As a next step, the typology could be tested and refined through expert interviews and practical case studies, deepening our insight into how these frameworks are best applied in organizational contexts."
  - Why unresolved: Current evaluation relied on Human-AI hybrid approach and limited empirical examples, lacking validation through human expert consensus or longitudinal field application.
  - What evidence would resolve it: Qualitative data from expert focus groups or results from field case studies demonstrating framework's validity in real-world deployment decisions.

- **Open Question 2:** To what extent does the framework's construction bias its dimensions toward LLM-based systems, limiting applicability to other AI paradigms?
  - Basis in paper: [explicit] Authors acknowledge limitation: "the prominence of LLM-based agentic systems may have introduced an unintended bias toward the types of agentic behavior these systems exhibit."
  - Why unresolved: Literature review and evaluation phases focused heavily on recent advancements dominated by LLMs, potentially skewing dimensions toward language-centric capabilities.
  - What evidence would resolve it: Comparative analysis applying typology to diverse set of non-LLM agents (e.g., robotic process automation, reinforcement learning agents) to verify dimension fit and mutual exclusivity.

- **Open Question 3:** Does use of this typological framework significantly reduce risk of misalignment between agentic AI capabilities and organizational strategic objectives?
  - Basis in paper: [inferred] Introduction posits that "Without a systematic framework, we argue that organizations risk misalignment between agentic AI capabilities and practical applications," implying framework is solution to this risk.
  - Why unresolved: Paper demonstrates typology can classify systems but does not empirically prove using framework prevents specific misalignment risks identified in motivation.
  - What evidence would resolve it: Empirical studies comparing strategic alignment outcomes in organizations that adopt framework versus those that do not when integrating agentic AI.

## Limitations
- Theoretical nature anticipates capabilities that may not yet exist empirically (Level 3 "General Intelligence")
- Validation relies on 43 real-world systems whose complete list is not provided, limiting reproducibility
- Human-AI hybrid evaluation method introduces uncertainty due to undisclosed OpenAI Deep Research prompt engineering

## Confidence
- **High Confidence:** Framework's ability to systematically decompose agency into observable dimensions (Mechanism 1) is well-supported by literature review and concept matrix methodology
- **Medium Confidence:** Ordinal progression model (Mechanism 2) is logically sound but assumes linear capability development that may not reflect real-world AI evolution patterns
- **Medium Confidence:** Reduction to two high-level axes (Mechanism 3) is practically useful but may oversimplify complex capability profiles, particularly for systems with uneven dimensional scores

## Next Checks
1. **Inter-rater Reliability Test:** Apply framework to classify 10 diverse AI systems using multiple human evaluators and measure agreement rates on dimension assignments, particularly for borderline cases between levels

2. **Predictive Validity Assessment:** Track emerging AI systems over 12 months to evaluate whether framework accurately predicts and categorizes new agentic capabilities that emerge after publication

3. **Risk Analysis Validation:** Apply framework specifically to identify governance risks in high-agency systems by focusing on "Normative Alignment" and "Self-Improvement" dimensions, then compare findings against actual incidents of AI misalignment or unintended consequences