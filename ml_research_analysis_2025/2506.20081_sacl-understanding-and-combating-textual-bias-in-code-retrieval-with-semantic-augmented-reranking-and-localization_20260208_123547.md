---
ver: rpa2
title: 'SACL: Understanding and Combating Textual Bias in Code Retrieval with Semantic-Augmented
  Reranking and Localization'
arxiv_id: '2506.20081'
source_url: https://arxiv.org/abs/2506.20081
tags:
- code
- sacl
- retrieval
- file
- function
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of textual bias in code retrieval
  systems, where retrievers rely too heavily on surface-level features like docstrings
  and variable names rather than understanding code semantics. The authors systematically
  analyze this bias through controlled normalization experiments, revealing that current
  retrievers exhibit strong dependence on textual features and prefer well-documented
  code even when documentation is irrelevant.
---

# SACL: Understanding and Combating Textual Bias in Code Retrieval with Semantic-Augmented Reranking and Localization

## Quick Facts
- **arXiv ID:** 2506.20081
- **Source URL:** https://arxiv.org/abs/2506.20081
- **Reference count:** 9
- **Primary result:** Semantic augmentation improves code retrieval by up to 12.8% Recall@1 on HumanEval and 7.0% file localization accuracy on SWE-Bench-Lite.

## Executive Summary
This paper addresses textual bias in code retrieval systems, where retrievers rely too heavily on surface-level features like docstrings and variable names rather than understanding code semantics. Through systematic normalization experiments, the authors demonstrate that current retrievers exhibit strong dependence on textual features and prefer well-documented code even when documentation is irrelevant. They propose SACL, a framework that augments code retrieval with semantic information through natural language descriptions generated by LLMs. SACL significantly improves retrieval performance—by up to 12.8% Recall@1 on HumanEval, 9.4% on MBPP, and 7.0% file localization accuracy on SWE-Bench-Lite—by bridging the semantic gap between natural language queries and code through combined code-query and description-query similarity scores.

## Method Summary
SACL addresses textual bias in code retrieval by transforming cross-modal text-to-code comparison into intra-modal text-to-text comparison. The method generates natural language descriptions of code functionality using an LLM, then combines the similarity scores from both code-query and description-query comparisons. The final ranking score is computed as Score_final = (1−α)·Score_code + α·Score_desc with α tuned to 0.7. The framework includes semantic-augmented reranking for code retrieval and semantic-augmented in-context localization for repository-level tasks. The approach is evaluated on HumanEval, MBPP, and SWE-Bench-Lite, showing significant improvements in retrieval accuracy and downstream code generation performance.

## Key Results
- SACL improves Recall@1 by up to 12.8% on HumanEval, 9.4% on MBPP, and 7.0% file localization accuracy on SWE-Bench-Lite
- Code generation performance improves with up to 4.88% Pass@1 gains on HumanEval
- Optimal performance achieved at α=0.7, where pure-description (α=1) and pure-code (α=0) retrieval both underperform
- Descriptions maintain higher lexical overlap with queries than normalized code, especially under full normalization

## Why This Works (Mechanism)

### Mechanism 1: Cross-Modal to Intra-Modal Transformation
Converting code-to-text retrieval into text-to-text retrieval improves semantic matching when code lacks rich textual features. An LLM generates natural language descriptions of code functionality, transforming a cross-modal problem into a more tractable intra-modal comparison.

### Mechanism 2: Weighted Score Aggregation for Complementary Signals
Linearly combining code-query and description-query similarity scores balances lexical matching and semantic understanding. The aggregation leverages both syntactic information from code embeddings and high-level semantics from description embeddings.

### Mechanism 3: Lexical Overlap Enhancement Under Normalization
Descriptions improve retrieval by increasing lexical overlap between queries and relevant documents, particularly when code surface features are normalized. LLM-generated descriptions use natural language vocabulary that aligns with user query phrasing.

## Foundational Learning

- **Embedding-based retrieval and cosine similarity**
  - Why needed here: SACL fuses similarity scores from code and description embeddings; understanding how these scores are computed is essential for debugging and tuning.
  - Quick check question: Given two embedding vectors, can you compute their cosine similarity and explain what a higher score indicates?

- **Normalization ablation for feature importance**
  - Why needed here: The paper's core analysis systematically masks textual features to expose retriever dependencies; understanding this method helps interpret the bias findings.
  - Quick check question: If you replace all function and variable names with placeholders (e.g., `func_0`, `var_0`), what signal is removed while preserving functionality?

- **Cross-modal vs. intra-modal retrieval**
  - Why needed here: SACL's central insight is converting text-to-code comparison into text-to-text; understanding modal boundaries clarifies why this helps.
  - Quick check question: Why might text-to-text comparison be "more tractable" than text-to-code comparison for semantic matching?

## Architecture Onboarding

- **Component map:** Input query + code corpus -> Base retriever (GIST-large) -> Top-k candidates -> LLM description generation -> Score fusion (Score_final = (1-α)·Score_code + α·Score_desc) -> Re-ranked documents

- **Critical path:**
  1. Description quality determines reranking accuracy—errors propagate to final ranking
  2. α tuning is dataset-sensitive; verify optimal value before deployment
  3. For repo-level tasks, file descriptions must be generated before the localization prompt

- **Design tradeoffs:**
  - Description brevity (<100 words) vs. semantic completeness
  - Small summarization model for efficiency vs. larger model for accuracy
  - α=0.7 favors descriptions while retaining code signal; pure-description (α=1) underperforms

- **Failure signatures:**
  - Degraded retrieval when LLM misinterprets complex or poorly structured code
  - Minimal gain when code already has rich textual features (e.g., docstring-only normalization shows 0.0 improvement on HumanEval)
  - Diminishing returns in localization when filenames already convey semantics

- **First 3 experiments:**
  1. Replicate the normalization ablation (Section 2.1) on your retriever to confirm textual bias exists in your setting
  2. Sweep α from 0 to 1 in 0.1 increments on a validation set to identify the optimal weighting before deployment
  3. Evaluate description quality by comparing LLM summaries against ground-truth docstrings on a sample, measuring ROUGE overlap to catch systematic generation errors

## Open Questions the Paper Calls Out

- Can semantic augmentation be effectively integrated into agentic code generation frameworks? (Basis: The authors state they do not apply their techniques to agentic methods for code generation)
- Does SACL generalize to other repository-level coding benchmarks and programming languages? (Basis: The authors note they do not explore performance on other Repo-level coding benchmarks and all experiments are restricted to Python)
- Does the observed textual bias persist in retrieval tasks beyond the function level? (Basis: The authors limit their analysis scope to function retrieval when analyzing lexical-level bias)

## Limitations
- Localization Setup Scope: Only validates file-level localization on SWE-Bench-Lite, not line-level localization accuracy
- Dataset and Domain Dependence: Performance gains demonstrated only on Python programming tasks, effectiveness on other languages untested
- Description Quality Dependency: Performance contingent on LLM's ability to generate accurate descriptions, which may vary significantly with different models or code complexity

## Confidence
- High Confidence: Core finding that code retrieval systems exhibit textual bias is well-supported through systematic normalization experiments
- Medium Confidence: Effectiveness of weighted score aggregation mechanism (α=0.7) is demonstrated but optimal α may vary by domain
- Medium Confidence: Transformation from cross-modal to intra-modal comparison is theoretically sound but lacks extensive ablation studies isolating its contribution

## Next Checks
1. Implement and evaluate SACL's line-level localization capability on SWE-Bench-Lite to verify whether semantic descriptions improve precision beyond file-level localization
2. Test SACL on code retrieval tasks from different programming languages (e.g., Java, C++, JavaScript) to assess robustness of textual bias findings and generalizability of α=0.7 weighting
3. Systematically evaluate how description quality affects retrieval performance by comparing multiple LLM models and measuring ROUGE overlap between LLM descriptions and ground-truth docstrings