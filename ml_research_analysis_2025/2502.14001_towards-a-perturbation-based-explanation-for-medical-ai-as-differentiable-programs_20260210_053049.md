---
ver: rpa2
title: Towards a perturbation-based explanation for medical AI as differentiable programs
arxiv_id: '2502.14001'
source_url: https://arxiv.org/abs/2502.14001
tags:
- matrix
- learning
- input
- jacobian
- deep
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the need for objective explainability of AI
  models in medical applications, where black-box models are problematic. The authors
  propose a perturbation-based explanation (PBX) approach to measure how stably a
  model responds to small input perturbations, aiming to provide local, model-agnostic
  interpretability without requiring additional data.
---

# Towards a perturbation-based explanation for medical AI as differentiable programs

## Quick Facts
- arXiv ID: 2502.14001
- Source URL: https://arxiv.org/abs/2502.14001
- Reference count: 11
- One-line primary result: The Jacobian matrix of standard deep learning models can be computed exactly using a forward-pass algorithm, providing local sensitivity analysis for medical AI explainability.

## Executive Summary
This paper proposes a perturbation-based explanation (PBX) framework for medical AI models, addressing the critical need for interpretable black-box systems in healthcare. The core innovation is a forward-pass algorithm that computes the exact Jacobian matrix of trained deep learning models, measuring how stably each output responds to small input perturbations. This approach provides local, model-agnostic interpretability without requiring additional data, enabling medical practitioners to understand which input features most influence model predictions at specific instances.

## Method Summary
The method computes the exact Jacobian matrix of a trained deep learning model at a given input using a forward-pass algorithm that decomposes the computation across layers. For a model with L layers, the algorithm iteratively calculates per-layer Jacobian matrices by multiplying the activation function's gradient with the weight matrix and accumulating results from previous layers. This approach is more computationally efficient than finite-difference methods, requiring only one forward pass versus m+1 evaluations for m input features. The method assumes all activation functions have computable Jacobians at any point, enabling layer-wise decomposition via the chain rule.

## Key Results
- The Jacobian matrix of standard deep learning models can be computed exactly using a forward-pass algorithm.
- Forward-pass computation is more efficient than finite-difference methods, requiring only one evaluation versus m+1.
- The computed Jacobian provides instance-level local sensitivity analysis, showing which input features most influence each output dimension.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The Jacobian matrix of a deep learning model can be computed exactly through layer-wise decomposition using the chain rule.
- Mechanism: The model's Jacobian decomposes into a product of per-layer Jacobians: Jf = ∏(∂a[l]/∂a[l-1]) for l=2 to L. Each layer's contribution combines the activation function's gradient with the weight matrix, enabling forward-pass computation without backpropagation.
- Core assumption: Every activation function σ[l] has a computable Jacobian matrix at any point in its domain.
- Evidence anchors:
  - [abstract]: "The core idea is to compute the Jacobian matrix of trained deep learning models using a forward-pass algorithm that decomposes the computation across layers."
  - [Results, Theorem 1]: "f's Jacobian matrix is decomposed into σ[l](W[l]·)'s Jacobian matrices: Jf = ∂y/∂a[L-1] · ∂a[L-1]/∂a[L-2] · ... · ∂a[2]/∂x"
  - [corpus]: Weak direct support; neighbor papers focus on perturbation-based explanation reliability rather than Jacobian computation mechanics.
- Break condition: Non-differentiable activation functions (e.g., ReLU at zero) produce undefined gradients at singular points, causing numerical instability or incorrect results.

### Mechanism 2
- Claim: Forward-pass Jacobian computation is more computationally efficient than finite-difference approximation.
- Mechanism: Algorithm 1 evaluates the model once to compute the exact Jacobian, whereas finite-difference methods require m+1 evaluations (one per input feature plus baseline). The forward algorithm also produces intermediate Jacobians J[l] for partial model analysis as a byproduct.
- Core assumption: The autodiff framework can compute exact gradients of activation functions; no approximation error accumulates.
- Evidence anchors:
  - [Discussion]: "The proposed algorithm will outperform in terms of computational efficiency, as it evaluates the target multivariate function just in one pass, while the finite difference method evaluates the model m + 1 times."
  - [Discussion]: "The proposed algorithm calculates the exact value of the Jacobian matrix at given instance, provided that the computation of the gradient of activation functions is exact."
  - [corpus]: Related work on perturbation-based explanations (arXiv:2511.10439, arXiv:2506.19630) addresses reliability issues but does not challenge this efficiency claim.
- Break condition: If activation function gradients are approximated rather than exact, accumulated error across layers may degrade explanation quality for deep networks.

### Mechanism 3
- Claim: The computed Jacobian provides instance-level local sensitivity analysis independent of training data distribution.
- Mechanism: For a given input x, the Jacobian columns indicate which input features most influence the output per unit change; rows indicate which output elements are most affected. This is computed purely from the trained model's functional form, not from dataset statistics.
- Core assumption: Local linearity (first-order approximation) is sufficient to characterize model behavior near the input point.
- Evidence anchors:
  - [Introduction]: "It measures the stability of the output against the virtual variation of input features... returns a value for each instance, not the average value for the entire dataset, making it local."
  - [Discussion]: "Regarded as an indicator of the stability or sensitivity of a model's response to perturbed inputs, the computed gradient shows which input feature is the most influential per unit change."
  - [corpus]: Perturbation-based explanation reliability is linked to uncertainty calibration in related work (arXiv:2511.10439), suggesting local sensitivity alone may be insufficient for well-calibrated explanations.
- Break condition: Strong feature interactions require higher-order derivatives (Hessian or beyond); the Jacobian cannot capture interaction effects from simultaneous multi-feature perturbations.

## Foundational Learning

- Concept: **Jacobian matrix for multivariate functions**
  - Why needed here: The entire method rests on computing ∂f_i/∂x_j for all input-output pairs; without understanding Jacobians, the decomposition strategy is opaque.
  - Quick check question: Given y = f(x₁, x₂) = (x₁² + x₂, x₁·x₂), write the 2×2 Jacobian matrix.

- Concept: **Chain rule for matrix derivatives**
  - Why needed here: The layer-wise decomposition Jf = ∏J[σ[l]]·W[l] is a direct application of the multivariate chain rule; misunderstanding this leads to incorrect implementation.
  - Quick check question: If y = g(h(x)) where h: R³→R² and g: R²→R², what are the dimensions of ∂y/∂x?

- Concept: **Forward-mode vs. reverse-mode automatic differentiation**
  - Why needed here: Algorithm 1 is a forward-mode approach (propagating derivatives with the forward pass); understanding this distinction clarifies why it produces Jacobians directly rather than gradients of a scalar loss.
  - Quick check question: For a function f: R¹⁰⁰→R₅₀, which mode is more efficient for computing the full Jacobian?

## Architecture Onboarding

- Component map:
  - Input layer (l=1): Receives x, initializes J[1] = I_m (identity matrix of input dimension)
  - Hidden/output layers (l=2 to L): Each computes z[l] = W[l]·a[l-1], then a[l] = σ[l](z[l]), then updates J[l] = J_σ[l]|z[l] · W[l] · J[l-1]
  - Output: J[L] is the full model Jacobian at input x
  - Byproduct: Intermediate J[l] values enable partial-model sensitivity analysis

- Critical path:
  1. Ensure all activation functions have analytically defined, computable Jacobians (check for piecewise functions like ReLU)
  2. Implement or verify J_σ[l] computation for each layer type in your model
  3. Execute Algorithm 1 forward pass, accumulating Jacobians via matrix multiplication
  4. Inspect J[L] columns for input feature sensitivity; rows for output element vulnerability

- Design tradeoffs:
  - **Exactness vs. generality**: Method requires differentiable activations; models with hard thresholding or non-differentiable components are incompatible
  - **Local vs. global explanation**: Jacobian captures instantaneous sensitivity at one point, not global behavior across input space
  - **First-order vs. interaction analysis**: Jacobian is linear approximation; feature interactions require higher-order derivatives not provided

- Failure signatures:
  - **NaN or Inf in J[L]**: Likely caused by activation Jacobian evaluation at non-differentiable points (e.g., ReLU at 0)
  - **All-zero Jacobian rows**: Saturated activations (gradient ≈ 0) in later layers; model may be undertrained or input is out-of-distribution
  - **Inconsistent results vs. finite-difference**: Suggests numerical precision issues or incorrect J_σ[l] implementation

- First 3 experiments:
  1. Validate correctness on a small network (2-3 layers) by comparing Algorithm 1 Jacobian against finite-difference approximation; ensure agreement within numerical tolerance.
  2. Test on a model with ReLU activations using inputs that avoid zero; then deliberately pass inputs with zero activations to observe failure mode and quantify robustness.
  3. Apply to a trained medical classification model (e.g., image diagnosis) and visualize which input features (pixels/regions) have highest sensitivity for correct vs. incorrect predictions.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can higher-order partial derivatives be incorporated to capture interactions between input features?
- Basis in paper: [explicit] The Conclusion states the Jacobian matrix is insufficient for property (e) because it lacks information on interactions among input variables.
- Why unresolved: The current method computes only the first-order Jacobian matrix, which measures single-feature sensitivity but fails to quantify how simultaneous perturbations to multiple features affect the model's response.
- What evidence would resolve it: An extension of the algorithm that computes Hessian matrices or interaction terms, validating that these higher-order derivatives accurately reflect multi-feature dependencies in medical AI models.

### Open Question 2
- Question: How can the method be adapted to handle singular points in piecewise differentiable activation functions like ReLU?
- Basis in paper: [explicit] The Discussion highlights that the method assumes differentiable activation functions, noting that ReLU has a singular point at zero which may produce unexpected results.
- Why unresolved: The theorem provided requires a computable Jacobian matrix at arbitrary points, a condition violated by non-differentiable activation functions commonly used in practice.
- What evidence would resolve it: A modified algorithm or mathematical proof showing stable and valid gradient approximations at non-differentiable points for standard architectures utilizing ReLU.

### Open Question 3
- Question: Is this perturbation-based explanation framework applicable to non-deep learning algorithms used in medical settings?
- Basis in paper: [explicit] The Conclusion states that "some extension of this work is required for models built with machine learning algorithms other than deep learning."
- Why unresolved: The current formulation relies on a layered, differentiable structure specific to deep learning models, making direct transfer to models like decision trees or SVMs impossible without significant theoretical modification.
- What evidence would resolve it: A generalized definition of perturbation-based explanation that applies to non-differentiable models, or a specific adaptation strategy for widely used alternative algorithms.

## Limitations
- Requires differentiable activation functions; excludes models with hard thresholds or ReLU at singular points.
- Provides only local sensitivity analysis at single input points, not global behavior across input space.
- Cannot capture feature interactions, which may require higher-order derivatives beyond the Jacobian.

## Confidence
- **High**: The Jacobian computation algorithm is based on well-established chain rule principles and efficient forward-pass implementation.
- **Medium**: The perturbation-based explanation's clinical utility depends on downstream interpretation tasks and may require integration with uncertainty calibration.
- **Medium**: The core assumption that local first-order analysis suffices for explainability carries uncertainty, as complex feature interactions might require higher-order derivatives.

## Next Checks
1. Test algorithm robustness on networks with ReLU activations using both clean and edge-case inputs (including zeros).
2. Compare explanation quality against established methods like SHAP on a medical imaging benchmark.
3. Validate sensitivity analysis by measuring model performance degradation under targeted input perturbations guided by Jacobian directions.