---
ver: rpa2
title: Quadratic Interest Network for Multimodal Click-Through Rate Prediction
arxiv_id: '2504.17699'
source_url: https://arxiv.org/abs/2504.17699
tags:
- prediction
- multimodal
- user
- network
- quadratic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper presents the Quadratic Interest Network (QIN), a multimodal
  CTR prediction model that achieved 0.9798 AUC and second place in the WWW 2025 EReL@MIR
  Challenge. QIN integrates two key innovations: an Adaptive Sparse Target Attention
  (ASTA) mechanism for extracting multimodal user behavior features, and Quadratic
  Neural Networks (QNN) for capturing high-order feature interactions.'
---

# Quadratic Interest Network for Multimodal Click-Through Rate Prediction

## Quick Facts
- arXiv ID: 2504.17699
- Source URL: https://arxiv.org/abs/2504.17699
- Reference count: 30
- Achieved 0.9798 AUC and second place in WWW 2025 EReL@MIR Challenge

## Executive Summary
This paper presents the Quadratic Interest Network (QIN), a multimodal CTR prediction model that achieved 0.9798 AUC and second place in the WWW 2025 EReL@MIR Challenge. QIN integrates two key innovations: an Adaptive Sparse Target Attention (ASTA) mechanism for extracting multimodal user behavior features, and Quadratic Neural Networks (QNN) for capturing high-order feature interactions. The ASTA replaces SoftMax with ReLU to reduce computational overhead and improve focus on critical behaviors, while QNN explicitly models complex feature relationships through quadratic interactions. Ablation studies demonstrate that both components significantly contribute to performance, with QNN reducing validation AUC from 0.9701 to 0.7396 when removed, and ASTA dropping it to 0.9321.

## Method Summary
QIN processes multimodal user behavior sequences (text, image embeddings) through multiple ASTA modules that use ReLU attention instead of SoftMax normalization, then concatenates these with ID-based features before passing through a QNN backbone. The QNN uses Khatri-Rao products to compute quadratic feature interactions explicitly, followed by PReLU activation. The model is trained with binary cross-entropy loss on the MicroLens_1M_MMCTR dataset using FuxiCTR framework with learning rate 2e-3, batch size 8192, and embedding weight decay 2e-4.

## Key Results
- Achieved 0.9798 AUC on leaderboard and 0.9701 validation AUC in WWW 2025 EReL@MIR Challenge
- ASTA with ReLU outperforms SoftMax variant (0.9701 vs 0.9490 AUC)
- QNN is essential: removing it drops validation AUC from 0.9701 to 0.7396
- PReLU activation in QNN outperforms ReLU (0.9701 vs 0.9584)
- Dropout in ASTA is harmful (0.9701 vs 0.9681 with dropout)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Replacing SoftMax normalization with ReLU in target attention reduces noise dilution and computational overhead while preserving focus on relevant user behaviors.
- Mechanism: ASTA computes attention weights as ReLU(QK^T/√d_a)V + x_t, where negative attention scores are zeroed rather than soft-normalized across the full sequence. This produces sparse, hard attention that selects only positively-correlated behaviors.
- Core assumption: User behavior sequences contain substantial noise/irrelevant actions, and global normalization spreads attention too thinly across informative signals.
- Evidence anchors:
  - [abstract] "ASTA replaces SoftMax with ReLU to reduce computational overhead and improve focus on critical behaviors"
  - [section 2.1] Ablation shows ASTA w/ SoftMax achieves 0.9490 AUC vs 0.9701 for full QIN; removing ASTA drops to 0.9321
  - [corpus] Related papers on user interest modeling exist (e.g., "Time Matters" on temporal interest, FMR 0.43), but direct comparison of ReLU vs SoftMax in attention is not found in corpus neighbors
- Break condition: If behavior sequences are short (<10 items) or highly curated (low noise), sparse attention may discard useful weak signals that SoftMax would retain.

### Mechanism 2
- Claim: Quadratic polynomial interactions explicitly capture high-order feature dependencies that standard MLPs cannot efficiently represent.
- Mechanism: QNN computes X_{l+1} = σ(X_l • W_l X_l) via Khatri-Rao product, generating D² pairwise interaction terms per layer (e.g., x_i × x_j) with learned coupling weights, then applies PReLU activation.
- Core assumption: Click behavior arises from feature interplay (e.g., past purchase history × current item visual appeal) rather than independent feature effects.
- Evidence anchors:
  - [abstract] "QNN explicitly models complex feature relationships through quadratic interactions"
  - [section 2.2 + Table 3] Removing QNN (replacing with MLP [1024,512,256]) drops validation AUC from 0.9701 to 0.7396—a catastrophic 23% relative degradation
  - [corpus] "Revisiting Feature Interactions from the Perspective of Quadratic Neural Networks" (arxiv 2505.17999) discusses QNN for CTR but full methodology unclear; corpus evidence on QNN theory is limited
- Break condition: If feature dimension D is very large (>1000), D² interaction terms may cause memory issues; if features are largely independent, quadratic terms add noise without signal.

### Mechanism 3
- Claim: Concatenating multimodal behavior embeddings (text, image) with ID-based features provides complementary signals that improve interest representation.
- Mechanism: Multiple behavior sequences (S1, S2, ..., St) are processed through separate ASTA modules, producing attention-weighted outputs that are concatenated with user profile, target item, and context features before QNN processing.
- Core assumption: Different modalities capture distinct aspects of user preference (visual appeal vs. textual relevance) that are not redundant.
- Evidence anchors:
  - [abstract] "QIN integrates multimodal information utilization with low-latency requirements"
  - [section 2, Figure 1] Architecture shows "Multimodal User Behavior Features" feeding multiple K/V streams into ASTA
  - [corpus] "1st Place Solution of WWW 2025 EReL@MIR" (arxiv 2505.03543, FMR 0.55) confirms multimodal embeddings improved winning submission; "Decoupled Multimodal Fusion" paper addresses similar integration challenges
- Break condition: If modalities are highly correlated (e.g., image and text embeddings both dominated by category semantics), concatenation increases parameters without proportional signal gain.

## Foundational Learning

- Concept: Target Attention (Query-Key-Value formulation)
  - Why needed here: ASTA is built on target attention where the candidate item (query) retrieves relevant user history (keys/values); understanding Q/K/V shapes and dot-product attention is essential.
  - Quick check question: Given target embedding x_t ∈ R^64 and 100 behavior items x_b ∈ R^(100×64), with projection to d_a=32, what are the output shapes of Q, K, and V?

- Concept: Feature Interaction in CTR
  - Why needed here: QNN is motivated by the insight that click decisions depend on feature combinations (e.g., "premium user" × "luxury item"); learners must understand why linear models fail here.
  - Quick check question: If a model only learns weights w_premium and w_luxury separately, what interaction effect cannot be captured when both features are present?

- Concept: Khatri-Rao Product
  - Why needed here: QNN uses this specialized matrix product to compute quadratic terms efficiently; distinguishing it from element-wise and Kronecker products clarifies implementation.
  - Quick check question: For vectors a ∈ R^3 and b ∈ R^3, compare output shapes of Hadamard (a⊙b), Kronecker (a⊗b), and column-wise Khatri-Rao products.

## Architecture Onboarding

- Component map: Input layer -> ASTA modules (multiple parallel) -> Feature concatenation -> QNN backbone (L=4 layers) -> Output head (Linear + Sigmoid)

- Critical path:
  1. Verify all embeddings share dimension 128 before concatenation (mismatch will fail silently in broadcasting)
  2. Check attention scaling factor √d_a matches projection dimension to prevent gradient explosion
  3. QNN PReLU activation is essential—ablation shows ReLU drops AUC to 0.9584

- Design tradeoffs:
  - **ASTA sparse vs. dense attention**: +2.1% AUC (0.9701 vs 0.9490) but may miss weak signals in clean sequences
  - **QNN vs MLP**: QNN is non-negotiable (removal costs 23% AUC), but parameter count scales as O(D²×M×L)
  - **Dropout in ASTA**: Paper finds it harmful (0.9681 vs 0.9701) because ReLU already provides implicit regularization through sparsity

- Failure signatures:
  - Validation AUC ≈0.74: QNN likely not computing quadratic terms (check Khatri-Rao implementation or layer connectivity)
  - Validation AUC ≈0.93: Attention may be bypassed (mean pooling fallback) or sequences not loaded correctly
  - Training diverges: Check attention scaling; SoftMax variant requires √d_a scaling that ReLU may not need identically
  - Overfitting with Dropout: Expected—paper shows Dropout in ASTA hurts performance; remove it

- First 3 experiments:
  1. Reproduce DIN baseline (target: AUC ~0.8655) on challenge dataset to validate data pipeline and embedding loading
  2. Ablate ASTA: Replace with mean pooling (target: AUC ~0.93), then swap ReLU→SoftMax (target: AUC ~0.95) to isolate sparse attention benefit
  3. Ablate QNN: Substitute MLP [1024, 512, 256] (target: AUC ~0.74), then restore QNN and verify recovery to ~0.97

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does QIN generalize across different multimodal CTR datasets and industrial domains beyond the MicroLens_1M_MMCTR benchmark?
- Basis in paper: [inferred] The paper only evaluates QIN on a single challenge dataset, reporting 0.9798 AUC without cross-dataset validation.
- Why unresolved: No experiments on alternative datasets (e.g., Amazon, Taobao) or domains are presented to assess generalizability.
- What evidence would resolve it: Evaluation of QIN on multiple public multimodal recommendation datasets with comparative analysis.

### Open Question 2
- Question: What are the theoretical foundations explaining QNN's dramatic performance contribution (AUC drop from 0.9701 to 0.7396 when removed)?
- Basis in paper: [explicit] The authors state "For further details on QNN, refer to our forthcoming paper, which is currently under preparation."
- Why unresolved: The current paper lacks theoretical analysis of why quadratic polynomial inputs outperform MLP for CTR feature interactions.
- What evidence would resolve it: A follow-up paper with theoretical justification and analysis of learned quadratic interaction patterns.

### Open Question 3
- Question: What is the actual inference latency of QIN compared to DIN and other baselines in real-world deployment scenarios?
- Basis in paper: [inferred] The paper emphasizes "low-latency requirements of online inference" but provides no latency measurements or computational cost analysis.
- Why unresolved: While ASTA replaces SoftMax with ReLU to reduce overhead, no timing experiments are reported.
- What evidence would resolve it: Benchmarks of inference time per sample across different sequence lengths and batch sizes.

### Open Question 4
- Question: Which specific modalities (text, images, behavioral logs) contribute most to QIN's performance gains?
- Basis in paper: [inferred] The paper integrates multimodal features but conducts no per-modality ablation analysis.
- Why unresolved: Understanding modality importance would guide practical deployment decisions in resource-constrained settings.
- What evidence would resolve it: Systematic ablation removing individual modalities and measuring resulting AUC changes.

## Limitations
- Performance relies heavily on QNN component with no theoretical explanation for its effectiveness
- Generalizability across different CTR datasets and domains is untested
- No latency measurements provided despite claims of low-latency design
- Specific contribution of individual modalities is unknown

## Confidence
- **High confidence**: ASTA with ReLU outperforms SoftMax (0.9701 vs 0.9490 AUC); QNN is essential for performance (0.9701 vs 0.7396 without it); PReLU activation outperforms ReLU in QNN (0.9701 vs 0.9584)
- **Medium confidence**: Sparse attention provides computational benefits; multimodal concatenation provides complementary signals; Dropout in ASTA is harmful
- **Low confidence**: Generalizability of QNN benefits across CTR tasks; whether ReLU advantage extends to different sequence lengths/noise levels; specific contribution of each multimodal modality

## Next Checks
1. **Mechanism isolation test**: Implement the exact QNN architecture (verify Khatri-Rao product computation and layer dimensions) and measure AUC degradation when replaced with MLP on the challenge dataset.

2. **Attention mechanism comparison**: Systematically vary behavior sequence length and noise levels to test whether ReLU attention maintains advantage over SoftMax across different data regimes.

3. **Multimodal contribution analysis**: Train ablated versions using only single modalities (text-only, image-only) to quantify the marginal benefit of multimodal concatenation versus computational cost.