---
ver: rpa2
title: 'Spectral Text Fusion: A Frequency-Aware Approach to Multimodal Time-Series
  Forecasting'
arxiv_id: '2602.01588'
source_url: https://arxiv.org/abs/2602.01588
tags:
- time
- series
- frequency
- textual
- forecasting
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: SpecTF addresses multimodal time series forecasting by modeling
  how textual information influences time series across multiple temporal scales through
  frequency-domain fusion. The framework decomposes time series into spectral components
  and uses cross-attention mechanisms to adaptively reweight frequency bands based
  on textual relevance before mapping results back to the temporal domain.
---

# Spectral Text Fusion: A Frequency-Aware Approach to Multimodal Time-Series Forecasting

## Quick Facts
- arXiv ID: 2602.01588
- Source URL: https://arxiv.org/abs/2602.01588
- Reference count: 40
- Primary result: Achieves state-of-the-art performance on multimodal time series forecasting, outperforming existing approaches by 3.82% in MSE and 2.25% in MAE while using significantly fewer parameters and achieving faster inference speeds.

## Executive Summary
SpecTF introduces a frequency-domain approach to multimodal time series forecasting that models how textual information influences time series dynamics across multiple temporal scales. By decomposing time series into spectral components and using cross-attention mechanisms to adaptively reweight frequency bands based on textual relevance, SpecTF achieves state-of-the-art performance while maintaining parameter efficiency. The framework demonstrates consistent improvements across diverse domains including Agriculture, Climate, Health, and Environment benchmarks.

## Method Summary
SpecTF processes historical time series through real FFT (rFFT) to obtain spectral components, then embeds these using frequency-specific MLPs with positional encoding. Textual data is processed through a pretrained language model, temporally aligned with timestamps, and projected into the complex frequency space via separate MLPs for real and imaginary components. A frequency cross-modality fusion (FreqCMF) block uses cross-attention between spectral queries and textual keys/values, followed by complex multiplication fusion to modulate each frequency band. The fused spectral representation is then mapped through frequency-specific MLPs to forecast target frequencies, which are transformed back to the temporal domain using inverse rFFT.

## Key Results
- Achieves state-of-the-art performance with 3.82% improvement in MSE and 2.25% in MAE compared to existing approaches
- Uses significantly fewer parameters (397K vs. 1.72M-6.41M for baselines)
- Demonstrates faster inference speeds while maintaining superior accuracy
- Shows consistent improvements across nine domains in Time-MMD benchmark and two domains in TTC benchmark

## Why This Works (Mechanism)

### Mechanism 1: Multi-scale temporal influence modeling
Frequency-domain decomposition enables explicit modeling of text's multi-scale temporal influence by mapping time series into spectral components where different frequency bands correspond to different temporal scales. Complex multiplication in the fusion step simultaneously affects amplitude and phase, capturing both energy distribution and temporal alignment in one operation.

### Mechanism 2: Frequency-specific text relevance
Cross-attention in the frequency domain identifies which textual documents are relevant to which frequency bands by computing attention weights between time-series frequency queries and textual keys. This produces frequency-specific textual context that modulates each band independently based on learned relevance patterns.

### Mechanism 3: Parameter-efficient spectral processing
Complex-valued FreqMLPs exploit energy compaction in spectral representations for parameter efficiency by jointly modeling real and imaginary components through complex weight matrices. Real-world time series concentrate energy in fewer frequency components, allowing compact representations that reduce parameter count without sacrificing performance.

## Foundational Learning

- **Discrete Fourier Transform and Real FFT**: Needed to understand how rFFT maps time series to spectral components (L → L/2+1 complex values) and how irFFT reconstructs temporal predictions. Quick check: Given a time series of length 24, what is the shape of its rFFT output, and what does each component represent?

- **Complex-valued neural operations**: Required to understand FreqMLPs with complex weight matrices and the complex multiplication (⊙) in fusion. Quick check: If you multiply two complex numbers in the frequency domain, what happens to their amplitudes and phases? How does this relate to the convolution theorem?

- **Cross-attention with complex queries/keys/values**: Essential for understanding FreqCMF's attention computation with absolute value of complex products. Quick check: Why can't you directly apply softmax to complex-valued attention scores? What does the absolute value operation preserve or discard?

## Architecture Onboarding

- **Component map**: Time Series Embedding → FreqCMF → Forecaster → Projection
- **Critical path**: The FreqCMF block is the architectural innovation. Data flows: time series → rFFT → FreqMLP → Q̂; text → LM → MLPs → K̂/V̂; attention = softmax(|Q̂K̂^⊤|/√d_k); output = attention · V̂; fusion = X̂_emb ⊙ output.
- **Design tradeoffs**: Channel-independent processing (simpler, more scalable) vs. channel-mixing; rFFT (exploits Hermitian symmetry) vs. full FFT; pretrained LM size tradeoff (larger = better performance but higher inference cost).
- **Failure signatures**: Uniform attention weights indicate text embedding issues; predictions collapsing to mean suggests multiplication fusion problems; dimension mismatches at ⊙ operation; underperformance on Economy-like data indicates time-domain alignment may be preferable.
- **First 3 experiments**: 1) Baseline validation on Agriculture dataset to confirm implementation accuracy; 2) Ablation sweep removing each component to reproduce degradation patterns; 3) Attention visualization on Climate dataset to verify meaningful text-frequency relationships.

## Open Questions the Paper Calls Out

- **Explainability audit**: How can explainability tools be architected to effectively audit text-frequency interactions within the complex multiplication operations of the FreqCMF module? The paper explicitly calls for integrating explainability tools to audit these interactions in future work.

- **Fusion regime switching**: Under what theoretical conditions regarding textual noise levels and temporal localization does spectral fusion become detrimental compared to time-domain alignment? The paper notes SpecTF underperforms on Economy where text contains noise and short-term indicators better suited to time-domain alignment.

- **Semantic information retention**: Does the Complex Projection MLP lose significant semantic nuance compared to the direct spectral analysis of the time series? The text undergoes learned MLP projection while time series uses rigorous rFFT, potentially creating an "embedding gap."

## Limitations

- The frequency-domain approach may not generalize to domains where textual information has no predictable spectral signature in time series data
- The method's performance may degrade when time series have flat spectral distributions or white noise characteristics
- Limited analysis of failure modes where text-frequency alignment breaks down, particularly in domains with conflicting temporal scales

## Confidence

- **High confidence**: Claims about state-of-the-art performance metrics on benchmark datasets are well-supported by consistent improvements across multiple domains
- **Medium confidence**: Mechanism claims about frequency-domain cross-attention are theoretically grounded but lack extensive ablation studies isolating attention's contribution
- **Low confidence**: Parameter efficiency claims rely on assumptions about energy compaction that may not hold for domains with flat spectral distributions

## Next Checks

1. **Domain generalization test**: Apply SpecTF to datasets where textual context is known to be irrelevant or misleading (e.g., technical indicators with noisy news commentary) to measure performance degradation compared to time-domain baselines.

2. **Spectral sensitivity analysis**: Systematically vary lookback window length L and observe how frequency resolution affects performance, particularly for domains with different short-term vs. long-term text influences.

3. **Cross-attention visualization**: Generate attention heatmaps across all benchmark domains to verify the model learns domain-appropriate text-frequency relationships rather than memorizing spurious correlations, focusing on cases with conflicting temporal scales.