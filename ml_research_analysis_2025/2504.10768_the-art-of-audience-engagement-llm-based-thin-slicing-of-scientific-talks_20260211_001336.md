---
ver: rpa2
title: 'The Art of Audience Engagement: LLM-Based Thin-Slicing of Scientific Talks'
arxiv_id: '2504.10768'
source_url: https://arxiv.org/abs/2504.10768
tags:
- speech
- thin-slicing
- public
- research
- human
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents an LLM-based thin-slicing approach to evaluate
  scientific talk quality, demonstrating that even very short speech excerpts (as
  little as 1% of the content, around 15 words) can strongly predict overall presentation
  quality with correlations above 0.32. The study uses transcripts from over 100 real
  science presentations, evaluating full talks and thin slices using multiple LLMs
  and prompts, showing robust results across models and configurations.
---

# The Art of Audience Engagement: LLM-Based Thin-Slicing of Scientific Talks

## Quick Facts
- arXiv ID: 2504.10768
- Source URL: https://arxiv.org/abs/2504.10768
- Authors: Ralf Schmälzle; Sue Lim; Yuetong Du; Gary Bente
- Reference count: 18
- Primary result: LLM evaluations of short transcript excerpts predict full-talk quality with correlations above 0.32 using as little as 1% of content.

## Executive Summary
This paper demonstrates that LLM-based thin-slicing can efficiently evaluate scientific talk quality using very short transcript excerpts. The study shows that even 1% slices (approximately 15 words) can predict overall presentation quality with correlations above 0.32, while 10% slices achieve correlations of 0.61-0.76. LLM evaluations align closely with human ratings (r = 0.69), validating their use as scalable substitutes for human raters. The findings support practical applications for enhancing public speaking skills through LLM-based feedback tools.

## Method Summary
The study uses transcripts from 128 real science presentations, evaluating both full talks and thin slices using multiple LLMs (GPT-4o-mini and Gemini Flash 1.5) and five prompt variants. Slices are defined by percentage of word count (1%, 5%, 10%, 20%, 30%, 40%, 50%, 75%, 100%). The methodology involves audio-to-text transcription via Whisper, parametric subsampling at defined percentages, zero-shot LLM inference with structured prompts requesting 1-10 ratings, and correlation analysis between slice ratings and full-talk ratings. Human validation was performed on 20% of slices to establish baseline agreement.

## Key Results
- 1% slices (≈15 words) predict full-talk quality with correlations of 0.32-0.33
- Correlations plateau around 10% slice length, showing diminishing returns beyond this point
- LLM evaluations align closely with human ratings (r = 0.69, p < 0.0001)
- ICC values show high agreement both among human raters (ICC = 0.92) and across LLM prompts (ICC = 0.93)

## Why This Works (Mechanism)

### Mechanism 1
Brief transcript excerpts contain sufficient linguistic signal to predict overall presentation quality. Early speech content encodes rhetorical quality markers (clarity, engagement, structure) that persist throughout the talk and are extractable from partial text. This assumes quality-relevant linguistic patterns are front-loaded in presentations and LLMs can detect these patterns similarly to humans.

### Mechanism 2
LLM-based evaluations converge with human quality judgments, validating LLMs as scalable rater substitutes. LLMs internalize linguistic patterns associated with clarity, coherence, and engagement—patterns humans also use when evaluating rhetorical quality. This assumes the features humans use to judge speech quality are largely linguistic and extractable from text transcripts.

### Mechanism 3
Prediction accuracy plateaus at ~10% slice length, indicating diminishing returns from additional content. Information entropy decreases as talks progress—early segments establish speaker competence, topic framing, and structural cues that constrain later impressions. This assumes audiences form stable quality impressions early that are resistant to later revision.

## Foundational Learning

- **Thin-slicing paradigm**: Core theoretical framework where brief behavioral samples can predict holistic judgments. Quick check: Can you explain why a 30-second teacher video clip predicts semester-end evaluations, and how this logic transfers to text transcripts?

- **Intra-class correlation (ICC) and inter-rater reliability**: Required to validate both human and LLM rating consistency. Quick check: What does ICC > 0.90 indicate about rater agreement, and why does this matter before trusting averaged ratings?

- **Part-to-whole correlation analysis**: Statistical method for testing thin-slice validity. Quick check: If slice-to-full correlation is r = 0.65 at 10% and r = 0.70 at 50%, what does the small gain imply about information distribution?

## Architecture Onboarding

- **Component map**: Audio recordings -> Whisper transcription -> Text preprocessing -> Slicing engine (parametric subsampling) -> LLM evaluation layer (GPT-4o-mini/Gemini Flash 1.5 with 5 prompts) -> Rating extraction -> Validation layer (human rater subset) -> Analysis layer (Pearson correlations, ICC computation)

- **Critical path**: Transcription quality -> Slice boundary definition -> Prompt design -> Rating normalization -> Correlation computation. Errors in transcription propagate; slice boundaries must be word-accurate; prompts must constrain output format to integers.

- **Design tradeoffs**: Slice definition uses percentage-based vs. fixed word count (paper tested both); prompt complexity shows robustness to phrasing (ICC = 0.93); human validation scope limited to 20% slices due to cost constraints.

- **Failure signatures**: Low ICC among LLM prompts indicates prompt sensitivity (not observed; ICC = 0.93); human-LLM correlation near zero indicates LLM not capturing human-relevant features (not observed; r = 0.69); slice-to-full correlations not increasing with slice length indicates no thin-slicing effect (not observed); transcription errors distorting slice content would artificially deflate correlations at small slices.

- **First 3 experiments**:
  1. Reproduce slice-to-full correlation curve with different LLM (e.g., Claude, Llama) on same corpus
  2. Test break condition by correlating slice predictions with reversed talks (last 10% as slice)
  3. Perturbation robustness: Add synthetic filler words or remove first 5% of talks to test plateau sensitivity

## Open Questions the Paper Calls Out

- **Multimodal thin-slicing**: Does the inclusion of nonverbal and paralinguistic cues improve predictive accuracy compared to text transcripts alone? The study isolated the verbal channel using only text transcripts.

- **Audience engagement prediction**: Do negative evaluations from thin slices accurately predict when real audiences will disengage or stop paying attention? The study predicted overall quality ratings but did not measure real-time audience engagement behaviors.

- **Intervention efficacy**: Is LLM-based thin-slice feedback an effective intervention for improving public speaking skills? The study validated measurement of quality but did not test the efficacy of the system as a training tool.

## Limitations

- **Transcription dependency**: Study relies on Whisper-generated transcripts without validating impact of transcription errors on slice-to-full correlations, potentially missing quality markers conveyed through prosody or non-verbal cues.

- **Front-loading assumption**: Observed plateau at 10% slice length assumes quality information is front-loaded, but this may reflect presentation format conventions rather than universal truth.

- **Domain specificity**: Results based on science presentations only; thin-slicing effect may not generalize to other domains where quality markers distribute differently.

## Confidence

- **High confidence**: LLM-human rating alignment (r = 0.69) and overall thin-slicing correlation patterns
- **Medium confidence**: The 10% plateau effect (trend clear but mechanism speculative)
- **Low confidence**: Domain generalization claims (not tested across presentation types)

## Next Checks

1. **Break condition test**: Run thin-slicing analysis on reversed talk order (last 10% as slice) to verify whether front-loading drives the plateau effect

2. **Transcription error sensitivity**: Systematically degrade transcription quality at 5-10% increments and measure correlation decay to establish error tolerance thresholds

3. **Cross-domain validation**: Apply thin-slicing to non-scientific presentations (TED talks, business pitches) to test whether the 10% plateau generalizes beyond science communication