---
ver: rpa2
title: How Not to Detect Prompt Injections with an LLM
arxiv_id: '2507.05630'
source_url: https://arxiv.org/abs/2507.05630
tags:
- detection
- instruction
- injected
- prompt
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents DataFlip, an adaptive attack that bypasses known-answer
  detection (KAD) defenses by exploiting their structural vulnerability. KAD uses
  a detection LLM to identify contaminated inputs by checking if it follows a detection
  instruction and returns a secret key.
---

# How Not to Detect Prompt Injections with an LLM

## Quick Facts
- arXiv ID: 2507.05630
- Source URL: https://arxiv.org/abs/2507.05630
- Reference count: 40
- Primary result: DataFlip attack bypasses KAD defenses with 0% detection rate while achieving 91% malicious execution success

## Executive Summary
DataFlip presents an adaptive attack that exploits a fundamental vulnerability in Known-Answer Detection (KAD) defenses against prompt injections. The attack demonstrates that KAD's core assumption—that the secret key only appears when the detection instruction is followed—is flawed. By crafting injected instructions with IF/ELSE control flow that extract and return the secret key while simultaneously executing malicious tasks, DataFlip achieves perfect evasion against Strong KAD defenses while maintaining high attack success rates. The work reveals a critical tradeoff: fine-tuning detection LLMs to be more susceptible to injected instructions, intended to catch attacks, actually increases vulnerability to adaptive control-flow attacks.

## Method Summary
The DataFlip attack constructs prompts using IF/ELSE control flow to simultaneously satisfy KAD's detection mechanism and execute malicious instructions. The attack concatenates a detection instruction containing a secret key with user data containing the adaptive prompt. The IF/ELSE structure allows the model to extract the secret key (passing detection) while the ELSE branch executes the injected task on the backend LLM. The method requires no model training—only prompt engineering using the handcrafted template. The attack was evaluated against DataSentinel's fine-tuned Mistral-7B detector using 7 tasks with 100 samples each, plus 500 random keys for key extraction testing.

## Key Results
- DataFlip achieves 0% detection rate against Strong KAD defenses while maintaining 91% attack success rate
- The attack successfully extracts secret keys in 98% of cases across all task categories
- Fine-tuned "Strong KAD" detectors show worse performance against DataFlip than standard models, with up to 83% gap in detection rates for Grammar Correction tasks

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** If an injected instruction and a detection instruction share the same context window, the injected instruction can conditionally access and return the secret key to bypass detection.
- **Mechanism:** Known-Answer Detection (KAD) creates a prompt by concatenating a detection instruction (containing a secret key $k$) with user data. If the user data is contaminated with an adaptive prompt (like DataFlip), the LLM processes both simultaneously. DataFlip uses an `IF/ELSE` structure to "read" the detection instruction from the context, extract $k$, and output it, thereby mimicking a "clean" signal.
- **Core assumption:** LLMs process instructions within the context window in a way that allows later instructions to conditionally reference and act upon earlier parts of the prompt (stateful context interpretation).
- **Evidence anchors:**
  - [abstract] "KAD implicitly assumes that instructions within a prompt are processed in a sequential and stateless manner... We challenge this assumption."
  - [section 3.4] "Since $k$ is embedded in $s_d(k)$... an adaptive $s_e$ within $x_c$ can be crafted to instruct $g$ to extract and return $k$ from the surrounding context."
- **Break condition:** Architectural isolation where the detection instruction is processed in a sandboxed environment invisible to the data/instruction component of the query.

### Mechanism 2
- **Claim:** Increasing a detection model's susceptibility to instruction following (to catch attacks) likely increases its vulnerability to adaptive control-flow attacks.
- **Mechanism:** "Strong KAD" defenses (like DataSentinel) fine-tune detection LLMs to prefer following injected instructions over the detection instruction, assuming this helps identify contaminated inputs. DataFlip exploits this by presenting an injected instruction that *complies* with the detection goal (returning the key) in the `IF` branch. The fine-tuned model, now biased to follow injected instructions rigorously, executes the extraction logic more reliably.
- **Core assumption:** Fine-tuning for instruction susceptibility does not distinguish between "following an instruction to do harm" and "following an instruction to game the detection metric."
- **Evidence anchors:**
  - [abstract] "...fine-tuning detection LLMs to be more susceptible to injected instructions actually exacerbates the vulnerability."
  - [section 5.4] "FNR_DataFlip of the DataSentinel detector is slightly worse than Mistral-Instruct... Grammar Correction in particular having a 83% gap in performance."
- **Break condition:** Detection mechanisms that do not rely on executing the injected instruction (e.g., static analysis or isolated classifiers) or training regimes that penalize conditional key extraction.

### Mechanism 3
- **Claim:** If a defense relies solely on output observation (black-box behavior), it may fail to detect attacks that preserve the expected output format while executing malicious logic.
- **Mechanism:** KAD operates as a black-box check: does the output contain the secret key? DataFlip decouples the *output form* (returning the key) from the *execution intent* (conditional logic). By satisfying the output check ($k$ is present), the attack creates a false negative (Type II failure), allowing the payload to pass to the backend LLM.
- **Core assumption:** Security can be verified by checking if the LLM reproduces a specific token, regardless of the internal reasoning path taken to generate it.
- **Evidence anchors:**
  - [section 3.4] "Type II failure... The detection LLM $g$ follows the injected instruction $s_e$... but executing $s_e$... produces the secret key $k$."
- **Break condition:** "White-box" introspection (e.g., attention analysis) that verifies the model followed the specific *detection instruction* rather than an injected conditional branch.

## Foundational Learning

- **Concept:** **Context Window Concatenation**
  - **Why needed here:** Understanding that prompts are often constructed by concatenating system instructions, user data, and defense mechanisms into a single sequence of tokens. The vulnerability arises because the model sees this as one continuous text stream.
  - **Quick check question:** In a KAD setup, does the detection LLM process the secret key and the user data as separate logical entities or as a single combined text?

- **Concept:** **Prompt Injection / Indirect Injection**
  - **Why needed here:** The paper builds on the premise that LLMs struggle to distinguish between "data" (what to process) and "instruction" (how to process it), allowing attackers to embed commands in data fields.
  - **Quick check question:** If a user submits a resume containing the text "Ignore previous instructions," might an LLM summarizer prioritize this new command over the summarization task?

- **Concept:** **False Negatives in Security (Type I vs. Type II Errors)**
  - **Why needed here:** The paper distinguishes between standard classification errors (Type I) and structural failures where the detector's logic is subverted (Type II). Understanding this difference is key to grasping why fine-tuning fails.
  - **Quick check question:** If a security system checks for a specific "safe" signal, is it sufficient to verify the signal exists, or must you verify *why* it was generated?

## Architecture Onboarding

- **Component map:**
  - Target Task ($s_t, x_t$) -> Detection LLM ($g$) -> Backend LLM ($f$) -> User Response
  - Detection Instruction ($s_d(k)$) is concatenated with user data before detection

- **Critical path:**
  1. Input data ($x$) is received
  2. **Detection Phase:** Concatenate $s_d(k)$ with $x$. Query $g$
  3. **Gate:** Check if output $\neq k$. If true, **Block**
  4. **Production Phase:** If output $= k$, query Backend LLM $f$ with $s_t || x$
  5. **Failure Point:** In DataFlip, the input $x$ is crafted so $g$ outputs $k$ (passing gate), but $f$ executes $s_e$ (the attack)

- **Design tradeoffs:**
  - **Strong KAD (Fine-tuning):** Increases the model's propensity to follow instructions. This improves detection of naive attacks that ignore the detection instruction but *increases* vulnerability to adaptive attacks that leverage instruction following to return the key
  - **Standard KAD:** Lower susceptibility to adaptive logic but higher false positives/negatives on standard attacks

- **Failure signatures:**
  - **Type I Failure:** Detection LLM ignores the injected task and returns the key anyway (false negative/low robustness)
  - **Type II Failure (DataFlip):** Detection LLM follows the injected task, which *contains* logic to return the key (structural bypass)

- **First 3 experiments:**
  1. **Baseline Verification:** Implement the KAD setup with a standard (unhardened) LLM. Verify that basic attacks (e.g., "Ignore previous instructions") trigger a detection (key is missing)
  2. **Adaptive Attack Test:** Apply the DataFlip template (IF/ELSE logic) against the baseline. Observe if the extraction rate of the key rises near 98% as reported
  3. **Fine-tuning Paradox Check:** Compare a "Strong KAD" model (fine-tuned for susceptibility) against the baseline using DataFlip. Verify if the detection rate drops to 0% or near-zero, confirming the tradeoff

## Open Questions the Paper Calls Out

- **Open Question 1:** Can optimization-based triggers be effectively integrated with adaptive control-flow attacks to increase attack success rates?
  - **Basis in paper:** [Explicit] Appendix A.2 states that incorporating optimization-based triggers (e.g., from Universal or NeuralExec attacks) could "strengthen" attacks by tailoring patterns to selectively trigger specific branches
  - **Why unresolved:** The DataFlip attack presented in the paper relies solely on handcrafted IF/ELSE templates without optimization procedures
  - **What evidence would resolve it:** Empirical results showing improved Attack Success Values (ASV) or Extraction Rates when optimization algorithms are applied to the adaptive template structure

- **Open Question 2:** Can interpretability-based analysis of internal model states provide a robust defense against structural prompt injection vulnerabilities?
  - **Basis in paper:** [Explicit] Appendix A.3 suggests leveraging interpretability tools like attention analysis or influence tracing to create defenses based on the "internal reasoning process" rather than input-output behavior
  - **Why unresolved:** Current defenses like KAD are flawed because they rely on observable outputs; it is unknown if analyzing attention patterns can reliably distinguish between legitimate and injected instructions
  - **What evidence would resolve it:** A detection mechanism that successfully identifies contaminated inputs by analyzing attention maps, maintaining robustness against semantic variants of the DataFlip attack

- **Open Question 3:** Is it possible to generalize adversarial training to cover the unbounded semantic variants of conditional prompt injections?
  - **Basis in paper:** [Explicit] Appendix A.1 notes that while adversarial training helps, "generalizing across this space is inherently hard" due to the "practically unbounded set of semantically equivalent injected prompts"
  - **Why unresolved:** Fine-tuning effectively reduces false negatives for known patterns but fails against the structural flexibility of control-flow attacks
  - **What evidence would resolve it:** A detection model that, after adversarial training, maintains a high detection rate against unseen semantic variations of IF/ELSE prompt structures

## Limitations

- **Implementation specificity:** The attack's success depends critically on exact prompt templates for the 7 tasks, which are not fully specified in the paper
- **Fine-tuning methodology ambiguity:** The specific fine-tuning procedure for creating DataSentinel's detector_large model is not detailed, making it unclear if the vulnerability generalizes beyond this particular implementation
- **Detection threshold sensitivity:** The binary detection assumption (key present = clean) may not reflect real-world implementations with more nuanced matching or confidence thresholds

## Confidence

**High Confidence:** The core mechanism of DataFlip exploiting the structural vulnerability in KAD defenses is well-supported. The IF/ELSE control flow approach to simultaneously extracting the secret key and executing injected instructions represents a sound logical attack pattern that directly challenges KAD's sequential processing assumption.

**Medium Confidence:** The claim that fine-tuning detection LLMs for increased instruction susceptibility exacerbates vulnerability is supported by comparative results, but the specific magnitude of the tradeoff (83% gap for Grammar Correction) may be sensitive to implementation details and the particular fine-tuning methodology used.

**Low Confidence:** The generalization of these findings to other KAD implementations beyond DataSentinel's specific fine-tuned model. While the structural vulnerability appears fundamental, the attack's effectiveness against alternative detection mechanisms (different fine-tuning approaches, non-LLM detectors) remains unclear.

## Next Checks

1. **Prompt Template Reconstruction:** Attempt to reconstruct the exact prompt templates for all 7 tasks using the DataSentinel evaluation framework, then verify whether the reported 91% attack success rate is reproducible. This addresses the uncertainty about task-specific implementation details.

2. **Parameter Sensitivity Analysis:** Systematically vary backend LLM generation parameters (temperature, top_p, max_tokens) and measure their impact on Attack Success Value. This would clarify whether the reported success rates are robust to common implementation variations.

3. **Alternative Detection Mechanisms:** Implement a variant of KAD using static analysis or rule-based detection instead of fine-tuned LLM classification, then test whether DataFlip's approach still succeeds. This would validate whether the vulnerability is specific to the fine-tuned LLM approach or represents a more fundamental flaw in KAD architecture.