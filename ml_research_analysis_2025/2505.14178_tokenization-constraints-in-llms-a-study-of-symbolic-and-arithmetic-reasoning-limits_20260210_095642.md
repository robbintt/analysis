---
ver: rpa2
title: 'Tokenization Constraints in LLMs: A Study of Symbolic and Arithmetic Reasoning
  Limits'
arxiv_id: '2505.14178'
source_url: https://arxiv.org/abs/2505.14178
tags:
- counter
- step
- string
- found
- counting
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This work shows that tokenization granularity fundamentally limits\
  \ transformer models\u2019 symbolic and arithmetic reasoning ability, even when\
  \ using Chain-of-Thought prompting. By systematically evaluating arithmetic counting\
  \ and symbolic tasks, the authors demonstrate that merged tokens in BPE obscure\
  \ atomic reasoning units, causing performance degradation up to 80%."
---

# Tokenization Constraints in LLMs: A Study of Symbolic and Arithmetic Reasoning Limits

## Quick Facts
- arXiv ID: 2505.14178
- Source URL: https://arxiv.org/abs/2505.14178
- Authors: Xiang Zhang; Juntai Cao; Jiaqi Wei; Yiwei Xu; Chenyu You
- Reference count: 40
- Primary result: Merged tokens in BPE tokenization obscure atomic reasoning units, causing performance degradation up to 80% even with CoT prompting

## Executive Summary
This paper reveals that tokenization granularity fundamentally limits transformer models' symbolic and arithmetic reasoning ability, even when using Chain-of-Thought prompting. By systematically evaluating arithmetic counting and symbolic tasks, the authors demonstrate that merged tokens in BPE obscure atomic reasoning units, causing performance degradation up to 80%. Atomic-aligned tokenization dramatically improves results, enabling small models to outperform larger ones in structured reasoning. The proposed Token Awareness metric formalizes how poor token granularity disrupts logical alignment and generalization.

## Method Summary
The authors systematically evaluate arithmetic counting and symbolic reasoning tasks across different tokenization formats using standard BPE versus atomic-aligned representations. They conduct controlled experiments measuring accuracy differences (∆_tok) when switching from raw BPE inputs to delimiter-separated atomic-aligned formats. The study employs a formal Token Awareness function to quantify how token granularity affects access to task-relevant properties, and tests multiple model families (GPT-4o, Claude, Qwen) with consistent CoT prompts across all conditions.

## Key Results
- Switching from raw BPE inputs to atomic-aligned inputs improves accuracy by 54.1% for counting 'a' in strings
- Performance degradation up to 80% occurs when merged tokens obscure atomic reasoning units
- Low-frequency letters (e.g., 'z': 0.07%) show 3-14% accuracy advantage over high-frequency letters (e.g., 'e': 12.7%) in counting tasks
- Small models with atomic-aligned tokenization can outperform larger models with merged BPE tokens

## Why This Works (Mechanism)

### Mechanism 1: Token Granularity Obscures Atomic Reasoning Units
- Claim: BPE tokenization merges characters into opaque tokens, preventing models from accessing fine-grained properties needed for symbolic reasoning.
- Mechanism: When multiple characters are merged (e.g., "Strawberry" → ["Straw", "berry"]), the model's embedding layer cannot decompose the token to recover sub-token structure. The Token Awareness function—defined as TokenAware(ti, prop) := I[prop ∈ Emb(ti)]—returns 0 for properties like "character count" within merged tokens.
- Core assumption: Models cannot learn to extract sub-token character information from token embeddings trained on semantic tasks.
- Evidence anchors:
  - [abstract]: "merged tokens in BPE obscure atomic reasoning units, causing performance degradation up to 80%"
  - [section 5.2]: Defines Token Awareness formally; when TokenAware(ti, prop) = 0, "reasoning that relies on prop will fail"
  - [corpus]: Related work (Modular Arithmetic paper, FMR=0.61) confirms LLMs represent numbers digit-by-digit, supporting granularity importance
- Break condition: If models were trained with explicit character-level supervision or used character-aware embeddings.

### Mechanism 2: CoT Fidelity is Bounded by Token Vocabulary Expressiveness
- Claim: Chain-of-Thought's theoretical Turing completeness assumes unlimited token expressiveness; real vocabularies create a bottleneck.
- Mechanism: CoT simulates recurrence by externalizing hidden states h_{t-1} → o_{1:k} → h_t. The fidelity of this loop is bounded: Fidelity(CoT) ≤ |φ(S_h) ∩ S_o| / |S_h|. When vocabulary V is coarse, many latent states are untranslatable, forcing truncated or approximated reasoning.
- Core assumption: The theoretical guarantees for CoT (from Li et al. 2024, Zhang et al. 2024) assume ideal token-to-state mapping that doesn't exist in practice.
- Evidence anchors:
  - [abstract]: "success of such reasoning is fundamentally bounded by the structure of tokenized inputs"
  - [section 5.3-5.4]: Formalizes expressiveness bottleneck; "low expressiveness ⇒ low overlap ⇒ critical reasoning steps cannot be externalized"
  - [corpus]: Related papers on CoT efficiency (ReEfBench, ImgCoT) do not directly address tokenization as bottleneck; corpus evidence is weak here
- Break condition: If models used dynamic vocabulary expansion or latent CoT tokens.

### Mechanism 3: Task-Token Alignment Enables or Blocks Generalization
- Claim: When token boundaries align with task-relevant atomic units, models apply symbolic operations correctly; misalignment forces memorization.
- Mechanism: Delimiter-separated formats (types b–d) force tokens to align with characters, allowing attention and operations at proper resolution. Without alignment, models cannot increment counters or compare elements they cannot "see."
- Core assumption: Symbolic operations occur at token granularity; models do not implicitly decompose tokens during computation.
- Evidence anchors:
  - [section 6.2]: "Switching from raw BPE inputs (type a) to atomic-aligned inputs (type d) improves accuracy by ∆_tok = 54.1% for counting 'a'"
  - [tables 1-4]: Consistent ∆_tok values of 40–80% across tasks, controlled for task semantics
  - [corpus]: Related work on symbolic-aided CoT (FMR=0.0, newly released) supports structure-sensitive reasoning
- Break condition: If training data contained extensive character-level reasoning with mixed tokenization.

## Foundational Learning

- Concept: **BPE (Byte-Pair Encoding) Tokenization**
  - Why needed here: BPE's compression objective (merging frequent substrings) directly conflicts with symbolic tasks requiring access to individual characters/digits.
  - Quick check question: Given "abaaab," explain how BPE might merge this differently than character-level tokenization, and why that matters for counting 'a's.

- Concept: **Transformer Depth Constraints (TC0 Complexity)**
  - Why needed here: The paper builds on theoretical results that transformers have O(1) depth per token, motivating why CoT is necessary for arithmetic.
  - Quick check question: Why can't a 96-layer transformer count characters in a 1000-character string without generating intermediate tokens?

- Concept: **CoT as Recurrence Simulation**
  - Why needed here: The paper's core thesis is that CoT's recurrence simulation is bounded by token expressiveness, not just architecture.
  - Quick check question: In the cycle h_{t-1} → o_{1:k} → h_t, what happens if o_{1:k} cannot faithfully encode the counter state?

## Architecture Onboarding

- Component map: Tokenizer (BPE) -> Embedding layer -> Transformer backbone -> CoT output loop
- Critical path: Input string → Tokenization (damage type I) → Embedding → Transformer layers → CoT token generation (damage type II: bottleneck) → Re-embedding → Continue until answer
- Design tradeoffs:
  - Compression vs. granularity: BPE reduces sequence length but obscures atomic units
  - Vocabulary size vs. coverage: Larger vocab helps expressiveness but increases parameters
  - Natural vs. delimiter-separated format: Adding spaces/commas helps tokenization but changes task presentation
- Failure signatures:
  - Systematic undercounting with merged tokens (Figure 4: negative error shifts dominant)
  - Large accuracy variance (40–80%) across tokenization formats for identical tasks
  - Correct CoT steps but wrong final counts (model cannot "see" characters within merged tokens)
- First 3 experiments:
  1. Replicate counting task: Use identical letter strings in all four formats (a–d); measure ∆_tok on your model
  2. Letter frequency test: Compare counting accuracy for rare (z: 0.07%) vs. common (e: 12.7%) letters; expect 6–14% advantage for rare
  3. Supervised vs. unsupervised CoT: Control for step template quality; isolate tokenization effects by using identical prompts across formats

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do tokenization-induced reasoning failures scale with extreme context lengths (e.g., >100 tokens) during Chain-of-Thought (CoT) generation?
- Basis in paper: [explicit] The authors explicitly state in the Limitations section that they did not explore extreme context lengths (counting instances with more than several hundred tokens) due to instability in long CoT steps, noting an intent to investigate this aspect in future work.
- Why unresolved: Experiments were restricted to moderate lengths (up to 40-100 characters) because longer CoT steps currently lead to generation instability and retrieval issues in existing models.
- What evidence would resolve it: Empirical results from counting and sorting tasks performed on input sequences extending several hundred tokens, specifically analyzing error accumulation rates in atomic-aligned vs. merged token formats.

### Open Question 2
- Question: Do the token granularity constraints identified in proprietary models (GPT-4o, Claude) generalize to open-source decoder architectures like LLaMA and Mistral?
- Basis in paper: [explicit] The authors note in the Limitations section that testing was not extended to open-source LLMs like LLaMA or Mistral due to budget constraints and weaker instruction-following capabilities, despite the assumption that findings are likely universal.
- Why unresolved: The study focused on API-based models (GPT, Claude, Qwen) that exhibit strong instruction adherence, leaving the behavior of local, open-source models under identical token manipulations unverified.
- What evidence would resolve it: A replication of the symbolic counting and sorting experiments using LLaMA 3 or Mistral on the same atomic-aligned (type d) and merged-token (type a) inputs to compare $\Delta_{tok}$ scores.

### Open Question 3
- Question: Does the inverse relationship between token frequency and counting accuracy stem from reduced "semantic noise" in the embeddings of low-frequency tokens?
- Basis in paper: [inferred] The appendix notes that low-frequency letters (e.g., 'z') are counted more accurately than high-frequency ones (e.g., 'e'), hypothesizing that rare tokens carry less linguistic information and cause less distraction during attention, but this mechanism is not proven.
- Why unresolved: The paper quantifies the performance difference (3-14% accuracy gain for rare letters) but relies on a hypothesis regarding the internal embedding state rather than a mechanistic interpretability analysis.
- What evidence would resolve it: Attention head visualization or probing experiments that measure interference levels when processing high-frequency semantic tokens versus low-frequency symbolic tokens in isolation.

## Limitations

- Limited generalization to complex symbolic reasoning domains beyond simple counting and arithmetic tasks
- Focus on transformer-based models with BPE tokenization may not capture limitations in alternative architectures or tokenization schemes
- Delimiter-separated formats that improve performance also fundamentally alter task presentation, making it unclear whether improvements stem purely from better tokenization granularity

## Confidence

- **High Confidence (Mechanism 1: Token Granularity)**: The formal definition of Token Awareness and its mathematical relationship to reasoning failure is well-grounded. The empirical evidence across multiple tasks and models showing consistent performance degradation with merged tokens provides strong support.
- **Medium Confidence (Mechanism 2: CoT Fidelity Bounds)**: The theoretical framework for expressing CoT fidelity as a function of vocabulary expressiveness is sound, but the empirical validation is limited. The paper demonstrates correlation but doesn't conclusively prove that token expressiveness is the primary bottleneck versus other factors like attention limitations or model capacity.
- **Medium Confidence (Mechanism 3: Task-Token Alignment)**: The experimental design effectively controls for task semantics while varying tokenization, providing convincing evidence for the alignment hypothesis. However, the results may be specific to the particular task types studied and may not generalize to arbitrary symbolic reasoning tasks.

## Next Checks

1. Cross-Architecture Validation: Replicate the counting and symbolic reasoning experiments using models with different tokenization schemes (WordPiece, SentencePiece, character-level) and architectures (RNN-based, hybrid) to determine whether the observed limitations are specific to BPE-transformer combinations or represent a more fundamental constraint.

2. Complexity Gradient Testing: Extend beyond simple counting to more complex symbolic operations (nested parentheses validation, arithmetic expression evaluation with operator precedence, logical formula manipulation) to establish whether token granularity effects scale predictably with task complexity or exhibit threshold behaviors.

3. Dynamic Tokenization Intervention: Implement an adaptive tokenization layer that can split merged tokens during reasoning steps, then measure whether this intervention specifically rescues the symbolic reasoning failures while maintaining efficiency for non-symbolic tasks. This would test whether the core issue is permanent token embedding limitations versus temporary information access constraints.