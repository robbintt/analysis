---
ver: rpa2
title: 'AgoraSpeech: A multi-annotated comprehensive dataset of political discourse
  through the lens of humans and AI'
arxiv_id: '2501.06265'
source_url: https://arxiv.org/abs/2501.06265
tags:
- political
- chatgpt
- speeches
- dataset
- sentiment
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'AgoraSpeech is a multi-annotated dataset of 171 Greek political
  speeches from the 2023 national elections, encompassing 5,279 paragraphs and 31,674
  annotations. The dataset includes six NLP tasks: text classification, topic identification,
  sentiment analysis, named entity recognition, polarization, and populism detection.'
---

# AgoraSpeech: A multi-annotated comprehensive dataset of political discourse through the lens of humans and AI

## Quick Facts
- arXiv ID: 2501.06265
- Source URL: https://arxiv.org/abs/2501.06265
- Reference count: 36
- 171 Greek political speeches with 5,279 paragraphs and 31,674 annotations across six NLP tasks

## Executive Summary
AgoraSpeech is a multi-annotated dataset of 171 Greek political speeches from the 2023 national elections, featuring 5,279 paragraphs and 31,674 annotations. The dataset employs a hybrid intelligence annotation pipeline, combining ChatGPT-generated annotations with exhaustive human-in-the-loop validation by journalists and political scientists. This approach enables high-quality annotations for six NLP tasks: text classification, topic identification, sentiment analysis, named entity recognition, polarization, and populism detection. The dataset demonstrates strong performance in sentiment (93%) and text classification (89%) tasks, while revealing challenges in topic classification (61% accuracy).

## Method Summary
The methodology employs a two-step hybrid intelligence annotation pipeline. First, ChatGPT (gpt-3.5-turbo) generates annotations for six NLP tasks using task-specific prompts that include context like speaker identity and expert-crafted definitions. Second, human experts (journalists and political scientists) validate these annotations against the original Greek texts, catching nuanced errors and generating ground truth labels. Speeches are collected from party sources and media, transcribed from audio/video, segmented into paragraphs, and translated from Greek to English using DeepL API. The final dataset contains 5,279 paragraphs with 20 features per paragraph, stored on Zenodo.

## Key Results
- ChatGPT achieved 93% accuracy on sentiment analysis and 89% accuracy on text classification when validated against human experts
- Topic classification with 33 categories achieved 61% accuracy, indicating the inherent difficulty of mapping diverse political discourse to predefined categories
- The dataset provides a robust benchmark for analyzing political discourse, revealing patterns in rhetorical strategies and ideological positioning across Greek political parties

## Why This Works (Mechanism)

### Mechanism 1: Hybrid Intelligence Annotation Pipeline
A two-step annotation process (LLM generation + expert validation) produces high-quality labels more efficiently than purely manual annotation. ChatGPT generates initial annotations at scale; human experts review and correct, catching nuanced errors while avoiding exhaustive from-scratch labeling. This works because LLM annotations are sufficiently accurate to serve as a useful starting point, reducing but not eliminating human effort. Break condition: If LLM accuracy drops significantly (<50%) on a task, the correction burden may exceed the effort of manual annotation from scratch.

### Mechanism 2: Context-Enriched Prompting for Bias Mitigation
Supplementing prompts with domain-specific context (speaker identity, expert definitions) improves annotation accuracy and reduces model bias. Prompts include speaker/party names for topic classification but deliberately omit them for polarization/populism to avoid stereotypical associations. This works because ChatGPT's pre-training includes political biases that can be partially counteracted through careful prompt design. Break condition: If prompts become overly specific or leading, they may introduce new biases or constrain model outputs inappropriately.

### Mechanism 3: Translation-Mediated Cross-Lingual Processing
Translating non-English texts to English for LLM annotation, then validating against original texts, enables processing of under-resourced languages. Greek speeches are translated to English via DeepL; ChatGPT annotates English text; human validators compare against original Greek to catch translation-induced errors. This works because translation quality is sufficient to preserve semantic content relevant to the annotation tasks. Break condition: If sarcasm, intonation, or culturally-specific rhetoric is central to the analysis, translation may lose critical signal before annotation.

## Foundational Learning

- **Concept: Human-in-the-loop (HITL) annotation**
  - Why needed here: LLM annotations are imperfect, especially for subjective, domain-specific tasks like populism detection; expert review ensures quality and provides ground truth for benchmarking.
  - Quick check question: Can you identify at least one NLP task where ChatGPT's accuracy (88-93%) was comparable to a dummy classifier, suggesting human validation was essential?

- **Concept: Multi-task annotation heterogeneity**
  - Why needed here: Different NLP tasks have varying difficulty and subjectivity; sentiment (93% accuracy) is more tractable than topic classification with 33 categories (61% accuracy).
  - Quick check question: Why might topic classification with 33 predefined categories be inherently harder than binary text classification (criticism vs. agenda)?

- **Concept: Political discourse annotation subjectivity**
  - Why needed here: Concepts like polarization and populism lack universal definitions; the paper uses expert-crafted definitions to standardize annotations, but subjectivity remains.
  - Quick check question: How might different cultural or disciplinary backgrounds lead annotators to disagree on whether a paragraph is "populist"?

## Architecture Onboarding

- **Component map:** Speech collection -> Transcription -> Paragraph segmentation -> Greek-to-English translation -> ChatGPT annotation -> Human validation -> CSV dataset
- **Critical path:** Paragraph-level segmentation impacts all downstream annotations; prompt design directly affects annotation quality; human validation catches errors and generates ground truth
- **Design tradeoffs:** Paragraph-level vs. speech-level analysis enables fine-grained annotation but loses broader rhetorical context; English annotation enabled use of gpt-3.5-turbo but introduced translation artifacts; expert validators ensure quality but are less scalable than crowd workers
- **Failure signatures:** High "Other" category in topic classification suggests prompt/list may be incomplete; missing values in sentiment analysis indicate unreliable ChatGPT outputs; ChatGPT overestimates populism levels compared to human experts
- **First 3 experiments:**
  1. Reproduce ChatGPT vs. human agreement rates by comparing `sentiment_chatgpt` and `sentiment_human` columns
  2. Analyze error patterns by politician by filtering disagreements and checking specific speakers
  3. Test alternative prompts by modifying 50 paragraphs and comparing accuracy to original ChatGPT outputs

## Open Questions the Paper Calls Out

- **Context Dependency:** Does incorporating broader speech context significantly improve the accuracy of populism and polarization detection compared to the paragraph-level segmentation used?
- **Native Processing:** Can native Greek language processing by modern LLMs outperform the translation-based pipeline (Greek-to-English) employed for the dataset's creation?
- **Acoustic Features:** To what extent does the absence of acoustic features limit the reliability of sentiment analysis in this dataset, particularly regarding sarcasm?

## Limitations

- ChatGPT model was not proficient in Greek, necessitating translation that may lose meaning, nuance, or local language intricacies
- Breaking text into paragraphs can obscure nuances for complex tasks like populism detection that rely heavily on broader speech context
- The dataset relies solely on transcriptions, completely stripping prosodic information that might define true sentiment, potentially leading to misclassification

## Confidence

- **High Confidence:** Sentiment analysis (93%) and text classification (89%) accuracy rates, as these binary/categorical tasks showed strong ChatGPT-human agreement
- **Medium Confidence:** Topic classification (61%) accuracy, which reflects the inherent difficulty of mapping diverse political discourse to 33 predefined categories
- **Low Confidence:** The complete robustness of the hybrid annotation pipeline, as the paper doesn't fully specify the polarization/populism definitions or provide detailed inter-annotator reliability metrics

## Next Checks

1. Reproduce accuracy benchmarks by loading the dataset and recalculating ChatGPT vs. human agreement rates across all six tasks
2. Examine cases where ChatGPT and human annotations disagree, focusing on whether specific speakers or political parties show systematic disagreement patterns
3. Test alternative prompt variations on a subset of 50-100 paragraphs to quantify how sensitive ChatGPT's outputs are to prompt design choices