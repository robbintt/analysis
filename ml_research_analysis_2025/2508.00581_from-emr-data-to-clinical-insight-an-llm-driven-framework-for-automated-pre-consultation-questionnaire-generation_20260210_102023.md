---
ver: rpa2
title: 'From EMR Data to Clinical Insight: An LLM-Driven Framework for Automated Pre-Consultation
  Questionnaire Generation'
arxiv_id: '2508.00581'
source_url: https://arxiv.org/abs/2508.00581
tags:
- questionnaire
- generation
- pre-consultation
- clinical
- medical
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a multi-stage LLM-driven framework to automate
  pre-consultation questionnaire generation from EMRs. The method extracts atomic
  assertions from EMRs, constructs personal causal networks, and synthesizes disease
  knowledge via network clustering.
---

# From EMR Data to Clinical Insight: An LLM-Driven Framework for Automated Pre-Consultation Questionnaire Generation

## Quick Facts
- arXiv ID: 2508.00581
- Source URL: https://arxiv.org/abs/2508.00581
- Authors: Ruiqing Ding; Qianfang Sun; Yongkang Leng; Hui Yin; Xiaojian Li
- Reference count: 4
- Key outcome: 84.2% personal key fact coverage, 92.2% disease-specific coverage, 8.5/10 diagnostic relevance, 10.4 min generation time (vs 33.8 min manual)

## Executive Summary
This paper introduces a multi-stage LLM-driven framework that automates pre-consultation questionnaire generation from unstructured electronic medical records (EMRs). The method extracts atomic assertions from EMRs, constructs personal causal networks, and synthesizes disease knowledge via network clustering. Using this structured intermediate representation, the framework generates both personal and disease-specific questionnaires. Evaluated on 3,000 EMRs, the approach achieves significantly higher coverage and diagnostic relevance compared to direct LLM generation while reducing generation time from 33.8 to 10.4 minutes.

## Method Summary
The framework processes EMR text through three stages: (1) LLM-based extraction of atomic assertions with temporal tagging, (2) construction of personal causal networks and synthesis of disease knowledge via hierarchical clustering of edge embeddings, and (3) generation of questionnaires using structured inputs. The system uses GPT-4o for LLM tasks, BCEmbedding for edge representation, and hierarchical clustering with average linkage to identify representative disease patterns. The approach separates fact extraction from reasoning to mitigate information loss and improve completeness.

## Key Results
- 84.2% personal key fact coverage (vs 42.1% for direct LLM)
- 92.2% disease-specific coverage
- Diagnostic relevance score: 8.5/10
- Understandability score: 9.1/10
- Generation time reduced from 33.8 to 10.4 minutes

## Why This Works (Mechanism)

### Mechanism 1: Semantic Decomposition for Context Preservation
The framework forces the LLM to strictly decompose unstructured EMR text into "semantic minimality" units (atomic assertions) with explicit temporal tagging. This acts as a state-preserving intermediate layer, separating fact extraction from reasoning, which prevents the "lost in the middle" phenomenon where LLMs overlook details in long contexts.

### Mechanism 2: Graph-Based Knowledge Synthesis via Clustering
The system constructs personal causal graphs and calculates similarity using edge embeddings. By clustering these graphs, the framework filters individual patient idiosyncrasies and isolates "representative causal networks" (centroids), effectively creating a data-driven disease ontology.

### Mechanism 3: Structured Context Injection for Questionnaire Generation
Instead of asking the LLM to "read" a raw EMR and "write" a questionnaire, the framework provides a "scaffold" (the causal network) and a "checklist" (atomic assertions). This reduces the cognitive load on the LLM, allowing it to focus on phrasing and relevance rather than fact retrieval.

## Foundational Learning

- **Concept: Atomic Assertions (Semantic Minimalism)**
  - Why needed here: The entire architecture depends on the quality of Stage 1. If you cannot break a complex sentence like "Patient has fever and cough" into separate temporal nodes, the causal network will be structurally flawed.
  - Quick check question: Given the phrase "Patient reports 3-day history of chest pain worsening on inspiration," what are the two atomic assertions and their relative times?

- **Concept: Graph Embedding & Similarity**
  - Why needed here: Stage 2 requires clustering graphs. Understanding that the paper embeds *edges* (cause-effect pairs) rather than just nodes is critical to replicating the similarity logic.
  - Quick check question: Why does the paper concatenate embeddings of the cause and effect nodes to represent an edge, rather than just embedding the edge text directly?

- **Concept: Hierarchical Clustering**
  - Why needed here: The system needs to identify "representative" disease pathways from a heterogeneous mix of patient histories.
  - Quick check question: In the context of this paper, what does the "weight" of a cluster represent regarding a specific disease?

## Architecture Onboarding

- **Component map:** Raw EMR text (JSON/Text) -> Stage 1 (Extraction) -> Atomic Assertions (JSONL) -> Stage 2 (Graphing) -> Personal Causal Networks + Disease Knowledge -> Stage 3 (Generation) -> Questionnaire
- **Critical path:** The transition from Stage 1 to Stage 2 is the bottleneck. If the atomic assertions lack temporal data, the causal network construction prompt will fail to establish temporal dependencies.
- **Design tradeoffs:** Latency vs. Accuracy (multi-stage takes longer but achieves >80% coverage); General vs. Specific (maintains Personal vs. Disease paths based on user intent).
- **Failure signatures:** "Generic" Loop (standard questions despite specific EMR facts), "Disconnected" Graph (nodes but no edges), Token Overflow (crash on large EMRs).
- **First 3 experiments:** 1) Unit Test Stage 1: Verify atomic output and temporal data preservation. 2) Edge Validity Test: Check clinical sense of generated causal edges. 3) A/B Test Generation: Compare Direct LLM vs Stage 3 questionnaire coverage.

## Open Questions the Paper Calls Out

- How does the framework's performance generalize across multi-center, linguistically diverse EMR corpora compared to the single-hospital dataset utilized? [explicit]
- To what extent can extraction robustness be maintained when replacing proprietary models (GPT-4o) with smaller, domain-specific open-source LLMs? [explicit]
- Does the deployment of these automated questionnaires significantly reduce actual consultation duration and improve diagnostic yield in a live clinical workflow? [inferred]

## Limitations

- Framework effectiveness depends on LLM accuracy for atomic assertion extraction and causal inference, which may vary across clinical domains
- Clustering approach assumes patient histories will converge into meaningful patterns, potentially problematic for rare diseases
- Evaluation metrics focus on coverage and relevance but not clinical accuracy of generated questionnaires
- Heavy reliance on GPT-4o creates reproducibility barriers and potential cost-effectiveness concerns

## Confidence

- High Confidence: Multi-stage framework architecture and component separation effectively addresses "lost in the middle" problem
- Medium Confidence: 84.2% personal key fact coverage and 92.2% disease-specific coverage supported by evaluation methodology
- Medium Confidence: Diagnostic relevance (8.5/10) and understandability (9.1/10) scores based on expert evaluation
- Low Confidence: 10.4-minute generation time improvement, though baseline manual time not independently verified

## Next Checks

1. **Temporal Assertion Verification:** Manually review 50 randomly selected atomic assertions from Stage 1 output to verify atomicity and correct temporal data capture.

2. **Causal Edge Validation:** For 10 EMRs processed through the full pipeline, manually verify a random sample of 20 causal edges to confirm they are grounded in source text rather than hallucinated.

3. **Expert Clinical Utility Test:** Have 3-5 clinicians independently evaluate clinical utility of generated questionnaires in actual pre-consultation scenarios, measuring impact on diagnostic decisions.