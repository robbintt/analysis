---
ver: rpa2
title: 'ThinkDial: An Open Recipe for Controlling Reasoning Effort in Large Language
  Models'
arxiv_id: '2508.18773'
source_url: https://arxiv.org/abs/2508.18773
tags:
- reasoning
- mode
- training
- performance
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ThinkDial, the first open-source framework
  that successfully replicates the controllable reasoning capabilities of proprietary
  systems like OpenAI's GPT-4o series. The key innovation is an end-to-end training
  paradigm that integrates budget-mode control throughout the entire pipeline, from
  supervised fine-tuning through reinforcement learning.
---

# ThinkDial: An Open Recipe for Controlling Reasoning Effort in Large Language Models

## Quick Facts
- arXiv ID: 2508.18773
- Source URL: https://arxiv.org/abs/2508.18773
- Authors: Qianyu He; Siyu Yuan; Xuefeng Li; Mingxuan Wang; Jiangjie Chen
- Reference count: 40
- Primary result: First open-source framework replicating controllable reasoning capabilities of proprietary systems like GPT-4o, achieving target compression-performance trade-offs with 50-75% token reduction and <10-15% accuracy degradation

## Executive Summary
ThinkDial introduces an end-to-end training paradigm that successfully replicates the controllable reasoning capabilities of proprietary systems like OpenAI's GPT-4o series. The framework enables seamless switching between three reasoning modes (High, Medium, Low) with specific token reduction and performance degradation targets. Through Budget-Mode Supervised Fine-tuning combined with two-phase budget-aware reinforcement learning, ThinkDial achieves controllable reasoning while maintaining target performance thresholds across mathematical reasoning benchmarks.

## Method Summary
ThinkDial combines Budget-Mode Supervised Fine-tuning (BM-SFT) with a two-phase reinforcement learning approach. The BM-SFT phase constructs paired training data where the same problem has different reasoning depths across modes, teaching the model to associate prompt semantics with output distributions. Phase 1 RL trains exclusively on High-mode data to establish peak performance, while Phase 2 applies budget-aware reward shaping with mode-specific length coefficients. A leak penalty mechanism prevents reasoning content from migrating to answer sections during compression. The framework achieves 50% token reduction in Medium mode with <10% performance degradation and 75% token reduction in Low mode with <15% performance degradation.

## Key Results
- Achieves 50% token reduction in Medium mode with <10% accuracy degradation
- Achieves 75% token reduction in Low mode with <15% accuracy degradation
- Successfully replicates accuracy-token trade-off curves of proprietary systems
- Demonstrates generalization to out-of-distribution tasks (GPQA)

## Why This Works (Mechanism)

### Mechanism 1: Budget-Mode Supervised Fine-tuning (BM-SFT) for Mode Conditioning
Embedding controllable reasoning during SFT prevents mode interference during RL training by constructing paired training data where the same problem has different reasoning depths across modes. The model learns to associate prompt semantics with output distributions before RL begins, establishing stable mode-conditioned distributions.

### Mechanism 2: Two-Phase Reinforcement Learning with Performance-First Foundation
Establishing peak performance before introducing compression constraints preserves the model's reasoning ceiling. Phase 1 trains exclusively on High-mode data with standard RL objectives to reach optimal accuracy, then Phase 2 applies budget-aware reward shaping. This sequencing ensures compression is built atop—not instead of—peak capability.

### Mechanism 3: Leak Penalty to Prevent Reasoning Length Hacking
Models exploit compression objectives by pushing reasoning into answer sections unless explicitly penalized. A binary reward (+0.5 if no transition keywords in answer section, -0.5 if detected) prevents reinforcement of pre-existing leakage patterns from aggressive Low-mode SFT compression.

## Foundational Learning

- **Chain-of-Thought (CoT) Reasoning**
  - Why needed here: ThinkDial explicitly controls the length and depth of CoT reasoning chains
  - Quick check question: Can you explain why longer CoT does not always mean better accuracy, and what "overthinking" refers to in LLM reasoning?

- **Policy Gradient Methods (PPO/DAPO)**
  - Why needed here: The RL phase uses DAPO, a variant of PPO, requiring understanding of importance sampling ratios and advantage estimates
  - Quick check question: In Equation 2, what does the clip operation protect against, and why is the advantage estimate normalized within each group?

- **Reward Shaping in RL**
  - Why needed here: The core innovation is the composite reward R_task + α(m)·R_length + R_leak
  - Quick check question: Why might rewarding only short outputs lead to "reasoning length hacking," and how does the leak penalty address this?

## Architecture Onboarding

- **Component map**: Foundation Model (Qwen-2.5-Instruct-32B) -> BM-SFT Data Pipeline -> Phase-1 RL (Warm-up) -> Phase-2 RL (Budget-Aware) -> Reward Computation Module -> Inference Interface

- **Critical path**: 1) Construct BM-SFT data with balanced mode ratios (6K BM + 12K original recommended) 2) Train SFT with mode-conditioned prompts 3) Run Phase-1 RL on High-mode data until performance plateaus 4) Enable Phase-2 RL with length rewards and leak penalty 5) Validate on held-out benchmarks

- **Design tradeoffs**: More BM-SFT data → stronger mode differentiation but potential performance ceiling drop; Higher α for Low mode → more compression but greater risk of accuracy degradation; Leak penalty threshold → stricter detection reduces spillover but may over-constrain answer expressiveness

- **Failure signatures**: High-mode performance falls below pre-training peak → likely skipped warm-up or BM-SFT ratio too high; Total tokens increase despite compression objective → leak penalty not enforced; Mode distributions collapse → BM-SFT data may be insufficient or mode prompts not distinctive enough

- **First 3 experiments**: 1) Ablate BM-SFT: Train with only original reasoning data, then apply Phase-2 RL. Verify that modes interfere and High performance drops 2) Skip Warm-up: Start directly with budget-aware RL. Confirm performance degradation in High/Medium modes 3) Remove Leak Penalty: Run Phase-2 RL without R_leak. Measure thinking tokens, answer tokens, and total tokens

## Open Questions the Paper Calls Out
None identified in the provided content.

## Limitations
- Evaluation limited to mathematical reasoning tasks, raising questions about generalizability to other domains
- Framework relies on Qwen-2.5-32B base model, limiting architectural generalization
- Leak penalty mechanism is heuristic and may not capture all forms of reasoning length hacking

## Confidence
- **High Confidence**: Two-phase RL training paradigm (performance-first warm-up, then budget-aware compression) is well-supported by ablation studies
- **Medium Confidence**: BM-SFT data construction methodology produces stable mode-conditioned distributions, but relies on assumptions about data quality
- **Low Confidence**: Generalizability to non-mathematical reasoning domains and other model architectures remains unproven

## Next Checks
1. **Domain Generalization Test**: Apply ThinkDial to non-mathematical reasoning benchmarks (e.g., commonsense reasoning datasets like StrategyQA, code generation tasks like HumanEval) and measure whether the budget-mode control maintains the claimed 50%/75% token reduction with <10%/<15% accuracy degradation across these new domains.

2. **Architectural Transferability**: Implement ThinkDial on a different model family (e.g., Llama or Mistral) and conduct controlled experiments comparing the optimal BM-SFT data ratios and RL hyperparameters to those reported for Qwen-2.5-32B, documenting any systematic differences in mode conditioning stability or performance ceilings.

3. **Leakage Evolution Stress Test**: Design adversarial reasoning chains that intentionally obscure the thinking/answer boundary (e.g., through nested reasoning patterns or disguised transition phrases) and evaluate whether the current leak penalty mechanism detects and penalizes these sophisticated forms of reasoning length hacking, or whether the model learns to circumvent it while maintaining compression benefits.