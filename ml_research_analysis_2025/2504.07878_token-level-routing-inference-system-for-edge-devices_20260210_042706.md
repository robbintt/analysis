---
ver: rpa2
title: Token Level Routing Inference System for Edge Devices
arxiv_id: '2504.07878'
source_url: https://arxiv.org/abs/2504.07878
tags:
- inference
- routing
- large
- token
- system
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a token-level routing inference system designed
  for efficient deployment of large language models on edge devices. The system uses
  a small language model for on-device inference and selectively routes critical tokens
  to a cloud-based large language model, guided by a confidence-based MLP router.
---

# Token Level Routing Inference System for Edge Devices

## Quick Facts
- arXiv ID: 2504.07878
- Source URL: https://arxiv.org/abs/2504.07878
- Reference count: 5
- Primary result: 60% performance gain on CommonsenseQA using 0.5B model on M1 MacBook with under 7% tokens routed to cloud LLM

## Executive Summary
This paper presents a token-level routing inference system designed for efficient deployment of large language models on edge devices. The system uses a small language model for on-device inference and selectively routes critical tokens to a cloud-based large language model, guided by a confidence-based MLP router. The routing mechanism is integrated into a full client-server system compatible with edge deployment, utilizing ONNX for on-device inference and SGLang for server-side LLM serving. The system achieves significant performance improvements while maintaining low latency, with only a small fraction of tokens requiring cloud processing.

## Method Summary
The system implements token-level routing between an on-device small language model (SLM) and cloud-based large language model (LLM). An MLP router analyzes the SLM's last-layer hidden states at each decoding step, routing tokens to the LLM when confidence falls below a threshold. The system uses ONNX for edge inference with modified computation graphs to expose hidden states, while the server uses SGLang for LLM serving. The architecture supports both joint decoding and small-only modes, with custom JSON APIs for communication between components.

## Key Results
- 60% accuracy improvement on CommonsenseQA using only 0.5B model on M1 MacBook
- Under 7% of tokens routed to cloud LLM while recovering most quality gap
- Generation speed of approximately 4 tokens per second on M1 chip
- Low routing ratio maintains low latency while achieving high accuracy

## Why This Works (Mechanism)

### Mechanism 1: Confidence-Based Selective Token Routing
Routing only low-confidence tokens to the LLM can recover most of the quality gap between SLM and LLM while invoking the cloud model on <7% of tokens. An MLP classifier consumes the SLM's last-layer hidden state at each decoding step and emits a routing score. If the score falls below a user-specified threshold τ, the token is deferred to the cloud LLM; otherwise, the SLM continues locally. This assumes tokens where the SLM has low confidence are the primary drivers of downstream quality degradation.

### Mechanism 2: Stateful Multi-Round Prefill-Decode Alternation
Collaborative decoding requires interleaved prefill and decode phases within a single request, which defeats standard serving engine assumptions. When a token is routed to the LLM, the full context must be re-prefilled on the cloud. After LLM generation, the new token(s) are returned to the edge, and the SLM must re-prefilled the extended sequence before continuing. This repeats for each routing event, with latency overhead accumulating as routing frequency increases.

### Mechanism 3: Hidden State Extraction via ONNX Graph Modification
The routing decision requires access to internal model states not exposed by standard inference APIs. The system programmatically modifies the ONNX computation graph to register the last-layer hidden state as an additional output node. The MLP router then operates on this extracted representation without requiring a custom model retrain, assuming last-layer hidden states contain sufficient signal for confidence estimation.

## Foundational Learning

- **Autoregressive decoding with KV-cache**
  - Why needed here: The paper assumes familiarity with prefill vs. decode phases and why KV-cache reuse matters for throughput
  - Quick check question: Explain why re-prefilling the entire sequence after each LLM routing event increases latency

- **Confidence calibration in language models**
  - Why needed here: The router's effectiveness hinges on whether SLM confidence scores correlate with actual error likelihood
  - Quick check question: What happens if the SLM is systematically overconfident on tokens where it is actually wrong?

- **ONNX computation graphs and node export**
  - Why needed here: Implementing the system requires modifying model graphs to expose intermediate states
  - Quick check question: How would you locate the last-layer hidden state node in a model that lacks standardized naming?

## Architecture Onboarding

- **Component map:**
  - Edge device: Qwen2.5-0.5B in ONNX format + MLP router (CPU inference)
  - Cloud server: Qwen2.5-32B on 2×A100 GPUs via SGLang (tensor parallelism)
  - Router: Threshold-gated MLP classifier consuming SLM hidden states
  - Communication: Custom JSON API carrying hidden states, token index, routing history, session metadata

- **Critical path:**
  1. User prompt → SLM prefill (TTFT ~0.5s on M1)
  2. For each token: extract hidden state → MLP router → decision
  3. If route: serialize state → HTTP POST → LLM prefill + decode → return token → SLM re-prefill → continue
  4. If local: SLM decode (~0.28s per token baseline, grows with routing events)

- **Design tradeoffs:**
  - Higher threshold → more routing → higher accuracy but latency increases (TBT can exceed 1s at τ=0.9)
  - Streaming vs. non-streaming: streaming provides perceived responsiveness but does not reduce total latency
  - ONNX CPU-only inference simulates worst-case edge devices; GPU-accelerated ONNX would improve SLM throughput

- **Failure signatures:**
  - Accuracy plateaus despite high routing ratios: router miscalibration or threshold misconfiguration
  - TBT grows unexpectedly: KV-cache not being reused; each routing event triggers full re-prefill
  - Automatic hidden-state node detection fails: non-standard layer naming requires manual graph inspection via Netron

- **First 3 experiments:**
  1. Baseline calibration: Run SLM-only (threshold ≤0.3) on 100 CommonsenseQA samples; measure accuracy and TBT. Confirm ~50% accuracy baseline matches paper.
  2. Routing sweep: Vary threshold from 0.4 to 0.9; plot accuracy vs. routing ratio and latency. Identify the operating point (threshold 0.7–0.8) that balances quality and throughput.
  3. Latency breakdown: Instrument code to log SLM inference time, network RTT, LLM inference time, and re-prefill overhead separately. Quantify which component dominates at different routing frequencies.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can efficient kv-cache management strategies be designed for token routing systems that involve multiple alternating prefill and decode stages?
- Basis in paper: The paper states that lack of kv-cache management "necessitates re-prefilling the entire sequence during each routing operation" and requires new strategies for efficient inference under this setting.
- Why unresolved: Current serving engines like SGLang and vLLM only optimize for single-stage prefill and decode, not the multiple alternating phases that token routing requires.
- What evidence would resolve it: Development and benchmarking of a kv-cache management system that maintains cache state across routing events without full re-prefilling, with latency measurements showing improvement.

### Open Question 2
- Question: Does the token routing approach generalize to tasks beyond commonsense reasoning, such as code generation, mathematical reasoning, or long-form text generation?
- Basis in paper: The evaluation is limited to CommonsenseQA, and the paper does not discuss whether the 60% improvement with 7% routing would transfer to tasks with different token dependency patterns.
- Why unresolved: CommonsenseQA involves relatively short sequences with discrete answer selection; tasks requiring longer coherent generation or different reasoning patterns may exhibit different optimal routing ratios.
- What evidence would resolve it: Systematic evaluation across diverse benchmark suites showing routing effectiveness and optimal threshold ranges per task type.

### Open Question 3
- Question: How does the router's confidence threshold need to adapt under varying network latency conditions to maintain quality-latency tradeoffs?
- Basis in paper: The paper assumes a fixed 170ms network delay and states that under more favorable network conditions the system is expected to exhibit significantly improved performance, but does not explore adaptive thresholding.
- Why unresolved: Real-world edge deployments face fluctuating network conditions; static thresholds cannot account for dynamic latency changes that affect the routing decision cost-benefit balance.
- What evidence would resolve it: Experiments measuring accuracy and user-perceived latency under simulated variable network conditions with adaptive threshold strategies compared to fixed thresholds.

## Limitations

- The system's effectiveness depends on accurate SLM confidence calibration, which may not generalize across different domains or model architectures
- Multi-round prefill-decode alternation introduces variable latency that grows with routing frequency, potentially making the system unusable for high-threshold settings
- ONNX graph modification for hidden state extraction may fail on non-standard model architectures, requiring manual intervention that breaks deployment workflow

## Confidence

- **High Confidence**: Architectural framework and implementation details are well-specified and reproducible. Quantitative results on CommonsenseQA are clearly presented.
- **Medium Confidence**: Routing mechanism's effectiveness depends on CITER router performance, which is referenced but not fully detailed. Accuracy claims rely on proper router calibration.
- **Low Confidence**: Latency characteristics under varying network conditions and system behavior at routing thresholds beyond reported range are not thoroughly characterized. Generalizability to other tasks and architectures remains speculative.

## Next Checks

1. **Router Calibration Validation**: Implement systematic evaluation of MLP router's confidence calibration across different threshold values on CommonsenseQA. Measure correlation between routing decisions and token-level accuracy improvements, identifying threshold range where routing provides net benefit versus unnecessary latency.

2. **KV-Cache Synchronization Protocol**: Instrument system to track KV-cache reuse patterns across routing events. Verify SLM properly maintains and reuses cached states when re-prefilling after LLM routing, quantifying overhead introduced by context transfer between edge and cloud components.

3. **Cross-Domain Generalization**: Test complete system on at least two additional tasks beyond CommonsenseQA (e.g., GSM8K for math reasoning, and text generation task). Compare routing ratios, accuracy improvements, and latency characteristics to determine whether 7% routing sweet spot and 60% accuracy gain are consistent across domains.