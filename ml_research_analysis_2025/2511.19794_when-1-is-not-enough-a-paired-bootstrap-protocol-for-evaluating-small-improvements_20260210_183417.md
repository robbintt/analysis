---
ver: rpa2
title: 'When +1% Is Not Enough: A Paired Bootstrap Protocol for Evaluating Small Improvements'
arxiv_id: '2511.19794'
source_url: https://arxiv.org/abs/2511.19794
tags:
- protocol
- small
- paired
- seeds
- bootstrap
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the challenge of determining when small improvements
  (1-2 percentage points) in machine learning benchmarks reflect real algorithmic
  advances versus noise, particularly under realistic compute budgets where only a
  few runs are affordable. The core method introduces a paired evaluation protocol
  that combines three elements: (1) paired multi-seed runs where baseline and variant
  models are trained under identical conditions for each seed, (2) bias-corrected
  and accelerated (BCa) bootstrap confidence intervals on per-seed deltas, and (3)
  sign-flip permutation tests on these deltas.'
---

# When +1% Is Not Enough: A Paired Bootstrap Protocol for Evaluating Small Improvements

## Quick Facts
- arXiv ID: 2511.19794
- Source URL: https://arxiv.org/abs/2511.19794
- Reference count: 19
- Primary result: A conservative paired evaluation protocol that combines BCa bootstrap confidence intervals and sign-flip permutation tests on per-seed deltas, designed to prevent false claims of small (1-2pp) improvements in ML benchmarks.

## Executive Summary
This paper addresses the challenge of determining whether small improvements (1-2 percentage points) in machine learning benchmarks reflect real algorithmic advances versus noise, particularly under realistic compute budgets where only a few runs are affordable. The core method introduces a paired evaluation protocol that combines paired multi-seed runs, bias-corrected and accelerated (BCa) bootstrap confidence intervals, and sign-flip permutation tests. The protocol is intentionally conservative, declaring significance only when the BCa interval lies entirely above zero AND the permutation p-value is below 0.05. Results from controlled experiments on CIFAR-10, CIFAR-10N, and AG News show that single runs and unpaired t-tests frequently report significant gains for 0.6-2.0 point improvements, but the paired protocol never declares significance in these settings, demonstrating its effectiveness as a guardrail against over-claiming small improvements.

## Method Summary
The protocol combines three elements: (1) paired multi-seed runs where baseline and variant models are trained under identical conditions for each seed, (2) bias-corrected and accelerated (BCa) bootstrap confidence intervals on per-seed deltas, and (3) sign-flip permutation tests on these deltas. The per-seed delta Δᵢ is computed as the difference between variant and baseline accuracy for each paired run. Significance is declared only when the BCa 95% confidence interval lies entirely above zero AND the permutation p-value is below 0.05. The method requires k paired seeds, with each seed training both baseline and variant models under identical random conditions (same initialization, data ordering, augmentation). BCa bootstrap adjusts for bias and skewness in the empirical distribution of deltas, while sign-flip permutation tests control false positives under weak distributional assumptions.

## Key Results
- Single runs and unpaired t-tests frequently report significant gains for 0.6-2.0 point improvements, especially on noisy labels and text
- With only three seeds, the paired protocol never declares significance in controlled experiments, even when BCa intervals are strictly positive
- The protocol effectively guards against false claims of small improvements while being computationally feasible (training two models per seed)
- Results demonstrate the protocol's effectiveness on CIFAR-10, CIFAR-10N (noisy labels), and AG News with synthetic no-improvement, small-gain, and medium-gain scenarios

## Why This Works (Mechanism)

### Mechanism 1: Paired design reduces variance in estimated improvement deltas compared to unpaired designs
- Claim: Pairing baseline and variant models under identical random seeds reduces variance in per-seed deltas
- Mechanism: By training both models under identical conditions, both experience correlated difficulty patterns. Since Var(X−Y) = Var(X) + Var(Y) − 2Cov(X,Y) and Cov(X,Y) > 0 in typical settings, the variance of per-seed deltas is lower than variance from independent runs
- Core assumption: Baseline and variant models share positively correlated performance across seeds
- Evidence anchors: [abstract] "paired multi-seed runs where baseline and variant models are trained under identical conditions for each seed"; [Section 3.2] "In typical settings, X and Y are positively correlated: both models perform slightly better on easy splits and slightly worse on hard ones, so Cov(X,Y) > 0. Thus pairing reduces the variance of X−Y relative to independent runs."
- Break condition: If baseline and variant have fundamentally different training dynamics, Cov(X,Y) may approach zero and pairing provides minimal variance reduction

### Mechanism 2: BCa bootstrap confidence intervals provide better coverage for small, skewed delta distributions than standard parametric intervals
- Claim: BCa (bias-corrected and accelerated) bootstrap adjusts confidence interval quantiles using bias correction and acceleration from skewness
- Mechanism: BCa bootstrap adjusts confidence interval quantiles using (a) bias correction from the proportion of bootstrap means below the sample mean, and (b) acceleration from jackknife estimates of skewness. This corrects for asymmetry in the sampling distribution when k is small
- Core assumption: The empirical distribution of deltas reasonably approximates the true sampling distribution after bias/skewness adjustment
- Evidence anchors: [abstract] "bias-corrected and accelerated (BCa) bootstrap confidence intervals on per-seed deltas"; [Section 3.3] "The BCa interval adjusts for bias and skewness in the empirical distribution of Δ̄ and is particularly useful when k is small and the distribution is asymmetric."
- Break condition: If k is extremely small or the delta distribution has heavy tails or multi-modality, BCa intervals may have poor coverage

### Mechanism 3: Sign-flip permutation tests control false positives at the nominal rate under weak distributional assumptions, and are intentionally conservative with tiny k
- Claim: Under symmetry of the delta distribution, sign-flip permutation tests control type-I error exactly at finite k
- Mechanism: Under H₀: Δ_true = 0, the signs of observed deltas are equally likely to be positive or negative if the distribution is symmetric around zero. By randomly flipping signs P times and computing test statistics, the empirical p-value estimates how extreme the observed mean delta is
- Core assumption: The delta distribution is approximately symmetric around zero under the null hypothesis
- Evidence anchors: [abstract] "sign-flip permutation tests on these deltas"; [Section 3.4] "Under symmetry of the Δᵢ distribution, this test controls type-I error exactly at finite k. In practice, it behaves conservatively when k is very small, which we accept and even desire for our guardrail purpose."; [Section 5.1] "all permutation p-values are either 0.24, 0.25, or larger, even when BCa intervals are strictly positive"
- Break condition: If delta distributions are severely asymmetric, the symmetry assumption is violated and type-I error control is not guaranteed

## Foundational Learning

- Concept: **Bootstrap resampling and confidence intervals**
  - Why needed here: The protocol relies on BCa bootstrap to estimate uncertainty from only 3-5 paired deltas. Without understanding resampling, bias correction, and accelerated quantiles, one cannot implement or debug the CI component
  - Quick check question: Given 3 deltas [0.5, 1.2, 0.8], can you explain why a simple percentile bootstrap might give misleading coverage compared to BCa?

- Concept: **Permutation testing and null distribution construction**
  - Why needed here: The sign-flip test generates the reference distribution for judging whether observed improvements exceed chance. Understanding how p-values are computed from this distribution is essential
  - Quick check question: With k=3 deltas, why can't the sign-flip permutation test ever produce p < 0.125 regardless of effect size?

- Concept: **Paired vs. unpaired experimental design**
  - Why needed here: The paper's variance reduction argument depends on understanding pairing as a form of blocking that exploits positive correlation
  - Quick check question: If Cov(X,Y) were negative rather than positive, would pairing increase or decrease the variance of X−Y?

## Architecture Onboarding

- Component map:
  Seed Loop -> Paired training -> Per-seed delta computation -> BCa Bootstrap + Sign-Flip Permutation -> Decision Rule

- Critical path: The per-seed delta computation is the linchpin—errors in pairing (e.g., using different data shuffles for M₀ and M₁) defeat the variance reduction. The decision rule is intentionally strict: both conditions must hold.

- Design tradeoffs:
  - **Conservatism vs. Power**: At k=3, the protocol almost never declares significance even for genuine 1-2pp gains. This is a feature for preventing over-claims, but may mask real improvements. Trade-off resolved in favor of guardrails
  - **Compute vs. Statistical Rigor**: Each seed requires training both models, but the statistical layer (bootstrap + permutation) is computationally trivial
  - **Generality vs. Specificity**: The protocol is task-agnostic but has only been validated on small vision (CIFAR) and text (AG News) tasks with ResNet-18 and TextCNN

- Failure signatures:
  - BCa interval all zeros: Likely means all deltas are identical (e.g., M₀ and M₁ are actually the same model or a bug in variant implementation). Check S0 rows in Table 2
  - Permutation p-value = 1.0 for non-zero deltas: Extremely rare; suggests all deltas are exactly zero or P is too small
  - BCa interval strictly positive but p > 0.05: Expected behavior at k=3 with small effects—the discrete permutation distribution cannot produce small p-values
  - Unpaired t-test p < 0.05 but paired protocol shows no significance: This is the intended guardrail behavior; unpaired tests underestimate variance

- First 3 experiments:
  1. **Sanity check (S0 replication)**: Train identical baseline vs. "variant" (same config) with k=3 seeds. Verify that mean Δ = 0, BCa = [0,0], and p = 1.0. If not, there's a pairing or logging bug
  2. **Known small gain (S1 replication)**: Add label smoothing (ε=0.05) to CIFAR-10 as the variant. Confirm that single-run and unpaired t-test may suggest significance, but the paired protocol does not declare it at k=3
  3. **Power scaling test**: With the same S1 setup, increase to k=10 seeds and observe whether the paired protocol begins declaring significance. This calibrates the sample size needed for your effect size range

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the paired BCa+permutation protocol behave on larger-scale tasks (e.g., ImageNet, LLM fine-tuning) where variance structures may differ from small-scale CIFAR/AG News experiments?
- Basis in paper: [explicit] "We only study small models on CIFAR-10/10N and AG News; larger-scale tasks may exhibit different variance structure."
- Why unresolved: The empirical validation is limited to ResNet-18 and TextCNN on relatively small datasets; no experiments on modern large-scale architectures
- What evidence would resolve it: Application of the protocol to large-scale benchmarks with controlled synthetic scenarios, reporting false positive/negative rates

### Open Question 2
- Question: Can the protocol be extended to fairness, robustness, or calibration metrics while accounting for multiple testing and metric dependencies?
- Basis in paper: [explicit] "Extending the protocol to fairness, robustness or calibration metrics is straightforward in principle but may raise issues with multiple testing and dependency."
- Why unresolved: The paper focuses solely on accuracy; other metrics may have different distributional properties and intercorrelations
- What evidence would resolve it: Experiments applying the protocol to multiple metrics simultaneously with appropriate multiple-testing corrections

### Open Question 3
- Question: How robust is the sign-flip permutation test when the delta distribution exhibits severe skewness rather than approximate symmetry?
- Basis in paper: [explicit] "The permutation test assumes approximate symmetry of the delta distribution; severe skewness may require alternative non-parametric tests or Bayesian treatment."
- Why unresolved: The paper does not evaluate scenarios with highly asymmetric improvement distributions
- What evidence would resolve it: Simulation studies with skewed synthetic deltas comparing sign-flip to alternative tests (e.g., Wilcoxon signed-rank, Bayesian approaches)

### Open Question 4
- Question: What is the false negative rate of the protocol when true improvements are larger (3-5 percentage points), and how many seeds are needed to reliably detect them?
- Basis in paper: [inferred] The paper focuses on guarding against false positives for 0.5-2pp gains; power analysis for larger genuine gains is not provided
- Why unresolved: The medium-gain (S2) scenarios still show ~1pp improvements, leaving larger effect sizes unexplored
- What evidence would resolve it: Controlled experiments with known 3-5pp improvements, varying k from 3 to 10 seeds, measuring detection rates

## Limitations
- The protocol's extreme conservatism at k=3 may prevent detection of real small improvements in contexts where such improvements are meaningful
- The method assumes symmetric delta distributions under H₀, which may not hold for bounded metrics
- The computational overhead of training two models per seed may be prohibitive for large-scale applications

## Confidence
- Paired design variance reduction: **High** - well-established statistical principle with clear empirical support
- BCa bootstrap effectiveness: **Medium** - theoretically sound but lacks direct corpus validation for ML evaluation contexts
- Sign-flip test control: **Medium** - guarantees exact type-I error under symmetry, but real-world deltas may violate this assumption

## Next Checks
1. **Sample size calibration**: Systematically vary k from 3 to 10 with a known small effect (e.g., S1 label smoothing) to map the relationship between sample size and protocol power, determining minimum k for practical use
2. **Metric asymmetry analysis**: Test the protocol on bounded metrics (e.g., AUC, F1) where floor/ceiling effects might violate permutation test symmetry assumptions, measuring type-I error inflation
3. **Alternative variance reduction**: Compare the paired design against blocking on training duration or early stopping criteria to assess whether pairing on seed alone captures all relevant sources of correlated noise