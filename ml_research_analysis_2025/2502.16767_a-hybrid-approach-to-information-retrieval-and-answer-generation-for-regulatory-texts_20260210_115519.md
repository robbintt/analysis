---
ver: rpa2
title: A Hybrid Approach to Information Retrieval and Answer Generation for Regulatory
  Texts
arxiv_id: '2502.16767'
source_url: https://arxiv.org/abs/2502.16767
tags:
- retrieval
- regulatory
- system
- semantic
- information
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a hybrid information retrieval system for regulatory
  texts that combines BM25 (lexical search) with a fine-tuned sentence transformer
  model (semantic search) to address the challenges of long, complex regulatory documents.
  The system uses a weighted combination of both approaches, with semantic matching
  given slightly higher weight.
---

# A Hybrid Approach to Information Retrieval and Answer Generation for Regulatory Texts

## Quick Facts
- arXiv ID: 2502.16767
- Source URL: https://arxiv.org/abs/2502.16767
- Reference count: 14
- A hybrid information retrieval system combining BM25 and semantic search achieves superior performance on regulatory text queries

## Executive Summary
This paper addresses the challenge of retrieving and synthesizing answers from long, complex regulatory documents by proposing a hybrid information retrieval approach. The system combines traditional BM25 lexical search with a fine-tuned semantic search model, using a weighted combination that emphasizes semantic matching. The retrieved passages are then synthesized using GPT-3.5 Turbo in a RAG framework. The approach demonstrates significant improvements over standalone methods, achieving high recall and mean average precision metrics while providing coherent answers to regulatory queries.

## Method Summary
The system employs a hybrid retrieval strategy that combines BM25 (lexical matching) with a fine-tuned sentence transformer model (semantic matching). The semantic model is trained on pharmaceutical regulatory documents using contrastive loss and hard negative mining. Retrieved passages are weighted 60% semantic and 40% lexical, then synthesized using GPT-3.5 Turbo. The fine-tuned model and implementation are made publicly available.

## Key Results
- Hybrid approach achieves Recall@10 of 0.8333 and MAP@10 of 0.7016
- Outperforms BM25 baseline (0.7611 and 0.6237) and semantic-only approaches (0.8103 and 0.6286)
- Answer synthesis achieves RePASs score of 0.57
- Fine-tuned semantic model shows consistent improvement over general-purpose models

## Why This Works (Mechanism)
The hybrid approach leverages complementary strengths of lexical and semantic matching. BM25 captures exact term matches and document structure, while the fine-tuned semantic model understands contextual meaning and domain-specific terminology. The weighted combination allows the system to balance precision from lexical matching with the contextual understanding of semantic matching, particularly important for regulatory texts where specific terminology and nuanced meanings are critical.

## Foundational Learning
- **BM25**: Traditional probabilistic retrieval model that scores documents based on term frequency and inverse document frequency. Why needed: Provides strong baseline performance for exact term matching in regulatory documents.
- **Semantic search with sentence transformers**: Uses pre-trained language models fine-tuned for domain-specific understanding. Why needed: Captures contextual meaning beyond exact keyword matches.
- **Contrastive learning**: Training approach that learns to distinguish relevant from irrelevant passages. Why needed: Improves semantic model's ability to identify truly relevant regulatory content.
- **RAG (Retrieval-Augmented Generation)**: Combines retrieved passages with language model generation. Why needed: Synthesizes coherent answers from multiple retrieved sources.
- **RePASs metric**: Evaluation framework for assessing answer quality in retrieval-augmented systems. Why needed: Provides standardized measure of synthesis quality beyond simple accuracy.

## Architecture Onboarding

Component Map: Query -> Hybrid Retriever (BM25 + Semantic) -> GPT-3.5 Turbo -> Answer

Critical Path: The system processes queries through the hybrid retriever, which scores passages using both BM25 and semantic models, then selects top passages for synthesis by the LLM.

Design Tradeoffs: The 60/40 weighting between semantic and lexical matching represents a balance between contextual understanding and precision. Alternative weightings or dynamic weighting based on query type could be explored.

Failure Signatures: Performance degradation occurs when regulatory terminology differs significantly from training data, or when queries require cross-document synthesis beyond the retriever's passage selection capability.

First Experiments:
1. Test retrieval performance on queries requiring exact terminology matching versus conceptual understanding
2. Evaluate answer quality with varying numbers of retrieved passages (5, 10, 15)
3. Compare performance across different regulatory domains (pharmaceutical vs. financial vs. environmental)

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation relies on a relatively small dataset of 80 question-document pairs from pharmaceutical regulatory texts
- No statistical significance testing reported for performance differences between methods
- RePASs score lacks comparison to human evaluation benchmarks
- Does not explore alternative weighting schemes for the hybrid combination

## Confidence

High: Hybrid retrieval performance claims - statistically significant improvements across metrics
Medium: Answer synthesis quality - reasonable RePASs score but limited comparison
Medium: Generalizability to other regulatory domains - demonstrated only on pharmaceutical texts
Low: Statistical significance of improvements - no significance testing reported

## Next Checks

1. Conduct significance testing (t-tests or Wilcoxon signed-rank) on Recall@10 and MAP@10 differences between hybrid and baseline methods across multiple regulatory domains

2. Perform ablation studies varying the semantic-to-lexical weight ratio to identify optimal weighting for different question types

3. Evaluate the system with human domain experts to validate RePASs scores and assess practical usability in real-world compliance scenarios