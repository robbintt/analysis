---
ver: rpa2
title: Supervised Fine-Tuning or In-Context Learning? Evaluating LLMs for Clinical
  NER
arxiv_id: '2510.22285'
source_url: https://arxiv.org/abs/2510.22285
tags:
- i-adr
- clinical
- text
- bert
- entity
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper compares BERT-style models and large language models
  (LLMs) for clinical named entity recognition on the CADEC corpus. Three BERT models
  (BERT Base, BioClinicalBERT, RoBERTa-large) are evaluated alongside GPT-4o using
  in-context learning with simple vs.
---

# Supervised Fine-Tuning or In-Context Learning? Evaluating LLMs for Clinical NER

## Quick Facts
- arXiv ID: 2510.22285
- Source URL: https://arxiv.org/abs/2510.22285
- Authors: Andrei Baroian
- Reference count: 35
- Primary result: GPT-4o SFT achieves F1 ≈ 87.1%, outperforming BERT and ICL approaches on clinical NER

## Executive Summary
This paper evaluates three approaches for clinical named entity recognition on the CADEC corpus: BERT-style models (BERT Base, BioClinicalBERT, RoBERTa-large), GPT-4o with in-context learning (simple vs. complex prompts), and GPT-4o with supervised fine-tuning. Results show RoBERTa-large and BioClinicalBERT offer minimal improvements over BERT Base, suggesting limited benefit from domain-specific pretraining on patient forum text. Among LLM approaches, simple in-context prompts outperform complex ones, while supervised fine-tuning achieves the highest overall F1 score of approximately 87.1%, though at higher cost. LLMs also perform better on simplified binary classification tasks.

## Method Summary
The study evaluates three model families on the CADEC corpus using standard NER metrics. BERT models (Base, BioClinicalBERT, RoBERTa-large) are fine-tuned with fixed hyperparameters (lr=2e-5, 5 epochs, weight_decay=0.01). GPT-4o is tested using in-context learning with simple and complex prompts containing few-shot examples, and supervised fine-tuning with 100 examples formatted as conversations. Data is split 64/16/20 (train/val/test), and models are evaluated on IOB-tagged entity recognition with 11 labels. The study also explores task simplification by reducing the label space to binary classification.

## Key Results
- RoBERTa-large improves F1 by 3% over BERT Base but takes 70× longer (143s vs 2s runtime)
- Simple in-context prompts outperform complex prompts by 3% F1 and are 50% faster
- Supervised fine-tuning achieves best overall F1 of 87.1% but requires higher computational cost
- LLMs perform better on simplified binary classification tasks than multi-class NER

## Why This Works (Mechanism)

### Mechanism 1
Simpler prompts yield better in-context learning performance for clinical NER than instruction-heavy prompts. Complex prompts with lengthy annotation guidelines and many examples may increase cognitive load on the model's attention mechanism, causing it to lose focus on the core labeling task. Shorter prompts with concise rules reduce noise in the context window. This may be specific to generative LLMs performing structured prediction.

### Mechanism 2
Supervised fine-tuning outperforms in-context learning for clinical NER, but with higher computational and financial cost. SFT directly updates model weights to optimize for the specific label distribution and entity boundaries of the target corpus, whereas ICL must infer the labeling scheme from examples without weight updates. The training data must be sufficiently representative of the test distribution for SFT to converge properly.

### Mechanism 3
Task simplification (reducing label space) improves LLM accuracy on clinical NER subtasks. Reducing the number of classes from 11 IOB labels to 3 narrows the decision boundary the model must learn, decreasing confusion between similar entity types and reducing the impact of class imbalance. A multi-model ensemble approach could recover full entity coverage without sacrificing per-entity accuracy.

## Foundational Learning

- **IOB (Inside-Outside-Beginning) Tagging**: The CADEC corpus uses IOB format for entity annotation; understanding B- prefix (entity start) and I- prefix (continuation) is essential for parsing predictions and ground truth. Quick check: Given "severe joint pain" as an ADR, what are the correct IOB labels?
- **Class Imbalance in Sequence Labeling**: CADEC has 6318 ADR instances vs 275 Symptom instances; models may overfit to majority classes. Quick check: Why might a model achieve high overall accuracy but fail on rare entity types like Symptom?
- **Token-Label Alignment**: LLMs output variable-length sequences; the paper notes issues with generating extra 'O' tokens, requiring explicit length constraints in prompts. Quick check: If a model outputs 15 labels for a 12-token sentence, how should you handle the mismatch?

## Architecture Onboarding

- **Component map**: BERT family (encoder-only transformers) -> ICL GPT-4o (few-shot prompting) -> SFT GPT-4o (weight updates)
- **Critical path**: Parse CADEC raw text + annotations → tokenize → align labels using boundary checking → fine-tune BERT or construct prompts for GPT-4o → evaluate with precision/recall/F1 on IOB labels → handle length mismatches in LLM outputs
- **Design tradeoffs**: BERT Base is fastest (2s runtime) but lowest F1 (0.6048); RoBERTa-large improves F1 by 3% but takes 70× longer (143s); SFT GPT-4o achieves best F1 (0.8710) but requires API costs and fine-tuning time not captured in eval runtime; ICL Simple offers best cost-performance balance for LLMs (F1 0.8306, no fine-tuning cost)
- **Failure signatures**: LLM generating excess 'O' tokens beyond input length (solved by prompting exact length); BioClinicalBERT showing minimal improvement over BERT Base suggests domain pretraining may not transfer to patient forum text; discrepancy between reported results and prior literature (Scaboro et al. 2023 found BERT F1 ~66.67%) indicates potential implementation issues
- **First 3 experiments**: Reproduce BERT baselines on CADEC with hyperparameter tuning to verify F1 scores against prior literature; Test SFT with Simple Prompt instead of Complex Prompt to isolate whether prompt simplicity benefits transfer to fine-tuning; Pilot ensemble approach: train 5 binary classifiers (one per entity type) on ADR-only, Drug-only, etc., then merge predictions; evaluate if weighted F1 improves over single-model approach

## Open Questions the Paper Calls Out

1. Does Supervised Fine-Tuning (SFT) with a simple prompt outperform SFT with a complex prompt for clinical NER? The authors only evaluated SFT using a complex prompt, leaving the combination of SFT and simple prompts untested.

2. Would an ensemble of binary classifiers (one per entity type) outperform a single multi-class fine-tuned model? The study evaluated the binary task in isolation but did not construct or evaluate the proposed ensemble architecture.

3. Can hyperparameter optimization close the performance gap between BERT-style models and Large Language Models? BERT models were trained with fixed default hyperparameters to ensure a fair baseline comparison, potentially suppressing their true capacity.

4. Are the reported results valid and generalizable to other established clinical NER datasets? The results on the CADEC corpus were surprisingly high compared to previous studies, raising concerns about validity or dataset-specific overfitting.

## Limitations

- Limited domain applicability as CADEC represents patient forum text, which differs from clinical notes or biomedical literature
- Incomplete cost-benefit analysis as computational costs for SFT and ICL are not quantified in terms of API calls or fine-tuning hours
- Unspecified selection criteria for the 100 SFT training examples could introduce bias
- Single dataset evaluation without external validation prevents assessment of model robustness across different clinical domains
- Ensemble approach hypothesized as a solution for label simplification was not empirically tested

## Confidence

**High Confidence**: BERT Base vs. domain-specific models (minimal improvement of BioClinicalBERT and RoBERTa-large over BERT Base is well-supported); Task simplification benefits (LLMs perform better on simplified binary classification is directly supported by empirical results)

**Medium Confidence**: Simple vs. complex prompts (3% F1 improvement and 50% runtime reduction are measurable but may not generalize); SFT vs. ICL cost-performance tradeoff (superiority of SFT demonstrated but higher cost claim lacks quantitative support)

**Low Confidence**: Ensemble decomposition strategy (purely speculative as never implemented or tested); IOB constraint violations in LLM outputs (mentioned as potential issue but not systematically evaluated)

## Next Checks

1. External Validation on Clinical Notes: Test the best-performing models (SFT GPT-4o and RoBERTa-large) on a clinical notes dataset like i2b2/VA to assess domain transfer and determine if BERT Base's poor performance on CADEC is task-specific or indicative of broader limitations.

2. Cost Quantification for SFT: Measure and report the actual computational costs (API calls, fine-tuning time, inference latency) for SFT GPT-4o compared to ICL approaches to enable meaningful cost-benefit analysis for clinical deployment decisions.

3. Ensemble Implementation Pilot: Implement the hypothesized ensemble approach by training five binary classifiers (one per entity type) on the CADEC dataset, then evaluate whether the weighted F1 score improves over the single-model approach while maintaining practical usability.