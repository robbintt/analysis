---
ver: rpa2
title: Howard's Policy Iteration is Subexponential for Deterministic Markov Decision
  Problems with Rewards of Fixed Bit-size and Arbitrary Discount Factor
arxiv_id: '2505.00795'
source_url: https://arxiv.org/abs/2505.00795
tags:
- policy
- bound
- reward
- upper
- state
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper proves a subexponential upper bound for Howard\u2019\
  s Policy Iteration (HPI) on deterministic Markov Decision Problems (DMDPs) with\
  \ discounted reward, parameterized by the bit-size of the rewards. The authors introduce\
  \ a threshold discount factor \u03B3Q, showing that for any discount factor above\
  \ \u03B3Q, HPI\u2019s trajectory is invariant."
---

# Howard's Policy Iteration is Subexponential for Deterministic Markov Decision Problems with Rewards of Fixed Bit-size and Arbitrary Discount Factor

## Quick Facts
- arXiv ID: 2505.00795
- Source URL: https://arxiv.org/abs/2505.00795
- Reference count: 16
- This paper proves a subexponential upper bound for Howard's Policy Iteration on deterministic MDPs with fixed bit-size rewards

## Executive Summary
This paper establishes the first subexponential upper bound for Howard's Policy Iteration (HPI) on deterministic Markov Decision Problems (DMDPs) with discounted reward. The key insight is the introduction of a threshold discount factor γQ, which ensures that for any discount factor above this threshold, HPI's trajectory becomes invariant. By analyzing the "Q-difference sign polynomial" and bounding its largest root below 1, the authors derive a running time bound of O(n^k · exp(O(√(nb log(n/b) + b)))). This result improves upon previous exponential bounds and holds even when the bit-size b grows slowly as o(n^(1-ε)).

## Method Summary
The paper analyzes Howard's Policy Iteration on DMDPs with rewards having fixed bit-size b. The core approach introduces a threshold discount factor γQ, showing that for γ > γQ, HPI's trajectory is invariant. The authors construct the Q-difference sign polynomial f^π_{s,a,a'}(γ) for potential action switches, which has integer coefficients derived from the bit-size bounded rewards. Using root-separation theorems for integer-coefficient polynomials, they bound the largest root below 1. This bound on γQ is then combined with existing γ-dependent bounds for HPI to derive the subexponential upper bound. The method applies to both discounted and average reward criteria.

## Key Results
- First subexponential upper bound for Howard's Policy Iteration on DMDPs
- Bound of O(n^k · exp(O(√(nb log(n/b) + b)))) for discounted reward
- Same subexponential bound applies under average reward, yielding pseudopolynomial bound O(n^5 · 4^b)
- Result holds when bit-size b grows as o(n^(1-ε)) for any ε > 0
- Bound also applies to DMDPs with only two reward values

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Howard's Policy Iteration (HPI) exhibits a trajectory that stabilizes and becomes invariant when the discount factor γ exceeds a specific threshold γQ.
- **Mechanism:** The algorithm compares Q-values to decide policy switches. These comparisons depend on γ. The paper proves there exists a threshold γQ such that for all γ > γQ, the relative ordering of Q-values (and thus the greedy policy switches) remains constant. This allows the analysis of worst-case iteration counts to focus on a fixed γQ rather than an infinitesimal gap near 1.
- **Core assumption:** The underlying MDP is deterministic (DMDP) and rewards have finite bit-size.
- **Evidence anchors:** [Section 4.1] Defines γQ and states: "for two MDPs M_γ and M_{γ'}... HPI will follow the same trajectory as long as γ, γ' ∈ (γQ, 1)." [Section 4.4] Shows that for γ ∈ (γQ, 1), HPI visits an identical sequence of policies as it would at the threshold. [Corpus] "Efficient Computation of Blackwell Optimal Policies..." discusses similar γ-thresholds (Blackwell discount factor), supporting the existence of such critical points in policy space.
- **Break condition:** If the MDP has stochastic transitions (not a DMDP), this specific invariance proof for γQ does not hold as described.

### Mechanism 2
- **Claim:** The threshold γQ is effectively "repelled" from 1 (approaches 1 slowly) due to constraints on the "Q-difference sign polynomial."
- **Mechanism:** The decision to switch actions depends on the sign of Q^π(s,a) - Q^π(s,a'). This difference can be modeled as a polynomial in γ with integer coefficients. The threshold γQ corresponds to the largest root of this polynomial below 1. By bounding the coefficients (height) of this polynomial using the bit-size of rewards, the paper limits how close the roots can cluster near 1.
- **Core assumption:** Rewards can be scaled to integers with bounded magnitude (fixed bit-size b).
- **Evidence anchors:** [Section 4.2] Defines f^π_{s,a,a'}(γ) as the "Q-difference sign polynomial" and notes it has integer coefficients. [Section 4.3] Theorem 4.2 provides an upper bound on the largest root τ < 1 based on polynomial height and degree. [Corpus] "Geometric Re-Analysis..." analyzes policy iteration through geometric lenses; while this paper uses algebraic root separation, both seek structural explanations for PI efficiency.
- **Break condition:** If rewards require infinite precision (infinite bit-size) or are non-rational, the integer coefficient assumption fails, and the root separation bound is lost.

### Mechanism 3
- **Claim:** The overall complexity is bounded subexponentially by combining the invariant trajectory length with the bounded γQ.
- **Mechanism:** The paper leverages existing γ-dependent bounds (specifically O(n^k / (1-γ) · log(...))). By substituting the upper bound for 1/(1-γQ) derived from the polynomial root analysis, the exponential dependency on n is reduced to a subexponential dependency of the form exp(O(√(nb log(n/b) + b))).
- **Core assumption:** The bit-size b is constant or grows slowly (o(n^(1-ε))).
- **Evidence anchors:** [Abstract] Claims a "subexponential upper bound... parameterised by the bit-size of the rewards." [Section 4.4] Derives 1/(1-γQ) ≤ U, where U is subexponential, yielding the final bound. [Corpus] "Lower Bound on Howard Policy Iteration..." provides counter-examples showing exponential lower bounds exist without these specific restrictions, highlighting the necessity of the bit-size assumption.
- **Break condition:** If existing γ-dependent bounds for PI are significantly weakened or if b scales linearly with n, the subexponential result collapses back to exponential.

## Foundational Learning

- **Concept: Policy Iteration (PI) vs. Value Iteration**
  - **Why needed here:** The paper optimizes the analysis of PI. You must understand that PI jumps between discrete policies (switching actions) rather than continuously updating values, which is why "switching rules" and "policy trajectories" are central.
  - **Quick check question:** Does Policy Iteration update the value of every state simultaneously or switch actions based on current value estimates?

- **Concept: Algebraic Number Theory (Root Separation)**
  - **Why needed here:** The core mathematical contribution uses properties of integer polynomials to bound γQ. Understanding that polynomials with "small" integer coefficients cannot have roots arbitrarily close to 1 without specific structural reasons is key.
  - **Quick check question:** Why does limiting the bit-size of rewards imply a limit on how close a "switching point" (root) can be to γ=1?

- **Concept: Blackwell Optimality**
  - **Why needed here:** The threshold γQ is conceptually similar to the Blackwell discount factor—the point after which the optimal policy stabilizes.
  - **Quick check question:** How does the concept of a "threshold discount factor" differ from simply setting γ=0.99?

## Architecture Onboarding

- **Component map:**
  - DMDP Graph (G_M) -> States (vertices), Deterministic transitions (edges), Rewards (weights)
  - Policy Node -> Specific subgraph where each state has exactly one outgoing edge
  - Comparator (Theoretical) -> Evaluates Q-difference sign polynomial for action pairs to determine switching
  - Trajectory Analyzer -> Logs the sequence of policy nodes visited by Greedy Switching logic

- **Critical path:**
  1. Define the DMDP with scaled integer rewards (bit-size b)
  2. Construct the Q-difference polynomial for potential action switches
  3. Identify the threshold γQ (conceptually, for the proof) to delimit the "hard" region of optimization
  4. Execute Greedy PI; the proof guarantees this path is subexponential in length

- **Design tradeoffs:**
  - **Generality vs. Bounds:** The architecture (HPI) is general, but the subexponential guarantee requires the specific constraint of fixed bit-size rewards. Standard HPI implementations do not check bit-size, yet they benefit from these bounds in practical scenarios where rewards are indeed float/double (fixed size).
  - **Greedy vs. Simplex:** The paper highlights that while Simplex variants have strongly polynomial bounds, HPI is faster in practice. This analysis bridges the theoretical gap for HPI on DMDPs.

- **Failure signatures:**
  - **Exponential Blow-up:** If constructed with adversarial real-valued rewards (ignoring float constraints) or specific graph topologies not covered by DMDP restrictions, HPI might still hit exponential iteration counts (as suggested by lower bounds in the corpus).
  - **Precision Loss:** In implementation, if γ is extremely close to 1 (exceeding the stable γQ logic effectively handled by floating-point precision), numerical instability could cause oscillation, though the theory predicts trajectory invariance.

- **First 3 experiments:**
  1. **Trajectory Invariance Test:** Run HPI on a fixed DMDP while sweeping γ from 0.5 to 0.999. Verify that the sequence of policies remains constant once γ > γ_est (an empirical estimate of γQ).
  2. **Bit-Size Scaling:** Generate DMDPs with fixed n but varying reward magnitude/precision (increasing bit-size b). Plot iteration counts to verify the dependency on b matches the theoretical O(exp(√(nb)...) trend.
  3. **Two-Value Reward Stress Test:** Construct DMDPs with only rewards {0,1} (minimal bit-size). Compare HPI iteration counts against random PI variants to validate that HPI retains efficiency even when the theoretical upper bound is tightest.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the exponential dependence on the reward bit-size b be eliminated or improved in the upper bounds for HPI?
- **Basis in paper:** [explicit] The conclusion states: "It is worth exploring if the dependence on b, which is currently exponential in both our upper bounds, can be improved."
- **Why unresolved:** The current proof relies on generic root-separation bounds for integer-coefficient polynomials, which inherently scale with the coefficient height (magnitude O(2^b)).
- **What evidence would resolve it:** A proof showing a bound that is polynomial in b, or a specific construction showing that exponential dependence on b is necessary.

### Open Question 2
- **Question:** Can the subexponential upper bound be extended to intermediate classes of MDPs that lie between deterministic MDPs (DMDPs) and general MDPs?
- **Basis in paper:** [explicit] The conclusion asks: "It would be interesting to investigate whether intermediate classes of MDPs (which interpolate in some manner between DMDPs and MDPs) can benefit from our approach."
- **Why unresolved:** The authors note that related work suggests the technique is unlikely to work for general MDPs, but the boundary of applicability for "intermediate" stochastic classes is unknown.
- **What evidence would resolve it:** A demonstration of subexponential bounds for MDPs with specific structural constraints (e.g., sparse transitions) using the threshold discount factor technique.

### Open Question 3
- **Question:** Can the subexponential bound be tightened by exploiting specific properties of the Q-difference sign polynomial?
- **Basis in paper:** [explicit] The conclusion posits: "It may also be possible to improve our subexponential upper bound by exploiting more properties of the Q-difference sign polynomial."
- **Why unresolved:** The current analysis treats the polynomial with generic bounds; the specific structure of the coefficients derived from DMDP paths may allow for tighter root-separation analysis.
- **What evidence would resolve it:** A refined mathematical analysis of the polynomial f^π_{s,a,a'} yielding a smaller upper bound on the largest root below 1, thereby lowering the threshold γQ.

## Limitations
- The result assumes fixed bit-size rewards, which may not hold for floating-point implementations
- The bound assumes bit-size b grows slowly as o(n^(1-ε)); behavior for larger b remains unclear
- The result is specific to deterministic MDPs and does not extend to general stochastic MDPs
- The exponential dependence on reward bit-size b cannot be eliminated with current techniques

## Confidence

- **High Confidence:** The existence of threshold discount factor γQ and its role in trajectory invariance (Mechanism 1). The algebraic framework connecting reward bit-size to polynomial coefficient bounds (Mechanism 2).
- **Medium Confidence:** The final subexponential bound derivation (Mechanism 3), as it combines multiple theoretical results with implicit constants.
- **Low Confidence:** Practical implications for floating-point implementations and whether numerical precision issues might invalidate the theoretical guarantees in edge cases.

## Next Checks

1. **Empirical γQ Verification:** Implement HPI on benchmark DMDPs with varying γ values to empirically verify trajectory invariance above a threshold, comparing against the theoretical γQ prediction.
2. **Bit-Size Scaling Experiment:** Systematically increase reward precision (b) while holding n constant to validate the subexponential scaling relationship O(exp(O(√(nb log(n/b) + b)))).
3. **Two-Value Reward Test:** Construct DMDPs with only rewards {0, 1} to test the theoretical worst-case scenario where bit-size constraints are tightest, comparing HPI performance against theoretical bounds.