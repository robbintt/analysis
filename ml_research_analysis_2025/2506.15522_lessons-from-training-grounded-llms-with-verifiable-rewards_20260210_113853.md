---
ver: rpa2
title: Lessons from Training Grounded LLMs with Verifiable Rewards
arxiv_id: '2506.15522'
source_url: https://arxiv.org/abs/2506.15522
tags:
- ground-grpo
- reasoning
- answer
- training
- trust
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the challenge of generating grounded, trustworthy
  responses in large language models (LLMs), particularly in retrieval-augmented generation
  (RAG) settings. The authors propose Ground-GRPO, a two-stage reinforcement learning
  framework based on Group Relative Policy Optimization (GRPO), which uses verifiable
  outcome-based rewards to improve answer correctness, citation quality, and refusal
  behavior without requiring gold reasoning traces.
---

# Lessons from Training Grounded LLMs with Verifiable Rewards

## Quick Facts
- arXiv ID: 2506.15522
- Source URL: https://arxiv.org/abs/2506.15522
- Reference count: 11
- Primary result: Two-stage GRPO with verifiable rewards improves grounded response generation in RAG, with reasoning models showing 11.5-point gains over instruct-only variants.

## Executive Summary
This work introduces Ground-GRPO, a two-stage reinforcement learning framework that uses verifiable outcome-based rewards to improve grounded response generation in large language models. The approach combines Group Relative Policy Optimizer (GRPO) with explicit citation penalties and a staged curriculum to train models that generate well-cited answers and appropriately refuse unanswerable queries. Experiments across four benchmarks demonstrate that reasoning-augmented models benefit more from RLVR than instruction-only variants, and that the two-stage setup stabilizes multi-objective optimization for answer correctness, citation quality, and refusal calibration.

## Method Summary
Ground-GRPO uses GRPO via the TRL library to optimize LLM policies with verifiable outcome rewards. The two-stage approach first trains on answerable-only data (Stage 1) to establish answer extraction and citation behaviors, then adds unanswerable examples with refusal rewards (Stage 2). Rewards include exact match for answer correctness, NLI-based citation verification, fuzzy refusal matching, and format compliance penalties. The training uses 8 sampled responses per query, 4 epochs, global batch size 384, and learning rate 1e-5 with cosine scheduling. Models tested include LLaMA3.1-8B, Qwen3-4B, and Qwen3-8B in both instruct and reasoning variants.

## Key Results
- Two-stage training improves Trust-Score by ~15% over single-stage approaches across all model-dataset combinations
- Reasoning models show 11.5-point average improvement vs. 7.0 points for instruction-only models
- Removing the bad citation penalty drives Answer Ratio (AR) to 100% and collapses citation quality metrics
- Distillation ordering matters: SFT/DPO before RLVR outperforms the reverse by ~15%

## Why This Works (Mechanism)

### Mechanism 1: Staged Curriculum for Multi-Objective Stability
Two-stage training produces more stable learning signals than joint optimization by first establishing answer/citation competencies on answerable data, then adding refusal behavior. This prevents reward interference between competing objectives during early training. Break condition: noisy answerability labels in Stage 1 or skewed answerability ratios in Stage 2 degrade benefits.

### Mechanism 2: Reasoning Models Extract Greater RLVR Benefit
Reasoning-augmented models outperform instruct-only variants because intermediate thought traces decompose grounding decisions, making reward attribution cleaner during GRPO optimization. Break condition: misaligned reasoning traces create contradictory advantage signals requiring process rewards.

### Mechanism 3: Negative Citation Penalties Prevent Over-Answering
-0.5 penalties for incorrect citations create asymmetric loss that constrains over-answering behavior. Larger models show more severe citation collapse when penalties are removed. Break condition: 4B models may be over-constrained, requiring model-specific penalty tuning.

## Foundational Learning

- **Group Relative Policy Optimization (GRPO)**
  - Why needed: Replaces PPO's critic with group-normalized advantages across sampled outputs, reducing memory overhead and stabilizing sparse/noisy rewards
  - Quick check: Can you explain why normalizing rewards across sampled outputs within a single prompt avoids the need for a learned value function?

- **Verifiable Outcome Rewards**
  - Why needed: Enables scalable training without human preference labels through programmatically computable rewards (EM, NLI-based citation, fuzzy refusal)
  - Quick check: What verification functions would you need to implement for each reward component (EM, citation, refusal)?

- **Trust-Score Metric Decomposition**
  - Why needed: Composite metric (F1GR + F1AC + F1GC)/3 allows diagnosing which grounding behaviors improve/degrade during training
  - Quick check: If a model's Trust-Score improves but F1AC drops significantly, what behavior pattern should you investigate?

## Architecture Onboarding

- **Component map:**
  - Policy Model (LLM with <think/> and <answer/> tags) -> Reward Functions (EM verifier, Citation verifier, Refusal scorer, Format reward) -> GRPO Trainer (TRL library, 8 samples per prompt) -> Policy Updates

- **Critical path:**
  1. Implement and validate each reward function independently before integration
  2. Verify Stage 1 training converges on answerable-only data (target: stable F1AC improvements without format violations)
  3. Introduce Stage 2 mixed data only after Stage 1 metrics plateau
  4. Monitor reasoning-answer alignment (%Align metric throughout—values below 80% indicate process supervision may be needed

- **Design tradeoffs:**
  - Data efficiency vs. stability: Paper uses 300 samples for Stage 1 and 3,000 for Stage 2; scale based on validation performance
  - Distillation ordering: Apply SFT/DPO before Ground-GRPO; reverse ordering underperforms by ~15%
  - Reward strictness: Bad citation penalty critical for 8B+ models but may over-constrain 4B models

- **Failure signatures:**
  - AR% → 100%: Missing or insufficient citation penalty; model ignoring grounding constraints
  - F1GR near 0 with high AR%: Refusal reward not properly computed or Stage 2 skipped
  - %Align dropping during training: Reasoning traces diverging from answers; consider adding process reward
  - Format reward always 0: Model not generating required tags; check prompt template and tokenizer

- **First 3 experiments:**
  1. Baseline sanity check: Run Ground-GRPO on ASQA with reasoning model, Stage 1 only, 300 samples. Verify format compliance (Rformat > 0.95) and F1AC improves vs. zero-shot prompt.
  2. Citation penalty ablation: Compare with vs. without bad citation penalty on Qwen-3-8B. Document AR% shift and F1GC change to calibrate penalty weight.
  3. Stage composition test: Run Stage 2-only vs. Stage 1+2 on same model. Confirm two-stage improves F1GR by expected margin (~15%) before scaling.

## Open Questions the Paper Calls Out
None

## Limitations
- Claims rest on curated Trust-Align dataset with controlled retrieval settings that may not generalize to open-domain RAG scenarios
- Staged curriculum effectiveness depends heavily on 1:1 answerability mix in Stage 2, but sensitivity to this ratio is unexplored
- Study focuses on medium-sized models (4B-8B parameters); scaling behavior to frontier models remains untested

## Confidence
- **High confidence:** Two-stage training stability and citation penalty effectiveness are directly supported by ablation studies and quantitative metrics
- **Medium confidence:** Reasoning models' superior performance shows consistent gains but relies on reasoning traces meaningfully decomposing grounding tasks, which the paper notes has unresolved alignment issues
- **Low confidence:** Distillation ordering claims are based on a single result without systematic exploration of alternative curricula

## Next Checks
1. Test staged training with varying Stage 2 answerability ratios (0.5, 0.7, 0.9) to determine curriculum sensitivity and optimal mix for different model sizes
2. Implement and measure the proposed process reward (NLI alignment between reasoning and final outputs) to quantify its impact on reasoning-answer misalignment
3. Scale the framework to a 70B parameter model and evaluate whether citation penalties require recalibration or if over-answering becomes more pronounced