---
ver: rpa2
title: Speech-to-Speech Translation Pipelines for Conversations in Low-Resource Languages
arxiv_id: '2506.01406'
source_url: https://arxiv.org/abs/2506.01406
tags:
- google
- microsoft
- whisper
- translation
- speech
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study addresses the challenge of automatic speech-to-speech
  translation for community interpreting in low-resource language pairs, specifically
  Turkish and Pashto to/from French. The authors collected conversational dialogue
  data, fine-tuned ASR and MT components, and evaluated over 60 pipeline combinations
  using both automatic metrics (BLEU, COMET, BLASER) and human assessments.
---

# Speech-to-Speech Translation Pipelines for Conversations in Low-Resource Languages
## Quick Facts
- **arXiv ID:** 2506.01406
- **Source URL:** https://arxiv.org/abs/2506.01406
- **Reference count:** 13
- **Primary result:** Fine-tuned Whisper ASR and Google MT components achieved "good" to "very good" quality for Turkish-French and Pashto-French community interpreting

## Executive Summary
This study addresses the critical challenge of automatic speech-to-speech translation for community interpreting in low-resource language pairs, specifically Turkish and Pashto to/from French. The researchers developed a comprehensive pipeline evaluation framework that tested over 60 different combinations of ASR, MT, and TTS components. Through both automatic metrics (BLEU, COMET, BLASER) and human assessments, they identified optimal configurations for real-time interpreting applications where professional interpreters are unavailable.

The results demonstrate that fine-tuned Whisper ASR consistently outperformed other ASR systems across all configurations, while Google MT achieved the highest translation quality. The best-performing pipelines achieved BLEU scores around 25 and COMET scores near 90, corresponding to "good" to "very good" ratings for meaning, correctness, and intonation in human evaluations. These findings provide practical guidance for deploying speech-to-speech translation systems in community settings for low-resource language pairs.

## Method Summary
The researchers developed a comprehensive evaluation framework for speech-to-speech translation pipelines targeting low-resource language pairs (Turkish-French and Pashto-French). They collected conversational dialogue data from 5 speakers, recording approximately 15-20 minutes of speech across 150 test utterances. The study evaluated 8 pipeline configurations combining different ASR systems (including fine-tuned Whisper), MT engines (Google, DeepL, GPT-4), and TTS components. Evaluation employed automatic metrics (BLEU, COMET, BLASER) and human assessments following standardized protocols with 15 participants rating meaning, correctness, and intonation. The methodology included both segment-level and document-level evaluation to assess pipeline performance across different granularities.

## Key Results
- Fine-tuned Whisper ASR consistently outperformed other ASR systems across all pipeline configurations
- Google MT achieved the highest translation quality among tested MT components
- The best pipelines achieved BLEU scores around 25 and COMET scores near 90, corresponding to "good" to "very good" human ratings
- Component rankings proved stable across different pipeline configurations

## Why This Works (Mechanism)
The effectiveness of the identified pipeline combinations stems from the complementary strengths of individual components optimized for low-resource languages. Fine-tuned Whisper ASR demonstrated superior performance due to its ability to adapt to the specific acoustic and linguistic characteristics of Turkish and Pashto through targeted fine-tuning on domain-specific data. Google MT's dominance in translation quality can be attributed to its large-scale pretraining and sophisticated contextual modeling capabilities, which proved particularly effective for handling the complex syntactic and semantic challenges inherent in low-resource language pairs. The stability of component rankings across configurations suggests that the fundamental strengths of each component remain consistent regardless of downstream processing steps.

## Foundational Learning
- **Speech-to-Speech Translation Pipeline Architecture**: Chain of ASR → MT → TTS components for real-time language conversion; needed for understanding end-to-end translation workflows
- **Automatic Evaluation Metrics (BLEU, COMET, BLASER)**: Quantitative measures of translation quality; needed for objective comparison of pipeline performance
- **Human Evaluation Protocols**: Standardized assessment methods for subjective quality dimensions; needed for validating automatic metric correlations
- **Fine-tuning vs. Zero-shot Learning**: Adaptation strategies for language models; needed for optimizing performance on low-resource languages
- **Low-resource Language Challenges**: Limited training data and linguistic resources; needed for understanding constraints in community interpreting applications

## Architecture Onboarding
- **Component Map**: ASR → MT → TTS (sequential processing chain)
- **Critical Path**: ASR output quality directly impacts downstream MT performance, making ASR the most critical component
- **Design Tradeoffs**: Real-time performance vs. translation quality; automatic vs. human evaluation reliability
- **Failure Signatures**: ASR errors propagate through MT, causing semantic drift; MT limitations create unnatural TTS output
- **3 First Experiments**: 1) Test fine-tuned Whisper vs. base Whisper on Turkish/Pashto audio samples, 2) Compare Google MT vs. DeepL on Turkish-French translation quality, 3) Evaluate segment-level vs. document-level BLEU correlation

## Open Questions the Paper Calls Out
None

## Limitations
- Reliance on synthetic parallel speech data rather than natural conversational speech
- Limited evaluation corpus of 150 utterances representing only 15-20 minutes of speech
- Focus on only 8 pipeline configurations may miss optimal combinations
- Small sample size of 15 participants for human evaluation

## Confidence
- **High Confidence**: Ranking stability of ASR and MT components across configurations
- **Medium Confidence**: BLEU score threshold of 25 for acceptable quality based on human evaluation correlations
- **Low Confidence**: Generalizability to other low-resource language pairs beyond Turkish-French and Pashto-French

## Next Checks
1. Test pipeline performance on naturally occurring conversational speech data rather than synthetic parallel data
2. Expand evaluation to additional low-resource language pairs with different linguistic characteristics
3. Conduct larger-scale human evaluations with professional interpreters across multiple community interpreting contexts