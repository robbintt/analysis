---
ver: rpa2
title: 'Multi-TW: Benchmarking Multimodal Models on Traditional Chinese Question Answering
  in Taiwan'
arxiv_id: '2508.01274'
source_url: https://arxiv.org/abs/2508.01274
tags:
- arxiv
- zhang
- audio
- chinese
- traditional
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Multi-TW, the first benchmark for evaluating
  any-to-any multimodal models on Traditional Chinese, combining image-text and audio-text
  tasks from authentic proficiency tests. The dataset includes 900 multiple-choice
  questions, equally split between vision-based and audio-based subsets.
---

# Multi-TW: Benchmarking Multimodal Models on Traditional Chinese Question Answering in Taiwan

## Quick Facts
- arXiv ID: 2508.01274
- Source URL: https://arxiv.org/abs/2508.01274
- Reference count: 40
- Introduces first benchmark for evaluating any-to-any multimodal models on Traditional Chinese, combining image-text and audio-text tasks from authentic proficiency tests

## Executive Summary
This paper introduces Multi-TW, the first benchmark for evaluating any-to-any multimodal models on Traditional Chinese, combining image-text and audio-text tasks from authentic proficiency tests. The dataset includes 900 multiple-choice questions, equally split between vision-based and audio-based subsets. Experiments on various models show that closed-source models generally outperform open-source ones, though open-source models can excel in audio tasks. End-to-end any-to-any pipelines demonstrate significant latency advantages over VLM plus ASR pipelines for audio tasks. The results highlight the need for Traditional Chinese fine-tuning and more efficient multimodal architectures.

## Method Summary
The Multi-TW benchmark evaluates zero-shot performance of multimodal models (Any-to-Any and VLM+ASR pipelines) on Traditional Chinese Question Answering. The dataset contains 900 items (450 image-text, 450 audio-text pairs) from authentic proficiency tests. Models are evaluated using exact-match accuracy (A/B/C/D selection) and latency measured as preprocessing + inference time. The evaluation uses specific Traditional Chinese prompts with output constrained to single letters. Open-source Any-to-Any models and VLMs using Whisper-large for transcription are compared against closed-source models on NVIDIA A100-SXM4 80GB hardware.

## Key Results
- Open-source any-to-any models show 40+ percentage point performance gaps between audio-text (77-89% accuracy) and image-text (34-48% accuracy) tasks
- End-to-end any-to-any pipelines achieve 2-3x latency improvements over VLM+ASR cascades on audio tasks
- UnifiedIO-2-XL exhibits systematic failure modes including echoing options and always selecting "A"

## Why This Works (Mechanism)

### Mechanism 1
- Claim: End-to-end any-to-any multimodal pipelines provide significant latency advantages over cascaded VLM + ASR approaches for audio tasks.
- Mechanism: Unified architectures process multiple modalities in a single forward pass through shared representations, eliminating the sequential overhead of running separate ASR transcription followed by VLM inference.
- Core assumption: The unified representation space enables efficient cross-modal reasoning without information loss from intermediate transcription steps.
- Evidence anchors:
  - [abstract] "End-to-end any-to-any pipelines demonstrate significant latency advantages over VLM plus ASR pipelines for audio tasks."
  - [section 4] "Open-source any-to-any models completed inference in a range of 467–744 seconds... In comparison, VLMs coupled with an ASR pipeline required 1,187–2,131 seconds."
  - [corpus] Cornserve paper (arXiv:2512.14098) addresses efficient serving systems for any-to-any models, supporting architectural efficiency claims.
- Break condition: Latency advantages may diminish if audio requires extensive pre-processing, or if the unified encoder becomes a computational bottleneck for long-form audio.

### Mechanism 2
- Claim: Models pretrained primarily on Simplified Chinese can achieve competitive performance on Traditional Chinese audio tasks but struggle with image-text tasks.
- Mechanism: Audio encoders (e.g., Whisper) provide language-agnostic acoustic representations that transfer across Chinese script variants, while vision components require precise orthographic understanding that differs significantly between Simplified and Traditional Chinese.
- Core assumption: The acoustic-phonetic properties of Mandarin speech remain consistent across regions, while visual character recognition is script-specific.
- Evidence anchors:
  - [section 4] "The Qwen2.5-Omni series and Baichuan-Omni-1.5... achieve competitive accuracy on Traditional Chinese inputs, particularly on audio-text tasks."
  - [section 4] "UnifiedIO-2-XL, with limited exposure to Chinese, often failed to produce meaningful answers."
  - [corpus] TCC-Bench (arXiv:2505.11275) confirms MLLMs exhibit limited effectiveness in non-Western cultural contexts, suggesting cultural/linguistic exposure matters.
- Break condition: Transfer effectiveness degrades when tasks require deep cultural knowledge or when Traditional Chinese contains idioms or vocabulary not present in Simplified Chinese training data.

### Mechanism 3
- Claim: Open-source any-to-any models exhibit asymmetric performance, excelling at audio-text tasks (77-89% accuracy) while underperforming on image-text tasks (34-48% accuracy).
- Mechanism: Audio encoders benefit from mature ASR pretraining (e.g., Whisper trained on 680K hours), while vision-language alignment for underrepresented scripts requires more sophisticated cross-modal grounding that current open-source models lack.
- Core assumption: The quality and diversity of modality-specific pretraining data directly impacts cross-modal reasoning capability.
- Evidence anchors:
  - [section 4] Qwen2.5-Omni-7B achieves 89.11% on audio-text but only 41.56% on image-text.
  - [section 4] "Results reveal a significant performance gap between open-source and closed-source models, especially in the image-text domain."
  - [corpus] Corpus evidence is limited on specific architectural causes; no directly comparable studies isolate this asymmetry mechanism.
- Break condition: Performance asymmetry may shift if vision encoders receive comparable multilingual pretraining scale, or if benchmark tasks disproportionately favor one modality.

## Foundational Learning

- **Concept: Any-to-Any Multimodal Architecture**
  - Why needed here: Understanding how unified models process text, image, and audio through shared representations is essential for interpreting latency and performance tradeoffs.
  - Quick check question: Can you explain why a single model processing audio directly differs architecturally from running ASR followed by a text model?

- **Concept: Script Variation in Chinese NLP**
  - Why needed here: Simplified and Traditional Chinese differ in character forms and orthographic conventions, directly affecting cross-script transfer and fine-tuning requirements.
  - Quick check question: What are two key differences between Simplified and Traditional Chinese that might affect vision-language model performance?

- **Concept: Latency vs. Accuracy Tradeoffs in Multimodal Systems**
  - Why needed here: The paper explicitly evaluates both accuracy and inference time, requiring understanding of when to prioritize each metric.
  - Quick check question: If a VLM+ASR pipeline achieves 2% higher accuracy but takes 3x longer than an any-to-any model, what factors would determine which to deploy?

## Architecture Onboarding

- **Component map:**
  - Any-to-any models: Unified encoder processes audio/image/text → shared transformer backbone → text output decoder
  - VLM + ASR pipeline: Audio → Whisper ASR → transcript → VLM (image + text encoder) → text output
  - Evaluation framework: JSON schema with question/options/answer fields, supports both modalities through file references

- **Critical path:**
  1. Load multimodal input (image file path or audio file path + question text)
  2. Preprocess according to modality (image normalization, audio feature extraction)
  3. Model inference with constrained output (single token: A/B/C/D)
  4. Extract answer via regex; fallback to random selection if extraction fails

- **Design tradeoffs:**
  - Latency vs. accuracy: Any-to-any offers 2-3x speedup on audio tasks but closed-source models achieve 15-25% higher accuracy
  - Modality coverage: VLM+ASR handles audio well but adds complexity; any-to-any simplifies deployment but may sacrifice vision performance
  - Language specificity: Fine-tuning on Traditional Chinese likely improves performance but requires curated data

- **Failure signatures:**
  - Models output echoes of options instead of selecting (UnifiedIO-2-XL echoed option descriptions in 78 cases)
  - Consistent selection of one option regardless of input (UnifiedIO-2-XL selected "A" in 807/900 cases)
  - Significant accuracy drop on image-text tasks compared to audio-text tasks for open-source models (>40 percentage point gap)

- **First 3 experiments:**
  1. **Baseline replication**: Run Qwen2.5-Omni-7B and Qwen2.5-VL-7B-Instruct on Multi-TW's 900 questions, measuring accuracy and latency per modality to reproduce reported benchmarks.
  2. **Ablation on audio duration**: Stratify audio-text results by audio length (short <60s, medium 60-120s, long >120s) to assess whether longer audio (avg 107.5s vs. OmniBench's 9.12s) correlates with performance degradation.
  3. **Cross-script fine-tuning test**: Fine-tune one open-source VLM (e.g., Qwen2-VL-7B) on a small Traditional Chinese image-text dataset (100-200 samples) and evaluate improvement on Multi-TW image-text subset to validate the paper's call for targeted fine-tuning.

## Open Questions the Paper Calls Out
None

## Limitations
- Data Accessibility and Bias: The Multi-TW dataset is not publicly available through standard repositories, requiring access via a Google Drive link. This limits reproducibility and raises concerns about potential selection bias in the proficiency test questions.
- Architectural Generalization: The performance advantages observed for any-to-any models on audio tasks are specific to the latency measurement setup and don't explore longer-form audio or degraded audio quality scenarios.
- Fine-tuning Generalization: While the paper identifies Traditional Chinese fine-tuning as a key need, it does not quantify how much data or what type of training would be required to achieve parity with Simplified Chinese models.

## Confidence
**High Confidence:**
- Open-source models show 40+ percentage point performance gaps between audio-text (77-89% accuracy) and image-text (34-48% accuracy) tasks.
- End-to-end any-to-any pipelines achieve 2-3x latency improvements over VLM+ASR cascades on audio tasks.
- UnifiedIO-2-XL's systematic failure modes (echoing options, always selecting "A") are reproducible through consistent output patterns across runs.

**Medium Confidence:**
- Pretrained Simplified Chinese models transfer effectively to Traditional Chinese audio tasks but struggle with image-text due to script-specific orthographic requirements.
- The need for Traditional Chinese fine-tuning is supported by performance gaps but lacks quantification of required dataset size or fine-tuning protocols.

**Low Confidence:**
- Claims about architectural efficiency of any-to-any models beyond latency (e.g., memory usage, scalability to longer sequences) are not empirically validated.
- The cultural specificity of Traditional Chinese challenges is asserted based on related work (TCC-Bench) but not directly measured in Multi-TW.

## Next Checks
1. **Data Diversity Audit:** Analyze the Multi-TW question distribution across difficulty levels, topics, and test formats to quantify potential biases and assess whether results generalize beyond the specific proficiency test context.

2. **Cross-Script Fine-tuning Experiment:** Conduct controlled fine-tuning experiments where open-source VLMs are trained on varying amounts (0, 50, 100, 500 samples) of Traditional Chinese image-text pairs to establish the data-efficiency curve and minimum requirements for performance parity.

3. **Audio Quality Robustness Test:** Evaluate model performance on Multi-TW audio tasks under varying quality conditions (original, compressed, noise-added, shortened) to determine whether latency advantages of any-to-any models hold when audio preprocessing complexity increases.