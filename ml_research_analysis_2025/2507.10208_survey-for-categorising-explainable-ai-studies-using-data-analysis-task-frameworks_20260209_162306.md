---
ver: rpa2
title: Survey for Categorising Explainable AI Studies Using Data Analysis Task Frameworks
arxiv_id: '2507.10208'
source_url: https://arxiv.org/abs/2507.10208
tags:
- studies
- users
- data
- systems
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study surveyed XAI research in data analysis tasks to categorize
  studies and resolve contradictions. Drawing on frameworks from visual analytics,
  cognition, and dashboard design, the authors proposed categorizing tasks across
  three dimensions: what (data type and complexity), why (sense-making and decision-making
  type), and who (user expertise in domain, analysis, and AI).'
---

# Survey for Categorising Explainable AI Studies Using Data Analysis Task Frameworks

## Quick Facts
- arXiv ID: 2507.10208
- Source URL: https://arxiv.org/abs/2507.10208
- Reference count: 40
- Primary result: XAI study contradictions stem from mismatched task characteristics, user expertise, and explanation types—proper alignment predicts outcomes.

## Executive Summary
This study surveyed XAI research in data analysis tasks to categorize studies and resolve contradictions. Drawing on frameworks from visual analytics, cognition, and dashboard design, the authors proposed categorizing tasks across three dimensions: what (data type and complexity), why (sense-making and decision-making type), and who (user expertise in domain, analysis, and AI). They found major problems in the literature: inadequate task descriptions, context-free studies, and insufficient testing with target users. The authors recommend studies report specific user expertise levels and follow guidelines for designing and reporting XAI tasks. By applying their framework, they explained contradictory findings in automation bias, global explanations, and explainability in conversational AI, showing how task characteristics impact results.

## Method Summary
The authors conducted a three-step literature search via Google Scholar with keyword filtering (sense making, exploratory data analysis, XAI, human-AI collaboration, etc.), excluding technical-only papers. They analyzed 13 survey papers, 9 studies, 1 book on data analysis, 11 AI survey papers, and 37 empirical XAI/CAI studies. The categorization framework maps studies across three dimensions: What (data type/complexity), Why (sense-making and decision-making type), and Who (user expertise in domain, analysis, and AI).

## Key Results
- XAI study contradictions stem from mismatched task characteristics, user expertise, and explanation types—proper alignment predicts outcomes
- Studies with realistic tasks and motivated participants generalize; context-free studies with crowd workers produce satisficing behavior that masquerades as XAI effects
- Different expertise profiles require systematically different explanation types and levels of detail

## Why This Works (Mechanism)

### Mechanism 1: Task-User-Explanation Alignment Reduces Contradictions
- Claim: XAI study contradictions stem from mismatched task characteristics, user expertise, and explanation types—proper alignment predicts outcomes.
- Mechanism: The framework decomposes studies along three dimensions (what: data type/complexity; why: sense-making and decision-making type; who: user expertise). When dimensions align (e.g., prognostic tasks + data-driven + domain experts), explanation effectiveness becomes predictable. Misalignment (e.g., crowd workers on prognostic tasks with no stake) produces automation bias artifacts.
- Core assumption: User behavior in XAI studies is primarily driven by task characteristics and expertise, not random variation.
- Evidence anchors:
  - [abstract] "By applying their framework, they explained contradictory findings in automation bias, global explanations, and explainability in conversational AI, showing how task characteristics impact results."
  - [section 6.1] Studies with domain experts + realistic tasks showed weaker or no automation bias; studies with crowd workers on artificial tasks showed strong bias. Five automation bias studies used Amazon Turk participants with no stake in tasks.
  - [corpus] Related surveys (arXiv:2502.09849, arXiv:2510.12201) similarly emphasize context-dependent evaluation but lack the explicit dimensional reconciliation framework.
- Break condition: If user behavior is driven primarily by individual personality traits rather than task/expertise dimensions, the framework's predictive power degrades.

### Mechanism 2: Expertise-Contingent Explanation Preferences
- Claim: Different expertise profiles require systematically different explanation types and levels of detail.
- Mechanism: Domain experts rely more on global explanations (understanding model behavior) and can make decisions from raw data—they prefer brief, actionable responses. Analysis experts want control over methods and workflow steps. AI experts demand model performance details (model cards, training data). Novices need simplified explanations without technical jargon (e.g., SHAP values are uninterpretable).
- Core assumption: Expertise is stable within a domain and measurably affects explanation processing.
- Evidence anchors:
  - [section 5.1] "Higher domain knowledge leads to decreased reliance on local explanations as domain experts can make decisions just by looking at raw data... Therefore, domain experts rely more on global explanations."
  - [section 6.3] "Domain experts, regardless of their analytical skills always wanted CXAI systems to verbally explain statistical processes... Analytical experts wanted control over defining every step."
  - [corpus] Weak direct validation—neighbor papers discuss human-centered evaluation but don't systematically test expertise as a moderating variable.
- Break condition: If expertise levels correlate with preference dimensions not captured (e.g., cognitive style, risk tolerance), the explanation-matching logic becomes incomplete.

### Mechanism 3: Task Realism Determines External Validity
- Claim: Studies with realistic tasks and motivated participants generalize; context-free studies with crowd workers produce satisficing behavior that masquerades as XAI effects.
- Mechanism: When users have no stake, they exhibit satisficing—minimal effort decisions without thorough reasoning. This creates false automation bias signals and inflated mental workload measures. Domain experts performing real tasks show more appropriate AI reliance and more consistent behavior.
- Core assumption: Motivation/stake affects cognitive effort allocation in predictable ways that can be distinguished from explanation quality effects.
- Evidence anchors:
  - [section 6.1] "Amazon turk participants aim to perform many tasks as fast as possible and thus do not put in as much effort compared to participants testing systems meant to help them at their jobs."
  - [section 7.4] "When users have no stake, interest, or experience in task, they exert less effort and thus do not reflect how the actual target groups would behave."
  - [corpus] Not directly addressed in neighbor papers—this is a distinctive contribution.
- Break condition: If intrinsic motivation varies unpredictably even among domain experts in real tasks, realism alone won't guarantee validity.

## Foundational Learning

- Concept: **Parasuraman's 10 Levels of Automation**
  - Why needed here: The paper references these levels (Table 1) to classify AI assistance—from full human control (Level 1) to full automation (Level 10). Understanding where a system sits is prerequisite to predicting explanation needs and user agency concerns.
  - Quick check question: If a system suggests one alternative and executes it after human approval, which automation level is this?

- Concept: **Local vs. Global Explanations**
  - Why needed here: The framework hinges on distinguishing local explanations (feature contributions for specific predictions, counterfactuals) from global explanations (model cards, feature importance across all predictions). Different task types and expertise levels require different balances.
  - Quick check question: A SHAP plot showing which features contributed to a single patient's diagnosis prediction—is this local or global?

- Concept: **Exploratory vs. Confirmatory vs. Inferential Analysis**
  - Why needed here: The "Why" dimension distinguishes sense-making types. Exploratory = no prior hypothesis; confirmatory = testing a hypothesis; inferential = explaining phenomena with incomplete data. Each has different decision points and AI assistance needs.
  - Quick check question: A clinician trying to understand why hospital readmission rates increased, without having all relevant variables—which analysis type?

## Architecture Onboarding

- Component map: User Profiler → [Domain expertise, Analysis expertise, AI expertise] → Task Classifier → [Sense-making type (D/P/I), Decision type (E/C), Data type (Q/I), Complexity, Severity] → Explanation Selector ← maps (User profile × Task class) → explanation type recommendations → Study Validator → checks: task realism, target user match, reporting completeness

- Critical path: User profiling → Task classification → Explanation selection. If any dimension is unspecified, the system cannot reliably predict explanation effectiveness.

- Design tradeoffs:
  - **Precision vs. practicality**: Measuring expertise objectively (certifications, years) is more accurate but harder to collect than self-reports.
  - **Task realism vs. experimental control**: Real tasks with domain experts are harder to instrument but more valid; synthetic tasks with crowd workers are easier but may produce artifacts.
  - **Explanation completeness vs. cognitive load**: More explanations can overwhelm (Section 2); fewer may be insufficient for trust calibration.

- Failure signatures:
  - Automation bias observed with crowd workers on artificial tasks → suspect satisficing, not real XAI effect
  - Global explanations show no effect → check if task is information-driven (users can verify visually) vs. data-driven
  - Domain experts ignore explanations → check if they can decide from raw data (high domain knowledge) or if explanations are too technical (SHAP values for novices)

- First 3 experiments:
  1. **Replication with expertise stratification**: Take an existing XAI study that found automation bias; replicate with (a) domain experts, (b) analysis experts, (c) crowd workers on the same task. Hypothesis: Expertise moderates bias magnitude.
  2. **Explanation type A/B test by task type**: For prognostic vs. diagnostic tasks, test local-only vs. local+global explanations. Hypothesis: Prognostic tasks benefit more from global explanations (per Section 6.2).
  3. **Task realism manipulation**: Same XAI system, same user population, but (a) artificial task with no stake vs. (b) realistic task with consequences. Hypothesis: Realism affects effort and automation bias measures.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do domain experts negotiate pre-existing hypotheses with AI explanations during confirmatory data analysis, compared to the exploratory tasks dominant in current research?
- Basis in paper: [explicit] Section 5.2.2 states that focusing on exploratory analysis "leaves a gap in understanding confirmatory analysis," and Section 7.3 notes this "limits our understanding of how users negotiate their hypotheses in confirmatory with XAI explanations."
- Why unresolved: The majority of XAI studies present recommendations before users form opinions (exploratory), failing to capture the workflow of experts who analyze data to confirm specific hypotheses.
- What evidence would resolve it: User studies that specifically task domain experts with validating or rejecting pre-defined hypotheses using XAI tools, measuring trust and reliance behaviors.

### Open Question 2
- Question: To what extent does the data type of a predicted value (categorical vs. ordered) impact mental workload and decision-making in XAI-assisted tasks?
- Basis in paper: [explicit] Section 5.3.3 states, "While no XAI studies to our knowledge compared categorical and ordered decisions, frameworks of mental workload explain that reducing the range of choices can lower the mental workload required to decide."
- Why unresolved: Existing literature has not empirically tested how the cognitive load differs when users must accept/reject a binary category versus estimating a continuous value (e.g., risk score).
- What evidence would resolve it: Comparative experiments measuring mental workload and decision accuracy for tasks involving categorical predictions versus ordered numerical predictions.

### Open Question 3
- Question: How should XAI systems support "inferential tasks" where users must explain phenomena using incomplete data?
- Basis in paper: [explicit] Section 5.2.1 proposes "inferential tasks" (e.g., explaining hospital performance without all variables) as a distinct category from diagnostic or prognostic tasks, noting a lack of research in this area.
- Why unresolved: Current task frameworks assume data is either complete (diagnostic) or predictive of the future (prognostic), overlooking common real-world scenarios where users must explain the present with missing variables.
- What evidence would resolve it: Studies evaluating XAI designs tailored for explanatory tasks with missing data, assessing how different explanation types aid inference.

## Limitations
- Framework's applicability beyond data analysis contexts remains untested, as the study corpus heavily emphasizes clinical and biomedical domains
- Expertise level operationalization relies on self-reported data in most studies, introducing measurement uncertainty
- Proposed explanation selection logic assumes linear relationships between expertise levels and explanation preferences, potentially oversimplifying complex cognitive interactions

## Confidence
- **High confidence**: Task-User-Explanation Alignment principle (supported by multiple contradictory finding resolutions across automation bias, global explanations, and CAI studies)
- **Medium confidence**: Expertise-Contingent Explanation Preferences (supported by theoretical arguments and limited empirical observations, but lacks systematic testing)
- **Low confidence**: Task Realism determines External Validity (based primarily on qualitative observations rather than experimental validation)

## Next Checks
1. **Cross-domain validation**: Apply the framework to XAI studies from non-clinical domains (finance, manufacturing, social media analysis) to test generalizability of the What-Why-Who dimensions

2. **Experimental expertise manipulation**: Design controlled studies where the same participants perform identical tasks with varying levels of domain/analysis/AI expertise to isolate expertise effects on explanation preferences

3. **Task realism controlled experiment**: Create identical XAI systems tested with both crowd workers on artificial tasks and domain experts on realistic tasks to quantify the impact of motivation/stake on automation bias and explanation effectiveness metrics