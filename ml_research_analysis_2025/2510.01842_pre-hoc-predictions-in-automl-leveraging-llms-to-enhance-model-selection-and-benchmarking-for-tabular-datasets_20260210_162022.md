---
ver: rpa2
title: 'Pre-Hoc Predictions in AutoML: Leveraging LLMs to Enhance Model Selection
  and Benchmarking for Tabular datasets'
arxiv_id: '2510.01842'
source_url: https://arxiv.org/abs/2510.01842
tags:
- dataset
- automl
- available
- pre-hoc
- datasets
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores pre-hoc AutoML model selection, which aims
  to predict the best-performing model for a dataset before running exhaustive searches.
  Traditional AutoML libraries rely on post-hoc model selection, requiring significant
  computation to train and test multiple models.
---

# Pre-Hoc Predictions in AutoML: Leveraging LLMs to Enhance Model Selection and Benchmarking for Tabular datasets

## Quick Facts
- arXiv ID: 2510.01842
- Source URL: https://arxiv.org/abs/2510.01842
- Reference count: 20
- Primary result: LLM-based pre-hoc model selection achieves 38% family accuracy on tabular classification datasets

## Executive Summary
This paper addresses the computational inefficiency of traditional post-hoc AutoML by proposing pre-hoc model selection methods that predict optimal algorithms before extensive training. The authors develop both traditional machine learning predictors and LLM-based approaches to select appropriate models from dataset metadata and descriptions. Their experiments on 175 tabular classification datasets from the AWS AutoGluon portfolio demonstrate that pre-hoc predictions can achieve reasonable accuracy (up to 61% for traditional methods) while potentially reducing the computational burden of exhaustive AutoML searches.

## Method Summary
The authors propose two main approaches for pre-hoc AutoML model selection. First, they develop traditional machine learning models that use statistical metadata and dataset descriptions to predict the best-performing model family. Second, they create an LLM-based AutoML agent using retrieval-augmented generation (RAG) to interpret dataset characteristics and recommend suitable models. The methodology is evaluated on the AWS AutoGluon portfolio dataset, which contains 175 tabular classification datasets from OpenML. The study compares the performance of both approaches in terms of family accuracy (selecting the correct model family) and model accuracy (selecting the specific best model).

## Key Results
- Traditional pre-hoc predictors achieve up to 61% accuracy in selecting the correct model family
- LLM-based approach achieves up to 38% family accuracy and 16% model accuracy
- Dataset characterization through metadata and textual descriptions can effectively inform model selection
- Pre-hoc predictions offer a promising direction for reducing computational overhead in AutoML workflows

## Why This Works (Mechanism)
The approach leverages the intuition that datasets with similar statistical properties and characteristics tend to perform well with similar types of machine learning models. By analyzing metadata such as feature counts, instance counts, missing values, and textual descriptions of dataset characteristics, the system can predict which model families are likely to perform best without exhaustively training multiple models. The LLM component adds the ability to interpret nuanced dataset descriptions that may capture characteristics not easily quantified in traditional metadata.

## Foundational Learning
- Tabular dataset characterization: Understanding how to represent dataset properties mathematically (why needed: to create meaningful features for prediction models; quick check: verify statistical measures capture dataset complexity)
- AutoML model portfolio knowledge: Familiarity with different model families and their strengths/weaknesses (why needed: to map dataset characteristics to appropriate algorithms; quick check: validate model performance patterns across dataset types)
- Retrieval-augmented generation: Understanding how to combine LLMs with external knowledge retrieval (why needed: to ground model recommendations in dataset-specific information; quick check: ensure RAG retrieves relevant dataset characteristics)

## Architecture Onboarding

**Component Map:**
Dataset Metadata Extraction -> Feature Engineering -> Traditional ML Predictor OR LLM Agent -> Model Recommendation

**Critical Path:**
Dataset → Metadata Extraction → Feature Vector → Prediction Model → Recommended Model Family

**Design Tradeoffs:**
The paper trades prediction accuracy for computational efficiency, accepting lower accuracy (38% vs 61%) from the LLM approach in exchange for potentially better generalization and interpretability. The traditional ML approach requires careful feature engineering and may miss nuanced dataset characteristics that textual descriptions could capture.

**Failure Signatures:**
Poor performance when datasets have atypical characteristics not well-represented in the training corpus, or when textual descriptions are vague or misleading. The LLM approach may struggle with highly structured tabular data that doesn't align with its natural language processing strengths.

**First Experiments:**
1. Test pre-hoc prediction accuracy on a held-out validation set of datasets
2. Compare computational time of pre-hoc prediction versus traditional exhaustive AutoML
3. Analyze feature importance to understand which dataset characteristics most influence model selection

## Open Questions the Paper Calls Out
The paper does not explicitly call out specific open questions, but several implicit questions remain regarding the generalizability of results beyond classification tasks and the cost-benefit tradeoff of using LLMs for pre-hoc predictions.

## Limitations
- Results are limited to 175 tabular classification datasets from OpenML, potentially limiting generalizability
- LLM-based approach shows significantly lower accuracy (38% family accuracy) compared to traditional methods (61% accuracy)
- The effectiveness of RAG for tabular dataset analysis remains unproven, as LLMs typically excel with natural language rather than structured data
- Computational overhead of LLM-based predictions versus potential AutoML search savings is not clearly established

## Confidence
- High confidence in the methodological framework and experimental setup
- Medium confidence in the LLM-based approach's effectiveness, given its lower accuracy compared to traditional methods
- Medium confidence in the practical utility of pre-hoc predictions, pending cost-benefit analysis
- Low confidence in generalizability to non-tabular or non-classification problems

## Next Checks
1. Test the pre-hoc prediction approach on additional diverse tabular datasets from different domains to assess generalizability
2. Compare the computational cost of LLM-based pre-hoc predictions against the time saved by reducing exhaustive AutoML searches
3. Evaluate the approach on regression and multi-task learning problems to determine if the methodology extends beyond classification tasks