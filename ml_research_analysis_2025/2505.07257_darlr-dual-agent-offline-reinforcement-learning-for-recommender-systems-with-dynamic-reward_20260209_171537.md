---
ver: rpa2
title: 'DARLR: Dual-Agent Offline Reinforcement Learning for Recommender Systems with
  Dynamic Reward'
arxiv_id: '2505.07257'
source_url: https://arxiv.org/abs/2505.07257
tags:
- reward
- learning
- darlr
- offline
- recommender
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper identifies that inaccuracies in frozen reward functions
  and static uncertainty penalties hinder the effectiveness of model-based offline
  reinforcement learning (RL) for recommender systems. To address this, it proposes
  DARLR, a dual-agent framework consisting of a selector and a recommender.
---

# DARLR: Dual-Agent Offline Reinforcement Learning for Recommender Systems with Dynamic Reward

## Quick Facts
- arXiv ID: 2505.07257
- Source URL: https://arxiv.org/abs/2505.07257
- Reference count: 40
- Primary result: Dual-agent framework with dynamic reward shaping and uncertainty adaptation outperforms state-of-the-art on four benchmark datasets

## Executive Summary
This paper addresses fundamental limitations in model-based offline reinforcement learning for recommender systems, specifically the use of frozen reward functions and static uncertainty penalties that hinder performance. The authors propose DARLR, a dual-agent framework consisting of a Selector agent that identifies reference users and a Recommender agent that uses aggregated information from these references to make recommendations. The system iteratively refines reward estimations and dynamically adapts uncertainty penalties based on the representativeness of selected reference users, achieving superior performance compared to state-of-the-art methods.

## Method Summary
DARLR implements a dual-agent offline reinforcement learning framework where a Selector agent identifies reference users by balancing similarity and diversity, and a Recommender agent uses these references to refine reward estimations and make item recommendations. The Selector learns to maximize an intrinsic reward based on similarity and diversity gains, while the Recommender uses dynamically shaped rewards and uncertainty penalties that adapt based on the quality of the selected reference set. Both agents are trained using Advantage Actor-Critic (A2C) methods, with the Selector operating sequentially to build context for the Recommender.

## Key Results
- DARLR achieves superior cumulative rewards compared to state-of-the-art methods, with 35.22 vs 33.25 on KuaiRec dataset
- Dynamic reward shaping and uncertainty adaptation significantly mitigate inaccuracies in offline data
- The framework demonstrates robust performance across datasets with varying data sparsity levels
- Better balance between exploitation and exploration compared to static baseline methods

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Aggregating predicted rewards from dynamically selected reference users mitigates the propagation of errors inherent in static, frozen world models.
- **Mechanism:** A Selector agent identifies a set of reference users ($u_S$) for the current user by maximizing an intrinsic reward based on similarity and diversity. The Recommender then estimates the current user's reward by averaging the world model's predictions for the reference users: $\hat{r}(u, i) = \frac{1}{|u_{S,t}|} \sum_{u' \in u_{S,t}} \hat{r}(u', i)$.
- **Core assumption:** Users with similar interaction patterns provide a more robust signal for unseen items than noisy single-user predictions, and these neighbors can be effectively identified via RL.
- **Evidence anchors:** [abstract] "selector... identifies reference users... so that the recommender can aggregate information... and iteratively refine reward estimations"; [section 3.3.1] "Dynamic reward shaping can be implemented by averaging over the set... easing the impact of the inaccuracy within world models."
- **Break condition:** If the dataset is extremely sparse such that no similar users exist for a given user, or if the embedding space is poorly structured, the reference set may be uncorrelated with the target user, turning the averaging process into noise injection.

### Mechanism 2
- **Claim:** Normalizing the magnitude of reward changes by the "representativeness" of the reference set provides a dynamic uncertainty penalty superior to static heuristics.
- **Mechanism:** Instead of a static uncertainty penalty, the system calculates $P'_U = \frac{|\hat{r} - \hat{r}_{-1}|}{r_s^{sel} + r_d^{sel}}$. This mechanism posits that a large shift in predicted reward is risky but acceptable if the set of reference users used to calculate it is highly representative (high similarity $r_s$ and diversity $r_d$).
- **Core assumption:** The quality of the reference set (similarity + diversity) acts as a proxy for the reliability of the new reward estimate.
- **Evidence anchors:** [abstract] "...statistical features of the selected users guide the dynamic adaptation of an uncertainty penalty..."; [section 3.3.2] "When reward predictions deviate significantly... such abrupt changes introduce risks... [penalty] ensures that the uncertainty penalty remains dynamic."
- **Break condition:** If the similarity and diversity gains ($r_s, r_d$) are consistently low (e.g., in a very diverse dataset with few clusters), the denominator remains small, potentially keeping uncertainty penalties high and blocking exploration.

### Mechanism 3
- **Claim:** Decoupling the "user search" task (Selector) from the "item recommendation" task (Recommender) allows for specialized policy optimization where the Selector learns to maximize representation quality.
- **Mechanism:** The Selector acts as a distinct RL agent with its own policy $\pi^{sel}$ and intrinsic reward $r^{sel} = \hat{r} + \lambda_s r_s^{sel} + \lambda_d r_d^{sel}$. It runs sequentially within a recommendation step to build the context for the Recommender. This allows the system to treat "finding good references" as a learnable skill rather than a static k-NN lookup.
- **Core assumption:** The intrinsic reward (similarity + diversity + predicted reward) is a valid proxy for the utility of the reference set in improving the downstream recommendation policy.
- **Evidence anchors:** [section 3.2.1] "In an offline setting... simulated world model cannot directly teach the selector... Therefore, an intrinsic reward function designed for the selector is urgent."; [section 3] "DARLR consists of two RL agents: the selector and the recommender."
- **Break condition:** If the Selector's intrinsic reward is misconfigured (e.g., $\lambda_d$ is too high), the agent might prioritize selecting diverse but irrelevant users, degrading the Recommender's reward shaping.

## Foundational Learning

- **Concept:** Model-Based Offline Reinforcement Learning (MOPO/MBPO)
  - **Why needed here:** DARLR is built on top of the "world model" concept common in model-based offline RL. You must understand that the agent learns from a simulated environment (the world model) rather than real users, and that the "frozen reward function" criticized in this paper is a component of that world model.
  - **Quick check question:** Can you explain why a world model trained on sparse offline logs might produce inaccurate reward estimates for unseen user-item pairs?

- **Concept:** Actor-Critic Methods (A2C)
  - **Why needed here:** Both the Selector and Recommender agents are trained using Advantage Actor-Critic (A2C). Understanding the separation of the Actor (policy) and Critic (value function) is necessary to follow the gradient flow described in the paper.
  - **Quick check question:** In the context of the Recommender, what does the Critic estimate the value of?

- **Concept:** The Exploration-Exploitation Trade-off & The Matthew Effect
  - **Why needed here:** The paper explicitly addresses the "Matthew Effect" (rich get richer) where popular items dominate. The uncertainty penalty and entropy penalty are mechanisms to force exploration and prevent the policy from collapsing onto a few high-reward but potentially overestimated items.
  - **Quick check question:** How does the proposed dynamic uncertainty penalty ($P'_U$) specifically help in balancing exploitation (using high rewards) and exploration (avoiding risky/uncertain high rewards)?

## Architecture Onboarding

- **Component map:** Offline Logs -> World Model (DeepFM) -> Selector Agent ($\pi^{sel}$) -> Recommender Agent ($\pi^{rec}$) -> Recommendations

- **Critical path:**
  1. Pre-training: Train the World Model (reward predictor) using supervised learning on offline logs
  2. Initialization: Initialize the Recommender policy (optional but recommended: initialize using static reward shaping like ROLeR to prevent early noise)
  3. Dual-Agent Loop:
     - Recommender observes state $s$
     - Selector executes policy, samples $K_{sel}$ reference users sequentially
     - Compute intrinsic reward for Selector (Sim + Div)
     - Recommender computes shaped reward $\hat{r}(u, i)$ using the selected users and dynamic uncertainty $P'_U$
     - Recommender selects item $a$
     - Update both agents via gradient backpropagation (A2C)

- **Design tradeoffs:**
  - Static vs. Dynamic Reward: Static is faster and stable; Dynamic (DARLR) is more accurate but computationally heavier due to the Selector loop
  - Selector Window Size ($K_{sel}$): Small $K$ is faster but may miss diverse references; Large $K$ adds noise and latency. Paper suggests small $K$ for dense data (KuaiRec) and large $K$ for sparse data (KuaiRand)
  - Static vs. Dynamic Uncertainty: Static uncertainty fails to adapt to policy changes; Dynamic uncertainty adds robustness but requires careful tuning of similarity/diversity coefficients ($\lambda_s, \lambda_d$)

- **Failure signatures:**
  - Premature Termination: If $P'_U$ is too aggressive, the agent becomes too conservative, leading to short episode lengths (Length metric drops)
  - Reward Hacking: The Selector might learn to pick reference users that artificially inflate the reward prediction without actual relevance (overfitting to the intrinsic reward)
  - Divergence: If the dynamic reward changes too rapidly, the Critic may fail to converge, leading to unstable training curves

- **First 3 experiments:**
  1. Ablation on Selector Utility: Run DARLR vs. DARLR without the Selector (using only static world model rewards). Verify if the "Dynamic Reward" curve converges faster and higher as claimed in Fig 1
  2. Hyperparameter Sensitivity ($\lambda_s, \lambda_d$): Test the impact of similarity vs. diversity in the Selector's intrinsic reward. Confirm that removing diversity ($r_d^{sel}$) reduces performance, validating the "diversity gain" hypothesis
  3. Dataset Density Test: Evaluate performance on KuaiRec (Dense) vs. KuaiRand (Sparse). Verify the paper's claim that DARLR's relative improvement is robust across different data sparsity levels compared to baselines like ROLeR

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can dynamic reward shaping methods be effectively scaled to large-scale industrial recommender systems with millions of users and items?
- Basis in paper: [explicit] "In future work, the development of dynamic reward shaping methods tailored for large-scale datasets will be investigated."
- Why unresolved: The paper only validates DARLR on relatively small benchmark datasets (maximum ~27,000 users and ~10,000 items), while industrial systems operate at scales orders of magnitude larger. The selector's user selection process may face computational bottlenecks when the user base grows significantly.
- What evidence would resolve it: Experiments on industrial-scale datasets demonstrating that DARLR maintains computational efficiency and recommendation quality when scaled to millions of users and items, with analysis of any modifications needed for such scaling.

### Open Question 2
- Question: Can more principled methods for training the selector agent be developed to replace the heuristic-based intrinsic reward function?
- Basis in paper: [inferred] The paper states that "the simulated world model cannot directly teach the selector whether the selected users are representative enough of the current users. Therefore, an intrinsic reward function designed for the selector is urgent." The current approach uses hand-crafted similarity and diversity gains (λs and λd) which require careful tuning.
- Why unresolved: The selector's reward function relies on hand-designed heuristics combining similarity and diversity metrics with tunable coefficients. This creates dependency on domain expertise and extensive hyperparameter tuning, which may not transfer optimally across different recommendation domains.
- What evidence would resolve it: Development of a learning-based or self-supervised approach for selector training that automatically learns to identify representative reference users without requiring manually tuned similarity/diversity coefficients, showing comparable or superior performance across diverse datasets.

### Open Question 3
- Question: How does DARLR perform in scenarios with non-stationary user preferences and concept drift over extended time periods?
- Basis in paper: [inferred] The paper assumes user embeddings are "time-invariant" (e_u ∈ R^d_U) and focuses on static offline datasets. While the dynamic reward shaping adapts during training, the paper does not address how the system would handle fundamental shifts in user preferences that occur over longer timescales in production environments.
- Why unresolved: Real-world recommender systems must adapt to evolving user interests, seasonal trends, and concept drift. The current framework evaluates on static snapshots of user behavior without testing temporal robustness or adaptation mechanisms for persistent preference changes.
- What evidence would resolve it: Longitudinal experiments where user preferences systematically change over time, demonstrating DARLR's ability to detect and adapt to concept drift, possibly with extensions for incremental world model updates or mechanisms to identify when reference user selection needs fundamental recalibration.

### Open Question 4
- Question: Can large language models (LLMs) or meta-learning approaches effectively automate hyperparameter tuning in DARLR to reduce manual effort?
- Basis in paper: [explicit] "Further, to achieve more efficient hyperparameter tuning and reduce manual effort, the adoption of large language models (LLMs) [68] and meta-learning [54] in DARLR will be explored."
- Why unresolved: DARLR has five key hyperparameters (Ksel, λs, λd, λU, λE) that significantly impact performance and require dataset-specific tuning. The hyperparameter sensitivity analysis shows performance varies substantially across parameter choices, creating deployment friction.
- What evidence would resolve it: Implementation and evaluation of LLM-based or meta-learning approaches that can automatically determine near-optimal hyperparameter configurations for new datasets, achieving performance within a small margin of manually tuned baselines while substantially reducing tuning time and expertise requirements.

## Limitations
- Dual-agent architecture introduces significant computational overhead during inference due to sequential reference user selection
- Performance is highly sensitive to Selector's intrinsic reward configuration, requiring careful tuning of similarity/diversity coefficients
- Assumes existence of meaningful user clusters in embedding space, which may not hold in extremely sparse or noisy datasets
- Dynamic uncertainty penalty mechanism relies on representativeness metrics that may not fully capture true uncertainty in high-dimensional spaces

## Confidence
- **High Confidence:** The fundamental premise that frozen reward functions and static uncertainty penalties are limiting factors in offline RL for recommendation. The ablation experiments comparing DARLR with static baselines are convincing.
- **Medium Confidence:** The specific mathematical formulation of the dynamic uncertainty penalty ($P'_U = \frac{|\hat{r} - \hat{r}_{-1}|}{r_s^{sel} + r_d^{sel}}$) and its superiority over static alternatives, as this requires careful hyperparameter tuning that may not generalize across all datasets.
- **Medium Confidence:** The robustness claims across different data sparsity levels, as the evaluation covers only four datasets with varying but limited characteristics.

## Next Checks
1. Conduct stress tests on extremely sparse datasets where reference users may not exist, measuring performance degradation and identifying failure thresholds
2. Implement an ablation study isolating the Selector's contribution by comparing DARLR against a static reference selection baseline (e.g., k-NN) to quantify the value of learned reference selection
3. Test the hyperparameter sensitivity of $\lambda_s$ and $\lambda_d$ across a broader range of values and dataset types to establish more robust configuration guidelines