---
ver: rpa2
title: Multimodal Multi-Agent Empowered Legal Judgment Prediction
arxiv_id: '2601.12815'
source_url: https://arxiv.org/abs/2601.12815
tags:
- legal
- prediction
- judgment
- framework
- case
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of legal judgment prediction
  (LJP), which aims to predict court outcomes from case facts. Traditional methods
  struggle with multiple allegations, diverse evidence, and lack adaptability.
---

# Multimodal Multi-Agent Empowered Legal Judgment Prediction

## Quick Facts
- arXiv ID: 2601.12815
- Source URL: https://arxiv.org/abs/2601.12815
- Reference count: 0
- Primary result: Multi-agent framework achieves 0.872 Law Articles, 0.882 Charges, and 0.409 Terms of Penalty accuracy on Chinese legal cases

## Executive Summary
This paper addresses legal judgment prediction (LJP) by proposing JurisMMA, a novel multi-agent framework that simulates courtroom procedures through six specialized agent roles working across six stages. The framework decomposes trial tasks into structured stages including fact extraction, knowledge retrieval, first-instance judgment, objection generation, appeal construction, and second-instance review. To support this work, the authors introduce JurisMM, a large dataset of over 100,000 recent Chinese judicial records with both text and multimodal video-text data. Experiments demonstrate that JurisMMA significantly outperforms existing state-of-the-art methods across all key LJP tasks on both the new dataset and the benchmark LawBench.

## Method Summary
JurisMMA implements a six-stage multi-agent workflow where six distinct agents (Junior Judge, Senior Judge, Chief Judge, Assistant, Defendant, Legal Counsel) collaborate to predict court outcomes from case facts. The process begins with fact extraction and validation, followed by hybrid retrieval from a knowledge base containing 438 criminal law articles, 483 charges, and 46,365 precedents. The framework uses hybrid scoring (BM25 + cosine similarity) for precedent retrieval and pure dense retrieval for statutes/charges. Two-instance trial simulation enables error correction through structured appeals, where objections are generated if they exceed validity thresholds and appeals are constructed by linking objections to supporting laws. The system achieves accuracy scores of 0.872 for Law Articles, 0.882 for Charges, and 0.409 for Terms of Penalty on the test set.

## Key Results
- Achieves 0.872 accuracy for Law Articles prediction on JurisMM-Text
- Achieves 0.882 accuracy for Charges prediction on JurisMM-Text
- Achieves 0.409 accuracy for Terms of Penalty prediction on JurisMM-Text
- Outperforms all baseline methods on both JurisMM and LawBench benchmarks
- Demonstrates multimodal improvements with 83 high-quality video samples in JurisMM-Video

## Why This Works (Mechanism)

### Mechanism 1
Multi-agent role decomposition improves LJP accuracy by distributing reasoning across specialized agents with explicit procedural constraints. Six agents operate across six stages, each with bounded responsibilities—JJ extracts facts, SJ validates and summarizes, CJ synthesizes judgments, Assistant retrieves knowledge, Defendant objects, Legal Counsel constructs appeals. This prevents single-model error cascades.

### Mechanism 2
Hybrid retrieval from domain-specific legal knowledge bases grounds predictions in current statutes and precedents. Assistant retrieves from three sources—statutes, charge definitions, precedents. Hybrid scoring (α·BM25 + (1-α)·cosine) balances sparse keyword matching with dense semantic similarity for precedents; pure dense retrieval for statutes/charges.

### Mechanism 3
Two-instance trial simulation with structured appeals enables error correction through adversarial validation. Defendant generates objections if they exceed validity thresholds. Legal Counsel constructs appeal briefs by linking objections to supporting laws. Chief Judge re-executes earlier stages if appeal validity exceeds acceptance thresholds.

## Foundational Learning

- **Chain-of-Thought (CoT) Prompting:** Why needed here? Figure 1 references "COT Process"—JurisMMA relies on LLMs performing multi-step reasoning across stages. Without understanding CoT, you cannot debug agent prompts or trace failure cascades. Quick check: Can you explain why extracting facts before retrieving laws reduces hallucination risk compared to joint extraction-retrieval?

- **Hybrid Retrieval (BM25 + Dense):** Why needed here? Equation 5 combines sparse and dense scoring for precedent retrieval. Understanding when BM25 outperforms dense retrieval—and vice versa—is essential for tuning α. Quick check: Given a novel charge with unusual phrasing, which retrieval method is more likely to fail, and why?

- **Multi-Agent Orchestration:** Why needed here? Six agents with sequential dependencies require coordination logic. Understanding message passing, state management, and failure propagation is prerequisite to extending the framework. Quick check: If JJ extracts incorrect facts, which downstream agents will produce invalid outputs, and at which stage can this be detected?

## Architecture Onboarding

- **Component map:** Case text T → JJ (fact extraction) → SJ (validation, summarization) → Assistant (retrieval from D_law, D_charge, D_precedent) → CJ (first-instance judgment) → Defendant (objection generation) → Legal Counsel (appeal brief construction) → CJ (second-instance review, final judgment J')

- **Critical path:** Stage I (extraction) → Stage II (retrieval) → Stage III (judgment). Errors propagate forward; Stages IV-VI are corrective but optional if no valid objections.

- **Design tradeoffs:** Token cost averages ~20,000 tokens per case; multi-agent orchestration increases latency. Threshold sensitivity: τ (objection validity), ε (appeal acceptance), θ (law filtering), α (retrieval balance) are not tuned in-paper—defaults may not generalize. Jurisdiction lock-in: Knowledge base is Chinese Criminal Law (2023 Amendment); transfer requires full KB reconstruction.

- **Failure signatures:** Low retrieval precision leads to incorrect laws/charges. Objection explosion occurs if τ too low, degrading appeal quality. Appeal deadlock happens if ε thresholds are improperly set.

- **First 3 experiments:**
  1. Reproduce Table 1 baselines on JurisMM-Text test split (7,050 cases) with GPT-4o configuration (temperature=0).
  2. Ablate retrieval sources individually: Remove D_precedent only, D_law only, D_charge only.
  3. Sweep threshold ε from 0.1 to 0.9, measuring appeal acceptance rate, re-processing frequency, and final judgment accuracy.

## Open Questions the Paper Calls Out

- Can the performance gains observed in the small JurisMM-Video subset be sustained when scaling the multimodal dataset beyond the current 83 samples? The paper states the video subset contains only 83 "high-quality multimodal samples" yet suggests visual data improves prediction.

- How can the framework improve the prediction accuracy for "Terms of Penalty," which significantly lags behind charge and law article prediction? Table 1 shows Terms of Penalty accuracy is 0.409, markedly lower than Law Articles (0.872) and Charges (0.882).

- Can the high computational cost (approx. 20,000 tokens per case) be reduced while maintaining the benefits of the six-stage multi-agent workflow? Section 3.2 notes that token consumption "averages around 20,000 tokens per input," creating a high cost barrier.

## Limitations

- Knowledge base quality and currency are assumed but not validated through precision/recall metrics or coverage analysis of edge cases in the test set.
- Critical thresholds (τ, ε, θ, α) are not systematically tuned, potentially limiting generalizability to other jurisdictions or case types.
- Multi-agent collaboration benefits are demonstrated through ablation but individual agent contributions remain unclear due to lack of isolation studies.

## Confidence

- **High Confidence:** Overall performance improvements over baselines are credible with clearly specified methodology and substantial accuracy gains.
- **Medium Confidence:** Multi-agent role decomposition benefits are supported by ablation results, but individual agent contributions require further isolation.
- **Low Confidence:** Two-instance trial simulation specifically enabling error correction is weakly supported due to lack of isolated appeal stage ablation and objection acceptance rate reporting.

## Next Checks

1. Perform systematic threshold sensitivity analysis by sweeping ε (appeal acceptance threshold) from 0.1 to 0.9 on the test set, measuring appeal acceptance rate, re-processing frequency, and final judgment accuracy.

2. Conduct ablation studies removing each knowledge source (D_law, D_charge, D_precedent) individually to determine which contributes most to each prediction task, and validate retrieval precision@k on sample cases.

3. Implement and test a simplified framework where agents operate independently rather than sequentially, comparing performance to the full multi-agent pipeline to determine necessity of courtroom simulation workflow.