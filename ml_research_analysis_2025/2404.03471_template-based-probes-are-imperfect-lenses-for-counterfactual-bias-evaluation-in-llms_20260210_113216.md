---
ver: rpa2
title: Template-Based Probes Are Imperfect Lenses for Counterfactual Bias Evaluation
  in LLMs
arxiv_id: '2404.03471'
source_url: https://arxiv.org/abs/2404.03471
tags:
- bias
- gaps
- group
- text
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work examines the reliability of template-based probes for
  measuring bias in large language models (LLMs) through counterfactual evaluations.
  It finds that such probes systematically produce distorted bias measurements, particularly
  showing that LLMs classify text explicitly associated with the White race as negative
  at disproportionately elevated rates.
---

# Template-Based Probes Are Imperfect Lenses for Counterfactual Bias Evaluation in LLMs

## Quick Facts
- arXiv ID: 2404.03471
- Source URL: https://arxiv.org/abs/2404.03471
- Reference count: 40
- Key outcome: Template-based probes systematically distort LLM bias measurements, showing elevated error rates for explicitly White-associated text

## Executive Summary
This paper examines the reliability of template-based probes for measuring bias in large language models through counterfactual evaluations. The authors find that such probes systematically produce distorted bias measurements, particularly showing that LLMs classify text explicitly associated with the White race as negative at disproportionately elevated rates. This pattern is consistent across multiple LLMs, datasets, and classification methods, suggesting the distortions are intrinsic to the models and probe design rather than artifacts of specific implementations.

The authors hypothesize this arises from linguistic asymmetries in pretraining data—specifically markedness, where White race is typically unmarked (e.g., "president" vs. "Black president"), while templates explicitly mark it, making such text appear out-of-domain and leading to inflated error rates. Experiments with gender and sexuality groups support this markedness hypothesis. The results highlight the need for more rigorous bias evaluation methodologies, such as using datasets that correct for markedness or designing probes that test both marked and unmarked versions of text, to ensure bias measurements reflect true model behavior rather than artifacts of linguistic conventions.

## Method Summary
The authors evaluate bias in LLMs using template-based counterfactual probes across racial groups (American Indian, Asian, African American, Hispanic, Pacific Islander, White) for ternary sentiment classification. They construct three template datasets: Amazon (profession descriptions), NS-Prompts (prefixed with race adjectives), and Regard (relationship phrases). Models are evaluated using two pipelines: fine-tuning on SST5 with LoRA for larger models, and prompting with zero-shot, 9-shot, and zero-shot CoT approaches. The key metric is FPR gaps defined as d_M(X) = M(X) - M̄ where M is false positive rate, measuring whether one group is disproportionately misclassified as negative.

## Key Results
- LLMs show elevated Negative-Sentiment FPR for White-associated text across multiple models, datasets, and evaluation methods
- This pattern persists regardless of whether models are fine-tuned or prompted, suggesting the effect is intrinsic to model behavior
- Gender and sexuality experiments support the markedness hypothesis, showing similar distortions for explicitly marked "unmarked" groups
- The distortions appear to stem from linguistic asymmetries in pretraining data where White race is typically unmarked

## Why This Works (Mechanism)
The paper demonstrates that template-based probes create systematic measurement distortions through the interaction between template syntax and linguistic markedness in pretraining data. When templates explicitly mark groups that are typically unmarked in natural language (like White race), the resulting text appears out-of-domain to the model, leading to inflated error rates. This mechanism is supported by consistent patterns across diverse models and datasets, and experiments showing similar effects for gender and sexuality groups when explicitly marked.

## Foundational Learning
- **False Positive Rate (FPR) gaps**: Measure the difference between a group's FPR and the average FPR across all groups; needed to quantify bias in classification
- **Markedness in linguistics**: The tendency for some social groups to be unmarked (e.g., "president") while others are marked (e.g., "Black president"); quick check: examine whether templates consistently mark groups differently
- **Template-based counterfactual probes**: Evaluation methodology that systematically varies protected attributes in text templates; quick check: verify templates maintain semantic content while varying only the protected attribute
- **Linguistic domain shift**: When text structure differs from pretraining data distribution, causing model performance degradation; quick check: compare template text statistics to natural text distributions

## Architecture Onboarding
**Component Map**: Template construction -> Model evaluation (fine-tuning/prompting) -> FPR computation -> Gap analysis -> Hypothesis validation

**Critical Path**: Construct race-marked templates → Run model inference → Compute group-specific FPRs → Calculate FPR gaps → Analyze patterns across models/datasets

**Design Tradeoffs**: Template-based probes offer controlled, systematic evaluation but may introduce domain shift artifacts; natural text evaluation avoids templates but lacks systematic control over attribute variation

**Failure Signatures**: Inconsistent FPR gaps across models suggest implementation errors; missing the characteristic elevated White FPR gap suggests improper template construction or evaluation setup

**3 First Experiments**:
1. Verify template construction correctly marks each racial group across all three datasets
2. Run inference on SST5 validation set to confirm model classification accuracy before bias evaluation
3. Compute FPR for a single group (e.g., Asian) to verify the basic evaluation pipeline works

## Open Questions the Paper Calls Out
**Open Question 1**: Can an LLM pre-trained on a dataset with consistent linguistic marking eliminate the measurement distortions observed in standard models? This requires training a model from scratch using a dataset with consistent marking to isolate markedness effects.

**Open Question 2**: How can counterfactual bias evaluation methodologies be redesigned to control for markedness effects without introducing new artifacts? This involves designing probes that test both marked and unmarked versions of text or using neutral placeholders.

**Open Question 3**: To what extent does the ratio of explicitly marked vs. unmarked demographic terms in pretraining data correlate with the magnitude of bias measurement distortions? This requires analyzing training corpus composition and correlating it with observed measurement errors.

## Limitations
- The paper cannot conclusively distinguish between genuine model biases and artifacts introduced by template construction methodology
- Results may not generalize to other types of bias evaluation beyond sentiment classification or to naturally occurring text
- The markedness hypothesis, while supported, is not definitively proven as the sole mechanism driving observed distortions

## Confidence
**High Confidence**: Core empirical finding of systematic distortion in template-based probes is well-supported by consistent patterns across multiple models and datasets
**Medium Confidence**: Markedness hypothesis as primary explanation is plausible but alternative mechanisms cannot be fully excluded
**Low Confidence**: Generalizability to other bias contexts and evaluation methodologies remains uncertain

## Next Checks
1. Construct parallel template datasets that explicitly test both marked and unmarked versions of text for each demographic group to isolate whether markedness or domain shift drives distortions
2. Evaluate models on naturally occurring text containing racial/gender identifiers to determine whether the same bias patterns emerge without template constraints
3. Test alternative probe designs that vary template syntax while holding racial markers constant to assess whether linguistic form rather than markedness drives observed distortions