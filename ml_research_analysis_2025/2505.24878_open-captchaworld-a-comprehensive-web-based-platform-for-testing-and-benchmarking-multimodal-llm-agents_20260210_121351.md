---
ver: rpa2
title: 'Open CaptchaWorld: A Comprehensive Web-based Platform for Testing and Benchmarking
  Multimodal LLM Agents'
arxiv_id: '2505.24878'
source_url: https://arxiv.org/abs/2505.24878
tags:
- reasoning
- captcha
- agents
- arxiv
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Open CaptchaWorld, a web-based benchmark
  designed to evaluate the visual reasoning and interaction capabilities of multimodal
  LLM agents through CAPTCHA puzzles. The benchmark includes 20 diverse CAPTCHA types
  totaling 225 puzzles, annotated with a new metric called CAPTCHA Reasoning Depth,
  which quantifies the number of cognitive and motor steps required to solve each
  puzzle.
---

# Open CaptchaWorld: A Comprehensive Web-based Platform for Testing and Benchmarking Multimodal LLM Agents

## Quick Facts
- arXiv ID: 2505.24878
- Source URL: https://arxiv.org/abs/2505.24878
- Reference count: 40
- Primary result: Humans achieve 93.3% success vs. best model (Openai-o3) at 40.0% on 225 CAPTCHA puzzles across 20 types

## Executive Summary
Open CaptchaWorld introduces a web-based benchmark for evaluating multimodal LLM agents' visual reasoning and interaction capabilities through CAPTCHA puzzles. The platform features 225 puzzles across 20 diverse CAPTCHA types, annotated with a novel CAPTCHA Reasoning Depth metric that quantifies cognitive and motor steps required for solution. Experimental results reveal a substantial performance gap between humans (93.3%) and even the best-performing MLLM agents (40.0%), highlighting current limitations in multimodal agent reasoning and autonomy.

## Method Summary
The benchmark employs a standardized web-based evaluation framework where agents interact with CAPTCHA puzzles through both reasoning and motor actions. Each puzzle is annotated with CAPTCHA Reasoning Depth, a metric quantifying the number of cognitive and motor steps needed for solution. The evaluation measures success rates across 225 puzzles spanning 20 CAPTCHA types, comparing performance between humans (12 participants) and multiple MLLM agents including Openai-o3, GPT-4o, and Gemini 2.5 Pro.

## Key Results
- Humans achieve near-perfect performance at 93.3% success rate across all CAPTCHA types
- Best-performing MLLM agent (Openai-o3) reaches only 40.0% success rate
- Models demonstrate "overthinking" behavior, breaking tasks into excessive fine-grained steps compared to humans' more efficient, holistic approaches
- CAPTCHA Reasoning Depth effectively captures complexity variations across different puzzle types

## Why This Works (Mechanism)
The benchmark works by providing standardized, diverse CAPTCHA puzzles that require both visual perception and physical interaction capabilities. The CAPTCHA Reasoning Depth metric captures the cognitive complexity by counting required steps, creating a quantifiable measure of task difficulty. The web-based platform enables consistent evaluation conditions across different agents and human participants, while the diversity of CAPTCHA types tests various aspects of multimodal reasoning from pattern recognition to motor control.

## Foundational Learning

**CAPTCHA Reasoning Depth** - A metric quantifying cognitive and motor steps required to solve puzzles. Why needed: Provides objective measure of task complexity beyond simple pass/fail. Quick check: Validate correlation between depth scores and actual solution time.

**Multimodal Interaction Evaluation** - Assessment framework combining visual perception with physical interaction capabilities. Why needed: Captures real-world agent requirements beyond pure reasoning. Quick check: Test whether motor-only failures differ systematically from reasoning failures.

**Cross-Modal Benchmarking** - Standardized comparison between human and AI agent performance. Why needed: Establishes realistic baseline for evaluating agent autonomy. Quick check: Verify human performance consistency across repeated trials.

## Architecture Onboarding

**Component Map**: Web Interface -> CAPTCHA Generator -> Interaction Handler -> Step Tracker -> Evaluation Engine

**Critical Path**: CAPTCHA Generation → Agent Interaction → Step Recording → Success Evaluation

**Design Tradeoffs**: Static vs. dynamic CAPTCHA generation; automated vs. human-in-the-loop evaluation; quantitative metrics vs. qualitative behavioral analysis

**Failure Signatures**: 
- Motor control failures (inability to execute physical interactions)
- Visual reasoning failures (misinterpretation of visual patterns)
- Step decomposition failures (excessive or insufficient task breakdown)
- Timing failures (inability to complete within reasonable time constraints)

**First Experiments**:
1. Baseline performance comparison across all 20 CAPTCHA types
2. Analysis of failure modes by categorizing unsuccessful attempts
3. Correlation analysis between CAPTCHA Reasoning Depth and success rates

## Open Questions the Paper Calls Out
None identified in the provided material.

## Limitations
- Small human evaluation sample (12 participants) may not represent broader human capabilities across diverse demographics
- CAPTCHA Reasoning Depth metric represents novel measure requiring further validation of correlation with actual reasoning complexity
- Static evaluation setup may not reflect dynamic, real-time interaction scenarios encountered in practical deployments

## Confidence
- Benchmark design and implementation: High confidence - Well-documented methodology with sound technical foundation
- Human vs. model performance gap: Medium confidence - Compelling results but limited by small human sample size
- Model "overthinking" behavior: Medium confidence - Based on qualitative analysis of step-by-step outputs

## Next Checks
1. Expand human evaluation to larger, more diverse participant pool (n > 100) across different demographics
2. Conduct cross-validation studies testing CAPTCHA Reasoning Depth correlation with alternative complexity measures
3. Implement dynamic CAPTCHA generation and real-time interaction testing for more realistic deployment scenarios