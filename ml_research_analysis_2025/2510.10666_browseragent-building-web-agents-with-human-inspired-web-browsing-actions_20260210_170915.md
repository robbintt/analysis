---
ver: rpa2
title: 'BrowserAgent: Building Web Agents with Human-Inspired Web Browsing Actions'
arxiv_id: '2510.10666'
source_url: https://arxiv.org/abs/2510.10666
tags:
- arxiv
- page
- reasoning
- information
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: BrowserAgent is a web agent framework that directly interacts with
  raw web pages using human-inspired browsing actions like scrolling, clicking, and
  typing, avoiding reliance on external tools to convert web content into static text.
  It employs a two-stage training pipeline (SFT followed by Rejection Fine-Tuning)
  and introduces an explicit memory mechanism to store key conclusions across steps,
  enhancing reasoning in long-horizon tasks.
---

# BrowserAgent: Building Web Agents with Human-Inspired Web Browsing Actions

## Quick Facts
- **arXiv ID:** 2510.10666
- **Source URL:** https://arxiv.org/abs/2510.10666
- **Reference count:** 40
- **Primary result:** BrowserAgent-7B achieves ~20% improvement over Search-R1 on multi-hop QA tasks like HotpotQA, 2Wiki, and Bamboogle using only 5.3K training trajectories.

## Executive Summary
BrowserAgent introduces a web agent framework that directly interacts with raw web pages through human-inspired browsing actions like scrolling, clicking, and typing, avoiding reliance on external tools to convert web content into static text. It employs a two-stage training pipeline (SFT followed by Rejection Fine-Tuning) and introduces an explicit memory mechanism to store key conclusions across steps, enhancing reasoning in long-horizon tasks. Despite using significantly less training data than prior work, BrowserAgent-7B achieves around 20% improvement over Search-R1 on multi-hop QA tasks like HotpotQA, 2Wiki, and Bamboogle, demonstrating strong performance and scalability in complex web-based reasoning.

## Method Summary
BrowserAgent operates directly on raw web pages via Playwright through predefined browser actions (page operation, tab management, URL navigation, completion), bypassing external content parsers. The framework employs a two-stage training pipeline: Supervised Fine-Tuning (SFT) on 5.3K trajectories to establish basic capabilities, followed by Rejection Fine-Tuning (RFT) that samples 4 candidates per question, filters for mixed correct/incorrect outputs, and selects the correct answer with the most reasoning steps. An explicit memory mechanism stores structured conclusions across steps, allowing the model to maintain context over 30-step interactions. The system uses Ray-based parallelization with 64 concurrent browsers on a 32-CPU machine for data generation and inference.

## Key Results
- BrowserAgent-7B achieves ~20% improvement over Search-R1 on multi-hop QA tasks (HotpotQA, 2Wiki, Bamboogle)
- Memory mechanism with 30-step interactions outperforms no-memory (0.392 vs 0.348 avg EM)
- RFT consistently outperforms SFT across multi-hop tasks (HotpotQA: 0.458 vs 0.441 EM)
- Model uses only 5.3K training trajectories compared to larger datasets used by competitors

## Why This Works (Mechanism)

### Mechanism 1
Direct browser interaction via atomic actions enables finer-grained information acquisition than static text summarization. BrowserAgent uses Playwright to execute scrolling, clicking, typing, and tab management directly on raw accessibility trees, bypassing external parsers/summarizers. This forces the model to actively locate and integrate information rather than relying on pre-digested summaries.

### Mechanism 2
Explicit memory storage preserves intermediate conclusions, enabling longer reasoning chains without context overflow. The model writes structured `<conclusion>` tags during reasoning. These are extracted and carried forward as memory, while redundant raw observations are trimmed. This allows 30-step interactions within practical context windows.

### Mechanism 3
Two-stage training (SFT → RFT) with reasoning-path selection improves multi-hop reasoning without complex RL. SFT establishes answer format and basic capabilities on 5.3K trajectories. RFT samples 4 candidates per question, filters for mixed correct/incorrect outputs (EM), then selects the correct answer with the most reasoning steps—privileging deeper thinking patterns. Final RFT mixes filtered data with 80% of SFT data to preserve format learning.

## Foundational Learning

- **ReAct-style reasoning (Thought → Action → Observation loops)**
  - Why needed here: BrowserAgent iterates through thinking, memory updates, and action execution; understanding this loop is essential to trace agent behavior.
  - Quick check question: Can you explain why separating reasoning text from executable actions improves multi-step task performance?

- **Accessibility tree representation**
  - Why needed here: BrowserAgent receives Playwright-parsed accessibility trees as observations, not raw HTML or screenshots.
  - Quick check question: What information is preserved vs. lost when representing a webpage as an accessibility tree versus its visual rendering?

- **Rejection sampling / RFT fundamentals**
  - Why needed here: Stage 2 training filters model outputs by correctness and reasoning depth; understanding tradeoffs is critical for reproducing results.
  - Quick check question: Why might selecting the longest correct reasoning chain improve generalization, and when might it backfire?

## Architecture Onboarding

- **Component map:** Qwen2.5-7B-Instruct base model -> Playwright browser automation (accessibility tree parsing) -> Ray-based Verl-Tool server with FastAPI -> SFT -> RFT with filtered data

- **Critical path:**
  1. Environment setup (Playwright + Ray parallelization + local Wikipedia via Kiwix)
  2. Dataset synthesis (6-step for basic, 30-step for complex; GPT-4.1 generates reasoning-action trajectories)
  3. SFT training on 5.3K trajectories
  4. RFT data generation: sample 4 candidates per question, filter by EM, select longest correct reasoning chains
  5. RFT training on mixed dataset (80% SFT + filtered RFT)

- **Design tradeoffs:**
  - Action-space granularity: Fine-grained actions (scroll, hover) vs. coarse (page summary); paper claims finer control improves depth but increases interaction cost
  - Memory trimming vs. retention: Trimming raw observations enables longer horizons but risks losing detail; ablation shows net positive for multi-hop QA
  - RFT over RL: Simpler pipeline, lower infrastructure cost, but may not explore novel strategies as RL would

- **Failure signatures:**
  - Low EM but high LLM-judge scores: Answers semantically correct but format mismatched
  - Circular search patterns: Memory ablation shows agents may lose task context without intermediate conclusion storage
  - Underutilized step budget: With 30-step limit, average steps remain ~3–5; if this doesn't increase on harder tasks, scaling may not be effective

- **First 3 experiments:**
  1. Reproduce SFT → RFT gain: Train BrowserAgent-SFT on 5.3K data, then BrowserAgent-RFT with proposed filtering; compare HotpotQA EM scores to validate reported ~2% improvement
  2. Memory ablation: Disable `<conclusion>` extraction, run on HotpotQA subset; expect performance drop and observe circular reasoning patterns
  3. Step-scaling test: Evaluate BrowserAgent-RFT with max steps = 6, 15, 30; confirm performance scales with interaction budget per Table 4 trends

## Open Questions the Paper Calls Out

- How can BrowserAgent be adapted to generalize across diverse, real-world websites beyond the Wikipedia environment used for training?
- What "intelligent memory" architectures can effectively replace the current `<conclusion>` tagging mechanism to handle increasingly long-horizon tasks?
- How can the framework utilize multi-agent collaboration to improve task decomposition and information gathering?

## Limitations
- External validation gap: Improvement claims rely on authors' evaluation setup with offline Wikipedia dumps
- Hyperparameter opacity: Key RFT implementation details (EM filtering thresholds, prompt templates) are underspecified
- Generalization boundary: Strong performance on Wikipedia tasks doesn't guarantee effectiveness on real-world dynamic websites

## Confidence
- **High confidence:** BrowserAgent's architectural novelty (direct browser interaction, explicit memory, two-stage training) and its clear performance advantage on reported benchmarks
- **Medium confidence:** The claimed ~20% improvement over Search-R1, due to lack of external validation
- **Low confidence:** Scalability to open web environments and long-term generalization beyond structured QA tasks

## Next Checks
1. Independent reproduction: Replicate the SFT→RFT pipeline on HotpotQA using the described 5.3K trajectories and evaluate against Search-R1 under identical conditions to verify the ~20% improvement claim
2. Real-world deployment test: Deploy BrowserAgent on a sample of live websites with varying complexity to assess robustness beyond the controlled Wikipedia environment
3. Memory mechanism stress test: Systematically vary the `<conclusion>` extraction rules and observe impact on performance across tasks of increasing complexity to validate the claimed benefits of explicit memory storage