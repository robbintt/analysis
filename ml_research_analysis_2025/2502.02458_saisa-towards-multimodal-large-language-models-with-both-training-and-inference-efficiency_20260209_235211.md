---
ver: rpa2
title: 'SAISA: Towards Multimodal Large Language Models with Both Training and Inference
  Efficiency'
arxiv_id: '2502.02458'
source_url: https://arxiv.org/abs/2502.02458
tags:
- visual
- saisa
- tokens
- attention
- inference
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces SAISA, an architecture designed to improve
  both training and inference efficiency of multimodal large language models (MLLMs).
  The authors identify a key inefficiency in existing MLLMs - attention among visual
  tokens in self-attention blocks, which leads to quadratically growing computational
  costs during inference.
---

# SAISA: Towards Multimodal Large Language Models with Both Training and Inference Efficiency

## Quick Facts
- arXiv ID: 2502.02458
- Source URL: https://arxiv.org/abs/2502.02458
- Reference count: 40
- Key outcome: SAISA reduces inference FLOPs by 66% and training budget by 26% while achieving superior accuracy across multiple benchmarks compared to LLaVA-1.5

## Executive Summary
This paper introduces SAISA, an architecture designed to improve both training and inference efficiency of multimodal large language models (MLLMs). The authors identify a key inefficiency in existing MLLMs - attention among visual tokens in self-attention blocks, which leads to quadratically growing computational costs during inference. To address this, they propose NAAViT (No Attention Among Visual Tokens), a self-attention mechanism that eliminates attention among visual tokens. Building on NAAViT, SAISA directly aligns visual features with the input spaces of NAAViT self-attention blocks, reducing computational overhead in both self-attention blocks and feed-forward networks (FFNs). Using the same configuration as LLaVA-1.5, SAISA reduces inference FLOPs by 66% and training budget by 26%, while achieving superior performance in terms of accuracy across various benchmarks. Comprehensive ablation studies validate the effectiveness of SAISA across different LLMs and visual encoders.

## Method Summary
SAISA addresses the inefficiency of attention among visual tokens in MLLMs by introducing NAAViT, which eliminates self-attention among visual tokens while maintaining performance. The architecture projects visual features directly into each layer's attention input space, bypassing FFN computations on visual tokens. Training uses a two-stage approach: first pre-training a shared MLP projector on 558k samples, then fine-tuning with replicated layer-specific MLPs alongside the LLM on 665k samples. The method achieves 66% inference FLOPs reduction and 26% training budget reduction compared to LLaVA-1.5 while improving accuracy on multiple benchmarks.

## Key Results
- NAAViT achieves 36.0 vs 35.7 on MMMU and 56.0 vs 53.4 on OK-VQA compared to vanilla self-attention
- SAISA reduces inference FLOPs by 66% while maintaining or improving accuracy across benchmarks
- Training budget reduced by 26% using shared MLP projector initialization strategy

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Eliminating attention among visual tokens maintains or improves performance while reducing computational cost.
- Mechanism: NAAViT modifies self-attention so only text tokens serve as queries. Visual tokens are keys/values that text can attend to, but visual tokens never attend to each other. This reduces attention complexity from O((v+t)²) to O(t(v+t)).
- Core assumption: Visual-to-visual attention within the LLM is redundant because spatial relationships are already encoded by the vision encoder.
- Evidence anchors:
  - [abstract] "Our pilot experiment on LLaVA-1.5 shows that attention among visual tokens is highly redundant."
  - [Section 3.3, Table 1] NAAViT achieves 36.0 vs 35.7 on MMMU and 56.0 vs 53.4 on OK-VQA compared to vanilla self-attention.
  - [corpus] No direct corpus validation; related work (TopV, VISA) focuses on token pruning rather than attention pattern modification.
- Break condition: If downstream tasks require explicit relational reasoning between visual regions (e.g., spatial reasoning benchmarks), removing visual-to-visual attention could degrade performance.

### Mechanism 2
- Claim: Bypassing FFN computations on visual tokens reduces inference cost without degrading task performance.
- Mechanism: In SAISA, visual tokens are projected directly into each layer's attention input space. After attention, only text token hidden states proceed to FFNs. Visual tokens never pass through FFN layers.
- Core assumption: Visual representations from the encoder are sufficiently refined; they only need cross-modal alignment via attention, not additional feature transformation through FFNs.
- Evidence anchors:
  - [abstract] "SAISA directly aligns visual features with the input spaces of NAAViT self-attention blocks, reducing computational overhead in both self-attention blocks and feed-forward networks."
  - [Section 4.1] "Ti+1 = FFNi(Hi) ∈ Rt×h" — only text hidden states are updated by FFN.
  - [corpus] Not directly validated in corpus; EE-MLLM uses aligners to update visual tokens, suggesting alternative approaches find value in updating visual tokens.
- Break condition: Tasks requiring visual feature refinement during inference (e.g., fine-grained recognition with long reasoning chains) may suffer.

### Mechanism 3
- Claim: Pre-training a shared MLP for all layers provides effective initialization while minimizing trainable parameters.
- Mechanism: During pre-training, a single MLP is trained for all LLM layers. During fine-tuning, this shared MLP is replicated n times (for n layers) and each copy is fine-tuned independently alongside the LLM.
- Core assumption: Early in training, all layers benefit from similar visual-text alignment; layer-specific specialization can emerge during fine-tuning.
- Evidence anchors:
  - [Section 4.3] "To improve training efficiency, we further reduce the number of trainable parameters. Specifically, we train a shared MLP for all layers of the LLM."
  - [Table 7] Shared MLP pre-training achieves 36.9 MMMU vs 34.8 for full projector pre-training.
  - [corpus] No corpus validation for this specific training strategy.
- Break condition: With larger pre-training datasets, full projector pre-training may outperform shared MLP initialization.

## Foundational Learning

- Concept: Self-attention computational complexity
  - Why needed here: Understanding why O((v+t)²) becomes a bottleneck with hundreds of visual tokens.
  - Quick check question: Given 576 visual tokens and 64 text tokens, what fraction of attention computations involve visual-to-visual pairs?

- Concept: MLLM alignment architectures (embedding space vs cross-attention)
  - Why needed here: SAISA positions itself as a third approach that borrows advantages from both.
  - Quick check question: Why does embedding space alignment require fewer trainable parameters than cross-attention alignment?

- Concept: Vision encoder feature spaces
  - Why needed here: The claim that visual-to-visual attention is redundant assumes spatial relationships are already encoded by ViT.
  - Quick check question: What spatial information does a ViT patch embedding capture before entering the LLM?

## Architecture Onboarding

- Component map:
  Visual Encoder (CLIP-ViT-L/14-336) -> Projector (n MLPs) -> NAAViT Self-Attention -> FFN -> LLM Backbone

- Critical path:
  1. Image → Vision Encoder → Z (visual features)
  2. Z → Projector → Vi for each layer i
  3. Vi + Ti → NAAViT attention → Hi (text-only output)
  4. Hi → FFN → Ti+1
  5. Repeat for n layers

- Design tradeoffs:
  - Projector parameter count: n MLPs adds substantial parameters vs single projector; linear projection reduces this but may hurt performance (Table 8 shows 36.9→35.7 MMMU).
  - Shared vs layer-specific pre-training: Shared MLP is more parameter-efficient but may limit expressivity; better with small pre-training data (558k samples).
  - Visual token count: More tokens preserve detail but increase memory; SAISA keeps original count (576 for CLIP) for fair comparison.

- Failure signatures:
  - Performance degradation on spatial reasoning tasks suggests visual-to-visual attention was providing useful signal.
  - Pre-training instability with full projector suggests overfitting on small data (Table 7).
  - Mistral models outputting "Unanswerable" on OK-VQA indicates attention pattern changes may interact with tokenizer/prompting choices.

- First 3 experiments:
  1. Replicate Table 1 (NAAViT vs vanilla self-attention) on LLaVA-1.5 to validate the core redundancy claim on your infrastructure.
  2. Ablate FFN bypass: Apply FFN to visual tokens and measure performance vs FLOPs tradeoff.
  3. Test shared MLP initialization with different pre-training dataset sizes to identify the data regime where full projector pre-training becomes preferable.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the SAISA projector be redesigned to be more parameter-efficient while maintaining the benefits of per-layer self-attention alignment?
- **Basis in paper:** [explicit] The authors acknowledge that using distinct MLPs for each LLM layer introduces "a number of parameters that cannot be ignored" and suggest that a more efficient projector is needed.
- **Why unresolved:** The current design prioritizes simplicity and effectiveness over parameter economy, leaving the trade-off between projector complexity and alignment quality unexplored.
- **What evidence would resolve it:** A modified SAISA architecture utilizing a shared-weight or adapter-based projector that achieves comparable benchmark accuracy with significantly fewer trainable parameters.

### Open Question 2
- **Question:** How can the SAISA architecture be effectively adapted to handle complex visual inputs such as high-resolution images, multiple images, or video streams?
- **Basis in paper:** [explicit] The limitations section states that SAISA is "not yet capable of processing more complicated visual information," specifically listing high-resolution images, multiple images, and videos.
- **Why unresolved:** These scenarios involve significantly higher visual token counts or temporal dependencies, which may challenge the NAAViT mechanism’s assumption that visual-to-visual attention is redundant.
- **What evidence would resolve it:** Successful application of SAISA to video understanding or high-res benchmarks (e.g., varying aspect ratios) without performance degradation or loss of inference efficiency.

### Open Question 3
- **Question:** Does the removal of visual-to-visual attention impose a bottleneck on complex reasoning tasks in specialized domains like medicine?
- **Basis in paper:** [explicit] The authors note SAISA's capabilities are "limited in... solving problems in specific domains such as medicine" and suggest scaling up domain-specific data.
- **Why unresolved:** It is unclear if the performance limit is solely due to data scale or if the NAAViT architecture (which prevents visual tokens from attending to each other) removes necessary context for fine-grained diagnostic reasoning.
- **What evidence would resolve it:** A comparative study on medical VQA benchmarks (e.g., PathVQA) showing SAISA's performance relative to vanilla attention models after domain-specific fine-tuning.

## Limitations

- The generalizability of NAAViT's core claim that visual-to-visual attention is redundant across diverse vision tasks remains uncertain, as improvements were measured primarily on LLaVA-1.5.
- The 26% training budget reduction relies on a shared MLP strategy that may not scale well with larger pre-training datasets, as the break condition remains untested.
- SAISA's performance on specialized domains like medicine and spatial reasoning tasks is limited, with unclear whether the NAAViT architecture or data scale is the bottleneck.

## Confidence

**High Confidence (Mechanistic Claims):**
- NAAViT's attention pattern modification is correctly described and implementable
- SAISA's FLOPs reduction calculations are accurate (66% inference reduction claimed)
- The two-stage training procedure with shared MLP initialization is clearly specified

**Medium Confidence (Empirical Claims):**
- NAAViT maintains or improves performance across diverse benchmarks
- FFN bypass doesn't degrade performance on standard MLLM tasks
- Shared MLP pre-training is superior to full projector pre-training for small datasets

**Low Confidence (Generalizability Claims):**
- NAAViT's performance advantages extend to all MLLM architectures and vision tasks
- The 26% training budget reduction generalizes beyond the specific dataset used
- NAAViT won't degrade performance on spatial reasoning or fine-grained recognition tasks

## Next Checks

1. **Benchmark Diversity Test**: Evaluate NAAViT on spatial reasoning benchmarks (e.g., NLVR2, VCR) and fine-grained recognition tasks (e.g., CUB, iNaturalist) beyond the standard OK-VQA and MMMU. Measure performance degradation and identify specific task categories where visual-to-visual attention proves essential.

2. **Data Scaling Experiment**: Systematically vary pre-training dataset size from 100k to 5M samples while comparing shared MLP vs full projector initialization. Identify the exact data threshold where full projector pre-training becomes preferable, establishing the break condition for the efficiency claim.

3. **Attention Pattern Ablation**: Implement a hybrid attention mechanism where visual-to-visual attention is selectively enabled based on task requirements. Compare against pure NAAViT on tasks where it performs well and tasks where it potentially underperforms, quantifying the tradeoff between efficiency and task coverage.