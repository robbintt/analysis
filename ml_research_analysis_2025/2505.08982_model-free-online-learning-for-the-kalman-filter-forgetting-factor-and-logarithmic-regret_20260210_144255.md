---
ver: rpa2
title: 'Model-free Online Learning for the Kalman Filter: Forgetting Factor and Logarithmic
  Regret'
arxiv_id: '2505.08982'
source_url: https://arxiv.org/abs/2505.08982
tags:
- have
- forgetting
- regret
- prediction
- regression
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies online prediction for unknown linear stochastic
  systems using model-free learning. The core challenge is that standard recursive
  least-squares suffers from severe overfitting due to the highly imbalanced regression
  structure caused by the exponentially decaying Markov parameters of the Kalman filter.
---

# Model-free Online Learning for the Kalman Filter: Forgetting Factor and Logarithmic Regret

## Quick Facts
- arXiv ID: 2505.08982
- Source URL: https://arxiv.org/abs/2505.08982
- Reference count: 40
- Primary result: Introduces an exponential forgetting factor to re-balance regression, achieving $O(\log^3 N)$ regret bound for online Kalman filtering

## Executive Summary
This paper addresses the fundamental challenge of overfitting in model-free online learning for Kalman filtering. Standard recursive least-squares (RLS) fails due to the exponentially decaying Markov parameters of the system, creating an imbalanced regression structure. The authors propose a novel algorithm that introduces an exponential forgetting factor to re-balance the regression model, effectively injecting an inductive bias that matches the true system structure. This approach is mathematically equivalent to a generalized ridge regression and yields a sharper logarithmic regret bound of $O(\log^3 N)$ compared to prior $O(\log^6 N)$ bounds, validated through numerical experiments.

## Method Summary
The proposed Online Prediction with Forgetting (OPF) algorithm uses a "doubling trick" to manage bias-variance trade-off, partitioning time into epochs that double in length. In each epoch, the history length is set to $p = \beta \log T_l$. The key innovation is applying an exponential forgetting factor $\gamma$ to re-weight the regression vector $Z_{k,p}$ using a diagonal matrix $D_p$, effectively performing generalized ridge regression. This re-balancing mitigates overfitting caused by the decaying Markov parameters. The algorithm updates the parameter estimate recursively and predicts future observations using the scaled regression model.

## Key Results
- Achieves $O(\log^3 N)$ regret bound compared to prior $O(\log^6 N)$ bounds
- Introduces exponential forgetting factor to re-balance regression and prevent overfitting
- Uses Hanson-Wright inequality for tighter error bounds on regression coefficients
- Numerical experiments confirm improved prediction accuracy and reduced overfitting

## Why This Works (Mechanism)

### Mechanism 1
Applying a structured exponential forgetting factor $\gamma$ re-balances the regression model, mitigating overfitting caused by decaying Markov parameters. The algorithm re-weights the regression vector $Z_{k,p}$ using a diagonal matrix $D_p$ (Eq. 10). Unlike traditional forgetting which discards data, this scales older observations by $\gamma^t$ to match the exponential decay of the true system parameters ($C(A-LC)^t L$). This is mathematically equivalent to a Generalized Ridge Regression (Eq. 18) where the regularization term $\lambda G D_p^{-2}$ encodes the inductive bias that parameters decay over time.

### Mechanism 2
Gradually increasing the past horizon $p$ using a "doubling trick" manages the bias-variance trade-off inherent in long-term memory systems. The algorithm partitions time into epochs that double in length. In the $l$-th epoch, the history length is set to $p = \beta \log T_l$. This increases the regression dimension only logarithmically. Crucially, the "self-cancellation" property of the accumulation error across these epochs ensures that the residual bias does not accumulate linearly.

### Mechanism 3
Utilizing the Hanson-Wright inequality allows for a tighter bound on regression error compared to standard self-normalized martingale bounds. Standard analysis bounds the regression error by the log-determinant of the Gram matrix, which scales as $\log^2 N$. By treating the regression error as a quadratic form of Gaussian variables and applying the Hanson-Wright inequality, the authors establish a bound that scales essentially as $\log N$.

## Foundational Learning

- **Concept: Markov Parameters & System Identification**
  - **Why needed here:** The paper re-parameterizes the Kalman filter prediction into a linear regression $y_k = G_p Z_{k,p} + \text{bias} + e_k$. Understanding that $G_p$ represents the impulse response (Markov parameters) of the closed-loop filter is essential to grasp why "imbalance" occurs (parameters decay exponentially).
  - **Quick check question:** Can you explain why the parameters $C(A-LC)^t L$ decay in magnitude as $t$ increases, and how this creates an "imbalanced" regression problem for standard Least Squares?

- **Concept: Regret Analysis in Online Learning**
  - **Why needed here:** The performance metric is Regret $R_N$, the cumulative loss difference between the learner and the optimal steady-state Kalman filter. The goal is not just stability, but sub-linear (specifically logarithmic) growth of this error.
  - **Quick check question:** Does a regret of $O(\log^3 N)$ imply that the instantaneous prediction error $\|y_k - \tilde{y}_k\|$ goes to zero as $k \to \infty$? (Hint: No, it implies the *accumulated* difference from the optimal grows very slowly).

- **Concept: Inductive Bias via Regularization**
  - **Why needed here:** The "forgetting factor" is re-interpreted as a generalized Ridge regression. You must understand that penalizing $\lambda G D_p^{-2}$ is equivalent to telling the optimizer: "Prefer solutions where the coefficients for older time steps are smaller."
  - **Quick check question:** In Eq. (18), if $\gamma \to 1$, how does the regularization term change, and what does that imply about the model's assumptions regarding the decay of parameters?

## Architecture Onboarding

- **Component map:**
  - Input Buffer -> Scaling Module -> Recursive Estimator -> Epoch Controller

- **Critical path:**
  1. **Initialization:** Set $\gamma, \beta, \lambda$. Run "Warm-Up" phase to collect initial data.
  2. **Epoch Start:** Calculate new $p$ based on current epoch length ($p = \beta \log T_l$).
  3. **Prediction:** Compute $\tilde{y}_{k+1} = \tilde{G}_{k,p} \tilde{Z}_{k+1,p}$.
  4. **Update:** Observe true $y_k$, update $\tilde{V}_{k,p}$ and $\tilde{G}_{k,p}$ recursively (Eq. 15).

- **Design tradeoffs:**
  - **Choice of $\gamma$:** The paper suggests $\gamma \approx \rho(A-LC)$ is optimal. However, $\rho(A-LC)$ is unknown in model-free settings.
    - *Guidance:* The paper proves robustness for $\gamma \in (\rho(A-LC), 1]$. If you set $\gamma$ too small (underestimating stability), regularization error dominates. If $\gamma=1$ (standard RLS), you suffer from overfitting the small "tail" parameters of the Markov sequence, degrading performance.
  - **Bias vs. Variance ($\beta$):** Selecting $\beta$ controls how fast history length $p$ grows.
    - *Guidance:* Larger $\beta$ reduces truncation bias but increases the dimensionality of $G$, increasing estimation variance. The paper suggests $\beta$ proportional to $\kappa$ (Jordan block order) and inversely proportional to stability margin.

- **Failure signatures:**
  - **Regret Spikes at Epoch Boundaries:** If the initialization of $\tilde{G}$ at the start of a new epoch is done via direct matrix inversion (Eq. E.1), numerical instability causes regret spikes.
    - *Fix:* Use the iterative update law (Eq. 15) even during the transition or initialization phase.
  - **Linear Regret Growth:** If $p$ is not increased, or if $\gamma$ is chosen poorly, the regret plot will show linear growth instead of logarithmic flattening.

- **First 3 experiments:**
  1. **Visualize Imbalance:** Plot the ground truth Markov parameters $G_p$ vs. the estimates $\tilde{G}_p$ from standard RLS ($\gamma=1$) vs. OPF ($\gamma \approx \rho$). Verify that RLS "wiggles" excessively on the small-magnitude tail coefficients (Figure 1).
  2. **Ablation on $\gamma$:** Run the system with $\gamma=1$ (no forgetting), $\gamma \approx \rho$ (optimal), and $\gamma < \rho$ (aggressive forgetting). Plot the accumulation error and regularization error separately to verify the trade-off claimed in Figure 3.
  3. **Regret Scaling:** Run the algorithm for $N=1000, 10000, 100000$. Plot $R_N / \log^3 N$. The curve should remain bounded (or stabilize) as $N$ increases, confirming the theoretical bound.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: Can the proposed model-free framework with forgetting factors be extended to structured nonlinear systems while maintaining logarithmic regret guarantees?
- **Basis in paper**: [explicit] The conclusion explicitly identifies "extending the framework to nonlinear systems, such as special classes of structured nonlinear models," as a future direction.
- **Why unresolved**: The current theoretical analysis relies on linear dynamics and Gaussian noise properties (e.g., mutually uncorrelated innovations) which do not hold for general nonlinear systems.
- **What evidence would resolve it**: A theoretical extension of the regret analysis to a specific nonlinear class (e.g., using Koopman embeddings) or empirical validation showing the forgetting strategy mitigates overfitting in nonlinear settings.

### Open Question 2
- **Question**: Can an adaptive forgetting strategy be developed that maintains theoretical regret guarantees in time-varying or non-stationary environments?
- **Basis in paper**: [explicit] The conclusion lists "exploring adaptive forgetting strategies" as a primary avenue for future work.
- **Why unresolved**: The current regret bound assumes a fixed forgetting factor $\gamma$. An adaptive strategy would introduce dynamic hyperparameters, potentially violating the martingale concentration inequalities and bias bounds used in the proof.
- **What evidence would resolve it**: An algorithm that dynamically adjusts $\gamma$ with a rigorous proof of non-asymptotic regret bounds against a non-stationary optimal filter.

### Open Question 3
- **Question**: Does the $O(\log^3 N)$ regret bound strictly hold for the extension to systems with control inputs without requiring restrictive assumptions on input excitability?
- **Basis in paper**: [inferred] Appendix F.1 sketches an extension to systems with control inputs $u_k$ but provides only a summary of the regression form without a rigorous regret analysis.
- **Why unresolved**: Control inputs change the persistent excitation properties of the regressor. The interaction between the forgetting factor's balancing effect and the input signal statistics remains unanalyzed.
- **What evidence would resolve it**: A complete proof of the regret bound for the input-case, explicitly detailing any constraints on the input sequence (e.g., uniform boundedness or stochasticity) required to maintain the bound.

## Limitations
- The optimal forgetting factor $\gamma$ requires knowledge of system spectral radius $\rho(A-LC)$, which is unknown in truly model-free settings
- Theoretical regret bound relies on precise hyperparameter tuning that may be difficult to achieve in practice
- Current analysis assumes linear Gaussian systems, limiting applicability to nonlinear domains

## Confidence
- **High Confidence:** The core mechanism of using exponential forgetting to inject inductive bias (Mechanism 1) and the improved regret bound (Mechanism 3) are well-supported by both theoretical derivation and numerical experiments
- **Medium Confidence:** The effectiveness of the doubling trick for managing bias-variance trade-off (Mechanism 2) is primarily supported by the paper's theoretical analysis, with limited direct evidence from the corpus
- **Low Confidence:** The practical applicability of the algorithm in truly model-free settings is uncertain, as optimal performance requires knowledge of system parameters for hyperparameter tuning

## Next Checks
1. **Reproduce Numerical Experiments:** Implement the algorithm and reproduce Figure 1 to visually confirm the overfitting mitigation when using the forgetting factor versus standard RLS
2. **Ablation Study on $\gamma$:** Systematically vary $\gamma$ around the theoretical optimum and verify the claimed trade-off between regularization error and truncation bias
3. **Regret Scaling Verification:** Run the algorithm for multiple horizons ($N=10^3, 10^4, 10^5$) and plot $R_N / \log^3 N$ to confirm the theoretical regret bound