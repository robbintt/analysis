---
ver: rpa2
title: 'Evaluating book summaries from internal knowledge in Large Language Models:
  a cross-model and semantic consistency approach'
arxiv_id: '2503.21613'
source_url: https://arxiv.org/abs/2503.21613
tags:
- summary
- book
- summaries
- generated
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluates whether large language models (LLMs) can generate
  accurate book summaries from their internal knowledge alone, without access to the
  original text. Using a diverse set of well-known books, the research employs an
  LLM-as-a-judge paradigm where each model evaluates not only its own summaries but
  also those generated by others to mitigate self-preference bias.
---

# Evaluating book summaries from internal knowledge in Large Language Models: a cross-model and semantic consistency approach

## Quick Facts
- arXiv ID: 2503.21613
- Source URL: https://arxiv.org/abs/2503.21613
- Reference count: 10
- Primary result: LLMs can generate semantically coherent book summaries from internal knowledge, but factual accuracy is inconsistent, with average scores around 6-7/10

## Executive Summary
This study investigates whether large language models can generate accurate book summaries using only their internal knowledge, without access to the original text. Using a diverse set of 50 well-known books, the research employs a cross-model LLM-as-a-judge paradigm where each model evaluates summaries generated by all participating models, including its own. Results show that while semantic similarity is high (BERTScore ~0.85), detailed evaluations reveal significant omissions and factual errors in generated summaries, with average scores around 6-7 out of 10. The study highlights that standard metrics like ROUGE and BERTScore correlate weakly with human/judge quality assessments for detailed, long-form summaries.

## Method Summary
The study used six LLMs (Gemini 2.0, GPT-4o-mini, Llama 3.1-8b, Mistral-7b, Phi4-14b, DeepSeek-R1 14b) to generate summaries for 50 well-known books without access to source text. Each model generated summaries five times per book at temperature 0.4 to capture variance. A cross-model evaluation approach was employed where each model evaluated all generated summaries (including its own) against human reference summaries using a detailed 1-10 rubric. Evaluation metrics included ROUGE-1/2/L, BERTScore (F1), and LLM-as-judge scores. The study found that while semantic similarity was consistently high, detailed evaluations revealed significant factual errors and omissions.

## Key Results
- LLM-as-judge cross-evaluation revealed factual gaps missed by standard metrics (ROUGE, BERTScore)
- Semantic similarity was high (BERTScore ~0.85) but factual accuracy inconsistent (average scores ~6-7/10)
- ROUGE-1 scores showed large variance across repetitions (~50% error bars), while BERTScore remained consistent
- No significant correlations found between different metrics and judge scores for detailed summaries

## Why This Works (Mechanism)

### Mechanism 1: Superposition Theory and Polysemantic Representations
- **Claim:** LLMs can generate semantically coherent book summaries from internal knowledge, but detailed factual accuracy is inconsistent due to how training data is encoded and retrieved.
- **Mechanism:** The paper invokes Superposition Theory (Elhage et al., 2022): neural networks represent more features than dimensions by encoding overlapping representations in the same parameter space, enabled by the Johnson-Lindenstrauss Lemma. Books with redundant, well-distributed representations in the latent space produce better summaries; those with diffuse or mixed representations produce errors.
- **Core assumption:** Models have seen enough discussion of these well-known books during training to form retrievable representations—not necessarily the full text.
- **Break condition:** If a book's training exposure was minimal or highly fragmented, superposition yields noisy retrieval—summaries will conflate elements or hallucinate (see DeepSeek-R1's TV-adaptation conflation in Appendix B).

### Mechanism 2: Cross-Model LLM-as-Judge Evaluation
- **Claim:** Cross-model LLM-as-judge evaluation reduces single-model bias and surfaces factual gaps that n-gram and semantic similarity metrics miss.
- **Mechanism:** Each model generates summaries and then blindly evaluates all summaries (including its own) against human references using a detailed 1–10 rubric. Aggregating across judges reduces individual quirks and exposes systematic omissions.
- **Core assumption:** Models can reliably identify factual errors and omissions when comparing to a reference, even if they cannot avoid them when generating.
- **Break condition:** If judges share a common failure mode, cross-evaluation may converge on inflated scores. The paper mitigates this by including human-written references.

### Mechanism 3: Limitations of Standard Metrics
- **Claim:** Standard metrics (ROUGE, BERTScore) correlate weakly with human/judge quality assessments for detailed, long-form summaries.
- **Mechanism:** ROUGE captures n-gram overlap; BERTScore captures embedding-level semantic similarity. Neither captures whether key plot points, characters, or events are correctly included or omitted. Small text changes can preserve scores while altering factual accuracy.
- **Core assumption:** High semantic similarity implies the model "understood" the task, but not that the summary is factually complete.
- **Break condition:** For short, fact-light summaries, BERTScore may suffice; the gap emerges specifically for detailed, long-form content.

## Foundational Learning

### Concept: LLM-as-a-Judge Paradigm
- **Why needed here:** The paper's core evaluation method; understanding its biases and design is essential to interpreting results.
- **Quick check question:** Can you explain why a model might rate its own output higher, and how cross-model evaluation mitigates this?

### Concept: Superposition and Polysemantic Representations
- **Why needed here:** The theoretical lens through which the paper explains why some books are better-memorized than others.
- **Quick check question:** What does it mean for a single neuron to participate in representing multiple unrelated features?

### Concept: ROUGE vs. BERTScore vs. Factuality Metrics
- **Why needed here:** The paper demonstrates a gap between surface metrics and factual correctness; understanding this distinction is critical for anyone building summarization evaluation.
- **Quick check question:** If two summaries have identical BERTScore but one omits a key character, which metric would detect the difference?

## Architecture Onboarding

### Component Map
Generator models (6 LLMs) -> Generate summaries (5 calls/book, temp=0.4) -> Cross-model evaluation (each model judges all summaries) -> Metrics layer (ROUGE, BERTScore, LLM-as-judge scores) -> Score extraction (Gemini + manual verification) -> Analysis by book/generator/judge

### Critical Path
1. Select 50 heterogeneous books; obtain human reference summaries
2. Prompt each generator model (5 calls/book for variance, temperature=0.4)
3. Run cross-evaluation: each judge scores all summaries against references
4. Compute ROUGE/BERTScore; extract and aggregate LLM-as-judge scores
5. Analyze by book, generator, and judge; check for self-preference bias

### Design Tradeoffs
- **Temperature 0.4:** Balanced between factual adherence and "imagination" (internal knowledge retrieval). Lower temps may constrain creativity; higher may increase hallucination.
- **5 calls per book:** Addresses known LLM inconsistency but increases compute cost.
- **Blind cross-evaluation:** Removes explicit self-identification but cannot fully eliminate style-based recognition.

### Failure Signatures
- **Hallucinated adaptations:** Model summarizes TV/movie version instead of book (see DeepSeek-R1 on "The Man in the High Castle")
- **Lenient judge:** Mistral-7b assigns near-ceiling scores regardless of quality
- **Plagiarism false-positives:** Llama 3.1-8b incorrectly accuses summaries of being verbatim from Wikipedia

### First 3 Experiments
1. **Reproduce the cross-evaluation heatmaps** for a subset of 5 books and 3 models to validate the pipeline and scoring extraction.
2. **Ablate temperature:** Run generation at 0.2 and 0.6 for 2 books; compare ROUGE, BERTScore, and judge scores to quantify the tradeoff.
3. **Test a new book not in the original 50:** Choose a moderately well-known book; evaluate whether scores and failure modes generalize beyond the curated set.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Do LLMs exhibit different internal knowledge summarization capabilities when prompted in languages other than English?
- **Basis in paper:** Future work may extend linguistic coverage, as these models present different biases depending on the prompted language.
- **Why unresolved:** The study only tested English-language summaries; it remains unknown whether cross-lingual internal knowledge retrieval affects summary quality, hallucination rates, or factual accuracy.
- **What evidence would resolve it:** Replicate the cross-model evaluation methodology across multiple languages (e.g., Spanish, Chinese, French) for the same book corpus and compare scores.

### Open Question 2
- **Question:** Can LLMs generate accurate "spoiler-free" partial summaries from internal knowledge up to a specified point in a book?
- **Basis in paper:** It could be interesting to study the ability of these models to summarize only parts of books, a "spoiler-free" summary up to a certain part (e.g., 50%, 70%), while contrasting it with cut versions.
- **Why unresolved:** The study only evaluated full-book summaries; partial summarization requires the model to understand narrative structure well enough to identify and halt at specified plot points without revealing later events.
- **What evidence would resolve it:** Prompt models to summarize only the first X% of books with known page-chapter mappings, then evaluate factual accuracy and completeness against cut reference summaries.

### Open Question 3
- **Question:** Why do LLMs produce high semantic similarity scores (BERTScore ~0.85) yet exhibit significant factual omissions and errors in internal-knowledge summaries?
- **Basis in paper:** The paper finds no significant correlations between the different metrics and scores, demonstrating that semantic similarity metrics fail to capture factual accuracy in complex summarization tasks.
- **Why unresolved:** The disconnect suggests BERTScore captures surface-level semantic overlap while missing detailed factual correctness; the representational mechanisms that produce this gap remain unexplored.
- **What evidence would resolve it:** Fine-grained error taxonomy analysis comparing BERTScore components against human-annotated factual error types across generated summaries.

## Limitations
- Results limited to 50 well-known books and may not generalize to less canonical or niche literature
- Temperature setting (0.4) represents a tradeoff between factual adherence and creative retrieval, but impact of other settings unexplored
- Possibility of shared failure modes across models not fully addressed despite cross-model evaluation

## Confidence
- **High confidence** in the core finding that LLM-as-judge cross-evaluation reveals factual gaps missed by standard metrics
- **Medium confidence** in the superposition theory explanation for differential summary quality across books
- **Medium confidence** in the generalizability of results beyond the curated book set

## Next Checks
1. Reproduce cross-evaluation heatmaps for a subset of 5 books and 3 models to validate the pipeline and scoring extraction
2. Ablate temperature settings (0.2, 0.4, 0.6) for 2 books to quantify the tradeoff between factual adherence and hallucination
3. Test a new, moderately well-known book not in the original 50 to assess whether scores and failure modes generalize beyond the curated set