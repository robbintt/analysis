---
ver: rpa2
title: Monte Carlo Sampling for Analyzing In-Context Examples
arxiv_id: '2503.22002'
source_url: https://arxiv.org/abs/2503.22002
tags:
- performance
- examples
- in-context
- each
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The authors propose a Monte Carlo sampling method to analyze the
  impact of the number of in-context examples while controlling for effects from example
  selection and ordering. They find that performance plateaus at previously suggested
  numbers of examples (k=4, k=8) are not consistently observable across different
  permutations, indicating that guidance on the number of examples may not generalize
  across different example sets and orderings.
---

# Monte Carlo Sampling for Analyzing In-Context Examples

## Quick Facts
- **arXiv ID:** 2503.22002
- **Source URL:** https://arxiv.org/abs/2503.22002
- **Authors:** Stephanie Schoch; Yangfeng Ji
- **Reference count:** 34
- **Primary Result:** Performance plateaus at k=4 and k=8 are not consistently observable across different permutations; robust exemplar selection yields lower accuracy than random selection.

## Executive Summary
This paper introduces a Monte Carlo sampling method to analyze in-context learning (ICL) performance while controlling for confounding effects from example selection and ordering. By averaging over multiple random permutations and trials, the authors isolate the impact of the number of in-context examples (k) on model performance. They find that previously reported performance plateaus are artifacts of averaging, with individual permutations showing erratic, non-monotonic behavior. Surprisingly, robust exemplar selection based on Z-scores yields lower accuracy than random selection, suggesting a potential tradeoff between performance and robustness in ICL.

## Method Summary
The authors propose Monte Carlo sampling to study ICL performance by averaging over multiple random permutations and trials. For each trial, they sample K=20 examples from training data, generate P=20 random permutations, and evaluate model accuracy at each position k along the permutation using 256 subsampled validation instances. They repeat this process for 5 trials, yielding 100 total permutations per dataset-model pair. The method computes average accuracy and standard deviation across permutations and trials, allowing isolation of k-effects from ordering and selection confounds. An extension computes per-exemplar Z-scores across all positions/permutations to identify high/low performers.

## Key Results
- Performance plateaus at k=4 and k=8 are not consistently observable across individual permutations, with erratic performance changes continuing through k=20.
- One-shot performance can vary significantly, sometimes outperforming and sometimes underperforming zero-shot settings depending on the selected example.
- Robust exemplar selection using Z-scores yields more stable performance across permutations and k values but results in overall performance decrease compared to random sampling.
- The results suggest ICL performance is more sensitive to example selection and ordering than previously recognized.

## Why This Works (Mechanism)

### Mechanism 1: Permutation Averaging Isolates k-Effects from Order/Selection Confounds
Monte Carlo sampling over permutations provides an unbiased estimate of in-context performance at each k by averaging out ordering effects. By sampling a subset SK, then computing performance across P random permutations while incrementally adding exemplars (k=0 to K), each exemplar appears in different positions across permutations. Averaging over P permutations and T trials marginalizes out position-dependent effects. Core assumption: P=20 permutations and T=5 trials are sufficient for the Monte Carlo estimate to converge to the true expected performance.

### Mechanism 2: Individual Permutation Trajectories Reveal Hidden Variability Masked by Averages
Previously reported performance plateaus at k=4 or k=8 are artifacts of averaging; individual permutations show erratic, non-monotonic performance curves. The averaging process smooths over the high variance between permutations. When disaggregated, single permutations show performance drops, spikes, and no consistent plateau point. Core assumption: The observed variability in individual permutations reflects true sensitivity rather than noise from limited test set size (256 samples).

### Mechanism 3: Z-Score Example Selection Produces Robust-But-Suboptimal Subsets
Selecting exemplars with consistently high Z-scores across permutations yields low variance across orderings but underperforms random selection. Z-score identifies exemplars whose average contribution is stable across permutations and k-values. However, this stability criterion may filter out exemplars that, while variable, provide high upside in favorable positions—creating a robustness-performance tradeoff. Core assumption: Z-score computed over 5 trials × 20 permutations reliably estimates true exemplar quality.

## Foundational Learning

- **Concept:** Monte Carlo Estimation
  - **Why needed here:** The method relies on approximating expected values by random sampling rather than exhaustive enumeration (computationally infeasible for nCk subsets × k! permutations).
  - **Quick check question:** If you doubled P from 20 to 40 permutations and saw negligible change in averaged performance curves, what would that suggest about convergence?

- **Concept:** In-Context Learning Sensitivity Factors
  - **Why needed here:** The paper explicitly controls for three factors—exemplar number (k), selection (which examples), and ordering (permutation π)—which interact non-trivially.
  - **Quick check question:** Given that one-shot performance can underperform zero-shot, what does this imply about the common advice to always include at least one demonstration?

- **Concept:** Variance-Bias Tradeoff in Selection
  - **Why needed here:** The negative result (robust but lower-performing selection) exemplifies a variance-reduction strategy that increases bias—common in regularization but unexpected here.
  - **Quick check question:** Why might "consistently good" exemplars underperform "sometimes great, sometimes bad" exemplars when averaged across many orderings?

## Architecture Onboarding

- **Component map:** Sampling Module → Evaluation Loop → Aggregation → Selection Extension
- **Critical path:**
  1. Load model M, dataset Dtrn/Dtst
  2. For T=5 trials: sample SK (20 exemplars) → generate P=20 permutations → evaluate at k=0,1,...,20
  3. Aggregate: µk = (1/P) × Σ VM(S[0:k]π) for each k
  4. (Optional) Compute per-exemplar Z-scores for selection experiments

- **Design tradeoffs:**
  - P=20 permutations balances computational cost vs. estimate quality; increasing P improves convergence but linearly increases inference calls
  - Subsampling test set to 256 examples reduces cost but increases variance in accuracy estimates
  - Using same SK across k-values (vs. resampling) eliminates selection confounds but prevents direct comparison of different subset sizes

- **Failure signatures:**
  - High standard deviation across trials suggests sensitivity to selected examples; may need more trials
  - Non-monotonic average performance curve (dips at certain k) indicates exemplar quality varies significantly within subset
  - Z-score selection yielding negative examples that don't degrade performance suggests Z-score may not capture harmful exemplars

- **First 3 experiments:**
  1. Reproduce Figure 1 for a single model/dataset: run full Monte Carlo procedure (T=5, P=20, K=20) and plot average performance with standard deviation bands to verify plateau behavior.
  2. Inspect individual permutation trajectories: plot P=20 individual curves overlaid with average to confirm high hidden variance.
  3. Test Z-score selection on a different dataset: compute Z-scores, select top-6 and bottom-6 exemplars, compare performance and variance against random baseline to assess generalizability of the robustness-performance tradeoff.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** What specific qualities of an in-context example cause performance degradation in one-shot settings compared to zero-shot baselines?
- **Basis in paper:** Section 5.1 notes that one-shot performance varies significantly and "raises the question of what qualities of in-context examples can lead to such significant performance degradation in one-shot settings."
- **Why unresolved:** The paper identifies the phenomenon where selected examples hurt performance but does not analyze the linguistic or semantic features responsible for this negative impact.
- **What evidence would resolve it:** A qualitative or statistical analysis linking specific example features (e.g., length, lexical overlap, sentiment) to negative performance deltas in one-shot settings.

### Open Question 2
- **Question:** Is there an inherent tradeoff between performance and robustness in in-context learning?
- **Basis in paper:** In Section 5.2, regarding the result that robust selection methods yielded lower accuracy, the authors ask: "This raises the question of whether there is an existing performance vs. robustness tradeoff."
- **Why unresolved:** The authors found that methods ensuring robustness (low variance) resulted in unexpected performance degradation compared to random selection, leaving the relationship between the two factors ambiguous.
- **What evidence would resolve it:** Empirical results demonstrating a selection strategy that achieves both low variance across orderings and higher accuracy than random baselines, or a theoretical proof showing such a combination is unattainable.

### Open Question 3
- **Question:** How can selection methods identify examples that maintain robustness while also achieving higher performance gains?
- **Basis in paper:** Section 5.2 asks "how methods can identify examples that possess these qualities that also lead to higher performance gains" after observing that high-scoring Monte Carlo examples lacked peak performance.
- **Why unresolved:** The proposed sampling method successfully identified examples that were robust to ordering but failed to identify examples that improved performance over random selection.
- **What evidence would resolve it:** A modified valuation metric or selection algorithm that prioritizes examples contributing positively to marginal performance while retaining stability across different permutations.

## Limitations

- **Test Set Size and Variance:** Using only 256 validation examples per dataset creates high variance in accuracy estimates, particularly problematic when analyzing individual permutation trajectories where performance can fluctuate dramatically.
- **Generalizability of Findings:** The robustness-performance tradeoff observed with Z-score selection may be specific to the experimental setup (P=20 permutations, T=5 trials, K=20 examples). Insufficient sampling could lead to unreliable Z-score estimates that don't generalize.
- **Computational Constraints vs. Statistical Power:** While the authors acknowledge that exhaustive enumeration is infeasible, the choice of P=20 permutations and T=5 trials represents a compromise that may not provide stable estimates for all datasets and models.

## Confidence

- **High Confidence:** The observation that previously reported performance plateaus at k=4 and k=8 are not consistently observable across individual permutations. This finding is directly supported by the disaggregated permutation analysis in Figure 2 and is robust across datasets.
- **Medium Confidence:** The claim that one-shot performance can underperform zero-shot settings depending on exemplar selection. While the data supports this observation, the mechanism behind why certain examples harm performance remains unclear.
- **Low Confidence:** The conclusion that Z-score-based exemplar selection produces robust-but-suboptimal subsets. This finding is highly sensitive to the number of permutations and trials used to compute Z-scores, and the tradeoff may not hold with different sampling parameters.

## Next Checks

1. **Convergence Analysis:** Systematically vary P (permutations per trial) from 5 to 50 and T (trials) from 1 to 10 for a single dataset-model pair. Plot how average performance curves and Z-score estimates change with increased sampling. If curves stabilize by P=20, this validates the computational choices; if not, more extensive sampling may be needed.

2. **Test Set Sensitivity:** Repeat the full analysis (T=5, P=20) using different test set sizes: 256, 512, and 1024 examples. Measure how variance in individual permutation performance and the visibility of plateaus changes with sample size. This will determine whether the observed erratic behavior is due to limited test data.

3. **Alternative Selection Criteria:** Implement and compare three different exemplar selection methods beyond random and Z-score: (a) Data Shapley values computed via Monte Carlo approximation, (b) exemplars with highest individual accuracy when used in isolation, and (c) exemplars that maximize pairwise diversity. Compare the robustness-performance tradeoff across these methods to determine if Z-score is suboptimal or if the tradeoff is inherent to robust selection.