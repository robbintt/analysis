---
ver: rpa2
title: 'MemEIC: A Step Toward Continual and Compositional Knowledge Editing'
arxiv_id: '2510.25798'
source_url: https://arxiv.org/abs/2510.25798
tags:
- knowledge
- visual
- textual
- editing
- compositional
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MemEIC, a method for continual and compositional
  knowledge editing in large vision-language models (LVLMs). MemEIC addresses the
  challenge of updating both visual and textual knowledge in LVLMs, which is crucial
  for maintaining accuracy as information evolves.
---

# MemEIC: A Step Toward Continual and Compositional Knowledge Editing

## Quick Facts
- arXiv ID: 2510.25798
- Source URL: https://arxiv.org/abs/2510.25798
- Reference count: 40
- This paper introduces MemEIC, a method for continual and compositional knowledge editing in large vision-language models (LVLMs).

## Executive Summary
MemEIC addresses the challenge of updating both visual and textual knowledge in LVLMs, which is crucial for maintaining accuracy as information evolves. The method employs a hybrid approach, combining external memory retrieval with internal model editing using modality-specific adapters. A key component is a brain-inspired knowledge connector that selectively integrates information across visual and textual modalities for compositional reasoning. Experiments on a new benchmark, CCKEB, demonstrate that MemEIC significantly outperforms existing methods, achieving high reliability in both visual and textual edits while maintaining strong compositional reasoning performance, even after multiple sequential edits.

## Method Summary
MemEIC uses a two-stage training approach with dual external memory (Mem-E) and dual internal adapters (Mem-I). Stage 1 trains external memory using DistilBERT for text and CLIP for images on the CCKEB dataset. Stage 2 trains dual LoRA adapters (rank 8 each) for visual and textual edits, plus a knowledge connector LoRA (rank unspecified) on FFN layers. The system employs a query decomposer (GPT-4o) to split inputs into visual and textual components, uses modality-specific retrieval from external memory, and activates the knowledge connector only when both visual and textual adapters are engaged. The model is trained with adversarial retrieval (70% hit rate) for robustness.

## Key Results
- MemEIC significantly outperforms existing methods on CCKEB benchmark with high reliability in both visual and textual edits
- Dual-LoRA separation prevents cross-modal interference, showing +17.77% improvement in Text-Loc compared to Single-LoRA
- Knowledge connector enables compositional reasoning, outperforming Dual-LoRA + RAG alone in compositional reliability
- Visual retrieval component improves reliability from ~48% to ~96% compared to text-only retrieval

## Why This Works (Mechanism)

### Mechanism 1: Disentangled Parameter Updates (Brain Lateralization)
The architecture enforces strict separation of parametric memory by instantiating two distinct LoRA adaptersâ€”one for visual facts and one for textual facts. This mimics brain lateralization, allowing visual edits to update weights without overwriting textual representations and vice versa. The core assumption is that knowledge edits are atomic; a single edit event rarely requires simultaneous updates to both the visual recognition pathway and the textual fact association pathway.

### Mechanism 2: Cross-Modal Evidence Retrieval
The system uses a dual external memory (Mem-E) that indexes edits using both text (DistilBERT) and image (CLIP) embeddings. Retrieval scores are computed as a weighted sum of text and visual similarity, ensuring that a query containing an image retrieves the correct visual edit context. This addresses the failure of standard text-based retrieval for external memory to locate visual edits because it ignores image-specific features.

### Mechanism 3: Selective Knowledge Fusion (Corpus Callosum)
A "Knowledge Connector" (LoRA-augmented self-attention) acts as a switch. It activates only when both the visual and textual adapters are engaged, fusing the hidden states. If only one modality is present, the connector acts as an identity operation, preserving the disentangled representation. This learned connector is required to resolve conflicts and integrate information for compositional reasoning, rather than simply combining retrieved text and edited visual features.

## Foundational Learning

- **Concept: Low-Rank Adaptation (LoRA)**
  - Why needed here: The method relies on efficiently updating model weights without full fine-tuning. Understanding that LoRA adds small, trainable rank-decomposition matrices to frozen weights is essential to grasping how MemEIC creates "lightweight" internal memories for visual and textual edits.
  - Quick check question: If the base model weights are frozen, how does the model change its behavior during an edit? (Answer: Only the LoRA adapter matrices are updated.)

- **Concept: Catastrophic Forgetting**
  - Why needed here: The paper frames its primary value proposition around "continual" editing. One must understand that sequential updates in neural networks typically overwrite previous knowledge to appreciate why the paper uses dual adapters and external memory.
  - Quick check question: Why can't you just fine-tune the model sequentially for every new fact? (Answer: New gradients will likely minimize the loss for new data while increasing loss/error on previously learned data.)

- **Concept: Retrieval-Augmented Generation (RAG)**
  - Why needed here: MemEIC is a hybrid system. Recognizing the distinction between "internal memory" (parametric knowledge/LoRA) and "external memory" (retrieved context/Vector DB) is crucial for understanding the architecture's flow.
  - Quick check question: In MemEIC, where is the "fact" stored if the model parameters are not changed? (Answer: In the External Memory store, retrieved at inference time.)

## Architecture Onboarding

- **Component map**: User Query & Image -> Query Decomposer -> Mem-E (External Memory) -> Mem-I (Internal Adapters) -> Knowledge Connector (when both modalities active) -> Base LVLM

- **Critical path**: 1) User Query & Image enter -> Decomposer splits query. 2) Split queries hit Mem-E to retrieve historical context. 3) Split queries + Image activate specific Mem-I adapters (Visual, Textual, or Both). 4) If both adapters active -> Connector fuses streams via attention. 5) Base LVLM generates response using retrieved context + fused internal states.

- **Design tradeoffs**: The system trades the simplicity of pure RAG (easy to update but limited reasoning) or pure FT (internalized but forgets) for architectural complexity to gain both long-term retention and internalized reasoning. The paper suggests training the connector with noisy (adversarial) retrieval rather than perfect "oracle" retrieval, trading peak performance in ideal conditions for robustness against retrieval failures in the wild.

- **Failure signatures**:
  - Representation Collapse: Occurs if using Single-LoRA for sequential edits (metrics drop ~30 points in long gaps).
  - Retrieval Disconnect: Occurs if visual cues are removed from Mem-E; the model fails to locate the correct visual edit (Table 2, Rel drops to ~48%).
  - Over-reliance: Occurs if the Connector is trained with perfect retrieval; it ignores internal edits when the retriever provides slightly incorrect info.

- **First 3 experiments**:
  1. Validate Modality Separation: Run Table 3 ablation. Compare "Single-LoRA" vs. "Dual-LoRA" on sequential visual edits. Look for the divergence in "Text-Loc" and "Image-Loc" scores as the gap increases.
  2. Test Connector Robustness: Replicate Figure 7 (Appendix E.3.2). Train the connector with 50%, 70%, and 100% retrieval accuracy. Verify that the 70% model performs best under realistic (noisy) test conditions.
  3. Visual Retrieval Ablation: Replicate Table 2. Disable the CLIP-based visual retrieval scoring in Mem-E and observe the drop in "Image-Loc" to confirm the visual grounding mechanism is active.

## Open Questions the Paper Calls Out

- Can the MemEIC framework be effectively extended to multimodal generation tasks rather than just understanding/recognition? The current architecture and loss functions are optimized for editing factual associations in recognition tasks, whereas generation requires consistent synthesis of novel visual or textual content.

- Does the performance of MemEIC scale effectively to extremely large vision-language models (e.g., >7B parameters)? The dual-LoRA and Knowledge Connector mechanisms may face different optimization challenges or interference patterns in higher-dimensional parameter spaces.

- How does MemEIC perform under non-paired, arbitrary sequences of edits compared to the constrained paired setting used in experiments? Real-world knowledge updates are often irregular and asynchronous; the model's ability to handle arbitrary sequences remains unverified.

## Limitations

- The dual-LoRA separation mechanism assumes edits are modality-atomic, potentially limiting applicability for intrinsically multimodal updates.
- The knowledge connector's performance depends on accurate query decomposition; errors in splitting compositional queries could prevent proper activation.
- Visual retrieval robustness relies on CLIP embeddings, which may struggle with ambiguous or out-of-distribution images.
- The method requires significant architectural complexity compared to pure RAG or fine-tuning approaches.

## Confidence

- High confidence: Dual-LoRA effectively prevents cross-modal interference (supported by Table 3 locality metrics).
- Medium confidence: Visual retrieval component significantly improves performance (based on Table 2 reliability comparisons).
- Medium confidence: Knowledge connector provides meaningful compositional reasoning (supported by Figure 4 ablation results).

## Next Checks

1. Test dual-LoRA separation with sequential multimodal edits that require simultaneous visual and textual updates to identify failure modes.
2. Evaluate knowledge connector performance with varying levels of query decomposition accuracy to determine sensitivity to decomposition errors.
3. Assess visual retrieval robustness using out-of-distribution or ambiguous images to measure CLIP embedding limitations.