---
ver: rpa2
title: 'ToolExpander: Extending the Frontiers of Tool-Using Reinforcement Learning
  to Weak LLMs'
arxiv_id: '2510.07737'
source_url: https://arxiv.org/abs/2510.07737
tags:
- training
- data
- hard
- few-shot
- samples
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the problem of training small-scale Large
  Language Models (LLMs) for tool use with GRPO, where hard samples cause low data
  utilization and training instability. The proposed solution, ToolExpander, introduces
  two key innovations: (1) Dynamic Multi-Round Hard Sampling, which replaces hard
  samples with high-quality few-shot demonstrations during training, and (2) Self-Exemplifying
  Thinking, an enhanced GRPO framework that encourages the model to autonomously generate
  and analyze few-shot examples via a minimal reward (0.01).'
---

# ToolExpander: Extending the Frontiers of Tool-Using Reinforcement Learning to Weak LLMs

## Quick Facts
- **arXiv ID:** 2510.07737
- **Source URL:** https://arxiv.org/abs/2510.07737
- **Reference count:** 9
- **Key outcome:** ToolExpander significantly improves tool-using capabilities in weak LLMs, especially small-scale models, with reduced hard samples and improved training stability. On the APIBank benchmark, the Qwen2.5-7B model achieved 81.76% accuracy, surpassing Tool-N1 (77.89%) and the original GRPO (62.66%).

## Executive Summary
This paper addresses the problem of training small-scale Large Language Models (LLMs) for tool use with GRPO, where hard samples cause low data utilization and training instability. The proposed solution, ToolExpander, introduces two key innovations: Dynamic Multi-Round Hard Sampling, which replaces hard samples with high-quality few-shot demonstrations during training, and Self-Exemplifying Thinking, an enhanced GRPO framework that encourages the model to autonomously generate and analyze few-shot examples via a minimal reward (0.01). Experimental results show that ToolExpander significantly improves tool-using capabilities in weak LLMs, especially small-scale models, with reduced hard samples and improved training stability.

## Method Summary
ToolExpander introduces two key innovations to address training instability in small-scale LLMs for tool use. The first innovation, Dynamic Multi-Round Hard Sampling, dynamically identifies and replaces hard samples during training with high-quality few-shot demonstrations, improving data utilization and reducing training instability. The second innovation, Self-Exemplifying Thinking, enhances the GRPO framework by encouraging the model to autonomously generate and analyze few-shot examples through a minimal reward mechanism (0.01). This dual approach aims to improve both the quality of training data and the model's ability to reason about tool use, leading to better performance on tool-using tasks.

## Key Results
- ToolExpander achieved 81.76% accuracy on the APIBank benchmark with Qwen2.5-7B, surpassing Tool-N1 (77.89%) and original GRPO (62.66%)
- The method significantly reduces hard samples during training while improving data utilization
- Demonstrated improved training stability for small-scale LLMs compared to baseline approaches

## Why This Works (Mechanism)
ToolExpander addresses the fundamental challenge of training small LLMs for tool use by tackling two interconnected problems: data quality and model reasoning capability. The Dynamic Multi-Round Hard Sampling mechanism identifies problematic samples that cause training instability and replaces them with curated few-shot demonstrations, effectively curating the training data to be more instructive. The Self-Exemplifying Thinking component enhances the model's reasoning ability by providing a minimal reward signal that encourages the generation of self-analyzing examples, creating a virtuous cycle where the model learns to both solve tool-using tasks and understand the reasoning behind successful solutions. This combination addresses the data inefficiency and instability issues that plague small LLM training for tool use.

## Foundational Learning
- **GRPO (Group Relative Policy Optimization):** A reinforcement learning algorithm adapted for language models that optimizes policies based on relative rewards within groups. Needed to handle the discrete nature of language generation tasks and enable effective policy updates.
- **Tool-Using LLMs:** Language models trained to interact with external tools/APIs through function calls. Required to extend LLM capabilities beyond pure text generation to practical applications.
- **Dynamic Hard Sampling:** A training technique that identifies and replaces difficult samples during training. Essential for maintaining training stability and improving convergence in challenging tasks.
- **Few-Shot Learning:** Learning approach where models are trained with limited examples. Critical for tool-using scenarios where extensive training data may not be available.
- **Reward Shaping:** Technique of modifying reward signals to guide learning behavior. Important for encouraging desired behaviors in reinforcement learning without disrupting the core optimization objective.

## Architecture Onboarding

**Component Map:**
LLM -> Tool Use Interface -> GRPO Trainer -> Dynamic Hard Sampler -> Few-Shot Generator -> LLM

**Critical Path:**
1. LLM generates tool use attempts
2. GRPO trainer evaluates and updates policies
3. Dynamic hard sampler identifies problematic samples
4. Few-shot generator creates replacement demonstrations
5. Updated training data feeds back to LLM

**Design Tradeoffs:**
- Balance between minimal reward (0.01) effectiveness vs. potential noise introduction
- Hard sample replacement frequency vs. training efficiency
- Few-shot demonstration quality vs. generation computational cost

**Failure Signatures:**
- Persistent hard sample identification despite replacement attempts
- Minimal reward causing unintended behavior reinforcement
- Convergence issues due to excessive few-shot generation overhead

**First Experiments:**
1. Validate minimal reward effectiveness through ablation studies (0.01 vs. alternative values)
2. Test hard sample identification accuracy against human-annotated difficulty labels
3. Measure few-shot generation quality through human evaluation and downstream task performance

## Open Questions the Paper Calls Out
None

## Limitations
- Claims about training stability improvements rely primarily on relative metrics rather than absolute stability measures
- The "hard sample reduction" metric is somewhat ambiguously defined and lacks clear correlation to actual model performance gains
- The self-exemplifying thinking mechanism's minimal reward of 0.01 appears arbitrarily chosen without ablation studies to justify this specific value

## Confidence

**High confidence:** The experimental setup methodology and benchmark selection appear sound and reproducible. The reported accuracy improvements on APIBank for the Qwen2.5-7B model are specific and measurable.

**Medium confidence:** The claims about training stability and data utilization improvements are supported by the presented metrics, though the causal relationship to the proposed innovations could be stronger. The few-shot demonstration generation process seems plausible but lacks detailed analysis of sample quality.

**Low confidence:** The generalizability of results to other weak LLM architectures and tool-using domains remains uncertain. The minimal reward choice for self-exemplifying thinking lacks theoretical justification or sensitivity analysis.

## Next Checks
1. Conduct ablation studies varying the minimal reward value (0.01) in self-exemplifying thinking to determine optimal settings and sensitivity.

2. Test ToolExpander across multiple LLM architectures (different base models, sizes) and diverse tool-using benchmarks beyond APIBank to assess generalizability.

3. Measure and report absolute training stability metrics (e.g., loss variance, convergence speed) rather than just relative hard sample reduction to better quantify stability improvements.