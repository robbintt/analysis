---
ver: rpa2
title: On the Influence of Discourse Relations in Persuasive Texts
arxiv_id: '2510.26124'
source_url: https://arxiv.org/abs/2510.26124
tags:
- discourse
- level-2
- relations
- dataset
- pdtb
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigated the relationship between discourse relations
  and persuasion techniques using large language models and prompt engineering. Since
  no dataset combined both types of annotations, the researchers developed LLM-based
  classifiers to automatically annotate the SemEval 2023 Task 3 dataset (with persuasion
  techniques) with discourse relations from PDTB 3.0.
---

# On the Influence of Discourse Relations in Persuasive Texts

## Quick Facts
- arXiv ID: 2510.26124
- Source URL: https://arxiv.org/abs/2510.26124
- Authors: Nawar Turk; Sevag Kaspar; Leila Kosseim
- Reference count: 32
- Key finding: Six discourse relations (Cause, Purpose, Contrast, Cause+Belief, Concession, Condition) significantly associate with specific persuasion techniques

## Executive Summary
This study investigates how discourse relations influence persuasive texts by automatically annotating a persuasion dataset with discourse relations using LLM ensembles. Since no dataset combines both persuasion technique and discourse relation annotations, the researchers developed a pipeline that uses multiple LLM configurations to generate silver-standard discourse relation labels. Statistical analysis reveals that certain discourse relations like Cause and Contrast are particularly important for techniques such as Loaded Language and Exaggeration/Minimisation, providing insights for detecting online propaganda and understanding effective communication.

## Method Summary
The researchers combined the SemEval 2023 Task 3 dataset (with human-annotated persuasion techniques) with automatic discourse relation annotations using LLM-based classifiers. They created 40 unique classifier configurations using four LLMs and ten different prompts, then applied ensemble models with majority-pooling strategies to generate five silver datasets with varying agreement thresholds. Statistical analysis using Fisher's exact test identified significant associations between discourse relations and persuasion techniques in the silver-labeled data.

## Key Results
- Six discourse relations (Cause, Purpose, Contrast, Cause+Belief, Concession, Condition) play crucial roles in persuasive texts
- These relations are particularly important for Loaded Language, Exaggeration/Minimisation, Repetition, and casting Doubt techniques
- Ensemble models show improved performance with higher agreement thresholds (F1 scores from 0.45 to 0.74)

## Why This Works (Mechanism)

### Mechanism 1: Ensemble Agreement Filtering
- Claim: Requiring multiple LLM+prompt configurations to agree on a prediction substantially increases discourse relation classification accuracy.
- Mechanism: Individual classifiers exhibit varying hallucination rates and prediction inconsistencies; majority voting across diverse prompt strategies filters out low-confidence predictions and systematic biases by retaining only instances where multiple independent classification paths converge.
- Core assumption: Different prompts elicit sufficiently different reasoning paths in LLMs such that agreement indicates higher confidence rather than shared systematic error.
- Evidence anchors:
  - [abstract] "Ensemble models using different majority-pooling strategies were used to create 5 silver datasets"
  - [section 4.1.2] "the macro F1 scores of these 5 ensemble classifiers increase steadily from 0.45 (for n = 5) to 0.74 (for n = 9). In addition, given the agreement constraint, none of the ensemble classifiers generated any hallucinations."
  - [corpus] Limited corpus validation—neighbor papers on persuasion detection (e.g., "A Hybrid Theory and Data-driven Approach") do not report comparable ensemble agreement strategies for this task.
- Break condition: If individual classifiers have highly correlated error patterns (e.g., all fail on the same PDTB senses), ensemble agreement gains will diminish or produce false confidence.

### Mechanism 2: Statistical Association Mining via Silver Labels
- Claim: Silver datasets combining human PT annotations with ensemble-predicted DR labels enable discovery of statistically significant PT-DR associations.
- Mechanism: By treating ensemble-predicted DR labels as proxies for ground truth and constructing contingency tables, Fisher's exact test identifies PT-DR co-occurrence patterns that exceed random expectation; odds ratios determine directionality.
- Core assumption: The silver DR labels are sufficiently accurate to reveal true underlying linguistic associations rather than classifier artifacts or bias patterns.
- Evidence anchors:
  - [abstract] "Statistical analysis of these silver datasets shows that six discourse relations (namely Cause, Purpose, Contrast, Cause+Belief, Concession, and Condition) play a crucial role"
  - [section 6] "We used Fisher's exact test to identify statistically significant associations between DRs and PTs. Since some contingency tables of PT-DR pairs contain values less than 5, we used Fisher's exact test as it is known to be more accurate than the χ2 test"
  - [corpus] Weak corpus validation—neighbor papers focus on persuasion detection methods rather than discourse-association discovery.
- Break condition: If silver DR labels have systematic bias toward certain senses (e.g., over-predicting "Cause"), false positive associations will emerge with PTs that co-occur with those senses in the source corpus.

### Mechanism 3: Prompt Design Diversity for Coverage
- Claim: Varying prompt design elements (definitions, examples, chain-of-thought reasoning, sense granularity) produces complementary classification behaviors across PDTB senses.
- Mechanism: Different prompts activate different knowledge representations in LLMs—zero-shot direct classification vs. chain-of-thought decomposition into Arg1/Arg2 segmentation → level-3 sense → level-2 rollup captures different reasoning pathways.
- Core assumption: LLMs possess latent knowledge of PDTB discourse relations that can be differentially accessed through varied prompting strategies.
- Evidence anchors:
  - [section 4.1] "prompts 1 to 5 asked directly to identify a level-2 sense, while prompts 6 to 10 followed a chain-of-thought reasoning, requiring the LLM to first segment the given instance into two discourse arguments, identify the level-3 sense before determining the level-2 parent sense"
  - [section 4.1] Table 1 shows systematic variation in sense level asked, definitions provided, examples provided, and chain-of-thought reasoning across 10 prompts
  - [corpus] No direct corpus comparison on prompt diversity effects for discourse parsing specifically.
- Break condition: If all prompts converge on similar failure modes for specific DRs (e.g., subtle implicit relations), prompt diversity provides no coverage benefit.

## Foundational Learning

- **Concept: Penn Discourse Treebank (PDTB) Sense Hierarchy**
  - Why needed here: The entire classification framework targets PDTB 3.0 level-2 senses; understanding the 3-tier hierarchy (level-1: 4 broad senses like Temporal/Contingency; level-2: 22 specific senses like Cause/Purpose; level-3: fine-grained) is essential for designing prompts and interpreting classifier outputs.
  - Quick check question: Given a sentence with "because," would you expect it to map to a level-1 "Contingency" or a level-2 "Cause" sense, and what's the practical difference for this paper's analysis?

- **Concept: Silver Dataset**
  - Why needed here: The paper's core contribution relies on creating datasets with automatic (not human-verified) DR labels; understanding their limitations—noise from classifier errors, systematic bias, size-confidence tradeoffs—is critical for interpreting the claimed PT-DR associations.
  - Quick check question: If Silver-5 has 1,281 instances with F1≈0.45 and Silver-9 has 204 instances with F1≈0.74, which would you use for exploratory analysis vs. hypothesis confirmation, and why?

- **Concept: Fisher's Exact Test for Sparse Contingency Tables**
  - Why needed here: The paper uses Fisher's exact test (not chi-square) for PT-DR association testing; understanding why—small cell counts when many PT-DR pairs have <5 co-occurrences—clarifies methodological validity.
  - Quick check question: When analyzing co-occurrence between a rare PT (e.g., "Doubt" appearing 30 times) and a moderately common DR, why might chi-square give misleading p-values?

## Architecture Onboarding

- **Component map:** SemEval 2023 Task 3 corpus -> Multi-label splitting -> Undersampling -> DR Classifier layer (4 LLMs × 10 prompts = 40 configs) -> Ensemble layer (majority voting with threshold n∈[5,9]) -> 5 silver datasets -> Analysis layer (contingency tables -> Fisher's exact test -> association heatmap)

- **Critical path:**
  1. Load preprocessed SemEval instances (2,064 after filtering)
  2. Run 9 ensemble classifiers on each instance, collect predictions
  3. Apply agreement threshold (n=5,6,7,8,9) to assign DR labels or discard
  4. For each PT-DR pair with ≥25 co-occurrences, build 2×2 contingency table
  5. Run Fisher's exact test (p≤0.05, OR>1) → identify significant positive associations

- **Design tradeoffs:**
  - **Size vs. confidence:** Silver-5 maximizes instances (1,281) but includes lower-confidence DR labels; Silver-9 maximizes accuracy (F1=0.74) but discards 90% of data
  - **Hallucination vs. coverage:** Claude achieves highest F1 (0.35) but highest hallucinations (1.6%); Gemini-flash has near-zero hallucinations but lower F1
  - **Single-DR assumption:** Paper assigns one DR per instance; implicit discourse relations are often multi-label, potentially missing secondary relations

- **Failure signatures:**
  - **Ensemble dead zone:** As agreement threshold increases, dataset shrinks dramatically (2,064 → 204), potentially biasing toward "easy" instances
  - **DR class imbalance:** Figure 6 shows Cause, Contrast, and Expansion senses dominate; rare senses (e.g., "Equivalence") excluded from analysis
  - **PT imbalance persistence:** Even after undersampling, Loaded Language remains most frequent PT

- **First 3 experiments:**
  1. **Classifier validation:** Reproduce Table 2/Figure 3 metrics on the 126-instance PDTB test set to confirm your LLM API access and prompt implementation match paper's setup.
  2. **Silver dataset sanity check:** Generate Silver-5 dataset and compare DR distribution against Figure 6; verify Loaded Language + Cause is the most frequent PT-DR pair.
  3. **Association spot-check:** Manually inspect 10 instances from the "Loaded Language ↔ Cause" association to assess whether the ensemble-assigned "Cause" labels appear linguistically valid.

## Open Questions the Paper Calls Out
None

## Limitations
- Silver labels inherit LLM biases and may not reflect true linguistic patterns
- The study only captures single-DR instances despite discourse relations often being implicit and multi-label
- The 25-instance minimum threshold for association testing may still include noisy data

## Confidence

| Claim | Confidence Level |
|-------|------------------|
| Ensemble agreement filtering reduces hallucinations and improves F1 scores (0.45-0.74) | Medium |
| Six discourse relations significantly associate with specific persuasion techniques | Low |

## Next Checks
1. Manually validate 50 random instances from Silver-9 to estimate true DR classification accuracy on persuasion texts
2. Apply the ensemble framework to a held-out persuasion corpus with human-annotated DR labels to benchmark silver dataset quality
3. Test whether the identified PT-DR associations replicate when using human-annotated DR labels from PDTB 3.0 on the same SemEval instances