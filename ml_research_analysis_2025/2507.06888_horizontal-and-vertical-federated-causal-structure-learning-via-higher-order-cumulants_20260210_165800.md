---
ver: rpa2
title: Horizontal and Vertical Federated Causal Structure Learning via Higher-order
  Cumulants
arxiv_id: '2507.06888'
source_url: https://arxiv.org/abs/2507.06888
tags:
- causal
- cumulant
- variables
- data
- federated
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses federated causal structure learning in settings
  where clients have incomplete, overlapping variable sets across both horizontal
  and vertical data partitions. The authors propose a method that leverages higher-order
  cumulants, which depend only on the joint distribution of observed variables and
  are unaffected by missing variables, to enable federated causal discovery.
---

# Horizontal and Vertical Federated Causal Structure Learning via Higher-order Cumulants

## Quick Facts
- arXiv ID: 2507.06888
- Source URL: https://arxiv.org/abs/2507.06888
- Authors: Wei Chen; Wanyang Gu; Linjun Peng; Ruichu Cai; Zhifeng Hao; Kun Zhang
- Reference count: 34
- One-line primary result: A method for federated causal discovery across clients with incomplete, overlapping variable sets using aggregated higher-order cumulants, achieving superior performance compared to existing methods.

## Executive Summary
This paper addresses federated causal structure learning in hybrid horizontal and vertical data partitioning scenarios where clients have incomplete, overlapping variable sets. The authors propose a novel approach that leverages higher-order cumulants, which depend only on the joint distribution of observed variables and are unaffected by missing variables. By aggregating local cumulant tensors from clients and recursively identifying source variables through elimination at the cumulant level, the method enables simultaneous reconstruction of causal graphs and estimation of causal strength coefficients while preserving privacy through exchange of aggregated statistics rather than raw data.

## Method Summary
The method operates by having each client compute local third-order cumulant matrices containing information about variable skewness and pairwise interactions. The server aggregates these via weighted averaging based on sample sizes to construct a global cumulant matrix. It then recursively identifies source variables (roots of the causal graph) by finding those satisfying specific cumulant-based constraints, removes their influence from the cumulant matrix, and repeats until the full causal ordering is obtained. The approach extends to linear Gaussian models using partial correlations, though this suffers from Markov equivalence limitations.

## Key Results
- Outperforms existing federated causal discovery methods on both synthetic and real-world datasets
- Successfully reconstructs causal graphs and estimates causal strength coefficients across hybrid horizontal/vertical partitions
- Preserves privacy through exchange of aggregated cumulant information rather than raw data
- Handles missing variable sets across clients without requiring complete overlap

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Global causal structure can be approximated by aggregating local higher-order cumulants without sharing raw data.
- **Mechanism:** Clients locally compute third-order cumulant matrices (capturing skewness and variable interactions). The server aggregates these into a global cumulant matrix using a weighted average based on sample sizes. Because cumulants are additive for independent samples, the aggregated statistic approximates the cumulant of the global distribution.
- **Core assumption:** The underlying data generating process is consistent across clients (identical causal strength matrix $B$), and samples are independent.
- **Break condition:** If clients have vastly different data distributions (non-identical structure $B$), the weighted aggregation of cumulants will not represent a coherent global structure.

### Mechanism 2
- **Claim:** Source variables (roots of the causal graph) can be identified strictly from aggregated cumulant statistics if the data is non-Gaussian.
- **Mechanism:** The method leverages the property that for a source variable $x_s$ in a Linear Non-Gaussian Acyclic Model (LiNGAM), specific cumulant-based constraints ($\tau_{sj}$) sum to zero. The server scans the global cumulant matrix to find the unique variable satisfying this condition.
- **Core assumption:** The data follows a Linear Non-Gaussian Acyclic Model (LiNGAM). Gaussian data has zero third-order cumulants, causing this specific identification mechanism to fail.
- **Break condition:** If noise terms are Gaussian, third-order cumulants vanish, and the source identification criterion becomes uninformative (though the paper notes extension to partial correlations for Gaussian cases).

### Mechanism 3
- **Claim:** Causal ordering can be resolved recursively without requiring every client to observe every variable.
- **Mechanism:** Once a source variable is identified, its influence is mathematically "subtracted" from the global cumulant matrix (analytically regressed out) rather than requiring clients to remove it from their raw local datasets. This transforms the remaining problem into a smaller causal discovery task on the "residualized" cumulants.
- **Core assumption:** The causal relationships are linear, allowing influence removal via linear operations on cumulants (Eq. 5-7).
- **Break condition:** If relationships are highly non-linear, linear cumulant updates will not correctly remove the causal influence, propagating errors to subsequent layers of the graph.

## Foundational Learning

- **Concept: Higher-Order Cumulants (Skewness/Kurtosis)**
  - **Why needed here:** Standard covariance (2nd order) captures Gaussian information but cannot distinguish causal direction in linear models. 3rd/4th order cumulants capture non-Gaussian "asymmetry" required to orient edges.
  - **Quick check question:** If my data is perfectly Normal (Gaussian), will the 3rd-order cumulant matrix be useful? (Answer: No, it will be zero).

- **Concept: Horizontal vs. Vertical Federated Learning**
  - **Why needed here:** This method specifically targets "hybrid" settings. Horizontal = same features, different users. Vertical = same users, different features. Understanding this distinction is crucial for seeing why standard federated averaging fails when clients have missing columns (variables).
  - **Quick check question:** Does the server need to align user IDs (samples) or feature names (variables) to aggregate cumulants? (Answer: Variables/Features, as per Eq 2 mapping).

- **Concept: LiNGAM (Linear Non-Gaussian Acyclic Model)**
  - **Why needed here:** The entire identification theory (Theorem 1) relies on the data adhering to this specific structural equation model. It provides the mathematical link between non-Gaussianity and causal ordering.
  - **Quick check question:** Does LiNGAM allow for cycles in the causal graph? (Answer: No, it assumes a Directed Acyclic Graph).

## Architecture Onboarding

- **Component map:** Client Nodes -> Server (Global Cumulant Matrix) -> Data Manager (Variable Mapping)
- **Critical path:**
  1. Check local data for non-Gaussianity (3rd order cumulants $\neq 0$).
  2. Client calculation of local cumulant tensors.
  3. Server aggregation (Union of variables, weighted sum of cumulants).
  4. Recursive Source Identification (finding $\tau = 0$).
  5. Global matrix update (Theorem 2 application).

- **Design tradeoffs:**
  - **Privacy vs. Utility:** Exchanging cumulant tensors preserves privacy better than raw data, but high-order statistics can still leak information about outliers. (Paper suggests Shamir's secret sharing as an enhancement).
  - **Gaussian vs. Non-Gaussian:** The primary method requires non-Gaussian data. While an extension to Gaussian (partial correlation) is mentioned in Section 6, it suffers from Markov equivalence (cannot distinguish certain graph structures).

- **Failure signatures:**
  - **Silent Failure on Gaussian Data:** The algorithm may return arbitrary results if applied blindly to Gaussian data where $\tau$ values are near zero due to distribution, not causal structure.
  - **Disconnected Variable Sets:** If Assumption 1 is violated (a variable exists on no client), the mapping fails.
  - **Communication Bottleneck:** While $O(d_k^2)$ is small, sending full tensors for permutation tests ($N$ times) increases bandwidth load significantly.

- **First 3 experiments:**
  1. **Unit Test (Synthetic):** Generate linear data with non-Gaussian noise (e.g., uniform or laplace). Verify that the server can reconstruct a known 5-node chain graph using 2 clients with 50% variable overlap.
  2. **Robustness Test:** Inject Gaussian noise into the synthetic data. Observe the degradation of Structural Intervention Distance (SID) as skewness approaches zero.
  3. **Scaling Test:** Fix sample size and increase variable count $d$. Measure the communication cost compared to a baseline federated PC-algorithm.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the proposed cumulant-based aggregation framework be extended to non-linear causal models while preserving the sample-wise decomposability required for federated learning?
- **Basis in paper:** [inferred] The method is explicitly restricted to linear structural equation models (Definition 2).
- **Why unresolved:** Non-linear relationships require statistical descriptors that may not aggregate linearly across clients as cumulants do.
- **What evidence would resolve it:** A theoretical extension of Theorem 1 to non-linear cases or empirical validation on datasets generated by non-linear structural equations.

### Open Question 2
- **Question:** How can the limitation of identifying only Markov equivalence classes in the linear Gaussian case be overcome without relying on non-Gaussianity?
- **Basis in paper:** [explicit] Section 6 states that "in the case of linear Gaussian, the common cause structure and the chain structure are Markov equivalent classes and cannot be distinguished."
- **Why unresolved:** Second-order statistics (partial correlations) utilized for the Gaussian extension are symmetric and inherently incapable of determining causal direction.
- **What evidence would resolve it:** Integration of interventional data or discovery of asymmetric constraints within the Gaussian federated setting.

### Open Question 3
- **Question:** What specific privacy guarantees does the exchange of higher-order cumulant matrices provide against membership inference or reconstruction attacks?
- **Basis in paper:** [explicit] Section 5.1 states that the base method protects privacy "to a certain extent" but suggests secure multi-party computation to "further avoid data privacy leakage."
- **Why unresolved:** Statistical summaries like cumulants can potentially be inverted to reveal properties of the underlying raw data.
- **What evidence would resolve it:** A formal differential privacy analysis of the shared cumulants or robustness testing against data reconstruction attacks.

## Limitations
- **Distributional Assumptions:** The method fundamentally requires non-Gaussian noise for source identification via third-order cumulants, becoming ineffective near-Gaussian data.
- **Communication Complexity:** Multiple permutation tests for statistical significance significantly increase bandwidth requirements beyond the theoretical O(d²) baseline.
- **Theoretical Gaps:** Limited empirical validation of the Gaussian extension and sparse details on handling multiple sibling source variables.

## Confidence
- **High Confidence:** The core mechanism of aggregating higher-order cumulants for federated causal discovery (Mechanisms 1-3) is theoretically sound and well-supported by the literature on LiNGAM.
- **Medium Confidence:** The recursive source identification and cumulant update procedures work as described for the non-Gaussian case, but practical implementation details (statistical tests, thresholds) require further specification.
- **Low Confidence:** The Gaussian extension and its comparative performance relative to the non-Gaussian approach needs more empirical validation.

## Next Checks
1. **Gaussian Robustness Test:** Systematically vary noise distribution from non-Gaussian to Gaussian and measure the degradation of SID/SHD metrics to quantify the operational limits of the non-Gaussian method.

2. **Communication Overhead Analysis:** Implement the permutation test framework and measure actual bandwidth consumption versus the theoretical O(d²) baseline across different values of N and d.

3. **Multiple Source Handling:** Design and execute a synthetic experiment with multiple independent source variables to validate the algorithm's ability to correctly identify and process all roots in the causal graph.