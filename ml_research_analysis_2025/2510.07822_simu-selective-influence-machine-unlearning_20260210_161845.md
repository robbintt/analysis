---
ver: rpa2
title: 'SIMU: Selective Influence Machine Unlearning'
arxiv_id: '2510.07822'
source_url: https://arxiv.org/abs/2510.07822
tags:
- unlearning
- layer
- neurons
- simu-graddiff
- influence
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces SIMU, a selective influence-based framework
  for machine unlearning in large language models. SIMU first identifies critical
  MLP neurons storing forget-set information via a gradient-based attribution method,
  then performs second-order unlearning updates only on those neurons while preserving
  attention layers and other parameters.
---

# SIMU: Selective Influence Machine Unlearning
## Quick Facts
- arXiv ID: 2510.07822
- Source URL: https://arxiv.org/abs/2510.07822
- Reference count: 32
- Selective influence-based machine unlearning framework for LLMs, targeting MLP neurons to improve utility while maintaining forgetting

## Executive Summary
SIMU introduces a selective influence machine unlearning framework for large language models that improves utility preservation while maintaining effective forgetting. The approach identifies and selectively updates only critical MLP neurons storing forget-set information using a gradient-based attribution method, leaving attention layers and other parameters untouched. Experiments on LLaMA2-7B and OLMo-1B models show 5-6% utility gains over prior methods while achieving comparable forgetting performance.

## Method Summary
SIMU employs a gradient-based attribution method to identify MLP neurons most responsible for storing forget-set information. Once these critical neurons are identified, the framework performs second-order unlearning updates exclusively on them, preserving other model components including attention layers. This selective targeting reduces approximation errors inherent in second-order unlearning while maintaining the effectiveness of information removal.

## Key Results
- 5-6% improvement in utility metrics compared to prior unlearning methods
- Maintains comparable forgetting performance to existing baselines
- Validated on LLaMA2-7B and OLMo-1B across TOFU and LUME benchmarks

## Why This Works (Mechanism)
SIMU works by leveraging the observation that specific MLP neurons disproportionately store information related to the forget-set data. By identifying and selectively updating only these neurons using second-order optimization, the method achieves more precise unlearning with less disruption to retained knowledge. The gradient-based attribution method ensures that updates are targeted where they matter most, reducing the approximation errors that typically plague second-order unlearning approaches when applied broadly across all parameters.

## Foundational Learning
**Second-order optimization**: Used for precise parameter updates during unlearning; needed for accurate curvature information; quick check: verify Hessian computation or approximation methods
**Gradient-based attribution**: Identifies influential neurons; needed for selective targeting; quick check: validate attribution scores correlate with forgetting effectiveness
**Selective parameter updating**: Updates only critical neurons; needed to preserve utility; quick check: compare utility loss between selective vs. full parameter updates

## Architecture Onboarding
**Component map**: Input -> Attention layers (untouched) -> MLP layers (selective updates) -> Output
**Critical path**: Gradient attribution identifies MLP neurons -> Second-order updates applied selectively -> Utility preserved while forgetting achieved
**Design tradeoffs**: Precision vs. computational cost, selective vs. comprehensive updates, MLP focus vs. attention layer involvement
**Failure signatures**: Utility degradation when too many neurons selected, incomplete forgetting when too few neurons targeted, computational inefficiency from poor attribution accuracy
**First experiments**: 1) Baseline utility with no unlearning, 2) Full parameter second-order unlearning comparison, 3) Attribution method ablation study

## Open Questions the Paper Calls Out
None

## Limitations
- Performance may not generalize to other architectures or larger models
- Reliance on second-order optimization may not scale well to very large models
- Limited evaluation on real-world data distributions and adversarial scenarios

## Confidence
**Core claims**: Medium - rigorous experimental setup with consistent utility gains, but lacking ablation studies on neuron selection granularity
**Scalability claims**: Low - limited to tested models and datasets, no evidence for larger architectures
**Real-world applicability**: Low - focused on benchmark datasets without addressing privacy guarantees or adversarial robustness

## Next Checks
1. Ablation on neuron selection: Systematically vary the number of MLP neurons selected for unlearning and measure the trade-off between utility preservation and forgetting effectiveness
2. Generalization to other architectures: Apply SIMU to models with different attention mechanisms (e.g., Mamba, RWKV) and evaluate if selective MLP neuron targeting remains effective
3. Long-term stability and adversarial robustness: Conduct repeated unlearning iterations and test model behavior on adversarially crafted inputs to assess stability and potential leakage of forget-set information