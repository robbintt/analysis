---
ver: rpa2
title: 'SLIDE: Sliding Localized Information for Document Extraction'
arxiv_id: '2503.17952'
source_url: https://arxiv.org/abs/2503.17952
tags:
- slide
- extraction
- knowledge
- context
- graph
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ''
---

# SLIDE: Sliding Localized Information for Document Extraction

## Quick Facts
- **arXiv ID**: 2503.17952
- **Source URL**: https://arxiv.org/abs/2503.17952
- **Reference count**: 5
- **Key outcome**: SLIDE improves entity and relationship extraction in knowledge graphs by preserving boundary-spanning context through overlapping windows

## Executive Summary
SLIDE addresses a critical limitation in GraphRAG systems: chunk boundaries cause entities and relationships to be fragmented or lost entirely during knowledge graph construction. By using overlapping sliding windows (n=10) to provide each chunk with n/2 preceding and succeeding chunks as context, SLIDE ensures that critical information spanning chunk boundaries remains connected. The method generates short English context for each chunk using an LLM, which is then appended before knowledge graph extraction. Experiments on English and Afrikaans texts show substantial improvements in entity/relationship coverage and question-answering performance compared to standard GraphRAG.

## Method Summary
SLIDE processes documents by first chunking them into standard segments (1200 tokens), then applying a sliding window approach where each chunk is enriched with 5 preceding and 5 succeeding chunks (total window size n=10). An LLM generates short English context situating each chunk within its neighborhood, which is appended to the chunk before knowledge graph extraction. The method was evaluated on two novels from Project Gutenberg (Pride and Prejudice in English and Bang vir die lewe in Afrikaans) using Microsoft GraphRAG with 4 gleanings. Entity/relationship extraction counts and QA evaluation via LLM-as-judge (Comprehensiveness, Diversity, Empowerment) were the primary metrics.

## Key Results
- Entity extraction improved from 344 to 472 entities for Afrikaans text using SLIDE
- Relationship extraction increased from 433 to 758 relationships for Afrikaans text
- SLIDE showed significant improvements in QA comprehensiveness and diversity across community levels 1-3 compared to standard GraphRAG

## Why This Works (Mechanism)

### Mechanism 1: Boundary-Spanning Context Preservation
Overlapping windows capture entity and relationship information that would be lost at standard chunk boundaries. For each chunk Ci, SLIDE retrieves n/2 preceding and n/2 succeeding chunks (window size n=10), feeding n+1 total chunks to the LLM to generate localized context. This ensures entities mentioned across chunk boundaries remain connected during knowledge graph construction.

### Mechanism 2: Token-Bounded Context Generation
Limiting context generation to window-sized local context avoids truncation while remaining computationally tractable. Unlike contextual retrieval (which embeds full document context and hits token limits), SLIDE caps token usage at window size × chunk size. The LLM generates a short English context situating the chunk, which is appended before extraction.

### Mechanism 3: Cross-Lingual Context Transfer
Generating context in a high-resource language (English) improves extraction for low-resource languages. The LLM generates context in English even for Afrikaans chunks, leveraging richer semantic representations in English to help ground entity/relationship detection when the target language has limited training signal.

## Foundational Learning

- **Knowledge Graph Construction via LLMs**: Why needed - SLIDE targets GraphRAG systems where LLMs extract entities (nodes) and relationships (edges). Quick check - Can you explain how an LLM converts a text chunk into entity-relationship triples?
- **Sliding Window Techniques**: Why needed - The core mechanism uses overlapping windows. Understanding window size trade-offs (coverage vs. computation) is essential for hyperparameter tuning. Quick check - Given a document of 100 chunks and window size 10, how many unique chunk-window combinations are processed?
- **LLM Context Length Constraints**: Why needed - SLIDE is motivated by token limit failures in contextual retrieval. You need to understand why naive approaches truncate information. Quick check - What happens when a document exceeds an LLM's maximum context window during standard RAG chunking?

## Architecture Onboarding

- **Component map**: Document → Standard Chunking (1200 tokens) → Sliding Window Assembly (add n/2 neighbors) → LLM Context Generation (short English context) → Context-Enriched Chunks → Knowledge Graph Extraction → GraphRAG Query Engine
- **Critical path**: Context generation is the bottleneck. Each of k chunks requires one LLM call processing n+1 chunks. With k=200 and n=10, this is 200 calls each processing ~13,200 tokens (11 × 1200).
- **Design tradeoffs**:
  - Window size (n): Larger n improves context but increases compute quadratically. Paper uses n=10 without ablation.
  - Chunk size: Smaller chunks = more granular context but more LLM calls. Paper tests 1200 and 2400 tokens.
  - Language for context: English context for all languages vs. native-language context. Paper uses English but does not ablate this choice.
- **Failure signatures**:
  - Entity duplication across overlapping windows
  - Context generation drift (LLM produces irrelevant or hallucinated context)
  - Compute explosion on very long documents (O(k × n) calls)
  - No precision/recall evaluation (paper acknowledges this limitation)
- **First 3 experiments**:
  1. Reproduce baseline comparison: Run GraphRAG with and without SLIDE on a single long document (>50K tokens), compare entity/relationship counts and query quality metrics.
  2. Ablate window size: Test n ∈ {4, 10, 20} on the same document to find compute/quality inflection point for your domain.
  3. Test precision trade-off: Manually sample 50 extracted entities and relationships to check if increased coverage comes with increased false positives.

## Open Questions the Paper Calls Out

### Open Question 1
**Question:** Does the increased entity and relationship coverage provided by SLIDE result in higher precision and recall, or does it primarily increase the rate of false positives?
**Basis in paper:** The authors note they "do not evaluate the accuracy of the extracted entities and relationships," making it "unclear whether the improvements in coverage come at the cost of increased false positives."
**Why unresolved:** The evaluation relies solely on raw counts (coverage) rather than standard Named Entity Recognition (NER) validation against ground truth.
**What evidence would resolve it:** Precision and recall scores derived from a labeled dataset comparing SLIDE's extractions against a baseline.

### Open Question 2
**Question:** How can the computational redundancy caused by the sliding window approach be optimized for large-scale datasets?
**Basis in paper:** The paper lists computational overhead as a key limitation, noting that repetitive processing of overlapping text "amplifies the computational cost" and acts as a bottleneck.
**Why unresolved:** The current implementation prioritizes context retention over efficiency, lacking mechanisms to mitigate the cost of processing the same text multiple times.
**What evidence would resolve it:** An optimized variant of the algorithm that reduces processing time (latency) without degrading the quality of the extracted knowledge graph.

### Open Question 3
**Question:** To what extent does the use of Llama 3.1 as an evaluator bias the reported performance improvements for low-resource languages like Afrikaans?
**Basis in paper:** The authors admit the evaluator "may not be an optimal evaluator due to potential biases" and limitations in understanding low-resource languages.
**Why unresolved:** The reported improvements in comprehensiveness and diversity rely on an evaluator that may lack competence in the target language.
**What evidence would resolve it:** A human evaluation study correlating Llama 3.1's scores with human judgments for the Afrikaans question-answering tasks.

## Limitations

- Computational overhead from processing overlapping text across windows creates a bottleneck for large-scale datasets
- Lack of precision/recall evaluation makes it unclear whether coverage gains come with increased false positives
- Effectiveness of cross-lingual context transfer depends heavily on the LLM's multilingual capabilities

## Confidence

**Medium** for computational efficiency claims - while reduced token usage vs. full-document contextual retrieval is demonstrated, runtime benchmarks and cost comparisons are absent.
**Low** for cross-lingual effectiveness claims - promising entity/relationship count improvements but no precision/recall evaluation to validate accuracy gains.
**Medium** for boundary-spanning context preservation - theoretically sound but lacks window size ablation to identify optimal balance.

## Next Checks

1. **Precision vs. Coverage Trade-off**: Manually evaluate 50 randomly sampled entities and relationships from both SLIDE and baseline runs to measure precision rates.
2. **Window Size Sensitivity Analysis**: Run experiments with n ∈ {4, 10, 20} on the same document to identify the inflection point where additional context no longer improves extraction while compute costs increase.
3. **Computational Cost Benchmarking**: Measure actual runtime and token usage for SLIDE vs. standard GraphRAG on documents of varying lengths (10K, 50K, 100K tokens) to validate claimed efficiency gains and identify scalability limits.