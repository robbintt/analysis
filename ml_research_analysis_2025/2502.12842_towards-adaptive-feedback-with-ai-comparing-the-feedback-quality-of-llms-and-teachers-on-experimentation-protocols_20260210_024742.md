---
ver: rpa2
title: 'Towards Adaptive Feedback with AI: Comparing the Feedback Quality of LLMs
  and Teachers on Experimentation Protocols'
arxiv_id: '2502.12842'
source_url: https://arxiv.org/abs/2502.12842
tags:
- feedback
- education
- science
- agent
- feed
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study evaluated an LLM-based feedback agent for student experimentation
  protocols, comparing its performance to human teachers and science education experts.
  Using real-world student data and a six-dimensional rating scheme (Feed Up, Feed
  Back, Feed Forward, Constructive Tone, Linguistic Clarity, and Technical Terminology),
  four blinded raters assessed feedback quality.
---

# Towards Adaptive Feedback with AI: Comparing the Feedback Quality of LLMs and Teachers on Experimentation Protocols

## Quick Facts
- arXiv ID: 2502.12842
- Source URL: https://arxiv.org/abs/2502.12842
- Reference count: 16
- Primary result: LLM-generated feedback matches human experts in overall quality but underperforms in contextual error identification

## Executive Summary
This study evaluates an LLM-based feedback agent for student experimentation protocols, comparing its performance to human teachers and science education experts. Using real-world student data and a six-dimensional rating scheme, four blinded raters assessed feedback quality. Results showed no significant difference in overall feedback quality between the LLM agent and humans, though the agent underperformed in the Feed Back dimension, which requires contextual error identification. The LLM agent demonstrated strengths in linguistic appropriateness and actionability but faced challenges in nuanced understanding. The findings suggest integrating LLM-generated feedback with human expertise can enhance educational practices by combining AI efficiency with educator insight.

## Method Summary
The study employed a zero-shot prompting approach using GPT-3.5-turbo-0125 to generate feedback on 40 student experimentation protocols containing 109 pre-identified errors. A 5-step chain-of-thought prompt guided the LLM through reformulation, diagnosis, fixing, drafting, and condensing feedback. Four blinded domain experts rated the generated feedback alongside human teacher feedback using a six-dimensional 5-point Likert scale. The evaluation focused on Feed Up (goal clarity), Feed Back (error identification), Feed Forward (actionability), Constructive Tone, Linguistic Clarity, and Technical Terminology.

## Key Results
- No significant difference in overall feedback quality between LLM agent and human teachers (p > 0.05)
- LLM agent underperformed in Feed Back dimension (contextual error identification)
- LLM agent demonstrated high performance in linguistic appropriateness and actionability dimensions
- Cost-efficient approach using GPT-3.5 rather than fine-tuning achieved human-expert level feedback in most dimensions

## Why This Works (Mechanism)

### Mechanism 1: Decomposed Chain-of-Thought (CoT) Prompting
Structuring the LLM generation process into a 5-step reasoning pipeline elevates feedback quality to human-expert levels in general dimensions. Instead of asking the model to output feedback directly, the prompt forces a "slow thinking" process where it must first paraphrase the problem and diagnose the error before generating solutions. This injects intermediate reasoning steps, reducing the chance of shallow or hallucinated advice.

### Mechanism 2: Role-Prompting for Linguistic Alignment
Explicitly assigning the persona "You are a science teacher" shifts the model's output distribution toward appropriate tone and terminology for the target age group (Grades 6-8). Role-prompting conditions the model on a specific subspace of its training data (pedagogical interactions), increasing the probability of generating "Constructive Tone" and appropriate "Technical Terminology."

### Mechanism 3: Contextual Inference Limitation (Failure Mode)
The LLM agent underperforms in the "Feed Back" dimension because it lacks the "world model" of the specific classroom context and student history available to human teachers. Human raters succeed in "Feed Back" by inferring student intent from noisy data based on pedagogical experience. The LLM relies solely on the provided text window, struggling to map the text to the specific error definition when protocols are unstructured or unusual.

## Foundational Learning

- **Concept: Hattie and Timperley's Feedback Model**
  - Why needed here: The entire evaluation framework is built on "Feed Up, Feed Back, Feed Forward." You cannot debug the agent's performance without understanding that "Feed Back" means diagnostic identification of the current error, while "Feed Forward" means actionable next steps.
  - Quick check question: If the model says "Try to control your variables," is that Feed Back or Feed Forward? (Answer: Feed Forward; Feed Back would be "You failed to control the temperature variable in step 2").

- **Concept: Zero-Shot Prompting vs. Fine-Tuning**
  - Why needed here: The authors used a base GPT-3.5 model with no task-specific weight updates. Understanding this explains why the system is cheap but prone to "reasoning" errors compared to specialized systems.
  - Quick check question: Does the system learn from the 40 protocols it graded? (Answer: No, that was the test set; it learned only from the prompt instructions and pre-training).

- **Concept: Blinded Evaluation**
  - Why needed here: The validity of the "no significant difference" claim rests on the raters not knowing which text was AI-generated.
  - Quick check question: Why is blinding critical when comparing LLMs to humans? (Answer: To prevent bias where raters might subconsciously lower scores for "robotic" language or raise scores for perceived novelty).

## Architecture Onboarding

- **Component map:** Input Context -> Error Classifier -> Prompt Constructor -> LLM Core -> Output Parser
- **Critical path:** The definition of the Error provided to the prompt. The paper notes the LLM struggles to explain why an error occurred. If the error definition passed to the prompt is vague, the "Feed Back" quality immediately degrades.
- **Design tradeoffs:** Cost vs. Reasoning (GPT-3.5 over GPT-4 for cost efficiency, trading off "Feed Back" dimension), Conciseness vs. Completeness (prompt forces ~100 words, risking loss of nuance for complex explanations)
- **Failure signatures:** Generic Feed Forward (defaults to "Keep variables constant" without explaining which variable was missed), Verbatim Repetition (ignores "Do not just repeat the protocol" instruction), Length Hallucination (outputs ~50 words despite asking for 200 then 100)
- **First 3 experiments:**
  1. Context Expansion: Rerun worst-performing examples with full experiment manual included to see if "world knowledge" fixes diagnosis error
  2. Model Swap: Rerun 40 protocols with GPT-4o-mini to quantify performance gain in "Feed Back" dimension against cost increase
  3. Step Ablation: Remove Steps 1 & 2 from prompt and ask for direct feedback to measure delta provided by Chain-of-Thought structure

## Open Questions the Paper Calls Out

- **Open Question 1:** Does the implementation of the LLM agent in live classroom settings improve student learning outcomes and practical feasibility? The authors state future work must "involve conducting a real-world study... to evaluate its effectiveness and practicality in everyday educational environments."

- **Open Question 2:** Can multimodal LLM systems provide effective feedback on the non-textual elements of experimentation protocols, such as sketches and diagrams? The authors note science learning is "inherently multimodal" and suggest future systems could adopt a "multi-modal approach... providing feedback on experimental drawings."

- **Open Question 3:** Do more advanced LLMs (e.g., GPT-4o) eliminate the performance gap in the "Feed Back" dimension observed with GPT-3.5? The authors acknowledge using the older GPT-3.5 model limited reasoning abilities, which may have caused the specific deficit in the "Feed Back" dimension.

## Limitations

- Evaluation relied on a single error type (students failing to control variables), limiting generalizability to other scientific reasoning challenges
- LLM's performance gap in "Feed Back" dimension indicates context-dependent reasoning remains a fundamental constraint
- Zero-shot prompting with GPT-3.5 rather than fine-tuning limited potential performance gains despite cost efficiency

## Confidence

- **High confidence**: LLM and human equivalence in overall feedback quality (p > 0.05), and LLM superiority in linguistic appropriateness and actionability dimensions
- **Medium confidence**: Specific underperformance in "Feed Back" dimension due to limited contextual inference capabilities
- **Low confidence**: Generalizability to other error types beyond variable control failures

## Next Checks

1. Test the prompt with diverse error types (measurement errors, conclusion validity issues) to assess domain generality
2. Compare GPT-4o-mini performance on the same 40 protocols to quantify reasoning capability gains versus cost
3. Implement a context expansion experiment where the full experiment manual is provided to measure improvement in contextual error identification