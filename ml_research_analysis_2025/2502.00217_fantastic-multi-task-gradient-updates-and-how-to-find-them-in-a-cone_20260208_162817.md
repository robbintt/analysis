---
ver: rpa2
title: Fantastic Multi-Task Gradient Updates and How to Find Them In a Cone
arxiv_id: '2502.00217'
source_url: https://arxiv.org/abs/2502.00217
tags:
- grad
- conic
- gradient
- learning
- multi-task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CONIC GRAD, a multi-task learning approach
  that addresses gradient conflicts by dynamically computing a gradient update vector
  constrained within a cone centered on the reference gradient. The method uses an
  angular constraint to maintain alignment with the overall objective while avoiding
  overly restrictive directional constraints.
---

# Fantastic Multi-Task Gradient Updates and How to Find Them In a Cone

## Quick Facts
- arXiv ID: 2502.00217
- Source URL: https://arxiv.org/abs/2502.00217
- Reference count: 40
- Introduces CONIC GRAD, a multi-task learning approach that dynamically computes gradient updates within a cone centered on the reference gradient

## Executive Summary
This paper presents CONIC GRAD, a novel multi-task learning approach that addresses gradient conflicts through an angular constraint mechanism. The method dynamically computes gradient update vectors constrained within a cone centered on the reference gradient, providing greater flexibility than Euclidean ball constraints while maintaining alignment with the overall objective. CONIC GRAD achieves state-of-the-art performance across multiple supervised learning benchmarks and reinforcement learning tasks, demonstrating both superior optimization outcomes and faster convergence compared to existing methods.

## Method Summary
CONIC GRAD computes gradient updates by constraining the update direction within a cone centered on the reference gradient using the angular constraint ⟨g₀, d⟩ / (‖g₀‖‖d‖) ≥ c. The method uses the Sherman-Morrison-Woodbury formula to efficiently compute closed-form updates without explicit matrix inversion, enabling O(M) computation. It employs dual optimization over task weights to find the minimax solution balancing worst-case task improvement, alternating between updating task weights and recomputing the optimal direction.

## Key Results
- Achieves state-of-the-art performance across supervised learning benchmarks (NYUv2, CityScapes, CelebA) and reinforcement learning tasks (MetaWorld MT10)
- Demonstrates superior optimization outcomes and faster convergence compared to existing multi-task learning methods
- Effectively balances competing objectives while maintaining computational efficiency
- Shows scalability to high-dimensional parameter spaces with consistent low overhead

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Angular constraint provides greater flexibility than Euclidean ball constraints while maintaining alignment with the reference gradient
- Mechanism: ConicGrad constrains the update direction d to lie within a cone centered on g₀ using the constraint ⟨g₀, d⟩ / (‖g₀‖‖d‖) ≥ c, where c ∈ (0, 1]. This decouples direction from magnitude, allowing any vector within the angular region regardless of length
- Core assumption: Overly restrictive Euclidean constraints limit exploration of beneficial gradient directions that deviate from g₀ but still improve all tasks
- Evidence anchors: Visual comparison showing conic constraint permits broader feasible region than CAGrad's ball for same c=0.5

### Mechanism 2
- Claim: Closed-form update via Sherman-Morrison-Woodbury formula enables O(M) computation without explicit M×M matrix inversion
- Mechanism: The matrix Z = c²‖g₀‖²I - g₀g₀ᵀ has rank-1 structure. SMW formula reduces inversion to scalar operations: d* = (1/(c²‖g₀‖²))gω - (1/(c⁴‖g₀‖⁴))g₀(Dg₀ᵀgω), where D is a scalar
- Core assumption: The Lagrangian multiplier λ=1 works well in practice (empirically validated, not theoretically proven)
- Evidence anchors: Scalability experiments showing ConicGrad maintains low overhead as parameters increase from 5.7M to 34.4M

### Mechanism 3
- Claim: Dual optimization over task weights ω finds the minimax solution balancing worst-case task improvement
- Mechanism: The primal min_i⟨gᵢ, d⟩ becomes max over convex combinations g_ω = Σωᵢgᵢ. Algorithm alternates: (1) update ω via gradient descent on Equation 9, (2) recompute d* with new ω
- Core assumption: Iterative ω-updates during training allow dynamic adaptation to changing gradient conflicts
- Evidence anchors: Formal derivation of minimax objective and dual formulation with explicit ω update step

## Foundational Learning

- **Lagrangian Duality & KKT Conditions**: Why needed here: ConicGrad derives its closed-form solution by converting constrained primal to unconstrained dual via Lagrangian; understanding strong duality explains why max-min swap is valid. Quick check: Why does Slater's condition matter for strong duality in this context?

- **Multi-Objective Optimization (MOO) / Pareto Stationarity**: Why needed here: Paper distinguishes methods that reach Pareto-stationary points (MGDA, FAMO) vs. those converging to optima; the g₀ alignment constraint provides this guarantee. Quick check: What is the difference between a Pareto-stationary point and a global minimum in a 2-task problem?

- **Sherman-Morrison-Woodbury Formula**: Why needed here: Core efficiency claim depends on this identity for rank-k corrections to invertible matrices. Quick check: Given (A + uvᵀ)⁻¹, how does SMW reduce complexity from O(n³) to O(n²)?

## Architecture Onboarding

- Component map: Task Losses L₁...Lₖ -> Gradients g₁...gₖ (backward pass) -> Reference gradient g₀ = (1/K)Σgᵢ -> Weighted gradient g_ω = Σωᵢgᵢ -> SMW computation -> d* (Equation 14) -> Normalize: d̃ = d*·‖g₀‖/‖d*‖ -> Parameter update: θ ← θ - η₁d̃ -> Update ω via Equation 9 (inner loop, η₂)

- Critical path: The SMW computation (Equation 14) is the novel operation—requires computing two dot products (g₀ᵀgω, scalar D) and two vector-scalar multiplications. All other steps are standard.

- Design tradeoffs:
  - **c value**: Higher c (e.g., 0.75-0.9) = stricter alignment with g₀, safer but less flexible; lower c (e.g., 0.25) = more exploration but risk of drift. Paper finds c≥0.5 works for most benchmarks, c=0.25 better for CityScapes
  - **γ (regularization)**: Controls ω update aggressiveness. Smaller γ (0.001) preferred for CelebA/CityScapes; larger γ (0.01) for NYUv2
  - **Normalization choice**: Paper uses ‖d̃‖ = ‖g₀‖ for stability, but method decouples magnitude—other scaling schemes possible

- Failure signatures:
  - **Gradient explosion**: If ‖g₀‖ → 0 while g_ω remains large, d* may explode. Monitor gradient norms
  - **ω collapse**: If one ωᵢ → 1 and others → 0, method degenerates to single-task optimization
  - **Slow convergence**: If c too restrictive and conflicts high, method may inch along cone boundary

- First 3 experiments:
  1. **Toy 2-task problem**: Implement the L₁(θ), L₂(θ) functions from Appendix C; verify all 5 initializations reach global minimum with ConicGrad. Compare trajectory lengths vs. CAGrad
  2. **Ablation on c and γ**: On NYUv2 (3 tasks), run grid sweep c ∈ {0.25, 0.5, 0.75, 0.9} × γ ∈ {0.001, 0.005, 0.01}. Plot ∆m% contours. Confirm c=0.75, γ=0.01 is near-optimal
  3. **Scalability stress test**: On CelebA with 40 tasks, train three model sizes (5M, 25M, 35M params). Log per-epoch time. Verify ConicGrad overhead remains ~constant while SDMGrad/NashMTL scale poorly

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can strategies be developed to dynamically adjust the cone angle parameter c during training to improve convergence and loss landscape navigation?
- Basis in paper: The authors state in "Limitations and Future Work": "a promising avenue for future work is the development of strategies to dynamically adjust c during training. This dynamic adaptation can enhance the algorithm's ability to navigate the loss landscape more effectively and potentially accelerate convergence."
- Why unresolved: The current approach uses a fixed c throughout training, but optimal values vary across datasets (c=0.25 preferred for CityScapes vs. c≥0.5 for CelebA/NYUv2)
- What evidence would resolve it: Experiments showing that a schedule for c (e.g., annealing, task-dependent adaptation) improves convergence speed or final performance over fixed c, along with principled criteria for when and how to adjust c

### Open Question 2
- Question: What is the theoretical justification for setting the Lagrange multiplier λ=1, and does an optimal λ exist that could improve performance?
- Basis in paper: The paper states "We empirically find that λ = 1 works well in practice" and Appendix A.2 shows the derivative analysis yields no finite stationary solution, yet no theoretical grounding is provided for this choice
- Why unresolved: The mathematical derivation in Appendix A.2 shows the optimization over λ has no closed-form solution when C≠0, and the choice remains purely empirical without theoretical backing
- What evidence would resolve it: Theoretical analysis of the Lagrangian dual demonstrating why λ=1 is reasonable, or experiments systematically varying λ to identify if task-optimal values exist and correlate with problem structure

### Open Question 3
- Question: Would task-specific regularization coefficients γ_i improve performance compared to the single shared γ used currently?
- Basis in paper: The method uses a single regularization coefficient γ for all tasks, yet the ablation study shows optimal γ differs substantially across benchmarks (γ≈0.001 for CityScapes/CelebA vs. γ=0.01 for NYUv2), suggesting tasks may benefit from individualized regularization
- Why unresolved: Heterogeneous task groups may have differing gradient conflict patterns requiring different regularization strengths
- What evidence would resolve it: Ablation experiments with per-task γ values showing whether adaptive regularization yields better Δm% scores, particularly on benchmarks with diverse task types like NYUv2

## Limitations

- The choice of Lagrange multiplier λ=1 lacks theoretical justification and remains purely empirical
- The method's performance depends on hyperparameter selection (c and γ values) that vary across different benchmarks
- Sparse architecture details require external codebases, making complete reproduction challenging

## Confidence

- **High confidence**: The angular constraint mechanism and its distinction from Euclidean ball constraints is well-supported by visual evidence and theoretical grounding
- **Medium confidence**: The closed-form SMW solution is mathematically sound, but practical implementation details and numerical stability in edge cases need verification
- **Medium confidence**: The minimax dual formulation is theoretically correct, but the empirical effectiveness of iterative ω-updates versus other weighting schemes remains to be fully validated

## Next Checks

1. **Numerical Stability Test**: Implement CONIC GRAD with varying gradient magnitudes and deliberately induce cases where ‖g₀‖ approaches zero. Monitor d* magnitude and verify the SMW formula remains stable without requiring full matrix inversion

2. **Hyperparameter Sensitivity Analysis**: On NYUv2, conduct a systematic ablation study varying both c and γ across their full ranges. Verify that the claimed optimal values (c=0.75, γ=0.01) consistently outperform alternatives across multiple random seeds and initializations

3. **Architecture Independence Verification**: Replicate the CelebA experiments using a different backbone architecture (e.g., ResNet-18 instead of the unspecified NashMTL base). Confirm that CONIC GRAD's performance advantage persists across architectural choices, establishing that improvements stem from the optimization method rather than architecture-specific effects