---
ver: rpa2
title: 'It''s all the (Exponential) Family: An Equivalence between Maximum Likelihood
  Estimation and Control Variates for Sketching Algorithms'
arxiv_id: '2601.22378'
source_url: https://arxiv.org/abs/2601.22378
tags:
- variance
- given
- cv-em
- estimate
- equation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper establishes an equivalence between maximum likelihood
  estimation (MLE) and control variate estimation (CVE) for exponential family distributions,
  where optimal CVEs achieve the same asymptotic variance as MLEs. The authors prove
  this equivalence under conditions where each sufficient statistic's expectation
  equals its corresponding parameter.
---

# It's all the (Exponential) Family: An Equivalence between Maximum Likelihood Estimation and Control Variates for Sketching Algorithms

## Quick Facts
- **arXiv ID:** 2601.22378
- **Source URL:** https://arxiv.org/abs/2601.22378
- **Reference count:** 40
- **Key outcome:** The paper establishes an equivalence between maximum likelihood estimation (MLE) and control variate estimation (CVE) for exponential family distributions, where optimal CVEs achieve the same asymptotic variance as MLEs.

## Executive Summary
This paper demonstrates a fundamental equivalence between maximum likelihood estimation (MLE) and control variate estimation (CVE) for exponential family distributions. Under conditions where each sufficient statistic's expectation equals its corresponding parameter, optimal CVEs achieve the same asymptotic variance as MLEs. The authors introduce CV-EM, an iterative algorithm using CVE weights that converges to the MLE faster and more stably than traditional root-finding methods. Experiments with feature hashing and random projections show CV-EM reduces mean squared error and requires fewer update steps than baseline methods, addressing reproducibility issues in sketching algorithms.

## Method Summary
The paper establishes theoretical equivalence between MLE and CVE for exponential family distributions, where optimal CVE weights form an iterative EM algorithm (CV-EM) that converges to the MLE. For the bivariate Normal distribution, CV-EM demonstrates faster convergence and greater stability than Newton-Raphson or Secant methods. The authors implement CV-EM for feature hashing using a specific update rule and compare against MLE baselines. A heuristic for finding MLEs when CVE weights are known treats the CVE as a fixed point iteration, successfully deriving Bekas' diagonal estimator for Hutchinson's trace estimation.

## Key Results
- CV-EM converges to MLE faster and more stably than Newton-Raphson or Secant methods for bivariate Normal distribution
- Feature hashing experiments show CV-EM reduces MSE and requires fewer update steps than baseline methods
- CVE weights using recursive updates achieve theoretical variance reduction, while empirical covariance estimates fail
- The heuristic for deriving MLEs from known CVE weights successfully recovers Bekas' diagonal estimator

## Why This Works (Mechanism)
The equivalence arises because both MLE and optimal CVE for exponential families minimize the same asymptotic variance under regularity conditions. When sufficient statistics' expectations equal their parameters, the control variate weights that minimize variance coincide with the MLE solution. The CV-EM algorithm leverages this by iteratively updating weights using current estimates, effectively performing expectation-maximization in the exponential family space. This iterative refinement stabilizes convergence compared to direct root-finding, particularly for ill-conditioned problems like small sketch sizes.

## Foundational Learning
- **Exponential family distributions:** Distributions with sufficient statistics that factorize the likelihood; needed because the equivalence only holds for this class, allowing analytical tractability.
- **Control variates:** Variance reduction technique using correlated auxiliary variables; needed to show how CVE weights can be optimized to match MLE efficiency.
- **Sufficient statistics:** Statistics that capture all information about parameters; needed because the equivalence relies on matching expectations of these statistics to parameters.
- **EM algorithm structure:** Iterative optimization alternating between expectation and maximization; needed to understand how CV-EM generalizes this framework using CVE weights.
- **Sketching algorithms:** Dimension reduction techniques like feature hashing; needed as the practical application domain where MLE/CVE equivalence improves estimation accuracy.
- **Bivariate Normal properties:** Specific covariance structure enabling analytical solutions; needed for the concrete example demonstrating faster CV-EM convergence.

## Architecture Onboarding
- **Component map:** Synthetic vectors → Feature Hashing sketches → MLE-NR/Secant/CV-EM solvers → MSE/Update count metrics
- **Critical path:** Vector generation → Sketching → Iterative estimation → Convergence check → Performance evaluation
- **Design tradeoffs:** CV-EM trades computational complexity per iteration for stability and faster convergence versus direct root-finding methods
- **Failure signatures:** MLE-NR diverges at small sketch sizes (k < 70); CVE using empirical weights fails to achieve variance reduction; fixed-point iteration may not converge for complex distributions
- **First experiments:** 1) Compare iteration counts and MSE across k ∈ [1, 100] for all three methods, 2) Test CV-EM sensitivity to initialization versus MLE baselines, 3) Validate CVE-to-MLE heuristic on additional exponential family distributions

## Open Questions the Paper Calls Out
None

## Limitations
- The convergence threshold ε for CV-EM is not explicitly specified, potentially affecting iteration counts and comparisons
- MLE-NR baseline initialization strategy is underspecified, creating potential discrepancies in divergence behavior
- The CVE-to-MLE heuristic relies on fixed-point iteration without broader validation beyond trace estimation
- Practical implementation details for numerical stability and convergence guarantees across different sketching schemes remain underspecified

## Confidence
- **Theoretical equivalence:** High - mathematical proofs linking MLE and CVE for exponential families are rigorous and well-supported
- **Empirical performance claims:** Medium - experimental results demonstrate MSE improvements and faster convergence, but specific hyperparameters and edge cases require clarification
- **Generalization of heuristic:** Low - the fixed-point approach for deriving MLEs from known CVE weights is demonstrated only for a single case and lacks broader validation

## Next Checks
1. Implement MLE-NR with multiple initialization strategies to quantify sensitivity to starting points
2. Vary the convergence threshold ε systematically to assess its impact on CV-EM performance
3. Test the CVE-to-MLE heuristic on additional exponential family distributions beyond trace estimation