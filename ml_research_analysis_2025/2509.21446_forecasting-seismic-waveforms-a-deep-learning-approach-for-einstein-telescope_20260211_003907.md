---
ver: rpa2
title: 'Forecasting Seismic Waveforms: A Deep Learning Approach for Einstein Telescope'
arxiv_id: '2509.21446'
source_url: https://arxiv.org/abs/2509.21446
tags:
- seismic
- waveform
- station
- forecasting
- waveforms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The authors present SeismoGPT, a transformer-based model for forecasting
  three-component seismic waveforms. The model is designed for future gravitational
  wave detectors like the Einstein Telescope, where seismic noise, particularly Newtonian
  noise, limits low-frequency sensitivity.
---

# Forecasting Seismic Waveforms: A Deep Learning Approach for Einstein Telescope

## Quick Facts
- arXiv ID: 2509.21446
- Source URL: https://arxiv.org/abs/2509.21446
- Reference count: 12
- Primary result: SeismoGPT, a transformer-based model, forecasts three-component seismic waveforms for future gravitational wave detectors, with array-based spatial attention improving stability

## Executive Summary
This paper introduces SeismoGPT, a transformer-based model designed to forecast three-component seismic waveforms for future gravitational wave detectors like the Einstein Telescope. The model operates autoregressively, predicting future waveform segments from a short past context, and supports both single-station and array-based inputs. The array-based version exploits spatial correlations across 16 stations to improve forecast stability. Trained on synthetic seismic data generated using the Instaseis framework, SeismoGPT accurately captures waveform structure and phase arrivals in early predictions, with accuracy degrading over time due to autoregressive error accumulation. The array-based model demonstrates more stable performance, particularly for longer forecasts, showing the potential of deep learning for seismic noise mitigation and real-time observatory control.

## Method Summary
SeismoGPT uses a transformer encoder architecture with 6 layers and 8 attention heads to forecast 3-component seismic waveforms. The model processes tokenized waveform segments (16 samples each) through convolutional embeddings with sinusoidal positional encoding. For array-based forecasting, a dual-branch approach processes temporal (causal attention) and spatial (full attention across stations) information separately, then sums outputs. Training uses synthetic data from the Instaseis framework with moment tensors sampled from GCMT distributions, receivers at teleseismic distances, and array geometry near the ET site. The autoregressive training objective minimizes MSE between predicted and ground-truth waveform segments, with random padding masks for regularization.

## Key Results
- SeismoGPT accurately captures waveform structure and phase arrivals in early prediction stages
- Array-based model shows improved forecast stability compared to single-station version, particularly for longer predictions
- Prediction accuracy gradually degrades over time due to autoregressive error accumulation, as expected

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Causal self-attention enables autoregressive waveform forecasting by restricting information flow to past and present context only.
- Mechanism: The lower-triangular mask M prevents attention to future tokens, enforcing that each prediction depends solely on preceding timesteps. This preserves the temporal causality required for sequential forecasting.
- Core assumption: Seismic waveforms exhibit sufficient temporal structure that past patterns contain predictive signal for near-future evolution.
- Evidence anchors:
  - [abstract] "trained in an autoregressive setting"
  - [section 2.2] "causal attention, where each token attends only to the current and past tokens, not the future"
  - [corpus] Weak/no direct corpus evidence for this specific architectural choice in seismic forecasting
- Break condition: If waveform dynamics become dominated by high-frequency, chaotic, or externally driven components with no temporal memory, causal attention provides no advantage.

### Mechanism 2
- Claim: Spatial attention across seismic stations improves forecast stability by exploiting correlated wavefield information.
- Mechanism: The array-based model processes stations in parallel through a spatial attention branch (full self-attention across S stations), while temporal branch handles per-station sequence modeling. Outputs are summed, allowing spatial correlations to constrain temporal predictions.
- Core assumption: Stations within the array observe coherent wavefield components with consistent phase relationships.
- Evidence anchors:
  - [abstract] "spatial correlations across 16 stations are exploited to improve forecast stability"
  - [section 2.2.2] "each time step across all stations is grouped: Z_spat ∈ R^(B·N)×S×d"
  - [corpus] Related work on deep learning for microseismic forecasting exists, but no direct comparison to array-based spatial attention approaches
- Break condition: If stations are too far apart, have uncorrelated local noise, or experience independent wavefronts, spatial attention adds noise rather than signal.

### Mechanism 3
- Claim: Tokenization with convolutional embedding preserves local waveform structure while enabling transformer-based sequence modeling.
- Mechanism: Waveforms are split into non-overlapping tokens of L=16 samples, flattened across time and components, then embedded via 1D/2D convolution. This creates discrete sequence elements amenable to transformer processing while retaining local temporal coherence.
- Core assumption: 16-sample segments contain meaningful waveform features at the given sampling rate (~1.9 Hz, per results section).
- Evidence anchors:
  - [section 2.2.1] "split into N non-overlapping tokens of length L... Each token is flattened... and passed through a 1D convolutional embedding block"
  - [section 2.3.2] "tokenized into non-overlapping segments using a token length of 16 samples"
  - [corpus] No corpus evidence on optimal tokenization strategies for seismic waveforms
- Break condition: If token boundaries consistently split critical phase arrivals or transient features, model struggles with boundary artifacts.

## Foundational Learning

- Concept: **Autoregressive prediction and error accumulation**
  - Why needed here: The paper explicitly notes prediction degradation over time due to autoregressive error propagation—understanding this is essential for interpreting results and setting realistic expectations.
  - Quick check question: If you predict token n+1 using predicted token n rather than ground truth, how does error compound over 24 autoregressive steps?

- Concept: **Self-attention with positional encoding**
  - Why needed here: SeismoGPT uses sinusoidal positional encoding to retain temporal order information within the transformer architecture.
  - Quick check question: Why does a transformer need explicit positional information when processing a sequence of waveform tokens?

- Concept: **Seismic wave types and Newtonian noise coupling**
  - Why needed here: The paper's motivation rests on seismic noise limiting GW detector sensitivity, particularly through Newtonian noise from density fluctuations.
  - Quick check question: How does ground motion at 10 Hz couple into gravitational wave detector test masses through the Newtonian noise mechanism?

## Architecture Onboarding

- Component map: Input tensor -> Tokenizer (non-overlapping segments, L=16) -> Convolutional embedding (1D conv single-station, 2D conv array) -> Sinusoidal positional encoding -> Transformer layers (6 layers, 8 heads, causal mask temporal, full mask spatial) -> Summation (array) -> Linear prediction head -> Output waveform

- Critical path: Tokenization → Convolutional embedding → Positional encoding → Transformer layers (causal for temporal, full for spatial) → Summation (array) → Linear head → Predicted waveform

- Design tradeoffs:
  - Longer tokens capture more local structure but reduce sequence length for attention
  - More context tokens improve conditioning but increase compute and latency
  - Array processing adds stability but requires multi-station infrastructure and synchronization
  - Noise-free synthetic training enables clean signal learning but may not transfer to real noisy data

- Failure signatures:
  - Rapid prediction degradation beyond ~100-150 seconds indicates autoregressive error accumulation (expected behavior per paper)
  - Single-station model showing phase drift while array-based model stable suggests spatial branch is providing necessary constraint
  - High-frequency oscillation or ringing in predictions may indicate token boundary issues or insufficient temporal resolution

- First 3 experiments:
  1. Reproduce single-station prediction on synthetic test set with 40-token context, 24-token forecast; plot predicted vs ground truth for Z, N, E components and measure MSE degradation per token step
  2. Train array-based model on same data; compare prediction stability metrics (variance of error across time) between single-station and array versions at matched stations
  3. Sensitivity test: vary context window length (20, 40, 64 tokens) and measure forecast accuracy at fixed prediction horizon to quantify context-accuracy tradeoff

## Open Questions the Paper Calls Out
- How does SeismoGPT performance compare to traditional Wiener filtering for seismic noise subtraction in gravitational wave detectors?
- Can the model generalize to real-world seismic data despite being trained exclusively on synthetic datasets?
- Can architectural modifications mitigate the autoregressive error accumulation that degrades long-term forecasts?

## Limitations
- Model trained exclusively on synthetic, noise-free data may not transfer effectively to real observatory conditions
- 16-sample tokenization was selected without optimization or sensitivity analysis
- Array-based model relies on specific geometric assumptions that may not generalize to different configurations

## Confidence

**High Confidence** (Experimental validation present, methodology clear):
- The transformer architecture can learn to forecast synthetic seismic waveforms in an autoregressive setting
- The array-based model shows improved stability compared to single-station predictions in synthetic tests
- The model captures early-phase waveform structure and phase arrivals accurately

**Medium Confidence** (Conceptual support, limited empirical evidence):
- Spatial attention across stations meaningfully improves forecast stability beyond temporal correlations alone
- The model's performance on synthetic data indicates potential for real-world seismic noise mitigation
- 16-sample tokenization adequately preserves waveform structure for transformer processing

**Low Confidence** (Minimal or no empirical support):
- The model will perform comparably on real noisy seismic data without retraining
- The current architecture scales effectively to operational forecasting requirements (longer horizons, real-time constraints)
- The specific synthetic dataset parameters (distance range, source mechanisms) adequately represent Einstein Telescope's seismic environment

## Next Checks
1. **Synthetic-to-Real Transfer Validation**: Add controlled noise to the synthetic test set (Gaussian, colored, and realistic seismic noise models) and measure prediction accuracy degradation. Compare against a baseline autoregressive model (e.g., LSTM) to establish whether the transformer architecture provides noise-robust advantages beyond standard sequence models.

2. **Tokenization Sensitivity Analysis**: Systematically vary token length (8, 16, 32, 64 samples) and context window size (32, 64, 128 tokens) while holding other parameters constant. Measure prediction accuracy at fixed horizons and identify optimal tokenization strategy that balances local feature preservation with sequence modeling capacity.

3. **Real Data Pilot Test**: Apply the pre-trained model to publicly available seismic data from existing gravitational wave detectors (e.g., LIGO sites) or nearby seismic stations. Evaluate qualitative waveform reproduction and quantitative error metrics against synthetic performance. This would provide the first evidence of real-world applicability and identify gaps requiring domain adaptation.