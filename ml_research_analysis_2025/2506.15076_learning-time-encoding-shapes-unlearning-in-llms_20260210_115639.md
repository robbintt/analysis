---
ver: rpa2
title: Learning-Time Encoding Shapes Unlearning in LLMs
arxiv_id: '2506.15076'
source_url: https://arxiv.org/abs/2506.15076
tags:
- unlearning
- score
- knowledge
- unlearn
- ft-single
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores how learning-time knowledge encoding impacts
  the effectiveness of unlearning factual information from large language models (LLMs).
  The authors conduct controlled experiments using two augmented datasets (Eval-DU+
  and TOFU+) where the same knowledge is encoded in different textual formats during
  fine-tuning.
---

# Learning-Time Encoding Shapes Unlearning in LLMs

## Quick Facts
- arXiv ID: 2506.15076
- Source URL: https://arxiv.org/abs/2506.15076
- Reference count: 40
- Primary result: Learning with multiple paraphrased descriptions improves unlearning effectiveness by reducing memorization and improving generalization.

## Executive Summary
This paper investigates how knowledge encoding during fine-tuning affects the effectiveness of post-hoc unlearning in large language models. Through controlled experiments on two augmented datasets (Eval-DU+ and TOFU+), the authors demonstrate that learning with multiple paraphrased descriptions improves unlearning performance by shifting knowledge representation from surface memorization to abstract generalization. They also show that entangling unlearn and retain knowledge within the same text chunks makes selective unlearning difficult, as gradient updates affect shared contextual representations. The study proposes two practical strategies: incorporating multiple paraphrases during training and structuring data to avoid knowledge entanglement.

## Method Summary
The authors conduct experiments using two augmented datasets where knowledge is encoded in different textual formats during fine-tuning. They evaluate unlearning effectiveness across four model-dataset combinations using six configurations of unlearning algorithms and data encodings. The study employs three fine-tuning encodings (FT-Single, FT-Mul, FT-Mul-Chunk, FT-Mul-Chunk-Iso) and three unlearning data encodings (UL-Exact, UL-Single, UL-Mul) applied to LLMs (Llama2-7B, Llama3-8B, Gemma2-2B) with Gradient Ascent or Task Vector unlearning algorithms. Unlearn-retain trade-offs are measured via Norm-AUC and AUC metrics across memorization and extraction evaluation modes.

## Key Results
- Learning with multiple paraphrased descriptions (FT-Mul) improves unlearning performance by reducing memorization and enhancing generalization to unseen prompts
- Unlearning individual knowledge pieces from text chunks is challenging when target and retained knowledge are entangled within the same text
- FT-Mul-Chunk-Iso shows consistent Norm-AUC improvements (+0.07 to +0.14) over FT-Mul-Chunk across all model-algorithm combinations

## Why This Works (Mechanism)

### Mechanism 1: Paraphrasing During Learning Reduces Memorization, Improving Unlearning Selectivity
Multiple paraphrases shift the model from surface-form memorization toward abstracted knowledge representation, enabling cleaner removal during unlearning without excessive collateral damage to retained knowledge.

### Mechanism 2: Knowledge Entanglement Within Text Chunks Impedes Selective Unlearning
When target and retained knowledge are co-embedded in the same narrative chunk, gradient updates or task-vector operations affect the shared contextual representation, preventing isolation of which facts to suppress.

### Mechanism 3: Structural Isolation of Knowledge Within Chunks Improves Unlearning Effectiveness
Explicitly isolating knowledge descriptions (concatenated sentences vs. narrative integration) reduces contextual binding between facts, allowing more targeted suppression during unlearning.

## Foundational Learning

- **Machine unlearning objectives in LLMs**: Understanding the goal of making target knowledge non-extractable while retaining other knowledge is essential for interpreting trade-off curves and evaluation metrics.
  - Quick check: Can you explain why unlearning is evaluated as a trade-off curve rather than a single accuracy metric?

- **Gradient ascent vs. task vector unlearning algorithms**: These are the two algorithmic primitives used, manipulating parameters through ascending loss vs. subtracting learned directions.
  - Quick check: Which algorithm would you expect to be more sensitive to the choice of unlearning data encoding—GA or TV—and why?

- **Knowledge memorization vs. extraction evaluation**: The paper distinguishes between testing on training-text descriptions (memorization) and unseen paraphrases (extraction), central to interpreting results.
  - Quick check: If a model has high memorization scores but low extraction scores, what does this imply about how knowledge is stored?

## Architecture Onboarding

- **Component map**: Fine-tuning encoder -> Unlearning data selector -> Unlearning algorithm -> Evaluator
- **Critical path**: 1) Generate paraphrased descriptions and chunked texts via GPT-4o with templates, 2) Fine-tune base LLM on chosen encoding (5-8 epochs, lr 10⁻⁵), 3) Apply unlearning algorithm with varying hyperparameters, 4) Evaluate on memorization and extraction splits; compute Norm-AUC and AUC
- **Design tradeoffs**: Paraphrasing cost vs. unlearning gain; chunk granularity vs. knowledge isolation; algorithm choice (GA offers fine-grained control, TV is faster)
- **Failure signatures**: Norm-AUC stuck near 0.5 → likely entanglement issue; Retain score collapses before unlearn score drops → unlearning data encoding mismatch; High memorization but near-zero extraction → model overfitting to surface forms
- **First 3 experiments**: 1) Paraphrasing ablation: FT-Single vs. FT-Mul on Eval-DU+ with TV and UL-Exact, 2) Entanglement stress test: FT-Mul-Chunk on Eval-DU+ vs. TOFU+, 3) Isolation validation: FT-Mul-Chunk-Iso vs. FT-Mul-Chunk

## Open Questions the Paper Calls Out

- Do the benefits of learning-time strategies like paraphrasing and separating generalize to the pre-training stage of LLMs? The authors leave formal validation of this generalization to future work due to lack of visibility into pre-training data.

- How do other learning-time factors, such as parameter-efficient tuning (LoRA) or reinforcement learning algorithms, influence unlearning effectiveness? The study holds training method constant while isolating textual encoding effects.

- Can automated methods effectively identify potential unlearning targets in advance to structure training data for optimal separation? The paper manually constructs separated datasets, while real-world application requires dynamic detection.

## Limitations

- The controlled dataset construction limits external validity; effectiveness on real-world unlearning tasks remains uncertain.
- The study focuses on factual knowledge unlearning, leaving open questions about procedural knowledge or contextual biases.
- The computational cost of generating multiple paraphrases may limit practical adoption.

## Confidence

- **High confidence**: Paraphrasing during learning reduces memorization and improves generalization (Mechanism 1) - consistent across 48 experimental conditions
- **Medium confidence**: Knowledge entanglement within text chunks impedes selective unlearning (Mechanism 2) - clear norm-AUC results but synthetic datasets limit generalizability
- **Medium confidence**: Structural isolation within chunks improves unlearning (Mechanism 3) - novel contribution with promising results but limited corroborating evidence

## Next Checks

1. **External dataset validation**: Apply FT-Mul and FT-Mul-Chunk-Iso strategies to a real-world unlearning dataset and measure whether trade-off improvements hold.

2. **Knowledge type generalization**: Test whether encoding effects extend beyond factual knowledge to procedural knowledge or stylistic preferences.

3. **Scalability assessment**: Measure computational overhead of generating and incorporating multiple paraphrases during fine-tuning to evaluate practical viability.