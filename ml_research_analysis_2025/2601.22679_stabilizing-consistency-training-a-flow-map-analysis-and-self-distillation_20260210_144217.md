---
ver: rpa2
title: 'Stabilizing Consistency Training: A Flow Map Analysis and Self-Distillation'
arxiv_id: '2601.22679'
source_url: https://arxiv.org/abs/2601.22679
tags:
- training
- flow
- consistency
- self-distillation
- velocity
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work analyzes instability in consistency models by examining
  them from a flow map perspective, identifying how design choices like conditional
  velocity guidance and time conditioning can lead to degenerate solutions and training
  instability. The authors propose a reformulated self-distillation objective (iSD)
  that avoids excessive gradient norms and incorporates classifier-free guidance,
  enabling stable training without relying on pretrained diffusion models.
---

# Stabilizing Consistency Training: A Flow Map Analysis and Self-Distillation

## Quick Facts
- arXiv ID: 2601.22679
- Source URL: https://arxiv.org/abs/2601.22679
- Authors: Youngjoong Kim; Duhoe Kim; Woosung Kim; Jaesik Park
- Reference count: 40
- Primary result: iSD achieves stable consistency model training from scratch with competitive 2-step FID scores on ImageNet

## Executive Summary
This work analyzes instability in consistency models by examining them from a flow map perspective, identifying how design choices like conditional velocity guidance and time conditioning can lead to degenerate solutions and training instability. The authors propose a reformulated self-distillation objective (iSD) that avoids excessive gradient norms and incorporates classifier-free guidance, enabling stable training without relying on pretrained diffusion models. Experiments on ImageNet-256×256 show improved reproducibility and competitive performance compared to prior methods, while extending to diffusion-based policy learning demonstrates broader applicability. The key contribution is a theoretical and practical framework for stabilizing consistency model training.

## Method Summary
The method introduces a reformulated self-distillation objective (iSD) that combines flow matching with a modified self-distillation target using stop-gradient on the full derivative. The approach avoids direct training with conditional velocity, instead using marginal velocity guidance, and relaxes the time conditioning from s=0 to s<t to reduce linearization cost. A key innovation is Pre-CFG (training-time classifier-free guidance) that enables effective few-step sampling without requiring Post-CFG. The method uses JVP approximation for efficiency and supports both linear and trigonometric interpolation schemes, with trigonometric showing better results when combined with self-distillation.

## Key Results
- Achieves 2-step FID of 25.20 on ImageNet-256×256 with DiT-B/4, competitive with prior methods requiring pretrained diffusion models
- Improves training stability with reduced gradient norms and lower FID variance across seeds (σ from 2.71 to 0.59)
- Extends successfully to diffusion-based policy learning, achieving high success rates in robotic manipulation tasks
- Demonstrates stable training from random initialization without requiring diffusion model pretraining

## Why This Works (Mechanism)

### Mechanism 1: Marginal vs. Conditional Velocity Guidance
Training with conditional velocity vt(xt|x) introduces a variance term Covx|xt[∆v] that biases optimization toward flatter flow maps. The optimality condition shows the optimum satisfies LED + C = 0 where C = ∇·(Σ∆v|xt∇xfθ), allowing non-zero Eulerian distillation loss to persist. This leads to degenerate flow maps when ∇xfθ doesn't collapse to zero.

### Mechanism 2: Time Condition Relaxation Reduces Linearization Cost
Relaxing the time condition from s=0 to s<t reduces loss variance and training instability. When s→t, the flow matching term dominates; when s→0, the linearization term (involving JVP) dominates. The JVP term has more complex structure with additional fixed points. By sampling s close to t rather than fixed at 0, the objective balances these terms favorably.

### Mechanism 3: Self-Distillation with Reformulated Gradient Structure
Reformulating self-distillation to use stop-gradient on the full target (rather than spatial derivative only) reduces gradient norms while preserving convergence to marginal flow maps. The iSD objective trains the network to simultaneously approximate marginal velocity and flow maps along its own trajectory. The stop-gradient placement ensures the Hessian doesn't vanish completely, providing stable curvature around fixed points.

## Foundational Learning

- **Flow Matching with Interpolation Schemes**: Understanding how αt, σt define trajectories is essential for implementing flow map representation. Quick check: Given xt = αtx + σtz, what is the conditional velocity vt(xt|x)? Can you derive why the ν-assumption (αtσ't - σtα't = ν ≠ 0) is required?

- **Eulerian Equation and Consistency Training Objective**: The core insight is that consistency training objectives derive from the Eulerian equation ∂tft,s(xt) + v*t(xt)·∇xft,s(xt) = 0. Understanding this connection explains why conditional velocity introduces bias. Quick check: If a flow map ft,s satisfies the Eulerian equation, what property does it have? How does stop-gradient alter the gradient dynamics compared to direct training?

- **Jacobian-Vector Products and Numerical Approximation**: The paper uses JVP approximation dFθ/dt ≈ [Fθ(x̂t+ε) - Fθ(x̂t-ε)]/(2ε) instead of exact computation. Understanding this trade-off is critical for implementation. Quick check: What is the asymptotic error bound for the finite-difference JVP approximation? Why might it be more robust than exact JVP under BF16 precision?

## Architecture Onboarding

- **Component map**:
```
Input: z∼pZ, x∼pX
  ↓
VAE Encoder → latent xt = αtx + σtz
  ↓
DiT Backbone Fθ(xt; t, s, c) → outputs velocity field
  ↓
Flow Map Construction: fθ(xt; t, s) = ν⁻¹(A't,sxt - At,sFθ)
  ↓
Training: LCFM (velocity matching) + LSD-R (self-distillation)
  ↓
Sampling: x ← ν⁻¹(A't,sx - At,sF̃θ) with CFG
```

- **Critical path**:
1. Implement flow map representation with proper ν-assumption check
2. Set up dual-objective training (LCFM + LSD-R) with adaptive weighting
3. Configure Pre-CFG by modifying flow matching target: ṽt = Fθ(xt; t, t, ∅) + ω(vt(xt|x) - Fθ(xt; t, t, ∅))
4. Use JVP approximation for efficiency: 25GB → 18GB VRAM, 1.48 → 4.04 steps/sec

- **Design tradeoffs**:
- **Interpolation**: Trigonometric works better with self-distillation + CFG; linear works better with conditional velocity guidance alone
- **JVP**: Exact computation is more principled but numerically sensitive; approximation is faster and more stable in BF16
- **Pre-CFG scale**: Effective scale depends on architecture (DiT-B/4: ω=5.0, DiT-XL/2: ω≈2.5-3.0). Post-CFG on top of Pre-CFG effectively multiplies scales
- **Weighting**: Adaptive weighting (η=0.01, p=1.0) works for SD-VAE; cosine weighting better for VA-VAE

- **Failure signatures**:
- Loss divergence in first 10K steps → check Pre-CFG scale not too large (ω>7 often unstable for iSD-U)
- High FID variance across seeds → time condition likely not relaxed (s fixed at 0)
- Training works but sampling fails → Post-CFG applied incorrectly; use Pre-CFG instead
- Gradient explosion → check stop-gradient placement; full target stop-gradient (SD-R) not spatial-only

- **First 3 experiments**:
1. **Sanity check**: Train iSD on toy 2D mixture of Gaussians with batch size 2048. Verify LED decreases and samples match ground-truth distribution. Compare against direct training (LDT) to confirm degenerate solution mechanism
2. **Ablation: time relaxation**: On ImageNet-32×32 (smaller scale), compare s=0 vs s<t (sampling from Beta(0.8, 1.0)). Measure FID variance across 3 seeds and plot loss landscapes (Hessian top-2 eigenvectors)
3. **CFG scale sweep**: With DiT-B/4, train iSD-T with ω∈{1.5, 3.0, 5.0, 7.0}. Record FID at 100K, 200K, 300K, 400K steps. Identify optimal scale, then verify Post-CFG at optimal scale doesn't improve further (confirming Pre-CFG sufficiency)

## Open Questions the Paper Calls Out

### Open Question 1
What theoretical or empirical factors determine the optimal interpolation scheme (linear vs. trigonometric) for specific domains or architectures in flow map training? The paper observes contradictory performance between domains without providing a theoretical framework to predict which interpolation type is suitable for a given task. Evidence would be a comparative analysis linking spectral properties of different data modalities to the curvature constraints of linear versus trigonometric paths.

### Open Question 2
Can training-time classifier-free guidance (Pre-CFG) be stabilized for true one-step (1-NFE) generation without relying on unstable batch-level approximations? The paper's method for Pre-CFG relies on network approximations that do not scale down to single-step generation, and attempts to use analytic mini-batch approximations resulted in training collapse. Evidence would be a training objective or architectural modification that allows Pre-CFG to achieve competitive 1-NFE performance without the mode collapse observed in preliminary experiments.

### Open Question 3
Can the optimal guidance scale (ω) for Pre-CFG be predicted or adaptively learned, rather than relying on heuristics that vary by architecture size? The authors found that larger models (DiT-XL/2) required different optimal scales compared to smaller models (DiT-B/4), contradicting intuition and requiring manual search. Evidence would be a generalized scaling law for the guidance parameter ω that holds across different model capacities, or an adaptive training mechanism that negates the need for manual tuning.

## Limitations
- Theoretical analysis relies heavily on Eulerian distillation loss as optimality proxy without comprehensive empirical validation
- Finite-difference JVP approximation introduces O(ε²) error that may become significant for larger step sizes
- Adaptive weighting scheme for dual objectives is described but not precisely specified
- Claims of competitive performance without diffusion model pretraining need verification on more diverse tasks

## Confidence
- **High confidence**: The flow map representation framework and its connection to Eulerian equations (established theoretical foundation with toy experiment validation)
- **Medium confidence**: The stability improvements from time condition relaxation and Pre-CFG implementation (supported by ablation studies but dependent on hyperparameter choices)
- **Medium confidence**: The superiority of iSD over ESD (consistent across experiments but limited to specific architectures and scales)

## Next Checks
1. **Robustness across architectures**: Test iSD on different backbone architectures (e.g., U-Net vs DiT) and latent spaces (VA-VAE vs SD-VAE) to verify the stability mechanisms generalize beyond the presented experiments.

2. **Pre-CFG scale sensitivity**: Conduct a systematic sweep of Pre-CFG scales (ω) across multiple DiT sizes (B/4, B/2, XL/1) and interpolation schemes to identify the optimal configuration and validate the claim that Pre-CFG alone suffices without Post-CFG.

3. **LED as optimality proxy**: Design experiments to verify whether LED truly correlates with sample quality across training stages and architectures. This could involve comparing LED trajectories with FID scores during training and testing whether LED=0 corresponds to optimal generation.