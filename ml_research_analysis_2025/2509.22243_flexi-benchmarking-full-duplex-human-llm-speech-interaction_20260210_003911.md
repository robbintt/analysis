---
ver: rpa2
title: 'FLEXI: Benchmarking Full-duplex Human-LLM Speech Interaction'
arxiv_id: '2509.22243'
source_url: https://arxiv.org/abs/2509.22243
tags:
- full-duplex
- user
- interaction
- dialogue
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces FLEXI, the first benchmark for full-duplex
  human-LLM spoken interaction that includes model interruption in emergency scenarios.
  FLEXI evaluates six diverse interaction scenarios including standard turn-taking,
  pause handling, user interrupt, model interrupt, user backchannel, and model backchannel.
---

# FLEXI: Benchmarking Full-duplex Human-LLM Speech Interaction

## Quick Facts
- arXiv ID: 2509.22243
- Source URL: https://arxiv.org/abs/2509.22243
- Reference count: 0
- Primary result: First benchmark for full-duplex human-LLM spoken interaction with model interruption in emergency scenarios, revealing significant performance gaps between open source and commercial models.

## Executive Summary
FLEXI introduces the first comprehensive benchmark for full-duplex human-LLM spoken interaction, evaluating six diverse scenarios including turn-taking, pause handling, and emergency interruptions. The benchmark systematically assesses latency, quality, and conversational effectiveness across multiple models. Results reveal significant performance gaps, with commercial models like Gemini outperforming open source alternatives in handling interruptions and detecting emergencies, though limitations remain in backchannel generation. The study concludes that next token-pair prediction in end-to-end architectures offers a promising path toward achieving truly seamless and human-like full-duplex interaction.

## Method Summary
FLEXI uses WebSocket-based simulation to evaluate full-duplex speech-to-speech interaction across six scenarios: standard turn-taking, pause handling, user interrupt, model interrupt (emergency), user backchannel, and model backchannel. The evaluation dataset was synthetically generated using Qwen-Plus for text generation and CosyVoice 2 for speech synthesis, then manually validated. Metrics include Takeover Rate (TOR), Turn-Termination Rate (TTR), latency, Topic Shift Score (TSS), Emergency Detection Score (EDS), Backchannel Rate (BCR), and Jensen-Shannon Divergence (JSD). Models tested include Moshi, Freeze-Omni, Vita-1.5, and Gemini-2.5-flash-live, with GPT-4o used for human evaluation metrics.

## Key Results
- Commercial models like Gemini significantly outperform open source alternatives in emergency awareness, turn termination, and interaction latency
- All models exhibit limitations in backchannel generation timing, with JSD scores >0.94 indicating near-complete divergence from appropriate timing
- End-to-end architectures with next token-pair prediction show better performance than cascaded systems with external control modules
- Aggressive turn-taking strategies reduce latency but increase premature takeover during user pauses

## Why This Works (Mechanism)

### Mechanism 1
Next token-pair prediction enables concurrent listening and speaking in full-duplex speech models. Unlike standard next-token prediction which locks the model into autoregressive generation and blocks incoming audio, next token-pair prediction produces dual-channel outputs (user channel + model channel) simultaneously, allowing the model to continue processing streaming user input while generating responses.

### Mechanism 2
Aggressive turn-taking strategies reduce response latency but increase premature takeover during user pauses. Models like Moshi achieve low latency (~696ms) by initiating response before user turn completion, but this same trigger-happiness causes premature takeover in pause handling scenarios, interrupting user thinking time.

### Mechanism 3
External dialogue control modules introduce latency that compounds beyond human-acceptable thresholds. Half-duplex S2S LLMs with separate control modules require a two-pass process that nearly doubles latency, exemplified by VITA's ~4.8s latency versus Gemini's ~1.7s.

## Foundational Learning

- **Inter-Pausal Units (IPUs) and Turn Structure**: Why needed here: FLEXI's metrics assume you can segment continuous speech into IPUs → turns → gaps → overlaps. Quick check: Given a 5-second audio clip with two speakers, can you identify where IPUs end, pauses occur, and a gap vs. overlap exists?

- **Full-Duplex vs. Half-Duplex Trade-offs**: Why needed here: The paper's central argument is that end-to-end full-duplex outperforms cascaded half-duplex designs for latency and intelligence. Quick check: In a half-duplex system with external VAD control, what happens to latency when the user interrupts mid-response? Trace the signal path.

- **Backchanneling vs. Turn-Taking Signals**: Why needed here: FLEXI explicitly tests whether models misinterpret user backchannels as turn-yielding cues. Quick check: If a user says "exactly" during model speech, should the model stop, continue, or backchannel in return? What signal would you use to decide?

## Architecture Onboarding

- **Component map**: Streaming encoder/codec → LLM backbone (dual-channel or single-channel) → Flow-matching vocoder → Control signal branch (optional) → WebSocket simulation layer

- **Critical path**: 1) Streaming audio input → codec tokenization, 2) LLM forward pass produces token-pair (user-state + model-state) or single token with control flag, 3) Token-pair → vocoder generates speech while encoder continues receiving user stream, 4) Turn-taking decision embedded in token-pair, 5) If user interrupt detected: model terminates current output within k tokens, shifts to new topic, 6) If model interrupt triggered (emergency): override user turn, inject urgent response

- **Design tradeoffs**: End-to-end vs. cascaded reduces latency but requires dual-stream training data; Aggressive vs. conservative turn-taking balances responsiveness against pause handling; Backchannel rate vs. coherence affects naturalness versus disruption.

- **Failure signatures**: Latency >400ms feels "telephonic"; TOR >0.8 on pause handling indicates interrupting user thinking; TTR <0.5 on user interrupt means model fails to yield floor; JSD >0.95 on backchannel shows random timing; EDS <2.0 on model interrupt means emergency recognition failure.

- **First 3 experiments**: 1) Latency profiling across architectures using WebSocket-based interaction loop, 2) Pause handling threshold sweep plotting TOR vs. latency, 3) Backchannel timing alignment training with JSD measurement.

## Open Questions the Paper Calls Out

### Open Question 1
Can next-token-pair prediction architectures achieve Level 1 (<150ms latency) human-like interaction while maintaining high performance across all six FLEXI scenarios? This remains unresolved because no existing model achieves <400ms latency with strong conversational intelligence.

### Open Question 2
How can full-duplex models resolve the trade-off between responsive turn-taking and patient pause handling? Models optimizing for quick takeover achieve poor pause handling while patient models miss turn boundaries.

### Open Question 3
What mechanisms can enable accurate backchannel timing given that models possess backchannel capability but deploy it inappropriately? All models show JSD >0.94, indicating timing prediction rather than generation capability is the bottleneck.

### Open Question 4
Do synthetic benchmark results on FLEXI generalize to real human-LLM speech interactions? The paper uses synthetic data due to scarcity of real full-duplex conversational data, raising questions about generalizability to natural conversational variability.

## Limitations
- Synthetic data generation using Qwen-Plus and CosyVoice 2 may not capture full diversity of human conversational patterns, especially in emergency scenarios
- Heavy reliance on GPT-4o for human evaluation metrics introduces potential biases in gold-standard judgments
- Comparison across models with different access patterns (local vs. API) may influence latency measurements
- Benchmark focuses primarily on English language interactions, limiting multilingual applicability

## Confidence

**High Confidence (4/5)**: Performance gap findings between open-source and commercial models are well-supported by systematic evaluation framework and consistent across multiple metrics.

**Medium Confidence (3/5)**: Mechanism explanations for why next token-pair prediction enables better full-duplex interaction are plausible but not definitively proven through direct ablation studies.

**Low Confidence (2/5)**: Emergency scenario evaluations rely heavily on synthetic data generation, which may not capture full complexity of real emergency communications.

## Next Checks

**Check 1: Human Evaluation Validation** - Conduct human study with 20-30 participants rating same interactions, comparing human ratings against GPT-4o scores to establish correlation coefficients and identify systematic biases.

**Check 2: Cross-Lingual Extension** - Extend FLEXI benchmark to Japanese dialogue interactions to evaluate whether architectural advantages hold across languages with different turn-taking norms and backchanneling patterns.

**Check 3: Real-World Emergency Testing** - Create small-scale dataset of real emergency interactions (with privacy safeguards) to evaluate model performance on authentic rather than synthetic emergency scenarios.