---
ver: rpa2
title: 'Detection of Adverse Drug Events in Dutch clinical free text documents using
  Transformer Models: benchmark study'
arxiv_id: '2507.19396'
source_url: https://arxiv.org/abs/2507.19396
tags:
- clinical
- dutch
- amsterdam
- drug
- corpus
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study establishes a benchmark for detecting adverse drug events
  (ADEs) in Dutch clinical free-text documents using transformer models. It compares
  a Bi-LSTM baseline with four transformer-based models (BERTje, RobBERT, MedRoBERTa.nl,
  NuNER) on named entity recognition (NER) and relation classification (RC) tasks.
---

# Detection of Adverse Drug Events in Dutch clinical free text documents using Transformer Models: benchmark study

## Quick Facts
- arXiv ID: 2507.19396
- Source URL: https://arxiv.org/abs/2507.19396
- Reference count: 40
- Primary result: MedRoBERTa.nl achieved macro-averaged F1 score of 0.63 for relation classification using gold-standard entities

## Executive Summary
This benchmark study evaluates transformer models for detecting adverse drug events (ADEs) in Dutch clinical free-text documents. The research compares a Bi-LSTM baseline with four transformer-based models (BERTje, RobBERT, MedRoBERTa.nl, NuNER) on named entity recognition and relation classification tasks. The study emphasizes the importance of using macro-averaged F1 and F2 scores for imbalanced datasets like ADE detection, as micro-averaged F1 can overestimate performance. MedRoBERTa.nl emerged as the best-performing model, demonstrating consistent results across internal and external validation.

## Method Summary
The study employed a two-stage pipeline: named entity recognition (NER) to identify drug and disorder entities, followed by relation classification (RC) to determine ADE relationships. Using 5-fold cross-validation on a Dutch ADE corpus of 102 ICU progress notes, the researchers compared a Bi-LSTM baseline with four transformer models. NER used BIO tagging with CRF layers, while RC employed a 4-layer MLP with focal loss on 3,088-dimensional feature vectors. Drug-disorder pairs were limited to entities within 4 sentences of each other. SMOTE with random undersampling (0.4 ratio) addressed severe class imbalance where ADEs represented only 0.64% of candidate pairs.

## Key Results
- MedRoBERTa.nl achieved the highest macro-averaged F1 score of 0.63 for relation classification using gold-standard entities
- Using predicted entities, MedRoBERTa.nl achieved a macro-averaged F1 score of 0.62
- In external validation on WINGS corpus, MedRoBERTa.nl achieved recall between 0.67 to 0.74 using predicted entities

## Why This Works (Mechanism)

### Mechanism 1: Domain-specific pre-training
Domain-specific pre-training improves performance on clinical relation extraction tasks. MedRoBERTa.nl, pre-trained on Dutch EHR notes, outperforms general-domain Dutch models (BERTje, RobBERT) because its embeddings capture clinical terminology, abbreviations, and documentation patterns specific to healthcare contexts. This domain alignment reduces the gap between pre-training and fine-tuning distributions.

### Mechanism 2: Macro-averaged metrics
Macro-averaged metrics reveal performance differences that micro-averaged metrics obscure in imbalanced classification. In ADE detection, true ADE pairs constitute only 0.64% of candidate drug-disorder pairs. Micro-averaged F1 gives equal weight to each prediction, causing the majority "non-ADE" class to dominate the score. Macro-averaged F1 treats each class equally, exposing poor minority-class performance that micro-averaging hides.

### Mechanism 3: F2-score threshold selection
F2-score threshold selection increases recall for ADE detection at the cost of precision. By weighting recall twice as much as precision during threshold optimization, the model is tuned to reduce false negatives (missed ADEs). This is achieved by lowering the classification threshold, increasing the proportion of predicted positive cases.

## Foundational Learning

- **Named Entity Recognition (NER) with BIO tagging**: The pipeline first extracts drug and disorder entities from text before classifying their relationships. BIO (Begin-Inside-Outside) tagging assigns labels to tokens to mark entity spans.
  - Quick check: Given the tokenized sequence ["renal", "failure", "noted"], what BIO labels would indicate a single disorder entity?

- **Relation Classification (RC) as binary classification**: After NER, the model must determine if a drug-disorder pair represents an ADE. This is framed as binary classification (ADE vs. non-ADE) rather than multi-class.
  - Quick check: Why would framing ADE detection as binary classification rather than multi-class (ADE/indication/neither) potentially improve performance?

- **SMOTE (Synthetic Minority Over-sampling Technique)**: With ADEs representing only 0.64% of candidate pairs, the training data is severely imbalanced. SMOTE generates synthetic minority class samples to balance training.
  - Quick check: How does SMOTE differ from simple random oversampling of the minority class?

## Architecture Onboarding

- **Component map**: Raw clinical text -> NER Model (Bi-LSTM or Transformer+CRF) -> BIO-tagged entities -> Candidate Pair Generation (drug-disorder pairs within 4 sentences) -> Feature Extraction (3088-dim vector) -> RC Model (4-layer MLP with focal loss) -> Binary ADE prediction -> Threshold Application (F1 or F2-optimized)

- **Critical path**: NER quality -> Candidate pair coverage -> RC feature quality -> Threshold selection. Errors propagate: poor NER recall means ADE pairs are never considered by RC.

- **Design tradeoffs**: 
  - Transformer vs. Bi-LSTM: Transformers (+0.02-0.04 macro F1) at cost of computational overhead
  - F1 vs. F2 threshold: F2 improves recall (0.67â†’0.74 on WINGS) but increases false positives
  - 4-sentence window: Covers 99.42% of ADE relations but increases candidate pairs and class imbalance

- **Failure signatures**:
  - Low macro F1 with high micro F1: Model failing on minority ADE class
  - High precision, low recall: Threshold too aggressive
  - NER precision much higher for drugs than disorders: Disorder terminology more variable

- **First 3 experiments**:
  1. Baseline reproduction: Train MedRoBERTa.nl+CRF for NER and 4-layer MLP for RC on Dutch ADE corpus with 5-fold CV; verify macro F1 ~0.62-0.63.
  2. Threshold sensitivity: Compare F1 vs. F2 threshold selection on WINGS external validation; measure precision-recall tradeoff.
  3. Ablation on pair distance: Test 2-sentence vs. 4-sentence window for candidate pair generation; measure impact on recall and candidate pair count.

## Open Questions the Paper Calls Out
None

## Limitations
- Potential data leakage between MedRoBERTa.nl's pre-training corpus and the Dutch ADE corpus due to overlap in Dutch EHR clinical notes
- Limited generalizability to languages other than Dutch
- Two-stage pipeline design where NER errors propagate to relation classification

## Confidence
- **High Confidence**: Superiority of macro-averaged F1 over micro-averaged F1 for imbalanced ADE detection tasks
- **Medium Confidence**: MedRoBERTa.nl's pre-training on Dutch EHR notes explains its superior performance
- **Low Confidence**: F2-score threshold selection is optimal for clinical ADE detection

## Next Checks
1. Request documentation from the MedRoBERTa.nl authors confirming that the Dutch ADE corpus was not included in the pre-training data. If overlap exists, retrain MedRoBERTa.nl excluding any overlapping documents and re-evaluate performance.

2. Fine-tune the best-performing model (MedRoBERTa.nl) on a similar ADE detection task in another language (e.g., English MIMIC-III notes) to assess generalizability of the approach beyond Dutch clinical text.

3. Implement an end-to-end transformer model that jointly performs NER and RC, comparing its performance against the two-stage pipeline to quantify the impact of error propagation and determine if joint training provides advantages for ADE detection.