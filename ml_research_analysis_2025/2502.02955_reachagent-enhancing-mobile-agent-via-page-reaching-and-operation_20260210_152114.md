---
ver: rpa2
title: 'ReachAgent: Enhancing Mobile Agent via Page Reaching and Operation'
arxiv_id: '2502.02955'
source_url: https://arxiv.org/abs/2502.02955
tags:
- page
- action
- task
- click
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of mobile AI agents focusing on
  local optimal solutions by prioritizing task-relevant elements at each step rather
  than the overall GUI flow. The authors propose ReachAgent, a two-stage framework
  that breaks down mobile control tasks into page reaching and operation subtasks.
---

# ReachAgent: Enhancing Mobile Agent via Page Reaching and Operation

## Quick Facts
- arXiv ID: 2502.02955
- Source URL: https://arxiv.org/abs/2502.02955
- Reference count: 30
- Primary result: 7.12% and 7.69% improvement in step-level IoU and Text Accuracy; 4.72% and 4.63% improvement at task level over prior state-of-the-art.

## Executive Summary
ReachAgent addresses the problem of mobile AI agents making locally optimal but globally suboptimal decisions by prioritizing task-relevant elements at each step rather than the overall GUI flow. The authors propose a two-stage framework that decomposes mobile control tasks into page reaching and operation subtasks. The first stage uses supervised fine-tuning (SFT) with a dataset called MobileReach containing page navigation, reaching, and operation tasks. The second stage employs reinforcement learning with a 4-level reward function to construct preference data and optimize the agent. Experimental results show that ReachAgent significantly improves accuracy metrics at both step and task levels compared to the state-of-the-art agent.

## Method Summary
ReachAgent introduces a two-stage framework for mobile GUI control. In Stage 1, the agent is fine-tuned on the MobileReach dataset, which breaks down tasks into page navigation, page reaching, and page operation subtasks. The agent processes screenshots, XML documents, action histories, and instructions through a VLM backbone. Stage 2 employs reinforcement learning with a 4-level reward function (Golden > Longer > Incomplete > Invalid) to construct preference data and optimize the agent via Direct Policy Optimization (DPO). The method also introduces an action alignment mechanism that restricts the agent's output to a predefined, discrete set of UI element-aligned actions extracted from the page's XML document.

## Key Results
- Step-level IoU Accuracy improved by 7.12% and Text Accuracy by 7.69% compared to state-of-the-art.
- Task-level IoU Accuracy improved by 4.72% and Text Accuracy by 4.63% compared to state-of-the-art.
- The 4-level reward function effectively constructs preference data for optimization.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Decomposing long-horizon GUI tasks into discrete "Reach" and "Operate" subtasks mitigates local optimal action selection by providing explicit intermediate goals.
- Mechanism: A high-level task is segmented into a sequence of subtasks (e.g., Reach(Product Page) → Operate(click "Add")). The agent is trained on these subtasks individually via SFT and optimized for overall flow efficiency via RL, shifting focus from single-step relevance to subtask completion. The MobileReach dataset explicitly provides these distinct task types.
- Core assumption: Complex GUI tasks can be effectively modeled as a linear sequence of independent but ordered page-arrival and on-page action events, and learning these primitives transfers to whole-task completion.
- Evidence anchors: [abstract] "we constructed a training dataset called MobileReach, which breaks the task into page reaching and operation subtasks." [section 1, Introduction] "Therefore, we break down the task into several subtasks and focus on the agent's subtask completion abilities." [corpus] CHOP (2503.03743) similarly structures operations into task, subtask, and action levels, validating the hierarchical decomposition strategy.
- Break condition: If a global task requires dynamic, non-page-based state tracking or cannot be reduced to a linear sequence of page arrivals, this decomposition will lose coherence.

### Mechanism 2
- Claim: Constraining the agent's output to a predefined, discrete set of UI element-aligned actions significantly reduces generation complexity and improves step-level accuracy.
- Mechanism: Instead of predicting arbitrary coordinates, the agent selects from a candidate action space extracted from the page's XML document. Actions are aligned to element centroids and types (click, scroll, input), reducing the problem from continuous coordinate prediction to classification over a structured action list.
- Core assumption: The XML view hierarchy is accurate, accessible, and all essential UI interactions can be represented as discrete operations on identified elements.
- Evidence anchors: [abstract] "ReachAgent introduces an action alignment mechanism to reduce candidate action space." [section 4.1] "We suggest that the agent only clicks on the center point of an element... extract all the candidate actions in the GUI page as the candidate action space." [corpus] No direct corpus evidence on this specific XML-based action alignment mechanism.
- Break condition: If an application uses non-standard UI elements, invisible touch targets, or dynamic canvas-based interfaces not reflected in the XML hierarchy, the generated action space will be incomplete or incorrect.

### Mechanism 3
- Claim: Reinforcement learning with a granular, multi-level reward function shapes the agent's policy to prefer shorter, successful GUI flows over longer or incomplete ones, improving task-level performance.
- Mechanism: A 4-level reward function (Golden, Longer, Incomplete, Invalid) ranks action trajectories. These ranked trajectories form preference pairs (chosen vs. rejected) for Direct Policy Optimization (DPO), guiding the model to learn the relative value of action sequences beyond simple step-wise imitation.
- Core assumption: It is possible to programmatically or heuristically determine the optimality of an action or sub-trajectory (e.g., if a path leads to the correct page) to construct reliable preference data.
- Evidence anchors: [abstract] "employs reinforcement learning with a 4-level reward function to construct preference data and optimize the agent." [section 4.3] "Naturally, for these four levels, we expect their reward scores to be: R(Golden)>R(Longer)>R(Incomplete)>R(Invalid)... Therefore, we adopt DPO." [corpus] DigRL (referenced in section 2) uses an automatic evaluator for reward assignment, a related but different approach. No direct corpus evidence for this specific 4-level manual reward function.
- Break condition: If the reward signal is sparse or the environment's transition dynamics are highly stochastic, constructing a reliable dataset of preferred flows becomes difficult, and optimization may be unstable.

## Foundational Learning

- **Vision-Language Models (VLMs) and Multimodal Action Spaces**
  - Why needed here: ReachAgent is built on a VLM backbone which must process visual information (screenshots) and textual information (instructions, action history, XML-based action space) to generate actions. Understanding how multimodal inputs are fused is critical.
  - Quick check question: Can you explain how a visual screenshot and a textual list of candidate actions are combined as input to an LLM, and how the model's text output is parsed into an executable action?

- **Direct Preference Optimization (DPO)**
  - Why needed here: The second stage of ReachAgent uses DPO, not traditional RL with a learned reward model. DPO optimizes the policy directly on preference data.
  - Quick check question: How does DPO differ from Proximal Policy Optimization (PPO) in its use of reward signals, and what is the specific format of training data it requires?

- **Android UI Automata and View Hierarchies**
  - Why needed here: The Action Alignment mechanism depends entirely on parsing the XML view hierarchy to generate the candidate action space. Knowledge of how Android structures UI elements is essential.
  - Quick check question: Given an Android XML snippet for a UI element, can you extract its bounding box coordinates and determine if it is clickable, scrollable, or inputtable?

## Architecture Onboarding

- **Component map**: MobileReach Dataset (Page Navigation, Page Reaching, Page Operation splits) -> ReachAgent Model (MobileVLM Backbone) -> Action Alignment Module (XML parsing for candidate action space) -> Two-Stage Training Pipeline (SFT then DPO)

- **Critical path**: Set up Android emulator environment → Process MobileReach data → Train Stage 1 SFT model → Sample trajectories and generate preference pairs via the 4-level reward → Train Stage 2 DPO model

- **Design tradeoffs**: Action Alignment trades flexibility for accuracy, failing on canvas-based apps. Dataset Construction via random walks may miss complex app features.

- **Failure signatures**:
  - XML Parsing Failure: Agent outputs invalid actions if the view hierarchy is malformed.
  - Local Optima (Pre-Stage 2): Agent reaches a relevant page but performs wrong actions.
  - Out-of-Distribution Apps: Performance degrades on apps not covered in the training set.

- **First 3 experiments**:
  1. **Baseline Reproduction**: Train MobileVLM on only the Page Navigation split to establish baseline accuracy.
  2. **Ablation on Subtask Data**: Retrain Stage 1 incrementally by adding Action Alignment, then Page Reaching, then Page Operation data to measure each component's contribution.
  3. **DPO Impact Evaluation**: Compare Stage 1 (SFT) vs. Stage 2 (DPO) models, specifically analyzing cases where task-level accuracy improves to confirm the reward optimization effect.

## Open Questions the Paper Calls Out

- **Open Question 1**: How can the exploration strategy be improved beyond random walks to ensure comprehensive functional coverage without incurring excessive computational costs? Basis in paper: [explicit] The authors state in the Limitations section: "Since the method of exploring the GUI graph is random walk, this may not cover all the functions of the app." Why unresolved: Random walks are computationally cheap but inherently lack direction, often missing deep or conditional functionalities that require specific navigation sequences. What evidence would resolve it: A comparative study showing coverage rates of different exploration strategies (e.g., intent-based or model-based testing) relative to the random walk baseline used in Mobile3M.

- **Open Question 2**: How can the validity of sampled GUI flows be automatically verified to ensure they represent coherent tasks rather than noise? Basis in paper: [explicit] The authors note: "We select GUI flows by random sampling, which may result in many invalid GUI flows that do not have corresponding tasks." Why unresolved: While GPT-4V is used for task generation, filtering "invalid" flows remains a challenge; distinguishing between a complex valid interaction and a random sequence of inputs is difficult for automated filters. What evidence would resolve it: Quantitative analysis of the "rejection rate" of generated tasks by the GPT-4V filter or a manual audit of the dataset's task-flow alignment accuracy.

- **Open Question 3**: To what extent does ReachAgent's performance degrade on applications with inaccessible or sparse XML hierarchies? Basis in paper: [inferred] The Action Alignment mechanism (Section 4.1) relies entirely on extracting candidate actions from the XML document ($P_{xml}$). It is assumed that this document is available and accurate. Why unresolved: Many modern apps (e.g., games, Flutter apps, or canvases) obscure view hierarchies or render elements as flat images, rendering the Action Alignment mechanism inoperable. What evidence would resolve it: Evaluation results on a benchmark of "black-box" apps where the agent must rely solely on visual inputs ($P_{vis}$) without XML metadata.

## Limitations

- **Dataset & Environment Dependency**: ReachAgent's performance is tightly coupled to the quality and scope of the MobileReach dataset, which is not yet public, limiting independent verification.

- **Reward Signal Reliability**: The 4-level reward function is hand-designed and may not generalize well to unseen apps or ambiguous user instructions without a learned reward model or human evaluation.

- **Action Space Restriction Trade-off**: While the action alignment mechanism improves step-level accuracy, it also constrains the agent's flexibility and may fail on complex interactions like drag-and-drop or multi-touch gestures.

## Confidence

- **High Confidence**: The two-stage training framework (SFT + DPO) is technically sound and aligns with established practices in language model fine-tuning. The decomposition of GUI tasks into subtasks is a well-motivated design choice supported by related work (e.g., CHOP).

- **Medium Confidence**: The experimental results show consistent improvements in IoU and Text Accuracy. However, without ablation studies on each component or cross-app generalization tests, it is difficult to attribute performance gains to specific mechanisms with certainty.

- **Low Confidence**: Claims about the agent's robustness to novel apps or instructions are not supported by the current evaluation setup. The use of a single benchmark dataset and lack of stress tests on edge cases reduce confidence in real-world applicability.

## Next Checks

1. **Cross-App Generalization Test**: Evaluate ReachAgent on a held-out set of apps not present in MobileReach to measure drop in performance and identify failure patterns.

2. **Ablation Study**: Retrain ReachAgent incrementally (SFT only, SFT + action alignment, full SFT + DPO) and measure the marginal contribution of each component to step-level and task-level accuracy.

3. **Human Evaluation of Preference Data**: Sample a subset of chosen/rejected action pairs from the DPO dataset and have human annotators verify whether the reward assignments reflect intuitive task success and efficiency.