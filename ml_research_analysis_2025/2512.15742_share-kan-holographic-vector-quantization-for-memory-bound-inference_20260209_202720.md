---
ver: rpa2
title: 'SHARe-KAN: Holographic Vector Quantization for Memory-Bound Inference'
arxiv_id: '2512.15742'
source_url: https://arxiv.org/abs/2512.15742
tags:
- quantization
- memory
- codebook
- int8
- compression
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the memory bottleneck in Kolmogorov-Arnold
  Networks (KANs) for deployment on bandwidth-constrained devices. KANs use learned
  B-spline basis functions, which create large parameter counts and high bandwidth
  demands.
---

# SHARe-KAN: Holographic Vector Quantization for Memory-Bound Inference

## Quick Facts
- **arXiv ID**: 2512.15742
- **Source URL**: https://arxiv.org/abs/2512.15742
- **Reference count**: 32
- **Primary result**: 88× memory reduction (1.13GB → 12.91MB) with <1% mAP loss on PASCAL VOC object detection

## Executive Summary
SHARe-KAN addresses the memory bottleneck in Kolmogorov-Arnold Networks (KANs) for deployment on bandwidth-constrained devices. KANs use learned B-spline basis functions that create large parameter counts and high bandwidth demands. Traditional pruning fails catastrophically due to KANs' holographic topology where information is distributed across spline interference rather than localized to specific edges. The paper introduces Gain-Shape-Bias Vector Quantization that decomposes each spline into shared codebook, per-edge index, gain, and bias, achieving dramatic compression while maintaining accuracy.

## Method Summary
The approach exploits KAN redundancy through holographic vector quantization. Each spline grid is normalized, clustered via mini-batch k-means (K=65,536) to form a shared codebook, then represented by an index, gain, and bias per edge. This decomposes runtime memory from 1.13 GB to 12.91 MB with less than 1% mAP loss on PASCAL VOC. The method is coupled with LUTHAM, a hardware-aware compiler with static memory planning, achieving >90% L2 cache residency on NVIDIA Ampere GPUs. This decouples the workload from DRAM bandwidth constraints while preserving the dense topological structure essential for KAN performance.

## Key Results
- 88× memory compression from 1.13 GB to 12.91 MB on PASCAL VOC object detection
- Less than 1% mAP drop (84.74% vs 85.23% baseline)
- >90% L2 cache residency on NVIDIA Ampere architecture
- Traditional pruning fails catastrophically (10% sparsity causes 40-point mAP drop)

## Why This Works (Mechanism)
KANs exhibit holographic topology where information is distributed across the interference pattern of B-spline basis functions rather than localized to individual edges. This distributed representation makes traditional sparsification approaches fail catastrophically. SHARe-KAN exploits this redundancy by using vector quantization that preserves the dense topology while compressing the basis functions themselves. The gain-shape-bias decomposition allows efficient representation while maintaining the interference patterns critical for KAN performance. LUTHAM's static memory planning ensures quantized weights stay resident in fast cache memory rather than competing for DRAM bandwidth.

## Foundational Learning
**Kolmogorov-Arnold Networks (KANs)**: Learnable B-spline basis functions that replace fixed activation functions in neural networks. *Why needed*: Understanding the distributed representation that makes KANs both powerful and challenging to compress. *Quick check*: Verify KAN layers can approximate complex functions with fewer parameters than MLPs when properly trained.

**Holographic Topology**: Information distributed across interference patterns of basis functions rather than localized to specific connections. *Why needed*: Explains why traditional pruning fails catastrophically in KANs. *Quick check*: Observe mAP collapse when applying magnitude pruning to KANs (should drop from ~85% to ~45% at 10% sparsity).

**Vector Quantization with Gain-Shape-Bias**: Decomposes continuous parameters into shared codebook, index, gain, and bias terms. *Why needed*: Enables compression while preserving distributed information patterns. *Quick check*: Verify reconstruction quality (R² > 0.9) when clustering spline grids with K=65,536.

**Hardware-Aware Compilation**: Static memory planning that ensures weights remain in cache rather than DRAM. *Why needed*: Critical for achieving claimed performance on bandwidth-constrained devices. *Quick check*: Profile L2 cache hit rates on target GPU architecture (>90% target).

## Architecture Onboarding

**Component Map**: ResNet-50 Backbone → KAN Detection Head → Vector Quantization → LUTHAM Compiler → Inference

**Critical Path**: Input Image → Backbone Feature Extraction → KAN Detection Head (B-spline layers) → Box/Classification Outputs

**Design Tradeoffs**: Dense topology preservation vs. compression ratio; codebook size (K) vs. accuracy; quantization precision (Int8 vs FP32) vs. OOD generalization.

**Failure Signatures**: Catastrophic accuracy collapse with traditional pruning; poor OOD generalization with aggressive Int8 quantization; cache thrashing if memory planning is suboptimal.

**First Experiments**:
1. Implement baseline KAN detection head with cubic B-splines (G=10) on PASCAL VOC, verify ~85% mAP with frozen ResNet-50.
2. Apply post-training VQ (K=65,536) and evaluate mAP drop; compare against traditional pruning failure case.
3. Profile L2 cache residency on A100/ampere using Nsight Compute to verify >90% hit rate.

## Open Questions the Paper Calls Out

**Open Question 1**: Does enforcing Lipschitz constraints during pre-training improve Int8 quantization robustness for OOD data? The authors suggest LipKAN regularization could produce basis functions more amenable to low-bit quantization, as standard Int8 causes 15.1-point COCO transfer accuracy drop due to outlier sensitivity.

**Open Question 2**: Can a universal codebook pre-trained on large-scale datasets (e.g., ImageNet) eliminate task-specific codebook learning? Current results rely on PASCAL VOC-specific codebooks, leaving transferability of a fixed visual dictionary unknown.

**Open Question 3**: Does SHARe-KAN's cache residency translate to significant energy savings on embedded platforms? The paper validates memory mechanics on A100 but leaves power profiling (Joules/inference) on embedded SoCs like Jetson Orin to future work.

## Limitations
- Relies on post-training quantization without fine-tuning, limiting generalizability
- LUTHAM compiler implementation is proprietary and not publicly available
- Evaluation limited to single dataset (PASCAL VOC) and backbone (ResNet-50)

## Confidence
- **High**: Core VQ compression methodology and PASCAL VOC effectiveness are well-supported
- **Medium**: L2 cache residency improvements depend on proprietary implementation details
- **Low**: Generalizability to other architectures, backbones, or detection frameworks remains unproven

## Next Checks
1. Apply SHARe-KAN to detectron2-style KAN detection heads on COCO dataset to verify <1% mAP degradation with 88× compression holds for larger-scale object detection.
2. Systematically vary K (codebook size) and G (grid points) to quantify the Pareto frontier between compression ratio and accuracy.
3. Compare post-training VQ performance against VQ-aware fine-tuning on PASCAL VOC to quantify potential accuracy gains from training-time quantization awareness.