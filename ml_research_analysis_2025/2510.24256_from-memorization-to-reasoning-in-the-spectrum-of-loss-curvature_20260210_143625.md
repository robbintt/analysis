---
ver: rpa2
title: From Memorization to Reasoning in the Spectrum of Loss Curvature
arxiv_id: '2510.24256'
source_url: https://arxiv.org/abs/2510.24256
tags:
- memorization
- k-fac
- memorized
- clean
- curvature
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work shows that loss curvature can be used to disentangle
  memorized and general-purpose model structures in Transformers. Using the K-FAC
  approximation, the authors decompose weight matrices into curvature-ordered components
  and show that lower-curvature directions disproportionately support verbatim memorization,
  while higher-curvature directions preserve generalization.
---

# From Memorization to Reasoning in the Spectrum of Loss Curvature

## Quick Facts
- arXiv ID: 2510.24256
- Source URL: https://arxiv.org/abs/2510.24256
- Reference count: 40
- Shows loss curvature decomposition can disentangle memorized from general-purpose model structures

## Executive Summary
This work demonstrates that the curvature spectrum of the Fisher Information Matrix, approximated via K-FAC, provides a natural ordering of weight components that separates memorization from generalization in neural networks. By decomposing weight matrices into eigenvectors ordered by curvature, the authors show that low-curvature directions disproportionately encode verbatim memorization while high-curvature directions preserve general reasoning capabilities. They introduce a weight-editing method that suppresses untargeted memorization (reducing strict accuracy on held-out quotes from 99.9% to 16.1%) while maintaining lower perplexity and outperforming a supervised unlearning baseline.

## Method Summary
The method uses K-FAC to approximate the Fisher Information Matrix and decompose weight matrices into eigenvectors ordered by curvature. Weight components are selected based on cumulative curvature mass (typically top 80-90%), and the weight matrix is reconstructed preserving only these high-curvature components. This selectively removes low-curvature directions that encode idiosyncratic memorization while retaining high-curvature directions that support general-purpose reasoning. The approach generalizes to vision transformers, where it also shows regularization benefits by improving validation accuracy.

## Key Results
- K-FAC reduces Historical Quotes strict accuracy from 99.9% to 16.1% while maintaining perplexity ~22.8
- ViT validation accuracy improves from 67% to 71.7% after K-FAC edit
- Arithmetic and closed-book fact retrieval are brittle to low-curvature removal, while logical reasoning and open-book QA are robust
- Outperforms BSN supervised unlearning baseline on perplexity and memorization reduction

## Why This Works (Mechanism)

### Mechanism 1
The K-FAC eigenbasis disentangles memorization from generalization by ordering weight components along a curvature spectrum. The Fisher Information Matrix approximated via K-FAC decomposes weight matrices into eigenvectors where eigenvalue magnitude correlates with how consistently a weight direction is used across the training distribution. High-curvature directions encode shared mechanisms; low-curvature directions encode idiosyncratic patterns. The core assumption is that population-averaged curvature meaningfully separates memorization (inconsistent across examples) from generalization (consistent across examples). This works because individual memorized points are sharp, but averaged across data they appear flat.

### Mechanism 2
Removing low-curvature components selectively suppresses verbatim recitation while preserving reasoning. By preserving only top k% of curvature mass, the edit removes weight directions that flatly encode specific training examples but coherently retain directions implementing shared computations. The curvature mass threshold maps cleanly to behavioral separation rather than being an artifact of numerical approximation. This succeeds because low-curvature directions contain critical reasoning machinery.

### Mechanism 3
Arithmetic and closed-book fact retrieval are brittle because they depend disproportionately on low-curvature specialized directions. These tasks require precise, narrowly-used circuits rather than broad reasoning patterns. The paper shows activation ratios with low-curvature components predict task degradation. Brittle behaviors correlate with how much a task's activations concentrate in low-curvature directions, not with whether individual examples are memorized. This occurs because arithmetic relies on specialized heuristics rather than general-purpose computation.

## Foundational Learning

- **Fisher Information Matrix and Hessian curvature**
  - Why needed here: K-FAC approximates curvature without computing the full Hessian; understanding this approximation is essential for interpreting the decomposition
  - Quick check question: Why does the FIM approximate the Hessian for cross-entropy loss?

- **Kronecker product structure in neural network layers**
  - Why needed here: K-FAC exploits the layered structure (activations ⊗ gradients) to make curvature estimation tractable
  - Quick check question: How does factoring G⊗A reduce computational complexity vs. full Hessian?

- **Population vs. instance-level curvature**
  - Why needed here: The key theoretical inversion—individual memorized points are sharp, but averaged across data they appear flat—drives the method
  - Quick check question: Why do idiosyncratic sharp directions cancel when averaged across examples?

## Architecture Onboarding

- **Component map:**
  K-FAC statistics collection -> Eigendecomposition -> Curvature mass selection -> Weight projection

- **Critical path:** The activation ratio analysis (Figure 2) validates that the curvature basis separates memorized/clean data before editing—skip this and you're operating blind.

- **Design tradeoffs:**
  - Higher ρ (e.g., 90%) preserves more capabilities but leaves more memorization
  - Layer selection matters: early/late MLP layers show strongest disentanglement (layers 23-25 in 7B LM; layers 0+11 in ViT)
  - Energy threshold vs. fixed component count: the paper uses curvature mass (energy) for adaptive selection

- **Failure signatures:**
  - Arithmetic degradation without corresponding memorization reduction suggests threshold too aggressive
  - Incoherent text generation (dropped function words) may indicate SVD is preferable to K-FAC for some applications
  - Perplexity spikes indicate over-pruning

- **First 3 experiments:**
  1. Replicate activation ratio analysis on your target model (Figure 2) to confirm memorization/clean separation exists in curvature basis
  2. Sweep ρ ∈ [0.5, 0.9] on a held-out memorization set, measuring strict accuracy vs. perplexity tradeoff
  3. Evaluate on arithmetic-heavy and logic-heavy benchmarks to characterize the brittleness spectrum for your use case

## Open Questions the Paper Calls Out
None

## Limitations
- The theoretical justification for why population-averaged curvature separates memorization from generalization remains incomplete
- Results are primarily validated on a single 7B language model and specific vision architectures
- The brittleness characterization for arithmetic and fact retrieval is empirically observed but mechanistically unclear

## Confidence

**High confidence:** The core empirical observation that K-FAC decomposition separates memorized and general-purpose weight directions is well-supported by multiple ablation studies and consistent across different evaluation tasks.

**Medium confidence:** The theoretical mechanism explaining why low-curvature directions encode memorization is plausible but not rigorously proven. The brittleness characterization for specific task types is empirically observed but mechanistically unclear.

**Low confidence:** Claims about the method's effectiveness across diverse architectures and scales beyond the tested configurations are not well-supported by the current evidence.

## Next Checks

1. **Ablation on arithmetic mechanisms:** Systematically test whether arithmetic degradation is due to removal of specialized calculation circuits versus general-purpose numerical processing. This could involve fine-tuning arithmetic capabilities on clean data after K-FAC editing, or testing whether arithmetic remains brittle when trained on perfectly de-duplicated datasets.

2. **Cross-architecture curvature analysis:** Apply the activation ratio analysis (Figure 2) to a diverse set of architectures (RNNs, MLPs, different attention variants) to determine whether the memorization/curvature correlation is universal or specific to Transformers. This would clarify whether the mechanism is fundamental to neural network training dynamics or an artifact of attention-based architectures.

3. **Dynamic curvature tracking during training:** Monitor how curvature mass distribution evolves during training across different data regimes (clean vs. noisy vs. memorized). This would reveal whether low-curvature memorization directions emerge as a training artifact or are inherent to how networks encode rare/verbatim patterns, providing stronger theoretical grounding for the population-averaging hypothesis.