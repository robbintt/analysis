---
ver: rpa2
title: While recognizing actions, LMMs struggle to detect core interaction events
arxiv_id: '2511.20162'
source_url: https://arxiv.org/abs/2511.20162
tags:
- frame
- something
- hand
- event
- video
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluates the ability of large multi-modal models (LMMs)
  to detect core interaction events in videos, such as when an object becomes attached
  to or detached from an agent (e.g., a hand). The researchers introduce a new dataset
  with over 20,000 annotated "contact" and "release" events from the Something-Something-V2
  dataset, manually labeled by 250 annotators with spatiotemporal locations.
---

# While recognizing actions, LMMs struggle to detect core interaction events

## Quick Facts
- **arXiv ID**: 2511.20162
- **Source URL**: https://arxiv.org/abs/2511.20162
- **Reference count**: 40
- **Primary result**: LMMs can describe actions and name objects but fail to pinpoint precise contact/release event frames and locations (accuracy <15% even with error tolerance)

## Executive Summary
This study evaluates large multi-modal models' ability to detect core interaction events in videos, specifically when objects become attached to or detached from agents. Using a new dataset with over 20,000 annotated contact and release events from Something-Something-V2, experiments with Qwen-2.5VL and GPT-4o show consistent failure to identify precise event frames and locations despite reliable action classification and object naming. The research reveals a fundamental gap in perceptual grounding, where models can generate coherent action descriptions without true understanding of underlying physical dynamics.

## Method Summary
The researchers introduce a new Contact-Release Interaction Dataset with 24,222 events across 10,130 SSv2 videos, manually labeled by 250 annotators with spatiotemporal locations. They evaluate Qwen-2.5VL-72B and GPT-4o under zero-shot, one-shot, and two-shot in-context learning regimes with optional reasoning and grounding conditions. The experimental setup uses 99 short video clips (10 frames each) from 33 SSv2 videos, with 3 isolated events per video. Models are prompted to predict event frames and bounding boxes, with accuracy measured by frame tolerance (0-5 frames) and spatial IoU against ground truth annotations.

## Key Results
- LMMs achieve <15% exact frame accuracy for contact/release events even with 2-shot examples and 5-frame error tolerance
- Models reliably name objects and classify actions but fail to identify precise event boundaries
- Bounding box predictions exceed frame dimensions with mean IoU <12%, indicating spatial calibration failure
- Chain-of-thought reasoning improves Qwen performance (+4%) but has negligible effect on GPT-4o
- Grounding condition unexpectedly decreases performance in most experiments

## Why This Works (Mechanism)

### Mechanism 1: Semantic-Visual Decoupling in LMMs
- Claim: LMMs can produce coherent action descriptions and name objects without grounding those descriptions to specific spatiotemporal locations in the visual input.
- Mechanism: Language model priors dominate output generation; visual representations from pretrained vision transformers are loosely integrated, allowing models to "tell a convincing story" about detected objects without true perceptual anchoring.
- Core assumption: The visual encoder and language decoder are trained separately, creating a representation gap that prevents fine-grained temporal grounding.
- Evidence anchors:
  - [abstract]: "although the models can reliably name the target objects, identify the action and provide coherent reasoning, they consistently fail to identify the frame where the interaction begins or ends"
  - [section 8]: "models struggle with the perceptual grounding of the core events underlying actions... despite their general ability to describe the action and participating objects"
  - [corpus]: Related work (Qi et al.) shows LMMs rely more on textual priors than visual evidence in image tasks
- Break condition: If visual and language representations were tightly coupled through joint training on fine-grained temporal annotations, this decoupling would diminish.

### Mechanism 2: In-Context Learning Provides Minor Gains for Temporal Localization
- Claim: Few-shot examples improve temporal event detection accuracy modestly (ZS: 11% → TS: 14-15% exact frame accuracy for Qwen), but gains remain insufficient.
- Mechanism: ICL provides task format calibration and output structure guidance, but does not transfer the underlying perceptual skill of detecting contact/release boundaries.
- Core assumption: ICL primarily calibrates output formatting and reasoning patterns rather than teaching novel perceptual capabilities.
- Evidence anchors:
  - [section 5, Table 2]: "TS performs mostly the best and ZS the lowest, as was demonstrated already in earlier studies"
  - [section 5]: Exact frame accuracy under TS with reasoning: 14% (Qwen), 13% (GPT-4o)
  - [corpus]: Weak corpus support—neighbor papers focus on long-video retrieval, not frame-level localization
- Break condition: If ICL examples contained explicit temporal reasoning chains tied to visual cues, gains might be larger—but current prompts do not enforce this.

### Mechanism 3: Chain-of-Thought Reasoning Improves Output Coherence Without Fixing Grounding
- Claim: Requiring step-by-step reasoning increases Qwen's performance (+4% exact frame accuracy) but has negligible effect on GPT-4o, and reasoning text often bears loose relationship to actual frames.
- Mechanism: CoT prompting forces models to generate plausible narrative structures, which may incidentally improve calibration for some models but does not enforce true visual grounding.
- Core assumption: Generated reasoning reflects post-hoc rationalization rather than grounded perception.
- Evidence anchors:
  - [section 5, Table 2]: "instructing the models to provide reasoning for their answers usually increases their performance. For Qwen... significant, while for GPT-4o it is not"
  - [section 6]: "the reasoning seems logical and realistic, but the relation to the actual video frames is often very loose"
  - [corpus]: Neighbor paper MESH addresses hallucinations in video models, consistent with loose grounding
- Break condition: If reasoning were constrained to reference specific visual features per frame (e.g., "hand centroid velocity changes in frame 3"), grounding would improve.

## Foundational Learning

- **Perceptual Grounding vs. Semantic Description**
  - Why needed here: The paper demonstrates that describing an action differs fundamentally from pinpointing when/where it occurs. Engineers must distinguish these capabilities when designing evaluation protocols.
  - Quick check question: Can your model correctly answer "What happened?" but fail at "At what timestamp did contact occur?" If yes, you have a grounding gap.

- **Event Boundary Detection**
  - Why needed here: Contact and release events require detecting frame-level state transitions (attached ↔ detached), not just recognizing action categories.
  - Quick check question: Does your model's predicted event frame fall within ±2 frames of human annotation? If not, it lacks temporal precision.

- **Spatiotemporal Localization**
  - Why needed here: The paper shows models output oversized bounding boxes (often exceeding frame boundaries) with mean IoU <12%, indicating spatial calibration failure.
  - Quick check question: Do predicted bounding boxes stay within frame dimensions and tightly enclose the contact region? If boxes are oversized or out-of-bounds, spatial calibration is broken.

## Architecture Onboarding

- **Component map**: Vision encoder (ViT) → Frame feature extraction → Temporal aggregation → Language model (LLM) → Text output (frame prediction + bounding box)
- **Critical path**:
  1. Frame extraction from video (10-frame clips used in experiments)
  2. Prompt construction (context + examples + task instruction)
  3. Model inference → text output with frame number and optionally bounding box
  4. Parse prediction → compare to ground truth frame and coordinates
- **Design tradeoffs**:
  - ZS vs. OS vs. TS: More shots improve accuracy marginally but increase inference cost and prompt length
  - Reasoning condition: Adds output tokens and latency; benefits model-dependent
  - Grounding condition: Actually degraded performance in most cases—forcing description before prediction may distract from task
- **Failure signatures**:
  - Exact frame accuracy ≤15% even with 2-shot ICL
  - Bounding boxes exceed frame dimensions (models ignore spatial scale)
  - CoT reasoning text appears logical but references wrong frames
  - Performance drops when grounding condition applied
- **First 3 experiments**:
  1. **Baseline temporal localization**: Test ZS frame prediction on 10-frame video clips with single contact/release events. Expect ~10% exact accuracy; if >20%, verify evaluation methodology.
  2. **Spatial calibration probe**: Request bounding boxes for event locations. Check if boxes exceed frame dimensions; compute IoU against ground truth (expect <15%).
  3. **Reasoning vs. grounding ablation**: Compare ZS + reasoning vs. ZS + grounding vs. ZS + both. If grounding helps, your model differs from Qwen/GPT-4o—investigate architectural differences.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What architectural or training modifications would enable LMMs to detect precise spatiotemporal event boundaries (contact/release frames and locations) in video?
- Basis in paper: [explicit] Authors conclude "these annotations may be used in future efforts to develop new foundation models with better understanding of visual dynamic interactions" and hypothesize "the main limitation is rooted in a loose integration between the visual representation...and the language representation, which are mostly trained separately."
- Why unresolved: Current LMMs fail to pinpoint frame (<15% accuracy with tolerance) and spatial location (mean IoU 6.2%), suggesting fundamental architectural limitations in integrating fine-grained temporal and spatial visual signals with language outputs.
- What evidence would resolve it: Improved temporal localization accuracy and spatial IoU scores after architectural interventions (e.g., joint vision-language temporal pretraining) or fine-tuning on contact-release annotations.

### Open Question 2
- Question: Why does explicit grounding instruction (describing video contents and prompt details) fail to improve or even reduce model performance on event detection?
- Basis in paper: [inferred] Table 3 shows grounding condition decreases accuracy in most experiments (e.g., ZS Qwen: 10%→8% exact), which the authors note is "somewhat in contrast with previous studies" and suggest "this level of visual grounding is not enough to enhance the models' understanding of core interaction events."
- Why unresolved: The mechanism by which grounding helps in prior image-based work but fails here for temporal video understanding is unclear—it may indicate grounding improves object recognition but not temporal dynamics understanding.
- What evidence would resolve it: Systematic ablations testing grounding on different temporal granularities, or analysis of attention patterns with/without grounding to see where the model's focus shifts.

### Open Question 3
- Question: Can the contact-release event annotations be leveraged to train models that generalize to other types of temporal boundary detection beyond hand-object interactions?
- Basis in paper: [explicit] The authors introduce 20K+ annotations specifically for contact/release events in hand-object interactions from SSv2, but acknowledge the dataset could enable "future efforts to develop new foundation models with better understanding of visual dynamic interactions."
- Why unresolved: The dataset focuses narrowly on contact/release events with hands; generalization to other event types (e.g., tool use, object-object collisions) or agents remains untested.
- What evidence would resolve it: Training on the contact-release dataset and evaluating zero-shot transfer to different event types, agents, or datasets with similar temporal boundary annotation needs.

## Limitations

- Dataset accessibility barrier: The exact repository URL for the Contact-Release Interaction Dataset is not provided, requiring extraction from SSv2 using specific metadata
- Model version uncertainty: Exact model hashes and API versions for Qwen-2.5VL-72B and GPT-4o are unspecified, potentially affecting reproducibility
- Evaluation protocol ambiguity: Frame sampling procedure and random seed for example selection in OS/TS regimes are not documented

## Confidence

- **High confidence**: Core finding that LMMs can describe actions but fail to identify precise event frames/locations (frame accuracy <15%, IoU <12%)
- **Medium confidence**: Explanation of semantic-visual decoupling as root cause, though relies on interpretation rather than direct architectural analysis
- **Low confidence**: Claim that CoT reasoning reflects post-hoc rationalization rather than grounded perception, based on qualitative observation

## Next Checks

1. **Dataset extraction verification**: Reproduce the 99 experimental clips by extracting 10-frame sequences from the specified 33 SSv2 videos using the event frames and crop start frames from S-Table 8. Verify that your extracted clips match the paper's experimental setup by checking that contact/release events are correctly isolated and that frame numbers align with ground truth annotations.

2. **Bounding box constraint validation**: Implement explicit spatial constraints in your evaluation pipeline to verify that predicted bounding boxes stay within frame boundaries and have reasonable scale. Compare your model's output against the paper's observation that models frequently predict oversized boxes exceeding frame dimensions, and compute IoU with the 120×120 ground truth boxes to confirm the <12% performance reported.

3. **Prompt wording fidelity check**: Reproduce the experimental conditions by using the exact prompt listings (1-4 for frame prediction, 6 for spatial localization) with proper formatting. Pay special attention to the Grounding condition prompt (Listing 3), which may be critical to understanding why it degraded performance in the paper's experiments. Verify that any performance differences from the paper's results are not due to prompt wording variations.