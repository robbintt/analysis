---
ver: rpa2
title: 'Advancing Retrieval-Augmented Generation for Persian: Development of Language
  Models, Comprehensive Benchmarks, and Best Practices for Optimization'
arxiv_id: '2501.04858'
source_url: https://arxiv.org/abs/2501.04858
tags:
- retrieval
- persian
- language
- generation
- datasets
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses challenges in developing Retrieval-Augmented
  Generation (RAG) systems for Persian, a low-resource language with complex morphology
  and syntax. It introduces Persian-specific models MatinaRoberta (masked language
  model) and MatinaSRoberta (fine-tuned Sentence-BERT), trained on a 73.11 billion
  token corpus.
---

# Advancing Retrieval-Augmented Generation for Persian: Development of Language Models, Comprehensive Benchmarks, and Best Practices for Optimization

## Quick Facts
- arXiv ID: 2501.04858
- Source URL: https://arxiv.org/abs/2501.04858
- Reference count: 23
- Primary result: Persian-specific MatinaSRoberta embeddings significantly outperform multilingual baselines on retrieval tasks across general knowledge, scientific, and formal organizational domains

## Executive Summary
This paper addresses challenges in developing Retrieval-Augmented Generation (RAG) systems for Persian, a low-resource language with complex morphology and syntax. The authors introduce MatinaRoberta (masked language model) and MatinaSRoberta (fine-tuned Sentence-BERT), trained on a 73.11 billion token corpus. Through comprehensive benchmarking across three Persian datasets, they establish optimal configurations for retrieval and generation, demonstrating that Persian-specific embeddings, temperature tuning (0.25), chunk size optimization (512 tokens), and document summary indexing significantly improve RAG performance.

## Method Summary
The authors developed MatinaRoberta by continually pretraining XLM-RoBERTa Large on a 73.11B token Persian corpus using MLM objective, then fine-tuned it to MatinaSRoberta for semantic similarity tasks using contrastive and triplet losses on Persian QA matching, paraphrase, and entailment datasets. They established a comprehensive benchmark using three Persian datasets: PQuad (general knowledge), scientific-specialized texts, and formal organizational reports. The RAG pipeline was evaluated with various LLMs (Llama-3.1, Qwen2, Gemma) and optimized through temperature tuning, chunk size adjustments, and document summary indexing. Training utilized 8×A800 GPUs with DeepSpeed Stage 0 for the base model and standard Sentence-BERT fine-tuning procedures.

## Key Results
- MatinaSRoberta achieved 47.70 average score on PQuad vs 30.85 for LaBSE, demonstrating superior retrieval accuracy for Persian
- Temperature 0.25 optimized generation accuracy across all domains, with higher temperatures degrading performance in formal/technical contexts
- Document summary indexing improved PQuad accuracy from 71.87 to 77.47 and organizational report accuracy from 84.07 to 87.31
- Larger models (Llama-3.1-70B) consistently achieved highest generation accuracy, while smaller models struggled with domain-specific and formal contexts

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Persian-specific sentence embeddings improve retrieval accuracy over multilingual baselines
- Mechanism: MatinaSRoberta fine-tuned on Persian semantic similarity pairs using contrastive and triplet losses, producing 1024-dim embeddings that better capture Persian's agglutinative morphology and flexible word order
- Core assumption: Embeddings trained on in-language semantic pairs generalize to retrieval tasks better than multilingual models trained on mixed-language data
- Evidence anchors:
  - [abstract] "MatinaSRoberta outperformed previous embeddings, achieving superior contextual relevance and retrieval accuracy across datasets"
  - [section 6.1] MatinaSRoberta achieved 47.70 average score on PQuad vs 30.85 for LaBSE; attributed to "ability to better handle Persian's linguistic features, including flexible sentence structures, morphological variations, and agglutination"
  - [corpus] Related work (Hakim, FARSIQA) confirms Persian-specific embeddings are underexplored; no direct comparison available
- Break condition: If target domain uses highly specialized jargon not represented in fine-tuning datasets (scientific, legal), performance gains may diminish

### Mechanism 2
- Claim: Lower temperature (0.25) optimizes generation accuracy for Persian RAG by reducing harmful randomness while maintaining flexibility for syntactic variation
- Mechanism: Temperature controls token sampling randomness; lower values produce more deterministic outputs, reducing hallucination risk in formal/technical contexts where Persian's polysemy and flexible word order already introduce ambiguity
- Core assumption: The trade-off between precision and diversity is domain-dependent; formal/technical texts benefit more from determinism
- Evidence anchors:
  - [abstract] "Temperature tuning (optimal at 0.25)"
  - [section 6.2.1] Table 7 shows temperature 0.25 achieved best scores on scientific (82.25) and organizational (84.87) datasets; higher temperatures degraded performance
  - [corpus] RAG evaluation surveys (arXiv:2504.14891) note temperature sensitivity varies by task type but lack Persian-specific data
- Break condition: For creative or conversational applications where diversity is valued over factual precision, higher temperatures may be appropriate

### Mechanism 3
- Claim: Document summary indexing improves retrieval efficiency for long, structured documents
- Mechanism: Generate LLM-produced summaries during indexing; at query time, first match against summaries to identify relevant documents before retrieving fine-grained chunks. Query classification distinguishes general vs. specific queries to route appropriately
- Core assumption: Summaries preserve sufficient semantic signal for relevance matching while reducing retrieval space
- Evidence anchors:
  - [abstract] "document summary indexing improved RAG performance"
  - [section 6.2.3] Table 9 shows summary indexing improved PQuad (77.47 vs 71.87) and organizational report (87.31 vs 84.07) accuracy
  - [corpus] GEC-RAG and Disco-RAG papers explore related hierarchical retrieval but for different domains; no direct Persian comparison
- Break condition: For documents where key information is distributed across sections rather than locally concentrated, summary-first retrieval may miss relevant chunks

## Foundational Learning

- Concept: **Sentence Embeddings and Semantic Similarity**
  - Why needed here: MatinaSRoberta produces dense vectors; understanding how cosine similarity ranks retrieved chunks is essential for debugging retrieval quality
  - Quick check question: Given two Persian sentences with shared morphology but different word order, would a multilingual embedding likely rank them as more or less similar than a Persian-specific embedding?

- Concept: **RAG Pipeline Components (Retriever → Generator)**
  - Why needed here: The paper evaluates both embedding models (retrieval) and LLMs (generation); errors can originate from either stage
  - Quick check question: If context precision is high but answer relevancy is low, which pipeline component is likely the bottleneck?

- Concept: **Masked Language Modeling vs. Sentence Fine-Tuning**
  - Why needed here: MatinaRoberta (MLM pretraining) provides the foundation; MatinaSRoberta (sentence-level fine-tuning) adapts it for retrieval. Understanding this transfer explains why both stages are necessary
  - Quick check question: Why wouldn't MLM pretraining alone suffice for semantic similarity tasks?

## Architecture Onboarding

- Component map:
  Corpus -> MatinaRoberta (MLM pretraining) -> MatinaSRoberta (sentence fine-tuning) -> Chunking (512 tokens) -> Retrieval (MatinaSRoberta embeddings) -> Optional Summary Index -> Generator LLMs (Llama-3.1, Qwen2, Gemma)

- Critical path:
  1. Preprocess Persian corpus (deduplication, noise removal)
  2. Train/obtain MatinaRoberta checkpoint
  3. Fine-tune on sentence similarity datasets (ParsiNLU, PQuad QA pairs, triplets)
  4. Chunk target documents at 512 tokens with overlap
  5. Generate embeddings and build vector index
  6. Configure retrieval (k=5) and generation (temperature=0.25)
  7. (Optional) Add summary index for long documents

- Design tradeoffs:
  - Chunk size: 512 tokens = higher precision, more compute; 2048 tokens = lower precision, faster
  - Model size: 70B models = best accuracy but higher inference cost; 7-8B models acceptable for general knowledge but struggle with formal/technical content
  - Summary index: Adds indexing overhead; benefits long/structured documents more than short texts
  - Temperature: Lower = more deterministic; higher = more diverse but risks factual inconsistency

- Failure signatures:
  - Low context precision → embedding model failing to rank relevant chunks highly; consider domain-specific fine-tuning
  - High context recall but low answer relevancy → generator LLM not effectively using retrieved context; may need larger model or prompt engineering
  - Strong general knowledge performance but weak scientific/formal → embedding or generator lacks domain exposure; few-shot examples or domain fine-tuning may help
  - Inconsistent results across temperature runs → temperature too high for the domain; reduce to 0.25 or lower

- First 3 experiments:
  1. **Baseline retrieval comparison**: Compare MatinaSRoberta vs. LaBSE vs. multilingual embeddings on a held-out Persian QA set; measure first-result accuracy and overall score
  2. **Temperature sweep**: On your target domain, test temperature values [0, 0.25, 0.5, 0.75] with a fixed generator (e.g., LLaMA-3.1-8B); identify optimal setting for precision vs. diversity
  3. **Chunk size ablation**: Test 512, 1024, 2048 token chunks on your longest documents; measure retrieval accuracy and latency to find the precision-efficiency sweet spot

## Open Questions the Paper Calls Out
None

## Limitations
- The fine-tuning procedures for MatinaSRoberta lack precise hyperparameter specifications, particularly learning rates and dataset-specific loss assignments
- Evaluation relies on domain-specific datasets (scientific textbooks, organizational reports) that may not represent full diversity of Persian language use
- Computational requirements (8×A800 GPUs) create significant barriers for independent verification

## Confidence

- **High Confidence**: The core finding that MatinaSRoberta outperforms multilingual baselines on Persian retrieval tasks is well-supported by direct comparisons showing 47.70 vs 30.85 average scores on PQuad
- **Medium Confidence**: The claim that document summary indexing improves RAG performance is supported by numerical improvements (77.47 vs 71.87 on PQuad) but lacks ablation studies showing relative contribution
- **Low Confidence**: The assertion that MatinaRoberta's 73.11B token corpus provides superior pretraining compared to alternatives lacks comparative analysis with other Persian corpus sizes

## Next Checks

1. **Hyperparameter Sensitivity Analysis**: Systematically vary temperature (0.1-1.0), chunk sizes (256-2048), and model scales (7B-70B) across all three domains to establish confidence intervals and identify breaking points where optimizations fail

2. **Cross-Domain Generalization Test**: Evaluate the complete RAG system on an external Persian dataset (e.g., news articles, social media) not seen during development to assess whether optimizations for scientific/organizational texts transfer to other domains

3. **Embedding Quality Diagnostic**: Conduct targeted experiments comparing MatinaSRoberta embeddings against multilingual baselines on Persian-specific linguistic phenomena (agglutination, word order flexibility) using controlled sentence pairs to isolate the contribution of Persian-specific training