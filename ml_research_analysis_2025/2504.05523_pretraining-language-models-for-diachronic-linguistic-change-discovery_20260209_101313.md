---
ver: rpa2
title: Pretraining Language Models for Diachronic Linguistic Change Discovery
arxiv_id: '2504.05523'
source_url: https://arxiv.org/abs/2504.05523
tags:
- pretrained
- finetuned
- language
- time
- change
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the challenge of using large language models
  (LLMs) for diachronic linguistic research, where models must respect historical
  boundaries. The authors propose a novel method: training multiple small models on
  time-segmented corpora using efficient pretraining techniques (BabyLlama-2), rather
  than fine-tuning larger models.'
---

# Pretraining Language Models for Diachronic Linguistic Change Discovery

## Quick Facts
- arXiv ID: 2504.05523
- Source URL: https://arxiv.org/abs/2504.05523
- Authors: Elisabeth Fittschen; Sabrina Li; Tom Lippincott; Leshem Choshen; Craig Messner
- Reference count: 23
- Key outcome: Pretraining small models on time-segmented corpora enables diachronic linguistic analysis with better temporal boundaries than fine-tuning large models

## Executive Summary
This paper addresses the challenge of using large language models for diachronic linguistic research, where models must respect historical boundaries. The authors propose a novel method: training multiple small models on time-segmented corpora using efficient pretraining techniques (BabyLlama-2), rather than fine-tuning larger models. They construct a temporally-segmented dataset of five 10-million-word slices from 1750-1940. Results show that their pretrained models train nearly twice as fast as fine-tuned baselines, better respect historical divisions, and enable detection of lexical, grammatical, and morphological changes across time periods. The method outperforms fine-tuning for tasks requiring precise temporal boundaries, offering a new approach for hypothesis discovery in diachronic linguistics and potentially other fields.

## Method Summary
The authors train five small language models (BabyLlama-2, 345M parameters) on five temporally-segmented 10-million-word corpora from 1750-1940, using knowledge distillation for efficiency. They compare these to five fine-tuned Llama3-8B models using DoRA adapters. The training pipeline includes creating a date-attribution system using Wikidata and LLM inference to filter Project Gutenberg data, training BPE tokenizers per slice, and implementing the BabyLlama-2 distillation recipe. Evaluation involves cross-period perplexity matrices, BLiMP linguistic tasks, and a novel OED-based cloze task to assess temporal specialization and hypothesis generation capabilities.

## Key Results
- Pretrained models train nearly twice as fast as fine-tuned baselines while achieving better temporal specificity
- Cross-period perplexity evaluation shows pretrained models better respect historical divisions without temporal leakage
- The battery of time-segmented models successfully detects lexical, grammatical, and morphological changes across periods
- The method enables novel forms of hypothesis discovery through contrastive perplexity and cloze analysis

## Why This Works (Mechanism)

### Mechanism 1: Domain-Restricted Pretraining for Temporal Isolation
Training language models from scratch on time-segmented corpora prevents "knowledge leakage" from future time periods, yielding models that better respect historical boundaries. By restricting pretraining data to specific temporal slices, model weights only encode information present during that period, avoiding the incomplete catastrophic forgetting seen in fine-tuned models that retain broad-temporal knowledge.

### Mechanism 2: Efficient Pretraining via Knowledge Distillation on Small Corpora
Small, time-segmented corpora (10M words) are sufficient to train useful language models using BabyLlama-2's distillation approach. The student model learns from teacher models trained on the same small corpus, converging faster and more efficiently than training from scratch or parameter-efficient fine-tuning of much larger models.

### Mechanism 3: Contrastive Perplexity and Cloze Analysis for Hypothesis Discovery
A battery of temporally-isolated models enables novel diachronic hypothesis discovery by contrasting their performance on tasks like perplexity and cloze completion. Differential model behavior across time slices serves as a signal for linguistic change, where words or constructions characteristic of later periods show high perplexity for earlier models and lower perplexity for later models.

## Foundational Learning

**Catastrophic Forgetting**: The core problem where neural networks lose previously learned information when learning new information. Understanding this is key to seeing why pretraining from scratch is the proposed solution. *Quick check*: What is the term for when a neural network loses information learned in previous training steps upon learning new information?

**Knowledge Distillation**: The efficient pretraining technique where a student model learns from the logits of teacher models. This allows small-corpus training to be viable. *Quick check*: In knowledge distillation, does the student model learn from the hard labels or the soft logits (probabilities) of the teacher model?

**Perplexity**: The primary metric measuring how "surprised" a language model is by a sequence of text. Lower perplexity indicates better fit to the data. *Quick check*: If a language model assigns high probability to a sequence of text, will the perplexity be high or low?

## Architecture Onboarding

**Component map**: Date Attribution Pipeline -> Model Training Module -> Hypothesis Discovery Engine

**Critical path**: The entire pipeline is critical. The Date Attribution Pipeline determines data quality, which directly impacts Model Training. The Hypothesis Discovery is only as valid as the temporal isolation achieved during training.

**Design tradeoffs**:
- Fluency vs. Specificity: Trading high fluency of large, fine-tuned models for superior temporal specificity of smaller, pretrained models
- Compute Efficiency vs. Model Scale: Choosing a very small model (345M parameters) and small data slices (10M words) to prioritize compute efficiency and accessibility

**Failure signatures**:
- Temporal Leakage in Pretrained Models: If an early-period model correctly completes a cloze task from a later period, indicating data attribution errors or train/test leakage
- Uniformly High Perplexity: If all models have high perplexity on given text, suggesting failed convergence or out-of-domain text

**First 3 experiments**:
1. Perplexity Cross-Validation: Calculate perplexity on all 5 test sets for each model, expecting lowest perplexity on matching time slice
2. NPI Licenser Task: Run "only NPI licenser present" BLiMP task, expecting temporal changes in preference across pretrained models
3. Sense Trajectory Analysis: Plot normalized perplexity of a word with known semantic shift across 5 models, expecting trajectory to match historical emergence of new sense

## Open Questions the Paper Calls Out

**Open Question 1**: Can domain-restricted pretraining effectively detect linguistic shifts across synchronic boundaries, such as genre, rather than just temporal ones? The current study validates exclusively on diachronic data.

**Open Question 2**: Does increasing the training corpus size allow for the discovery of knowledge-level, rather than purely linguistic, hypotheses? The current 10-million-word slices may lack volume for broader factual discovery.

**Open Question 3**: Can the fluency gap between small pretrained models and large fine-tuned models be closed without sacrificing historical specificity? It's unclear if fluency loss is an inherent cost of data restrictions or a side effect of the BabyLlama-2 recipe.

## Limitations

- Corpus size (10M words per slice) is quite small for modern language modeling, potentially limiting linguistic pattern capture
- Temporal resolution (50-year slices) may miss important linguistic shifts occurring on shorter timescales
- Study focuses on a single historical period (1750-1940) and language variety (primarily British English), limiting generalizability

## Confidence

**High Confidence**: Core claim that domain-restricted pretraining prevents temporal leakage better than fine-tuning is well-supported by perplexity cross-validation results

**Medium Confidence**: Claims about computational efficiency and training speed benefits are supported but would benefit from larger-scale validation

**Medium Confidence**: Hypothesis discovery capabilities through contrastive perplexity analysis are demonstrated but require additional validation with domain expert verification

## Next Checks

1. Replicate the temporal leakage phenomenon by conducting a controlled experiment where both pretrained and fine-tuned models are evaluated on a carefully constructed test set with known historical markers

2. Validate the hypothesis discovery claims by having historical linguists evaluate a sample of the top-10 linguistic change hypotheses generated by the contrastive perplexity method

3. Test scalability by repeating the experiment with different temporal resolutions (10-year vs. 50-year slices) and different corpus sizes (20M vs. 10M words) to identify the method's breaking points