---
ver: rpa2
title: Gamma Mixture Modeling for Cosine Similarity in Small Language Models
arxiv_id: '2510.05309'
source_url: https://arxiv.org/abs/2510.05309
tags:
- distribution
- gamma
- mixture
- similarity
- cosine
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper investigates the distribution of cosine similarities
  between sentence embeddings in small language models, finding that they are often
  well-modeled by shifted and truncated gamma distributions or gamma mixtures. A heuristic
  hierarchical clustering model is proposed to explain this phenomenon, where cosine
  similarities arise from mixtures of gamma distributions due to topic-based embeddings.
---

# Gamma Mixture Modeling for Cosine Similarity in Small Language Models

## Quick Facts
- arXiv ID: 2510.05309
- Source URL: https://arxiv.org/abs/2510.05309
- Reference count: 24
- Key outcome: Shifted gamma distributions and gamma mixtures accurately model cosine similarity distributions in small language models, providing a practical alternative to permutation tests with significantly reduced data requirements

## Executive Summary
This paper investigates the distribution of cosine similarities between sentence embeddings in small language models, finding that they are often well-modeled by shifted and truncated gamma distributions or gamma mixtures. A heuristic hierarchical clustering model is proposed to explain this phenomenon, where cosine similarities arise from mixtures of gamma distributions due to topic-based embeddings. The paper introduces an expectation-maximization (EM) algorithm for fitting shifted gamma mixtures, demonstrating through experiments on three small models across three datasets that single gamma distributions provide good fits for most cases, with gamma mixtures offering better approximations when needed. The proposed method provides a practical tool for modeling similarity distributions with significantly reduced data requirements compared to permutation tests.

## Method Summary
The method employs an ECM (Expectation-Conditional Maximization) algorithm to fit shifted gamma mixtures to cosine similarity distributions. The E-step computes posterior responsibilities γ_{t,i} for each data point belonging to mixture component i. The M-step updates mixture weights τ_i (via closed-form Lagrange multiplier), eliminates rate parameter λ_i (setting λ_i = α_i κ_i), solves for shape parameter α_i using bisection on a digamma equation, and finds shift parameter c_i through convex/concave root-finding. A warm-start strategy runs 95% of iterations on 1/20 of the data before refining on the full dataset. The approach models truncated gamma distributions on [-1,1] support, with truncation approximation valid when omitted mass is negligible.

## Key Results
- Shifted gamma distributions (α ≈ 13.3, c ≈ -0.28, λ ≈ 35.5) provide excellent fits to cosine similarity distributions in topical corpora
- Single gamma distributions suffice for most cases, with gamma mixtures needed only for complex multi-topic queries
- Warm-starting with 1/20 of data for 95% of iterations achieves order-of-magnitude speed improvements
- The method requires significantly less data than permutation tests while maintaining accuracy

## Why This Works (Mechanism)

### Mechanism 1
Cosine similarity distributions in topical corpora are well-approximated by shifted, truncated gamma distributions rather than symmetric alternatives. When embeddings are drawn from a domain-specific corpus S₀, the distribution D_q develops a positive mean and right skew. A gamma distribution G(α, c, λ), shifted by c and truncated to [-1,1], captures this asymmetry because the gamma's shape parameter α controls tail heaviness while the shift c centers the distribution appropriately. The corpus restriction to topical subsets is essential; uniform random sentence pairs would not exhibit this pattern.

### Mechanism 2
Hierarchical topic structure in embedding space induces gamma-mixture distributions through level-wise contributions. A binary tree generates embeddings by perturbing parent vectors toward children with correlation strength η. Documents sharing recent ancestors with the query yield high similarity; distant relations decay. Each tree level contributes an exponentially decaying number of samples with progressively lower similarity, and the convolution of exponential decay with symmetric kernel variance approximates gamma structure. Higher η (tighter clustering) produces single-gamma fits; moderate η yields mixtures.

### Mechanism 3
ECM algorithm with warm-starting efficiently fits shifted gamma mixtures with significantly reduced data requirements versus permutation tests. Standard EM is extended with coordinate-wise maximization: (1) E-step computes γ_{t,i} posterior assignments; (2) M-step updates τ_i (closed-form via Lagrange multiplier), eliminates λ_i via λ_i = α_i κ_i, solves for α_i via bisection on digamma equation, then finds c_i via convex/concave root-finding. Warm-starting runs 95% of iterations on 1/20 of data before refining on full data.

## Foundational Learning

- **Gamma distribution parameterization (shape α, rate λ, shift c)**: Why needed here: The paper uses non-standard shifted gamma parameterization; understanding how α controls tail shape and c re-centers the distribution is essential for interpreting fitted parameters and debugging convergence. Quick check question: Given α=13.3, c=-0.28, λ=35.5, what is the approximate mode of the distribution before truncation?

- **Expectation-Conditional Maximization (ECM) vs. standard EM**: Why needed here: The algorithm performs coordinate-wise updates rather than joint maximization due to non-convexity; understanding why λ elimination precedes α optimization clarifies the update ordering constraints. Quick check question: Why can't α_i and c_i be updated jointly in a single convex optimization step?

- **Truncation to bounded support [-1,1]**: Why needed here: Cosine similarity is bounded but gamma is defined on [0,∞); the paper models using the truncated tail portion where omitted mass is negligible, but this requires numerical care in PDF normalization. Quick check question: Under what conditions would the truncation approximation fail (i.e., significant mass beyond x=1)?

## Architecture Onboarding

- **Component map**: Corpus sentences → sentence transformer (all-MiniLM-L6-v2 / mpnet / roberta) → normalized embeddings ∈ R^n → cosine similarity computation → histogram → shifted gamma mixture fitting → fitted distribution parameters

- **Critical path**: 1. Embed corpus with chosen sentence transformer (ensure normalization) 2. Compute all cosine similarities against reference query 3. Initialize mixture parameters (single gamma: method of moments; mixture: k-means or prior) 4. Run ECM with warm-start schedule 5. Validate fit (visual inspection, KS test, or likelihood)

- **Design tradeoffs**: Single gamma vs. mixture: Single is faster and sufficient for most cases; mixtures needed for complex multi-topic queries but increase parameters and local optima risk. Component count s: Not automatically selected; overfitting possible with too many components. Initialization quality: Random starts may converge to poor local optima; warm-start helps but assumes subsample representativeness.

- **Failure signatures**: Left-skewed empirical distribution: vMF would fit better; gamma assumption violated. c_i approaches or exceeds data minimum: Shift parameter hitting boundary indicates poor model specification. γ_{t,i} collapse: All mass assigned to one component; reduce s or reinitialize. Bisection non-convergence: α_i or c_i updates fail when Q-function has unexpected curvature; check data for outliers.

- **First 3 experiments**: 1. Replicate single-gamma fit on arXiv abstracts with all-MiniLM-L6-v2: compute D_q for 5 random queries, fit gamma, report (α, c, λ) and KS-statistic; compare to Figure 2 parameters as sanity check. 2. Ablate corpus restriction: Fit gamma on D_q(S₀) vs. D_q(random sentence pairs); quantify shift in mean and fit quality to validate topical-coherence mechanism. 3. Test mixture necessity: For queries where single gamma fits poorly, fit 2-component mixture and compare AIC/BIC; document which query types require mixtures.

## Open Questions the Paper Calls Out
None

## Limitations
- The hierarchical clustering model remains heuristic without corpus validation
- Mixture component count selection is manual, risking overfitting
- Truncation approximation validity for heavy-tailed cases is not quantified
- Superiority claims rely on visual comparisons rather than rigorous statistical tests

## Confidence

- **High confidence**: The empirical observation that shifted gamma distributions fit cosine similarity histograms well - directly supported by histogram comparisons in Figures 2-8 and quantitative fits.
- **Medium confidence**: The ECM algorithm's convergence properties and computational efficiency gains from warm-starting - theoretically sound but lacks comprehensive timing benchmarks.
- **Low confidence**: The hierarchical clustering mechanism's explanatory power for gamma-mixture emergence - remains a theoretical construct without corpus validation or alternative generative model comparisons.

## Next Checks

1. **Statistical significance testing**: Apply Kolmogorov-Smirnov or likelihood ratio tests to quantify the superiority of gamma vs. vMF fits across all model-corpus combinations, not just visual inspection.

2. **Component selection automation**: Implement and validate an information criterion (AIC/BIC) or cross-validation approach for automatic selection of mixture component count s, addressing the current manual selection limitation.

3. **Truncation approximation validation**: Quantify the approximation error introduced by truncating gamma distributions to [-1,1] by computing the omitted probability mass for fitted parameters, particularly for right-skewed cases with heavy tails.