---
ver: rpa2
title: 'Neural Incompatibility: The Unbridgeable Gap of Cross-Scale Parametric Knowledge
  Transfer in Large Language Models'
arxiv_id: '2505.14436'
source_url: https://arxiv.org/abs/2505.14436
tags:
- knowledge
- arxiv
- parametric
- parameters
- lora
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates parametric knowledge transfer between cross-scale
  large language models (LLMs). It identifies alignment in parametric space as the
  critical prerequisite for successful transfer, and distinguishes between Post-Align
  PKT (PostPKT) - which requires fine-tuning after knowledge injection using LoRA
  initialization, and Pre-Align PKT (PrePKT) - which aims to align parameters before
  injection to avoid additional training.
---

# Neural Incompatibility: The Unbridgeable Gap of Cross-Scale Parametric Knowledge Transfer in Large Language Models

## Quick Facts
- arXiv ID: 2505.14436
- Source URL: https://arxiv.org/abs/2505.14436
- Reference count: 40
- Key outcome: LaTen achieves strong performance with minimal alignment steps, improving GSM8K by 4.40 points and showing an average improvement of 1.86 across datasets compared to baseline models.

## Executive Summary
This paper investigates parametric knowledge transfer between cross-scale large language models (LLMs) and identifies alignment in parametric space as the critical prerequisite for successful transfer. The authors distinguish between Post-Align PKT (PostPKT) which requires fine-tuning after knowledge injection using LoRA initialization, and Pre-Align PKT (PrePKT) which aims to align parameters before injection to avoid additional training. They propose LaTen (Locate-Then-Align), a method using neuron-level attribution to identify task-relevant neurons and a hypernetwork to align parametric spaces across scales. Experiments with Llama-2 models demonstrate that both paradigms face challenges due to Neural Incompatibility - the ethological and structural differences between cross-scale models.

## Method Summary
LaTen (Locate-Then-Align) is a Pre-Align PKT method that extracts task-specific knowledge from a larger LLM (Llama-2-13B) and transfers it to a smaller LLM (Llama-2-7B) without extensive fine-tuning. The method consists of two key phases: (1) Neuron-level attribution identifies the most important neurons in the source model that contribute to target task performance, and (2) A lightweight hypernetwork learns to transform these extracted parameters into a format compatible with the smaller model's parametric space. The alignment is performed on a small dataset (<100 samples) using language modeling loss, and the aligned parameters are injected into the target model for immediate evaluation.

## Key Results
- LaTen improves GSM8K performance by 4.40 points compared to baseline models
- Average improvement across MMLU, GSM8K, HumanEval, and MBPP datasets is 1.86 points
- LaTen outperforms language-based distillation approaches in low-resource settings
- Direct unaligned transfer causes catastrophic performance collapse (scores near 0.0 on some tasks)

## Why This Works (Mechanism)

### Mechanism 1: Knowledge Localization via Neuron Attribution
Effective transfer requires isolating specific "knowledge neurons" rather than transferring entire layers. LaTen uses static neuron-level attribution to identify neurons that maximize target token probability, extracting a sparse subset of task-critical parameters from the larger model while filtering out noise.

### Mechanism 2: Parametric Space Alignment via Hypernetworks
Raw extracted parameters cannot be directly injected due to distributional and dimensional mismatches. A lightweight MLP-based hypernetwork learns to transform the extracted delta parameters from the larger model's space into the smaller model's space using a small alignment dataset.

### Mechanism 3: The Neural Incompatibility Hypothesis
Cross-scale models possess fundamentally different "ethological" (behavioral) and structural representations. CKA analysis reveals low similarity in representation behavior between 7B and 13B models, while parametric structural analysis shows that LoRA parameters derived from larger models have near-zero similarity to the original weights of smaller models.

## Foundational Learning

- **Concept: Low-Rank Adaptation (LoRA)**
  - **Why needed here:** The paper frames PKT in the context of LoRA initialization and injection. Understanding how rank decomposition ($W = W_0 + BA$) works is essential to grasp why "delta parameters" are being extracted and how they are mathematically injected into the student model.
  - **Quick check question:** How does the rank $r$ of the LoRA matrix constrain the amount of information that can be transferred from $M_l$ to $M_s$?

- **Concept: Centered Kernel Alignment (CKA)**
  - **Why needed here:** The paper uses CKA as the primary metric to prove "Neural Incompatibility." Without understanding CKA, one cannot verify the authors' claim that the models have low "ethological similarity."
  - **Quick check question:** If two models have high CKA similarity, does that imply their weight matrices are identical, or just their representational behaviors?

- **Concept: Hypernetworks**
  - **Why needed here:** The core proposed solution (LaTen) relies on a hypernetwork to bridge the scale gap. You must understand that this is a network generating weights for another network, rather than a standard feed-forward classifier.
  - **Quick check question:** Why is a hypernetwork suitable for dimensional reduction in this context compared to a fixed linear projection (like PCA)?

## Architecture Onboarding

- **Component map:** Source Model ($M_l$) -> Attribution Module -> Hypernetwork -> Target Model ($M_s$)
- **Critical path:**
  1. **Extract:** Run input through $M_l$ → Calculate Attribution Scores → Select Top-K Neurons
  2. **Align:** Input extracted weights into Hypernetwork → Optimize Hypernetwork weights using small $D_{align}$ (Language Modeling Loss)
  3. **Inject:** Update $M_s$ weights via $\Theta_s = \Theta_s + \Delta\Theta_{align}$

- **Design tradeoffs:**
  - **PostPKT vs. PrePKT:** PostPKT is stable but expensive (requires ~1000 samples). PrePKT (LaTen) is cheap (<100 samples) but unstable and sensitive to hyperparameters.
  - **Self-Derivation vs. Cross-Scale:** Deriving LoRA from the model's own weights (PiSSA) is more effective than deriving from a larger model, suggesting a tradeoff between "knowledge gain" and "compatibility."

- **Failure signatures:**
  - **Catastrophic Collapse:** Performance dropping to near zero (e.g., 0.0 in MMLU baselines) when unaligned parameters disrupt the target model's distribution.
  - **Negative Transfer:** Performance degrades below the baseline $M_s$ when extracted parameters from $M_l$ conflict with $M_s$'s "ethological" patterns.
  - **Optimization Instability:** The PrePKT alignment process lacks a straightforward minimum, leading to high variance in results across different seeds.

- **First 3 experiments:**
  1. **Baseline Validation:** Attempt direct weight splicing between $M_l$ and $M_s$ on MMLU to reproduce the "failure of unaligned transfer" and confirm Neural Incompatibility.
  2. **Alignment Ablation:** Train the LaTen Hypernetwork with varying alignment set sizes ($|D_{align}| = 10, 50, 100$) to measure sensitivity.
  3. **CKA Verification:** Compute CKA heatmaps between $M_l$ and $M_s$ for FFN vs. Attention layers to verify which modules exhibit the highest "incompatibility."

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Why does utilizing a stronger, specialized source model ($M_l$) result in inferior parametric knowledge transfer compared to a general source model?
- **Basis in paper:** The authors state in the Limitations that "it is also uncertain why a stronger $M_l$ yields no improvement," despite experiments showing specialized models (e.g., WizardMath) performed worse as sources than general ones.
- **Why unresolved:** The paper empirically observes this counter-intuitive degradation but lacks a theoretical explanation for the parametric incompatibility between specialized and target models.
- **What evidence would resolve it:** A mechanistic analysis identifying specific parametric divergences in fine-tuned models that inhibit alignment with smaller, general models.

### Open Question 2
- **Question:** Is it possible to achieve effective Pre-Align Parametric Knowledge Transfer (PrePKT) without relying on language-based supervision for alignment?
- **Basis in paper:** Section 7 highlights that current methods "still rely on language for supervision" and suggests that "developing simpler and more efficient approaches that do not depend on language guidance" is a necessary future direction.
- **Why unresolved:** The proposed LaTen method uses language modeling loss to train the hypernetwork for alignment, essentially reverting to a form of distillation rather than pure parameter manipulation.
- **What evidence would resolve it:** A method that aligns parametric spaces using only geometric or statistical constraints on weights, achieving transfer without calculating token probabilities.

### Open Question 3
- **Question:** How can the optimization stability of Pre-Align PKT be improved to remove the need for manual checkpoint selection?
- **Basis in paper:** Section 5.2 notes that the "parameter space alignment process does not exhibit a straightforward minimum point," making it "challenging to optimize" and requiring multiple experiments to find the best checkpoint.
- **Why unresolved:** The non-monotonic nature of the alignment training suggests the loss landscape is complex or noisy, preventing consistent convergence.
- **What evidence would resolve it:** A modified alignment objective or regularization technique that results in a monotonic correlation between alignment loss and downstream task performance.

## Limitations

- The paper identifies Neural Incompatibility as a fundamental barrier but doesn't conclusively prove these represent truly "unbridgeable" barriers versus current technical limitations.
- The PrePKT approach shows performance improvements but suffers from optimization instability with no straightforward convergence path.
- The neuron attribution approach assumes knowledge is sparsely localized in specific neurons, which may not hold for all types of knowledge or model architectures.

## Confidence

- **High Confidence:** Experimental results showing LaTen's performance improvements over baselines (4.40 points on GSM8K, 1.86 average across datasets) are well-documented and reproducible.
- **Medium Confidence:** The Neural Incompatibility hypothesis is supported by CKA analysis and parametric similarity measurements, but alternative explanations exist.
- **Low Confidence:** The claim that neuron-level attribution captures all task-critical knowledge may be overstated and likely varies significantly across different knowledge types.

## Next Checks

1. **CKA Heatmap Validation:** Generate layer-by-layer CKA similarity heatmaps between 7B and 13B models for both FFN and Attention modules to empirically verify which components exhibit the highest incompatibility before attempting transfer.
2. **Alignment Dataset Sensitivity:** Systematically vary the alignment set size (D_align = 10, 50, 100, 500) to quantify how sample efficiency affects the reliability and stability of the hypernetwork alignment process.
3. **Knowledge Type Ablation:** Test LaTen's effectiveness across different knowledge types (factual knowledge, reasoning skills, code generation) to determine whether the neuron attribution approach generalizes or is limited to specific capabilities.