---
ver: rpa2
title: 'MisSynth: Improving MISSCI Logical Fallacies Classification with Synthetic
  Data'
arxiv_id: '2510.26345'
source_url: https://arxiv.org/abs/2510.26345
tags:
- synthetic
- fallacy
- data
- https
- premise
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents MisSynth, a novel pipeline that leverages retrieval-augmented
  generation (RAG) to generate synthetic fallacy data, which is then used to fine-tune
  large language models for logical fallacy classification in health misinformation.
  The approach addresses the challenge of limited annotated data by producing context-aware
  synthetic examples grounded in source scientific articles.
---

# MisSynth: Improving MISSCI Logical Fallacies Classification with Synthetic Data

## Quick Facts
- **arXiv ID**: 2510.26345
- **Source URL**: https://arxiv.org/abs/2510.26345
- **Reference count**: 13
- **Primary result**: Achieves 35%+ absolute F1 improvement on logical fallacy classification using RAG-generated synthetic data

## Executive Summary
MisSynth addresses the challenge of limited annotated data for logical fallacy classification by introducing a pipeline that generates synthetic training data using retrieval-augmented generation. The approach retrieves source passages from scientific articles cited in MISSCI examples, then uses a large language model to generate context-aware fallacious and accurate premise samples. These synthetic examples are used to fine-tune base models with LoRA adapters, achieving substantial performance gains over baseline models across multiple architectures. The method demonstrates that grounding synthetic data in authentic source material enables effective transfer to real-world fallacy detection.

## Method Summary
The MisSynth pipeline begins by retrieving source passages from scientific articles using PubMedBERT embeddings and a same-source filter to ensure grounding in cited material. A large language model (GPT-5) generates synthetic data consisting of fallacious premises and accurate claim-premise pairs based on these retrieved excerpts. The synthetic dataset is then used to fine-tune base models (Mistral Small 3.2, LLaMA 3.1 8B, Gemma 2 9B) using LoRA adapters with rank-8 on attention weight matrices. The fine-tuning process uses instruction-formatted data and validates against the original MISSCI dev split, achieving significant performance improvements on the test set.

## Key Results
- Fine-tuned LLaMA 3.1 8B improves F1-score by over 35% compared to baseline (0.334 → 0.690)
- Best performance achieved with K=30 fallacious premises and M=15 claim-premise pairs per instance
- LoRA fine-tuning shows consistent validation loss reduction across all tested model architectures
- Synthetic dataset generation requires only one-time processing of 96 MISSCI dev samples

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Same-source RAG retrieval produces synthetic data that generalizes to real-world fallacies because grounding in actual scientific text constrains generation to realistic argument structures
- **Mechanism**: The pipeline retrieves passages from the same source article cited in MISSCI examples (via metadata filtering), then conditions generation on this retrieved context. This enforces that synthetic fallacies reference actual study findings rather than fabricated or generic content
- **Core assumption**: Grounding in authentic source text produces distributional similarity to human-authored misinformation
- **Evidence anchors**: [abstract] "pipeline that applies retrieval-augmented generation (RAG) to produce synthetic fallacy samples"; [Section 3.1] "the same-source filter enforces the MISSCI assumption that fallacious reasoning is based on the cited source"; [Section 2.2] contrasts their approach with "templated data, like the LFUD dataset, which lacks the complexity of real-world arguments"

### Mechanism 2
- **Claim**: LoRA fine-tuning with rank-8 adapters on attention projections enables efficient specialization without catastrophic forgetting of base reasoning capabilities
- **Mechanism**: LoRA injects trainable low-rank matrices (A, B) into frozen W_q and W_v projections. The paper uses r=8, which constrains adaptation to a ~64x smaller subspace than full fine-tuning while preserving the pretrained model's broader reasoning patterns
- **Core assumption**: Fallacy classification can be learned through low-rank perturbations without requiring full-weight updates
- **Evidence anchors**: [Section 2.3] "LoRA allows effective fine-tuning on consumer-grade hardware without sacrificing performance"; [Section 3.3] provides formal objective: maximizing conditional likelihood over adapter parameters Θ while keeping Φ_0 fixed; [Table 5] shows consistent validation loss drops across all models

### Mechanism 3
- **Claim**: Data augmentation with synthetic claim-premise pairs (M=15) improves robustness by diversifying input structures beyond the fixed claim-premise pairs in original dev data
- **Mechanism**: Generating M synthetic claim/accurate-premise pairs per instance creates variation in the input distribution while preserving label structure. This appears to prevent the model from overfitting to specific claim formulations
- **Core assumption**: Diversity in claim/premise phrasing improves generalization to unseen test examples
- **Evidence anchors**: [Table 1] K=30, M=15 achieves best F1 (0.690) vs. K=40, M=20 (0.647), suggesting optimal diversity threshold; [Section 3.2.2] "to increase diversity of inputs, since each K fallacious premises from above contain the same real claim–premise pairs per instance"

## Foundational Learning

- **Concept: Retrieval-Augmented Generation (RAG)**
  - Why needed here: Core mechanism for grounding synthetic data. Without understanding dual-encoder retrieval and top-k similarity search, the same-source constraint in Section 3.1 is opaque
  - Quick check question: Given a query embedding e_q and passage embedding e_j, how would you compute their cosine similarity? What does k=5 mean in the retrieval step?

- **Concept: Low-Rank Adaptation (LoRA)**
  - Why needed here: All experimental results depend on this PEFT method. The rank parameter r=8 directly controls the adaptation capacity
  - Quick check question: If W_0 is a 4096×4096 matrix and LoRA uses rank r=8, how many trainable parameters does the adapter add? (Answer: 2 × 4096 × 8 = 65,536)

- **Concept: Macro F1-score for multi-class imbalance**
  - Why needed here: MISSCI has skewed class distributions (e.g., "Impossible Expectations" is 1.32% of test set). Macro F1 treats each class equally, making it sensitive to minority-class performance
  - Quick check question: Why would accuracy alone be misleading if a model predicts only the majority class "Fallacy of Exclusion" (27.53% of test data)?

## Architecture Onboarding

- **Component map**: MISSCI dev instance → PubMedBERT encoder → Vector index → Query (claim only) → Top-k retrieval (same-source filter) → Excerpt E_i → Generation LLM (GPT-5) → Synthetic samples (K fallacies, M claim-premise pairs) → Instruction formatting → LoRA fine-tuning (target model) → MISSCI test split → Evaluation (Acc, Macro F1)

- **Critical path**: The quality of the synthetic dataset hinges on three sequential dependencies: (1) retrieval must surface relevant source passages (ROUGE recall >0.6), (2) generation LLM must produce valid JSON with fallacy classes from the inventory, (3) LoRA must successfully adapt without overfitting. If any step fails, downstream gains collapse

- **Design tradeoffs**:
  - Generator quality vs. cost: GPT-5 produces best F1 (0.705) but has higher API costs than o4-mini (F1 0.690)
  - Data volume vs. performance: K=30/M=15 is optimal; K=40/M=20 shows degradation (Table 1)
  - LoRA rank vs. capacity: r=8 used consistently; paper does not ablate this, leaving potential gains unexplored

- **Failure signatures**:
  - Random baseline underperforms vanilla (F1 0.512 vs 0.550) → model learns from synthetic content, not template structure
  - Validation loss drops but F1 plateaus → overfitting to synthetic distribution
  - Specific class degrades post-fine-tuning (False Equivalence: 0.614 → 0.479) → synthetic data may underrepresent or mischaracterize that fallacy type

- **First 3 experiments**:
  1. **Reproduce baseline**: Load vanilla LLaMA 3.1 8B, evaluate on MISSCI test split to confirm F1 ≈ 0.334 (Table 5)
  2. **Ablate retrieval**: Replace RAG-based excerpts with random passages from different articles; expect F1 degradation similar to random baseline
  3. **Scale LoRA rank**: Test r=4, r=16, r=32 on same synthetic data to identify capacity ceiling; monitor validation loss for overfitting signals

## Open Questions the Paper Calls Out

- **Can the MisSynth pipeline effectively generalize to other fallacy detection benchmarks such as MAFALDA?**
  - Basis in paper: [explicit] The "Future work" section explicitly states an aim to adapt the method for other benchmarks like MAFALDA
  - Why unresolved: The current study is restricted exclusively to the MISSCI benchmark, and the released synthetic dataset is specialized for this specific domain
  - What evidence would resolve it: Applying the MisSynth generation pipeline to the MAFALDA dataset and evaluating the performance delta of fine-tuned models on that benchmark

- **Does fine-tuning with MisSynth data improve performance on the generation of fallacious premises?**
  - Basis in paper: [explicit] The "Limitations" section notes that the methodology addresses only the classification sub-task and does not evaluate the generation of fallacious premises
  - Why unresolved: The current evaluation metrics (Accuracy, F1) measure classification labels only, leaving the model's ability to generate coherent fallacious arguments untested
  - What evidence would resolve it: Evaluating the fine-tuned models on the generative components of the MISSCI or MISSCI-Plus benchmarks using standard generation metrics

## Limitations

- Dependence on GPT-5 for synthetic data generation, which is not publicly documented or accessible, making the approach potentially non-reproducible with open alternatives
- Assumption that RAG-grounded synthetic data will transfer to real-world fallacies, but this transfer is only validated within the MISSCI benchmark and may not generalize to broader misinformation contexts
- LoRA rank is fixed at r=8 without systematic ablation, leaving open questions about whether the reported performance gains are optimal or merely sufficient

## Confidence

- **High confidence**: The LoRA fine-tuning mechanism and its efficiency gains (validation loss drops from 2.45 to 0.05) are well-supported by consistent results across multiple model architectures
- **Medium confidence**: The effectiveness of same-source RAG retrieval for producing generalizable synthetic data is supported by the performance gains, but transfer to broader contexts remains unproven
- **Low confidence**: The optimal hyperparameters (K=30, M=15) for synthetic data generation are presented without systematic sensitivity analysis, making it unclear if these represent true optima or local maxima

## Next Checks

1. **Reproduce baseline performance**: Fine-tune LLaMA 3.1 8B with synthetic data generated by GPT-4o (instead of GPT-5) to test if performance gains transfer to accessible models
2. **Ablate retrieval grounding**: Compare fine-tuning results using synthetic data generated with random (non-same-source) passages versus same-source passages to quantify the contribution of RAG grounding
3. **Test LoRA rank sensitivity**: Systematically evaluate fine-tuning with r=4, r=8, r=16, r=32 on the same synthetic dataset to identify whether the rank-8 choice represents an optimal or merely adequate capacity