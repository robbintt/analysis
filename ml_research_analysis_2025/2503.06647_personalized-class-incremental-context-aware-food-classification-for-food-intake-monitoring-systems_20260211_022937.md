---
ver: rpa2
title: Personalized Class Incremental Context-Aware Food Classification for Food Intake
  Monitoring Systems
arxiv_id: '2503.06647'
source_url: https://arxiv.org/abs/2503.06647
tags:
- food
- classification
- meal
- learning
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a personalized, class-incremental food classification
  model designed to overcome limitations of existing methods in food intake monitoring
  systems. The model adapts to new food classes while maintaining high accuracy through
  personalization, prioritizing foods based on individual eating habits such as meal
  frequency, times, and locations.
---

# Personalized Class Incremental Context-Aware Food Classification for Food Intake Monitoring Systems

## Quick Facts
- **arXiv ID:** 2503.06647
- **Source URL:** https://arxiv.org/abs/2503.06647
- **Reference count:** 40
- **Primary result:** Proposed PDSN model shows improved classification accuracy for both new and existing food classes compared to conventional and class-incremental models

## Executive Summary
This paper introduces a personalized class-incremental food classification model designed to overcome limitations of existing methods in food intake monitoring systems. The model adapts to new food classes while maintaining high accuracy through personalization, prioritizing foods based on individual eating habits such as meal frequency, times, and locations. A modified dynamic support network (DSN) with a personalization plug-in is used to handle incremental class learning. The proposed framework integrates the model into a food intake monitoring system that analyzes meal images, estimates food weight using a smart scale, retrieves nutritional information from a database, and creates a dietary user profile via a mobile app. Experimental evaluations on FOOD101-Personal and VFN-Personal datasets show improved classification accuracy for both new and existing classes compared to conventional and class-incremental models. The personalization approach significantly boosts performance, especially in datasets with limited data.

## Method Summary
The paper proposes a Personalized Dynamic Support Network (PDSN) that combines class-incremental learning with context-aware personalization. The model uses a backbone network (ResNet/VGG) with a Feature Mapper, Supporter modules for incremental classes, and a Personalizer Plug-in. The key innovation is the Dynamic Gamma generation, which learns context-aware weights instead of using fixed hyperparameters. The personalization plug-in incorporates individual eating patterns (frequency, time, location) to adjust classification probabilities. The system processes meal images, estimates food weight using a smart scale, retrieves nutritional information from a database, and creates a dietary user profile via a mobile app. The model is trained incrementally, adding new Supporter modules while freezing the backbone, and uses a feedback loop to update personalization vectors based on user verification.

## Key Results
- PDSN achieves higher Top-1 Accuracy (Average and Total) than conventional and class-incremental models on FOOD101-Personal and VFN-Personal datasets
- Personalization significantly improves performance, particularly in datasets with limited data
- Meal frequency has the strongest impact on accuracy, followed by meal time, with location having the least pronounced effect
- The dynamic Gamma generation (learned MLP vs. fixed hyperparameter) contributes to improved adaptability

## Why This Works (Mechanism)
The model works by combining traditional image classification with personalized user behavior patterns. During inference, it calculates base class probabilities, applies dynamic Gamma weights learned from context, and then modulates these with personalized vectors that track individual eating frequency, meal times, and locations. This creates a context-aware probability distribution that prioritizes foods the user typically consumes. The forgetting factors (α_f=0.003, α_t=0.04, α_l=0.04) control how quickly the personalization adapts to new patterns while maintaining historical preferences. The incremental learning approach allows adding new food classes without retraining on all data, preventing catastrophic forgetting of base classes.

## Foundational Learning

**Class-Incremental Learning (CIL)**
*Why needed:* Traditional deep learning requires all classes upfront, but food monitoring systems encounter new foods over time
*Quick check:* Can the model learn new food classes without forgetting previously learned ones

**Cosine Similarity in Classification**
*Why needed:* Normalizing features and weights enables dynamic weight generation through cosine similarity
*Quick check:* Are feature vectors and class weights properly L2 normalized before Gamma calculation

**Personalization Vectors**
*Why needed:* Individual eating patterns vary significantly, requiring user-specific adjustments to classification
*Quick check:* Do personalization vectors update correctly based on feedback and maintain reasonable distributions

## Architecture Onboarding

**Component Map**
Image Input -> Backbone Network -> Feature Mapper -> Dynamic Gamma Generator -> Supporter Modules (Base + Incremental) -> Personalizer Plug-in -> Final Classification

**Critical Path**
Image features flow through the backbone to the Supporter modules, where dynamic Gamma weights modulate the output. The Personalizer plug-in then applies user-specific context vectors to produce final probabilities.

**Design Tradeoffs**
The model trades increased computational complexity (additional Supporter modules and personalization vectors) for improved accuracy and adaptability. Fixed Gamma hyperparameters would be simpler but less flexible than the learned approach.

**Failure Signatures**
- Incorrect L2 normalization causes Gamma fusion to fail
- High forgetting factors lead to over-adaptation and loss of base class knowledge
- Poor initial user history vectors cause personalization to dominate feature-based predictions

**First Experiments**
1. Verify L2 normalization implementation on feature vectors and class weights
2. Test incremental class addition with frozen backbone to confirm no catastrophic forgetting
3. Simulate personalization update equations with uniform initial vectors to validate feedback mechanism

## Open Questions the Paper Calls Out

**Real-world vs. Simulated Data**
How does performance change when applied to real-world longitudinal data compared to the simulated survey-based datasets used in this study? The current evaluation uses FOOD101-Personal and VFN-Personal derived from online surveys where participants "simulated one week of food consumption patterns," rather than capturing actual, passive eating behaviors.

**Location Data Granularity**
To what extent does the "least pronounced effect" of meal location stem from the feature's limited predictive power versus the coarse granularity of location data in the datasets? The ablation study concludes that "meal location, while contributing to improvement, had the least pronounced effect among the factors studied."

**Feedback Loop Robustness**
How sensitive are the personalization update equations (forgetting factors) to sparse or incorrect user feedback during long-term model usage? The methodology relies on a feedback loop where "User feedback to verifies the correctness," but experiments assume ground truth labels rather than modeling user fatigue or error.

## Limitations

- Performance evaluation relies on simulated user datasets rather than real-world passive data collection
- The Gamma Generator network architecture (hidden layers, dimensions) is not fully specified
- Location data granularity may be too coarse to capture meaningful spatial patterns
- The forgetting factors may not be optimal for all user populations or dietary patterns

## Confidence

**Data Simulation** (Medium): The exact "Personal" dataset splits are unclear, requiring assumptions about user behavior patterns
**Network Architecture** (High): Core DSN structure is specified, but Gamma Generator depth is unspecified
**Personalization Logic** (High): Plug-in equations and forgetting factors are clearly defined
**Incremental Learning** (Medium): Mechanism is clear but depends on data simulation quality

## Next Checks

1. Verify that L2 normalization of weights and features is correctly implemented; incorrect normalization will break the cosine similarity-based Gamma fusion
2. Test the forgetting factor update equations with uniform initial user history vectors to ensure personalization does not dominate feature-based predictions
3. Simulate a small-scale incremental learning scenario with synthetic user streams to validate that new classes can be added without catastrophic forgetting of base classes