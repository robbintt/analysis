---
ver: rpa2
title: HY-MT1.5 Technical Report
arxiv_id: '2512.24092'
source_url: https://arxiv.org/abs/2512.24092
tags:
- translation
- hy-mt1
- wang
- language
- xcomet-xxl
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents HY-MT1.5-1.8B and HY-MT1.5-7B, two high-performance
  machine translation models developed using a holistic training framework. The models
  are designed to address the challenge of balancing translation quality and efficiency,
  particularly for resource-constrained deployment scenarios.
---

# HY-MT1.5 Technical Report

## Quick Facts
- **arXiv ID:** 2512.24092
- **Source URL:** https://arxiv.org/abs/2512.24092
- **Reference count:** 10
- **Primary result:** HY-MT1.5-1.8B achieves ~90% of Gemini-3.0-Pro performance with 0.18s average latency

## Executive Summary
HY-MT1.5-1.8B and HY-MT1.5-7B are high-performance machine translation models developed through a holistic training framework that balances translation quality and efficiency. The 1.8B parameter model achieves approximately 90% of the performance of ultra-large proprietary models like Gemini-3.0-Pro while significantly outperforming larger open-source baselines. Both models support advanced features including terminology intervention, context-aware translation, and format preservation, with the smaller model demonstrating efficient deployment capabilities at sub-200ms latency.

## Method Summary
The training pipeline integrates MT-oriented pre-training, supervised fine-tuning, on-policy distillation from a 7B teacher model, and reinforcement learning with a rubrics-based evaluation system. The process chains MT-oriented continuous pretraining and supervised fine-tuning on translation instruction data, followed by strong-to-weak on-policy distillation using per-token reverse KL divergence, and reinforcement learning with Group Relative Policy Optimization (GRPO) guided by multi-dimensional rubric rewards across accuracy, fluency, consistency, cultural appropriateness, and readability dimensions.

## Key Results
- HY-MT1.5-1.8B achieves XCOMET-XXL scores of 0.8361 on Flores-200 and 0.5308 on WMT25
- HY-MT1.5-7B reaches 95% of Gemini-3.0-Pro performance on Flores-200 and surpasses it on WMT25 and Mandarin-minority benchmarks
- The 1.8B model maintains high translation quality at an average response time of 0.18 seconds
- Both models significantly outperform larger open-source baselines like Tower-Plus-72B and Qwen3-32B

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Multi-stage training pipeline enables small models to achieve translation quality competitive with larger models
- **Core assumption:** Knowledge distillation and RLHF-style alignment successfully compress larger teacher capabilities into smaller students
- **Evidence anchors:** Outperforms Tower-Plus-72B and Qwen3-32B; on-policy distillation from 7B teacher enables 1.8B to inherit superior performance
- **Break condition:** Distillation fails to transfer nuanced linguistic reasoning, causing performance plateau on complex tasks

### Mechanism 2
- **Claim:** Rubrics-based multi-dimensional evaluation provides finer-grained optimization than single holistic score
- **Core assumption:** LLM evaluator reliably scores across five dimensions without systematic bias
- **Evidence anchors:** Multi-dimensional evaluation guides LLM with greater granularity; uses Accuracy, Fluency, Consistency, Cultural Appropriateness, Readability
- **Break condition:** Evaluator bias leads to reward hacking, optimizing wrong objectives

### Mechanism 3
- **Claim:** FP8 quantization maintains near-original quality while enabling efficient deployment
- **Core assumption:** Precision reduction doesn't degrade translation semantics for supported language pairs
- **Evidence anchors:** 0.18s average response time; FP8 preserves accuracy close to original (ZH â‡” XX: 0.8379 vs 0.8361)
- **Break condition:** Non-uniform quantization error disproportionately affects low-resource languages

## Foundational Learning

- **Concept: Knowledge Distillation (On-Policy)**
  - **Why needed here:** Essential for understanding how 1.8B model derives performance from 7B teacher
  - **Quick check question:** Can you explain why aligning student output distribution with teacher's given student's own samples (on-policy) differs from using static dataset (off-policy)?

- **Concept: Reinforcement Learning from Human Feedback (RLHF) / GRPO**
  - **Why needed here:** Uses GRPO and custom rubrics-based reward model
  - **Quick check question:** What is the "policy" being optimized and what acts as the "reward signal" in this context?

- **Concept: LLM Quantization (PTQ & FP8)**
  - **Why needed here:** Key contribution is efficiency via Post-Training Quantization
  - **Quick check question:** What is the primary trade-off between Int4 and FP8 quantization for HY-MT1.5-1.8B model?

## Architecture Onboarding

- **Component map:** HY-1.8B-Base / HY-7B-Base -> [MT-CPT] -> [SFT] -> HY-MT1.5-7B (Teacher) & HY-MT1.5-1.8B-preview (Student) -> [On-Policy Distillation] -> [RL (GRPO + Rubrics)] -> HY-MT1.5-1.8B (Final) -> [Quantization (GPTQ)] -> HY-MT1.5-1.8B-FP8 / Int4

- **Critical path:** Distillation stage from 7B teacher to 1.8B student is most critical for smaller model's high performance per parameter

- **Design tradeoffs:** Performance vs Efficiency (7B offers higher quality but >2x latency); Quantization Accuracy (FP8 preserves near-original quality, Int4 offers more compression with noticeable degradation)

- **Failure signatures:** Terminology Hallucination without prompt intervention; Contextual Disambiguation Failure selecting common sense over correct meaning

- **First 3 experiments:**
  1. Run provided models on Flores-200 subset to replicate reported XCOMET-XXL scores
  2. Construct evaluation set to measure performance with/without terminology/context/format prompts
  3. Deploy FP8 and Int4 variants to measure inference latency, memory, and quality on Mandarin-Minority pairs

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How does translation performance scale when expanding to broader range of low-resource and underrepresented languages?
- **Basis in paper:** Future work focuses on expanding language coverage to more low-resource languages
- **Why unresolved:** Current focus on 33 languages; doesn't evaluate wider spectrum of low-resource languages
- **Evidence would resolve it:** Benchmarking results on full Flores-200 or specialized low-resource benchmarks

### Open Question 2
- **Question:** Can performance-efficiency ratio be improved by developing more efficient distillation and RL algorithms?
- **Basis in paper:** Future work focuses on optimizing training framework with more efficient distillation and RL
- **Why unresolved:** Current framework successful but optimization identified as necessary future step
- **Evidence would resolve it:** Comparative experiments showing higher quality or faster convergence with new algorithms

### Open Question 3
- **Question:** To what extent can advanced prompt engineering and domain adaptation enhance handling of industry-specific customized translation needs?
- **Basis in paper:** Future work focuses on deepening customized translation research with advanced prompt engineering and domain adaptation
- **Why unresolved:** Demonstrates current features via prompts but suggests advanced techniques needed for industrial applications
- **Evidence would resolve it:** Evaluations on domain-specific datasets showing significant improvements with advanced techniques

## Limitations

- Model scalability to smaller sizes (e.g., 1B parameters) remains untested and may result in significant quality degradation
- Reliability of LLM-based rubrics evaluator not validated across all five dimensions for less common language pairs
- Detailed quantization impact analysis missing for individual language pairs, particularly low-resource Mandarin-minority languages

## Confidence

- **Performance Claims:** High confidence (well-supported by XCOMET-XXL/CometKiwi scores on established benchmarks)
- **Efficiency Claims:** Medium confidence (specific latency but lacks hardware/benchmarking details)
- **Feature Claims:** Low confidence (demonstrated through case studies but lacks quantitative evaluation)

## Next Checks

1. Conduct human evaluation study to measure correlation between LLM-based rubrics evaluator and human judgments across all five dimensions for diverse language pairs
2. Perform detailed analysis of FP8 quantization's impact on translation quality for each individual language pair, especially Mandarin-minority languages
3. Attempt replication with significantly smaller student model (e.g., 1B parameters) to assess limits of approach