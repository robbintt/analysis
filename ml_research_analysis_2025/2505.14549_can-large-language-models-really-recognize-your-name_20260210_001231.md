---
ver: rpa2
title: Can Large Language Models Really Recognize Your Name?
arxiv_id: '2505.14549'
source_url: https://arxiv.org/abs/2505.14549
tags:
- name
- names
- llms
- human
- privacy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Large language models (LLMs) are increasingly used in privacy pipelines
  to detect and redact sensitive information, particularly human names. This study
  investigates whether LLMs can reliably recognize human names, especially those that
  are ambiguous due to their orthographic similarity to non-human entities (e.g.,
  locations, organizations).
---

# Can Large Language Models Really Recognize Your Name?

## Quick Facts
- arXiv ID: 2505.14549
- Source URL: https://arxiv.org/abs/2505.14549
- Reference count: 40
- Large language models show 20-40% lower recall for ambiguous human names that are orthographically similar to non-human entities

## Executive Summary
This study investigates the reliability of large language models (LLMs) in recognizing human names, particularly those that are ambiguous due to their similarity to locations or organizations. The authors created AmBench, a benchmark dataset of over 12,000 real human names that differ by one letter from non-human entities. Experiments with 12 state-of-the-art LLMs reveal that recall for these ambiguous names drops by 20-40% compared to more recognizable names. The study also demonstrates that benign prompt injections can make ambiguous names up to four times more likely to be ignored in real-world LLM-powered privacy tools, highlighting significant vulnerabilities in LLM-based privacy solutions.

## Method Summary
The researchers constructed AmBench, a novel benchmark dataset containing over 12,000 real human names that are orthographically similar to non-human entities (locations, organizations). Each name was placed in concise text snippets compatible with multiple entity types to create ambiguity. The study tested 12 state-of-the-art LLMs on this dataset and compared their performance to more recognizable names. Additionally, they evaluated how benign prompt injections—instruction-like user texts—affect name recognition in a real-world LLM-powered enterprise privacy tool.

## Key Results
- LLM recall for ambiguous human names drops by 20-40% compared to more recognizable names
- Benign prompt injections can make ambiguous names up to four times more likely to be ignored in privacy tools
- Performance degradation is consistent across 12 tested state-of-the-art LLMs

## Why This Works (Mechanism)
LLMs rely on learned patterns from training data to identify entities, and ambiguous names that are orthographically similar to non-human entities create confusion in the model's pattern recognition. When these names appear in brief text snippets without clear contextual disambiguation, the models struggle to confidently classify them as human names rather than locations or organizations.

## Foundational Learning
- Named Entity Recognition (NER): The task of identifying and classifying named entities in text into predefined categories such as person names, organizations, and locations. Why needed: Understanding how LLMs perform NER is fundamental to evaluating their name recognition capabilities.
- Orthographic similarity: The visual similarity between words based on their spelling patterns. Quick check: Test model performance on names with increasing levels of orthographic similarity to non-human entities.
- Context window limitations: The maximum amount of text an LLM can process at once. Why needed: Understanding how context affects name disambiguation in short text snippets.
- Benign prompt injections: Seemingly innocuous user instructions that can manipulate LLM behavior. Quick check: Evaluate how different types of benign prompts affect entity recognition across multiple models.
- Privacy redaction pipelines: Automated systems that detect and remove sensitive information from text. Why needed: Understanding the real-world applications and implications of LLM-based privacy tools.

## Architecture Onboarding
Component map: Input text -> Name recognition module -> Entity classification -> Redaction decision
Critical path: The name recognition module must correctly identify ambiguous names despite orthographic similarity and potential prompt injections
Design tradeoffs: The study uses concise text snippets for controlled testing but acknowledges this may not reflect real-world document complexity
Failure signatures: Significant drops in recall for ambiguous names, increased vulnerability to prompt injections when names are ambiguous
First experiments:
1. Test AmBench with additional diverse LLMs to verify generalizability
2. Evaluate name recognition in longer, contextually rich documents
3. Investigate mitigation strategies through fine-tuning or prompt engineering

## Open Questions the Paper Calls Out
The paper highlights the need for systematic investigation into LLM failure modes and fairness implications in privacy applications. It calls for further research on how to improve name recognition for ambiguous names and how to make privacy tools more robust against benign prompt injections.

## Limitations
- The benchmark dataset may not fully capture naming diversity across cultures and languages
- Concise text snippets may not reflect the complexity of real-world documents where context aids disambiguation
- The study focuses on one specific type of ambiguity (names differing by one letter from non-human entities)

## Confidence
- Recall drop of 20-40% for ambiguous names: High
- Benign prompt injections making names up to four times more likely to be ignored: Medium

## Next Checks
1. Test the AmBench dataset with additional diverse LLMs, including open-source models trained on different datasets, to verify the generalizability of the recall drop findings
2. Evaluate name recognition performance in longer, more contextually rich documents to assess whether real-world performance differs from the controlled snippet-based experiments
3. Investigate whether fine-tuning or prompt engineering can mitigate the vulnerability to benign prompt injections, and if so, measure the effectiveness across different model architectures