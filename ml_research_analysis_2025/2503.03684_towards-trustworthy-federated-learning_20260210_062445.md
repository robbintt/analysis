---
ver: rpa2
title: Towards Trustworthy Federated Learning
arxiv_id: '2503.03684'
source_url: https://arxiv.org/abs/2503.03684
tags:
- learning
- gradients
- byzantine
- privacy
- fairness
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a comprehensive framework for trustworthy federated
  learning that simultaneously addresses fairness, privacy preservation, and Byzantine
  attack robustness. The proposed approach introduces a Two-sided Norm Based Screening
  (TNBS) mechanism to filter malicious gradients, adopts q-fair federated learning
  to promote egalitarian fairness across clients, and applies differential privacy
  via Gaussian noise addition to protect raw data.
---

# Towards Trustworthy Federated Learning

## Quick Facts
- arXiv ID: 2503.03684
- Source URL: https://arxiv.org/abs/2503.03684
- Reference count: 0
- Proposed method achieves O(1/√T) convergence rates while maintaining higher accuracy under Byzantine attacks compared to benchmarks

## Executive Summary
This paper presents a comprehensive framework for trustworthy federated learning that simultaneously addresses fairness, privacy preservation, and Byzantine attack robustness. The proposed approach introduces a Two-sided Norm Based Screening (TNBS) mechanism to filter malicious gradients, adopts q-fair federated learning to promote egalitarian fairness across clients, and applies differential privacy via Gaussian noise addition to protect raw data. Theoretical analysis establishes convergence guarantees under both non-convex and convex settings. Experimental results on MNIST and Spam datasets demonstrate superior performance compared to benchmarks like trimmed mean, Krum, and H-nobs.

## Method Summary
The framework operates through a central server coordinating multiple clients in a federated learning setup. Each client computes local gradients on their private data, applies differential privacy by adding Gaussian noise, and transmits the result to the server. The server aggregates these gradients using the Two-sided Norm Based Screening (TNBS) mechanism, which sorts gradients by norm and removes those at both extremes (defined by quantile parameter p) to filter out Byzantine attackers. The remaining gradients are averaged to update the global model. The q-fair federated learning scheme modifies the loss function to prioritize clients with higher losses, promoting egalitarian fairness. The entire process iterates until convergence is achieved.

## Key Results
- Theoretical convergence guarantees established with O(1/√T) rate under both non-convex and convex settings
- TNBS mechanism maintains higher accuracy than baselines (trimmed mean, Krum, H-nobs) under sign flipping, label flipping, and Gaussian attacks
- Successfully manages privacy-accuracy trade-off, with experimental results showing effective performance at noise variance σ²=0.2
- Achieves more uniform accuracy distribution across clients through q-fair learning with q=1 parameter

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Two-sided Norm Based Screening (TNBS) filters out Byzantine attackers by cropping gradients with the lowest and highest norms.
- Mechanism: The central server receives gradients from clients, sorts them by norm magnitude, and discards a specified proportion at both extremes (defined by quantile parameter p), retaining only gradients within the middle range. The remaining gradients are averaged for the update.
- Core assumption: Malicious gradients (Byzantine attacks) will exhibit norms significantly different (either much larger or much smaller) from honest gradients.
- Evidence anchors:
  - [abstract] "...TNBS mechanism which allows the central server to crop the gradients that have l lowest norms and h highest norms. TNBS functions as a screening tool to filter out potential malicious participants whose gradients are far away from the honest ones."
  - [section 2] "TNBS retains only those gradients whose norms fall within the specified quantiles p... Then, TNBS crops the Byzantine attackers by only keeping those gradients whose index fall within the computed thresholds."
  - [corpus] Weak/missing direct support for two-sided norm screening specifically; related work focuses on distance-based (Krum) or single-sided norm screening (NBS).
- Break condition: If Byzantine attackers craft gradients with norms similar to honest clients (e.g., sophisticated bounded attacks), TNBS will fail to filter them and may discard honest clients.

### Mechanism 2
- Claim: The q-fair federated learning (q-FFL) scheme promotes egalitarian fairness by reweighting the loss function to prioritize clients with higher losses.
- Mechanism: A fairness control parameter q is introduced into the objective function. By adjusting q, the optimization process increases the weight of clients currently performing poorly (high loss), driving the global model toward a more uniform accuracy distribution.
- Core assumption: Increasing the influence of high-loss clients during aggregation reduces the variance in accuracy across all clients without destroying the global model's overall utility.
- Evidence anchors:
  - [abstract] "To promote egalitarian fairness, we adopt the q-fair federated learning (q-FFL)."
  - [section 2] "A higher q value increases the emphasis on clients with higher losses, aiming for a more uniform accuracy distribution across devices."
  - [corpus] Supported generally as a technique for fair resource allocation, though specific q-FFL details are internal to this paper's framework.
- Break condition: If q is set too high, the model may overfit to outlier or difficult clients (or poisoned clients with high loss), degrading overall system performance and convergence speed.

### Mechanism 3
- Claim: Differential Privacy (DP) prevents raw data inference by adding calibrated Gaussian noise to model updates.
- Mechanism: Clients add random Gaussian noise to their computed gradients before transmission. The variance is calibrated based on sensitivity C, privacy budget epsilon, and failure probability delta.
- Core assumption: The added noise is sufficient to mask the contribution of any single data point, preventing model inversion attacks, while the aggregation process averages out the noise sufficiently for learning.
- Evidence anchors:
  - [abstract] "...adopt a differential privacy-based scheme to prevent raw data at local clients from being inferred by curious parties."
  - [section 2] "DP encryption in FL acts as a safeguard that guarantees the model does not provide any information about the data of any client..."
  - [corpus] Standard technique in robust FL frameworks (e.g., "Practical Framework for Privacy-Preserving and Byzantine-robust Federated Learning").
- Break condition: If the noise variance is set too high relative to the signal (gradient magnitude), the model will fail to converge or will converge to a suboptimal solution with high error.

## Foundational Learning

### Concept: Federated Learning (FL) & Non-IID Data
- Why needed here: The entire framework operates on distributed clients training a global model. Understanding that clients hold different data distributions (Non-IID) is critical because it motivates the fairness mechanism and complicates robustness.
- Quick check question: Why is aggregating gradients from Non-IID data more challenging than from IID data in the context of fairness?

### Concept: Byzantine Attacks (Gradient Manipulation)
- Why needed here: To understand why TNBS is necessary. You must recognize that "Byzantine" means arbitrary/malicious behavior (sign flipping, label flipping, Gaussian noise), not just random noise.
- Quick check question: How does a "sign flipping" attack differ from a simple communication error in its effect on a gradient aggregator?

### Concept: Differential Privacy (DP) Basics
- Why needed here: To interpret the trade-offs in the privacy mechanism. You need to know that lower epsilon means higher privacy but typically requires more noise.
- Quick check question: If a client requires stricter privacy (lower epsilon), what must happen to the noise variance added to their gradient?

## Architecture Onboarding

### Component map:
Client-Side: Local Data -> Loss Calc -> Fairness Reweight -> Gradient Calc -> DP Encryption (Add Noise) -> Transmit.
Server-Side: Receive Gradients -> TNBS Module (Sort norms, Crop high/low) -> Average Gradients -> Update Global Model.

### Critical path:
1. Server broadcasts model
2. Clients compute and encrypt gradients
3. Server applies TNBS to filter the sorted list of gradients
4. Server aggregates remaining gradients to produce new global model
5. Loop until convergence

### Design tradeoffs:
- **Accuracy vs. Privacy:** Higher noise improves privacy but lowers accuracy
- **Robustness vs. Efficiency:** Stricter screening removes more potential attackers but discards more valid compute power
- **Fairness vs. Global Performance:** Higher q helps worst-case clients but may reduce average accuracy

### Failure signatures:
- **Model Divergence/Explosion:** Likely caused by Byzantine gradients bypassing TNBS (attack scale within threshold) or learning rate too high for the noise level
- **Stagnation (High Loss):** Likely caused by excessive DP noise or q-parameter set too high for the data distribution
- **High Variance in Client Accuracy:** Fairness mechanism (q-FFL) is ineffective or q is too low

### First 3 experiments:
1. **Robustness Validation:** Run a baseline (no attack) vs. Sign Flipping/Gaussian attacks to verify TNBS successfully maintains higher accuracy than "No Screening" and standard "NBS"
2. **Privacy-Accuracy Curve:** Sweep noise variance to generate the trade-off curve and identify the "knee" where accuracy drops significantly
3. **Fairness Sensitivity:** Test different q values on Non-IID data to measure variance in client accuracy to find the optimal fairness setting

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Is the TNBS mechanism robust against adaptive attacks where malicious clients deliberately manipulate gradient direction while constraining norms to evade detection?
- Basis in paper: [inferred] The paper relies on Two-sided Norm Based Screening (TNBS), which filters gradients solely based on their norm magnitudes.
- Why unresolved: While TNBS filters extreme norms, it may fail to detect "stealthy" attacks where gradients are crafted to have normal norms but malicious orientations.
- What evidence would resolve it: Experimental results against adaptive adversaries who optimize gradients to stay within the calculated norm quantiles while maximizing model damage.

### Open Question 2
- Question: How does the simultaneous tuning of the fairness parameter $q$ and differential privacy noise variance $\sigma^2$ impact the convergence rate on complex, non-convex models?
- Basis in paper: [inferred] The paper acknowledges a trade-off but fixes hyperparameters ($q=1, \sigma^2=0.2$) and tests primarily on simpler datasets like MNIST.
- Why unresolved: It is unclear if high fairness emphasis (high $q$) amplifies the destabilizing effects of Gaussian noise (high $\sigma$) in deep neural networks.
- What evidence would resolve it: An ablation study analyzing convergence speed and accuracy across a grid of $q$ and $\sigma^2$ values on complex datasets (e.g., ImageNet).

### Open Question 3
- Question: Does the theoretical convergence bound for convex objectives in Theorem 2 remain tight as the fraction of Byzantine nodes ($\alpha$) approaches the assumed limit of $1/3$?
- Basis in paper: [inferred] Theorem 2 guarantees convergence assuming $\alpha < 1/3$, but the empirical evaluation uses a fixed, lower fraction of attackers (4 out of 20 clients).
- Why unresolved: Theoretical bounds often loosen significantly near threshold values; the actual breaking point of the algorithm may occur before $\alpha$ reaches 0.33.
- What evidence would resolve it: Empirical validation of model accuracy and convergence behavior as the Byzantine ratio is incrementally increased toward the $1/3$ limit.

## Limitations
- The TNBS mechanism may fail against sophisticated attacks where malicious gradients have similar norms to honest gradients
- The q-FFL fairness parameter q requires careful tuning and may overfit to outlier clients if set too high
- The framework assumes Byzantine attackers constitute less than 1/3 of the client population, which may not hold in all scenarios

## Confidence

### Confidence Labels
- **TNBS Mechanism**: Medium confidence - theoretically sound but vulnerable to norm-similar attacks
- **q-FFL Fairness**: Medium confidence - well-established technique but parameter sensitivity unvalidated
- **DP Privacy**: High confidence - standard application of Gaussian mechanism
- **Theoretical Convergence**: Medium confidence - proof structure valid but assumptions strong

## Next Checks

1. **Implementation Fidelity**: Recreate the framework with specified MNIST/Spambase splits and validate whether the reported accuracy differentials (e.g., 30% gap between TNBS and NBS) can be reproduced with reasonable architectural assumptions

2. **Attack Robustness Testing**: Systematically test TNBS against gradient attacks with controlled norm distributions to quantify the boundary where filtering fails

3. **Privacy-Accuracy Calibration**: Empirically determine the minimum noise variance that achieves meaningful privacy (ε < 10) while maintaining baseline accuracy above 80% on MNIST