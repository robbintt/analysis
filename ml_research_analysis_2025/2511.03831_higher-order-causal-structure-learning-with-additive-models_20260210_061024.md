---
ver: rpa2
title: Higher-Order Causal Structure Learning with Additive Models
arxiv_id: '2511.03831'
source_url: https://arxiv.org/abs/2511.03831
tags:
- which
- additive
- structure
- causal
- directed
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of learning causal structures
  in the presence of higher-order interactions between variables, which are common
  in real-world systems but often ignored by traditional methods. The authors extend
  the causal additive model (CAM) framework to directed hypergraphs (HDAGs), allowing
  explicit representation of multi-variable causal relationships.
---

# Higher-Order Causal Structure Learning with Additive Models

## Quick Facts
- arXiv ID: 2511.03831
- Source URL: https://arxiv.org/abs/2511.03831
- Reference count: 28
- One-line primary result: HCAM extends CAM to directed hypergraphs, achieving significantly better structure recovery on higher-order interaction data where traditional methods fail.

## Executive Summary
This paper addresses the challenge of learning causal structures with higher-order interactions, which are common in real-world systems but often ignored by traditional methods. The authors extend the causal additive model (CAM) framework to directed hypergraphs (HDAGs), allowing explicit representation of multi-variable causal relationships. They introduce theoretical tools including the hyper Markov property and prove identifiability results for HDAGs, showing they can be distinguished up to hyper Markov equivalence classes. Empirically, the authors develop and test a higher-order extension of CAM (HCAM) on synthetic data with 1D, 2D, and 3D additive interactions, demonstrating superior performance on higher-order data.

## Method Summary
HCAM extends CAM by learning directed acyclic hypergraphs through a three-step process: (1) Candidate search using an MLP with ARCHIPELAGO interaction importance scoring to select top 10 candidate hyperedges per node; (2) Greedy hyperedge addition via SIAN-based higher-order additive models trained for 5 epochs each, selecting by MSE drop while enforcing acyclicity; (3) Final end-to-end training with L1 regularization and pruning threshold 1.0e-4 on function variance. The method assumes Gaussian noise and additive separability across hyperparent sets, with causal sufficiency (no latent confounders).

## Key Results
- HCAM achieves lowest SHD (107.00) on ER4 2D data compared to baselines
- Standard methods like CAM and GES perform well on simple 1D cases but fail on higher-order 2D interactions
- None of the algorithms tested could reliably recover 3D interaction structures at d=30, n=10,000
- HO-SHD (higher-order SHD) reveals limitations of traditional SHD for evaluating hypergraph recovery

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Hypergraphical structure extends DAGs to represent higher-order causal interactions additively.
- Mechanism: Each child variable receives contributions from multiple hyperparent sets (S,j), where function f_S→j(x_S) captures interactions among parents in S. This replaces the single block function f_Pa(j)→j with a sum over subsets: X_j = Σ_{S∈HypPa(j)} f_S→j(x_S) + ε_j. The hierarchical constraint (S∈HypPa → all subsets also present) ensures downward closure.
- Core assumption: Additive separability across hyperparent sets; causal sufficiency (no latent confounders).

### Mechanism 2
- Claim: HDAGs are identifiable up to hyper Markov equivalence classes (HMECs) via conditional multi-independence tests.
- Mechanism: Standard MECs use skeleton + unshielded colliders. HMECs use body (undirected hyperedges) + unshielded multi-colliders. Multi-independence tests whether log p(x_i,x_j,x_k) requires a 3D energy term θ_ijk or suffices with 2D terms θ_ij + θ_ik + θ_jk. The Z normalization term in directed models creates multi-colliders when marginalized over children.
- Core assumption: Positive density; faithfulness (no cancellation of θ terms under marginalization).

### Mechanism 3
- Claim: Greedy selection with neural candidate pruning makes hyperedge search tractable.
- Mechanism: Step 1 uses MLP + ARCHIPELAGO XAI to score ~10 candidate hyperedges per variable from exponential space. Step 2 greedily adds hyperedges by MSE reduction, maintaining acyclicity via partial order matrix. Step 3 prunes low-variance additive terms (threshold 1.0e-4). The neural preselection is critical—without it, O(2^d) hyperedges are intractable.
- Core assumption: Gaussian noise (MSE = negative log-likelihood); neural XAI importance correlates with true causal hyperedges.

## Foundational Learning

- **Causal Additive Models (CAM)**
  - Why needed here: HCAM extends CAM's additive assumption to higher-order terms. Understanding CAM's three-step pipeline (candidate search, greedy selection, pruning) is prerequisite.
  - Quick check question: Given X_3 = f_1(X_1) + f_2(X_2) + ε, can you identify the causal direction if f_1, f_2 are nonlinear and ε is non-Gaussian?

- **Markov Equivalence Classes (MEC)**
  - Why needed here: HMECs generalize MECs. Without understanding that DAGs are only identifiable up to skeleton + colliders, the multi-collider generalization won't make sense.
  - Quick check question: Why can't PC or GES distinguish X→Y→Z from X←Y←Z from observational data alone?

- **Additive Noise Model Identifiability**
  - Why needed here: Theorems 2-4 extend Hoyer et al.'s ANM identifiability arguments to multi-parent cases via differential constraints.
  - Quick check question: In ANM Y = f(X) + ε with ε independent of X, why does linear-f + Gaussian ε prevent identifying causal direction?

## Architecture Onboarding

- **Component map:**
  Input: Data matrix X ∈ R^{n×d}
  ↓
  Step 1: Candidate Search
    - For each j: train MLP f_θ(X_{-j}) → X_j
    - Run ARCHIPELAGO → importance scores for hyperedge subsets
    - Select top-10 candidates per variable → Ĥ
  ↓
  Step 2: Greedy Selection
    - Initialize empty HDAG H = ∅, partial order matrix P
    - Repeat: train SIAN additive models for candidates in Ĥ
    - Select (S*, j*) = argmin MSE reduction (acyclic only)
    - Add to H, update P, replenish candidates if <5 remain
  ↓
  Step 3: Final Pruning
    - Train end-to-end SIAN with L1 regularization
    - Remove terms with variance < 1.0e-4
  ↓
  Output: HDAG H, structural functions {f_S→j}

- **Critical path:** Step 1 candidate quality determines everything. If true hyperedges aren't in top-10 candidates, Step 2 cannot recover them. The neural XAI bottleneck is the highest-risk component.

- **Design tradeoffs:**
  - More candidates per variable → better recall but slower (training SIAN models is expensive)
  - Lower pruning threshold → more false positives; higher → miss weak edges
  - 5-epoch early stopping in Step 2 → faster but noisier scores
  - Sample complexity: O(n^K) for K-th order interactions means 3D requires ~10× more samples than 2D

- **Failure signatures:**
  - Empty graph baseline outperforms learned structure (observed on 3D data) → insufficient samples or candidate search failed
  - SHD close to random DAG → acyclicity constraint too restrictive or MSE not discriminating
  - Only singleton hyperedges recovered → XAI missed interactions, returned to CAM regime

- **First 3 experiments:**
  1. **Sanity check:** Generate 1D CAM data (d=10, n=5000). Verify HCAM recovers standard CAM baseline. If SHD > 50, debug candidate search.
  2. **Interaction scaling:** Generate 2D data with known ground-truth hyperedges. Sweep n ∈ {1000, 5000, 10000}. Plot SHD vs n to estimate sample complexity.
  3. **Ablation:** Replace ARCHIPELAGO with random candidate selection. Expect dramatic SHD increase, confirming neural preselection necessity.

## Open Questions the Paper Calls Out

- **Open Question 1:** Can the theoretical and algorithmic framework for higher-order causal structure learning be extended to systems with latent variables or latent confounding?
  - Basis in paper: [explicit] The conclusion explicitly identifies "Extension to appropriately handling latent variables and latent confounding" as a "direction of serious interest" for future work.
  - Why unresolved: The current theoretical identifiability results and the HCAM algorithm rely on the assumption of "causal sufficiency" (fully observed variables), which is often violated in real-world biological and social systems.
  - What evidence would resolve it: A formal proof of identifiability for HDAGs with latent variables, or an algorithm that successfully recovers hypergraph structure in the presence of unobserved confounders.

- **Open Question 2:** What algorithmic advancements are required to reliably learn causal structures with 3rd-order or higher interactions?
  - Basis in paper: [inferred] The experiments section notes that on 3D datasets, "none of the algorithms we run are able to find empirical success over the zero baseline," indicating a failure to scale practically despite theoretical identifiability.
  - Why unresolved: The statistical complexity increases significantly with interaction order (noted in Section 3.2 regarding sample complexity), and the current greedy search heuristic combined with neural network estimation fails to capture these complex dependencies from finite samples.
  - What evidence would resolve it: An algorithm that achieves a Structural Hamming Distance (SHD) significantly lower than the empty graph baseline on synthetic 3D or 4D interaction datasets.

- **Open Question 3:** Does the empirical performance of HDAG learning degrade significantly under non-Gaussian noise, and can the algorithm be adapted to remain robust?
  - Basis in paper: [inferred] The methodology assumes Gaussian noise for algorithmic convenience (minimizing MSE), and Theorem 4 establishes specific theoretical overlaps under Gaussian assumptions, leaving the practical behavior under non-Gaussian noise unexplored.
  - Why unresolved: While the theoretical identifiability (Theorem 3) holds for general additive noise, the specific optimization landscape and convergence properties of the HCAM algorithm may rely heavily on the Gaussian likelihood assumption used in the SIAN integration.
  - What evidence would resolve it: Experimental results on synthetic data with non-Gaussian additive noise terms demonstrating whether the HCAM algorithm retains its ability to recover the correct hypergraph structure.

## Limitations

- The HMEC equivalence class concept is novel but lacks corpus validation - no other papers reference or build on this framework.
- The neural candidate selection step (ARCHIPELAGO) is a critical bottleneck that could fail silently if true hyperedges aren't ranked highly, yet the paper doesn't report candidate recall rates.
- The 3D interaction results show HCAM failing to beat an empty-graph baseline, suggesting fundamental sample complexity limitations that aren't fully addressed.

## Confidence

- **High confidence:** Theoretical identifiability results (Theorems 1-4) and additive structure assumptions
- **Medium confidence:** HCAM algorithm's practical performance, given the lack of ablation studies isolating the neural candidate selection's impact
- **Low confidence:** Scalability beyond 3D interactions due to the O(n^K) sample complexity constraint

## Next Checks

1. **Candidate recall analysis:** Report the fraction of true hyperedges appearing in the top-10 candidates per variable. If this drops below 50%, the neural XAI bottleneck is the primary failure mode.

2. **Sample complexity sweep:** For 2D data, systematically vary n from 1000 to 20000 and plot SHD vs sample size. This will quantify when higher-order interactions become identifiable.

3. **Ablation of neural preselection:** Replace ARCHIPELAGO with random candidate selection. If SHD increases by >50% on 2D data, this confirms the neural bottleneck's critical role.