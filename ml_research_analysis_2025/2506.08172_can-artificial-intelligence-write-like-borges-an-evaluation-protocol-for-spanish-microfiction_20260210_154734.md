---
ver: rpa2
title: Can Artificial Intelligence Write Like Borges? An Evaluation Protocol for Spanish
  Microfiction
arxiv_id: '2506.08172'
source_url: https://arxiv.org/abs/2506.08172
tags:
- literary
- evaluation
- microfictions
- texts
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study introduces GrAImes, a novel evaluation protocol for
  assessing the literary quality of Spanish microfictions, both human-written and
  AI-generated. Grounded in literary theory, GrAImes evaluates thematic coherence,
  textual clarity, interpretive depth, and aesthetic quality through a 15-question
  framework.
---

# Can Artificial Intelligence Write Like Borges? An Evaluation Protocol for Spanish Microfiction

## Quick Facts
- **arXiv ID:** 2506.08172
- **Source URL:** https://arxiv.org/abs/2506.08172
- **Reference count:** 40
- **Primary result:** GrAImes evaluation protocol shows good-to-acceptable reliability for assessing Spanish microfiction literary quality across human-written and AI-generated texts.

## Executive Summary
This study introduces GrAImes, a 15-question evaluation framework for assessing Spanish microfictions based on literary theory. The protocol evaluates thematic coherence, textual clarity, interpretive depth, and aesthetic quality through three dimensions: Story Overview, Technical, and Editorial. Experiments involved literary experts evaluating human-written texts and literature enthusiasts assessing AI-generated microfictions from ChatGPT-3.5 and a fine-tuned GPT-2 baseline (Monterroso). Results showed good to acceptable internal consistency, with expert-written texts receiving higher scores. AI-generated texts demonstrated slightly better commercial appeal but lower interpretive depth, highlighting the importance of structured frameworks and expert input in literary assessment.

## Method Summary
The study fine-tuned a GPT-2 Spanish model (DeepESP/gpt2-spanish) on 1,222 Spanish microfictions to create "Monterroso," then generated texts using identical prompt titles for comparison with ChatGPT-3.5 outputs. A 15-question Likert-scale questionnaire (GrAImes) was administered to two evaluator groups: literary experts (PhD holders) and literary enthusiasts. Internal consistency was validated using Cronbach's Alpha and inter-rater reliability was assessed through ICC and Kendall's W statistics. The protocol was tested on 6 microfictions (2 expert, 2 medium, 2 emerging human authors) plus AI-generated samples.

## Key Results
- Expert-written microfictions exhibited Cronbach's Alpha values of 0.75-0.89, indicating good to acceptable internal consistency
- ChatGPT-3.5 microfictions scored slightly higher on editorial and commercial appeal, while Monterroso showed better technical execution
- Individual question ICC analysis revealed Q13 ("Would you give it as a present?") had negative reliability, suggesting it introduces noise
- Evaluator stratification revealed distinct assessment profiles: experts focused on structural complexity while enthusiasts prioritized accessibility

## Why This Works (Mechanism)

### Mechanism 1: The Reception-Based Evaluator Stratification
IF literary assessment is inherently subjective and dependent on reader competence, THEN stratifying evaluators (Experts vs. Enthusiasts) creates a more valid multi-dimensional signal than a random sample. The protocol separates "technical/originality" assessment (Experts) from "accessibility/appeal" assessment (Enthusiasts), with experts providing high internal consistency on structural complexity while enthusiasts reflect market viability. Literary value is assumed to be a composite of technical mastery (requiring expertise to detect) and reader reception (requiring a general audience).

### Mechanism 2: Triangulation via Editorial Decomposition
IF "literariness" is an abstract construct, THEN decomposing it into three concrete dimensions (Story Overview, Technical, Editorial) with 15 questions reduces variance and improves reliability. The 15-question framework forces evaluators to convert holistic aesthetic impressions into specific judgments, allowing statistical validation of individual criteria rather than a vague total score. The three proposed dimensions from literary theory (verisimilitude, codification, deferred character) are assumed to sufficiently capture the editorial decision-making process.

### Mechanism 3: The Verisimilitude-Engagement Trade-off
IF AI models are optimized for general coherence (ChatGPT), they score higher on "credibility" and "commercial appeal," whereas models fine-tuned on specific literary corpora (Monterroso) may score higher on "reader participation" (complexity) but lower on polish. General LLMs produce "smooth" text that satisfies casual readers (high enthusiasm scores on Q5/Q11), while niche models produce "rougher" text that demands active reconstruction (high scores on Q6). "Credibility" in this context correlates with generic narrative tropes, while "participation" correlates with the specific structural gaps defined in microfiction theory.

## Foundational Learning

- **Concept: Microfiction as a "Gapped" Structure**
  - Why needed here: Standard narrative metrics look for coherence. Microfiction specifically relies on *disarticulating* the plot and forcing the reader to fill gaps. Understanding this is prerequisite to interpreting why "reader participation" (Q6) is a core metric.
  - Quick check question: Does a high "credibility" score indicate a good microfiction, or does it potentially mean the text failed to challenge the reader (lacked gaps)?

- **Concept: Reception Theory (Rezeptionsästhetik)**
  - Why needed here: The paper grounds its evaluation in the idea that a text's meaning is co-produced by the reader. This explains why the protocol includes diverse evaluator groups rather than seeking a single "objective" truth.
  - Quick check question: Why might an "Expert" and an "Enthusiast" give drastically different scores to the same text's "interpretive depth"?

- **Concept: Inter-rater Reliability (Cronbach’s Alpha & ICC)**
  - Why needed here: The paper claims validity based on "internal consistency." You need to understand that these statistics measure if the *questions* are measuring the same underlying construct or if the judges are agreeing, not just the average score.
  - Quick check question: If Q13 ("Would you give it as a present?") has a negative ICC, what does that imply about the question's utility in the protocol?

## Architecture Onboarding

- **Component map:** Input Layer (Textual Stimuli) -> Processing Layer (GrAImes 15-item questionnaire) -> Agent Layer (Stratified Evaluators) -> Analysis Layer (Statistical Validation)
- **Critical path:** Define Dimensions -> Formulate 15 Questions -> Recruit/Stratify Evaluators -> Collect Data -> Filter Noise (remove negative ICC questions) -> Calculate Alpha/W -> Interpret Divergence (Expert vs. Enthusiast)
- **Design tradeoffs:**
  - Specificity vs. Generalizability: Protocol is tuned for Spanish Microfiction; applying to novels or poetry would likely break validity of "Technical" questions
  - Expert vs. Scale: Experts provide high validity but are hard to scale (N=5 in study); Enthusiasts scale better but introduce higher variance (SD ≈ 1.4)
- **Failure signatures:**
  - The "Generic" Trap: AI generates cliché story; Enthusiasts rate high (familiarity), Experts rate low (lack of originality)
  - The "Ambiguity" Noise: Questions like "Does it propose a new vision of reality?" showed low ICC (0.29) due to subjectivity
  - Sample Instability: Study notes Cronbach's Alpha is sensitive to small sample size, potentially biasing "Good/Acceptable" consistency claims
- **First 3 experiments:**
  1. Baseline Validation: Run 15 questions on known "Canonical" vs. "Amateur" human texts to verify if Cronbach's Alpha replicates "Good" consistency
  2. Question Pruning: Calculate ICC for each question; remove/refine questions with ICC < 0.5 (e.g., Q13) to tighten protocol
  3. Model A/B Testing: Generate texts using general LLM (GPT-4) and literary-fine-tuned LLM; compare Q6 (Participation) vs. Q11 (Commercial Appeal) gap

## Open Questions the Paper Calls Out

**Open Question 1:** Can the GrAImes evaluation protocol maintain its validity and internal consistency when applied to literary genres other than microfiction or to texts in languages other than Spanish? The authors state that "further experiments are required to test the applicability of GrAImes to other literary genres and other languages." Cross-genre and cross-cultural validity remains unproven.

**Open Question 2:** How do AI-generated self-assessments of literary quality using GrAImes compare to evaluations provided by human literary experts and enthusiasts? A forthcoming experiment will apply the protocol to chatbot-generated microfictions, incorporating expert, reader, and chatbot self-assessments. The capacity for an AI to accurately critique its own literary output has not yet been tested.

**Open Question 3:** Does increasing the sample size of evaluators stabilize the statistical measures (ICC, Cronbach's alpha) that showed instability in the initial small-sample study? The paper notes that "the experimental dataset of our experiments is small, and some statistical methods used for validation (ICC, Cronbach's alpha, and Kendall's W) are sensitive to the sample size." It is unclear if "questionable" or "unacceptable" internal consistency scores were artifacts of the small number of evaluators.

## Limitations

- Small sample sizes (N=5 experts, N=10 enthusiasts) may artificially inflate reliability metrics
- Specific prompt words used for AI generation were not disclosed, preventing exact replication of evaluated texts
- Protocol's generalizability beyond Spanish microfiction remains unclear

## Confidence

- **High Confidence:** Internal consistency results (Cronbach's Alpha 0.75-0.89) for expert-written texts and basic framework of stratified evaluation are well-supported
- **Medium Confidence:** Claim that ChatGPT-3.5 shows "slightly better commercial appeal" is supported but may reflect prompt bias rather than inherent model superiority
- **Low Confidence:** Specific trade-off between "verisimilitude" and "reader participation" across models is weakly supported, as individual question ICC values suggest some items introduce noise

## Next Checks

1. Test the 15-question framework on a known set of canonical vs. amateur human microfictions to verify if the reported "Good" consistency (Cronbach's Alpha) replicates outside the study's corpus
2. Calculate and prune individual question ICC values, removing items with ICC < 0.5 (notably Q13) to strengthen the protocol's signal-to-noise ratio
3. Generate texts using both a general LLM (GPT-4) and a literary-fine-tuned model with identical prompts, then specifically compare Q6 (Participation) vs. Q11 (Commercial Appeal) scores to validate the proposed verisimilitude-engagement trade-off mechanism