---
ver: rpa2
title: Contrastive Learning-Enhanced Large Language Models for Monolith-to-Microservice
  Decomposition
arxiv_id: '2502.04604'
source_url: https://arxiv.org/abs/2502.04604
tags:
- microservices
- decomposition
- applications
- code
- classes
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the challenge of decomposing monolithic applications
  into microservices by introducing MonoEmbed, a Language Model-based approach. The
  method leverages state-of-the-art Large Language Models (LLMs) and representation
  learning techniques to generate embedding vectors for monolithic components, which
  are then clustered to form microservices.
---

# Contrastive Learning-Enhanced Large Language Models for Monolith-to-Microservice Decomposition

## Quick Facts
- arXiv ID: 2502.04604
- Source URL: https://arxiv.org/abs/2502.04604
- Authors: Khaled Sellami; Mohamed Aymen Saied
- Reference count: 40
- Key outcome: Introduces MonoEmbed, a LLM-based approach that fine-tunes models with contrastive learning to generate superior embeddings for decomposing monoliths into cohesive microservices

## Executive Summary
This paper tackles the challenge of decomposing monolithic applications into microservices by introducing MonoEmbed, a Language Model-based approach. The method leverages state-of-the-art Large Language Models (LLMs) and representation learning techniques to generate embedding vectors for monolithic components, which are then clustered to form microservices. By evaluating various pre-trained models and applying fine-tuning techniques such as Contrastive Learning and Low Rank Adaptation (LoRA), MonoEmbed optimizes these representations for microservice partitioning. The evaluation showcases that fine-tuned models significantly improve the quality of representation vectors compared to pre-trained models and traditional representations.

## Method Summary
MonoEmbed generates microservice decompositions by first creating embedding vectors for Java classes using pre-trained LLMs, then fine-tuning these models with triplet contrastive learning to optimize for semantic similarity within microservices. The approach uses a dataset of approximately 340K triplets (anchor, positive, negative) mined from GitHub microservice repositories. During fine-tuning, models learn to place classes from the same microservice close together in embedding space while pushing apart classes from different services. The fine-tuned embeddings are normalized using z-score standardization and clustered using algorithms like Affinity Propagation to form microservice boundaries. The method specifically employs LoRA for fine-tuning larger LLMs while using full fine-tuning for smaller models, with contrastive learning forcing the model to learn domain/business logic patterns rather than superficial code similarities.

## Key Results
- Fine-tuned models achieved embedding quality scores of 0.5626, outperforming pre-trained VoyageAI's score of 0.6387
- MonoEmbed scored highest in cohesion metrics (CHM, CHD, COV) across multiple evaluation applications
- LLMs like VoyageAI, NVEmbed, and SFR Mistral generated better representation vectors than traditional static analysis methods
- Triplet contrastive learning with hard negatives from the same application significantly improved embedding quality compared to pre-trained models

## Why This Works (Mechanism)

### Mechanism 1: Contrastive Learning with Triplet Loss
- Claim: Fine-tuning LLMs with contrastive learning improves embedding quality for decomposition by teaching models to prioritize semantic similarity over syntactic similarity.
- Mechanism: The model receives triplets (anchor, positive, negative) where anchor and positive belong to the same microservice, and the negative is a "hard negative" from the same application. The triplet loss L(a,p,n) = max(0, ||a-p||₂ - ||a-n||₂ + α) forces the model to learn domain/business logic patterns rather than superficial code similarities.
- Core assumption: Classes within the same microservice share semantic/domain patterns that can be learned, and these patterns generalize across applications.
- Evidence anchors: [abstract] "MonoEmbed employs contrastive learning to fine-tune pre-trained models, optimizing their embeddings for microservice partitioning." [section 4.3] "Using hard negatives in triplet CL has been shown to enhance the training... encourages the model to focus on the semantics related to microservices and business logic instead."
- Break condition: Performance degrades when training triplets come from applications with inconsistent microservice design patterns, or when hard negatives are not truly semantically similar to anchors.

### Mechanism 2: LLM-Based Code Representation
- Claim: Pre-trained LLMs generate richer representations of source code than traditional static/dynamic analysis methods, enabling better decomposition.
- Mechanism: Transformer-based LLMs encode class-level source code into high-dimensional embedding vectors. These capture contextual, semantic, and structural relationships that go beyond explicit call graphs or AST features.
- Core assumption: The pre-trained knowledge in LLMs (including code-specific models) contains transferable patterns relevant to identifying bounded contexts.
- Evidence anchors: [abstract] "MonoEmbed leverages state-of-the-art Large Language Models (LLMs) and representation learning techniques to generate representation vectors for monolithic components." [section 5.2.5] "LLM based encoders, like VoyageAI, NVEmbed and SFR Mistral, generate better representation vectors in the decomposition context than the methods employed in related works (e.g. static analysis, Code2Vec, CodeBERT)."
- Break condition: Fails when code contains domain-specific idioms not well-represented in pre-training data, or when class-level granularity loses important inter-method relationships.

### Mechanism 3: Clustering-Based Partitioning
- Claim: Normalized LLM embeddings, when clustered using appropriate algorithms, produce cohesive and balanced microservice boundaries.
- Mechanism: Embeddings are z-score standardized, then partitioned using clustering algorithms. Affinity Propagation performed best on average by identifying exemplars that represent microservice centers without requiring a pre-specified cluster count.
- Core assumption: The embedding space distance correlates with microservice boundary appropriateness—closer embeddings should be in the same service.
- Evidence anchors: [section 5.4.3] "Affinity Propagation provides on average better results while K-Means and Hierarchical have higher scores when given the number of microservices." [section 5.5.3] "MonoEmbed scored highest in CHM, CHD, and COV... demonstrating its ability to group cohesive classes across multiple aspects."
- Break condition: Clustering fails when embedding quality is poor, or when damping parameters for Affinity Propagation fall outside the (0.5, 0.8) range where performance degrades.

## Foundational Learning

- Concept: **Contrastive Learning (Triplet Loss)**
  - Why needed here: Understanding how the model learns to separate microservice classes requires grasping how triplet loss shapes embedding space.
  - Quick check question: Given two similar-looking code snippets from different microservices, how does the model learn to place them in different regions of embedding space?

- Concept: **Transformer Embeddings vs. Static Analysis**
  - Why needed here: The paper's core claim depends on LLM embeddings outperforming traditional representations; understanding this distinction is critical.
  - Quick check question: Why might a class with near-identical syntax to another class (e.g., service entry points) need to be placed in different embedding regions?

- Concept: **Clustering Evaluation Metrics (CHM, CHD, BCP, ICP)**
  - Why needed here: Evaluating decomposition quality requires understanding what each metric measures (cohesion vs. coupling vs. coverage).
  - Quick check question: If a decomposition has high cohesion but also high inter-service coupling (ICP), is it a good decomposition?

## Architecture Onboarding

- Component map: Triplet dataset creation -> Model selection -> Contrastive fine-tuning with LoRA -> Source code input -> Fine-tuned LLM encoder -> Z-score normalization -> Clustering (Affinity Propagation) -> Decomposition output

- Critical path:
  1. Acquire/create triplet dataset from microservice applications (exclude evaluation targets)
  2. Select base model (LLM2Vec, SFR-Mistral, or UnixCoder performed best)
  3. Fine-tune with LoRA (learning rate ~2e-5 for smaller models) using triplet loss
  4. For inference: encode classes, normalize, cluster with Affinity Propagation (damping 0.5-0.8)

- Design tradeoffs:
  - **Model size vs. performance**: LLM2Vec-8B outperformed smaller ET models, but requires more compute for both training and inference
  - **Dataset size vs. returns**: Table 5 shows diminishing returns beyond 34K samples; scaling from 34K→340K had less impact than 3K→34K
  - **Clustering algorithm choice**: Affinity Propagation doesn't require specifying cluster count but has slightly lower peak performance than K-Means with known K

- Failure signatures:
  - High embedding quality score (>0.6) indicates poor class separation in embedding space
  - High ICP in decompositions suggests excessive inter-microservice coupling
  - Small applications (e.g., JPetStore with 43 classes) may not benefit from LLM-based approaches; simpler methods may suffice

- First 3 experiments:
  1. **Baseline validation**: Run pre-trained VoyageAI or NVEmbed on your monolith, cluster with Affinity Propagation, measure CHM/CHD/ICP to establish baseline.
  2. **Fine-tuning impact test**: Fine-tune UnixCoder (smaller, faster) on 10K triplets from your domain, compare embedding quality score against baseline.
  3. **Clustering algorithm sweep**: Test Affinity Propagation (damping 0.5, 0.6, 0.7), K-Means (K=known count), and HDBSCAN on same embeddings to identify best performer for your application scale.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can MonoEmbed embeddings enhance Graph Neural Network (GNN)-based decomposition methods when used as node features?
- Basis in paper: [explicit] The authors state, "In future work, we would like to experiment with integrating our models with existing decomposition methods," specifically suggesting that "GNN based methods... can be extended by using ME-LLM2Vec as the encoder."
- Why unresolved: The current evaluation isolates MonoEmbed as a standalone clustering approach and does not test its interoperability or performance uplift when injected into other architectures like CoGCN or CHGNN.
- What evidence would resolve it: An experimental study measuring the performance delta of GNN-based approaches when their default encoders are replaced by fine-tuned MonoEmbed models.

### Open Question 2
- Question: How does the approach perform when applied to method-level granularity or non-code modalities?
- Basis in paper: [explicit] The conclusion notes, "In future work, we would like to... explore new granularities and modalities," while the discussion mentions the potential to incorporate "unstructured inputs (e.g., resources, configurations, documentation)."
- Why unresolved: The current study limits input to class-level Java source code; method-level clustering or the inclusion of configurations/docs remains untested.
- What evidence would resolve it: Evaluation results from applying the fine-tuned models to method-level decomposition tasks and datasets containing mixed modalities (code + configuration files).

### Open Question 3
- Question: Can the fine-tuning process be modified to minimize inter-service coupling (ICP) while maintaining high cohesion?
- Basis in paper: [inferred] The results section notes that while MonoEmbed achieved the best aggregate score, it resulted in a "higher-than-average ICP score suggests increased inter-microservice coupling."
- Why unresolved: The current contrastive learning objective focuses on semantic similarity (grouping related classes) but lacks a mechanism to explicitly penalize structural dependencies (calls) crossing service boundaries.
- What evidence would resolve it: A modified loss function or post-processing step that reduces the Inter-Call Percentage (ICP) metric to levels comparable with dynamic analysis baselines like Mono2Micro.

## Limitations
- The performance comparison contains a critical inconsistency showing VoyageAI's pre-trained score as better than the fine-tuned models despite claiming the opposite
- Training dataset composition and exact LoRA hyperparameters remain unspecified, limiting reproducibility
- The reliance on GitHub repositories with specific star counts and directory structures may introduce selection bias

## Confidence
- **High confidence**: The mechanism of using triplet contrastive learning with hard negatives from the same application to improve semantic embeddings
- **Medium confidence**: The superiority of LLM-based embeddings over traditional static analysis methods
- **Medium confidence**: The overall decomposition framework effectiveness

## Next Checks
1. **Replicate the embedding quality comparison** by fine-tuning VoyageAI with the same triplet methodology to ensure fair comparison between pre-trained and fine-tuned models
2. **Test domain transfer capability** by evaluating MonoEmbed on applications from different domains than the training data to assess generalization beyond the specific GitHub repositories used
3. **Analyze coupling metrics more deeply** by examining specific inter-service dependencies in high-ICP decompositions to understand whether increased coupling represents genuine business logic relationships or suboptimal boundaries