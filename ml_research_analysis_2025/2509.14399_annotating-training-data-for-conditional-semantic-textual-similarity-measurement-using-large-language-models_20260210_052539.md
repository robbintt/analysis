---
ver: rpa2
title: Annotating Training Data for Conditional Semantic Textual Similarity Measurement
  using Large Language Models
arxiv_id: '2509.14399'
source_url: https://arxiv.org/abs/2509.14399
tags:
- condition
- similarity
- sentence
- conditions
- c-sts
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses issues in the Conditional Semantic Textual
  Similarity (C-STS) dataset, including ambiguous conditions and inaccurate similarity
  ratings. The authors use Large Language Models (LLMs) to refine condition statements
  and re-annotate similarity scores.
---

# Annotating Training Data for Conditional Semantic Textual Similarity Measurement using Large Language Models

## Quick Facts
- arXiv ID: 2509.14399
- Source URL: https://arxiv.org/abs/2509.14399
- Authors: Gaifan Zhang; Yi Zhou; Danushka Bollegala
- Reference count: 29
- Primary result: 5.4% statistically significant improvement in Spearman correlation for C-STS task

## Executive Summary
This paper addresses data quality issues in the Conditional Semantic Textual Similarity (C-STS) dataset, specifically ambiguous conditions and inaccurate similarity ratings. The authors employ Large Language Models (LLMs) to refine condition statements for clarity and re-annotate similarity scores using multiple LLMs. By combining these LLM ratings with human annotations and training a supervised model on the cleaned dataset, they achieve a 5.4% statistically significant improvement in performance compared to previous methods.

## Method Summary
The method involves two main data cleaning steps: (1) using GPT-4o to refine condition statements by removing stopwords, standardizing phrasing, and improving specificity, and (2) using GPT-4o and Claude-3.7-Sonnet to re-annotate similarity ratings (1-5) for sentence pairs under the modified conditions. The final rating is computed by averaging the original human rating with both LLM ratings and rounding to the nearest integer. A Supervised Non-Linear Projection (SNPro) model is then trained on this cleaned dataset using NV-Embed-v2 embeddings with condition-aware prompts, achieving improved Spearman correlation on the test set.

## Key Results
- 5.4% statistically significant improvement in Spearman correlation compared to previous methods
- Full training dataset (75%) cleaned using LLMs versus 15% manually cleaned by Tu et al. (2024)
- LLM-based condition refinement and rating aggregation both contribute to performance gains

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Refining condition statements improves consistency and interpretability, leading to better model performance
- Mechanism: GPT-4o rewrites ambiguous, subjective, or inconsistently phrased conditions to be clear, specific, and uniform (e.g., "type of animal" instead of "The animal"), reducing noise in the training signal
- Core assumption: GPT-4o's prompt strategy is sufficient to correct human errors without introducing new biases that degrade data quality
- Evidence anchors: [abstract], [section 2.1], weak corpus evidence
- Break condition: If GPT-4o systematically alters semantic meaning in ways contradicting human intuition for large portions of the dataset

### Mechanism 2
- Claim: Aggregating ratings from multiple LLMs and human annotators reduces label noise and increases reliability
- Mechanism: Averaging ratings from GPT-4o, Claude-3.7-Sonnet, and original human annotators cancels out uncorrelated errors and outliers, creating more robust target labels
- Core assumption: Errors made by human annotators and two different LLMs are not systematically aligned
- Evidence anchors: [abstract], [section 2.2], weak corpus evidence
- Break condition: If LLMs share strong systematic biases that are more incorrect than human noise, averaging could amplify these biases

### Mechanism 3
- Claim: Larger high-quality LLM-cleaned training datasets enable better learning of generalizable condition-aware embeddings
- Mechanism: Scaling data cleaning from 15% (Tu et al.) to 75% (14,176 instances) provides more accurate examples for supervised learning, allowing SNPro to learn better transformations
- Core assumption: LLM cleaning produces sufficient quality that scale benefits outweigh residual errors
- Evidence anchors: [abstract], [section 1, Table 1], corpus evidence from related SNPro and PoLi-RL papers
- Break condition: If LLM-generated labels are consistently lower quality than human labels, scaling with this data might not improve performance

## Foundational Learning

- **Concept: Conditional Semantic Textual Similarity (C-STS)**
  - Why needed here: This is the core task where similarity judgment depends on specific conditions; understanding C-STS is essential to grasp the paper's contribution
  - Quick check question: Given "A dog chases a ball" and "A cat chases a mouse," how would their similarity rating differ under condition A ("type of animal") vs. condition B ("action being performed")?

- **Concept: LLM-as-a-Judge / Annotator**
  - Why needed here: The paper's primary method uses LLMs not just as models but as judges to refine conditions and re-annotate labels; understanding LLM annotation strengths and limitations is critical
  - Quick check question: What are two potential risks of using an LLM to re-annotate a dataset intended for training a different, smaller model?

- **Concept: Ensembling / Rating Aggregation**
  - Why needed here: The paper averages outputs from two LLMs and original human annotator as a practical ensemble technique; understanding why averaging reduces noise is key
  - Quick check question: Why is averaging three different ratings (one human, two LLMs) likely better than using a single LLM's rating?

## Architecture Onboarding

- **Component map:** Sentence Pair + Condition -> Condition Modifier (GPT-4o) -> Modified Condition -> Re-annotators (GPT-4o & Claude) -> Aggregated Rating -> Training Set
- **Critical path:** Raw data -> condition refinement -> re-annotation -> embedding generation -> SNPro training -> evaluation; failure in condition refinement propagates noise through entire pipeline
- **Design tradeoffs:** Cost vs. scale (LLM vs. manual annotation); specificity vs. generality in conditions (overly specific conditions might not generalize)
- **Failure signatures:** Low Spearman correlation (check data cleaning first); overfitting to condition phrasing (poor performance on original-style conditions)
- **First 3 experiments:**
  1. Ablation on condition modification: Train SNPro using original vs. LLM-modified conditions (with/without stopword removal) to isolate condition refinement impact
  2. Ablation on rating aggregation: Train models using only human labels, only GPT-4o, only Claude, and combinations to prove averaging is optimal
  3. Scale comparison: Train SNPro on small human-reannotated validation set vs. full LLM-cleaned training set to demonstrate benefit of scaling data cleaning

## Open Questions the Paper Calls Out

- **Question:** Does LLM-based data cleansing effectively transfer to morphologically rich or low-resource languages for C-STS tasks?
  - Basis in paper: [explicit] Authors state study limited to English due to data availability and identify creating multilingual datasets as important future work
  - Why unresolved: Current work relies on English-specific phrasing and LLM capabilities; unknown if prompt strategy works across different linguistic typologies
  - What evidence would resolve it: Performance metrics (Spearman correlation) of models trained on LLM-cleaned datasets for non-English languages compared to English baseline

- **Question:** To what extent does LLM-based re-annotation amplify or mitigate social biases present in original C-STS conditions?
  - Basis in paper: [explicit] Authors acknowledge LLMs exhibit social biases and state influence on data quality regarding social bias is not evaluated
  - Why unresolved: Replacing human annotations with LLM averages could encode or worsen systemic biases related to sensitive attributes in conditions
  - What evidence would resolve it: Bias audit comparing similarity score distributions across sensitive demographic groups in original vs. LLM-re-annotated datasets

- **Question:** Can performance gains from data cleansing be further improved by addressing underlying distributional imbalance of condition types?
  - Basis in paper: [explicit] Authors note that despite re-annotation, imbalanced conditions still exist as overall distribution hasn't changed (e.g., "number of #" and "type of #" dominate)
  - Why unresolved: Paper demonstrates cleaning improves performance, but dominance of specific condition types may limit model's ability to generalize to rarer conditions
  - What evidence would resolve it: Comparative study training models on re-annotated dataset with and without condition-balancing techniques to isolate impact of distribution vs. quality

## Limitations
- Reliance on LLM judgment for data cleaning introduces potential for systematic bias not present in human annotation
- Long-term generalization to conditions outside training distribution remains uncertain
- Specific prompt engineering and model choices (GPT-4o, Claude-3.7-Sonnet) may have contributed substantially to success, raising reproducibility questions

## Confidence

- **High Confidence:** The 5.4% statistically significant improvement in Spearman correlation is well-supported by experimental results and methodology; ablation studies provide strong evidence
- **Medium Confidence:** Assumption that LLM-generated labels maintain sufficient quality when scaling from 15% to 75% of dataset; results support this but lacks extensive error analysis
- **Low Confidence:** Long-term robustness to conditions not seen during training; whether improvements would generalize to other embedding models beyond NV-Embed-v2

## Next Checks
1. **Error Analysis:** Manually examine 50-100 instances where LLM annotations differed substantially from human annotations to identify systematic patterns or failures
2. **Generalization Test:** Evaluate trained model on conditions deliberately excluded or minimally represented in training set to assess robustness to novel condition types
3. **Prompt Sensitivity Analysis:** Reproduce main results using different prompt variations (temperature settings, different wording) to determine sensitivity to prompt engineering choices