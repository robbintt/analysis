---
ver: rpa2
title: 'Deep Learning and Foundation Models for Weather Prediction: A Survey'
arxiv_id: '2501.06907'
source_url: https://arxiv.org/abs/2501.06907
tags:
- weather
- arxiv
- learning
- data
- forecasting
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper presents a comprehensive survey of deep learning and
  foundation models for weather prediction, categorizing existing models into three
  training paradigms: deterministic predictive learning, probabilistic generative
  learning, and pre-training with fine-tuning. The survey systematically reviews model
  architectures including Transformers, Graph Neural Networks, RNNs/CNNs, Mamba, GANs,
  and diffusion models, analyzing their strengths, limitations, and applications across
  various weather domains such as precipitation, air quality, sea surface temperature,
  flood, drought, tropical storms, and wildfires.'
---

# Deep Learning and Foundation Models for Weather Prediction: A Survey

## Quick Facts
- **arXiv ID**: 2501.06907
- **Source URL**: https://arxiv.org/abs/2501.06907
- **Reference count**: 40
- **Key outcome**: Presents a comprehensive survey categorizing DL weather models into three training paradigms (deterministic, probabilistic, foundation) and outlines ten critical research directions focusing on trustworthy AI and multi-modal learning.

## Executive Summary
This survey provides a systematic taxonomy of deep learning and foundation models for weather prediction, organizing existing approaches into three training paradigms: deterministic predictive learning, probabilistic generative learning, and pre-training with fine-tuning. The paper reviews major model architectures including Transformers, Graph Neural Networks, RNNs/CNNs, Mamba, GANs, and diffusion models, analyzing their strengths and limitations across various weather domains. The authors provide extensive resources including benchmark datasets and open-source code repositories, while highlighting recent advances like GenCast achieving 97.4% accuracy on weather prediction targets. The survey emphasizes the trade-offs between different modeling approaches in terms of accuracy, efficiency, uncertainty quantification, and adaptability.

## Method Summary
This is a systematic review paper that categorizes DL weather prediction models into three paradigms: deterministic predictive learning (minimizing point-wise loss like MSE), probabilistic generative learning (using diffusion/GANs for ensemble sampling), and foundation models (pre-training on massive datasets followed by fine-tuning). The survey identifies primary data sources including ERA5 reanalysis data and WeatherBench 2 benchmark, while comparing models based on accuracy metrics (RMSE), skill scores, uncertainty quantification, and inference efficiency. The paper defines the methodological framework for evaluating these approaches and provides curated resources through a GitHub repository for reproducibility.

## Key Results
- Presents a comprehensive taxonomy of DL weather models organized into three training paradigms
- Reviews major architectures including Transformers, Graph Neural Networks, and diffusion models with their applications across precipitation, air quality, sea surface temperature, flood, drought, tropical storms, and wildfires
- Highlights GenCast achieving 97.4% accuracy on weather prediction targets compared to traditional models
- Identifies ten critical research directions focusing on trustworthy AI, retrieval-augmented foundation models, and multi-modal learning
- Provides extensive resources including benchmark datasets (WeatherBench 2) and open-source code repositories

## Why This Works (Mechanism)

### Mechanism 1: Spatiotemporal Dependency Approximation
Deep learning models approximate weather evolution by learning complex spatiotemporal dependencies from historical data, potentially bypassing the computational cost of solving differential equations directly. Architectures like Transformers and Graph Neural Networks replace numerical solvers by capturing long-range spatial interactions and local-global dynamics through self-attention and message passing. The core assumption is that historical observations contain sufficient implicit information to approximate Navier-Stokes equations without explicit physical constraints. This mechanism fails when predicting rare extreme weather or Out-of-Distribution events not well-represented in training history.

### Mechanism 2: Probabilistic Diffusion for Uncertainty Quantification
Probabilistic generative models, specifically diffusion models, resolve the "blurry prediction" issue inherent in deterministic MSE-trained models by learning the full probability distribution of future states. Instead of regressing to a mean value, diffusion models learn to denoise random Gaussian noise into weather states conditioned on past observations, allowing them to generate ensemble forecasts that capture both aleatoric (data) and epistemic (model) uncertainty. The core assumption is that the forward diffusion process effectively destroys structure such that the reverse process learns the true data distribution. Unconstrained diffusion models might generate physically implausible predictions.

### Mechanism 3: Transfer Learning via Foundation Pre-training
Pre-training on massive, heterogeneous datasets allows models to learn universal atmospheric representations that can be efficiently adapted for specific downstream tasks. Foundation models use Masked Autoencoder or predictive pre-training on diverse climate variables to build a robust prior knowledge base, reducing the data requirement for specific tasks like air quality prediction or downscaling. The core assumption is that physical laws governing atmospheric dynamics are consistent across different spatial/temporal scales, allowing features learned from one dataset to transfer to another. Retrieval-Augmented Generation can further enhance these foundations by integrating external knowledge.

## Foundational Learning

- **Concept: Reanalysis Data (ERA5)**
  - **Why needed here**: This is the standard "ground truth" for training and benchmarking global weather models, providing consistent, gridded global coverage unlike sparse station data.
  - **Quick check question**: Can you distinguish between "station-based observation data" (sparse, precise) and "gridded reanalysis data" (complete, modeled)?

- **Concept: Autoregressive Rollout**
  - **Why needed here**: Medium-range forecasting typically uses the model's own previous output as input, making error accumulation understanding critical.
  - **Quick check question**: Why does minimizing Mean Squared Error (MSE) in a single-step loss function often lead to blurry outputs over long autoregressive rollouts?

- **Concept: Aleatoric vs. Epistemic Uncertainty**
  - **Why needed here**: The survey highlights that deterministic models fail to quantify uncertainty, distinguishing between noise in the data (aleatoric) and model ignorance (epistemic) is key to choosing between deterministic and generative architectures.
  - **Quick check question**: Which type of uncertainty does a diffusion model primarily address that a standard deterministic Transformer cannot?

## Architecture Onboarding

- **Component map**: Gridded Data (ERA5) or Station Data -> Patching (ViT) or Graph Construction (GNN) -> Transformer Blocks (Attention) / GNN Layers (Message Passing) / ODE Solvers (Physics-AI) -> Projection back to weather variables -> Future State (Deterministic) or Probability Distribution (Generative)

- **Critical path**: Data Preprocessing (Normalizing ERA5) -> Backbone Selection (Transformer for global scale, GNN for mesh dynamics) -> Training Objective (MSE vs. Diffusion Loss) -> Autoregressive Inference

- **Design tradeoffs**:
  - Transformers vs. GNNs: Transformers are efficient for global grids but may struggle with mesh irregularity; GNNs handle arbitrary meshes better but can be computationally heavier
  - Deterministic vs. Probabilistic: Deterministic models are fast and accurate for general patterns but fail at extreme events and uncertainty quantification; Probabilistic models capture uncertainty but are computationally expensive

- **Failure signatures**:
  - Blurry predictions: Indicates the model is averaging possibilities rather than resolving distinct features
  - Physically implausible outputs: Common in unconstrained generative models; requires "Physics-AI" or constraints
  - Error Accumulation: Forecasts diverge rapidly from reality after a few steps in autoregressive models

- **First 3 experiments**:
  1. Baseline Implementation: Implement a standard ConvLSTM or U-Net on a subset of ERA5 data to establish a performance baseline
  2. Backbone Upgrade: Replace the backbone with a Vision Transformer (ViT) or Swin Transformer to measure the accuracy improvement in capturing long-range dependencies
  3. Generative Integration: Implement a conditional Diffusion Model to compare the "sharpness" and uncertainty quantification against the deterministic baseline

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the Information Bottleneck (IB) principle be effectively adapted to enhance the explainability of deep learning models in weather modeling?
- Basis in paper: The authors explicitly advocate exploring how the information bottleneck method can enhance the explainability of weather modeling, as weather data inherently constitute time series.
- Why unresolved: Current deep learning models act as "black boxes," and while tools like SHAP and LIME exist, the specific application of IB to compress spatiotemporal weather dynamics into interpretable latent representations remains unexplored.
- What evidence would resolve it: A model architecture utilizing IB that yields latent features corresponding to physical weather patterns, validated by higher interpretability scores without degrading predictive accuracy.

### Open Question 2
- Question: How can Retrieval-Augmented Generation (RAG) be leveraged to improve weather prediction accuracy, particularly for extreme events?
- Basis in paper: Section notes that RAG's application to weather and climate modeling remains underexplored and suggests it could fetch historical weather patterns similar to the current state to refine predictions.
- Why unresolved: Generative models struggle with the rarity of extreme events; utilizing RAG to inject historical analogs into the generation process is a proposed solution that currently lacks implementation frameworks.
- What evidence would resolve it: A diffusion or foundation model that integrates a retrieval mechanism for historical data, demonstrating improved skill scores on extreme weather benchmarks compared to non-retrieval baselines.

### Open Question 3
- Question: How can deep learning models be architected to handle training data with varying spatial and temporal resolutions natively?
- Basis in paper: The paper highlights that an important challenge is to build models that can handle training data of varying resolutions and also reliably predict at a different resolution.
- Why unresolved: Weather data sources (stations, radar, satellites) have vastly different granularities, and current models typically require homogenization which loses information.
- What evidence would resolve it: A flexible architecture that ingests multi-resolution inputs directly and outperforms models trained on regridded data.

## Limitations
- The survey focuses on model architectures and paradigms without extensive empirical validation of real-world deployment constraints and computational costs
- Claims about foundation models' transferability assume consistent atmospheric physics across scales, which may not hold for highly localized phenomena or rare extreme events
- Discussion of multi-modal learning and retrieval-augmented generation for weather prediction is speculative, lacking substantial empirical validation in operational contexts

## Confidence
- **High Confidence**: The taxonomy of three training paradigms and architectural overview of Transformers, GNNs, and diffusion models are well-supported by cited literature and benchmark comparisons
- **Medium Confidence**: Claims about foundation models' transferability and efficiency gains assume consistent atmospheric physics across scales, which may not hold for localized phenomena
- **Low Confidence**: The survey's discussion of multi-modal learning and RAG for weather prediction is speculative, as these areas lack substantial empirical validation in operational contexts

## Next Checks
1. **Benchmark Verification**: Replicate Pangu-Weather or GenCast inference on WeatherBench 2 to confirm reported skill scores and RMSE metrics against IFS HRES baselines
2. **Computational Cost Analysis**: Quantify the GPU hours and memory requirements for fine-tuning foundation models (e.g., Aurora, Prithvi WxC) on downstream tasks like air quality prediction or flood forecasting
3. **Extreme Event Generalization**: Test deterministic and probabilistic models (e.g., FourCastNet vs. GenCast) on out-of-distribution extreme weather scenarios to validate uncertainty quantification claims and identify failure modes