---
ver: rpa2
title: 'MedSpeak: A Knowledge Graph-Aided ASR Error Correction Framework for Spoken
  Medical QA'
arxiv_id: '2602.00981'
source_url: https://arxiv.org/abs/2602.00981
tags:
- medical
- medspeak
- knowledge
- correction
- speech
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MedSpeak is a knowledge graph-aided ASR error correction framework
  designed to improve spoken medical question answering by addressing the challenge
  of accurately recognizing medical terminology in noisy ASR transcripts. The core
  idea integrates semantic and phonetic relationships from a medical knowledge graph
  with fine-tuned LLM reasoning to correct transcription errors and enhance downstream
  answer prediction.
---

# MedSpeak: A Knowledge Graph-Aided ASR Error Correction Framework for Spoken Medical QA

## Quick Facts
- arXiv ID: 2602.00981
- Source URL: https://arxiv.org/abs/2602.00981
- Reference count: 0
- Primary result: Achieves 93.4% QA accuracy and reduces WER to 29.9% on medical SQA benchmarks

## Executive Summary
MedSpeak addresses the challenge of accurately recognizing medical terminology in noisy ASR transcripts for spoken medical question answering. The framework integrates semantic and phonetic relationships from a medical knowledge graph with fine-tuned LLM reasoning to correct transcription errors and enhance downstream answer prediction. By leveraging both semantic and phonetic information encoded in the KG alongside the reasoning power of LLMs, MedSpeak achieves state-of-the-art performance on medical SQA benchmarks, significantly outperforming baseline systems.

## Method Summary
MedSpeak builds a medical knowledge graph from UMLS MRREL semantic relationships and CMU Pronouncing Dictionary phonetic information. The framework uses Whisper Small as an ASR frontend to generate noisy transcripts, then retrieves budgeted KG context (600 semantic + 300 phonetic tokens) to provide domain knowledge. A Llama-3.1-8B-Instruct model is fine-tuned on ~10k QA pairs with a unified two-line supervision format that couples transcription correction with answer prediction. The output format is "Corrected Text: [text]\nCorrect Option: [A/B/C/D]", allowing the model to jointly optimize both tasks through causal language modeling loss.

## Key Results
- Achieves 93.4% QA accuracy on medical SQA benchmarks
- Reduces WER from 52.7% to 29.9% compared to baseline systems
- Outperforms state-of-the-art baselines including Med-MiniGPT, Med-Flan, and MedGraphen

## Why This Works (Mechanism)

### Mechanism 1
Joint phonetic-semantic knowledge graph retrieval improves disambiguation of medically critical ASR errors. Double Metaphone generates phonetic candidates; Levenshtein Distance filters false positives. Surviving candidates receive "phonetic" edge tags in the KG, enabling the LLM to reason over both phonetic similarity and semantic context during correction. Break condition: If ASR errors are dominated by non-phonetic causes (e.g., audio noise, speaker accent not in CMU dictionary), phonetic edges may mislead correction.

### Mechanism 2
Unified two-line supervision couples transcription correction with answer prediction, improving both tasks. Training samples format the LLM output as "Corrected Text: [GT] \n Correct Option: [A-D]". The causal language modeling loss jointly optimizes over both lines, allowing answer-option context to inform transcription correction (and vice versa). Break condition: If options are adversarially misleading or transcription errors affect words not in any option, coupling may introduce bias without benefit.

### Mechanism 3
Budgeted KG context injection (900 tokens) preserves domain knowledge without truncating gold labels. Semantic (600) and phonetic (300) KG partitions are truncated to fixed budgets. This prevents the prompt from exceeding the 2048-token sequence limit while ensuring the target (corrected text + option) remains in context. Break condition: If relevant entities are distributed beyond early retrieval positions, fixed-budget truncation may exclude critical context.

## Foundational Learning

- **Concept: Double Metaphone phonetic encoding**
  - Why needed here: Converts medical terms to pronunciation keys; enables retrieval of phonetically similar but orthographically distinct candidates
  - Quick check question: Given "choorioretinitis," what phonetic codes does Double Metaphone produce? Would "chorioamnionitis" share a code?

- **Concept: Causal language modeling loss**
  - Why needed here: The training objective optimizes next-token prediction over the full dialogue; understanding autoregressive training clarifies why later tokens (answer option) can influence earlier token predictions only indirectly through learned representations
  - Quick check question: In the loss L(θ) = -Σ log P(y_i | x, y_<i), which tokens condition the prediction of the answer option?

- **Concept: Knowledge graph edge typing (semantic vs. phonetic)**
  - Why needed here: Distinguishing edge types allows the LLM to weight phonetic and semantic evidence differently during reasoning
  - Quick check question: If an edge is tagged "phonetic" between "hypoplasia" and "hyperplasia," how should the model interpret this differently from a "due_to" edge?

## Architecture Onboarding

- **Component map**: Audio → Whisper → noisy text → keyword extraction → KG retrieval → prompt construction → fine-tuned LLM → two-line output → parsed answer
- **Critical path**: The latency bottleneck is LLM inference; KG retrieval is O(1) for fixed budget
- **Design tradeoffs**: 
  - 900-token KG budget vs. retrieval quality: larger budget improves context but risks truncating gold labels
  - Fine-tuning on noisy ASR transcripts vs. ground truth: training on ASR text improves robustness but may overfit to Whisper error patterns
  - Unified two-line output vs. separate models: coupling may improve accuracy but reduces modularity for debugging
- **Failure signatures**:
  - High WER but reasonable QA accuracy → LLM ignoring transcription correction, relying only on options
  - Low QA accuracy on new datasets → KG lacks coverage; phonetic/semantic edges missing for novel terms
  - Output format violations (non-two-line) → insufficient supervision epochs or distribution shift in prompts
- **First 3 experiments**:
  1. Ablate phonetic edges: Retrain with only semantic KG edges; compare WER on phonetically confusable pairs to quantify phonetic contribution
  2. Vary KG budget: Test 300/600/900/1200 token budgets; measure WER and QA accuracy to find saturation point
  3. Cross-ASR generalization: Replace Whisper with Wav2Vec2 ASR frontend; assess whether fine-tuned LLM corrects errors from a different ASR system or overfits to Whisper patterns

## Open Questions the Paper Calls Out

### Open Question 1
How does MedSpeak perform on real human speech with natural acoustic variations compared to the synthetic TTS-generated speech used in all experiments? All speech data was synthesized using local pyttsx3 text-to-speech engine, lacking accent variation, disfluencies, and background noise that characterize clinical environments.

### Open Question 2
What is the minimum training data required for MedSpeak to achieve acceptable performance, given that data insufficiency limits domain-specific ASR approaches? Only the full ~10k training set was evaluated, without ablation on smaller sizes, despite the introduction stating domain-specific fine-tuning is restricted by data insufficiency.

### Open Question 3
Does MedSpeak generalize across different ASR backends, or does the correction mechanism learn Whisper-specific error patterns? All experiments exclusively use "Whisper Small Model" as the ASR frontend without testing alternatives like Wav2Vec2.

### Open Question 4
What categories of medically significant errors persist at 29.9% WER, and are they safe for clinical deployment? No error categorization by clinical impact or potential harm level is provided, despite the residual WER potentially posing patient safety risks.

## Limitations
- Evaluation conducted entirely on multiple-choice question answering datasets rather than real-world spoken medical dialogue scenarios
- Knowledge graph relies on UMLS MRREL and CMU Pronouncing Dictionary, neither specifically designed for medical domain completeness or phonetic accuracy of medical terminology
- Fine-tuning data representativeness is unspecified, making it unclear whether the model will generalize to medical domains underrepresented in training data

## Confidence
- **High Confidence**: The core claim that MedSpeak achieves state-of-the-art performance on evaluated benchmarks (93.4% QA accuracy, 29.9% WER) is well-supported by experimental results and comparison against established baselines
- **Medium Confidence**: The claim that "joint phonetic-semantic KG retrieval improves disambiguation of medically critical ASR errors" is supported by mechanism description and results, but lacks ablation studies isolating phonetic vs. semantic contributions
- **Low Confidence**: The assertion that "unified two-line supervision couples transcription correction with answer prediction, improving both tasks" lacks direct evidence and ablation comparisons against separate models

## Next Checks
1. **Phonetic vs. Semantic Ablation Study**: Retrain and evaluate MedSpeak variants with only semantic KG edges, only phonetic KG edges, and the full combined approach. Compare WER specifically on phonologically confusable medical term pairs to quantify each component's contribution.

2. **Cross-Domain Medical Knowledge Graph**: Construct a KG using domain-specific medical ontologies (e.g., SNOMED CT, ICD-10) and evaluate whether performance improves for medical terminology not well-covered in UMLS.

3. **Natural Speech Evaluation**: Evaluate MedSpeak on a dataset of real clinical conversations or physician-patient interactions rather than TTS-synthesized data to validate generalization to natural speech characteristics.