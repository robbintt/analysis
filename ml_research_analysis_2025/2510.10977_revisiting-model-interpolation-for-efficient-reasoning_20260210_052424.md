---
ver: rpa2
title: Revisiting Model Interpolation for Efficient Reasoning
arxiv_id: '2510.10977'
source_url: https://arxiv.org/abs/2510.10977
tags:
- arxiv
- reasoning
- thinking
- mean
- interpolation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper revisits direct model interpolation between Instruct\
  \ and Thinking models, revealing a three-stage evolutionary paradigm in performance\
  \ dynamics. Rather than evolving linearly, the model\u2019s reasoning capability,\
  \ token efficiency, and accuracy transition through distinct phases as the interpolation\
  \ coefficient \u03BB varies."
---

# Revisiting Model Interpolation for Efficient Reasoning

## Quick Facts
- arXiv ID: 2510.10977
- Source URL: https://arxiv.org/abs/2510.10977
- Reference count: 32
- One-line primary result: Strategic model interpolation achieves superior reasoning quality-efficiency trade-offs compared to sophisticated baselines by exploiting a three-stage evolutionary paradigm.

## Executive Summary
This paper revisits direct model interpolation between Instruct and Thinking models, revealing a three-stage evolutionary paradigm in performance dynamics. Rather than evolving linearly, the model's reasoning capability, token efficiency, and accuracy transition through distinct phases as the interpolation coefficient λ varies. Empirical results on Qwen3 models show that a strategically interpolated model (e.g., λ=0.8) consistently outperforms sophisticated merging baselines like Task Arithmetic and TIES on benchmarks such as AIME'25, IFEval, and GPQA-Diamond. The work also offers practical guidance for achieving targeted reasoning behaviors and efficiency.

## Method Summary
The method employs weight-wise linear interpolation between a Thinking model (strong explicit reasoning) and an Instruct model (short CoT, direct answers) using coefficient λ. The interpolation is applied across the full parameter set or selectively to specific layer ranges (particularly middle-to-late layers). Decoding uses the Thinking model's strategy (T=0.6, Top-p=0.95). Performance is evaluated across reasoning quality metrics (Think #R), efficiency metrics (Token #N), and accuracy benchmarks (Pass@k, Mean@k).

## Key Results
- A three-stage evolutionary paradigm emerges: Stage #1 (Instruct-dominant, minimal reasoning), Stage #2 (emergent explicit reasoning with optimal trade-off), Stage #3 (overthinking with diminishing returns)
- Interpolating only the last two-thirds of layers achieves near-full thinking behavior while maintaining efficiency
- FFN modules primarily drive explicit thinking patterns while attention modules ensure reasoning correctness
- λ=0.8 interpolated model outperforms Task Arithmetic and TIES baselines on AIME'25, IFEval, and GPQA-Diamond benchmarks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Model interpolation between Instruct and Thinking models follows a predictable three-stage evolutionary paradigm rather than linear performance dynamics.
- Mechanism: As the interpolation coefficient λ sweeps from 0 to 1, the merged model transitions through: Stage #1 (Instruct-dominant, no explicit reasoning), Stage #2 (rapid emergence of thinking patterns with optimal quality-efficiency trade-off), and Stage #3 (diminishing returns with over-thinking).
- Core assumption: Thinking and Instruct models share sufficient weight similarity for interpolation to yield coherent hybrid behaviors without catastrophic interference.
- Evidence anchors:
  - [abstract]: "Performance dynamics do not evolve linearly but follow distinct stages"
  - [Section 4.2]: "Stage #2 marks a critical and dramatic phase transition... the Think #R abruptly rises from nearly 0 to 1"
  - [corpus]: Weak direct evidence; related work "System 1&2 Synergy via Dynamic Model Interpolation" addresses interpolation but not the three-stage paradigm specifically.
- Break condition: If model pairs have low weight similarity (high σ values), interpolation may produce incoherent outputs rather than staged transitions.

### Mechanism 2
- Claim: Reasoning capabilities are distributed non-uniformly across layers, with middle and later layers predominantly storing thinking patterns.
- Mechanism: Interpolating only the last two-thirds of layers achieves near-full thinking behavior (Think #R = 100%), while interpolating any single third fails to induce explicit reasoning.
- Core assumption: Thinking-specific representations are localized in deeper transformer layers rather than uniformly distributed.
- Evidence anchors:
  - [Section 5.2]: "Interpolating only the last two-thirds is remarkably effective, achieving a Think #R of 100%"
  - [Table 3]: Layer ablation showing [12,35] range achieves 100% thinking ratio with strong performance
  - [corpus]: No direct corpus evidence for layer-wise reasoning distribution; this appears to be a novel finding.
- Break condition: Architectures with different layer distributions (e.g., early exit models) may not exhibit the same layer localization pattern.

### Mechanism 3
- Claim: FFN modules primarily drive the explicit thinking pattern, while attention modules ensure reasoning correctness.
- Mechanism: Skipping FFN sub-layers during interpolation collapses Think Ratio from 99.95% to 0.68%. Skipping MHA preserves thinking ratio but drops Mean@64 from 80.47 to 71.46.
- Core assumption: Functional separation exists where FFNs encode "how to think in steps" and MHAs provide "knowledge to think correctly."
- Evidence anchors:
  - [Section 5.3]: "FFN modules from the Thinking model are the primary drivers for the pattern of long CoT reasoning"
  - [Figure 5]: Module ablation showing complementary roles of Attn and FFN
  - [corpus]: Weak evidence; "Beyond Speedup -- Utilizing KV Cache" discusses attention but not FFN-specific reasoning roles.
- Break condition: Architectures with merged or alternative sub-layer designs (e.g., Mamba) may not exhibit this functional separation.

## Foundational Learning

- Concept: Task Arithmetic / Model Merging
  - Why needed here: Model interpolation is mathematically equivalent to Task Arithmetic with asymmetric scaling factors on Thinking and Instruct task vectors.
  - Quick check question: Can you derive why Θ(Base) + λTV(Thi) + (1-λ)TV(Ins) = λΘ(Thi) + (1-λ)Θ(Ins)?

- Concept: Chain-of-Thought (CoT) Reasoning and Over-thinking
  - Why needed here: The paper's efficiency goal hinges on understanding the trade-off between explicit reasoning (CoT) and redundant computation (over-thinking).
  - Quick check question: How would you distinguish beneficial intermediate reasoning steps from over-thinking in model outputs?

- Concept: Transformer Sub-layer Functions (MHA vs FFN)
  - Why needed here: The ablation study reveals distinct roles for attention and feed-forward modules in reasoning.
  - Quick check question: What is the hypothesized functional difference between MHA and FFN in the reasoning process per this paper?

## Architecture Onboarding

- Component map:
  - Source models: Instruct model (short CoT, direct answers) + Thinking model (long CoT, explicit reasoning)
  - Interpolation engine: Weight-wise linear combination with coefficient λ
  - Target layers: Focus on middle-to-late layers (e.g., layers 12-35 in 36-layer model)
  - Modules: FFN sub-layers for thinking pattern, MHA sub-layers for reasoning quality

- Critical path:
  1. Verify weight similarity (σ) between candidate model pairs
  2. Sweep λ from 0 to 1 on validation set to identify stage boundaries
  3. Target Stage #2 (typically λ∈[0.4,0.8]) for optimal efficiency-effectiveness trade-off
  4. Apply decoding strategy from Thinking model (T=0.6, Top-p=0.95)

- Design tradeoffs:
  - Higher λ → stronger reasoning but more tokens; lower λ → faster but weaker on complex tasks
  - Full-layer interpolation vs. partial-layer: full achieves peak performance; last 2/3 achieves efficiency with minimal loss
  - Base model choice: Pre-trained base may hurt complex reasoning quality vs. hybrid base

- Failure signatures:
  - Stage #1 behavior: High token count with near-zero Think Ratio → model is verbose but not reasoning
  - Stage #3 behavior: Saturated Think Ratio with declining Pass@k → over-thinking
  - Low σ similarity: Incoherent or degraded outputs at any λ

- First 3 experiments:
  1. **Stage boundary identification**: Sweep λ∈{0.0, 0.1, ..., 1.0} on validation tasks, plot Pass@k, Mean@k, Think #R, and Token #N to identify three stages.
  2. **Layer ablation**: Interpolate only layers [0,11], [12,23], [24,35], [0,23], [12,35] with fixed λ=0.8 to confirm middle/later layer importance.
  3. **Module ablation**: Skip all MHA or all FFN sub-layers during interpolation at λ=0.8 to verify functional separation claims.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the three-stage evolutionary paradigm hold for diverse model families outside of Qwen3, such as Llama or Mistral?
- Basis in paper: [explicit] The authors state in the Limitations section: "Verifying that this predictable three-stage dynamic holds true for other diverse model families, such as Llama or Mistral, would strength our findings."
- Why unresolved: The empirical validation in this study is centered exclusively on the Qwen3 series (4B and 30B-A3B).
- What evidence would resolve it: Replicating the interpolation experiments on Llama-3 or Mistral variants and observing if the distinct Stage #1, #2, and #3 performance dynamics appear.

### Open Question 2
- Question: Can the model interpolation framework be extended to simultaneously merge three or more specialist models?
- Basis in paper: [explicit] The authors list this as a limitation: "Extending this framework to the simultaneous interpolation of three or more specialist models presents an exciting direction."
- Why unresolved: The current method is formulated for and tested on only two models (Instruct and Thinking).
- What evidence would resolve it: A demonstration of a weighted interpolation scheme involving a Base, Instruct, and Thinking model simultaneously, showing that the trade-offs remain manageable.

### Open Question 3
- Question: What underlying factors cause the optimal interpolation coefficient ($\lambda$) and stage boundaries to shift based on model scale?
- Basis in paper: [inferred] The paper observes that for the 30B model, "Stage #2 occurs later" compared to the 4B model, suggesting larger models need stronger influence from the Thinking model, but offers no theoretical explanation for this shift.
- Why unresolved: The specific $\lambda$ ranges for the three stages are determined empirically for specific model sizes without a generalizable rule.
- What evidence would resolve it: Controlled experiments correlating model width or depth with the $\lambda$ threshold required to trigger the "emergent thinking" phase (Stage #2).

## Limitations

- Cross-architecture generalizability: Three-stage paradigm and layer-wise reasoning distribution findings are demonstrated primarily on Qwen3 models; generalizability to other architectures remains unproven
- Training data overlap effects: Analysis assumes Thinking and Instruct models are trained on sufficiently distinct data without quantifying or controlling for overlap
- Static interpolation limitation: Focuses on static weight interpolation rather than dynamic approaches that could adapt to input complexity

## Confidence

**High Confidence**: The empirical demonstration of three-stage performance dynamics on Qwen3 models, the layer ablation results showing middle-to-late layer importance, and the module ablation revealing FFN vs MHA functional separation.

**Medium Confidence**: The generalizability of the three-stage paradigm to other model architectures and the claimed efficiency gains over sophisticated merging baselines.

**Low Confidence**: The mechanistic explanation that FFN modules "primarily drive thinking patterns" while MHA modules ensure "reasoning correctness" is somewhat speculative and may over-attribute causality from correlational ablation results.

## Next Checks

1. **Cross-architecture validation**: Test the three-stage evolutionary paradigm on at least two additional model families (e.g., Llama, Mistral) with different base model scales and training recipes. Validate whether stage boundaries, optimal λ ranges, and layer localization patterns remain consistent or require recalibration.

2. **Dynamic vs static interpolation comparison**: Implement and evaluate a dynamic interpolation controller that adjusts λ based on input complexity (e.g., problem length, domain difficulty). Compare token efficiency and accuracy against the static λ=0.8 baseline on the same benchmarks to quantify potential gains from adaptive merging.

3. **Training data independence analysis**: Train matched Instruct and Thinking model pairs with controlled training data overlap (0%, 50%, 100%) and evaluate interpolation performance at each overlap level. This would quantify how data similarity affects the three-stage dynamics and optimal λ identification, providing guidance for future model pairing strategies.