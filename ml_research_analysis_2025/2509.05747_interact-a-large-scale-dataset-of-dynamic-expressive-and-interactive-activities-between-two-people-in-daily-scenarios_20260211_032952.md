---
ver: rpa2
title: 'InterAct: A Large-Scale Dataset of Dynamic, Expressive and Interactive Activities
  between Two People in Daily Scenarios'
arxiv_id: '2509.05747'
source_url: https://arxiv.org/abs/2509.05747
tags:
- motion
- dataset
- body
- facial
- motions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces InterAct, a novel multi-modal dataset capturing
  dynamic, expressive, and interactive activities between two people in daily scenarios.
  The dataset includes 241 motion sequences, each over one minute long, with synchronized
  audio, body motions, and facial expressions.
---

# InterAct: A Large-Scale Dataset of Dynamic, Expressive and Interactive Activities between Two People in Daily Scenarios

## Quick Facts
- arXiv ID: 2509.05747
- Source URL: https://arxiv.org/abs/2509.05747
- Authors: Leo Ho; Yinghao Huang; Dafei Qin; Mingyi Shi; Wangpok Tse; Wei Liu; Junichi Yamagishi; Taku Komura
- Reference count: 22
- Primary result: Novel multi-modal dataset of 241+ minute-long motion sequences capturing dynamic, expressive, and interactive activities between two people

## Executive Summary
This paper introduces InterAct, a novel multi-modal dataset capturing dynamic, expressive, and interactive activities between two people in daily scenarios. The dataset includes 241 motion sequences, each over one minute long, with synchronized audio, body motions, and facial expressions. Unlike previous datasets focusing on short clips or static interactions, InterAct emphasizes objective-driven, dynamic, and semantically consistent interactions. A diffusion-based method is proposed to estimate interactive facial expressions and body motions from speech inputs, using hierarchical regression for body motions and a fine-tuning mechanism to improve lip accuracy. The dataset and method demonstrate significant improvements in capturing realistic two-person interactions, with high entropy and variance in body and facial animations, indicating diverse and dynamic performances.

## Method Summary
The method employs diffusion probabilistic models to generate interactive full-body motions (BVH) and facial expressions (ARKit blendshapes) for two people simultaneously from speech audio and action labels. The approach uses hierarchical decomposition: a lower-body model predicts global positioning, which conditions an upper-body model for gestures. For facial expressions, a diffusion Transformer is trained on the full InterAct dataset and fine-tuned on a lip-specific dataset, with a weight-swapping technique that preserves identity while improving lip-sync accuracy. Cross-modal attention mechanisms incorporate partner speech and gaze features to enable realistic non-verbal reactions in dyadic scenarios.

## Key Results
- InterAct dataset contains 241 sequences with synchronized audio, body motions, and facial expressions
- Hierarchical diffusion approach outperforms monolithic networks on FID and Diversity metrics for two-person interactions
- Weight injection technique preserves facial identity while correcting lip-sync errors common in emotional speech datasets
- Generated animations show high entropy and variance, indicating diverse and dynamic performances
- User studies confirm effectiveness in generating realistic and emotionally expressive interactions

## Why This Works (Mechanism)

### Mechanism 1: Hierarchical Decomposition of Interactive Motion
Decomposing joint regression into lower-body (global trajectory) and upper-body (gestures) stages improves stability and accuracy for high-dimensional, two-person interactions. The model first regresses the root and lower-body joints using audio and text features, then feeds these predicted lower-body parameters as conditions into a second diffusion model for the upper body and hands. This reduces the curse of dimensionality in the output space and ensures global positioning is causally upstream of local gestures.

### Mechanism 2: Denoiser Weight Injection for Lip Synchronization
Swapping only the denoiser weights from a lip-focused fine-tuned model into a base emotional model preserves facial identity while correcting lip-sync errors. A base model is trained on the full InterAct dataset, while a separate model is fine-tuned on a small, high-accuracy lip dataset. During inference, the denoiser weights of the fine-tuned model are used while keeping the audio encoder and projection layers from the base model.

### Mechanism 3: Cross-Modal Attention for Interaction Awareness
Injecting partner speech and gaze features via biased cross-attention enables generation of non-verbal reactions appropriate for dyadic scenarios. The facial diffusion model accepts concatenated audio features and a facing-pose embedding, using a bias term to prioritize conditional inputs and learn correlations like "when Partner B speaks, Partner A nods."

## Foundational Learning

- **Diffusion Probabilistic Models (DDPMs)**: The core architecture relies on diffusion models to map audio features to motion. Understanding the iterative denoising process is crucial for implementing the Denoiser component and inference loop. *Quick check: How does adding Gaussian noise during training help the model learn to generate diverse, non-deterministic motions from speech?*

- **Audio Feature Extraction (Wav2Vec2 & Mel-Spectrograms)**: The system uses distinct audio representations: Wav2Vec2 for semantic/content alignment (Face) and Mel-spectrograms for low-level rhythm/prosody (Body). *Quick check: Why would a model use BERT (text) in addition to audio features for body motion generation?*

- **Exponential Maps (Rotation Representation)**: The paper explicitly states: "Exponential map is adopted as the motion representation." This is crucial for handling joint rotations without the discontinuities of Euler angles or the over-parameterization of Quaternions. *Quick check: What is the advantage of using an exponential map (axis-angle) over quaternions for neural network regression of 3D rotations?*

## Architecture Onboarding

- **Component map**: Preprocessors (VisualVoice, Wav2Vec2/BERT, Least-squares solver) -> Face Pipeline (Diffusion Denoiser + Fine-tuning Module) -> Body Pipeline (Hierarchical Diffusion) -> Control Signals (Action Labels, Facing Label, Speaker ID)

- **Critical path**: The Hierarchical Body Pipeline is the most complex implementation detail. Ensure the output of Network 1 (lower body) is correctly reshaped and concatenated to the conditioning vector of Network 2. The data loader must synchronize the two distinct modalities (Face vs. Body) and the two actors per sample.

- **Design tradeoffs**: Hierarchical vs. Monolithic - the paper argues that a monolithic network fails on high-dimensional two-person data due to diverse joint statistics. The tradeoff is engineering complexity (two training loops) vs. stability. Lip Accuracy vs. Emotion - standard fine-tuning improves lips but kills emotion/identity. The "Weight Injection" hack trades standard training simplicity for a custom inference loader that manages two sets of weights.

- **Failure signatures**: Floating/Skating - lower-body network fails to lock feet/root position. Stiff Upper Body - if hierarchical connection is too strong, upper body motions may freeze. Identity Drift - if fine-tuned denoiser is used without freezing the encoder, lip shapes may look correct but face will look like fine-tuning actor.

- **First 3 experiments**:
  1. Ablation on Hierarchy: Train a single network (LDA2 approach) on InterAct and compare FID scores against hierarchical approach.
  2. Lip Fine-tuning Validation: Implement weight-swapping inference. Compare 3 conditions: (a) Base model only, (b) Fully fine-tuned model, (c) Weight-swapped model. Measure Lip Vertex Error and visually inspect for identity retention.
  3. Conditioning Drop-out: Randomly drop "Partner Audio" or "Facing Pose" during inference to verify if model has learned to use these signals for interaction or relies solely on primary speaker's audio.

## Open Questions the Paper Calls Out

1. **Physical Plausibility**: How can the model's architecture be modified to ensure physical plausibility, specifically preventing self-penetrations and inter-character collisions in generated motions? The current diffusion-based baseline focuses on motion synthesis from audio but lacks explicit constraints to enforce non-penetration or precise hand articulation.

2. **Richer Contextual Signals**: What specific "richer contextual signals" beyond speech and simple action labels are necessary to achieve finer-grained control over the complexity of dyadic behaviors? The current method relies on audio, action labels, and BERT features, which the authors admit limits the nuance and controllability of generated interactions.

3. **Dataset Generalization**: To what extent does the "acting" nature of the InterAct dataset limit the generalization of models to genuine, naturalistic human interactions? The authors acknowledge that data is "acting... as opposed to capturing reality" and may be regarded as less realistic, potentially creating a domain gap between the dataset and spontaneous, unscripted daily interactions.

## Limitations
- Hierarchical decomposition assumes conditional independence between lower-body and upper-body motions, which may not hold for certain expressive gestures
- Lip accuracy preservation through weight injection lacks quantitative metrics comparing identity preservation across different expressions and lighting conditions
- Facing-pose binary label may be insufficient for capturing nuanced attention dynamics in complex conversational scenarios

## Confidence

- **High Confidence**: Dataset collection methodology and size (241 sequences) are well-documented and verifiable. Overall architecture of using diffusion models for motion generation from speech is technically sound.
- **Medium Confidence**: Performance improvements (FID and Diversity scores) are based on comparisons with specific baselines, but exact implementation details and evaluation protocol reproducibility could affect validity.
- **Low Confidence**: Cross-modal attention mechanism's effectiveness in generating realistic non-verbal reactions is primarily demonstrated through qualitative examples rather than rigorous user studies.

## Next Checks

1. **Hierarchical Conditioning Validation**: Implement an ablation study where upper-body network is trained without conditioning on lower-body outputs, then compare physical plausibility and FID scores against full hierarchical approach.

2. **Lip Accuracy Identity Preservation Test**: Using weight injection method, generate facial animations for multiple speakers and quantify identity drift using face recognition metrics (e.g., ArcFace similarity) across different emotional expressions.

3. **Attention Mechanism Ablation**: During inference, systematically remove partner speech and gaze features from face generation model. Measure changes in FID scores and conduct a small user study to assess perceived naturalness of generated interactions.