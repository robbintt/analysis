---
ver: rpa2
title: 'Enhancing the Learning Experience: Using Vision-Language Models to Generate
  Questions for Educational Videos'
arxiv_id: '2505.01790'
source_url: https://arxiv.org/abs/2505.01790
tags:
- questions
- question
- videos
- video
- https
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates the potential of vision-language models
  (VLMs) for generating learning-oriented questions from educational videos. The research
  explores zero-shot performance, fine-tuning effects, modality impacts, and qualitative
  aspects like relevance and answerability.
---

# Enhancing the Learning Experience: Using Vision-Language Models to Generate Questions for Educational Videos

## Quick Facts
- arXiv ID: 2505.01790
- Source URL: https://arxiv.org/abs/2505.01790
- Reference count: 40
- Fine-tuned VLMs achieve 52.74% relevant question rate on educational videos

## Executive Summary
This study investigates the use of Vision-Language Models (VLMs) for generating educational questions from video content. The research evaluates zero-shot and fine-tuned performance across different modalities (visual, audio, transcript) using TED-Ed and Khan Academy datasets. Results demonstrate that fine-tuning significantly improves question quality and relevance compared to zero-shot inference, though challenges remain in content alignment and question diversity. The study introduces a novel Inner-Class-Diff metric to better assess content relevance and identifies modality bias as a key limitation in current VLM architectures.

## Method Summary
The research employs VLMs (Video-LLaVA and PG-Video-LLaVA) to generate questions from educational videos. The LearningQ dataset is filtered to include only "useful for learning" questions, excluding cloze tests, with an 80/10/10 train/validation/test split. Models are fine-tuned on a 13B parameter version using batch size 2 with learning rate adjusted via linear scaling rule. Evaluation uses automatic metrics (ROUGE-L, BERTScore with roberta-large, and custom ICD) plus manual assessment. Three distinct prompts target different cognitive levels per Bloom's taxonomy.

## Key Results
- Fine-tuning increases relevant question generation to 52.74% compared to lower zero-shot performance
- Audio transcripts prove more critical than visual frames for question generation
- ICD metric better captures content relevance than traditional ROUGE/BERTScore alone
- Models predominantly generate "What" questions regardless of prompt complexity targeting

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Fine-tuning VLMs aligns model behavior with educational objectives better than zero-shot inference, primarily by enforcing output format and content relevance.
- **Mechanism:** Pre-trained VLMs often default to generic descriptions or hallucinations. Fine-tuning on a dataset like LearningQ conditions the model to associate specific video features with the act of questioning, reducing empty outputs and increasing the "relevant question" rate to approx. 52.74%.
- **Core assumption:** The ground-truth questions in the training set accurately represent "quality" educational inquiry, and the model has sufficient capacity to learn these associations.
- **Evidence anchors:**
  - [abstract] Highlights fine-tuning improves question quality and relevance.
  - [section 4.2] Shows fine-tuning increased the number of generated questions and improved metric scores (ROUGE/BERTScore) compared to zero-shot.
  - [corpus] Corpus papers (e.g., "Context Selection for Video-based QG") support the general principle that context alignment is critical for QG, though specific VLM fine-tuning data is novel to this paper.
- **Break condition:** Fails if the training dataset lacks diversity (e.g., too many "What" questions) or contains timestamps misaligned with visual content, leading to hallucination or repetitive questioning.

### Mechanism 2
- **Claim:** VLMs exhibit modality bias, often relying heavily on audio/transcripts over visual frames for question generation in current architectures.
- **Mechanism:** In ablation studies, models performed surprisingly well with audio-only but degraded or hallucinated when deprived of audio/text, suggesting the visual encoder is not fully grounding the educational content in zero-shot or standard fine-tuning regimes.
- **Core assumption:** The visual features extracted from educational videos (which may include handwriting or diagrams) are sufficiently encoded by the vision tower.
- **Evidence anchors:**
  - [section 4.3] Notes that Video-LLaMA performed similarly with audio-only, while PG-Video-LLaVA hallucinated when lacking a modality.
  - [section 5] Concludes that the textual baseline (Mistral-7B) often outperformed multimodal models, implying visual integration is currently weak.
  - [corpus] Corpus evidence regarding modality bias in this specific architecture is weak; general NLP literature supports transcript reliance.
- **Break condition:** Fails in video types where visual information is critical (e.g., diagrams without verbal descriptions) and the model lacks strong visual grounding capabilities.

### Mechanism 3
- **Claim:** Prompt design dictates the cognitive level of the generated question, but model adherence is inconsistent without fine-grained instruction tuning.
- **Mechanism:** Prompts targeting "comprehension" vs. "knowledge assessment" steer the LLM head. However, without specific tuning, models often revert to the most statistically probable question type (typically "What" or recall-based), limiting diversity.
- **Core assumption:** The model's instruction-following capability is robust enough to distinguish between nuanced pedagogical goals.
- **Evidence anchors:**
  - [section 3.3] Defines three distinct prompts to target different cognitive skills (Bloom's taxonomy).
  - [section 4.4] Shows models predominantly generate "What" questions regardless of prompt nuance, indicating a bias in training data or model architecture.
  - [corpus] Related work (e.g., "From Objectives to Questions") suggests planning frameworks are needed to enforce cognitive levels, supporting the finding that naive prompting is insufficient.
- **Break condition:** Fails when the model ignores prompt constraints, generating generic questions (e.g., "What is the video about?") regardless of the requested complexity.

## Foundational Learning

- **Concept: Vision-Language Models (VLMs)**
  - **Why needed here:** The core engine (e.g., Video-LLaVA, PG-Video-LLaVA) processes video frames and text. Understanding that these models bind visual tokens to text tokens is essential for debugging why they might "hallucinate" visual content.
  - **Quick check question:** If a VLM generates a question about a diagram not mentioned in the transcript, is this retrieval or hallucination?

- **Concept: Zero-shot vs. Fine-tuning**
  - **Why needed here:** The paper explicitly contrasts "out-of-the-box" performance against trained performance. You must understand that zero-shot relies on pre-trained priors, while fine-tuning minimizes loss on a specific educational dataset distribution.
  - **Quick check question:** Why might a zero-shot model generate a summary instead of a question?

- **Concept: Evaluation Metrics (ROUGE/BERTScore vs. ICD)**
  - **Why needed here:** Standard NLP metrics (ROUGE) measure n-gram overlap, which fails to capture semantic relevance in open-ended generation. The authors introduce ICD (Inner-Class-Diff) to measure content-alignment against transcripts.
  - **Quick check question:** Why is high ROUGE score not a guarantee that a question is "answerable" or "educational"?

## Architecture Onboarding

- **Component map:** Data Pre-processing (LearningQ) -> Frame/Transcript Alignment -> Prompt Construction -> Model Inference -> Metric Evaluation (ICD/Manual)

- **Critical path:** Data Pre-processing (LearningQ) -> Frame/Transcript Alignment -> Prompt Construction -> Model Inference -> Metric Evaluation (ICD/Manual)

- **Design tradeoffs:**
  - *Frame Sampling:* Uniform sampling (Video-LLaVA) vs. Dense sampling (PG-Video-LLaVA). Dense sampling captures more detail but increases compute cost and noise.
  - *Modality:* Using Audio vs. Transcript. Audio preserves tone but requires robust ASR; Transcript is cleaner but loses paralinguistic cues.
  - *Metric reliance:* Relying on BERTScore vs. Human Eval. BERTScore is fast but misses "relevance" to the specific video segment.

- **Failure signatures:**
  - **Empty Output:** PG-Video-LLaVA returning empty strings (Section 4.1).
  - **Reformulation:** Model returning "What is the content of the video?" (Section 4.1).
  - **Hallucination:** Generating questions about objects not present in the video (Section 4.3).
  - **Statement Generation:** Model outputting a fact/summary instead of a question (Section 4.1).

- **First 3 experiments:**
  1. **Sanity Check (Zero-Shot):** Run Video-LLaVA on a short TED-Ed clip with the "Create a question" prompt. Verify if it returns a valid question string or a summary.
  2. **Modality Ablation:** Run the fine-tuned model on a video with *black frames* (audio only) vs. *mute* (video only) to determine which modality the specific model relies on for the target domain (Math vs. History).
  3. **Metric Validation:** Generate 10 questions and calculate ROUGE-L vs. ICD. Manually inspect if ICD correlates better with "relevance" than ROUGE-L for a video where the ground truth question differs in wording.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the inclusion of time-stamped question annotations impact the precision of content alignment in educational video question generation?
- Basis in paper: [explicit] The authors state that existing datasets lack time-stamp information, making it challenging to locate relevant video segments, and identify this as a crucial requirement for future datasets.
- Why unresolved: Current experiments were forced to use entire videos rather than relevant segments due to the lack of temporal annotations in available datasets like LearningQ.
- What evidence would resolve it: A comparative evaluation of models trained on temporally grounded data versus full-video data, measuring improvements in content alignment metrics.

### Open Question 2
- Question: How does the relative importance of visual versus audio modalities fluctuate across different educational video presentation styles?
- Basis in paper: [explicit] The conclusion notes that modality importance varies by video type (e.g., slides, animations, handwriting), necessitating research to optimize models for each format.
- Why unresolved: The current datasets lacked sufficient style diversity to isolate the significance of each modality within specific presentation styles.
- What evidence would resolve it: An ablation study on a style-stratified dataset (e.g., separating handwritten math from animated history) analyzing performance deltas when audio or visual inputs are removed.

### Open Question 3
- Question: To what extent can ensemble methods combining non-generative models with LLMs reduce hallucinations in generated questions?
- Basis in paper: [explicit] The authors propose investigating "alternative approaches like ensemble methods" and "non-generative models" (e.g., for analyzing visual content) to mitigate hallucinations and content irrelevance.
- Why unresolved: The study found that current VLMs often hallucinate or generate irrelevant content, particularly when deprived of a modality, and single-model approaches showed limitations.
- What evidence would resolve it: An experiment measuring hallucination frequency and factual consistency in questions generated by a hybrid ensemble model compared to standalone VLMs.

## Limitations
- Narrow educational domain representation (TED-Ed and Khan Academy only)
- Limited visual feature capture for handwritten text and diagrams
- Weak control over question cognitive complexity despite prompt variations
- Modality bias toward audio/transcripts over visual content

## Confidence
- **High Confidence:** Fine-tuning improves question relevance and reduces empty outputs
- **Medium Confidence:** VLMs exhibit modality bias toward audio/transcripts
- **Low Confidence:** Prompt design reliably controls cognitive level of questions

## Next Checks
1. **Dataset Diversity Test:** Evaluate the same fine-tuned models on a broader range of educational video sources (e.g., YouTube educational content, university lectures) to assess generalization beyond TED-Ed and Khan Academy.

2. **Visual Grounding Assessment:** Conduct a controlled experiment where videos contain critical visual information without corresponding audio descriptions to determine if models can generate questions based on visual content alone.

3. **Pedagogical Effectiveness Study:** Implement a small-scale classroom study where students interact with questions generated by different VLM variants to measure actual learning outcomes, not just automated metrics.