---
ver: rpa2
title: 'ProtoFlow: Interpretable and Robust Surgical Workflow Modeling with Learned
  Dynamic Scene Graph Prototypes'
arxiv_id: '2512.14092'
source_url: https://arxiv.org/abs/2512.14092
tags:
- surgical
- workflow
- protoflow
- scene
- graph
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ProtoFlow introduces a prototype learning framework for surgical
  workflow modeling using dynamic scene graphs. It combines self-supervised pretraining
  and prototype-based fine-tuning to capture interpretable, recurring patterns in
  surgical interactions.
---

# ProtoFlow: Interpretable and Robust Surgical Workflow Modeling with Learned Dynamic Scene Graph Prototypes

## Quick Facts
- arXiv ID: 2512.14092
- Source URL: https://arxiv.org/abs/2512.14092
- Reference count: 22
- Primary result: >80% accuracy and 68.66% F1 on CAT-SG cataract surgery dataset with exceptional few-shot learning (39.49% with 1 video)

## Executive Summary
ProtoFlow introduces a prototype learning framework for surgical workflow modeling using dynamic scene graphs. It combines self-supervised pretraining and prototype-based fine-tuning to capture interpretable, recurring patterns in surgical interactions. Evaluated on the CAT-SG dataset for cataract surgery, ProtoFlow outperforms GNN baselines, achieving over 80% accuracy and 68.66% F1 score. It demonstrates exceptional robustness in few-shot learning, maintaining strong performance with as few as one training video. The learned prototypes provide interpretable insights into surgical sub-techniques and workflow deviations, such as rare complications. ProtoFlow addresses data scarcity and interpretability challenges, advancing transparent, reliable AI systems for clinical adoption in surgical training and decision support.

## Method Summary
ProtoFlow uses a four-stage pipeline for surgical phase classification from dynamic scene graphs (DSGs). First, a GNN encoder-decoder is pretrained via graph autoencoding to reconstruct node features and adjacency. Second, k-means clustering initializes prototypes in the embedding space. Third, the encoder and prototypes are fine-tuned with a combined loss including classification, reconstruction, and prototype regularization. Fourth, inference uses distance-based classification to nearest prototypes, providing both predictions and interpretability. The method leverages temporal context by aggregating scene graphs across frames with temporal edges, capturing workflow evolution.

## Key Results
- Achieves >80% accuracy and 68.66% F1 score on CAT-SG cataract surgery dataset
- Outperforms GNN baselines significantly in few-shot learning (39.49% accuracy with 1 video vs 15.43% baseline)
- Discovers interpretable prototypes corresponding to clinically meaningful surgical sub-techniques (e.g., cystotome-only vs forceps-only vs combined usage)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Distance-based classification to learned prototypes enables both accurate phase prediction and inherent interpretability.
- Mechanism: GNN encoder maps input Dynamic Scene Graphs (DSGs) to embeddings z_t. During fine-tuning, classification is computed via softmax over Euclidean distances to K prototypes per class (Equation 2). Nearest prototypes directly indicate which learned workflow pattern the input resembles.
- Core assumption: Surgical workflow phases exhibit recurring, clusterable sub-structures in the embedding space that correspond to clinically meaningful variations.
- Evidence anchors:
  - [abstract] "discovers and refines core prototypes that encapsulate recurring, clinically meaningful patterns of surgical interaction"
  - [Page 7, Figure 3] Shows three discovered prototypes for "Capsulorhexis" corresponding to cystotome-only, forceps-only, and combined instrument usage—uncovered automatically via clustering.
  - [corpus] CAT-SG dataset paper provides the scene graph foundation, but prototype learning specifically for interpretability is not extensively covered in neighbors.
- Break condition: If DSG embeddings do not form class-consistent clusters during pretraining, prototype initialization will be meaningless and distance-based classification will fail.

### Mechanism 2
- Claim: Self-supervised graph autoencoding creates a stable, semantically meaningful latent space that transfers to few-shot scenarios.
- Mechanism: Stage 1 pretrains encoder f_θ and decoder g_φ to reconstruct node features and adjacency (L_rec). This forces the encoder to capture structural relationships without label supervision. Prototypes are then initialized in this pre-structured space.
- Core assumption: Graph reconstruction loss captures task-relevant structural information about surgical interactions, not just low-level features.
- Evidence anchors:
  - [Page 4] "To stabilize the latent space and promote discriminative embeddings, we use a decoder g_φ to reconstruct the original graph."
  - [Page 6, Table 2] Few-shot results: ProtoFlow achieves 39.49% accuracy with 1 video vs. 15.43% for GATv2 baseline (24 p.p. improvement).
  - [corpus] Multimodal Graph Representation Learning paper addresses robustness via disentanglement but does not use prototype learning; suggests alternative mechanisms for similar goals.
- Break condition: If reconstruction loss dominates and prevents discriminative class separation, the pretrained embeddings will not support prototype clustering.

### Mechanism 3
- Claim: Temporal context aggregation via dynamic scene graphs improves recognition by capturing workflow evolution.
- Mechanism: Scene graphs from multiple frames are aggregated by connecting same-class nodes across adjacent frames with temporal edges, forming one large graph. The GNN encoder then processes this temporal structure.
- Core assumption: Surgical phase identity depends on temporal evolution of interactions, not just instantaneous scene state.
- Evidence anchors:
  - [Page 5] "Scene graphs for multiple frames are aggregated by connecting nodes of the same class from temporally adjacent frames with temporal edges."
  - [Page 6, Figure 2] Accuracy increases from ~0.69 at 1 frame to ~0.80 at 30-60 frames; gains plateau after ~20 frames (60 seconds).
  - [corpus] DSTED paper addresses temporal stabilization for workflow recognition but uses different mechanism (decoupled temporal/discriminative enhancement).
- Break condition: If temporal window is too short, transient states dominate; if too long, graph complexity increases with diminishing returns and potential phase-boundary blurring.

## Foundational Learning

- **Concept: Graph Neural Networks (GNNs) and Message Passing**
  - Why needed here: ProtoFlow uses GATv2 layers to encode DSGs. Understanding how nodes aggregate neighbor information is essential for debugging embedding quality.
  - Quick check question: Given a 3-layer GNN with node degree varying from 2 to 10, what happens to the receptive field size and potential over-smoothing?

- **Concept: Prototype Learning and Distance-Based Classification**
  - Why needed here: The core innovation. Classification is not via linear head but via distance to learned prototypes. Understanding k-means initialization and prototype regularization is critical.
  - Quick check question: If all prototypes collapse to similar embeddings during fine-tuning, what would happen to the distance-based softmax predictions?

- **Concept: Scene Graphs and Dynamic Extensions**
  - Why needed here: Input representation. Nodes = surgical objects; edges = spatial/semantic/temporal relations. Dynamic extension adds temporal edges across frames.
  - Quick check question: How does adding temporal edges between same-class nodes differ from simply concatenating frame-level graphs without inter-frame connections?

## Architecture Onboarding

- **Component map:**
  Input: DSG G_t = (A_t, X_t) [adjacency + node features]
      ↓
  Stage 1: GNN Encoder (3× GATv2) → z_t | Decoder reconstructs Â_t, X̂_t
      ↓
  Stage 2: k-NN clustering per class → Initialize K prototypes per class
      ↓
  Stage 3: Fine-tune encoder + prototypes with L_cls + L_reg + L_rec
      ↓
  Stage 4: Inference → distances to prototypes → softmax → prediction + explanation

- **Critical path:**
  1. Pretraining quality (L_rec convergence) determines embedding space structure
  2. Prototype initialization depends on well-separated class clusters
  3. Fine-tuning regularization (L_reg) prevents prototype drift from meaningful centroids

- **Design tradeoffs:**
  - Temporal window: 1 frame (fast, lower accuracy) vs. 30-60 frames (good accuracy, plateau beyond) vs. >60 frames (diminishing returns, higher compute)
  - Prototypes per class: Too few misses sub-techniques; too many dilutes interpretability and increases overfitting risk
  - L_reg weight: High regularization preserves cluster structure but may limit adaptation; low regularization allows drift and loses interpretability

- **Failure signatures:**
  - Prototypes collapsing to similar vectors → check L_reg weight and cluster quality after pretraining
  - Poor few-shot transfer → pretraining may not capture transferable structure; verify reconstruction quality
  - Accuracy plateaus early → temporal window may be insufficient or graph construction is missing key edges
  - Interpretability degrades after fine-tuning → prototypes have drifted too far from initial centroids

- **First 3 experiments:**
  1. **Ablation on temporal window (1, 5, 10, 20, 30, 60 frames)** to reproduce Figure 2 and identify optimal context length for your dataset.
  2. **Few-shot learning curve (1, 2, 5, 10, 25 videos)** to validate self-supervised pretraining benefit and compare against baseline GATv2 without prototypes.
  3. **Prototype inspection per class** to verify learned prototypes correspond to clinically meaningful variations (e.g., instrument sub-techniques) before clinical deployment.

## Open Questions the Paper Calls Out
- Can the learned prototypes generalize effectively to other surgical procedures with distinct anatomical structures and workflow dynamics, such as laparoscopic cholecystectomy?
- Is the proposed framework computationally efficient enough for real-time intraoperative decision support?
- How does the arbitrary selection of the number of prototypes per class (K) affect the model's ability to capture sub-techniques without overfitting?
- To what extent do the automatically discovered "clinically meaningful" variations align with expert surgical knowledge?

## Limitations
- The paper does not specify kNN clustering parameters (k value, distance metric) for prototype initialization, which could affect prototype quality
- Loss weighting between L_cls, L_reg, and L_rec components is unspecified, making exact reproduction difficult
- No explicit discussion of potential biases in the CAT-SG dataset or how surgical technique variations might affect prototype learning

## Confidence
- **High Confidence**: Prototype-based classification mechanism works as described; achieved accuracy (>80%) and F1 scores (68.66%) are credible based on reported metrics and comparison baselines
- **Medium Confidence**: Few-shot learning claims are well-supported by experimental results, though external validation on other surgical datasets would strengthen generalizability
- **Medium Confidence**: Interpretability claims are plausible given the prototype inspection methodology, but clinical validation of prototype meaningfulness is not shown

## Next Checks
1. **Reconstruct Figure 2** with ablation on temporal window (1, 5, 10, 20, 30, 60 frames) to identify optimal context length and verify claimed plateau after 20 frames
2. **Validate few-shot learning curve** (1, 2, 5, 10, 25 videos) to confirm self-supervised pretraining benefit and quantify improvement over GATv2 baseline without prototypes
3. **Inspect learned prototypes per surgical phase** through embedding visualization (t-SNE) and prototype distance analysis to verify they correspond to clinically meaningful variations before deployment