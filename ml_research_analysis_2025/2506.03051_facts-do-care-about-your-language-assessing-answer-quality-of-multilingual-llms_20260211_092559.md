---
ver: rpa2
title: 'Facts Do Care About Your Language: Assessing Answer Quality of Multilingual
  LLMs'
arxiv_id: '2506.03051'
source_url: https://arxiv.org/abs/2506.03051
tags:
- language
- languages
- llama
- llms
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluates the factuality of Llama 3.1 responses across
  12 languages on 54 middle and high school-level factual questions. Bilingual evaluators
  manually assessed keyword coverage, factual accuracy, and extraneous information
  in translated responses.
---

# Facts Do Care About Your Language: Assessing Answer Quality of Multilingual LLMs

## Quick Facts
- arXiv ID: 2506.03051
- Source URL: https://arxiv.org/abs/2506.03051
- Reference count: 6
- Large multilingual LLMs show significant performance disparities across languages, with high-resource languages like English achieving near-perfect keyword coverage while low-resource languages like Tulu and Māori perform substantially worse

## Executive Summary
This study evaluates the factuality of Llama 3.1 responses across 12 languages on 54 middle and high school-level factual questions. Bilingual evaluators manually assessed keyword coverage, factual accuracy, and extraneous information in translated responses. The findings reveal strong positive correlations between language speaker count and keyword presence (0.426–0.463 Spearman), with English and high-resource languages achieving near-complete keyword coverage (>94%) and low incorrectness/extraneousness scores. Low-resource languages like Tulu and Māori showed significantly poorer performance, with keyword coverage below 60%. Larger models (70B) performed better than smaller ones (8B) across all languages, highlighting that multilingual LLMs reinforce linguistic biases by providing less factual and more verbose responses in underrepresented languages.

## Method Summary
The study evaluated Llama 3.1 8B and 70B models across 12 languages using 54 factual questions at middle and high school levels. For each language, evaluators first generated ground-truth answers and keywords by consulting web sources. Responses were then generated in English and translated into each target language using a professional translation service. Bilingual evaluators manually assessed responses across three dimensions: keyword coverage (presence of key facts), incorrectness (factual errors), and extraneousness (irrelevant information). The evaluation included six high-resource languages (English, Spanish, French, Portuguese, German, Italian) and six low-resource languages (Tamil, Hindi, Swahili, Tulu, Māori, Assamese).

## Key Results
- Strong positive correlation between language speaker count and keyword presence (Spearman 0.426–0.463)
- High-resource languages achieved >94% keyword coverage with minimal incorrectness/extraneousness, while low-resource languages like Tulu and Māori scored below 60% coverage
- Larger models (70B) consistently outperformed smaller models (8B) across all languages in keyword coverage and factual accuracy

## Why This Works (Mechanism)
Unknown: The paper does not explicitly explain the underlying mechanism for why multilingual LLMs show performance disparities across languages. The correlation between speaker population and performance suggests training data imbalance, but the specific architectural or training-related reasons are not detailed.

## Foundational Learning
- Multilingual evaluation methodology: Understanding how to systematically assess LLM performance across multiple languages with consistent metrics and evaluation criteria
- Bilingual evaluation framework: Need to ensure accurate cross-language assessment through professional translators and bilingual evaluators who can properly judge factual correctness and relevance
- Keyword coverage metrics: Essential for quantifying factual completeness in responses, measuring what percentage of key information from ground-truth answers appears in model outputs
- Statistical correlation analysis: Required to establish relationships between language demographics (speaker population) and model performance metrics
- Model scaling effects: Understanding how parameter count influences multilingual performance and whether larger models provide more equitable coverage across languages
- Prompt translation impact: Recognizing how translation quality of evaluation prompts affects model responses and subsequent assessment of multilingual capabilities

## Architecture Onboarding

**Component map:** Translation Service -> LLM (Llama 3.1 8B/70B) -> Evaluator Assessment -> Statistical Analysis

**Critical path:** Professional translation of prompts → Model response generation → Bilingual evaluation of keyword coverage/incorrectness/extraneousness → Correlation analysis with language demographics

**Design tradeoffs:** Manual bilingual evaluation provides high-quality assessments but limits scalability; professional translation ensures prompt consistency but adds cost and potential delay; focusing on factual questions provides clear evaluation criteria but may miss other quality dimensions

**Failure signatures:** Poor keyword coverage indicates incomplete factual responses; high incorrectness scores reveal factual errors; high extraneousness suggests verbosity or hallucination; correlation with speaker population reveals linguistic bias patterns

**3 first experiments:**
1. Compare keyword coverage metrics using automatically generated reference answers vs. manually curated ground truth
2. Evaluate the impact of prompt translation quality by testing different translation services or direct multilingual prompting
3. Assess model calibration by measuring confidence scores and comparing them to actual accuracy across high-resource and low-resource languages

## Open Questions the Paper Calls Out
Unknown: The paper does not explicitly call out specific open questions in the text provided. This section would need additional context from the full paper to identify research directions or unanswered questions raised by the study.

## Limitations
- Evaluation limited to middle and high school-level factual questions, potentially not generalizable to complex or domain-specific knowledge
- Manual evaluation process relies on bilingual evaluators, introducing potential subjective bias in assessments
- Study examines only two model sizes (8B and 70B parameters) and two Llama variants, limiting generalizability to other architectures
- Exclusive focus on factual correctness and keyword coverage may overlook other important aspects like coherence, context-appropriateness, or cultural sensitivity

## Confidence
High: Multilingual LLMs exhibit systematic performance disparities across languages, with strong correlation between speaker population and keyword coverage
Medium: These disparities directly reinforce existing linguistic biases, though confounding factors like data quality differences are not controlled

## Next Checks
1. Replicate evaluation using automatically generated reference answers to assess robustness of keyword coverage metrics
2. Conduct controlled experiment varying prompt translation quality to isolate its effect on multilingual performance
3. Expand evaluation to include domain-specific knowledge questions and measure model calibration in low-resource languages