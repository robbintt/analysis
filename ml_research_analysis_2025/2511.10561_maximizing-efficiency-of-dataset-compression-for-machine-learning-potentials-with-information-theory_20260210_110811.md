---
ver: rpa2
title: Maximizing Efficiency of Dataset Compression for Machine Learning Potentials
  With Information Theory
arxiv_id: '2511.10561'
source_url: https://arxiv.org/abs/2511.10561
tags:
- mean
- dataset
- ours
- random
- k-means
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces an information-theoretic framework for compressing
  atomistic datasets used in machine learning interatomic potentials (MLIPs). The
  core method solves the minimum set cover (MSC) problem over atom-centered environments,
  identifying the smallest subset of structures that maximally preserves information
  from the original dataset while removing redundancies.
---

# Maximizing Efficiency of Dataset Compression for Machine Learning Potentials With Information Theory

## Quick Facts
- **arXiv ID:** 2511.10561
- **Source URL:** https://arxiv.org/abs/2511.10561
- **Authors:** Benjamin Yu; Vincenzo Lordi; Daniel Schwalbe-Koda
- **Reference count:** 40
- **Primary result:** An information-theoretic Minimum Set Cover algorithm that compresses atomistic datasets for MLIPs while preserving diversity, outliers, and long-tail force distributions.

## Executive Summary
This paper presents a novel information-theoretic framework for compressing atomistic datasets used in training machine learning interatomic potentials (MLIPs). The method formulates dataset compression as a Minimum Set Cover (MSC) problem over atom-centered environments, using information entropy to identify the smallest subset of structures that maximally preserves information from the original dataset. By selecting structures that cover the space of atomic environments most efficiently, the algorithm removes redundancies while retaining critical outliers and diversity.

The MSC method was extensively validated on GAP-20, TM23, and 64 diverse datasets from the ColabFit repository. It consistently outperformed baseline methods like random sampling, k-means clustering, and farthest-point sampling in both diversity retention and overlap with original datasets. Critically, MLIPs trained on MSC-compressed datasets exhibited reduced error for out-of-distribution data, especially in low-data regimes, while preserving long-tail force distributions that are essential for accurate simulations.

## Method Summary
The compression framework computes atom-centered descriptors for each environment in the dataset, then uses Gaussian kernel density estimation to calculate per-structure entropy. The Minimum Set Cover algorithm iteratively selects structures that maximize differential entropy coverage - choosing the structure that adds the most new information to the growing compressed set. The greedy selection process continues until the target compression ratio is reached. The algorithm is implemented in the open-source QUESTS package and requires computing descriptors with k=32 neighbors and a 5 Å cutoff, then running the greedy MSC selection based on differential entropy calculations.

## Key Results
- MSC consistently outperformed random sampling, k-means, and farthest-point sampling across 64 diverse datasets in preserving dataset diversity and overlap metrics
- MLIPs trained on MSC-compressed datasets showed reduced error for out-of-distribution data, particularly in low-data regimes
- The method preserved long-tail force distributions even at high compression rates (10-25%), which is critical for accurate simulations
- Computational cost is manageable (few CPU-hours for 10⁵ environments) and the algorithm is implemented in the open-source QUESTS package

## Why This Works (Mechanism)
The method works by quantifying the information content of atomic environments using differential entropy, then greedily selecting structures that maximize coverage of the environment space. By formulating compression as a set cover problem, it ensures that each selected structure contributes unique information not already represented in the compressed set. This information-theoretic approach naturally preserves outliers and diverse structures that might be lost with clustering-based methods.

## Foundational Learning
- **Information entropy in atomic environments**: Measures uncertainty in descriptor distributions; needed to quantify information content of structures; quick check: verify entropy values are positive and correlate with structural complexity
- **Differential entropy**: Extension of discrete entropy to continuous spaces; needed for measuring information in descriptor vectors; quick check: confirm bandwidth h=0.015 Å⁻¹ produces smooth density estimates
- **Minimum Set Cover problem**: Combinatorial optimization framework; needed to formulate optimal structure selection; quick check: verify greedy algorithm converges to reasonable solution size
- **Kernel density estimation**: Non-parametric method for estimating probability distributions; needed for computing entropy values; quick check: test different bandwidth values to ensure stability
- **Atom-centered descriptors**: Mathematical representation of local atomic environments; needed as input to entropy calculations; quick check: verify 2-body and 3-body terms are properly concatenated
- **Greedy algorithm approximation**: Heuristic approach to NP-hard optimization; needed for tractable computation; quick check: compare results with random sampling baseline

## Architecture Onboarding

**Component map:** Descriptor computation -> Entropy calculation -> MSC greedy selection -> Validation (overlap/diversity)

**Critical path:** The MSC greedy selection algorithm is the core component where structures are iteratively added based on maximizing differential entropy coverage against the current compressed set

**Design tradeoffs:** Computational cost vs. compression quality (MSC is more expensive than random sampling but yields better preservation of diversity and outliers); descriptor complexity vs. entropy estimation accuracy (richer descriptors provide better coverage but increase computational burden)

**Failure signatures:** Poor overlap at extreme compression ratios (e.g., 10% compression yielding 0.08 overlap); high computational cost as compressed set grows; sensitivity to bandwidth parameter h in kernel density estimation

**First experiments:** 1) Run MSC on a small dataset (e.g., 100 structures) and compare diversity/overlap with random sampling; 2) Test MSC at different compression ratios (10%, 25%, 50%) on the same dataset; 3) Verify that long-tail force distributions are preserved by comparing force histograms between compressed and full datasets

## Open Questions the Paper Calls Out
- **Can a mathematically rigorous definition of dataset diversity be formulated** that guarantees a non-decreasing relationship with dataset size in continuous atomic environment spaces? The current diversity metric is an approximation that doesn't strictly satisfy monotonicity properties, suggesting the need for more rigorous mathematical formulations.

- **How can the computational complexity of the MSC algorithm be optimized** to handle datasets containing millions of atomic environments without sacrificing selection precision? While the current implementation is tractable for 10⁵-10⁶ environments, approximate algorithms may be needed for "foundation" datasets with millions of structures.

- **Do the observed correlations between differential entropy and model generalization error** hold consistently across different neural network architectures and multi-component chemical systems? The validation was primarily on single-component systems using SevenNet, leaving open questions about generalizability to complex chemistries.

## Limitations
- Assumes atomic environment descriptors fully capture structural diversity relevant to MLIP performance, which may break down for systems with important long-range correlations or electronic structure effects
- Greedy MSC algorithm provides no guarantee of finding globally optimal compression set due to the NP-hard nature of the underlying optimization problem
- Computational cost scales with compressed set size since differential entropy must be computed against all selected structures, making it more expensive than trivial baseline methods

## Confidence
- **High Confidence**: MSC algorithm outperforming baseline methods in diversity retention and overlap metrics across 64 datasets with consistent results
- **Medium Confidence**: MLIP error reduction for out-of-distribution data is well-supported for low-data regimes but needs validation across more diverse architectures beyond SevenNet
- **Medium Confidence**: Computational efficiency claims are plausible given algorithmic complexity but depend on unspecified hardware configurations and implementation details

## Next Checks
1. **Replicate MSC vs. Baselines**: Reproduce compression experiments on 3-5 diverse datasets (combining GAP-20 and ColabFit subsets) to verify consistent superiority in diversity retention and overlap metrics
2. **Test MLIP Architecture Independence**: Validate that error reduction for out-of-distribution data holds when training compressed datasets with different MLIP architectures beyond SevenNet
3. **Analyze Long-tail Force Distribution Preservation**: Quantitatively assess preservation of long-tail force distributions across multiple compression ratios using statistical tests (e.g., KS test) on force histograms for 3 representative datasets