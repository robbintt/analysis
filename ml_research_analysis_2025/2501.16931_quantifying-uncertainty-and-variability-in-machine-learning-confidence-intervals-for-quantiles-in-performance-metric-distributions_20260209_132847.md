---
ver: rpa2
title: 'Quantifying Uncertainty and Variability in Machine Learning: Confidence Intervals
  for Quantiles in Performance Metric Distributions'
arxiv_id: '2501.16931'
source_url: https://arxiv.org/abs/2501.16931
tags:
- quantile
- level
- sample
- confidence
- bootstrap
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the problem of quantifying uncertainty and
  variability in machine learning model performance metrics. The authors propose analyzing
  the distribution of performance metrics (such as accuracy, F1 score, or RMSE) across
  multiple training runs with varied conditions (different train-test splits, initial
  weights, hyperparameters, etc.) rather than relying on single-point estimates.
---

# Quantifying Uncertainty and Variability in Machine Learning: Confidence Intervals for Quantiles in Performance Metric Distributions

## Quick Facts
- arXiv ID: 2501.16931
- Source URL: https://arxiv.org/abs/2501.16931
- Reference count: 12
- Authors: Christoph Lehmann; Yahor Paromau
- One-line primary result: Quantile analysis of ML performance metrics with confidence intervals provides more comprehensive insights than single-point estimates

## Executive Summary
This work addresses the problem of quantifying uncertainty and variability in machine learning model performance metrics by analyzing the distribution of metrics across multiple training runs with varied conditions. Instead of relying on single-point estimates, the authors propose estimating quantiles and their confidence intervals from empirical distributions of performance metrics. The study demonstrates that with sample sizes of 15-25, it is possible to reliably estimate quantiles up to the 90% level, and that analyzing quantiles provides more comprehensive insights into model performance than traditional single-value metrics.

## Method Summary
The method involves running ML pipelines multiple times while varying confounding factors like train-test splits, initialization, and hyperparameters through controlled random number generator states. This generates an empirical distribution of the Target Metric of Interest (TMoI). The authors present two point estimators for quantiles (sample quantile and linear interpolated estimator) and three nonparametric approaches for constructing confidence intervals: exact nonparametric intervals using order statistics and binomial distribution, asymptotic nonparametric intervals based on normality assumptions, and semiparametric bootstrap intervals with tail extrapolation. The approach shifts analysis from single-point metric optimization to quantile estimation, allowing quantification of stability and risk.

## Key Results
- With sample sizes of 15-25, reliable estimation of quantiles up to 90% level is possible
- Nonparametric exact intervals provide exceptional empirical coverage across all sample sizes
- Semiparametric bootstrap intervals offer practical alternatives when exact methods fail due to small sample sizes
- Analysis of quantiles and their uncertainties provides more comprehensive insights than traditional single-value metrics

## Why This Works (Mechanism)

### Mechanism 1: Distributional Perspective via Seed-Controlled Repetition
Shifting analysis from single-point metric optimization to quantile estimation of an empirical distribution allows for quantification of stability and risk. By running the ML pipeline n times while varying confounding factors (different train-test splits, initial weights, hyperparameters, etc.) via specific random number generator (RNG) states, one generates an empirical distribution of the Target Metric of Interest (TMoI). Analyzing the 90% quantile of this distribution provides a bound on worst-case performance ("risk-oriented") that the mean obscures. The core assumption is that repeated measurements are independent and identically distributed (i.i.d.) relative to the specific confounding factor being varied.

### Mechanism 2: Nonparametric Exact Confidence Intervals via Order Statistics
Exact nonparametric intervals provide robust coverage for quantiles without assuming the underlying metric distribution is Normal. This approach utilizes the ordered sample statistics X(1) ≤ ... ≤ X(n). It calculates the probability that the true quantile lies between two order statistics X(k) and X(l) using the Binomial distribution. It specifically employs a randomized optimization (linear programming) to select indices k, l that minimize interval length while guaranteeing the confidence level. The core assumption is that the TMoI follows a continuous distribution (smooth CDF), which justifies the probability calculations between discrete order statistics.

### Mechanism 3: Semiparametric Bootstrap for Tail Extrapolation
Semiparametric bootstrap enables interval estimation for extreme quantiles in small samples where exact methods fail, by mathematically extrapolating into the distribution tails. Standard bootstrap fails for quantiles because it cannot generate values outside the observed min/max. The semiparametric approach (Hutson's method) samples quantile levels from a Uniform distribution and transforms them using a tailored quantile function ˆQT(u). This function uses logarithmic extrapolation based on the nearest observed order statistics to generate synthetic "tail" values. The core assumption is that the tail behavior of the distribution can be approximated by the local geometry (spacing) of the extreme observed order statistics.

## Foundational Learning

- **Concept: Order Statistics**
  - Why needed here: The exact and asymptotic CI methods do not use the raw data values directly, but their sorted ranks (X(k)). Understanding that confidence is derived from the *positions* of values, not just their magnitude, is crucial.
  - Quick check question: If you have 10 samples, which specific sample value represents the estimate for the 50th percentile (median)? What about the 90th percentile?

- **Concept: Confidence Level vs. Quantile Level**
  - Why needed here: It is easy to confuse "90% Quantile" (the value below which 90% of data falls) with "90% Confidence Level" (the probability the interval contains the true value). This paper relies on distinguishing these to define "Uncertainty of the Quantile."
  - Quick check question: If I calculate a 95% Confidence Interval for the 50th Quantile, am I saying 95% of the data falls in this range? (Answer: No, I am saying I am 95% sure the *true median* lies in this range).

- **Concept: Random Number Generator (RNG) State Control**
  - Why needed here: The methodology relies on "seed-controlled train runs." Unlike standard cross-validation, this requires deep control over library-specific RNGs (NumPy vs. PyTorch vs. Python random) to ensure true independence and reproducibility.
  - Quick check question: If I set `torch.manual_seed(42)` but forget to set `numpy.random.seed(42)`, and my data augmentation uses NumPy, is my data split experiment reproducible?

## Architecture Onboarding

- **Component map:**
  Experiment Runner -> Collector -> Estimator Core -> Validator
  1. **Experiment Runner:** Executes n training runs, varying one confounding factor (e.g., data split) while controlling seeds across 7 potential RNGs.
  2. **Collector:** Aggregates n scalar TMoIs (e.g., Accuracy) into a vector X.
  3. **Estimator Core:**
     - *Point Estimator:* Sorts X and applies Linear Interpolation (Eq 5) or Sample Quantile (Eq 4).
     - *Interval Engine:* Routes to (A) Exact Binomial Optimizer (Eq 7), (B) Asymptotic Normal approximator (Eq 10), or (C) Semiparametric Bootstrap Generator (Eq 11).
  4. **Validator:** Checks if n meets minimum requirements (Table 1/2) for the selected quantile/confidence level.

- **Critical path:** The **RNG State Management** is the highest risk. The paper notes (Page 17) that 7 different RNGs may be involved (e.g., `torch.Generator` vs `numpy.random.RandomState`). If the state is not globally controlled, the "seed-controlled" assumption breaks, invalidating the i.i.d. sample.

- **Design tradeoffs:**
  - **Exact vs. Asymptotic:** Exact is conservative and robust but may fail to produce an interval for extreme quantiles at small n. Asymptotic works for smaller n but assumes large-sample convergence.
  - **Bootstrap Risk:** The Semiparametric Bootstrap produces intervals for small n where Exact cannot, but risks generating values outside natural metric bounds (e.g., >100% accuracy), which requires clipping logic.

- **Failure signatures:**
  - **Empty Interval:** Exact method returns None or fails because n is too small for the requested quantile level (constraint Eq 8 not met).
  - **Implausible Extrapolation:** Bootstrap intervals extend beyond [0, 1] for accuracy metrics (Page 15).
  - **Low Coverage:** If n=10, asymptotic intervals may undercover (empirical coverage < theoretical level).

- **First 3 experiments:**
  1. **Sanity Check (Mean vs. Quantile):** Run a classifier 25 times. Calculate the t-interval for the mean accuracy and the Exact CI for the 50th quantile. Verify they overlap significantly.
  2. **Stress Test (Sample Size):** Estimate the 90% quantile with confidence 0.9 using n=15. Compare the width of the Exact CI vs. the Semiparametric Bootstrap CI. Expect Bootstrap to be narrower but riskier.
  3. **Boundary Case:** Train a high-accuracy model (avg 0.95). Estimate the 99% quantile using Bootstrap. Check if the upper bound exceeds 1.0 and implement the necessary clipping logic.

## Open Questions the Paper Calls Out

### Open Question 1
Can semiparametric bootstrap methods be modified to handle distributions with two-sided bounded support (e.g., accuracy in [0,1]) without producing implausible extrapolated values? The authors state that the semiparametric bootstrap "could lead to non-plausible values for distributions with compact support as the extrapolation could exceed the boundaries" and note that prior work like [WWH15] "do not consider distributions with bounded support." Modified tail-extrapolation estimators that respect bounds, validated via simulations on Beta-like distributions showing empirical coverage comparable to unbounded cases would resolve this.

### Open Question 2
Does importance sampling for bootstrap CI bounds, as applied to other statistical quantities, improve quantile CI estimation accuracy for small samples? "Another extension could be the application of importance sampling for the CI bounds based on estimating quantiles from the bootstrap distribution as in [HS08]... Thus, the impact on quantile estimation needs to be investigated." Importance sampling has only been tested on regression parameters and correlations, not on quantiles specifically. Simulation study comparing standard semiparametric bootstrap CIs versus importance-sampling-enhanced versions across sample sizes n=10-50 and quantile levels (e.g., 5%, 95%) would resolve this.

### Open Question 3
How should simultaneous confidence intervals for multiple quantiles be constructed to account for dependency among quantiles? "An additional consideration and extension is the distinction between single interval estimations for specific quantiles of interest versus simultaneous confidence intervals... For quantile estimation, this distinction is important because of the dependency among quantiles." Current paper only addresses individual quantile CIs; quantiles from the same sample are correlated, affecting joint coverage. Implementation of simultaneous CI methods (referenced [Hay14]) tested on TMoI distributions, showing maintained family-wise coverage rates would resolve this.

### Open Question 4
Under what conditions can the normality assumption for TMoI distributions be safely used as a simpler alternative to nonparametric quantile methods? The authors observe that "real-data use cases suggest that a normality assumption may be reasonable in many cases" based on goodness-of-fit tests, but only tested limited use cases with mostly symmetric distributions. The paper does not systematically characterize when TMoI distributions deviate from normality (e.g., with specific optimizers like SGD with hyperparameter optimization showed left-skewed distributions). Broader empirical study across model types, optimizers, and datasets identifying factors that produce non-normal TMoI distributions would resolve this.

## Limitations

- **RNG State Control Complexity:** The methodology requires controlling 7 different RNG sources (NumPy, PyTorch, Python random, etc.) to ensure truly independent and reproducible seed-controlled runs, potentially affecting the i.i.d. assumption.
- **Extreme Quantile Estimation Issues:** The semiparametric bootstrap method can generate physically impossible values (e.g., accuracy > 1.0) in 33-80% of cases, requiring ad-hoc clipping that raises theoretical validity questions.
- **Small Sample Performance Constraints:** Exact nonparametric intervals fail to produce valid results for extreme quantiles (e.g., 95% quantile at 0.95 confidence level) when n < 50, limiting applicability for high-stakes applications.

## Confidence

- **Quantile Distribution Analysis:** High confidence - well-supported by empirical results showing reliable estimation of quantiles up to 90% level with n=15-25
- **Nonparametric Exact CI Performance:** High confidence - exceptional empirical coverage demonstrated across all sample sizes and theoretically grounded
- **Semiparametric Bootstrap Utility:** Medium confidence - effective for small samples but tendency to produce implausible values and reduced coverage (~0.85) limits reliability

## Next Checks

1. **RNG Independence Verification:** Implement a controlled experiment varying only RNG seeds across multiple ML frameworks to verify that performance metric distributions are truly independent and identically distributed, confirming the foundational assumption.

2. **Bounded Metric Correction Analysis:** Systematically evaluate the impact of clipping bootstrap-extrapolated values for bounded metrics (accuracy, F1) on interval validity and coverage, quantifying the tradeoff between theoretical soundness and practical utility.

3. **Extreme Quantile Robustness Test:** Conduct a comprehensive study with varying sample sizes (n=10, 15, 25, 50) to map the boundary conditions where exact methods fail and bootstrap coverage drops below acceptable thresholds, providing clear guidance for method selection.