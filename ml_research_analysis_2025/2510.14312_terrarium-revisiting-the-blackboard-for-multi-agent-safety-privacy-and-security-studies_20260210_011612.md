---
ver: rpa2
title: 'Terrarium: Revisiting the Blackboard for Multi-Agent Safety, Privacy, and
  Security Studies'
arxiv_id: '2510.14312'
source_url: https://arxiv.org/abs/2510.14312
tags:
- agents
- agent
- communication
- time
- your
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Terrarium, a modular framework for studying
  multi-agent system (MAS) safety, privacy, and security. It repurposes the blackboard
  architecture to create a configurable testbed where LLM-driven agents collaborate
  on instruction-augmented distributed constraint optimization problems.
---

# Terrarium: Revisiting the Blackboard for Multi-Agent Safety, Privacy, and Security Studies

## Quick Facts
- **arXiv ID**: 2510.14312
- **Source URL**: https://arxiv.org/abs/2510.14312
- **Reference count**: 23
- **Key outcome**: Modular framework for studying MAS safety, privacy, and security using blackboard architecture and instruction-augmented DCOPs

## Executive Summary
Terrarium introduces a modular framework for studying multi-agent system (MAS) safety, privacy, and security using a blackboard-based architecture. The system repurposes the classical blackboard design to create a configurable testbed where LLM-driven agents collaborate on instruction-augmented distributed constraint optimization problems (DCOPs). The framework enables systematic evaluation of attack vectors including misalignment, data stealing, and denial-of-service across three domains: Meeting Scheduling, Personal Assistant, and Smart Home. Experiments demonstrate solid utility performance while showing the feasibility of targeted attacks, with the modular design enabling rapid prototyping of defenses.

## Method Summary
Terrarium uses a blackboard architecture where LLM-driven agents communicate through shared blackboards rather than direct messaging, enabling fine-grained observation and control of interactions. The framework frames multi-agent tasks as instruction-augmented DCOPs, providing a ground-truth global objective function for quantitative attack evaluation. Agents are instantiated with specific instructions and tools, including Model Context Protocol (MCP) capabilities for blackboard communication and environmental actions. The system runs in two phases: planning (agents communicate via blackboards) and execution (agents take actions in the environment). Three domains are implemented with synthetic and real-world data, and attacks are evaluated using normalized joint utility and Attack Success Rate metrics.

## Key Results
- Agents achieve solid utility performance across three domains (Meeting Scheduling, Personal Assistant, Smart Home) with normalized joint utility scores demonstrating effective collaboration
- Confidentiality attacks show high success rates (100%/50%/0% accuracy for benign/adversarial/malicious agents) when tested with LLM-as-a-judge
- Integrity attacks successfully reduce global utility, though impact remains relatively weak in magnitude
- Availability attacks cause context overflow and API errors when message volume exceeds agent LLM context windows

## Why This Works (Mechanism)

### Mechanism 1: Centralized Blackboard Communication Proxy
A shared, structured blackboard workspace enables fine-grained observation and control of inter-agent communication, which is critical for systematically evaluating safety and security vulnerabilities in MAS. The architecture decouples agents from direct pairwise messaging, creating a central, inspectable log of all interactions that allows researchers to inject attacks, modify communication state, and audit complete collaboration trajectories.

### Mechanism 2: Instruction-Augmented DCOPs as a Formal Evaluation Backbone
Framing multi-agent tasks as instruction-augmented DCOPs provides a ground-truth global objective, enabling quantitative measurement of attack impact through utility loss and constraint violations. The framework defines a formal cooperative objective function composed of factor functions based on agent variables, allowing precise measurement of how suboptimal or malicious behavior negatively impacts measurable utility.

### Mechanism 3: Modular Design for Rapid Attack and Defense Iteration
The modular separation of agents, environment, communication proxy, tools, and protocol enables researchers to independently configure and stress-test each component, accelerating defense development. By decoupling components, researchers can introduce malicious agents, poison communication logs, or change environmental rules without re-architecting the entire system, enabling focused experimentation on specific components.

## Foundational Learning

- **Concept: Blackboard Architecture (from Classical AI)**
  - Why needed here: The paper's core architectural choice is to "revisit" this classical design. Understanding that a blackboard is a shared memory space where independent "knowledge sources" (agents) post and read information is essential to grasp how Terrarium decouples agents and centralizes control.
  - Quick check question: How does a blackboard system differ from a direct peer-to-peer messaging model in terms of coupling and observability?

- **Concept: Distributed Constraint Optimization Problems (DCOPs)**
  - Why needed here: The paper uses "instruction-augmented DCOPs" as its problem formalism. Understanding that a DCOP involves multiple agents controlling variables to maximize a global objective function defined over constraints is essential.
  - Quick check question: In a DCOP, does a single agent have access to the global utility function directly, or does it only have local information? How does this create a security surface?

- **Concept: CIA Triad in the Context of LLM Agents**
  - Why needed here: The paper explicitly maps attack vectors to Confidentiality, Integrity, and Availability (CIA). "Integrity" is explored through "misalignment" (degrading task utility), not just data corruption.
  - Quick check question: How does the paper's "Integrity" attack, involving an adversarial agent modifying messages to lower joint utility, map to traditional cybersecurity concepts of integrity?

## Architecture Onboarding

- **Component map**: Agents -> Blackboards -> Environment -> Tools -> Communication Protocol
- **Critical path**:
  1. Define a problem (e.g., Meeting Scheduling) by specifying DCOP variables, domains, and factor graph
  2. Instantiate N agents and assign to blackboards per factor graph
  3. Run simulation: agents communicate via blackboard tools (planning), then execute environment actions
  4. Environment evaluates final joint action against ground-truth objective to compute utility
  5. To attack: modify a component (replace agent with malicious one, inject false messages) and re-run

- **Design tradeoffs**:
  - Centralization vs. Realism: Blackboard centralizes communication for observation/control (strength) but reduces realism compared to direct/federated messaging
  - Modularity vs. Simplicity: Highly modular design enables flexible experimentation but adds complexity; authors note this is for testbed use, not optimized production deployment
  - DCOP Backbone vs. General Tasks: DCOPs provide clear quantitative objectives for attack evaluation but limit scope to problems fitting this formalism

- **Failure signatures**:
  - Incomplete Evaluation: Smaller models fail to assign values to all variables, creating invalid trajectories requiring filtering
  - Context Overflow: Availability attacks succeed when message volume exceeds agent's LLM context window, causing API errors
  - Misalignment (Low Utility): Integrity attacks manifest as measurable drops in final joint-utility score compared to baseline

- **First 3 experiments**:
  1. Reproduce the Baseline: Run Meeting Scheduling with benign agents (GPT-4.1-mini) and report normalized joint utility to confirm setup
  2. Implement a Confidentiality Attack: Design a test agent that queries another agent for private time preferences; measure information leakage using LLM-as-a-judge
  3. Implement an Integrity Attack: Replace one benign agent with an adversarial agent instructed to suggest suboptimal meeting times; measure utility drop compared to baseline

## Open Questions the Paper Calls Out

### Open Question 1
How do attack vectors differ in competitive or negotiation-based multi-agent environments compared to the cooperative DCOPs currently implemented in Terrarium? The authors state in "Limitations and Extensions" that they "plan to explore other MAS environments that involve competition and negotiation" in future work, but the current study strictly focuses on cooperative instruction-augmented DCOPs, leaving self-interested agent dynamics unexplored.

### Open Question 2
Can modular defense mechanisms be integrated into Terrarium to effectively mitigate the observed high success rates of confidentiality and availability attacks? While the paper aims to help "iterate on defenses," the experimental section focuses on quantifying vulnerabilities (e.g., 100% ASR for data leakage) without implementing or validating specific defensive countermeasures.

### Open Question 3
Why do adversarial agent and communication poisoning attacks result in non-significant utility drops despite successful execution? The authors note in the "Experiments" section that the reduction in utility for integrity attacks was "not significantâ€”indicating that while these attacks succeed, their impact remains relatively weak," but do not analyze whether system robustness, agent reasoning, or attack design caused the limited impact.

## Limitations
- The blackboard proxy may lose observability advantage if agents develop out-of-band communication channels
- DCOP formalism may not capture all real-world MAS tasks, particularly those requiring open-ended collaboration or creative problem-solving
- Attack implementations are described conceptually but lack precise implementation details, affecting reproducibility

## Confidence

- **High Confidence**: The modular blackboard architecture effectively enables fine-grained control and observation of agent communication for vulnerability analysis
- **Medium Confidence**: Instruction-augmented DCOPs provide a valid quantitative framework for measuring attack impact on global utility, though scope may be limited to well-defined optimization problems
- **Medium Confidence**: The three-domain experimental setup demonstrates baseline utility performance and attack feasibility, though reproducibility threshold remains unclear due to missing implementation specifics

## Next Checks

1. **Implementation Completeness**: Verify that the GitHub repository contains all necessary code for blackboard tool definitions, factor graph specifications, and attack implementations described in the paper

2. **Baseline Replication**: Run the Meeting Scheduling domain with GPT-4.1-mini across all 30 seeds and confirm normalized joint utility falls within the reported range

3. **Attack Vector Fidelity**: Implement one attack (e.g., the confidentiality information-leakage test) and measure LLM-as-a-judge accuracy against the paper's reported 100%/50%/0% results for benign/adversarial/malicious cases