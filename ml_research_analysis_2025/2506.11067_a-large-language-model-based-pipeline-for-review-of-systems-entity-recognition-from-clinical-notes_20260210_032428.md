---
ver: rpa2
title: A Large Language Model Based Pipeline for Review of Systems Entity Recognition
  from Clinical Notes
arxiv_id: '2506.11067'
source_url: https://arxiv.org/abs/2506.11067
tags:
- attribution
- entity
- sample
- pipeline
- systems
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study introduces a pipeline for extracting Review of Systems
  (ROS) entities from clinical notes using open-source large language models (LLMs)
  and a novel attribution algorithm. The pipeline segments ROS sections, extracts
  and classifies diseases/symptoms, detects negation, and links extracted entities
  to their source text.
---

# A Large Language Model Based Pipeline for Review of Systems Entity Recognition from Clinical Notes

## Quick Facts
- arXiv ID: 2506.11067
- Source URL: https://arxiv.org/abs/2506.11067
- Reference count: 0
- Primary result: Up to 85.6% exact-match and 97.6% relaxed-match accuracy for ROS entity extraction using open-source LLMs

## Executive Summary
This paper introduces a pipeline that extracts Review of Systems (ROS) entities from clinical notes using open-source large language models (LLMs) and a novel attribution algorithm. The pipeline segments ROS sections, extracts and classifies diseases/symptoms, detects negation, and links extracted entities to their source text. Evaluated on 24 clinical notes with 340 annotated entities, the pipeline achieved up to 85.6% exact-match and 97.6% relaxed-match accuracy for entity recognition. The attribution algorithm improved performance by correcting rephrased or reordered entities, reducing error rates significantly. The approach offers a scalable, cost-effective solution for easing clinical documentation burden.

## Method Summary
The pipeline uses three sequential tasks with few-shot prompting: ROS section extraction via SecTag header terminology, entity recognition with negation detection using open-source LLMs (Llama3.1:8b, Gemma3:27b, Mistral3.1:24b, gpt-oss:20b), and body system classification. A novel attribution algorithm maps LLM outputs back to original text spans via semantic similarity using sentence-transformer embeddings. The system processes 24 general medicine notes from MTSamples, achieving up to 85.6% exact-match and 97.6% relaxed-match accuracy. Models were run via Ollama and LangChain with temperature=0, seed=42, top-k=10, top-p=0.5 settings.

## Key Results
- Best F1 score: 0.952 (relaxed match with attribution, Gemma3:27b)
- Exact-match accuracy: up to 85.6% across all models
- Attribution algorithm reduces error rates by 8-16 percentage points
- Gemma3:27b and Mistral3.1:24b showed highest F1 scores (0.906, 0.884)

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Few-shot prompting enables open-source LLMs to perform ROS entity extraction without task-specific fine-tuning.
- **Mechanism:** In-context learning provides examples within prompts that guide the model toward desired extraction behavior and JSON output format. The model relies on pre-trained language patterns rather than weight updates.
- **Core assumption:** The LLM's pre-training corpus contains sufficient medical terminology and entity-extraction patterns to generalize from few examples.
- **Evidence anchors:**
  - [abstract] "few-shot LLMs to identify ROS entities such as diseases or symptoms, their positive/negative status and associated body systems"
  - [PAGE 5] "Zero-shot and few-shots learning settings in LLMs have been increasingly adopted in clinical NLP tasks such as entity recognition and information extraction... The model relies purely on its general knowledge and understanding of language patterns from pretraining"
  - [corpus] Related work on "LLM-based Prompt Ensemble for Reliable Medical Entity Recognition from EHRs" supports prompt-based extraction approaches, though direct comparison is limited.
- **Break condition:** If entity vocabulary extends far beyond common medical terminology seen during pre-training, few-shot performance may degrade significantly.

### Mechanism 2
- **Claim:** The attribution algorithm corrects LLM rephrasing and reordering errors by mapping outputs back to original text spans via semantic similarity.
- **Mechanism:** For each LLM-identified entity that doesn't exactly match source text, the algorithm enumerates all n-grams (n=1 to m tokens), computes embeddings using all-MiniLM-L6-v2, and selects the n-gram with highest cosine similarity to the LLM output.
- **Core assumption:** Rephrased entities share sufficient semantic similarity with their source spans for embedding-based matching to succeed.
- **Evidence anchors:**
  - [abstract] "a novel attribution algorithm that aligns LLM-identified ROS entities with their source text, addressing non-exact and synonymous matches"
  - [PAGE 10] "The core idea of our proposed attribution algorithm is to locate the n-gram within a document span that is most similar to the LLM-generated ROS entity"
  - [PAGE 13] Table 2 shows error rates dropping from 25.6-43.8% (without attribution) to 17.6-27.9% (with attribution) for exact matches
  - [corpus] No direct corpus evidence for this specific attribution approach; this appears novel.
- **Break condition:** If LLM hallucinates entities with no corresponding span in source text, the algorithm will map to semantically similar but incorrect spans (e.g., "fever" mapped to "nausea" in example on PAGE 16).

### Mechanism 3
- **Claim:** Modular pipeline separation reduces prompt complexity and improves per-task accuracy.
- **Mechanism:** The pipeline splits extraction into three sequential tasks—segmentation (SecTag), entity recognition, body system classification—rather than combining them in single prompts. This prevents "overly lengthy prompts, which in our preliminary experiments increased confusion and reduced model accuracy."
- **Core assumption:** Errors do not compound catastrophically across pipeline stages; early-stage errors can be partially recovered.
- **Evidence anchors:**
  - [PAGE 9] "Note that we separated this classification step from the prior Disease and Symptom Recognition step to avoid overly lengthy prompts, which in our preliminary experiments increased confusion and reduced model accuracy"
  - [PAGE 12-15] Results show consistent performance across all three tasks, suggesting modularity does not severely propagate errors
  - [corpus] "Semantic NLP Pipelines for Interoperable Patient Digital Twins" similarly uses modular NLP pipelines for EHR processing, supporting the architectural pattern.
- **Break condition:** If Segmentation (SecTag) fails to capture complete ROS sections, downstream stages miss entities—PAGE 16 notes one note where "ROS section was only partially captured by SecTag, resulting in 15 under-detected entities."

## Foundational Learning

- **Concept: Few-shot in-context learning**
  - Why needed here: The pipeline avoids fine-tuning costs by using prompt-embedded examples to guide extraction behavior.
  - Quick check question: Can you explain why temperature=0 is set for the LLM calls, and what tradeoff this introduces?

- **Concept: Named Entity Recognition (NER) evaluation metrics**
  - Why needed here: Understanding exact-match vs. relaxed-match, precision/recall/F1, and error rate calculation is essential for interpreting the reported 85.6% / 97.6% accuracy claims.
  - Quick check question: What is the difference between an "over detection" (false positive) and "under detection" (false negative) in this context?

- **Concept: Embedding-based semantic similarity**
  - Why needed here: The attribution algorithm relies on cosine similarity between sentence-transformer embeddings to match rephrased entities to source spans.
  - Quick check question: Why might cosine similarity on embeddings fail to distinguish between "fever" and "nausea" in a clinical context?

## Architecture Onboarding

- **Component map:** SecTag header terminology → extracts ROS section boundaries from clinical notes → Few-shot LLM (Ollama + LangChain) → extracts disease/symptom entities + negation status, outputs JSON → Few-shot LLM → classifies entities into 14 body systems → all-MiniLM-L6-v2 embeddings + cosine similarity → maps rephrased entities back to source n-grams

- **Critical path:** Segmentation → Entity Recognition → Attribution → Body System Classification. If Segmentation fails, all downstream steps operate on incomplete text.

- **Design tradeoffs:**
  - Larger models (Gemma3:27b, Mistral3.1:24b) achieve higher F1 (0.906, 0.884) but require 15-17GB VRAM vs. Llama3.1:8b at 5GB VRAM with F1=0.850
  - Exact-match evaluation is stricter but clinically more valuable; relaxed-match inflates accuracy by accepting overlapping spans
  - Attribution adds ~2.2 seconds per note but reduces error rates by 8-16 percentage points

- **Failure signatures:**
  - SecTag partial capture: PAGE 16 documents 15 missed entities from one note with incomplete section detection
  - Hallucination mapping: Attribution maps hallucinated entities ("fever", "headache") to wrong spans ("nausea", "vomiting")
  - Rephrasing beyond recovery: If LLM output shares insufficient semantic overlap with source, attribution fails

- **First 3 experiments:**
  1. **Baseline validation:** Run the pipeline on the provided 24-note dataset (available on GitHub) with Llama3.1:8b to reproduce F1=0.850 exact-match results; verify your Ollama + LangChain setup.
  2. **Ablation study:** Disable the attribution layer and measure error rate increase; expect 8-16 percentage point degradation per Table 2.
  3. **Section boundary stress test:** Manually truncate ROS sections in sample notes to simulate SecTag failures; measure under-detection rate increase to quantify segmentation criticality.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How does pipeline performance vary across diverse medical specialties and different EHR systems?
- **Basis in paper:** [explicit] The authors note the study used a "single data source" focused on "general medicine," limiting generalizability.
- **Why unresolved:** MTSamples data may not reflect the structural and linguistic variability of notes found in other specialties or institutions.
- **What evidence would resolve it:** Evaluation on multi-institutional datasets covering a wider range of clinical domains.

### Open Question 2
- **Question:** Can the current methodology be effectively adapted for History of Present Illness (HPI) entity recognition?
- **Basis in paper:** [explicit] The conclusion identifies adaptation to HPI as a specific focus for "future work."
- **Why unresolved:** HPI sections often contain complex narratives that may differ structurally from the list-like format of ROS sections.
- **What evidence would resolve it:** Benchmarks showing F1 scores and error rates when the pipeline is applied to annotated HPI sections.

### Open Question 3
- **Question:** What are the actual time savings and documentation burden reductions for clinicians in a live deployment?
- **Basis in paper:** [explicit] The limitations section states that error rates are only an "approximation" and actual time savings require "practical trials."
- **Why unresolved:** Metric-based performance (e.g., F1 score) does not directly measure user workflow efficiency or cognitive load.
- **What evidence would resolve it:** User studies measuring documentation time with and without the tool in a real-world clinical setting.

## Limitations

- Attribution algorithm may map hallucinated entities to incorrect but semantically similar spans
- Pipeline relies on SecTag for ROS section detection, with documented failures causing 15 missed entities in one case
- Evaluation on only 24 clinical notes from a single source limits generalizability to broader clinical documentation contexts

## Confidence

- **High confidence:** Few-shot prompting enables effective ROS entity extraction without fine-tuning (supported by consistent performance across multiple open-source models)
- **Medium confidence:** Attribution algorithm significantly improves accuracy by mapping rephrased entities (shown to reduce error rates by 8-16 percentage points, though semantic similarity limitations acknowledged)
- **Medium confidence:** Modular pipeline architecture improves per-task accuracy (supported by performance metrics, though error compounding from segmentation failures documented)

## Next Checks

1. **Attribution robustness test:** Run the pipeline with and without attribution on a held-out test set, measuring error rate changes and identifying cases where semantic similarity mapping fails (e.g., hallucinated entities mapped to incorrect spans)

2. **Cross-institutional generalization:** Evaluate the pipeline on ROS sections from a different clinical note corpus (different institution or specialty) to assess performance degradation and identify vocabulary/domain-specific limitations

3. **Section boundary completeness audit:** Systematically truncate ROS sections in test notes to simulate SecTag failures, measuring the cascade effect on downstream entity detection and classification accuracy