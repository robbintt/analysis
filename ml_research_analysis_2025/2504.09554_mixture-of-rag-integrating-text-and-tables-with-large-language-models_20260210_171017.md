---
ver: rpa2
title: 'Mixture-of-RAG: Integrating Text and Tables with Large Language Models'
arxiv_id: '2504.09554'
source_url: https://arxiv.org/abs/2504.09554
tags:
- retrieval
- document
- table
- documents
- reasoning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces MixRAG, a framework designed to improve
  retrieval-augmented generation (RAG) for heterogeneous documents containing both
  text and hierarchical tables. The proposed approach addresses limitations in existing
  RAG systems that struggle with the complex structure of mixed-modality documents
  by introducing a three-stage framework: (1) hierarchical row-and-column-level (H-RCL)
  representation that preserves table structure and relationships, (2) ensemble retrieval
  combining BM25 and embedding-based methods with LLM-based reranking, and (3) multi-step
  reasoning using a RECAP prompt strategy.'
---

# Mixture-of-RAG: Integrating Text and Tables with Large Language Models

## Quick Facts
- arXiv ID: 2504.09554
- Source URL: https://arxiv.org/abs/2504.09554
- Authors: Chi Zhang; Qiyang Chen; Mengqi Zhang
- Reference count: 40
- Primary result: 46% improvement in top-1 retrieval accuracy over baselines for mixed-modality documents

## Executive Summary
MixRAG introduces a novel framework for handling heterogeneous documents containing both text and hierarchical tables, addressing a significant gap in current retrieval-augmented generation (RAG) systems. The framework employs a three-stage approach that preserves table structure while integrating multiple retrieval strategies and LLM-based reasoning. Using the newly introduced DocRAGLib dataset of 2,178 heterogeneous documents, MixRAG demonstrates state-of-the-art performance with a 32.28% exact match score, representing a 6.97% absolute improvement over the best baseline. The system shows particular strength in handling the complex relationships inherent in mixed-modality documents.

## Method Summary
MixRAG addresses heterogeneous document retrieval through a three-stage framework. First, it employs Hierarchical Row-and-Column-Level (H-RCL) representation that flattens tables while preserving structural relationships and generates context-aware embeddings. Second, it implements ensemble retrieval combining BM25 for keyword matching and embedding-based methods for semantic relevance, with an LLM-based reranker to select optimal evidence. Third, it uses a RECAP prompt strategy to guide multi-step reasoning and extract relevant table rows. The framework introduces DocRAGLib, a dataset of 2,178 heterogeneous documents with 4,468 QA pairs, to support research in this domain. MixRAG also includes a RankFormer model for context-aware table row ranking.

## Key Results
- Achieves 46% improvement in top-1 retrieval accuracy over strong baselines
- Establishes new state-of-the-art performance for mixed-modality document grounding with 32.28% EM score
- Demonstrates superior scalability and efficiency compared to iterative approaches
- Introduces DocRAGLib dataset with 2,178 heterogeneous documents and 4,468 QA pairs

## Why This Works (Mechanism)
The effectiveness of MixRAG stems from its comprehensive approach to heterogeneous document processing. By preserving table structure through H-RCL representation, the system maintains critical relationships that would be lost in traditional text-only embeddings. The ensemble retrieval strategy combines the strengths of both keyword-based and semantic approaches, while LLM reranking ensures the most relevant evidence is selected. The RECAP prompt strategy enables effective reasoning over the retrieved evidence, allowing the model to handle the complex interplay between text and tabular data that characterizes heterogeneous documents.

## Foundational Learning

**Hierarchical Row-and-Column-Level (H-RCL) Representation**: A method that flattens tables while preserving structural relationships and generating context-aware embeddings. Why needed: Traditional text-only embeddings fail to capture the complex relationships in hierarchical tables. Quick check: Verify that table relationships and hierarchies are preserved after flattening by examining output embeddings.

**Ensemble Retrieval**: Combining multiple retrieval methods (BM25 and embedding-based) with LLM-based reranking. Why needed: No single retrieval method optimally handles both keyword and semantic queries across heterogeneous documents. Quick check: Compare retrieval performance of individual methods versus the ensemble to confirm improvement.

**RECAP Prompt Strategy**: A structured prompting approach for multi-step reasoning over retrieved evidence. Why needed: Standard prompting approaches struggle with the complex reasoning required for mixed-modality documents. Quick check: Test reasoning accuracy with and without the RECAP strategy on sample questions.

## Architecture Onboarding

**Component Map**: Document Preprocessing -> H-RCL Representation -> Ensemble Retrieval (BM25 + Embedding + Rerank) -> RECAP Reasoning -> Final Answer Generation

**Critical Path**: Document preprocessing with H-RCL representation forms the foundation, followed by ensemble retrieval for evidence selection, culminating in RECAP-guided reasoning for answer generation. The LLM reranking step is critical for ensuring high-quality evidence selection.

**Design Tradeoffs**: The framework trades increased preprocessing complexity for improved retrieval accuracy. While the H-RCL representation adds computational overhead, it enables more accurate retrieval by preserving table structure. The ensemble approach requires more computation than single-method retrieval but yields significantly better results.

**Failure Signatures**: The system may struggle with highly nested tables where relationships become too complex to preserve through flattening. It may also underperform on documents where text and tables are poorly integrated or when questions require extensive cross-referencing between multiple tables.

**First 3 Experiments to Run**:
1. Compare retrieval accuracy using only BM25, only embedding-based methods, and the ensemble approach to quantify the contribution of each component
2. Test the system with questions that require only text understanding versus those requiring table comprehension to assess modality-specific performance
3. Evaluate the impact of the LLM reranking step by comparing results with and without this component

## Open Questions the Paper Calls Out
The paper does not explicitly call out specific open questions in the provided content.

## Limitations
- Evaluation primarily focused on the DocRAGLib dataset, limiting generalizability assessment
- Claims of 46% improvement lack validation on additional heterogeneous document benchmarks
- Computational efficiency comparisons only relative to iterative approaches, without broader benchmarking against other state-of-the-art RAG systems

## Confidence
- **High confidence**: The methodological framework (H-RCL representation, ensemble retrieval, RECAP prompt strategy) is well-defined and technically sound
- **Medium confidence**: The performance improvements over baselines are substantial but require validation on additional datasets
- **Medium confidence**: The efficiency claims relative to iterative approaches are reasonable but need broader benchmarking

## Next Checks
1. Test MixRAG on established heterogeneous document benchmarks beyond DocRAGLib to assess generalizability across different document types and domains
2. Conduct ablation studies to quantify the individual contributions of the H-RCL representation, ensemble retrieval, and RECAP prompt strategy components
3. Benchmark MixRAG against other leading RAG systems using standardized efficiency metrics (e.g., latency, memory usage) on identical hardware configurations