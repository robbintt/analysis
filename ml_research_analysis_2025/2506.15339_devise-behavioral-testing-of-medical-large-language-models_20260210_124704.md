---
ver: rpa2
title: 'DeVisE: Behavioral Testing of Medical Large Language Models'
arxiv_id: '2506.15339'
source_url: https://arxiv.org/abs/2506.15339
tags:
- clinical
- notes
- counterfactual
- patient
- severity
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: DeVisE is a behavioral testing framework that evaluates medical
  large language models using controlled counterfactuals. It creates admission notes
  from ICU discharge summaries and generates counterfactuals by perturbing single
  clinical variables such as demographics and vital signs.
---

# DeVisE: Behavioral Testing of Medical Large Language Models

## Quick Facts
- arXiv ID: 2506.15339
- Source URL: https://arxiv.org/abs/2506.15339
- Authors: Camila Zurdo Tagliabue; Heloisa Oss Boll; Aykut Erdem; Erkut Erdem; Iacer Calixto
- Reference count: 23
- Primary result: Behavioral testing framework using counterfactuals reveals that zero-shot medical LLMs show more coherent clinical reasoning patterns than fine-tuned models, with demographic factors subtly influencing predictions.

## Executive Summary
DeVisE introduces a behavioral testing framework that evaluates medical large language models using controlled counterfactuals. The framework creates admission notes from ICU discharge summaries and generates counterfactuals by perturbing single clinical variables such as demographics and vital signs. Models are assessed through two approaches: task-independent evaluation using perplexity shifts and task-specific evaluation using mortality and length-of-stay prediction changes. The evaluation reveals that zero-shot models exhibit more coherent reasoning patterns with perplexity increasing consistently with severity, while fine-tuned models show greater stability but reduced sensitivity to clinically meaningful changes. Demographic factors subtly influence predictions, highlighting fairness concerns.

## Method Summary
The framework extracts admission notes from MIMIC-IV discharge summaries, extracting demographics from structured data and vital signs from PHYSICAL EXAM sections using LLaMA-3.3-70B-Instruct. Counterfactuals are generated by perturbing single clinical variables across severity bins (very low to very high) with 5 samples per bin. Evaluation uses zero-shot inference on 8 LLMs with both task-independent perplexity analysis and task-specific mortality/LoS prediction assessment. Template-based notes amplify model sensitivity compared to raw notes by removing contextual confounders. The framework computes KL divergence, label flip rates, correct directionality, and monotonicity metrics to characterize model behavior.

## Key Results
- Zero-shot models show perplexity increasing consistently with clinical severity, while fine-tuned models exhibit greater stability but less sensitivity to meaningful changes
- Template-based notes amplify sensitivity to vital sign changes (KL 0.15→1.34) compared to raw notes, suggesting contextual information dilutes single-variable signals
- Demographic factors influence predictions with label flip rates up to 23% near decision boundaries, highlighting fairness concerns in clinical decision support
- General-purpose models display higher sensitivity but less consistent directionality than medical or reasoning-focused models

## Why This Works (Mechanism)

### Mechanism 1
Single-variable counterfactuals isolate model reasoning about specific clinical variables. By perturbing exactly one variable while holding all else constant, changes in model output can be causally attributed to that variable, controlling for confounding from natural text variation. Models that understand clinical semantics should adjust outputs proportionally to clinical severity of perturbations.

### Mechanism 2
Perplexity shifts serve as a task-independent probe of internal model sensitivity to clinical plausibility. Perplexity measures how "surprised" a language model is by input text. Clinically implausible counterfactuals should increase perplexity relative to original notes, revealing whether models encode physiological priors without explicit task training.

### Mechanism 3
Template-based notes amplify model sensitivity to clinical variables by removing contextual confounders. Raw notes contain rich contextual information that dilutes the signal from any single variable. Templates strip away this context, forcing models to rely more heavily on explicitly presented variables, revealing underlying sensitivity patterns.

## Foundational Learning

- **Counterfactual evaluation in ML**:
  - Why needed here: DeVisE's entire framework depends on understanding how controlled interventions reveal causal model behavior vs. correlation-based predictions
  - Quick check question: If a model predicts higher mortality for patients with elevated heart rate, how would you test whether this reflects genuine physiological reasoning vs. correlation with unobserved variables?

- **Perplexity as probability proxy**:
  - Why needed here: Task-independent evaluation relies on interpreting perplexity shifts as signals of clinical plausibility
  - Quick check question: Given perplexity = exp(-1/N Σ log P(token)), what does a decrease in perplexity for a counterfactual note indicate about the model's learned distribution?

- **Clinical severity scales and vital sign interpretation**:
  - Why needed here: The framework maps continuous vital signs to clinically meaningful ordinal severity levels (-2 to +2) to evaluate monotonic reasoning
  - Quick check question: Why might treating "very low" and "very high" severity as symmetric (+2 shift) be problematic for some clinical outcomes?

## Architecture Onboarding

- **Component map**: MIMIC-IV discharge summaries → admission notes preprocessing → demographics extraction (structured data) → vitals extraction (LLaMA-3.3-70B-Instruct) → counterfactual generation (single-variable perturbations) → automatic diff validation → manual review → zero-shot inference → perplexity and prediction analysis

- **Critical path**: Counterfactual generation quality determines validity of all downstream conclusions. The ~5% error rate in LLM-extracted vitals required manual correction—automated validation via diff tools catches most issues but human review remains essential.

- **Design tradeoffs**: Templates vs. raw notes (templates increase sensitivity but may not reflect deployment conditions), zero-shot vs. fine-tuned (zero-shot reveals intrinsic reasoning; fine-tuned shows task-optimized behavior), severity symmetry (treating extreme high/low equivalently simplifies analysis but may miss clinically important asymmetries)

- **Failure signatures**: Non-monotonic responses (predictions that don't scale with severity), high flip rates on demographic changes (>20% on templates raises fairness concerns), inconsistent directionality (%CD below ~70% suggests misalignment), GPT-OSS-120B pattern (very high ∆PPL 2.5±5.1 suggests instability rather than calibrated sensitivity)

- **First 3 experiments**: 1) Validate counterfactual generation pipeline on 50 manually reviewed samples, 2) Run perplexity analysis on template-based notes for 3 models to establish baseline sensitivity patterns, 3) Test monotonicity on heart rate perturbations first as this vital sign has clearest clinical severity gradations and lowest missingness (5.7%)

## Open Questions the Paper Calls Out

- **Generalizability to other languages and healthcare systems**: The authors explicitly state that important future work lies in validating the framework's generalizability across languages and healthcare systems beyond MIMIC-IV's US dataset.

- **Temporal reasoning evaluation**: The paper notes that evaluating the temporal reasoning of LLMs is left to future work, as DeVisE uses static admission notes while clinical reasoning inherently involves temporal progression.

- **Integration into fine-tuning or alignment**: The framework reveals a trade-off between stability and sensitivity that suggests current training objectives may not reward calibrated counterfactual reasoning.

## Limitations

- ~5% error rate in LLM-extracted vital signs requiring manual correction, limiting confidence in single-variable effect isolation
- Limited evidence linking perplexity shifts to genuine medical understanding versus linguistic novelty
- Template-based amplification may not reflect real-world model deployment contexts
- Evaluation limited to single snapshot of admission notes without temporal dynamics

## Confidence

- **High confidence**: The framework successfully reveals systematic differences between zero-shot and fine-tuned models, with zero-shot models showing more coherent severity-monotonic reasoning patterns
- **Medium confidence**: The finding that demographic variables subtly influence predictions is methodologically sound but requires further investigation to determine clinical significance
- **Low confidence**: The interpretation of perplexity shifts as evidence of internal model sensitivity to clinical plausibility lacks strong supporting evidence

## Next Checks

1. **Ecological validity test**: Run the full evaluation pipeline on a held-out set of raw clinical notes without template conversion to verify whether sensitivity patterns persist in deployment-like conditions

2. **Perplexity validation**: Conduct controlled experiments where models are tested on counterfactuals with known clinical plausibility (expert-annotated) to establish whether perplexity correlates with clinical validity rather than just linguistic novelty

3. **Demographic bias investigation**: Perform subgroup analysis on the 23% flip rate near decision boundaries to determine whether observed demographic sensitivity reflects legitimate clinical differences or systematic bias requiring mitigation