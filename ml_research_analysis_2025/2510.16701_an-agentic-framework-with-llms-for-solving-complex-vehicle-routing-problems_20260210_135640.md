---
ver: rpa2
title: An Agentic Framework with LLMs for Solving Complex Vehicle Routing Problems
arxiv_id: '2510.16701'
source_url: https://arxiv.org/abs/2510.16701
tags:
- problem
- code
- capacity
- depot
- each
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes an Agentic Framework with Large Language Models
  (AFL) for solving complex vehicle routing problems. AFL achieves full automation
  from problem instance to solution by extracting knowledge from raw inputs and generating
  self-contained code without external solvers.
---

# An Agentic Framework with LLMs for Solving Complex Vehicle Routing Problems

## Quick Facts
- arXiv ID: 2510.16701
- Source URL: https://arxiv.org/abs/2510.16701
- Reference count: 40
- This paper proposes an Agentic Framework with Large Language Models (AFL) for solving complex vehicle routing problems. AFL achieves full automation from problem instance to solution by extracting knowledge from raw inputs and generating self-contained code without external solvers.

## Executive Summary
This paper introduces AFL, an agentic framework that achieves full automation in solving complex vehicle routing problems (VRPs) by decomposing the pipeline into three manageable subtasks handled by four specialized agents. AFL demonstrates competitive performance against meticulously designed algorithms while substantially outperforming existing LLM-based baselines in code reliability and solution feasibility. The framework shows promise for creating general-purpose VRP solvers that can handle diverse problem variants without requiring manual algorithm design.

## Method Summary
AFL employs four specialized agents (Generation, Judgment, Revision, Error Analysis) working within a three-subtask pipeline to solve VRP instances end-to-end. The framework extracts problem constraints from VRPLIB files, generates self-contained Python solver code using a destroy-insert heuristic with simulated annealing, and iteratively refines both problem descriptions and code through judgment-revision loops. The generated solvers include functions for reading instances, calculating distances, initializing routes, destroying and inserting customers, validating constraints, computing costs, and executing the main optimization loop. AFL uses GPT-4.1 as the default LLM and operates on CPU cores with iteration counts ranging from 500 to 10,000.

## Key Results
- AFL achieves ~100% runtime error-free rates and solution feasibility across 60 diverse VRP variants
- Solution quality gap from SOTA HGS-PyVRP is only ~2.12% on CVRP n=50 instances
- AFL substantially outperforms existing LLM-based baselines (SGE: 94.1% error rate, DRoC: 82.4% error rate)

## Why This Works (Mechanism)

### Mechanism 1: Subtask Decomposition with Specialized Agent Roles
Decomposing the VRP-solving pipeline into three subtasks and assigning specialized agents to each reduces cognitive load and improves reliability compared to end-to-end generation. Four agent types collaborate within their designated subtasks to enforce cross-functional consistency.

### Mechanism 2: Iterative Judgment-Revision Loops for Error Correction
Multi-turn feedback loops between Judgment and Revision agents substantially improve both problem description accuracy and code correctness compared to single-pass generation. After each generation step, Judgment agents evaluate output against instance constraints and requirements, triggering revisions until correctness is confirmed.

### Mechanism 3: Sequential Function Generation with Constraint Propagation
Generating solver functions sequentially while propagating constraint requirements through each step maintains consistency across the codebase better than generating all functions simultaneously. Each function is generated, judged, and revised before the next begins, with constraint definitions explicitly referenced throughout.

## Foundational Learning

- **Concept: Vehicle Routing Problem (VRP) Variants and Constraints**
  - **Why needed here:** AFL must automatically identify and handle diverse constraint combinations (capacity, time windows, distance limits, backhaul, multi-depot, electric vehicles)
  - **Quick check question:** Given a VRPLIB file with CAPACITY, DEMAND_SECTION, TIME_WINDOW_SECTION, and DISTANCE_LIMIT fields, what is the standard problem type name and which constraints must the generated solver enforce?

- **Concept: LLM Code Generation Reliability Issues**
  - **Why needed here:** The paper's central challenge is that LLMs frequently produce syntactically correct but logically flawed code
  - **Quick check question:** What are three common failure modes when an LLM generates VRP solver code without iterative validation?

- **Concept: Heuristic Search for VRPs (Destroy-Insert Paradigm)**
  - **Why needed here:** AFL adopts a unified destroy-insert heuristic as its solution strategy
  - **Quick check question:** In a destroy-insert heuristic for CVRP with capacity constraints, what must the insert function verify before accepting a customer into a route?

## Architecture Onboarding

- **Component map:** Problem Description Subtask (GA extracts constraints → JA verifies → RA corrects → Store in buffer) → Code Generation Subtask (GA generates functions sequentially → JA evaluates each → RA revises) → Solution Derivation Subtask (Execute code → If error: EAA diagnoses → RA revises → JA validates → Re-execute)

- **Critical path:** Problem description accuracy is foundational—errors here propagate to all downstream code generation; the validate() function is invoked at every insertion step during improvement; Error Analysis Agent triggers only on runtime failures

- **Design tradeoffs:** Completeness vs. Latency (more iterations improve quality but increase runtime); Self-containment vs. Performance (AFL generates all code from scratch, accepting slightly worse solutions for full automation); Generality vs. Specialization (unified heuristic handles 60+ variants but may underperform specialized algorithms)

- **Failure signatures:** Runtime Error Rate >0% indicates JA/RA loop not catching logical bugs; Infeasible Solutions indicate validate() function incorrectly implemented; High Gap from Baseline (>10%) may indicate weak destroy/insert operators; Slow Convergence indicates ambiguous prompts

- **First 3 experiments:**
  1. Validate problem description extraction: Run AFL on 5 CVRP instances and manually verify extracted constraints match VRPLIB file content
  2. Test JA→RA loop isolation: Generate code for read_vrp function, intentionally introduce bug, verify JA detects and RA corrects within 1-2 iterations
  3. Measure reliability vs. baseline: On 10 instances each of TSP, CVRP, and VRPTW, compare AFL's Runtime Error Rate and Success Rate against single-pass GPT-4 code generation

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can evolutionary search strategies be effectively integrated into AFL to improve both code quality and search efficiency?
- **Basis in paper:** The conclusion states: "As future work, we plan to incorporate strategies such as evolutionary search to guide code generation and further enhance both code quality and search efficiency."
- **Why unresolved:** The current framework relies on destroy-insert heuristics with simulated annealing; evolutionary search mechanisms have not yet been explored within the agentic architecture.
- **What evidence would resolve it:** Comparative experiments showing AFL with evolutionary search achieving lower objective values and higher solution quality on standard VRP benchmarks.

### Open Question 2
- **Question:** How does AFL's performance scale to problem instances exceeding 1000 customers or nodes?
- **Basis in paper:** The paper evaluates instances up to 1000 customers but does not systematically test larger-scale problems or analyze computational scaling behavior.
- **Why unresolved:** Scalability limits of the LLM-generated destroy-insert heuristics remain unstudied, particularly whether algorithmic complexity varies unpredictably across larger instances.
- **What evidence would resolve it:** Experiments on instances with 2000+ nodes showing consistent runtime growth patterns and solution quality relative to traditional solvers.

### Open Question 3
- **Question:** To what extent does the choice of underlying LLM affect AFL's code reliability and solution feasibility?
- **Basis in paper:** The paper uses GPT-4.1 as the default LLM without ablation studies on model selection.
- **Why unresolved:** Different LLMs may exhibit varying code generation accuracy, constraint understanding, and error recovery behaviors within the agentic framework.
- **What evidence would resolve it:** Ablation experiments running AFL with multiple LLM backbones, reporting Runtime Error Rate and Success Rate across the same VRP variants.

## Limitations
- AFL's performance heavily relies on GPT-4.1's capabilities and may degrade with different LLM versions
- Framework requires 5-83 minutes per instance depending on iteration count, limiting practical deployment for real-time applications
- Claims about full automation and generality across all VRP variants remain somewhat untested for extremely complex or novel constraints

## Confidence
- **High Confidence:** AFL's effectiveness in achieving competitive solution quality (average 2.12% gap from SOTA) and substantially outperforming baseline LLM methods in code reliability and feasibility
- **Medium Confidence:** Claims about full automation and generality across all VRP variants
- **Medium Confidence:** The assertion that AFL "substantially outperforms existing LLM-based baselines"

## Next Checks
1. Test AFL on additional VRP variants with combined constraints not explicitly covered in the 60 variants to verify true generality
2. Reproduce key results using different LLM versions (e.g., GPT-3.5, Claude) to assess dependency on specific model capabilities
3. Evaluate AFL's performance on larger instances (n > 1000 nodes) and measure how iteration count affects both solution quality and practical feasibility for time-sensitive applications