---
ver: rpa2
title: Permutation-Invariant Spectral Learning via Dyson Diffusion
arxiv_id: '2510.08535'
source_url: https://arxiv.org/abs/2510.08535
tags:
- graph
- graphs
- diffusion
- learning
- section
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of graph representation in diffusion
  models, where each graph with n nodes has up to n! adjacency matrix representations.
---

# Permutation-Invariant Spectral Learning via Dyson Diffusion

## Quick Facts
- arXiv ID: 2510.08535
- Source URL: https://arxiv.org/abs/2510.08535
- Reference count: 40
- Primary result: Outperforms existing graph diffusion models on standard benchmarks including WL-equivalent graphs, community-small graphs, and brain ego-graphs

## Executive Summary
This paper addresses the challenge of graph representation in diffusion models where each graph with n nodes has up to n! adjacency matrix representations. Existing graph diffusion models using permutation-equivariant learning architectures struggle to distinguish certain graph families, leading to a "blind spot" in distinguishing non-isomorphic graphs. The authors propose the Dyson Diffusion Model (DyDM), which leverages Dyson's Brownian Motion from random matrix theory to extract the spectral dynamics of an Ornstein-Uhlenbeck process on the adjacency matrix while retaining all non-spectral information.

## Method Summary
The method generates graph spectra by simulating Dyson's Brownian Motion (DBM), a singular diffusion process on ordered eigenvalues that naturally respects permutation invariance. Instead of learning the adjacency matrix directly, DyDM learns the score function for the spectrum's evolution under DBM, allowing the use of generic architectures like MLPs. The forward pass uses an adaptive step-size controller to ensure numerical stability when eigenvalues approach each other, while the backward pass employs a "shooting mechanism" to handle singularities when the learned score is insufficient.

## Key Results
- Eliminates the "blind spot" where GNNs fail to distinguish non-isomorphic graphs in the same WL-equivalence class
- Achieves lower mean distance and marginal Wasserstein distance metrics compared to baselines on standard benchmark datasets
- Does not require ad hoc data augmentation while avoiding hallucination problems common in GNN-based approaches

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** If an Ornstein-Uhlenbeck (OU) process is applied to a symmetric matrix (e.g., an adjacency matrix), the dynamics of the eigenvalues (spectrum) and eigenvectors decouple analytically.
- **Mechanism:** The diffusion of the matrix entries induces a specific stochastic differential equation (SDE) on the ordered eigenvalues known as Dyson's Brownian Motion (DBM). This SDE includes a repulsion term ($\propto 1/|\lambda_k - \lambda_\ell|$) that prevents eigenvalue crossing, ensuring the spectrum remains ordered without external constraints.
- **Core assumption:** The input matrix is symmetric and has a simple spectrum (no repeated eigenvalues), or can be perturbed to satisfy this.
- **Evidence anchors:** [abstract] "employs Dyson's Brownian Motion to capture the spectral dynamics of an Ornstein-Uhlenbeck process... while retaining all non-spectral information." [section 3.1] Theorem 3.1 derives the Eigenvalue SDE (Dyson-BM) directly from the matrix OU process.

### Mechanism 2
- **Claim:** Moving the learning target from the adjacency matrix to the spectrum shifts the inductive bias from the neural network architecture to the data dynamics, bypassing the "WL-equivalence" blind spot of Graph Neural Networks (GNNs).
- **Mechanism:** Standard GNNs use permutation-equivariant architectures (message passing) which cannot distinguish non-isomorphic graphs that fall into the same Weisfeiler-Leman (WL) equivalence class (e.g., $k$-regular graphs). By learning the spectrum (which is permutation-invariant by definition) using Dyson-BM, the model can distinguish these graphs because WL-equivalent graphs often have distinct spectra. This allows the use of generic architectures (like MLPs) instead of specialized GNNs.
- **Core assumption:** The graph distribution of interest is distinguishable by spectral properties (i.e., non-isomorphic graphs in the dataset do not share the exact same spectrum).
- **Evidence anchors:** [abstract] "allows learning the spectrum without requiring GNNs... expanding the scope of suitable learning architectures." [section 2] Lemma 2.1 proves $k$-regular graphs are WL-equivalent, highlighting the blind spot.

### Mechanism 3
- **Claim:** The singularities in the Dyson-BM drift term (where eigenvalues approach each other) necessitate an adaptive step-size controller and a "shooting mechanism" to ensure numerical stability and valid sampling.
- **Mechanism:** The repulsion force in Dyson-BM scales inversely with eigenvalue distance. As eigenvalues approach, standard fixed-step solvers overshoot (causing crossing) or crash. The authors use a forward step-size controller that conditions on non-crossing. During inference (reverse time), if the learned score is weak and allows crossing, a "shooting mechanism" temporarily replaces the learned score with the invariant-state drift to force repulsion.
- **Core assumption:** The eigenvalue paths must remain strictly ordered (inside the Weyl Chamber) at all times $t$.
- **Evidence anchors:** [section 3.2] "Dyson-BM is singular at the boundary of the Weyl Chamber... To overcome this, we implement an adaptive step-size algorithm." [section 6/Appendix G] Describes the "equilibrium shooting mechanism" required when the learned score is insufficient to prevent crossing during reverse diffusion.

## Foundational Learning

- **Concept: Dyson's Brownian Motion & The Weyl Chamber**
  - **Why needed here:** This is the core mathematical object being simulated. You must understand that eigenvalues behave like charged particles (Coulomb gas) that repel each other, confining them to an ordered state ($\lambda_1 > \lambda_2 > \dots > \lambda_n$).
  - **Quick check question:** Why does the drift term $\sum_{\ell \neq k} \frac{1}{\lambda_k - \lambda_\ell}$ prevent eigenvalues from crossing?

- **Concept: Score-Based Generative Models (SGM) & Time Reversal**
  - **Why needed here:** The method generates samples by reversing a diffusion process. This requires learning the "score" (gradient of the log-density) to guide eigenvalues from noise back to the data distribution.
  - **Quick check question:** In Equation (5), how does the learned score function $s(\lambda, t)$ modify the reverse drift compared to the forward drift?

- **Concept: Weisfeiler-Leman (WL) Isomorphism Test**
  - **Why needed here:** This defines the "blind spot" the paper claims to solve. Understanding WL-equivalence (specifically how $k$-regular graphs collapse to the same representation in GNNs) explains why a spectral approach is proposed.
  - **Quick check question:** Why would a GNN fail to distinguish two distinct 3-regular graphs, while a spectral method might succeed?

## Architecture Onboarding

- **Component map:** Graph -> Eigendecomposition -> Sorted Eigenvalues $\lambda$ -> Simulate Dyson-BM SDE (Adaptive Step-Size Controller) -> Noise -> Learn score network MLP -> Sample from GOE invariant distribution -> Run backward SDE (Shooting Mechanism) -> Spectra
- **Critical path:** The Adaptive Step-Size Controller is the most critical engineering detail. Without it, numerical solvers will crash or produce invalid unordered spectra when eigenvalues get close.
- **Design tradeoffs:**
  - Vs. GNNs: DyDM uses simple MLPs (efficient, permutation-invariant by design) but currently only models the spectrum ($n$ dims) rather than the full adjacency matrix ($n^2$ dims).
  - Vs. Standard Diffusion: DyDM avoids simulation-free training (must simulate forward SDE paths) to handle the non-Gaussian conditional density of Dyson-BM.
- **Failure signatures:**
  - Numerical Crash: "Leaving the Weyl Chamber" errors indicate the step-size controller or shooting mechanism failed.
  - Hallucination: Generating spectra that don't exist in data (though paper claims this is reduced compared to GNNs).
  - Mode Collapse: If the dataset has many co-spectral graphs, the model cannot distinguish them.
- **First 3 experiments:**
  1. **Sanity Check (WL-Bimodal):** Replicate the "Graph A vs. Graph B" experiment on two WL-equivalent graphs. Verify the model distinguishes them while a GNN baseline fails (Figure 2).
  2. **Ablation on Step Size:** Attempt training with a fixed Euler-Maruyama step size. Observe the frequency of Weyl Chamber violations compared to the adaptive controller.
  3. **Spectrum Distribution:** Generate samples and plot the marginal distributions of $\lambda_k$ vs. ground truth (Figure 1 style) to verify the score network has learned the correct invariant density.

## Open Questions the Paper Calls Out

- **Open Question 1:** Can the Eigenvector SDE be effectively parameterized and integrated with the spectral dynamics to generate complete graph structures without succumbing to WL-blindness?
  - **Basis in paper:** [explicit] Section 6 states, "future work could implement a model of Eigenvector-SDE and generate entire graphs."
  - **Why unresolved:** The current study derives the Eigenvector SDE theoretically but experimentally validates DyDM only on the generation of graph spectra, leaving the generation of the full adjacency matrix as an unimplemented extension.
  - **What evidence would resolve it:** An end-to-end generative model utilizing both SDEs that successfully generates non-isomorphic, WL-equivalent graphs with correct eigenvector distributions.

- **Open Question 2:** How does DyDM perform when applied to the Graph Laplacian or complex-weighted graphs represented by Hermitian matrices?
  - **Basis in paper:** [explicit] Section 6 proposes "generalizations to complex-weighted graphs" via Hermitian ensembles and learning "the combinatorial graph Laplacian."
  - **Why unresolved:** The theoretical framework and experiments are restricted to real symmetric adjacency matrices; the specific dynamics and repulsion forces for Hermitian or Laplacian matrices are derived but not empirically benchmarked.
  - **What evidence would resolve it:** Successful generation of spectra for datasets defined by Laplacians or complex weights, demonstrating that the Dyson-BM repulsion terms adapt correctly to these domains.

- **Open Question 3:** Does the computational overhead of adaptive step-size control and eigendecomposition limit the scalability of DyDM compared to simulation-free diffusion models?
  - **Basis in paper:** [inferred] The paper notes that "accurate eigendecomposition is computationally costly" and implements an "adaptive step-size algorithm" to handle singularities, implying potential scaling challenges.
  - **Why unresolved:** While the authors claim the method is efficient, the complexity of enforcing non-crossing constraints and computing eigenvalues at every step may grow non-linearly compared to standard matrix diffusions on large graphs.
  - **What evidence would resolve it:** A complexity analysis and runtime comparison against baselines like GDSS or DiGress on graphs with $n > 100$ vertices.

## Limitations
- Currently only models the spectrum rather than the full adjacency matrix, limiting applicability to tasks requiring complete graph structure
- Performance depends on the assumption that non-isomorphic graphs in the dataset have distinct spectra
- Implementation requires careful numerical handling (adaptive step-size, shooting mechanism) that serves a similar purpose to ad hoc data augmentation

## Confidence
- **High confidence:** The mathematical framework of Dyson-BM and its relationship to OU processes on matrices is well-established and correctly applied. The elimination of WL-equivalence blind spots through spectral learning is theoretically sound.
- **Medium confidence:** The empirical results show strong performance on benchmark datasets, but the exact hyperparameter configurations and implementation details for step-size adaptation and shooting mechanisms are not fully specified, which could affect reproducibility.
- **Low confidence:** The claim about not requiring ad hoc data augmentation is somewhat relative - while the method doesn't need explicit augmentation, it does require careful numerical handling (adaptive step-size, shooting mechanism) that serves a similar purpose.

## Next Checks
1. **WL-equivalence test:** Create a dataset with multiple non-isomorphic but WL-equivalent graph families (e.g., different 3-regular graphs). Train both DyDM and a GNN baseline, then verify that DyDM successfully distinguishes these graphs while the GNN cannot.
2. **Step-size sensitivity analysis:** Systematically vary the parameters of the adaptive step-size controller (minimum step size, overshoot tolerance) and measure the frequency of Weyl Chamber violations and overall training stability across different graph sizes.
3. **Co-spectral graph robustness:** Construct pairs of non-isomorphic graphs with nearly identical spectra (perturbed to be co-spectral). Test whether DyDM can still distinguish them and measure the degradation in performance as spectral similarity increases.