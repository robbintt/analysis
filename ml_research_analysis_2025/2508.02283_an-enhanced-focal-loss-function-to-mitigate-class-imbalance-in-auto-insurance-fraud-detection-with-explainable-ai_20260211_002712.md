---
ver: rpa2
title: An Enhanced Focal Loss Function to Mitigate Class Imbalance in Auto Insurance
  Fraud Detection with Explainable AI
arxiv_id: '2508.02283'
source_url: https://arxiv.org/abs/2508.02283
tags:
- loss
- convex
- fraud
- class
- focal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of detecting fraudulent auto-insurance
  claims under severe class imbalance, where legitimate cases vastly outnumber fraudulent
  ones. The authors propose a three-stage training framework that integrates a convex
  surrogate of focal loss for stable initialization, a controlled non-convex intermediate
  loss to improve feature discrimination, and the standard focal loss to refine minority-class
  sensitivity.
---

# An Enhanced Focal Loss Function to Mitigate Class Imbalance in Auto Insurance Fraud Detection with Explainable AI

## Quick Facts
- arXiv ID: 2508.02283
- Source URL: https://arxiv.org/abs/2508.02283
- Authors: Francis Boabang; Samuel Asante Gyamerah
- Reference count: 27
- Primary result: Improved minority-class F1-scores and AUC in auto-insurance fraud detection through a three-stage convex-to-non-convex focal loss framework.

## Executive Summary
This paper tackles the challenge of detecting fraudulent auto-insurance claims in datasets with severe class imbalance, where legitimate claims vastly outnumber fraudulent ones. The authors propose a three-stage training framework that combines a convex surrogate of focal loss for stable initialization, a controlled non-convex intermediate loss for improved feature discrimination, and the standard focal loss to refine minority-class sensitivity. The approach is validated on a proprietary dataset, showing improved performance and interpretability through SHAP analysis.

## Method Summary
The method employs a three-stage LSTM training process to address class imbalance in auto-insurance fraud detection. Stage 1 uses a convex surrogate focal loss (γ=0, softplus activation) for stable initialization over 10 epochs. Stage 2 introduces intermediate non-convex loss (α_t/2) for 40 epochs to improve feature discrimination. Stage 3 applies standard focal loss (γ=4) for 50 epochs to refine minority-class sensitivity. The approach integrates partial SMOTE resampling (80% level) and provides interpretable feature attributions through SHAP analysis.

## Key Results
- Improved minority-class F1-scores and AUC compared to conventional focal-loss training and resampling baselines
- Demonstrated effectiveness of staged convex-to-non-convex training in mitigating class imbalance
- Provided interpretable feature-attribution patterns through SHAP analysis for transparency

## Why This Works (Mechanism)

### Mechanism 1
A convex surrogate focal loss provides stable initialization before transitioning to non-convex optimization. Setting γ=0 and using softplus activation yields a strictly convex loss surface in logit space, acting as a "warm start" that avoids early convergence to poor local minima caused by the rugged non-convex landscape of standard focal loss.

### Mechanism 2
The staged convex-to-non-convex transition functions as an implicit continuation method, smoothing early optimization. By first minimizing a well-conditioned convex surrogate, then gradually introducing non-convexity, the optimizer traverses a progressively more complex loss landscape, reducing sensitivity to initialization and avoiding premature discounting of easy examples.

### Mechanism 3
Standard focal loss (γ>0) in later stages concentrates gradient updates on hard minority-class examples. The modulating factor (1-p_t)^γ down-weights well-classified (majority) samples and amplifies gradients for misclassified or low-confidence predictions, which is especially valuable when fraudulent cases are rare and hard to separate.

## Foundational Learning

- **Concept:** Focal Loss and Modulating Factor
  - **Why needed here:** Central to the paper's approach for reweighting easy vs. hard examples under imbalance.
  - **Quick check question:** For a sample with p_t=0.9 and γ=2, what is the focal weight (1-p_t)^γ, and how does it compare to p_t=0.5?

- **Concept:** Convex vs. Non-Convex Optimization Landscapes
  - **Why needed here:** The three-stage framework relies on understanding why convex surrogates stabilize early training and why non-convex loss surfaces introduce local minima risks.
  - **Quick check question:** Given f''(p) = γ/p + 1/p², is f(p) convex for p∈(0,1) and γ>0? What happens if w_t varies with p_t?

- **Concept:** Class Imbalance Handling Strategies
  - **Why needed here:** The paper combines focal loss with SMOTE-based resampling; understanding how data-level and loss-level interventions complement each other is essential.
  - **Quick check question:** If SMOTE synthesizes minority-class samples, does focal loss remain necessary? What distinct role does each play?

## Architecture Onboarding

- **Component map:** Input sequences → Dense (32 units) → LSTM (128 units) → Probability output
- **Critical path:**
  1. Preprocess and apply partial SMOTE (80% resampling level)
  2. Train Stage 1 for E₁=10 epochs with convex surrogate
  3. Train Stage 2 for 40 epochs with intermediate non-convex loss
  4. Train Stage 3 for 50 epochs with standard focal loss
  5. Compute SHAP explanations for transparency
- **Design tradeoffs:**
  - Convex warm start vs. added complexity: More stable but requires careful stage timing
  - γ=4 focusing parameter: Strong emphasis on hard examples; may amplify noise if minority samples are mislabeled
  - Partial SMOTE: Reduces synthetic distortion but may not fully balance classes
- **Failure signatures:**
  - Stage 1 only: High precision, extremely low recall (over-conservative, underfitting)
  - Non-convex only (α=0.25): Very low recall and F1 despite low loss (misaligned optimization)
  - Early transition to non-convex: Unstable gradients, erratic loss curves
  - SHAP attributions heavily concentrated on one feature: Possible overfitting or data leakage
- **First 3 experiments:**
  1. **Baseline comparison:** Replicate Table 2 (convex-only, three-stage, non-convex α=0.25, non-convex α=0.5) on the same dataset to verify F1 and AUC improvements
  2. **Ablation on stage timing:** Vary E₁ (5, 10, 20) and E₂ (40, 50, 60) to assess sensitivity of transition points; monitor validation loss and minority-class recall
  3. **γ sensitivity analysis:** Test γ ∈ {2, 4, 6} in Stage 3 to evaluate the tradeoff between hard-example focus and potential overfitting to noisy minority samples

## Open Questions the Paper Calls Out

### Open Question 1
Can the transition points between the convex initialization and non-convex refinement stages be determined adaptively rather than fixed empirically?
- Basis: The authors explicitly recommend that future studies focus on "adaptive mechanisms for automatically determining the transition points between convex and non-convex phases."
- Why unresolved: The current framework relies on fixed epoch cutoffs (e.g., E₁=10, E₂=50) defined a priori, which may not be optimal for datasets with different convergence rates.
- What evidence would resolve it: A comparative study showing that a dynamic scheduling algorithm (based on validation loss curvature or gradient norms) outperforms the fixed schedule across multiple datasets.

### Open Question 2
Does the three-stage focal loss framework retain its performance advantages in non-insurance domains with severe class imbalance, such as cybersecurity or healthcare?
- Basis: The conclusion suggests extending the proposed framework to "other high-impact domains such as cybersecurity, mission-critical operational analytics, and broader financial fraud detection."
- Why unresolved: The empirical validation is restricted to a single proprietary auto-insurance dataset, leaving the generalizability of the convex-to-non-convex transition unproven in other contexts.
- What evidence would resolve it: Benchmarks on standard public datasets (e.g., credit card fraud or network intrusion detection) demonstrating that the method maintains higher minority-class F1-scores than standard focal loss.

### Open Question 3
To what extent are the reported performance gains dependent on the specific SMOTE-based resampling strategy versus the three-stage loss function itself?
- Basis: The methodology integrates the loss function with a "hybrid resampling strategy," and the SHAP analysis notes that oversampling "reshapes the local decision boundary," suggesting the results rely on a coupled solution.
- Why unresolved: The paper does not isolate the loss function's contribution by testing it on the raw, severely imbalanced data without the SMOTE preprocessing step.
- What evidence would resolve it: An ablation study comparing the proposed loss function against standard losses on the raw, un-resampled data to quantify the marginal improvement of the loss formulation alone.

## Limitations
- The SMOTE resampling configuration (partial at 80% level) is ambiguous and may affect reproducibility
- Results are based on a proprietary dataset, limiting external validation despite proxy availability
- Convexity proofs assume fixed focal weights, which may not hold during stage transitions in practice

## Confidence

**High confidence** in the staged convex-to-non-convex training mechanism's theoretical motivation and general applicability.
**Medium confidence** in the specific transition thresholds (E₁=10, E₂=50) and γ=4 focusing parameter for this dataset.
**Low confidence** in generalizability to datasets with different imbalance ratios or noise characteristics without recalibration.

## Next Checks
1. Replicate the ablation experiments varying stage transition points (E₁, E₂) and γ values to quantify sensitivity to hyperparameter choices
2. Test the three-stage framework on a public imbalanced dataset (e.g., Kaggle credit card fraud) to assess external validity
3. Perform SHAP-based feature attribution analysis on the convex-only and full three-stage models to verify that staged training produces more stable and interpretable explanations