---
ver: rpa2
title: 'Internet 3.0: Architecture for a Web-of-Agents with it''s Algorithm for Ranking
  Agents'
arxiv_id: '2509.04979'
source_url: https://arxiv.org/abs/2509.04979
tags:
- agents
- competence
- agentrank-uc
- agent
- usage
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes DOVIS, a five-layer protocol for minimal, privacy-preserving
  telemetry collection in agent ecosystems, and AgentRank-UC, a ranking algorithm
  that combines usage and competence into a unified score. DOVIS specifies how agents
  publish aggregate interaction statistics (OAT-Lite), how reports are orchestrated,
  verified, incentivized, and semantically standardized.
---

# Internet 3.0: Architecture for a Web-of-Agents with it's Algorithm for Ranking Agents

## Quick Facts
- arXiv ID: 2509.04979
- Source URL: https://arxiv.org/abs/2509.04979
- Reference count: 35
- Primary result: AgentRank-UC combines usage and competence into a unified ranking, outperforming single-signal baselines and resisting Sybil manipulation in simulations.

## Executive Summary
This paper proposes DOVIS, a five-layer protocol for minimal, privacy-preserving telemetry collection in agent ecosystems, and AgentRank-UC, a ranking algorithm that combines usage and competence into a unified score. DOVIS specifies how agents publish aggregate interaction statistics (OAT-Lite), how reports are orchestrated, verified, incentivized, and semantically standardized. AgentRank-UC extends PageRank to two evolving graphs—usage and competence—using recency weighting, Beta-Bernoulli smoothing, and a geometric fusion of ranks. Theoretical analysis guarantees existence, uniqueness, monotonicity, cold-start fairness, and stability. Simulations show that AgentRank-UC adapts quickly to performance shocks, resists Sybil-style manipulation, and surfaces higher-quality agents than usage-only or competence-only baselines.

## Method Summary
The method involves simulating synthetic agent interactions over 40 epochs, aggregating time-decayed statistics (call counts, success counts, quality/latency/cost/risk averages), constructing usage and competence kernels from these aggregates, solving coupled fixed-point equations via power iteration, and fusing the resulting ranks geometrically. The ranking engine uses exponential decay for recency, Beta-Bernoulli smoothing for sparse success counts, and softplus-transformed utility functions for competence weighting. Validation metrics include Quality@k, NDCG@k, Regret@k, and Sybil Mass.

## Key Results
- AgentRank-UC adapts quickly to performance shocks and outperforms usage-only and competence-only baselines in NDCG@10 and Quality@10.
- The algorithm resists Sybil manipulation: theoretical bounds limit Sybil rank mass, and simulations show reduced Sybil capture under AgentRank-UC versus usage-only.
- Privacy is preserved through minimal telemetry (aggregate statistics only) without exposing raw prompts or responses.

## Why This Works (Mechanism)

### Mechanism 1: Dual-Signal Ranking via Coupled Fixed-Point Iteration
Combining usage frequency with competence quality produces rankings that surface higher-quality agents than either signal alone. AgentRank-UC constructs two row-stochastic kernels—P (usage) and Q (competence)—from time-decayed telemetry, solves coupled fixed-point equations x = αP^T x + (1−α)v and y = βQ^T y + (1−β)w via power iteration, then fuses via r = normalize(x^p ⊙ y^{1−p}). Geometric fusion penalizes imbalance (popular-but-incompetent or excellent-but-undiscovered agents). Core assumption: Agents with demonstrated recent success, quality, and low latency/cost/risk will continue performing well; usage patterns correlate with trust but not necessarily competence. Evidence: AgentRank-UC "substantially outperforms usage-only and nearly matches competence-only" in tracking ground truth. Break condition: If telemetry is systematically biased or if usage and competence are completely uncorrelated, the fusion provides no benefit over single-signal baselines.

### Mechanism 2: Privacy-Preserving Minimal Telemetry (OAT-Lite)
Aggregate statistics—call counts, success counts, summed quality/latency/cost/risk—constitute sufficient statistics for AgentRank-UC without exposing raw prompts or responses. Callers submit per-epoch snapshots keyed by (caller, callee, task, epoch) containing only aggregates. Exponential decay ω(t) = e^{−λτ} ensures recency. Indexers reconstruct decayed counts N_{ij}^{(k)}, success totals S_{ij}^{(k)}, and means for quality, latency, cost, risk—exactly the inputs for P and Q kernels. Core assumption: Callers can accurately judge success, quality, and risk; they report honestly or are deterred by verification/incentives. Evidence: "DOVIS specifies how agents publish aggregate interaction statistics (OAT-Lite)". Break condition: If fine-grained outcome signals cannot be captured in aggregates, ranking degrades to coarse-grained discrimination.

### Mechanism 3: Sybil Resistance via Teleportation and Competence Weighting
Collusive usage-pumping cannot amplify rank beyond a hard ceiling because teleportation injects prior mass and competence weighting requires genuine performance. Theorem 4.17 proves x_S ≤ α + (1−α)v_S for any Sybil clique S (usage-only bound). When fused with competence (p ∈ (0,1]), if clique competence share y_S < 1, the fused mass r_S < 1 with explicit gap controlled by outside prior floor and (1 − y_S). Competence kernel Q penalizes latency, cost, risk. Core assumption: Sybils cannot fake genuine competence at scale; at least some non-colluding agents contribute higher-quality outcomes. Evidence: "Usage-only allocates the most rank mass to the Sybil clique... AgentRank-UC assigns substantially less mass to Sybils". Break condition: If Sybils can fabricate competence signals, the bound weakens. Verification layer mitigates but does not eliminate this risk.

## Foundational Learning

- Concept: **PageRank and Random Walks with Teleportation**
  - Why needed here: AgentRank-UC extends PageRank to two graphs (usage and competence). Understanding contraction mappings, stochastic matrices, and the role of teleportation (α, β parameters) is essential.
  - Quick check question: If α = 0.85 and a node has no inbound edges in P, what determines its usage rank x_j?

- Concept: **Beta-Bernoulli Smoothing**
  - Why needed here: Success rates p̂_{ij}^{(k)} are estimated via Beta(α₀, β₀) posteriors to handle sparse edges and guarantee monotonicity.
  - Quick check question: With α₀ = β₀ = 1 (uniform prior), what is p̂ after observing 0 successes in 0 trials? After 5 successes in 10 trials?

- Concept: **Geometric vs. Arithmetic Fusion**
  - Why needed here: Final rank r = normalize(x^p ⊙ y^{1−p}) uses geometric mean, which penalizes imbalance more strongly than arithmetic averaging.
  - Quick check question: If x_j = 0.1 and y_j = 0.9, compare r_j under p = 0.5 (geometric) vs. p = 0.5 (arithmetic mean). Which fusion penalizes the imbalance more?

## Architecture Onboarding

- Component map:
  - Callers -> Telemetry Layer (OAT-Lite records) -> Verification Layer (signatures, audits) -> Indexer -> Ranking Engine (P, Q kernels) -> Published ranks r^{(k)}
  - In parallel: Incentive Layer (rewards/penalties) and Semantic Layer (OAT-Lite schema, task taxonomy)

- Critical path:
  1. Epoch closes → callers submit OAT-Lite snapshots.
  2. Indexer deduplicates, validates signatures, applies decay.
  3. Construct U_{ij} = Σ_k N_{ij}^{(k)} (usage weights) and C_{ij} = Σ_k N_{ij}^{(k)} φ(u_{ij}^{(k)}) (competence weights via softplus-transformed utility).
  4. Row-normalize to P and Q with prior backoff for empty rows.
  5. Iterate x and y to convergence (∥·∥₁ tolerance, typically O(log 1/ε) iterations).
  6. Fuse and publish r^{(k)} for each task type.

- Design tradeoffs:
  - **Half-life H (λ)**: Shorter H → faster shock response but higher variance. Paper recommends 1–7 days.
  - **Balance p**: Lower p → competence-dominated; higher p → usage-dominated. Paper finds p ≈ 0.3–0.5 works well in realistic regimes.
  - **Teleport α, β**: Controls robustness vs. sensitivity. Standard choice α = 0.85 mirrors PageRank; paper explores sensitivity.
  - **Prior strength**: Uniform priors guarantee cold-start fairness; informative priors can bootstrap trusted newcomers.

- Failure signatures:
  - **Divergence/non-convergence**: Check that P and Q are row-stochastic with no NaN/Inf; ensure priors v, w > 0.
  - **Sybil mass spikes**: Monitor r_S for known cliques; if > α + (1−α)v_S, verify competence kernel Q is correctly penalizing latency/cost/risk.
  - **Cold-start invisibility**: If newcomers have r_j ≈ 0, check that priors are strictly positive and teleport (1−α), (1−β) are non-zero.

- First 3 experiments:
  1. **Convergence sanity check**: Generate synthetic telemetry from the paper's archetype model (n=100 agents, 3 tasks). Verify power iteration converges in ≤ 50 iterations with ε = 1e−6 tolerance. Plot x(t) and y(t) trajectories.
  2. **Balance parameter sweep**: Freeze telemetry, vary p ∈ {0.0, 0.125, ..., 1.0}. Plot Quality@10 and NDCG@10 vs. p. Confirm interpolation between competence-only (p=0) and usage-only (p=1) baselines.
  3. **Sybil injection test**: Add a 10-agent Sybil clique with intra-clique usage pumping but mediocre competence (θ ≈ 0.5). Compare r_S under AgentRank-UC vs. usage-only over 40 epochs. Verify r_S < 1 and declines after burn-in.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the balance parameter p be dynamically adapted based on task type or data sparsity to optimize AgentRank-UC performance in real-time?
- Basis in paper: [explicit] Section 6 notes that rather than fixing p, one "could adapt p dynamically as a function of task type or traffic sparsity, or even learn it end-to-end."
- Why unresolved: Current experiments evaluate static p values; an adaptive policy requires definition and validation in non-stationary environments.
- What evidence would resolve it: Simulations showing an adaptive p algorithm outperforming fixed baselines (NDCG@k) across varying traffic densities and shock scenarios.

### Open Question 2
- Question: How can differential privacy mechanisms be integrated into the OAT-Lite aggregation pipeline without compromising the convergence guarantees of the ranking algorithm?
- Basis in paper: [explicit] Section 6 calls for "Privacy-preserving ranking" combining DOVIS telemetry with "differential privacy" to prevent leakage of sensitive usage patterns.
- Why unresolved: The paper relies on minimal aggregation and signatures for privacy but does not model the noise-utility trade-off required for formal privacy guarantees.
- What evidence would resolve it: A theoretical analysis of privacy budgets (ε) alongside empirical measurements of ranking accuracy degradation under noise injection.

### Open Question 3
- Question: Is it possible to achieve consistent AgentRank-UC scores via distributed consensus among multiple marketplaces without a centralized indexer?
- Basis in paper: [explicit] The paper identifies "decentralized ranking" where "multiple marketplaces collectively maintain discovery scores through distributed consensus" as a "natural evolution."
- Why unresolved: The DOVIS protocol currently assumes a centralized indexer for assembly and normalization; the architectural and algorithmic adjustments for federation are undefined.
- What evidence would resolve it: A specification for distributed computation of the fixed-point iterations and proof of convergence in asynchronous network environments.

## Limitations
- **Telemetry assumptions:** The privacy-preserving design hinges on honest reporting; the paper acknowledges but does not deeply validate audit costs or effectiveness against sophisticated falsification.
- **Parameter sensitivity:** Optimal values for balance parameter p, decay rate λ, and teleportation weights α, β are not empirically determined beyond illustrative sweeps; real-world calibration may be non-trivial.
- **Sybil resistance:** Theoretical bounds assume that competence signals cannot be fabricated at scale. If a Sybil clique can manipulate success/quality reports (e.g., via collusion), the robustness guarantees weaken.
- **Static world assumption:** Simulations use fixed agent archetypes; real ecosystems may have more dynamic competence and usage patterns that could challenge convergence or ranking stability.

## Confidence
- **High:** Existence and uniqueness of fixed points under mild conditions (Theorem 4.11); monotonicity under improved performance (Theorem 4.15); basic convergence of power iteration.
- **Medium:** Effectiveness of dual-signal fusion in simulations; practical privacy guarantees under honest reporting; qualitative comparison to baselines.
- **Low:** Resilience against coordinated Sybil manipulation in the presence of compromised competence reporting; generalization to non-synthetic, highly dynamic agent ecosystems.

## Next Checks
1. **Parameter sensitivity study:** Run the full simulation suite across a grid of (p, λ, α, β) values; report Quality@k, NDCG@k, and Sybil Mass as functions of each parameter to identify robust operating regimes.
2. **Realistic telemetry audit:** Introduce a realistic audit mechanism (e.g., 2% random spot-checks with a penalty for falsification) and quantify its effectiveness in reducing Sybil mass and preserving ranking accuracy.
3. **Dynamic competence shocks:** Extend the archetype model to allow agent competence to drift over time (e.g., via a random walk or performance decay); verify that AgentRank-UC tracks ground truth better than baselines under these non-stationary conditions.