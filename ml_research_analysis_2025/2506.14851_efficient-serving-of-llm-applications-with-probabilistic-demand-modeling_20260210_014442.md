---
ver: rpa2
title: Efficient Serving of LLM Applications with Probabilistic Demand Modeling
arxiv_id: '2506.14851'
source_url: https://arxiv.org/abs/2506.14851
tags:
- uni00000013
- applications
- demand
- application
- uni00000011
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the inefficiency of serving Large Language
  Model (LLM) applications in cloud environments, where applications have uncertain
  resource demands and diverse backend types, leading to long queuing delays and backend
  warm-up delays. To solve this, the authors propose Hermes, which uses Probabilistic
  Demand Graph (PDGraph) to model the resource demands of LLM applications in a probabilistic
  and structured manner.
---

# Efficient Serving of LLM Applications with Probabilistic Demand Modeling

## Quick Facts
- arXiv ID: 2506.14851
- Source URL: https://arxiv.org/abs/2506.14851
- Authors: Yifei Liu; Zuo Gan; Zhenghao Gan; Weiye Wang; Chen Chen; Yizhou Shan; Xusheng Chen; Zhenhua Han; Yifei Zhu; Shixuan Sun; Minyi Guo
- Reference count: 40
- Primary result: Reduces average LLM application completion time by over 70% and P95 completion time by over 80%

## Executive Summary
This paper addresses the inefficiency of serving Large Language Model (LLM) applications in cloud environments, where applications have uncertain resource demands and diverse backend types, leading to long queuing delays and backend warm-up delays. To solve this, the authors propose Hermes, which uses Probabilistic Demand Graph (PDGraph) to model the resource demands of LLM applications in a probabilistic and structured manner. PDGraph captures the distribution of resource demands for each functional unit and their dependencies, enabling accurate demand estimation. Hermes leverages this information to optimize task scheduling using the Gittins policy, minimizing average application completion time, and to prewarm backends at optimal times to reduce latency. The system also extends to deadline-constrained scenarios using the LSTF algorithm. Experiments show Hermes reduces average completion time by over 70% and P95 completion time by over 80%, with effective backend prewarming and online demand refinement. The overhead of the Gittins policy is minimal, under 3 ms, and the approach is extensible to advanced inference architectures.

## Method Summary
The paper proposes Hermes, a system for efficient LLM application serving in cloud environments. The core innovation is the Probabilistic Demand Graph (PDGraph), which models the resource demands of LLM applications in a probabilistic and structured manner. PDGraph captures the distribution of resource demands for each functional unit and their dependencies, enabling accurate demand estimation. Hermes uses this information to optimize task scheduling using the Gittins policy, minimizing average application completion time, and to prewarm backends at optimal times to reduce latency. The system also extends to deadline-constrained scenarios using the LSTF algorithm. Experiments demonstrate significant improvements in completion time and latency, with minimal overhead.

## Key Results
- Reduces average LLM application completion time by over 70%
- Reduces P95 completion time by over 80%
- Minimal overhead of the Gittins policy (under 3 ms)

## Why This Works (Mechanism)
Hermes works by modeling the resource demands of LLM applications using a Probabilistic Demand Graph (PDGraph). This graph captures the distribution of resource demands for each functional unit and their dependencies, enabling accurate demand estimation. The system then uses this information to optimize task scheduling using the Gittins policy, which minimizes average application completion time. Additionally, Hermes prewarms backends at optimal times to reduce latency. The approach is extensible to deadline-constrained scenarios using the LSTF algorithm.

## Foundational Learning
- **Probabilistic Demand Graph (PDGraph)**: A structured representation of resource demands for LLM applications. Why needed: To capture the uncertainty and variability in resource demands. Quick check: Verify that the graph accurately models the demand distributions for each functional unit.
- **Gittins Policy**: An optimal scheduling policy for minimizing average completion time. Why needed: To efficiently schedule tasks based on their probabilistic demands. Quick check: Confirm that the policy correctly prioritizes tasks based on their demand distributions.
- **Backend Prewarming**: A strategy to reduce latency by prewarming backends at optimal times. Why needed: To minimize the warm-up delay when processing requests. Quick check: Measure the latency reduction achieved by prewarming.
- **LSTF Algorithm**: An extension for deadline-constrained scenarios. Why needed: To handle applications with strict deadlines. Quick check: Validate that the algorithm meets the deadline constraints for all applications.

## Architecture Onboarding
- **Component Map**: Application Requests -> PDGraph Demand Estimation -> Gittins Scheduler -> Backend Prewarming -> Execution
- **Critical Path**: Application Request -> PDGraph Demand Estimation -> Gittins Scheduler -> Backend Execution
- **Design Tradeoffs**: Balancing accuracy of demand estimation with computational overhead; optimizing scheduling policy for completion time vs. resource utilization.
- **Failure Signatures**: Inaccurate demand estimation leading to suboptimal scheduling; backend prewarming failures causing increased latency.
- **First Experiments**: 1) Test PDGraph accuracy with synthetic demand distributions. 2) Validate Gittins policy performance under controlled workloads. 3) Measure latency reduction from backend prewarming.

## Open Questions the Paper Calls Out
None

## Limitations
- Effectiveness of PDGraph construction and maintenance in dynamic production environments is unclear.
- Gittins policy assumes stationary and well-characterized demand distributions, which may not hold for real LLM workloads.
- Focus on completion time optimization may not adequately address resource utilization efficiency or cost-effectiveness in multi-tenant cloud environments.

## Confidence
- **High confidence**: The technical feasibility of the PDGraph representation and its ability to capture functional unit dependencies is well-supported by the paper's formalism and experimental validation.
- **Medium confidence**: The effectiveness of the Gittins policy for scheduling optimization is demonstrated in controlled experiments, but its performance under highly dynamic, real-world conditions requires further validation.
- **Medium confidence**: The backend prewarming strategy shows promising results in reducing latency, but the energy and resource overhead of maintaining warm backends is not fully quantified.

## Next Checks
1. **Robustness Testing**: Evaluate Hermes's performance under bursty workloads and sudden shifts in application popularity to assess the resilience of PDGraph updates and demand predictions.
2. **Multi-Tenant Scalability**: Test the system's behavior in a multi-tenant cloud environment with competing resource demands to verify the trade-offs between completion time optimization and resource utilization.
3. **Energy and Cost Analysis**: Quantify the energy and operational cost overhead of backend prewarming to ensure the latency benefits justify the resource expenditure in production settings.