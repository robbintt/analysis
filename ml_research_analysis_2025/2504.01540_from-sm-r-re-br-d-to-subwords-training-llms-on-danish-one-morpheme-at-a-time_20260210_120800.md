---
ver: rpa2
title: "From Sm\xF8r-re-br\xF8d to Subwords: Training LLMs on Danish, One Morpheme\
  \ at a Time"
arxiv_id: '2504.01540'
source_url: https://arxiv.org/abs/2504.01540
tags:
- morphological
- danish
- tokenizer
- llama
- tokenizers
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluates the impact of morphologically-informed subword
  tokenization on Danish language modeling. The authors develop custom tokenizers
  using a semi-supervised Morfessor framework trained on an annotated Danish morphological
  dataset.
---

# From Smør-re-brød to Subwords: Training LLMs on Danish, One Morpheme at a Time

## Quick Facts
- arXiv ID: 2504.01540
- Source URL: https://arxiv.org/abs/2504.01540
- Reference count: 13
- Primary result: Morphological tokenization improves Danish LLM performance in linguistic acceptability and human evaluation tasks

## Executive Summary
This study investigates the impact of morphologically-informed subword tokenization on Danish language modeling. The authors develop custom tokenizers using a semi-supervised Morfessor framework trained on annotated Danish morphological data, comparing them against standard and Danish-specific BPE tokenizers. Their evaluation shows that morphological tokenizers achieve significantly better morphological segmentation (F1 58.84 vs 39.28 for Danish BPE) and demonstrate superior performance in linguistic acceptability and human evaluation tasks, though BPE-based models perform better in summarization. The research demonstrates that incorporating Danish morphological segmentation strategies into tokenizers leads to improved performance in generative transformer models for Danish language tasks.

## Method Summary
The authors developed custom morphological tokenizers using a semi-supervised Morfessor framework trained on an annotated Danish morphological dataset. They compared four tokenization approaches: standard BPE, Danish BPE, morphological, and a mixed approach combining morphological and BPE. The morphological tokenizer was trained on annotated data and then applied to unannotated data, creating a semi-supervised system. These tokenizers were evaluated both directly on morphological segmentation tasks and indirectly through downstream evaluation using LLaMA 3.2 1B and Cerebras-GPT 111M models on linguistic acceptability, human evaluation, and summarization tasks.

## Key Results
- Morphological tokenizer achieved F1 score of 58.84 on morphological segmentation, outperforming Danish BPE (F1 39.28)
- Models trained with morphological tokenizers showed superior performance in linguistic acceptability and human evaluation
- BPE-based models performed better in summarization tasks despite morphological tokenizer advantages in other areas

## Why This Works (Mechanism)
The morphological tokenizer leverages Danish's rich morphological structure by segmenting words into their constituent morphemes rather than treating them as atomic units. This approach captures linguistic patterns more naturally, allowing the model to learn relationships between word forms and their components. By preserving morphological information that BPE-based methods might obscure, the tokenizer enables better generalization and understanding of word formation patterns, particularly beneficial for languages with complex morphology like Danish.

## Foundational Learning
- Morfessor framework: Semi-supervised morphological segmentation algorithm; needed for handling morphologically rich languages where word formation follows systematic patterns
- Subword tokenization: Breaking words into smaller units to balance vocabulary size and out-of-vocabulary handling; critical for efficient language modeling
- Danish morphology: Understanding how Danish forms words through compounding and inflection; essential for creating effective tokenizers for this language
- BPE (Byte-Pair Encoding): Greedy algorithm that merges frequent character pairs; baseline comparison for subword segmentation methods
- Morphological segmentation: Dividing words into smallest meaningful units; improves model understanding of word formation and relationships

## Architecture Onboarding

Component Map:
Annotated Danish Corpus -> Morfessor Training -> Morphological Tokenizer -> LLaMA 3.2 1B/Cerebras-GPT 111M -> Downstream Tasks

Critical Path:
Morphological Tokenizer Training (semi-supervised Morfessor) -> Model Training (LLaMA 3.2 1B or Cerebras-GPT 111M) -> Evaluation (acceptability, human, summarization)

Design Tradeoffs:
- Morphological vs BPE: Better linguistic understanding vs computational efficiency
- Model size: Smaller models (1B parameters) for controlled experiments vs scalability to larger models
- Task selection: Linguistic acceptability vs practical tasks like summarization

Failure Signatures:
- Poor morphological segmentation F1 scores indicate tokenizer training issues
- Inconsistent downstream performance suggests mismatch between tokenizer and model architecture
- Task-specific failures reveal limitations in morphological approach for certain applications

First 3 Experiments:
1. Compare morphological tokenizer F1 scores against Danish BPE baseline
2. Evaluate LLaMA 3.2 1B performance on linguistic acceptability with different tokenizers
3. Test human evaluation preferences between morphological and BPE-based outputs

## Open Questions the Paper Calls Out
None identified in the source material.

## Limitations
- Small evaluation corpus (1,000 sentences for acceptability/human evaluation, 500 for summarization) may limit generalizability
- Experiments conducted on relatively small models (1B and 111M parameters) rather than larger contemporary LLMs
- Study focuses exclusively on Danish, limiting conclusions about cross-linguistic applicability

## Confidence
High: Morphological tokenizer superiority on morphological segmentation task (F1 58.84 vs 39.28)
Medium: Downstream task performance shows task-specific benefits but inconsistent across different evaluation types
Low-Medium: Danish-specific results may not generalize to other morphologically rich languages

## Next Checks
1. Scale up experiments to larger models (e.g., LLaMA 3.2 8B or similar) to verify whether morphological tokenization benefits persist at scale
2. Expand evaluation to multiple morphologically rich languages from different families (e.g., Turkish, Finnish, Arabic) to test the approach's cross-linguistic applicability
3. Conduct ablation studies varying corpus sizes and training durations to determine the minimum requirements for morphological tokenizer benefits to manifest