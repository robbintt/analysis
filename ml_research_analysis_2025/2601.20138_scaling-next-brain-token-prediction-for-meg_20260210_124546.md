---
ver: rpa2
title: Scaling Next-Brain-Token Prediction for MEG
arxiv_id: '2601.20138'
source_url: https://arxiv.org/abs/2601.20138
tags:
- distance
- prefix
- duration
- ratio
- bandpower
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: We introduce FlatGPT, a large autoregressive model for source-space
  MEG that scales next-token prediction to long context across datasets and scanners.
  FlatGPT uses a modified SEANet-style vector-quantizer to compress multichannel MEG
  into a flattened token stream, which is then modeled by a Qwen2.5-VL backbone trained
  from scratch to predict the next brain token and generate minutes of MEG from up
  to a minute of context.
---

# Scaling Next-Brain-Token Prediction for MEG

## Quick Facts
- arXiv ID: 2601.20138
- Source URL: https://arxiv.org/abs/2601.20138
- Reference count: 40
- One-line primary result: FlatGPT scales next-token prediction to long contexts across datasets and scanners, generating minutes of MEG from up to a minute of context with measurable conditional specificity.

## Executive Summary
FlatGPT introduces a large autoregressive model for source-space MEG that compresses multichannel brain signals into a flattened token stream using a modified SEANet-style vector-quantizer, then predicts the next brain token using a Qwen2.5-VL backbone. Trained on CamCAN and Omega datasets, FlatGPT generates minutes of MEG from up to a minute of context while maintaining on-manifold stability and conditional specificity. The model demonstrates cross-dataset generalization and remains stable over long rollouts, with generations closer to correct continuations than swapped controls across multiple metrics.

## Method Summary
FlatGPT uses BrainTokMix, a causal SEANet encoder with multichannel convolutions and residual vector quantization (RVQ), to compress 68-channel MEG (100 Hz) into a flattened discrete token stream (400 tokens/s vs. 6800 raw samples/s). This compression enables minute-scale contexts for Transformers. The flattened tokens are modeled by a Qwen2.5-VL-style decoder-only architecture trained from scratch to predict the next brain token. Generation uses sliding KV-cache with window-aligned strides, producing continuations up to 235 seconds from 61-second prompts. The system trains on CamCAN and Omega (420h total) and evaluates on held-out MOUS data.

## Key Results
- FlatGPT generates minutes of MEG from up to a minute of context while maintaining on-manifold stability
- Cross-dataset generalization achieved from CamCAN+Omega training to held-out MOUS evaluation
- Generations remain stable over long rollouts and are closer to correct continuations than swapped controls across multiple metrics
- On-manifold stability maintained with conditional specificity persisting beyond the conditioning window

## Why This Works (Mechanism)

### Mechanism 1
Discrete tokenization of multichannel MEG enables scalable autoregressive modeling with tractable context lengths. BrainTokMix compresses C×T MEG into a flattened discrete token stream using causal SEANet encoder with multichannel convolutions, followed by residual vector quantization (RVQ), reducing sequence length by 17× while preserving spatiotemporal structure through early channel mixing. Core assumption: RVQ bottleneck retains sufficient information for downstream generation fidelity; reconstruction quality bounds signal quality. Evidence: 17× compression ratio demonstrated, PCC=0.944 reconstruction quality. Break condition: If reconstruction PCC drops below 0.90, reduce compression ratio or increase RVQ levels.

### Mechanism 2
Flattening spatiotemporal tokens with MRoPE preserves structural identity while enabling full causal attention. The 3D token grid is serialized with RVQ level iterating fastest, and MRoPE applies axis-specific rotary embeddings to attention head subspaces, allowing distinct positional reasoning across time, latent space, and code levels while maintaining a single causal mask. Core assumption: Arbitrary ordering over latent spatial axis is learned and does not impair cross-stream dependencies. Evidence: Full attention across time and latent spatial streams under standard causal mask. Break condition: If cross-channel covariance structure degrades, investigate whether spatial attention ordering requires explicit spatial attention.

### Mechanism 3
Long-context conditioning without auxiliary metadata yields prompt-specific generations with measurable conditional specificity. The model conditions solely on tokenized prompt, and conditional specificity emerges from attention over long temporal context, enabling session/subject/task patterns to influence generation without explicit labels. Core assumption: Prompt contains sufficient signal about latent session characteristics to guide continuation. Evidence: Correct continuations reduce covariance distance by 0.088-0.130 relative to prompt-swap controls at 235.5s generated. Break condition: If prompt-swap gap shrinks to non-significant levels, context length is insufficient for that task type.

## Foundational Learning

- **Residual Vector Quantization (RVQ)**: Used in tokenizer to discretize latent vectors; understanding how residuals are sequentially quantized is essential for debugging reconstruction and token distribution. Quick check: Given a latent vector z, what is r^(2) after first RVQ level selects codebook entry e_k^(1)?
- **Causal vs. Non-Causal Convolutions in Tokenizers**: BrainTokMix uses strictly causal SEANet to enable autoregressive generation; non-causal variants would leak future information during inference. Quick check: Why does a causal encoder-decoder prevent "cheating" during open-loop generation?
- **MRoPE (Multimodal Rotary Position Embeddings)**: Position encoding must handle 3D structure in flattened sequences; MRoPE partitions embedding dimensions by axis. Quick check: How does MRoPE differ from standard 1D RoPE when processing a serialized (t, h, q) token grid?

## Architecture Onboarding

- **Component map**: Raw MEG (68×T) → BrainTokMix Encoder → Latent (T'×4096) → Reshape → RVQ → Flattened Tokens (L=T'×4×4) → RVQ-level Embeddings → Qwen2.5-VL Decoder → Next-token Predictions → Decode via Tokenizer → Generated MEG
- **Critical path**: Tokenizer reconstruction quality (PCC≥0.94 on held-out data), per-RVQ-level embedding tables with tied output heads, MRoPE position ids as (t,h,q) triplets, sliding KV-cache aligned to 10.24s tokenizer windows
- **Design tradeoffs**: Longer tokenizer windows improve reconstruction but introduce sawtooth artifacts in token statistics; more RVQ levels improve fidelity but increase tokens/s and destabilize long rollouts; folding RVQ levels into hidden dimension speeds training but destabilizes rollouts
- **Failure signatures**: High-frequency attenuation in PSD (gamma band underrepresented) indicates tokenizer bottleneck; prompt-swap gap collapses on specific tasks indicates context length insufficient; rollouts diverge off-manifold early indicates sampling temperature too high or RVQ prediction unstable
- **First 3 experiments**: 1) Validate tokenizer: Train BrainTokMix on 10.24s windows, verify PCC≥0.94 on held-out MOUS; inspect codebook usage perplexity. 2) Sanity check autoregressive loss: Train FlatGPT with 60s context, confirm loss decreases with expected sawtooth pattern. 3) Prompt-swap control: Generate 235s continuations from 60s prompts; compute covariance/PSD distances for correct vs. swapped pairings.

## Open Questions the Paper Calls Out
None

## Limitations
- Sufficiency of purely autoregressive prediction for modeling complex spatiotemporal brain dynamics over hours rather than minutes remains untested
- 17× tokenizer compression ratio may be insufficient for certain frequency bands or fine-grained spatial patterns, evidenced by gamma-band attenuation
- MRoPE implementation for 3D position encoding lacks detailed specification, creating potential reproducibility gaps

## Confidence
- **High Confidence**: On-manifold stability over long rollouts, cross-dataset training setup (420h across CamCAN+Omega), conditional specificity with prompt-swap controls, tokenizer reconstruction quality
- **Medium Confidence**: MRoPE positional encoding effectiveness for flattened spatiotemporal tokens, sufficiency of 17× compression ratio for full spectral fidelity, generalization claims across scanner types
- **Low Confidence**: Ability to maintain coherence over hours vs. minutes, capture of task-specific cognitive processes beyond statistical patterns, robustness to completely different acquisition protocols

## Next Checks
1. **Multi-hour generation stability test**: Generate continuations from 60s prompts extending to 4+ hours, evaluating on-manifold stability metrics at 30-minute intervals to identify temporal degradation patterns.
2. **Cross-protocol generalization**: Train FlatGPT on CamCAN+Omega, then evaluate generation quality and conditional specificity on a completely independent MEG dataset with different scanner types and experimental paradigms.
3. **Fine-grained spectral analysis**: Quantify generation fidelity across all frequency bands using multitaper spectral estimates, comparing prompt-swap vs. correct continuations with statistical tests to identify specific frequency ranges where compression artifacts persist.