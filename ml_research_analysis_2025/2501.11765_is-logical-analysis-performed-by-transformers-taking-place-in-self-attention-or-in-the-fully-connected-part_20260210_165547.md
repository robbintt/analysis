---
ver: rpa2
title: Is logical analysis performed by transformers taking place in self-attention
  or in the fully connected part?
arxiv_id: '2501.11765'
source_url: https://arxiv.org/abs/2501.11765
tags:
- qtrue
- matrix
- which
- then
- where
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper investigates whether logical processing in transformers
  occurs in self-attention or fully connected layers, using grammatical category pair
  prediction as a testbed. Three handcrafted one-level transformer solutions are presented:
  two use self-attention to combine token information, with logical analysis in fully
  connected layers, while one uses self-attention directly for category pair identification.'
---

# Is logical analysis performed by transformers taking place in self-attention or in the fully connected part?

## Quick Facts
- arXiv ID: 2501.11765
- Source URL: https://arxiv.org/abs/2501.11765
- Authors: Evgeniy Shin; Heinrich Matzinger
- Reference count: 8
- Primary result: Logical processing in transformers can occur in either self-attention or fully connected layers, with best performance from unconstrained models combining multiple solutions

## Executive Summary
This paper investigates whether logical processing in transformers occurs in self-attention or fully connected layers using grammatical category pair prediction as a testbed. The authors present three handcrafted one-level transformer solutions: two use self-attention to combine token information with logical analysis in fully connected layers, while one uses self-attention directly for category pair identification. The study examines gradient descent optimization challenges and identifies degenerate zero-gradient points, showing they can be avoided with softmax or normalization. Experiments train modified encoder layers with different attention weight configurations, revealing that unconstrained models combining multiple solutions achieve the best performance.

## Method Summary
The study trains single-level encoder layers on synthetic data where adjacent token categories must be predicted using a bilinear functional. Three attention configurations are tested: attention only on positional encoding (aggregation mode), attention only on category content (logical mode), and combined approaches. The models use dot-product attention without softmax initially, then with LayerNorm to prevent degenerate solutions. Training employs L-BFGS optimizer for 50 iterations with MSE loss on batch size 1000. The input matrix concatenates one-hot category and position encodings, with targets generated from a sampled bilinear function.

## Key Results
- Unconstrained attention models combining multiple solutions outperform constrained variants
- Softmax or LayerNorm prevents gradient descent from getting stuck in degenerate zero-gradient points
- The specific location of logical analysis (attention vs FC layers) depends on architectural constraints and optimization dynamics
- Transformers can process sequential information through multiple pathways, not just a single predetermined mechanism

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Self-attention can directly perform logical analysis (identifying category pairs) via bilinear products, rather than merely aggregating token information.
- Mechanism: The weight matrix $k^T q$ is handcrafted to represent a functional matrix $A_2$ of logical pairs. The attention score $w_{ji} = \vec{X}_j^T k^T q \vec{X}_i$ computes the functional value of the category pair directly. The $v$ matrix shifts this result into the positional encoding space for extraction.
- Core assumption: The logical relationship can be expressed as a bilinear form or functional matrix $A_2$ accessible to the attention weights.
- Evidence anchors: [abstract] "the logical analysis can also be performed within the self-attention." [section 2.2] "we want to use the matrix A2 for the bilinear product in self attention... tells us the category pair."

### Mechanism 2
- Claim: Standard transformers likely perform logical analysis in the fully connected (FC) layers after self-attention aggregates adjacent tokens.
- Mechanism: Self-attention uses positional encoding (matrix $D_{-1}$) to move the previous token $\vec{X}_{i-1}$ to the current position $i$ (aggregation). The skip connection adds the current token $\vec{X}_i$. The resulting sum is passed to a large FC layer (Matrix $B$ + ReLU) which is mathematically constructed to recognize specific category pair patterns (logical analysis).
- Core assumption: The FC layer has sufficient width (up to $N^2$ rows) to map all possible category pair combinations.
- Evidence anchors: [section 2.1] "determines the pair of categories... from these summed vector... via the fully connected layer." [abstract] "Traditionally, self-attention is seen as a mechanism for aggregating information..."

### Mechanism 3
- Claim: Gradient descent avoids degenerate zero-gradient points when using Softmax or normalization.
- Mechanism: In unconstrained attention, gradient descent can converge to degenerate solutions (e.g., matrices $q, k, v$ becoming zero). Softmax enforces constraints ($\sum q_{ji} = 1$) that eliminate these saddle points, forcing the optimizer to find the correct logical weights.
- Core assumption: The loss landscape contains "undesired zeros" where gradients vanish prematurely.
- Evidence anchors: [section 3.1] "Without precaution, gradient descent gets stuck in undesirable local minimae." [section 3.2] "With softmax, gradient descent... can not get stuck in a local minimum, but finds the correct solution!"

## Foundational Learning

- Concept: **Bilinear Forms & Functional Matrices**
  - Why needed here: The paper encodes "logic" not as sequential rules but as a matrix $A_2$ where entry $(i, j)$ represents the output for category pair $(i, j)$. Understanding how dot products retrieve these values is essential.
  - Quick check question: How does multiplying a one-hot row vector and a one-hot column vector by a matrix $A_2$ retrieve a specific entry $A_{ij}$?

- Concept: **Positional vs. Content Encoding Separation**
  - Why needed here: The solutions rely heavily on splitting the input matrix $X$ into content ($X_C$) and position ($X_P$). Some mechanisms (Solution 1) use attention *only* on $X_P$ to move vectors, while others (Solution 2) use it *only* on $X_C$ for logic.
  - Quick check question: In Solution 2, why is the result of the logical calculation stored in the *positional* part of the vector space?

- Concept: **ReLU Gating with Bias**
  - Why needed here: The paper uses a "large constant + negative bias + ReLU" trick to extract specific entries (the diagonal) from a matrix of values, effectively acting as a filter.
  - Quick check question: Why does adding 100 to a target entry and then applying `ReLU(x - 100)` zero out all other entries?

## Architecture Onboarding

- Component map: Input(X) -> Attention(Q,K,V) -> LayerNorm -> Linear(B) -> ReLU -> Linear(C) -> Output
- Critical path:
  1. **Encode:** Combine token ID and position ID into $X$
  2. **Attend:** Calculate $Q K^T$. In logical mode, this computes $q_{true}(X_j, X_i)$
  3. **Shift:** Use $V$ to move the result to the position of the *next* token (diagonal extraction preparation)
  4. **Select:** Add Skip Connection (adds 1 to diagonal) $\to$ ReLU(bias) $\to$ isolates $q_{true}(X_{i-1}, X_i)$

- Design tradeoffs:
  - **Interpretability vs. Size:** Solution 1 (FC Layer logic) requires $O(N^2)$ parameters for the matrix $B$. Solution 2 (Attention logic) requires only $O(N^2)$ for the logical matrix in $k^T q$ but avoids the massive FC layer, effectively distributing the logic.
  - **Optimization Stability:** Unconstrained attention allows multiple solutions but risks gradient zeros; Softmax stabilizes training but constrains the attention weights to be probabilities.

- Failure signatures:
  - **Gradient Zero:** Training loss stays static; inspection reveals $q \approx 0$ or $v \approx 0$. (Fix: Add Softmax/LayerNorm)
  - **Wrong Logic:** Model predicts category $i$ instead of pair $(i-1, i)$. (Fix: Check $V$ matrix shift logic)
  - **Memory Explosion:** Attempting Solution 1 with large vocabulary causes OOM due to $B$ matrix size.

- First 3 experiments:
  1. **Verify Aggregation:** Implement Solution 1 (Section 2.1). Train on small $N=4$ to verify the FC layer learns the "summing" logic.
  2. **Verify Logical Attention:** Implement Solution 2 (Section 2.2) *without* Softmax. Confirm if it gets stuck at a local minimum (Section 3.1).
  3. **Softmax Recovery:** Re-run Experiment 2 with Softmax or LayerNorm (Section 4.1) to confirm convergence to the correct logical matrix.

## Open Questions the Paper Calls Out

- **Generalization to real NLP tasks:** Do the findings regarding logical processing locations generalize to multi-layer transformers and real natural language text? The authors state they do not consider real sentences but generate vectors with i.i.d. digit entries and rely on synthetic data.

- **Tasks requiring memorization:** How does the division of logical processing between self-attention and fully connected layers differ in tasks requiring memorization or long-range dependencies? The current methodology focuses exclusively on the functional of adjacent tokens ($Y_i = q(X_{i-1}, X_i)$).

- **Hybrid solution preference:** What causes the unconstrained model to converge on a combination of solutions rather than a single mechanism, and is this preference robust? The experiments show unconstrained models combining solutions achieve the best performance, but the paper does not explain why the hybrid approach is preferred by gradient descent.

## Limitations
- Study is limited to synthetic data and a single encoder layer rather than natural language or multi-layer architectures
- Findings may not generalize to standard transformer training with SGD/Adam optimizers
- The term "logical analysis" conflates computational mechanisms with semantic understanding, as the model only learns predefined bilinear mappings

## Confidence
- **High confidence:** The demonstration that unconstrained attention can converge to degenerate zero-gradient solutions is empirically validated and well-supported by the mathematical analysis of the loss landscape.
- **Medium confidence:** The specific architectural solutions for distributing logical computation are well-defined for the synthetic task, but their applicability to natural language processing tasks remains speculative.
- **Low confidence:** The broader implications about where transformers "perform logical analysis" in practice are not empirically supported, particularly for pre-trained transformers and complex downstream tasks.

## Next Checks
1. **Generalization to real NLP tasks:** Test whether the same distributional patterns of logical computation hold when applying these attention mechanisms to actual language tasks like syntactic dependency prediction or semantic role labeling.

2. **Scaling to deeper architectures:** Implement the three solutions in a multi-layer transformer and examine whether logical computation remains distributed similarly, or whether deeper layers develop different computational strategies.

3. **Alternative optimizers and initialization:** Replicate the gradient descent experiments using standard optimizers (AdamW) and various initialization schemes to determine whether the degenerate solution problem is specific to L-BFGS.