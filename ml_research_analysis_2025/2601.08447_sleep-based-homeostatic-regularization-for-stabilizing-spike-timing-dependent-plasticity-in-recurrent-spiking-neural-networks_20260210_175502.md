---
ver: rpa2
title: Sleep-Based Homeostatic Regularization for Stabilizing Spike-Timing-Dependent
  Plasticity in Recurrent Spiking Neural Networks
arxiv_id: '2601.08447'
source_url: https://arxiv.org/abs/2601.08447
tags:
- sleep
- learning
- weight
- synaptic
- spiking
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the problem of weight instability in recurrent
  spiking neural networks (SNNs) trained with spike-timing-dependent plasticity (STDP).
  Without intervention, recurrent connections lead to unbounded weight growth, catastrophic
  forgetting, and loss of representational diversity.
---

# Sleep-Based Homeostatic Regularization for Stabilizing Spike-Timing-Dependent Plasticity in Recurrent Spiking Neural Networks

## Quick Facts
- arXiv ID: 2601.08447
- Source URL: https://arxiv.org/abs/2601.08447
- Reference count: 14
- One-line primary result: Periodic sleep-based synaptic renormalization prevents weight explosion in recurrent STDP networks while preserving learned structure.

## Executive Summary
This paper tackles the problem of weight instability in recurrent spiking neural networks (SNNs) trained with spike-timing-dependent plasticity (STDP). Without intervention, recurrent connections lead to unbounded weight growth, catastrophic forgetting, and loss of representational diversity. The authors introduce a neuromorphic-inspired sleep-wake cycle: periodic offline phases where external inputs are suppressed, weights undergo stochastic decay toward a homeostatic baseline, and spontaneous activity enables memory consolidation. The method is fully local, compatible with neuromorphic hardware, and adds negligible computational overhead. Empirically, low-to-intermediate sleep durations (10–20% of training) significantly improve stability and performance on MNIST-like benchmarks in the STDP-SNN model, with no data-specific hyperparameter tuning. In contrast, the same sleep intervention provides no measurable benefit for a surrogate-gradient SNN model, suggesting the approach is particularly suited to Hebbian learning. The results demonstrate that periodic, sleep-based renormalization can stabilize local Hebbian learning while preserving learned structure.

## Method Summary
The method introduces a sleep-wake cycle into recurrent SNN training. During wake phases, Poisson-encoded images drive spiking activity and STDP updates. When total synaptic weight exceeds a threshold or time interval elapses, the network enters a sleep phase: external inputs are suppressed, intrinsic membrane noise drives spontaneous activity, and synaptic weights decay multiplicatively toward a homeostatic baseline via power-law decay. This continues until weights return below the threshold or maximum sleep duration is reached. The approach stabilizes weight dynamics while preserving learned structure through selective decay of stronger synapses.

## Key Results
- Sleep-based regularization prevents weight explosion in recurrent STDP networks while maintaining accuracy
- Optimal sleep duration is 10-20% of training time, balancing stability and learning capacity
- Power-law decay preserves relative synaptic ordering while preventing unbounded growth
- Sleep intervention provides no benefit for surrogate-gradient trained SNNs, highlighting locality compatibility

## Why This Works (Mechanism)

### Mechanism 1: Power-Law Weight Renormalization
- Claim: Periodic power-law decay toward a baseline target prevents weight explosion while preserving relative synaptic ordering.
- Mechanism: During sleep phases, weights update via $w_i(t+1) = w_{tgt} \left( \frac{w_i(t)}{w_{tgt}} \right)^\lambda$ where $\lambda < 1$ causes higher-magnitude weights to decay faster, maintaining learned structure.
- Core assumption: The synaptic homeostasis hypothesis (SHy) accurately models biological sleep's renormalization function.
- Evidence anchors:
  - [abstract]: "synaptic weights undergo stochastic decay toward a homeostatic baseline"
  - [Section 2.2.4]: Equation 4 defines power-law decay; Figure 3 shows oscillating weight dynamics between wake growth and sleep regularization
  - [corpus]: Limited direct support; paper 73689 addresses homeostasis but via different mechanisms
- Break condition: If sleep duration exceeds 20-30% or decay rate is too aggressive, weights collapse toward uniform values.

### Mechanism 2: Consolidation Through Spontaneous Replay
- Claim: STDP-active sleep phases enable implicit memory consolidation via noise-driven reactivation.
- Mechanism: External input suppression + intrinsic membrane noise ($\xi_i \sim \mathcal{N}(0, \sigma^2)$) produces spontaneous spiking; ongoing STDP selectively reinforces previously learned co-activation patterns.
- Core assumption: Spontaneous activity during sleep recapitulates task-relevant correlations learned during wake.
- Evidence anchors:
  - [abstract]: "spontaneous activity enables memory consolidation"
  - [Section 2.2.4]: "membrane dynamics are driven by intrinsic noise, producing spontaneous spiking activity that can continue to elicit STDP updates"
  - [corpus]: Deperrois et al. (2022), cited within paper, demonstrates perturbed dreaming improves generalization
- Break condition: If noise amplitude is too low, no replay occurs; if too high, learned structure degrades.

### Mechanism 3: Locality-Compatibility Constraint
- Claim: Sleep regularization benefits local Hebbian learning but provides no systematic gain for gradient-based optimization.
- Mechanism: STDP and sleep decay both operate on local pre/post-synaptic statistics; surrogate-gradient methods rely on global error signals that conflict with multiplicative weight renormalization.
- Core assumption: Locality preservation is fundamental to neuromorphic compatibility.
- Evidence anchors:
  - [abstract]: "same sleep intervention yields no measurable benefit for the surrogate-gradient SNN"
  - [Section 4]: "global renormalization increasingly conflicts with gradient-based optimization"
  - [corpus]: No direct corpus evidence on this specific incompatibility
- Break condition: Do not apply this protocol to backpropagation-trained SNNs expecting performance gains.

## Foundational Learning

- Concept: **Spike-Timing-Dependent Plasticity (STDP)**
  - Why needed here: This is the unstable learning rule being regularized; understanding its asymmetric potentiation/depression window is prerequisite.
  - Quick check question: Why does presynaptic-before-postsynaptic spiking cause potentiation while the reverse causes depression?

- Concept: **Recurrent Excitatory-Inhibitory Dynamics**
  - Why needed here: Pathological weight growth emerges specifically from recurrent excitatory feedback amplifying perturbations; inhibition alone insufficient without sleep.
  - Quick check question: What positive-feedback loop drives runaway potentiation in recurrent STDP networks?

- Concept: **Homeostatic Plasticity vs. Hebbian Plasticity**
  - Why needed here: Sleep implements homeostatic renormalization; understanding how it counterbalances Hebbian potentiation clarifies the stabilization mechanism.
  - Quick check question: How does multiplicative synaptic scaling differ from the power-law decay used here?

## Architecture Onboarding

- Component map:
  - Input: 225 Poisson-encoded neurons (rate coding from 15×15 images)
  - Excitatory: 200 LIF neurons, 15% recurrent connectivity, STDP plastic
  - Inhibitory: 50 LIF neurons, 25% → excitatory, iSTDP plastic
  - Sleep controller: Triggers when total weight $W_x(t)$ exceeds baseline; applies power-law decay + intrinsic noise

- Critical path:
  1. Wake: Input drives spiking → STDP updates → weights grow linearly
  2. Sleep trigger: $W_x(t) > \alpha_{trig} \cdot W_x(0)$ or interval elapsed
  3. Sleep: Input suppressed → noise-driven spontaneous activity → power-law decay continues while STDP remains active
  4. Termination: $W_x(t) \leq \alpha_{base} \cdot W_x(0)$ or max sleep duration reached

- Design tradeoffs:
  - Sleep ratio 10-20%: Optimal range; 0% → instability, >30% → diminishing returns
  - Decay power λ=0.9997: Aggressive decay preserves boundedness but risks over-pruning
  - Noise σ=3mV: Controls replay strength vs. structure corruption
  - Sign-clamping: Required to maintain excitatory/inhibitory polarity

- Failure signatures:
  - Weight explosion in <5 batches: Sleep ratio too low or decay too weak
  - Uniform weight collapse: Sleep duration or decay rate excessive
  - No class clustering in t-SNE: Noise too low (no replay) or too high (structure destroyed)
  - Flat performance across sleep ratios: Likely using gradient-based learning (SG-SNN incompatible)

- First 3 experiments:
  1. Replicate toy geometric task (4 classes) with/without sleep to validate weight stability before scaling.
  2. Grid search sleep ratio [0%, 10%, 20%, 30%] on MNIST subset to locate optimal operating point.
  3. Ablate intrinsic noise (set σ=0) during sleep to isolate pure decay effects from replay contributions.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the proposed sleep mechanism generalize to deeper architectures and complex, large-scale datasets?
- Basis in paper: [explicit] The authors state that "embedding the proposed sleep regularization into deeper SNN architectures and evaluating it on larger-scale datasets would help assess scalability."
- Why unresolved: Empirical validation was restricted to downscaled 15x15 MNIST-family benchmarks and shallow network topologies.
- What evidence would resolve it: Demonstrating stable learning on high-dimensional datasets (e.g., CIFAR-100, ImageNet) using multi-layer SNNs.

### Open Question 2
- Question: How does intrinsic membrane noise interact with sleep duration to facilitate memory consolidation?
- Basis in paper: [explicit] The authors suggest "systematically varying noise levels across different sleep rates could reveal interaction effects and clarify the conditions under which sleep most effectively supports learning."
- Why unresolved: The current study fixes noise variance ($\lambda_\sigma$) while varying sleep ratios, leaving the specific contribution of noise to stability uncertain.
- What evidence would resolve it: Ablation studies mapping performance across a grid of noise magnitudes and sleep durations.

### Open Question 3
- Question: Is the lack of benefit for surrogate-gradient models attributable to the feedforward architecture rather than the learning rule?
- Basis in paper: [inferred] The authors note the SG-SNN is purely feedforward while the STDP-SNN is recurrent, admitting observed differences "cannot be attributed solely to the presence or absence of sleep-based regularization."
- Why unresolved: It remains unclear if sleep requires recurrent dynamics to enable replay-based consolidation or if it is fundamentally incompatible with gradient-based optimization.
- What evidence would resolve it: Applying the sleep protocol to a recurrent SNN trained with surrogate gradients (e.g., using BPTT).

### Open Question 4
- Question: How does sleep-based synaptic downscaling interact with other homeostatic mechanisms like intrinsic plasticity?
- Basis in paper: [explicit] The authors propose to "examine how sleep interacts with other forms of homeostatic regulation, such as intrinsic plasticity."
- Why unresolved: The study focuses on synaptic scaling but does not evaluate whether it complements or conflicts with neuronal excitability adaptations.
- What evidence would resolve it: Analyzing stability when both synaptic sleep-phases and adaptive firing thresholds are combined.

## Limitations
- Limited exploration of noise amplitude and decay rate sensitivity
- Comparison with single surrogate-gradient architecture may not capture full incompatibility landscape
- Biological plausibility relies on SHy but doesn't validate power-law decay matches observed synaptic renormalization patterns

## Confidence

- **High confidence:** Weight instability in recurrent STDP networks without intervention (supported by weight explosion observations across multiple studies)
- **Medium confidence:** Power-law decay prevents catastrophic forgetting while preserving learned structure (supported by empirical results but limited ablation studies)
- **Medium confidence:** Sleep duration 10-20% represents optimal operating range (based on limited grid search; exact optimal point varies by task)
- **Low confidence:** Locality-compatibility constraint explanation (minimal empirical support for incompatibility claim)

## Next Checks
1. Conduct systematic ablation study varying noise amplitude σ (0.1-10 mV) during sleep to quantify replay contribution vs. pure decay effects
2. Test sleep intervention on gradient-based SNNs with different architectures (deeper networks, different regularization methods) to better characterize incompatibility boundaries
3. Measure representational similarity between pre-sleep and post-sleep weight configurations to quantify information preservation during renormalization