---
ver: rpa2
title: 'Taming Hallucinations: Boosting MLLMs'' Video Understanding via Counterfactual
  Video Generation'
arxiv_id: '2512.24271'
source_url: https://arxiv.org/abs/2512.24271
tags:
- video
- arxiv
- counterfactual
- question
- visual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper addresses visual hallucinations in multimodal large\
  \ language models (MLLMs) caused by over-reliance on language priors when processing\
  \ counterfactual videos. The authors propose a two-part solution: first, a novel\
  \ data synthesis framework called DualityForge that uses diffusion-based video editing\
  \ to transform real videos into counterfactual scenarios with embedded structured\
  \ context; second, a two-stage training method (DNA-Train) that applies supervised\
  \ fine-tuning followed by reinforcement learning with pair-wise \u2113\u2081 advantage\
  \ normalization to balance learning across real and counterfactual video pairs."
---

# Taming Hallucinations: Boosting MLLMs' Video Understanding via Counterfactual Video Generation

## Quick Facts
- arXiv ID: 2512.24271
- Source URL: https://arxiv.org/abs/2512.24271
- Reference count: 40
- Primary result: 24.0% relative improvement in hallucination detection over baseline

## Executive Summary
This paper addresses visual hallucinations in multimodal large language models (MLLMs) caused by over-reliance on language priors when processing counterfactual videos. The authors propose a two-part solution: first, a novel data synthesis framework called DualityForge that uses diffusion-based video editing to transform real videos into counterfactual scenarios with embedded structured context; second, a two-stage training method (DNA-Train) that applies supervised fine-tuning followed by reinforcement learning with pair-wise ℓ₁ advantage normalization to balance learning across real and counterfactual video pairs. Experiments on their curated dataset show a 24.0% relative improvement over the Qwen2.5-VL-7B baseline on hallucination detection, while also improving performance on general video understanding benchmarks, demonstrating strong generalization. The approach effectively reduces hallucinations by compelling the model to ground reasoning in visual evidence rather than language priors.

## Method Summary
The proposed method combines counterfactual video generation with specialized training to address hallucinations in MLLMs. The DualityForge framework creates counterfactual videos by applying diffusion-based editing to real videos, transforming them into scenarios that require models to rely on visual evidence rather than language priors. The DNA-Train approach uses supervised fine-tuning on real videos followed by reinforcement learning with pair-wise ℓ₁ advantage normalization, training the model to distinguish between real and counterfactual scenarios while maintaining general video understanding capabilities. This dual approach forces MLLMs to ground their reasoning in visual evidence rather than defaulting to language-based assumptions.

## Key Results
- 24.0% relative improvement over Qwen2.5-VL-7B baseline on hallucination detection
- Performance improvements extend to general video understanding benchmarks
- Strong generalization demonstrated across different video comprehension tasks

## Why This Works (Mechanism)
The approach works by disrupting the model's tendency to rely on language priors through exposure to counterfactual scenarios during training. By generating videos that contradict expected language-based assumptions, the model learns to prioritize visual evidence over linguistic patterns. The two-stage training method (supervised fine-tuning followed by reinforcement learning) ensures the model first learns basic video understanding before being challenged to distinguish real from counterfactual scenarios. The pair-wise ℓ₁ advantage normalization in the reinforcement learning phase helps balance learning across the two video types, preventing the model from over-specializing in either real or counterfactual contexts.

## Foundational Learning
**Counterfactual Video Generation**
- Why needed: To create training data that challenges language priors and forces visual grounding
- Quick check: Verify generated counterfactuals maintain semantic coherence while contradicting expected outcomes

**Diffusion-Based Video Editing**
- Why needed: To enable controlled transformation of real videos into counterfactual scenarios
- Quick check: Assess edit quality and temporal consistency across video frames

**Pair-wise ℓ₁ Advantage Normalization**
- Why needed: To balance learning between real and counterfactual video pairs during reinforcement learning
- Quick check: Monitor training stability and convergence across both video types

## Architecture Onboarding

**Component Map**
DualityForge (counterfactual generator) -> DNA-Train (two-stage training pipeline) -> MLLM model

**Critical Path**
1. Real video input → DualityForge → Counterfactual video generation
2. Real + counterfactual videos → DNA-Train (SFT phase) → Pre-trained MLLM
3. Fine-tuned model + video pairs → DNA-Train (RL phase with ℓ₁ normalization) → Final MLLM

**Design Tradeoffs**
- Computational cost vs. hallucination reduction effectiveness
- Quality of generated counterfactuals vs. training stability
- Balance between real and counterfactual video exposure

**Failure Signatures**
- Over-reliance on language priors despite training
- Inconsistent performance between real and counterfactual scenarios
- Training instability during RL phase

**First Experiments**
1. Ablation study: Remove counterfactual generation to quantify its specific contribution
2. Cross-dataset evaluation on established video understanding benchmarks
3. Scalability test on larger MLLM variants (14B+ parameters)

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation primarily conducted on a single curated dataset, raising generalizability concerns
- Diffusion-based video editing may introduce artifacts affecting training quality
- Two-stage training approach requires substantial computational resources

## Confidence
- **High confidence**: Core methodology for hallucination mitigation is technically sound
- **Medium confidence**: Generalization claims need validation across more diverse benchmarks
- **Medium confidence**: Effectiveness of ℓ₁ advantage normalization requires more extensive ablation studies

## Next Checks
1. Conduct cross-dataset evaluation using established video understanding benchmarks (e.g., ActivityNet, Something-Something) to verify generalization beyond the curated test set
2. Perform systematic ablation studies removing the counterfactual generation component to quantify its specific contribution versus the supervised fine-tuning baseline
3. Test the approach on larger MLLM variants (14B+ parameters) to assess scalability and determine if improvements hold at scale