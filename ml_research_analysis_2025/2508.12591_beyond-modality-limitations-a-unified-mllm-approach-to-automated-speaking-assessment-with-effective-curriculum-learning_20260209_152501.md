---
ver: rpa2
title: 'Beyond Modality Limitations: A Unified MLLM Approach to Automated Speaking
  Assessment with Effective Curriculum Learning'
arxiv_id: '2508.12591'
source_url: https://arxiv.org/abs/2508.12591
tags:
- assessment
- language
- learning
- multimodal
- speech
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study presents the first systematic investigation of Multimodal
  Large Language Models (MLLMs) for Automated Speaking Assessment (ASA). Traditional
  ASA systems face modality limitations: text-based approaches lack acoustic information
  while audio-based methods miss semantic context.'
---

# Beyond Modality Limitations: A Unified MLLM Approach to Automated Speaking Assessment with Effective Curriculum Learning

## Quick Facts
- arXiv ID: 2508.12591
- Source URL: https://arxiv.org/abs/2508.12591
- Reference count: 32
- First systematic investigation of MLLMs for Automated Speaking Assessment with curriculum learning

## Executive Summary
This study presents the first systematic investigation of Multimodal Large Language Models (MLLMs) for Automated Speaking Assessment (ASA). Traditional ASA systems face modality limitations: text-based approaches lack acoustic information while audio-based methods miss semantic context. The authors propose Speech-First Multimodal Training (SFMT), a curriculum learning strategy that first establishes robust acoustic foundations before cross-modal integration. Using Phi-4-multimodal, the system processes both raw audio and ASR transcripts through specialized pathways for comprehensive assessment across content, delivery, language use, and holistic aspects. Experiments on the TEEMI dataset demonstrate MLLM superiority with Pearson Correlation Coefficient (PCC) values consistently above 0.82, representing significant improvements over state-of-the-art models.

## Method Summary
The system uses Phi-4-multimodal with LoRA (rank=320) on the audio encoder, employing a two-stage Speech-First Multimodal Training (SFMT) curriculum. Stage 1 optimizes audio-only with null text placeholders, while Stage 2 introduces transcripts. The architecture processes raw audio through a 460M-parameter Conformer encoder and ASR transcripts through tokenization, both mapping to a shared embedding space via a projector before integration by the 3.8B decoder backbone. Three aspect-specific models (Content, Delivery, Language Use) generate CEFR-aligned scores, with holistic assessment aggregating results. Training uses AdamW (lr=4e-5), 3 epochs, batch size 32, gradient accumulation steps 16, bfloat16, and Flash Attention on a single NVIDIA RTX 3090.

## Key Results
- MLLM-based systems elevate holistic assessment performance from PCC 0.783 to 0.846
- SFMT achieves 4% absolute accuracy improvement over conventional training, particularly excelling in delivery assessment (PCC 0.848)
- Cross-corpus evaluation on Speak & Improve Corpus validates generalization with RMSE of 0.387
- Audio-only configuration achieves 0.835 PCC for delivery vs. 0.776 for text-only

## Why This Works (Mechanism)

### Mechanism 1: Speech-First Curriculum Establishes Acoustic Foundations Before Text Integration
Staged training with audio-only preceding multimodal input improves delivery assessment accuracy by approximately 4% absolute over simultaneous multimodal training. SFMT exploits a learning hierarchy where audio modality demonstrates faster convergence and stronger initial performance. Stage 1 optimizes LoRA audio adapter parameters on audio-only data, forcing the model to develop robust acoustic representations without textual shortcuts. Stage 2 then introduces transcripts while preserving learned acoustic discrimination. This prevents the "modality imbalance" where models preferentially optimize toward computationally efficient text features.

### Mechanism 2: Unified Cross-Modal Attention Synchronizes Acoustic and Semantic Information
Processing raw audio and ASR transcripts within a single MLLM framework achieves higher holistic assessment correlation (PCC 0.846) than late-fusion approaches combining separate unimodal systems. The mixture-of-LoRAs architecture employs modality-specific adapters that map audio and text into a shared embedding space before integration by the 3.8B decoder backbone. Cross-modal attention enables genuine synchronization rather than output-level fusion. Each specialized pathway preserves modality-specific features while the decoder learns joint representations across content, delivery, and language use aspects.

### Mechanism 3: Audio Modality Encodes Delivery-Relevant Information Lost in ASR Transcription
Text-only models systematically underperform on delivery assessment because ASR transcription discards prosodic, fluency, and pronunciation features essential for evaluating spoken production quality. Raw audio signals preserve the complete spectrum of speech information including stress patterns, intonation contours, and speech rhythm. ASR systems trained predominantly on native speech introduce systematic biases and achieve only 14.75% WER on TEEMI, propagating errors to text-based assessment. Audio-only MLLM configurations access ground-truth acoustic patterns directly, enabling discrimination of fine-grained delivery characteristics that text representations cannot encode.

## Foundational Learning

- **Curriculum Learning (Bengio et al., 2009):** Why needed here: SFMT applies curriculum learning at the modality level rather than sample-difficulty level. Quick check: Can you explain why training on audio-only before adding text constitutes a "curriculum," and what makes audio "simpler" or more "foundational" than text in this context?

- **LoRA (Low-Rank Adaptation):** Why needed here: The architecture uses "mixture-of-LoRAs" with rank=320 applied to the audio encoder. Quick check: What is the computational advantage of updating only LoRA adapter parameters (rank=320) versus full fine-tuning of the 460M-parameter audio encoder?

- **Pearson Correlation Coefficient (PCC) for Assessment Evaluation:** Why needed here: All performance claims use PCC as the primary metric. Quick check: A PCC improvement from 0.783 to 0.846 represents what percentage increase in explained variance (R²)?

## Architecture Onboarding

- **Component map:**
  Raw Audio → Audio Encoder (460M, Conformer blocks) → Audio Projector → Shared Embedding Space → 3.8B Decoder (Phi-4) → CEFR Scores (C/D/L/H)
  ASR Transcript → Tokenizer ─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┘
  LoRAaudio Adapter (460M, rank=320) ──────────────────────────────────────────────────────────────────────────────────────────────────────────────────┘

- **Critical path:**
  1. Audio input undergoes feature extraction via Conformer-based encoder
  2. Audio projector maps acoustic features to shared embedding space aligned with text tokens
  3. LoRA adapter learns task-specific acoustic traits without modifying base language capabilities
  4. Decoder generates aspect-specific CEFR-aligned scores (8-level scale: Pre-A1 to B2)

- **Design tradeoffs:**
  - SFMT vs. standard training: +4% delivery accuracy vs. additional training stage complexity
  - Separate aspect models vs. unified model: Three specialized models (C, D, L) enable focused optimization but increase inference cost; holistic score integrates all three
  - Whisper vs. Phi-4 ASR: Whisper achieves 14.75% WER vs. Phi-4's 18.25% on TEEMI, but integrated ASR simplifies pipeline

- **Failure signatures:**
  - Text modality dominance: Delivery PCC drops significantly (e.g., <0.80) while content/language use remain high → model learned to rely on text shortcuts
  - Cascading ASR errors: Content assessment performance degrades for low-proficiency speakers with poor pronunciation → ASR quality bottleneck
  - Overfitting to training prompts: Unseen task (A02) performance drops >10% PCC → insufficient generalization

- **First 3 experiments:**
  1. **Modality ablation baseline:** Train Phi-4 with audio-only, text-only, and multimodal configurations on TEEMI A01. Compare PCC across all four assessment aspects. Expected result: audio-only outperforms text-only for delivery (per Table III).
  2. **SFMT vs. simultaneous training comparison:** Implement two-stage SFMT (audio-only Stage 1 for 1.5 epochs, multimodal Stage 2 for 1.5 epochs) vs. standard 3-epoch multimodal training. Measure absolute accuracy improvement on delivery aspect. Target: ≥4% improvement.
  3. **Cross-corpus generalization test:** Fine-tune on TEEMI A01, evaluate on Speak & Improve Corpus without additional training. Compare RMSE and PCC against BERT and W2V baselines. Target: SFMT achieves lower RMSE (0.387 vs. 0.394 for W2V per Table V).

## Open Questions the Paper Calls Out
- **Multi-task learning framework:** Can a unified multi-task learning framework simultaneously assess all aspects (Content, Delivery, Language Use) while maintaining or exceeding the performance of separately trained specialized models? Current approach trains three separate models for each aspect with aspect-specific instructions.
- **Explanatory feedback generation:** Can MLLM-based ASA systems generate explanatory, pedagogically useful feedback alongside proficiency scores? Current study focuses only on score prediction despite MLLM's text generation capabilities.
- **Diverse population generalization:** Does SFMT's effectiveness generalize across diverse L1 backgrounds, age groups, and proficiency levels beyond the tested populations (Chinese university learners, CEFR Pre-A1 to B2)? TEEMI corpus contains L2 learners with implied homogeneous L1 background.

## Limitations
- **TEEMI dataset accessibility:** Core experimental results rely entirely on the TEEMI corpus, which is not publicly available, blocking current validation attempts
- **ASR quality impact:** System uses Whisper large v2 with 14.75% WER, raising questions about performance in high-quality transcription scenarios
- **Cross-corpus generalization scope:** Speak & Improve Corpus evaluation shows generalization but to similarly limited contexts (L2 English with CEF levels B1-C1)

## Confidence
- **High Confidence:** The unified MLLM architecture with cross-modal attention outperforms late-fusion approaches; Audio modality provides essential delivery assessment information not recoverable from text; SFMT curriculum learning strategy improves delivery assessment by approximately 4% absolute
- **Medium Confidence:** The 0.846 holistic PCC represents a robust improvement over state-of-the-art; Cross-corpus generalization maintains performance without catastrophic forgetting; Mixture-of-LoRAs architecture provides optimal parameter efficiency
- **Low Confidence:** The exact magnitude of SFMT's advantage without access to TEEMI; Performance with perfect (oracle) transcripts vs. noisy ASR; Generalization to proficiency levels below B1 and non-English languages

## Next Checks
1. **Modality Ablation with Public Data:** Replicate the core finding that audio-only outperforms text-only for delivery assessment using a publicly available L2 speech corpus (e.g., IEMOCAP with CEFR-style annotations or TED-LIUM with proficiency labels).
2. **ASR Quality Sensitivity Analysis:** Evaluate the same MLLM model with progressively degraded ASR quality (controlled WER from 5% to 25%) on any available speech corpus to quantify the actual contribution of the audio pathway versus the ASR pathway.
3. **Low-Resource Generalization Test:** Fine-tune the MLLM on a minimal subset of TEEMI (e.g., 10% of speakers, 20% of responses) and evaluate on both held-out TEEMI and Speak & Improve Corpus to test whether the architecture maintains its performance advantages under realistic resource constraints.