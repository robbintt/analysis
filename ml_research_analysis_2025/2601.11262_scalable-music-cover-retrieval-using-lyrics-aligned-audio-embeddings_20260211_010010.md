---
ver: rpa2
title: Scalable Music Cover Retrieval Using Lyrics-Aligned Audio Embeddings
arxiv_id: '2601.11262'
source_url: https://arxiv.org/abs/2601.11262
tags:
- audio
- retrieval
- lyrics
- music
- cover
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: LIVI achieves competitive performance for music cover retrieval
  by aligning audio embeddings directly with lyric-based text embeddings, avoiding
  the need for transcription at inference. Trained to match cosine similarities in
  a fixed multilingual semantic space, LIVI outperforms audio baselines and rivals
  state-of-the-art methods, especially on large-scale datasets like Discogs-VI, with
  substantially lower latency and model complexity.
---

# Scalable Music Cover Retrieval Using Lyrics-Aligned Audio Embeddings

## Quick Facts
- **arXiv ID:** 2601.11262
- **Source URL:** https://arxiv.org/abs/2601.11262
- **Reference count:** 0
- **Primary result:** LIVI achieves competitive performance for music cover retrieval by aligning audio embeddings directly with lyric-based text embeddings, avoiding transcription at inference.

## Executive Summary
LIVI is a novel approach to music cover retrieval that bypasses transcription by directly mapping audio into a fixed multilingual semantic space where lyrics are the primary invariant. By training a lightweight audio encoder to mimic the output of a full ASR+Text pipeline, LIVI achieves competitive performance while substantially reducing inference latency. The method excels on large-scale datasets like Discogs-VI, where it outperforms traditional audio baselines and rivals state-of-the-art methods.

## Method Summary
LIVI trains a student audio encoder to map vocal segments directly to text embeddings from a multilingual semantic space (gte-multilingual-base), avoiding the need for transcription at inference. The approach uses a frozen Whisper encoder with attention-based temporal pooling, followed by a 4-layer MLP projection head. Training combines cosine similarity and pairwise geometry preservation losses on 1.5M training pairs from Discogs-VI, with evaluation on Covers80, SHS100k-TEST, and Discogs-VI using MR1, HR@1, and MAP@10 metrics.

## Key Results
- Outperforms audio baselines and rivals state-of-the-art methods on Discogs-VI dataset
- Achieves substantial latency reduction compared to transcription-based approaches
- Competitive performance on established benchmarks (Covers80, SHS100k-TEST)

## Why This Works (Mechanism)

### Mechanism 1
If lyrics are transcribed and embedded into a multilingual semantic space, cover songs cluster together regardless of musical arrangement changes. The system projects audio into a fixed text embedding space where similarity is determined by semantic meaning rather than acoustic proximity, bypassing complex harmonic invariance modeling.

### Mechanism 2
A lightweight audio encoder can replace a heavy autoregressive transcription pipeline by learning to directly mimic the latents of the transcription process. LIVI decouples the ASR encoder from the decoder, training a student encoder to map audio directly to the text embedding that the full pipeline would produce.

### Mechanism 3
Training with a geometry-preserving loss (pairwise similarity matching) creates more robust embedding structure than simple pointwise matching. The loss function combines Cosine Similarity with an MSE term that forces relationships between audio embeddings to mirror relationships between text embeddings.

## Foundational Learning

- **Whisper (ASR) Architecture**
  - Why needed here: LIVI repurposes the encoder part of Whisper. You must understand that Whisper converts audio to Log-Mel Spectrograms and then to latent states, which the paper treats as a rich phonetic representation.
  - Quick check question: What is the output dimension of the Whisper encoder used in the paper, and why is it frozen?

- **Attention Pooling (Aggregation)**
  - Why needed here: The model must compress variable-length frame-level features into a single fixed vector. The paper uses a learnable query token ([CLS]) to weigh the importance of different time frames.
  - Quick check question: How does the "Attention-based Temporal Pooling" decide which parts of the 30s window are most important for the final embedding?

- **Multilingual Text Embeddings (Sentence-BERT/gte)**
  - Why needed here: The "target" for the audio model is a vector from a text model. Understanding that these vectors capture semantic meaning is crucial to grasping why LIVI works for retrieval.
  - Quick check question: Why does the paper choose gte-multilingual-base over simpler monolingual models for the target embedding space?

## Architecture Onboarding

- **Component map:** Vocal Detection (Musicnn) -> 30s Segmentation -> Log-Mel Spectrogram -> Whisper Encoder (Frozen) -> Attention Pooling with [CLS] token -> 4-Layer MLP Projection Head -> 768-dim Vector

- **Critical path:** The Vocal Detection stage is critical; if non-vocal audio is passed in, Whisper tends to "hallucinate" text, and the projection head will generate garbage embeddings.

- **Design tradeoffs:** Freezing Whisper ensures stability and lowers compute but limits adaptation to non-speech vocalizations. The system sacrifices universality (instrumentals fail) for efficiency and lyrical robustness.

- **Failure signatures:** Low similarity on covers indicates instrumental solos or spoken-word tracks confusing the vocal detector. High latency likely stems from preprocessing rather than LIVI inference itself. Hallucinated matches suggest pure noise or instrumental audio with low vocal detection threshold.

- **First 3 experiments:**
  1. Measure cosine similarity between g_audio and g_text outputs on test set to ensure projection head convergence (>0.85 mean similarity).
  2. Replace Attention Pooling with Mean Pooling to quantify performance drop.
  3. Benchmark inference time of LIVI pipeline against full transcription pipeline to confirm ~20x speedup.

## Open Questions the Paper Calls Out

1. **Harmonic Integration:** How can harmonic features be optimally integrated with LIVI's lyrics-aligned embeddings to handle instrumental tracks or covers with substantially altered lyrics?

2. **Text Embedding Fine-tuning:** To what extent does task-specific fine-tuning of the text embedding model yield performance gains over the off-the-shelf multilingual model?

3. **Vocal Detection Overhead:** Can the computational overhead of the vocal detection preprocessing stage be reduced to achieve substantial runtime improvements relative to audio-only systems?

## Limitations
- The system fundamentally fails on instrumental covers or spoken-word content where lyrics are absent or minimal.
- Proprietary vocal detection model weights and training data are not disclosed, requiring approximation with open alternatives.
- Multilingual generalization is not explicitly tested across different languages.

## Confidence
- **High Confidence:** Competitive performance on established benchmarks and significant latency reduction claims.
- **Medium Confidence:** Superior performance on Discogs-VI dataset (relies on proprietary data).
- **Low Confidence:** Claims about multilingual semantic space effectiveness without specific cross-language testing.

## Next Checks
1. Evaluate LIVI on a multilingual cover song dataset (English, Spanish, Chinese, Arabic) to verify multilingual semantic space effectiveness.
2. Systematically vary the vocalness threshold Î» and measure its impact on retrieval performance, comparing against ground truth vocal segments.
3. Create a test set of instrumental covers and measure performance to validate the system's limitation on non-lyrical content.