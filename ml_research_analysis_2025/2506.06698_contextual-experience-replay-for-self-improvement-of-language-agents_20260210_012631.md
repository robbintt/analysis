---
ver: rpa2
title: Contextual Experience Replay for Self-Improvement of Language Agents
arxiv_id: '2506.06698'
source_url: https://arxiv.org/abs/2506.06698
tags:
- page
- agent
- agents
- tasks
- experiences
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Contextual Experience Replay (CER), a training-free
  framework that enables language agents to improve through self-learning during inference.
  CER accumulates past experiences into a dynamic memory buffer, distilling environment
  dynamics and decision-making patterns from previous trajectories, then retrieving
  and replaying this knowledge in new tasks to enhance adaptability.
---

# Contextual Experience Replay for Self-Improvement of Language Agents

## Quick Facts
- arXiv ID: 2506.06698
- Source URL: https://arxiv.org/abs/2506.06698
- Reference count: 40
- Primary result: 36.7% success rate on WebArena (51% relative improvement over baseline)

## Executive Summary
This paper introduces Contextual Experience Replay (CER), a training-free framework enabling language agents to improve through self-learning during inference. CER accumulates past experiences into a dynamic memory buffer, distilling environment dynamics and decision-making patterns from previous trajectories, then retrieving and replaying this knowledge in new tasks to enhance adaptability. Evaluated on WebArena and VisualWebArena benchmarks, CER achieves competitive performance: 36.7% success rate on WebArena (51% relative improvement over GPT-4o baseline) and 31.9% on VisualWebArena.

## Method Summary
CER operates as a training-free framework where a base agent (implemented with ReAct pattern) executes tasks while accumulating experiences. The framework comprises four modules: an explorer/annotator collects trajectories, a distillation module abstracts these into "dynamics" (page descriptions, URLs) and "skills" (step-by-step action patterns), an experience buffer stores these distilled experiences, and a retrieval module selects relevant experiences for new tasks. Retrieved experiences are formatted as natural language and injected into the agent's context window before execution. The system supports offline (human-annotated data), online (self-generated experiences), and hybrid learning settings, with GPT-4o as the backbone model using temperature 0.1.

## Key Results
- CER achieves 36.7% success rate on WebArena (51% relative improvement over GPT-4o baseline)
- CER demonstrates 41% new problem types solved, proving its validity as a self-improvement framework
- CER shows strong stability and plasticity with 93% stability and 141% plasticity metrics

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Distilling abstracted environment dynamics and decision-making patterns (skills) from past trajectories enables better generalization than raw replay.
- Mechanism: CER separates experiences into "dynamics" (page descriptions, URLs, usages) and "skills" (step-by-step action patterns with variables). The distillation module uses an LLM (GPT-4o in the paper) to abstract trajectories into generalized rules. These are stored in a buffer and retrieved based on task relevance. This reduces the large state/action space to a subset of promising actions, acting as a heuristic for decision-making.
- Core assumption: LLMs can effectively generalize from specific trajectory examples to abstract rules, and these rules can be correctly applied to new, unseen tasks.
- Evidence anchors:
  - [abstract] "CER accumulates and synthesizes past experiences into a dynamic memory buffer. These experiences encompass environment dynamics and common decision-making patterns..."
  - [section 3.1] "The model is required to output the final distillation in an abstract and general way... to ensure that the experiences can be broadly applied."
  - [section 5.2] "CER shows a significant improvement in cross-template success rates... validates that the improvement of CER does not come from memorizing the whole trajectory."
  - [corpus] Related work like "Scaling Agent Learning via Experience Synthesis" supports the idea of synthesizing experiences for learning. "Memory-Driven Self-Improvement for Decision Making" also uses memory for decision refinement.
- Break condition: Distillation produces overly specific or incorrect abstractions, leading to negative transfer. Retrieval fails to surface relevant experiences for novel tasks.

### Mechanism 2
- Claim: In-context replay of relevant experiences guides the agent's policy during inference without requiring model updates.
- Mechanism: Retrieved experiences (dynamics and skills) are formatted as natural language and injected into the LLM agent's system prompt. This augment's the agent's context (C') before it takes actions. The LLM then uses its reasoning capabilities (ReAct-style) to follow the retrieved patterns, effectively performing a form of few-shot or in-context learning on its own past, distilled behavior.
- Core assumption: The LLM's context window is sufficient to hold relevant experiences, and the model can follow provided abstract instructions/skills effectively. The base LLM has sufficient reasoning capability to interpret and execute the skills.
- Evidence anchors:
  - [abstract] "...allowing the agents to retrieve and augment themselves with relevant knowledge in new tasks..."
  - [section 3.3] "...transform the selected k experiences... into natural language experience descriptions... integrate them into the model's context C..."
  - [section 5.2] "With the highlighted promising states, actions, and decision-making patterns, the agent can issue a correct action much more easily."
  - [corpus] "Get Experience from Practice: LLM Agents with Record & Replay" explores a similar concept, highlighting this paradigm's relevance.
- Break condition: Context window limits are exceeded; injected experiences are irrelevant or contradictory, confusing the agent; the base model is too weak to follow the complex instructions.

### Mechanism 3
- Claim: The system exhibits stability and plasticity by continually accumulating new experiences while preserving old ones in a unified buffer.
- Mechanism: New experiences are distilled and "merged" into the dynamic buffer. Retrieval is performed over this growing knowledge base. This allows the agent to solve new task types (plasticity) while maintaining the ability to solve previously learned types (stability), avoiding catastrophic forgetting since the base model's weights are not updated.
- Core assumption: The retrieval mechanism can effectively prioritize relevant experiences from a growing and potentially noisy buffer. The merging process avoids redundancy or contradiction.
- Evidence anchors:
  - [section 1] "...demonstrating its generalizability and effectiveness as a self-improvement system."
  - [section 5.3] "CER demonstrates 41% new problem types solved, proving the validity of CER as a self-improvement framework."
  - [table 4] Shows explicit stability (93%) and plasticity (141%) metrics.
  - [corpus] "Reliability-Adjusted Prioritized Experience Replay" is a related concept in RL, though CER applies it in-context for LLMs.
- Break condition: The buffer grows too large for efficient retrieval or context injection; noisy or failed trajectories corrupt the buffer, degrading performance.

## Foundational Learning

- Concept: **ReAct (Reasoning + Acting) Pattern**
  - Why needed here: The paper's agent is implemented using the ReAct framework (section 4.1.1), which interleaves "Thought" and "Action" steps. Understanding this pattern is crucial because the distilled "skills" are generated in a step-by-step format and the agent is expected to follow a similar structure.
  - Quick check question: Can you explain how an agent using the ReAct pattern decides on its next step?

- Concept: **In-Context Learning (ICL)**
  - Why needed here: CER is a "training-free" method that relies entirely on the LLM's ability to learn from information provided in its context window. The mechanism of replaying experiences is a form of ICL.
  - Quick check question: What are the primary limitations of relying on in-context learning versus fine-tuning a model?

- Concept: **Retrieval-Augmented Generation (RAG)**
  - Why needed here: CER uses a retrieval module to select relevant experiences from a buffer before each task. This is a RAG-like architecture applied to an agent's own memory.
  - Quick check question: How does the retrieval step in CER differ from a standard RAG system used for external knowledge?

## Architecture Onboarding

- Component map: Explorer/Annotator -> Distillation Module (D) -> Experience Buffer (Îµ) -> Retrieval Module (R) -> Decision-Making Agent (A)
- Critical path: A new task comes in -> Retrieval Module queries the Buffer for relevant dynamics and skills -> Retrieved experiences are formatted and injected into the Agent's system prompt -> The Agent executes the task, generating a new trajectory -> The Distillation Module processes this new trajectory -> Merged into the Buffer.
- Design tradeoffs:
  - **Generalization vs. Specificity:** The distillation prompt must be tuned to abstract away specific details (e.g., "forum name" instead of "books") to ensure experiences transfer across tasks.
  - **Token Cost vs. Context Size:** Retrieving more experiences (higher `k`) provides more guidance but consumes more of the context window, potentially pushing out important observations.
  - **Trajectory Quality:** The system can learn from both successful and failed trajectories (Section 5.6), but noisy exploration data can degrade performance (Section A.5).
- Failure signatures:
  - **Negative Transfer:** Agent makes incorrect decisions because a retrieved skill or dynamic was abstracted incorrectly or applied in the wrong context.
  - **Retrieval Miss:** Agent fails to solve a task that requires a specific skill that was present in the buffer but not retrieved.
  - **Format Drift:** The distillation module produces skills in a format that the agent cannot follow, or the agent fails to follow the ReAct format, leading to execution errors (mentioned in section 5.4 regarding Llama-3.1-70B).
- First 3 experiments:
  1.  **Ablation on Distillation Granularity:** Run the system on a subset of tasks (e.g., WebArena Forum) using three settings: (1) raw trajectory replay, (2) CER with only dynamics, (3) CER with only skills, and (4) CER with both. Compare success rates to quantify the value of each distilled component.
  2.  **Sensitivity to Retrieval Size (`k`):** Run the online CER system with different values for `k_dynamics` and `k_skills` (e.g., k=1, 3, 5, 10). Plot success rate and token cost to find the optimal balance between guidance and context usage.
  3.  **Robustness to Noisy Data:** In the offline setting, provide the distillation module with a mix of high-quality human-annotated trajectories and low-quality "random exploration" trajectories (as described in Section A.5). Measure the performance degradation compared to using only high-quality data.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can CER's performance be improved by more sophisticated filtering or weighting of failed trajectories during distillation?
- Basis in paper: [explicit] The paper states CER distills from both successful and failed trajectories, noting that failed ones can be misleading. Results show a performance gap (33.5% vs 31.4%) when using only successful trajectories (Section 5.6).
- Why unresolved: The current framework treats all trajectories similarly during distillation, relying on the LLM to implicitly filter noise. A dedicated mechanism to downweight or refine lessons from failed attempts is unexplored.
- What evidence would resolve it: A modified CER implementation with a trajectory scoring or filtering module that yields higher performance than the baseline and approaches the "CER success" oracle.

### Open Question 2
- Question: How does the utility of the "environment dynamics" module (URLs, page summaries) transfer to non-web environments?
- Basis in paper: [explicit] In Limitations (Section 7), the authors note that dynamics are highly effective in web environments partially because agents can navigate directly via URL. They ask: "how we can utilize the environment dynamics in other agent tasks, such as real-world navigation".
- Why unresolved: The current framework relies on web-specific affordances. It is unclear if the concept of "dynamics" would generalize to environments without discrete, directly-addressable states.
- What evidence would resolve it: Evaluation of a modified CER framework on a non-web benchmark (e.g., embodied AI or OS interaction) where state access is less direct.

### Open Question 3
- Question: What are the failure modes when CER is applied to weaker or smaller open-source models, and can targeted prompt engineering mitigate them?
- Basis in paper: [inferred] While experiments with Llama-3.1-70B show improvement, the gains are smaller than with GPT-4o. The paper attributes this to weaker models' difficulty in formatting and distilling multi-step skills robustly (Section 5.4).
- Why unresolved: The analysis is high-level. It is not known if the bottleneck is in the agent's action generation, the distillation of skills, or the retrieval and application of the distilled experiences.
- What evidence would resolve it: A detailed error analysis on an open-source model run, breaking down failures by component (agent, distiller, retriever), followed by targeted prompt refinements that improve performance.

### Open Question 4
- Question: Can the efficiency of offline data collection be improved beyond human annotation or simple self-guided exploration?
- Basis in paper: [explicit] The paper notes that offline learning from self-guided exploration trajectories can be detrimental due to noise and lack of structure. It concludes by asking "how to generate such trajectories in a more efficient way" (Section 7, Limitations).
- Why unresolved: The paper explores two extremes (human-annotated and random exploration) but does not investigate methods for generating goal-oriented, structured trajectories automatically.
- What evidence would resolve it: An automated trajectory generation method that produces higher-quality experiences than self-guided exploration, leading to improved performance in the offline setting.

## Limitations
- The framework's performance depends heavily on the quality of the underlying LLM for distillation and retrieval, with weaker models showing significantly reduced gains
- The experience buffer may accumulate noise over time, though long-term stability data is not provided
- VisualWebArena performance gains are more modest (31.9%) compared to WebArena (36.7%), suggesting visual grounding integration may be a limiting factor

## Confidence
- **High Confidence**: The core mechanism of distilling and retrieving abstracted experiences works as described, supported by ablation studies and the observed 51% relative improvement over baseline.
- **Medium Confidence**: Claims about stability and plasticity are supported by metrics but could be affected by buffer growth and retrieval noise over extended use.
- **Medium Confidence**: The assertion that CER generalizes rather than memorizes is supported by cross-template success rates, but the test templates may still be too similar to training data.

## Next Checks
1. **Long-term Buffer Stability**: Run CER for 100+ tasks on WebArena, monitoring success rate decay and retrieving the same experiences multiple times to measure performance consistency.
2. **Cross-Domain Transfer**: Test CER-trained experiences on a novel website (e.g., email client) to verify whether distilled skills truly generalize beyond the training domains.
3. **Weak Model Distillation**: Replace GPT-4o with GPT-3.5 or Llama in the distillation module and measure the impact on abstracted experience quality and downstream performance.