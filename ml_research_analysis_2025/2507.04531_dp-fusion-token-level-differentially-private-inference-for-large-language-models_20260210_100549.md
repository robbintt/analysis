---
ver: rpa2
title: 'DP-Fusion: Token-Level Differentially Private Inference for Large Language
  Models'
arxiv_id: '2507.04531'
source_url: https://arxiv.org/abs/2507.04531
tags:
- privacy
- private
- dp-fusion
- baseline
- document
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of privacy leakage at inference
  time in large language models, where generated outputs can inadvertently reveal
  sensitive information from the context, such as personally identifiable information.
  The authors propose DP-Fusion, a token-level Differentially Private Inference (DPI)
  mechanism that provably bounds the influence of sensitive tokens on the LLM's output.
---

# DP-Fusion: Token-Level Differentially Private Inference for Large Language Models

## Quick Facts
- arXiv ID: 2507.04531
- Source URL: https://arxiv.org/abs/2507.04531
- Reference count: 40
- Primary result: 6× lower perplexity than related DPI methods while providing formal privacy guarantees

## Executive Summary
This paper introduces DP-Fusion, a token-level Differentially Private Inference (DPI) mechanism designed to prevent large language models from inadvertently leaking sensitive information during inference. The method addresses a critical gap where existing DP mechanisms focus on training but leave inference vulnerable to privacy breaches. DP-Fusion works by blending output distributions from inference runs with and without sensitive tokens, ensuring the final output remains within a bounded distance of the baseline while provably limiting the influence of sensitive content.

## Method Summary
DP-Fusion implements a novel token-level differentially private inference approach that operates by maintaining two parallel inference states: one with all context tokens and one without sensitive tokens. At each generation step, the method blends the output distributions from these two states using a carefully designed mechanism that ensures the final output distribution is within a bounded distance (measured by KL divergence) from the baseline. This token-level approach allows for fine-grained privacy protection while maintaining generation quality. The method provides formal differential privacy guarantees through mathematical bounds on how much any single sensitive token can influence the output distribution.

## Key Results
- Achieves 6× lower perplexity compared to existing DPI methods on document privatization tasks
- Maintains average perplexity between 1.42–1.46 at privacy levels ε = 16–66
- Provides formal differential privacy guarantees through provable bounds on sensitive token influence

## Why This Works (Mechanism)
The mechanism works by exploiting the probabilistic nature of LLM inference to create a privacy-preserving blend of outputs. By running inference twice—once with full context and once without sensitive tokens—DP-Fusion can quantify and bound the influence of sensitive information on the output distribution. The key insight is that the difference between these two distributions represents the "privacy risk" from sensitive tokens, which can be mathematically bounded using differential privacy principles. The blending mechanism ensures that the final output cannot deviate too far from what would have been generated without the sensitive information, effectively "washing out" the influence of potentially private content while preserving overall generation quality.

## Foundational Learning
- Differential Privacy: Mathematical framework for quantifying privacy loss; needed to provide formal guarantees on information leakage, quick check: ε parameter bounds maximum privacy loss
- KL Divergence: Measure of difference between probability distributions; needed to quantify how much sensitive tokens influence output, quick check: symmetric measure of distribution distance
- Token-Level Privacy: Granular approach to privacy protection; needed to address specific sensitive tokens rather than entire contexts, quick check: operates at individual token granularity
- Inference-Time Privacy: Focus on protecting information during generation; needed since training-time privacy doesn't prevent context leakage, quick check: applies to already-trained models
- Distribution Blending: Technique for combining probability outputs; needed to merge private and non-private inference results, quick check: weighted combination of distributions
- Privacy Budget (ε): Numerical value quantifying privacy-utility tradeoff; needed to calibrate the strength of privacy protection, quick check: lower ε = stronger privacy

## Architecture Onboarding

**Component Map:**
Context Processor -> Dual Inference Engine -> Distribution Blender -> Output Generator

**Critical Path:**
1. Identify sensitive tokens in input context
2. Run parallel inference (with and without sensitive tokens)
3. Blend distributions using privacy-preserving mechanism
4. Generate final output token

**Design Tradeoffs:**
- Fine-grained token-level privacy vs. computational overhead of dual inference
- Strong privacy guarantees (low ε) vs. generation quality and utility
- Mathematical rigor of formal DP guarantees vs. practical implementation complexity
- Per-token processing overhead vs. overall inference latency

**Failure Signatures:**
- High perplexity indicating poor generation quality
- Inconsistent outputs across multiple runs with same input
- Excessive computational resource usage due to parallel inference
- Privacy budget exhaustion leading to degraded performance

**First Experiments:**
1. Test on synthetic documents with known sensitive information to verify privacy guarantees
2. Compare perplexity and generation quality against baseline non-private inference
3. Evaluate performance impact on different model sizes and sequence lengths

## Open Questions the Paper Calls Out
None

## Limitations
- Limited evaluation on diverse real-world datasets with various types of sensitive information
- No characterization of computational overhead for larger models or longer sequences
- Privacy budget values (ε = 16–66) may still allow information leakage depending on sensitivity requirements
- Lack of user studies or adversarial testing to validate practical privacy protection

## Confidence
- High confidence in mathematical formulation and formal privacy guarantees
- Medium confidence in experimental results due to limited task diversity
- Medium confidence in practical privacy protection claims given the evaluation scope

## Next Checks
1. Evaluate DP-Fusion on diverse real-world datasets containing various types of sensitive information (e.g., medical records, financial data) to assess practical privacy protection and utility trade-offs.
2. Test the method's performance and scalability on longer sequences and larger models (e.g., GPT-4 scale) to understand computational overhead and potential limitations.
3. Conduct user studies or adversarial testing to verify that generated outputs do not inadvertently reveal sensitive information, complementing the formal privacy guarantees.