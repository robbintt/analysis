---
ver: rpa2
title: Evaluating Answer Reranking Strategies in Time-sensitive Question Answering
arxiv_id: '2503.04972'
source_url: https://arxiv.org/abs/2503.04972
tags:
- answer
- temporal
- most
- answers
- questions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study examines how temporal characteristics of answers impact
  performance in question answering (QA) systems over diachronic document collections.
  We evaluate multiple temporal reranking strategies using two QA datasets with explicit
  and implicit temporal questions.
---

# Evaluating Answer Reranking Strategies in Time-sensitive Question Answering

## Quick Facts
- arXiv ID: 2503.04972
- Source URL: https://arxiv.org/abs/2503.04972
- Reference count: 18
- This study examines how temporal characteristics of answers impact performance in question answering (QA) systems over diachronic document collections.

## Executive Summary
This study evaluates multiple temporal reranking strategies for question answering systems using two datasets with explicit and implicit temporal questions. The research compares temporal approaches (grouping answers by publication dates) against non-temporal methods (combining retrieval scores with reader confidence) across a diachronic news corpus. The findings reveal that non-temporal approaches, particularly a hybrid method combining BM25 retrieval with BERT reader confidence, generally outperform temporal methods for both question types. However, temporal grouping by year shows promise for capturing relevant temporal information when answers are concentrated around specific events.

## Method Summary
The study employs a two-stage QA pipeline using the NYT Annotated Corpus (1.8M articles, 1987-2007) divided into 1,194,730 passages. BM25 retrieves top-100 passages per query, then a BERT reader extracts candidate answer spans with confidence scores. Eight reranking strategies are tested: four non-temporal (Retrieval-Based, Reader-Based, Most Common Answer, Hybrid Retrieval-Reader) and four temporal (Most Recent, Oldest, Most Common Date, Temporal Monthly/Yearly Grouping). Performance is measured using Exact Match (EM) and F1 scores on ArchivalQA (15k QA pairs) and TemporalQuestions (1k questions) datasets.

## Key Results
- Hybrid Retrieval-Reader method achieves 50.05 EM / 66.47 F1 on ArchivalQA explicit questions, outperforming all temporal methods
- Temporal Yearly Grouping method achieves best performance among temporal approaches at 33.27 EM / 45.94 F1
- Explicit and implicit temporal questions show different performance patterns across strategies, suggesting need for separate treatment

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Combining retrieval scores with reader confidence signals outperforms temporal-only heuristics for answer selection in diachronic corpora.
- Mechanism: A weighted linear combination (CombinedScore = (1 − μ) * S_BM25 + μ * S_BERT) balances lexical relevance from BM25 with semantic plausibility from a BERT-based reader. When μ = 0.5, both signals contribute equally to final answer ranking.
- Core assumption: Candidate answers with both strong lexical match and high reader confidence are more likely correct than answers selected purely by temporal position.
- Evidence anchors:
  - [abstract] "non-temporal approaches, particularly the Hybrid Retrieval-Reader method, generally outperform temporal methods for both question types"
  - [section 4, Table 2] Hybrid Retrieval-Reader achieves 50.05 EM / 66.47 F1 on ArchivalQA explicit questions versus 42.97 EM for Retrieval-Based alone and 34.79 EM for Reader-Based alone

### Mechanism 2
- Claim: Grouping candidate answers by publication year and selecting from the densest temporal cluster captures relevant temporal signals better than finer-grained or single-point temporal heuristics.
- Mechanism: Answers are binned by year; the year with the highest answer count is selected; within that year, the highest Hybrid Retrieval-Reader scored answer is returned. This captures "bursty" coverage around events.
- Core assumption: Event-related questions induce correlated publication activity concentrated in specific years; yearly granularity is coarse enough to aggregate signal but fine enough to isolate relevant periods.
- Evidence anchors:
  - [abstract] "Temporal Yearly Grouping methods achieve the best performance, suggesting that grouping answers by publication dates can capture relevant temporal information"
  - [section 5.2, Table 4] Temporal Yearly Grouping achieves 33.27 EM / 45.94 F1 (ArchivalQA explicit), substantially outperforming Temporal Monthly Grouping at 24.87 EM

### Mechanism 3
- Claim: Explicit and implicit temporal questions respond differently to reranking strategies, indicating separate treatment may improve overall QA accuracy.
- Mechanism: Explicit temporal questions contain date references that may align with specific temporal heuristics; implicit temporal questions lack such markers and rely more on content-based ranking. The study observes differing performance patterns across these types.
- Core assumption: The presence or absence of explicit temporal cues in the question changes which answer features (temporal vs. semantic) are predictive of correctness.
- Evidence anchors:
  - [abstract] "explicit and implicit temporal questions benefit from different ranking strategies, indicating the need for separate treatment of these question types"
  - [section 6, Tables 2-4] Reader-Based Reranking outperforms Retrieval-Based on TemporalQuestions implicit (36.60 EM vs. 26.40 EM), while patterns differ for explicit questions

## Foundational Learning

- Concept: **Retriever-Reader QA Architecture**
  - Why needed here: The entire experimental framework assumes a two-stage pipeline—BM25 retrieves passages, then a BERT reader extracts candidate answers. Understanding score semantics is essential for reranking design.
  - Quick check question: Given 100 retrieved passages and a μ=0.5 hybrid score, what happens to ranking if BM25 scores are normalized to [0,1] but BERT scores remain unnormalized?

- Concept: **Temporal Granularity in IR**
  - Why needed here: The paper compares day-level, month-level, and year-level temporal groupings. Understanding why coarser groupings can outperform finer ones is key to interpreting results.
  - Quick check question: Why might a "most common date" (day granularity) heuristic underperform compared to yearly grouping for questions about multi-week events?

- Concept: **Exact Match (EM) and F1 for QA Evaluation**
  - Why needed here: All results are reported in EM and F1. EM is strict string match; F1 allows partial credit via token overlap. Misinterpreting these metrics leads to wrong conclusions about relative performance.
  - Quick check question: If an answer span is "1997" but ground truth is "1997-1998," what would EM and F1 scores be?

## Architecture Onboarding

- Component map:
  Corpus (NYT Annotated Corpus) -> Passage segmentation (1,194,730 passages) -> BM25 Retriever (top-100 passages) -> BERT Reader (candidate answers with scores) -> Reranker (8 strategies) -> Final answer -> EM/F1 evaluation

- Critical path:
  1. Query → BM25 retrieval (top 100 passages)
  2. Passages → BERT reader → candidate answer spans with scores
  3. Candidates → reranking strategy → single final answer
  4. Final answer → EM/F1 evaluation

- Design tradeoffs:
  - **BM25 vs. DPR**: Paper found BM25 outperformed DPR (64.60 EM vs. lower on ArchivalQA sample) but this may not generalize to other corpora or query types.
  - **Temporal vs. non-temporal reranking**: Temporal methods add complexity without consistent gains in this study; however, they may be essential for specific domains (legal, medical) where recency matters.
  - **Yearly vs. monthly grouping**: Yearly is more robust but may miss sub-year events; monthly is finer but noisier.

- Failure signatures:
  - **Extremely low EM for direct temporal methods** (e.g., Most Recent Answer at 4.39 EM) suggests temporal position alone is a weak signal—expect failure if you rely solely on recency without semantic validation.
  - **Divergent performance across datasets** (ArchivalQA vs. TemporalQuestions) indicates dataset-specific tuning is required; a strategy that works on one may fail on another.
  - **Low retrieval recall**: The 100-passage cutoff targets ~86% recall per BERTSerini; if your corpus has different characteristics, you may miss relevant passages entirely.

- First 3 experiments:
  1. **Baseline replication**: Implement the Hybrid Retrieval-Reader (μ=0.5) on a sample of ArchivalQA; verify EM/F1 within ±2 points of reported 50.05 EM to confirm pipeline correctness.
  2. **Temporal grouping ablation**: Compare Temporal Yearly Grouping vs. Temporal Monthly Grouping on a held-out subset; measure whether yearly granularity consistently outperforms monthly and quantify the gap.
  3. **Explicit vs. implicit question stratification**: Classify a new dataset into explicit/implicit temporal questions; run both Hybrid Retrieval-Reader and Temporal Yearly Grouping; verify if performance patterns diverge as the paper suggests.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can a combined approach integrating temporal features with non-temporal scores outperform the standard Hybrid Retrieval-Reader method?
- Basis in paper: [explicit] The authors state in the Conclusion and Future Work that they aim to "explore effective ways of combining temporal and non-temporal reranking approaches."
- Why unresolved: The experiments evaluated temporal and non-temporal methods in isolation; while non-temporal methods won, the potential synergy of combining them was not tested.
- What evidence would resolve it: Experimental results showing a composite metric (e.g., temporal grouping weighted by reader scores) achieving higher EM/F1 scores than the non-temporal baseline.

### Open Question 2
- Question: Do the findings regarding temporal reranking generalize to domain-specific datasets such as Legal or Medical QA?
- Basis in paper: [explicit] The authors explicitly list continuing the analysis on "other temporal datasets, including also domain-specific datasets (e.g., Medical or Legal QA)" as a necessary step for future work.
- Why unresolved: The current study is limited to the New York Times Annotated Corpus (general news), and the effectiveness of these strategies in specialized domains remains unknown.
- What evidence would resolve it: A comparative evaluation of the proposed temporal grouping strategies on a labeled legal or medical diachronic corpus.

### Open Question 3
- Question: How do temporal reranking strategies impact the performance of modern Large Language Models (LLMs) or Retrieval-Augmented Generation (RAG) systems?
- Basis in paper: [inferred] The authors explicitly limited their methodology to BM25 and BERT-based readers "instead of LLMs and RAG to minimize the risk of hallucinations," leaving the interaction with generative models unexplored.
- Why unresolved: It is unclear if the "Most Common Date" or "Temporal Yearly Grouping" strategies provide signal value to generative models that already possess strong internal reasoning capabilities.
- What evidence would resolve it: Benchmarks comparing LLM performance with and without the proposed temporal reranking filters on the TemporalQuestions dataset.

## Limitations

- The study relies on a single news corpus (NYT Annotated Corpus), which may not generalize to other domains or time-sensitive question answering contexts.
- Temporal methods' performance may be particularly sensitive to the quality and consistency of date metadata in the corpus, which is not thoroughly evaluated.
- The study assumes that questions can be cleanly classified as explicit or implicit temporal, but in practice, this classification may be ambiguous and could affect strategy selection performance.

## Confidence

- **High confidence**: The Hybrid Retrieval-Reader method outperforming pure temporal approaches is well-supported by multiple experiments and consistent across both datasets.
- **Medium confidence**: The observation that explicit and implicit temporal questions require different strategies is supported by the data but could be influenced by dataset-specific characteristics.
- **Medium confidence**: The superiority of yearly temporal grouping over finer-grained temporal methods is demonstrated, but this finding may be specific to the bursty nature of news publication patterns.

## Next Checks

1. **Dataset generalization test**: Apply the reranking strategies to a temporally diverse corpus from a different domain (e.g., scientific papers or legal documents) to verify whether the Hybrid Retrieval-Reader advantage persists outside the news domain context.

2. **Temporal noise sensitivity analysis**: Systematically corrupt or remove publication dates from a subset of the corpus and measure the degradation of temporal methods versus non-temporal methods to quantify the impact of temporal metadata quality.

3. **Question classification robustness check**: Implement multiple explicit/implicit temporal question classifiers and evaluate how classification errors affect the performance gap between strategies for each question type, determining whether the observed differences are robust to classification uncertainty.