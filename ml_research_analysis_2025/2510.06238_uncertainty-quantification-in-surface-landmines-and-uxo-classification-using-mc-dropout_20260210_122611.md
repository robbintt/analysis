---
ver: rpa2
title: Uncertainty Quantification In Surface Landmines and UXO Classification Using
  MC Dropout
arxiv_id: '2510.06238'
source_url: https://arxiv.org/abs/2510.06238
tags:
- uncertainty
- dropout
- class
- learning
- landmine
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study introduces uncertainty quantification using Monte Carlo
  (MC) Dropout into a fine-tuned ResNet-50 model for surface landmine and UXO classification,
  tested on a simulated dataset. The approach quantifies epistemic uncertainty during
  inference, enabling detection of unreliable predictions under adversarial and noisy
  conditions.
---

# Uncertainty Quantification In Surface Landmines and UXO Classification Using MC Dropout

## Quick Facts
- arXiv ID: 2510.06238
- Source URL: https://arxiv.org/abs/2510.06238
- Reference count: 15
- Primary result: MC Dropout integrated into ResNet-50 quantifies epistemic uncertainty, enabling detection of unreliable predictions under adversarial attacks and noisy conditions for landmine/UXO classification.

## Executive Summary
This study introduces uncertainty quantification using Monte Carlo (MC) Dropout into a fine-tuned ResNet-50 model for surface landmine and UXO classification, tested on a simulated dataset. The approach quantifies epistemic uncertainty during inference, enabling detection of unreliable predictions under adversarial and noisy conditions. On clean test images, the model correctly classified all inputs while exhibiting varying uncertainty levels, with low uncertainty for clear samples and higher uncertainty for ambiguous ones. Under adversarial attacks (PGD and FGSM), misclassifications were accompanied by significantly increased uncertainty, indicating unreliable predictions. Similarly, noisy training samples showed higher uncertainty despite correct classification. The results demonstrate that uncertainty quantification can serve as a supplementary metric for reliable decision-making in demining operations, highlighting the vulnerability of deterministic models to adversarial threats and emphasizing the need for more robust, uncertainty-aware approaches in practical applications.

## Method Summary
The authors fine-tuned a ResNet-50 backbone pre-trained on ImageNet-1K for landmine and UXO classification by modifying the final fully connected layer to output four classes (grenade, landmine, projectile, rocket). They unfroze only blocks 3 and 4 of the network while freezing earlier layers, and added a single dropout layer with rate 0.1 before the final classification layer. For uncertainty quantification, they enabled dropout during inference and ran 100 forward passes per input, computing the mean prediction and variance across passes as the uncertainty metric. The model was trained on a simulated dataset of 1,487 surface landmine and UXO images for 100 epochs with batch size 16 and learning rate 0.0001, achieving 99% validation accuracy. Adversarial attacks (PGD and FGSM) and noisy inputs were used to evaluate uncertainty behavior under challenging conditions.

## Key Results
- The model achieved 99% validation accuracy and correctly classified all clean test images (248 samples)
- Under PGD attack (ε=0.03), uncertainty increased from 0.222 to 5.614 while accuracy dropped to 40%
- Under FGSM attack (ε=0.05), uncertainty increased from 0.222 to 0.555 while accuracy dropped to 52%
- Noisy training samples showed elevated uncertainty (0.839) even when correctly classified

## Why This Works (Mechanism)

### Mechanism 1
Enabling dropout during inference and running multiple stochastic forward passes approximates Bayesian inference for epistemic uncertainty estimation. Each forward pass samples different dropout masks (Bernoulli-distributed neuron retention), creating an ensemble of predictions. The variance across N=100 passes serves as a proxy for model uncertainty—high variance indicates the model's weights are inconsistent for that input, signaling unreliable predictions. Core assumption: MC Dropout provides a valid variational approximation to a Bayesian neural network posterior in this image classification domain. Evidence anchors: [abstract]: "Integrating the MC Dropout approach helps quantify epistemic uncertainty, providing an additional metric for prediction reliability"; [section II.A]: Equations 1-4 formalize dropout masks as Bernoulli sampling, with variance across passes as the uncertainty estimate; cites Gal & Ghahramani [7] for theoretical grounding. Break condition: If the number of forward passes is insufficient (<20-30 typically yields noisy variance estimates), uncertainty calibration fails.

### Mechanism 2
Fine-tuning only later ResNet-50 blocks (3 and 4) with a single low dropout rate (0.1) achieves high validation accuracy while maintaining stable learning dynamics. Transfer learning preserves ImageNet low-level features (edges, textures) in frozen early blocks while adapting high-level semantic features in later blocks. A single dropout layer at 0.1 provides regularization without over-constraining the network's capacity to learn domain-specific patterns. Core assumption: ImageNet pre-trained features transfer meaningfully to simulated landmine imagery despite domain shift. Evidence anchors: [section II.B, Table I]: Configuration with blocks 3-4 unfrozen, dropout 0.1 achieved 99% validation accuracy with stable loss (0.01), outperforming other configurations. Break condition: If too many blocks are frozen, the model cannot adapt to domain-specific features; if too few are frozen, overfitting occurs (Table I shows validation loss spike to 0.86 with high dropout and no unfrozen blocks).

### Mechanism 3
Adversarial perturbations and noisy inputs cause elevated predictive uncertainty, enabling detection of unreliable predictions even when the model misclassifies. Out-of-distribution inputs (adversarial noise, sensor artifacts) fall outside the training data manifold, causing stochastic forward passes to produce inconsistent outputs. This manifests as increased variance—clean images yielded uncertainty ~0.15-0.53, while PGD attacks drove uncertainty to 2.45-5.61. Core assumption: Adversarial and noisy perturbations push inputs sufficiently outside the training distribution for MC Dropout to detect via variance increases. Evidence anchors: [abstract]: "under adversarial attacks (PGD and FGSM), misclassifications were accompanied by significantly increased uncertainty, indicating unreliable predictions"; [section III.C, Fig. 4]: PGD attack (eps=0.03) increased uncertainty from 0.222 (clean) to 5.614; FGSM (eps=0.05) increased to 0.555. Break condition: If adversarial perturbations are subtle enough to remain within the learned distribution, uncertainty may not elevate.

## Foundational Learning

- **Bayesian Neural Networks and Variational Inference**
  - Why needed here: MC Dropout is not a heuristic—it's theoretically grounded as variational inference. Understanding this helps debug why uncertainty estimates might fail (e.g., poor posterior approximation).
  - Quick check question: Can you explain why dropout at inference time approximates sampling from a posterior distribution over weights, rather than just adding noise?

- **Epistemic vs. Aleatoric Uncertainty**
  - Why needed here: The paper quantifies only epistemic uncertainty (model uncertainty from limited data/knowledge). Aleatoric uncertainty (inherent data noise) is not captured, which limits the approach for real-world sensor noise.
  - Quick check question: If you have a perfect model but noisy sensor data, which type of uncertainty would MC Dropout capture?

- **Adversarial Attack Taxonomy (FGSM vs. PGD)**
  - Why needed here: The paper tests both FGSM (single-step gradient-based) and PGD (iterative multi-step) attacks. Understanding their differences explains why PGD produced higher uncertainty (5.61) than FGSM (0.56) at comparable perturbation magnitudes.
  - Quick check question: Why would an iterative attack like PGD typically produce more severe model confusion than a single-step attack like FGSM?

## Architecture Onboarding

- **Component map**: ResNet-50 (ImageNet pre-trained) -> Frozen blocks 1-2 -> Unfrozen blocks 3-4 + dropout(0.1) -> Final FC layer (4 classes) -> MC Dropout wrapper (100 forward passes)

- **Critical path**:
  1. Load pre-trained ResNet-50, replace final FC layer with 4-class output
  2. Insert dropout layer (p=0.1) before FC, freeze blocks 1-2
  3. Train for 100 epochs, batch size 16, LR=0.0001 (~25 min on RTX A4000)
  4. At inference: override `model.eval()` to keep dropout enabled
  5. For each input: run 100 forward passes, compute prediction mean and variance
  6. Flag predictions with variance above domain-specific threshold for human review

- **Design tradeoffs**:
  - **More unfrozen blocks**: Better domain adaptation but higher overfitting risk and compute cost
  - **Higher dropout rate**: More stable uncertainty estimates but potential underfitting (Table I shows val loss spike at 0.6)
  - **More forward passes**: Better uncertainty calibration but linear increase in inference latency
  - **Dataset choice**: Simulated data enables proof-of-concept; real-world deployment requires threshold recalibration

- **Failure signatures**:
  - **Validation loss >> training loss with high dropout**: Overfitting blocked by excessive regularization—reduce dropout or unfreeze more blocks
  - **Low uncertainty on misclassifications**: Model is overconfident; may need calibration, deeper dropout placement, or ensemble methods
  - **High uncertainty on all clean inputs**: Underfitting; reduce dropout, increase model capacity, or extend training
  - **Inconsistent uncertainty across runs**: Increase N (forward passes) or fix random seeds for reproducibility

- **First 3 experiments**:
  1. **Baseline calibration**: Train the modified ResNet-50 on clean data, measure prediction accuracy and uncertainty distribution across all 4 classes on the held-out test set (248 images). Establish typical uncertainty ranges for correct vs. borderline predictions.
  2. **Controlled adversarial sweep**: Apply FGSM and PGD attacks across epsilon values [0.001, 0.01, 0.02, 0.03, 0.05], plotting accuracy degradation vs. uncertainty increase. Identify epsilon thresholds where uncertainty reliably signals attack presence.
  3. **Noise robustness test**: Inject Gaussian noise at varying SNR levels into test images, measure uncertainty response. Compare against adversarial results to determine if uncertainty patterns differ between structured (adversarial) and unstructured (random) perturbations.

## Open Questions the Paper Calls Out

- **Can uncertainty quantification via MC Dropout be effectively integrated into state-of-the-art object detection architectures (e.g., DETR, YOLO, Faster R-CNN) for landmine detection?**
  - Basis in paper: [explicit] The authors explicitly state in the Discussion that "Future research could extend this and other uncertainty estimation approaches to state-of-the-art object detection models... applied to real-world surface landmine datasets."
  - Why unresolved: The current study is restricted to a classification pipeline using a modified ResNet-50, whereas practical demining requires localization capabilities found in detection models.
  - What evidence would resolve it: Successful implementation of MC Dropout in detection frameworks, demonstrating reliable bounding box prediction and uncertainty estimation on standard landmine datasets.

- **How does the proposed method perform on real-world landmine datasets compared to the simulated data used in this study?**
  - Basis in paper: [explicit] The authors acknowledge the use of a simulated dataset due to security concerns and note that "Further validation... using real-world datasets could support the establishment of uncertainty thresholds."
  - Why unresolved: Real-world environments introduce complex variability (e.g., diverse terrain, lighting, sensor noise) that simulated datasets may not fully capture, potentially affecting uncertainty calibration.
  - What evidence would resolve it: Benchmarking the model's uncertainty scores on field-acquired data containing verified surface landmines and UXOs.

- **How does predictive variance trend relative to different types of adversarial attacks and noise levels beyond the specific FGSM and PGD configurations tested?**
  - Basis in paper: [explicit] The authors state, "we also recognize that our approach may not be universally effective against all types of adversarial attacks... studying how variance trends with respect to different adversarial noise levels is an equally important aspect."
  - Why unresolved: The paper only tests two attack methods (FGSM, PGD) and specific noise levels; it is unknown if other perturbations would yield the same increase in uncertainty.
  - What evidence would resolve it: Evaluation against a broader suite of adversarial attacks (e.g., C&W, DeepFool) and analysis of the correlation between perturbation strength and variance.

## Limitations
- Simulated dataset basis may not capture real-world sensor noise and environmental variability
- Limited to classification; does not address object localization needed for practical demining
- MC Dropout may underfit or be computationally demanding in some contexts

## Confidence
- **High confidence**: Controlled experimental conditions (simulated data, clean test set, deterministic attack models) showing clear uncertainty-elevation patterns
- **Medium confidence**: Real-world applicability due to simulated dataset limitation—transfer to actual sensor data untested
- **Medium confidence**: Assumption that MC Dropout provides reliable uncertainty quantification across all attack types and noise patterns

## Next Checks
1. **Real-world data validation**: Test the uncertainty quantification approach on actual landmine/UFO sensor data or established real-world demining datasets to verify pattern consistency.
2. **Multi-attack robustness analysis**: Systematically test across diverse adversarial attack families (white-box, black-box, physical-world attacks) and structured noise patterns to identify uncertainty failure modes.
3. **Uncertainty calibration under domain shift**: Evaluate how well uncertainty estimates generalize when training and test distributions differ (e.g., different soil types, lighting conditions, or sensor configurations).