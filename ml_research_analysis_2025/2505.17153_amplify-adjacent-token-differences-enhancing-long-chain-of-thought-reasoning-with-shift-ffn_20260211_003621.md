---
ver: rpa2
title: 'Amplify Adjacent Token Differences: Enhancing Long Chain-of-Thought Reasoning
  with Shift-FFN'
arxiv_id: '2505.17153'
source_url: https://arxiv.org/abs/2505.17153
tags:
- lora
- reasoning
- shift-ffn
- arxiv
- full
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper identifies a Cyclical Reasoning issue in fine-tuning
  large language models on long Chain-of-Thought (CoT) data, where models repeatedly
  reiterate previous inference steps until reaching maximum length limits. Analysis
  reveals that smaller representation differences between adjacent tokens correlate
  with higher Cyclical Reasoning rates.
---

# Amplify Adjacent Token Differences: Enhancing Long Chain-of-Thought Reasoning with Shift-FFN

## Quick Facts
- arXiv ID: 2505.17153
- Source URL: https://arxiv.org/abs/2505.17153
- Reference count: 40
- Primary result: LoRA + Shift-FFN achieves 51.2% average accuracy on math benchmarks, surpassing full fine-tuning baseline

## Executive Summary
This paper addresses Cyclical Reasoning in long Chain-of-Thought (CoT) fine-tuning, where models repeatedly reiterate inference steps until reaching length limits. The authors identify that smaller representation differences between adjacent tokens correlate with higher cyclical reasoning rates. They propose Shift Feedforward Networks (Shift-FFN), which amplifies these differences using an Editor module before the FFN layer. Experimental results show that LoRA combined with Shift-FFN achieves higher accuracy and lower cyclical reasoning rates compared to standard approaches across mathematical reasoning tasks.

## Method Summary
The Shift-FFN architecture introduces an Editor module before each FFN layer that dynamically amplifies representation differences between adjacent tokens. The Editor computes a modification vector using both current and preceding token representations, which is added to the current token before FFN processing. The paper uses LoRA (rank 256) combined with Shift-FFN on Qwen2.5-7B-Instruct, training on long CoT math data (OpenThoughts subset) filtered to ≤16k tokens. Key components include the gating mechanism using both tokens, zero-initialized Wc for training stability, and application across all transformer layers.

## Key Results
- LoRA (rank 256) + Shift-FFN achieves 51.2% average accuracy on math benchmarks, surpassing full fine-tuning baseline
- Reduces length-exceeded outputs from 15.0% to 12.7% compared to standard LoRA
- M(X) metric (relative change between adjacent tokens) shows negative correlation with Length Exceeded Percentage across datasets
- Ablation studies confirm the necessity of the gate mechanism and proper initialization

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Amplifying representation differences between adjacent tokens reduces cyclical reasoning in long CoT fine-tuning
- Mechanism: The Editor module computes a modification vector using both current and preceding token representations, then adds this to the current token before FFN processing, increasing the relative change metric M(X)
- Core assumption: Larger inter-token representation differences correlate with forward reasoning progression rather than repetitive loops
- Evidence anchors:
  - Smaller differences in representations between adjacent tokens correlates with higher tendency toward Cyclical Reasoning
  - Exceed samples exhibit lower mean relative change than Normal samples across both LoRA and full fine-tuned models
- Break condition: If increased M(X) doesn't correlate with reduced Length Exceeded Percentage across different architectures or task domains

### Mechanism 2
- Claim: Shift-FFN extends FFN's single-key retrieval mechanism to multi-key retrieval, analogous to multi-query attention
- Mechanism: The FFN layer functions as key-value memory; Shift-FFN modifies retrieval weights by incorporating contributions from the previous token's transformed representation
- Core assumption: FFN layers operate as unnormalized key-value memories (based on prior work by Geva et al.)
- Evidence anchors:
  - Mathematical derivation shows weight coefficient becomes w_{i,j} = σ(x_i^T k_j) + σ(x̂_{i-1}^T k_j)
  - This process is also similar to applying Multi-Query Attention in the FFN
- Break condition: If the key-value memory interpretation of FFN doesn't hold for the specific model architectures tested

### Mechanism 3
- Claim: Shift-FFN introduces a correction term to subsequent attention scores based on adjacent token information
- Mechanism: Modified FFN outputs propagate through residual connections, altering attention computations in the next layer by adding correction terms dependent on (i-1)th and (j-1)th tokens
- Core assumption: The attention score modification meaningfully influences reasoning trajectories at a scale detectable in task performance
- Evidence anchors:
  - Derives α_{i,j} = α'_{i,j} + A_i[σ(W_up W_s x_{j-1})] + A_j[σ(W_up W_s x_{i-1})]
  - Negative correlation between M(X) and Length Exceeded Percentage across datasets
- Break condition: If attention correction effects are negligible relative to other architectural factors

## Foundational Learning

- Concept: **LoRA Rank and Expressiveness**
  - Why needed here: The paper shows long CoT learning requires higher LoRA rank (256 vs. typical 32); understanding this distinction is essential for interpreting the gap between LoRA and full fine-tuning
  - Quick check question: Why might a task requiring 16k+ token reasoning chains need higher-rank adaptation than commonsense reasoning tasks?

- Concept: **Chain-of-Embedding Analysis (M(X) Metric)**
  - Why needed here: The paper diagnoses cyclical reasoning by measuring relative changes in hidden states across tokens and layers; this metric underpins the entire motivation
  - Quick check question: Given hidden states x^l_{i-1} and x^l_i at layer l, how would you compute their relative change?

- Concept: **Cyclical Reasoning vs. Length Exceeded**
  - Why needed here: The paper uses "Length Exceeded Percentage" as a proxy for cyclical reasoning; understanding this approximation's boundaries prevents misinterpretation
  - Quick check question: What percentage of length-exceeded samples show exact textual repetition according to the paper, and what other cyclical patterns might exist?

## Architecture Onboarding

- Component map:
  * Attention outputs x_i and cached x_{i-1}
  * Concatenation: [x_{i-1}; x_i] → W_b → ReLU
  * Gate computation: ⊙ (W_a·x_{i-1})
  * Projection: W_c produces modification vector
  * Addition to x_i before FFN input
  * Standard FFN and residual flow continues

- Critical path:
  1. Attention outputs x_i and cached x_{i-1}
  2. Concatenation: [x_{i-1}; x_i] → W_b → ReLU
  3. Gate computation: ⊙ (W_a·x_{i-1})
  4. Projection: W_c produces modification vector
  5. Addition to x_i before FFN input
  6. Standard FFN and residual flow continues

- Design tradeoffs:
  * Gate mechanism (using both tokens) is critical—ablation shows removal drops performance below standard LoRA
  * MLP transformation of x_{i-1} is necessary; linear combination alone provides no benefit
  * Zero initialization of W_c ensures training stability but slows early learning

- Failure signatures:
  * Performance equal to or worse than standard LoRA → likely gate ablated or W_c not zero-initialized
  * Training instability → check W_c initialization
  * No improvement in Length Exceeded Percentage → Editor may not be learning meaningful transformations; verify gradient flow

- First 3 experiments:
  1. Reproduce M(X) vs. Length Exceeded correlation on your base model to validate the foundational observation before implementing Shift-FFN
  2. Ablate the gate mechanism (use only x_{i-1} in gate) to confirm the paper's finding that both tokens are necessary
  3. Test on non-mathematical long CoT tasks (e.g., extended reasoning in code or legal analysis) to assess domain generalization—paper only validates on math benchmarks

## Open Questions the Paper Calls Out

- **Question**: Does the Shift-FFN architecture maintain its effectiveness when applied to significantly larger models (e.g., 32B+ parameters) and larger training datasets (e.g., 1M samples)?
  - Basis in paper: The authors explicitly state in Appendix A that due to resource constraints, they could not test on larger datasets or models, leaving the scalability of Shift-FFN as an open question
  - Why unresolved: The paper restricts experiments to models up to 8B parameters and training samples up to 80k; it is unknown if the "Cyclical Reasoning" issue or the solution behaves differently at larger scales
  - What evidence would resolve it: Reproducing the fine-tuning experiments using Shift-FFN on a 32B or 70B parameter model trained on a 1M sample dataset

- **Question**: What are the underlying theoretical or mechanistic causes that link smaller representation differences between adjacent tokens to the emergence of Cyclical Reasoning?
  - Basis in paper: The authors note in Appendix A that while they observed the correlation between small adjacent token differences and cyclical reasoning, they did not conduct further analysis into the deeper reasons behind this phenomenon
  - Why unresolved: The paper establishes a correlation but lacks an explanation for why low "drift" in hidden states forces the model into repetitive loops rather than other failure modes
  - What evidence would resolve it: A causal analysis or theoretical framework explaining how gradient descent on long sequences minimizes token variance, leading to degenerate solutions

- **Question**: Can Shift-FFN effectively mitigate Cyclical Reasoning in long Chain-of-Thought tasks outside of mathematical reasoning, such as code generation or logical inference?
  - Basis in paper: The experimental evaluation (Section 4) is restricted exclusively to mathematical benchmarks (AIME24, AMC23, MATH500, OlympiadBench)
  - Why unresolved: Mathematical reasoning relies on specific symbolic structures; it is unclear if the representation dynamics leading to Cyclical Reasoning are universal to all Long CoT tasks or specific to math
  - What evidence would resolve it: Experimental results applying Shift-FFN to long CoT datasets for coding (e.g., HumanEval) or planning tasks

## Limitations
- The Length Exceeded Percentage is an imperfect proxy for cyclical reasoning, with only 7.4% of length-exceeded samples showing exact textual repetition
- The key-value memory interpretation of FFN layers remains theoretical and not directly validated within this paper
- The approach is only validated on mathematical reasoning tasks, leaving domain generalization uncertain

## Confidence
- **High Confidence**: Empirical results showing LoRA+Shift-FFN outperforms both standard LoRA and full fine-tuning across multiple mathematical reasoning benchmarks
- **Medium Confidence**: Diagnostic analysis using M(X) to identify cyclical reasoning patterns shows consistent correlations, but causal mechanism remains inferential
- **Low Confidence**: Theoretical interpretation of FFN layers as key-value memories and subsequent attention modifications relies on external theoretical work not validated within this paper

## Next Checks
1. Design an experiment where M(X) is artificially manipulated during inference while keeping model weights fixed to provide stronger causal evidence for the proposed mechanism
2. Evaluate Shift-FFN on long CoT tasks outside mathematical reasoning, such as extended code generation, legal document analysis, or scientific reasoning
3. Implement more sophisticated cyclical reasoning detection beyond exact textual repetition using sequence alignment or embedding-based similarity measures