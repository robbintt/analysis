---
ver: rpa2
title: Boosting Maximum Entropy Reinforcement Learning via One-Step Flow Matching
arxiv_id: '2602.01606'
source_url: https://arxiv.org/abs/2602.01606
tags:
- policy
- flow
- entropy
- learning
- matching
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes FLAME, a novel framework that integrates flow
  matching with maximum entropy reinforcement learning to achieve expressive, one-step
  action generation. The key challenges addressed are the intractability of the energy-based
  target policy and the bias in log-likelihood estimation for entropy regularization.
---

# Boosting Maximum Entropy Reinforcement Learning via One-Step Flow Matching

## Quick Facts
- arXiv ID: 2602.01606
- Source URL: https://arxiv.org/abs/2602.01606
- Reference count: 40
- Achieves 11945.08±461.47 on HalfCheetah-v5, surpassing SAC and SDAC while maintaining 8× faster inference than diffusion methods

## Executive Summary
This paper proposes FLAME, a novel framework that integrates flow matching with maximum entropy reinforcement learning to achieve expressive, one-step action generation. The key challenges addressed are the intractability of the energy-based target policy and the bias in log-likelihood estimation for entropy regularization. FLAME introduces a Q-Reweighted Flow Matching objective that bypasses partition function estimation via importance reweighting, and a decoupled entropy estimator that corrects discretization bias while maintaining efficient one-step inference. By integrating the MeanFlow formulation, FLAME achieves high-fidelity control with NFE=1.

## Method Summary
FLAME combines flow matching with maximum entropy reinforcement learning through a Q-Reweighted Flow Matching objective that avoids partition function estimation and a decoupled entropy estimator that corrects discretization bias. The framework leverages the MeanFlow formulation to achieve one-step inference while maintaining high policy expressivity. The approach integrates entropy regularization with flow matching by treating the target policy as an energy-based model and using importance reweighting to handle intractable partition functions.

## Key Results
- FLAME-R achieves 11945.08±461.47 on HalfCheetah-v5, surpassing SAC (11379.74±474.51) and SDAC (11165.70±164.25)
- Maintains 8× faster inference than diffusion methods while matching multi-step diffusion policies
- Demonstrates superior performance to Gaussian baselines on MuJoCo benchmarks

## Why This Works (Mechanism)
The framework addresses the fundamental challenge of combining flow matching with entropy regularization by treating the target policy as an energy-based model. The Q-Reweighted Flow Matching objective enables training without partition function estimation through importance reweighting, while the decoupled entropy estimator corrects discretization bias. The MeanFlow formulation allows for one-step inference, significantly reducing computational overhead compared to multi-step diffusion methods.

## Foundational Learning
- **Flow Matching**: A generative modeling technique that learns to transform noise into samples through a continuous path; needed for expressive action generation in RL
- **Maximum Entropy RL**: Reinforcement learning that maximizes both expected return and policy entropy; needed for exploration and robustness
- **Q-Reweighted Flow Matching**: A variant that avoids partition function estimation through importance reweighting; needed to make the training objective tractable
- **Decoupled Entropy Estimator**: A method to correct discretization bias in entropy estimation; needed for accurate entropy regularization
- **MeanFlow Formulation**: A continuous normalizing flow formulation; needed for efficient one-step inference

## Architecture Onboarding
**Component Map**: Environment -> Policy Network -> Flow Matching Layer -> Action Output -> Q-function Estimation -> Loss Computation
**Critical Path**: State observation → Policy Network → Flow Matching transformation → Action sampling → Environment step → Q-value update
**Design Tradeoffs**: One-step inference vs. multi-step diffusion (speed vs. expressiveness), invertible flows vs. partition function estimation (tractability vs. flexibility)
**Failure Signatures**: Poor performance on multimodal action distributions, sensitivity to discretization levels, degraded performance with high-dimensional actions
**3 First Experiments**: 1) Test on simpler continuous control tasks (Pendulum, MountainCar) to validate basic functionality, 2) Ablation study removing entropy regularization to isolate its contribution, 3) Compare against standard SAC with Gaussian policies on same benchmarks

## Open Questions the Paper Calls Out
None

## Limitations
- Theoretical analysis lacks rigorous proofs for convergence guarantees and sample complexity bounds
- Assumes invertible normalizing flows which may be restrictive for complex multimodal action distributions
- Primarily validated on MuJoCo benchmarks, leaving scalability to more challenging domains open

## Confidence
**High**: Empirical results on MuJoCo benchmarks are reproducible; computational efficiency claims (8× faster inference) are well-supported; architectural choices are technically sound
**Medium**: Theoretical claims regarding bias correction need additional empirical validation; generalization beyond tested environments is reasonable but unproven
**Low**: Scalability analysis to high-dimensional action spaces is not thoroughly investigated; impact of invertible flow assumption on multimodal scenarios remains unclear

## Next Checks
1. Test FLAME on more complex control tasks with multimodal action distributions to evaluate the expressiveness limits of the invertible flow assumption
2. Conduct ablation studies on the decoupled entropy estimator to quantify its contribution to performance across varying discretization levels
3. Perform scalability analysis on high-dimensional action spaces (e.g., humanoid with 17+ joints) to identify computational bottlenecks and verify the claimed inference efficiency benefits