---
ver: rpa2
title: Latent Graph Learning in Generative Models of Neural Signals
arxiv_id: '2508.16776'
source_url: https://arxiv.org/abs/2508.16776
tags:
- neural
- graph
- connectivity
- neuron
- network
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of extracting interpretable
  latent graph representations from neural signals using foundation models. The authors
  develop a systematic framework to extract directed connectivity from Neuroformer's
  attention mechanisms and compare it against a GLM baseline on a simulated hub network
  with known ground-truth connectivity.
---

# Latent Graph Learning in Generative Models of Neural Signals

## Quick Facts
- **arXiv ID**: 2508.16776
- **Source URL**: https://arxiv.org/abs/2508.16776
- **Reference count**: 26
- **Primary result**: Transformer attention mechanisms effectively capture higher-order shared-input structure in neural networks, with strong co-input graph alignment (r²=0.67) despite weak direct edge reconstruction.

## Executive Summary
This paper develops a framework to extract interpretable latent graph representations from neural signals using foundation models, specifically addressing the challenge of recovering directed connectivity from Neuroformer's attention mechanisms. The authors systematically compare attention-derived connectivity against a GLM baseline on a simulated hub network with known ground-truth connectivity. While direct edge-level reconstructions show modest alignment (GLM r²=0.262, Neuroformer r²=0.110), the co-input graph representations demonstrate strong alignment (Neuroformer r²=0.670), suggesting foundation models internalize collective input-output patterns rather than direct synaptic connections. This work bridges predictive performance with biophysical interpretability by revealing how transformers capture network organization through attention mechanisms.

## Method Summary
The method extracts directed connectivity from Neuroformer's self-attention mechanisms by aggregating attention weights across layers, heads, and sequences using an incidence matrix, then normalizing to obtain neuron-level connectivity matrices. For comparison, a GLM baseline with raised-cosine basis expansion extracts signed coupling weights as the peak of impulse response functions. Both methods are evaluated on simulated LIF hub networks (N=300 neurons, 3 hub neurons with strong outbound connections) using Pearson/Spearman correlations on edge weights and co-input representations B=AAᵀ, plus spectral graph divergence. The framework tokenizes spike trains into (neuron_id, interspike_interval) pairs, embeds them with neuron and Fourier temporal embeddings, and processes through a causal transformer with separate output heads for next-neuron and next-interval prediction.

## Key Results
- Direct edge weight alignment is modest: GLM achieves r²=0.262 while Neuroformer achieves r²=0.110 on Pearson correlation
- Co-input graph representations show strong alignment: Neuroformer achieves r²=0.670 on Spearman correlation
- GLM provides signed, interpretable weights while Neuroformer attention is unsigned
- Spectral graph divergence with shift/scale invariance confirms topological differences
- Attention captures higher-order structural regularities rather than individual synaptic weights

## Why This Works (Mechanism)

### Mechanism 1: Co-input Graph Alignment via Attention
- **Claim**: Transformer attention patterns internalize higher-order shared-input structure more effectively than direct synaptic weights
- **Mechanism**: Aggregated attention weights are projected into co-input representation space using B = AAᵀ, quantifying shared presynaptic inputs between neuron pairs
- **Core assumption**: Attention heads learn to attend to neurons with correlated activity patterns reflecting common input sources
- **Evidence**: Strong co-input alignment (r²=0.670) despite weak edge alignment, demonstrating statistical dependency capture

### Mechanism 2: Attention as Soft Adjacency with Causal Masking
- **Claim**: Causal attention matrices serve as soft adjacency matrices over complete neural token graphs
- **Mechanism**: Self-attention with causal mask A(k,h) = softmax(QKᵀ/√dₕ + M) computes directed token-pair weights
- **Core assumption**: Next-token prediction requires learning directed temporal dependencies reflecting functional influence
- **Evidence**: Temporal directionality enforced by Mₖⱼ = -∞ for j ≥ k, yielding zero softmax output for future tokens

### Mechanism 3: Foundation Model Latent Structure
- **Claim**: Pre-trained neural foundation models encode generalizable latent graph structure extractable as downstream task
- **Mechanism**: Large-scale pre-training on heterogeneous neural data forces embeddings to organize around shared dynamics
- **Core assumption**: Underlying network structure is a learnable regularity across sessions/animals/modalities
- **Evidence**: Foundation models show promise in cell type and brain region identification using POYO+

## Foundational Learning

**Concept: Co-input Graph (Bibliographic Coupling)**
- **Why needed here**: This is the paper's central positive result—alignment is strong in co-input space (B = AAᵀ), weak in direct edge space
- **Quick check**: For connectivity matrix A, what does entry Bᵢⱼ of AAᵀ represent? (Common input strength to neurons i and j)

**Concept: Causal Self-Attention**
- **Why needed here**: Directed graph extraction depends entirely on causal mask enforcing past→present information flow
- **Quick check**: How does attention mask prevent position t from attending to position t+1? (Sets attention score to -∞, yielding zero softmax output)

**Concept: Leaky Integrate-and-Fire (LIF) Neurons**
- **Why needed here**: Ground-truth data comes from simulated LIF networks; understanding LIF dynamics is essential to interpret "true" connectivity
- **Quick check**: What happens to LIF neuron's membrane potential immediately after spike emission? (Resets to Vreset)

## Architecture Onboarding

**Component map**: Spike train → Tokenization → Embedding → Transformer → Attention aggregation → Co-input projection B = CCᵀ → Ground-truth comparison

**Critical path**: Spike train → (neuron_id, Δt) tokenization → zₖ = eₙₖ + u(Δtₖ) embedding → L causal transformer blocks → attention aggregation C = Nₐ ⊘ Nᵧ → co-input B = CCᵀ → correlation/spectral divergence evaluation

**Design tradeoffs**: Tokenization preserves spike timing precision but increases sequence length vs. binning; averaging attention across layers/heads may obscure layer-specific functional roles; transformer captures context while GLM provides clearer sign-interpretable weights

**Failure signatures**: Low co-input r² (<0.3) indicates model failed to capture shared-input structure; dense/noisy C matrix suggests SNR too low for edge-level inference; high spectral divergence indicates fundamental graph-structure mismatch

**First 3 experiments**:
1. Reproduce Table 1: Train Neuroformer on LIF data, extract C, compute edge and co-input correlations vs. ground truth
2. Layer-wise analysis: Evaluate connectivity from each layer separately to test whether earlier layers capture direct edges and later layers capture higher-order structure
3. Hub strength ablation: Vary hub neuron outbound weights in simulation and measure sensitivity of alignment metrics

## Open Questions the Paper Calls Out

**Open Question 1**: How can graph-based geometric constraints be explicitly incorporated into pretraining or architecture of foundation models to improve recovery of biophysical connectivity?
- **Basis**: Abstract concludes findings "motivate paths towards incorporating graph-based geometric constraints"
- **Unresolved**: Current work extracts representations post-hoc from standard transformer without testing architectural solutions
- **Evidence needed**: Modified Neuroformer with geometric loss showing significantly higher Pearson r² on direct edge-weight reconstruction

**Open Question 2**: Do strong higher-order co-input alignments observed in simulated LIF networks generalize to diverse, large-scale biological neural recordings?
- **Basis**: Discussion states "scaling these approaches will require... evaluating across diverse neural modalities"
- **Unresolved**: Results rely on simulated network of N=300 neurons lacking noise and complexity of in-vivo data
- **Evidence needed**: Application to multi-region biological datasets demonstrating retention of high co-input graph alignment (r² > 0.5)

**Open Question 3**: Can attention-based extraction methods be refined to disambiguate direct causal synaptic connections from correlations induced by common drive?
- **Basis**: High co-input alignment (r²=0.670) but low direct connectivity alignment (r²=0.110) suggests model conflates direct edges with shared inputs
- **Unresolved**: Framework does not currently distinguish direct connection A→B from shared input C→{A,B}
- **Evidence needed**: Evaluation on datasets where common drive is decorrelated from direct connectivity, showing extracted attention tracks direct paths

## Limitations
- Results based on single simulated hub network configuration may not generalize to different network topologies
- Key Neuroformer hyperparameters and exact implementation details are not explicitly specified
- Relationship between attention-derived co-input strength and actual shared presynaptic drive in biological networks remains qualitative
- Modest edge-level correlations may reflect genuine limitations rather than insufficient data

## Confidence

**High Confidence**: Co-input graph representations show substantially stronger alignment (r²=0.67) than direct edge weights (r²=0.11), consistent with mathematical relationship B=AAᵀ and well-established causal masking behavior

**Medium Confidence**: Interpretation that foundation models internalize collective input-output patterns follows logically from data but requires validation on diverse network topologies and real neural recordings

**Low Confidence**: Specific numerical values of edge-level correlations may be sensitive to implementation details, random initialization, and particular hub network configuration

## Next Checks
1. **Layer-wise Attention Analysis**: Extract connectivity from individual transformer layers to determine whether earlier layers capture direct edge information while later layers specialize in higher-order co-input representations

2. **Topology Transferability**: Repeat analysis on alternative network architectures (Erdős-Rényi random graphs, scale-free networks, or spatially embedded connectomes) to assess co-input alignment robustness across different ground-truth structures

3. **Real Data Validation**: Apply framework to multi-electrode array recordings from cultured neural networks where partial connectivity information can be obtained through staining or paired recordings, validating whether attention-derived co-input graphs align with empirical synaptic connectivity