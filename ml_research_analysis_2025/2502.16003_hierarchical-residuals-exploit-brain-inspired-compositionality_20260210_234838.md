---
ver: rpa2
title: Hierarchical Residuals Exploit Brain-Inspired Compositionality
arxiv_id: '2502.16003'
source_url: https://arxiv.org/abs/2502.16003
tags:
- hierarchical
- residual
- connections
- residuals
- conv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Hierarchical Residual Networks (HiResNets) introduce long-range
  residual connections between layers at different hierarchical levels in deep convolutional
  neural networks. Inspired by brain organization, HiResNets add skip connections
  from early layers to all subsequent blocks, enabling faster gradient flow and improved
  feature compositionality.
---

# Hierarchical Residuals Exploit Brain-Inspired Compositionality

## Quick Facts
- **arXiv ID**: 2502.16003
- **Source URL**: https://arxiv.org/abs/2502.16003
- **Authors**: Francisco M. López; Jochen Triesch
- **Reference count**: 13
- **Primary result**: HiResNets outperform ResNet-18, plain networks, and ResNeXt across CIFAR-10, CIFAR-100, and Tiny-ImageNet datasets

## Executive Summary
Hierarchical Residual Networks (HiResNets) introduce long-range residual connections between layers at different hierarchical levels in deep convolutional neural networks. Inspired by brain organization, HiResNets add skip connections from early layers to all subsequent blocks, enabling faster gradient flow and improved feature compositionality. The method uses 1x1 convolutions and pooling to compress feature maps across different hierarchical levels, allowing deeper layers to learn relative representations. Experiments show HiResNets outperform ResNet-18, plain networks, and ResNeXt across CIFAR-10, CIFAR-100, and Tiny-ImageNet datasets, achieving higher classification accuracy with fewer parameters.

## Method Summary
HiResNets extend ResNet's residual connections by adding hierarchical projections from early blocks to all subsequent blocks. Each block output combines the standard residual F(x) with compressed projections Pₖ from all previous blocks, where Pₖ applies average pooling (to match spatial dimensions) followed by 1×1 convolution (to match channels) and batch normalization. The full HiResNet connects every block to all subsequent blocks, while HiResNet-Out connects all blocks to the final block only. This architecture enables deeper layers to learn feature representations relative to compressed inputs from earlier hierarchy levels.

## Key Results
- HiResNets achieve 89.46% Top-1 accuracy on CIFAR-10 versus 89.15% for ResNet-18
- HiResNet-Out (simplified variant) matches full HiResNet performance with ~1% fewer parameters
- Skip connections from early layers to late blocks are identified as most beneficial
- HiResNets outperform plain networks and ResNeXt across all tested datasets

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Long-range skip connections provide faster gradient propagation to early layers during backpropagation.
- **Mechanism**: Hierarchical residuals create shortcuts from loss to each block via projection paths. Gradients bypass intermediate transformations through compressed projections (pooling + 1×1 conv + batch norm), reducing vanishing gradient effects in early layers.
- **Core assumption**: The projection compressions preserve sufficient signal for meaningful gradient updates.
- **Evidence anchors**: [abstract] "enabling faster gradient flow"; [section 3.2] "the compression provided by the HiResNet projections is useful to backpropagate the loss, as it provides shortcuts for each layer when computing the gradient descent"

### Mechanism 2
- **Claim**: Deeper layers learn feature representations *relative to* compressed inputs from earlier hierarchy levels, enabling hierarchical compositionality.
- **Mechanism**: Rather than learning unreferenced features, block outputs combine F(x) with multiple projected priors P₁, P₂, ..., Pₗ. The residual F learns increments over a compressed summary of earlier abstractions. This forces the network to compose features across hierarchy levels.
- **Core assumption**: The additive combination of projections provides a meaningful reference frame for residual learning.
- **Evidence anchors**: [abstract] "learning feature maps relative to the compressed representations provided by the skip connections"; [section 3.2] "the activations of the projections are approximately half of those of the residual functions" and "the model is exploiting combinations of the representations at the different levels"

### Mechanism 3
- **Claim**: Skip connections from early layers to late blocks (input-to-output shortcuts) are the most beneficial hierarchical residuals.
- **Mechanism**: HiResNet-Out (all blocks to final block) achieves near-full performance with fewer parameters. Long-range shortcuts enable the deepest layers to reference raw or lightly-processed input, improving final classification.
- **Core assumption**: Early-layer features contain task-relevant information that persists through depth.
- **Evidence anchors**: [section 3.3, Table 2] HiResNet-Out achieves 89.37% vs. full HiResNet 89.46% on CIFAR-10; individual P₁, P₂, P₃ ablations underperform; [section 3.3] "the residuals that connect the input to the output faster are most beneficial, likely because they enable better hierarchical compositionality"

## Foundational Learning

- **Concept**: **Residual Learning (ResNet)**
  - **Why needed here**: HiResNets extend ResNet's identity skip to hierarchical projections. Without understanding why F(x) + x works, the hierarchical generalization is opaque.
  - **Quick check question**: Can you explain why adding the input x to a learned residual F(x) mitigates the degradation problem in deep networks?

- **Concept**: **1×1 Convolutions for Channel Projection**
  - **Why needed here**: Hierarchical projections require matching channel dimensions across blocks with different feature counts. 1×1 convolutions perform this alignment cheaply.
  - **Quick check question**: Given input feature maps of shape (H, W, C_in), what output shape does a 1×1 convolution with C_out filters produce, and what computational cost does it have?

- **Concept**: **Feature Map Pooling (Spatial Compression)**
  - **Why needed here**: Early blocks have larger spatial dimensions than later blocks. Average pooling aligns spatial dimensions before projection addition.
  - **Quick check question**: If Block 1 outputs 32×32 feature maps and Block 3 outputs 8×8, what pooling stride is needed to project Block 1's features to Block 3's resolution?

## Architecture Onboarding

- **Component map**: Input → Conv3x3 → Block1 → Block2 → Block3 → GAP → FC
- **Critical path**: Input passes through first convolution (3×3, stride 1 or 2); each block ℓ receives: (a) standard input xₗ, (b) projected outputs from all previous blocks; projections are added to residual output F(xₗ); final block feeds global average pooling → fully connected classifier
- **Design tradeoffs**: Parameter overhead: Each Pₖ adds cₗ₋ₖ × cₗ parameters (small due to 1×1 conv). Full connectivity scales O(ℓ²) with block count. HiResNet-Out vs. Full HiResNet: Out achieves ~same accuracy with ~1% fewer parameters; use Out for deeper networks to control parameter growth. Projection strength: Paper uses identity-strength addition; Assumption: weighting projections (learnable scalar) could further tune contribution ratios.
- **Failure signatures**: Accuracy matches plain baseline: Projection dimensions mismatched (check pooling stride and conv channels) or projections not added correctly. Training diverges: BatchNorm missing from projection path (unnormalized features added to normalized residuals). Marginal gain over ResNet: Network too shallow (ResNet-18 is borderline; benefits clearer at depth ≥50)
- **First 3 experiments**: 1) Baseline comparison: Train ResNet-18 and HiResNet-18 on CIFAR-10 with identical hyperparameters (Adam, lr=0.01, halved every 20 epochs, batch=100, 80 epochs). Verify HiResNet achieves ~0.3-0.5% improvement. 2) Ablation on connection scope: Compare Full HiResNet vs. HiResNet-Out vs. HiResNet-P1 on CIFAR-10. Confirm Out ≈ Full > P1 > ResNet. 3) Activation ratio analysis: At epoch 40, compute mean |activation| for F(xₗ) vs. Σₖ Pₖ(xₗ₋ₖ) across validation set. Target ratio: projections ~0.5× residuals (per Fig. 2).

## Open Questions the Paper Calls Out
- **Open Question 1**: Do hierarchical residuals provide similar performance benefits in very deep networks (e.g., ResNet-101) compared to the shallower ResNet-18 tested? [explicit] The conclusion states, "Further experiments are required to determine whether hierarchical residuals are also beneficial in very deep networks, e.g. ResNet-101..." [Why unresolved]: The experiments were limited to 18-layer networks; degradation problems or optimization dynamics might differ in significantly deeper configurations. [What evidence would resolve it]: Training and evaluating HiResNet configurations on standard benchmarks (e.g., ImageNet) using ResNet-50, ResNet-101, or deeper backbones.

- **Open Question 2**: Can hierarchical residuals be effectively integrated into non-convolutional architectures like Vision Transformers? [explicit] The authors explicitly list "other classes of architectures, e.g. Vision Transformers" as a direction for further experiments in the conclusion. [Why unresolved]: The current method relies on specific mechanisms for CNNs (1x1 convolutions, pooling) to match dimensions, and it is unclear how these projections would function within the attention mechanisms or patch embeddings of Transformers. [What evidence would resolve it]: Adapting the hierarchical skip connection logic to a Vision Transformer architecture and comparing training convergence and accuracy against a baseline ViT.

- **Open Question 3**: Does the HiResNet architecture better reproduce hierarchical processing effects observed in biological brains, such as incongruences between low- and high-level features? [explicit] The conclusion notes, "it remains to be seen whether the biological inspiration of the HiResNet translates into this model better reproducing hierarchical processing effects of biological neural networks, e.g. incongruences between low- and high-level features." [Why unresolved]: While the architecture is neuro-inspired, the paper only evaluates artificial performance metrics (accuracy), not fidelity to biological processing constraints or errors. [What evidence would resolve it]: Direct comparison of HiResNet activations against neural data during tasks designed to trigger low-level/high-level feature incongruences.

- **Open Question 4**: Does increasing the complexity of the shortcut connection mechanisms (beyond simple pooling and 1x1 convolution) yield further performance gains? [explicit] The conclusion suggests that benefits might be realized "potentially by increasing the complexity of the shortcut connections." [Why unresolved]: The current implementation uses straightforward compressions; whether these are sufficient or optimal for feature compositionality in larger models is unverified. [What evidence would resolve it]: An ablation study replacing the simple 1x1/pooling projections with learnable sub-networks or attention-based projections to measure performance impact.

## Limitations
- The core claims about hierarchical compositionality are based on empirical observations rather than theoretical guarantees
- The connection between brain-inspired compositionality and the proposed architecture is asserted but not mechanistically detailed
- The projection compression mechanism could potentially discard critical gradient information in certain tasks
- The paper demonstrates architectural improvements without fully explaining the cognitive analogy

## Confidence
- **High confidence**: HiResNets improve classification accuracy over ResNet-18 and plain networks on standard benchmarks (CIFAR-10, CIFAR-100, Tiny-ImageNet)
- **Medium confidence**: The claim that long-range skip connections enable "hierarchical compositionality" by having deeper layers learn relative representations
- **Medium confidence**: The assertion that input-to-output shortcuts (HiResNet-Out) are most beneficial
- **Low confidence**: The brain-inspired compositionality analogy - the work demonstrates architectural benefits but does not establish concrete connections to brain organization

## Next Checks
1. **Gradient flow analysis**: Instrument the training process to measure gradient magnitudes at different layers for ResNet-18 vs. HiResNet-18. Verify that HiResNets indeed provide stronger gradient signals to early layers during backpropagation, particularly for the input-to-output skip connections.
2. **Projection sensitivity study**: Systematically vary the pooling parameters (kernel size, stride) and 1×1 convolution channel counts in the hierarchical projections. Measure how these architectural choices affect both accuracy and the relative contribution ratio between residuals and projections.
3. **Generalization to deeper networks**: Extend experiments to ResNet-50 and ResNet-101 architectures to determine if HiResNets provide proportionally larger benefits at greater depths, as suggested by the mechanism of mitigating vanishing gradients in early layers.