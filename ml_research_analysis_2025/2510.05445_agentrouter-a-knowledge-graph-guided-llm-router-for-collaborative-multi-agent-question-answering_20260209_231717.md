---
ver: rpa2
title: 'AgentRouter: A Knowledge-Graph-Guided LLM Router for Collaborative Multi-Agent
  Question Answering'
arxiv_id: '2510.05445'
source_url: https://arxiv.org/abs/2510.05445
tags:
- agent
- agents
- reasoning
- graph
- routing
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces AgentRouter, a knowledge-graph-guided routing
  framework that formulates multi-agent QA as a graph-supervised collaboration problem.
  It constructs a heterogeneous knowledge graph encoding queries, entities, and agents,
  then trains a type-aware GNN to learn routing distributions over agents from empirical
  performance signals.
---

# AgentRouter: A Knowledge-Graph-Guided LLM Router for Collaborative Multi-Agent Question Answering

## Quick Facts
- **arXiv ID:** 2510.05445
- **Source URL:** https://arxiv.org/abs/2510.05445
- **Reference count:** 32
- **Primary result:** Knowledge-graph-guided GNN routing consistently outperforms single-agent and ensemble baselines on four QA benchmarks, with top-K clipping improving multi-hop reasoning.

## Executive Summary
AgentRouter introduces a framework for multi-agent QA that formulates routing as a graph-supervised collaboration problem. By constructing a heterogeneous knowledge graph encoding queries, contextual entities, and agents, and training a type-aware GNN on empirical F1 performance, the system learns to predict routing distributions over agents. Experiments show consistent improvements over single-agent and ensemble baselines across multiple QA tasks, with particular gains in multi-hop reasoning when using top-K agent subsets.

## Method Summary
AgentRouter converts QA instances into heterogeneous knowledge graphs containing query, entity, relation, and agent nodes with various edge types. A type-aware heterogeneous GNN (RouterGNN) propagates contextual signals across node types to produce task-aware routing distributions over agents. The model is trained using soft supervision via KL divergence between predicted distributions and empirical F1-derived targets. At inference, agent outputs are weighted by predicted probabilities through weighted voting. The framework uses 24 agents (4 LLM backbones × 6 agent designs) and is evaluated on four QA benchmarks with 500 training examples each.

## Key Results
- AgentRouter consistently outperforms single-agent and ensemble baselines across four QA benchmarks
- Top-K clipping (K=5-10) significantly improves performance on multi-hop reasoning tasks while sometimes hurting direct QA
- The learned router generalizes across tasks with task-specific adaptations, though performance degrades when training and test benchmarks differ substantially
- Type-aware message passing and soft supervision via KL divergence are critical components for capturing complementary agent strengths

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Converting QA instances into heterogeneous knowledge graphs enables context-aware routing by explicitly encoding relationships among queries, entities, and agents.
- **Mechanism:** The framework extracts named entities from context via spaCy, creates entity nodes with type and frequency features, establishes query-entity edges for explicit mentions, and builds entity-entity edges through dependency parsing. Relations are materialized as nodes to keep edge vocabulary manageable while preserving semantic detail.
- **Core assumption:** Fine-grained contextual structure (entities, their relations, and agent attention patterns) contains actionable signals for predicting which agents will perform well on a given query.
- **Evidence anchors:**
  - [abstract] "convert QA instance into a knowledge graph that jointly encodes queries, contextual entities, and agents"
  - [Section 3.1] "Entity nodes are introduced to preserve the contextual signals... Each entity node stores its surface form, NER type, and frequency"
  - [corpus] Related work (GraphRouter, NG-Router) confirms graph-based routing is an active direction, though corpus papers don't validate this specific KG construction approach
- **Break condition:** If extracted entities are sparse, noisy, or irrelevant to the reasoning task (e.g., purely lexical QA without relational structure), the KG provides little signal and routing degenerates to heuristics.

### Mechanism 2
- **Claim:** A type-aware heterogeneous GNN (RouterGNN) learns to propagate contextual signals across node types, producing task-aware routing distributions over agents.
- **Mechanism:** Each node type receives a type-specific projection. Messages flow through edge types with learnable scalar weights. After L layers, query and agent embeddings are concatenated and scored via MLP, then softmaxed to produce p(a|q,G). This allows the router to integrate entity-level context into agent compatibility estimation.
- **Core assumption:** Message passing across the heterogeneous graph transfers useful information from context entities to agent nodes, and the resulting embeddings encode query-agent compatibility.
- **Evidence anchors:**
  - [abstract] "train a heterogeneous graph neural network (GNN) to propagate information across node types and produce task-aware routing distributions"
  - [Section 3.2] "Messages from different edge types are then combined with learnable gates, producing the node update"
  - [corpus] Corpus evidence on heterogeneous GNNs for routing is limited; related papers focus on routing strategies rather than GNN architectures specifically
- **Break condition:** If message passing depth is insufficient or oversmoothing occurs, node embeddings become uninformative; the paper notes deeper layers sometimes hurt performance (Figure 4).

### Mechanism 3
- **Claim:** Soft supervision via temperature-scaled F1 scores and KL divergence training captures complementary agent strengths more effectively than hard labels or cross-entropy.
- **Mechanism:** For each query, all agents produce answers scored by F1 against gold. These F1 scores are converted to soft targets via softmax(F1/τ). The router is trained to minimize KL divergence between predicted and target distributions. At inference, predicted distributions weight agent outputs via weighted voting.
- **Core assumption:** Agent F1 performance on training queries reflects generalizable strengths that transfer to unseen queries; soft targets preserve relative agent quality better than hard one-hot labels.
- **Evidence anchors:**
  - [abstract] "leveraging soft supervision and weighted aggregation of agent outputs, AgentRouter learns principled collaboration schemes"
  - [Section 3.2] "KL divergence is especially appropriate... enforces alignment across the entire distribution, ensuring that the router captures the relative strengths of both primary and secondary agents"
  - [corpus] Corpus does not provide comparative evidence on KL vs. cross-entropy for routing; this is a design choice specific to this paper
- **Break condition:** If training data is small or unrepresentative, empirical F1 scores are noisy, leading to poor supervision signal. The paper uses only 500 training examples per benchmark.

## Foundational Learning

- **Concept: Heterogeneous Graph Neural Networks**
  - **Why needed here:** RouterGNN must handle multiple node types (query, entity, agent) and edge types (query-entity, entity-entity, agent-entity, query-agent) with type-specific message functions.
  - **Quick check question:** Can you explain why using the same message function for all edge types would lose information in this architecture?

- **Concept: Soft Label / Distribution Matching (KL Divergence)**
  - **Why needed here:** Routing supervision uses continuous F1-derived distributions rather than discrete labels; KL divergence aligns full distributions rather than just top predictions.
  - **Quick check question:** What happens to gradient signal for low-probability agents if you use cross-entropy instead of KL divergence?

- **Concept: Weighted Voting / Ensemble Aggregation**
  - **Why needed here:** Final answers are produced by weighting each agent's output by the router's predicted probability, not selecting a single agent.
  - **Quick check question:** If all agents output different answers, how does weighted voting resolve disagreements?

## Architecture Onboarding

- **Component map:** KG Constructor -> Edge Builder -> RouterGNN -> Training Loop -> Inference
- **Critical path:** KG construction quality -> meaningful message passing -> accurate soft targets -> effective routing. If entity extraction fails or agent outputs are poor, downstream components cannot recover.
- **Design tradeoffs:**
  - **Top-K clipping vs. full ensemble:** Paper shows K=5–10 often outperforms K=24 on multi-hop tasks (noise reduction) but can hurt on direct QA (diversity helps). No universal optimum.
  - **GNN depth:** More layers don't consistently help; 2 layers is default. Oversmoothing risk.
  - **Hidden dimension:** Larger dimensions provide more stable gains than depth (Figure 4).
- **Failure signatures:**
  - Router collapses to uniform distribution (KL loss not decreasing) -> check soft target diversity, learning rate
  - Top-K clipping hurts significantly -> task may benefit from full ensemble; try higher K
  - Transfer across tasks fails sharply (Table 3, 4) -> router is task-specific; retrain on target distribution
- **First 3 experiments:**
  1. **Reproduce baseline comparison:** Run AgentRouter vs. Best Agent vs. Majority Vote on HotpotQA with default K=24; verify F1 improvement over Best Agent baseline.
  2. **Ablate KG components:** Remove entity-entity edges or agent-entity edges; measure F1 drop to isolate which edges contribute most.
  3. **Vary Top-K on held-out benchmark:** Train on 2Wiki, test on TriviaQA with K ∈ {3, 5, 10, 24}; observe transfer degradation pattern as in Table 3.

## Open Questions the Paper Calls Out

- **Cost-efficiency trade-offs:** How can AgentRouter be modified to optimize for cost-efficiency alongside accuracy? The authors explicitly state they did not address cost-performance trade-offs and suggest incorporating cost-aware objectives as future work. Evidence would be a modified loss function demonstrating a Pareto frontier of performance relative to monetary cost or latency.

- **Dynamic agent pools:** Can the routing framework generalize to a dynamic or automatically generated pool of agents? The paper notes it currently operates on a fixed pool of manually designed agents and identifies integrating automated generation as a promising direction. Evidence would be experiments with expanding agent pools where the router incorporates new agents in zero-shot or few-shot manner.

- **Top-K clipping mechanism:** Why does Top-K clipping improve performance on multi-hop reasoning but degrade it on single-hop tasks? The authors observe this phenomenon but only propose a "plausible explanation" regarding noise reduction. Evidence would be detailed error analysis quantifying noise vs. complementary signal contributed by low-weight agents specifically on multi-hop versus single-hop data subsets.

## Limitations
- The framework currently operates on a fixed pool of manually designed agents without addressing cost-performance trade-offs
- Empirical success hinges on quality of soft F1-derived supervision from limited training data (500 examples)
- Entity extraction quality is critical but unvalidated, and poor extraction could undermine the entire routing mechanism

## Confidence

| Claim | Confidence |
|-------|------------|
| Knowledge-graph-guided routing improves QA performance | High |
| KL divergence training with soft F1 targets is effective | Medium |
| Top-K clipping benefits multi-hop reasoning | Medium |
| Router generalizes across tasks with task-specific adaptations | Low (degrades with different benchmarks) |

## Next Checks
1. **Validate KG construction quality:** Extract entities and relations from sample queries using spaCy; manually verify entity relevance and relation accuracy for 10-20 queries across different benchmarks.
2. **Test soft supervision robustness:** Train AgentRouter with varying training set sizes (100, 250, 500 examples) and compare performance degradation to assess sensitivity to noisy F1 supervision.
3. **Cross-benchmark transfer validation:** Train on one benchmark (e.g., HotpotQA) and test on another (e.g., TriviaQA) with different Top-K values to quantify transfer degradation patterns as shown in Table 3.