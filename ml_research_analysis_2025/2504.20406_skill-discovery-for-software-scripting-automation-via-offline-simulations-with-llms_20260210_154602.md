---
ver: rpa2
title: Skill Discovery for Software Scripting Automation via Offline Simulations with
  LLMs
arxiv_id: '2504.20406'
source_url: https://arxiv.org/abs/2504.20406
tags:
- tasks
- code
- software
- task
- apis
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper proposes an offline simulation framework for discovering\
  \ and curating verified scripting skills in software applications like Adobe Illustrator.\
  \ It uses two complementary task creation strategies\u2014top-down functionality\
  \ exploration and bottom-up API synergy modeling\u2014to generate diverse tasks,\
  \ then iteratively refines and validates corresponding scripts using execution feedback."
---

# Skill Discovery for Software Scripting Automation via Offline Simulations with LLMs

## Quick Facts
- arXiv ID: 2504.20406
- Source URL: https://arxiv.org/abs/2504.20406
- Reference count: 22
- Primary result: Offline simulation framework achieves 44.7% success rate vs 28.7% runtime generation, with near-zero runtime token costs

## Executive Summary
This paper introduces an offline simulation framework for discovering and curating verified scripting skills in software applications like Adobe Illustrator. The system generates tasks through two complementary strategies—top-down functionality exploration and bottom-up API synergy modeling—then iteratively refines and validates corresponding scripts using execution feedback. A GNN-based link prediction model identifies synergistic API pairs to improve task diversity and coverage of underutilized APIs. Experiments show the framework significantly outperforms runtime code generation, achieving 44.7% success rate versus 28.7%, with near-zero runtime token costs and much faster response times.

## Method Summary
The framework operates in offline simulation mode to build a skillset of verified ExtendScript scripts. It employs a top-down approach where the LLM generates tasks based on functionality categories from Adobe Illustrator documentation, and a bottom-up approach using a GNN to predict synergistic API pairs from an API synergy graph. For each generated task, the system iteratively refines code through up to three trials using execution feedback captured via visual outcomes. The final skillset enables runtime retrieval through semantic matching, reducing response time from 4.0s to 0.1s while achieving 44.7% success rate compared to 28.7% for baseline runtime generation.

## Key Results
- 44.7% success rate achieved versus 28.7% for baseline runtime code generation
- Response time reduced from 4.0s to 0.1s with near-zero runtime token costs
- GNN link prediction achieves 37.3% Hit@5 versus 16.7% for semantic matching baseline
- Iterative validation with LVLM improves success rate from ~17-23% to ~35-47% after three trials

## Why This Works (Mechanism)

### Mechanism 1: Execution-Grounded Verification Loop
Iterative code refinement using offline execution feedback significantly increases script success rates compared to single-pass generation. The system generates a script, executes it in Adobe Illustrator, captures outputs (errors, visual changes), and uses an LVLM validator to reason over the code and visual outcome. The generator LLM then repairs the script based on this feedback for the next trial. This loop is critical because related work shows the difficulty of one-shot correctness, reinforcing the need for constraints or iterations. The loop fails if the LVLM misinterprets visual artifacts or if the LLM lacks sufficient API knowledge to fix identified errors.

### Mechanism 2: GNN-Guided API Synergy Exploration
Modeling API relationships as a graph and predicting links allows the system to discover diverse, long-tail tasks that standard semantic search might miss. The framework constructs an API Synergy Graph where nodes are APIs and edges represent co-occurrence in verified scripts. A Graph Convolutional Network learns structural and semantic patterns to predict compatibility between unlinked APIs, prompting the LLM to generate tasks involving underutilized APIs. This approach is validated by showing 37.3% Hit@5 versus 16.7% for semantic matching, demonstrating the value of graph-based navigation over libraries.

### Mechanism 3: Offline Pre-computation vs. Runtime Retrieval
Shifting computational burden to an offline phase yields superior latency and reliability during user interaction. Instead of generating code at runtime (requiring seconds and tokens), the system pre-builds a skillset database mapping natural language descriptions to verified code. At runtime, user queries are embedded and matched to the nearest skill description via cosine similarity. This approach trades flexibility for speed, as the system cannot solve unseen runtime queries but achieves 0.1s response times compared to 4.0s for baseline generation.

## Foundational Learning

- **Concept**: Link Prediction in Graph Neural Networks (GNNs)
  - Why needed: The bottom-up task creation requires predicting which API pairs work well together, not just listing APIs. You need to understand how GCNs aggregate neighbor features to predict missing edges in a graph.
  - Quick check: Given a graph of APIs where edges represent co-usage, how would a 2-layer GCN predict if two currently unconnected APIs are compatible?

- **Concept**: LLM-based Code Validation (Reflexion/Iteration)
  - Why needed: The paper relies on a trial mechanism where an LLM fixes its own code. Understanding how to structure execution feedback (errors + visual context) into a prompt is critical for reproducing results.
  - Quick check: What specific inputs must be passed to the Validator LLM to allow it to generate actionable feedback for the next code iteration?

- **Concept**: Dense Retrieval (Semantic Search)
  - Why needed: The runtime system matches text embeddings, not code understanding. You need to grasp how models like all-mpnet-base-v2 map queries to skill descriptions to bridge the user intent gap.
  - Quick check: Why might pure semantic matching fail for highly technical API queries (e.g., "set pathItem.strokeDashes"), and how does the paper's offline generation mitigate this?

## Architecture Onboarding

- **Component map**: Task Creator -> Skill Generator (with Executor and LVLM Validator) -> Skillset DB -> Runtime Retriever -> User Query
- **Critical path**: The Skill Generation with Trials loop. If the validator is too strict or too lenient, the skillset quality collapses. The paper notes the LVLM is "slightly conservative" (80% recall), implying valid skills might be discarded.
- **Design tradeoffs**: The system trades flexibility (cannot solve unseen runtime queries) for speed (0.1s response). It focuses on "atomic" skills, so complex user requests requiring skill chaining will likely fail without manual intervention.
- **Failure signatures**: Static skillset stagnation occurs if software APIs change without re-simulation. Long-tail blindness persists despite GNN efforts, with only 151 of 378 method APIs successfully used.
- **First 3 experiments**:
  1. GNN Ablation: Replace GNN link predictor with random pairing or semantic similarity to quantify API coverage lift (reproduce Table 5)
  2. Trial Limit Impact: Plot success rate curve against allowed trials to find diminishing returns point (validate Table 6 jump)
  3. Retrieval Granularity Test: Evaluate if concatenating multiple atomic skills solves multi-step tasks or if retrieval granularity limits utility

## Open Questions the Paper Calls Out

- **Open Question 1**: How can domain-specific training strategies enhance LLMs' ability to generate accurate scripts for underutilized or complex APIs?
  - Basis: Section 6 states general-purpose LLMs failed to generate relevant tasks for many APIs due to limited knowledge, suggesting "Training a domain-specific LLM" as a future direction.
  - Why unresolved: Current experiments relied exclusively on general-purpose models (GPT-4o) without domain-specific fine-tuning.
  - Evidence needed: Comparative study showing improved success rates for long-tailed APIs using a model fine-tuned on software-specific documentation.

- **Open Question 2**: What types of real-world behavioral data are most effective for mining API interaction patterns, and how can they be collected responsibly?
  - Basis: Section 6 notes that leveraging real-world usage data is an open avenue and explicitly asks "what kinds of behavioral data are useful... and how to collect them responsibly."
  - Why unresolved: Current skillset derives from static documentation and simulation rather than live user behaviors.
  - Evidence needed: Framework implementation incorporating anonymized user logs to weight API synergy, resulting in a skillset that better matches real-world user needs.

- **Open Question 3**: How can the system adapt to solve ambiguous or high-level user queries that require the composition of multiple atomic skills?
  - Basis: Limitations section highlights that real-world queries "might be ambiguous or involve higher-level tasks that require combining multiple skills," which the current retrieval method does not handle.
  - Why unresolved: Current framework retrieves single "atomic" skills based on semantic similarity, lacking a mechanism for skill composition.
  - Evidence needed: Evaluation results on a dataset of complex tasks requiring multi-step reasoning and skill chaining to achieve user goals.

## Limitations

- Static skillset requires complete re-simulation when software APIs change, creating maintenance bottlenecks
- Complex, multi-step tasks requiring skill composition are outside the system's scope
- Only 151 of 378 method APIs were successfully used, indicating persistent LLM limitations with obscure APIs

## Confidence

- **High confidence**: Execution-grounded verification loop demonstrably improves success rates (Table 6: ~17-23% → ~35-47%)
- **Medium confidence**: GNN link prediction shows clear lift over semantic matching (37.3% vs 16.7% Hit@5), but generalizability depends on initial script corpus quality
- **Low confidence**: "Near-zero runtime token costs" claim is technically true but potentially misleading—heavy offline computation is shifted, not eliminated

## Next Checks

1. **Validator Reliability Audit**: Take a random sample of 50 skills from the final skillset and have human experts verify task completion to independently measure the LVLM's true precision and recall.

2. **API Evolution Stress Test**: Simulate API deprecation by removing 10% of APIs from the synergy graph and re-running the bottom-up task generation to quantify the brittleness of the offline skillset.

3. **Complex Task Chaining Experiment**: Manually concatenate 2-3 atomic skills from the retrieved set to attempt solving multi-step tasks from the 94-task test set, measuring whether retrieval granularity is the true bottleneck.