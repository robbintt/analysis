---
ver: rpa2
title: Mechanistic Interpretability for Transformer-based Time Series Classification
arxiv_id: '2511.21514'
source_url: https://arxiv.org/abs/2511.21514
tags:
- uni00000003
- time
- uni00000037
- causal
- layer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper adapts mechanistic interpretability techniques from
  NLP to transformer-based time series classification models, addressing the challenge
  of understanding how these "black-box" models make decisions. The authors systematically
  apply activation patching, attention saliency, and sparse autoencoders to probe
  internal causal structures within the model.
---

# Mechanistic Interpretability for Transformer-based Time Series Classification

## Quick Facts
- arXiv ID: 2511.21514
- Source URL: https://arxiv.org/abs/2511.21514
- Reference count: 40
- Primary result: Adaptation of mechanistic interpretability techniques to transformer-based time series classification models

## Executive Summary
This paper adapts mechanistic interpretability techniques from NLP to transformer-based time series classification models, addressing the challenge of understanding how these "black-box" models make decisions. The authors systematically apply activation patching, attention saliency, and sparse autoencoders to probe internal causal structures within the model. They identify critical attention heads and timesteps by measuring changes in true-class probability when patching activations from correctly classified instances into misclassified ones.

## Method Summary
The authors use a 3-layer encoder-only Transformer (8 heads) with a convolutional front-end on the JapaneseVowels dataset (9 classes, 12 channels, T=25 timesteps). They implement activation patching via PyTorch forward hooks at layer, head, and position levels, using clean/corrupt instance pairs (clean: P(y_true)>0.95; corrupt: P(y_true)<0.50). Attention saliency is computed as the average attention weight per timestep across all tokens for each head. Sparse Autoencoders are trained on Layer 0 MLP outputs to identify interpretable latent features.

## Key Results
- Patching Layer 0 activations recovers up to 0.89 true-class probability, demonstrating early-layer dominance
- Attention saliency analysis identifies critical heads and timesteps for classification
- SAEs uncover interpretable features like class-specific temporal motifs (e.g., Neuron 15 activates for Class 8 with peaks in channels 2,5,9)
- Non-additive interference effects observed when patching multiple timesteps simultaneously

## Why This Works (Mechanism)

### Mechanism 1: Early-Layer Causal Localization
In the Time Series Transformer, the causal computation for classification is heavily concentrated in the first encoder layer (Layer 0), rather than being uniformly distributed across depth. Patching experiments show Layer 0 activation replacement recovers up to 0.89 true-class probability, suggesting initial temporal feature transformation determines classification trajectory.

### Mechanism 2: Sparse Feature Disentanglement via SAE
Sparse Autoencoders isolate class-discriminative temporal motifs from entangled transformer activations by imposing L1 sparsity penalties. This forces the model to map high-dimensional MLP activations to specific directions corresponding to semantic patterns like waveform shapes across channels.

### Mechanism 3: Non-Linear Timestep Interference
The causal influence of specific timesteps is non-additive; patching multiple positions simultaneously can result in interference (lower confidence) compared to individual patches. This occurs because self-attention and residual connections create dependencies where combined effects are not simple sums.

## Foundational Learning

- **Activation Patching (Causal Intervention)**: Why needed: Unlike correlation-based attribution, patching creates counterfactual internal states to prove necessity and sufficiency. Quick check: If patching Head A's activation from correct to incorrect run fixes output, does Head A cause correct answer or just correlate? (Answer: Causally sufficient in this context).

- **Clean vs. Corrupt Pair Selection**: Why needed: Methodology relies on contrasting "gold standard" internal state against "broken" state. Quick check: Why important clean/corrupt instances belong to same ground-truth class? (Answer: To ensure semantically aligned activations for meaningful patching).

- **Polysemanticity & Superposition**: Why needed: Transformers often pack multiple features into single neurons. Quick check: Why can't we just look at max-activating neurons to find "Class 8 feature"? (Answer: That neuron might also fire for Class 3 in different context due to superposition).

## Architecture Onboarding

- **Component map**: Input (T=25, C=12) -> 3-layer 1D ConvNet -> Transformer Encoder (L=3, H=8) -> Temporal Max-Pooling -> Linear Layer -> Softmax

- **Critical path**: Convolutional Front-end -> Layer 0 Attention pipeline. Layer 0 identified as bottleneck where classification fate is largely sealed.

- **Design tradeoffs**: Encoder-only vs. Encoder-Decoder (omitted decoder to reduce parameters for fixed-length classification); Patching Scope (focus on Attention Heads only, excluding MLPs and Residual streams for isolation).

- **Failure signatures**: Misclassification via Interference (model activates wrong class features); Non-monotonic Recovery (patching "too many" components causes interference and reduces confidence).

- **First 3 experiments**: 1) Layer Sweep: Run corrupt instance, patch all heads in Layer 0, then 1, then 2 from clean instance to identify dominant layer. 2) Head Isolation: Within dominant layer, patch Head 6 and Head 7 individually to measure ΔP and identify "crucial" head. 3) SAE Feature Hunting: Train SAE on Layer 0 MLP outputs, visualize Neuron 15 to verify Class 8 motif tracking.

## Open Questions the Paper Calls Out

- Do identified causal circuits, particularly early attention layer dominance, generalize across diverse time series domains? (Basis: Discussion questions universality across datasets).

- What specific causal roles do MLP blocks and residual streams play in TST decision-making compared to attention heads? (Basis: Authors acknowledge patching experiments focus solely on attention heads).

- Can intervening on specific latent features discovered by Sparse Autoencoders causally change model's classification output? (Basis: Paper suggests future work should test causal role of sparse features by amplifying chosen dimension).

## Limitations
- Analysis focuses exclusively on attention heads, omitting MLP layers and residual stream contributions
- Sparse Autoencoder interpretation depends on correct sparsity hyperparameter tuning
- Results derive from single small-scale model (3-layer, 8-head) on JapaneseVowels dataset

## Confidence
- High confidence: Layer 0 dominance finding (supported by clear ΔP gradients across layers) and clean/corrupt pairing methodology
- Medium confidence: Attention saliency correlations and SAE feature isolation, pending hyperparameter sensitivity analysis
- Low confidence: Generalization of interference patterns and non-linear patching effects beyond specific model configuration

## Next Checks
1. Test whether early-layer localization persists in deeper models (L=6,12) by repeating layer-wise patching sweep
2. Systematically vary sparsity coefficient λ and measure feature activation distributions to verify stability of interpretable neurons
3. Extend patching methodology to include MLP activations and residual stream components to quantify their contribution to classification decisions