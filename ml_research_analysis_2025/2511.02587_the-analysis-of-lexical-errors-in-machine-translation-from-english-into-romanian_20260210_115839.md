---
ver: rpa2
title: The Analysis of Lexical Errors in Machine Translation from English into Romanian
arxiv_id: '2511.02587'
source_url: https://arxiv.org/abs/2511.02587
tags:
- translation
- romanian
- machine
- errors
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ''
---

# The Analysis of Lexical Errors in Machine Translation from English into Romanian

## Quick Facts
- arXiv ID: 2511.02587
- Source URL: https://arxiv.org/abs/2511.02587
- Authors: Angela Stamatie
- Reference count: 0
- One-line primary result: Collocation errors are the dominant failure mode (31.7%) in English-to-Romanian machine translation of Covid-19 texts.

## Executive Summary
This study analyzes lexical errors in machine translation outputs from English to Romanian, focusing on 230 Covid-19-related texts. Using a hierarchical error taxonomy, the research identifies collocation errors as the most frequent issue (31.7%), followed by borrowings and false friends. The analysis reveals that automated metrics like BLEU and WER are insufficient for capturing semantic errors in specialized medical domains. The findings highlight the challenges of translating into "minor" languages like Romanian due to data scarcity and structural differences between English and Romanian.

## Method Summary
The study manually annotated 230 Covid-19 texts translated via Google Translate from English to Romanian. A hierarchical error taxonomy (Formal, Semantic, Stylistic) was applied to categorize errors. BLEU and WER scores were calculated for quantitative assessment, but the focus was on qualitative error classification. The analysis compared machine outputs against official Romanian terminology and medical documents.

## Key Results
- Collocation errors account for 31.7% of total errors, making them the dominant failure mode.
- BLEU score: 0.44; WER: 0.89, indicating low accuracy and high error rate.
- Untranslated borrowings (17.2%) and false friends (10.3%) are significant contributors to semantic errors.

## Why This Works (Mechanism)

### Mechanism 1: Hierarchical Error Taxonomy Classification
Applying a hierarchical taxonomy (Formal, Semantic, Stylistic) to MT output reveals that lexical errors are the dominant failure mode, specifically driven by collocation failures rather than grammar. The study utilizes a derived classification scheme (based on Flanagan, Hsu, and Hemchua & Schmitt) to categorize 230 texts. By quantifying specific error types—distinguishing between "formal" errors (spelling) and "semantic" errors (meaning)—the analysis isolates *why* fluency is compromised.

### Mechanism 2: Automated Metric Validation via BLEU & WER
Standard automated metrics like BLEU and WER serve as necessary but insufficient baselines; they detect volume but miss the *semantic gravity* of medical mistranslations. The study calculates a BLEU score of 0.44 and a WER of 0.89. This quantitative mismatch (low BLEU, high error rate) demonstrates that the system fails to capture n-gram precision in specialized domains.

### Mechanism 3: Linguistic Resource Scarcity & Transfer Failure
The frequency of "false friends" and untranslated "borrowings" suggests the NMT system lacks sufficient high-quality parallel corpora for Romanian, forcing it to rely on English structural patterns. The analysis highlights specific failures (e.g., translating "curfew" as "stare de asediu" [state of siege] via false cognate paths, or leaving "screening" untranslated). This indicates the model defaults to statistical patterns from English or high-resource languages when Romanian-specific domain data is missing.

## Foundational Learning

- **Concept: Collocation vs. Grammar Errors**
  - **Why needed here:** The study identifies collocation errors (31.7%) as the primary failure mode, distinct from grammar. Understanding that MT often gets syntax right but word *pairings* wrong is crucial for debugging.
  - **Quick check question:** Does the system output "strong coffee" (correct collocation) or "powerful coffee" (semantic/lexical error)?

- **Concept: Dynamic Equivalence (Nida)**
  - **Why needed here:** The paper critiques literal translation of "false friends." Understanding Nida's concept of "dynamic equivalence" (matching the impact rather than the form) explains why "lockdown" -> "blocare" fails to convey the *function* of the restriction.
  - **Quick check question:** Is the goal to translate the word "mask" or the *act* of wearing one?

- **Concept: BLEU Score Limitations**
  - **Why needed here:** The paper argues BLEU is insufficient for medical texts. You must understand that BLEU measures *overlap*, not *truth*, to see why a high BLEU score in a medical context could still be dangerous.
  - **Quick check question:** If a system translates "Do not take aspirin" as "Take aspirin," what would a 4-gram overlap metric likely report?

## Architecture Onboarding

- **Component map:** Source Text -> Google Translate (NMT) -> Human Reference (Official Romanian terms) -> Hierarchical Error Taxonomy -> WER/BLEU
- **Critical path:** Source Text is parsed by the NMT encoder. If no direct Romanian equivalent exists in the attention mechanism, the decoder selects a high-probability English lexical item (Borrowing) or a False Friend. The error is identified by comparing against the "Official Romanian Term" ground truth.
- **Design tradeoffs:**
  - **Metric Selection:** The study trades off the speed of automated metrics (BLEU) for the accuracy of human evaluation (MQM), noting BLEU fails to catch dangerous semantic drift.
  - **Data Scope:** Focused on Covid-19 domain. Generalizes poorly to literary translation but offers high precision for medical terminology.
- **Failure signatures:**
  - **High Collocation Error (31.7%):** System pairs words statistically but not idiomatically.
  - **Untranslated Borrowings (17.2%):** System copies source words when target vocabulary is missing.
  - **False Friends (10.3%):** System maps form rather than meaning (e.g., "Curfew" -> "Siege").
- **First 3 experiments:**
  1. **Collocation Stress Test:** Input sentences containing the top 5 identified collocation failures (e.g., "flatten the curve," "herd immunity") and measure if the system corrects them or defaults to the error pattern.
  2. **Acronym Disambiguation:** Feed ambiguous acronyms (e.g., "CFR" - Case Fatality Rate vs. Căile Ferate Române) into the engine to test if context vectors resolve the medical meaning.
  3. **Metric Contrast:** Calculate BLEU scores for the specific "semantic error" subset identified by the human annotator to quantify the "false negative" rate of automated testing.

## Open Questions the Paper Calls Out
None

## Limitations
- Manual annotation introduces subjectivity in categorizing errors like "collocation" versus "semantic" mistakes.
- The exact source corpus (230 texts) is not fully specified, limiting reproducibility.
- BLEU and WER metrics are acknowledged as insufficient for capturing semantic errors in medical translation, highlighting a gap between automated metrics and domain-specific quality.

## Confidence

- **High confidence:** The identification of collocation errors as the dominant failure mode (31.7%) is well-supported by the error taxonomy and quantitative breakdown.
- **Medium confidence:** The claim that BLEU/WER scores inadequately reflect medical translation quality is plausible but not empirically validated with alternative metrics.
- **Low confidence:** The assertion that data scarcity (Romanian being a "minor language") is the primary driver of errors lacks direct evidence linking corpus size to specific error types.

## Next Checks

1. Replicate the error taxonomy classification with a new set of Romanian-English medical texts to test inter-annotator reliability.
2. Supplement BLEU/WER with domain-specific metrics (e.g., semantic similarity scores) to validate claims about metric inadequacy.
3. Analyze a subset of errors after fine-tuning the NMT system on a small, curated Romanian medical corpus to isolate the impact of data scarcity.