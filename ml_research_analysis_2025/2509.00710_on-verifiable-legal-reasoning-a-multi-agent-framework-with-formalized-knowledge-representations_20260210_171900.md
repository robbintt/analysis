---
ver: rpa2
title: 'On Verifiable Legal Reasoning: A Multi-Agent Framework with Formalized Knowledge
  Representations'
arxiv_id: '2509.00710'
source_url: https://arxiv.org/abs/2509.00710
tags:
- legal
- reasoning
- knowledge
- structured
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a modular multi-agent framework (SOLAR) that
  improves legal reasoning by separating knowledge extraction and application stages.
  The framework uses specialized agents to extract legal concepts from statutes, formalize
  them into ontologies, and apply symbolic reasoning to answer tax calculation queries.
---

# On Verifiable Legal Reasoning: A Multi-Agent Framework with Formalized Knowledge Representations

## Quick Facts
- arXiv ID: 2509.00710
- Source URL: https://arxiv.org/abs/2509.00710
- Reference count: 40
- Primary result: SOLAR achieves 76.4% accuracy vs 18.8% zero-shot on tax calculation benchmark

## Executive Summary
This paper introduces SOLAR, a modular multi-agent framework for verifiable legal reasoning. The system separates knowledge extraction from application, using specialized agents to formalize legal statutes into ontologies and then apply symbolic reasoning for tax calculations. Evaluation on the SARA numeric dataset shows significant improvements over baseline approaches, with SOLAR achieving 76.4% accuracy compared to 18.8% for zero-shot methods. The framework provides transparent, verifiable reasoning pathways while narrowing the performance gap between reasoning and foundational models.

## Method Summary
SOLAR employs a two-stage approach: Stage I extracts legal concepts from statutes using specialized agents to create a formalized TBox ontology containing classes, properties, and rules in first-order logic. Stage II applies this knowledge to answer queries through symbolic inference using an SMT solver. The framework uses Python 3.13 with LangChain, LangGraph, NLTK's SMT solver, and Pydantic v2.x. Implementation involves a knowledge acquisition phase where agents extract and validate legal concepts, followed by a knowledge application phase that maps case facts to an ABox and performs symbolic inference to generate answers. The system was evaluated on 96 cases from the SARA numeric dataset, measuring accuracy at a 10% tolerance threshold.

## Key Results
- SOLAR achieves 76.4% accuracy on SARA numeric dataset vs 18.8% for zero-shot approaches
- Successfully narrows performance gap between reasoning and foundational models
- Provides transparent, verifiable reasoning pathways through formalized ontologies
- Reduces token usage and latency compared to chain-of-code baselines

## Why This Works (Mechanism)
SOLAR's effectiveness stems from separating knowledge extraction from application, allowing specialized agents to build comprehensive legal ontologies that capture statutory nuances. By formalizing legal concepts into TBox structures and applying symbolic reasoning, the system avoids the brittleness of pure language model approaches while maintaining interpretability. The modular architecture enables targeted improvements and validation at each stage, ensuring that reasoning errors can be traced and corrected systematically.

## Foundational Learning
- **TBox vs ABox distinction**: Why needed - separates schema (TBox) from instance data (ABox) for modular reasoning; Quick check - verify TBox contains only class/property definitions while ABox contains only facts about specific cases
- **First-order logic formalization**: Why needed - enables precise rule representation and automated inference; Quick check - ensure all rules can be expressed as logical implications without ambiguity
- **SMT solver integration**: Why needed - provides sound symbolic reasoning over formalized knowledge; Quick check - validate solver can correctly handle all TBox rules on test ABox assertions
- **Multi-agent orchestration**: Why needed - distributes complex tasks across specialized components; Quick check - confirm each agent produces valid intermediate outputs for subsequent stages

## Architecture Onboarding

**Component Map**
Knowledge Acquisition Agents -> TBox Generation -> TBox Interpreter -> Query Analysis Agent -> ABox Construction -> Symbolic Inference Agent -> Answer Generation

**Critical Path**
Concept extraction → Rule formulation → TBox validation → Query analysis → ABox mapping → SMT inference → Answer generation

**Design Tradeoffs**
Formalization precision vs. modeling effort: More detailed ontologies improve accuracy but require greater manual oversight. Symbolic reasoning vs. neural approaches: Provides verifiable reasoning but may miss implicit connections. Modularity vs. performance: Specialized agents enable targeted improvements but introduce coordination overhead.

**Failure Signatures**
Ontology gaps causing missing predicates in ABox construction. Incorrect property usage patterns leading to wrong rule application. Status determination inconsistencies when multiple filing statuses apply. SMT solver timeouts on complex rule combinations.

**3 First Experiments**
1. Validate TBox generation on a single tax provision with known rules
2. Test ABox construction accuracy using simplified case facts
3. Verify SMT solver can correctly apply basic tax calculation rules

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation limited to single domain (U.S. federal tax law) with 96 curated cases
- Performance gains measured only against baseline LLM approaches, not state-of-the-art reasoning models
- Framework not validated on diverse legal domains or real-world statutory ambiguity
- Reliance on high-capability models (e.g., o1) for TBox generation may introduce brittleness

## Confidence

**High confidence**: SOLAR improves accuracy (76.4% vs 18.8%) for SARA numeric dataset
**Medium confidence**: SOLAR "significantly narrows the performance gap between reasoning and foundational models" within tested domain
**Low confidence**: Claims about generalizability and robustness across diverse legal domains

## Next Checks
1. Apply SOLAR to a second legal domain (e.g., contract law) and evaluate on comparable benchmark dataset
2. Introduce synthetic cases with ambiguous statutory provisions to test framework's handling of legal uncertainty
3. Measure performance scaling as statutory corpus size and ontology complexity increase