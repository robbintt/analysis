---
ver: rpa2
title: Revisiting Data Attribution for Influence Functions
arxiv_id: '2508.07297'
source_url: https://arxiv.org/abs/2508.07297
tags:
- influence
- training
- data
- functions
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper provides a comprehensive review of influence functions
  for data attribution in deep learning. The authors examine how influence functions
  estimate the impact of training examples on model predictions without expensive
  retraining.
---

# Revisiting Data Attribution for Influence Functions

## Quick Facts
- arXiv ID: 2508.07297
- Source URL: https://arxiv.org/abs/2508.07297
- Authors: Hongbo Zhu; Angelo Cangelosi
- Reference count: 20
- One-line result: Comprehensive review of influence functions for data attribution and mislabeled data detection in deep learning

## Executive Summary
This paper provides a comprehensive review of influence functions for data attribution in deep learning, examining how these methods estimate the impact of training examples on model predictions without expensive retraining. The authors analyze algorithmic advances including LiSSA and EK-FAC for efficient inverse-Hessian-vector product estimation, and evaluate effectiveness on data attribution and mislabel detection tasks. Experiments on MNIST, FashionMNIST, Flowers102, and Food101 datasets demonstrate that influence functions successfully identify positively and negatively influential training examples that align with human intuition. The paper also shows effectiveness in detecting mislabeled data through self-influence scores, with quantitative results showing high detection accuracy. The authors conclude by identifying future directions including sample removing and label repairing based unlearning approaches.

## Method Summary
The paper reviews influence functions as a first-order approximation to estimate how training data affects model predictions without retraining. The core approach uses inverse-Hessian-vector products (IHVP) to approximate parameter changes when removing training points. The method computes influence scores as the dot product between IHVP of test gradients and training gradients. Two approximation methods are analyzed: LiSSA (iterative stochastic approximation) and EK-FAC (Kronecker-factored eigenvalue inversion). Self-influence scores are used to detect mislabeled data by measuring how much upweighting a point reduces its own loss. The framework is evaluated across multiple datasets using Spearman correlation (LDS) for attribution quality and detection accuracy for mislabeled data.

## Key Results
- Influence functions successfully identify positively and negatively influential training examples that align with human intuition on MNIST and FashionMNIST
- Self-influence scores effectively detect mislabeled training data, with high detection accuracy at 10-50% inspection intervals
- EK-FAC demonstrates superior stability and lower error compared to LiSSA for IHVP approximation in ResNet18 experiments
- The method works across diverse architectures including CNN, ResNet50, and ViT-B/16 on various datasets

## Why This Works (Mechanism)

### Mechanism 1: First-Order Parameter Shift via Inverse-Hessian Vector Products (IHVP)
- Claim: The change in model parameters resulting from removing a training point can be approximated without retraining by using the inverse Hessian and the gradient of the lost point.
- Mechanism: Instead of resolving the optimization problem (retraining), the method uses a Taylor expansion. It assumes that upweighting a point $z$ by a small $\epsilon$ shifts the optimal parameters $\hat{\theta}$ by $-H^{-1}\nabla L(z, \hat{\theta})$. By setting $\epsilon \approx -1/n$, this approximates the removal of the point.
- Core assumption: The empirical risk is twice differentiable and strictly convex at $\hat{\theta}$, ensuring the Hessian $H$ is positive definite and invertible.
- Evidence anchors:
  - [abstract]: "Influence functions... offer an efficient, first-order approximation to estimate the impact... without the need for expensive retraining."
  - [section 3.2]: Eq. (1) defines $I_{params}(z) = -H_{\hat{\theta}}^{-1} \nabla_\theta L(z, \hat{\theta})$.
  - [corpus]: Neighbor paper "Rescaled Influence Functions" validates that IF methods rely on "first-order Taylor approximation."
- Break condition: If the model has not converged to a (local) minimum where the Hessian is positive definite, the inversion becomes unstable.

### Mechanism 2: Self-Influence as a Proxy for Label Noise
- Claim: Mislabeled or ambiguous training examples exhibit high "self-influence" scores compared to clean data.
- Mechanism: Self-influence $I_{loss}(z, z)$ measures how much upweighting a point $z$ reduces the loss on itself. Models tend to "overfit" or exert excessive "effort" to accommodate outliers or wrong labels. Clean examples, supported by neighboring data, require less individual parameter bending.
- Core assumption: Valid/clean data points lie on the manifold learned by the model, whereas errors are treated as outliers requiring significant parameter shifts to fit.
- Evidence anchors:
  - [section 7.2]: "Mislabeled training examples often exhibit strong self-influence... typical of outliers or mislabeled data."
  - [section 7.2, Fig 2]: Visual confirmation that top-ranked self-influence samples in MNIST were visually ambiguous or mislabeled (e.g., a malformed '3' looking like an '8').
  - [corpus]: N/A (Corpus lacks specific evidence regarding self-influence; reliance is on the primary text).
- Break condition: If the model is under-parameterized or the noise is systematic (not outlier-based), self-influence may not separate clean from noisy data effectively.

### Mechanism 3: Gauss-Newton Hessian (GNH) for Stability in Non-Convex Settings
- Claim: Replacing the true Hessian with the Gauss-Newton Hessian (plus damping) allows influence functions to work in deep learning where the true Hessian is often indefinite (non-convex).
- Mechanism: Deep networks are rarely trained to strict global convexity, leading to negative eigenvalues in the true Hessian $H$. GNH approximates $H$ by linearizing the network mapping, guaranteeing positive semi-definiteness. Damping ($G + \lambda I$) ensures invertibility.
- Core assumption: The loss landscape can be locally approximated by a convex bowl sufficiently to estimate curvature effects.
- Evidence anchors:
  - [section 3.3]: "The GNH is always positive semidefinite... making it a more robust substitute for H under practical deep learning settings."
  - [section 3.3, Eq 3]: Updates the influence formula to use $(G + \lambda I)^{-1}$.
  - [corpus]: Neighbor paper "Better Hessians Matter" supports the use of GGN and K-FAC to handle the "ill-conditioned Hessian matrix."
- Break condition: If the damping factor $\lambda$ is too small, numerical instability persists; if too large, the curvature information is washed out, reducing influence accuracy.

## Foundational Learning

- Concept: **Hessian-Vector Products (HVPs)**
  - Why needed here: The core bottleneck. You cannot store the Hessian matrix for a modern LLM. You must understand how to compute the product of the Hessian and a vector ($Hv$) implicitly using Pearlmutter's trick or algorithmic differentiation without materializing $H$.
  - Quick check question: Can you explain why computing $H^{-1}v$ is computationally cheaper than computing $H^{-1}$?

- Concept: **Convexity and Strict Convexity**
  - Why needed here: The theoretical derivation relies on a unique optimal point $\hat{\theta}$. Understanding that non-convexity (standard in DL) breaks the positive definite assumption is key to understanding why approximations like GNH are necessary.
  - Quick check question: Why does a non-convex loss landscape pose a problem for standard Newton's method updates?

- Concept: **Empirical Risk Minimization (ERM)**
  - Why needed here: The paper frames the training process as minimizing $R(\theta) = \frac{1}{n}\sum L(z_i, \theta)$. Influence functions estimate how the minimizer $\hat{\theta}$ moves if the distribution of $z$ changes.
  - Quick check question: If you upweight a specific training point, does the empirical risk at that point generally increase or decrease? (Trick question: the *weight* of the risk increases, but the *loss* at that point typically decreases as the model fits it better).

## Architecture Onboarding

- Component map:
  - Trained Model -> Gradient Calculator -> IHVP Solver -> Influence Scorer

- Critical path:
  1. Select a test query $z_{test}$.
  2. Compute the test gradient $g_{test} = \nabla L(z_{test})$.
  3. Pass $g_{test}$ into the IHVP Solver to get the inverse curvature vector $v$.
  4. For every training sample (or a candidate subset), compute the gradient dot product with $v$.

- Design tradeoffs:
  - **LiSSA vs. EK-FAC**:
    - *LiSSA*: Lower memory overhead initially, but computationally expensive ($O(J \cdot bp)$) and slow convergence (requires thousands of iterations). Better for quick implementation on small models.
    - *EK-FAC*: High upfront cost (eigenvalue fitting), but extremely fast inference once factors are computed. Better for large-scale or repeated queries. Section 6.2 proves EK-FAC error is fixed and generally lower than LiSSA.
  - **Damping ($\lambda$)**: Critical hyperparameter. Must be tuned to balance stability vs. signal fidelity.

- Failure signatures:
  - **Divergence in LiSSA**: If step size $\alpha$ is too large or damping is too small, the recursive estimation explodes (NaN).
  - **Random Attribution**: If LDS (Linear Datamodeling Score) correlation is near 0, the Hessian approximation is likely invalid or the model is under-trained.
  - **Out of Memory (OOM)**: Attempting explicit Hessian inversion or storing full covariances for large layers.

- First 3 experiments:
  1. **Sanity Check (Visual)**: Train a simple CNN on MNIST. Pick a "9" from the test set. Compute influences. Verify that the top positive influencers are other "9"s and negatives are "4"s or "7"s (visual similarity).
  2. **Noise Detection (Quantitative)**: Corrupt 10% of FashionMNIST labels. Calculate self-influence scores. Plot the retrieval curve: what percentage of corrupted samples are found in the top 10% of self-influence scores? (Reference: Paper Figure 3).
  3. **Approximation Error Analysis**: Compare LiSSA vs. EK-FAC on a ResNet18. Measure the trade-off: How many iterations of LiSSA are needed to match the stability of EK-FAC, and what is the wall-clock time difference?

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the proposed sample-removing Newton update effectively perform machine unlearning without causing performance degradation on retained data?
- Basis in paper: [explicit] Section 8 identifies "Sample Removing Based Unlearning" as a future direction to mitigate harmful samples without retraining.
- Why unresolved: The paper provides the theoretical update formula $\theta_{new} = \theta + \frac{1}{n} H^{-1}_{\theta} \nabla_{\theta} L(z, \theta)$ but does not evaluate the stability or fidelity of the resulting model.
- What evidence would resolve it: Empirical results showing that the updated model "forgets" specific training points while maintaining accuracy comparable to a gold-standard retrained model.

### Open Question 2
- Question: Does the label-repairing parameter update successfully correct the model's behavior for mislabeled examples?
- Basis in paper: [explicit] Section 8 proposes "Label Repairing Based Unlearning" as a method to correct mislabeled points using a counterfactual gradient update.
- Why unresolved: While the update rule is derived, the paper does not experimentally validate if this single step sufficiently adjusts the decision boundary for the corrected label.
- What evidence would resolve it: Quantitative benchmarks showing improved accuracy on corrected samples and their corresponding test classes after applying the update.

### Open Question 3
- Question: How can influence functions be improved to robustly handle the discrepancies arising from non-convex optimization and stochastic convergence?
- Basis in paper: [explicit] Section 2 states that the "applicability of IFs in deep, non-convex settings remains an open challenge" due to variance and optimization dynamics.
- Why unresolved: The paper notes that while approximations like EK-FAC help, fundamental discrepancies between influence estimates and exact leave-one-out retraining persist in neural networks.
- What evidence would resolve it: A new approximation method that maintains high correlation with ground-truth leave-one-out effects across varying initialization seeds and non-convex loss landscapes.

## Limitations

- The theoretical foundation assumes local convexity around the optimum, which may not hold for complex deep learning models with non-convex loss landscapes
- The computational expense of LiSSA iterations (thousands of steps) remains a practical barrier for very large models, despite EK-FAC improvements
- The effectiveness of self-influence for mislabeled data detection relies on distributional assumptions about label noise that may not generalize across all datasets

## Confidence

- **High Confidence**: The mathematical framework of influence functions and their application to data attribution (Sections 3.2-3.3) - the derivation is well-established and the empirical demonstrations align with theoretical expectations
- **Medium Confidence**: The effectiveness of self-influence for mislabeled data detection - while experiments show quantitative improvements, the mechanism relies on distributional assumptions about label noise that may not generalize
- **Medium Confidence**: The comparative evaluation of LiSSA vs EK-FAC (Section 6.2) - the theoretical advantages are clear, but real-world performance depends heavily on implementation details and dataset characteristics

## Next Checks

1. **Convergence Sensitivity Analysis**: Systematically vary the damping parameter Î» and LiSSA iteration count J across different datasets to establish robust hyperparameter ranges and quantify sensitivity to these choices

2. **Large-Scale Transfer Test**: Apply influence functions to a pretrained vision transformer (ViT) on a large-scale dataset like ImageNet to evaluate whether the improvements demonstrated on moderate datasets scale to realistic deep learning scenarios

3. **Adversarial Robustness Evaluation**: Test whether influence function attributions remain stable under adversarial perturbations to both training and test data, addressing the vulnerability of gradient-based attribution methods to small input changes