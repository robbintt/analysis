---
ver: rpa2
title: On the interplay of Explainability, Privacy and Predictive Performance with
  Explanation-assisted Model Extraction
arxiv_id: '2505.08847'
source_url: https://arxiv.org/abs/2505.08847
tags:
- noise
- privacy
- explanations
- level
- queries
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates the effectiveness of differential privacy
  (DP) in mitigating model extraction attacks (MEAs) that exploit counterfactual explanations
  in MLaaS environments. The research evaluates DP applied at the model level (DP-SGD),
  explainer level (DP-based GAN), and both simultaneously, analyzing the trade-offs
  between privacy, model performance, and explanation quality.
---

# On the interplay of Explainability, Privacy and Predictive Performance with Explanation-assisted Model Extraction

## Quick Facts
- arXiv ID: 2505.08847
- Source URL: https://arxiv.org/abs/2505.08847
- Reference count: 24
- This study shows that applying differential privacy at both the model and explainer levels provides synergistic protection against model extraction attacks exploiting counterfactual explanations.

## Executive Summary
This paper investigates the effectiveness of differential privacy (DP) in mitigating model extraction attacks (MEAs) that exploit counterfactual explanations in MLaaS environments. The research evaluates DP applied at the model level (DP-SGD), explainer level (DP-based GAN), and both simultaneously, analyzing the trade-offs between privacy, model performance, and explanation quality. Results show that DP effectively reduces MEA success by limiting an adversary's ability to reconstruct the target model, with higher noise levels providing stronger protection but at the cost of degraded model accuracy and explanation realism.

## Method Summary
The study evaluates differential privacy mechanisms against model extraction attacks that leverage counterfactual explanations. The experimental setup uses two datasets (Housing and EEG Eye State) with a 16-layer DNN target model trained with DP-SGD via TensorFlow Privacy. A CounterGAN explainer generates counterfactuals, with DP applied to the generator. The attack uses knowledge distillation-based MEA with random queries, training a threat model to mimic the target. The framework evaluates four DP scenarios: no DP, DP-Model only, DP-Explainer only, and both combined. Metrics include MEA Agreement (prediction match between extracted and target model), Prediction Gain (ΔP in classifier confidence), Realism (L2 distance from autoencoder reconstruction), and standard classification metrics.

## Key Results
- DP effectively reduces MEA success, with higher noise levels providing stronger protection but degrading model accuracy and explanation quality
- Applying DP at both model and explainer levels provides synergistic protection against MEA
- Increasing noise levels reduce model performance (e.g., accuracy dropping from 94% to 85% in EEG dataset) and decrease explanation quality (prediction gain reduced from 0.488 to 0.055, realism scores increasing)

## Why This Works (Mechanism)
DP introduces calibrated noise to model gradients (DP-SGD) or generator parameters (DP-based GAN), which perturbs the decision boundaries and counterfactual generation process. This makes it harder for adversaries to accurately reconstruct the target model through knowledge distillation using (prediction, counterfactual) pairs. The synergistic effect occurs because noise at both levels compounds the uncertainty in the extracted model's decision boundaries.

## Foundational Learning
- Differential Privacy (DP): A mathematical framework for quantifying and limiting information leakage from individual data points. Why needed: Provides the theoretical foundation for privacy guarantees. Quick check: Verify privacy budget (ε, δ) calculations match noise levels used.
- Counterfactual Explanations: Minimal changes to input features that alter model predictions. Why needed: Serve as the attack vector for MEA. Quick check: Confirm counterfactuals change prediction labels and maintain proximity to original inputs.
- Knowledge Distillation: Training a student model to mimic a teacher model using soft labels. Why needed: Forms the basis of the MEA framework. Quick check: Validate student model achieves reasonable accuracy on validation set.

## Architecture Onboarding
- Component Map: Random Queries -> Target Model (DP or No DP) -> Counterfactual Generator (DP or No DP) -> Knowledge Distillation Attacker -> Extracted Model
- Critical Path: Query generation → Target prediction + CF generation → KD training → MEA Agreement evaluation
- Design Tradeoffs: Higher DP noise → better privacy but worse accuracy and explanation quality; simultaneous DP at both levels → synergistic protection but amplified utility loss
- Failure Signatures: Excessive accuracy drop with high DP noise; CF quality degradation (low Prediction Gain, high Realism); MEA Agreement not decreasing as expected
- First Experiments: 1) Train baseline 16-layer DNN on Housing dataset without DP; 2) Implement and test CounterGAN on baseline model; 3) Apply DP-SGD with noise 0.1 and measure accuracy drop

## Open Questions the Paper Calls Out
- How do alternative DP mechanisms for counterfactual generation compare to the DP-based GAN approach in balancing privacy protection, explanation quality, and MEA resistance? The conclusion states "Further research will include testing other DP-based methods to generate CFs, MEA methods and more datasets."
- How do the observed privacy-utility trade-offs generalize to non-tabular data modalities such as images and text? The experimental evaluation is limited to two tabular datasets.
- Can a principled framework be developed to automatically select optimal noise levels and DP application points given application-specific constraints on accuracy and explanation quality? The paper quantifies trade-offs but provides no systematic method for determining the optimal balance point.

## Limitations
- The reported privacy-utility trade-offs rely heavily on the KD-based MEA framework and DP parameter choices; small changes could shift results significantly
- The autoencoder for realism is only described abstractly, making quality metrics potentially sensitive to implementation details
- Confidence in the synergistic claim depends on assumptions about the adversary's ability to extract the same counterfactuals across both mechanisms

## Confidence
- Performance drop numbers: High (directly observable from training logs)
- Causality between DP noise and MEA Agreement: Medium (inferred rather than experimentally isolated)
- Realism and prediction gain trends: Medium (dependent on unspecified autoencoder)

## Next Checks
1. Re-implement the autoencoder used for realism scores and verify that its reconstruction errors are stable across noise settings
2. Conduct an ablation study isolating DP-Model and DP-Explainer effects on MEA Agreement to confirm synergy
3. Test the KD-based MEA with alternative query budgets and feature ranges to assess robustness of the attack success trends