---
ver: rpa2
title: 'The Silicon Psyche: Anthropomorphic Vulnerabilities in Large Language Models'
arxiv_id: '2601.00867'
source_url: https://arxiv.org/abs/2601.00867
tags:
- human
- vulnerabilities
- vulnerability
- security
- psychological
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a novel framework for assessing anthropomorphic
  vulnerabilities in Large Language Models (LLMs) by applying the Cybersecurity Psychology
  Framework (CPF), a 100-indicator taxonomy of human psychological vulnerabilities.
  The authors introduce the Synthetic Psychometric Assessment Protocol (SILICONPSYCHE)
  to convert CPF indicators into adversarial scenarios targeting LLM decision-making.
---

# The Silicon Psyche: Anthropomorphic Vulnerabilities in Large Language Models

## Quick Facts
- **arXiv ID:** 2601.00867
- **Source URL:** https://arxiv.org/abs/2601.00867
- **Reference count:** 29
- **Primary result:** LLMs exhibit susceptibility to authority-gradient manipulation, temporal pressure exploitation, and convergent-state attacks mirroring human cognitive failure modes.

## Executive Summary
This paper proposes that Large Language Models inherit human psychological vulnerabilities from training on human-generated text, creating exploitable attack surfaces analogous to social engineering on humans. The authors introduce the Synthetic Psychometric Assessment Protocol (SILICONPSYCHE) to convert the Cybersecurity Psychology Framework's 100 human vulnerability indicators into adversarial scenarios for LLMs. They hypothesize that models demonstrate critical susceptibility to authority compliance, urgency-driven decision-making, and multi-vector convergent attacks that produce multiplicative risk amplification rather than additive effects. The research aims to develop "psychological firewalls" to protect AI agents operating in adversarial environments.

## Method Summary
The SILICONPSYCHE protocol converts CPF indicators into adversarial scenarios, presents them to target LLM agents via standardized API calls (temperature T=0.3), and classifies responses using ternary scoring (Green/Yellow/Red) by three independent raters. Vulnerability is quantified through weighted category scores and convergence indices that model multiplicative risk amplification. The method requires the full CPF taxonomy (100 indicators), specific system prompts establishing agent context, and detailed rubrics for response classification. Current implementation uses example scenarios for Indicators 1.6 (Authority Gradient), 2.1 (Urgency-Induced Bypass), and 6.7 (Fight-Flight Postures) with models like GPT-5.2, Claude 4.5, and Llama 4 via OpenRouter or Novita.ai.

## Key Results
- LLMs demonstrate robust defenses against traditional jailbreaks but exhibit critical susceptibility to authority-gradient manipulation
- Temporal pressure exploitation creates urgency-driven decision shortcuts in model behavior
- Convergent-state attacks combining multiple psychological vectors achieve multiplicative bypass rates exceeding single-vector attacks

## Why This Works (Mechanism)

### Mechanism 1: Statistical Pattern Absorption
LLMs absorb human psychological response patterns from training data, encoding predictable manipulation surfaces. When human text shows consistent compliance with authority cues or urgency-driven action, these statistical regularities become embedded in model probability distributions, teaching the model not just syntax but decision patterns.

### Mechanism 2: RLHF-Induced Typicality Collapse
Reinforcement learning from human feedback amplifies anthropomorphic vulnerabilities by rewarding "typical" human-aligned responses. Human raters consistently reward deference to authority and responsiveness to urgency as socially normative, creating strong gradient signals toward authority-compliant behavior.

### Mechanism 3: Convergent State Amplification
Attacks combining multiple psychological vectors (authority + urgency + social proof) produce multiplicative, not additive, bypass rates. The convergence index models compounding risk as each signal independently shifts probability mass toward compliance, stacking multiplicatively before higher-level safety reasoning can integrate and reject them.

## Foundational Learning

- **Pre-cognitive processing in humans vs. early-layer attention in LLMs**
  - Why needed here: The paper's central analogy requires understanding that human decisions begin ~300-500ms before conscious awareness, and that LLM attention patterns may function similarly as fast, automatic pattern-matching preceding deliberative reasoning.
  - Quick check question: Can you explain why the authors argue attention pattern priors are functionally analogous to human pre-cognitive processes, even though LLMs lack consciousness?

- **RLHF objective conflicts (helpfulness vs. harmlessness)**
  - Why needed here: The "AI Neurosis" concept depends on understanding that models receive contradictory training signals, creating exploitable instability when both are activated simultaneously (e.g., urgent help request for a harmful action).
  - Quick check question: How does an attacker exploit the tension between "be helpful" and "be harmless" training objectives?

- **Cialdini's influence principles (authority, scarcity, social proof, etc.)**
  - Why needed here: The CPF framework maps directly to these principles; understanding them is prerequisite to interpreting scenario constructions and vulnerability categories.
  - Quick check question: Which Cialdini principle does Indicator 3.3 ("Everyone else has approved this") exploit?

## Architecture Onboarding

- **Component map:** CPF Taxonomy (100 indicators / 10 categories) → SILICONPSYCHE Protocol (indicators → adversarial scenarios → ternary scoring) → Threat Model (Autonomous Cognitive Agent + Dual-source Attacker + Psychological Interface) → CPIF / Psychological Firewalls (intervention layer)

- **Critical path:** 1) Select CPF indicator → 2) Construct adversarial scenario with SOC/enterprise context → 3) Present to LLM via API (T=0.3, controlled system prompt) → 4) Three raters classify response (Green/Yellow/Red) → 5) Compute category scores and convergence indices → 6) If CI exceeds threshold, trigger Psychological Firewall intervention

- **Design tradeoffs:** Scenario realism vs. reproducibility (highly contextualized vs. abstracted scenarios), temperature setting (low T=0.3 for reproducibility vs. higher T for distributional tail risk), inter-rater adjudication (detailed rubrics improve κ but add latency vs. automated classification missing subtle patterns)

- **Failure signatures:** False negatives (Yellow responses that effectively bypass), category misapplication (human-specific indicators incorrectly scored), convergence underdetection (single-vector tests pass but multi-vector attacks succeed in deployment)

- **First 3 experiments:** 1) Baseline vulnerability profiling across model families with standardized scenarios, 2) Convergent state validation (authority + urgency + social proof combinations vs. single vectors), 3) Intervention prototype test (pre-prompt debiasing instruction to measure vulnerability reduction vs. capability degradation)

## Open Questions the Paper Calls Out

- **Open Question 1:** Do multiple psychological manipulation vectors combine to produce multiplicative risk amplification (Convergent State Amplification) in LLMs, or is the effect merely additive? The paper predicts multiplicative CI but requires empirical validation showing combined vector bypass rates significantly exceed summed individual probabilities.

- **Open Question 2:** To what extent do distinct LLM architectures and training paradigms (RLHF vs. chain-of-thought fine-tuning) exhibit heterogeneous vulnerability profiles? The paper acknowledges different architectures may produce substantially different vulnerability profiles, requiring comparative analysis across model families.

- **Open Question 3:** Can "Psychological Firewalls" utilizing mechanisms like verbalized sampling and cognitive debiasing effectively mitigate AVI without critically degrading agent utility? The paper proposes intervention mechanisms but lacks empirical data on whether safety layers introduce latency or refusal rates that render agents impractical for production use.

## Limitations
- The central thesis of anthropomorphic vulnerability inheritance remains empirically unproven within the paper
- Critical gaps include undefined weights for scoring functions, unspecified system prompts, and incomplete specification of the 100-indicator CPF taxonomy
- The mechanism linking psychological manipulation patterns in human text to exploitable model behavior is inferred rather than directly measured

## Confidence
- **High Confidence:** The theoretical framework mapping human psychological vulnerabilities to LLM testing surfaces is methodologically sound
- **Medium Confidence:** The mechanism of statistical pattern absorption and RLHF-induced typicality collapse is plausible but lacks direct causal evidence
- **Low Confidence:** The claim that LLMs inherit "pre-cognitive" vulnerabilities analogous to human psychological architecture overstates current mechanistic understanding

## Next Checks
1. **Empirical Vulnerability Profiling:** Execute SILICONPSYCHE across at least five model families using standardized scenarios to measure category-level vulnerability scores and test convergence index correlation with bypass rates.

2. **Mechanism Isolation Study:** Compare vulnerability profiles between models trained on human-generated vs. synthetic corpora, and between RLHF vs. Constitutional AI alignment methods to test whether anthropomorphic vulnerabilities are specifically inherited from human data.

3. **Intervention Efficacy Testing:** Implement and evaluate the proposed Psychological Firewall components against baseline vulnerability scores, measuring both vulnerability reduction and capability degradation on standard benchmarks to establish practical utility.