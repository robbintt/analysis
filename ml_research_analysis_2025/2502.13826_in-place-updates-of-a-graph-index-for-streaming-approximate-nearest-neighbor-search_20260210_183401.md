---
ver: rpa2
title: In-Place Updates of a Graph Index for Streaming Approximate Nearest Neighbor
  Search
arxiv_id: '2502.13826'
source_url: https://arxiv.org/abs/2502.13826
tags:
- graph
- index
- algorithm
- search
- recall
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents IP-DiskANN, the first algorithm for in-place
  updates of a proximity graph index for streaming approximate nearest neighbor search
  (ANNS). Traditional approaches struggle with deletions in singly-linked proximity
  graphs, often resorting to batch consolidation or expensive rebuilds.
---

# In-Place Updates of a Graph Index for Streaming Approximate Nearest Neighbor Search

## Quick Facts
- arXiv ID: 2502.13826
- Source URL: https://arxiv.org/abs/2502.13826
- Reference count: 40
- One-line primary result: First algorithm for in-place updates of proximity graph index handling deletions without batch rebuilds.

## Executive Summary
This paper introduces IP-DiskANN, an algorithm for in-place updates of proximity graph indices in streaming approximate nearest neighbor search (ANNS). Traditional graph-based ANNS methods like DiskANN and HNSW struggle with deletions because they only store outgoing edges, making in-neighbor identification costly. IP-DiskANN solves this by approximating in-neighbors during deletion using a re-search, then adding selective replacement edges to maintain graph navigability. Experimental results show IP-DiskANN maintains stable recall across various update patterns while offering better query throughput and update speed compared to batch consolidation and HNSW baselines.

## Method Summary
The method builds on DiskANN's graph structure and incrementally inserts new points using GreedySearch with beam search and RobustPrune with α=1.2. For deletions, IP-DiskANN performs an in-place update (Algorithm 5): it searches for the point to delete, approximates its in-neighbors from the search result, and adds c=3 replacement edges from affected neighbors to maintain connectivity. The algorithm uses a lightweight consolidation step (Algorithm 6) that periodically removes dangling edges when deletions exceed 20% of the index. Key parameters include R=64 for high-recall and R=32 for low-recall configurations, with search beam sizes l_b=l_s=128.

## Key Results
- IP-DiskANN maintains stable recall@10 across SlidingWindow, ExpirationTime, and Clustered runbooks while FreshDiskANN shows degradation
- Achieves 0.4-2.3 percentage points higher average recall compared to FreshDiskANN on MSTuring-10M
- Processes deletions significantly faster than both FreshDiskANN and HNSW on large datasets
- Offers better query throughput and update speed than HNSW while maintaining comparable recall

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** In-coming graph edges can be approximated by re-traversing the graph during deletion.
- **Mechanism:** The algorithm executes a `GreedySearch` for the vector $x_p$ to be deleted. It identifies nodes in the `Visited` list that have an active out-edge to $x_p$ and treats them as the in-neighbors ($N'_{in}(p)$). This avoids the memory overhead of storing a doubly-linked graph.
- **Core assumption:** The search path taken to find $p$ during deletion sufficiently overlaps with the paths used by other nodes to reach $p$ during normal queries.
- **Evidence anchors:**
  - [section 3.3]: "Therefore, it is reasonable to expect that if we search for $x_p$ once more (when we aim to delete $p$), the new visited list approximates the old one."
  - [Algorithm 5]: Defines the approximation $N'_{in}(p)$ based on the `Visited` set from `GreedySearch`.
  - [corpus]: Corpus evidence is weak; the related papers focus on GPU acceleration or compression rather than the topology of edge approximation in streaming graphs.
- **Break condition:** If the graph topology changes drastically between insertion and deletion, the search may fail to find the true in-neighbors, leading to orphaned subgraphs.

### Mechanism 2
- **Claim:** Graph navigability can be maintained by adding a small, fixed number of replacement edges ($c$) rather than a full cartesian product of neighbors.
- **Mechanism:** When deleting node $p$, for each approximate in-neighbor $z$, the algorithm connects $z$ to the closest $c$ points in the deletion search result. Similarly, it ensures out-neighbors of $p$ are connected back to the local community. This adds $O(cR)$ edges instead of $O(R^2)$.
- **Core assumption:** The "Candidates" (nearest neighbors to $x_p$) serve as viable proxies to maintain connectivity for both the incoming and outgoing traffic of the deleted node.
- **Evidence anchors:**
  - [section 3.3]: "Yet, too small a value would impact the navigability... We found $c=3$ a reasonable trade-off."
  - [Table 3b]: Empirical data shows recall saturates as $c$ increases, supporting the claim that full connectivity is unnecessary.
  - [corpus]: "CleANN" (corpus) discusses dynamism but does not validate this specific bounded-edge heuristic.
- **Break condition:** If $c$ is set too low in high-density clusters, the graph may suffer from "dead ends" or reduced recall (as seen in Table 3b where $c=1$ is lower than $c=3$).

### Mechanism 3
- **Claim:** Periodic background consolidation can be reduced to a simple memory scan without distance recomputation.
- **Mechanism:** Unlike FreshDiskANN, which merges segments and rebuilds edges, IP-DiskANN's consolidation (Algorithm 6) merely iterates through the index to remove edges pointing to deleted nodes. It relies on the in-place algorithm (Mechanism 2) to have already established valid alternative paths.
- **Core assumption:** Dangling edges (pointing to deleted nodes) are tolerable for short periods and do not critically degrade query latency between consolidation cycles.
- **Evidence anchors:**
  - [Algorithm 6]: Shows the consolidation logic is a set subtraction operation ($N_{out}(p) \setminus D$).
  - [section 3.3]: "...this offline consolidation procedure is extremely lightweight and does not do any distance calculations."
  - [corpus]: "The novel vector database" (corpus) highlights the cost of index updates, indirectly supporting the need for lighter consolidation, though not validating this specific algorithm.
- **Break condition:** If the consolidation threshold is set too high (e.g., waiting too long), the accumulation of dangling edges might degrade query throughput.

## Foundational Learning

**Concept:** Singly-linked Proximity Graphs
- **Why needed here:** The entire problem stems from the inability to find in-neighbors in a standard graph index (like DiskANN or HNSW) without storing them.
- **Quick check question:** If you delete a node with out-degree $R$, how many edges point *to* it (in-degree), and why don't we store them?

**Concept:** RobustPrune (Algorithm 3)
- **Why needed here:** This paper relies on the existing DiskANN pruning strategy to handle degree overflows after adding replacement edges. The parameter $\alpha > 1$ is critical for keeping "long-range" links.
- **Quick check question:** Why does the pruning algorithm keep some neighbors that are *not* the closest (i.e., why use $\alpha=1.2$ instead of $\alpha=1.0$)?

**Concept:** Streaming Workloads (Runbooks)
- **Why needed here:** The paper evaluates performance using "Runbooks" (SlidingWindow, Clustered). Understanding these patterns is necessary to interpret why the "Clustered" runbook is considered the most challenging.
- **Quick check question:** In a "SlidingWindow" scenario, why might "Clustered" deletions cause more damage to graph connectivity than random deletions?

## Architecture Onboarding

**Component map:**
Graph Index -> In-Place Updater -> Background Consolidator

**Critical path:** The `In-place Deletion` logic (Algorithm 5). This is where the new contribution lies. Specifically, the step where `Visited` nodes are checked for edges to the deleted node $p$ is the performance bottleneck.

**Design tradeoffs:**
- **Deletion Latency vs. Recall:** Increasing search width ($l_d$) and candidate list size ($k$) improves recall (finding better replacement edges) but slows down the deletion operation (Table 3c).
- **Consolidation Frequency:** Frequent consolidation cleans dangling edges faster but adds I/O overhead.

**Failure signatures:**
- **Silent Recall Drop:** If $c$ (edges per delete) is too low, the graph gradually fragments, leading to low recall despite the index appearing "healthy."
- **Throughput Spikes:** If consolidation is delayed too long, queries traverse many deleted edges, wasting CPU cycles.

**First 3 experiments:**
1. **Baseline Stability:** Implement the `SlidingWindow` runbook and plot recall over time (steps) to verify the algorithm maintains a stable line compared to FreshDiskANN.
2. **Parameter Sensitivity:** Run the ablation study on $c$ (edges per deletion) on the `Clustered` runbook to find the "break point" where recall drops significantly.
3. **Consolidation Overhead:** Measure the wall-clock time of Algorithm 6 vs. Algorithm 4 to quantify the "lightweight" claim made in the text.

## Open Questions the Paper Calls Out

**Open Question 1**
- **Question:** Can the in-place update strategy be effectively adapted to other graph-based indices, such as Hierarchical Navigable Small World (HNSW) or Navigating Spreading-out Graphs (NSG)?
- **Basis in paper:** [explicit] The authors state in Section 6 that while they focused on DiskANN, "It is possible our algorithm lends itself to be incorporated into other graph indices—a topic for future work."
- **Why unresolved:** The specific heuristic of using a search to approximate in-neighbors was tested on the singly-linked flat graph structure of DiskANN; hierarchical or complex DAG structures may require different edge replacement logic.
- **What evidence would resolve it:** An implementation of the in-place deletion algorithm for HNSW or NSG showing stable recall and throughput on standard benchmarks.

**Open Question 2**
- **Question:** How does IP-DiskANN perform under adversarial workloads specifically designed to stress the replacement operation?
- **Basis in paper:** [explicit] Section 7 lists "finding other adversarial workloads to test robustness, such as one for replacements" as a direction for future work.
- **Why unresolved:** The current experiments use sliding window, expiration time, and clustered runbooks, but do not explicitly target "replacements" as a distinct, high-frequency adversarial pattern.
- **What evidence would resolve it:** Evaluation results on a runbook with high-volume, spatially correlated replacements to test if the in-neighbor approximation fails or if degree bounds cause degradation.

**Open Question 3**
- **Question:** How does IP-DiskANN compare against state-of-the-art streaming methods for non-graph data structures, such as IVF and LSH?
- **Basis in paper:** [explicit] The authors conclude in Section 7 that future work should include "an experimental comparison of IP-DiskANN with the best streaming methods for other data structures, such as IVF and LSH."
- **Why unresolved:** The paper primarily compares against graph-based baselines (FreshDiskANN, HNSW) and mentions IVF-based systems (SPFresh) but defers the "apples-to-apples" comparison.
- **What evidence would resolve it:** Benchmark results comparing recall/latency trade-offs against methods like Ada-IVF or streaming LSH on the same normalized workloads.

**Open Question 4**
- **Question:** Does the continuous process of in-place edge replacement result in a graph topology with superior navigability compared to a static index built from scratch?
- **Basis in paper:** [inferred] In Section 4, the authors note the surprising result that "building from scratch" did not yield optimal recall compared to the dynamic version. They hypothesize that "deleting and adding new edges over time gradually improves the graph connectivity," but do not verify this structural change.
- **Why unresolved:** It is unclear if the observed improvement is a fundamental property of the dynamic maintenance or an artifact of the specific test runs.
- **What evidence would resolve it:** A topological analysis comparing the average path lengths, edge diversity, or search hop counts of a dynamic IP-DiskANN index versus a freshly built static index containing the exact same set of active vectors.

## Limitations
- The approximation of in-neighbors using re-search may fail under highly dynamic or adversarial insertion/deletion patterns where the search path diverges significantly from normal query paths.
- The fixed c=3 replacement edges may be insufficient for maintaining connectivity in high-density clusters, potentially causing silent recall degradation.
- The paper lacks exhaustive empirical evidence for worst-case scenarios, particularly rapid random insertions and deletions that could challenge the in-neighbor approximation mechanism.

## Confidence

**High:** Improved throughput and faster deletions compared to FreshDiskANN and HNSW are supported by clear experimental data across multiple datasets and runbooks.

**Medium:** The claim that c=3 is universally sufficient shows some sensitivity in the Clustered runbook ablation study, indicating potential limitations in high-density scenarios.

**Low:** Generalization of the approximation mechanism to other graph indices or distance metrics remains unverified and requires further validation.

## Next Checks

1. **Stress Test Approximation Robustness:** Implement a runbook with rapid, random insertions and deletions (e.g., delete 10% of current data every 10 steps) and measure recall stability over 100 steps to stress-test the in-neighbor approximation.

2. **Vary c Parameter:** Systematically vary c from 1 to 10 on the Clustered runbook and plot recall to identify the precise point where the trade-off between connectivity and edge overhead becomes detrimental.

3. **Compare Against HNSW Under Updates:** Replicate the query throughput and update latency comparison on a third, large-scale dataset (e.g., SIFT1M or Deep1B) to confirm the relative performance gains over HNSW hold under different data distributions.