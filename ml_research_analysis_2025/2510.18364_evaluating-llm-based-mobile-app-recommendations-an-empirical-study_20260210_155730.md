---
ver: rpa2
title: 'Evaluating LLM-Based Mobile App Recommendations: An Empirical Study'
arxiv_id: '2510.18364'
source_url: https://arxiv.org/abs/2510.18364
tags:
- ranking
- criteria
- recommendations
- llms
- mobile
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study examines how commercial LLMs generate mobile app recommendations,
  revealing that they use a broader set of criteria than traditional ASO metrics,
  often emphasizing qualitative and user-centric aspects. Through systematic experiments,
  the authors elicit a taxonomy of 16 ranking criteria and assess their impact and
  consistency across five LLMs.
---

# Evaluating LLM-Based Mobile App Recommendations: An Empirical Study

## Quick Facts
- arXiv ID: 2510.18364
- Source URL: https://arxiv.org/abs/2510.18364
- Reference count: 40
- This study examines how commercial LLMs generate mobile app recommendations, revealing that they use a broader set of criteria than traditional ASO metrics, often emphasizing qualitative and user-centric aspects.

## Executive Summary
This empirical study investigates how commercial large language models generate mobile app recommendations, revealing that they employ a broader set of criteria than traditional app store optimization metrics. Through systematic experiments with five different LLMs, the authors elicit a taxonomy of 16 ranking criteria and assess their impact and consistency across models. The research demonstrates that LLMs prioritize qualitative and user-centric aspects over technical specifications, with moderate overlap between blind and criteria-guided recommendations.

## Method Summary
The study employs a multi-agent service to query five commercial LLMs (GPT-4o, Claude-3.5-Sonnet, Gemini-1.5-Pro, Llama-3.1-70B-Instruct, and Mixtral-8x7B-Instruct) via API. Researchers designed feature-based prompt templates combining user requests with system persona patterns, generating blind recommendations and collecting raw ranking criteria from model outputs. Using Sentence-BERT embeddings and hierarchical clustering, they processed 5,000+ raw criteria into a 16-item taxonomy. The methodology includes systematic consistency measurements using Rank-Biased Overlap (RBO) and Jaccard similarity across multiple runs and models.

## Key Results
- Moderate overlap between blind and guided recommendations (average RBO ≈ 0.50)
- Criteria like "Feature Set" and "Ease of Use" have the strongest influence on rankings
- Top-ranked apps are stable within models, but cross-model agreement is limited
- Criteria reporting is less consistent than recommendations themselves

## Why This Works (Mechanism)

### Mechanism 1: Criterion-Guided Semantic Diversification
Explicitly specifying ranking criteria in prompts alters the recommendation list, often surfacing apps absent from blind recommendations. The prompt acts as a strong semantic signal that shifts the model's retrieval and ranking probabilities away from default popularity heuristics toward the requested attribute. This allows discovery of niche apps that optimize for the specific constraint. Core assumption: The LLM possesses accurate internal knowledge linking specific apps to requested qualitative attributes. Evidence: Results show criteria like "Feature Set" and "Ease of Use" having the strongest influence; recommendations conditioned on criteria frequently introduce novel apps.

### Mechanism 2: Hierarchical Consistency Decay
Recommendation stability is highest for top-ranked positions (k=3) and degrades significantly as list depth increases (k=20). Generative models assign high probability mass to a small set of "canonical" apps for a given query. As generation proceeds to lower ranks, the probability distribution flattens, leading to higher variance and stochastic selection of "long-tail" candidates. Core assumption: Users and evaluation metrics prioritize top-k results, allowing lower-ranked instability to go unnoticed. Evidence: Top-ranked apps are stable within models, but cross-model agreement is limited; recommendations tend to be consistently stable for top-ranking positions while variability increases substantially for deeper ranks.

### Mechanism 3: Post-Hoc Rationale Drift
The rationale (ranking criteria) reported by the LLM is less stable and consistent than the actual app recommendations. LLMs likely select apps based on opaque similarity or popularity metrics, then generate plausible natural language justifications post-hoc. The generation of these justifications is an open-ended language task with higher variance than the selection of entity names. Core assumption: There is a disconnect between the decision boundary used for selection and the vocabulary used for justification. Evidence: Criteria reporting is less consistent than recommendations themselves; ranking criteria consistency demonstrates systematically lower stability, reflecting intrinsic challenges in generating consistent qualitative justifications.

## Foundational Learning

- **Concept: Rank-Biased Overlap (RBO)**
  - Why needed: RBO is the primary metric used to measure similarity between ranked lists, weighting top positions higher to reflect user behavior
  - Quick check: If two lists share the same apps but in different orders, how does RBO distinguish between a swap at Rank 1 vs. Rank 20?

- **Concept: Zero-Shot Prompting**
  - Why needed: The study relies on the LLM's ability to perform recommendation tasks without fine-tuning, using only natural language instructions
  - Quick check: How does the "system prompt" (persona pattern) differ from the "user prompt" (feature request) in guiding the zero-shot behavior?

- **Concept: Taxonomy Elicitation via Embedding Clustering**
  - Why needed: The study processes 5,000+ raw criteria into a 16-item taxonomy, requiring understanding how semantic embeddings capture similarity between phrases like "User friendly" and "Ease of Use"
  - Quick check: Why use hierarchical clustering (Ward linkage) over simple keyword deduplication for consolidating ranking criteria?

## Architecture Onboarding

- **Component map:** Input Layer (Feature-based prompt templates) -> Execution Layer (Multi-agent service querying 5 LLMs) -> Processing Layer (JSON parser, Sentence-BERT, clustering) -> Evaluation Layer (RBO and Jaccard calculators)

- **Critical path:** 1) Define feature set (16 features across 8 categories) 2) Execute blind prompts to generate raw list + criteria 3) Process raw text → Embeddings → Clustering → Taxonomy (16 criteria) 4) Execute guided prompts using taxonomy 5) Compare Blind vs. Guided lists using RBO to determine criteria impact

- **Design tradeoffs:** Stability vs. Diversity (high temperature 0.7-1.0 kept for real user simulation), Recall vs. Precision (limit to k=20 to manage costs), Generalization vs. Specificity (taxonomy filters out domain-specific criteria)

- **Failure signatures:** Hallucination (generating non-existent app names), Schema Drift (failing to output valid JSON), Rationale Inconsistency (contradictory criteria across runs)

- **First 3 experiments:** 1) Run "blind" prompt for "Access to movies" 10 times on single model, calculate Jaccard similarity to verify top ranks are stable 2) Take "Security and Privacy" criterion, run guided prompt for communication app feature, check if privacy-focused apps rise vs. blind run 3) Extract 50 random raw criteria, embed them, check if they cluster into "Security" or "Ease of Use" categories

## Open Questions the Paper Calls Out

- Do LLMs' broader rationales lead to genuinely different app selections compared to app stores, or do they merely re-rank already prominent apps? The authors note a direct comparison with app store results is absent, which would reveal whether LLMs' broader rationales lead to genuinely different app selections.

- To what extent are apps surfaced by specific criteria (e.g., "Security and Privacy") objectively aligned with ground truth data for those criteria? The study does not assess correctness of app recommendations or align stated ranking criteria with measurable ground truth data.

- How does the moderate consistency and uneven criteria adherence observed in LLMs affect user trust and decision-making quality? The paper calls for future efforts to assess how users perceive the reliability of LLM-generated app rankings and whether grounded, criteria-driven prompting can improve recommendation quality and trust.

## Limitations
- Black-box nature of commercial LLMs prevents definitive tracing of recommendation mechanisms
- Taxonomy may miss domain-specific criteria relevant to certain app categories
- Focus on top-20 recommendations and stable API access limits generalizability to full-catalog exploration

## Confidence
- **High confidence:** Top-k recommendation stability within models, RBO-based similarity measurements, taxonomy construction methodology
- **Medium confidence:** Cross-model criterion influence patterns and qualitative ranking mechanisms
- **Low confidence:** Absolute accuracy of individual app recommendations (due to hallucination risk) and universal applicability of 16-criterion taxonomy across all app domains

## Next Checks
1. Hallucination audit: Cross-reference all recommended apps against official app stores to quantify false positives
2. Temperature sensitivity test: Repeat key experiments at temperature=0.0 to measure consistency vs. diversity tradeoffs
3. Category-specific taxonomy validation: Apply clustering methodology to single app category (e.g., games) to identify missing criteria unique to that domain