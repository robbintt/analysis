---
ver: rpa2
title: 'Tomographic Foundation Model -- FORCE: Flow-Oriented Reconstruction Conditioning
  Engine'
arxiv_id: '2506.02149'
source_url: https://arxiv.org/abs/2506.02149
tags:
- reconstruction
- data
- imaging
- image
- medical
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces FORCE (Flow-Oriented Reconstruction Conditioning
  Engine), a novel CT reconstruction framework based on Poisson flow generative models
  (PFGM++). The method addresses the challenge of CT image reconstruction under adverse
  conditions such as low-dose, sparse-view, and metal artifact corruption, where paired
  training data is scarce.
---

# Tomographic Foundation Model -- FORCE: Flow-Oriented Reconstruction Conditioning Engine

## Quick Facts
- arXiv ID: 2506.02149
- Source URL: https://arxiv.org/abs/2506.02149
- Authors: Wenjun Xia; Chuang Niu; Ge Wang
- Reference count: 40
- Primary result: FORCE achieves PSNR of 37.32 vs 36.17 baseline for low-dose CT reconstruction

## Executive Summary
This paper introduces FORCE (Flow-Oriented Reconstruction Conditioning Engine), a novel CT reconstruction framework based on Poisson flow generative models (PFGM++) that addresses challenging reconstruction scenarios including low-dose, sparse-view, and metal artifact reduction. The method uniquely integrates data fidelity constraints with a learned generative prior, enabling unsupervised reconstruction without requiring paired training data. By treating reconstruction as posterior sampling and leveraging the score function from PFGM++ to estimate log-prior gradients, FORCE guides sampling trajectories toward feasible solutions at the intersection of the data manifold and observation hyperplane.

The approach demonstrates superior performance across multiple reconstruction tasks, achieving higher PSNR (37.32 vs 36.17), SSIM, and lower FID scores compared to competing methods. FORCE's foundation-model-like architecture offers a generalizable solution that can be adapted to diverse clinical CT scenarios while preserving structural details and reducing artifacts. The method's key innovation lies in combining the stability and efficiency of PFGM++ with task-specific conditioning strategies, providing a unified framework for tackling various CT reconstruction challenges without retraining for each scenario.

## Method Summary
FORCE is an unsupervised CT reconstruction framework that integrates data fidelity constraints with a learned generative prior from PFGM++ to enable reconstruction without paired training data. The method treats CT reconstruction as a posterior sampling problem where the score function estimates log-prior gradients, replacing hand-crafted regularizers. Task-specific conditioning (RED for low-dose, OS-SART for sparse-view, sinogram inpainting for metal artifacts) guides sampling toward feasible solutions. The approach uses optimal initialization from a mid-level noisy point identified through FID analysis, balancing artifact suppression with structural preservation. Experiments on AAPM-Mayo and CT-MAR datasets demonstrate FORCE's effectiveness across low-dose (25% photons), sparse-view (96 projections), and metal artifact reduction scenarios.

## Key Results
- FORCE achieves PSNR of 37.32 vs 36.17 baseline for low-dose CT reconstruction
- The method demonstrates superior performance with higher SSIM and lower FID scores across multiple reconstruction tasks
- FORCE successfully handles diverse clinical CT scenarios including low-dose, sparse-view, and metal artifact reduction without requiring separate training for each task

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Integrating data fidelity constraints with a learned generative prior enables unsupervised CT reconstruction without paired training data.
- Mechanism: FORCE treats CT reconstruction as a posterior sampling problem where the score function from PFGM++ estimates the gradient of the log-prior distribution, replacing hand-crafted regularizers. Data fidelity constraints (Hx = p) are injected during sampling to guide trajectories toward feasible solutions at the intersection of the data manifold and observation hyperplane.
- Core assumption: The learned generative prior adequately captures the distribution of valid CT images, and conditioning can constrain sampling without requiring ground-truth pairs.
- Evidence anchors:
  - [abstract] "integrates data fidelity constraints with a learned generative prior, enabling unsupervised reconstruction by guiding sampling trajectories toward feasible solutions"
  - [section II.C] "the score function in DDPM, which estimates the gradient of the log-prior distribution, can be viewed as a learned counterpart to hand-crafted regularizers"
  - [corpus] D2IP paper demonstrates similar unsupervised tomographic reconstruction using Deep Image Prior, supporting the training-data-free approach; corpus evidence for this specific PFGM++ conditioning approach is limited
- Break condition: If the generative prior is trained on data that poorly matches the target distribution (e.g., different anatomy or scanner protocols), conditioning may not prevent hallucination or may converge to invalid solutions.

### Mechanism 2
- Claim: PFGM++ provides more stable and efficient sampling than diffusion models for CT reconstruction due to its deterministic ODE formulation.
- Mechanism: PFGM++ treats data as charged particles in an augmented (N+D)-dimensional space, where the forward process follows electric field force lines to a spherical prior. Unlike diffusion's stochastic differential equation (SDE), PFGM++ uses an ordinary differential equation (ODE), enabling larger step sizes and fewer sampling iterations while maintaining trajectory stability.
- Core assumption: The augmented dimensionality parameter D can be tuned to balance between diffusion-like behavior (large D) and Poisson flow characteristics (smaller D).
- Evidence anchors:
  - [abstract] "based on Poisson flow generative models (PFGM++)"
  - [section II.A] "PFGM is based on electrostatics and generates samples by following deterministic trajectories defined by a Poisson equation... results in faster, more stable sampling"
  - [corpus] SDUM paper addresses similar multi-protocol reconstruction challenges but uses protocol-specific models; corpus does not provide comparative evidence on PFGM++ vs diffusion efficiency
- Break condition: If the noise schedule or dimensionality parameter D is poorly chosen, the ODE may diverge or require excessive steps to converge.

### Mechanism 3
- Claim: Optimal initialization from a mid-level noisy point (Phase II alignment zone) balances artifact suppression with structural preservation.
- Mechanism: Rather than starting from pure noise (standard in generation tasks), FORCE initializes from a point where added noise masks input artifacts but preserves structural signal. FID analysis between low-dose and normal-dose distributions reveals three phases: artifact-dominant (I), alignment (II), and noise-dominant (III). Phase II provides the optimal trade-off.
- Core assumption: The corrupted input and clean target distributions converge under moderate noise injection, creating a shared region for initializing reverse sampling.
- Evidence anchors:
  - [section II.C] "Starting Sampling Time" describes the FID curve with three phases
  - [section II.C, Fig. 3] shows Phase II as the "Alignment Zone" ideal for sampling
  - [corpus] No direct corpus evidence for this initialization strategy; related work on deep priors (D2IP) uses different initialization approaches
- Break condition: If the noise level at initialization is too low (Phase I), artifacts persist; if too high (Phase III), anatomical structure is lost and sampling becomes inefficient.

## Foundational Learning

- Concept: **Model-Based Iterative Reconstruction (MBIR)**
  - Why needed here: FORCE builds on the MBIR Bayesian framework where reconstruction optimizes data fidelity plus a prior term. Understanding Eq. 9 (min_x ½‖Hx−p‖² + λR(x)) is essential to see how the score function replaces R(x).
  - Quick check question: Can you explain why the score function ∇_x log p(x; σ) can substitute for a hand-crafted regularizer in MBIR?

- Concept: **Score-Based Generative Models**
  - Why needed here: Both diffusion and PFGM++ train networks to estimate score functions (gradients of log-density). The denoising objective L(θ) = E‖f_θ(y+σε;σ)−y‖² must be understood to implement training (Algorithm 1).
  - Quick check question: How does the score function relate to the denoising network output via Eq. 6?

- Concept: **Poisson Flow in Augmented Space**
  - Why needed here: PFGM++ operates in an (N+D)-dimensional space where data points have augmented coordinates x̃ = (x, z). The radius r = σ√D maps between EDM noise scale and PFGM radial coordinate.
  - Quick check question: Why does setting r = σ√D ensure consistency between EDM and PFGM++ formulations?

## Architecture Onboarding

- Component map: Denoising Network f_θ -> Conditioning Module -> TV Regularizer -> Momentum Accelerator -> Forward Operator H
- Critical path:
  1. Initialize x_0 from corrupted reconstruction (e.g., FBP) plus sampled noise at Phase II level
  2. For each sampling step i: Apply conditioning → Denoise with f_θ → Apply TV+momentum → Update along Poisson field direction
  3. Iterate until convergence to feasible reconstruction
- Design tradeoffs:
  - **Conditioning strength vs. prior strength**: Aggressive conditioning (e.g., POCS) risks converging to noisy solutions; weak conditioning risks hallucination
  - **D parameter**: Large D approaches diffusion behavior (more stable but slower); small D accelerates sampling but may reduce quality
  - **TV weight λ**: High λ suppresses noise but may over-smooth; low λ preserves detail but may retain artifacts
- Failure signatures:
  - **Streaking artifacts persist**: Conditioning too weak or D parameter too small
  - **Over-smoothing/blurring**: TV weight too high or conditioning too aggressive
  - **Hallucinated structures**: Prior trained on mismatched data distribution or insufficient conditioning
  - **Slow convergence**: Initialization in Phase I or III rather than Phase II
- First 3 experiments:
  1. **Baseline PFGM++ training**: Train denoising network on clean CT images using Algorithm 1; validate on held-out images by adding/removing noise to confirm score estimation quality
  2. **Ablation on initialization noise level**: Run reconstruction with varying initial σ values across Phases I, II, III; plot PSNR/SSIM vs. σ to empirically identify optimal alignment zone
  3. **Single-task conditioning validation**: Implement RED conditioning for low-dose CT only; compare against FBP, BM3D, and CycleGAN baselines on AAPM-Mayo dataset before extending to multi-task

## Open Questions the Paper Calls Out
None

## Limitations
- The paper underspecifies critical implementation details including network architecture, exact hyperparameter values (λ, η, D), and optimal noise level for Phase II initialization
- While the method claims superior performance, comparative ablation studies on conditioning strategy versus alternative priors are absent
- The computational efficiency advantages over diffusion models are theoretical, as runtime comparisons are not provided

## Confidence
- **High confidence**: The core mechanism of integrating data fidelity with generative priors for unsupervised reconstruction is well-supported by the mathematical framework and demonstrates empirical improvements over baselines
- **Medium confidence**: The PFGM++ formulation and its advantages over diffusion models are theoretically sound, but practical efficiency gains remain to be validated through runtime analysis
- **Medium confidence**: The Phase II initialization strategy is novel and intuitively appealing, but lacks ablation studies and the optimal noise parameters are not explicitly specified

## Next Checks
1. **Ablation study on initialization strategy**: Systematically test reconstruction quality across all three FID phases (I, II, III) with multiple noise levels to empirically validate the optimal Phase II initialization claim
2. **Conditioning strength sensitivity analysis**: Evaluate reconstruction performance across a range of conditioning strengths (RED, OS-SART, MAR parameters) to quantify the trade-off between data fidelity and prior influence
3. **Runtime efficiency benchmarking**: Measure wall-clock time for FORCE versus diffusion-based CT reconstruction methods across identical hardware to validate the claimed computational advantages of PFGM++