---
ver: rpa2
title: 'SQUARE: Semantic Query-Augmented Fusion and Efficient Batch Reranking for
  Training-free Zero-Shot Composed Image Retrieval'
arxiv_id: '2509.26330'
source_url: https://arxiv.org/abs/2509.26330
tags:
- image
- retrieval
- square
- reranking
- images
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents SQUARE, a training-free framework for zero-shot
  composed image retrieval (ZS-CIR). The key innovation is a two-stage approach that
  leverages Multimodal Large Language Models (MLLMs) to enhance retrieval accuracy
  without task-specific training.
---

# SQUARE: Semantic Query-Augmented Fusion and Efficient Batch Reranking for Training-free Zero-Shot Composed Image Retrieval

## Quick Facts
- arXiv ID: 2509.26330
- Source URL: https://arxiv.org/abs/2509.26330
- Reference count: 36
- Outperforms state-of-the-art ZS-CIR methods across multiple benchmarks using a training-free approach

## Executive Summary
SQUARE introduces a novel training-free framework for zero-shot composed image retrieval that leverages Multimodal Large Language Models (MLLMs) to enhance retrieval accuracy. The method operates in two stages: first, Semantic Query-Augmented Fusion (SQAF) enriches traditional VLM-based queries with MLLM-generated target image captions to capture high-level semantic intent; second, Efficient Batch Reranking (EBR) uses the MLLM to jointly reason across multiple candidate images arranged in a labeled grid. Experiments on four benchmarks demonstrate that SQUARE achieves state-of-the-art performance without requiring task-specific training or annotated triplets, particularly excelling when using lightweight CLIP models.

## Method Summary
SQUARE addresses zero-shot composed image retrieval through a two-stage training-free approach. The first stage, Semantic Query-Augmented Fusion (SQAF), combines VLM embeddings of the reference image and modification text with MLLM-generated target captions. The composed query embedding is computed as q = (1-β)·q_vlm + β·E_txt(Tt), where q_vlm = (1-α)·E_img(Ir) + α·E_txt(Tm). The second stage, Efficient Batch Reranking (EBR), presents the top-16 candidate images as a 4×4 annotated grid to the MLLM, which performs joint visual-semantic reasoning to output a reranked list of indices. The final ranking combines the reranked top-16 with remaining candidates in their original order.

## Key Results
- Achieves state-of-the-art performance on CIRR, CIRCO, FashionIQ, and GeneCIS benchmarks
- Outperforms previous methods across different CLIP backbones (ViT-B/32, ViT-L/14, ViT-G/14)
- Particularly strong results with lightweight models, maintaining high accuracy without task-specific training
- EBR significantly improves retrieval accuracy by enabling joint reasoning across multiple candidates

## Why This Works (Mechanism)
SQUARE works by addressing two key limitations in traditional ZS-CIR approaches. First, SQAF overcomes the semantic gap between low-level visual features and high-level semantic concepts by incorporating MLLM-generated target captions that capture abstract concepts like "elegant" or "modern" which are difficult for VLMs to represent directly. Second, EBR leverages the MLLM's ability to perform cross-image comparison and reasoning across multiple candidates simultaneously, enabling more nuanced judgments about which candidates best satisfy the composition constraint than single-image ranking allows.

## Foundational Learning
- **Composed Image Retrieval (CIR)**: A retrieval task where the query consists of a reference image and a text modification, requiring the model to understand both visual similarity and semantic changes. Needed to contextualize the problem SQUARE addresses; quick check: understand how CIR differs from standard image retrieval.
- **Zero-Shot Learning**: Model performance without task-specific training or fine-tuning on the target domain. Critical for SQUARE's training-free approach; quick check: verify no model parameters are updated during SQUARE's pipeline.
- **Multimodal Large Language Models (MLLMs)**: AI models capable of processing and reasoning across both text and images. Core to SQUARE's semantic enhancement and reranking; quick check: understand GPT-4o's visual reasoning capabilities.
- **Cross-Modal Embeddings**: Vector representations that capture relationships between different modalities (text and images). Essential for the initial candidate retrieval step; quick check: understand how CLIP creates unified embedding space.
- **Semantic Query Augmentation**: The process of enriching retrieval queries with higher-level semantic information. Central to SQAF's effectiveness; quick check: compare retrieval results with and without caption augmentation.
- **Batch Reranking**: Reordering candidate results by considering multiple items simultaneously rather than independently. Key innovation in EBR; quick check: understand how joint reasoning across candidates improves selection.

## Architecture Onboarding

Component Map:
Reference Image + Modification Text -> CLIP Encoders -> VLM Embeddings -> SQAF -> Initial Ranking -> Top-16 Candidates -> EBR Grid Formation -> MLLM Reranking -> Final Ranking

Critical Path:
The critical path is: CLIP encoding of gallery images (offline) → SQAF generation and ranking → EBR grid creation and MLLM reasoning → final reranked output. Each stage must complete successfully for accurate retrieval.

Design Tradeoffs:
- MLLM dependency: High accuracy but requires API access and computational resources vs. fully self-contained VLM approaches
- Grid size vs. reasoning capacity: 4×4 grid balances MLLM's visual reasoning ability with candidate coverage vs. larger grids that overwhelm the model
- Caption generation quality vs. latency: More detailed captions improve semantic guidance but increase MLLM processing time

Failure Signatures:
- Incomplete MLLM index output: MLLM returns fewer than 16 indices, requiring fallback to original ordering
- Grid misinterpretation: Small thumbnail size or annotation overlap causes MLLM to misidentify images
- Caption hallucination: MLLM generates details not present in reference or inconsistent with modification

First Experiments:
1. Run SQAF-only baseline (β=0) to measure impact of semantic caption augmentation
2. Test EBR with 3×3 grid (top-9 candidates) to verify reranking effectiveness at smaller scale
3. Compare lightweight (ViT-B/32) vs. heavy (ViT-G/14) CLIP backbone performance to validate lightweight efficiency claims

## Open Questions the Paper Calls Out
**Open Question 1**: What architectural or prompting strategies could enable effective MLLM-based reranking with larger candidate sets (e.g., beyond 16 images per grid)? The paper shows reranking performance degrades significantly as grid size increases beyond 4×4, with 6×6 grids performing worse than no reranking at all, likely due to increased visual complexity overwhelming the MLLM's reasoning capacity. Experiments with hierarchical reranking, attention-based candidate grouping, or multi-pass selection strategies could address this limitation.

**Open Question 2**: How can MLLM-based CIR methods be improved to handle compound multi-attribute modifications and spatial reasoning queries? The paper demonstrates failure cases where the method struggles with spatial reasoning (e.g., "dog's head closer to camera") and compound attribute modifications (e.g., simultaneously changing color, neckline, and style). Targeted evaluations on compound modification subsets, ablation studies with decomposition-based prompting, or comparison with methods explicitly designed for multi-attribute reasoning could provide solutions.

**Open Question 3**: What specific capabilities distinguish highly effective MLLM rerankers from less effective ones, and can these be transferred to smaller models? The paper shows ChatGPT-4.1 substantially outperforms ChatGPT-4o for reranking, despite both being capable models, indicating that general MLLM capability does not directly translate to reranking effectiveness. Probing studies comparing cross-image comparison abilities, analysis of attention patterns during reranking, or fine-tuning experiments targeting specific reasoning skills could identify the distinguishing factors.

## Limitations
- Requires GPT-4o API access and computational resources for MLLM-based caption generation and reranking
- Performance depends on the quality and reliability of MLLM outputs, which can vary
- Limited to 16 candidates for reranking due to MLLM's capacity constraints for joint visual reasoning
- Specific few-shot examples and exact prompt templates are not fully disclosed in the paper

## Confidence
High: Relative improvements over baselines are well-established through ablation studies and comparisons across multiple benchmarks
Medium: Absolute performance values depend on implementation details of MLLM prompts and preprocessing that are not fully specified
Low: The paper does not address scalability to larger candidate sets or robustness to caption generation errors

## Next Checks
1. Verify SQAF caption generation by comparing MLLM outputs with and without the few-shot examples in the prompt template
2. Test EBR grid presentation by running a small-scale experiment with known ground truth to check if the 4×4 layout affects ranking accuracy
3. Replicate the lightweight model results (ViT-B/32) to confirm that performance gains are maintained with smaller CLIP backbones