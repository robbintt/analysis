---
ver: rpa2
title: 'CreBench: Human-Aligned Creativity Evaluation from Idea to Process to Product'
arxiv_id: '2511.13626'
source_url: https://arxiv.org/abs/2511.13626
tags:
- creative
- creativity
- idea
- evaluation
- visual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces CreBench, a comprehensive benchmark designed\
  \ to evaluate multimodal large language models (MLLMs) on human-aligned creativity\
  \ assessment. The benchmark covers creativity across three dimensions\u2014idea,\
  \ process, and product\u2014with 12 fine-grained indicators."
---

# CreBench: Human-Aligned Creativity Evaluation from Idea to Process to Product

## Quick Facts
- arXiv ID: 2511.13626
- Source URL: https://arxiv.org/abs/2511.13626
- Reference count: 13
- This paper introduces CreBench, a comprehensive benchmark designed to evaluate multimodal large language models (MLLMs) on human-aligned creativity assessment across 12 fine-grained indicators spanning three dimensions.

## Executive Summary
This paper introduces CreBench, a comprehensive benchmark designed to evaluate multimodal large language models (MLLMs) on human-aligned creativity assessment. The benchmark covers creativity across three dimensions—idea, process, and product—with 12 fine-grained indicators. To operationalize this framework, the authors construct CreMIT, a multimodal instruction-tuning dataset containing 2.2K creative instances, 79.2K expert annotations, and 4.7M instruction-following samples across six question types. Using this dataset, they fine-tune LLaVA-1.5 to create CreExpert, a creativity evaluation expert model. Extensive experiments show that CreExpert significantly outperforms state-of-the-art MLLMs, including GPT-4V and Gemini-Pro-Vision, achieving an overall score of 65.50% versus 29.27% and 27.78%, respectively. The improvements are most pronounced in originality and immersion/divergence, demonstrating strong alignment with human creativity judgments. The work provides both a novel benchmark and a capable model for assessing creativity in open-ended, multimodal tasks.

## Method Summary
The authors construct CreBench, a comprehensive benchmark for evaluating MLLMs on human-aligned creativity assessment across three dimensions: Creative Idea (Originality, Appropriateness), Creative Process (Immersion/Preparation, Divergence, Structuring, Evaluation, Elaboration), and Creative Product (Effectiveness, Aesthetic, Novelty, Manufacturability, Systemic Complexity). To operationalize this framework, they create CreMIT, a multimodal instruction-tuning dataset with 2.2K creative instances, 79.2K expert annotations, and 4.7M instruction-following samples across six question types. Using this dataset, they fine-tune LLaVA-1.5 to create CreExpert, a creativity evaluation expert model. The evaluation measures Pearson Correlation Coefficient between model predictions and human expert ratings across all 12 dimensions. The model architecture uses CLIP-ViT-L14 vision encoder (336×336 input, 576 tokens), a two-layer MLP projection, and Vicuna-v1.5 language decoder, with the vision encoder frozen and only the projection module and LLM fine-tuned using LoRA within the LLaMA-Factory framework.

## Key Results
- CreExpert achieves an overall Pearson correlation score of 65.50% compared to 29.27% for GPT-4V and 27.78% for Gemini-Pro-Vision
- Largest improvements observed in originality (28.37% → 77.93%) and immersion/divergence (18.95% → 76.60%) dimensions
- CreExpert shows significant gains in evaluating creative process indicators (28.37% → 61.45%) and creative product indicators (32.73% → 66.50%)

## Why This Works (Mechanism)
The approach works by aligning model evaluation with human creativity judgments through a comprehensive multi-dimensional framework. By constructing a large-scale instruction-tuning dataset (CreMIT) with expert annotations across 12 fine-grained creativity indicators, the model learns to assess creativity in a way that mirrors human evaluation criteria. The fine-tuning process enables the model to understand the nuances of creative tasks across idea generation, process evaluation, and final product assessment. The significant performance improvements, particularly in originality and immersion/divergence, demonstrate that the model has successfully internalized the human-aligned creativity evaluation criteria through exposure to expert-annotated examples.

## Foundational Learning
- **Multimodal Instruction Tuning**: Why needed: Enables models to understand and respond to diverse creative assessment tasks. Quick check: Verify dataset contains all 6 question types (Reasoning, What, How, Why, Y/N, MCQ) with balanced representation.
- **Human-Aligned Evaluation Framework**: Why needed: Ensures model assessments match human expert judgments. Quick check: Confirm 12 indicators are properly distributed across the three creativity dimensions.
- **LoRA Fine-tuning**: Why needed: Efficiently adapts large models to specialized creativity evaluation tasks. Quick check: Validate that only projection and LLM layers are updated during training.
- **Pearson Correlation as Metric**: Why needed: Quantifies alignment between model predictions and human expert ratings. Quick check: Ensure correlation is calculated correctly across all 12 dimensions.
- **Multimodal Creative Tasks**: Why needed: Provides diverse context for creativity evaluation. Quick check: Verify all 4 visual design tasks are represented in both training and evaluation sets.
- **Expert Annotation Process**: Why needed: Establishes ground truth for model training and evaluation. Quick check: Confirm expert annotations cover all 12 creativity indicators with sufficient inter-rater reliability.

## Architecture Onboarding

**Component Map**: LLaVA-1.5 (CLIP-ViT-L14 vision encoder -> MLP projection -> Vicuna-v1.5 LLM) -> LoRA adapters -> CreExpert model

**Critical Path**: Input image/text -> CLIP encoder (frozen) -> MLP projection (fine-tuned) -> Vicuna decoder (fine-tuned) -> 1-5 creativity score output

**Design Tradeoffs**: Uses frozen CLIP vision encoder to preserve multimodal understanding while fine-tuning only projection and LLM layers for efficiency. This balances computational cost with task-specific adaptation. The choice of LoRA enables parameter-efficient fine-tuning compared to full model training.

**Failure Signatures**: Poor performance on Creative Product dimensions (baseline scores 7-32%) indicates potential issues with understanding final product quality. Uneven performance across question types or tasks suggests training data imbalance or prompt format issues.

**Three First Experiments**:
1. Verify basic model functionality by evaluating on a single held-out instance and checking output format matches expected 1-5 score range
2. Test per-dimension performance to identify which creativity indicators show strongest/weakest alignment
3. Conduct task-specific evaluation to determine if performance varies significantly across the four visual design tasks

## Open Questions the Paper Calls Out
None

## Limitations
- The evaluation relies on GPT-4o for converting expert annotations into instruction-following samples, introducing potential model bias into the training data
- The benchmark focuses on visual design tasks, which may limit generalizability to other creative domains
- The paper reports Pearson correlations but does not provide variance estimates or statistical significance tests across the 12 dimensions

## Confidence
- **High confidence**: The benchmark construction methodology (three dimensions with 12 fine-grained indicators) and the overall performance gap between CreExpert and baseline models (65.50% vs. 29.27% and 27.78%) are well-documented and reproducible
- **Medium confidence**: The specific improvements in originality and immersion/divergence dimensions are supported, but without statistical significance testing, the practical importance of these gains remains uncertain
- **Low confidence**: The model's ability to generalize beyond the four visual design tasks used in training, given the domain-specific nature of the benchmark

## Next Checks
1. Conduct statistical significance testing (e.g., bootstrap confidence intervals) on Pearson correlation differences between CreExpert and baselines across all 12 dimensions to determine if improvements are meaningful
2. Evaluate CreExpert on a held-out set of creative tasks from different domains (e.g., writing, music composition) to assess cross-domain generalization beyond visual design
3. Perform ablation studies on the 4.7M instruction-following samples to quantify how much of the performance gain comes from the fine-tuning data versus the original LLaVA-1.5 architecture