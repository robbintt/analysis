---
ver: rpa2
title: One-Sided Matrix Completion from Ultra-Sparse Samples
arxiv_id: '2601.12213'
source_url: https://arxiv.org/abs/2601.12213
tags:
- matrix
- entries
- equation
- error
- completion
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper revisits matrix completion under an ultra-sparse sampling
  regime, where each entry of an unknown matrix is observed independently with probability
  $p = C/d$ for some fixed constant $C \ge 2$. In this regime, exact recovery of the
  entire matrix is impossible, so the focus shifts to estimating the row span of the
  matrix, equivalently the second-moment matrix $T = M^\top M / n$.
---

# One-Sided Matrix Completion from Ultra-Sparse Samples

## Quick Facts
- **arXiv ID**: 2601.12213
- **Source URL**: https://arxiv.org/abs/2601.12213
- **Reference count**: 40
- **Primary result**: Proposes a Hájek estimator for unbiased second-moment matrix recovery in ultra-sparse sampling regime, with gradient descent optimization

## Executive Summary
This paper addresses matrix completion in the ultra-sparse sampling regime where each entry is observed independently with probability $p = C/d$ for a constant $C \ge 2$. Unlike standard matrix completion, exact recovery is impossible here, so the focus shifts to estimating the row span of the matrix via the second-moment matrix $T = M^\top M / n$. The authors propose a Hájek estimator that normalizes each observed entry by its empirical frequency, followed by gradient descent to impute missing entries of $T$. Under a rank-$r$ factor model with incoherence conditions, they prove that if the number of rows $n$ is sufficiently large relative to $d$, $r$, and $C$, any local minimum of the gradient-descent objective recovers $T$ with small error. Experiments show the method outperforms existing baselines, reducing bias by 88% on MovieLens data and 59% error in recovering $T$ on an Amazon reviews dataset.

## Method Summary
The method constructs an unbiased estimator of the second-moment matrix $T$ by normalizing observed entries with their empirical frequencies (Hájek estimator), then applies gradient descent on a regularized objective to recover the full matrix. The approach handles the ultra-sparse regime where standard matrix completion fails by targeting row-span recovery rather than full matrix reconstruction. The optimization includes an incoherence regularizer to prevent spurious local minima and ensure theoretical convergence guarantees.

## Key Results
- Hájek estimator achieves finite-sample unbiasedness with 99% reduction in bias compared to Horvitz-Thompson estimator
- Under rank-$r$ factor model, recovers $T$ with error $\epsilon^2$ when $n \geq O(dr^5 \epsilon^{-2} C^{-2} \log d)$
- Outperforms baselines by 88% bias reduction on MovieLens data and 59% error reduction on Amazon reviews dataset

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The Hájek estimator provides finite-sample unbiased estimation of the second-moment matrix T on the observed index set Ω
- Mechanism: Each entry $(i,j)$ in the empirical second-moment matrix is normalized by its observed frequency $(I^T I)_{i,j}$ rather than the true sampling probability, creating a ratio estimator that is unbiased because conditioned on $(I^T I)_{i,j} = k$, the $k$ observed pairs are uniformly random among all $(n \text{ choose } k)$ possibilities
- Core assumption: Each entry of M is observed independently with probability $p$, and observation patterns are symmetric across rows
- Evidence anchors: Proves $E[\bar{T}_{i,j} | (i,j) \in \Omega] = T_{i,j}$ for any $1 \leq i,j \leq d$; related work on non-random missingness addresses dependent missingness but doesn't achieve finite-sample unbiasedness
- Break condition: Fails if observation probabilities vary non-uniformly across entries in ways that correlate with underlying values

### Mechanism 2
- Claim: The Hájek estimator achieves lower variance than the Horvitz-Thompson (HT) estimator, particularly for off-diagonal entries under ultra-sparsity
- Mechanism: HT divides by expected count $np^2$ (very small under $p = C/d$), amplifying variance. Hájek divides by actual count $(I^T I)_{i,j}$, which self-normalizes and reduces variance through denominator reduction by factor $np^2/(1-p^2)$ and negative second term in variance decomposition
- Core assumption: Sampling probability $p$ is small enough that $np^2$ is bounded by $O(\log d)$
- Evidence anchors: Derives variance approximations showing $\text{Var}(\bar{T}_{i,i}) \approx (1-p)/(np) \Sigma M^2_{k,i} - (1-p)/(np) T^2_{i,i}$, strictly less than HT variance
- Break condition: When $p$ is large ($p > 0.5$), variance advantage diminishes as both estimators converge

### Mechanism 3
- Claim: Gradient descent on the regularized loss converges to an approximately global minimizer with bounded error
- Mechanism: The incoherence regularizer $R(X) = \Sigma_i (||X_i|| - \alpha)^4_+$ penalizes large row norms, ensuring $X$ satisfies incoherence condition and eliminating spurious local minima. Weighted projection $P_\Omega$ handles non-random missingness pattern by reweighting diagonal vs off-diagonal entries differently
- Core assumption: Rows of $M$ follow rank-$r$ factor model satisfying incoherence condition $\mu(U) = (d/r) \max_i ||U_{ei}||^2 / ||U||^2_F$ is bounded
- Evidence anchors: Proves any local minimizer satisfies $||XX^T - T||^2_F \leq \epsilon^2$ with high probability when $n \geq O(dr^5 \epsilon^{-2} C^{-2} \log d)$
- Break condition: Fails if true rank $r$ is misspecified, incoherence condition is violated, or $n$ is too small relative to $d$

## Foundational Learning

- **Concept**: Hájek vs Horvitz-Thompson estimators in survey sampling
  - Why needed here: The paper's core contribution builds on classical survey sampling theory. Understanding that HT divides by true probability while Hájek divides by empirical frequency is essential
  - Quick check question: Given observations $x_1, x_2, ..., x_k$ from $n$ total possible observations with sampling probability $p$, what is the Hájek estimate of the population mean?

- **Concept**: Incoherence condition in matrix completion
  - Why needed here: Theoretical guarantees require that row norms of low-rank factors are balanced. Understanding $\mu = (d/r) \max ||U_{ei}||^2$ helps interpret why regularizer is necessary
  - Quick check question: If $U \in \mathbb{R}^{d \times r}$ has rank $r$ with equal row norms, what is the coherence $\mu$?

- **Concept**: Second-moment matrix $T = M^T M/n$ as row-space summary
  - Why needed here: Under ultra-sparsity, full matrix recovery is impossible. Target shifts to recovering $T$, which captures row-span structure useful for downstream tasks
  - Quick check question: For tall matrix $M \in \mathbb{R}^{n \times d}$ with $n \gg d$, what information does $T = M^T M/n$ preserve that is sufficient for subspace recovery?

## Architecture Onboarding

- **Component map**:
  1. Observation Mask $I$ -> Co-occurrence Counter $I^T I$ -> Hájek Estimator $\bar{T}$ -> Weighted Projection $P_\Omega$ -> Factor Matrix $X$ -> Incoherence Regularizer $R(X)$ -> Gradient Descent

- **Critical path**:
  1. Construct $I$ from observed entries of $M$
  2. Compute $I^T I$ and identify $\Omega$ (non-zero entries)
  3. Compute $M^T M$ on observed entries only
  4. Apply Hájek: $\bar{T} = (M^T M) \oslash (I^T I)$ on $\Omega$ (element-wise division)
  5. Initialize $X_0 \sim N(0, d^{-1})$
  6. Iterate: $X_{t+1} = X_t - \eta \nabla \ell(X_t)$
  7. Output: $\hat{T}$ (observed from $\bar{T}$) combined with $X_t X_t^T$ (imputed for entries outside $\Omega$)

- **Design tradeoffs**:
  - **Rank selection**: Must specify $r$ in advance; cross-validation needed. Higher $r$ risks overfitting under sparsity
  - **Regularization strength $\lambda$**: Controls incoherence enforcement. Too small → spurious local minima; too large → underfitting
  - **Threshold $\alpha$ in $R(X)$**: Roughly corresponds to row norm magnitude
  - **Diagonal reweighting factor $q$**: Must match expected off-diagonal observation probability $q \approx np^2$

- **Failure signatures**:
  - **Zero denominator**: If $(I^T I)_{i,j} = 0$ for off-diagonal entries (very rare when $n$ is large), skip that entry or add small constant
  - **Exploding gradients**: If $\lambda$ is too small and $X$ has highly imbalanced row norms, gradients may explode
  - **Slow convergence**: Under extreme sparsity ($C=2$), GD may require many iterations
  - **Rank misspecification**: If true rank $> r$, error plateaus at non-zero level

- **First 3 experiments**:
  1. **Sanity check on synthetic data**: Generate $M$ from $N(1/\sqrt{d}, 1/d)$, truncate to rank $r=10$, sample with $p=C/d$ for $C=2,10,50$. Verify $\bar{T}$ has lower bias than HT estimator on $\Omega$ (should see ~99% reduction)
  2. **Sample complexity scaling**: Fix $d=1000$, $r=10$, vary $n$ from 1000 to 10000 with $C=2$. Plot recovery error $||XX^T - T||_F$
  3. **Real-world validation on MovieLens**: Apply to MovieLens-20M with 80-20 train-test split. Compare $T$ recovery error against alternating-GD and softImpute-ALS

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the sample complexity bound of $O(d \cdot r^5 \cdot \epsilon^{-2} \cdot C^{-2} \cdot \log d)$ be improved to reduce the high dependence on the rank $r$ of the latent factor model?
- Basis in paper: The conclusion states: "our current sample-complexity bound exhibits a high dependence on the rank of the latent factor model; improving this rank dependence remains another interesting open question"
- Why unresolved: Proof techniques involving incoherence regularization and reduction of empirical to population optimality conditions introduce polynomial dependencies on $r$ that may not be fundamental to problem structure
- What evidence would resolve it: Refined analysis showing sample complexity scaling as $O(d \cdot r \cdot \text{polylog}(d))$ or lower bound demonstrating higher rank dependence is information-theoretically necessary

### Open Question 2
- Question: What are the theoretical guarantees for the sensitivity of the Hájek estimator under noise injection, particularly in the context of differential privacy?
- Basis in paper: The conclusion states: "it would be valuable to theoretically analyze the sensitivity of the Hájek estimator to noise injection, which could inform the design of privacy-preserving matrix completion algorithms in the ultra-sparse sampling regime"
- Why unresolved: While paper empirically estimates sensitivity in Appendix C.3, formal bounds on how perturbations to $\hat{M}$ propagate through nonlinear Hájek estimator and subsequent gradient descent remain uncharacterized
- What evidence would resolve it: Deriving upper bounds on the $\ell_2$-sensitivity $\Delta_2(A)$ of the Hájek-GD algorithm as function of $n$, $d$, and $C$

### Open Question 3
- Question: Can the common means assumption (where each row $M_i$ is drawn uniformly from $r$ factors) be relaxed to allow each $M_i$ to be a linear combination of the $r$ factors?
- Basis in paper: Remark 3.5 states: "Another plausible extension is to instead assume that each $M_i$ is a linear combination of the $r$ low-rank factors. This setting appears to be significantly more challenging... This is left as an open question for future work"
- Why unresolved: Current proof relies on mixture structure to analyze bias and variance of $\hat{T}$; linear combinations would require new estimators to disentangle mixing coefficients before subspace recovery
- What evidence would resolve it: Modified estimator with provable guarantees under generative model where $M_i = \Sigma_j w_{ij} \cdot u_j$ with random weights $w_{ij}$, or lower bounds showing this is fundamentally harder

### Open Question 4
- Question: Can adaptive optimization methods automatically estimate or learn the true rank $r$ without requiring it as input to the gradient-descent procedure?
- Basis in paper: Remark 4.3 states: "It is an interesting question to examine adaptive optimization methods that can automatically learn or estimate the true rank without knowing it"
- Why unresolved: Current algorithm requires specifying rank $r$ a priori; while cross-validation is suggested, there is no principled method to adaptively determine $r$ from ultra-sparse observations
- What evidence would resolve it: Algorithm with provable guarantees that outputs both rank estimate $\hat{r}$ and recovered matrix, with bounds showing $\hat{r} = r$ with high probability when $n$ exceeds threshold

## Limitations
- Theoretical guarantees require stringent conditions on rank-$r$ factor model, including strict incoherence bounds and sufficiently large sample sizes relative to dimension
- Rank $r$ must be specified in advance, and cross-validation may be unreliable under extreme sparsity
- Method assumes missingness is completely at random, which may not hold in many practical applications

## Confidence
- **High confidence**: The unbiasedness of the Hájek estimator - supported by finite-sample proofs and direct comparison to classical survey sampling theory
- **Medium confidence**: The variance reduction claim - theoretically sound but requires empirical validation across diverse sparsity levels
- **Medium confidence**: Gradient descent convergence guarantees - theoretical bounds exist but depend on strong assumptions about data model and initialization

## Next Checks
1. **Robustness to dependent missingness**: Test the method when observation probabilities correlate with underlying values (not MCAR), measuring bias degradation compared to theoretical guarantees
2. **Scalability verification**: Implement on a large-scale real dataset (e.g., 10M×10K) to verify the claimed linear scaling in number of observed entries holds empirically
3. **Rank estimation capability**: Develop and test a cross-validation protocol for rank selection under ultra-sparse conditions, measuring recovery error sensitivity to rank misspecification