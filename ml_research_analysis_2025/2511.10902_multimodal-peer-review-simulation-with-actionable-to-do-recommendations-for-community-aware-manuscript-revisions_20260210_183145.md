---
ver: rpa2
title: Multimodal Peer Review Simulation with Actionable To-Do Recommendations for
  Community-Aware Manuscript Revisions
arxiv_id: '2511.10902'
source_url: https://arxiv.org/abs/2511.10902
tags:
- review
- multimodal
- peer
- system
- feedback
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a multimodal, community-aware peer review
  simulation system designed to help authors improve manuscripts before submission.
  The system integrates text and visual content using multimodal LLMs, enhances review
  quality via retrieval-augmented generation (RAG) grounded in OpenReview data, and
  converts feedback into structured actionable to-do lists in Action:Objective[] format.
---

# Multimodal Peer Review Simulation with Actionable To-Do Recommendations for Community-Aware Manuscript Revisions

## Quick Facts
- arXiv ID: 2511.10902
- Source URL: https://arxiv.org/abs/2511.10902
- Authors: Mengze Hong; Di Jiang; Weiwei Zhao; Yawen Li; Yihang Wang; Xinyuan Luo; Yanjie Sun; Chen Jason Zhang
- Reference count: 17
- Primary result: Multimodal RAG system achieves 68%, 100%, and 87% win rates over text-only, image-only, and non-RAG baselines in human preference evaluations for simulated peer reviews.

## Executive Summary
This paper introduces MMReview, a multimodal peer review simulation system that processes both text and visual content from academic manuscripts to generate actionable feedback. The system uses OCR and hierarchical summarization for text, concatenates high-resolution page images for visual context, retrieves relevant reviews via RAG from OpenReview data, and formats outputs as structured Action:Objective[#] to-do lists. Evaluation on 30 ICLR 2023 papers demonstrates significant improvements over unimodal and non-RAG baselines in human preference tests, with actionable feedback increasing from 2 to 5 items per review.

## Method Summary
MMReview processes PDF manuscripts through parallel OCR-based text extraction and page image rendering, then applies hierarchical document summarization to compress content while preserving logical flow. The system retrieves top-2 similar papers from a 1,000+ paper OpenReview corpus using cosine similarity on SentenceTransformer embeddings, summarizing their reviews for contextual grounding. A multimodal LLM (GPT-4o/DeepSeek) generates structured reviews following ICLR reviewer guidelines, which are then converted to Action:Objective[#] format with executable instructions, rationales, and precise locators. The system outputs markdown reviews with checkbox-enabled to-do lists for practical manuscript revision.

## Key Results
- Multimodal RAG approach outperforms text-only, image-only, and non-RAG multimodal baselines with 68%, 100%, and 87% win rates in human preference evaluations
- Actionable feedback increases by 57.6%, yielding 5 actionable items per review versus 2 in plain text format
- RAG grounding improves review alignment with community standards through retrieval of similar paper reviews from target venues

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multimodal input (text + images) produces more comprehensive reviews than text-only baselines.
- Mechanism: Parallel processing streams—OCR-based text extraction with hierarchical summarization, plus high-resolution page images concatenated for spatial continuity—allow the MLLM to correlate figures/tables with textual claims, reducing blind spots in assessment.
- Core assumption: Human reviewers form judgments using both visual layout and text; MLLMs can approximate this cross-modal reasoning when both modalities are preserved and aligned.
- Evidence anchors:
  - [abstract] "multimodal RAG approach outperforms text-only, image-only, and non-RAG multimodal baselines, achieving 68%, 100%, and 87% win rates"
  - [section 2.1] "each page of the PDF is rendered as a high-resolution PNG image... concatenated into a single composite representation"
  - [corpus] Weak direct corpus support—MMReview (FMR=0.0, uncited) addresses multimodal review but lacks empirical benchmarks; most neighbors focus on text-only evaluation frameworks.
- Break condition: If OCR fails on scanned figures, or if image resolution/token limits force aggressive downsampling, cross-modal alignment degrades and review quality may drop below text-only for technically dense manuscripts.

### Mechanism 2
- Claim: RAG grounding in venue-specific OpenReview data improves review alignment with community standards.
- Mechanism: Title+abstract embedding retrieves top-2 similar papers from the target venue; their associated reviews are summarized and injected as context, steering generation toward venue-specific critique patterns.
- Core assumption: Review norms are partially venue-specific and can be transferred via example reviews from similar papers.
- Evidence anchors:
  - [abstract] "enhances review quality via retrieval-augmented generation (RAG) grounded in web-scale OpenReview data"
  - [section 2.2.2] "retrieved top-2 most relevant research papers... corresponding reviews... inserted into the review generation process as contextual guidance"
  - [corpus] No direct corpus comparison of RAG vs non-RAG in peer review; corpus papers emphasize evaluation frameworks (ReviewEval, LLM-REVal) rather than retrieval mechanisms.
- Break condition: If the retrieval corpus lacks venue coverage (e.g., closed-review conferences), or if title/abstract embeddings poorly reflect review-relevant similarity, injected context may be irrelevant or misleading.

### Mechanism 3
- Claim: Structured Action:Objective[#] format increases actionable feedback extraction by authors.
- Mechanism: Each review item is decomposed into (1) executable instruction, (2) rationale, and (3) precise locator—reducing ambiguity and enabling checkbox-based tracking in the interface.
- Core assumption: Authors struggle to translate free-form criticism into concrete revision steps; explicit structure and locators reduce cognitive overhead.
- Evidence anchors:
  - [abstract] "to-do format increases actionable feedback by 57.6%, yielding 5 actionable items per review versus 2 in plain text"
  - [section 2.2.3] "Action:Objective[#] format... ensures each feedback item is concrete, explainable, and traceable"
  - [corpus] No corpus papers evaluate structured vs unstructured feedback formats; focus is on review quality metrics, not actionability.
- Break condition: If generated locators hallucinate (e.g., referencing non-existent page numbers), trust in the format degrades; if actions are overly generic ("Improve clarity"), structure adds little value.

## Foundational Learning

- Concept: **Hierarchical document summarization for long-context LLMs**
  - Why needed here: Academic manuscripts exceed typical context windows; the system uses recursive clustering (maximizing B(C) + αE(C)) to preserve logical flow while compressing content.
  - Quick check question: Given a 25-page paper, can you explain why simple truncation would lose section-level context that reviewers rely on?

- Concept: **Multimodal LLM token economics (image vs text)**
  - Why needed here: Concatenating 20+ pages of images creates significant token overhead; understanding resolution/compute tradeoffs is critical for latency-sensitive deployments.
  - Quick check question: If each page image costs ~170 tokens (low-res) or ~1105 tokens (high-res) in GPT-4o, what's the approximate token budget for a 15-page paper with both modalities?

- Concept: **RAG retrieval scoring (cosine similarity vs alternatives)**
  - Why needed here: The system uses cosine similarity over title+abstract embeddings; understanding why this works (and when it fails) informs corpus curation decisions.
  - Quick check question: If two papers share similar abstracts but entirely different methodologies, will cosine similarity on embeddings correctly retrieve relevant review patterns?

## Architecture Onboarding

- Component map: PDF upload -> parallel OCR (text) + page renderer (images) -> hierarchical text clustering + image concatenation -> SentenceTransformer embeddings -> top-2 OpenReview papers retrieval -> review summarization -> MLLM with RAG context -> structured review -> markdown output with Action:Objective[#] to-do list -> Streamlit web interface

- Critical path: PDF parsing accuracy -> multimodal alignment (text-to-figure correspondence) -> retrieval relevance -> prompt quality -> actionable output format. OCR errors early in the pipeline propagate through all downstream components.

- Design tradeoffs:
  - Local deployment (DeepSeek/Llama) preserves privacy but increases hardware requirements vs cloud API
  - Top-2 retrieval balances context richness against prompt length; fewer examples may under-specify norms, more may dilute relevance
  - Fixed image resolution trades visual fidelity for token efficiency; complex figures may lose detail critical for review

- Failure signatures:
  - "Hallucinated locators": to-do items reference non-existent pages/figures -> indicates poor PDF-to-structure alignment
  - "Generic actions": outputs like "Revise for clarity" without specific objectives -> prompt tuning degradation or insufficient RAG context
  - "Image-only dominance": reviews focus excessively on figures while missing textual arguments -> visual stream overweighted in prompt design
  - "Venue mismatch": reviews suggest norms inappropriate for target venue -> retrieval corpus lacks venue coverage

- First 3 experiments:
  1. **Ablation on retrieval depth**: Test top-1, top-2, top-5 retrieved papers; measure review alignment with human references (ROUGE/BERTScore) and latency impact. Expect diminishing returns beyond top-3.
  2. **Locator accuracy audit**: Sample 20 generated to-do items; manually verify page/section references exist in source PDF. Flag hallucination rate; if >15%, add post-hoc grounding step.
  3. **Modality stress test**: Submit papers with (a) text-heavy/figure-sparse content, (b) figure-heavy methodology (diagrams, plots), (c) malformed PDFs (embedded scans, corrupted fonts). Compare review quality across conditions to identify breaking points in the data pipeline.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can dynamic retrieval and interactive LLM-driven dialogue systems be integrated to maintain alignment with evolving user demands during manuscript revision?
- Basis in paper: [explicit] The Conclusion explicitly encourages future work to build upon the project by "enhancing contextual grounding through dynamic retrieval and incorporating interactive user feedback mechanisms via an LLM-driven dialogue system."
- Why unresolved: The current system relies on a static RAG mechanism (top-2 papers) and one-shot generation, which cannot adapt to specific follow-up questions or changing user needs during a revision session.
- What evidence would resolve it: A user study demonstrating that an interactive, dynamic retrieval interface improves user satisfaction or revision quality compared to the current static baseline.

### Open Question 2
- Question: Can the system be effectively adapted to academic venues that do not publicly release review data?
- Basis in paper: [explicit] The Conclusion identifies a limitation regarding the "reliance on a limited set of publicly available review data, which restricts adaptation to venues where review data are not released."
- Why unresolved: The current RAG framework is heavily dependent on OpenReview data to ground reviews in community standards; venues without such data lack the specific contextual grounding the system relies on.
- What evidence would resolve it: Successful application and evaluation of the system on datasets from venues with closed review processes, potentially utilizing transfer learning or synthetic data generation to compensate for the lack of public records.

### Open Question 3
- Question: Does incorporating raw LaTeX sources directly improve review accuracy or detail compared to the current OCR-based document parsing?
- Basis in paper: [explicit] The Conclusion lists the "dependence on OCR-based document parsing" as a limitation and suggests it "could be improved by incorporating raw LaTeX sources directly."
- Why unresolved: While OCR extracts text effectively, it may misinterpret complex mathematical notation or structural hierarchy; the quantitative benefit of direct LaTeX parsing remains unmeasured in this work.
- What evidence would resolve it: An ablation study comparing the precision of generated feedback (specifically regarding technical formulations) when the model processes raw LaTeX code versus OCR-extracted text.

## Limitations
- Evaluation relies entirely on human preference judgments rather than objective quality metrics, creating potential subjectivity in the reported win rates
- 57.6% improvement in actionable items lacks rigorous statistical significance testing and may be sensitive to rater subjectivity
- System's performance on papers from other venues or disciplines remains unknown, as does its behavior with malformed PDFs or highly technical content requiring domain expertise

## Confidence
- **High Confidence**: The multimodal processing pipeline (text + image extraction and concatenation) is technically specified and implementable based on the paper's descriptions
- **Medium Confidence**: The RAG mechanism's effectiveness is supported by the win rate comparisons, but the limited baseline diversity and small evaluation set reduce generalizability
- **Low Confidence**: The claimed practical utility (5 actionable items vs 2) and the 57.6% improvement lack rigorous statistical validation and may be sensitive to rater subjectivity

## Next Checks
1. **Statistical significance testing**: Apply paired t-tests or non-parametric equivalents to the human preference data to confirm the win rates are statistically significant (p < 0.05)
2. **Cross-venue generalization**: Test the system on papers from venues outside ICLR (e.g., CVPR, NeurIPS, or domain-specific journals) to assess venue transfer and identify domain-specific failure modes
3. **Actionability verification**: Conduct a controlled user study where authors use both structured and unstructured reviews to revise manuscripts, measuring revision completion rates and perceived usefulness to validate the claimed 57.6% improvement