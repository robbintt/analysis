---
ver: rpa2
title: 'snnTrans-DHZ: A Lightweight Spiking Neural Network Architecture for Underwater
  Image Dehazing'
arxiv_id: '2504.11482'
source_url: https://arxiv.org/abs/2504.11482
tags:
- image
- underwater
- snntrans-dhz
- color
- images
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces snnTrans-DHZ, a lightweight Spiking Neural
  Network (SNN) designed for underwater image dehazing. The model converts static
  underwater images into time-dependent sequences, processes them in both RGB and
  LAB color spaces, and reconstructs haze-free images using a combined K estimator,
  background light estimator, and soft image reconstruction module.
---

# snnTrans-DHZ: A Lightweight Spiking Neural Network Architecture for Underwater Image Dehazing

## Quick Facts
- **arXiv ID**: 2504.11482
- **Source URL**: https://arxiv.org/abs/2504.11482
- **Reference count**: 40
- **Primary result**: snnTrans-DHZ achieves 21.68 dB PSNR and 0.8795 SSIM on UIEB with only 0.5670M parameters and 0.0151 J energy

## Executive Summary
snnTrans-DHZ is a lightweight Spiking Neural Network designed for underwater image dehazing that processes static images as time-dependent sequences using adaptive spiking neurons. The model operates on both RGB and LAB color spaces in parallel, extracting haze-relevant transmission features while preserving color fidelity. Using surrogate gradient backpropagation through time with adaptive Leaky Integrate-and-Fire neurons, it achieves state-of-the-art efficiency with 21.68 dB PSNR on UIEB at just 7.42 GSOPs and 0.0151 J energy consumption.

## Method Summary
snnTrans-DHZ converts static underwater images into time sequences by repeating them across 10 timesteps, processing through parallel RGB and LAB branches with Adaptive Leaky Integrate-and-Fire (ALIF) neurons. The architecture uses a spiking transformer with self-attention operating on binary spike patterns rather than floating-point values, estimating both the transmission map (K) and background light (B) for physics-based reconstruction. Training employs surrogate gradient backpropagation through time with a composite loss combining MSE, SSIM, and total variation regularization.

## Key Results
- Achieves 21.68 dB PSNR and 0.8795 SSIM on UIEB benchmark
- Requires only 0.5670 million parameters and 7.42 GSOPs
- Consumes 0.0151 J energy, significantly outperforming existing methods
- Ablation study shows adaptive thresholds improve PSNR by ~1 dB over fixed thresholds

## Why This Works (Mechanism)

### Mechanism 1: Adaptive Threshold Temporal Coding
The ALIF neuron treats threshold membrane potential ($V_{th}$) as learnable, allowing dynamic adjustment of firing sensitivity across timesteps. This prevents information loss during rate-coding where static images are repeated over time ($T=10$).

### Mechanism 2: Hybrid Color Space Decoupling
Processing luminance (LAB) and chromaticity (RGB) in parallel branches reduces color cast more effectively than single-stream RGB processing. The LAB space separates lightness from color opponents, allowing haze extraction without conflating with color distortions.

### Mechanism 3: Surrogate Gradient Backpropagation Through Time
Direct training of SNNs is enabled using a fast-sigmoid surrogate to approximate the non-differentiable spike function, allowing errors to propagate through time steps via BPTT where the Heaviside step function has zero gradient almost everywhere.

## Foundational Learning

- **Rate Coding / Direct Coding**: The model converts static images into time-dependent sequences ($T$ timesteps) to accumulate evidence over time. *Quick check*: Why does the paper input the same static image repeatedly for $T=10$ steps rather than a video?
- **Spiking Self-Attention (ASBSA)**: This is not standard Transformer attention - it operates on binary spikes ($0/1$) rather than floating-point values, replacing expensive MAC operations with accumulations. *Quick check*: In the ASBSA mechanism (Eq. 33), how does the $Q \times K^T$ interaction differ computationally from a standard Vision Transformer?
- **Physics-based Image Formation Model (IFM)**: The network output is a reconstruction based on estimated physical parameters ($K$ and $B$) rather than just a "clean image." *Quick check*: What physical quantity does the "K estimator" approximate, and how does it relate to the background light $B$ in the final reconstruction equation (Eq. 40)?

## Architecture Onboarding

- **Component map**: Image Input $\to$ [RGB/LAB Split] $\to$ [Spike Coding] $\to$ [Dual Encoder] $\to$ [Transformer Blocks] $\to$ [K & B Heads] $\to$ [Soft Reconstruction] $\to$ Loss
- **Critical path**: Image Input $\to$ [RGB/LAB Split] $\to$ [Spike Coding] $\to$ [Dual Encoder] $\to$ [Transformer Blocks] $\to$ [K & B Heads] $\to$ [Soft Reconstruction] $\to$ Loss
- **Design tradeoffs**: Uses shallow depth (2 layers) and fewer channels (64) vs. standard Transformers to hit 0.0151 J energy, sacrificing some peak PSNR (21.68 vs. 25.98 in SOTA Transformers)
- **Failure signatures**: 
  - Color Cast: If LAB branch is ablated, expect strong green/blue tints
  - Gradient Vanishing: If surrogate gradient $\lambda$ is too small, model fails to converge
  - Blurry Output: If TV loss weight $\beta$ is too high, image looks overly smooth
- **First 3 experiments**:
  1. Reproduce ablation on thresholds: Fixed Vth vs. Adaptive Vth on UIEB subset to verify ~1dB PSNR gain
  2. Timestep scaling: Measure energy consumption and PSNR for $T \in \{1, 5, 10, 20\}$ to find efficiency curve "knee"
  3. Loss function mismatch: Train with MSE-only vs. combined loss to visualize perceptual quality differences

## Open Questions the Paper Calls Out
- How sensitive is reconstruction quality to the number of inference timesteps ($T$), and can $T$ be optimized for varying water turbidity levels?
- What are the actual latency and energy efficiency metrics when deployed on physical neuromorphic hardware like the Loihi chip?
- Does snnTrans-DHZ maintain robust performance when integrated into closed-loop control systems for autonomous underwater vehicles in real-time?

## Limitations
- Energy metrics calibration relies on theoretical SOP calculations rather than hardware validation
- Color space contribution analysis lacks isolation of L channel benefits from full LAB transformation
- Temporal resolution choice of T=10 appears arbitrary without systematic accuracy-energy tradeoff analysis

## Confidence
- **High confidence**: Architectural description, loss function formulation, and basic training procedure are clearly specified
- **Medium confidence**: ALIF neuron implementation and surrogate gradient approach are well-described but initialization affects convergence
- **Low confidence**: Energy efficiency claims lack hardware validation and rely on theoretical calculations

## Next Checks
1. Implement the model on a neuromorphic processor (Loihi, TrueNorth) to verify claimed energy efficiency through empirical measurements
2. Conduct granular ablation study isolating L channel contribution from full LAB transformation to understand color improvement mechanism
3. Systematically evaluate PSNR-energy tradeoff across T âˆˆ [1, 20] to identify optimal temporal resolution for deployment scenarios