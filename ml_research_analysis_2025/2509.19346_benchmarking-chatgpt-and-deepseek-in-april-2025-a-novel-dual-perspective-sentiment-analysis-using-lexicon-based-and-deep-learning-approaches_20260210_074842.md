---
ver: rpa2
title: 'Benchmarking ChatGPT and DeepSeek in April 2025: A Novel Dual Perspective
  Sentiment Analysis Using Lexicon-Based and Deep Learning Approaches'
arxiv_id: '2509.19346'
source_url: https://arxiv.org/abs/2509.19346
tags:
- reviews
- chatgpt
- deepseek
- sentiment
- positive
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study presents a dual-perspective sentiment analysis of ChatGPT
  and DeepSeek user reviews, integrating lexicon-based TextBlob analysis with deep
  learning classification models (CNN and Bi-LSTM). A dataset of 4,000 user reviews
  was collected from the Google Play Store, balanced using RandomOverSampler, and
  evaluated using a 1,700-review test set.
---

# Benchmarking ChatGPT and DeepSeek in April 2025: A Novel Dual Perspective Sentiment Analysis Using Lexicon-Based and Deep Learning Approaches

## Quick Facts
- arXiv ID: 2509.19346
- Source URL: https://arxiv.org/abs/2509.19346
- Reference count: 29
- Primary result: CNN outperforms Bi-LSTM with 96.41% accuracy on balanced 1,700-review test set

## Executive Summary
This study presents a dual-perspective sentiment analysis framework combining TextBlob lexicon-based labeling with deep learning classification to evaluate user sentiment toward ChatGPT and DeepSeek applications. The research collected 4,000 user reviews from the Google Play Store, applied RandomOverSampler to address class imbalance, and trained both CNN and Bi-LSTM models for three-class sentiment classification. Results demonstrate CNN's superiority in handling short, phrase-based reviews typical of app store feedback, achieving 96.41% accuracy with near-perfect negative review classification. The methodology establishes a new standard for LLM sentiment evaluation by integrating qualitative user feedback analysis with quantitative deep learning classification.

## Method Summary
The methodology involves scraping 2,000 English-language reviews each for ChatGPT and DeepSeek from the Google Play Store using the `google-play-scraper` library. Reviews undergo preprocessing (lowercase, non-alphabetic removal) and TextBlob-based labeling with ±0.1 polarity thresholds. RandomOverSampler addresses the original 8:1 positive-to-negative ratio imbalance. The balanced dataset trains CNN (Conv1D[128 filters, kernel=5]→GlobalMaxPooling→Dense[64]) and Bi-LSTM (Bidirectional LSTM[64]) architectures with shared embedding layer (5000 vocab, 64 dim), dropout(0.5), and softmax(3) output. Models use Adam optimizer, sparse_categorical_crossentropy loss, 72/8/20% train/val/test split, batch size 32, and early stopping over 50 epochs.

## Key Results
- CNN achieved 96.41% accuracy versus Bi-LSTM's 93.12% on balanced test set
- CNN demonstrated near-perfect negative review classification with 98.83% recall
- ChatGPT received more positive sentiment (69.35%) than DeepSeek (61.75%) via TextBlob analysis

## Why This Works (Mechanism)

### Mechanism 1
- Claim: CNN outperforms Bi-LSTM for short-text sentiment classification of app reviews
- Mechan: 1D convolutions with kernel size 5 extract local n-gram sentiment patterns; global max pooling retains strongest signals per filter, reducing noise from longer but less relevant sequences
- Core assumption: User reviews are phrase-sentiment rich rather than requiring deep sequential context
- Evidence anchors:
  - [abstract] "CNN outperforming Bi-LSTM by achieving 96.41 percent accuracy and near perfect classification of negative reviews"
  - [section 10] "CNN performed well due to the shortness of user reviews, which are phrase-based and sentiment-rich, making them more suitable for convolutional feature extraction"
  - [corpus] Related work (Alsaleh et al., 2024) achieved 94% with hybrid BERT+Bi-LSTM+CNN, supporting that CNN components contribute strongly to sentiment tasks
- Break condition: If review texts were substantially longer (>200 words) with complex argument structures, Bi-LSTM's sequential modeling may become necessary

### Mechanism 2
- Claim: RandomOverSampler enables fair multi-class sentiment evaluation by correcting class imbalance
- Mechan: Replicates minority class samples until all sentiment categories have equal representation; training then receives balanced gradient signals from all classes
- Core assumption: Minority class samples are representative of their true distribution
- Evidence anchors:
  - [section 5.1] "The Distribution of classes was skewed... we used oversampling to retain all the instances in the majority class and produce a more balanced training set without the loss of information"
  - [section 5.3.3] Original distribution: Positive ~69%, Neutral ~23%, Negative ~8%
  - [corpus] No direct corpus comparison of oversampling vs. downsampling for LLM review datasets was found
- Break condition: If minority class samples contain high noise or outliers, oversampling amplifies errors; synthetic generation (SMOTE variants) would be needed

### Mechanism 3
- Claim: Dual-perspective (lexicon + DL) analysis captures complementary sentiment signals
- Mechan: TextBlob provides interpretable polarity thresholds (>0.1 positive, <-0.1 negative, between neutral); DL models learn implicit features that lexicons miss, including negation patterns and context-dependent sentiment
- Core assumption: Combining rule-based and learned approaches reduces systematic blind spots
- Evidence anchors:
  - [section 2] "this combination of both techniques offers a more reliable, balanced, and sophisticated approach to user perception"
  - [section 6] ChatGPT: 69.35% positive, 23.40% neutral, 7.24% negative via TextBlob
  - [corpus] Related Reddit sentiment analysis (arxiv 2502.18513) similarly combines topic modeling with sentiment classification for LLM perception studies
- Break condition: If lexicon thresholds don't match domain-specific language (e.g., technical jargon with different connotations), labeling noise propagates to DL training

## Foundational Learning

- Concept: **1D Convolutions for Text**
  - Why needed here: Understanding why kernel_size=5 and 128 filters extract sentiment-bearing phrases
  - Quick check question: What n-gram range does a kernel_size=5 capture, and why might "not good at all" require context beyond a single kernel window?

- Concept: **Class Imbalance & Oversampling**
  - Why needed here: Original dataset had ~8:1 ratio of positive to negative reviews; models would overpredict majority class without correction
  - Quick check question: If you oversample a minority class 5x, what happens to the effective sample size for variance estimation?

- Concept: **TextBlob Polarity Scoring**
  - Why needed here: Labels for DL training are generated via TextBlob thresholds, not human annotation
  - Quick check question: Given polarity thresholds ±0.1, how might the phrase "this app is okay I guess" be classified, and what are the implications for downstream model training?

## Architecture Onboarding

- Component map:
Data Collection (Google Play Scraper) → Preprocessing (lowercase, remove non-alpha) → TextBlob Labeling → RandomOverSampler → Tokenization + Padding → Embedding(5000 vocab, 64 dim) → [CNN branch | Bi-LSTM branch] → Dense(64, ReLU) → Dropout(0.5) → Softmax(3)

- Critical path:
  1. Label quality depends on TextBlob threshold calibration (±0.1)
  2. Oversampling ratio must achieve class balance but avoid extreme duplication
  3. Early stopping monitors validation loss to prevent overfitting on padded sequences

- Design tradeoffs:
  - CNN: Faster inference, better for short texts, captures local patterns (chosen: 96.41% accuracy)
  - Bi-LSTM: Better for sequential dependencies, higher capacity, slower training (achieved: 93.12% accuracy)
  - Assumption: The paper does not report inference time or model size comparison

- Failure signatures:
  - High neutral-positive confusion (Bi-LSTM: 60 positive→neutral, 44 neutral→positive misclassifications) suggests sentiment boundary is linguistically fuzzy
  - If negative recall drops significantly, oversampling may be insufficient or negative reviews have higher lexical diversity

- First 3 experiments:
  1. **Label sensitivity test**: Manually annotate 200 reviews and compare against TextBlob labels; quantify labeling error rate
  2. **Architecture ablation**: Test CNN with kernel_size=[3,5,7] and filter counts [64,128,256] to verify optimal configuration
  3. **Imbalance robustness**: Train on original imbalanced data without oversampling; measure per-class recall degradation to quantify oversampling benefit

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the sentiment gap between ChatGPT and DeepSeek persist across different platforms, specifically the iOS App Store?
- Basis in paper: [explicit] The authors state they "Laid the groundwork for subsequent research on user experience with AI across various platforms," while the current study is restricted to the Google Play Store.
- Why unresolved: The dataset is limited to Android users, excluding the iOS user base which may exhibit different behaviors.
- What evidence would resolve it: Applying the proposed dual-perspective framework to a newly scraped dataset from the Apple App Store.

### Open Question 2
- Question: Can transformer-based classifiers (e.g., BERT) outperform the CNN benchmark established in this study?
- Basis in paper: [inferred] The study benchmarks CNN and Bi-LSTM but omits transformer-based classification models, despite the subject matter involving Transformer-based LLMs.
- Why unresolved: The experimental scope is limited to specific deep learning architectures.
- What evidence would resolve it: Comparative experiments including BERT or RoBERTa models on the same 1,700-review test set.

### Open Question 3
- Question: How does user sentiment differ for DeepSeek in non-English linguistic contexts?
- Basis in paper: [inferred] The data collection explicitly filtered for "reviews written in English," yet DeepSeek is noted in the literature review for advancements in Chinese text comprehension.
- Why unresolved: The geographic and linguistic scope was restricted during the scraping process.
- What evidence would resolve it: A multilingual sentiment analysis of reviews from regions where DeepSeek has high adoption.

## Limitations
- The study relies on TextBlob-based labeling without human annotation validation, potentially introducing systematic bias in sentiment classification
- Dataset collection is limited to English-language reviews from US users, restricting generalizability to other linguistic and cultural contexts
- The research focuses exclusively on Google Play Store reviews, excluding potentially different user sentiment patterns from iOS App Store or other platforms

## Confidence

- **High Confidence**: CNN architecture superiority (96.41% accuracy) and its better performance on negative review classification, given the explicit architectural differences and clear performance metrics reported
- **Medium Confidence**: TextBlob labeling accuracy and its impact on downstream model performance, as the threshold-based approach is transparent but lacks empirical validation against human annotations
- **Medium Confidence**: Oversampling effectiveness in addressing class imbalance, as the methodology is sound but the paper doesn't explore alternative balancing techniques or their comparative impact
- **Low Confidence**: Generalization of findings to other LLM applications or review platforms, given the single-platform (Google Play) and two-app focus

## Next Checks
1. **Label Validation Study**: Manually annotate 200 randomly selected reviews from the dataset and compute Cohen's kappa against TextBlob-generated labels to establish labeling accuracy and potential systematic biases
2. **Cross-Platform Generalization**: Collect and analyze user reviews from alternative sources (Reddit, Trustpilot, App Store) for the same applications to assess whether sentiment patterns hold across platforms and user demographics
3. **Model Architecture Sensitivity**: Conduct ablation studies varying kernel sizes (3, 5, 7), filter counts (64, 128, 256), and embedding dimensions to determine if the reported CNN superiority is robust to architectural variations or specific to the chosen configuration