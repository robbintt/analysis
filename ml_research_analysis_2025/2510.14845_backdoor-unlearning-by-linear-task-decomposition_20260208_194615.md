---
ver: rpa2
title: Backdoor Unlearning by Linear Task Decomposition
arxiv_id: '2510.14845'
source_url: https://arxiv.org/abs/2510.14845
tags:
- clean
- backdoor
- unlearning
- attack
- tbar
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work investigates backdoor unlearning in vision-language foundation
  models like CLIP. It shows that backdoors are encoded as disentangled task vectors
  in the model's weight space, separate from clean knowledge.
---

# Backdoor Unlearning by Linear Task Decomposition

## Quick Facts
- arXiv ID: 2510.14845
- Source URL: https://arxiv.org/abs/2510.14845
- Reference count: 33
- This work investigates backdoor unlearning in vision-language foundation models like CLIP. It shows that backdoors are encoded as disentangled task vectors in the model's weight space, separate from clean knowledge. Leveraging this, the authors introduce TBAR (Trigger removal by Backdoor Arithmetic), a lightweight method that unlearns backdoors via weight-space task negation. When the trigger is known, TBAR removes over 98% of backdoor attacks while retaining ~96% of clean accuracy. In attack-agnostic settings using reverse-engineered triggers, TBAR still removes backdoors effectively while preserving over 90% clean accuracy, outperforming existing defenses.

## Executive Summary
This paper introduces TBAR, a novel method for removing backdoor triggers from vision-language models like CLIP by exploiting the linear structure of task vectors in weight space. The key insight is that backdoor behavior and clean knowledge are encoded as quasi-orthogonal directions that can be disentangled and subtracted. TBAR achieves over 98% backdoor removal while preserving 96% clean accuracy when the trigger is known, and remains effective at over 90% clean accuracy in attack-agnostic settings using reverse-engineered triggers. The approach outperforms existing defenses by leveraging weight-space arithmetic rather than retraining or activation modification.

## Method Summary
TBAR removes backdoors by extracting and negating a trigger task vector from a backdoored model's weights. Given a backdoored model θ_b, the method fine-tunes it on a small forget set of triggered examples to obtain θ_{b+t}, then computes the trigger vector τ̂_t = θ_{b+t} - θ_b. The clean model is recovered via weight subtraction: θ̂_c = θ_b - α·τ̂_t, where α is selected via grid search on a validation set. For attack-agnostic settings, TBAR combines with DECREE to generate proxy triggers for constructing synthetic forget sets. The approach exploits the assumption that backdoor and clean task vectors are quasi-orthogonal in weight space.

## Key Results
- TBAR removes over 98% of backdoor attacks while retaining ~96% clean accuracy when triggers are known
- In attack-agnostic settings using reverse-engineered triggers, TBAR maintains over 90% clean accuracy
- TBAR outperforms existing defenses by leveraging weight-space arithmetic rather than retraining
- The method works across multiple attack types including BadNet, Blended, WaNet, and BadCLIP

## Why This Works (Mechanism)

### Mechanism 1: Weight Space Disentanglement of Backdoor and Clean Tasks
- Claim: Backdoor behavior and clean task knowledge encode as quasi-orthogonal directions in the weight space of CLIP vision encoders.
- Mechanism: When a model is backdoored via poisoned fine-tuning, the parameter updates separate into (1) a clean task vector and (2) a trigger task vector. These vectors satisfy the disentanglement property: the combined model applies each vector's function only on its respective domain (clean vs. triggered inputs).
- Core assumption: Pre-trained vision-language models exhibit linear mode connectivity and task vector locality at sufficient scale.
- Evidence anchors:
  - [abstract] "backdoors are encoded as disentangled task vectors in the model's weight space, separate from clean knowledge"
  - [Section 4] Equation 3 formalizes disentanglement: f(x; θ_pre + α_cτ_c + α_tτ_t) decomposes cleanly by domain
  - [Section 5.1, Figure 2] Weight disentanglement error ξ(α_c, α_t) shows large low-error regions at center, indicating orthogonal directions
  - [corpus] Limited corpus support; related work on backdoor unlearning (e.g., "Backdoor Token Unlearning") explores similar themes but does not directly validate disentanglement
- Break condition: Disentanglement may degrade if (a) trigger and clean data distributions strongly overlap, (b) model scale is too small, or (c) attacks are explicitly designed to entangle representations (e.g., adaptive attacks).

### Mechanism 2: Trigger Vector Estimation via Targeted Fine-Tuning
- Claim: Fine-tuning a backdoored model on a small forget set of triggered examples yields a parameter update that primarily captures the backdoor direction.
- Mechanism: Given backdoored weights θ_b, fine-tuning on triggered data produces θ_{b+t}. The difference τ̂_t = θ_{b+t} - θ_b approximates the trigger task vector because the model is already saturated on triggered behavior—additional fine-tuning reinforces this direction with minimal clean interference.
- Core assumption: The backdoor direction has not already been maximally expressed; there exists residual gradient signal along τ_t.
- Evidence anchors:
  - [abstract] "fine-tuning the model on similarly constructed triggered data isolates the parameter shift associated with triggered information"
  - [Section 4, Eq. 4-5] Explicitly defines τ̂_t estimation and subtraction: θ̂_c = θ_b - α·τ̂_t
  - [Section 5.2] Trigger vectors transfer across datasets with same attack type, suggesting they encode trigger-to-misdirection patterns, not dataset-specific semantics
  - [corpus] "Backdoor Token Unlearning" paper similarly uses fine-tuning to expose backdoors in language models, providing cross-domain corroboration
- Break condition: Estimation fails if (a) forget set is contaminated with clean examples, (b) trigger pattern is inconsistent, or (c) learning rate/epochs cause overshooting into unrelated directions.

### Mechanism 3: Attack-Agnostic Unlearning via Reverse-Engineered Triggers
- Claim: Combining TBAR with trigger inversion methods (e.g., DECREE) enables backdoor removal without prior knowledge of the attack.
- Mechanism: DECREE optimizes a minimal trigger pattern that induces consistent embeddings across inputs. This proxy trigger is used to construct synthetic triggered pairs for TBAR vector extraction. The estimated vector, while imperfect, still captures enough backdoor signal for effective negation.
- Core assumption: Reverse-engineered triggers share sufficient directional overlap with true triggers in weight space.
- Evidence anchors:
  - [abstract] "using reverse-engineered triggers, TBAR still removes backdoors effectively while preserving over 90% clean accuracy"
  - [Section 6, Table 3 bottom] TBAR+DECREE achieves 0.33-0.90% ASR across BadNet/Blended/WaNet vs. Gradient Ascent's 0.33-76.40%
  - [Section 6, Figure 4] Gradient ascent with reverse-engineered triggers is unstable; TBAR maintains stable CA-ASR tradeoffs
  - [corpus] No direct corpus evidence for reverse-engineering + unlearning pipelines; this appears novel
- Break condition: Fails when (a) reverse-engineering cannot recover meaningful triggers (e.g., BadCLIP per Section 6), (b) backdoor is activated by semantic patterns rather than pixel-space triggers, or (c) proxy trigger activates different model biases than true trigger.

## Foundational Learning

- **Task Vectors (Ilharco et al., 2022a)**
  - Why needed here: TBAR's core operation—subtracting a trigger vector—requires understanding that fine-tuned weights encode task-specific directions that can be arithmetically manipulated.
  - Quick check question: Given pre-trained weights θ_pre and fine-tuned weights θ_task, what does the vector τ = θ_task - θ_pre represent, and what happens when you compute θ_pre - ατ?

- **Weight Disentanglement (Ortiz-Jimenez et al., 2024)**
  - Why needed here: Justifies why subtracting τ_t doesn't catastrophically erase clean knowledge—the triggered and clean directions are functionally isolated.
  - Quick check question: If task vectors τ_A and τ_B are disentangled, what is the expected output when both are added to a model, evaluated on inputs from domain A only?

- **Backdoor Attacks in Vision-Language Models**
  - Why needed here: Provides threat model context—understanding that backdoors are triggered by specific patterns and map to target labels is essential for constructing forget sets.
  - Quick check question: In a BadNet attack on CLIP, what two conditions must hold for the attack to be successful at inference time?

- **CLIP Architecture (Contrastive Language-Image Pre-Training)**
  - Why needed here: TBAR operates primarily on the visual encoder; understanding the dual-encoder structure explains why text encoder can remain frozen.
  - Quick check question: In CLIP's zero-shot classification, how are image embeddings matched to class labels without a classification head?

## Architecture Onboarding

- **Component map:**
  - Input -> Trigger Vector Extraction -> Scaling Coefficient Search -> Unlearning Operation -> Clean Model
  - Input: Backdoored CLIP model (θ_b), forget set (triggered image-label pairs), optional clean validation set
  - Trigger Vector Extraction: Fine-tune θ_b on forget set → θ_{b+t}, compute τ̂_t = θ_{b+t} - θ_b
  - Scaling Coefficient Search: Grid search over α using validation set; select α that minimizes ASR while maintaining CA > threshold
  - Unlearning Operation: θ̂_c = θ_b - α·τ̂_t (element-wise subtraction)
  - Attack-Agnostic Extension: DECREE module generates proxy trigger → construct synthetic forget set → proceed with TBAR

- **Critical path:**
  1. Construct forget set with consistent trigger pattern (same trigger, same target label)
  2. Fine-tune with conservative hyperparameters (1 epoch, LR 1e-5 to 1e-6) to avoid drifting from trigger direction
  3. Extract τ̂_t immediately after fine-tuning (before any additional training)
  4. Search α in range [0.5, 3.0] with step 0.1-0.5; prioritize ASR < 1% then maximize CA
  5. Apply negation and evaluate on held-out clean and triggered test sets

- **Design tradeoffs:**
  - **Forget set size vs. estimation quality**: Paper finds 1.5k-2k samples sufficient; larger sets don't improve results (Figure 5) but increase compute
  - **Scaling coefficient selection**: Higher α removes more backdoor but risks CA loss; use early stopping condition (CA threshold or ASR floor)
  - **Text encoder freezing vs. full model**: Frozen text encoder simplifies task vector isolation; full model fine-tuning may be needed for caption-based attacks
  - **Gradient Ascent vs. TBAR**: GA can work but is unstable beyond 1-2 epochs; TBAR's directional constraint provides robustness

- **Failure signatures:**
  - **ASR remains high after unlearning**: Trigger vector not isolated → check forget set purity, increase fine-tuning epochs slightly, verify trigger consistency
  - **CA drops >10%**: Over-negation → reduce α, check for clean examples in forget set, verify validation set is representative
  - **Reverse-engineered trigger fails**: DECREE returns large trigger norm → backdoor may not exist, or attack is resistant to inversion (e.g., BadCLIP)
  - **Transfer across datasets fails**: Trigger vector encodes dataset-specific patterns → re-extract on target dataset

- **First 3 experiments:**
  1. **Reproduce single-task unlearning (Table 1)**: Take CLIP ViT-B/32, backdoor on CIFAR100 with BadNet (3% poison rate), extract TBAR vector from 2k triggered samples, sweep α ∈ [0.5, 2.5], report CA and ASR. Target: ASR < 2%, CA retention > 94%.
  2. **Validate disentanglement hypothesis (Figure 2)**: For the backdoored model, compute clean vector τ̂_c = τ_b - α*·τ̂_t, sweep (α_c, α_t) ∈ [-2, 2] × [-2, 2], plot disentanglement error ξ. Expect low error at center.
  3. **Test attack-agnostic pipeline (Table 3 bottom)**: Apply DECREE to backdoored model with 5k clean ImageNet samples, construct proxy forget set, run TBAR with CA threshold 90%. Compare ASR/CA against Gradient Ascent baseline.

## Open Questions the Paper Calls Out

- **Question**: Can TBAR remain effective against adaptive backdoor attacks specifically optimized to maximize weight entanglement with clean tasks or resist task negation?
  - Basis in paper: [inferred] The paper validates TBAR on existing attacks (BadNet, WaNet, BadCLIP) but does not evaluate against adaptive adversaries who might optimize triggers to specifically interfere with the linear decomposition property.
  - Why unresolved: The defense relies on the hypothesis that backdoor and clean task vectors are linearly disentangled; an adaptive attack could theoretically optimize the backdoor to lie in a direction that overlaps with the clean task vector, making subtraction destructive.
  - What evidence would resolve it: Experimental results showing TBAR's performance on backdoors explicitly trained with a regularization penalty against task vector orthogonality or negation.

## Limitations

- **Architecture Dependence**: The disentanglement mechanism may not generalize beyond CLIP to other vision-language models or pure vision models with different training objectives.
- **Semantic Backdoor Resistance**: TBAR with DECREE fails on semantic backdoors (e.g., BadCLIP) where triggers activate through class-specific semantics rather than consistent pixel patterns.
- **Trigger Vector Estimation Sensitivity**: The quality of trigger vector estimation depends on forget set purity, fine-tuning hyperparameters, and trigger consistency, with no robust guarantees across attack types.

## Confidence

- **High Confidence**: Single-task unlearning with known triggers (Table 1). The mechanism is clearly specified, results are reproducible, and the weight space disentanglement analysis (Figure 2) provides strong empirical support.
- **Medium Confidence**: Transferability of trigger vectors across datasets (Section 5.2). While the paper shows vectors transfer for same attack types, the mechanism for why trigger-to-misdirection patterns are dataset-invariant is not fully explained.
- **Low Confidence**: Attack-agnostic unlearning with DECREE (Section 6). This is the most novel extension but also the most fragile. The failure on BadCLIP and the lack of theoretical guarantees for proxy trigger alignment make this the weakest link in the proposed pipeline.

## Next Checks

1. **Cross-Model Disentanglement Test**: Apply TBAR to backdoor another vision-language model (e.g., BLIP-2 or OpenCLIP) with BadNet. Verify that the disentanglement error ξ(α_c, α_t) shows similar low-error regions. If disentanglement fails, identify architectural differences (e.g., attention patterns, pre-training objectives) that break the mechanism.

2. **Estimation Sensitivity Analysis**: Systematically vary the forget set size (100 to 10,000 samples), fine-tuning learning rate (1e-7 to 1e-4), and epochs (1 to 5). For each configuration, measure trigger vector norm, cosine similarity to true trigger, and unlearning performance. Identify the Pareto frontier for estimation quality vs. computational cost.

3. **Semantic Backdoor Inversion Test**: Design a semantic backdoor (e.g., "banana" caption triggers dog classification) and attempt to reverse-engineer it with DECREE. If DECREE fails to produce a meaningful trigger, explore alternative inversion methods (e.g., gradient-based token optimization) or determine if semantic backdoors are fundamentally unlearnable via weight-space negation.