---
ver: rpa2
title: Clustered KL-barycenter design for policy evaluation
arxiv_id: '2503.02735'
source_url: https://arxiv.org/abs/2503.02735
tags:
- policy
- policies
- target
- behavior
- sample
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the problem of designing behavior policies
  for efficiently evaluating multiple target policies in stochastic bandit models.
  The authors introduce a clustered KL-barycenter approach (CKL-PE) that first groups
  target policies based on probabilistic similarity and then assigns each cluster
  its own KL-barycenter as a behavior policy.
---

# Clustered KL-barycenter design for policy evaluation

## Quick Facts
- arXiv ID: 2503.02735
- Source URL: https://arxiv.org/abs/2503.02735
- Reference count: 40
- Primary result: A clustered KL-barycenter approach improves sample efficiency for policy evaluation in stochastic bandits by grouping target policies and using per-cluster behavior policies.

## Executive Summary
This paper addresses the challenge of efficiently evaluating multiple target policies in stochastic bandit models. The authors introduce a clustered KL-barycenter approach (CKL-PE) that first groups target policies based on probabilistic similarity and then assigns each cluster its own KL-barycenter as a behavior policy. This addresses limitations of single KL-barycenter approaches, where sample complexity can scale unfavorably with the number of target policies. The method provides a practical solution for scaling policy evaluation to large policy sets without requiring additional bandit interactions.

## Method Summary
The method computes a KL-barycenter as the element-wise average of target policies to minimize average KL divergence, then clusters these policies using Hellinger distance to reduce maximal importance weights. For each cluster, a separate KL-barycenter behavior policy is used to collect samples via importance sampling. The best policy is selected based on estimated values. The approach guarantees bounded importance weights when using mixture policies and provides theoretical upper bounds on sample complexity that improve with clustering.

## Key Results
- Clustering target policies based on similarity reduces the maximal importance weight, improving sample complexity
- CKL-PE outperforms both single KL-barycenter and naive Monte Carlo approaches, particularly when target policies exhibit structure
- The number of clusters should be chosen to minimize M·σ_c², where M is the number of clusters and σ_c is the maximal importance weight within clusters
- Empirical results show CKL-PE achieves lower regret than baselines across varying sample sizes

## Why This Works (Mechanism)

### Mechanism 1: KL-Barycenter Minimizes Average KL Divergence
The arithmetic mean of all target policies minimizes the average KL divergence between each target policy and a single shared behavior policy. This provides a computationally trivial way to construct a theoretically grounded behavior policy for multiple targets, but requires all target policies to be strictly positive.

### Mechanism 2: Clustering Reduces Maximum Importance Weights
Partitioning target policies into clusters based on similarity and using a separate KL-barycenter per cluster reduces the maximal importance weight. This improves sample complexity by bounding the key complexity term M·σ_c², where σ_c is the maximal weight within each smaller cluster rather than across all N policies.

### Mechanism 3: Bounded Weights Enable Regret Analysis
Using the KL-barycenter as a behavior policy guarantees that importance weights are bounded by N, which allows for finite-sample regret upper bounds. This boundedness enables the use of concentration inequalities to derive upper bounds on excess risk that scale with σ_KL and 1/sqrt(n).

## Foundational Learning

- **Concept**: Importance Sampling (IS) Estimator
  - Why needed here: This is the core estimation technique used to evaluate target policies using data from a different behavior policy. Understanding its variance properties is essential.
  - Quick check question: Why does a large mismatch between the behavior policy πb and target policy π lead to high variance in the IS estimator?

- **Concept**: Kullback-Leibler (KL) Divergence
  - Why needed here: The paper's theoretical contribution is built on minimizing the average KL divergence. Understanding this measure of probabilistic similarity is crucial for grasping the barycenter design.
  - Quick check question: What does a KL divergence of zero signify? Why does it become undefined if the behavior policy assigns zero probability to an action the target policy might take?

- **Concept**: Regret / Excess Risk
  - Why needed here: The primary goal is best-policy selection, not just value estimation. Performance is measured by regret, the value difference between the optimal and selected policies.
  - Quick check question: Is it possible to have accurate value estimates but still suffer high regret? Why or why not?

## Architecture Onboarding

- **Component map**: Policy Pre-processor -> Clustering Module -> Behavior Policy Designer -> Data Collector -> Evaluator & Selector
- **Critical path**: Cluster policies -> Compute cluster barycenters -> Sample data using barycenters -> Estimate values and select best policy
- **Design tradeoffs**: Number of Clusters (M) balances risk of high σ_KL (small M) against efficiency loss (large M). Sample allocation could be optimized by considering respective σ_cj rather than uniform distribution.
- **Failure signatures**: Infinite IS weights if behavior policy has zero probability for an action a target policy selects; high Regret if clustering fails to group similar policies; poor Scaling if target policies lack structure.
- **First 3 experiments**:
  1. Sanity Check: Verify computed KL-barycenter is element-wise average and importance weights bounded by N
  2. Parameter Sweep: For structured policy set, sweep different cluster counts M and plot M·σ_c² vs M
  3. Regret Comparison: Run full CKL-PE pipeline with optimal M and compare average regret against baselines

## Open Questions the Paper Calls Out

1. **Extension to Contextual Bandits/MDPs**: Can the clustered KL-barycenter design be extended to contextual bandits or Markov Decision Processes while maintaining theoretical guarantees?

2. **Optimal Sample Allocation**: How can samples be allocated optimally across clusters based on their individual variance characteristics rather than distributed equally?

3. **Adaptive Cluster Elimination**: Can an adaptive algorithm be designed to sequentially eliminate underperforming clusters during evaluation to improve efficiency?

4. **Optimal Number of Clusters**: Is there a theoretically grounded method to determine the optimal number of clusters M for a given set of target policies?

## Limitations

- The effectiveness heavily depends on target policies having inherent structure; clustering provides no benefit for unstructured policies
- Theoretical guarantees are asymptotic or finite-sample bounds that may not reflect practical performance gaps
- Experimental setup reproducibility is limited by unspecified policy generation parameters and boosting mechanisms

## Confidence

- **High**: The core mechanism of KL-barycenter minimizing average KL divergence and the bound on importance weights are mathematically proven
- **Medium**: The theoretical claim that clustering reduces M·σ_c² is supported by Proposition 4.6 and Figure 1a, but relies on good clustering structure
- **Low**: The practical benefit of CKL-PE over baselines in complex, real-world policy sets is suggested by experiments but not rigorously validated beyond synthetic setup

## Next Checks

1. **Sensitivity to Target Policy Structure**: Generate policy sets with varying degrees of structure and verify that optimal M increases with structure amount, with CKL-PE providing no benefit when policies are unstructured.

2. **Diagnostic for Optimal Clustering**: Implement a diagnostic metric to evaluate clustering quality and predict whether CKL-PE will outperform single KL-barycenter for a given policy set.

3. **Robustness to Policy Generation**: Reproduce experiments with modified policy generation process using more precisely defined parameters to confirm consistent performance improvements.