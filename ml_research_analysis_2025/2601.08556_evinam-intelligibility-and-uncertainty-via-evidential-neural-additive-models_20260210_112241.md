---
ver: rpa2
title: 'EviNAM: Intelligibility and Uncertainty via Evidential Neural Additive Models'
arxiv_id: '2601.08556'
source_url: https://arxiv.org/abs/2601.08556
tags:
- uncertainty
- evinam
- neural
- feature
- additive
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes EviNAM, an extension of evidential learning
  that integrates the interpretability of Neural Additive Models (NAMs) with principled
  uncertainty estimation. Unlike standard Bayesian neural networks and previous evidential
  methods, EviNAM enables, in a single pass, both the estimation of aleatoric and
  epistemic uncertainty as well as explicit feature contributions.
---

# EviNAM: Intelligibility and Uncertainty via Evidential Neural Additive Models

## Quick Facts
- arXiv ID: 2601.08556
- Source URL: https://arxiv.org/abs/2601.08556
- Authors: Sören Schleibaum; Anton Frederik Thielmann; Julian Teusch; Benjamin Säfken; Jörg P. Müller
- Reference count: 5
- Primary result: EviNAM matches state-of-the-art predictive performance while providing both uncertainty estimates and feature contributions in a single pass

## Executive Summary
EviNAM extends evidential learning to Neural Additive Models (NAMs), enabling simultaneous estimation of aleatoric and epistemic uncertainty alongside interpretable feature contributions. Unlike standard Bayesian neural networks and previous evidential methods, EviNAM provides both uncertainty estimates and feature attributions in a single forward pass without sampling. The key innovation involves forwarding nonlinearity to the feature level to preserve additivity while satisfying distributional constraints, allowing explicit decomposition of each feature's contribution to distributional parameters.

## Method Summary
EviNAM parameterizes predictions using Normal-Inverse-Gamma (NIG) distributions with four parameters (γ, ν, α, β), where each feature has dedicated neural networks for each parameter. The model applies link functions (identity for γ, softplus for ν and β, softplus+1 for α) at the feature level before summation, preserving additivity. Uncertainty is computed directly from NIG parameters: aleatoric uncertainty as $u_{al,i} = \sqrt{\beta_i(1+\nu_i)/(\alpha_i\nu_i)}$ and epistemic uncertainty as $u_{ep,i} = \nu_i^{-1/2}$. The model is trained using NIG negative log-likelihood with a regularization term from Meinert et al. (2023).

## Key Results
- On 24 regression datasets, EviNAM achieves competitive performance with average rank 1.92 on CRPS and 1.92 on NLL
- EviNAM uniquely provides both uncertainty estimates and feature contributions, unlike competing methods
- For classification, EviNAM trails baseline methods slightly but remains competitive while offering interpretable feature-additive class probabilities
- Ablation studies confirm feature-level nonlinearity placement is crucial for maintaining additivity and performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Forwarding nonlinearity to the feature level preserves both additivity (interpretability) and distributional constraints (valid uncertainty estimates).
- Mechanism: Instead of applying nonlinear link functions after summing feature outputs—which violates additivity—EviNAM applies them to each feature's contribution individually before summation. For NIG parameters: $k_i = \phi_k(b_k) + \sum_j \phi_k(f^k_{\theta_j}(x_{ij}))$.
- Core assumption: The link function $\phi_k$ must be compatible with per-feature application; Assumption: this architectural choice does not fundamentally alter the learned distributional semantics.
- Evidence anchors:
  - [abstract] "The key innovation is forwarding nonlinearity to the feature level to preserve additivity while satisfying distributional constraints."
  - [section 3] "We propose to forward the nonlinearity to the feature-level for each distributional parameter by replacing Equation 6 with... to make the contribution of each feature to each distributional parameter explicit while preserving both the additivity... and the distribution's requirements."
  - [corpus] No direct corpus evidence on this specific mechanism; related work on evidential methods does not address additivity preservation.

### Mechanism 2
- Claim: Evidential learning enables single-pass, O(1) inference for both aleatoric and epistemic uncertainty without sampling.
- Mechanism: By placing a Normal-Inverse-Gamma (NIG) prior over the Gaussian likelihood parameters ($\mu \sim N(\gamma, \sigma^2\nu^{-1})$, $\sigma^2 \sim \Gamma^{-1}(\alpha, \beta)$), the network learns distributional parameters directly. Aleatoric uncertainty: $u_{al,i} = w_{St,i} = \sqrt{\beta_i(1+\nu_i)/(\alpha_i\nu_i)}$. Epistemic uncertainty: $u_{ep,i} = \nu_i^{-1/2}$.
- Core assumption: The NIG prior appropriately captures the epistemic uncertainty structure; Assumption: the refined loss from Meinert et al. (2023) resolves overparameterization issues sufficiently for practical use.
- Evidence anchors:
  - [abstract] "Unlike standard Bayesian neural networks and previous evidential methods, EviNAM enables, in a single pass, both the estimation of aleatoric and epistemic uncertainty as well as explicit feature contributions."
  - [section 3] "Unlike sampling-dependent Bayesian Neural Networks (BNNs) methods, DER enables explainees to validate predictions using uncertainty estimates in a single pass."
  - [corpus] ConfEviSurrogate (arXiv:2504.02919) similarly leverages evidential approaches for uncertainty quantification without sampling, supporting the general paradigm.

### Mechanism 3
- Claim: Feature-wise neural networks in NAM architecture provide interpretable contributions while maintaining competitive predictive performance.
- Mechanism: Each feature $j$ has dedicated neural networks $f^k_{\theta_j}$ for each distributional parameter $k \in \{\gamma, \nu, \alpha, \beta\}$. The contribution of feature $j$ to any output is explicitly $\phi_k(f^k_{\theta_j}(x_{ij}))$, enabling direct attribution.
- Core assumption: Feature interactions are negligible or can be approximated additively; Assumption: the interpretability benefit outweighs potential performance loss from disallowing interactions.
- Evidence anchors:
  - [section 3] Agarwal et al. (2021) "trained one neural network per feature... to learn a function per feature. Such Neural Additive Models (NAMs) can offer strong predictive performance, handle large datasets, and explicitly reveal all feature contributions."
  - [section 4, Table 2] EviNAM achieved average rank 2.38 on NLL and 1.92 on CRPS across 24 regression datasets, competitive with NAMLSS (1.17 and 1.29).

## Foundational Learning

- Concept: **Aleatoric vs. Epistemic Uncertainty**
  - Why needed here: EviNAM explicitly estimates both; understanding the distinction is essential for interpreting outputs. Aleatoric = irreducible data noise (e.g., sensor blur). Epistemic = model uncertainty from unfamiliar inputs.
  - Quick check question: If you collect more training data, which uncertainty type should decrease?

- Concept: **Normal-Inverse-Gamma (NIG) Distribution**
  - Why needed here: EviNAM parameterizes predictions via NIG parameters $(\gamma, \nu, \alpha, \beta)$. Understanding that $\gamma$ is the mean prediction, $\nu$ relates to epistemic uncertainty, and $\alpha, \beta$ govern variance is critical for implementation.
  - Quick check question: Which NIG parameter must exceed 1 to ensure finite variance?

- Concept: **Generalized Additive Models (GAMs)**
  - Why needed here: EviNAM inherits from GAMs the core principle that predictions decompose into sum of per-feature functions. This enables the interpretability guarantees.
  - Quick check question: Why can't a standard deep MLP with SHAP provide the same guarantees as an additive model?

## Architecture Onboarding

- Component map:
  Input features $x_j$ for $j \in \{1, ..., J\}$ -> Feature networks (4 per feature: one per NIG parameter) -> Link functions $\phi_k$ applied per-feature -> Sum aggregation per parameter + bias -> NIG parameters -> Output (mean, aleatoric uncertainty, epistemic uncertainty)

- Critical path:
  1. For each feature $j$, compute raw outputs from 4 feature networks
  2. Apply parameter-specific link functions to each
  3. Sum across all features per parameter
  4. Compute NIG loss and uncertainties via Eq. 3-4
  5. Backpropagate to update all feature networks jointly

- Design tradeoffs:
  - Interpretability vs. performance: DERMLP (non-additive) achieves MAE 0.23 vs. EviNAM's 0.49 on average; EviNAM provides explicit feature contributions
  - Training time vs. inference efficiency: EviNAM trains slower than NAM (120s vs. 82s per 100 epochs) but infers faster than BNNs (no sampling needed)
  - Uncertainty richness vs. parameter count: 4 NIG parameters vs. 2 for Gaussian NAMLSS; enables epistemic estimation at cost of complexity

- Failure signatures:
  - Negative or NaN $\alpha$ values: link function misconfigured
  - Epistemic uncertainty not increasing outside training domain: check $\nu$ network learning rate
  - MAE significantly worse than NAMLSS: feature interactions likely important; consider interaction terms
  - Calibration errors in classification: verify Dirichlet parameters $\alpha_c \geq 1$

- First 3 experiments:
  1. **Synthetic 1D/2D regression** ($y = x^3 + \epsilon$): Verify epistemic uncertainty increases outside training domain. Compare to Figure 1 patterns.
  2. **Ablation on nonlinearity placement**: Compare A1 (feature-level) vs. A2 (sum-level) link functions on 5 OpenML datasets. Expect A1 to maintain additivity and perform comparably.
  3. **California Housing interpretability check**: Replicate Figure 2—plot feature contributions to both prediction and uncertainties for `median_income` and `longitude`. Verify aleatoric uncertainty increases in sparse regions.

## Open Questions the Paper Calls Out
- Can EviNAM be extended to incorporate pairwise feature interactions while preserving the single-pass inference and interpretability characteristics?
- What specific factors contribute to EviNAM trailing baseline methods in classification tasks, and can this gap be closed without sacrificing uncertainty estimation?
- Does the additive constraint in EviNAM mitigate or exacerbate the known difficulties evidential learning faces in accurately quantifying epistemic uncertainty for out-of-distribution data?

## Limitations
- Additive Assumption: EviNAM's interpretability relies on feature additivity, which may not hold for complex interactions
- Computational Overhead: Training time is higher than standard NAMs (120s vs. 82s per 100 epochs), and maintaining 4 networks per feature increases parameter count
- Epistemic Uncertainty Calibration: Confidence is Medium in reliability for out-of-distribution detection, given recent critiques of evidential methods in this domain

## Confidence
- Mechanism 1 (Feature-level Nonlinearity): High confidence - Directly supported by architectural description and ablation results
- Mechanism 2 (Single-pass Uncertainty): High confidence - Core theoretical claim with empirical validation across multiple datasets
- Mechanism 3 (Interpretability): Medium confidence - Strong theoretical grounding, but real-world interpretability benefits depend on domain and data structure

## Next Checks
1. Test EviNAM with and without explicit feature interaction terms on datasets known to require interactions to quantify performance trade-offs
2. Evaluate epistemic uncertainty calibration on established OOD detection benchmarks (e.g., CIFAR-10 vs. SVHN) to verify reliability for safety-critical applications
3. Assess performance and training efficiency on high-dimensional datasets (>100 features) to determine practical scalability limits and identify optimal network architectures per feature