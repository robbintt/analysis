---
ver: rpa2
title: 'DESAMO: A Device for Elder-Friendly Smart Homes Powered by Embedded LLM with
  Audio Modality'
arxiv_id: '2508.18918'
source_url: https://arxiv.org/abs/2508.18918
tags:
- audio
- desamo
- speech
- language
- smart
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: DESAMO is an on-device smart home system that uses an embedded
  Audio LLM to enable natural voice interaction and emergency detection for elderly
  users. It directly processes raw audio through Qwen2.5-Omni 3B, avoiding the transcription
  errors common in ASR-based pipelines.
---

# DESAMO: A Device for Elder-Friendly Smart Homes Powered by Embedded LLM with Audio Modality

## Quick Facts
- arXiv ID: 2508.18918
- Source URL: https://arxiv.org/abs/2508.18918
- Reference count: 19
- DESAMO achieves 98% accuracy on elderly speech intent classification with a 3.45 GB model

## Executive Summary
DESAMO is an on-device smart home system that uses an embedded Audio LLM to enable natural voice interaction and emergency detection for elderly users. It directly processes raw audio through Qwen2.5-Omni 3B, avoiding the transcription errors common in ASR-based pipelines. The system supports intent classification via function calling and passive monitoring for falls or cries for help, all running locally on NVIDIA Jetson Orin Nano. In a pilot evaluation on elderly speech, DESAMO achieved 98% accuracy with only 3.45 GB of model size, outperforming Whisper + LLM cascades (96.33–97.33% accuracy, 3.64–5.20 GB). This design enhances privacy and robustness, with future work targeting multimodal inputs and reduced latency.

## Method Summary
DESAMO processes raw audio directly using an embedded Qwen2.5-Omni-3B model with Whisper large-v3 encoder on NVIDIA Jetson Orin Nano. The system implements function calling for intent classification and passive monitoring for emergencies. Evaluation used 300 samples from Fluent Speech Commands filtered for speakers aged ≥65. The pipeline includes trigger detection, audio recording, semantic embedding via encoder, LLM inference with task-specific prompts, and response parsing for device control or alerts.

## Key Results
- Achieved 98% intent classification accuracy on elderly speech (vs 96.33–97.33% for cascaded baselines)
- Model size of 3.45 GB (smaller than cascaded alternatives at 3.64–5.20 GB)
- Inference latency of ~5.3 seconds in headless mode
- Direct audio processing eliminates intermediate transcription errors

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Direct audio processing via Audio LLM reduces intent classification errors compared to ASR-LLM cascades, particularly for elderly speech with unclear articulation.
- Mechanism: Conventional pipelines transcribe speech to text before LLM processing, allowing transcription errors to propagate downstream. DESAMO bypasses this by encoding raw audio directly into semantic embeddings via a Whisper-based audio encoder, then feeding these to the LLM. This preserves acoustic-prosodic cues that aid intent disambiguation.
- Core assumption: The audio encoder extracts sufficient semantic content from raw waveforms that the LLM can reason over without intermediate text.
- Evidence anchors:
  - [abstract]: "DESAMO leverages an Audio LLM to process raw audio input directly... eliminating the need for intermediate text representations"
  - [section 1]: ASR-LLM cascades "remain vulnerable to transcription errors, particularly when processing the unclear articulation of elderly users"
  - [section 3, Table 1]: DESAMO achieved 98% accuracy vs 96.33–97.33% for cascaded baselines with smaller model size (3.45GB vs 3.64–5.20GB)
- Break condition: If audio input is severely degraded (e.g., extreme noise, very low volume), the encoder may fail to extract sufficient semantic signal, potentially underperforming cascades with robust ASR.

### Mechanism 2
- Claim: Audio-domain function calling enables structured device control from natural spoken commands without requiring precise syntax.
- Mechanism: Upon trigger detection, the system captures a short audio segment (e.g., "It's getting hot") and prompts the Audio LLM to generate a structured function call (e.g., `ACOn()`). The response layer parses this output to execute device actions. This extends text-based function calling paradigms to speech input.
- Core assumption: The 3B-scale model retains sufficient instruction-following capability after quantization to reliably map indirect speech to structured outputs.
- Evidence anchors:
  - [section 2.2]: "DESAMO extends function calling to the audio domain, allowing users to issue commands using natural speech rather than text-based input"
  - [section 2.2]: Prior work shows "2B to 7B scale models can effectively retrieve and generate functions based on user queries"
- Break condition: Poorly designed prompts or ambiguous utterances may yield incorrect or unparseable function calls; edge cases in indirect expressions may exceed model reasoning capacity.

### Mechanism 3
- Claim: Passive ambient audio monitoring through a unified Audio LLM enables emergency detection (falls, cries for help) alongside voice commands.
- Mechanism: The system samples short ambient audio segments at regular intervals, each processed with an emergency-detection prompt. The model outputs structured labels like `Alert('fall')` or `Alert('help')`, triggering notifications or alarms. This avoids separate specialized detectors.
- Core assumption: The model can discriminate emergency sounds from normal household audio with acceptable false positive/negative rates.
- Evidence anchors:
  - [abstract]: DESAMO enables "robust understanding of user intent and critical events, such as falls or calls for help"
  - [section 2.3]: "DESAMO uses passive audio monitoring to detect emergencies, such as falls or distress"
  - [corpus]: Weak direct evidence—neighbor papers address LLM integration, privacy, and security in smart homes but do not evaluate audio-based emergency detection.
- Break condition: High ambient noise or acoustically similar non-emergency sounds could cause false positives; quiet or distant emergency events may be missed.

## Foundational Learning

- Concept: **Audio LLMs and speech encoders**
  - Why needed here: DESAMO uses Qwen2.5-Omni 3B with a Whisper-based encoder to process raw audio. Understanding encoder-LLM integration is essential for debugging and optimization.
  - Quick check question: Explain how an Audio LLM differs from an ASR+text-LLM cascade in terms of information flow and potential failure modes.

- Concept: **Model quantization (FP16, INT4)**
  - Why needed here: The system uses 16-bit audio encoders and a 4-bit quantized LLM in GGUF format to fit within edge memory constraints.
  - Quick check question: What accuracy and latency tradeoffs are typically expected when reducing LLM precision from FP16 to INT4?

- Concept: **Function calling with LLMs**
  - Why needed here: Intent classification is implemented as function calling from audio to structured outputs like `Call('daughter')` or `ACOn()`.
  - Quick check question: How does function calling differ from standard text generation, and what prompt structures support reliable structured output?

## Architecture Onboarding

- Component map:
  Audio capture -> Audio encoder (Whisper large-v3) -> LLM (Qwen2.5-Omni 3B) -> Prompting layer -> Response layer -> Device control/alerts

- Critical path:
  1. Audio capture → encoder → semantic embeddings (~assumed sub-second)
  2. Embeddings + prompt → LLM inference → structured output (~majority of latency)
  3. Output parsing → device control or alert dispatch

- Design tradeoffs:
  - Accuracy vs. model size: DESAMO achieves 98% accuracy at 3.45GB, smaller than cascaded baselines
  - Latency vs. locality: ~5.3s inference in headless mode; local execution enhances privacy but limits responsiveness
  - Precision allocation: 16-bit encoder preserves audio fidelity; 4-bit LLM reduces memory footprint
  - Unified model vs. specialized detectors: Simplifies deployment but may lack granularity of dedicated emergency classifiers

- Failure signatures:
  - Trigger detection failures: System remains passive when users speak
  - High false positive rate: Frequent spurious alerts erode user trust
  - Latency spikes: Delayed responses make voice interaction feel unresponsive
  - Memory overflow: Model exceeds available edge RAM under concurrent workloads
  - Unparseable outputs: LLM generates malformed function calls the response layer cannot interpret

- First 3 experiments:
  1. Noise robustness test: Evaluate intent classification accuracy across SNR levels (clean, 10dB, 5dB, 0dB) to characterize performance degradation in realistic home environments.
  2. Emergency detection calibration: Collect ambient audio samples with labeled emergency/non-emergency events; measure precision, recall, and false positive rate per hour of monitoring.
  3. Latency profiling: Instrument each pipeline stage to quantify time spent in encoding vs. LLM inference vs. post-processing; identify bottlenecks for optimization.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can inference latency be reduced while maintaining accuracy?
- Answer: The paper suggests this as future work but doesn't provide concrete solutions. Potential approaches could include model distillation, more aggressive quantization, or hardware acceleration optimizations.

### Open Question 2
- Question: How can multimodal inputs (audio + visual) be integrated for enhanced context?
- Answer: The paper identifies this as an opportunity for future work, particularly for improving emergency detection and reducing false positives by incorporating visual cues from cameras.

### Open Question 3
- Question: What is the optimal balance between passive monitoring frequency and battery/power consumption?
- Answer: Not explicitly addressed, but would require empirical testing to determine the sweet spot between detection latency and resource usage.

## Limitations

- Emergency detection evaluation lacks quantitative metrics (precision, recall, false positive rate) despite being a core feature.
- The 98% accuracy claim is based on only 300 elderly speech samples, which is small for generalization claims.
- The 5.3s latency measurement is from headless mode without UI rendering, potentially underestimating real-world end-to-end delay.

## Confidence

- **High confidence**: Direct audio processing reduces transcription errors compared to ASR-LLM cascades, supported by the 1.67-1.67% accuracy improvement and smaller model size.
- **Medium confidence**: Audio function calling reliably maps natural speech to structured device commands, though edge cases and indirect expressions may exceed model reasoning capacity.
- **Low confidence**: Emergency detection performance metrics are missing; the system's false positive/negative rates and reliability in noisy environments are unknown.

## Next Checks

1. Measure emergency detection precision, recall, and false positive rate per hour on diverse ambient audio datasets.
2. Evaluate intent classification robustness across varying signal-to-noise ratios (clean, 10dB, 5dB, 0dB) to assess real-world performance.
3. Profile end-to-end latency including UI rendering and network communication to determine practical responsiveness.