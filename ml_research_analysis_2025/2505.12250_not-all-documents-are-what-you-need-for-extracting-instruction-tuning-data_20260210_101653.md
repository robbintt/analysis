---
ver: rpa2
title: Not All Documents Are What You Need for Extracting Instruction Tuning Data
arxiv_id: '2505.12250'
source_url: https://arxiv.org/abs/2505.12250
tags:
- data
- pairs
- documents
- arxiv
- equal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces EQUAL, a framework that efficiently extracts
  instruction-tuning data from web documents by iteratively selecting document clusters
  and extracting high-quality QA pairs using multi-armed bandits and optimal transport
  scoring. It addresses the challenge of prohibitive computational costs and irrelevant
  data in large-scale document processing.
---

# Not All Documents Are What You Need for Extracting Instruction Tuning Data

## Quick Facts
- arXiv ID: 2505.12250
- Source URL: https://arxiv.org/abs/2505.12250
- Reference count: 39
- This paper introduces EQUAL, a framework that efficiently extracts instruction-tuning data from web documents by iteratively selecting document clusters and extracting high-quality QA pairs using multi-armed bandits and optimal transport scoring.

## Executive Summary
This paper addresses the challenge of extracting high-quality instruction-tuning data from large web corpora while avoiding prohibitive computational costs. The authors propose EQUAL, a framework that combines contrastive learning for document embedding alignment, multi-armed bandit strategies for cluster selection, and optimal transport scoring for distributional relevance. By iteratively selecting document clusters and extracting QA pairs from only a subset of documents, EQUAL achieves 5-10x computational efficiency gains while improving downstream task accuracy by 2.5% on AutoMathText and StackOverflow datasets.

## Method Summary
EQUAL extracts instruction-tuning QA pairs through an iterative process: first, it samples 5% of documents to extract QA pairs and align document and QA embeddings via contrastive learning; second, it clusters all documents using K-Means on the aligned embeddings; third, it uses a multi-armed bandit framework to select clusters based on an optimal transport score plus exploration bonus, sampling documents from selected clusters and extracting QA pairs; finally, it fine-tunes LLaMA-3.1-8B or Mistral-7B models on the accumulated QA pairs. The framework balances exploration and exploitation to efficiently identify high-value clusters while maintaining data diversity.

## Key Results
- Reduces computational costs by 5-10x compared to exhaustive extraction methods
- Improves downstream task accuracy by 2.5% on AutoMathText and StackOverflow datasets
- Outperforms baseline extraction approaches across all tested settings
- Demonstrates effectiveness with both LLaMA-3.1-8B and Mistral-7B models

## Why This Works (Mechanism)

### Mechanism 1: Contrastive Learning for Document-QA Embedding Alignment
Documents contain irrelevant content that misaligns raw embeddings from their extractable QA pairs. During warm-up, sampling 5% of documents and using their QA pairs as positive examples while other pairs serve as negatives fine-tunes an embedding model to pull documents with similar QA pairs closer in embedding space. This alignment enables more coherent clustering by ensuring document embeddings reflect their QA content distribution.

### Mechanism 2: Multi-Armed Bandit (MAB) for Exploration-Exploitation in Cluster Selection
Each document cluster is treated as a bandit arm with a Document Sampling (DS) score combining estimated Optimal Transport scores (exploitation) with uncertainty bonuses based on sampling frequency (exploration). The upper confidence bound formula promotes under-sampled clusters to avoid local optima and improve data diversity. This approach enables sample-efficient identification of high-value clusters while maintaining diversity across the corpus.

### Mechanism 3: Optimal Transport (OT) Score for Distributional Relevance
Rather than selecting QA pairs based on individual similarity to reference samples, EQUAL computes minimal transportation cost between the empirical distribution of extracted QA pairs and the reference set. Lower OT scores indicate closer distributional alignment with the target task, providing a more robust reward signal than pointwise similarity metrics. This distribution-level measure ensures selected data aligns with the target task distribution rather than just individual examples.

## Foundational Learning

- **Concept: Multi-Armed Bandits (Upper Confidence Bound)**
  - Why needed: EQUAL relies on UCB-style balancing of exploration (sampling rarely-visited clusters) and exploitation (prioritizing high-OT clusters) to efficiently search document clusters.
  - Quick check: If you have 100 clusters and can only sample 5% of documents, how would UCB change your sampling compared to greedy selection?

- **Concept: Optimal Transport / Wasserstein Distance**
  - Why needed: The reward signal for bandit arms uses OT to measure distributional similarity between extracted QA and reference data, not just pointwise similarity.
  - Quick check: Why might OT be preferable to average cosine similarity when comparing two sets of embeddings?

- **Concept: Contrastive Learning for Cross-Modal Alignment**
  - Why needed: Documents and QA pairs live in different semantic spaces; contrastive learning projects them into a shared space where clustering is meaningful.
  - Quick check: What would happen to cluster quality if you skipped contrastive alignment and clustered raw document embeddings?

## Architecture Onboarding

- **Component map**: Warm-up Module → Embedding Alignment → Clustering Module → MAB Selector → QA Extraction Module → OT Scorer → Fine-tuning Pipeline

- **Critical path**: Warm-up → Embedding Alignment → Clustering → MAB Loop (Select Cluster → Sample → Extract QA → Update OT Score → Update DS Scores) → Accumulate QA Pairs → Fine-tune LLM

- **Design tradeoffs**:
  - Cluster count (k): Low k (~50) causes high within-cluster variance; high k (~50,000) fragments data, hindering exploration. Paper finds ~1000 robust (Elbow method).
  - Warm-up sample ratio: 5% chosen; smaller ratios risk poor alignment; larger ratios increase upfront LLM cost.
  - α schedule in DS score: Higher early α favors exploration; annealing favors exploitation. Formula provided but sensitivity not extensively ablated.
  - Reference set choice: Paper uses GSM8K/MBPP training splits; domain mismatch would degrade OT guidance.

- **Failure signatures**:
  - High variance in OT scores across iterations: Likely cluster heterogeneity; consider increasing k or re-clustering.
  - Accuracy plateaus early: MAB may be over-exploiting; check α schedule or sampling frequency T(Cj).
  - Computational cost higher than expected: Warm-up ratio or per-iteration sample size too large; monitor FLOPs across extraction stages.

- **First 3 experiments**:
  1. Ablate warm-up contrastive learning (no-CL) on a held-out domain: Compare clustering visualization (t-SNE) and downstream accuracy to validate alignment contribution.
  2. Sweep cluster count (k=50, 500, 1000, 5000, 20000) on AutoMathText subset: Plot accuracy vs. k to verify Elbow method and identify sensitivity bounds.
  3. Compare reward functions (OT vs. average similarity vs. perplexity) within MAB framework: Isolate OT contribution using identical MAB and clustering settings.

## Open Questions the Paper Calls Out

### Open Question 1
Can EQUAL be integrated with advanced intra-document extraction methods to address the formatting and hallucination issues identified as orthogonal to this work? The study isolates document selection strategies from the extraction mechanism, leaving the potential synergy between selective sampling and extraction quality unexplored.

### Open Question 2
How does EQUAL perform in domains where high-quality, task-specific reference sets ($D_r$) are unavailable or ill-defined? The framework relies on Optimal Transport scores calculated against specific reference sets, which may not exist for general domains.

### Open Question 3
How robust is the framework to noise or distribution shifts in the reference set ($D_r$)? If the reference set contains noise, the optimal transport reward might guide the bandit algorithm to select clusters that degrade model performance rather than enhance it.

### Open Question 4
Is the fixed 5% warm-up ratio for contrastive learning optimal across varying corpus sizes and diversities? A fixed ratio might be insufficient for highly heterogeneous corpora or wasteful for homogeneous data.

## Limitations

- Primary empirical validation limited to two domains (Math and Code), leaving generalization to broader instruction-following tasks untested
- Computational efficiency gains contingent on cluster homogeneity assumptions that may not hold for more diverse corpora
- Reliance on single reference set per domain creates potential overfitting risks if reference distributions poorly represent target tasks
- Warm-up phase's 5% sampling ratio introduces upfront computational overhead that may offset gains for smaller document collections

## Confidence

- **High Confidence**: The MAB + OT scoring framework for cluster selection is technically sound and the reported computational efficiency improvements (5-10x reduction in FLOPs) are well-supported by the experimental methodology.
- **Medium Confidence**: The downstream accuracy improvements (2.5% gain) are promising but require broader validation across more diverse instruction-tuning tasks.
- **Low Confidence**: The claim that EQUAL "extracts higher quality data" is primarily validated through downstream task performance rather than intrinsic data quality metrics.

## Next Checks

1. **Cross-Domain Robustness Test**: Apply EQUAL to a general instruction-following dataset (e.g., FLAN, Alpaca) using diverse reference sets to evaluate whether the framework generalizes beyond Math and Code domains.

2. **Intrinsic Quality Assessment**: Develop automated metrics (factuality scores, coherence measures) to evaluate extracted QA pairs independently of downstream task performance, comparing EQUAL outputs against baseline extraction methods.

3. **Scalability Analysis**: Systematically vary document corpus size (10K → 1M → 10M documents) to quantify how computational gains scale and identify breaking points where cluster homogeneity assumptions fail.