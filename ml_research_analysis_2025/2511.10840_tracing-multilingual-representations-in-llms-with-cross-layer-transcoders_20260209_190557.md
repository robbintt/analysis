---
ver: rpa2
title: Tracing Multilingual Representations in LLMs with Cross-Layer Transcoders
arxiv_id: '2511.10840'
source_url: https://arxiv.org/abs/2511.10840
tags:
- multilingual
- english
- language
- across
- layers
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates how multilingual large language models
  internally represent and process different languages using Cross-Layer Transcoders
  (CLTs) and attribution graphs. The authors train models on various multilingual
  data mixtures and find that models consistently form shared multilingual representations
  in middle layers regardless of English dominance, with language-specific decoding
  emerging in later layers.
---

# Tracing Multilingual Representations in LLMs with Cross-Layer Transcoders

## Quick Facts
- arXiv ID: 2511.10840
- Source URL: https://arxiv.org/abs/2511.10840
- Reference count: 40
- Key outcome: Models consistently form shared multilingual representations in middle layers regardless of English dominance, with language-specific decoding emerging in later layers.

## Executive Summary
This paper investigates how multilingual large language models internally represent and process different languages using Cross-Layer Transcoders (CLTs) and attribution graphs. The authors train models on various multilingual data mixtures and find that models consistently form shared multilingual representations in middle layers regardless of English dominance, with language-specific decoding emerging in later layers. Language identity is encoded linearly from early layers and primarily decoded through a small set of high-frequency features in final layers. Model-diffing experiments reveal that non-English underperformance stems from weak late-layer features, insufficient middle-layer cluster activation, and tokenization bias toward English. Targeted fine-tuning strengthens these features and their connections, improving token assembly and language-specific decoding, providing a mechanistic explanation for multilingual gaps.

## Method Summary
The study uses GPT-2 style models (12L, 177M) and TinyStories (4L, 68M) trained on multilingual data mixtures with varying English dominance (20%-90%). Cross-Layer Transcoders decompose MLP activations into interpretable features, enabling attribution graphs that track feature interactions across layers. The analysis pipeline includes CLT training, multilingual entropy score computation (H(f)), attribution graph extraction with pruning, feature clustering, and intervention validation. Models are trained from scratch with balanced tokenization, and fine-tuning experiments validate the mechanistic explanations for performance gaps.

## Key Results
- Multilingual models form shared representations in middle layers regardless of English dominance
- Language identity is encoded linearly from early layers and decoded via high-frequency features in final layers
- Non-English underperformance stems from weak late-layer features, under-activated middle-layer clusters, and tokenizer bias
- Targeted fine-tuning strengthens these features and connections, improving multilingual performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multilingual LLMs form shared language-agnostic representations in middle layers regardless of training data dominance.
- Mechanism: Early layers encode language-specific features (low entropy), middle layers integrate into shared multilingual space (high entropy), late layers re-specialize for language-specific decoding (lower entropy). This U-shaped entropy pattern emerges from the model's need to first parse language-specific surface forms, then extract abstract semantics, then generate language-specific outputs.
- Core assumption: The entropy-based multilingual score (H(f)) correctly captures cross-lingual feature sharing.
- Evidence anchors:
  - [abstract] "models consistently form shared multilingual representations in middle layers regardless of English dominance"
  - [Section 4.1.2] "Layerwise entropy follows a U-shaped trend... middle layers integrate information into a shared multilingual space"
  - [corpus] "Cross-Lingual Generalization and Compression" finds similar language-specific to shared neuron transitions during pre-training
- Break condition: Very shallow models (<4 layers) may lack depth for distinct phase separation—the paper notes 4-layer TinyStories models show uniform entropy across layers.

### Mechanism 2
- Claim: Language-specific decoding relies on a small set of high-frequency features in final layers that linearly read language identity from early layers.
- Mechanism: Final layers contain language-specific features that activate on 50-100% of tokens within their target language. These features receive linear projections from early-layer features and token embeddings, propagating language identity hierarchically. Intervening on these features can suppress one language and substitute another.
- Core assumption: Language identity information is preserved through the network via linear pathways.
- Evidence anchors:
  - [abstract] "Decoding relies partly on a small set of high-frequency features in the final layers, which linearly encode language identity from early layers"
  - [Section 4.2] "Intervening on these features allows one language to be suppressed and another substituted"
  - [corpus] Weak direct evidence—neighbor papers focus on neuron-level analysis rather than feature-frequency decoding mechanisms
- Break condition: Sequence beginnings where language identity is ambiguous show inactive or confused high-frequency features (Figure 5).

### Mechanism 3
- Claim: Non-English performance gaps stem from weak late-layer feature activation, under-activated middle-layer clusters, and tokenizer-induced fragmentation—not missing multilingual circuits.
- Mechanism: Tokenization bias fragments non-English words (especially Arabic: 89% meaningless fragments vs. 58% English), forcing early layers to reassemble tokens rather than activate semantic clusters. This weakens edge strength to downstream circuits. Fine-tuning strengthens these features and connections, improving token assembly and decoding.
- Core assumption: Feature activation strength, not circuit existence, determines performance.
- Evidence anchors:
  - [abstract] "underperformance arises from dim late-layer features, weak middle-layer clusters, and tokenizer bias toward English"
  - [Section 4.3.1] "The largest changes occur in the later layers" after fine-tuning
  - [Appendix K.3] "Fragmented tokens lack semantic content, forcing early layers (0–3) to first reconstruct word boundaries"
  - [corpus] "The Transfer Neurons Hypothesis" proposes similar early-layer language conversion → middle-layer reasoning → late-layer output framework
- Break condition: Even with balanced tokenizers, morphological properties of languages (e.g., Arabic's non-concatenative morphology) inherently cause more fragmentation.

## Foundational Learning

- Concept: Cross-Layer Transcoders (CLTs)
  - Why needed here: CLTs decompose MLP outputs into interpretable features with layer-specific decoder matrices, enabling attribution graphs that track feature interactions across layers—essential for understanding how multilingual representations transform through the network.
  - Quick check question: Can you explain why CLTs are preferable to Sparse Autoencoders for circuit analysis across layers?

- Concept: Attribution Graphs with Pruning
  - Why needed here: Attribution graphs quantify feature-to-feature influence via Jacobian computations. Pruning (retaining features for 80% cumulative logit effect, edges for 95%) makes multilingual circuits tractable for analysis.
  - Quick check question: What does the Jacobian term J capture in the attribution score formula?

- Concept: Multilingual Entropy Score
  - Why needed here: H(f) = -Σ p_l(f) log p_l(f) quantifies whether a feature is language-specific (low entropy) or multilingual (high entropy). This metric reveals the U-shaped layerwise organization of multilingual processing.
  - Quick check question: If a feature activates equally across 5 languages, what is its normalized entropy score?

## Architecture Onboarding

- Component map:
  - Tokenizer: Debiased BPE on balanced 5-language data (119,547 vocab)
  - Model variants: GPT-2 style (12L, 177M) and TinyStories (4L, 68M) trained on 20%/50%/70%/90% English mixtures
  - CLT architecture: Per-layer encoder W_enc^ℓ, cross-layer decoders W_dec^{ℓ→ℓ′}, ~24K features with JumpReLU activation
  - Analysis pipeline: CLT training → Attribution graph extraction → Pruning → Circuit clustering → Intervention validation

- Critical path:
  1. Train CLT on balanced multilingual activations until L0 < 10 and explained variance > 70%
  2. Compute multilingual scores H(f) for all features
  3. Extract attribution graphs for target prompts across languages
  4. Cluster features by entropy and interpretability descriptions
  5. Validate causality via steering interventions on clusters

- Design tradeoffs:
  - Expansion factor (32× for GPT-2, 4× for LLaMA due to memory): higher = more granular features but more expensive
  - Pruning thresholds (80% nodes, 95% edges): stricter = cleaner graphs but may miss weak multilingual connections
  - Tokenizer balancing: fair vocabulary allocation doesn't solve morphological fragmentation issues

- Failure signatures:
  - Shallow models (<4 layers): Uniform entropy, no distinct multilingual phase
  - High English dominance (90%): Missing clusters in non-English circuits (e.g., Men&Women cluster absent in Arabic)
  - Fragmented tokens: Weak edge strength from embeddings to semantic clusters, early layers consumed by word reassembly
  - Dead features: High dead counts in early/late layers (check Figures 8-10)

- First 3 experiments:
  1. **Replicate entropy analysis**: Train a 12L model on 20% English data, compute rate-weighted multilingual scores across layers, verify U-shaped pattern with peak at layers 5-6.
  2. **Intervention validation**: For a failed Arabic prediction, identify missing cluster via graph analysis, construct steering vector from cluster features, sweep scaling coefficients (1-30 for reasoning, -30 to -1 for copying) to restore correct output.
  3. **Tokenizer fragmentation quantification**: For each language, compute token fertility and morphological coherence on held-out data, correlate with edge strength to task-relevant clusters.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal tokenization strategy for multilingual models to minimize performance gaps across languages?
- Basis in paper: [explicit] The Limitations section states: "We do not systematically compare alternative tokenization strategies (e.g., character-level, morpheme-aware, or language-specific tokenizers) that might better preserve semantic information across scripts. The optimal tokenization approach for multilingual models remains an open question."
- Why unresolved: The study identifies tokenization fragmentation as a key failure mechanism (Arabic shows 89% meaningless fragments vs. 58% for English) but only tests a single BPE tokenizer trained on balanced data.
- What evidence would resolve it: Systematic comparison of morpheme-aware, character-level, and language-specific tokenizers measuring downstream multilingual performance and circuit activation patterns.

### Open Question 2
- Question: Do the observed multilingual mechanisms (shared middle-layer representations, late-layer language-specific decoding) scale to production-scale models (10B+ parameters)?
- Basis in paper: [explicit] The Limitations section notes: "our findings may not fully generalize to production-scale models (10B+ parameters) or models trained with different architectural choices. Larger models may develop additional mechanisms or more nuanced multilingual representations."
- Why unresolved: Primary analysis uses 68M-177M parameter models, with only LLaMA-3.2-1B for validation. CLT training at scale remains computationally prohibitive.
- What evidence would resolve it: Training CLTs on models ≥10B parameters and comparing entropy curves, circuit structures, and failure modes across scales.

### Open Question 3
- Question: What is the minimum model depth required for the U-shaped multilingual entropy pattern (shared middle-layer space) to emerge?
- Basis in paper: [inferred] The paper notes: "For the 4-layer TinyStories model, the up-and-down pattern is absent and all layers have similar entropy scores, suggesting a minimum model size for this behavior to emerge." No further investigation identifies the threshold.
- Why unresolved: Only 4-layer and 12-layer models were tested; the transition point and architectural factors (attention heads, FFN dimension) remain unexplored.
- What evidence would resolve it: Systematic training of models with varying depths (5-12 layers) while controlling for total parameters, measuring when the U-shaped entropy pattern stabilizes.

### Open Question 4
- Question: What is the complete set of features and intervention magnitudes required for reliable cross-lingual output control?
- Basis in paper: [inferred] The paper demonstrates language-switching interventions but states: "Perfect prediction typically requires additional feature modifications or sweeping intervention values." The intervention protocol is validated on specific cases but not generalized.
- Why unresolved: Interventions succeed on antonym/category tasks but may not transfer to open-ended generation; the feature space may be incomplete due to CLT reconstruction limits (75-80% explained variance).
- What evidence would resolve it: Large-scale intervention benchmarking across diverse task types with automated sweep procedures, combined with analysis of reconstruction error effects on intervention fidelity.

## Limitations
- The paper identifies tokenization fragmentation as a key failure mechanism but only tests a single BPE tokenizer, without comparing alternative tokenization strategies.
- Findings may not generalize to production-scale models (10B+ parameters) or models with different architectural choices.
- The entropy-based multilingual score captures feature sharing patterns but doesn't directly measure semantic equivalence across languages.

## Confidence
- **High confidence**: U-shaped layerwise entropy pattern, language-specific features in final layers, tokenizer-induced fragmentation effects
- **Medium confidence**: Causal attribution of non-English underperformance to weak late-layer features and under-activated middle-layer clusters
- **Medium confidence**: CLT-based circuit analysis capturing cross-layer feature interactions

## Next Checks
1. **Intervention specificity test**: For each target language, identify the minimum feature subset required for successful language substitution through systematic ablation studies, measuring how many features can be removed while maintaining steering effectiveness.

2. **Cross-architectural validation**: Apply the same CLT and attribution graph analysis to a transformer variant (e.g., Mamba or RWKV) trained on identical multilingual data to test whether the U-shaped entropy pattern and shared representation emergence are architecture-dependent or general phenomena.

3. **Real-time token-by-token analysis**: Instrument the fine-tuned model to log feature activation patterns during generation, measuring whether strengthened middle-layer clusters show immediate improvement in token assembly quality before late-layer language-specific decoding occurs.