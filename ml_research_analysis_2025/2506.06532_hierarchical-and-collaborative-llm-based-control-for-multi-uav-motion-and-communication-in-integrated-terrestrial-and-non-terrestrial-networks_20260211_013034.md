---
ver: rpa2
title: Hierarchical and Collaborative LLM-Based Control for Multi-UAV Motion and Communication
  in Integrated Terrestrial and Non-Terrestrial Networks
arxiv_id: '2506.06532'
source_url: https://arxiv.org/abs/2506.06532
tags:
- haps
- action
- control
- each
- aerial
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work proposes a hierarchical and collaborative LLM-based control
  framework for multi-UAV motion and communication in integrated terrestrial and non-terrestrial
  networks. The framework deploys an LLM on the HAPS for global network orchestration
  and separate LLMs on each UAV for local motion planning and communication decisions.
---

# Hierarchical and Collaborative LLM-Based Control for Multi-UAV Motion and Communication in Integrated Terrestrial and Non-Terrestrial Networks

## Quick Facts
- arXiv ID: 2506.06532
- Source URL: https://arxiv.org/abs/2506.06532
- Reference count: 19
- Key outcome: Proposed LLM-based hierarchical framework achieves higher rewards, lower costs, and reduced UAV collision rates compared to deep RL baselines in integrated aerial-terrestrial networks.

## Executive Summary
This work introduces a hierarchical and collaborative LLM-based control framework for managing multi-UAV motion and communication in integrated terrestrial and non-terrestrial networks. The framework deploys an LLM on the HAPS for global network orchestration and separate LLMs on each UAV for local motion planning and communication decisions. The HAPS-level LLM manages UAV access control and load balancing, while UAV-level LLMs handle acceleration, deceleration, and lane changes. Experimental results show the proposed method achieves higher system rewards, lower operational costs, and significantly reduced UAV collision rates compared to baseline deep reinforcement learning approaches across varying traffic densities.

## Method Summary
The proposed method implements a two-tier LLM framework where a HAPS-level LLM serves as a meta-controller for global network orchestration, handling UAV access control and load balancing, while UAV-level LLMs function as edge-agents for local motion planning and communication decisions. The system uses in-context learning through few-shot prompting, where the LLM receives serialized state information and past experiences in text format rather than through traditional weight updates. The framework operates on a discretized state space with specific action sets for both transportation (acceleration, deceleration, lane changes) and telecommunication (association, admission control). The MDP formulation at both levels enables cooperative decision-making across the network, with joint reward functions that balance transportation efficiency against communication quality.

## Key Results
- The hierarchical LLM framework achieved higher total rewards compared to DDQN baseline across all UAV densities tested
- Collision rates were significantly lower than baseline methods, with the largest improvement observed at high UAV densities (40 UAVs)
- Telecommunication costs were reduced through effective load balancing and handover management by the HAPS-level LLM

## Why This Works (Mechanism)

### Mechanism 1: Hierarchical Task Decomposition
- **Claim:** The framework reduces decision-making complexity by separating global network orchestration from local kinematic control.
- **Mechanism:** A HAPS LLM acts as meta-controller managing bandwidth quotas and handovers, while edge-based LLMs on UAVs handle second-to-second motion. This prevents a single agent from modeling the full state space of both network traffic and 3D motion dynamics simultaneously.
- **Core assumption:** The optimization landscape can be cleanly split into "network-wide resource allocation" and "local collision avoidance" without losing significant optimality.
- **Evidence anchors:** [abstract] "an LLM deployed on the HAPS performs UAV access control, while another LLM onboard each UAV handles motion planning..."; [section 3] "This two-tier LLM framework enables cooperative decision-making across meta-control and edge-control levels..."

### Mechanism 2: In-Context Policy Inference
- **Claim:** The system substitutes traditional gradient-based policy updates with semantic reasoning over state descriptions and past experiences.
- **Mechanism:** The system feeds current state and "good/bad" past experiences directly into the LLM context window, predicting the next action by analogizing from examples (few-shot learning) and following explicit textual rules.
- **Core assumption:** The LLM's pre-trained knowledge of spatial relationships and "traffic" concepts transfers effectively to aerial highways.
- **Evidence anchors:** [abstract] "...leverages the rich knowledge embedded in pre-trained models..."; [appendix] Shows explicit prompt structures containing "Environment Features," "Rules," and "Experience Replay" examples.

### Mechanism 3: Joint Communication-Motion Reward Shaping
- **Claim:** Success is driven by a coupled reward function that forces the LLM to trade off speed against connectivity costs.
- **Mechanism:** The reward function penalizes collisions, lane-change frequency, and handover overhead. By presenting this as a "Task Goal" in the prompt, the LLM learns to slow down or change lanes not just for physics, but to maintain stable links.
- **Core assumption:** The text-based description of the reward function is sufficient for the LLM to balance these competing objectives effectively.
- **Evidence anchors:** [section 3.2.3] "The transportation reward... and telecommunication reward... yields a Pareto-efficient trade-off..."; [results] Fig 4 shows reduced collision rates and telecommunication costs simultaneously.

## Foundational Learning

- **Concept: Markov Decision Processes (MDPs)**
  - **Why needed here:** The paper explicitly models both the HAPS and UAV control loops as MDPs (States, Actions, Transitions, Rewards). Understanding state-space formulation is required to interpret the input matrices.
  - **Quick check question:** Can you identify the state vector components for a single UAV listed in Section 3.2.1?

- **Concept: In-Context Learning (Few-Shot Prompting)**
  - **Why needed here:** The mechanism relies on feeding "Experience Replay" examples into the prompt (Appendix B). This is distinct from fine-tuning; the model learns from the prompt context at inference time.
  - **Quick check question:** How does the "Experience Replay" block in the sample prompt (Appendix A) differ from a standard Replay Buffer used in Deep Q-Learning?

- **Concept: Non-Terrestrial Networks (NTN) & HAPS**
  - **Why needed here:** The physics of the problem depend on Line-of-Sight (LoS) probabilities and specific path-loss models between UAVs, Ground BSs, and HAPS.
  - **Quick check question:** According to Equation (6), under what condition is the LoS probability equal to 1?

## Architecture Onboarding

- **Component map:** HAPS Meta-Controller (LLM) -> UAV Edge-Agent (LLM) -> Simulator (Environment) -> Prompt Generator (Interface)
- **Critical path:**
  1. **Serialize:** Numerical state (matrices) â†’ Text prompt (Task Description + Rules + History)
  2. **Infer:** LLM generates text output `<meta_action>` and `<tran_action>`
  3. **Parse:** Extract action tokens from LLM output
  4. **Execute:** Apply action to simulator; calculate reward
  5. **Update:** Store experience in the buffer for future prompting

- **Design tradeoffs:**
  - **Inference Latency vs. Context Size:** Using larger LLMs improves reasoning but introduces control loop delay. The paper mentions using Ollama on P100 GPUs, implying a specific latency budget.
  - **Centralized vs. Distributed:** The HAPS LLM reduces "ping-pong" handovers (centralized view) but creates a single point of failure for network logic.

- **Failure signatures:**
  - **Action Hallucination:** The LLM outputs an action not in the rule set (e.g., "Move Up" when only "Left/Right" exist). *Mitigation:* Constrained decoding or regex parsing.
  - **Context Overflow:** With >40 UAVs, the text description of all neighbors exceeds token limits. *Mitigation:* Filter to only "nearest neighbors" (as suggested by the observation matrix size in Appendix B).

- **First 3 experiments:**
  1. **Sanity Check (Static Prompt):** Input the exact prompt from Appendix A into an LLM API. Verify it returns the exact XML tag format `<meta_action>Offload{4}</meta_action>`.
  2. **Single-Agent Loop:** Run one UAV in the simulator with the "Edge-Agent" prompt. Verify it does not crash immediately and that it successfully parses the reward string.
  3. **Scaling Stress Test:** Increase UAV density to 10+ to observe if the LLM can still maintain collision rates lower than the DDQN baseline (Fig 4c) before hitting context limits.

## Open Questions the Paper Calls Out
- **Question:** How can the framework be adapted to maintain robust control in GPS-denied or urban canyon environments where precise positioning is unavailable?
- **Question:** Does integrating multi-modal sensory inputs (e.g., vision, LiDAR) directly into the LLM control loop improve robustness compared to the current state-vector approach?
- **Question:** Can the LLM-based control loop consistently meet the strict real-time latency requirements necessary for high-speed collision avoidance?

## Limitations
- **Limitation 1:** The paper does not specify a detailed policy update mechanism for LLM-based agents, creating uncertainty about long-term learning capability and convergence properties.
- **Limitation 2:** Critical hyperparameters such as reward weights, exact LLM model specifications for the proposed method, and state discretization schemes are not disclosed.
- **Limitation 3:** The simulation setup lacks validation in real-world conditions and does not address potential single points of failure in the HAPS-based centralized control architecture.

## Confidence
- **High Confidence:** The hierarchical task decomposition mechanism and its implementation details are well-specified through the MDP formulation and prompt structure.
- **Medium Confidence:** The experimental results showing improved performance over DDQN baselines, as the methodology is described but key implementation details (LLM model, hyperparameters) are missing.
- **Low Confidence:** The learning dynamics and policy update mechanism, as the paper mentions experience replay but provides no concrete algorithm for how the LLM policy is updated or trained.

## Next Checks
1. **Prompt Format Validation:** Input the exact prompt template from Appendix A into an LLM API (e.g., GPT-4) to verify it produces the expected XML-formatted action output format `<meta_action>Offload{4}</meta_action>`.

2. **Single-Agent Stability Test:** Implement one UAV in the simulator with the edge-agent prompt structure. Run 1000 timesteps and verify it maintains stable flight without collisions and successfully parses reward feedback from the environment.

3. **Context Window Scaling Analysis:** Gradually increase UAV density from 5 to 40+ agents while monitoring collision rates and inference latency. Identify the point where performance degrades due to context window limitations or where the LLM cannot maintain superiority over the DDQN baseline.