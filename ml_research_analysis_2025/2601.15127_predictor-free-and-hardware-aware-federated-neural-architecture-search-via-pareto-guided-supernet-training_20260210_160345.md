---
ver: rpa2
title: Predictor-Free and Hardware-Aware Federated Neural Architecture Search via
  Pareto-Guided Supernet Training
arxiv_id: '2601.15127'
source_url: https://arxiv.org/abs/2601.15127
tags:
- search
- supernet
- federated
- training
- deepfednas
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: DeepFedNAS introduces a Pareto-guided approach to federated neural
  architecture search that overcomes the limitations of unguided supernet training.
  The method uses a multi-objective fitness function combining network entropy, effectiveness,
  and structural penalties to evaluate architectures.
---

# Predictor-Free and Hardware-Aware Federated Neural Architecture Search via Pareto-Guided Supernet Training

## Quick Facts
- arXiv ID: 2601.15127
- Source URL: https://arxiv.org/abs/2601.15127
- Authors: Bostan Khan; Masoud Daneshtalab
- Reference count: 40
- Primary result: Achieves up to 1.21% accuracy improvement on CIFAR-100 with 61× speedup in search pipeline

## Executive Summary
DeepFedNAS introduces a novel Pareto-guided approach to federated neural architecture search that addresses the limitations of unguided supernet training. The framework employs a multi-objective fitness function combining network entropy, effectiveness, and structural penalties to evaluate architectures, enabling predictor-free search that directly optimizes for optimal subnets. By pre-computing a cache of Pareto-optimal subnets and training the supernet using a guided curriculum, the method achieves superior accuracy while dramatically reducing search time from hours to seconds. The approach demonstrates robust performance under non-IID federated conditions while maintaining hardware awareness.

## Method Summary
The DeepFedNAS framework introduces a Pareto-guided supernet training approach that overcomes the limitations of random sampling in traditional federated neural architecture search. The method employs a multi-objective fitness function that evaluates architectures based on network entropy (to measure diversity and effectiveness), effectiveness metrics, and structural penalties to prevent degenerate solutions. A key innovation is the pre-computation of a cache containing Pareto-optimal subnets, which guides the supernet training process through a curriculum of high-fitness architectures rather than random sampling. This predictor-free approach directly optimizes the fitness function during search, enabling the identification of optimal subnets in seconds rather than hours. The framework maintains hardware awareness throughout the search process while demonstrating strong performance under non-IID federated conditions.

## Key Results
- Achieves up to 1.21% absolute accuracy improvement on CIFAR-100 compared to baseline methods
- Delivers 61× speedup in post-training search pipeline time
- Maintains strong performance under non-IID federated conditions
- Enables predictor-free search that finds optimal subnets in seconds rather than hours

## Why This Works (Mechanism)
The Pareto-guided approach works by addressing the fundamental limitation of random sampling in supernet training, which often results in inefficient exploration of the architecture space and suboptimal final models. By using a multi-objective fitness function that balances network entropy, effectiveness, and structural considerations, the method ensures that the supernet is trained on architectures that are both diverse and high-performing. The pre-computed cache of Pareto-optimal subnets serves as a curriculum that guides the training process toward promising regions of the architecture space, avoiding the pitfalls of unguided exploration. This systematic approach to architecture selection during supernet training enables the predictor-free search to efficiently identify optimal subnets without requiring additional training or expensive predictor models.

## Foundational Learning
- **Multi-objective optimization**: Balances competing objectives (accuracy, efficiency, diversity) through Pareto optimality. Why needed: Single-objective approaches often fail to capture the complex tradeoffs in neural architecture design. Quick check: Verify that selected architectures represent true Pareto fronts by testing dominance relationships.
- **Network entropy in architecture search**: Measures the diversity and effectiveness of architectural components. Why needed: Ensures exploration of diverse architectural space while maintaining performance. Quick check: Monitor entropy values during training to ensure sufficient diversity.
- **Federated learning under non-IID conditions**: Addresses the challenge of training across clients with different data distributions. Why needed: Real-world federated scenarios rarely have IID data, requiring robust approaches. Quick check: Evaluate performance degradation under varying degrees of data heterogeneity.
- **Supernet training dynamics**: Understanding how different sampling strategies affect final model quality. Why needed: The quality of the supernet directly impacts the effectiveness of the subsequent search. Quick check: Compare architecture rankings from supernet with standalone training results.
- **Hardware-aware neural architecture search**: Incorporates device constraints into the optimization process. Why needed: Ensures that discovered architectures are practical for deployment. Quick check: Verify that top architectures meet specified hardware constraints.

## Architecture Onboarding
- **Component map**: Data clients -> Federated aggregation -> Supernet training (Pareto-guided) -> Fitness evaluation -> Architecture search
- **Critical path**: Architecture evaluation (fitness function) → Supernet training with guided sampling → Pareto-optimal cache generation → Final architecture selection
- **Design tradeoffs**: The method trades pre-computation overhead (Pareto cache generation) for faster search time and better final accuracy. The guided sampling approach requires careful calibration of fitness function weights but eliminates the need for predictor models.
- **Failure signatures**: Poor performance may indicate: (1) miscalibrated fitness function weights leading to suboptimal sampling, (2) insufficient diversity in Pareto cache causing premature convergence, or (3) inadequate supernet training preventing accurate architecture ranking.
- **First experiments**: (1) Verify Pareto optimality of cached architectures through dominance testing, (2) Ablation study on fitness function weight combinations to identify optimal settings, (3) Test search time versus accuracy tradeoff curve to validate the 61× speedup claim.

## Open Questions the Paper Calls Out
None

## Limitations
- Requires careful calibration of entropy and effectiveness weights for different datasets or tasks
- Caching mechanism demands significant computational resources for pre-computing Pareto-optimal subnets, potentially limiting scalability
- Performance generalization to complex vision tasks beyond CIFAR-100/CIFAR-10 remains unproven
- Assumes homogeneous client capabilities in federated setup, not reflecting real-world heterogeneity

## Confidence
- Accuracy improvements on CIFAR-100: High
- 61× speedup claim: Medium (dependent on specific hardware setup)
- Performance under non-IID conditions: High
- Predictor-free search efficacy: Medium (needs validation on larger datasets)

## Next Checks
1. Evaluate performance on ImageNet-scale datasets to verify scalability of the Pareto-guided approach
2. Test the framework with heterogeneous client capabilities in federated settings to assess real-world applicability
3. Conduct ablation studies on different entropy-effectiveness weight combinations across multiple datasets to validate robustness of hyperparameter choices