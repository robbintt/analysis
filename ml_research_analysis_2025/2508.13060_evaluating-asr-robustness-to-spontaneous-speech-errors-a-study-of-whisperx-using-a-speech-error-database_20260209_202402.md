---
ver: rpa2
title: 'Evaluating ASR robustness to spontaneous speech errors: A study of WhisperX
  using a Speech Error Database'
arxiv_id: '2508.13060'
source_url: https://arxiv.org/abs/2508.13060
tags:
- error
- errors
- speech
- word
- transcription
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study introduces the Simon Fraser University Speech Error
  Database (SFUSED) English as a diagnostic tool for evaluating automatic speech recognition
  (ASR) robustness to spontaneous speech errors. Using WhisperX to transcribe 5,300
  annotated speech errors, the research demonstrates that contextual, correction,
  and completion conditions significantly impact transcription accuracy.
---

# Evaluating ASR robustness to spontaneous speech errors: A study of WhisperX using a Speech Error Database

## Quick Facts
- arXiv ID: 2508.13060
- Source URL: https://arxiv.org/abs/2508.13060
- Reference count: 0
- This study evaluates WhisperX's robustness to spontaneous speech errors using the SFUSED English database, revealing differential impacts of contextual cues and correction status on transcription accuracy.

## Executive Summary
This study introduces the Simon Fraser University Speech Error Database (SFUSED) English as a diagnostic tool for evaluating automatic speech recognition (ASR) robustness to spontaneous speech errors. Using WhisperX to transcribe 5,300 annotated speech errors, the research demonstrates that contextual, correction, and completion conditions significantly impact transcription accuracy. Sound errors achieved 83% overall accuracy compared to 74% for word errors, with incomplete error words surprisingly showing higher accuracy than complete ones. The database's cross-classification variables reveal that human corrections and contextual cues have differential effects on sound versus word errors, providing valuable insights for improving ASR model performance, particularly for atypical speakers and contextual biasing applications.

## Method Summary
The study used WhisperX with whisper-large-v2 to transcribe 5,300 annotated speech errors from the SFUSED English database, extracted from 360 hours of podcast audio. The researchers employed VAD segmentation, beam search decoding (beam size=5), and wav2vec 2.0 force-alignment for timestamps. A fuzzy string matching algorithm aligned machine output with hand-annotated error timestamps, and a three-way classification system (Corrected/Faithful/Incorrect) evaluated transcription accuracy. The analysis cross-classified errors by type (word vs. sound), contextual influence, correction status, and completion status to reveal differential effects on transcription performance.

## Key Results
- Sound errors achieved 83% transcription accuracy compared to 74% for word errors
- Incomplete error words showed higher accuracy than complete ones, particularly for word errors
- Contextual word errors decreased transcription accuracy, while contextual sound errors showed marginal improvement
- Human corrections increased "Corrected" rate but decreased overall accuracy for word errors

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Incomplete speech errors yield higher transcription accuracy than complete productions, particularly for word-level errors.
- Mechanism: Signal degradation from mid-word interruption reduces acoustic evidence for the error word, lowering the model's confidence in the incorrect form. This allows contextual priors (syntactic, semantic) to dominate transcription decisions, similar to cloze tasks in psycholinguistics.
- Core assumption: WhisperX's non-causal architecture integrates bidirectional context weighted against degraded acoustic signals.
- Evidence anchors:
  - [abstract]: "incomplete error words surprisingly showing higher accuracy than complete ones"
  - [section 4]: "Incomplete production reduces evidence for the error word... Word truncation may reduce the model's commitment to specific transcription targets, enabling greater reliance on linguistic context"
  - [corpus]: Weak direct corpus evidence; related work on dysarthric speech normalization (arXiv:2504.12279) suggests signal transformation approaches, but not directly comparable.
- Break condition: If acoustic degradation is too severe (e.g., <2 phonemes preserved), even contextual integration may fail to recover the intended word.

### Mechanism 2
- Claim: Contextual word errors (where the error term appears nearby) decrease transcription accuracy, while contextual sound errors show marginal improvement.
- Mechanism: Repeated words in contextual word errors inflate beam search probabilities for the error form, overriding richer semantic/syntactic context. For sound errors, repeated phonemes minimally impact decoding compared to the broader acoustic similarity between error and intended forms.
- Core assumption: WhisperX's beam search (beam size=5) assigns disproportionate weight to locally repeated lexical items.
- Evidence anchors:
  - [section 3]: "Corrected transcriptions dropped by 6.23% and Incorrect transcriptions increased by 5.27%" for contextual word errors
  - [section 4]: "repeated words in contextual word errors likely influence beam search probabilities more substantially... This interference may override richer contextual cues"
  - [corpus]: CALM (arXiv:2601.22792) addresses contextual biasing for multi-speaker ASR but does not test repeated-error interference directly.
- Break condition: If beam size is reduced or repetition penalty is applied, this interference pattern may diminish.

### Mechanism 3
- Claim: Human corrections increase "Corrected" transcription rate but decrease overall accuracy for word errors.
- Mechanism: Corrections introduce both error word and intended word in the audio, creating competing transcription targets. The model lacks explicit signals about which form is authoritative, treating both as valid candidates—problematic when error and intended forms are phonetically distinct (word errors).
- Core assumption: WhisperX does not distinguish self-corrections from other disfluencies during decoding.
- Evidence anchors:
  - [section 3]: "while Corrected transcriptions increased (10.91% improvement), Incorrect transcriptions also rose significantly (13.31% increase), meaning that human correction decreased overall accuracy in word errors"
  - [section 4]: "the presence of both error and correction creates competing transcription targets"
  - [corpus]: No direct corpus evidence on self-correction handling in related papers.
- Break condition: If explicit correction detection (e.g., prosodic cue analysis) is added, competing target interference could be mitigated.

## Foundational Learning

- Concept: Word Error Rate (WER) vs. task-specific accuracy metrics
  - Why needed here: The paper uses a non-standard accuracy definition (Corrected + Faithful) rather than WER, since ground-truth transcriptions are unavailable. Understanding this distinction is critical for interpreting results.
  - Quick check question: If WhisperX outputs "book" for the error "mov" (intended: "book"), is this counted as Corrected, Faithful, or Incorrect?

- Concept: Sound errors vs. word errors in speech production
  - Why needed here: Error type is the primary independent variable; sound errors involve phonological substitution while word errors involve complete lexical replacement with different phonetics.
  - Quick check question: In the error "Re[k]ar=" for "Regarding," is this a sound error or word error? What about "/username" for intended "password"?

- Concept: Contextual biasing in ASR decoding
  - Why needed here: The paper connects speech error patterns to contextual biasing applications (voice search, earnings calls). Understanding how external context influences beam search is essential for practical improvements.
  - Quick check question: How might a contextual biasing algorithm use the pattern "Amazon → Amazon" to improve named entity recognition?

## Architecture Onboarding

- Component map: Whisper large-v2 -> Voice Activity Detection (VAD) -> Speaker diarization -> wav2vec 2.0 force-alignment -> Beam search decoder

- Critical path:
  1. Long-form audio (30-60 min episodes) → VAD segmentation
  2. Whisper transcription with beam search
  3. wav2vec 2.0 force-alignment for word-level timestamps
  4. Fuzzy string matching (Levenshtein distance) to align machine output with hand-annotated error timestamps
  5. Classification algorithm (Corrected/Faithful/Incorrect) with iterative validation

- Design tradeoffs:
  - Non-causal architecture enables full bidirectional context but precludes streaming use cases; causal variants may show different error patterns
  - No baseline WER available (SFUSED lacks human ground-truth transcriptions)
  - Classification algorithm achieved 85% accuracy after three iterations—manual spot-checking remains necessary

- Failure signatures:
  - Low Faithful rate for sound deletions/additions (excluded from position analysis due to insufficient cases)
  - Word errors with corrections: rising Incorrect rate despite human repair
  - Contextual word errors: accuracy degradation despite additional linguistic cues

- First 3 experiments:
  1. Replicate the SFUSED evaluation on a causal (streaming) Whisper variant to isolate bidirectional context effects on incomplete vs. complete errors.
  2. Add repetition penalty to beam search scoring and measure impact on contextual word error accuracy.
  3. Test explicit correction detection (pause duration + F0 reset) as a post-hoc filter to resolve competing target interference.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do causal, streaming ASR models compare to non-causal models in transcribing speech errors, given their lack of access to future linguistic context?
- Basis in paper: [explicit] The authors state that "future research could examine causal models’ performance" and note that their variables would be "particularly valuable in comparing causal versus non-causal models."
- Why unresolved: This study exclusively evaluated WhisperX, which uses a non-causal architecture with bidirectional context; it did not test streaming models that must process audio sequentially.
- What evidence would resolve it: A comparative benchmark evaluating causal ASR models on the SFUSED dataset to measure the performance gap caused by the lack of future context.

### Open Question 2
- Question: Can speech error analysis be effectively utilized to improve contextual biasing algorithms for named entity recognition and atypical speakers?
- Basis in paper: [explicit] The conclusion proposes that "speech error analysis could enhance named entity recognition" and explicitly states, "Our future work looks to investigate contextual biasing algorithms to improve atypical speaker ASR."
- Why unresolved: The current paper is diagnostic, focusing on evaluating existing model robustness rather than developing or testing methods for model improvement or personalization.
- What evidence would resolve it: Experiments implementing contextual biasing techniques informed by SFUSED error patterns, demonstrating improved Word Error Rates (WER) for targeted vocabulary or speaker groups.

### Open Question 3
- Question: How do unexplored variables in the SFUSED database, such as phonetic complexity and error direction, affect ASR transcription accuracy?
- Basis in paper: [explicit] The authors note that "SFUSED English contains several additional unexplored variables that could provide novel ASR evaluation metrics," specifically listing phonetic complexity, malapropisms, and error direction.
- Why unresolved: The study restricted its analysis to error type (word/sound), contextual influence, correction status, and completion status, leaving other rich annotations in the database unused.
- What evidence would resolve it: Extending the current analysis script to cross-classify transcription results with these specific linguistic tags to determine their statistical significance.

## Limitations
- Limited scope of error types: The study focuses on a subset of 5,300 errors, excluding sound deletions/additions due to insufficient cases
- Ground-truth absence: Without human reference transcriptions, the study relies on a three-way classification system rather than standard WER metrics
- Fixed model configuration: Analysis uses WhisperX with specific parameters without exploring how different architectures might affect error handling

## Confidence
- High confidence in: The differential impact of contextual cues on sound versus word errors, the accuracy advantage of incomplete over complete error words for word errors, and the systematic reduction in accuracy for contextual word errors
- Medium confidence in: The exact magnitude of accuracy differences across conditions, as these depend on the classification algorithm's performance
- Low confidence in: Extrapolating these findings to streaming ASR systems, as WhisperX uses a non-causal architecture

## Next Checks
1. Replicate on causal Whisper variant: Test the same SFUSED error subset using Whisper's streaming/causal model to isolate the bidirectional context effects on incomplete versus complete error word accuracy.

2. Validate classification algorithm: Manually review 100 randomly selected transcription outcomes to verify the Corrected/Faithful/Incorrect classification accuracy matches the reported 85%, particularly for borderline cases where error and intended words share partial phonetic overlap.

3. Test repetition penalty impact: Modify WhisperX's beam search with a repetition penalty parameter and measure changes in contextual word error accuracy to confirm whether repeated words drive the observed interference pattern.