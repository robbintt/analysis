---
ver: rpa2
title: LM Agents May Fail to Act on Their Own Risk Knowledge
arxiv_id: '2508.13465'
source_url: https://arxiv.org/abs/2508.13465
tags:
- agent
- user
- tool
- risky
- action
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper identifies a critical gap between LM agents' risk awareness
  and their safe execution behaviors. While agents demonstrate near-perfect knowledge
  of risks in abstract settings (98% pass rates), they fail to identify risks in concrete
  scenarios (performance drops 23%) and often execute risky actions directly (<26%
  pass rates).
---

# LM Agents May Fail to Act on Their Own Risk Knowledge

## Quick Facts
- arXiv ID: 2508.13465
- Source URL: https://arxiv.org/abs/2508.13465
- Reference count: 40
- Agents demonstrate strong risk awareness in abstract settings (>98% pass rates) but fail to identify risks in concrete scenarios and often execute risky actions directly

## Executive Summary
This paper identifies a critical gap between LM agents' risk awareness and their safe execution behaviors. While agents demonstrate near-perfect knowledge of risks in abstract settings (>98% pass rates), they fail to identify risks in concrete scenarios (performance drops >23%) and often execute risky actions directly (<26% pass rates). This gap persists across model scales and even in specialized reasoning models like DeepSeek-R1, suggesting that scaling base models or inference compute alone cannot resolve safety concerns. To address this, the authors develop a risk verifier system that independently critiques agents' proposed actions using the same base LM, augmented with an abstractor that converts specific execution trajectories into abstract descriptions. This approach effectively leverages the observed knowledge-identification and identification-execution gaps.

## Method Summary
The paper evaluates three progressive tests to measure LM agent safety: Knowledge Test (abstract risk knowledge), Identification Test (risk detection in concrete scenarios), and Execution Test (actual risky action execution). The authors develop a risk verifier system using the same base LM to critique proposed actions, augmented with an abstractor that converts specific execution trajectories into abstract descriptions. The approach uses single-round critique (k=1) to balance safety gains with computational overhead. The framework is evaluated using ToolEmu, a simulated environment with 144 test cases across 9 risk categories, generating 328 risky trajectories from an initial 934.

## Key Results
- Agents show >98% pass rates on abstract risk knowledge but drop >23% when evaluating concrete scenarios
- Execution safety is particularly poor: <26% pass rates for vanilla agents versus 72.6-79.6% with verifier+abstractor
- The identification-execution gap persists across model scales and even in specialized reasoning models like DeepSeek-R1
- Risk verifier with abstractor achieves 55.3% reduction in risky action execution while maintaining comparable helpfulness scores

## Why This Works (Mechanism)

### Mechanism 1: Generator-Validator Gap Exploitation
- **Claim:** Separating action generation from risk verification improves safety because LMs verify better than they generate safely.
- **Mechanism:** The agent (generator) proposes actions while an independent verifier using the same base LM critiques them. This leverages the observation that models can identify risks in trajectories better than they can avoid executing them.
- **Core assumption:** The identification-execution gap (models identifying risks at ~78% vs executing safely at ~13%) reflects a fundamental asymmetry in how LMs process versus produce safe behavior.
- **Evidence anchors:** [abstract] "...resemble the generator-validator gaps observed in LMs"; [Section 4.2] "our observed identification-execution and knowledge-identification gaps can be viewed as a generalization of the generator-validator gaps"
- **Break condition:** If the verifier's identification capability degrades to match the generator's execution safety rate (~13%), the gap disappears and this approach fails.

### Mechanism 2: Abstraction-Mediated Risk Identification
- **Claim:** Converting concrete execution trajectories to abstract descriptions enables the verifier to leverage models' strong abstract risk knowledge (>98%).
- **Mechanism:** The abstractor summarizes specific actions into high-level descriptions, allowing the verifier to match against abstract risk patterns where models perform well.
- **Core assumption:** The knowledge-identification gap (>23% performance drop from abstract to concrete) stems from concrete details masking risk patterns, not from fundamentally different reasoning processes.
- **Evidence anchors:** [abstract] "...with an abstractor that converts specific execution trajectories into abstract descriptions where LMs can more effectively identify the risks"; [Figure 4] Shows abstractor enabling verifier to prioritize fundamental issues
- **Break condition:** If abstraction loses critical risk-relevant details or introduces hallucinations, the verifier operates on incomplete or incorrect premises.

### Mechanism 3: Iterative Critique-Feedback with Early Stopping
- **Claim:** Single-round critique (k=1) captures most safety gains; additional iterations yield diminishing returns.
- **Mechanism:** After the agent proposes an action, the verifier critiques it once. If risky, the agent regenerates with the critique as guidance. The paper finds k=1 provides significant gains with minimal overhead.
- **Core assumption:** The first critique surfaces the dominant risk; subsequent critiques add marginal value while increasing latency and compute cost.
- **Evidence anchors:** [Section 4.3, Figure 6] "...while safety scores continue improving with higher k values, the most significant gains occur at k = 1"; [Table 1] Verifier with Abstractor achieves 72.6-79.6% pass rates
- **Break condition:** If risks are contextually nested (fixing one reveals another), single-round critique misses cascading failures.

## Foundational Learning

- **Concept: Generator-Validator Gap (Li et al., 2023)**
  - **Why needed here:** The paper frames its findings as an instantiation of this gap in safety-critical scenarios. Without this context, the two-gap decomposition (knowledge-identification, identification-execution) seems ad hoc.
  - **Quick check question:** Given a model that correctly labels "sudo rm -rf /*" as dangerous in QA, would you expect it to avoid executing this command in an agent context? (The paper shows you shouldn't.)

- **Concept: Tool-Use Agent Architecture**
  - **Why needed here:** The evaluation framework assumes agents operate via Action/Action Input/Observation loops with external tools. The risk patterns are specific to this paradigm (tool misuse, underspecified constraints, fabricated information).
  - **Quick check question:** In an agent trajectory, where would you look for evidence of "unwarranted assumptions"? (Answer: Search actions with limit=1, or tool inputs with placeholders filled by single retrieved values.)

- **Concept: LM-Based Evaluation (ToolEmu framework)**
  - **Why needed here:** The paper uses an LM as both safety and helpfulness evaluator (0-3 scales). Understanding how this evaluation works is critical for interpreting pass rates and assessing potential evaluator bias.
  - **Quick check question:** If the safety evaluator assigns score 3 (Certain No Risk), what must be true about the trajectory? (Answer: All successfully executed tool calls have near-zero probability of leading to risky outcomes.)

## Architecture Onboarding

- **Component map:** [Partial Trajectory] → [Agent] → [Proposed Action] → [Abstractor] → [High-Level Description] → [Verifier] → [Critique or Approval] → [Agent regenerates OR executes]

- **Critical path:** The abstractor-verifier pair. If the abstractor fails to capture risk-relevant structure, the verifier operates blindly. The paper shows the abstractor enables the verifier to prioritize fundamental issues over surface details (Figure 4).

- **Design tradeoffs:**
  1. **Computational overhead vs. safety gain:** Single critique (k=1) balances both; k>1 yields ~5-10% additional safety at 2-3x latency cost.
  2. **Abstraction granularity vs. information preservation:** Too abstract loses specifics (which file, which recipient); too concrete defeats the purpose. The paper's abstractor prompt targets "high-level overview" while "highlighting significant entities."
  3. **Shared vs. separate base LM:** The paper uses the same LM for agent, verifier, and abstractor, enabling deployment flexibility. Assumption: the generator-validator gap persists within a single model.

- **Failure signatures:**
  1. **Verifier false negatives:** Agent executes risky action after verifier approval. Signature: verifier focuses on surface details (e.g., date range) while missing structural risks (e.g., sending sensitive data without consent).
  2. **Helpfulness collapse:** Agent becomes overly cautious, repeatedly seeking user confirmation for benign actions. Signature: helpfulness score drops significantly (the paper shows ~0.07 average decrease, acceptable).
  3. **Abstractor hallucination:** Summary includes fabricated entities or omits critical actions. Signature: verifier critiques risks not present in actual trajectory.

- **First 3 experiments:**
  1. **Baseline gap measurement:** Run Knowledge, Identification, and Execution tests on your target model using the paper's 328-trajectory dataset (or construct equivalent). Confirm the two gaps exist before implementing mitigation.
  2. **Ablation study:** Compare (a) vanilla agent, (b) safety-prompted agent, (c) agent with verifier only, (d) agent with verifier + abstractor. The paper shows (d) outperforms (c) by enabling the verifier to catch fundamental issues.
  3. **Iteration depth sweep:** Test k=1, 2, 3 critique rounds on a sample of 50 trajectories. Plot safety score vs. helpfulness score vs. latency. The paper's Figure 6 suggests k=1 is sufficient for most models; verify this holds for your deployment constraints.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** What are the underlying mechanistic causes of the knowledge-identification and identification-execution gaps in LM agents?
- **Basis in paper:** [explicit] The authors state in Limitations: "the underlying causes of these gaps remain unclear. We hypothesize that this may be partly due to the alignment procedures of frontier LMs, which are typically conducted in conversational scenarios rather than agentic ones."
- **Why unresolved:** The paper demonstrates the existence and persistence of these gaps but does not isolate the specific training or architectural factors responsible for them.
- **What evidence would resolve it:** Ablation studies comparing agents fine-tuned on agentic trajectories versus conversational data, or mechanistic interpretability analysis of attention patterns during risk assessment across different context formats.

### Open Question 2
- **Question:** Would direct agentic alignment training eliminate the gaps more effectively than runtime verification systems?
- **Basis in paper:** [explicit] "A promising future direction is to conduct more extensive agentic alignment training with the ToolEmu framework... where a large scale of agentic execution trajectories can be collected in emulation across diverse scenarios."
- **Why unresolved:** The paper evaluates only a verifier-based mitigation strategy, not a training-based approach.
- **What evidence would resolve it:** Training agents on safety-labeled agentic trajectories and comparing the resulting gap sizes against verifier-augmented baselines on the same test suite.

### Open Question 3
- **Question:** Do the observed gaps and verifier effectiveness generalize to real-world agent deployments beyond simulated sandbox environments?
- **Basis in paper:** [inferred] The evaluation uses ToolEmu, which "uses an LM as a virtual sandbox to simulate tool executions" rather than actual tool integrations with real systems.
- **Why unresolved:** Simulated observations may not reflect real tool behaviors, failure modes, or the complexity of genuine execution contexts.
- **What evidence would resolve it:** Deployment of both vanilla and verifier-augmented agents in controlled real-world tool-use scenarios (e.g., actual file systems, live APIs) with safety outcome measurements.

## Limitations
- The evaluation relies entirely on LLM judges (0-3 scales) rather than ground-truth human assessments, making safety and helpfulness scores subject to evaluator bias.
- The abstractor's performance directly impacts verifier effectiveness, yet the paper doesn't analyze failure cases where abstraction loses critical risk signals or introduces hallucinations.
- The study focuses on single-task execution (k=1 iteration), leaving open questions about multi-turn interactions where risks compound or cascade across iterations.

## Confidence

- **High confidence:** The existence of knowledge-identification and identification-execution gaps is well-supported by the experimental data (p<0.05 across all tested models).
- **Medium confidence:** The claim that base model scaling alone cannot resolve safety concerns relies on correlation with Chatbot Arena and MMLU scores but doesn't establish causation.
- **Low confidence:** The generalizability to real-world deployment scenarios beyond the ToolEmu benchmark remains unproven.

## Next Checks
1. **Ground-truth validation:** Run a subset of trajectories (n=50) through human evaluation panels to validate LM judge scores, particularly focusing on false negative rates where the verifier approves risky actions.
2. **Multi-turn stress test:** Design a sequential task requiring 5+ tool interactions where each step depends on previous outcomes. Test whether the k=1 verifier catches risks that emerge through interaction chains versus isolated actions.
3. **Abstraction fidelity audit:** Implement systematic testing of the abstractor's output against ground-truth trajectory summaries. Measure information preservation and hallucination rates.