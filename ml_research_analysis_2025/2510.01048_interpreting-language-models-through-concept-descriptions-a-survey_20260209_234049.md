---
ver: rpa2
title: 'Interpreting Language Models Through Concept Descriptions: A Survey'
arxiv_id: '2510.01048'
source_url: https://arxiv.org/abs/2510.01048
tags:
- language
- linguistics
- association
- computational
- description
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper provides the first comprehensive survey of methods for
  generating natural language concept descriptions for model components and abstractions
  in language models. The survey systematically categorizes approaches into those
  targeting native components (neurons, attention heads) and learned abstractions
  (SAE features, circuits).
---

# Interpreting Language Models Through Concept Descriptions: A Survey

## Quick Facts
- arXiv ID: 2510.01048
- Source URL: https://arxiv.org/abs/2510.01048
- Authors: Nils Feldhus; Laura Kopf
- Reference count: 21
- This paper provides the first comprehensive survey of methods for generating natural language concept descriptions for model components and abstractions in language models.

## Executive Summary
This survey systematically categorizes methods for generating natural language descriptions of concepts in language model components, distinguishing between approaches targeting native components (neurons, attention heads) and learned abstractions (SAE features, circuits). The authors organize evaluation methods into five families: predictive simulation, input-based, output-based, semantic similarity, and human evaluation. The survey highlights the field's shift from neuron-level to SAE-based descriptions due to polysemanticity issues, and notes the maturation of evaluation beyond simple correlation scores. Key challenges identified include computational costs of SAE training, lack of rigorous causal evaluation methods, and need for standardized benchmarks.

## Method Summary
The paper provides a comprehensive taxonomy of concept description methods, organizing them by their target components (native vs. learned abstractions) and their operational mechanism (generative vs. discriminative). For evaluation, the authors categorize approaches into five families based on whether they simulate predictions, analyze input/output behavior, measure semantic similarity, or rely on human judgment. The survey critically examines each method's strengths and limitations, identifying patterns such as the field's movement toward SAE-based descriptions due to the polysemanticity of individual neurons.

## Key Results
- Shift from neuron-level to SAE-based descriptions due to polysemanticity of individual neurons
- Evaluation methods have matured beyond simple correlation scores to include predictive simulation and causal analysis
- Computational costs of SAE training and lack of rigorous causal evaluation methods remain key challenges

## Why This Works (Mechanism)
Concept description methods work by bridging the gap between high-dimensional model representations and human-understandable concepts through various mechanisms. Generative approaches use language models to produce descriptions conditioned on model activations, while discriminative methods classify activations into predefined concept categories. SAE-based methods decompose complex representations into sparse feature sets that can be more easily described. The effectiveness of these methods depends on the assumption that model representations correspond to human-interpretable concepts, and that language models can accurately map between activation patterns and natural language descriptions.

## Foundational Learning
1. **Polysemanticity in Neural Networks** - Individual neurons often encode multiple concepts, making them poor units for interpretation; quick check: visualize neuron activations across diverse inputs to observe overlapping patterns
2. **Sparse Autoencoders (SAEs)** - Learned decompositions that create more monosemantic features than individual neurons; quick check: examine feature activation distributions for sparsity
3. **Causal Attribution** - Understanding whether identified concepts actually drive model behavior rather than merely correlating with it; quick check: ablate or amplify features and measure prediction changes
4. **Evaluation Taxonomy** - Different evaluation approaches (predictive, input-based, output-based, semantic, human) capture different aspects of interpretability; quick check: apply multiple evaluation methods to same concept description
5. **Concept Drift** - Model representations may change across training stages or domains; quick check: compare concept descriptions across different checkpoints
6. **Faithfulness vs. Fidelity** - Distinction between whether descriptions accurately represent model behavior (faithfulness) versus whether they are useful for humans (fidelity)

## Architecture Onboarding
Component map: Input activations -> Feature extraction (SAE/neurons) -> Concept generation -> Evaluation metrics -> Human interpretation

Critical path: Feature extraction (SAE training) -> Concept generation -> Causal evaluation -> Benchmark validation

Design tradeoffs: Computational cost of SAE training vs. quality of monosemantic features; granularity of concepts vs. interpretability; automation vs. human oversight

Failure signatures: Overfitting to specific datasets; concepts that fail to generalize across model versions; evaluation metrics that don't capture true interpretability

First experiments: 1) Train SAE on base model and generate concept descriptions; 2) Apply multiple evaluation methods to same concept set; 3) Test concept generalization across different model architectures

## Open Questions the Paper Calls Out
- How to develop more rigorous causal evaluation methods for concept descriptions
- Whether current concept description methods can generalize across different model architectures and tasks
- How to balance computational costs of SAE training with the quality of generated descriptions

## Limitations
- Evaluation metrics remain underdeveloped, particularly for assessing faithfulness and causal impact
- Focus primarily on encoder-decoder and decoder-only architectures, with limited discussion of encoder-only or multimodal models
- Many evaluation methods rely on proxy tasks or correlation measures that may not capture true interpretability

## Confidence
- High: Systematic categorization of methods and evaluation approaches
- Medium: Comparative analysis of SAE versus neuron-level descriptions
- Low: Assertions about the field's future trajectory given rapid evolution

## Next Checks
1. Conduct head-to-head comparison of SAE-based versus neuron-level concept descriptions on standardized benchmarks
2. Develop and validate new evaluation metrics that assess both predictive accuracy and causal faithfulness of concept descriptions
3. Create benchmark dataset specifically designed to test generalizability of concept description methods across different model architectures and tasks