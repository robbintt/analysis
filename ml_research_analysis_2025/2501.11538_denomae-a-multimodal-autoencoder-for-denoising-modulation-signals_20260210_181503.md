---
ver: rpa2
title: 'DenoMAE: A Multimodal Autoencoder for Denoising Modulation Signals'
arxiv_id: '2501.11538'
source_url: https://arxiv.org/abs/2501.11538
tags:
- modulation
- classification
- denomae
- learning
- pretraining
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: DenoMAE addresses the challenge of denoising modulation signals
  in communication systems by introducing a multimodal autoencoder framework that
  treats noise as an explicit modality alongside constellation diagrams and signal
  representations. The method employs masked autoencoding with 75% random masking
  across five input modalities (noisy image, noisy signal, noiseless image, noiseless
  signal, and noise), enabling the model to learn cross-modal denoising capabilities
  through shared latent space representations.
---

# DenoMAE: A Multimodal Autoencoder for Denoising Modulation Signals

## Quick Facts
- arXiv ID: 2501.11538
- Source URL: https://arxiv.org/abs/2501.11538
- Reference count: 36
- Primary result: 83.5% AMC accuracy with 10× less pretraining data and 3× less fine-tuning data

## Executive Summary
DenoMAE addresses the challenge of denoising modulation signals in communication systems by introducing a multimodal autoencoder framework that treats noise as an explicit modality alongside constellation diagrams and signal representations. The method employs masked autoencoding with 75% random masking across five input modalities, enabling the model to learn cross-modal denoising capabilities through shared latent space representations. The pre-trained model achieves state-of-the-art accuracy of 83.5% in automatic modulation classification with only 10,000 unlabeled pretraining samples and 1,000 labeled fine-tuning samples, representing a 10× reduction in pretraining data and 3× reduction in fine-tuning data compared to existing approaches.

## Method Summary
DenoMAE uses a transformer-based encoder-decoder architecture where five modalities (noisy image, noisy signal, noiseless image, noiseless signal, and noise) are processed through a shared encoder with 75% random masking. The model learns to reconstruct masked patches across all modalities in a shared latent space, enabling cross-modal denoising. The encoder (12-layer ViT-Base) processes only visible patches (25% of total), while five modality-specific decoders (4 layers each) reconstruct the masked regions. The pre-trained encoder is then fine-tuned for automatic modulation classification using a small labeled dataset, achieving significant performance gains over non-pretrained models, particularly at low SNR conditions.

## Key Results
- Achieves 83.5% AMC accuracy with only 10,000 unlabeled pretraining samples and 1,000 labeled fine-tuning samples
- Demonstrates robust performance at low SNR, maintaining 77.5% accuracy at -10 dB SNR
- Shows impressive extrapolation capabilities on unseen lower SNRs (-11 dB to -20 dB), with up to 22.1% improvement over non-pretrained models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Treating noise as an explicit input modality enables the model to learn disentangled noise representations that improve denoising and downstream classification.
- Mechanism: By providing noise as a separate modality alongside noisy and noiseless signal/image pairs, the encoder must learn to correlate noise patterns with their effects on signals, effectively learning a noise-to-signal mapping that can be inverted during reconstruction.
- Core assumption: Noise patterns in communication signals exhibit learnable structure that correlates across time-series and constellation diagram representations.
- Evidence anchors:
  - [abstract] "incorporating multiple input modalities, including noise as an explicit modality, to enhance cross-modal learning"
  - [section III.A] "This unique framework helps the network gain a comprehensive understanding of noise across modalities"
  - [corpus] DenoMAE2.0 extends this with local patch classification, suggesting the original noise modeling approach was effective enough to warrant refinement
- Break condition: If noise is truly random/Gaussian without exploitable structure across modalities, explicit noise modeling would provide no benefit over implicit learning.

### Mechanism 2
- Claim: Cross-modal reconstruction with shared latent representations forces the encoder to learn signal features invariant to representation format.
- Mechanism: The shared encoder processes visible patches from all five modalities and projects them to a unified latent space. The decoder must then reconstruct masked patches for each modality, requiring the latent space to encode information sufficient for generating both time-series signals and 2D constellation diagrams.
- Core assumption: Modulation type information is preserved across signal-to-constellation transformation and can be captured in a shared representation.
- Evidence anchors:
  - [abstract] "learning to reconstruct their equivalent noiseless signals and diagrams"
  - [Table I] Progressive accuracy improvement from 81.30% (single modality) to 83.50% (all five modalities) demonstrates cross-modal contribution
  - [corpus] Weak direct corpus support; neighbor papers focus on single-modality MAE applications
- Break condition: If modalities contain conflicting or orthogonal information, shared encoding would degrade rather than improve performance.

### Mechanism 3
- Claim: Aggressive 75% masking prevents shortcut learning and forces global context aggregation.
- Mechanism: With only 25% of patches visible, local interpolation strategies fail; the model must learn to aggregate information across distant patches and modalities to reconstruct missing content, building representations that capture global signal structure.
- Core assumption: Modulation classification requires understanding global signal patterns (constellation shape, symbol distribution) rather than local features.
- Evidence anchors:
  - [section III.A.2] "randomly mask a high proportion (e.g., 75%) of the input patches for each modality"
  - [Figure 2] Visual evidence shows reconstruction preserves constellation shape even at low SNR
  - [corpus] Masked Autoencoders as Universal Speech Enhancer (arXiv:2602.02413) reports similar masking ratios effective for signal enhancement
- Break condition: If modulation information is highly localized in small signal segments, aggressive masking could remove critical information irrecoverably.

## Foundational Learning

- Concept: **Masked Autoencoding (MAE)**
  - Why needed here: This is the core pretraining paradigm. Understanding that MAE trains models to reconstruct randomly masked input regions by learning useful intermediate representations.
  - Quick check question: Can you explain why masking 75% of input is harder but potentially more beneficial than masking 25%?

- Concept: **Constellation Diagrams**
  - Why needed here: Primary downstream input modality. These are 2D visualizations of I/Q (in-phase/quadrature) signal components where modulation type determines characteristic patterns (e.g., 16 points for 16-QAM).
  - Quick check question: Would you expect BPSK and QPSK constellation diagrams to look similar or different, and why?

- Concept: **Signal-to-Noise Ratio (SNR)**
  - Why needed here: Core evaluation dimension. Understanding that lower (negative) SNR means noise power exceeds signal power, making classification harder.
  - Quick check question: At -10 dB SNR, is the noise power higher or lower than the signal power, and by how much?

## Architecture Onboarding

- Component map:
  Input Embedding -> Patch + Position Embedding -> 75% Mask -> Shared Encoder -> Shared Latent Space -> 5 Modality-Specific Decoders -> MSE Loss per modality

- Critical path: Input → Patch + Position Embedding → 75% Mask → Encoder (visible only) → Latent Projection → Decoder → MSE Loss per modality. Only encoder weights transfer to downstream.

- Design tradeoffs:
  - 5 modalities vs. fewer: More modalities improve accuracy (+2.2% from 1 to 5) but increase pretraining compute and data requirements
  - 75% masking ratio: Reduces encoder compute by ~4× but may lose fine-grained signal details
  - Shared vs. separate encoders: Shared enables cross-modal learning but may dilute modality-specific features

- Failure signatures:
  - Constellation outputs become amorphous blobs → encoder learned local smoothing rather than global structure
  - Large accuracy gap between high/low SNR → pretraining failed to learn noise-robust features
  - Single-modality fine-tuning matches full model → cross-modal learning didn't activate

- First 3 experiments:
  1. **Sanity check**: Train with 0% masking on single modality (noisy image only); should achieve reasonable reconstruction but poor downstream transfer
  2. **Ablation**: Remove noise modality; expect ~0.2% accuracy drop based on Table I progression
  3. **SNR sweep**: Fine-tune and evaluate at -20 dB to -10 dB (unseen during training); should observe graceful degradation with pretraining advantage widening at lower SNRs

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the DenoMAE architecture be optimized to reduce computational overhead while maintaining its denoising and classification performance?
- Basis in paper: [explicit] The conclusion states, "Future work could... further optimize its architecture for lower computational overhead."
- Why unresolved: The current transformer-based encoder-decoder structure (12-layer encoder, 4-layer decoder) involves significant parameter counts and processing costs associated with high-resolution inputs (224x224) and multimodal patch reconstruction.
- What evidence would resolve it: A study comparing model compression techniques (e.g., pruning, quantization) or efficient architectures (e.g., linear attention mechanisms) against the current baseline, measuring throughput (FPS) and FLOPs without dropping accuracy below 83.5%.

### Open Question 2
- Question: Does the explicit inclusion of noise as an input modality function effectively in real-world scenarios where ground-truth noise signals are unavailable or must be estimated blindly?
- Basis in paper: [inferred] The paper utilizes "noise" as a distinct input modality alongside signals and images. In the methodology, the noise modality is likely derived from the difference between generated noisy and noiseless samples (synthetic data), which presupposes knowledge of the clean signal.
- Why unresolved: The paper does not clarify how the "noise" modality is acquired during the inference phase in a blind real-world channel where the noiseless signal is unknown. If the model relies on a perfect noise input that is impractical to extract in the field, deployment is limited.
- What evidence would resolve it: An evaluation of DenoMAE on over-the-air captured data where the "noise" modality is either omitted, set to zero, or estimated via blind noise estimation techniques, compared against the synthetic data performance.

### Open Question 3
- Question: To what extent does DenoMAE generalize to diverse signal types and channel impairments beyond the ten modulation formats and SNR variations tested?
- Basis in paper: [explicit] The conclusion suggests, "Future work could explore the extension of DenoMAE to more diverse signal types."
- Why unresolved: The current experiments are limited to ten specific modulation formats and standard SNR variations. The model's robustness against other common channel impairments mentioned in the introduction (e.g., fading, Doppler shifts) or more complex modulation schemes (e.g., OFDM) remains unquantified.
- What evidence would resolve it: Benchmarking the pre-trained model on datasets containing frequency-selective fading channels, higher-order modulations (e.g., 256-QAM), or non-constant envelope signals to observe if the cross-modal denoising generalizes or requires retraining.

## Limitations

- Exact 10 modulation class composition not specified, making dataset replication challenging
- Noise modality formulation lacks mathematical definition (residual vs. estimation unclear)
- Exponential decay rates (α) for constellation enhancement not provided, affecting visual feature quality
- Performance at extremely low SNRs (-11 dB to -20 dB) relies on extrapolation from models trained only on -10 dB to 10 dB

## Confidence

- **High Confidence**: Cross-modal reconstruction benefits (Table I shows clear accuracy progression from 1 to 5 modalities) and SNR-dependent performance degradation (accurate at -10 dB vs. 0.5+ dB)
- **Medium Confidence**: 10× pretraining data reduction claim relative to existing approaches—requires external benchmarking data not fully specified
- **Low Confidence**: Extrapolation performance at -11 dB to -20 dB SNR—based on testing outside the training distribution without systematic analysis

## Next Checks

1. **Reconstruct low-SNR outputs**: Generate visualizations of decoder outputs at -15 dB and -20 dB SNR to verify that constellation shapes remain distinguishable despite extreme noise, confirming that the model learned robust cross-modal features rather than memorizing training examples.

2. **Ablation of noise modality**: Systematically remove the noise input modality and measure accuracy impact. The paper suggests cross-modal benefits of ~2.2% from 1 to 5 modalities, but the specific contribution of explicit noise modeling needs isolated measurement.

3. **Pretraining data scaling**: Train DenoMAE with varying amounts of unlabeled data (1K, 5K, 10K, 20K) to empirically validate the claimed 10× reduction in pretraining requirements and identify potential diminishing returns or overfitting thresholds.