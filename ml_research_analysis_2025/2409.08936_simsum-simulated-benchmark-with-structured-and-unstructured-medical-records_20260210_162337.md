---
ver: rpa2
title: 'SimSUM: Simulated Benchmark with Structured and Unstructured Medical Records'
arxiv_id: '2409.08936'
source_url: https://arxiv.org/abs/2409.08936
tags:
- patient
- note
- symptoms
- clinical
- notes
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces SimSUM, a simulated benchmark dataset linking
  structured tabular data with unstructured clinical notes for research on clinical
  information extraction. The dataset contains 10,000 patient records generated from
  an expert-defined Bayesian network and clinical notes produced by GPT-4o, with symptoms
  annotated via automated span extraction.
---

# SimSUM: Simulated Benchmark with Structured and Unstructured Medical Records

## Quick Facts
- arXiv ID: 2409.08936
- Source URL: https://arxiv.org/abs/2409.08936
- Reference count: 40
- Primary result: Expert-verified synthetic clinical dataset with 10,000 records combining structured tabular data and unstructured clinical notes for multimodal information extraction research.

## Executive Summary
SimSUM is a synthetic benchmark dataset designed to evaluate multimodal clinical information extraction by combining structured tabular patient data with unstructured clinical notes. The dataset contains 10,000 patient records generated from an expert-defined Bayesian network, with clinical notes produced by GPT-4o and symptoms annotated through automated span extraction. Expert evaluation confirmed high consistency and realism in the notes, with clinical accuracy rated very highly. The dataset supports research on multimodal clinical reasoning, causal estimation, and synthetic data generation, though it is not intended for training production clinical systems.

## Method Summary
The dataset was generated through a two-phase process combining structured data synthesis with unstructured text generation. First, a Bayesian network defined by medical experts generated structured tabular data for 10,000 synthetic patients, including 16 symptoms, 15 comorbidities, demographic information, treatment variables, and outcomes. Second, GPT-4o generated clinical notes based on the structured data, with symptoms annotated via automated span extraction using a rule-based system validated by clinicians. The synthetic notes were evaluated by domain experts for clinical accuracy and realism, with high ratings achieved for both aspects. Baseline models demonstrated that integrating tabular background features with text improves symptom extraction performance, particularly for challenging cases.

## Key Results
- Expert evaluation found clinical accuracy rated very highly with consistency and realism in notes
- Baseline models showed performance improvements when integrating tabular features with text for symptom extraction
- Dataset supports multimodal clinical reasoning and synthetic data generation research applications

## Why This Works (Mechanism)
The approach works by explicitly modeling the relationship between structured clinical data and unstructured narrative text through a probabilistic framework. By using an expert-defined Bayesian network to generate the structured component, the dataset ensures realistic conditional dependencies between symptoms, comorbidities, and demographic factors. The GPT-4o generation of clinical notes based on this structured data creates coherent narrative text that maintains these relationships while adding contextual richness. The automated annotation process using rule-based extraction followed by LLM verification provides consistent symptom labeling at scale, enabling reliable benchmarking of extraction algorithms.

## Foundational Learning
- **Bayesian Networks**: Probabilistic graphical models representing conditional dependencies between variables - needed to generate realistic patient data with correct symptom-comorbidity relationships; quick check: verify the conditional probability tables align with medical knowledge.
- **Multimodal Learning**: Techniques for combining information from different data modalities (tabular and text) - needed to evaluate models that can leverage both structured and unstructured clinical data; quick check: test baseline models with different feature combination strategies.
- **Clinical Information Extraction**: NLP tasks focused on extracting medical entities and relationships from clinical text - needed to benchmark performance on realistic clinical documentation; quick check: evaluate symptom extraction performance across different model architectures.

## Architecture Onboarding

**Component Map**: Bayesian Network -> Structured Data Generation -> GPT-4o Text Generation -> Rule-based Annotation -> Expert Validation

**Critical Path**: The critical path flows from the Bayesian network through to expert validation, as each step depends on the previous one. The structured data generation must complete before text generation can begin, and the automated annotation must be verified by experts before the dataset is considered ready for benchmarking.

**Design Tradeoffs**: The dataset prioritizes clinical realism and controlled complexity over real-world messiness. Using synthetic data enables perfect ground truth but sacrifices the full complexity of real clinical documentation, including abbreviations, typos, and varied writing styles.

**Failure Signatures**: Models that overfit to synthetic patterns may show excellent performance on SimSUM but poor generalization to real clinical text. Performance degradation when evaluated on real clinical data would indicate the synthetic notes lack critical complexity.

**First Experiments**:
1. Evaluate baseline models on the dataset to establish performance baselines for symptom extraction
2. Test different multimodal fusion strategies to determine optimal integration of tabular and text features
3. Conduct ablation studies removing structured features to assess their contribution to performance

## Open Questions the Paper Calls Out
### Open Question 1
- Question: How can the explicit structure of the expert-defined Bayesian network be integrated into predictive models to improve clinical information extraction over naive feature concatenation?
- Basis in paper: [explicit] The Conclusion states, "Future research can explore more advanced hybrid models that leverage domain knowledge to link structured and unstructured data," referencing the "key objective" in Section 3.4 to realize the approach in Figure 2.
- Why unresolved: The baseline models in the paper used a simple concatenation of text embeddings and tabular features (`neural-text-tab`), which did not consistently outperform text-only baselines on normal notes (Table 4).
- What evidence would resolve it: A model architecture that explicitly encodes the Bayesian network's conditional probabilities or graph topology, demonstrating statistically significant improvements in F1-scores for symptom extraction compared to the reported baselines.

### Open Question 2
- Question: Can SimSUM serve as a valid benchmark for estimating causal treatment effects in the presence of textual confounders?
- Basis in paper: [explicit] Section 3.4 lists "causal effect estimation in the presence of tabular and/or textual confounders" as a secondary use, enabled by the inclusion of treatment (`antibiotics`) and outcome (`days at home`) variables.
- Why unresolved: The paper constructs the dataset and validates the notes' realism, but it does not execute or validate any causal inference experiments to prove the dataset is suitable for this specific task.
- What evidence would resolve it: A study successfully applying causal inference algorithms (e.g., backdoor adjustment or meta-learners) to the dataset to recover the known ground-truth causal effects defined by the generative model.

### Open Question 3
- Question: To what extent do models trained on the LLM-generated "clean" notes of SimSUM transfer to real-world clinical text?
- Basis in paper: [inferred] Section 3.1.2 reports that clinical experts found the note content accurate but the "format did not [seem] realistic," noting the synthetic notes lack the complexity and "abbreviations" of real documentation.
- Why unresolved: The dataset is intended for "prototyping," but the specific performance gap (sim-to-real domain shift) caused by the difference between GPT-4o's prose and actual clinical shorthand is not quantified.
- What evidence would resolve it: Zero-shot or fine-tuning transfer experiments measuring the performance drop when models trained on SimSUM are evaluated on a dataset of genuine, unstructured clinical notes (e.g., MIMIC-III).

### Open Question 4
- Question: Is SimSUM a reliable benchmark for multi-modal synthetic data generation methods?
- Basis in paper: [explicit] Section 3.4 identifies "multi-modal synthetic data generation" as a secondary use, suggesting the dataset could benchmark methods that generate joint tabular-text records.
- Why unresolved: While the authors provide a generative process, they do not benchmark existing generative models (e.g., joint VAEs or LLMs) against the dataset to establish quality thresholds for this task.
- What evidence would resolve it: A comparative analysis of multi-modal generative models evaluated on SimSUM using metrics for cross-modal consistency (e.g., does the generated text imply the generated tabular symptom?).

## Limitations
- Synthetic data generation using GPT-4o may not capture full complexity and variability of real-world clinical documentation
- Dataset focuses on 16 specific symptoms and 15 comorbidities, representing a narrow slice of clinical reality
- Automated annotation approach may miss nuanced symptom descriptions or context that human annotators would capture

## Confidence
- Claims about performance improvements from multimodal integration: High
- Claims about dataset utility for causal estimation and broader research applications: Medium

## Next Checks
1. Evaluate the same models on real clinical data to assess whether the performance gains generalize beyond synthetic data
2. Test the dataset's utility for causal estimation tasks by attempting to validate known clinical relationships
3. Conduct a longitudinal study to assess whether models trained on this synthetic data maintain performance when deployed on evolving clinical documentation patterns