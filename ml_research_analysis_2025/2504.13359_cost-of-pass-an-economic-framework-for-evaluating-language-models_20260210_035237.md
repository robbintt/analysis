---
ver: rpa2
title: 'Cost-of-Pass: An Economic Framework for Evaluating Language Models'
arxiv_id: '2504.13359'
source_url: https://arxiv.org/abs/2504.13359
tags:
- cost-of-pass
- cost
- frontier
- economic
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces cost-of-pass, an economic metric for evaluating
  language models by integrating both performance and inference cost into a single
  measure: the expected monetary cost to generate a correct output. Grounded in production
  theory, the authors define frontier cost-of-pass as the minimum achievable cost-of-pass
  across available models or human experts.'
---

# Cost-of-Pass: An Economic Framework for Evaluating Language Models

## Quick Facts
- arXiv ID: 2504.13359
- Source URL: https://arxiv.org/abs/2504.13359
- Reference count: 40
- Primary result: Introduces cost-of-pass metric that integrates performance and inference cost into a single economically interpretable measure, revealing distinct economic roles for different model families across task categories.

## Executive Summary
This paper introduces cost-of-pass, an economic metric that measures the expected monetary cost to generate a correct solution using language models. The framework integrates both performance accuracy and inference cost into a single dollar-denominated metric grounded in production theory. By tracking the frontier cost-of-pass across model families and comparing against human-expert baselines, the authors reveal how different model categories excel at distinct task types: lightweight models for basic quantitative tasks, large models for knowledge-intensive problems, and reasoning models for complex quantitative reasoning. The framework provides a principled tool for measuring AI progress and guiding deployment decisions.

## Method Summary
The cost-of-pass metric (v(m,p) = Cm(p) / Rm(p)) measures expected cost per correct solution by dividing per-attempt cost by success rate. The framework estimates accuracy Rm(p) from multiple independent inference trials and computes per-attempt costs from token usage and pricing. Frontier cost-of-pass tracks minimum achievable cost across available models plus human experts, forming a non-increasing sequence over time. Counterfactual analysis quantifies each model family's essential contribution by measuring cost increases when that family is removed from consideration.

## Key Results
- Lightweight models dominate basic quantitative tasks (93.5% contribution to Two Digit Addition frontier)
- Large models are most impactful for knowledge-based benchmarks (68.6% contribution to GPQA frontier)
- Reasoning models are essential for complex quantitative problems (81% contribution to AIME24 frontier)
- Recent cost-efficiency gains stem primarily from model-level innovations rather than inference-time techniques
- Complex quantitative task costs have halved approximately every 2.6 months over the past year

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** The cost-of-pass metric (v(m,p) = Cm(p) / Rm(p)) provides an economically interpretable measure of model efficiency by expressing expected cost per correct output.
- **Mechanism:** The formulation treats LMs as stochastic producers where the expected number of attempts to achieve one correct output follows a geometric distribution with mean 1/Rm(p). Multiplying by per-attempt cost Cm(p) yields expected total cost.
- **Core assumption:** Trials are independent and the probability Rm(p) is stable across attempts.
- **Evidence anchors:**
  - [abstract]: "We introduce 'cost-of-pass', the expected monetary cost of generating a correct solution."
  - [Section 2.2]: "Since the model output is stochastic, the expected number of attempts to obtain the first correct solution is 1/Rm(p), assuming independent trials."
  - [corpus]: Related work "Beyond Benchmarks: The Economics of AI Inference" proposes similar economics-of-inference framing but does not formalize this specific metric.
- **Break condition:** When Rm(p) approaches zero (model cannot solve task), cost-of-pass diverges to infinity; when accuracy variance across attempts is high, the geometric expectation becomes unreliable.

### Mechanism 2
- **Claim:** The frontier cost-of-pass tracks ecosystem-level progress by taking the minimum cost-of-pass across all available models plus a human-expert baseline.
- **Mechanism:** As new models are released, the frontier Vp(Mt) = min over all strategies of v(m,p) forms a monotonically non-increasing sequence. Each new model can only lower or maintain the frontier.
- **Core assumption:** Human experts achieve near-perfect accuracy (Rexpert(p) ≈ 1) for tasks they are qualified for.
- **Evidence anchors:**
  - [Section 2.5]: "As new LM models {mt} are released, the set expands such that Mt = Mt−1 ∪ {mt}. Consequently, the frontier cost-of-pass Vp(Mt) forms a non-increasing sequence over time t."
  - [Section 3.3]: Empirical tracking shows frontier cost-of-pass halved approximately every 2.6 months for MATH500.
  - [corpus]: "Economic Evaluation of LLMs" discusses Pareto frontiers but does not incorporate temporal tracking or human baselines.
- **Break condition:** If no model in M can solve problem p, the frontier remains infinite without the human baseline anchor.

### Mechanism 3
- **Claim:** Counterfactual frontier analysis quantifies the essential contribution of each model family by measuring relative cost increase when that family is removed.
- **Mechanism:** For model family g, compute Gp∼D(Mg, MT \ Mg) / Vp∼D(MT). This measures the fractional increase in frontier cost-of-pass if family g were unavailable.
- **Core assumption:** Model families have complementary strengths that are not fully substitutable.
- **Evidence anchors:**
  - [Section 3.4]: "We find that innovations in lightweight models have been instrumental in reducing costs on basic quantitative tasks. Large models, by contrast, have been most impactful for knowledge-based benchmarks."
  - [Figure 3]: Shows lightweight models contribute 93.5% to Two Digit Addition frontier; reasoning models contribute 81% to AIME24 frontier.
  - [corpus]: Corpus papers do not directly address counterfactual model-family analysis; this appears novel.
- **Break condition:** If model families become redundant (complete substitutability), counterfactual removals show near-zero impact.

## Foundational Learning

- **Concept: Production theory and stochastic frontier functions (Farrell 1957; Aigner et al. 1977)**
  - Why needed here: The paper's entire framework adapts classical economic production theory where producers convert inputs to outputs under uncertainty.
  - Quick check question: Given a producer with stochastic output, what does the "frontier cost" represent?

- **Concept: Geometric distribution and expected attempts**
  - Why needed here: The cost-of-pass formula derives from the expected number of Bernoulli trials needed to achieve first success.
  - Quick check question: If a model succeeds with probability 0.8 per attempt, how many attempts are expected to get one correct answer?

- **Concept: Token-based pricing in LLM APIs**
  - Why needed here: The cost component Cm(p) is computed from per-token pricing. Understanding input vs. output token costs is essential.
  - Quick check question: Why might output tokens cost more than input tokens in commercial LLM APIs?

## Architecture Onboarding

- **Component map:** Problem datasets P sampled from distributions D -> Multiple model families with configurable sampling -> Token tracking × per-token pricing -> n=8 independent trials per problem -> Cost-of-pass per (model, problem) -> Frontier computation across models
- **Critical path:**
  1. Define task distribution and human-expert baseline costs
  2. Run multiple inference trials per (model, problem) pair
  3. Compute per-attempt cost from token logs and pricing tables
  4. Calculate cost-of-pass = cost / accuracy for each model
  5. Take minimum across models for frontier
  6. Track frontier over time or compute counterfactuals
- **Design tradeoffs:**
  - Trial count (n=8): More trials improve accuracy estimation but increase evaluation cost
  - Temperature settings (0.7 vs 1.0 for reasoning models): Affects variance of outputs
  - Human baseline estimation: Upper-bound rates ensure conservative comparison but may overstate human cost
- **Failure signatures:**
  - Infinite cost-of-pass for all models indicates task is unsolved
  - Frontier shows no decrease over time indicates stagnation or evaluation issues
  - Counterfactual showing 0% impact from all families suggests redundancy
- **First 3 experiments:**
  1. Replicate single-model frontier analysis on a new dataset to validate metric behavior
  2. Compute temporal frontier for past 6 months on your domain to identify current best model family
  3. Apply inference-time techniques (majority voting with k=3,4) and verify marginal accuracy gains don't justify costs

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How does including verification costs for incorrect outputs alter the relative cost-efficiency of language models?
- **Basis in paper:** [explicit] The authors state their cost computation "neglects indirect or overhead costs associated with generating incorrect outputs, such as subsequent verification costs."
- **Why unresolved:** The current framework treats the cost of a wrong answer solely as the inference cost, ignoring the human or computational expense required to check and reject it.
- **What evidence would resolve it:** An updated cost model incorporating a variable cost term for validation mechanisms applied to failed attempts.

### Open Question 2
- **Question:** Does incorporating the variance of success rates into the cost-of-pass metric change the optimal model selection?
- **Basis in paper:** [explicit] The authors note the framework "currently does not account for variance information, limiting its practical interpretability."
- **Why unresolved:** Two models might have the same expected cost-of-pass, but one might have high variance while the other is consistent, which matters for reliability.
- **What evidence would resolve it:** A comparative analysis using a modified metric (e.g., Mean-Variance optimization) to see if the identified "frontier" models shift.

### Open Question 3
- **Question:** How sensitive is the human-expert baseline comparison to the assumption of near-perfect expert accuracy?
- **Basis in paper:** [explicit] The authors assume "experts typically achieve near-perfect correctness" but suggest future work evaluate "expert performance variability."
- **Why unresolved:** If experts have significant failure rates on complex tasks, their true cost-per-correct-solution increases, potentially changing the point where LMs become cheaper than humans.
- **What evidence would resolve it:** Empirical data from human subject studies measuring actual error rates of qualified experts on the benchmark datasets used in the paper.

## Limitations

- The framework assumes independent trials with stable accuracy across attempts, but real-world performance may exhibit state-dependent or time-varying accuracy.
- Human-expert baseline assumes near-perfect accuracy and fixed per-task costs, which may not hold for all problem categories.
- Token-based pricing models may not capture full operational costs including hosting, context window constraints, or API-specific optimizations.

## Confidence

- **High confidence:** The mathematical formulation of cost-of-pass as expected cost per correct solution is well-grounded in geometric distribution theory. The monotonic frontier property is mathematically guaranteed.
- **Medium confidence:** Empirical findings about model family essentialness depend on the specific model pool and task distributions tested. Results may vary with different model releases or task selections.
- **Medium confidence:** Counterfactual analysis assumes model families have complementary strengths, but this may not hold if future models demonstrate cross-family generalization.

## Next Checks

1. **Replicate frontier analysis on a new task domain** with 20+ problems to verify whether the observed patterns (lightweight → basic, large → knowledge, reasoning → complex) generalize beyond the current datasets.

2. **Implement confidence interval estimation** by increasing trial count to n=30 for a subset of problems to quantify statistical uncertainty in cost-of-pass estimates and assess whether current findings are robust to sampling variance.

3. **Test dynamic accuracy assumption** by running multiple evaluation sessions separated by days/weeks to measure Rm(p) stability over time, checking whether the independence assumption holds in practice.