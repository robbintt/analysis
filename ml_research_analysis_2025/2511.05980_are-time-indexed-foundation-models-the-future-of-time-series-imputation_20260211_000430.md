---
ver: rpa2
title: Are Time-Indexed Foundation Models the Future of Time Series Imputation?
arxiv_id: '2511.05980'
source_url: https://arxiv.org/abs/2511.05980
tags:
- pointwise
- blocks
- time
- datasets
- series
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents the first large-scale empirical study of time-indexed
  foundation models for zero-shot time series imputation. The authors evaluate TabPFN-TS
  and MoTM across 33 out-of-domain datasets, demonstrating that TabPFN-TS achieves
  the highest overall performance with a notable margin (MAE=0.293, avg.
---

# Are Time-Indexed Foundation Models the Future of Time Series Imputation?

## Quick Facts
- **arXiv ID**: 2511.05980
- **Source URL**: https://arxiv.org/abs/2511.05980
- **Authors**: Etienne Le Naour; Tahar Nabil; Adrien Petralia; Ghislain Agoua
- **Reference count**: 40
- **One-line primary result**: Time-indexed foundation models (TabPFN-TS and MoTM) achieve state-of-the-art zero-shot time series imputation without retraining, with TabPFN-TS demonstrating highest overall performance.

## Executive Summary
This paper presents the first large-scale empirical study of time-indexed foundation models for zero-shot time series imputation. The authors evaluate TabPFN-TS and MoTM across 33 out-of-domain datasets, demonstrating that TabPFN-TS achieves the highest overall performance with a notable margin (MAE=0.293, avg. rank=1.35), while MoTM also surpasses all supervised and local baselines. Both models can integrate covariates at inference time without retraining, drastically improving accuracy in domains where covariates strongly inform the target variable. The study also highlights practical considerations including computational efficiency, with MoTM offering favorable accuracy-efficiency trade-offs. The results demonstrate that time-indexed foundation models represent a powerful step toward general-purpose, zero-shot imputation for real-world time series.

## Method Summary
The paper evaluates two time-indexed foundation models - TabPFN-TS (TabPFN for Time Series) and MoTM (Model for Time-series Modeling) - on zero-shot time series imputation. Both models reformulate imputation as continuous-time regression using temporal representations H(t) at each timestamp. TabPFN-TS uses a transformer pre-trained on synthetic tabular tasks with handcrafted Fourier time features, while MoTM uses modulated INR basis functions with ridge regression. The evaluation covers 33 out-of-domain datasets with various missingness patterns (50% and 70% pointwise, 2-day and 4-day blocks) and three datasets with covariates. Models operate without retraining on target datasets, using only the temporal index and observed values.

## Key Results
- TabPFN-TS achieves the highest overall performance with MAE=0.293 and avg. rank=1.35, statistically superior to all competitors
- MoTM surpasses all supervised and local baselines with competitive accuracy and up to 100× faster inference
- Both models can integrate covariates at inference time without retraining, with substantial gains when covariates strongly inform the target
- MoTM offers favorable accuracy-efficiency trade-offs compared to TabPFN-TS's slower inference (1s per 672-step chunk on H100)

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Time-indexed models enable zero-shot imputation by reformulating the task as continuous-time regression rather than discrete sequence completion.
- **Mechanism**: Both TabPFN-TS and MoTM learn a contextual representation H(t) at each timestamp t, then apply a regressor r_θ(·) to map H(t) → x(t). This decouples the representation from any fixed sampling grid, allowing the model to query arbitrary timestamps including unobserved ones.
- **Core assumption**: The relationship between temporal position and series value can be captured through learned or engineered features that generalize across domains.
- **Evidence anchors**: [abstract] "These models share a common philosophy that places them within the family of time-indexed foundation models... enables missing value recovery without retraining"; [Section 2.2] "In essence, these two time-indexed models learn a contextual representation H(t) at every timestamp t. A regressor r_θ(·) is then applied to map H(t) to the observed time series value x(t)."; [corpus] "MoTM: Towards a Foundation Model for Time Series Imputation based on Continuous Modeling" corroborates the continuous modeling approach for imputation.
- **Break condition**: If target time series exhibit domain-specific temporal structures that cannot be captured by generic time-index features (e.g., irregular event-driven patterns without periodicity), the continuous-time formulation may underfit.

### Mechanism 2
- **Claim**: Large-scale synthetic pretraining combined with handcrafted temporal encodings confers measurable advantage over learned representations with simple regressors.
- **Mechanism**: TabPFN-TS uses a transformer pre-trained on hundreds of millions of synthetic tabular regression tasks. It ingests (H(t_obs), x(t_obs)) pairs as a "prompt" and infers functional relationships via attention, then predicts at query timestamps. MoTM uses modulated INR basis functions for richer learned representations but applies a simple ridge regressor.
- **Core assumption**: The pretraining distribution of TabPFN covers function classes that align with real-world temporal patterns encoded via normalized time index + daily/weekly Fourier features.
- **Evidence anchors**: [Section 2.2] "TabPFN is a large transformer-based architecture pre-trained on hundreds of millions of synthetically generated tabular regression tasks. Its defining characteristic is in-context learning"; [Section 3.1.1] "TabPFN-TS attains the best average rank and is statistically superior to all competitors (MAE=0.293, avg. rank=1.35)"; [Section D.1] Ablation shows TabPFN-TS is "highly sensitive to the choice of time-index features" — random periods degrade performance drastically.
- **Break condition**: If the handcrafted Fourier features (daily/weekly) mismatch the target domain's periodicity (e.g., hourly, monthly, or non-periodic data), performance may degrade; Table 10 confirms this sensitivity.

### Mechanism 3
- **Claim**: Covariates can be integrated at inference time without retraining by simple concatenation to the temporal representation.
- **Mechanism**: Assuming the covariate is fully observed, its value c(t) is stacked with H(t). The regression procedure (ridge for MoTM, in-context for TabPFN) proceeds unchanged, allowing the regressor to learn the H(t), c(t) → x(t) mapping from context points alone.
- **Core assumption**: The covariate provides orthogonal or complementary information to the temporal features, and the regressor has sufficient capacity to jointly model both.
- **Evidence anchors**: [Section 2.2] "Covariates integration with no retraining... additional contextual information available at timestamp t are simply stacked to the target contextual representation H(t)"; [Section 3.2] "Substantial gains are observed on datasets for which the covariate strongly informs about the target variable... MoTM to outperform the impressive univariate performances of TabPFN-TS"; [corpus] No direct corpus evidence on covariate integration; related papers focus on forecasting rather than imputation with covariates.
- **Break condition**: If covariates are partially observed or misaligned, naive concatenation fails; the paper acknowledges this limitation in Section E.2 and suggests pre-imputation or resampling as workarounds.

## Foundational Learning

- **Concept: Zero-shot generalization**
  - **Why needed here**: The paper's central claim is that foundation models can impute unseen datasets without retraining. Understanding what enables zero-shot transfer is essential for assessing when these models apply.
  - **Quick check question**: Can you explain why a model trained on synthetic tabular data might generalize to real-world time series imputation?

- **Concept: Implicit Neural Representations (INRs)**
  - **Why needed here**: MoTM's architecture relies on modulated INRs as basis functions. Without this background, the mechanism for learning continuous-time representations remains opaque.
  - **Quick check question**: How does an INR parameterize a continuous function, and why might modulation (dynamic parameter generation) help capture diverse temporal patterns?

- **Concept: In-context learning in transformers**
  - **Why needed here**: TabPFN-TS's effectiveness stems from its ability to infer functional relationships from context points within a single forward pass. This differs fundamentally from gradient-based fine-tuning.
  - **Quick check question**: What does it mean for a transformer to perform in-context regression, and how does the prompt structure enable this?

## Architecture Onboarding

- **Component map**: Time-indexed Foundation Model -> H(t) Generation -> Regressor -> Covariate Integration (optional)
  - H(t) Generation: TabPFN-TS: [normalized_t, sin(2πt/P_day), cos(...), sin(2πt/P_week), cos(...)] ; MoTM: Pre-trained modulated INR basis → K feature vectors concatenated
  - Regressor: TabPFN-TS: Frozen TabPFN transformer (in-context learning) ; MoTM: Ridge regression fitted per-window
  - Covariate Integration: Concatenate c(t) to H(t) before regression

- **Critical path**:
  1. Preprocess time series to extract context points (observed) and query points (missing)
  2. Construct H(t) for all timestamps using the appropriate encoder
  3. For MoTM: fit ridge regressor on (H(t_obs), x(t_obs)); for TabPFN-TS: pass context as prompt
  4. Predict x(t_miss) by applying regressor to H(t_miss)

- **Design tradeoffs**:
  - **Accuracy vs. speed**: TabPFN-TS achieves best accuracy but ~1s per 672-step chunk on H100; MoTM is up to 100× faster with competitive accuracy.
  - **Feature engineering vs. learned representations**: TabPFN-TS requires correct periodicity specification; MoTM learns flexible INRs but uses a weaker regressor.
  - **Assumption**: TabPFN-TS's Fourier features assume daily/weekly seasonality; MoTM's INR basis assumes sufficient training diversity.

- **Failure signatures**:
  - Sudden performance drop on low-frequency or non-periodic data → check time-index feature alignment (Table 10 shows sensitivity)
  - Large block missingness → local methods fail; foundation models should dominate (Figure 7c,d)
  - Covariate integration hurts performance → covariate may be noisier than target; try univariate mode first (Section E.1)

- **First 3 experiments**:
  1. **Reproduce univariate benchmark on 2-3 datasets** (e.g., BDG2-Bear, Jena Weather 1H) with both TabPFN-TS and MoTM across all four missingness scenarios to validate expected MAE rankings and understand variance across domains.
  2. **Ablate temporal features for TabPFN-TS** by testing: (a) original features, (b) daily-only, (c) time-index only, (d) random periods. Confirm sensitivity findings from Section D.1 before deploying to new domains.
  3. **Test covariate integration** on a domain with known strong covariate-target relationship (e.g., PV production + solar irradiance). Measure improvement vs. univariate and verify MoTM can sometimes exceed TabPFN-TS when covariates are informative.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: Can automatic feature learning or meta-learned temporal encodings match or exceed the performance of handcrafted time-index features in TabPFN-TS?
- **Basis in paper**: [explicit] Section D.1 states "an important direction for future research is the automatic construction or selection of temporal features... Instead of relying on manually crafted sinusoidal components, one could learn feature representations jointly with the model."
- **Why unresolved**: The ablation study shows TabPFN-TS is highly sensitive to feature design (random features cause severe degradation, MAE 0.904 vs 0.252), but all tested variants use fixed, pre-defined encodings.
- **What evidence would resolve it**: A learned encoding module trained jointly with the regressor or via separate meta-learning, evaluated across heterogeneous datasets with varying temporal granularities.

### Open Question 2
- **Question**: Would replacing MoTM's ridge regressor with an in-context learning model achieve TabPFN-level accuracy while maintaining MoTM's computational efficiency?
- **Basis in paper**: [explicit] The conclusion states: "A promising avenue to combine performance and efficiency would be to build a more powerful regressor on top of the modulated INR features used by MoTM, replacing the current ridge regressor with a model trained via in-context learning."
- **Why unresolved**: MoTM uses a simple ridge regressor (fast but less expressive), while TabPFN-TS uses a large transformer (expressive but slow). The trade-off between these design choices is unexplored.
- **What evidence would resolve it**: A hybrid model using MoTM's modulated INR representations with TabPFN-style in-context regression, benchmarked for both accuracy (MAE) and inference time.

### Open Question 3
- **Question**: How can time-indexed foundation models jointly model partially observed targets and covariates to improve imputation when covariates are incomplete or misaligned?
- **Basis in paper**: [explicit] Section E.2 notes that current covariate integration assumes full observation and temporal alignment, and states: "jointly model[ing] both the target and the covariate... remain[s] open research directions."
- **Why unresolved**: The current approach concatenates covariates to representations assuming they are fully available; the paper only discusses workarounds (impute covariates first, resample) rather than principled joint modeling solutions.
- **What evidence would resolve it**: A method that learns shared functional embeddings for target and covariate series, evaluated on datasets with realistic missingness patterns in both modalities.

## Limitations

- The handcrafted Fourier features in TabPFN-TS are highly sensitive to period specification, potentially limiting generalization to domains with non-standard temporal structures (Table 10 shows MAE degrades from 0.252 to 0.904 with random periods)
- Current covariate integration assumes full observation and temporal alignment, failing when covariates are partially observed or misaligned (Section E.2 acknowledges this limitation)
- The paper does not specify random seeds for missingness pattern generation or ridge regression hyperparameters, creating reproducibility challenges

## Confidence

- **High confidence**: The core mechanism of time-indexed foundation models (Mechanism 1) is well-supported by both the paper's results and the corpus evidence on MoTM's continuous modeling approach. The superiority of TabPFN-TS over supervised and local baselines (avg. rank=1.35) is robust across datasets.
- **Medium confidence**: The claim that TabPFN-TS's large-scale synthetic pretraining provides an advantage (Mechanism 2) is supported by ablation studies showing sensitivity to temporal features, but the assumption that synthetic pretraining aligns with real-world temporal patterns is not fully validated. The sensitivity to periodicity suggests potential brittleness.
- **Low confidence**: The integration of covariates at inference time without retraining (Mechanism 3) is demonstrated in the paper, but the corpus lacks direct evidence on this capability. The assumption that covariates are fully observed and aligned may not hold in practice, and the paper's acknowledgment of this limitation suggests it is an area for future work.

## Next Checks

1. **Validate temporal feature sensitivity**: Replicate the ablation study on TabPFN-TS by testing different time-index feature configurations (original, daily-only, time-index only, random periods) on 2-3 diverse datasets to confirm the reported sensitivity and understand the impact on real-world deployment.

2. **Test covariate integration on noisy data**: Evaluate the effectiveness of covariate integration on a dataset where the covariate is partially observed or noisy (e.g., solar irradiance with cloud cover gaps) to assess the robustness of the concatenation approach and the need for pre-imputation or resampling.

3. **Benchmark against domain-specific models**: Compare the performance of TabPFN-TS and MoTM against models specifically designed for non-periodic or low-frequency data (e.g., event-driven or monthly patterns) to quantify the extent of the "domain-specific temporal structures" limitation identified in the paper.