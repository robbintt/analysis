---
ver: rpa2
title: Enhancing Multi-Modal Video Sentiment Classification Through Semi-Supervised
  Clustering
arxiv_id: '2501.06475'
source_url: https://arxiv.org/abs/2501.06475
tags:
- sentiment
- classification
- data
- clustering
- multimodal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper addresses the challenge of video sentiment classification\
  \ by integrating multimodal data\u2014video, text, and acoustic features\u2014using\
  \ a semi-supervised clustering-based pretraining approach. The authors propose leveraging\
  \ Deep Embedded Clustering (DEC) to learn meaningful representations from a large\
  \ unlabeled subset of the CMU-MOSI dataset, guided by a smaller labeled subset."
---

# Enhancing Multi-Modal Video Sentiment Classification Through Semi-Supervised Clustering

## Quick Facts
- arXiv ID: 2501.06475
- Source URL: https://arxiv.org/abs/2501.06475
- Reference count: 23
- Primary result: Semi-supervised clustering pretraining achieves 81.5% accuracy on CMU-MOSI, within 2% of state-of-the-art while using significantly fewer parameters.

## Executive Summary
This paper addresses video sentiment classification by integrating multimodal data—video, text, and acoustic features—using a semi-supervised clustering-based pretraining approach. The authors propose leveraging Deep Embedded Clustering (DEC) to learn meaningful representations from a large unlabeled subset of the CMU-MOSI dataset, guided by a smaller labeled subset. After pretraining, the model is fine-tuned for supervised sentiment classification. Experiments demonstrate that this approach achieves strong performance in low-label scenarios, reaching 75.86% accuracy when trained on only 40% of labeled data.

## Method Summary
The approach uses a two-phase training strategy. First, a multimodal autoencoder is pretrained with reconstruction and disentanglement losses to learn basic feature representations. Then, DEC-based semi-supervised pretraining adds clustering and supervised cross-entropy losses to refine the latent space structure. The model employs separate Transformer encoders for each modality (video, text, acoustic), followed by mean pooling and concatenation for late fusion. The pretrained encoders are then transferred to a classifier and fine-tuned on labeled data.

## Key Results
- Achieves 81.5% accuracy on full CMU-MOSI dataset
- Reaches 75.86% accuracy using only 40% of labeled data
- Uses approximately 851K parameters, ~37% fewer than state-of-the-art MARNN (84.31% accuracy)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Semi-supervised clustering pretraining creates sentiment-aligned latent representations that improve downstream classification, particularly in low-label regimes.
- Mechanism: The DEC-based pretraining phase uses a KL divergence loss (L_cluster) to iteratively refine cluster assignments, while a small labeled subset guides cluster formation via cross-entropy supervision. This dual objective causes the latent space to organize around sentiment-relevant structures rather than arbitrary patterns.
- Core assumption: The clustering structure discovered in unlabeled multimodal data corresponds meaningfully to sentiment categories.
- Evidence anchors:
  - [abstract] "utilizes clustering-based semi-supervised pre-training to extract meaningful representations from the data"
  - [section 3.3] Equation 8 shows the combined loss integrating clustering, reconstruction, supervision, and disentanglement terms
- Break condition: If cluster centroids do not align with sentiment classes, the transfer to classification will degrade.

### Mechanism 2
- Claim: Separate Transformer encoding per modality preserves modality-specific temporal dynamics before fusion, enabling more effective cross-modal integration.
- Mechanism: Each modality passes through a dedicated Transformer encoder with positional encoding and 2 layers of 4-head attention. Mean aggregation compresses temporal sequences before concatenation.
- Core assumption: Modality-specific features are best learned in isolation before fusion.
- Evidence anchors:
  - [section 3.2] "Each modality is encoded using a dedicated Transformer-based encoder to capture its unique sequential dependencies"
- Break condition: If temporal alignment across modalities is critical, late fusion may miss cross-modal timing cues.

### Mechanism 3
- Claim: Disentanglement regularization improves latent space quality by reducing feature correlation, leading to more interpretable and effective representations.
- Mechanism: The disentanglement loss penalizes off-diagonal elements of the latent covariance matrix, encouraging orthogonal latent dimensions.
- Core assumption: Disentangled representations generalize better for downstream tasks.
- Evidence anchors:
  - [section 3.3] Equations 5-6 define covariance computation and the penalty term
- Break condition: Over-regularization can suppress useful correlated features.

## Foundational Learning

- Concept: **Deep Embedded Clustering (DEC)**
  - Why needed here: The core pretraining mechanism. You must understand how iterative cluster refinement via Student's t-distribution and target distribution updates shapes the latent space.
  - Quick check question: Can you explain why KL divergence between Q and P distributions improves cluster compactness over successive iterations?

- Concept: **Semi-supervised learning with mixed objectives**
  - Why needed here: The method balances four loss terms (clustering, reconstruction, supervision, disentanglement). Understanding how to weight and schedule these is critical.
  - Quick check question: What happens if β (supervision weight) is set too high during pretraining?

- Concept: **Multimodal fusion strategies**
  - Why needed here: Late fusion via concatenation is a design choice with tradeoffs. Understanding alternatives (early fusion, attention-based fusion, tensor fusion) helps evaluate when this approach applies.
  - Quick check question: Why might mean aggregation before fusion lose important temporal information?

## Architecture Onboarding

- Component map: Input features → Modality-specific Transformers → Mean pooling → Concatenation → FC layers → Classification head
- Critical path:
  1. Pretrain autoencoder (200 epochs, reconstruction + disentanglement only)
  2. Train clustering network (100 epochs, add clustering + supervision losses)
  3. Transfer encoder weights to classifier
  4. Fine-tune on labeled data for sentiment classification
- Design tradeoffs:
  - Parameter count (851K) vs. accuracy: ~2% below SOTA (MARNN: 84.31%) but ~37% fewer parameters
  - Late fusion simplicity vs. potential loss of cross-modal temporal dynamics
- Failure signatures:
  - High training accuracy with low validation accuracy → overfitting to limited labels
  - Cluster centroids not separating by sentiment class → pretraining not sentiment-aligned
  - Reconstruction loss dominates → reduce α; clustering signal being suppressed
- First 3 experiments:
  1. Reproduce baseline: Train classifier from scratch on 40% labeled data; expect ~73% accuracy with overfitting signs
  2. Ablate pretraining phases: Compare (a) autoencoder-only pretraining, (b) full DEC pretraining, (c) no pretraining; quantify contribution of each phase
  3. Vary labeled data ratio: Test with 20%, 40%, 60%, 80% labeled data to characterize the low-label benefit curve

## Open Questions the Paper Calls Out

- **Generalizability**: Does the proposed semi-supervised clustering framework generalize effectively to other multimodal sentiment benchmarks beyond CMU-MOSI? The authors note this as a validation need, as their analysis was limited to CMU-MOSI.
- **Domain Adaptation**: Can domain adaptation techniques be successfully integrated into this framework to improve versatility in cross-domain scenarios? Section 5.3 suggests this as future work, as the current model assumes the same distribution for pretraining and fine-tuning.
- **Disentanglement Impact**: What is the impact of the disentanglement loss on clustering performance and downstream classification accuracy when active? The paper disabled this term during clustering training without analyzing the consequences.

## Limitations

- Key architectural details (embedding dimensions, latent space size, dropout rates, exact FC layer sizes, number of clusters) are not specified, blocking exact replication.
- The assumption that cluster structure aligns with sentiment is not empirically validated—cluster-label correspondence is not measured.
- Evaluation is limited to binary classification accuracy on a single dataset, with no ablation on cross-modal timing alignment or disentanglement benefits.

## Confidence

- **High confidence**: Pretraining improves accuracy in low-label regimes; multimodal fusion via late concatenation works; DEC pretraining creates useful representations.
- **Medium confidence**: Disentanglement loss meaningfully improves latent space quality; separate modality encoders preserve signal better than early fusion.
- **Low confidence**: Clustering structure directly aligns with sentiment categories; parameter efficiency claim is complete given missing architectural specs.

## Next Checks

1. Measure and report cluster-to-sentiment alignment (e.g., normalized mutual information) to validate that pretraining targets are sentiment-relevant.
2. Perform ablation on the number of clusters in DEC to confirm binary clustering (k=2) is optimal for this task.
3. Compare late fusion vs. cross-modal attention fusion to test whether temporal cross-modal cues are being lost.