---
ver: rpa2
title: Can GPT tell us why these images are synthesized? Empowering Multimodal Large
  Language Models for Forensics
arxiv_id: '2504.11686'
source_url: https://arxiv.org/abs/2504.11686
tags:
- llms
- image
- images
- detection
- forgery
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates the use of multimodal large language models
  (LLMs) for synthetic image forensics, addressing the limitations of traditional
  detection methods that struggle with generalization and lack semantic interpretation.
  The authors propose a two-stage framework where GPT-4V first classifies images as
  real or fake, then provides detailed analysis including localization of tampered
  regions, description of forged content, reasoning for forgery judgment, and identification
  of generation methods (GAN or Diffusion).
---

# Can GPT tell us why these images are synthesized? Empowering Multimodal Large Language Models for Forensics

## Quick Facts
- **arXiv ID:** 2504.11686
- **Source URL:** https://arxiv.org/abs/2504.11686
- **Authors:** Yiran He; Yun Cao; Bowen Yang; Zeyu Zhang
- **Reference count:** 40
- **Primary result:** 92.1% accuracy on Diffusion-based forgeries using two-stage LLM framework with prompt engineering

## Executive Summary
This study demonstrates that multimodal large language models can effectively detect and analyze synthetic images through semantic reasoning rather than traditional signal-level artifact detection. The authors propose a two-stage framework where GPT-4V first classifies images as real or fake, then provides detailed forensic analysis including localization, content description, and method identification. Through meticulous prompt engineering and in-context learning, the approach achieves competitive performance with state-of-the-art methods while providing human-interpretable explanations of detection reasoning.

## Method Summary
The framework employs a two-stage sequential approach using GPT-4V, Llama-3.2-11B-Vision, and DeepSeek-VL2. Stage 1 performs binary classification (Real/Fake) using short prompts with 2-shot in-context learning exemplars. Stage 2 conducts detailed analysis only on images classified as fake, examining localization of tampered regions, forged content description, reasoning for forgery judgment, and identification of generation methods (GAN vs. Diffusion). The method uses 5-fold cross-validation on 4,000 real and 4,000 forged images from multiple datasets, with results averaged over 5 queries per image to handle variance.

## Key Results
- GPT-4V achieves 92.1% accuracy on Diffusion-based forgeries and 86.3% on GAN-based forgeries
- Two-stage architecture reduces rejection rate from 83.4% to 12.3% while maintaining high accuracy
- Localization accuracy scores average 3.72/5 on LaMa (GAN) and 3.38/5 on AutoSplice (Diffusion)
- Performance degrades on face images due to semantic complexity and safety filters

## Why This Works (Mechanism)

### Mechanism 1: Semantic-Level Anomaly Detection via Pre-Trained World Knowledge
Multimodal LLMs detect synthetic images by identifying semantic inconsistencies rather than signal-level artifacts. During pre-training on vast image-text corpora, models encode world knowledge about natural scene properties (lighting coherence, texture consistency, physical plausibility). When presented with synthetic images, the model detects violations of these learned expectations—unnatural edges, inconsistent shadows, implausible object relationships—which manifest as higher perplexity or semantic conflict. This mechanism degrades to random guessing if synthetic images achieve semantic parity with real images.

### Mechanism 2: Two-Stage Cognitive Decomposition Reduces Hallucination Interference
Separating detection (Stage 1) from detailed analysis (Stage 2) prevents prompt-length-induced hallucination bias. Stage 1 uses minimal prompts to elicit binary judgments before analytical reasoning activates. Stage 2 deploys detailed, multi-task prompts only after a "fake" verdict. This prevents the observation that longer prompts tend to increase false positive rates. The approach assumes detection and analysis require different cognitive modes—semantic matching vs. spatial reasoning—that interfere when combined in single-pass inference.

### Mechanism 3: In-Context Learning Activates Latent Forensic Reasoning Patterns
Few-shot exemplars unlock forensic analysis capabilities without weight updates by providing reasoning templates and output formatting. Two-shot examples serve dual functions: demonstrating expected output schema and activating latent reasoning pathways in pre-trained weights that associate visual anomaly patterns with explanatory language. The 12% accuracy gain from 0→2 shots suggests the model contains but cannot spontaneously deploy forensic reasoning without demonstrations. This mechanism degrades if exemplar distribution diverges significantly from test distribution.

## Foundational Learning

- **Concept: Semantic vs. Signal-Level Forgery Detection**
  - *Why needed here:* The paper's central thesis is that LLMs succeed where traditional CNNs fail because they detect semantic rather than pixel-level inconsistencies. Understanding this distinction is prerequisite to interpreting why the method generalizes across unseen generators.
  - *Quick check question:* A CNN trained on ProGAN images achieves 99% detection on ProGAN but 1% on StyleGAN. Would you expect an LLM with no forensic training to show more balanced performance across both? Explain why.

- **Concept: Prompt Length vs. Hallucination Trade-off**
  - *Why needed here:* The authors document that longer prompts increase false positive rates—a counterintuitive finding if you assume more context always helps. This trade-off motivates the two-stage architecture.
  - *Quick check question:* If you observe that adding detailed forensic instructions to a prompt causes GPT-4V to classify more real images as fake, what hypothesis about model behavior does this suggest?

- **Concept: ICL (In-Context Learning) as Pattern Activation vs. Fine-Tuning**
  - *Why needed here:* The method relies entirely on ICL—no weight updates occur. Engineers must understand that exemplars work by activating existing knowledge rather than teaching new capabilities.
  - *Quick check question:* You have 100 forensic exemplars. Should you use all 100 in your prompt? What does the paper's ablation suggest about marginal returns?

## Architecture Onboarding

- **Component map:**
  Input Image + Text Prompt -> Stage 1: Classification (Binary output) -> If "Fake" -> Stage 2: Analysis (Structured Forensic Report)

- **Critical path:**
  1. Prompt engineering following 5 principles (Profile, Goal, Constraint, Workflow, Style)—Prompt #4 selected for Stage 1
  2. Exemplar curation (10 total created, 2-shot sampling strategy)
  3. Two-stage sequential invocation with conditional Stage 2 execution
  4. Multi-round querying (5 rounds) for probabilistic scoring and AUC computation

- **Design tradeoffs:**
  - Model size vs. performance: GPT-4V (AUC 83.5% Diffusion, 81.3% GAN) significantly outperforms Llama-3.2-11B (77.9%, 75.6%) and DeepSeek-2.8B (~67%, 68%)—but at $150 API cost vs. free local deployment
  - Prompt length vs. rejection rate: Longer prompts reduce rejection (83.4% → 12.3%) but risk over-classification as fake
  - Shot count vs. cost: 0→2 shots yields ~12% accuracy gain; 2→4 yields only 1.5% gain with 2× token cost

- **Failure signatures:**
  - High rejection on face images without exemplars: GPT-4V refuses 80% of face forgery queries with simple prompts (safety filters)
  - Localization accuracy drop on Diffusion forgeries: GPT-4V achieves 66.25% on AutoSplice vs. 72% on LaMa (GAN) because Diffusion produces smoother boundaries
  - Real image over-classification as fake: DeepSeek-VL2 shows 40% gap vs. traditional methods on real images—semantic "unusualness" misinterpreted as forgery
  - Face-specific semantic complexity: All LLMs perform worse on faces than general images (GPT-4V: 76.7% real faces vs. 87.1% real general)

- **First 3 experiments:**
  1. Prompt ablation replication: Test Prompts #1-#5 on a held-out 200-image subset (100 real, 100 fake from unseen generator). Measure ACC, AUC, and rejection rate. Validate the token-count vs. accuracy correlation.
  2. ICL sensitivity with stratified exemplars: Create two exemplar pools—(a) GAN-heavy, (b) Diffusion-heavy. Test 2-shot sampling from each on both generator types. Quantify exemplar-distribution mismatch impact.
  3. Stage 1 error propagation analysis: Manually label Stage 1 false negatives. Characterize what forgery types bypass detection (subtle local edits vs. global synthesis). Determine if specific semantic anomaly types are systematically missed.

## Open Questions the Paper Calls Out

- **Question 1:** Does a hybrid framework using an LLM as a task allocator for specialized forensic tools improve localization precision compared to a standalone LLM approach?
  - *Basis in paper:* Section 4.5 proposes a "potential improvement" where the LLM acts as a connector, assigning sub-tasks to specialized small models to achieve fine-grained analysis.
  - *Why unresolved:* The current two-stage method relies solely on LLM reasoning, which struggles with fine-grained localization; the hybrid architecture is conceptualized but not implemented.
  - *What evidence would resolve it:* Implementation of the hybrid system demonstrating higher localization accuracy (e.g., IoU scores) against ground truth masks than the LLM-only baseline.

- **Question 2:** How can the proposed two-stage forensic framework be adapted to detect temporal inconsistencies in AI-generated videos?
  - *Basis in paper:* The Conclusion identifies "strategies for expanding LLM applications to cover other media formats such as video" as an "exciting avenue of research."
  - *Why unresolved:* The study is restricted to static images, while video forgeries present unique challenges such as temporal consistency that current multimodal LLMs do not explicitly analyze.
  - *What evidence would resolve it:* An extension of the framework applied to video datasets (e.g., face-swap videos) that successfully detects inter-frame artifacts or unnatural motion.

- **Question 3:** What specific mechanisms can mitigate the performance drop in LLM-based face forgery detection caused by high semantic complexity?
  - *Basis in paper:* The Abstract and Section 4.2 highlight that detection limitations remain for face images, and Section 4.2 notes LLMs often misinterpret "unusual" real features (like motion blur) as forgeries.
  - *Why unresolved:* The paper identifies that semantic complexity (age, expression) degrades performance but does not propose a solution beyond the standard prompt engineering used for general images.
  - *What evidence would resolve it:* Ablation studies showing that specialized prompts or fine-tuning on face-specific attributes significantly reduces the error rate for real-face classification.

## Limitations
- Face image detection performance degrades significantly across all models due to semantic complexity and safety filters
- Diffusion forgeries show systematically worse localization than GAN forgeries due to boundary characteristics
- Critical reproducibility barrier: exact ICL exemplars and judge system prompts are not provided in paper

## Confidence
- **Semantic detection mechanism:** Medium-High - supported by ablation studies but world-knowledge assumption unverified
- **Two-stage architecture effectiveness:** High - directly measured through rejection rate and accuracy improvements
- **ICL generalization:** Medium - demonstrated through exemplar variation but transfer mechanisms unclear

## Next Checks
1. **ICL Transfer Sensitivity Test:** Create two exemplar pools (GAN-heavy vs. Diffusion-heavy) and test cross-distribution performance to quantify exemplar-diversity impact on detection accuracy

2. **Semantic Anomaly Cataloging:** Analyze Stage 1 false negatives to identify systematically missed forgery types (e.g., subtle local edits vs. global synthesis) and correlate with specific semantic anomaly categories

3. **Real Image Unusualness Audit:** Examine false positive cases on real images to determine if semantic "unusualness" features (motion blur, focus issues) are systematically misinterpreted as forgery artifacts