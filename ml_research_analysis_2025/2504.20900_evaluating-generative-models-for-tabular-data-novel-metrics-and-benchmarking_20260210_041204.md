---
ver: rpa2
title: 'Evaluating Generative Models for Tabular Data: Novel Metrics and Benchmarking'
arxiv_id: '2504.20900'
source_url: https://arxiv.org/abs/2504.20900
tags:
- data
- mode
- generative
- metrics
- tabular
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces three novel evaluation metrics\u2014FAED,\
  \ FPCAD, and RFIS\u2014to assess generative models for tabular data, addressing\
  \ the lack of reliable evaluation methods in this domain. FAED and FPCAD are inspired\
  \ by the Fr\xE9chet Inception Distance (FID) used in image evaluation, leveraging\
  \ autoencoder embeddings and PCA-transformed representations, respectively, to compare\
  \ real and synthetic data distributions."
---

# Evaluating Generative Models for Tabular Data: Novel Metrics and Benchmarking

## Quick Facts
- arXiv ID: 2504.20900
- Source URL: https://arxiv.org/abs/2504.20900
- Reference count: 31
- Introduces three novel metrics (FAED, FPCAD, RFIS) for evaluating generative models on tabular data, showing superior detection of generative modeling failures compared to existing methods

## Executive Summary
This paper addresses the challenge of evaluating generative models for tabular data by introducing three novel metrics: FAED, FPCAD, and RFIS. These metrics are designed to detect quality degradation, mode drop, and mode collapse in synthetic tabular data generation. The authors validate their approach on three network intrusion detection datasets, demonstrating that FAED and RFIS effectively identify generative modeling issues while existing metrics like SDV Fidelity, TSTR, and TRTS fail to reliably detect key problems. The study provides a more robust framework for synthetic data evaluation, though further refinements are needed for FPCAD's consistency across diverse datasets.

## Method Summary
The paper introduces three novel evaluation metrics for generative models of tabular data. FAED (Fréchet AutoEncoder Distance) measures distributional differences between real and synthetic data using autoencoder latent representations and Fréchet distance. FPCAD (Fréchet PCA-based Distance) uses PCA-transformed representations instead of learned embeddings for a pre-training-free alternative. RFIS (Random Forest Inception Score) adapts the Inception Score concept to tabular data by measuring entropy differences in label distributions using a Random Forest classifier. The metrics were evaluated against existing methods (SDV Fidelity, Utility, TSTR, TRTS) on three network intrusion detection datasets (UNSW-NB15, TON-IoT, CICIDS-2017) under controlled experiments introducing quality decrease, mode drop, and mode collapse challenges.

## Key Results
- FAED effectively detected all three generative modeling issues (quality decrease, mode drop, mode collapse) across all datasets
- FPCAD showed promising but inconsistent performance, with paradoxical score increases on CICIDS-2017 under noise injection
- RFIS captured quality degradation and mode collapse well but struggled with mode drop detection on TON-IoT
- Existing metrics (SDV Fidelity, TSTR, TRTS) failed to reliably identify key generative modeling problems
- The relative score metric (score_base - score_gen) proved effective for quantifying sensitivity across all evaluation metrics

## Why This Works (Mechanism)

### Mechanism 1: Latent-Space Distribution Matching via FAED
Autoencoder latent representations preserve structural features of tabular data, enabling Fréchet distance to detect distributional mismatches between real and synthetic data. A pre-trained autoencoder compresses both real and synthetic tabular records into a shared latent space, and the Fréchet distance between Gaussian-modeled distributions of these embeddings captures both mean-shift and covariance-divergence, detecting subtle structural distortions that column-wise statistical tests miss. Core assumption: Latent vectors approximately follow a multivariate Gaussian distribution. Evidence: FAED showed the most consistent detection across all three failure modes in controlled experiments. Break condition: If the autoencoder fails to capture salient features or if latent distributions are highly non-Gaussian, FAED sensitivity degrades.

### Mechanism 2: Reference-Free Quality Assessment via RFIS
Entropy differences between conditional and marginal label distributions, measured via a Random Forest classifier, reflect synthetic data quality without requiring real data at inference time. A Random Forest is trained on real data to predict labels, and for synthetic samples, the classifier outputs soft label distributions. High-quality synthetic data yields confident predictions (low conditional entropy) while maintaining diversity across classes (high marginal entropy). RFIS quantifies this gap. Core assumption: The Random Forest classifier generalizes well to synthetic samples. Evidence: RFIS scores increased with noise level, reflecting reduced classification confidence. Break condition: If the classifier is miscalibrated or if synthetic data contains out-of-distribution samples, RFIS becomes unreliable.

### Mechanism 3: PCA-Based Distributional Distance via FPCAD
PCA-transformed representations can substitute for learned embeddings, providing a pre-training-free alternative for distributional comparison. PCA projects both real and synthetic data onto their top principal components, preserving maximal variance, and Fréchet distance is then computed on these projections. This avoids autoencoder training overhead but relies on linear feature extraction. Core assumption: Principal components capture the most relevant features for distinguishing real from synthetic data. Evidence: FPCAD showed inconsistent performance across datasets, with unexpected score increases under noise. Break condition: If the true generative failure manifests in directions with lower variance, FPCAD fails to detect it.

## Foundational Learning

- **Fréchet Distance (Wasserstein-2)**: Core mathematical foundation for FAED and FPCAD; measures similarity between two multivariate Gaussians via mean and covariance. Why needed: Enables comparison of high-dimensional distributions. Quick check: Given two distributions with identical means but different covariances, will Fréchet distance be zero? (No)

- **Autoencoder Latent Representations**: FAED depends on the quality and structure of learned embeddings. Why needed: Determines what features are preserved for distributional comparison. Quick check: What happens to FAED if the autoencoder is trained only on synthetic data? (Likely degraded performance due to distribution shift)

- **Entropy in Classification (Conditional vs. Marginal)**: RFIS relies on the difference between predictive certainty and overall label diversity. Why needed: Captures both quality and diversity aspects of synthetic data. Quick check: If a generative model produces only one class, what happens to RFIS? (Marginal entropy low, conditional entropy also low, score approaches zero)

## Architecture Onboarding

- **Component map**:
  - FAED pipeline: Real/Synthetic data → Pre-trained Autoencoder → Latent vectors → Mean/Covariance estimation → Fréchet Distance
  - FPCAD pipeline: Real/Synthetic data → PCA fit on combined data → Top-k components → Mean/Covariance → Fréchet Distance
  - RFIS pipeline: Synthetic data → Pre-trained Random Forest → Soft predictions → Entropy computation (conditional vs. marginal) → Score

- **Critical path**:
  1. Train autoencoder on real data (for FAED)
  2. Train Random Forest classifier on real data (for RFIS)
  3. Compute baseline scores comparing held-out real vs. real (score_base)
  4. Generate synthetic samples; compute score_gen
  5. Compute relative score: rel(score) = score_base − score_gen

- **Design tradeoffs**:
  - FAED: Highest sensitivity but requires autoencoder pre-training; assumption of Gaussian latents may break
  - FPCAD: No pre-training required, computationally lighter, but inconsistent across datasets
  - RFIS: Reference-free at inference, good for mode collapse and quality degradation, but struggles with mode drop
  - Existing metrics: Computationally simple but systematically fail to detect key issues

- **Failure signatures**:
  - FAED: Fails if autoencoder latent space is poorly structured or non-Gaussian
  - FPCAD: Inconsistent on datasets where critical variance lies in minor components
  - RFIS: Fails on mode drop when Random Forest predictions remain confident despite missing modes

- **First 3 experiments**:
  1. Baseline sanity check: Split real data into train/test; compute all metrics comparing test vs. train. Confirm rel(score) ≈ 0 for all metrics.
  2. Controlled noise injection: Inject Gaussian noise at increasing levels (α = 0.1 to 0.5) into test data. Verify FAED and RFIS show monotonic degradation.
  3. Mode collapse stress test: Replace test data with collapsed representations (mean values per class). Confirm FAED shows largest relative score drop.

## Open Questions the Paper Calls Out

### Open Question 1
What specific methodological refinements are required to stabilize FPCAD performance across diverse tabular datasets? The paper states "further research is needed to refine FPCAD and enhance its robustness across diverse datasets," noting that it currently exhibits "promising but inconsistent performance." The experimental results showed FPCAD produced paradoxical score increases in the CICIDS-2017 dataset during noise insertion tests, indicating the current PCA-based approach lacks universal reliability. What evidence would resolve it: A modified FPCAD metric demonstrating statistically consistent sensitivity to noise and mode drop across all three benchmark datasets without dataset-specific anomalies.

### Open Question 2
Can FAED and RFIS effectively evaluate generative models in domains with high class imbalance or different structural complexity, such as healthcare or finance? The authors explicitly list exploring "broader domains such as financial fraud detection, healthcare analytics, and synthetic data generation" as a primary direction for future work. The study validated the metrics exclusively on network intrusion detection datasets; it is unproven whether the autoencoder latent spaces or Random Forest entropy calculations remain discriminative in domains with different data distributions. What evidence would resolve it: Benchmarking FAED and RFIS on standard financial or clinical tabular datasets, demonstrating they detect quality decrease and mode collapse with the same efficacy shown in cybersecurity data.

### Open Question 3
To what extent does the violation of the multivariate Gaussian assumption in autoencoder latent spaces bias the Fréchet AutoEncoder Distance (FAED)? The paper acknowledges that FAED "depends on the assumption that the feature space follows a multivariate Gaussian distribution, which may not always hold across diverse datasets." While the metric performed well empirically, the paper does not quantify the error introduced when the latent features of the tabular data deviate from this Gaussian assumption. What evidence would resolve it: A comparative analysis measuring the divergence between FAED and non-parametric distribution distance metrics on datasets engineered to possess non-Gaussian latent representations.

## Limitations
- Evaluation limited to three network intrusion detection datasets, all binary classification tasks
- Lack of detailed hyperparameter specifications makes exact reproduction challenging
- Robustness across different data types (continuous, mixed, high-cardinality categorical) and multi-class scenarios remains untested
- FPCAD shows dataset-specific inconsistencies that need further investigation

## Confidence
- **High confidence**: FAED's mechanism and effectiveness in detecting generative modeling issues are well-supported by experimental results
- **Medium confidence**: FPCAD shows promise but inconsistent performance across datasets, particularly paradoxical score increases on CICIDS-2017
- **Medium confidence**: RFIS effectively captures quality degradation and mode collapse, but failure to detect mode drop on TON-IoT suggests sensitivity to classifier generalization

## Next Checks
1. **Dataset Diversity Test**: Evaluate FAED, FPCAD, and RFIS on a broader range of tabular datasets including multi-class classification, regression, and high-cardinality categorical features to assess generalizability
2. **Hyperparameter Sensitivity Analysis**: Systematically vary autoencoder architecture (latent dimension, layers), Random Forest hyperparameters, and PCA component count to quantify their impact on metric stability and performance
3. **Latent Distribution Validation**: Test the Gaussian assumption underlying FAED by applying normality tests (e.g., Shapiro-Wilk) on latent embeddings across datasets and assess metric performance when this assumption is violated