---
ver: rpa2
title: 'MARS: Co-evolving Dual-System Deep Research via Multi-Agent Reinforcement
  Learning'
arxiv_id: '2510.04935'
source_url: https://arxiv.org/abs/2510.04935
tags:
- system
- reasoning
- information
- mars
- multi-agent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MARS addresses excessive token consumption and outdated knowledge
  in Large Reasoning Models by introducing a co-evolutionary dual-system framework.
  System 1 (fast, intuitive processing) and System 2 (deliberate reasoning) are jointly
  optimized through multi-agent reinforcement learning, enabling them to develop complementary
  strategies where System 1 learns to distill task-relevant information for System
  2's reasoning.
---

# MARS: Co-evolving Dual-System Deep Research via Multi-Agent Reinforcement Learning

## Quick Facts
- arXiv ID: 2510.04935
- Source URL: https://arxiv.org/abs/2510.04935
- Reference count: 40
- 8B MARS achieves 8.17% accuracy on Humanity's Last Exam, outperforming WebThinker (32B with SFT, 6.87%) and narrowing the gap with Claude 3.7 Sonnet (7.89%)

## Executive Summary
MARS introduces a co-evolutionary dual-system framework for deep research that addresses excessive token consumption and outdated knowledge in Large Reasoning Models. The framework trains System 1 (fast, intuitive processing) and System 2 (deliberate reasoning) jointly via multi-agent reinforcement learning, enabling them to develop complementary strategies where System 1 learns to distill task-relevant information for System 2's reasoning. Trained under a Zero RL setting without supervised fine-tuning, MARS achieves 8.17% accuracy on Humanity's Last Exam while delivering an average 8.9% improvement across seven knowledge-intensive tasks.

## Method Summary
MARS extends Group Relative Policy Optimization (GRPO) for multi-agent settings by introducing decoupled gradient computation, where System 1 and System 2 receive shared rewards but compute gradients over non-overlapping token sets. The framework uses First Fit Decreasing (FFD) bin-packing to organize tool outputs into optimally-sized chunks for parallel System 1 processing, and advantage-weighted balanced sampling to prevent training collapse from imbalanced sample counts. Trained from Qwen2.5-7B-Instruct/Qwen3-8B without supervised fine-tuning, the single-model checkpoint is instantiated with different prompts for each system's distinct roles.

## Key Results
- 8.17% accuracy on Humanity's Last Exam, outperforming WebThinker (32B with SFT, 6.87%)
- 8.9% average improvement across seven knowledge-intensive tasks compared to baselines
- 1.91% HLE drop (7.38%→5.47%) when System 1 is removed, confirming active contribution beyond passive compression

## Why This Works (Mechanism)

### Mechanism 1: Credit Assignment via Decoupled Gradients
Joint training with shared rewards enables co-evolution without one system dominating learning. System 1 and System 2 compute gradients over non-overlapping token sets, with System 2's loss masking System 1's output tokens and vice versa. This ensures each system learns "what made my behavior better than alternatives" rather than raw success/failure.

### Mechanism 2: Purpose-Conditioned Information Distillation
System 1 learns task-relevant extraction strategies that specifically support System 2's reasoning. System 2 generates a "purpose" with each tool call, and System 1 conditions its distillation on this purpose, learning to extract entities, relations, and constraints relevant to the current reasoning step.

### Mechanism 3: Bin-Packing + Balanced Sampling for Training Stability
FFD bin-packing organizes variable-length tool outputs into optimally-sized chunks for parallel System 1 processing. Advantage-weighted balanced sampling pre-computes advantages for all samples, then balances System 1 samples to match System 2 count, preventing either system from dominating gradient updates.

## Foundational Learning

- **Group Relative Policy Optimization (GRPO)**: MARS extends GRPO for multi-agent settings by normalizing rewards within groups. Quick check: Can you explain how GRPO computes advantages differently from standard PPO?
- **Dual-Process Theory (Cognitive Science)**: The System 1/System 2 naming and functional division draws from this framework. Quick check: What cognitive characteristics distinguish fast/intuitive (System 1) from deliberate (System 2) processing?
- **Credit Assignment in Multi-Agent RL**: The core challenge MARS solves is assigning credit correctly when agents share rewards. Quick check: Why is credit assignment harder in multi-agent settings than single-agent RL?

## Architecture Onboarding

- **Component map**: System 2 -> Tool Execution -> FFD Bin-Packing -> Parallel System 1 Processing -> Distilled Outputs -> System 2 Reasoning -> Answer
- **Critical path**: System 2 generates reasoning + tool request with purpose → Tool executes → Bin-packing organizes outputs → System 1 processes chunks in parallel → Distilled info updates context → Repeat until answer → Binary reward → Advantages normalized → Balanced sampling → Gradient update
- **Design tradeoffs**: Single shared model vs. separate models (shared parameters enable tighter co-adaptation but require careful masking); Zero RL vs. SFT warmup (harder optimization but cleaner evaluation); Binary reward vs. shaped rewards (simpler but sparser learning signal)
- **Failure signatures**: System 1 produces generic summaries (check purpose conditioning and masking); Tool calls excessive or missing (check System 2's tool selection learning); Training collapses (verify balanced sampling is active); Context overflow (verify bin-packing truncation logic)
- **First 3 experiments**: Ablation: Remove System 1 (expect ~2% HLE drop); Ablation: Single tool only (identify tool contribution per domain); Training dynamics: Plot tools per question and response lengths over training

## Open Questions the Paper Calls Out

### Open Question 1
How does MARS's co-evolution framework scale to significantly larger models (e.g., 70B+ parameters), and does the relative benefit of co-evolution diminish, persist, or increase at scale? The paper only evaluates 7B/8B models, comparing them against larger baselines but not studying scale effects.

### Open Question 2
Can the co-evolutionary dual-system framework generalize to non-QA domains such as code generation, mathematical theorem proving, or multi-modal reasoning? All evaluation benchmarks are QA tasks, and the reward design and tool set are tailored to knowledge-intensive question answering.

### Open Question 3
Would using separate model checkpoints for System 1 and System 2 yield better specialization than the current single-checkpoint design? The authors explicitly address this design choice but haven't empirically compared against a dual-checkpoint alternative.

## Limitations
- Training duration and convergence criteria are unspecified, making it difficult to assess whether results reflect optimal performance
- The 8B model's HLE accuracy (8.17%) remains substantially below Claude 3.7 Sonnet (7.89%) despite claims of narrowing the gap
- Zero RL training from scratch without SFT is significantly more challenging, yet the paper doesn't compare against SFT-pretrained baselines

## Confidence
- **High**: The dual-system architectural design and GRPO extension with decoupled gradients are technically sound and well-specified
- **Medium**: The HLE results and multi-task improvements are supported by reported numbers, though absolute performance gaps warrant scrutiny
- **Low**: Claims about "narrowing the gap" with Claude 3.7 require careful verification given the arithmetic inconsistency

## Next Checks
1. **Ablation Study Replication**: Remove System 1 to verify the 1.91% HLE drop (7.38%→5.47%) reported in Table 5
2. **Purpose Conditioning Test**: Systematically evaluate System 1's distillation quality with/without purpose conditioning
3. **Training Dynamics Analysis**: Track tools per question and response length distributions throughout training to confirm the progression from single-tool to multi-tool usage patterns