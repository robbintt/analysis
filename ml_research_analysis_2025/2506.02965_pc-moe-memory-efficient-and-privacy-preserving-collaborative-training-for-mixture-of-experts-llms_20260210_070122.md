---
ver: rpa2
title: 'PC-MoE: Memory-Efficient and Privacy-Preserving Collaborative Training for
  Mixture-of-Experts LLMs'
arxiv_id: '2506.02965'
source_url: https://arxiv.org/abs/2506.02965
tags:
- training
- expert
- each
- party
- experts
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of collaborative training of
  large Mixture-of-Experts (MoE) language models (LLMs) in a setting where parties
  have limited GPU memory and data resources, while also protecting the privacy of
  each party's training data. The proposed Privacy-preserving Collaborative Mixture-of-Experts
  (PC-MoE) method leverages the sparsity of the MoE architecture to distribute expert
  parameters across multiple parties.
---

# PC-MoE: Memory-Efficient and Privacy-Preserving Collaborative Training for Mixture-of-Experts LLMs

## Quick Facts
- **arXiv ID:** 2506.02965
- **Source URL:** https://arxiv.org/abs/2506.02965
- **Authors:** Ze Yu Zhang; Bolin Ding; Bryan Kian Hsiang Low
- **Reference count:** 40
- **One-line primary result:** Reduces per-party GPU RAM usage by up to 70% while maintaining or exceeding centralized baseline accuracy across seven LLM benchmarks, via sparse expert parameter sharding and privacy-preserving training.

## Executive Summary
PC-MoE enables collaborative fine-tuning of Mixture-of-Experts (MoE) LLMs across multiple parties with limited GPU memory and private datasets. It distributes expert parameters across parties and uses top-k sparse routing to share only small activation and gradient signals, preserving privacy and reducing memory by up to 70%. The approach achieves accuracy that matches or exceeds centralized baselines on seven popular LLM benchmarks, demonstrating strong resistance to gradient-inversion attacks.

## Method Summary
PC-MoE distributes expert parameters across n parties via round-robin sharding, with each party hosting m/n experts. During training, only top-k expert activations and gradients are communicated across parties, keeping most of the model and data private. The method uses sequential round-robin updates without global synchronization, allowing each party to update only its local backbone and hosted experts. This design reduces per-party GPU RAM usage significantly while maintaining model performance and privacy through sparse gradient exposure.

## Key Results
- Reduces per-party GPU RAM usage by up to 70% compared to centralized training
- Achieves accuracy matching or exceeding centralized baselines across seven LLM benchmarks
- Demonstrates strong resistance to gradient-inversion attacks with ROUGE-1/2/L scores of only 2–8%

## Why This Works (Mechanism)

### Mechanism 1: Expert Sharding with Sparse Cross-Party Activation
- Claim: Distributing experts across parties reduces per-party GPU memory while preserving access to full model capacity
- Mechanism: Each party hosts exactly m/n experts via round-robin assignment. The gating network selects top-k experts per token; only hidden activations for those k experts are transmitted across the network. Gradients retrace the identical sparse path during backpropagation.
- Core assumption: Expert parameters dominate total trainable parameters (paper reports 93.11%); backbone layers are small enough to fit locally.
- Evidence anchors:
  - [abstract]: "parties only share sparse activation signals and gradients for the few experts invoked, dramatically reducing communication overhead"
  - [section 3.4]: "our proposed training method on average only costs k/n of the RAM used by expert layers a vanilla MoE training approach uses"
  - [corpus]: RevFFN (arXiv:2512.20920) addresses MoE fine-tuning memory via reversible blocks, but focuses on single-device efficiency rather than collaborative distribution.
- Break condition: If k approaches m (near-full activation), per-party memory savings collapse and inter-party communication becomes a bottleneck.

### Mechanism 2: Privacy via Sparse Gradient Exposure
- Claim: Sparse expert activation limits observable gradient signal, degrading gradient-inversion attack effectiveness
- Mechanism: Top-k routing uniformly distributes activated experts across n parties. Any single adversary observes only expected k/n activation-gradient pairs per layer. The paper models collusion risk with an exponential decay prior (Pr[collusion size s] = Kγ^s), yielding a per-step risk bound proportional to k/n.
- Core assumption: Assumption A3 (Appendix B) asserts reconstruction probability is bounded when only partial gradients from b ≤ k experts are observed.
- Evidence anchors:
  - [abstract]: "demonstrating strong resistance to gradient-inversion attacks"
  - [section 4, Table 3]: Partial-gradient attack recovers only 2–8% of unigrams and virtually no bigrams across three datasets.
  - [corpus]: PM-MOE (arXiv:2502.00354) combines MoE with federated learning for personalization but does not analyze gradient-inversion resistance under sparse sharing.
- Break condition: If a novel attack exploits correlations across multiple sparse gradient observations over time, or if k/n is too large (e.g., few parties, high top-k), privacy guarantees may weaken.

### Mechanism 3: Sequential Local Updates Without Global Synchronization
- Claim: Round-robin training with purely local parameter updates can approximate centralized training performance
- Mechanism: Parties iterate over local batches in alternating order. After each forward/backward pass, only the data owner's backbone and the invoked experts' hosts update their parameters. No gradient aggregation or weight averaging occurs.
- Core assumption: Sequential exposure to diverse party datasets provides sufficient gradient signal for convergence, approximating pooled-data training.
- Evidence anchors:
  - [abstract]: "almost matches (and sometimes exceeds) the performance and convergence rate of a fully centralized model"
  - [section 3.2]: "At the end of each iteration, there is no global synchronization. All updates happen locally for Li and at the owning party for each expert Ej that was invoked."
  - [corpus]: Douillard et al. [5] (DiLoCo) uses local SGD for distributed LLM training but does not address privacy or memory constraints explicitly.
- Break condition: If party data distributions are severely non-IID, convergence may slow or accuracy may degrade relative to centralized baseline.

## Foundational Learning

- **Mixture-of-Experts (MoE) Routing**:
  - Why needed here: PC-MoE's core idea depends on top-k gating; without this, you cannot understand why only sparse signals are shared.
  - Quick check question: In an MoE layer with 64 experts and top-2 routing, how many experts process each token?

- **Gradient Inversion Attacks**:
  - Why needed here: Privacy claims are evaluated against reconstruction attacks; understanding what gradients leak clarifies why sparsity helps.
  - Quick check question: What does a gradient reveal about its corresponding training input?

- **Federated Learning vs. Distributed Training**:
  - Why needed here: PC-MoE differs from standard FL (no central aggregation) and from model parallelism (experts are owned, not temporarily placed).
  - Quick check question: In FedAvg, what does the central server compute from client gradients?

## Architecture Onboarding

- **Component map**: Local backbone Li -> gating Gi selects top-k experts -> sends activations to remote expert hosts Ej -> remote expert forward pass -> returns outputs to data owner -> compute loss

- **Critical path**:
  1. Initialize all parties from identical checkpoint (common random seed)
  2. Forward: Local backbone → gating selects top-k experts → send activations to each expert's host → host runs expert forward pass locally → return outputs to data owner → compute loss
  3. Backward: Send ∇h_out to each invoked expert's host → expert host backpropagates locally → data owner backprops through gating and backbone
  4. No global sync; proceed to next party's batch

- **Design tradeoffs**:
  - ↑ parties (n): Greater memory savings per party, but more remote expert calls and network latency
  - ↑ top-k: Better capacity utilization, but more gradient exposure and communication
  - Skip first/last expert layers (Table 4): May reduce overfitting slightly but generally harms accuracy and stability

- **Failure signatures**:
  - Convergence stalls: Verify all parties initialized identically; check round-robin scheduling fairness
  - Memory not reduced: Confirm expert shards are correctly partitioned (each party should hold m/n experts, not full set)
  - Privacy breach suspected: Audit whether full gradients are accidentally transmitted; verify k << n

- **First 3 experiments**:
  1. **Baseline validation**: Run PC-MoE vs. isolated vs. centralized on ARC-C. Report accuracy, convergence rounds, and peak GPU RAM per party.
  2. **Privacy stress test**: Apply partial-gradient attack (Li et al., 2024) assuming worst-case adversary sees all expert gradients. Compute ROUGE-1/2/L scores.
  3. **Scalability check**: Train with 2, 4, and 8 parties on OpenBookQA. Plot accuracy vs. number of parties to confirm scaling behavior.

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions in a dedicated section, but several areas are left unresolved in the main text, particularly regarding scalability, pre-training applicability, and robustness to active adversaries.

## Limitations
- **Reproducibility constraints**: Exact MoE architecture and training hyperparameters are not provided; reproduction depends on supplementary code that is not yet public.
- **Distribution and partition details**: Data partitioning across parties and per-party dataset sizes are not specified, making it difficult to assess real-world privacy or convergence under non-IID splits.
- **Attack simulation assumptions**: Privacy evaluations use a partial-gradient inversion attack with 1000 optimization steps, but the exact attack setup is not detailed, potentially affecting privacy claim validity.

## Confidence
- **High confidence** in the memory reduction mechanism and the 70% RAM savings claim, as this follows directly from the expert sharding logic and is supported by a clear derivation in the paper.
- **Medium confidence** in the convergence and accuracy claims, since the sequential round-robin update strategy is plausible for IID or mildly non-IID data, but exact performance depends on unspecified hyperparameters and data splits.
- **Medium confidence** in the privacy claims, because the sparse gradient exposure model is sound, but real-world privacy under adaptive or multi-round attacks is not fully explored.

## Next Checks
1. **Baseline reproduction**: Implement PC-MoE on a simple MoE LLM (e.g., 8 experts, top-2 routing) and run a single benchmark (e.g., ARC-C) comparing memory use and accuracy against isolated and centralized training. Verify the 70% memory saving and accuracy match.
2. **Privacy attack replication**: Replicate the partial-gradient attack on synthetic or public data, varying k and n to measure how gradient reconstruction quality changes with sparsity. Compute ROUGE scores for different exposure ratios.
3. **Non-IID convergence test**: Partition a benchmark dataset into highly imbalanced, non-IID subsets and run PC-MoE. Measure accuracy drop and convergence slowdown compared to IID splits to assess robustness to distribution skew.