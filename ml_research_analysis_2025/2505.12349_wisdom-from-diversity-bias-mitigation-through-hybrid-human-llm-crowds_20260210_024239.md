---
ver: rpa2
title: 'Wisdom from Diversity: Bias Mitigation Through Hybrid Human-LLM Crowds'
arxiv_id: '2505.12349'
source_url: https://arxiv.org/abs/2505.12349
tags:
- llms
- biases
- groups
- human
- bias
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study analyzes biases in large language models (LLMs) by\
  \ comparing their responses to bias-eliciting headlines with human responses. The\
  \ research demonstrates that while LLMs exhibit similar counterfactual biases as\
  \ humans\u2014particularly favoring positive outcomes for White individuals over\
  \ African Americans\u2014they are less susceptible to framing effects."
---

# Wisdom from Diversity: Bias Mitigation Through Hybrid Human-LLM Crowds

## Quick Facts
- arXiv ID: 2505.12349
- Source URL: https://arxiv.org/abs/2505.12349
- Authors: Axel Abels; Tom Lenaerts
- Reference count: 21
- One-line primary result: Hybrid human-LLM crowds using locally weighted aggregation (ExpertiseTrees) effectively reduce LLM bias while maintaining accuracy across demographic contexts.

## Executive Summary
This study analyzes biases in large language models (LLMs) by comparing their responses to bias-eliciting headlines with human responses. The research demonstrates that while LLMs exhibit similar counterfactual biases as humans—particularly favoring positive outcomes for White individuals over African Americans—they are less susceptible to framing effects. To mitigate these biases, the study explores crowd-based aggregation strategies. Simple averaging of LLM responses reinforces biases due to limited diversity, but locally weighted aggregation methods (ExpertiseTrees) effectively reduce biases and improve accuracy. Hybrid crowds combining humans and LLMs achieve the best performance, leveraging human diversity and LLM accuracy to minimize biases across ethnic and gender contexts.

## Method Summary
The study evaluates LLM responses to headlines containing demographic markers (age, gender, ethnicity) × sentiment (positive/negative) × status (genuine/altered). Eighteen LLMs plus human respondents are prompted with 4-shot examples using temperature=0 to rate headlines on 5-point Likert scales. Aggregation methods include simple averaging, category-weighted averaging, and ExpertiseTrees (context-partitioned weighted averages). The paper measures accuracy, counterfactual bias (Δg,g'(s,σ) = Eh~Hs,σ,g[ph] - Eh'~Hs,σ,g'[ph']), framing effects, and classifier diversity via Q-statistic.

## Key Results
- Simple averaging of LLM outputs reinforces biases due to high correlation (Q-statistic ~0.855) between LLM predictions
- Hybrid crowds containing both humans and LLMs significantly enhance performance and further reduce biases across ethnic and gender-related contexts
- ExpertiseTrees reduce counterfactual bias to non-significant levels while achieving 0.813 accuracy, outperforming both simple averaging and category-weighted methods

## Why This Works (Mechanism)

### Mechanism 1
Hybrid human-LLM crowds reduce bias more effectively than either group alone by combining complementary strengths—human diversity with LLM accuracy. Humans exhibit higher response diversity (lower Q-statistic correlation ~0.387) while LLMs show higher individual accuracy but high correlation (~0.855). When combined, ExpertiseTrees can route different inputs to the most reliable and least biased predictors, canceling errors that homogeneous groups would amplify. Core assumption: Diversity of errors is required for collective error cancellation; correlated errors cannot be averaged away. Evidence anchors: [abstract] "Hybrid crowds containing both significantly enhance performance and further reduce biases across ethnic and gender-related contexts." [Section 4.2] "The average Q-statistic within human ensembles is 0.387 ± 0.33, while that of LLM ensembles is 0.855 ± 0.08."

### Mechanism 2
Locally weighted aggregation (ExpertiseTrees) mitigates bias better than simple averaging by assigning context-specific weights to predictors based on their demonstrated reliability per category. ExpertiseTrees partition the input space (e.g., headline categories) and fit separate linear aggregation models per partition. This allows down-weighting biased predictors for specific contexts (e.g., ethnicity headlines) while up-weighting them where they perform well. Core assumption: Individual bias patterns vary systematically across contexts, enabling context-aware reweighting. Evidence anchors: [Section 3.3] "ExpertiseTrees exploit this specialization by learning a distinct set of weights for each category, allowing for context-sensitive aggregation." [Table 1] ExpertiseTree(hybrid+) reduces counterfactual bias to non-significant levels while achieving 0.813 accuracy.

### Mechanism 3
Simple averaging of LLM outputs reinforces—rather than reduces—bias because LLMs lack response diversity and make correlated errors. Averaging only cancels uncorrelated errors. High Q-statistic (~0.855) among LLMs means when one model is wrong, others likely make the same mistake. Averaging amplifies shared biases toward consensus on incorrect answers. Core assumption: Aggregation benefit requires error independence or negative correlation. Evidence anchors: [abstract] "Simple averaging of LLM outputs reinforces biases due to limited diversity within LLM crowds." [Section 4.2] "This implies that when one LLM makes a mistake, others are likely to make the same mistake."

## Foundational Learning

- **Concept**: Wisdom of the crowd / Condorcet's Jury Theorem
  - Why needed here: Explains why diversity matters for aggregation—group accuracy increases with size only if members are independent and better than random.
  - Quick check question: Would adding more identical predictors improve ensemble performance?

- **Concept**: Counterfactual bias (conditional statistical parity)
  - Why needed here: The paper measures bias as systematic differences in likelihood assigned to headlines after demographic group swapping.
  - Quick check question: If a model rates "White men succeed" as more believable than the identical claim with "Black women," what type of bias is this?

- **Concept**: Q-statistic for classifier diversity
  - Why needed here: Quantifies prediction correlation between classifiers; high Q means similar error patterns, low Q means diverse errors amenable to aggregation.
  - Quick check question: Two classifiers with Q=0.95 both fail on the same 30% of inputs. Will averaging help?

## Architecture Onboarding

- **Component map**: Headlines → 18 LLMs + human respondents → (1) Simple average, (2) Static weighted average (stacking), (3) ExpertiseTree (context-partitioned weighted averages) → Aggregated likelihood score + bias metrics

- **Critical path**:
  1. Collect predictions from all crowd members on each headline
  2. Partition headlines by category (age/ethnicity/gender) for ExpertiseTree
  3. Learn context-specific weights via cross-validation
  4. Aggregate using weighted combination; compute accuracy and counterfactual bias

- **Design tradeoffs**:
  - LLM-only crowds: Higher individual accuracy but low diversity → limited aggregation gains, bias persistence
  - Human-only crowds: Higher diversity but lower accuracy → steady gains with size but lower ceiling
  - Hybrid crowds: Best of both but requires human annotation infrastructure
  - Benchmark-based LLM selection (LLM+): Higher average accuracy but further reduced diversity

- **Failure signatures**:
  - Simple average shows no accuracy gain beyond group size 4 for LLMs
  - Counterfactual bias remains significant even in aggregated LLM outputs
  - Adding more high-performing LLMs decreases performance (redundancy without diversity)

- **First 3 experiments**:
  1. Replicate headline experiment on your LLM set; compute per-category accuracy and counterfactual bias using Equation 1.
  2. Compute pairwise Q-statistics between all predictors; visualize clustering to assess diversity.
  3. Implement ExpertiseTree aggregation; compare simple average vs. weighted vs. locally-weighted on held-out headlines, measuring both accuracy and bias reduction.

## Open Questions the Paper Calls Out

### Open Question 1
To what extent do these bias mitigation findings generalize to datasets with different cultural or demographic contexts outside of the specific headline experiment? Basis in paper: [explicit] The authors state in the Limitations section that the analysis was conducted using a single dataset and that "results may differ for datasets that have different cultural or demographic contexts." Why unresolved: The study focused on a controlled environment (the headline dataset) to systematically compare humans and LLMs, leaving the cross-cultural validity of the hybrid aggregation methods untested. What evidence would resolve it: Replicating the hybrid crowd and ExpertiseTree experiments on datasets containing non-Western demographics or different social groups to verify if the complementary strengths of human diversity and LLM accuracy persist.

### Open Question 2
Can diversity engineering techniques, such as persona-based prompting or fine-tuning, successfully induce the necessary variance in LLM-only crowds to mitigate bias without human input? Basis in paper: [explicit] The Limitations section notes the study "did not incorporate techniques to engineer diversity within LLMs, such as fine-tuning or prompting models to adopt varied personas," suggesting this as a specific gap. Why unresolved: The paper demonstrates that LLM crowds lack natural diversity (high Q-statistic), but it relies on human inclusion to solve this. It is unknown if synthetic diversity can mimic the "cognitive" diversity humans provide. What evidence would resolve it: Experiments applying persona prompts or varied few-shot examples to a single LLM to generate a synthetic crowd, followed by the application of ExpertiseTrees to measure bias reduction.

### Open Question 3
How robust are the observed bias mitigation effects of hybrid crowds as underlying LLMs are updated or retrained over time? Basis in paper: [explicit] The Limitations section highlights that "observed biases and performance reflect the specific versions of LLMs tested," noting that future versions may exhibit different behaviors. Why unresolved: LLMs are "moving targets" that change frequently; a hybrid aggregation strategy optimized for current model biases (e.g., GPT-4o) may lose effectiveness if future models alter their specific bias profiles. What evidence would resolve it: A longitudinal study evaluating the performance of fixed ExpertiseTree configurations across successive versions of the same model families (e.g., GPT-4 to GPT-4.5/5).

## Limitations
- The analysis was conducted using a single dataset; results may differ for datasets with different cultural or demographic contexts.
- The study did not incorporate techniques to engineer diversity within LLMs, such as fine-tuning or prompting models to adopt varied personas.
- Observed biases and performance reflect the specific versions of LLMs tested; future versions may exhibit different behaviors.

## Confidence

- **High confidence**: Simple averaging reinforces bias in LLM-only crowds due to high correlation (Q ~0.855) - directly supported by Q-statistic data and theoretical reasoning.
- **Medium confidence**: Hybrid crowds combining humans and LLMs achieve the best performance by leveraging complementary strengths - supported by results but depends on specific human-LLM combinations tested.
- **Medium confidence**: ExpertiseTrees effectively reduce biases through context-specific weighting - supported by results but algorithm details require external verification.

## Next Checks

1. **Validate diversity effect**: Compute Q-statistics on your own LLM set and test whether simple averaging improves or degrades performance based on diversity levels.
2. **Replicate counterfactual bias calculation**: Apply the paper's Equation 1 to measure Δg,g'(s,σ) for your headline set to confirm bias patterns.
3. **Test aggregation sensitivity**: Compare simple average, weighted average, and ExpertiseTree methods on held-out data, measuring both accuracy gains and bias reduction to verify the claimed advantages.