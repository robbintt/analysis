---
ver: rpa2
title: 'MapFormer: Self-Supervised Learning of Cognitive Maps with Input-Dependent
  Positional Embeddings'
arxiv_id: '2511.19279'
source_url: https://arxiv.org/abs/2511.19279
tags:
- cognitive
- maps
- matrices
- learning
- which
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "MapFormers are a new class of transformer architectures designed\
  \ to learn cognitive maps\u2014internal models that capture abstract relationships\
  \ among entities\u2014by disentangling structural relationships from their specific\
  \ content. This is achieved through input-dependent positional embeddings, allowing\
  \ the model to update its internal position based on actions while leaving the semantic\
  \ content unchanged by observations."
---

# MapFormer: Self-Supervised Learning of Cognitive Maps with Input-Dependent Positional Embeddings

## Quick Facts
- arXiv ID: 2511.19279
- Source URL: https://arxiv.org/abs/2511.19279
- Authors: Victor Rambaud; Salvador Mascarenhas; Yair Lakretz
- Reference count: 40
- Key outcome: MapFormers achieve near-perfect OOD generalization on navigation and selective copy tasks, outperforming baseline models including RoPE, CoPE, and TAPE.

## Executive Summary
MapFormers introduce a new class of transformer architectures that learn cognitive maps—internal models capturing abstract relationships among entities—by disentangling structural relationships from content. This is achieved through input-dependent positional embeddings that update based on actions while preserving semantic content. Two variants, MapFormer-EM (Episodic Memory) and MapFormer-WM (Working Memory), unify absolute and relative positional encoding to model distinct memory systems. The architecture demonstrates superior OOD generalization on tasks requiring structural reasoning, with insights into how actions are represented as matrices and observations as vectors.

## Method Summary
MapFormers extend transformer positional encoding by making rotation angles input-dependent rather than fixed. Input tokens are projected to rotation durations Δt via a learned low-rank projection, then integrated along the sequence using cumsum to compute cumulative rotation angles. These angles parameterize block-diagonal rotation matrices applied to positional embeddings. MapEM uses absolute positional encoding with separate content and position attention matrices, while MapWM applies input-dependent rotations directly to queries and keys for implicit relative positioning. The architecture leverages Lie group theory to enable parallel path integration while preserving the structure of the underlying transformation group.

## Key Results
- MapFormers achieve near-perfect OOD generalization on selective copy tasks, maintaining >0.9 accuracy across sequence lengths 64-512 while baselines degrade significantly
- On 2D grid navigation, MapEM-os reaches 0.87 accuracy on OOD-sparse data vs 0.35 for MapWM, demonstrating explicit structural representation advantages
- The model successfully learns to represent actions as matrices and observations as vectors, with learned action representations showing expected geometric properties (opposite actions have cos(Δt) ≈ -1)

## Why This Works (Mechanism)

### Mechanism 1: Structure-Content Disentanglement via Input-Dependent Rotations
Separating "where" (structure) from "what" (content) enables OOD generalization by allowing the model to apply learned structure to novel observations. Input tokens are mapped to rotation durations Δt via a learned projection, with actions producing non-zero rotations updating position while observations produce near-zero rotations, leaving position unchanged. This mechanism relies on the underlying structure being representable as a compact, commutative Lie group.

### Mechanism 2: Parallel Path Integration via Lie Algebra Computation
Computing path integration in the Lie algebra (addition) rather than Lie group (multiplication) enables parallel training while preserving group structure. Instead of sequentially multiplying matrices, the model computes integrated angles via cumsum and applies the exponential map once. This requires the Lie group to be abelian, allowing exp(ΣA_i) = ∏exp(A_i).

### Mechanism 3: Dual Memory Systems via Absolute vs. Relative Positional Encoding
Episodic memory (EM) and working memory (WM) systems are unified as absolute vs. relative positional encoding strategies. MapEM computes separate attention matrices for content and position, combined via element-wise product, while MapWM directly rotates queries and keys to encode relative positions implicitly. EM scales better on recall tasks but doubles attention computation.

## Foundational Learning

- **Concept: Lie Groups and Lie Algebras**
  - Why needed here: The entire MapFormer mechanism relies on representing actions as elements of a Lie group and computing path integration in the corresponding Lie algebra via linear operations
  - Quick check question: Given three actions with rotation angles θ₁, θ₂, θ₃, write the final integrated position using both sequential group multiplication and parallel Lie algebra approaches

- **Concept: Transformer Positional Encoding (RoPE, Absolute PE)**
  - Why needed here: MapFormers extend RoPE from fixed rotations to input-dependent rotations, building on how RoPE encodes relative position via Q/K rotations
  - Quick check question: In RoPE, how does rotating Q by angle θ_i and K by angle θ_j encode their relative distance? What happens in MapWM when θ is input-dependent?

- **Concept: Path Integration in Neuroscience**
  - Why needed here: The paper draws direct inspiration from biological cognitive maps and grid cells, understanding how animals track position by integrating self-motion
  - Quick check question: If an agent takes actions [right, up, left, down] on a 2D grid starting at origin, what is its final position? How does cumsum(Δt) where Δt encodes each action's displacement compute this?

## Architecture Onboarding

- **Component map:**
  Input X → Projection W_Δ → Integration durations Δt → Cumsum along time → θ_{PI} = cumsum(ω·Δt) → Exponential map → R_{θ_{PI}} (block-diagonal rotations)

- **Critical path:** The low-rank projection W_Δ determines how input tokens map to rotation durations, where action/observation differentiation emerges self-supervised. Monitor the norm of Δt for action vs. observation tokens during training (should diverge).

- **Design tradeoffs:**
  | Choice | Benefit | Cost |
  |--------|---------|------|
  | MapEM vs MapWM | EM scales better on recall, more interpretable | EM doubles attention computation (two attention matrices) |
  | Block size b=2 vs b>2 | b=2 uses explicit rotation formula, fastest | Larger blocks needed for non-commutative groups (slower, no parallel path integration) |
  | Skew-symmetric S vs learned S | Theoretically grounded, stable inverses | Less expressive; cannot represent non-rotation transformations |

- **Failure signatures:**
  - Observation tokens have non-zero Δt norm: Model failed to learn action/observation distinction
  - Opposite actions have cos(Δt) ≈ 0 instead of -1: Lie group structure not learned
  - OOD performance degrades on longer sequences: For commutative groups, should not happen
  - MapWM outperforms MapEM: Unusual; suggests content noise is dominating structure signal

- **First 3 experiments:**
  1. Sanity check on 1D selective copy: Train MapWM on sequences like "ABBCB → ABC" with blanks to ignore, verify learned Δt distinguishes blank tokens from content tokens
  2. 2D navigation with interpretability probing: Train MapEM-os on 4-direction grid navigation, extract Δt for each action token and compute cosine similarities, verify opposite actions have cos ≈ -1
  3. OOD length generalization stress test: Train on sequence length 128, grid 64, test on lengths [64, 128, 256, 512] with varying sparsity, MapFormers should maintain >0.9 accuracy

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do MapFormer-EM models maintain their superiority over MapFormer-WM models in complex reasoning tasks?
- Basis in paper: Section 7 states results confirm EM superiority only on recall tasks, and "further experiments should compare these two models on more complex tasks such as reasoning"
- Why unresolved: The paper only evaluates on navigation and selective copy tasks, unknown if EM scales better than WM for abstract reasoning
- What evidence would resolve it: Comparative evaluation on benchmarks for logical or relational reasoning

### Open Question 2
- Question: Can the MapFormer formalism be adapted for non-causal problems such as visual processing?
- Basis in paper: Section 7 notes the current method sums rotation angles along the temporal dimension, meaning "our formalism only applies to causal transformers"
- Why unresolved: The reliance on cumsum for path integration restricts the model to causal sequences, preventing application to bidirectional tasks like image understanding
- What evidence would resolve it: Successful adaptation of the path integration mechanism to bidirectional contexts and subsequent performance benchmarks on vision tasks

### Open Question 3
- Question: How does the structural bias of MapFormers impact performance when scaled to larger model sizes and datasets?
- Basis in paper: Section 7 explicitly lists as a limitation: "we did not try to scale our method to larger model sizes and larger datasets"
- Why unresolved: Experiments are on relatively small, synthetic tasks; unclear if structural biases hinder or help convergence in large-scale, real-world data regimes
- What evidence would resolve it: Training results on large-scale benchmarks comparable to standard Transformer baselines

### Open Question 4
- Question: How can the computation of rotation angles be improved for non-commutative group structures?
- Basis in paper: Appendix B.2.2 states that "Further work should investigate how to improve the computation of rotation angles... that clearly has an impact on final performances" for non-commutative Lie groups
- Why unresolved: The paper demonstrates that learning non-commutative structures is difficult; a linear mapping fails to capture the curvature of the manifold
- What evidence would resolve it: Development of a specialized mapping function maintaining high accuracy on non-commutative tasks without performance degradation

## Limitations
- The mechanism fundamentally relies on commutative Lie groups (SO(2) rotations), severely limiting application to real-world navigation involving 3D rotations or non-commutative transformations
- Empirical validation focuses heavily on synthetic navigation and selective copy tasks, with limited testing on real-world datasets or more complex cognitive mapping scenarios
- The computational overhead of maintaining separate attention matrices in MapEM (doubling attention computation) may offset benefits for large-scale applications

## Confidence

- **High confidence:** The core mathematical framework (Lie group path integration, input-dependent rotations) is well-established and correctly implemented. The empirical results on synthetic tasks are internally consistent and demonstrate claimed OOD generalization advantages over baselines.
- **Medium confidence:** The biological plausibility claims connecting to grid cells and cognitive mapping mechanisms are somewhat speculative and not extensively validated against neural data. The claims about EM vs WM memory system tradeoffs are supported by ablation studies but could benefit from more extensive analysis.
- **Low confidence:** The generalizability of the approach to real-world scenarios with complex, non-commutative structures or unknown underlying geometry. The paper provides theoretical frameworks for extensions but lacks empirical validation of these scenarios.

## Next Checks

1. Test MapFormer on non-commutative navigation tasks (e.g., 3D rotations or maze navigation with non-abelian transformations) to validate the MapEM-NC variant's performance and computational tradeoffs compared to the parallel version.

2. Evaluate the model on real-world sequential data (e.g., robot navigation logs, video game playthroughs) where the underlying structure is unknown, to assess whether self-supervised learning of cognitive maps transfers beyond synthetic domains.

3. Conduct systematic ablation studies varying the Lie group structure (SO(2) vs SO(3) vs other groups) and dimensionality to quantify performance degradation and computational overhead as the problem moves away from the ideal commutative case.