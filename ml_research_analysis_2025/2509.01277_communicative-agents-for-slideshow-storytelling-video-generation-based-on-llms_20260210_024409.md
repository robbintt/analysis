---
ver: rpa2
title: Communicative Agents for Slideshow Storytelling Video Generation based on LLMs
arxiv_id: '2509.01277'
source_url: https://arxiv.org/abs/2509.01277
tags:
- video
- generation
- vgteam
- arxiv
- system
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study introduces VGTeam, a multi-agent system for slideshow
  storytelling video generation that leverages large language models and API-based
  workflows to reduce computational costs. The system uses specialized agents (director,
  editor, painter, composer) operating in a structured Chat Tower architecture to
  convert user text prompts into complete videos.
---

# Communicative Agents for Slideshow Storytelling Video Generation based on LLMs

## Quick Facts
- arXiv ID: 2509.01277
- Source URL: https://arxiv.org/abs/2509.01277
- Reference count: 29
- Primary result: 98.4% success rate with $0.103 average cost per video using API-driven multi-agent system

## Executive Summary
VGTeam is a multi-agent system that transforms text prompts into slideshow storytelling videos using a structured Chat Tower architecture. The system employs specialized agents (director, editor, painter, composer) that communicate sequentially through a memory stream to maintain narrative coherence. By leveraging external APIs for media generation rather than model training, VGTeam achieves cost efficiency while demonstrating strong performance across varied prompts and language models.

## Method Summary
The system uses a Chat Tower architecture where a director agent receives user prompts and coordinates specialized agents in a waterfall-style workflow. The director decomposes tasks and passes directives to an editor (script generation), painter (image generation via API), and composer (audio generation via API). A memory stream persists context across agent interactions, while iterative approval ensures quality standards. The system assembles final videos using MoviePy, achieving efficiency through API-based generation rather than model training.

## Key Results
- 98.4% success rate across 300 video generations with varied prompts and language models
- Average cost of $0.103 per video using external APIs instead of model training
- Short prompts (1-5 words) showed instability with all system failures occurring in this category

## Why This Works (Mechanism)

### Mechanism 1: Structured Sequential Agent Communication (Chat Tower)
Sequential top-down agent communication reduces ambiguity and execution errors compared to free-form multi-agent dialogue. The Chat Tower constrains information flow—user interacts only with the director agent, who cascades structured directives to downstream agents. This waterfall-style architecture limits hallucination propagation by ensuring each agent receives task-specific, filtered instructions rather than raw user input.

### Mechanism 2: Memory Stream for Cross-Agent Context Persistence
Maintaining a persistent record of prior agent outputs improves narrative coherence across the pipeline. The memory stream stores director instructions, editor decisions, and generated content, allowing each agent to reference earlier outputs. This mitigates LLM "forgetting" and ensures the painter and composer align their outputs with established narrative tone and pacing.

### Mechanism 3: API-Driven Component Generation with Cost-Quality Tradeoff
Delegating media generation to external APIs reduces computational costs while introducing dependency on third-party service quality. Instead of training or hosting diffusion models, the system calls ERNIE-iRAG for images, TEXT2AUDIO for voice, and TEXT-to-MUSIC API for audio. The LLM agents generate text prompts for these APIs rather than producing media directly.

## Foundational Learning

- **Concept: Multi-Agent Role Specialization via Prompt Engineering**
  - Why needed here: The system transforms general-purpose LLMs into role-specific agents through carefully designed system prompts that define task objectives, input/output requirements, and performance standards.
  - Quick check question: Can you articulate how a single system prompt changes an LLM's behavior from general text generation to role-constrained task execution?

- **Concept: Sequential vs. Parallel Agent Coordination**
  - Why needed here: The Chat Tower uses waterfall-style sequential coordination rather than parallel or debate-based agent interaction, which affects latency, error propagation, and output coherence.
  - Quick check question: What are the latency vs. robustness tradeoffs between sequential agent pipelines and parallel multi-agent debate systems?

- **Concept: API Abstraction Layers for Multimodal Generation**
  - Why needed here: The system treats text-to-image, text-to-audio, and text-to-music APIs as black-box components, abstracting away model details while accepting their output variability.
  - Quick check question: How would you design fallback mechanisms if an external API returns inconsistent or poor-quality outputs?

## Architecture Onboarding

- **Component map:**
  User Interface -> Chat Tower (Director -> Editor -> Painter/Composer) -> Memory Stream -> External API Layer -> Video Assembly Layer

- **Critical path:**
  1. User prompt → Director (role definition, task decomposition)
  2. Director → Editor (script generation, stored in memory stream)
  3. Editor output → Painter & Composer (parallel API calls for visuals/audio)
  4. Director approval loop (iterative refinement if quality standards not met)
  5. All components → Video assembly (MoviePy post-processing)

- **Design tradeoffs:**
  - **Cost vs. Quality**: API-based generation reduces compute costs ($0.103/video) but introduces third-party dependency and quality variability (22.7% inappropriate content rate)
  - **Prompt Length vs. Stability**: Long prompts (>10 words) provide more context but increase execution time (avg 304.42s vs 224.3s for short); short prompts (1-5 words) are more stable but all system failures occurred with short prompts
  - **LLM Selection**: DeepSeek-V3 and ERNIE 4.5-Turbo show consistent behavior; Qwen3-235B produces shorter outputs but has wider execution time distribution

- **Failure signatures:**
  - **Network instability**: API call failures beyond system control
  - **Character confusion**: Agent misinterprets task scope (e.g., claims inability to process non-textual elements); mitigated by prompt optimization
  - **Infinite loops**: Director fails to approve output, triggering repeated attempts; mitigated by loop-limiting strategies
  - **Inappropriate content**: Repetitive visuals or semantically incoherent image prompts from painter agent; mitigated by stricter painter constraints
  - **All failures occurred with short prompts**: Insufficient context leads to task ambiguity

- **First 3 experiments:**
  1. **Baseline pipeline test**: Run 10 video generations with long prompts (>10 words) across diverse topics (vehicle, concert, animal, food) to establish success rate and average cost. Measure token length, loop count, and communication time for each run.
  2. **Prompt length sensitivity analysis**: Generate 10 videos each with short prompts (1-5 words) vs. long prompts (>10 words) using the same LLM backend. Compare failure rates, inappropriate content rates, and execution time distributions.
  3. **LLM comparison**: Run identical 20-prompt set across DeepSeek-V3, ERNIE 4.5-Turbo, and Qwen3-235B. Profile token usage, loop frequency, and output quality to determine optimal model selection for different use cases (verbose narratives vs. concise content).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the integration of dynamic generation methods, such as keyframe animation or motion capture, improve the visual narrative quality of LLM-based slideshow videos?
- Basis in paper: The Discussion section states that the current reliance on static imagery "constrains the expressive capacity" and suggests future work explore "sophisticated visual technologies" like 3D modeling.
- Why unresolved: The current VGTeam implementation is limited to static images with basic animations, lacking the technical integration for generating dynamic video content.
- What evidence would resolve it: Comparative user studies evaluating narrative engagement and visual quality between the current static-output system and a modified version incorporating dynamic video generation APIs.

### Open Question 2
- Question: To what extent can deterministic control mechanisms or fine-tuned prompt strategies mitigate output inconsistencies in multi-agent video generation?
- Basis in paper: The authors note that the stochastic nature of LLMs introduces unpredictability and suggest future iterations could incorporate "deterministic control mechanisms" to enhance reliability.
- Why unresolved: The current system relies on standard LLM outputs, which inherently vary, making consistent video production difficult for similar inputs.
- What evidence would resolve it: Ablation studies measuring output variance across multiple identical runs, comparing standard prompts against those using deterministic decoding or refined prompt engineering.

### Open Question 3
- Question: What specific architectural modifications are required to stabilize the system when processing short or low-context user prompts?
- Basis in paper: The experiments revealed that all system failures (1.7%) and a significant portion of inappropriate content (22.7%) occurred during trials with short prompts, indicating architectural sensitivity to input length.
- Why unresolved: The "Chat Tower" appears to lack robust mechanisms for handling ambiguity, leading to character confusion or infinite loops when the director agent receives insufficient context.
- What evidence would resolve it: Experiments testing a context-expansion agent or a pre-processing module designed specifically to enrich short prompts before they enter the agent workflow.

## Limitations
- **Unvalidated API dependency**: Reliance on external APIs (ERNIE-iRAG, Baidu TEXT2AUDIO, Manolis Teletos) introduces significant variability not controlled by the authors
- **Memory stream scalability concerns**: Paper doesn't address memory stream growth management or token limit impacts on longer conversations
- **Generalizability constraints**: Performance with diverse prompt types, longer videos, or different cultural contexts remains untested beyond 5 curated categories

## Confidence
- **High Confidence**: API-driven cost efficiency ($0.103/video average) and success rate (98.4%) - directly measured across 300 runs with clear methodology
- **Medium Confidence**: Memory stream effectiveness for narrative coherence - supported by system design but lacking ablation studies
- **Low Confidence**: Prompt length effects on execution time - correlation established but causation and optimal prompt length not rigorously tested

## Next Checks
1. **API stability stress test**: Run 100+ video generations over 48 hours to measure API availability, response consistency, and quality fluctuations. Track whether failure rates correlate with specific providers or time windows.
2. **Memory stream capacity experiment**: Generate videos requiring 50+ agent interactions to test memory stream performance. Measure output quality degradation as conversation history approaches LLM token limits.
3. **Cross-domain generalization**: Test system with prompts outside the 5 categories (Vehicle, Concert, Association Football, Animal, Food). Include abstract concepts, technical documentation, and multilingual prompts to assess true versatility beyond the curated dataset.