---
ver: rpa2
title: What Changed? Detecting and Evaluating Instruction-Guided Image Edits with
  Multimodal Large Language Models
arxiv_id: '2505.20405'
source_url: https://arxiv.org/abs/2505.20405
tags:
- image
- prompt
- editing
- detected
- differences
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces DICE, a model designed to evaluate instruction-guided
  image editing by detecting and assessing object-level differences between original
  and edited images. The method employs an autoregressive Multimodal Large Language
  Model (MLLM) in two stages: difference detection to localize edits and coherence
  estimation to judge alignment with the user prompt.'
---

# What Changed? Detecting and Evaluating Instruction-Guided Image Edits with Multimodal Large Language Models

## Quick Facts
- arXiv ID: 2505.20405
- Source URL: https://arxiv.org/abs/2505.20405
- Reference count: 40
- This paper introduces DICE, a model designed to evaluate instruction-guided image editing by detecting and assessing object-level differences between original and edited images.

## Executive Summary
This paper introduces DICE, a model designed to evaluate instruction-guided image editing by detecting and assessing object-level differences between original and edited images. The method employs an autoregressive Multimodal Large Language Model (MLLM) in two stages: difference detection to localize edits and coherence estimation to judge alignment with the user prompt. DICE is trained using self-supervision, distillation, and full supervision, and is tested on various MLLMs, with Idefics3-8B performing best. On the DICE-D dataset, it achieves high accuracy in detecting and classifying edits (AP up to 22.3) and coherence estimation (85% accuracy). When integrated into CLIP-based metrics, DICE improves correlation with human judgment, notably raising CLIP-I from 51.1 to 54.5 and CLIP-T from 21.5 to 24.6 in Pearson’s correlation. User studies show DICE rankings align with human preferences, validating its effectiveness and explainability.

## Method Summary
DICE employs a two-stage autoregressive Multimodal Large Language Model (MLLM) approach for instruction-guided image editing evaluation. The first stage, difference detection, localizes and classifies edits between original and edited images using an MLLM. The second stage, coherence estimation, evaluates whether the edits align with the user's instruction. The model is trained using self-supervision, distillation, and full supervision techniques, with Idefics3-8B emerging as the best-performing MLLM. DICE is evaluated on the DICE-D dataset, demonstrating high accuracy in edit detection (AP up to 22.3) and coherence estimation (85% accuracy). Integration into CLIP-based metrics improves correlation with human judgment, and user studies confirm alignment with human preferences and model explainability.

## Key Results
- DICE achieves high accuracy in edit detection (AP up to 22.3) and coherence estimation (85% accuracy) on the DICE-D dataset.
- Integration of DICE into CLIP-based metrics improves correlation with human judgment, notably raising CLIP-I from 51.1 to 54.5 and CLIP-T from 21.5 to 24.6 in Pearson’s correlation.
- User studies show DICE rankings align with human preferences, validating its effectiveness and explainability.

## Why This Works (Mechanism)
DICE works by leveraging an autoregressive Multimodal Large Language Model (MLLM) in a two-stage process. In the first stage, the MLLM detects and localizes edits between original and edited images, classifying them based on their nature. The second stage uses the same MLLM to estimate the coherence of these edits with respect to the user's instruction. This dual approach allows DICE to provide both detailed edit analysis and overall coherence evaluation. The model is trained using a combination of self-supervision, distillation, and full supervision, which enables it to generalize well across different editing scenarios and instructions. The use of MLLMs allows DICE to understand complex visual and textual relationships, making it effective for instruction-guided image editing tasks.

## Foundational Learning
- Multimodal Large Language Models (MLLMs): These models combine visual and textual understanding, enabling them to process and generate responses based on both image and text inputs. They are crucial for tasks like image editing evaluation where both visual changes and textual instructions must be considered.
- Autoregressive Models: These models generate outputs one element at a time, conditioning each new element on previous ones. In DICE, this allows for sequential processing of edits and coherence estimation.
- Self-Supervision: A training method where the model learns from the data itself without explicit labels. This is useful for DICE as it can leverage large amounts of image pairs without requiring manual annotations.
- Distillation: A technique where a smaller model is trained to mimic the behavior of a larger, more complex model. This helps in making DICE more efficient while retaining performance.
- Full Supervision: Traditional supervised learning where the model is trained on labeled data. This provides DICE with explicit examples of correct edit detection and coherence estimation.

## Architecture Onboarding

Component Map:
DICE (MLLM) -> Difference Detection -> Coherence Estimation -> CLIP-based Metrics

Critical Path:
The critical path involves the MLLM processing the original and edited images, detecting differences, and then evaluating coherence with the instruction. This path directly impacts the model's ability to accurately assess instruction-guided edits.

Design Tradeoffs:
The use of an autoregressive MLLM allows for detailed, sequential processing of edits but may increase computational complexity. The choice of training methods (self-supervision, distillation, full supervision) balances the need for generalization with the availability of labeled data. The integration with CLIP-based metrics leverages existing frameworks but may limit flexibility in evaluation metrics.

Failure Signatures:
Potential failures could arise from the MLLM's inability to accurately detect subtle edits or misunderstand complex instructions. Over-reliance on self-supervision might lead to biases in the training data, affecting generalization. The coherence estimation might struggle with ambiguous or contradictory instructions.

First Experiments:
1. Evaluate DICE's performance on a small, manually curated dataset of image pairs with known edits to verify basic functionality.
2. Test the model's ability to handle edge cases, such as minimal edits or highly complex instructions, to assess robustness.
3. Compare DICE's performance with a baseline model that uses only CLIP embeddings without the autoregressive MLLM to quantify the added value of the MLLM approach.

## Open Questions the Paper Calls Out
None

## Limitations
- The DICE-D dataset is introduced by the authors, and while it appears comprehensive, the lack of independent validation limits confidence in the generalizability of results.
- The study primarily focuses on Idefics3-8B as the best-performing MLLM, but comparisons across different model architectures could be expanded.
- The evaluation metrics, while improved, still show moderate Pearson correlation values, suggesting room for further refinement.

## Confidence
- High confidence in the model's architecture and two-stage approach (difference detection and coherence estimation)
- Medium confidence in the dataset quality and benchmark results, pending independent validation
- Medium confidence in the improved CLIP-based metric correlations, given the moderate Pearson values
- Low confidence in the generalizability of user study results due to limited participant details

## Next Checks
1. Independent validation of the DICE-D dataset and benchmark results by external researchers
2. Testing the DICE model on additional MLLM architectures beyond Idefics3-8B to assess generalizability
3. Conducting user studies with larger, more diverse participant pools to confirm explainability and preference alignment findings