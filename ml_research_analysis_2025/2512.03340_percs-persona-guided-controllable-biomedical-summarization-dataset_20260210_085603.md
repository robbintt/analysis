---
ver: rpa2
title: 'PERCS: Persona-Guided Controllable Biomedical Summarization Dataset'
arxiv_id: '2512.03340'
source_url: https://arxiv.org/abs/2512.03340
tags:
- summaries
- summary
- persona
- biomedical
- abstract
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces PERCS, a dataset of 500 biomedical abstracts
  with 2,000 expert-annotated persona-specific summaries tailored for four audiences:
  laypersons, pre-medical students, non-medical researchers, and medical experts.
  The dataset was created by generating initial summaries with GPT-4, then having
  medical experts review and correct them for factual accuracy and persona alignment
  using a detailed error taxonomy.'
---

# PERCS: Persona-Guided Controllable Biomedical Summarization Dataset

## Quick Facts
- arXiv ID: 2512.03340
- Source URL: https://arxiv.org/abs/2512.03340
- Reference count: 40
- This paper introduces PERCS, a dataset of 500 biomedical abstracts with 2,000 expert-annotated persona-specific summaries tailored for four audiences.

## Executive Summary
This paper introduces PERCS, a dataset of 500 biomedical abstracts with 2,000 expert-annotated persona-specific summaries tailored for four audiences: laypersons, pre-medical students, non-medical researchers, and medical experts. The dataset was created by generating initial summaries with GPT-4, then having medical experts review and correct them for factual accuracy and persona alignment using a detailed error taxonomy. Evaluation showed high inter-rater agreement (Krippendorff's alpha 0.79–1.0) and consistently high quality across personas. Technical analysis revealed systematic differences in readability, vocabulary, and content depth by persona. Benchmarking four LLMs showed that few-shot prompting outperformed zero-shot and self-refine approaches, with GPT-4o and LLaMA-3.1 providing the most stable performance across personas.

## Method Summary
The PERCS dataset contains 500 PubMed abstracts (250 from PLOS Medicine, 250 from PLABA) with four persona-specific summaries each (2,000 total). Initial summaries were generated using GPT-4 with persona-specific prompts, then reviewed by two medical experts for factual accuracy and persona alignment using an 11-category error taxonomy. The dataset is split 350/150 for training/testing. Four LLMs (GPT-4o, Mistral-8-7B-Instruct, Gemini-2.0 Flash Lite, LLaMA-3 70B) were benchmarked using zero-shot, few-shot (3 exemplars), and self-refine prompting strategies. Evaluation combined automatic metrics (ROUGE, SARI, readability indices, SummaC) with human assessment of comprehensiveness, layness, factuality, and usefulness on 5-point Likert scales.

## Key Results
- Systematic readability differences across personas: Layperson DCRS=8.17, CLI=10.48 vs. Expert DCRS=11.76, CLI=17.18; word counts decrease from 270.91 (Lay) to 166.43 (Expert)
- High human evaluation scores post-correction: Faithfulness 4.34–5.0, Comprehensiveness 4.47–4.94 across personas
- Few-shot prompting consistently outperformed zero-shot and self-refine approaches across all LLMs and personas
- GPT-4o and LLaMA-3.1 showed the most stable performance across personas in benchmark tests

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Persona-specific prompting guides LLMs to systematically adjust lexical complexity, sentence structure, and information depth based on target audience.
- Mechanism: Prompts encode explicit instructions about audience background knowledge and information needs (e.g., "avoid jargon" for laypersons vs. "retain domain-specific terminology" for experts). This conditions the model's generation toward persona-appropriate output without fine-tuning.
- Core assumption: LLMs possess sufficient latent knowledge about audience-appropriate communication patterns to follow persona instructions accurately.
- Evidence anchors:
  - [abstract]: "Technical validation shows clear differences in readability, vocabulary, and content depth across personas."
  - [section]: Table 4 shows systematic variation—Layperson DCRS=8.17, CLI=10.48 vs. Expert DCRS=11.76, CLI=17.18; word counts decrease from 270.91 (Lay) to 166.43 (Expert).
  - [corpus]: Neighbor paper "Adapting Biomedical Abstracts into Plain language using Large Language Models" confirms LLM capability for plain language adaptation (FMR=0.58).
- Break condition: If source abstracts contain highly specialized jargon without lay-accessible synonyms, prompts may fail to produce genuinely accessible summaries regardless of instruction quality.

### Mechanism 2
- Claim: Few-shot prompting provides more stable persona-aligned summarization than zero-shot or self-refine approaches.
- Mechanism: In-context examples demonstrate the expected mapping from source abstract to persona-specific output, giving the model concrete patterns for lexical choice, sentence complexity, and information selection. Self-refine iteratively critiques and revises but lacks external grounding.
- Core assumption: Three exemplars sufficiently capture the persona adaptation pattern without overfitting to example-specific content.
- Evidence anchors:
  - [abstract]: "Few-shot prompting outperformed zero-shot and self-refine approaches, with GPT-4o and LLaMA-3.1 providing the most stable performance."
  - [section]: Table 5 shows few-shot GPT-4o for Lay: R-1=0.602, SARI=53.51 vs. self-refine R-1=0.529, SARI=48.37.
  - [corpus]: Weak direct corpus evidence on few-shot vs. self-refine comparison in biomedical domain; generalization from this paper only.
- Break condition: If exemplars are not representative of test distribution (topic shift, study design variation), few-shot gains may diminish or reverse.

### Mechanism 3
- Claim: Expert review with structured error taxonomy improves factual accuracy and persona alignment beyond raw LLM output.
- Mechanism: Medical experts identify error types (incorrect definitions, contradictions, hallucinations, persona relevance violations) and correct them, catching domain-specific mistakes that automatic metrics miss.
- Core assumption: Two practicing physicians provide sufficient coverage of medical expertise for the abstract topics sampled.
- Evidence anchors:
  - [abstract]: "Each summary in PERCS was reviewed by physicians for factual accuracy and persona alignment using a detailed error taxonomy."
  - [section]: Table 3 shows post-correction human evaluation scores—Faithfulness 4.34–5.0, Comprehensiveness 4.47–4.94 across personas (5-point scale).
  - [corpus]: Neighbor paper "PlainQAFact" addresses factual consistency evaluation for plain language summarization, indicating this is an active concern.
- Break condition: If expert annotators have specialty mismatch with abstract topics (e.g., pediatricians reviewing oncology), correction quality may vary.

## Foundational Learning

- Concept: **Persona modeling and audience analysis**
  - Why needed here: The entire dataset structure depends on defining coherent, distinguishable audience segments with different literacy levels and information needs.
  - Quick check question: Can you articulate the knowledge gap between a "pre-med student" and a "non-medical researcher" that would require different summary treatments?

- Concept: **In-context learning and prompting strategies**
  - Why needed here: Benchmarking uses zero-shot, few-shot, and self-refine approaches; understanding their tradeoffs is essential for interpreting results and designing systems.
  - Quick check question: Why might providing three in-context examples improve output more than asking a model to critique and revise its own output?

- Concept: **Automatic summarization evaluation metrics**
  - Why needed here: The paper uses ROUGE, SARI, FKGL, DCRS, CLI, LENS, SummaC—each captures different aspects (coverage, readability, faithfulness).
  - Quick check question: Which metric would you prioritize if your primary concern was detecting hallucinations in medical summaries?

## Architecture Onboarding

- Component map: Source Abstracts (PLOS + PLABA, 500) → Persona-Specific Prompts (4 variants) → LLM Generation (GPT-4 initial drafts) → Expert Annotation Interface (error taxonomy, 11 types) → Corrected Summaries (2,000 total) → Quality Evaluation (human raters, 4 criteria) → Benchmark Suite (4 LLMs × 3 prompting strategies) → Automatic Metrics (comprehensiveness, readability, faithfulness)

- Critical path: Prompt design → Expert error taxonomy → Human evaluation rubric. Errors in prompt specificity or taxonomy completeness propagate through all downstream quality signals.

- Design tradeoffs:
  - Dataset size (500 abstracts) vs. annotation cost (2,000 summaries with expert review)
  - GPT-4 generation quality vs. reproducibility (closed model)
  - Four personas vs. granularity—more personas increase coverage but multiply annotation effort

- Failure signatures:
  - High lexical overlap (BERTScore) but low layness scores: model retaining jargon despite persona instructions
  - Self-refine showing lower ROUGE than zero-shot: model over-correcting toward shorter, less comprehensive output
  - Inter-annotator agreement dropping below 0.7: persona definitions ambiguous for annotators

- First 3 experiments:
  1. **Prompt ablation**: Remove one instruction category (e.g., "avoid jargon") from layperson prompt; measure impact on DCRS and layness scores to validate mechanism importance.
  2. **Few-shot exemplar sensitivity**: Vary number of exemplars (1, 3, 5) and source similarity; plot performance variance to identify stability boundaries.
  3. **Cross-persona transfer**: Train/fine-tune on one persona's summaries, test on another; quantify the penalty to understand persona-specificity of learned patterns.

## Open Questions the Paper Calls Out

- Question: Can hybrid methods combining few-shot examples with targeted feedback mechanisms improve faithfulness and information control beyond standard few-shot prompting?
- Basis in paper: [explicit] The Discussion section states, "future work should explore hybrid methods that combine in-context examples with targeted feedback mechanisms to further improve faithfulness and information control in persona-based summarization."
- Why unresolved: The paper only benchmarks standard zero-shot, few-shot, and self-refine strategies, finding that few-shot is superior but self-refine offers limited gains; it does not test combined approaches.
- What evidence would resolve it: Benchmark results on PERCS showing a hybrid model (e.g., few-shot + external verifier feedback) achieving higher faithfulness (SummaC) and persona alignment scores than the current few-shot baselines.

## Limitations

- Dataset relies entirely on GPT-4 for initial summaries, introducing potential bias in the reference standard that subsequent evaluations measure against
- Expert annotations from two physicians may not capture the full spectrum of medical domain knowledge across all subspecialties represented
- 500-abstract corpus size limits statistical power for detecting subtle persona differences

## Confidence

- High confidence: Dataset quality (human evaluation scores 4.3-5.0, Krippendorff's alpha 0.79-1.0), systematic readability differences across personas, benchmarking methodology
- Medium confidence: Claims about LLM capability for persona adaptation (limited by GPT-4 dependency and small sample size), generalization of prompting strategies beyond biomedical domain
- Low confidence: Long-term stability of few-shot performance across topic domains, cross-annotator reliability for fine-grained error types

## Next Checks

1. Replicate the full evaluation pipeline using an open-source LLM (e.g., LLaMA-3.1) as the reference summary generator to assess GPT-4 bias in benchmark scores
2. Conduct a blind human evaluation comparing persona alignment between expert-corrected and raw LLM outputs to quantify expert annotation impact
3. Test cross-domain generalization by applying the persona prompts to legal or technical abstracts and measuring performance decay relative to biomedical domain