---
ver: rpa2
title: Advancing Question Generation with Joint Narrative and Difficulty Control
arxiv_id: '2506.06812'
source_url: https://arxiv.org/abs/2506.06812
tags:
- difficulty
- narrative
- control
- questions
- question
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of generating questions that
  are both narrative-aware and difficulty-controllable, which is crucial for personalized
  education. The authors propose a joint control strategy that combines narrative
  attributes (like character, action, causal relationships) with difficulty levels
  estimated using Item Response Theory.
---

# Advancing Question Generation with Joint Narrative and Difficulty Control

## Quick Facts
- arXiv ID: 2506.06812
- Source URL: https://arxiv.org/abs/2506.06812
- Reference count: 18
- Primary result: Fine-tuned Flan-T5-large achieves narrative-aware, difficulty-controllable question generation with improved ROUGE-L and consistent learner performance patterns across difficulty levels.

## Executive Summary
This paper addresses the challenge of generating questions that are both narrative-aware and difficulty-controllable for personalized education. The authors propose a joint control strategy combining narrative attributes (character, action, causal relationships, etc.) with IRT-based difficulty levels. The method involves fine-tuning Flan-T5 on FairyTaleQA data augmented with difficulty labels estimated via Item Response Theory. The evaluation demonstrates that incorporating narrative control improves question quality, while difficulty control shows consistent patterns in simulated learner performance, though joint control exhibits less consistent performance at intermediate difficulty levels.

## Method Summary
The method fine-tunes a Flan-T5-large model on the FairyTaleQA dataset (10,580 QA pairs from 278 stories) augmented with IRT-estimated difficulty labels. The training pipeline involves three main steps: (1) training 5 simulated-learner QA systems on SQuAD v1.1 and using them to answer all FairyTaleQA questions, (2) applying Rasch model IRT to estimate question difficulty and discretizing to 5 levels, and (3) fine-tuning the Flan-T5 model with prompts that condition generation on both narrative labels and difficulty levels using special tokens ⟨QU⟩ and ⟨AN⟩. Inference uses top-k sampling (k=50, p=0.9, temp=1.2) to generate 5 QA pairs per section.

## Key Results
- Narrative control improves ROUGE-L-F1 scores, showing the model successfully controls narrative elements in generated questions
- Simulated learner QA accuracy decreases consistently as difficulty increases, validating difficulty control effectiveness
- Joint narrative-difficulty control shows less consistent performance at intermediate difficulty levels, with feeling and prediction attributes being harder to control than causal and outcome elements
- Linguistic analysis reveals increased lexical novelty at higher difficulty levels, while lower difficulty questions show greater specificity

## Why This Works (Mechanism)
The approach works by leveraging IRT to estimate question difficulty based on simulated learner responses, then conditioning the language model generation on both difficulty and narrative attributes. This dual conditioning allows the model to generate questions that are both contextually appropriate to the narrative and calibrated to specific difficulty levels. The effectiveness stems from the model learning to adjust linguistic complexity, specificity, and semantic focus based on the combined control signals during fine-tuning.

## Foundational Learning

**Item Response Theory (IRT)**
- Why needed: Provides principled difficulty estimation based on learner performance patterns rather than arbitrary heuristics
- Quick check: Verify that IRT-estimated difficulties correlate with actual learner performance on a validation set

**Rasch Model**
- Why needed: Simplest IRT model that estimates both item difficulty and learner ability on the same scale
- Quick check: Confirm that the Rasch model fits the simulated learner response data adequately (goodness-of-fit statistics)

**Flan-T5 Fine-tuning**
- Why needed: Enables the model to learn conditional generation patterns for both narrative and difficulty attributes
- Quick check: Monitor validation loss and ROUGE scores during training to ensure effective learning

## Architecture Onboarding

**Component Map**
FairyTaleQA data -> Simulated learner QA systems (DeBERTaV3, RoBERTa, BERT, DistilBERT, GPT-2) -> IRT difficulty estimation -> Flan-T5 fine-tuning -> Joint narrative-difficulty controlled question generation

**Critical Path**
1. Generate simulated learner responses for all training questions
2. Apply IRT to estimate difficulty levels
3. Fine-tune Flan-T5 with joint control prompts
4. Generate and evaluate QA pairs

**Design Tradeoffs**
- Top-k sampling vs. beam search: Top-k enables better control but causes hallucinations; beam search reduces hallucinations but creates repetitive outputs
- 5 difficulty levels vs. fewer: Finer granularity provides better control but less reliable estimates due to data sparsity
- Simulated vs. real learners: Simulated learners enable large-scale difficulty estimation but may not reflect actual student cognition

**Failure Signatures**
- Repetitive questions across difficulty levels for same narrative: Indicates beam search limitations
- Inconsistent control at medium/moderate difficulty: Suggests insufficient data for joint control learning
- Hallucinations in generated QA pairs: Points to temperature or sampling parameter issues

**First Experiments**
1. Verify IRT difficulty estimation by checking difficulty distributions match reported values
2. Test narrative control effectiveness by measuring ROUGE improvement over baseline
3. Evaluate difficulty control by measuring simulated learner performance across generated difficulty levels

## Open Questions the Paper Calls Out

**Open Question 1**
Would increasing dataset size and balancing the distribution of questions across narrative-difficulty category combinations improve the model's ability to consistently control intermediate difficulty levels (medium and moderate)? The current FairytaleQA dataset has ~10k instances with imbalanced representation, which the authors identify as limiting the model's learning ability for joint control.

**Open Question 2**
What inference methods or hybrid decoding strategies can jointly minimize hallucinations and repetitive outputs while maintaining fine-grained difficulty controllability? Top-k sampling enables control but causes hallucinations (14% in error analysis), while beam search reduces hallucinations but generates repetitive questions across difficulty levels for the same narrative element.

**Open Question 3**
To what extent do simulated-learner QA system responses correlate with real student performance on generated questions? The IRT-based difficulty estimation uses QA models as proxies for learners, which may not reflect actual student cognitive processes and response patterns.

## Limitations
- Joint control shows inconsistent performance at intermediate difficulty levels, particularly for feeling and prediction narrative attributes
- The method relies on simulated learner responses rather than real student data, potentially limiting ecological validity
- Fine-tuning details (learning rate, batch size, hardware specifications) are unspecified, making exact reproduction difficult

## Confidence
- **High confidence**: Narrative control mechanism successfully influences generated question content
- **Medium confidence**: Difficulty control shows clear patterns but joint control exhibits inconsistent performance at intermediate levels
- **Medium confidence**: Linguistic feature analysis shows meaningful correlations with difficulty levels

## Next Checks
1. Reproduce IRT difficulty estimation by training the 5 simulated-learner QA systems and verifying estimated difficulty distributions match reported values
2. Test joint control stability by generating QA pairs across all combinations and measuring consistency using a held-out QA system
3. Evaluate control granularity by repeating experiments with 3 difficulty levels instead of 5 to assess impact on control consistency