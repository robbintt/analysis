---
ver: rpa2
title: 'ProMind-LLM: Proactive Mental Health Care via Causal Reasoning with Sensor
  Data'
arxiv_id: '2505.14038'
source_url: https://arxiv.org/abs/2505.14038
tags:
- mental
- health
- data
- arxiv
- promind-llm
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ProMind-LLM, a proactive mental health risk
  assessment system that integrates objective behavioral sensor data with subjective
  mental health records using large language models (LLMs). It addresses the limitation
  of relying solely on subjective textual inputs by combining domain-specific pretraining,
  self-refine mechanism for numerical data processing, and causal chain-of-thought
  reasoning.
---

# ProMind-LLM: Proactive Mental Health Care via Causal Reasoning with Sensor Data

## Quick Facts
- arXiv ID: 2505.14038
- Source URL: https://arxiv.org/abs/2505.14038
- Reference count: 40
- Primary result: ProMind-LLM achieves F1 scores of 0.712 on PMData and 0.765 on Globem, significantly outperforming general LLMs on mental health risk prediction

## Executive Summary
This paper introduces ProMind-LLM, a proactive mental health risk assessment system that integrates objective behavioral sensor data with subjective mental health records using large language models. The system addresses the limitation of relying solely on subjective textual inputs by combining domain-specific pretraining, self-refine mechanism for numerical data processing, and causal chain-of-thought reasoning. Evaluated on two real-world datasets, PMData and Globem, ProMind-LLM significantly outperforms general LLMs, achieving improved accuracy and interpretability for mental health risk prediction.

## Method Summary
ProMind-LLM employs a two-stage training pipeline on LLaMA3-8B/InternLM2-7B models, starting with continuous pretraining on ~100K mental health articles (80M tokens) followed by counterfactual-augmented supervised finetuning on 112K samples from IMHI, ANGST, Depression Reddit, and CPsyCoun datasets. The system integrates numerical sensor data through a self-refine mechanism that iteratively compresses and restructures behavioral data to reduce perplexity, then applies causal chain-of-thought reasoning that combines factual analysis with counterfactual exploration to assess mental health risks. The model is trained with Deepspeed ZeRO-2, flash-attention2, and evaluated on binary classification tasks with accuracy, precision, recall, and F1 metrics.

## Key Results
- ProMind-LLM achieves F1 scores of 0.712 on PMData and 0.765 on Globem datasets
- Self-refine mechanism reduces tokens from 211 to 29 and perplexity from 6.18-17.22 to 4.31
- Domain-specific pretraining improves F1 from 0.521 to 0.712 on PMData (26.8% improvement)

## Why This Works (Mechanism)

### Mechanism 1: Domain-Specific Training Pipeline
- **Claim:** Continuous pretraining on mental health literature combined with counterfactual-augmented supervised finetuning improves mental health risk assessment accuracy compared to general-purpose LLMs.
- **Mechanism:** The two-stage training first injects domain knowledge (~80M tokens from 100K professional mental health articles), then improves robustness to subjective data uncertainties by exposing the model to counterfactual samples with misleading information (personality traits, stigma, lack of awareness distortions).
- **Core assumption:** Domain-specific corpora improve mental health reasoning more than general web-scale data; counterfactual augmentation transfers to real-world reporting inconsistencies.
- **Evidence anchors:** [abstract] "domain-specific pretraining to tailor the LLM for mental health contexts"; [section 5.3] InternLM2-base-7B with PT+SFT achieves F1 of 0.712 vs 0.521 with SFT-only on PMData (26.8% improvement)

### Mechanism 2: Self-Refine Mechanism for Numerical Data
- **Claim:** Iterative self-refinement of behavioral sensor data format reduces token length and improves LLM comprehension, as measured by perplexity.
- **Mechanism:** The LLM retrieves raw behavior data via RAG, evaluates format redundancy, generates an improved version through self-feedback loops, and outputs a compressed structured summary (e.g., "Average: 3764 kcal" instead of raw sequences).
- **Core assumption:** LLMs can accurately judge their own comprehension of numerical formats; perplexity correlates with downstream task performance on sensor data.
- **Evidence anchors:** [abstract] "self-refine mechanism to optimize the processing of numerical behavioral data"; [section 3.2, Table 1] Self-refine reduces tokens from 211 to 29 and perplexity from 6.18-17.22 to 4.31

### Mechanism 3: Causal Chain-of-Thought with Counterfactual Exploration
- **Claim:** Combining factual causal analysis with counterfactual "what-if" reasoning improves robustness against uncertainties in subjective mental records.
- **Mechanism:** First, ProMind identifies correlated risk indicators between behavior data (D) and mental records (R) using thresholded conditional probability. Then, counterfactual reasoning tests hypothetical scenarios (e.g., "What if sleep improved?") to validate or weaken causal links. Finally, insights are integrated for final prediction.
- **Core assumption:** Causal relations exist between behavioral patterns and mental states; counterfactual reasoning helps disentangle confounding factors in self-reported data.
- **Evidence anchors:** [abstract] "causal chain-of-thought reasoning to enhance the reliability and interpretability"; [section 3.3, Figure 2] Shows factual analysis followed by counterfactual refinement

## Foundational Learning

- **Concept: Counterfactual Reasoning**
  - **Why needed here:** ProMind uses counterfactuals in two places: (1) during SFT to train resilience to misleading reports, and (2) during inference to test "what-if" scenarios. Understanding how counterfactuals differ from factual prediction is essential.
  - **Quick check question:** Given a user with high stress and reduced social interaction, what counterfactual would test whether social interaction is causally linked to stress reduction?

- **Concept: Multimodal Data Alignment**
  - **Why needed here:** The system must integrate numerical sensor streams (heart rate, sleep) with text-based mental records, which have different temporal granularities and noise profiles.
  - **Quick check question:** If behavioral data is aggregated weekly but mental records are sporadic, how might this misalignment affect causal inference?

- **Concept: LLM Limitations with Numerical Data**
  - **Why needed here:** The self-refine mechanism exists because LLMs struggle with long numerical sequences. Understanding tokenization and perplexity helps diagnose when this fails.
  - **Quick check question:** Why might perplexity decrease even if diagnostic accuracy doesn't improve?

## Architecture Onboarding

- **Component map:** Raw sensor data -> Weekly aggregation -> Self-refine preprocessing -> RAG retrieval -> Factual CoT analysis -> Counterfactual exploration -> Final prediction
- **Critical path:** 1. Curate and clean mental health pretraining corpus (quality affects downstream performance) 2. Generate counterfactual samples from SFT pairs (critical for robustness to subjective noise) 3. Implement self-refine loop with convergence criteria (max iterations k, perplexity threshold) 4. Design causal CoT prompts that elicit structured reasoning
- **Design tradeoffs:** Small model (7-8B parameters) vs. large API models; token compression vs. information loss; counterfactual depth vs. inference cost
- **Failure signatures:** Low recall despite high precision; high perplexity on behavior data; inconsistent evidence-outcome mapping
- **First 3 experiments:** 1. Ablate self-refine: Compare raw sensor input vs. refined format on token count, perplexity, and F1 score 2. Test counterfactual SFT impact: Train with/without counterfactual augmentation; measure recall 3. Validate causal CoT interpretability: Run human evaluation on 50 correct/50 incorrect predictions

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does ProMind-LLM perform in real-world, uncontrolled environments across diverse populations regarding age, gender, and race?
- Basis in paper: [explicit] The authors explicitly state in the Limitations section that effectiveness in diverse populations remains "insufficiently validated."
- Why unresolved: The current study relies on specific datasets (PMData, Globem) which may not capture the full demographic variance or unpredictability of global real-world deployment.
- What evidence would resolve it: Successful evaluation on a multi-centric dataset containing balanced representation of age, gender, and race in uncontrolled settings.

### Open Question 2
- Question: Can the proposed domain-specific training pipeline be effectively scaled to larger language models (e.g., 70B+ parameters)?
- Basis in paper: [explicit] The paper notes that due to resource constraints, the training pipeline has only been evaluated on "small-scale LLMs" (LLaMA3-8B, InternLM2-7B).
- Why unresolved: It is unclear if the relative performance gains from continuous pre-training and counterfactual SFT transfer linearly to models with significantly larger parameter counts.
- What evidence would resolve it: Comparative benchmarking of the ProMind pipeline applied to 70B parameter models against their vanilla counterparts.

### Open Question 3
- Question: What specific interventions can reduce the rate of "underestimation of missed risks" and "incorrect causal links" in false negatives?
- Basis in paper: [explicit] The Limitations section calls for further investigation into misclassified cases, specifically citing the "underestimation of missed risks."
- Why unresolved: While the paper introduces counterfactual learning to improve robustness, the error analysis shows these specific failure modes persist.
- What evidence would resolve it: An ablation study testing targeted loss functions or prompt engineering strategies designed specifically to penalize false negatives in mental risk assessment.

## Limitations

- The self-refine mechanism's effectiveness depends heavily on the quality of RAG retrieval and the LLM's ability to evaluate its own comprehension, with no validation that perplexity correlates with actual diagnostic performance improvements
- Counterfactual reasoning assumes causal relationships exist between behavioral patterns and mental states, but the paper doesn't validate this assumption or test robustness when these relationships are weak or absent
- The model's performance on binary classification masks potential failures in nuanced risk assessment, particularly for individuals with complex comorbidity profiles

## Confidence

- **High Confidence:** The two-stage training pipeline (domain pretraining + counterfactual SFT) demonstrably improves performance over baseline LLMs on the reported datasets, with clear quantitative improvements in F1 scores (0.712 vs 0.521 on PMData)
- **Medium Confidence:** The causal chain-of-thought reasoning provides interpretability benefits, though the paper doesn't demonstrate that this reasoning actually improves clinical decision-making or patient outcomes
- **Low Confidence:** The claim that self-refine reduces perplexity and improves comprehension lacks validation that this translates to better downstream task performance

## Next Checks

1. Conduct ablation studies on the causal reasoning component by comparing performance with factual-only CoT vs. full causal CoT on out-of-distribution test sets to verify robustness claims
2. Test model performance on datasets with different demographic compositions to assess potential bias in the mental health corpus pretraining
3. Implement a clinical pilot study where ProMind-LLM predictions are compared against licensed psychologist assessments in real-world settings to validate practical utility beyond benchmark performance