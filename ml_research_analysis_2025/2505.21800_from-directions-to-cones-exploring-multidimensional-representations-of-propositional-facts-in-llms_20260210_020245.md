---
ver: rpa2
title: 'From Directions to Cones: Exploring Multidimensional Representations of Propositional
  Facts in LLMs'
arxiv_id: '2505.21800'
source_url: https://arxiv.org/abs/2505.21800
tags:
- arxiv
- truth
- cone
- behavior
- direction
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper extends the concept cone framework to truth modeling
  in LLMs. It shows that truthfulness of simple propositions is mediated by a multi-dimensional
  subspace rather than a single direction.
---

# From Directions to Cones: Exploring Multidimensional Representations of Propositional Facts in LLMs

## Quick Facts
- arXiv ID: 2505.21800
- Source URL: https://arxiv.org/abs/2505.21800
- Reference count: 25
- Primary result: Truthfulness in LLMs is mediated by multi-dimensional subspaces (concept cones), not single directions

## Executive Summary
This paper extends the concept cone framework to model truthfulness, showing that simple propositions are represented in multi-dimensional subspaces rather than single directions. The authors demonstrate that interventions on learned cones can reliably flip model responses on factual statements with success rates of 53-100% across multiple model families and sizes. The approach reveals richer geometric structure governing truth in LLMs and shows that concept cones are a powerful interpretability tool that minimally impacts unrelated capabilities.

## Method Summary
The method uses gradient-based optimization to discover orthonormal basis vectors spanning a cone in the residual stream that mediates truth. The optimization balances three objectives: maximizing the probability of "Yes" for false statements (addition), "No" for true statements (ablation), and KL divergence retention on instruction-following tasks. Interventions are applied at layers 60-75% of normalized depth, with ablation removing the projection of activations onto the cone direction and addition adding it. The framework is tested on simple binary propositions across Qwen and Gemma model families.

## Key Results
- Truthfulness is mediated by multi-dimensional subspaces (concept cones) rather than single directions
- Interventions on learned cones flip factual responses with 53-100% success rates across models
- First cone dimension aligns with prior DIM methods, but subsequent dimensions capture orthogonal structure
- Interventions minimally affect unrelated capabilities (KL divergence <0.05 on instruction-following tasks)

## Why This Works (Mechanism)

### Mechanism 1: Multi-Dimensional Subspace Mediation
The paper assumes truth is encoded in a multi-dimensional cone rather than a single linear direction. By learning orthonormal basis vectors spanning this volume, interventions create robust steering effects that single-dimension methods miss. The core assumption is that the semantic concept of "truth" for simple propositions is linearly separable or can be approximated by a convex cone in the activation space.

### Mechanism 2: Gradient-Based Orthogonal Decomposition
Gradient descent isolates specific causal directions for truth by optimizing a composite loss balancing steering power against behavioral retention. The system iteratively finds basis vectors by maximizing probability of "Yes" for false statements and "No" for true statements while enforcing orthogonality via cone constraints.

### Mechanism 3: Surgical Residual Stream Ablation
Ablating cone dimensions disrupts truth processing while leaving other capabilities intact because the residual stream allows selective erasure of specific directions. If the cone accurately isolates "truth," removing this component forces the model to rely on remaining features, causing it to default to untruthful responses without crashing generation.

## Foundational Learning

- **Residual Stream & Layer Normalization**: Interventions are applied directly to the residual stream at specific layers. Understanding how the residual stream aggregates information is crucial to visualizing why vector addition affects final logits. Quick check: If you add a vector $r$ to the residual stream at layer $l$, does it affect the attention calculation at layer $l$? (Yes).

- **Causal Interventions (Ablation vs. Addition)**: The paper relies on two distinct intervention types to validate the "truth" property. Adding the vector induces truth while ablating it erases truth. Quick check: What is the expected outcome of adding a "truth" vector to a prompt containing a false statement? (The model should shift toward labeling it as true).

- **Orthogonality & Linear Subspaces**: The core finding is that truth is a cone (multi-dimensional). Understanding that orthogonal vectors represent independent features is essential to interpret why Dimension 2 and 3 add new information beyond Dimension 1. Quick check: If two vectors are orthogonal, what is their dot product? (Zero).

## Architecture Onboarding

- **Component map**: Input Processor -> Extractor (gradient optimizer) -> Intervention Engine -> Evaluator
- **Critical path**: 1) Layer Localization: Run 1D cone search to find "Truth Layer" (0.60-0.75 depth) 2) Basis Construction: Extract $k$ orthonormal vectors 3) Monte Carlo Validation: Sample random non-negative combinations to ensure entire cone mediates truth
- **Design tradeoffs**: Increasing dimensions captures more "truth" structure but risks including noise (ASR drops in smaller models with >3 dimensions). Ablating at all layers is robust but computationally heavy; layer-specific intervention is faster but relies on accurate localization.
- **Failure signatures**: Semantic Drift (high KL divergence >0.1 indicates cone includes unrelated features). Low ASR (model ignores intervention, suggesting failed optimization or wrong target layer).
- **First 3 experiments**: 1) Layer Scan: Train 1D cone at every layer to identify truth-mediating range 2) DIM Baseline Comparison: Compare cosine similarity to verify cone discovers new orthogonal structures 3) Retention Test: Ablate full cone on Alpaca prompts and measure KL divergence (target: <0.05)

## Open Questions the Paper Calls Out

### Open Question 1
Do the orthogonal basis vectors of the truth cone correspond to distinct, interpretable semantic facets like fact modality, certainty, or domain specificity? The authors are yet to find semantically meaningful labels for the basis vectors and suggest future work could use clustering or sparse autoencoding to assign meaning.

### Open Question 2
Can concept cone interventions reliably mediate truthfulness for context-dependent or subjective claims, or are they restricted to simple, unambiguous propositions? The operationalization of truth is deliberately narrow and does not capture more complex notions of truth.

### Open Question 3
Does the causal efficacy of truth cones scale to larger frontier models or persist across diverse alignment protocols like RLHF? Experiments were limited to small models (1.5B-7B parameters) and may not generalize to larger frontier models.

### Open Question 4
Why does the concept cone framework fail to identify coherent subspaces for sentiment and toxicity, and can the optimization be adapted for these concepts? The authors failed to find a meaningful concept cone for sentiment and toxicity using current methods.

## Limitations

- The method's effectiveness for complex, context-dependent propositions remains untested
- Specific gradient projection technique for maintaining orthonormality is not detailed
- Exact loss weighting parameters are unspecified, potentially affecting results

## Confidence

- **High Confidence**: Truthfulness is mediated by multi-dimensional subspaces rather than single directions (strongly supported by ASR metrics and low KL divergence)
- **Medium Confidence**: Subsequent cone dimensions capture orthogonal structure beyond first dimension (cosine similarity analysis supports this, but semantic meaning unclear)
- **Medium Confidence**: Interventions minimally affect unrelated capabilities (KL divergence <0.05 is promising but tested capabilities are limited)

## Next Checks

1. Systematically vary intervention layer within 60-75% depth range and measure how ASR and KL divergence change to determine if "Truth Layer" is optimal point or broader region.

2. Apply cone extraction method to different abstract concept (e.g., "safety" or "creativity") and compare resulting subspace geometry to identify unique properties of truth cone.

3. Construct dataset of multi-clause or context-dependent propositions and test whether learned cones still effectively steer truth judgments to determine if framework is limited to atomic facts.