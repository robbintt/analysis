---
ver: rpa2
title: Multi-Agent VLMs Guided Self-Training with PNU Loss for Low-Resource Offensive
  Content Detection
arxiv_id: '2511.13759'
source_url: https://arxiv.org/abs/2511.13759
tags:
- data
- learning
- labeled
- clip
- detection
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of detecting offensive content
  on social media in low-resource settings, where labeled data is scarce. The proposed
  method, MA-VLMs Guided Self-Training with PNU Loss, leverages abundant unlabeled
  data through a novel self-training framework that combines a lightweight classifier
  with Multi-Agent Vision-Language Models (MA-VLMs).
---

# Multi-Agent VLMs Guided Self-Training with PNU Loss for Low-Resource Offensive Content Detection

## Quick Facts
- arXiv ID: 2511.13759
- Source URL: https://arxiv.org/abs/2511.13759
- Reference count: 15
- Key outcome: MA-VLMs Guided Self-Training with PNU Loss achieves 74.22% accuracy and 72.68% macro-F1 on FHM dataset, outperforming CLIP with self-training by 3.09 macro-F1 points

## Executive Summary
This paper addresses the challenge of detecting offensive content on social media in low-resource settings, where labeled data is scarce. The proposed method, MA-VLMs Guided Self-Training with PNU Loss, leverages abundant unlabeled data through a novel self-training framework that combines a lightweight classifier with Multi-Agent Vision-Language Models (MA-VLMs). The MA-VLMs simulate dual perspectives (moderator and user) to enhance pseudo-label reliability, while the PNU loss integrates labeled, high-confidence pseudo-labeled (Agreed-Unknown), and ambiguous (Disagreed-Unknown) data. Experiments on four benchmark datasets show that the method significantly outperforms supervised baselines and rivals larger models, achieving robust performance with as few as 50 labeled samples.

## Method Summary
The framework uses CLIP-Large as a lightweight classifier and Qwen-2.5-VL-72B as a frozen MA-VLM verifier. The process involves: (1) training CLIP on labeled data, (2) predicting on unlabeled data and selecting top-k (k=500) samples by confidence, (3) MA-VLMs (Moderator and User agents) independently assess each sample and issue final decisions only if they agree with each other and the classifier, (4) Agreed samples receive soft pseudo-labels (0.67/0.33) and Disagreed samples remain unlabeled, (5) retraining using PNU loss with γ-tuned parameters, and (6) validation check with rollback if performance degrades. The cycle repeats until the unlabeled pool is exhausted.

## Key Results
- On FHM dataset: 74.22% accuracy, 72.68% macro-F1 with only 100 labeled samples
- Outperforms CLIP with self-training by 3.09 macro-F1 points on FHM
- MA-VLMs contribute +1.59 macro-F1 improvement over CLIP-only self-training
- Robust performance across all four datasets (FHM, MAMI, HSOL, Sent140) with n=50-250 labeled samples

## Why This Works (Mechanism)

### Mechanism 1
Multi-agent VLM negotiation improves pseudo-label reliability by surfacing implicit harms that single-perspective classifiers miss. Two VLM agents with opposing personas (strict moderator vs. lenient user) independently assess each sample, then review each other's rationales before final decisions. Only unanimous classifier+agent agreement yields pseudo-labels. This filters annotation bias and captures nuanced offensiveness (e.g., "jerk off to a girl is compliment" masks hate as flattery—Table 2).

### Mechanism 2
Partitioning unlabeled data into Agreed-Unknown and Disagreed-Unknown sets enables differentiated learning that extracts signal from ambiguous samples without corrupting the training pool. Classifier confidence ranking selects top-k candidates; MA-VLMs verify. Agreed samples receive soft pseudo-labels (ŷₚ=0.67, ŷₙ=0.33) to prevent overfitting. Disagreed samples remain unlabeled but contribute via PU/NU loss formulation, treating them as "suspicious unlabeled" rather than discarding.

### Mechanism 3
PNU loss with dataset-specific γ weighting balances positive-negative supervision against unlabeled data contribution, adapting to varying annotation bias levels. γ∈[-1,1] controls PU/NU influence: γ<0 favors NU learning, γ>0 favors PU learning, γ=0 reduces to standard PN. The paper finds γ=0.0 optimal for imbalanced FHM (3:7 ratio) and γ=0.1 for balanced datasets with offensive-lexicon bias in negative samples.

## Foundational Learning

- **Concept: Positive-Unlabeled (PU) Learning**
  - Why needed here: Standard binary classification assumes clean negative labels; PU learning estimates negative risk from unlabeled data, essential when "non-offensive" samples may contain unmarked offensive content
  - Quick check question: Can you explain why treating all unlabeled data as negative causes classifier bias in offensive content detection?

- **Concept: Self-Training with Confidence Thresholding**
  - Why needed here: The iterative pseudo-label expansion mechanism depends on selecting reliable candidates; poor thresholding causes error propagation
  - Quick check question: How does the validation-check rollback (step 5) prevent confirmation bias from corrupting the training pool?

- **Concept: Soft Label / Label Smoothing**
  - Why needed here: Agreed-Unknown pseudo-labels use soft targets (0.67/0.33 vs. hard 1.0/0.0) to acknowledge uncertainty and prevent overconfidence
  - Quick check question: Why would hard pseudo-labels degrade generalization compared to soft targets in this framework?

## Architecture Onboarding

- Component map: [Labeled Data (n=50-250)] → [CLIP-Large Classifier] → [Confidence Rankings] → [Top-k Selection (k=500)] → [MA-VLMs: Moderator + User Agents] → [Agreed-Unknown + Disagreed-Unknown] → [PNU Loss (γ-tuned)] → [Classifier Retraining] → [Validation Check] → Rollback if no improvement

- Critical path: Labeled data → Initial classifier → MA-VLM verification → PNU loss retraining. Breaks if MA-VLMs timeout/return inconsistent formats, or if validation never improves (indicating pseudo-label drift).

- Design tradeoffs:
  - k=500 balances coverage vs. precision; smaller k increases label quality but slows expansion
  - Qwen72B as frozen VLM avoids fine-tuning cost but limits domain adaptation
  - Soft targets (0.67/0.33) are arbitrary—heuristic could be calibrated per dataset

- Failure signatures:
  - Early-round M-F1 collapse (>10% drop): classifier overfits to noisy pseudo-labels—reduce γ or increase k
  - Stagnant pseudo-label M-F1 across rounds: confident samples exhausted—terminate self-training
  - High Disagreed-Unknown rate (>70%): MA-VLMs too conservative—relax persona prompts

- First 3 experiments:
  1. Baseline ablation: Run SelfTrain with CLIP-only pseudo-labeling (no MA-VLMs) vs. MA-VLMs-only vs. combined—quantify verification contribution (expect +1.59 M-F1 from Table 1)
  2. γ sensitivity sweep: Test [-0.1, 0.0, 0.1, 0.2] on held-out validation—identify dataset-specific optimal γ before full training
  3. Extreme low-resource stress test: n=25 labeled samples—determine if framework maintains >65 M-F1 or if more aggressive augmentation is needed

## Open Questions the Paper Calls Out

### Open Question 1
Can the MA-VLMs framework effectively generalize to low-resource languages and cultural contexts beyond English, without requiring language-specific prompt engineering or annotation? The introduction states that "many regions and communities remain underprotected" and that "building robust and equitable systems remains difficult due to the scarcity of high-quality labeled data, especially for minority languages and underrepresented communities." All experiments were conducted on English-only benchmarks.

### Open Question 2
Is the 72B parameter scale of Qwen-2.5-VL essential for reliable MA-VLM negotiation, or can comparable pseudo-label quality be achieved with smaller VLMs? The framework relies on Qwen72B for all MA-VLM pseudo-labeling, which has substantial computational costs. The related work section explicitly notes that LLM approaches face "high inference costs limit scalability for real-time moderation."

### Open Question 3
Can the dataset-specific hyperparameters (γ in PNU loss, k for top-k selection, soft target values) be automatically estimated without requiring a labeled development set? The methodology section states that "optimal γ values are determined via ablation" and uses a development set for "early stopping and the retention or removal of top-k pseudo-labels." In true low-resource scenarios, even development data may be unavailable.

### Open Question 4
How does the framework perform in dynamic, streaming environments where unlabeled data arrives continuously and concept drift may occur? The self-training pipeline operates in offline batches, processing fixed unlabeled pools over discrete rounds. The related work mentions "real-time moderation" as a deployment challenge, but all experiments assume static datasets with no temporal distribution shift.

## Limitations
- Absence of cross-dataset validation—method only tested within-dataset, leaving domain transfer limits unknown
- Frozen Qwen-2.5-VL-72B assumption constrains domain adaptation potential
- No runtime or cost analysis provided, obscuring practical overhead of dual-agent verification
- Soft-label values (0.67/0.33) are heuristic without ablation support

## Confidence

- **High Confidence**: The quantitative superiority claim (3.09 macro-F1 improvement over CLIP self-training on FHM) is directly supported by Table 1; the ablation showing MA-VLMs contribute +1.59 macro-F1 is also well-grounded.
- **Medium Confidence**: The PNU loss formulation and its dataset-specific γ tuning (0.0 for FHM, 0.1 for MAMI/HSOL) are methodologically sound, but the underlying assumption that lexicon-constructed negatives contain offensive cues lacks direct empirical validation in this work.
- **Low Confidence**: The qualitative examples of MA-VLMs surfacing implicit harms (e.g., "jerk off to a girl is compliment") are illustrative but not systematically measured; the mechanism by which dual-perspective reasoning improves pseudo-label reliability remains largely theoretical.

## Next Checks
1. **Cross-dataset robustness**: Apply the trained MA-VLMs + PNU model from FHM to MAMI and vice versa; measure performance drop to quantify domain transfer limits.
2. **γ sensitivity and ablation**: Systematically sweep γ across [-0.2, -0.1, 0.0, 0.1, 0.2] on each dataset; report validation curves to confirm optimal γ per domain and test whether γ=0.0 is truly optimal for FHM.
3. **MA-VLM verification overhead**: Instrument the pipeline to record per-sample verification time and cost; compare with CLIP-only self-training to quantify practical scalability limits.