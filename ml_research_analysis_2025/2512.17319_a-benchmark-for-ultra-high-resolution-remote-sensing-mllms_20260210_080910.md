---
ver: rpa2
title: A Benchmark for Ultra-High-Resolution Remote Sensing MLLMs
arxiv_id: '2512.17319'
source_url: https://arxiv.org/abs/2512.17319
tags:
- image
- object
- question
- answer
- sensing
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces RSHR-Bench, a large-scale ultra-high-resolution
  remote sensing benchmark for multimodal large language models (MLLMs). It addresses
  the gap between existing low-resolution benchmarks and real-world remote sensing
  imagery, which often contains up to hundreds of megapixels.
---

# A Benchmark for Ultra-High-Resolution Remote Sensing MLLMs

## Quick Facts
- **arXiv ID:** 2512.17319
- **Source URL:** https://arxiv.org/abs/2512.17319
- **Reference count:** 40
- **Primary result:** Ultra-high-resolution RS benchmark (5,329 images ≥4K) exposes uniform low performance across VLMs, highlighting resolution bottleneck.

## Executive Summary
RSHR-Bench introduces a large-scale ultra-high-resolution remote sensing benchmark for multimodal large language models (MLLMs), addressing the gap between existing low-resolution benchmarks and real-world RS imagery. The dataset features 5,329 full-scene images with long sides of at least 4,000 pixels and includes diverse tasks such as multiple-choice VQA, open-ended VQA, image captioning, and single-image evaluation. All tasks undergo rigorous human and LLM verification to ensure visual grounding and minimize language priors. Evaluations reveal uniformly low performance across a broad suite of open-source, closed-source, and RS-specific VLMs, highlighting the need for further progress in high-resolution visual understanding.

## Method Summary
The benchmark employs a multi-stage pipeline: image collection from existing RS datasets filtered for ultra-high resolution (≥4K long side), bounding box annotation, question generation using Qwen2.5-VL-7B and GPT-5 Thinking with 13 prompt templates, human verification of correctness and ambiguity, and LLM adversarial filtering using text-only models (Qwen3-8B, Llama3-8B) to remove questions solvable without visual evidence. The final dataset contains 1,932 tasks across multiple-choice VQA, open-ended VQA, image captioning, and single-image evaluation categories, with performance measured through accuracy, GPT-4o judging, BLEU/METEOR/ROUGE-L, and dialog-level exact match metrics.

## Key Results
- Accuracy drops markedly at 100M/200M pixel regime across all models
- Text-only LLMs achieve 33.9%-42.2% on reasoning tasks even after adversarial filtering
- Specialized RS VLMs fail to significantly outperform general-purpose VLMs on open-ended VQA
- Multi-turn tasks expose cascading errors, with most models below 10% MTEM@1

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Adversarial filtering removes questions solvable via language priors alone, forcing models to use visual evidence.
- **Mechanism:** Text-only LLMs (Qwen3-8B, Llama3-8B) attempt each question without image access. If accuracy exceeds ~30%, the item is flagged for revision or removal. Questions are rewritten to eliminate lexical hints until visual grounding becomes necessary.
- **Core assumption:** Questions answerable without images indicate flawed benchmark design rather than legitimate reasoning; filtering them exposes genuine visual understanding gaps.
- **Evidence anchors:** Abstract notes text-only models can perform competitively without images; Section 3.1 describes filtering process; Section 7 shows Qwen3-8B achieved 51.6% accuracy on XLRS-Bench reasoning without images.
- **Break condition:** If text-only LLM performance on filtered questions remains high (>35%), either filtering was insufficient or task domain inherently allows commonsense reasoning.

### Mechanism 2
- **Claim:** Ultra-high-resolution inputs (≥4K long side, up to 3×10^8 pixels) overwhelm standard tiling and tokenization strategies, revealing architectural bottlenecks.
- **Mechanism:** Most VLMs tile inputs at 336×336 to 448×448 patches or cap at 2K–4K. When images span tens of megapixels, aggressive downsampling or fragmentation destroys long-range spatial context and small-object details. RSHR-Bench preserves native resolution, forcing models to either scale token budgets (computationally expensive) or lose critical information.
- **Core assumption:** Real-world RS tasks require both global context and fine-grained detail; current architectures cannot efficiently handle both simultaneously.
- **Evidence anchors:** Section 3.1 states images have long side ≥4K with pixel counts up to ~3×10^8; Table 5 shows most open-source VLMs constrained by tiling; Figure 4 shows accuracy drops markedly at 100M/200M pixel regime.
- **Break condition:** If a model achieves >60% accuracy on 100M+ pixel images without architectural changes, the benchmark may not be sufficiently resolution-demanding.

### Mechanism 3
- **Claim:** Multi-turn and multi-image dialog tasks expose cascading errors and cross-image reasoning failures invisible in single-turn evaluations.
- **Mechanism:** Tasks like Future Prediction (two-image comparison) and multi-round Anomaly Detection require models to maintain coherent state across turns, reference multiple visual contexts, and perform comparative reasoning. Dialog-level exact match requires all turns correct; errors compound rather than average out.
- **Core assumption:** Real-world RS analysis workflows involve iterative refinement and temporal comparison; single-turn accuracy does not reflect operational capability.
- **Evidence anchors:** Section 3.2 describes Single-Image Multi-Round and Multi-Image Single-Round tasks; Table 2 shows GPT-5 achieves MTEM@1 of 52.6% but most models below 10%; Section 4 defines MTEM@t requiring minimum score across all turns.
- **Break condition:** If a model shows strong multi-turn performance but weak single-turn, it may be exploiting dialog structure for guessing rather than genuine reasoning.

## Foundational Learning

- **Concept: Language prior exploitation**
  - Why needed here: Understanding that benchmarks can be "gamed" through statistical regularities in text alone—e.g., "forest" correlates with "green" in language space—is essential for interpreting why adversarial filtering matters.
  - Quick check question: *Can you explain why Qwen3-8B achieving 77% accuracy on Existence & Counting Reasoning (ECR) in XLRS-Bench without seeing images indicates a benchmark flaw rather than model capability?*

- **Concept: Tiling vs. global context trade-off**
  - Why needed here: Standard ViT-based encoders tile images into fixed-size patches; understanding this helps diagnose why ultra-high-resolution inputs cause performance collapse.
  - Quick check question: *If an image is 10,000×10,000 pixels and a model tiles at 448×448, roughly how many patches result, and what context might be lost?* (Answer: ~500 patches; long-range spatial relationships between distant regions become harder to integrate.)

- **Concept: Perception-to-reasoning taxonomy**
  - Why needed here: RSHR-Bench separates nine perception tasks from four reasoning tasks. Conflating these obscures where models fail.
  - Quick check question: *Why might a model excel at "color detection" but fail at "anomaly detection and interpretation"?* (Answer: The former requires localized attribute extraction; the latter requires causal inference from visual evidence plus domain knowledge.)

## Architecture Onboarding

- **Component map:** Image Sources → Resolution Filtering (≥4K) → Question Generation (Qwen2.5-VL-7B, GPT-5 Thinking) → Human Annotation (boxes, labels) → Human Verification (correctness, ambiguity) → LLM Adversarial Filtering (text-only) → Final QA Pairs: 1,932

- **Critical path:** 1. Image selection with resolution verification (long side ≥4,000 pixels) 2. Bounding box annotation by trained annotators (6 people, ~300 hours) 3. Two-stage verification: human correctness check → LLM adversarial filtering 4. Iterative revision until text-only accuracy drops below threshold

- **Design tradeoffs:** Resolution vs. computational cost (full 3×10^8 pixel images preserve detail but require either aggressive patch tokenization or downsampling); Multiple-choice vs. open-ended (MCQ enables standardized comparison but risks option priors; open-ended avoids this but requires LLM-based scoring); Human vs. automated verification (fully human annotation is costly but highest quality; semi-automated scales to thousands of items)

- **Failure signatures:** High text-only accuracy on filtered benchmark (indicates filtering failed or task relies on commonsense); Sharp accuracy drop at specific resolution thresholds (suggests token budget or memory limit); Large gap between MTEM@1 and per-turn accuracy (reveals inability to maintain coherent reasoning); GeoLLaVA-8K's 0% on MRJC/MRJCS (model may have option bias rather than visual reasoning failure)

- **First 3 experiments:** 1. Baseline text-only probe: Run Qwen3-8B and Llama3-8B on all VQA tasks without images. If >30% accuracy on any task category, investigate whether task design allows shortcuts. 2. Resolution sensitivity analysis: Evaluate your model on RSHR-Bench images downsampled to 2K, 4K, 8K, and native resolution. Plot accuracy vs. resolution to identify where performance collapses. 3. Error analysis by task type: Compare perception vs. reasoning accuracy. If perception is strong but reasoning is weak, focus on vision-language alignment; if both are weak, investigate basic visual encoding failures first.

## Open Questions the Paper Calls Out

- **Question:** How can multimodal architectures be adapted to prevent the severe performance degradation observed when scaling from 4K–8K inputs to 100–200 megapixel images?
- **Basis in paper:** Page 7, Figure 4 (right) and "Results on Single Image Evaluation" section state accuracy drops "markedly at 100M/200M" and remains "uniformly low (around 30%)" across model types. Introduction lists "Long-range structure under high resolution" as a key challenge.
- **Why unresolved:** Current strategies like patch-based processing or token compression often fragment global context or lose fine-grained details necessary for the dense, small objects present in 100MP+ remote sensing scenes.
- **What evidence would resolve it:** Development of a model architecture that maintains consistent accuracy (>50%) across the 4K, 8K, and 200M resolution bins.

## Limitations
- Benchmark relies on proprietary models (GPT-5 Thinking, GPT-5) for generation and evaluation, limiting reproducibility
- 30% threshold for adversarial filtering is somewhat arbitrary and may unfairly penalize commonsense reasoning tasks
- Custom 100MP UAV dataset has limited specification details, making independent validation difficult

## Confidence
- **High Confidence:** Existence of resolution bottlenecks in standard VLMs (tiling at 448×448 cannot handle 100MP images) is well-supported by Table 5 and Figure 4
- **Medium Confidence:** Adversarial filtering mechanism effectively removes language-prior solvable questions, though exact threshold and revision process could be more transparent
- **Medium Confidence:** Multi-turn tasks genuinely expose cascading reasoning failures, though impact varies significantly across models

## Next Checks
1. **Text-only baseline verification:** Run Qwen3-8B and Llama3-8B on all VQA tasks without image access to confirm that filtered questions achieve <30% accuracy while unfiltered tasks show higher text-only performance

2. **Resolution sensitivity analysis:** Evaluate a standard VLM (e.g., InternVL2.5-8B) on RSHR-Bench images downsampled to 2K, 4K, 8K, and native resolution, plotting accuracy curves to identify specific resolution thresholds where performance collapses

3. **Cross-dataset generalization:** Test models on both RSHR-Bench and XLRS-Bench using identical evaluation protocols to determine whether ultra-high-resolution is the primary difficulty factor or if task complexity itself drives performance differences