---
ver: rpa2
title: 'Trans${^2}$-CBCT: A Dual-Transformer Framework for Sparse-View CBCT Reconstruction'
arxiv_id: '2506.17425'
source_url: https://arxiv.org/abs/2506.17425
tags:
- cbct
- reconstruction
- features
- point
- transformer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the challenge of reconstructing high-quality
  cone-beam computed tomography (CBCT) volumes from extremely sparse X-ray projections,
  aiming to reduce radiation dose and scanning time. The authors propose a dual-transformer
  framework, combining TransUNet for extracting multi-scale, globally-aware 2D features
  from sparse projections and a neighbor-aware Point Transformer to enforce 3D spatial
  coherence.
---

# Trans${^2}$-CBCT: A Dual-Transformer Framework for Sparse-View CBCT Reconstruction

## Quick Facts
- arXiv ID: 2506.17425
- Source URL: https://arxiv.org/abs/2506.17425
- Authors: Minmin Yang; Huantao Ren; Senem Velipasalar
- Reference count: 40
- Primary result: Dual-transformer framework combining TransUNet feature extraction with Point Transformer refinement improves sparse-view CBCT reconstruction by 1.8 dB PSNR and 2.8×10-2 SSIM over best baseline

## Executive Summary
This paper addresses the challenge of reconstructing high-quality CBCT volumes from extremely sparse X-ray projections, aiming to reduce radiation dose and scanning time. The authors propose a dual-transformer framework that combines TransUNet for extracting multi-scale, globally-aware 2D features from sparse projections with a neighbor-aware Point Transformer to enforce 3D spatial coherence. The method demonstrates consistent improvements across 6-10 projection views on both LUNA16 and ToothFairy datasets, achieving state-of-the-art performance while maintaining computational efficiency.

## Method Summary
The method processes sparse CBCT projections through a two-stage pipeline. First, TransUNet extracts multi-scale 2D features from each projection image. For each 3D point in the volume, view-specific features are queried via bilinear interpolation and max-pooled across views, then concatenated into a 464-dimensional feature vector. Second, a Point Transformer with 3D positional encodings and k=3 neighbor-aware attention refines these features to improve spatial consistency. The final attenuation head predicts per-point values using a simple Conv1D-BN-ReLU-Sigmoid pipeline. The framework is trained end-to-end with MSE loss and evaluated on PSNR and SSIM metrics.

## Key Results
- Trans2-CBCT achieves 1.8 dB PSNR and 2.8×10-2 SSIM improvement over best baseline on LUNA16 with six views
- Consistent improvements across 6-10 projection views on both LUNA16 and ToothFairy datasets
- Ablation studies confirm the effectiveness of positional encodings and k=3 neighbor attention
- Point Transformer refinement adds 0.63 dB PSNR over Trans-CBCT baseline

## Why This Works (Mechanism)

### Mechanism 1: TransUNet Feature Extraction
- **Claim:** Replacing UNet/ResNet encoders with TransUNet improves sparse-view CBCT reconstruction quality
- **Mechanism:** TransUNet's hybrid CNN-Transformer architecture captures both fine-grained local details (via convolutional layers) and long-range global context (via self-attention layers)
- **Core assumption:** Global context and long-range dependencies are essential for reconstructing coherent volumes from sparse views
- **Evidence anchors:** 1.17 dB PSNR improvement on LUNA16 with six views; TransUNet's robust feature extraction capabilities for segmentation tasks
- **Break condition:** If improvement were solely due to increased model capacity rather than CNN-Transformer hybrid design

### Mechanism 2: 3D Positional Encodings
- **Claim:** Explicit 3D positional encodings enable the network to reason directly about spatial geometry
- **Mechanism:** Each sampled 3D point receives a learnable positional encoding based on its coordinates
- **Core assumption:** Explicit spatial encoding is necessary for the network to understand geometric relationships
- **Evidence anchors:** Positional encoding described in section III-C; performance improvements in spatial consistency
- **Break condition:** Ablation showing no performance drop when removing positional encodings

### Mechanism 3: Neighbor-Aware Attention
- **Claim:** Neighbor-aware attention over k-nearest neighbors enforces local smoothness while preserving detail
- **Mechanism:** Each point aggregates information from only its k=3 nearest neighbors with Gaussian-weighted spatial distance
- **Core assumption:** Local neighborhoods contain sufficient information for refinement
- **Evidence anchors:** k=3 achieves best performance; larger k degrades PSNR/SSIM; KNN-based attention mechanism described in abstract
- **Break condition:** If performance continued improving with larger k or showed no sensitivity to neighborhood size

## Foundational Learning

- **Concept: TransUNet architecture**
  - **Why needed here:** Understanding how CNN-Transformer hybrids work is essential for grasping the 2D feature extraction pipeline
  - **Quick check question:** Can you explain why TransUNet uses skip connections between CNN encoder stages and the upsampling decoder?

- **Concept: Point-based neural representations**
  - **Why needed here:** The method samples discrete 3D points rather than operating on dense voxel grids
  - **Quick check question:** How does querying view-specific features for each 3D point differ from traditional voxel-based backprojection?

- **Concept: Self-attention with positional bias**
  - **Why needed here:** The neighbor-aware attention combines standard attention with distance-based weighting
  - **Quick check question:** How does adding log(w_ij) to attention scores in Equation 7 change the attention distribution compared to standard dot-product attention?

## Architecture Onboarding

- **Component map:** Projection images → TransUNet multi-scale features → point-wise feature querying → Point Transformer refinement → per-point attenuation prediction
- **Critical path:** The pipeline processes sparse projections through TransUNet to extract 4-scale features, queries these features for each 3D point, refines with Point Transformer, and predicts attenuation values
- **Design tradeoffs:** More sampled points improve reconstruction up to 10,000 then degrade; k=3 neighbor attention optimal; Trans2-CBCT adds 0.63 dB PSNR but increases inference time 5×
- **Failure signatures:** Blurry reconstructions with lost fine structures (k too large); streak artifacts (projection geometry issues); slow inference (KNN computation dominating)
- **First 3 experiments:**
  1. Reproduce Trans-CBCT baseline on 6-view LUNA16, verify ~30.40 dB PSNR
  2. Ablate positional encoding, measure performance drop
  3. Vary k in neighbor attention (k ∈ {1, 3, 6, 9}), verify k=3 optimum

## Open Questions the Paper Calls Out
None

## Limitations
- Inference time for Trans2-CBCT (53.7s/case) is significantly slower than Trans-CBCT (9.9s/case) due to KNN computation
- Limited evaluation on only two datasets (LUNA16 and ToothFairy) with specific imaging parameters
- Unclear whether performance gains are due to CNN-Transformer hybrid architecture versus increased model capacity

## Confidence
- **High:** Effectiveness of Point Transformer's positional encoding and neighbor-aware attention mechanisms
- **Medium:** Contribution of TransUNet encoder to overall performance gains
- **Low:** Generalizability across different CBCT datasets and imaging parameters

## Next Checks
1. **Ablate TransUNet:** Replace TransUNet with standard UNet and retrain Trans-CBCT baseline to isolate CNN-Transformer contribution
2. **Test on unseen dataset:** Evaluate on third CBCT dataset with different anatomy (e.g., pelvis) to assess generalizability
3. **Vary projection angles:** Test with non-uniform projection distributions (e.g., [0°, 90°, 180°]) to evaluate robustness to angular sampling patterns