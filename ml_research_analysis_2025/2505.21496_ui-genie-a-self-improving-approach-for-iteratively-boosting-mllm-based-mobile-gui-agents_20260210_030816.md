---
ver: rpa2
title: 'UI-Genie: A Self-Improving Approach for Iteratively Boosting MLLM-based Mobile
  GUI Agents'
arxiv_id: '2505.21496'
source_url: https://arxiv.org/abs/2505.21496
tags:
- reward
- agent
- task
- action
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: UI-Genie is a self-improving framework that addresses the challenges
  of verifying GUI agent trajectories and scaling high-quality training data. It introduces
  UI-Genie-RM, a specialized reward model with image-text interleaved architecture
  for accurate action-level and task-level reward evaluation.
---

# UI-Genie: A Self-Improving Approach for Iteratively Boosting MLLM-based Mobile GUI Agents

## Quick Facts
- arXiv ID: 2505.21496
- Source URL: https://arxiv.org/abs/2505.21496
- Reference count: 40
- UI-Genie achieves state-of-the-art performance on GUI agent benchmarks, with success rates up to 77.0% on AndroidLab and 20.4% on Android Arena

## Executive Summary
UI-Genie addresses the challenge of generating high-quality training data for mobile GUI agents by introducing a self-improving framework that iteratively enhances both agent and reward models. The framework introduces UI-Genie-RM, a specialized reward model with image-text interleaved architecture that processes historical context to evaluate GUI actions accurately. Through reward-guided exploration and process supervision, UI-Genie generates synthetic trajectories without manual annotation, achieving superior performance on GUI benchmarks while producing the first large-scale reward dataset and synthetic trajectory dataset for GUI agents.

## Method Summary
UI-Genie employs a self-improving framework that iteratively trains both a GUI agent and a reward model. The reward model, UI-Genie-RM, uses an image-text interleaved architecture to process the last five screenshots alongside action summaries, enabling context-aware reward evaluation. The framework generates synthetic trajectories through reward-guided beam search, where successful trajectories expand the agent training data and failed trajectories contribute labeled intermediate steps via continuation rollouts. This process repeats for three rounds, with each round introducing increasingly complex tasks. The approach produces large-scale datasets (UI-Genie-RM-517k and UI-Genie-Agent-16k) without manual annotation, leveraging rule-based verification, trajectory corruption, and hard negative mining strategies.

## Key Results
- Achieves state-of-the-art performance with 77.0% success rate on AndroidLab and 20.4% on Android Arena
- Demonstrates iterative improvement across three rounds, with success rates increasing from 18.1% to 38.7%
- Produces first large-scale reward dataset (UI-Genie-RM-517k) and synthetic trajectory dataset (UI-Genie-Agent-16k) without manual annotation

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Processing historical screenshot-action context enables accurate GUI action evaluation that single-frame approaches cannot achieve.
- **Mechanism:** The image-text interleaved architecture maintains the 5 most recent screenshots alongside summarized action descriptions, allowing the model to track task progress and understand action dependencies across time.
- **Core assumption:** GUI action correctness is context-dependent; the same action can be correct or incorrect depending on prior steps.
- **Evidence anchors:**
  - [abstract] "features an image-text interleaved architecture that efficiently processes historical context and unifies action-level and task-level rewards"
  - [section 3.1] "the correct action in Notes (typing the headline) depends on knowing what was seen earlier in News, not just the present screenshot"
  - [section 4.3.2] Ablation shows performance drops from 79.6% to 61.3% when removing historical information

### Mechanism 2
- **Claim:** Reward-guided beam search efficiently discovers successful trajectories where Monte Carlo Tree Search (MCTS) fails.
- **Mechanism:** GUI action spaces contain many "neutral" actions (clicking blank areas) that don't change state. Beam search with top-5 path retention based on cumulative rewards avoids wasted exploration on invalid actions while MCTS would allocate computation to branches that don't progress.
- **Core assumption:** Invalid actions that maintain the last state are common enough to significantly impact search efficiency.
- **Evidence anchors:**
  - [section 3.3] "conventional MCTS fails to effectively address the unique characteristics of GUI interaction spaces... GUI interactions often include invalid actions that maintain the last state"

### Mechanism 3
- **Claim:** Iterative co-improvement of agent and reward models creates a positive feedback loop for handling increasingly complex tasks.
- **Mechanism:** Successful trajectories expand agent training data; failed trajectories contribute correctly-labeled intermediate steps via continuation rollouts to reward model training. Each generation introduces harder tasks (initial → expansion → complex), so improved models from earlier rounds enable solving previously unsolvable tasks.
- **Core assumption:** Intermediate steps from failed trajectories can be correctly labeled by checking if continuation rollouts succeed.
- **Evidence anchors:**
  - [abstract] "iteratively improves both agent and reward models via process supervision and outcome verification"
  - [section 4.3.3] Success rate improves from 18.1% (round 0) to 38.7% (round 3); reward accuracy from 68.2% to 79.6%

## Foundational Learning

- **Concept: Process-supervision reward models (PRMs)**
  - **Why needed here:** UI-Genie-RM provides step-level rewards rather than just trajectory-level outcomes, enabling finer-grained guidance during exploration.
  - **Quick check question:** Can you explain why labeling intermediate steps via continuation rollouts is more scalable than human annotation?

- **Concept: Next-token prediction as reward formulation**
  - **Why needed here:** The paper formulates reward prediction as predicting ⟨|+|⟩ or ⟨|−|⟩ tokens rather than regression, leveraging MLLM training infrastructure.
  - **Quick check question:** How does the binary classification objective L_RM in Equation 1 differ from traditional reward modeling via scalar value prediction?

- **Concept: Hard negative mining**
  - **Why needed here:** Initial reward model correctly classifies easy negatives but struggles on ambiguous cases; mining its false positives as training data improves discrimination.
  - **Quick check question:** Why would samples incorrectly classified as positive by an initial model make especially valuable negative training examples?

## Architecture Onboarding

- **Component map:** AndroidControl/AMEX/AndroidLab datasets → Rule-based verification → Trajectory corruption → Hard negative mining → UI-Genie-RM training → Initial agent training → Reward-guided beam search → Process supervision → Iterative fine-tuning → UI-Genie-Agent-16k

- **Critical path:**
  1. Train initial agent on AndroidControl/AMEX/AndroidLab
  2. Construct 458k reward training samples via rule-based + corruption + hard negative strategies
  3. Train UI-Genie-RM on 458k + collect 59k process-reward samples during exploration
  4. Run 3 rounds of reward-guided beam search (10 candidates, retain top-5), outcome verification, dataset expansion, fine-tuning

- **Design tradeoffs:**
  - Context window: 5 screenshots balances coverage vs. computation; earlier actions summarized as text
  - Unified reward architecture handles both action-level and task-level via "task completion" as special action vs. separate specialized models
  - Full fine-tuning for 3B/7B models; LoRA (rank 64, alpha 256) for 72B due to memory constraints

- **Failure signatures:**
  - Reward model provides suboptimal signals → failed trajectories contaminate training data
  - Early truncation in beam search → misses solutions requiring longer exploration
  - Hard tasks (>10 steps) show degraded reward accuracy (68.7% vs. 83.5% for easy tasks)

- **First 3 experiments:**
  1. Reproduce ablation on historical context depth (1 vs. 3 vs. 5 screenshots) on held-out AndroidControl subset to validate context scaling
  2. Test best-of-N sampling with N=5,10,20 to verify reward model discriminative power scales with candidate pool size
  3. Run single self-improvement round on new app not in training data to assess generalization of the co-improvement loop

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can the reliability of reward model supervision be improved to prevent error accumulation during iterative data expansion?
- **Basis in paper:** [explicit] The authors state in the Limitations section that the reward model "may occasionally generate suboptimal rewards signals," which results in failed trajectories being incorporated into training data.
- **Why unresolved:** The current framework relies on the reward model for its own training data generation; noise in this loop creates a feedback risk that is acknowledged but not fully solved.
- **What evidence would resolve it:** A demonstration of a noise-filtering mechanism or a robust outlier detection method that maintains data quality even with an imperfect reward model.

### Open Question 2
- **Question:** Is the fixed historical context window of five screenshots sufficient for maintaining state awareness in extremely long-horizon tasks?
- **Basis in paper:** [inferred] The Method section specifies using "only the five most recent screenshots" to balance cost and context, assuming summaries cover earlier steps. This represents a potential information bottleneck for complex tasks where visual state is critical.
- **Why unresolved:** Summaries may lose fine-grained visual details necessary for disambiguating UI states in tasks spanning dozens of steps.
- **What evidence would resolve it:** Ablation studies on tasks with extended horizons (e.g., 20+ steps) comparing fixed-window performance against full-history or adaptive-context approaches.

### Open Question 3
- **Question:** Can the framework's dependency on large-scale computation be reduced to mitigate the environmental impact noted by the authors?
- **Basis in paper:** [explicit] Under Broader Impact, the authors list "high computation cost" and "significant carbon emission" as negative societal consequences of the training methodology.
- **Why unresolved:** The iterative self-improvement pipeline involves repeated fine-tuning of large models (up to 72B parameters) and extensive exploration rollouts, which are inherently resource-intensive.
- **What evidence would resolve it:** Research into distillation techniques or efficient fine-tuning methods (e.g., parameter-efficient tuning) that maintain the "data-model co-evolution" performance without the full computational load.

## Limitations
- Reward model reliability concerns may lead to error accumulation during iterative data expansion
- Fixed five-screenshot context window may be insufficient for extremely long-horizon tasks
- High computation costs and significant carbon emissions due to iterative fine-tuning of large models

## Confidence
- **High confidence:** The architectural innovations (image-text interleaved context, next-token reward prediction) are well-specified and technically sound.
- **Medium confidence:** The self-improvement loop mechanics and dataset construction strategies appear feasible but depend heavily on implementation details not fully disclosed.
- **Low confidence:** Claims about scaling to truly complex GUI tasks (>10 steps) and generalization to unseen apps are based on limited empirical evidence.

## Next Checks
1. **Context Window Scaling Experiment:** Systematically test UI-Genie-RM performance with context windows of 1, 3, 5, and 10 screenshots on a subset of AndroidControl tasks requiring different memory lengths. Measure how performance degrades as tasks exceed the 5-screenshot limit.

2. **Cross-App Generalization Test:** Deploy the round-3 models on a completely new app not represented in any training data (e.g., a banking or shopping app). Evaluate success rates and analyze failure modes to determine if the self-improvement loop generalizes beyond its training distribution.

3. **Error Amplification Analysis:** Track the distribution of training samples across self-improvement rounds, specifically monitoring the ratio of samples from initially failed vs. initially successful trajectories. Correlate this with reward model accuracy changes to quantify potential error accumulation through the iterative process.