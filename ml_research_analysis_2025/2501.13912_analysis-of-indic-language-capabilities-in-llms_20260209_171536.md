---
ver: rpa2
title: Analysis of Indic Language Capabilities in LLMs
arxiv_id: '2501.13912'
source_url: https://arxiv.org/abs/2501.13912
tags:
- language
- languages
- arxiv
- indic
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study analyzes the capabilities of 28 large language models
  in Indic languages to guide future benchmark development. Researchers reviewed existing
  evaluations and datasets, then assessed models based on training data, licensing,
  access type, and developer origin.
---

# Analysis of Indic Language Capabilities in LLMs

## Quick Facts
- arXiv ID: 2501.13912
- Source URL: https://arxiv.org/abs/2501.13912
- Authors: Aatman Vaidya; Tarunima Prabhakar; Denny George; Swair Shah
- Reference count: 40
- This study analyzes the capabilities of 28 large language models in Indic languages to guide future benchmark development.

## Executive Summary
This study systematically analyzes the capabilities of 28 large language models across 29 Indic languages to identify priority languages for future safety benchmarks. The research reveals that Hindi is the most widely represented language in models, with top-five languages (Hindi, Bengali, Marathi, Telugu, Tamil) showing relatively strong performance. However, performance for other Indic languages varies unpredictably and correlates poorly with speaker numbers, indicating that digital data scarcity rather than population size drives model competence. The analysis also highlights that most evaluation datasets are translated from English, potentially overestimating true linguistic and cultural competence in Indic languages.

## Method Summary
The study employed desk research using an Ecosystem Graphs framework to catalog 28 LLMs' attributes including training data composition, licensing, and developer origin. Researchers systematically reviewed 14 evaluation papers covering 12 major Indic languages, extracting performance metrics across NLU and NLG tasks. The analysis categorized languages by performance levels (high/medium/low) based on aggregated scores from benchmarks like IndicGenBench, MEGA, and IndicXTREME. The team also examined training data sources including Common Crawl, Wikipedia, and custom Indic corpora to assess representation patterns across languages.

## Key Results
- Hindi is the most widely represented language in models, with top-five languages (Hindi, Bengali, Marathi, Telugu, Tamil) showing relatively strong performance
- Performance for Indic languages beyond the top five varies unpredictably and correlates poorly with speaker numbers
- Evaluation studies mostly cover 12 major languages using translated English datasets, revealing a significant gap between high- and low-resource languages

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Higher representation in pre-training corpora (Common Crawl, Wikipedia) is a likely causal driver of superior performance in high-resource Indic languages.
- **Mechanism:** LLMs rely on statistical patterns learned during pre-training. Languages like Hindi and Bengali have higher token percentages in datasets like mC4 and Common Crawl, allowing the model to form robust semantic and syntactic representations compared to languages with sparse data.
- **Core assumption:** Performance metrics on standard NLU/NLG tasks are valid proxies for the model's internal representation quality of a language.
- **Evidence anchors:** Abstract notes Hindi is most widely represented with performance varying unpredictably for others; Section 4.1 attributes performance gaps to lack of pre-training data; BhashaKritika paper confirms synthetic data is explored to fix uneven distribution.

### Mechanism 2
- **Claim:** Current evaluation methods likely overestimate true cultural and linguistic competence for non-English languages due to "translation artifacts."
- **Mechanism:** Many evaluation datasets are machine-translated from English. High scores on these benchmarks may indicate the model is merely good at translation alignment rather than possessing deep cultural understanding or reasoning capabilities in the native Indic language.
- **Core assumption:** High scores on translated benchmarks do not equate to "socio-cultural understanding" or real-world utility.
- **Evidence anchors:** Section 4.1 notes that translated datasets are a serious limitation as models might score well but fare poorly on tasks requiring cultural context; abstract confirms evaluation studies use translated English datasets.

### Mechanism 3
- **Claim:** "Transfer learning" from high-resource languages (English/Hindi) works for top-tier Indic languages but fails for extremely low-resource languages lacking linguistic cousins or shared scripts.
- **Mechanism:** Multilingual models use cross-lingual attention. If a language shares tokens or linguistic structures with high-resource data, the model transfers knowledge effectively. If a language is isolated, the model cannot bridge the gap.
- **Core assumption:** The architecture has sufficient capacity to store distinct language representations, but the data acts as the bottleneck for transfer.
- **Evidence anchors:** Section 4.1 cites Doddapaneni et al. attributing performance gaps to absence of linguistic cousins; Section 3.2 notes Common Crawl is majority English with Indic languages comprising only 0.5-7% of data.

## Foundational Learning

- **Concept: Resource Scarcity vs. Speaker Population**
  - **Why needed here:** Engineers often prioritize languages by market size. This paper proves this is flawed; Oriya has more speakers than Malayalam but performs worse due to less digital data.
  - **Quick check question:** When selecting a target language, do you check the "Common Crawl" token percentage or the Census population data?

- **Concept: Tokenizer Fertility**
  - **Why needed here:** Different Indic languages break into sub-words differently. High fertility (many tokens per word) increases sequence length, degrading context window efficiency and performance.
  - **Quick check question:** Does your tokenizer require 3x more tokens for Tamil than English to say the same sentence?

- **Concept: Benchmark Contamination & Native Evaluation**
  - **Why needed here:** Simply translating MMLU or HELM into Hindi might test the model's translation memory, not its reasoning.
  - **Quick check question:** Is your evaluation dataset a machine translation of an English benchmark, or was it written natively in the target language?

## Architecture Onboarding

- **Component map:** Foundation (Global Models) -> Mid-layer (Indic-specific Pre-training) -> Top-layer (Instruction Tuning) -> Evaluation (Benchmarks)
- **Critical path:**
  1. Audit Data: Verify % of Indic tokens in training using Common Crawl stats
  2. Select Candidate: Prioritize the "High Performing" group (Hindi, Bengali, Marathi, Telugu, Tamil) for safety/capability pilots
  3. Native Testing: Validate performance on non-translated prompts to ensure cultural grounding
- **Design tradeoffs:**
  - Synthetic Data: Using LLM-generated data increases volume but risks model collapse or drift from native vernacular
  - Translation vs. Native Eval: Translated benchmarks are cheap and easy to compare; native benchmarks are expensive but accurate for cultural nuance
- **Failure signatures:**
  - Hallucination in Translation: Model gives vague or different answers when same query is translated
  - Script Confusion: Model fails to distinguish between languages sharing scripts if not fine-tuned properly
- **First 3 experiments:**
  1. Tokenizer Fertility Test: Compare token count for standard legal document in English vs. Hindi vs. Tamil
  2. Top-5 Baseline: Run high-performing languages against native dataset to establish "real" baseline vs. English
  3. Safety Probe: Execute safety benchmarks specifically on Hindi and Tamil to verify safety alignment transfers

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How should safety benchmarks prioritize Indic languages where model performance is low but potential real-world user volume is high?
- Basis in paper: The authors state that for languages beyond the top five, "performance... may be low. Yet, the number of people using the model in the language may be higher and therefore repercussion from unsafe usage might be greater."
- Why unresolved: The authors find it "difficult to unequivocally recommend languages beyond the five most widely spoken" for safety benchmarks due to these complexities.
- What evidence would resolve it: A risk-analysis framework mapping user demographics against model failure rates for specific medium and low-resource languages.

### Open Question 2
- Question: To what extent do translated evaluation datasets mask deficiencies in the socio-cultural understanding of Indic LLMs?
- Basis in paper: Section 4.1 notes that "a majority of these datasets are a direct translation of existing English datasets which is a serious limitation," as models may score well but fail on tasks requiring cultural context.
- Why unresolved: The authors note this limitation but rely on these datasets for the current analysis, highlighting the absence of culturally native benchmarks.
- What evidence would resolve it: Comparative performance data using new benchmarks created natively in Indic languages rather than translated from English.

### Open Question 3
- Question: What are the primary drivers of performance unpredictability in Indic languages beyond the top five?
- Basis in paper: The paper observes that performance "correlates poorly with speaker numbers" and varies unpredictably after the top tier.
- Why unresolved: While the paper notes the gap is due to training data, it does not determine if unpredictability is driven more by lack of data, tokenization efficiency, or lack of linguistic relatives.
- What evidence would resolve it: Ablation studies analyzing correlation between specific training corpus attributes and performance variance for these languages.

## Limitations
- Reliance on reported metrics from existing studies rather than direct experimentation introduces measurement inconsistencies
- Performance classifications depend on aggregating heterogeneous benchmarks that vary in quality and whether they use translated or native datasets
- Detailed training data composition is often unavailable for closed-source models like GPT-4 or Claude 3.5

## Confidence
- **High Confidence:** Identification of Hindi, Bengali, Marathi, Telugu, and Tamil as top five performing Indic languages is well-supported by multiple evaluation studies
- **Medium Confidence:** Causal link between web-corpus representation and superior performance is plausible but not definitively proven due to synthetic data use in some models
- **Medium Confidence:** Concern that translated benchmarks overestimate true linguistic competence is reasonable but direct empirical evidence is limited

## Next Checks
1. **Data Transparency Audit:** For each of the 28 models, attempt to obtain or estimate precise Indic language token percentages from training corpora to validate reported performance gaps
2. **Native vs. Translated Benchmark Comparison:** Select a subset of models and run them on both translated (e.g., IndicXNLI) and native (e.g., MILU) benchmarks for the same languages to quantify overestimation effect
3. **Safety Alignment Transfer Test:** Evaluate safety alignment benchmarks specifically on Hindi and Tamil to determine if safety capabilities transfer from English as effectively as linguistic fluency does