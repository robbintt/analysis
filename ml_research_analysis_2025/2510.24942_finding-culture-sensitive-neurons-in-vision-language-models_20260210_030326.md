---
ver: rpa2
title: Finding Culture-Sensitive Neurons in Vision-Language Models
arxiv_id: '2510.24942'
source_url: https://arxiv.org/abs/2510.24942
tags:
- neurons
- culture
- cultural
- language
- cultures
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates the presence of culture-sensitive neurons
  in vision-language models (VLMs) by analyzing their behavior on culturally diverse
  visual question answering tasks. Using the CVQA benchmark, the authors develop a
  three-stage pipeline to identify neurons whose activations are preferentially modulated
  by specific cultural contexts and assess their importance through targeted ablation
  experiments across three VLMs (Qwen2.5-VL-7B, Pangea-7B, and LLaVA-v1.6-Mistral-7B)
  and 25 cultural groups.
---

# Finding Culture-Sensitive Neurons in Vision-Language Models

## Quick Facts
- arXiv ID: 2510.24942
- Source URL: https://arxiv.org/abs/2510.24942
- Authors: Xiutian Zhao; Rochelle Choenni; Rohit Saxena; Ivan Titov
- Reference count: 33
- Key outcome: Culture-sensitive neurons identified in VLMs show selective performance degradation when ablated, with CAS method outperforming existing approaches.

## Executive Summary
This study investigates the presence of culture-sensitive neurons in vision-language models (VLMs) by analyzing their behavior on culturally diverse visual question answering tasks. Using the CVQA benchmark, the authors develop a three-stage pipeline to identify neurons whose activations are preferentially modulated by specific cultural contexts and assess their importance through targeted ablation experiments across three VLMs (Qwen2.5-VL-7B, Pangea-7B, and LLaVA-v1.6-Mistral-7B) and 25 cultural groups. The authors introduce Contrastive Activation Selection (CAS), a margin-based method that outperforms existing probability- and entropy-based approaches in isolating culture-sensitive neurons. Results show that ablating these neurons disproportionately reduces model performance on questions tied to the corresponding culture while having minimal effects on others, with the largest self-deactivation accuracy drops observed for CAS (Qwen2.5-VL-7B: -5.52%; Pangea-7B: -4.33%). Layer-wise analyses reveal that culture-sensitive neurons cluster in mid-to-late decoder layers, with patterns largely consistent across models and cultures. These findings provide empirical evidence for culture-sensitive neurons in VLMs and demonstrate their causal role in culturally grounded information processing, offering insights for targeted evaluation and intervention to mitigate cultural biases.

## Method Summary
The authors employ a three-stage pipeline to identify and validate culture-sensitive neurons in VLMs. First, they record decoder MLP activations from the SwiGLU nonlinearity branch on correctly answered CVQA samples across 25 culture groups. Second, they score neurons using multiple methods including their proposed Contrastive Activation Selection (CAS), which computes the margin between a neuron's top culture and second-best culture. Third, they perform targeted ablation by zeroing the top r% (1%) of neurons identified for each culture and measure the resulting performance changes. The intervention tests whether ablating culture-sensitive neurons disproportionately harms performance on the associated culture while preserving performance on others, using accuracy drops and flip rates as primary metrics.

## Key Results
- CAS outperforms existing probability- and entropy-based methods in isolating culture-sensitive neurons
- Ablating culture-sensitive neurons causes self-deactivation accuracy drops of -5.52% (Qwen2.5-VL-7B) and -4.33% (Pangea-7B) with minimal cross-cultural spillover
- Culture-sensitive neurons cluster in mid-to-late decoder layers (layers 6-8) across different models
- Largest effects observed when comparing self-deactivation vs cross-deactivation performance gaps

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Contrastive Activation Selection (CAS) more effectively isolates culture-sensitive neurons in high-variance VLMs than mean-difference or entropy methods.
- Mechanism: CAS scores neurons by the margin between their top-responding culture and the second-best culture, rather than deviation from the mean. This reduces false positives from neurons with high intrinsic activation variance unrelated to cultural specialization.
- Core assumption: Culture specialization manifests as a preference gap between the top culture and competitors, not just high average activation.
- Evidence anchors:
  - [abstract] "we propose a new margin-based selector—Contrastive Activation Selection (CAS), and show that it outperforms existing probability- and entropy-based methods"
  - [Section 3.4] "preliminary analysis revealed that in QWEN2.5-VL-7B and PANGEA-7B, a substantial fraction of neurons (12.27% and 9.57%) exhibit high activation variance across cultures"
  - [corpus] Corpus provides limited corroboration—no prior work on margin-based neuron selection for culture in VLMs; this appears novel.
- Break condition: If activation variance across cultures is low, CAS and MAD should identify similar neurons (acknowledged in Section 3.4).

### Mechanism 2
- Claim: Ablating culture-sensitive neurons selectively degrades performance on the associated culture with minimal cross-cultural spillover.
- Mechanism: These neurons encode or gate culture-specific visual-concept associations (e.g., recognizing Tilgul with Makar Sankrant). Zeroing their activations disrupts this pathway while leaving shared multimodal processing intact, as evidenced by preserved instruction-following and format compliance post-ablation.
- Core assumption: Causal contribution is localized enough that a small subset (1% of MLP neurons) can meaningfully affect culture-specific outputs.
- Evidence anchors:
  - [abstract] "ablating these neurons disproportionately reduces model performance on questions tied to the corresponding culture while having minimal effects on others"
  - [Section 5.2, Table 2] CAS self-deactivation accuracy drops: Qwen2.5-VL-7B: -5.52%, Pangea-7B: -4.33%, with cross-deactivation <1%
  - [Figure 1] Example showing model still follows instruction format after ablation but selects incorrect cultural answer.
- Break condition: If cultural knowledge is diffusely distributed (as suggested for LLaVA-v1.6-Mistral-7B), small ablations yield smaller self–cross gaps.

### Mechanism 3
- Claim: Culture-sensitive neurons cluster in early-to-mid decoder layers because cultural reasoning requires integrated multimodal representations.
- Mechanism: Early layers perform basic feature fusion; mid layers handle semantic integration where cultural concepts (food–occasion associations, regional objects) are resolved. Late layers focus on output formatting rather than knowledge retrieval.
- Core assumption: Layer position reflects functional role in information processing hierarchy.
- Evidence anchors:
  - [Section 5.3] "culture-sensitive neurons generally cluster in the first layer (layer 0) and the early-mid layers (6–8), with relatively sparse presence in deeper blocks"
  - [Figure 5] CAS identifies neurons more evenly across mid-to-late layers than MAD, which bypasses central layers (15–18).
  - [corpus] No direct corpus confirmation for cultural neuron layer distribution in VLMs; related work focuses on modality-specific neuron localization.
- Break condition: Different architectures may redistribute this pattern; LLaVA shows more diffuse cultural encoding (Section D.2).

## Foundational Learning

- Concept: SwiGLU MLP architecture in transformers
  - Why needed here: The paper instruments the nonlinearity branch (g = SiLU(u)) of SwiGLU blocks to record neuron activations; understanding this structure is required to implement hooks correctly.
  - Quick check question: Which branch of SwiGLU produces the scalar activations used for culture-sensitivity scoring—the gated nonlinearity or the linear projection?

- Concept: Activation-based neuron interpretability (network dissection)
  - Why needed here: The entire pipeline depends on correlating neuron activation patterns with input properties (culture labels) to infer functional specialization.
  - Quick check question: Why might high average activation for a culture be insufficient to identify culture-sensitive neurons?

- Concept: Ablation as causal intervention
  - Why needed here: Correlation (activation patterns) does not establish causation; ablation tests whether identified neurons are necessary for culture-specific performance.
  - Quick check question: What does a small self–cross gap indicate about the neuron selection method?

## Architecture Onboarding

- Component map: CVQA sample -> VLM forward pass -> hook records g activations -> aggregate K(c), S(c), T(c) per neuron–culture -> compute P(c), M(c) -> apply selector -> generate mask -> masked inference -> evaluate accuracy change and flip rate.

- Critical path: CVQA sample → VLM forward pass → hook records g activations → aggregate K(c), S(c), T(c) per neuron–culture → compute P(c), M(c) → apply selector → generate mask → masked inference → evaluate accuracy change and flip rate.

- Design tradeoffs:
  - r% selection threshold: Paper uses 1%; smaller values increase specificity but risk missing important neurons; larger values increase spillover.
  - Monolingual (English) setting: Controls for language proficiency but may miss culture–language interactions.
  - Decoder-only instrumentation: Excludes vision encoder and attention heads, which may also encode cultural signals.

- Failure signatures:
  - High cross-deactivation spillover: Indicates selector captured shared rather than culture-specific features (observed with LAP).
  - No self–cross gap: Suggests cultural knowledge is diffusely encoded (observed with LLaVA-v1.6-Mistral-7B).
  - Near-zero baseline accuracy on a culture: Insufficient correct samples for activation statistics (noted for JPN subset in Pangea).

- First 3 experiments:
  1. Reproduce CAS scoring on a single culture subset (e.g., India-Marathi) with r=1% and verify self-deactivation accuracy drop exceeds cross-deactivation by >3 percentage points.
  2. Sweep r ∈ {0.5, 1, 2, 5}% to characterize the tradeoff between self-impact and cross-spillover.
  3. Compare layer-wise histograms of CAS-identified neurons across two architectures (Qwen2.5-VL vs. LLaVA) to confirm clustering patterns differ by model.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the identified culture-sensitive neurons be utilized for activation steering to mitigate bias or enhance cultural alignment without retraining?
- Basis: [explicit] The conclusion states future work should "pair identification with activation steering."
- Why unresolved: The current study focuses exclusively on ablation (deactivation) to demonstrate causal importance rather than modifying activations to steer behavior.
- What evidence would resolve it: Experiments showing that adding specific activation vectors to these neurons adjusts model outputs toward desired cultural behaviors.

### Open Question 2
- Question: Are culture-sensitive neurons present in model components other than decoder MLPs, such as vision encoders or attention heads?
- Basis: [explicit] The Limitations section states the analysis "is restricted to decoder MLP neurons and does not cover attention heads, vision encoders."
- Why unresolved: The current methodology instruments only the SwiGLU non-linearity branch, ignoring other potential locations for cultural encoding.
- What evidence would resolve it: Applying the CAS pipeline to attention layers and the visual backbone to determine if they exhibit similar culture-selective patterns.

### Open Question 3
- Question: Do the observed neuron distributions and ablation effects persist when models process inputs in their native languages rather than English translations?
- Basis: [explicit] The Limitations section notes that due to the English-only constraint, "it remains unknown whether our observations would still emerge" in multilingual contexts.
- Why unresolved: Experiments were strictly monolingual to decouple language proficiency from cultural recognition.
- What evidence would resolve it: Replicating the identification and intervention experiments using non-English subsets of the CVQA benchmark.

## Limitations

- The study's findings are constrained by the monolingual (English) setting of CVQA, which limits generalizability to multilingual or code-switched cultural contexts where language and culture are tightly coupled.
- The causal attribution via ablation is based on a relatively small subset of neurons (1% of MLP neurons), raising questions about whether identified neurons are necessary, sufficient, or merely correlated with cultural performance.
- The layer-wise clustering patterns are consistent across models but not explicitly validated against alternative architectures or culture-specific attention mechanisms.

## Confidence

- **High**: Culture-sensitive neurons can be identified through activation-based methods and causally validated via ablation (supported by self-deactivation accuracy drops of -4.33% to -5.52% with minimal cross-spillover).
- **Medium**: CAS outperforms entropy-based methods in isolating culture-specific neurons, particularly in high-variance models (consistent with margin-based advantage for noisy distributions).
- **Medium**: Culture-sensitive neurons cluster in mid-to-late decoder layers (consistent with multimodal integration hypothesis, though not explicitly tested for causality).

## Next Checks

1. Replicate ablation experiments with r ∈ {0.5, 2, 5}% to assess the tradeoff between cultural specificity and functional preservation.
2. Apply CAS to a multilingual VQA benchmark (e.g., MM-CBC) to test whether culture-sensitive neurons are language-dependent or invariant.
3. Instrument attention heads in addition to MLPs to determine if cultural encoding occurs in cross-attention or vision encoder layers.