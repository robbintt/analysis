---
ver: rpa2
title: Enhancing Disinformation Detection with Explainable AI and Named Entity Replacement
arxiv_id: '2502.04863'
source_url: https://arxiv.org/abs/2502.04863
tags:
- news
- dataset
- detection
- fake
- disinformation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of detecting disinformation
  in text, highlighting issues with current methods that rely on surface-level features
  and biased training data. The authors propose a two-stage post-hoc explainable AI
  approach using SHAP (SHapley Additive exPlanations) to identify spurious features
  in disinformation detection models.
---

# Enhancing Disinformation Detection with Explainable AI and Named Entity Replacement

## Quick Facts
- arXiv ID: 2502.04863
- Source URL: https://arxiv.org/abs/2502.04863
- Reference count: 40
- Primary result: Post-hoc explainability with SHAP and Named Entity Replacement improves disinformation detection generalizability by 65.78% average F1-score on external datasets

## Executive Summary
This paper addresses the challenge of detecting disinformation in text, highlighting issues with current methods that rely on surface-level features and biased training data. The authors propose a two-stage post-hoc explainable AI approach using SHAP (SHapley Additive exPlanations) to identify spurious features in disinformation detection models. They find that models often focus on non-informative elements like URLs, emoticons, and named entities (e.g., "Rwanda," "Netflix"), which can introduce bias and reduce generalizability. To address this, they apply extended preprocessing to remove spurious features and replace named entities with their categories (e.g., "Rwanda" → "LOC"). The enhanced model is evaluated on three COVID-19-related datasets, showing significant improvements in external validation: an average 65.78% increase in F1-score without a significant drop in internal performance. This demonstrates that the approach improves model transparency, reduces bias, and enhances generalizability in disinformation detection.

## Method Summary
The authors employ a two-stage post-hoc explainable AI approach to improve disinformation detection. First, they use SHAP values to identify spurious features that inappropriately influence classification decisions. Second, they apply extended preprocessing that removes non-informative elements (URLs, emoticons, hashtags, @mentions) and replaces named entities with category tokens using Named Entity Recognition (NER). The methodology is evaluated using a RoBERTa base model fine-tuned on COVID-19 disinformation datasets, with performance measured through both internal and external validation across three different datasets.

## Key Results
- SHAP analysis revealed that disinformation models often rely on spurious features like URLs, emoticons, and named entities (e.g., "Rwanda," "Netflix") rather than semantic content
- Extended preprocessing (removing spurious features + Named Entity Replacement) improved external validation F1-score by 65.78% on average across three datasets
- Internal performance remained stable (97.80% F1) despite preprocessing, while external validation improved from 38.93% to 65.78% F1
- Named Entity Replacement specifically addressed entity-frequency overfitting by substituting specific entities (e.g., "Rwanda") with category tokens (e.g., "LOC")

## Why This Works (Mechanism)

### Mechanism 1: Post-hoc Explainability for Spurious Feature Attribution
- Claim: SHAP values can identify which features inappropriately influence disinformation classification decisions.
- Mechanism: SHAP calculates the marginal contribution of each token to the model's prediction across all possible feature coalitions. High SHAP values for non-semantic tokens (punctuation, URL placeholders) or rare named entities reveal spurious correlations the model has learned from training data distribution rather than genuine disinformation indicators.
- Core assumption: Tokens with high attribution that lack semantic connection to veracity represent dataset artifacts rather than learnable disinformation patterns.
- Evidence anchors:
  - [abstract] "we apply a post-hoc explainability method (SHAP, SHapley Additive exPlanations) to identify spurious elements with high impact on the classification models"
  - [section 3.2] Figure 3 shows "$" and "URL" among highest-importance tokens; model attends to geographic entities ("southern") and organizations ("cdc") inappropriately
  - [corpus] Limited direct corpus evidence; corpus papers focus on multi-agent detection systems and persuasion-based approaches rather than SHAP-based feature analysis
- Break condition: If SHAP attributions are inconsistent across different model architectures for identical inputs, the identified "spurious" features may be model-specific artifacts rather than genuine dataset biases.

### Mechanism 2: Spurious Feature Removal Forces Distributional Robustness
- Claim: Eliminating non-informative elements (URLs, emoticons, hashtags, @mentions) compels models to learn from semantic content that transfers across datasets.
- Mechanism: Preprocessing removes tokens that serve as dataset-specific distributional markers. Without these shortcuts, the model must extract features from linguistic patterns that are more likely to generalize to unseen data from different sources.
- Core assumption: Semantic patterns distinguishing disinformation are more universal than surface-level artifacts specific to any single collection procedure.
- Evidence anchors:
  - [abstract] "non-informative elements (e.g., URLs and emoticons) should be removed... to avoid models' bias and increase their generalization capabilities"
  - [section 3.3] After preprocessing, internal Macro-F1 dropped from 98.41% to 97.80%; Section 5 shows external improvements ranging from 39.32% to 80.34% across dataset combinations
  - [corpus] No corpus papers directly address preprocessing-based debiasing; approaches focus on multi-agent systems and persuasion knowledge infusion
- Break condition: If feature removal consistently degrades both internal and external performance, the removed features contained legitimate discriminative signal rather than spurious correlation.

### Mechanism 3: Named Entity Categorization Reduces Entity-Frequency Overfitting
- Claim: Replacing specific named entities with category tokens (PER, ORG, LOC, MISC) prevents models from memorizing entity-class correlations that reflect sampling bias.
- Mechanism: NER identifies entities and substitutes them with generic labels (e.g., "Rwanda" → "LOC"). The model learns patterns based on entity types and contexts rather than memorizing which specific entities correlate with classes in training data—entities that may have different distributions in deployment.
- Core assumption: Specific entities appearing predominantly in one class reflect dataset construction bias, not genuine causal relationships with disinformation.
- Evidence anchors:
  - [abstract] "named entities (e.g., Rwanda) should be pseudo-anonymized before training to avoid models' bias"
  - [section 3.4] Figure 5 shows "Rwanda," "Tanzania," "Netflix," "Nicaragua" among highest-importance tokens; uses BERT-based NER trained on CoNLL-2003
  - [corpus] NE-PADD paper addresses named entities for audio deepfakes but in different modality; no direct evidence for entity replacement in text disinformation
- Break condition: If specific entities consistently appear in disinformation across independent datasets and time periods, replacement may discard valuable signal rather than bias.

## Foundational Learning

- Concept: SHAP (SHapley Additive exPlanations) values
  - Why needed here: Required to audit which tokens drive classification decisions; distinguishes legitimate semantic reasoning from spurious correlation exploitation.
  - Quick check question: A text is classified as "fake news" with "CDC" showing high positive SHAP attribution—is this legitimate health-misinformation detection or dataset-specific entity bias?

- Concept: Named Entity Recognition (NER) with category mapping
  - Why needed here: Enables systematic identification and replacement of proper nouns; prevents ad-hoc entity filtering and ensures consistent preprocessing.
  - Quick check question: If NER misclassifies "Netflix" as LOC instead of ORG, how does this affect what pattern the downstream classifier learns?

- Concept: External validation for generalization testing
  - Why needed here: Internal train/test splits cannot reveal overfitting to dataset artifacts; only independent datasets from different sources expose true generalization capacity.
  - Quick check question: Model achieves 98% F1 internally but 31% on external dataset—what does this 67-point gap indicate about learned features?

## Architecture Onboarding

- Component map: RoBERTa (pretrained on 97M COVID tweets) -> SHAP Explainer -> Preprocessing Pipeline (Remove URLs/emoticons/hashtags/@mentions + NER via BERT-CoNLL2003 + Entity Replacement) -> Enhanced Classifier -> Internal/External Validation
- Critical path: 1. Train baseline classifier -> 2. Generate SHAP values on test set -> 3. Identify high-attribution spurious tokens -> 4. Implement preprocessing + NER replacement -> 5. Retrain -> 6. Validate externally
- Design tradeoffs:
  - Internal performance vs. external generalization: Preprocessing reduced internal F1 by 1-8 points but improved external F1 by 65.78% average
  - Interpretability overhead vs. insight: SHAP adds computation and requires human interpretation but reveals hidden failure modes
  - Entity specificity vs. robustness: Retaining entity names may capture some signal but risks overfitting; replacement prioritizes generalization
- Failure signatures:
  - High internal F1 (>95%) with low external F1 (<40%) = artifact overfitting
  - SHAP attributions concentrated on punctuation, URL tokens, or rare entities = spurious feature reliance
  - High variance in external improvement across datasets (e.g., +71% vs. +7%) = incomplete artifact removal
- First 3 experiments:
  1. Baseline SHAP audit: Train RoBERTa on Constraint AAAI, extract top-20 highest-attribution tokens; classify as semantic vs. spurious based on domain knowledge
  2. Ablation study: Compare three variants—(a) baseline preprocessing, (b) extended preprocessing only, (c) extended preprocessing + NER replacement—measure internal and external F1 deltas
  3. Entity distribution analysis: For high-attribution named entities, compute class distribution in training data; validate that skewed distributions correlate with spurious attribution patterns

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Would creating a custom language model specifically pre-trained for the Automatic Disinformation Detection (ADD) domain improve the handling of domain-specific terms and explainability compared to general-purpose models?
- Basis in paper: [explicit] The authors state, "a custom language model specifically designed for the ADD domain could be created to improve explainability and handle domain-specific terms better."
- Why unresolved: The current study relies on fine-tuning existing general models (RoBERTa/DigitalEpidemiology), which may lack specialized knowledge for the nuances of disinformation vocabulary.
- What evidence would resolve it: Training a domain-specific model from scratch or further pre-training, followed by a comparative analysis of F1-scores and SHAP explanations against the current baseline.

### Open Question 2
- Question: Can the proposed XAI-driven methodology of identifying spurious features and applying extended preprocessing be successfully adapted to Natural Language Processing tasks other than disinformation detection?
- Basis in paper: [explicit] The authors propose that "the methodology developed for combating misinformation could be adapted to other NLP tasks, offering a broader application scope beyond its current domain."
- Why unresolved: The paper only validates the methodology within the specific context of disinformation classification; its utility for tasks like sentiment analysis or summarization remains untested.
- What evidence would resolve it: Applying the SHAP-based feature removal and Named Entity Replacement pipeline to a different NLP task and measuring the impact on external validation metrics.

### Open Question 3
- Question: Can alternative explainability techniques, such as Integrated Gradients, provide deeper or more consistent insights into the decision-making processes of disinformation models than SHAP?
- Basis in paper: [explicit] The authors note that "different explainability techniques, particularly Integrated Gradients, could be explored to gain deeper insights into the models’ decision-making processes."
- Why unresolved: The study relies exclusively on SHAP values; different XAI methods might identify different sets of spurious features or offer more robust explanations.
- What evidence would resolve it: A comparative study running both SHAP and Integrated Gradients on the same models to see if they highlight consistent spurious features or if one method leads to better debiasing results.

### Open Question 4
- Question: Does the observed improvement in generalizability persist when applying Named Entity Replacement to disinformation domains with different linguistic patterns, such as political news or financial scams?
- Basis in paper: [inferred] While the authors argue that ADD is a "multifaceted" issue, the experimental validation is restricted exclusively to COVID-19-related datasets, leaving the methodology's effectiveness on other disinformation genres unproven.
- Why unresolved: It is unclear if replacing entities (e.g., political figures) with generic tags (PER) degrades the semantic context necessary for detecting political disinformation, which relies heavily on specific entity relations.
- What evidence would resolve it: Evaluating the enhanced model on external datasets from non-health domains (e.g., political fact-checking datasets like LIAR) to verify cross-domain generalizability.

## Limitations

- Dataset Representativeness: Improvements demonstrated only on COVID-19-related datasets; effectiveness on other disinformation domains (political, financial) remains untested
- SHAP Attribution Reliability: Interpretation of "spurious" features relies on human judgment rather than rigorous statistical validation
- Named Entity Granularity Trade-off: Coarse category replacement (PER, ORG, LOC, MISC) may discard useful semantic information about well-known institutions

## Confidence

- High Confidence: The 65.78% average F1 improvement in external validation is empirically demonstrated with clear statistical significance
- Medium Confidence: The interpretation of SHAP attributions as indicators of spurious features relies on reasonable but not definitively proven assumptions
- Low Confidence: Claims about the universality of semantic patterns versus surface artifacts across all disinformation domains are extrapolated beyond the COVID-19 focus

## Next Checks

1. Cross-Domain Generalization Test: Apply the complete pipeline to disinformation datasets from different domains (political elections, financial scams, climate misinformation) to verify if the 65.78% improvement generalizes beyond COVID-19

2. Controlled Attribution Analysis: Create synthetic datasets with known spurious vs. genuinely informative tokens, run SHAP attribution, and verify the method correctly distinguishes between these categories with precision >90%

3. Entity Granularity Ablation: Systematically vary entity replacement granularity (full anonymization, fine-grained categories like "MEDICAL_ORG" vs. "GOV_ORG", selective retention of high-reliability entities) to determine if the four-category approach is optimal