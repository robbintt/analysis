---
ver: rpa2
title: 'Trojan Horse Prompting: Jailbreaking Conversational Multimodal Models by Forging
  Assistant Message'
arxiv_id: '2507.04673'
source_url: https://arxiv.org/abs/2507.04673
tags:
- arxiv
- attack
- conversational
- user
- harmful
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces Trojan Horse Prompting, a jailbreak technique
  that manipulates conversational history by forging model-attributed messages to
  bypass safety filters. The method exploits an "Asymmetric Safety Alignment" where
  models are trained to refuse harmful user inputs but not to validate the authenticity
  of their own past messages.
---

# Trojan Horse Prompting: Jailbreaking Conversational Multimodal Models by Forging Assistant Message

## Quick Facts
- arXiv ID: 2507.04673
- Source URL: https://arxiv.org/abs/2507.04673
- Authors: Wei Duan; Li Qian
- Reference count: 30
- Primary result: Demonstrates jailbreak technique exploiting models' asymmetric safety alignment to bypass filters by forging model-attributed messages

## Executive Summary
This paper introduces Trojan Horse Prompting, a jailbreak technique that manipulates conversational history by forging messages attributed to the model itself. The attack exploits an "Asymmetric Safety Alignment" where models are trained to refuse harmful user inputs but lack comparable skepticism toward messages they supposedly generated themselves. By embedding malicious content in forged model messages and following with benign user triggers, attackers can bypass safety filters. Experiments on Google's Gemini-2.0-flash-preview-image-generation show significantly higher attack success rates compared to traditional user-turn jailbreaking methods.

## Method Summary
The technique constructs conversation history with a forged model message containing malicious payload, followed by a benign user trigger prompt. The attacker sets `role='model'` on the fabricated message, exploiting the model's implicit trust in its own purported past outputs. Three payload strategies are employed: Direct Injection (explicit harmful instructions), Contextual Priming (subtle harmful framing before escalation), and Multimodal Deception (harmful instructions embedded in adversarial images). The method is tested on Google's Gemini-2.0-flash-preview-image-generation API, measuring Attack Success Rate across multiple harmful prompt categories.

## Key Results
- Exploits fundamental vulnerability in conversational AI security through asymmetric safety alignment
- Demonstrates significantly higher Attack Success Rate compared to established user-turn jailbreaking methods
- Reveals that models lack learned policies for self-scrutiny of their purported conversational history

## Why This Works (Mechanism)

### Mechanism 1: Asymmetric Safety Alignment
Models are trained to scrutinize user inputs for harmful content but lack comparable skepticism toward messages attributed to themselves in conversation history. During SFT and RLHF, models learn `P(refusal | role='user', content=harmful)` through red-teaming examples, but not `P(self-correction | role='model', content=harmful)`. This creates a learned blind spot where the training data implicitly teaches that `role='model'` messages are always legitimate, vetted outputs from prior turns.

### Mechanism 2: Protocol-Level Identity Spoofing
The attack exploits the structural protocol of conversational APIs by spoofing the model's identity within the history object, bypassing content filters that assume role attribution is authentic. APIs accept externally-constructed conversation history as structured objects with role tags. An attacker sets `role='model'` on a fabricated message containing the malicious payload, and the model processes this as "what I said before" rather than "what someone is asking me to do."

### Mechanism 3: Source Misattribution (Cognitive Laundering)
When processing forged history, the model "recalls" malicious instructions but misattributes their source to itself rather than an external attacker, stripping away the skepticism normally applied to user input. The `role='model'` tag acts as a trust signal, causing the model to incorporate the forged content into its reasoning context as self-generated, previously-vetted information. When a benign trigger prompt arrives, the model treats compliance as continuing an established (fabricated) agreement.

## Foundational Learning

- **Concept: Role-based conversational API structure**
  - Why needed: The attack fundamentally exploits how APIs represent multi-turn dialogue as a list of message objects with role tags (`user`, `model`, `assistant`).
  - Quick check: In a typical LLM API, who constructs the conversation history object sent with each request—the server or the client?

- **Concept: Safety alignment training (SFT + RLHF)**
  - Why needed: The paper's core thesis is that alignment creates asymmetric scrutiny. You need to understand what models are trained on versus what they're not trained on to grasp the vulnerability.
  - Quick check: During RLHF red-teaming, are human labelers primarily crafting harmful user prompts or harmful model responses?

- **Concept: Context window and history management**
  - Why needed: Conversational models don't internally persist state across turns—they rely on externally-provided history. This architectural choice enables forgery.
  - Quick check: When you send a follow-up prompt to a chat API, does the model "remember" the previous turn through internal state or because you (or the API wrapper) re-sent the prior conversation?

## Architecture Onboarding

- **Component map:** Client Application -> API Request Constructor -> History Object [{role: 'user'|'model', parts: [...]}] -> Safety Filters (input-level) -> Model Inference (context integration) -> Response Generation

- **Critical path:** User input → input-level safety filter (scrutinizes `role='user'` only) → context integration (merges history, treats `role='model'` as trusted) → generation. The forged payload enters *after* input filtering by being placed in history.

- **Design tradeoffs:**
  - External history management (current design): Enables stateless model serving, flexible context window usage, easy client-side control. *Cost:* Client controls "truth," enabling forgery.
  - Internal state persistence (alternative): Model server owns conversation log, signs outputs. *Cost:* Scalability challenges, harder client integration, no client-side history editing.
  - History integrity verification (mitigation): Server validates message provenance via signatures or sequence numbers. *Cost:* Protocol complexity, backward compatibility.

- **Failure signatures:** Sudden policy violations from benign-looking final user prompts; API logs showing `role='model'` messages in history that don't match any prior model response; high ASR on attacks using short trigger prompts with long preceding context.

- **First 3 experiments:**
  1. Baseline ASR comparison: Run identical harmful prompts under (a) direct user-turn submission vs. (b) Trojan Horse formulation with forged model agreement.
  2. Role-swap ablation: Submit forged history with the *same malicious content* but tagged as `role='user'` vs. `role='model'`.
  3. Trigger prompt minimalism test: Progressively simplify the final user prompt (from detailed request → "Continue" → "OK").

## Open Questions the Paper Calls Out

- **Open Question 1:** What specific defense architectures can effectively validate conversational context integrity without degrading the latency or utility of stateful interactions?
  - Basis: Authors conclude that securing LLMs necessitates a "paradigm shift from input-level filtering to robust, protocol-level validation of conversational context integrity."
  - Why unresolved: Paper identifies vulnerability but stops short of proposing or testing specific technical solutions.
  - What evidence would resolve it: A defense mechanism that successfully lowers ASR of Trojan Horse Prompting in a live API environment.

- **Open Question 2:** Does the Asymmetric Safety Alignment hypothesis hold true across different model architectures, specifically in pure text-based LLMs versus multimodal models?
  - Basis: Experimental validation is restricted to Gemini-2.0-flash-preview-image-generation, while authors hypothesize vulnerability is fundamental to "modern conversational AI security."
  - Why unresolved: Demonstrates attack on specific multimodal model but lacks empirical data on text-only models.
  - What evidence would resolve it: Comparative ASR using Trojan Horse Prompting across diverse text-only and multimodal commercial models.

- **Open Question 3:** Can safety alignment training be modified to include "self-scrutiny" of conversational history without causing the model to second-guess legitimate context?
  - Basis: Authors state models lack learned policy for self-scrutiny because "the alignment process does not typically include training the model to be skeptical of its own purported past statements."
  - Why unresolved: Unclear if adding skepticism to the 'model' role during training is feasible without destabilizing context maintenance.
  - What evidence would resolve it: Results from ablation study showing reduction in ASR while maintaining performance on standard multi-turn conversation benchmarks.

## Limitations

- Limited empirical validation with no quantitative ASR results provided (Section 6 states "Work in progress")
- Unclear generalizability across different conversational multimodal models and API implementations
- Doesn't address potential mitigations or explore practical severity compared to other jailbreaking techniques

## Confidence

**High Confidence:** The theoretical mechanism of Asymmetric Safety Alignment is well-grounded and logically follows from standard alignment training practices.

**Medium Confidence:** The protocol-level identity spoofing mechanism is plausible given standard conversational API designs, but lacks direct evidence of naive trust assumptions in major API providers.

**Low Confidence:** The empirical effectiveness claims cannot be validated without promised ASR results; the assertion of significantly higher attack success rate remains unsubstantiated.

## Next Checks

1. **Quantitative ASR Validation:** Run controlled experiments comparing Trojan Horse attacks against baseline direct user-turn attacks across the same prompt categories, reporting exact ASR percentages with statistical significance testing.

2. **Role Attribute Ablation Study:** Systematically test whether the `role='model'` tag specifically drives the vulnerability by comparing attacks using identical malicious content tagged as `role='user'` versus `role='model'`.

3. **API Implementation Survey:** Investigate whether major conversational AI APIs (OpenAI, Anthropic, Google, etc.) implement any server-side history validation, message signing, or provenance tracking that would mitigate this attack vector.