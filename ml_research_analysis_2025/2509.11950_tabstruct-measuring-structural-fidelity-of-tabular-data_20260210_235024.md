---
ver: rpa2
title: 'TabStruct: Measuring Structural Fidelity of Tabular Data'
arxiv_id: '2509.11950'
source_url: https://arxiv.org/abs/2509.11950
tags:
- global
- utility
- data
- tabular
- local
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: TabStruct introduces a principled benchmark framework for evaluating
  tabular data generators by jointly assessing structural fidelity and conventional
  metrics. It proposes global utility, an SCM-free metric that measures structural
  fidelity through utility per feature, enabling evaluation even without ground-truth
  causal structures.
---

# TabStruct: Measuring Structural Fidelity of Tabular Data

## Quick Facts
- arXiv ID: 2509.11950
- Source URL: https://arxiv.org/abs/2509.11950
- Reference count: 40
- Key outcome: TabStruct introduces a principled benchmark framework for evaluating tabular data generators by jointly assessing structural fidelity and conventional metrics. It proposes global utility, an SCM-free metric that measures structural fidelity through utility per feature, enabling evaluation even without ground-truth causal structures. Across 13 generators and 29 datasets, TabStruct reveals that existing methods often capture local correlations but fail to preserve global data structures. Diffusion-based models, due to their permutation-invariant generation process, consistently outperform others in structural fidelity. The findings highlight that structural fidelity should be a core evaluation dimension in tabular generation, complementing density estimation, privacy preservation, and ML efficacy. TabStruct is open-source, extensible, and offers actionable guidance for model selection and development.

## Executive Summary
TabStruct introduces a principled benchmark framework for evaluating tabular data generators by jointly assessing structural fidelity and conventional metrics. It proposes global utility, an SCM-free metric that measures structural fidelity through utility per feature, enabling evaluation even without ground-truth causal structures. Across 13 generators and 29 datasets, TabStruct reveals that existing methods often capture local correlations but fail to preserve global data structures. Diffusion-based models, due to their permutation-invariant generation process, consistently outperform others in structural fidelity. The findings highlight that structural fidelity should be a core evaluation dimension in tabular generation, complementing density estimation, privacy preservation, and ML efficacy. TabStruct is open-source, extensible, and offers actionable guidance for model selection and development.

## Method Summary
TabStruct evaluates 13 tabular data generators across 29 datasets using four evaluation dimensions: density estimation, privacy preservation, ML efficacy, and structural fidelity. The key innovation is global utility, which measures structural fidelity by training predictors to predict each variable from all others, then normalizing and aggregating performance. The framework uses 80/20 train-test splits with 90/10 reference-validation splits from training data. Hyperparameter tuning is performed via Optuna with a 2-hour budget per generator per run. Downstream predictors include AutoGluon ensembles and TabPFN with 1-hour tuning per feature. Evaluation is repeated 10 times, and results are aggregated using ADTM (average distance to minimum) metric.

## Key Results
- Diffusion-based models (TabDDPM, TabSyn, TabDiff) consistently achieve highest global CI scores (0.80-0.92) and global utility (0.81-0.90) across all dataset types
- Global utility strongly correlates with global CI scores (r_s = 0.84, p < 0.001) on SCM datasets, validating it as an effective structural fidelity proxy
- SMOTE achieves highest local utility (0.94-0.96) but lowest global CI scores (0.21-0.35), confirming local metrics fail to capture global structure

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Diffusion-based models capture global tabular structure through permutation-invariant conditional distribution learning.
- Mechanism: During training, noise is added independently to each feature; the diffusion network learns to reconstruct each feature by conditioning on all others (p(x_j | X \ {x_j})). This enforces learning of inter-feature dependencies without fixed ordering bias.
- Core assumption: Tabular features have permutation-invariant relationships rather than sequential dependencies.
- Evidence anchors:
  - [abstract] "Diffusion-based models, due to their permutation-invariant generation process, consistently outperform others in structural fidelity."
  - [section 4.3] "diffusion models impose no ordering constraint. This results in efficient computation and permutation-invariant conditional distributions, a property that aligns naturally with the structure of tabular data."
  - [corpus] Related paper "How Well Does Your Tabular Generator Learn the Structure of Tabular Data?" similarly investigates structural learning in tabular generators, supporting this as an active research direction.
- Break condition: If features have genuine sequential dependencies (e.g., time-indexed columns), permutation invariance becomes counterproductive.

### Mechanism 2
- Claim: Global utility serves as an effective proxy for structural fidelity by measuring predictive power across all variables.
- Mechanism: Each variable is treated as a prediction target; an ensemble of predictors learns to predict it from remaining variables. Normalized utility scores aggregate across all features, correlating with conditional independence preservation.
- Core assumption: High-fidelity synthetic data should enable accurate conditional prediction of each variable from others, analogous to Markov blanket relationships.
- Evidence anchors:
  - [abstract] "global utility... enables evaluation even without ground-truth causal structures."
  - [section 4.2] "Global utility serves as an effective metric for global structural fidelity... strong monotonic correlation between global utility and global CI scores (r_s = 0.84, p < 0.001)."
  - [corpus] "Does TabPFN Understand Causal Structures?" investigates TabPFN's ability to infer causal relationships, suggesting foundation models may implicitly capture structure—but corpus lacks direct validation of utility-based metrics.
- Break condition: If downstream predictors have systematic biases favoring certain feature types, utility scores may conflate structural fidelity with predictor limitations.

### Mechanism 3
- Claim: Local metrics (density estimation, ML efficacy) fail to capture global causal structure because they optimize for task-specific targets.
- Mechanism: Local utility focuses on p(y|x), which only requires learning feature-target relationships, not inter-feature dependencies. Methods like SMOTE excel at local structure by interpolating within-class samples but violate global conditional independence.
- Core assumption: Preserving the target's prediction boundary ≠ preserving the full joint distribution's causal structure.
- Evidence anchors:
  - [section 1] "SMOTE yields synthetic data whose evaluations most closely match real data... the generated synthetic data only preserves local structure and violates most physical laws."
  - [section 3.2.2] "local utility may reward 'myopic' generators while missing the holistic data structure."
  - [corpus] "Causal Data Augmentation for Robust Fine-Tuning" emphasizes generating structurally consistent augmentation data, supporting the importance of global structure—but does not directly compare local vs. global metrics.
- Break condition: If the only downstream use case is single-target prediction and global structure is irrelevant, local metrics may be sufficient.

## Foundational Learning

- Concept: Conditional Independence (CI) Testing
  - Why needed here: Structural fidelity is quantified by comparing CI statements in real vs. synthetic data; CI tests (chi-square, partial correlation, residualization-based) detect whether conditional independence relationships are preserved.
  - Quick check question: Can you explain why X ⫫ Y | Z implies that Z "screens off" the dependence between X and Y?

- Concept: Markov Equivalence Classes and CPDAGs
  - Why needed here: The paper evaluates at the CPDAG level (completed partially directed acyclic graphs), which represents equivalence classes of DAGs sharing the same CI statements—avoiding unidentifiable edge directions.
  - Quick check question: Why might two different DAGs imply the same set of conditional independence relationships?

- Concept: Diffusion Model Denoising Objective
  - Why needed here: Understanding how diffusion models learn p(x_j | X \ {x_j}) per feature explains their structural fidelity advantage; the per-feature noise-then-denoise process forces learning of conditional dependencies.
  - Quick check question: How does learning to denoise each feature conditioned on all others differ from autoregressive factorization p(x_1, ..., x_d) = ∏ p(x_i | x_{<i})?

## Architecture Onboarding

- Component map:
  - Data Pipeline: 29 datasets (6 expert-validated SCM, 23 real-world) → train/val/test splits (80/20, with 90/10 within train)
  - Generators: 13 models across 9 categories (SMOTE, BN, VAE, GAN, Flow, Tree, Diffusion, EBM, LLM)
  - Evaluation Dimensions: Density estimation (Shape, Trend, α-precision, β-recall), Privacy (DCR, δ-Presence), ML efficacy (Local utility), Structural fidelity (Local/Global CI scores, Global utility)
  - Global Utility Computation: Ensemble of 9 predictors (AutoGluon-full + TabPFN), normalized performance per variable, aggregated across all D+1 variables

- Critical path:
  1. **Dataset selection**: Ensure SCM datasets have ground-truth graphs for validation; real-world datasets must be contamination-free (excluded from TabPFN meta-validation)
  2. **Generator training**: Train on D_ref, generate synthetic data with N_ref samples, preserve stratification for classification
  3. **Global utility computation**: For each variable, train ensemble predictors on synthetic data, evaluate on D_test, normalize against D_ref performance, aggregate
  4. **Correlation analysis**: Compare global utility rankings to global CI scores on SCM datasets to validate the proxy

- Design tradeoffs:
  - **Full-tuned vs. Tiny-default predictors**: Full-tuned (9 predictors, 1h tuning per feature) provides reliable local utility; Tiny-default (3 untuned predictors: KNN, XGBoost, TabPFN) suffices for global utility rankings
  - **SCM vs. real-world datasets**: SCM enables ground-truth CI validation; real-world tests generalizability but lacks gold standard
  - **CI test significance level**: α = 0.01 balances false positives vs. statistical power

- Failure signatures:
  - **Generator converges but global utility near 0**: Synthetic data fails to preserve any predictive relationships; check for mode collapse or feature-independent sampling
  - **Global CI ≈ 1 but local utility < 0.5**: Synthetic data preserves CI relationships but target feature is poorly predicted; may indicate noisy target or insufficient target-relevant features
  - **Large variance across repeats (±0.20+)**: Unstable generation; check hyperparameter sensitivity or random seed handling

- First 3 experiments:
  1. **Baseline validation**: Run TabDDPM, SMOTE, and GReaT on a single SCM dataset (e.g., Insurance); verify TabDDPM achieves highest global CI and global utility, SMOTE highest local utility
  2. **Ablation on predictor ensemble**: Compute global utility using Full-tuned, Light-tuned, Tiny-tuned, and Tiny-default; confirm rankings are stable across configurations
  3. **Sanity check on trivial data**: Generate synthetic data by independent per-feature sampling; verify global utility drops sharply (≈ 0.1-0.2) while Shape/Trend remain high

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the strong empirical correlation between Global Utility and structural fidelity be formalized into a provable theoretical framework?
- **Basis in paper:** [explicit] Section 5 and Section F.5 state that developing a "theoretically provable" metric is a major challenge and necessary future direction to bridge the gap between empirical measurements and structural causal models (SCMs).
- **Why unresolved:** The paper validates Global Utility empirically against ground-truth SCMs ($r_s=0.84$) but does not provide a theoretical derivation linking normalized predictive utility to conditional independence properties.
- **What evidence would resolve it:** A theoretical proof establishing bounds on the error of Global Utility as an estimator for the true Markov equivalence class of the data.

### Open Question 2
- **Question:** Can tabular generators explicitly optimized for structural constraints (e.g., via causal loss terms) outperform the permutation-invariant diffusion models identified as current leaders?
- **Basis in paper:** [explicit] Section F.5 proposes "structure-aware tabular data generators" and suggests a shift away from standard likelihood-driven generation toward models that "explicitly optimised for structural fidelity."
- **Why unresolved:** The current top-performing models (TabDDPM, TabSyn, TabDiff) rely on implicit learning principles like permutation invariance, rather than explicitly embedding or enforcing structural constraints during training.
- **What evidence would resolve it:** A novel generator incorporating structural fidelity loss functions (e.g., penalizing conditional independence violations) that achieves superior Global Utility scores compared to diffusion baselines.

### Open Question 3
- **Question:** How can the Global Utility metric be effectively adapted to evaluate structural fidelity in dynamic or temporal tabular data?
- **Basis in paper:** [explicit] Section F.5 identifies the extension to "dynamic and temporal data modalities" as a promising direction, noting that many real-world domains exhibit longitudinal structures that static SCMs cannot model.
- **Why unresolved:** The current TabStruct framework and Global Utility metric are restricted to static datasets and cannot assess how well synthetic data preserves time-dependent causal relationships or interventional distributions over time.
- **What evidence would resolve it:** A modified Global Utility metric incorporating time-lagged predictive utility, tested on temporal benchmarks (e.g., healthcare time-series) with known dynamic causal structures.

## Limitations

- Ground-truth structural fidelity: While global utility correlates well with global CI scores on SCM datasets (r_s = 0.84), the proxy relationship remains unproven on real-world data without ground-truth causal structures.
- Predictor bias in utility measurement: The global utility metric depends heavily on the choice of downstream predictors, which could distort structural fidelity rankings despite ensemble approaches.
- Permutation invariance assumption: The claimed advantage of diffusion models assumes tabular features lack sequential dependencies, which may not hold for time-series or naturally ordered features.

## Confidence

- **High confidence**: Diffusion models' superior structural fidelity (directly measured via global CI scores), correlation between global utility and global CI (statistically significant), local metrics' failure to capture global structure (consistent across all datasets).
- **Medium confidence**: Global utility as reliable proxy for structural fidelity (strong correlation on SCM datasets, but extrapolation to real-world untested), permutation-invariant advantage of diffusion models (mechanism sound but domain-specific limitations unclear).
- **Low confidence**: Exact threshold where structural fidelity becomes critical for downstream tasks (varies by application), whether TabPFN's implicit structural learning matches explicitly structural models (corpus shows active research but no definitive answer).

## Next Checks

1. **Cross-domain structural fidelity test**: Apply TabStruct to time-series tabular datasets where feature ordering matters. If diffusion models' advantage diminishes or reverses, this validates the permutation invariance limitation.

2. **Target-task correlation study**: Measure whether global utility rankings predict downstream task performance (e.g., fairness, robustness, OOD generalization) better than local metrics across diverse real-world applications.

3. **Extreme structural complexity test**: Evaluate generators on datasets with known complex causal structures (e.g., collider bias, latent confounders) to identify breaking points where even high-global-utility methods fail to preserve critical dependencies.