---
ver: rpa2
title: 'Less is More: Undertraining Experts Improves Model Upcycling'
arxiv_id: '2506.14126'
source_url: https://arxiv.org/abs/2506.14126
tags:
- merging
- training
- learning
- experts
- steps
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper demonstrates that overtraining experts can degrade performance
  in model upcycling. Across multiple settings including merging fully fine-tuned
  and LoRA-adapted models, as well as MoErging, the authors find that experts trained
  longer yield worse results when combined.
---

# Less is More: Undertraining Experts Improves Model Upcycling

## Quick Facts
- arXiv ID: 2506.14126
- Source URL: https://arxiv.org/abs/2506.14126
- Reference count: 40
- Key outcome: Overtrained experts degrade model upcycling performance; early stopping improves merged accuracy while reducing training costs

## Executive Summary
This paper challenges the conventional wisdom that better fine-tuned experts lead to better merged models. Through extensive experiments across vision and NLP tasks, the authors demonstrate that overtraining experts can significantly harm performance when these models are combined via merging or MoErging. The core insight is that late-stage training focuses on memorizing difficult examples whose features are lost during the merging process. The proposed solution is task-dependent aggressive early stopping, which recovers optimal upcycling accuracy while substantially reducing computational costs.

## Method Summary
The paper systematically studies how training duration affects model upcycling performance. It fine-tunes foundation models (CLIP ViT-B-32 for vision, T5-base for NLP) on multiple tasks for varying step counts (2-2048), then merges them using several methods (Task Arithmetic, TIES, DARE for full fine-tuning; MoErging for LoRA adapters). The authors analyze the impact of training duration on merged model accuracy and propose an early stopping strategy based on validation accuracy plateaus. LoRA adapters with rank 8 are used as a parameter-efficient alternative to full fine-tuning, with additional experiments varying the adapter rank to study robustness to overtraining.

## Key Results
- Merged models from overtrained experts show up to 5% accuracy degradation compared to those from optimally trained experts
- Optimal training duration occurs around 256 steps for vision tasks, well before full convergence at 2048 steps
- LoRA adapters are more severely affected by overtraining than fully fine-tuned models
- Higher LoRA ranks (e.g., 64 vs 8) provide greater robustness to overtraining but reduce efficiency
- MoErging with overtrained experts results in 2% lower multi-task accuracy due to poor initialization quality

## Why This Works (Mechanism)

### Mechanism 1: Differential Feature Learning Dynamics
- Claim: Undertrained experts retain more generalizable features that survive merging, while fully converged experts learn dataset-specific memorization patterns that are lost during merging
- Core assumption: Early training captures common, generalizable features while late training memorizes difficult outliers; merging averages parameters, destroying these specialized updates
- Evidence anchors: Difficult examples (top 10%) account for over 50% of loss during most training; over 50% of forgotten examples during merging are from the top 30% most difficult

### Mechanism 2: Parameter Space Divergence in Low-Rank Adaptation
- Claim: Extended fine-tuning more severely impacts LoRA merging due to constrained parameter space
- Core assumption: Low-rank updates must encode both generalizable features and difficult-example memorization within a limited subspace, creating greater interference during merging
- Evidence anchors: LoRA experts show worse merging performance than FFT; higher rank adapters (râ‰¥64) consistently improve merging accuracy and reduce overtraining sensitivity

### Mechanism 3: Incompatible Initializations for Multi-Task Learning
- Claim: Overtrained LoRA adapters serve as poor initializations for downstream MoErging due to narrow specialization
- Core assumption: Sharp minima from overtrained experts create poor starting points for subsequent multi-task optimization
- Evidence anchors: MoE-fied models with overtrained LoRA experts achieve ~2% lower final multi-task accuracy than those with undertrained experts

## Foundational Learning

**Data Difficulty/Memorization**
- Why needed here: The paper's argument depends on understanding how difficult examples affect training dynamics and why their memorization is lost during merging
- Quick check question: What is the EL2N score and how does it correlate with memorization?

**Model Merging / Upcycling**
- Why needed here: Understanding what model merging is and its practical challenges is essential to appreciate the problem being solved
- Quick check question: Name two methods for merging fine-tuned models and briefly describe how they work (e.g., Task Arithmetic, TIES)

**Low-Rank Adaptation (LoRA)**
- Why needed here: The finding that overtraining effects are more pronounced with LoRA requires understanding how LoRA updates work and the role of rank
- Quick check question: How does LoRA update a pre-trained model's weights, and what is the role of the rank `r`?

## Architecture Onboarding

**Component map:**
Pre-trained Foundation Model -> Early Stopping (Key Step) -> Expert Models -> Merging or MoErging -> Final Multi-Task Model

**Critical path:** Pre-trained Model -> **Early Stopping (Key Step)** -> Expert Models -> Merging or MoErging -> Final Multi-Task Model. The critical new step is the *early stopping* of expert training.

**Design tradeoffs:**
- **Expert Accuracy vs. Mergeability:** Longer training improves individual task performance but degrades merging results
- **LoRA Rank vs. Robustness:** Lower rank is more efficient but more sensitive to overtraining; higher rank is more robust but less efficient
- **Task-Specific vs. Uniform Early Stopping:** Optimal stopping time varies by task, but uniform strategies are simpler

**Failure signatures:**
- **Degraded Merged Accuracy:** Final merged model performs worse than constituent experts on their own tasks
- **MoErging Plateau:** Multi-task training of MoErged model yields suboptimal accuracy compared to models with undertrained expert initialization

**First 3 experiments:**
1. **Replicate the FFT Merging Degradation:** Train experts on multiple vision tasks for varying steps (e.g., 8, 32, 128, 512, 2048). Merge using Task Arithmetic and plot average merged accuracy vs. training steps. Verify peak is before full convergence.
2. **Validate the LoRA Effect:** Repeat first experiment using LoRA adapters (e.g., r=8) instead of full fine-tuning. Compare severity of performance drop to FFT case.
3. **Implement the Early Stopping Strategy:** Implement proposed aggressive early stopping on a single task. Train LoRA adapter, monitoring validation accuracy and halving learning rate upon plateau. Compare merge performance to fixed-duration training.

## Open Questions the Paper Calls Out

**Open Question 1**
- Question: Does the observed degradation in upcycling performance caused by expert overtraining persist in significantly larger foundation models?
- Basis in paper: [explicit] The conclusion states, "Since the models considered in this study are relatively small by current standards, it would be valuable in future work to investigate whether the same phenomenon holds for larger models."
- Why unresolved: Empirical validation was limited to ViT-B/32 and T5-base; dynamics may not scale linearly to models with billions of parameters
- What evidence would resolve it: Replicating training duration experiments on larger architectures (e.g., Llama-3 or ViT-L) and measuring correlation between training steps and merging accuracy

**Open Question 2**
- Question: How does the relationship between expert training duration and upcycling performance change with different PEFT methods?
- Basis in paper: [explicit] The conclusion suggests, "exploring a broader range of merging techniques and PEFT methods would help assess the generality of our findings."
- Why unresolved: Study focused on LoRA and full fine-tuning; other methods (e.g., prefix tuning, adapters) might encode difficult example knowledge differently
- What evidence would resolve it: Comparing merging performance curves across various PEFT techniques while systematically varying training duration

**Open Question 3**
- Question: Is it possible to recover the utility of already published, fully converged expert models for upcycling without access to intermediate checkpoints?
- Basis in paper: [inferred] Authors note findings "do not offer actionable insights regarding already published adapters," despite prevalence in repositories
- Why unresolved: Proposed solution relies on "aggressive early stopping," impossible for already converged models; no method exists to reverse overtraining
- What evidence would resolve it: Developing and testing post-hoc pruning or unlearning techniques targeting parameters associated with late-stage memorization

## Limitations
- Core memorization mechanism is empirically observed but not mechanistically proven
- Early stopping strategy requires task-dependent validation monitoring, limiting generalizability
- NLP experiments limited to T5-base may not transfer to larger language models
- MoErging results based on single multi-task setup may depend heavily on specific MoE architecture

## Confidence
- **High confidence:** Empirical finding that undertrained experts outperform fully converged ones in merging scenarios (FFT and LoRA) is well-supported across multiple architectures and datasets
- **Medium confidence:** Mechanism explaining memorization of difficult examples and their loss during merging is plausible but not definitively proven; alternative explanations may contribute
- **Low confidence:** MoErging results showing poor initialization quality are based on single multi-task training setup and may depend heavily on specific MoE architecture and training procedure

## Next Checks
1. **Mechanism isolation experiment:** Design experiment to test whether degradation is specifically due to memorization of difficult examples by training experts with noisy labels or outliers and observing if these examples dominate late-stage loss
2. **Alternative merging method validation:** Test undertrained expert hypothesis with merging method that explicitly preserves individual dataset features (e.g., through routing or gating) rather than averaging, to determine if effect persists
3. **Scaling study:** Verify whether optimal early stopping point scales predictably with model size and dataset size by conducting experiments across multiple model scales (e.g., ViT-S, ViT-L, ViT-H) on same vision tasks