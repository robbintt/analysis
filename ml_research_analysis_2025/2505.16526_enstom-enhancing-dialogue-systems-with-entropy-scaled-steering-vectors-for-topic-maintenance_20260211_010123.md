---
ver: rpa2
title: 'EnSToM: Enhancing Dialogue Systems with Entropy-Scaled Steering Vectors for
  Topic Maintenance'
arxiv_id: '2505.16526'
source_url: https://arxiv.org/abs/2505.16526
tags:
- steering
- distractor
- on-topic
- entropy
- layer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: EnSToM improves topic consistency in task-oriented dialogue systems
  by applying entropy-scaled steering vectors to small large language models. It dynamically
  adjusts steering intensity based on layer-wise entropy, enhancing distractor refusal
  while preserving on-topic accuracy.
---

# EnSToM: Enhancing Dialogue Systems with Entropy-Scaled Steering Vectors for Topic Maintenance

## Quick Facts
- arXiv ID: 2505.16526
- Source URL: https://arxiv.org/abs/2505.16526
- Authors: Heejae Suh; Yejin Jeon; Deokhyung Kang; Taehee Park; Yejin Min; Gary Geunbae Lee
- Reference count: 37
- Primary result: EnSToM achieves 0.81 distractor accuracy and 0.89 on-topic accuracy on CantTalkAboutThis dataset

## Executive Summary
EnSToM addresses topic maintenance in task-oriented dialogue systems by combining entropy-scaled steering vectors with small large language models. The method dynamically adjusts steering intensity based on layer-wise entropy, enabling effective distractor refusal while preserving on-topic response quality. It offers a resource-efficient alignment strategy suitable for low-data, resource-constrained environments.

## Method Summary
EnSToM extracts steering vectors from contrastive response pairs and applies them conditionally based on layer-wise entropy during inference. The method computes entropy at specific layers during the first few tokens of generation, using the difference between distractor and on-topic entropy distributions to scale the steering vector through a sigmoid function. This dynamic modulation enhances distractor refusal while preventing the degradation of on-topic responses that occurs with uniform steering approaches.

## Key Results
- Achieved 0.81 distractor accuracy and 0.89 on-topic accuracy on CantTalkAboutThis dataset
- Outperformed prompt-only baselines and vanilla steering approaches
- Demonstrated cross-architecture generalizability with Ministral-8B-Instruct-2410 experiments
- Layer-wise entropy analysis revealed consistent patterns across domains

## Why This Works (Mechanism)

### Mechanism 1
Layer-wise entropy in LLMs differentiates distractor inputs from on-topic inputs, enabling conditional steering. EnSToM computes entropy at specific layers during the first few tokens of generation. Distractor inputs exhibit distinct entropy distributions compared to on-topic inputs due to differences in attention patterns across semantic processing layers.

Core assumption: The entropy difference is consistent and separable enough to serve as a reliable discriminator between input types, despite some overlap in distributions.

### Mechanism 2
Sigmoid-scaled coefficients dynamically modulate steering vector strength, preventing uniform over-steering. A sigmoid function transforms layer-wise entropy into a scaling coefficient, ensuring that high steering is applied when entropy indicates a distractor, while low steering preserves natural behavior for on-topic inputs.

Core assumption: A single threshold and slope can effectively separate the two input types for a given model and layer.

### Mechanism 3
Steering vectors derived from contrastive pairs encode a "refusal direction" that generalizes across domains. The steering vector is computed as the average difference between hidden representations for desired and undesired behaviors. When added to a layer's activations during inference, it biases the model toward refusal without parameter updates.

Core assumption: The steering vector captures a generalizable refusal mechanism, not just domain-specific patterns.

## Foundational Learning

**Activation Steering / Activation Addition**
- Why needed here: EnSToM is built on this paradigm. Understanding how vectors modify hidden states at inference time is critical.
- Quick check question: What is the difference between fine-tuning and activation steering in terms of when and how model behavior is altered?

**Entropy in Language Models**
- Why needed here: The method uses layer-wise generation entropy as a signal to discriminate input types.
- Quick check question: At a given layer, would high entropy suggest more or less confidence in the next token prediction?

**Contrastive Pair Extraction**
- Why needed here: The steering vector is built from pairs of "desired" and "undesired" responses to the same prompt.
- Quick check question: If you have a prompt, how would you construct a contrastive pair (q^p, q^n) to encourage refusal of off-topic questions?

## Architecture Onboarding

**Component map:**
1. Steering Vector Extraction Module: Constructs contrastive pairs and computes normalized steering vector from hidden state differences
2. Entropy Computation Module: Performs 2-token forward pass to compute entropy at specified layers
3. Sigmoid Scaling Module: Transforms entropy value into dynamic scaling coefficient
4. Steering Application Module: Adds scaled steering vector to hidden state at chosen injection layer

**Critical path:**
1. Data Preparation: Generate/select contrastive pairs for vector extraction
2. Vector Extraction: Forward pass through model, compute differences at layer l, average and normalize
3. Entropy Analysis: Identify layers with clear entropy separation between distractor/on-topic inputs
4. Threshold Tuning: Set threshold t and other parameters based on entropy distribution
5. Inference: Compute entropy on new input, calculate scaling coefficient, apply scaled vector, generate response

**Design tradeoffs:**
- Layer Selection for Entropy vs. Steering: L=16 gave better results than L=19 for entropy extraction
- Threshold (t): Low t prioritizes on-topic accuracy (risks missing distractors), high t prioritizes distractor refusal (risks degrading on-topic responses)
- Sample Size: More pairs improve vector stability, but method works with as few as 10 samples

**Failure signatures:**
- Uniform Refusal: Vanilla steering (no scaling) may refuse on-topic queries (on-topic accuracy drops from 0.94 to 0.70)
- Incoherent Output: If steering coefficient is too large (>3), outputs may become repetitive or nonsensical
- Hard Negatives: Inputs with entropy in overlapping regions may be misclassified and steered incorrectly

**First 3 experiments:**
1. Entropy Distribution Analysis: Plot entropy at layers 10-25 for held-out distractor and on-topic inputs to identify layers with maximal separation
2. Vanilla vs. Scaled Steering Baseline: Compare distractor/on-topic accuracy for prompt-only, vanilla steering, and EnSToM with default t=7.5
3. Threshold Ablation: Run EnSToM across range of thresholds (t=2 to 9) and plot distractor/on-topic accuracy tradeoff curve

## Open Questions the Paper Calls Out

**Open Question 1**
Can a learning-based heuristic be developed to automatically determine the optimal entropy extraction layer (L) and scaling threshold (t) for different model architectures? Currently, L and t must be identified empirically by manually inspecting layer-wise entropy distributions, which hinders plug-and-play deployment across diverse models.

**Open Question 2**
How can the method be refined to handle "hard negatives" where on-topic and distractor inputs exhibit overlapping entropy distributions? The current reliance on a single metric (layer-wise entropy) fails to distinguish inputs when their uncertainty values are similar.

**Open Question 3**
How can the steering vector construction be modified to effectively adapt EnSToM for safety-critical tasks like jailbreak defense? While entropy differences exist for jailbreak inputs, the current mechanism for applying steering does not translate successfully to the jailbreak domain.

## Limitations
- Requires manual tuning of entropy thresholds and layer selection for each model and domain combination
- Relies on GPT-4o for contrastive pair construction, introducing potential reproducibility issues
- Performance with non-dialogue tasks or different steering objectives remains untested

## Confidence

**High Confidence:**
- EnSToM effectively improves distractor refusal while preserving on-topic accuracy on CantTalkAboutThis dataset
- Layer-wise entropy distributions differ between distractor and on-topic inputs in LLaMA-2-7B-Chat
- Sigmoid scaling prevents degradation observed with vanilla steering

**Medium Confidence:**
- Entropy difference patterns generalize across domains (banking and crosstalk)
- Cross-architecture experiments with Ministral-8B confirm generalizability
- Steering vectors encode generalizable refusal directions

**Low Confidence:**
- Method requires minimal data (100 samples) for effective performance across all scenarios
- Entropy-based scaling will consistently outperform threshold-based methods without calibration
- Optimal layer selection generalizes to all model architectures and sizes

## Next Checks

1. **Domain Transfer Experiment**: Evaluate EnSToM on three additional domains (healthcare, customer service, technical support) using the same entropy threshold (t=7.5) to test cross-domain robustness.

2. **Model Architecture Stress Test**: Apply EnSToM to a diverse set of small LLMs (Mistral-7B, Gemma-7B, Qwen-7B) and compare entropy layer patterns and threshold requirements.

3. **Pair Construction Ablation**: Systematically vary the number of contrastive pairs (10, 50, 100, 500) and measure steering vector stability and downstream performance. Test alternative pair construction methods to quantify sensitivity to training data quality and quantity.