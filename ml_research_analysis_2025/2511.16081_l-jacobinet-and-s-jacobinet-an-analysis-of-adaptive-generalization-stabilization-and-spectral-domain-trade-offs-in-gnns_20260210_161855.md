---
ver: rpa2
title: 'L-JacobiNet and S-JacobiNet: An Analysis of Adaptive Generalization, Stabilization,
  and Spectral Domain Trade-offs in GNNs'
arxiv_id: '2511.16081'
source_url: https://arxiv.org/abs/2511.16081
tags:
- domain
- chebynet
- s-jacobinet
- static
- l-jacobinet
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the limitations of spectral Graph Neural
  Networks (GNNs), particularly ChebyNet, which suffer from heterophily and over-smoothing
  due to their static, low-pass filter design. The authors propose the Adaptive Orthogonal
  Polynomial Filter (AOPF) class as a solution and introduce two models operating
  in the [-1, 1] domain: L-JacobiNet, an adaptive generalization of ChebyNet with
  learnable shape parameters, and S-JacobiNet, a novel baseline stabilized using LayerNorm.'
---

# L-JacobiNet and S-JacobiNet: An Analysis of Adaptive Generalization, Stabilization, and Spectral Domain Trade-offs in GNNs

## Quick Facts
- arXiv ID: 2511.16081
- Source URL: https://arxiv.org/abs/2511.16081
- Authors: Huseyin Goksu
- Reference count: 30
- Primary Result: S-JacobiNet (ChebyNet + LayerNorm) outperforms adaptive L-JacobiNet on 4/5 benchmarks, showing stabilization beats adaptation in [-1, 1] domain

## Executive Summary
This paper addresses the limitations of spectral Graph Neural Networks (GNNs), particularly ChebyNet, which suffer from heterophily and over-smoothing due to their static, low-pass filter design. The authors propose the Adaptive Orthogonal Polynomial Filter (AOPF) class as a solution and introduce two models operating in the [-1, 1] domain: L-JacobiNet, an adaptive generalization of ChebyNet with learnable shape parameters, and S-JacobiNet, a novel baseline stabilized using LayerNorm. Through extensive analysis comparing these models against AOPFs in the [0, ∞) domain, the authors reveal critical trade-offs. They find that the [0, ∞) domain is superior for modeling heterophily, while the [-1, 1] domain provides superior numerical stability at high polynomial degrees. Most significantly, S-JacobiNet (ChebyNet+LayerNorm) outperforms the adaptive L-JacobiNet on 4 out of 5 benchmark datasets, identifying stabilization, not adaptation, as the key factor for performance in the [-1, 1] domain. This suggests that ChebyNet's main flaw was stabilization, not its static nature, and that adaptation in this domain can lead to overfitting.

## Method Summary
The paper introduces the Adaptive Orthogonal Polynomial Filter (AOPF) class that generalizes spectral graph filtering across multiple domains. Two specific implementations are proposed: L-JacobiNet uses learnable shape parameters (α, β) for the Jacobi polynomials in the [-1, 1] domain, while S-JacobiNet uses the static Chebyshev basis (α=β=-0.5) with LayerNorm for stabilization. The method compares performance across spectral domains ([0, ∞) vs [-1, 1]) and between adaptive vs. static polynomial bases. The architecture consists of a 2-layer PolyBaseModel with 16 hidden units, operating on symmetric normalized Laplacian matrices. Key innovations include the stabilization framework (LayerNorm) for the [-1, 1] domain and the comparative analysis revealing trade-offs between heterophily modeling, numerical stability, and filter adaptivity.

## Key Results
- S-JacobiNet (ChebyNet + LayerNorm) outperforms adaptive L-JacobiNet on 4 out of 5 benchmark datasets
- The [0, ∞) domain is superior for modeling heterophily, while [-1, 1] domain provides superior numerical stability at high polynomial degrees
- L-JacobiNet's adaptive parameters lead to overfitting on standard benchmarks, suggesting stabilization trumps adaptation in the [-1, 1] domain
- LaguerreNet (in [0, ∞) domain) catastrophically collapses at K=25, while L-JacobiNet remains stable up to K=30

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** In the $[-1, 1]$ spectral domain, effective stabilization via LayerNorm contributes more to performance than increasing filter adaptivity via learnable shape parameters.
- **Mechanism:** Standard ChebyNet filters suffer from high variance and numerical instability. Introducing LayerNorm acts as a strong regularizer, reducing this variance. In contrast, making the polynomial basis adaptive (L-JacobiNet) increases model capacity (lower bias) but introduces higher variance, leading to overfitting on standard benchmarks.
- **Core assumption:** The Chebyshev basis (fixed $\alpha=\beta=-0.5$) is sufficient for the spectral requirements of the tested datasets, meaning the performance bottleneck is numerical instability rather than a lack of expressive power.
- **Evidence anchors:**
  - [abstract] "S-JacobiNet (ChebyNet+LayerNorm) outperforms the adaptive L-JacobiNet on 4 out of 5 benchmark datasets... suggesting that adaptation in the [-1, 1] domain can lead to overfitting."
  - [section IV.D] Analysis of Adaptation vs. Stabilization notes the "benefit of stabilization... is greater than the benefit of adaptation."
  - [corpus] Weak external validation for this specific trade-off in prior work; this appears to be a novel finding of the paper.
- **Break condition:** This trade-off may reverse on extremely large or complex spectral distributions where the static Chebyshev basis is insufficient to model the required filter shape (high bias becomes the bottleneck).

### Mechanism 2
- **Claim:** The choice of spectral domain causally determines success in heterophilic settings, with the $[0, \infty)$ domain outperforming the $[-1, 1]$ domain.
- **Mechanism:** The $[0, \infty)$ domain (via $L_{scaled} = 0.5 \cdot L_{sym}$) allows the model to learn band-pass or high-pass filters required to process dissimilar neighbors (heterophily). The $[-1, 1]$ domain (via $\hat{L} = L_{sym} - I$) centers the spectrum, introducing a bias toward low-pass filtering which is detrimental to heterophilic signals.
- **Core assumption:** Heterophily requires the preservation or amplification of high-frequency graph signals, which is mathematically constrained by the symmetry of the $[-1, 1]$ mapping.
- **Evidence anchors:**
  - [abstract] "The [0, ∞) domain is superior for modeling heterophily."
  - [section IV.B] Analysis shows filters in $[-1, 1]$ "perform poorly" on Texas/Cornell because the mapping is "biased towards low-pass responses."
  - [corpus] Consistent with HeroFilter and SplitGNN (neighbor papers) which link heterophily to high-frequency spectral responses.
- **Break condition:** If a graph exhibits "mixed" heterophily (complex label-signal correlations), a single domain might be insufficient, necessitating the "Dual-Domain" approach suggested in the paper's future work.

### Mechanism 3
- **Claim:** Finite spectral domains ($[-1, 1]$) provide superior numerical stability for high-degree polynomial approximations compared to semi-infinite domains ($[0, \infty)$).
- **Mechanism:** The finite bounds of the Jacobi domain ($[-1, 1]$) inherently constrain coefficient growth. When combined with LayerNorm, this prevents the gradient/activation explosion that occurs in semi-infinite domains (like LaguerreNet) where $O(k^2)$ polynomial coefficients are unbounded.
- **Core assumption:** LayerNorm alone is insufficient to stabilize unbounded coefficient growth on semi-infinite domains at very high degrees ($K > 20$).
- **Evidence anchors:**
  - [abstract] "...the [-1, 1] domain provides superior numerical stability at high polynomial degrees."
  - [section IV.C] Table III shows LaguerreNet collapsing at $K=25$ while L-JacobiNet remains stable up to $K=30$.
  - [corpus] LaguerreNet and GegenbauerNet papers (neighbors) discuss stability trade-offs but this specific high-K comparison is detailed here.
- **Break condition:** If specific bounded polynomials (like Krawtchouk) are used in the semi-infinite domain, stability might be improved, though the paper focuses on continuous polynomials for that domain.

## Foundational Learning

- **Concept: Spectral Graph Convolutions**
  - **Why needed here:** The entire paper is a comparison of how different polynomial bases (Chebyshev, Jacobi, Laguerre) approximate spectral filters $g_\theta(\Lambda)$. Without understanding that graph signals are filtered via the Laplacian eigenvalues, the domain trade-offs are unintelligible.
  - **Quick check question:** Explain the difference between filtering in the spatial domain (message passing) vs. the spectral domain (Laplacian eigenvalues).

- **Concept: Orthogonal Polynomials & Weight Functions**
  - **Why needed here:** L-JacobiNet adapts the "weight function" of the polynomials via $\alpha, \beta$. You must grasp that changing these parameters changes the *orthogonality* of the basis functions, thereby altering the filter's shape and inductive bias.
  - **Quick check question:** What happens to the shape of a Jacobi polynomial if you change the $\alpha$ and $\beta$ parameters away from -0.5 (Chebyshev)?

- **Concept: Heterophily vs. Homophily**
  - **Why needed here:** The paper identifies a critical failure mode of standard ChebyNet (low-pass bias) on heterophilic graphs. Distinguishing these graph types is essential to selecting the correct domain ($[0, \infty)$ vs $[-1, 1]$).
  - **Quick check question:** In a heterophilic graph (e.g., a social network where opposite political views connect), does the signal require a low-pass or high-pass filter?

## Architecture Onboarding

- **Component map:** Input features X and Symmetric Laplacian L_sym -> Domain Scaling (branch logic: [-1, 1] vs [0, ∞)) -> Polynomial Filter (Static Chebyshev vs Adaptive Jacobi) -> Stabilization (LayerNorm) -> Readout (Linear classification)
- **Critical path:** The **Domain Scaling** and **Stabilization** steps are the operational crux. S-JacobiNet is effectively `ChebyNet` + `LayerNorm` on the $\hat{L}$ domain.
- **Design tradeoffs:**
  - **Heterophily vs. Stability:** If your target graph is heterophilic, you *must* prioritize the $[0, \infty)$ domain (e.g., LaguerreNet). If you require deep propagation ($K > 20$) on a homophilic graph, you *must* prioritize the $[-1, 1]$ domain (S/L-JacobiNet).
  - **Complexity vs. Robustness:** L-JacobiNet (Adaptive) adds parameters and complexity but often underperforms the simpler S-JacobiNet (Static) due to overfitting.
- **Failure signatures:**
  - **Heterophily Failure:** High accuracy on homophilic datasets (Cora) but random-guess accuracy on heterophilic datasets (Texas) indicates you are likely using the $[-1, 1]$ domain or a static low-pass filter.
  - **High-K Collapse:** Accuracy dropping to ~0% or NaNs at $K > 20$ indicates you are likely using the $[0, \infty)$ domain (LaguerreNet) without sufficient stabilization or bounded coefficients.
- **First 3 experiments:**
  1. **Sanity Check (S-JacobiNet Baseline):** Implement `ChebyNet` and add `LayerNorm`. Run on Cora/CiteSeer. Confirm if performance matches or exceeds standard `ChebyNet`.
  2. **Heterophily Domain Test:** Train `S-JacobiNet` ($[-1, 1]$) vs. `LaguerreNet` ($[0, \infty)$) on a heterophilic benchmark (e.g., Texas). Verify the performance gap reported in Table II.
  3. **Stability Limit Test:** Increase polynomial degree $K$ from 2 to 30 on PubMed. Plot accuracy curves for `L-JacobiNet` vs. `LaguerreNet` to observe the "collapse" point of the semi-infinite domain.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can a "Dual-Domain" GNN effectively route spectral signals between a $[0, \infty)$ branch (for heterophily) and a $[-1, 1]$ branch (for stability) to leverage the strengths of both domains?
- Basis in paper: [explicit] Section V.D states, "A key research question would be how to route the signals; this could be achieved with a learnable gating mechanism or a spectral attention mechanism."
- Why unresolved: The paper identifies the trade-offs (heterophily vs. stability) and proposes the high-level architecture but does not implement or test the complex routing logic required to make the branches work together.
- What evidence would resolve it: A unified model architecture with a learned gating mechanism that outperforms single-domain baselines on a dataset containing both heterophilic signals and requiring deep stability.

### Open Question 2
- Question: Does the superiority of the static S-JacobiNet over the adaptive L-JacobiNet persist on large-scale datasets, or is it merely a result of regularization effects on the small benchmarks used in this study?
- Basis in paper: [inferred] Section V.C suggests L-JacobiNet's lower performance is due to overfitting on "small benchmark datasets," implying that the high variance of adaptive parameters might be beneficial if more data were available.
- Why unresolved: The experiments are limited to standard, relatively small benchmarks (Cora, CiteSeer, etc.), making it difficult to distinguish between a fundamental flaw in $[-1, 1]$ domain adaptation and a simple lack of data capacity.
- What evidence would resolve it: Benchmarking L-JacobiNet against S-JacobiNet on large-scale graph datasets (e.g., OGB) to see if the adaptive capacity provides statistically significant gains over the static baseline.

### Open Question 3
- Question: Can the numerical collapse of semi-infinite $[0, \infty)$ domain filters (like LaguerreNet) at high polynomial degrees ($K>25$) be prevented, or is instability an intrinsic property of the domain?
- Basis in paper: [inferred] Section IV.C notes that LaguerreNet "catastrophically collapses" at $K=25$, while the finite domain L-JacobiNet remains stable, suggesting a fundamental mathematical limitation of the semi-infinite domain.
- Why unresolved: The paper tests LayerNorm but does not explore if other stabilization techniques (e.g., gradient clipping, different normalizations) could rescue the semi-infinite domain at high depths.
- What evidence would resolve it: A mathematical analysis or empirical demonstration showing whether bounded coefficients are a strict requirement for stability at $K>20$, or if specific regularization can stabilize the unbounded $[0, \infty)$ domain.

## Limitations

- **LayerNorm Placement Unspecified:** The paper mentions a "LayerNorm framework" but does not specify if normalization is applied to the polynomial coefficients, the aggregated feature map, or the intermediate hidden states, creating ambiguity for reproduction.
- **Small Benchmark Scope:** All experiments are conducted on standard citation datasets (Cora, CiteSeer, PubMed) and small web graphs, limiting generalizability to more complex real-world scenarios with mixed heterophily patterns.
- **Overfitting Attribution:** The claim that adaptation causes overfitting is primarily based on benchmark results; this may not generalize to larger-scale datasets or may simply reflect insufficient regularization rather than a fundamental limitation of the adaptive approach.

## Confidence

- **High Confidence:** The numerical stability advantage of the [-1, 1] domain for high-degree polynomials is well-supported by the empirical results and mathematically sound.
- **Medium Confidence:** The heterophily performance gap between domains is convincing on the tested datasets, though the mechanism (low-pass bias) could benefit from more rigorous spectral analysis.
- **Low-Medium Confidence:** The claim that adaptation causes overfitting rather than improving performance is primarily based on benchmark results; this may not generalize to more challenging datasets or larger-scale problems.

## Next Checks

1. **LayerNorm Placement Verification:** Systematically test LayerNorm applied at different positions (after polynomial filtering, after aggregation, after hidden layer) to identify the critical stabilization point and validate the paper's implicit implementation.

2. **Complex Heterophily Benchmark:** Evaluate the domain trade-offs on a dataset with mixed heterophily patterns (e.g., WebKB with varied label distributions) to test whether the [-1, 1] domain's low-pass bias consistently harms performance or if more nuanced filtering is required.

3. **Overfitting Analysis on L-JacobiNet:** Conduct a controlled experiment with regularization (dropout, weight decay) on L-JacobiNet to determine if the observed overfitting is truly due to excessive adaptation or insufficient regularization, isolating the adaptation effect from capacity control.