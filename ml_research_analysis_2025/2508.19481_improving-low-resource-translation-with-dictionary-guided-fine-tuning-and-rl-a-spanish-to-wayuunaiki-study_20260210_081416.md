---
ver: rpa2
title: 'Improving Low-Resource Translation with Dictionary-Guided Fine-Tuning and
  RL: A Spanish-to-Wayuunaiki Study'
arxiv_id: '2508.19481'
source_url: https://arxiv.org/abs/2508.19481
tags:
- translation
- dictionary
- tool
- training
- bleu
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a novel method for improving machine translation
  for low-resource languages, focusing on the Spanish-Wayuunaiki language pair. The
  approach combines supervised fine-tuning with reinforcement learning (RL) and an
  external dictionary tool to create a tool-augmented translation system.
---

# Improving Low-Resource Translation with Dictionary-Guided Fine-Tuning and RL: A Spanish-to-Wayuunaiki Study

## Quick Facts
- **arXiv ID:** 2508.19481
- **Source URL:** https://arxiv.org/abs/2508.19481
- **Reference count:** 40
- **Primary result:** +3.37 BLEU improvement over prior work using dictionary-augmented RL fine-tuning

## Executive Summary
This paper proposes a novel method for improving machine translation for low-resource languages, focusing on the Spanish-Wayuunaiki language pair. The approach combines supervised fine-tuning with reinforcement learning (RL) and an external dictionary tool to create a tool-augmented translation system. The model is trained to selectively use the dictionary during translation, guided by BLEU similarity scores as rewards. The method achieves up to a +3.37 BLEU improvement over previous work and an 18% relative gain compared to a supervised baseline without dictionary access. The best-performing model, Qwen2.5-0.5B+SFT+RL, demonstrates extensive dictionary usage, employing it in every case and averaging 3.94 calls per sample. These findings highlight the promise of combining LLMs with external tools and RL in improving translation quality in low-resource language settings.

## Method Summary
The researchers developed a tool-augmented translation system that integrates an external dictionary through reinforcement learning. They started with Qwen2.5-0.5B pretrained on Spanish, performed supervised fine-tuning on a parallel corpus, then applied RL to optimize dictionary usage during translation. The RL component uses BLEU similarity scores as rewards, encouraging the model to selectively consult the dictionary when it improves translation quality. The dictionary is implemented as an external tool that can be called during inference, and the model learns when and how often to use it through the RL training process.

## Key Results
- Achieved up to +3.37 BLEU improvement over previous work
- Demonstrated 18% relative gain compared to supervised baseline without dictionary access
- Qwen2.5-0.5B+SFT+RL model used dictionary in every case, averaging 3.94 calls per sample
- BLEU scores improved from 5.48 to 7.10 for the best model configuration

## Why This Works (Mechanism)
The approach works by training the model to learn when dictionary consultation is beneficial rather than relying on the dictionary for every translation. This selective usage prevents over-reliance on external tools while leveraging them strategically for difficult translations. The BLEU-based reward signal provides clear guidance on when dictionary use improves output quality, creating a reinforcement learning environment where the model optimizes for both accuracy and efficiency. The method is particularly effective for agglutinative languages like Wayuunaiki, where words are formed by chaining subwords, making isolated dictionary lookups challenging but potentially valuable.

## Foundational Learning

**Reinforcement Learning in NLP:** Training models through reward-based optimization rather than direct supervision. Why needed: Enables learning complex behaviors like selective tool usage that are difficult to specify with supervised examples. Quick check: Verify the model's ability to optimize for the defined reward function without overfitting to specific patterns.

**Low-resource Language Translation:** Techniques for handling language pairs with limited parallel corpora. Why needed: Standard translation models require large datasets that don't exist for many language pairs. Quick check: Compare performance against models trained on the same limited data to ensure genuine improvement.

**Tool-Augmented Language Models:** Integrating external knowledge sources and tools into language model workflows. Why needed: Allows models to access specialized knowledge without increasing model size or training data requirements. Quick check: Measure dictionary usage patterns and correlate with translation quality improvements.

**BLEU Score Evaluation:** Automated metric comparing machine translations against reference translations using n-gram precision. Why needed: Provides objective, reproducible evaluation for translation quality. Quick check: Ensure reference translations are of high quality and representative of target language usage.

## Architecture Onboarding

**Component Map:** Input Text -> Language Model (Qwen2.5-0.5B) -> Dictionary Tool (Optional) -> Output Translation

**Critical Path:** Spanish Input → SFT Fine-tuned Model → RL Policy (Dictionary Decision) → BLEU Reward Computation → Parameter Updates

**Design Tradeoffs:** The system balances between pure model-based translation and tool-augmented approaches. Using an external dictionary adds computational overhead and complexity but provides access to specialized knowledge that may not be captured in the model parameters. The RL component adds training time but enables adaptive tool usage rather than rigid, always-on dictionary consultation.

**Failure Signatures:** Performance degradation when dictionary calls are made unnecessarily or when the dictionary lacks coverage for domain-specific terminology. Models may also fail to generalize beyond the specific patterns in the training data.

**First Experiments:**
1. Baseline comparison: Train and evaluate without RL or dictionary integration
2. Ablation study: Test with RL but no dictionary, then with dictionary but no RL
3. Usage pattern analysis: Track when and how often the model uses the dictionary during inference

## Open Questions the Paper Calls Out

**Open Question 1:** Why does the CharacTer reward signal lead to performance regression in low-resource translation RL while BLEU signals improve performance?
- **Basis in paper:** The Discussion section notes that CharacTer led to regressions and states, "Future research could investigate the properties that make a reward function effective."
- **Why unresolved:** The authors observed that character-level matching (CharacTer) degraded BLEU scores by 10.4% but did not determine the specific linguistic or optimization dynamics that caused this failure compared to BLEU.
- **What evidence would resolve it:** An analysis of reward correlation with human judgment in low-resource settings, or an ablation study testing hybrid reward functions on morphological rich languages.

**Open Question 2:** How do native speakers qualitatively rate the fluency and cultural adequacy of tool-augmented translations compared to automatic metrics?
- **Basis in paper:** The Limitations section states that "a thorough qualitative analysis of the generated translations by native speakers is still pending."
- **Why unresolved:** The study relied entirely on BLEU scores for evaluation because the researchers lacked access to native Wayuunaiki speakers during the experimental phase.
- **What evidence would resolve it:** Conducting a human evaluation study using native Wayuunaiki speakers to assess the semantic fidelity and cultural appropriateness of the model outputs.

**Open Question 3:** Does dictionary-augmented RL provide greater relative gains for non-agglutinative languages compared to the agglutinative structure of Wayuunaiki?
- **Basis in paper:** The Discussion section hypothesizes that "for non-agglutinative languages, the benefits could be even greater" since words are easier to translate independently.
- **Why unresolved:** The current study is restricted to Spanish-to-Wayuunaiki, an agglutinative language where words are formed by chaining subwords, complicating the isolated dictionary lookup process.
- **What evidence would resolve it:** Applying the identical SFT+RL pipeline to a low-resource language pair featuring isolating or fusional morphology and comparing the relative BLEU improvements.

## Limitations
- Limited to single language pair (Spanish-Wayuunaiki), restricting generalizability to other low-resource settings
- Relies solely on BLEU score for evaluation, which may not fully capture translation quality for morphologically rich languages
- Does not address computational overhead or practical deployment considerations of the dictionary-assisted approach
- No investigation of performance degradation in domains not covered by the external dictionary

## Confidence

**High Confidence:** The reported BLEU score improvements (+3.37 over previous work, +18% relative to baseline) are based on direct quantitative comparisons using standard evaluation metrics.

**Medium Confidence:** The claim that the model "selectively" uses the dictionary is supported by average usage statistics (3.94 calls per sample) but lacks deeper analysis of when and why dictionary usage is triggered.

**Medium Confidence:** The generalizability of the approach to other low-resource language pairs is suggested by the methodology but not empirically validated beyond the Spanish-to-Wayuunaiki case.

## Next Checks
1. Test the dictionary-guided RL approach on additional low-resource language pairs to assess cross-linguistic generalizability and identify language-specific factors affecting performance.
2. Conduct human evaluation studies to complement BLEU scores and assess translation quality in terms of fluency, adequacy, and preservation of cultural nuances, particularly for morphologically complex languages.
3. Analyze the computational overhead and inference latency introduced by the dictionary-assisted approach to evaluate its practical deployment feasibility compared to standard supervised fine-tuning.