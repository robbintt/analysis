---
ver: rpa2
title: 'Random Initialization Can''t Catch Up: The Advantage of Language Model Transfer
  for Time Series Forecasting'
arxiv_id: '2506.21570'
source_url: https://arxiv.org/abs/2506.21570
tags:
- time
- series
- language
- transfer
- loss
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the effective transfer of pre-trained language
  models (LMs) to time series forecasting in low-data regimes. The authors examine
  how different design choices impact transfer performance, including upstream post-training,
  tokenization methods, and model size.
---

# Random Initialization Can't Catch Up: The Advantage of Language Model Transfer for Time Series Forecasting

## Quick Facts
- arXiv ID: 2506.21570
- Source URL: https://arxiv.org/abs/2506.21570
- Reference count: 10
- Primary result: Pre-trained language models consistently outperform randomly initialized models for time series forecasting, with a persistent transfer gap that doesn't vanish with training.

## Executive Summary
This paper demonstrates that pre-trained language models (LMs) provide significant advantages when transferred to time series forecasting tasks, particularly in low-data regimes. The authors systematically evaluate how different initialization strategies, tokenization methods, and model sizes affect transfer performance. Their experiments reveal a persistent "transfer gap" where models initialized with LM weights consistently achieve lower validation loss than randomly initialized models, regardless of training duration or design choices. Notably, they find that instruction-tuned models (Flan-T5) initially perform worse than random initialization, suggesting that upstream language tuning may negatively impact transfer to time series tasks.

## Method Summary
The study adapts T5-efficient encoder-decoder transformers to univariate time series forecasting using the LOTSA dataset. Three model sizes (60M, 220M, 770M parameters) are tested with three initialization strategies: random (HuggingFace defaults), pre-trained T5, and Flan-T5 instruction-tuned. Three tokenization approaches are compared: naive (direct value projection), lag (current value plus lag indices), and bin (4096 linearly-spaced bins treating forecasting as classification). The embedding layer is replaced for pre-trained models—discrete uses first B vocab vectors, continuous uses mean vocab vector. Training uses hyperparameter sweeps with learning rate (1e-4 to 1e-3), batch size (64-128), weight decay (0-0.1), and warmup (0-2%).

## Key Results
- Models initialized with pre-trained LM weights consistently outperform randomly initialized models with a non-vanishing transfer gap that persists across all design choices
- Bin tokenization achieves the lowest validation loss and smoothest convergence by aligning time series data with discrete token expectations
- Larger models show greater effective transfer in low-data regimes, with differences most pronounced at the start of training
- Instruction-tuned models (Flan-T5) initially underperform random initialization, suggesting upstream language tuning may harm cross-modal transfer

## Why This Works (Mechanism)

### Mechanism 1: Non-Vanishing Transfer Gap from Pre-trained Initialization
Pre-training biases model parameters toward weight regions well-suited for temporal patterns, providing a favorable starting point in the loss landscape. Random initialization converges to higher-loss plateaus early and cannot escape, creating a persistent gap.

### Mechanism 2: Tokenization-Representation Alignment via Discretization
Binning reduces vocabulary size and recasts forecasting as classification, aligning with discrete token expectations learned during language pre-training. This compatibility reduces representation mismatch and yields smoother training curves.

### Mechanism 3: Inverse Relationship Between Upstream Specialization and Downstream Transfer
Instruction tuning may deform the representation landscape to optimize for language-specific task structure, creating a "jagged" parameter space less amenable to cross-modal transfer. This manifests as initial underperformance and noisier training curves.

## Foundational Learning

- **Transfer Learning and Effective Data Transfer**: Understanding how to quantify transfer as data savings (DT(ℓ) = L⁻¹R(ℓ) - L⁻¹P(ℓ)) is essential for interpreting the transfer gap results.
  - Quick check: Given a validation loss target ℓ, how many fewer training tokens does a pre-trained model require compared to random initialization?

- **Tokenization as Representation Engineering**: Different tokenization strategies (discrete vs. continuous) materially affect transformer attention and convergence behavior.
  - Quick check: Why would a model designed for discrete language tokens struggle with continuous numerical inputs?

- **Loss Landscape Geometry and Training Dynamics**: Pre-trained and random models exhibit different convergence patterns—random plateaus early while pre-trained continues improving, suggesting different basins in the loss landscape.
  - Quick check: What does it mean when one initialization plateaus while another continues decreasing, and what does this imply about their respective local minima?

## Architecture Onboarding

- **Component map**: T5-Efficient backbone -> Tokenizer + Embedding layer -> Probabilistic forecasting head
- **Critical path**: 1) Select backbone size based on data regime (larger for lower data) 2) Choose tokenizer (bin recommended) 3) Initialize embedding layer appropriately 4) Train on LOTSA with standard hyperparameter sweep
- **Design tradeoffs**: Bin tokenization offers smoother training and lower loss but loses precise numerical resolution; Flan-T5 gives better language performance but worse initial transfer; larger models provide greater transfer advantage but require more compute
- **Failure signatures**: Validation loss plateauing early suggests random initialization or suboptimal hyperparameters; noisy, spiky loss curves may indicate instruction-tuned initialization or learning rate too high; higher-than-expected zero-shot loss likely indicates tokenizer-embedding mismatch
- **First 3 experiments**: 1) Baseline comparison: pre-trained vs. random initialization with bin tokenization on same hyperparameters 2) Tokenizer ablation: compare bin vs. lag vs. naive with pre-trained T5-base 3) Scale sensitivity: test pre-trained T5 across small/base/large variants in low-data regime

## Open Questions the Paper Calls Out

- **Open Question 1**: Is effective transfer due to increased training data volume or shared structural distributional similarities between language and time series? The paper cannot disentangle whether benefits come from pre-training scale versus intrinsic modality similarities.
- **Open Question 2**: Why does upstream instruction-tuning degrade transfer performance compared to purely pre-trained weights? The paper observes Flan-T5 underperforms initially but only conjectures about "jagged" representation landscapes.
- **Open Question 3**: Do transfer findings generalize beyond T5 architecture to other language model backbones? All results use T5-efficient variants, leaving open whether decoder-only or other encoder-decoder architectures show similar effects.

## Limitations
- Tokenization range specification: Bin scheme uses B=4096 bins but range [-a, a] is unspecified, creating ambiguity in exact discretization
- Data preprocessing specifics: LOTSA dataset is referenced but exact normalization parameters, train/validation splits, and sampling strategy are unspecified
- Upstream model provenance: "T5-Efficient" pre-trained weights lack clarity on whether from original T5 paper or subsequent variant

## Confidence
- High confidence: Core finding that pre-trained LM initialization consistently outperforms random initialization with non-vanishing transfer gap
- Medium confidence: Observation that instruction-tuned models initially underperform random initialization, though mechanism remains speculative
- Low confidence: Exact quantitative impact of tokenizer choice depends on unspecified parameters like bin range and lag indices

## Next Checks
1. **Parameter sensitivity test**: Systematically vary the bin range parameter 'a' (e.g., [0.5, 1, 2, 3]) and measure resulting validation loss and convergence speed
2. **Cross-dataset generalization**: Replicate pre-trained vs. random initialization comparison on a second time series forecasting dataset (e.g., M4 competition data)
3. **Instruction tuning ablation**: Compare T5 base, Flan-T5, and T5 with intermediate fine-tuning to isolate whether language-specific instruction tuning or broader pre-training objectives cause initial underperformance