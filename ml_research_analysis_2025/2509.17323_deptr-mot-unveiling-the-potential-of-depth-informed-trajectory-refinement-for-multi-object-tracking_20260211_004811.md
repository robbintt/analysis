---
ver: rpa2
title: 'DepTR-MOT: Unveiling the Potential of Depth-Informed Trajectory Refinement
  for Multi-Object Tracking'
arxiv_id: '2509.17323'
source_url: https://arxiv.org/abs/2509.17323
tags:
- depth
- tracking
- cues
- deptr
- detection
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces DepTR-MOT, a method that extends standard
  2D detectors with instance-level depth perception for multi-object tracking. It
  addresses the limitations of 2D-only tracking methods in handling occlusions and
  close-proximity interactions by incorporating depth cues without requiring explicit
  3D annotations.
---

# DepTR-MOT: Unveiling the Potential of Depth-Informed Trajectory Refinement for Multi-Object Tracking

## Quick Facts
- arXiv ID: 2509.17323
- Source URL: https://arxiv.org/abs/2509.17323
- Reference count: 40
- Primary result: Achieves HOTA scores of 27.59 (DanceTrack) and 44.47 (QuadTrack) with average gains of +2.2 HOTA and +2.9 IDF1 across trackers on QuadTrack

## Executive Summary
DepTR-MOT introduces a method to enhance multi-object tracking by integrating instance-level depth perception into standard 2D detectors. The approach leverages knowledge distillation from foundation models to train detectors to output depth information during inference, without requiring explicit 3D annotations. By incorporating depth cues into the tracking-by-detection pipeline, the method significantly improves robustness in challenging scenarios involving occlusions and close-proximity interactions.

## Method Summary
DepTR-MOT extends the Tracking-by-Detection paradigm by adding depth perception to the detection stage. It uses foundation models (Video Depth Anything and SAM2) to generate soft depth labels from 2D bounding boxes during training. A DETR-based detector with a Depth Awareness Module is trained to predict both 2D boxes and instance-level depth values. During inference, the predicted depth is fused into the tracker's association cost matrix, improving object linking across frames. The method is evaluated on DanceTrack and QuadTrack datasets, showing substantial improvements in tracking accuracy.

## Key Results
- HOTA scores of 27.59 (DanceTrack) and 44.47 (QuadTrack)
- Average gains of +2.2 HOTA and +2.9 IDF1 across trackers on QuadTrack
- Enhanced robustness in occlusion and close-proximity scenarios

## Why This Works (Mechanism)
The method addresses a fundamental limitation of 2D-only tracking: occlusions and close-proximity interactions create ambiguous 2D spatial cues. By predicting instance-level depth, the tracker gains an additional dimension to disambiguate objects. The depth-aware association layer fuses this depth information into the matching cost, reducing identity switches and improving trajectory consistency.

## Foundational Learning

- **Concept: Tracking-by-Detection (TBD) paradigm.**
  - **Why needed here:** DepTR-MOT is designed to drop into existing TBD systems. You must understand that these systems first find all objects in a frame and then link them across time. The method improves the "linking" part by adding depth to the detection output.
  - **Quick check question:** Can you explain the two distinct stages of a generic Tracking-by-Detection pipeline and where DepTR-MOT fits in?

- **Concept: Knowledge Distillation from Foundation Models.**
  - **Why needed here:** The core training strategy is "no-depth-label." The model doesn't learn from ground truth depth sensors. It learns to mimic a large, pre-trained model (Video Depth Anything). Understanding that the goal is to transfer this capability to a smaller, faster model is critical.
  - **Quick check question:** Why is a direct MSE loss against the output of the teacher model (Video Depth Anything) considered a form of distillation rather than standard supervised learning?

- **Concept: Data Association in Multi-Object Tracking (MOT).**
  - **Why needed here:** The ultimate output is an improved tracker. The key metric is how well the system links a detection in frame `t` to the correct trajectory in frame `t-1`. The depth cue is a new feature for this matching process.
  - **Quick check question:** In a standard MOT system using IoU matching, what specifically causes a failure during an occlusion? How does an additional depth cue conceptually solve this?

## Architecture Onboarding

- **Component map:**
  1. **Promptable Depth Estimation Model (PEDM):** Training-only. Takes video + 2D boxes. Uses SAM2 (for masks) and Video Depth Anything (for depth maps) to generate instance-level soft labels.
  2. **DepTR Detector (Encoder + Decoder):** The main deployable model. A DETR-based architecture.
  3. **Depth Awareness Module:** A sub-component within the Decoder. Uses the box prediction as an anchor to predict a depth offset and value.
  4. **Depth-Aware Association Layer:** Post-detection logic. Fuses the predicted depth into the matching cost matrix for the tracker.

- **Critical path:**
  1. **Training Path:** `Image` -> `Video Depth Anything` -> `Dense Depth Map`. `Image` + `Box Annot.` -> `SAM2` -> `Mask`. `Mask` + `Dense Depth Map` -> `Soft Label` (this is the key supervision).
  2. **Inference Path (Depth Prediction):** `Image` -> `DepTR Encoder` -> `Decoder (Box Branch)` -> `Depth Awareness Module` -> `Depth Value`.
  3. **Inference Path (Tracking):** `DepTR Output (Boxes + Depth)` -> `Tracker's Cost Matrix` -> `Association (e.g., with ByteTrack)` -> `Final Trajectories`.

- **Design tradeoffs:**
  - **Real-time performance vs. Depth Quality:** The method avoids running the heavy foundation models (SAM2, Video Depth Anything) at inference, accepting a potentially lower depth accuracy for a massive gain in speed (FPS).
  - **2D Detector Stability vs. Depth Learning:** The ablation study (Table III) suggests that learning depth can be unstable without proper global supervision (`L_align`). An improper setup could degrade the primary 2D detection task.
  - **Hyperparameter Sensitivity:** The depth fusion weight `γ` is critical. As seen in Table V, over-weighting depth cues can catastrophically lower performance.

- **Failure signatures:**
  - **High Identity Switches despite Depth:** If `γ` is too low, the depth cue is ignored. If it's too high, objects at similar depths but different 2D locations might be incorrectly associated.
  - **Noisy Depth Output:** If the distillation from the teacher model is insufficient, the predicted depth may be inconsistent across frames, leading to erratic association scores. This can be diagnosed by visualizing the predicted depth values for a single tracked object over time.

- **First 3 experiments:**
  1. **Ablation on Fusion Weight:** Run the tracker on a validation set while sweeping the depth fusion weight `γ` (e.g., from 0.0 to 0.8) with a fixed `λ`. Plot HOTA/IDF1 scores to find the optimal balance point, as suggested by Table V.
  2. **Label Quality Ablation:** Train two models: one using the full PEDM pipeline (SAM2 + Depth Anything) for labels, and a simplified version using just a bounding-box-cropped depth average. Compare their tracking performance to validate the mechanism of instance-masking.
  3. **Teacher vs. Student Analysis:** On a small test set, run inference with both the heavy foundation model (the teacher) and the trained DepTR (the student). Compare the depth values directly (correlation, MSE) to quantify the quality of the distillation.

## Open Questions the Paper Calls Out

- **Open Question 1:** Can the proposed depth-aware training strategy be effectively generalized to end-to-end joint-detection-and-tracking frameworks?
  - **Basis in paper:** [explicit] The authors state their current design is "mainly targeted at TBD-based trackers" and explicitly list extending the strategy to "end-to-end tracking frameworks" as future work.
  - **Why unresolved:** End-to-end trackers (e.g., MOTR, TrackFormer) couple detection and association in a single Transformer, whereas DepTR-MOT is currently optimized for decoupled Tracking-by-Detection pipelines where depth aids a separate association step.
  - **What evidence would resolve it:** Successful integration of the depth distillation loss into an end-to-end query propagation architecture, resulting in improved AssA and IDF1 scores on benchmarks like DanceTrack without disrupting the joint optimization.

- **Open Question 2:** How can the fusion of depth cues be made adaptive to prevent performance degradation in scenarios where depth estimation is unreliable?
  - **Basis in paper:** [inferred] Table V reveals high sensitivity to the depth weighting hyperparameter `γ`; performance drops sharply when `γ > 0.4` because the model overemphasizes depth, leading to "incorrect associations."
  - **Why unresolved:** The current method relies on a fixed weighting scheme (`λ` and `γ`), which cannot dynamically adjust when the underlying depth estimation is noisy or ambiguous (e.g., under severe occlusion).
  - **What evidence would resolve it:** The development of a dynamic weighting mechanism or uncertainty-aware gating that adjusts the influence of depth cues per-track or per-frame, maintaining high HOTA scores even as scene depth reliability fluctuates.

- **Open Question 3:** Does distillation from foundation models impose a performance ceiling or domain bias compared to learning from ground truth depth?
  - **Basis in paper:** [inferred] The method relies on "foundation model-based soft depth labels" (Video Depth Anything) because ground truth depth is unavailable, but it does not validate the *accuracy* of these soft labels, only the downstream tracking gain.
  - **Why unresolved:** The student model (DepTR) might inherit systematic errors or biases from the teacher foundation model, potentially limiting tracking robustness in environments where the foundation model's depth estimation fails.
  - **What evidence would resolve it:** A comparative analysis on a dataset with LiDAR or synthetic ground truth depth to quantify the gap between DepTR's predictions and the foundation model's soft labels versus true metric depth.

## Limitations

- The method requires computationally expensive foundation models (Video Depth Anything, SAM2) during training, which may limit accessibility.
- Performance is sensitive to the depth fusion weight γ, with a narrow optimal range (Table V); improper tuning can lead to catastrophic performance drops.
- The approach assumes a stable camera and calibrated depth scale, which may not hold in dynamic or unstructured environments.

## Confidence

- **High Confidence:** The architectural design (DETR-based detector with Depth Awareness Module) and training pipeline (knowledge distillation from Video Depth Anything) are clearly specified and technically sound.
- **Medium Confidence:** The empirical results on DanceTrack and QuadTrack are reproducible based on the provided details, but generalization to other MOT benchmarks (e.g., MOT17, nuScenes) remains unproven.
- **Low Confidence:** The paper claims robustness without extensive testing on diverse occlusion patterns or sensor noise; the reliance on instance masks from SAM2 introduces a potential failure point if the segmentation is inaccurate.

## Next Checks

1. **Generalization Test:** Evaluate DepTR-MOT on a standard MOT benchmark (e.g., MOT17) to assess performance outside the specialized DanceTrack and QuadTrack domains.
2. **Failure Case Analysis:** Systematically test the method under varying occlusion levels and depth ambiguities (e.g., objects at identical depths but different 2D positions) to identify failure modes.
3. **Efficiency Benchmark:** Measure the computational overhead of the Depth Awareness Module and its impact on real-time tracking performance compared to baseline detectors.