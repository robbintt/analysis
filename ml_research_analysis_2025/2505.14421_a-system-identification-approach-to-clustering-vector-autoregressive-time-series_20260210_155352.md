---
ver: rpa2
title: A system identification approach to clustering vector autoregressive time series
arxiv_id: '2505.14421'
source_url: https://arxiv.org/abs/2505.14421
tags:
- time
- clustering
- series
- algorithm
- k-lmvar
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of clustering vector autoregressive
  (VAR) time series by explicitly considering their underlying dynamics, addressing
  a gap in current clustering methods that often ignore autocorrelation patterns.
  The authors propose a system identification approach using a mixture VAR model.
---

# A system identification approach to clustering vector autoregressive time series

## Quick Facts
- arXiv ID: 2505.14421
- Source URL: https://arxiv.org/abs/2505.14421
- Authors: Zuogong Yue; Xinyi Wang; Victor Solo
- Reference count: 40
- One-line primary result: A computationally stable hard-clustering algorithm for VAR time series that scales to large dimensions and selects model order via extended BIC.

## Executive Summary
This paper addresses the challenge of clustering vector autoregressive (VAR) time series by explicitly considering their underlying dynamics, addressing a gap in current clustering methods that often ignore autocorrelation patterns. The authors propose a system identification approach using a mixture VAR model. They first develop an EM-based soft clustering algorithm (cMVAR) but encounter computational issues due to numerical underflow. To overcome this, they derive a simplified hard-clustering algorithm (k-LMVAR) by taking a small-noise limit of cMVAR, which is computationally manageable and avoids numerical problems. They also develop a BIC criterion for model selection.

## Method Summary
The method involves clustering N vector autoregressive time series into K clusters based on their underlying VAR dynamics. The core algorithm, k-LMVAR, iterates between label updates (assigning each time series to its nearest cluster by minimizing a quadratic dissimilarity) and parameter updates (re-estimating VAR parameters for each cluster via weighted least squares). The approach uses QR decomposition for numerical stability in parameter updates and Cholesky decomposition for computing dissimilarities. Model selection is performed using an extended BIC criterion that accounts for the discrete nature of the model space. The algorithm is initialized either randomly or via a naive two-step procedure that fits individual VARs and clusters their parameters.

## Key Results
- k-LMVAR achieves higher clustering accuracy (measured by Rand Index and NMI) than state-of-the-art methods on synthetic VAR data.
- The algorithm scales linearly with the number of time series and handles large dimensions (m=6, K=84) where EM-based approaches fail due to numerical underflow.
- Extended BIC successfully identifies the true number of clusters in synthetic experiments, with minimum BIC achieved at ground truth K=10.

## Why This Works (Mechanism)

### Mechanism 1: Small-Noise Limit Eliminates Numerical Underflow
Taking the small-noise limit (γ → 0) of the soft-clustering cMVAR algorithm produces a hard-clustering variant (k-LMVAR) that avoids exponential underflow while preserving clustering accuracy. In cMVAR, the posterior probability τn,k involves exp(−½ψn,k) where ψn,k sums quadratic forms over T time points and m dimensions. When T or m is large, ψn,k becomes very large, causing exp(−½ψn,k) to underflow to zero. By parameterizing the covariance as Ωk = γk·Ω̃k and taking γ → 0, the soft assignment τn,k → τ*n,k converges to a binary indicator: τ*n,k = 1 if k = arg min ψn,k, else 0. This removes the exponential computation entirely.

### Mechanism 2: Coordinate Descent on a Constructed Objective Guarantees Convergence
k-LMVAR can be interpreted as coordinate descent on the objective f(τ, Θ̃, Ω̃) = Σn Σk τn,k Dn,k(Θ̃k, Ω̃k) with dissimilarity Dn,k = (T−p)log|Ω̃k| + Σt e′nkt Ω̃k−1 enkt, guaranteeing non-increasing cost and finite termination. The algorithm alternates between (1) label update: minimize over τ with fixed parameters, which reduces to assigning each series to its nearest cluster by Dn,k; (2) parameter update: minimize over Θ̃, Ω̃ with fixed labels, solved by weighted least squares. Each step reduces or maintains f, and since there are finitely many label configurations, the algorithm terminates.

### Mechanism 3: Extended BIC Addresses Model Space Size Bias
Standard BIC favors larger K because it implicitly weights model classes proportionally to their size; extended BIC (with γ > 0) corrects this by adding a term 2γ log card(Sj) that penalizes larger model spaces. The model space partitions by K into sets Sj of size (Np+K−1 choose K). Extended BIC adds 2γ log card(Sj) to the standard penalty. With γ = 1, each Sj has equal prior probability; γ = 0.5 handles cases where NkNp = O((NT)^κ) with κ < 1.

## Foundational Learning

- Concept: Vector Autoregressive (VAR) Model Stability
  - Why needed here: The algorithm assumes each time series is generated by a stable VAR; instability would cause explosive behavior and invalid likelihood.
  - Quick check question: Given coefficient matrices Θ1,...,Θp, how would you verify the VAR(p) is stable? (Answer: Check that all roots of det(I − Θ1z − ... − Θpzp) = 0 lie outside the unit circle.)

- Concept: EM Algorithm for Mixture Models
  - Why needed here: cMVAR is derived via EM; understanding E-step (posterior computation) and M-step (parameter maximization) clarifies why underflow occurs.
  - Quick check question: In a Gaussian mixture, what does the E-step compute for each data point? (Answer: The posterior probability that the point belongs to each component.)

- Concept: QR and Cholesky Decompositions for Numerical Stability
  - Why needed here: The paper uses QR for least squares (Θ̃k update) and Cholesky for covariance inversion (ψn,k computation) to avoid numerical issues.
  - Quick check question: Why is solving Lk x = enkt via forward substitution more stable than explicitly computing Lk−1? (Answer: Explicit inversion amplifies rounding errors; forward substitution operates on the original well-conditioned matrix.)

## Architecture Onboarding

- Component map: Pre-computation (QR decompositions) -> Initialization (random or naive 2-step) -> Main loop (label update -> parameter update) -> Convergence check -> Model selection (Extended BIC)
- Critical path: Pre-compute QR decompositions (O(NT(1+mp)²) total) -> Initialize cluster labels and parameters -> Iterate: label assignment (O(NK(T−p)m²)) → parameter update (O(K|Ik|(1+mp)³)) until convergence -> Compute BIC for each candidate (K, p); select minimum
- Design tradeoffs: Random initialization is faster but may require more iterations; naive 2-step provides better starting points but costs O(N) VAR fits upfront. Common vs cluster-specific VAR orders: Setting p1 = ... = pK simplifies computation (one Xnk per series, not K); the paper notes clustering is "not sensitive to the choice of model order p." Parallelism: Label assignment (O(NK) independent) and parameter updates (O(K) independent) parallelize well; QR pre-computation (O(N) independent) also parallelizes.
- Failure signatures: Empty cluster: If Ik = ∅ for some k, parameter update divides by zero. Mitigation: Re-initialize or merge with nearest cluster. Singular covariance: If residuals within a cluster are collinear, Ωk is singular. Mitigation: Add small diagonal regularization or reduce cluster count. Slow convergence: For large K, poor initialization increases iterations. The paper observes "increasing variance of computation time when K goes large." Underflow in cMVAR: For T > 400 or m > 6, cMVAR fails "100% of the time" (Figure 3). k-LMVAR avoids this entirely.
- First 3 experiments: (1) Reproduce the scalability benchmark (Figure 2): Generate synthetic VAR data with m = 6, T = 100, vary K from 2 to 84; verify k-LMVAR computation time scales linearly while cMVAR fails beyond K ≈ 40. (2) Validate BIC model selection: Generate data with known K = 10, p = 5; run k-LMVAR for K ∈ {2,4,...,20} and p ∈ {2,...,8}; confirm BIC minimum at (K=10, p=5). (3) Test robustness to initialization: Run k-LMVAR on the same dataset with (a) random label initialization, (b) naive 2-step initialization; compare clustering accuracy (RI, NMI) and iteration count. Expect naive 2-step to converge faster but achieve similar final accuracy.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can a general derivation of the hard-clustering algorithm be found that bypasses the EM-based soft-clustering starting point entirely?
- Basis in paper: [explicit] The authors state, "We have not yet found a general derivation that dispenses with the EM setting."
- Why unresolved: The current derivation relies on taking a "small-noise limit" of the computationally prohibitive cMVAR algorithm to arrive at k-LMVAR. A direct derivation might offer better theoretical insights or computational stability.
- What evidence would resolve it: A mathematical proof that derives the k-LMVAR update equations directly from a cost function or optimality condition without invoking the mixture model limit.

### Open Question 2
- Question: Does the proposed BIC criterion, which utilizes a surrogate log-likelihood, theoretically guarantee consistency in model selection?
- Basis in paper: [explicit] The paper notes that applying BIC to the k-means-like k-LMVAR is "not easy" and admits to using a "surrogate" construction rather than an exact likelihood.
- Why unresolved: While the Extended BIC is adapted heuristically, the theoretical properties of using a surrogate likelihood for this specific clustering algorithm remain unverified.
- What evidence would resolve it: A theoretical analysis proving that the proposed Extended BIC criterion converges to the true model order and cluster number as data size increases.

### Open Question 3
- Question: How robust is the k-LMVAR algorithm to violations of the VAR and small-noise assumptions in real-world applications?
- Basis in paper: [inferred] The experimental validation is conducted exclusively on synthetic data generated by stable VAR models, whereas the introduction lists applications (e.g., stock prices, ECG) that often exhibit non-stationarity or complex noise.
- Why unresolved: The algorithm is theoretically derived using a "small-noise limit"; it is unclear if the method degrades gracefully when this limit is violated or when the data is not strictly VAR.
- What evidence would resolve it: Comparative studies on real-world benchmarks (e.g., financial or biological time series) demonstrating performance against baselines when model assumptions are not strictly met.

## Limitations
- The convergence proof depends on data sufficiency assumptions that may not hold in high-dimensional, short time-series regimes.
- Model selection via extended BIC is novel for VAR clustering but lacks comparative validation against alternative criteria.
- Claims about robustness to VAR order selection and initialization strategies are supported by limited experiments.

## Confidence
- High confidence: The mechanism of numerical underflow in cMVAR and its resolution via hard clustering (k-LMVAR) is well-explained and theoretically sound.
- Medium confidence: The coordinate descent convergence proof is valid but assumes data conditions that may not always hold in practice.
- Medium confidence: Extended BIC formulation is mathematically rigorous but its superiority over standard BIC needs more empirical validation.
- Low confidence: Claims about robustness to VAR order selection and initialization strategies are supported by limited experiments.

## Next Checks
1. Test k-LMVAR on real-world multivariate time series datasets (e.g., EEG, climate, financial data) to verify scalability and accuracy beyond synthetic data.
2. Evaluate model selection performance by comparing extended BIC against cross-validation and information criteria from related work on VAR clustering.
3. Investigate the impact of non-uniform noise scales across clusters by generating data with varying γk values and measuring clustering accuracy degradation.