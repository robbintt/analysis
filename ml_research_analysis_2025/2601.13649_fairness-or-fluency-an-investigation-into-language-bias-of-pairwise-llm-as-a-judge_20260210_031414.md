---
ver: rpa2
title: Fairness or Fluency? An Investigation into Language Bias of Pairwise LLM-as-a-Judge
arxiv_id: '2601.13649'
source_url: https://arxiv.org/abs/2601.13649
tags:
- language
- answer
- bias
- languages
- perplexity
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates language bias in pairwise LLM-as-a-judge
  systems, focusing on performance disparities between languages when judging same-language
  pairs and preferences when judging cross-language pairs. The authors use the MMMLU
  dataset with 14 languages and evaluate multiple judge models including GPT-5.1,
  various Qwen and Llama models, Aya-Expanse, and expert judge models.
---

# Fairness or Fluency? An Investigation into Language Bias of Pairwise LLM-as-a-Judge

## Quick Facts
- arXiv ID: 2601.13649
- Source URL: https://arxiv.org/abs/2601.13649
- Authors: Xiaolin Zhou; Zheng Luo; Yicheng Gao; Qixuan Chen; Xiyang Hu; Yue Zhao; Ruishan Liu
- Reference count: 40
- Key outcome: Language bias in pairwise LLM-as-a-judge systems persists beyond perplexity effects, with European languages consistently outperforming African languages and English answers being preferred in cross-language comparisons.

## Executive Summary
This paper investigates language bias in pairwise LLM-as-a-judge systems using the MMMLU dataset with 14 languages. The authors examine two scenarios: same-language judging (where performance varies by language family) and inter-language judging (where English answers are systematically preferred). Through comprehensive experiments across multiple judge models, they find that language bias cannot be fully explained by low-perplexity bias alone, demonstrating that language identity explains significant additional variance in model preferences beyond what perplexity accounts for. The bias is particularly pronounced for culturally-related subjects and low-resource languages.

## Method Summary
The study uses the MMMLU dataset with 14 languages and constructs pairwise comparison tasks between ground-truth answers and adjacent incorrect options in a cyclic manner. For same-language judging, accuracy is measured per language family, while inter-language judging compares English answers against target language answers across four configurations (EN,EN; EN,TG; TG,EN; TG,TG). The authors measure "Answer Effect" to quantify English preference and conduct regression analysis comparing reduced models (perplexity only) against full models (perplexity + language identity) using F-tests to determine if language identity explains significant additional variance beyond perplexity.

## Key Results
- European languages consistently outperform African languages in same-language judging, with performance disparities more pronounced in culturally-related subjects like Social Sciences and Humanities compared to STEM domains
- In inter-language judging, most models favor English answers, with answer language having more influence on preference than question language
- While perplexity shows slight correlation with language bias, language identity explains significant additional variance beyond perplexity alone, especially for low-resource languages like Yoruba
- Position bias is controlled by averaging results over swapped option orders, ensuring observed preferences reflect content rather than ordering effects

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** In inter-language judging, LLMs systematically prefer English answers over those in other languages, and this preference is driven more by answer language than question language.
- **Mechanism:** The LLM judge evaluates a pair where one answer is in English and the other is in a target language. Accuracy is measured in four configurations (Acc_EN,EN✓, Acc_EN,TG✓, Acc_TG,EN✓, Acc_TG,TG✓). The analysis computes an "Answer Effect" (Eq. 4) as the average accuracy drop when the correct answer is in the target language instead of English. A positive Answer Effect indicates the model has a bias toward selecting the English answer, regardless of its correctness. The results show this effect is predominantly positive across models and languages.
- **Core assumption:** Assumes position bias is controlled by averaging results over swapped option orders.
- **Evidence anchors:**
  - [abstract] "In inter-language judging, most models favor English answers, and that this preference is influenced more by answer language than question language."
  - [section 5.2, Equations 3-4] "The answer effect is mostly positive, which means that the English answers is undesirably preferred, regardless of whether they are truly correct."
  - [corpus] Paper "Fragile Preferences: A Deep Dive Into Order Effects in Large Language Models" confirms the critical need to manage order effects in comparative evaluations.
- **Break condition:** If position bias is not properly controlled, observed language preferences could be an artifact of option ordering. If translation quality is poor, the model may correctly reject a garbled answer in a target language, conflating quality with language.

### Mechanism 2
- **Claim:** In same-language judging, performance varies by language family (European > Asian > African) and is worse for culturally-related subjects (Social Sciences, Humanities) compared to language-invariant subjects (STEM).
- **Mechanism:** When judging two options in the same language, the model's accuracy is measured per language. The performance gap between English and a target language is calculated for each of the 57 MMMLU subjects. This gap is larger for subjects like Psychology and Economics (which rely on cultural context) and smaller for Math and Physics (which are more language-invariant), indicating that the model's representational weakness for low-resource languages is more acute in culturally grounded domains.
- **Core assumption:** Assumes the MMMLU dataset provides semantically equivalent questions and answers across all languages via professional translation.
- **Evidence anchors:**
  - [abstract] "European languages consistently outperform African languages, and this bias is more pronounced in culturally-related subjects."
  - [section 4.3, Figure 4, 9] Shows the accuracy gap (EN - Target) is consistently larger for Humanities and Social Sciences than for STEM across languages and models.
  - [corpus] Paper "MyCulture: Exploring Malaysia's Diverse Culture under Low-Resource Language Constraints" supports the broader challenge of low-resource cultural representation in LLMs.
- **Break condition:** If translation quality for African languages is systematically lower than for European languages, the observed disparity could be due to translation artifacts.

### Mechanism 3
- **Claim:** Language bias cannot be fully reduced to low-perplexity bias; language identity explains significant additional variance in model preference.
- **Mechanism:** The hypothesis is that LLMs prefer high-resource languages because they have lower perplexity (are more "familiar"). To test this, the paper measures the correlation between model preference (log-prob difference) and perplexity difference between options, finding only a slight correlation (~ -0.3 to -0.4). It then uses two linear regression models: a "reduced" model with perplexity difference and a "full" model adding language identity. An F-test shows the full model explains significantly more variance (higher R²), proving language bias is a distinct phenomenon beyond a preference for low-perplexity text.
- **Core assumption:** Assumes the perplexity pipeline (styled response synthesis) accurately isolates the model's surprisal at the answer content from its surprisal at its own response framing.
- **Evidence anchors:**
  - [abstract] "We find that while perplexity is slightly correlated with language bias, language bias cannot be fully explained by perplexity only."
  - [section 6.3, Figure 8] R² decomposition shows a substantial increase in explained variance when language terms are added, especially for low-resource languages (e.g., Yoruba).
  - [corpus] Paper "UDA: Unsupervised Debiasing Alignment for Pair-wise LLM-as-a-Judge" discusses preference biases, but the specific interaction with perplexity is less explored in related literature.
- **Break condition:** If the "styled response synthesis" introduces systematic artifacts or fails to mimic the judge's style accurately, perplexity measurements may be noisy, obscuring the true relationship.

## Foundational Learning

- **Concept: Pairwise LLM-as-a-Judge**
  - **Why needed here:** This is the core evaluation paradigm. It involves prompting an LLM to select the better of two options. Its reliability is compromised by biases like position and language bias.
  - **Quick check question:** In a pairwise comparison between two equally good options, what would the accuracy of a perfectly random (unbiased) judge be? (Answer: 50%)

- **Concept: Perplexity**
  - **Why needed here:** It measures how "surprised" a model is by text. The paper investigates if language bias is just a proxy for the model preferring low-perplexity (high-probability) text, which is common in high-resource languages.
  - **Quick check question:** For a model trained primarily on English, would it assign a higher or lower perplexity to a sentence in Swahili compared to an equivalent sentence in English? (Answer: Higher)

- **Concept: Controlled Experiment Design**
  - **Why needed here:** The study's conclusions depend on isolating variables. It controls for position bias by swapping option orders and isolates question vs. answer language effects through specific experiment configurations.
  - **Quick check question:** Why is it insufficient to simply run one evaluation per question-pair when measuring LLM judge performance? (Answer: To control for position bias.)

## Architecture Onboarding

- **Component map:**
  1.  **Benchmark Adapter**: Preprocesses the MMMLU dataset into pairwise comparison tasks for same-language and inter-language settings.
  2.  **Inference Harness**: Executes evaluation prompts on the Judge LLM, collecting judgments ("1" or "2") and top token log-probabilities. Manages position swapping.
  3.  **Perplexity Pipeline**: A three-step module: (a) collects the judge's natural response style, (b) synthesizes a styled response using GPT-4.1 to embed the dataset answer, (c) computes masked perplexity over the core answer tokens.
  4.  **Statistical Analysis Module**: Computes accuracy metrics, runs linear regressions (Eq. 7-8) to decompose variance, and performs F-tests.

- **Critical path:**
  1.  **Data Prep**: Generate paired examples from MMMLU, ensuring language and correctness labels are correct.
  2.  **Judge Execution**: Run all pairwise comparisons, collecting both the final judgment and the log-probability for tokens "1" and "2".
  3.  **Perplexity Scoring**: For each option in a pair, use the dedicated pipeline to compute its masked perplexity.
  4.  **Correlation & Regression**: Calculate preference-perplexity correlations. Train reduced and full regression models.
  5.  **Hypothesis Testing**: Run F-tests to validate that language identity adds significant explanatory power beyond perplexity.

- **Design tradeoffs:**
  - **Judge Selection**: General-purpose models (e.g., GPT, Llama) vs. specialized judge models (e.g., Prometheus). Specialized models may be more robust but less generalizable.
  - **Perplexity Method**: The styled-synthesis pipeline is complex but aims for accuracy. A simpler method (e.g., raw perplexity on concatenated text) is faster but noisier.
  - **Prompting**: Explicitly instructing the judge to ignore language (Appendix B) is proven ineffective, highlighting the need for architectural or fine-tuning solutions.

- **Failure signatures:**
  - **Near-random accuracy**: If judge accuracy for a language pair is close to 50%, it cannot reliably judge quality in that language.
  - **High variance on position swap**: If accuracy flips dramatically when option order is changed, the judge is not evaluating content.
  - **Low R² in regression**: If the full regression model explains very little variance, the measured predictors (perplexity, language) are not capturing the true drivers of preference.

- **First 3 experiments:**
  1.  **Inter-language Preference Baseline**: Pick one high-resource (French) and one low-resource (Yoruba) language. Run a simplified inter-language judging experiment with a small model. Compare the "Answer Effect" to verify the basic preference for English exists.
  2.  **Subject-Specific Bias Check**: Select two subjects (Math vs. Psychology) and one low-resource language. Run same-language judging. Compare the accuracy gap to confirm the bias is larger for the culturally-dependent subject.
  3.  **Perplexity Correlation Test**: For a single model and one target language, compute perplexity for a sample of options and plot it against the model's preference log-prob difference. Visually inspect for a strong correlation to sanity-check the "low-perplexity bias" hypothesis.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How does the activation of reasoning capabilities (e.g., Chain-of-Thought) in judge models influence the relationship between language identity and judging preference?
- **Basis in paper:** [explicit] The Limitations section (Page 8) states, "Most recent LLMs include reasoning... we did not consider the potential impact of reasoning in our perplexity experiments."
- **Why unresolved:** The study focused on models without explicit reasoning steps. Reasoning tokens may either amplify language bias through the language of the rationale or mitigate it by decoupling the final decision from the surface form of the answers.
- **What evidence would resolve it:** A comparative study measuring the preference-perplexity correlation in "thinking" models (like GPT-5.1 with reasoning enabled) versus standard models to observe if the incremental $R^2$ contribution of language identity decreases.

### Open Question 2
- **Question:** Can the observed language bias be fully explained by perplexity if a noise-free method for isolating semantic content from stylistic framing is developed?
- **Basis in paper:** [explicit] Page 8 notes that the data preprocessing could not "fully mitigate the portion of perplexity that are not related to the answer choices, like answer framing style mismatch," potentially causing the $R^2$ of the reduced model to be lower than the ground truth.
- **Why unresolved:** The current findings show language explains variance beyond perplexity, but this may be an artifact of the "styled response" synthesis pipeline. It remains unclear if the "unexplained variance" is true bias or measurement noise from imperfect style matching.
- **What evidence would resolve it:** Development of a masked perplexity metric that requires no synthetic style transfer, applied to the regression analysis to see if perplexity alone becomes a sufficient predictor of preference.

### Open Question 3
- **Question:** What mitigation strategies are effective for reducing language bias in pairwise judging, particularly for low-resource languages where bias is least attributable to perplexity?
- **Basis in paper:** [inferred] The conclusion (Page 8) determines that "language bias cannot be fully explained by perplexity only" and highlights this as a "fundamental limitation," implying that current perplexity-based optimization does not solve the issue.
- **Why unresolved:** While the paper establishes the existence and causes of the bias, it does not test interventions. Since perplexity minimization is insufficient for low-resource languages (like Yoruba), effective debiasing techniques are currently unknown.
- **What evidence would resolve it:** Experiments applying interventions—such as contrastive multilingual fine-tuning or language-agnostic instruction tuning—demonstrating a statistically significant reduction in the language-specific coefficients ($\gamma_n$) identified in the paper's regression analysis.

## Limitations
- The complexity of the perplexity pipeline introduces potential measurement artifacts that may affect the precision of the perplexity-bias relationship findings
- The study exclusively focuses on pairwise evaluation, which may not capture bias patterns present in alternative assessment frameworks
- Reliance on MMMLU's professional translations raises questions about whether observed disparities reflect model limitations or translation quality variations across languages

## Confidence
- **High Confidence**: Findings on same-language performance disparities (European > African languages) and inter-language English preference effects are supported by comprehensive experimental evidence across multiple models and languages
- **Medium Confidence**: The conclusion that language bias cannot be fully explained by perplexity is statistically well-supported, though the complex pipeline for measuring perplexity introduces some measurement uncertainty
- **Low Confidence**: The cultural subject analysis showing larger biases in Humanities vs. STEM domains is suggestive but requires additional validation with more diverse cultural content

## Next Checks
1. **Translation Quality Control**: Conduct human evaluation of translation quality for African languages versus European languages in the MMMLU dataset to isolate whether disparities stem from translation artifacts versus model bias

2. **Alternative Perplexity Validation**: Implement a simpler perplexity measurement approach (raw perplexity on concatenated text) and compare results with the styled-synthesis method to validate the robustness of the perplexity-bias relationship findings

3. **Cross-Paradigm Replication**: Test the identified language bias patterns using non-pairwise evaluation frameworks (such as rubric-based scoring or direct rating) to determine whether observed biases are specific to pairwise comparison or reflect broader model behavior