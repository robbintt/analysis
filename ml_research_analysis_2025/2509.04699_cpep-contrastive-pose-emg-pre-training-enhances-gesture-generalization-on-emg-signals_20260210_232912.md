---
ver: rpa2
title: 'CPEP: Contrastive Pose-EMG Pre-training Enhances Gesture Generalization on
  EMG Signals'
arxiv_id: '2509.04699'
source_url: https://arxiv.org/abs/2509.04699
tags:
- pose
- gesture
- cpep
- classification
- encoder
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes CPEP, a contrastive pre-training framework
  that aligns EMG and pose representations to improve gesture classification from
  biosignals. The core idea is to learn an EMG encoder that produces pose-informative
  embeddings by contrastive alignment with high-quality pose representations, enabling
  zero-shot classification of unseen gestures.
---

# CPEP: Contrastive Pose-EMG Pre-training Enhances Gesture Generalization on EMG Signals

## Quick Facts
- arXiv ID: 2509.04699
- Source URL: https://arxiv.org/abs/2509.04699
- Reference count: 25
- Primary result: CPEP achieves 21% better in-distribution and 72% better unseen gesture classification than state-of-the-art methods

## Executive Summary
CPEP introduces a contrastive pre-training framework that aligns EMG and pose representations to improve gesture classification from biosignals. The approach learns EMG embeddings that capture pose-informative structure through contrastive alignment with high-quality pose representations, enabling zero-shot classification of unseen gestures. Trained on the large-scale emg2pose dataset, CPEP outperforms existing benchmarks by significant margins on both in-distribution and unseen gesture tasks, with zero-shot performance matching linear probing without requiring task-specific training.

## Method Summary
CPEP operates through a two-stage pipeline: first, separate MAE pre-training (100 epochs each) for EMG and pose encoders with 50% masking and MSE reconstruction; second, contrastive alignment where the frozen pose encoder provides stable targets for the EMG encoder via symmetric InfoNCE loss. The EMG encoder processes 2-second windows into 50 tokens using patch length S=50, while the pose encoder processes synchronized joint angles into 20 tokens using S=200. Both use 4-layer transformers outputting [CLS] token embeddings. The contrastive phase trains only the EMG encoder and a projection head (256→256) for 100 epochs with learnable temperature initialization. Zero-shot inference uses k-NN retrieval (k=10) with majority voting.

## Key Results
- Achieves 21% higher macro accuracy on in-distribution gesture classification compared to state-of-the-art methods
- Improves unseen gesture classification by 72% over baseline approaches
- Zero-shot classification performance matches linear probing results without task-specific training

## Why This Works (Mechanism)

### Mechanism 1
Aligning noisy EMG representations with structured pose embeddings via contrastive learning enables generalization to unseen gestures. The InfoNCE loss pulls matching EMG-pose pairs together in embedding space while pushing apart mismatched pairs, forcing the EMG encoder to capture pose-relevant structure that it cannot learn from noisy signals alone, creating a shared semantic space where gesture relationships are preserved.

### Mechanism 2
Freezing the pre-trained pose encoder during contrastive alignment is essential for training convergence and representation quality. The frozen pose encoder provides stable, high-quality anchor points in embedding space, preventing the "moving target" problem where both encoders drift during alignment, which destroys the pre-trained structure and causes divergence.

### Mechanism 3
MAE pre-training initialization is critical for both encoders before contrastive alignment. Masked autoencoder reconstruction forces each encoder to learn temporal dependencies and modality-specific patterns, providing a structured starting point that accelerates contrastive convergence and improves final representation quality compared to random initialization.

## Foundational Learning

- **Contrastive Learning with InfoNCE Loss**: Core training objective that creates bidirectional alignment between EMG and pose embedding spaces through pull-push dynamics on matched/unmatched pairs. *Quick check*: Can you explain why the symmetric InfoNCE loss includes both EMG-to-pose and pose-to-EMG directions, and what would happen if only one direction were used?

- **Masked Autoencoders (MAE) for Time Series**: Provides the essential pre-training phase that gives both encoders meaningful initial representations before they are aligned. *Quick check*: How does masking 50% of temporal tokens and reconstructing them force the model to learn useful representations versus simply memorizing local patterns?

- **Zero-shot Classification via k-NN Retrieval**: Evaluation protocol and deployment strategy that enables gesture recognition without task-specific training labels. *Quick check*: Why does majority voting over k=10 nearest pose neighbors work better than simply taking the single closest match?

## Architecture Onboarding

- **Component map**: EMG Encoder (4-layer Transformer, 50 tokens) -> Projection Head (256→256) -> InfoNCE Loss; Pose Encoder (4-layer Transformer, 20 tokens, frozen) -> InfoNCE Loss

- **Critical path**: 1) Pre-train EMG-MAE and Pose-MAE independently with 50% masking, MSE reconstruction loss; 2) Load MAE weights, freeze pose encoder, attach projection head to EMG encoder; 3) Train with symmetric InfoNCE loss (batch size 256, learnable temperature τ=0.02); 4) For inference: embed EMG query → retrieve top-10 pose neighbors → majority vote on gesture labels

- **Design tradeoffs**: Patch length: S=50 for EMG (shorter captures finer temporal dynamics), S=200 for pose (longer captures smoother kinematics); [CLS] token (75.7% zero-shot) vs. average pooling (71.1%); Must freeze pose encoder; updating both causes divergence

- **Failure signatures**: Training loss NaN or divergence → check that pose encoder is frozen, both encoders initialized from MAE; Zero-shot accuracy <50% on in-distribution gestures → verify [CLS] token used, not average pooling; Poor unseen gesture generalization → check that MAE pre-training completed successfully, projection head attached correctly

- **First 3 experiments**: 1) Reproduce MAE pre-training on emg2pose dataset: train separate EMG-MAE and Pose-MAE with 50% masking, verify reconstruction loss converges; 2) Ablate frozen vs. trainable pose encoder: expect trainable version to diverge or underperform frozen baseline significantly; 3) Evaluate zero-shot classification on Deval split: precompute all pose embeddings, run k-NN retrieval with k=10, verify macro accuracy ~75% on in-distribution and ~48% on unseen gestures

## Open Questions the Paper Calls Out

- **Cross-biosignal generalization**: Can the CPEP framework effectively generalize to other biosignals with different temporal characteristics, such as Inertial Measurement Units (IMUs)? The authors state future work includes extending CPEP to additional EMG datasets and other biosignals (e.g. IMU), as well as exploring tasks such as human activity recognition.

- **Encoder update dynamics**: Why does allowing the pose encoder to update during contrastive pre-training cause the model to fail to converge? The ablation study notes that making the pose encoder trainable alongside the EMG encoder also fails to converge, but the specific optimization dynamics or representation collapse mechanisms responsible for the divergence are not analyzed.

- **Sensor variability robustness**: How robust are the learned EMG representations to inter-session variability and electrode displacement? While the model is evaluated on the emg2pose dataset, the paper does not test how classification accuracy changes when the EMG armband is deliberately rotated or shifted between training and inference phases.

## Limitations

- Evaluation is constrained to a single dataset (emg2pose) with limited gesture and user diversity, making it unclear whether performance gains will transfer to more complex or varied EMG signal domains
- Lacks comparison to state-of-the-art EMG-specific pre-training approaches like EMG-UP, which directly addresses cross-user generalization
- Analysis focuses on downstream classification performance without examining the quality of learned embeddings through intrinsic measures

## Confidence

- **High Confidence**: MAE initialization is critical for training stability and performance
- **High Confidence**: Frozen pose encoder is essential for CPEP to work
- **High Confidence**: [CLS] token pooling outperforms average pooling for zero-shot classification
- **Medium Confidence**: CPEP's generalization to unseen gestures is state-of-the-art
- **Medium Confidence**: Zero-shot performance matches linear probing without task-specific training

## Next Checks

1. **Cross-Dataset Generalization Test**: Evaluate CPEP embeddings on a different EMG dataset (e.g., NinaPro, CapgMyo) to verify that pose alignment provides meaningful signal structure transfer beyond emg2pose

2. **Cross-User Zero-Shot Evaluation**: Test zero-shot classification on unseen users within the emg2pose dataset, comparing CPEP against EMG-UP to quantify relative performance on the cross-user generalization challenge

3. **Embedding Quality Analysis**: Compute intrinsic metrics such as CKA similarity, alignment/separation scores, and nearest-neighbor semantic consistency to validate that contrastive alignment produces meaningful gesture representations beyond classification accuracy