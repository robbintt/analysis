---
ver: rpa2
title: Exploring Adversarial Obstacle Attacks in Search-based Path Planning for Autonomous
  Mobile Robots
arxiv_id: '2504.06154'
source_url: https://arxiv.org/abs/2504.06154
tags:
- obstacle
- path
- robot
- adversarial
- attacks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study examined how vulnerable the A path planning algorithm
  is to adversarial obstacle attacks. The attack method involved calculating the optimal
  placement of a single obstacle along the robot's path to maximize delay, using a
  brute-force approach that tested all possible positions.
---

# Exploring Adversarial Obstacle Attacks in Search-based Path Planning for Autonomous Mobile Robots

## Quick Facts
- arXiv ID: 2504.06154
- Source URL: https://arxiv.org/abs/2504.06154
- Reference count: 25
- Primary result: A* path planning is significantly vulnerable to adversarial obstacle attacks, with average delays of 36% in simulation and 32.49% in real-world scenarios

## Executive Summary
This study demonstrates that A* path planning algorithms are vulnerable to adversarial obstacle attacks where a single obstacle is strategically placed to maximize navigation delay. The attack uses a brute-force approach that calculates optimal obstacle placement by testing all possible positions along the robot's path. Experiments in both simulation (TurtleBot in Gazebo) and real-world (Unitree Go1 robot) environments show significant delay increases, particularly in constrained environments like tunnels where delays reached up to 86%. The attack achieved 91.3% success rate in simulation and 100% in real-world deployment, highlighting the need for more robust path planning algorithms to protect autonomous robots.

## Method Summary
The attack method involves calculating optimal placement of a single obstacle along the robot's path to maximize delay using a brute-force approach. The process starts with running A* to obtain the baseline path, then iteratively re-runs A* for each waypoint along that path with a candidate obstacle inserted. The position yielding the longest alternative path is selected and injected into the cost map. The attack runs in parallel with normal navigation via two ROS threads - one executing navigation and another computing the optimal obstacle position. The attack targets the global planner's cost map input while relying on the local planner (TEB) to execute the rerouted trajectory.

## Key Results
- Average delay of 36% in simulation environments across different map layouts
- Maximum delays reaching up to 86% in constrained environments like tunnels
- Real-world experiments showed average delays of 3.95 seconds (32.49%) across all scenarios
- Attack success rate of 91.3% in simulation and 100% in real-world deployment

## Why This Works (Mechanism)

### Mechanism 1
- Claim: A brute-force search over all positions along the original path identifies the obstacle placement that maximizes route deviation
- Mechanism: The attack runs A* once to get the baseline path, then re-runs A* iteratively for each waypoint along that path with a candidate obstacle inserted. The position yielding the longest alternative path is selected and injected into the cost map
- Core assumption: The planner is deterministic; the adversary knows the map, start, and goal; replanning occurs after the obstacle appears in the cost map
- Break condition: If the planner uses non-deterministic tie-breaking, maintains multiple candidate paths, or validates cost-map updates against live sensor data, the predicted optimal position may not cause the expected delay

### Mechanism 2
- Claim: Environmental constriction amplifies attack impact by limiting viable alternative routes
- Mechanism: In geometries such as tunnels or narrow corridors, blocking the primary path forces the robot onto disproportionately longer detours because short alternative paths do not exist
- Core assumption: The environment has choke points where path diversity is low
- Break condition: In open environments with many near-equivalent alternatives, a single obstacle yields smaller relative delays

### Mechanism 3
- Claim: Virtual cost-map manipulation achieves attack goals without physical environmental changes
- Mechanism: The adversary injects a false obstacle into the 2D cost map via software compromise or a man-in-the-middle attack, causing the global planner to reroute while the physical scene remains unchanged
- Core assumption: The adversary can gain write access to the cost map or the ROS node publishing it
- Break condition: If the robot cross-checks the cost map against live sensor observations and rejects inconsistent obstacles, the virtual injection may be ignored or flagged

## Foundational Learning

- Concept: A* Search on Grid/Graph Cost Maps
  - Why needed here: The attack exploits A*'s determinism and path optimality to predict which obstacle placement causes maximum rerouting
  - Quick check question: Given a grid cost map, start, and goal, does A* guarantee a unique optimal path if edge costs are uniform and ties are resolved deterministically?

- Concept: ROS Navigation Stack (Global + Local Planners)
  - Why needed here: The attack targets the global planner's cost map input while relying on the local planner (TEB) to execute the rerouted trajectory
  - Quick check question: Which component in the ROS nav stack consumes the global cost map, and what happens when an obstacle appears mid-execution?

- Concept: Threat Modeling (Adversary Knowledge & Capabilities)
  - Why needed here: The paper's claims conditionally depend on the adversary knowing the map and having cost-map write access
  - Quick check question: List three assumptions about adversary knowledge/capabilities that, if false, would invalidate the attack's effectiveness

## Architecture Onboarding

- Component map:
  - Global Planner (A*) -> 2D Cost Map -> Local Planner (TEB) -> Robot Motion
  - Attack Node (malicious ROS node) -> 2D Cost Map (via injection)

- Critical path:
  1. Goal issued → A* computes initial global path
  2. Attack thread iterates over each waypoint, running adversarial A* with candidate obstacles
  3. Optimal obstacle position selected → injected into cost map
  4. On next planning cycle, global planner reroutes; local planner follows longer trajectory
  5. Robot reaches goal with measurable delay

- Design tradeoffs:
  - Single vs. multiple obstacles: Paper restricts to one obstacle for baseline analysis and stealth; multiple smaller obstacles could cumulatively increase delay with lower conspicuousness
  - Obstacle size: Chosen to force rerouting without fully blocking all alternatives; overly large obstacles risk trivial detection or infeasibility
  - Parallel vs. sequential attack execution: Parallel threads reduce latency between planning and injection; sequential would cause detectable delays before motion starts

- Failure signatures:
  - Obstacle placed too late (robot has passed the target cell) → no reroute, logged as failure (8.7% of simulation runs)
  - Early-path injection → smaller delays because replanning can explore alternatives with minimal backtracking
  - Open-map injection → lower relative delay due to multiple near-optimal alternatives

- First 3 experiments:
  1. Baseline measurement: Run TurtleBot in Gazebo warehouse across all goal positions without attack; record time-to-goal vs. Euclidean distance
  2. Attack replication: Implement Algorithm 1 as a standalone ROS node; for each goal, record the computed optimal obstacle position, resulting path length, and delay percentage
  3. Topology comparison: Deploy the attack in at least two contrasting maps (constrained tunnel-like vs. open warehouse); compare mean percentage delays to validate the constriction-amplification effect

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the vulnerability of A* to adversarial obstacle attacks compare to other path planning algorithms (e.g., RRT, Dijkstra, D*, sampling-based planners)?
- Basis in paper: [explicit] The authors state they "examined the robustness of search-based A* path planning" and call for "developing more resilient algorithms," but only tested A*
- Why unresolved: No comparison experiments were conducted with alternative path planning algorithms
- What evidence would resolve it: Comparative experiments applying the same brute-force obstacle attack methodology to other planning algorithms across identical environments

### Open Question 2
- Question: Can multiple smaller obstacles achieve comparable or greater delays than a single large obstacle while remaining less detectable?
- Basis in paper: [explicit] The authors acknowledge "in real-world scenarios, an adversary might employ multiple smaller obstacles to achieve a similar delay while maintaining a lower profile"
- Why unresolved: The study intentionally simplified analysis by using only one obstacle per attack
- What evidence would resolve it: Experiments comparing delay magnitude and detection rates between single-obstacle and multi-obstacle attack configurations

### Open Question 3
- Question: What defensive mechanisms can detect or mitigate adversarial obstacle attacks in real-time without significantly degrading navigation performance?
- Basis in paper: [inferred] The paper concludes by highlighting "the urgent need for more robust path planning algorithms capable of withstanding adversarial attacks" but proposes no countermeasures
- Why unresolved: The focus was exclusively on characterizing vulnerabilities, not developing defenses
- What evidence would resolve it: Design and evaluation of detection algorithms or resilient planning modifications that reduce attack success rates while maintaining competitive traversal times

### Open Question 4
- Question: How does attack efficacy scale with map complexity and size, given the computational constraints of the brute-force approach?
- Basis in paper: [inferred] The paper notes "the larger the map, the longer it takes to find a feasible path" and relied on parallel execution; the 8.7% simulation failure rate was attributed to timing issues
- Why unresolved: Only three map types were tested, and scalability analysis was not performed
- What evidence would resolve it: Systematic experiments across varying map sizes and complexity levels, measuring both attack computation time and success rates

## Limitations

- The attack's effectiveness is highly dependent on the adversary having precise knowledge of the map, start/goal positions, and ability to inject obstacles into the cost map
- The 8.7% simulation failure rate indicates timing constraints where the robot may pass obstacle positions before injection
- Attack impact is significantly amplified in constrained environments, suggesting reduced effectiveness in open environments with multiple path alternatives

## Confidence

- **High confidence**: The brute-force attack mechanism (Algorithm 1) and its implementation are well-documented and reproducible
- **Medium confidence**: The quantitative results are internally consistent but depend on specific experimental conditions
- **Medium confidence**: The distinction between virtual and physical attacks is clearly articulated, though real-world attack vectors are not fully explored

## Next Checks

1. Cross-validation with alternative path planners: Test the same attack methodology against Dijkstra's algorithm and D* Lite to determine if A*'s specific properties make it particularly vulnerable

2. Dynamic environment robustness: Implement cost-map validation against live sensor data to assess whether robots can detect and reject virtual obstacle injections

3. Multi-obstacle attack scenarios: Extend the attack to multiple smaller obstacles placed strategically along the path to evaluate cumulative delay effects versus single-obstacle approaches