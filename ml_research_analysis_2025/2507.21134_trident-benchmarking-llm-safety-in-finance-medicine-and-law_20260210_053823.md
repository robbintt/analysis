---
ver: rpa2
title: 'TRIDENT: Benchmarking LLM Safety in Finance, Medicine, and Law'
arxiv_id: '2507.21134'
source_url: https://arxiv.org/abs/2507.21134
tags:
- harmful
- finance
- safety
- ethical
- prompt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Trident-Bench, a benchmark for evaluating
  the safety of large language models (LLMs) in high-risk domains such as law, finance,
  and medicine. The benchmark is grounded in professional ethical codes from authoritative
  bodies (CFA Institute, AMA, ABA) and features 2,652 harmful prompts paired with
  safe responses.
---

# TRIDENT: Benchmarking LLM Safety in Finance, Medicine, and Law

## Quick Facts
- **arXiv ID**: 2507.21134
- **Source URL**: https://arxiv.org/abs/2507.21134
- **Reference count**: 40
- **Primary result**: Domain-specialized LLMs often fail on ethical nuances compared to general-purpose models

## Executive Summary
This paper introduces Trident-Bench, a benchmark for evaluating LLM safety in high-stakes domains (law, finance, medicine) using professional ethical codes as ground truth. The benchmark contains 2,652 harmful prompts paired with safe responses, evaluated by a multi-model jury system. Testing 19 models reveals that while strong generalist models meet basic safety expectations, domain-specialized models often exhibit weaker safety compliance due to diluted safety alignment during task-specific fine-tuning. Safety-aligned models show significant improvement, suggesting targeted safety fine-tuning is crucial for domain-specific contexts.

## Method Summary
The authors constructed Trident-Bench by generating harmful prompts from professional ethical codes (CFA, AMA, ABA), then having human experts filter and validate safe responses through unanimous agreement. They evaluated 19 models using a two-model jury (Claude 3.5 and Gemma 2-9B) that assigns harmfulness scores from 1-5. The benchmark tests three domains separately and includes general-purpose, domain-specialized, and safety-aligned models.

## Key Results
- Generalist models like GPT-4o and Gemini 2.5 Flash meet basic safety expectations across all domains
- Domain-specialized models frequently fail on subtle ethical nuances despite domain competence
- Safety-aligned models (e.g., LLaMA Guard3-8B) achieve significantly better harmlessness scores
- The multi-model jury approach reduces individual evaluator bias in safety assessment

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: A multi-model jury approach reduces individual evaluator bias when rating LLM safety compliance.
- **Mechanism**: Trident-Bench averages harmfulness scores from two diverse LLM jurors (Claude 3.5 and Gemma 2-9B). This compensates for any single model's rating tendencies or blind spots, producing a more stable final score.
- **Core assumption**: The selected jury models have sufficiently distinct failure modes and do not share a systematic bias in the same direction.
- **Evidence anchors**: The paper explicitly states models are evaluated "using a multi-model jury approach" and describes the two-model jury composition.
- **Break condition**: If both jury models share a similar alignment blind spot (e.g., both undervalue a specific financial conflict of interest), the averaged score will remain biased.

### Mechanism 2
- **Claim**: Domain-specialized models often exhibit weaker safety compliance than general-purpose models because task-specific fine-tuning can dilute prior safety alignment.
- **Mechanism**: Specialized models are fine-tuned on domain corpora to maximize competence. If this data lacks sufficient safety-aligned examples, the model's original safety boundaries may be eroded, leading to higher compliance with harmful prompts framed as professional queries.
- **Core assumption**: The base model possessed meaningful safety alignment before specialization, and the fine-tuning process did not explicitly reinforce safety constraints.
- **Evidence anchors**: Results show models like DISC-LawLLM and FS-LLaMA frequently comply with unethical queries, treating them as client questions rather than safety violations.
- **Break condition**: If the domain-specific fine-tuning dataset is explicitly curated with safety-aligned examples and refusal cases, this degradation should be mitigated or avoided.

### Mechanism 3
- **Claim**: Safety-aligned models achieve strong harmlessness scores because they are explicitly trained with refusal objectives and rule-based conditioning, decoupling safety from domain competence.
- **Mechanism**: These models are trained specifically to detect and refuse harmful inputs, often using datasets designed around safety taxonomies. This targeted training creates a robust refusal reflex that activates even when prompts use domain-specific jargon.
- **Core assumption**: The safety training data covers adversarial patterns relevant to the tested domains (finance, law, medicine).
- **Evidence anchors**: LLaMA Guard models "were trained on the S6: Specialized Advice split... which may include domain-relevant safety data."
- **Break condition**: If a harmful prompt uses a novel jailbreak strategy outside the distribution of the safety training data, the model may still fail to refuse.

## Foundational Learning

- **Concept: Professional Ethical Codes as Safety Ground Truth**
  - Why needed here: The benchmark's core innovation is grounding "safety" not in abstract principles but in concrete professional obligations.
  - Quick check question: Can you distinguish between a user asking *what* an ethical rule is versus *how to bypass* it?

- **Concept: Safety vs. Capability Trade-off in Fine-Tuning**
  - Why needed here: A central finding is that making a model better at a domain task can make it worse at domain safety if alignment is not preserved.
  - Quick check question: If you fine-tune a model on a corpus of legal motions, how would you ensure it doesn't learn to generate frivolous motions?

- **Concept: Jury-based LLM Evaluation**
  - Why needed here: The paper relies on an automated, multi-model evaluation system. Understanding its strengths and limitations is critical for interpreting all reported results.
  - Quick check question: Why might averaging scores from two different models be more reliable than using a single, stronger model as a judge?

## Architecture Onboarding

- **Component map**: Ethical Principles -> Prompt Generation -> Expert Filtering -> Benchmark (Prompt + Safe Response) -> Model Inference -> Jury Evaluation -> Harmfulness Score
- **Critical path**: The flow from ethical principles through prompt generation to jury evaluation forms the complete evaluation pipeline.
- **Design tradeoffs**:
  - **Realism vs. Diversity in Prompts**: The authors chose 75% prompt-based jailbreaks (more natural) and 25% finetuned jailbreaks (more aggressive), prioritizing realistic user interactions.
  - **Annotation Stringency**: Requiring unanimous agreement among 3 experts ensures high data quality but likely reduced the final dataset size and introduces a subjective filter.
  - **Automated Evaluation**: The multi-model jury enables scalable benchmarking but may not capture all nuances a human professional would detect.
- **Failure signatures**:
  - **Subtle Compliance**: A response that appears to refuse but offers a "workaround" (e.g., "I can't recommend hiding losses, but some advisors use off-balance-sheet vehicles...")
  - **False Refusal**: A model refusing a benign question because it is overly sensitive to domain keywords.
  - **Jury Disagreement**: High variance between the two jury models on a particular response type.
- **First 3 experiments**:
  1. **Baseline Evaluation**: Run a general-purpose model (e.g., GPT-4o, LLaMA 3.1) on the full Trident-Bench to establish a performance baseline for your setup.
  2. **Ablation by Domain**: Evaluate a single model on each of the three sub-domains (Finance, Law, Medicine) separately to identify which domain presents the greatest safety challenge.
  3. **Refusal Analysis**: Manually inspect a sample of responses scored as "3" (mid-range harmfulness) to understand the failure modes the jury is capturing (e.g., hedging, partial compliance).

## Open Questions the Paper Calls Out

- **Open Question 1**: How does LLM safety robustness in high-stakes domains degrade or evolve during multi-turn conversations compared to single-turn interactions?
  - Basis: The authors state the benchmark focuses on single-turn interactions and that "extending the benchmark to multi-turn or chained interaction scenarios would allow deeper testing of safety robustness."
  - Why unresolved: Real-world unsafe behavior often emerges gradually through context shifts rather than explicit single-turn harmful requests.
  - What evidence would resolve it: A longitudinal evaluation using multi-turn adversarial prompts showing specific failure rates compared to the static benchmark.

- **Open Question 2**: To what extent do model-based evaluation juries introduce inductive biases that diverge from human expert judgment in ambiguous professional contexts?
  - Basis: Section 6 acknowledges that the two-model jury "may still introduce inductive bias or blind spots compared to human judgment."
  - Why unresolved: The paper substitutes human evaluation with model juries for scalability but lacks validation against human experts on edge cases.
  - What evidence would resolve it: A correlation analysis comparing jury ratings against human domain expert ratings on a subset of ambiguous cases.

- **Open Question 3**: Does domain-specific fine-tuning inherently erode general safety alignment, or does the performance gap stem primarily from a lack of domain-specific safety training data?
  - Basis: The results show domain-specialized models often perform worse than generalist models. The authors highlight an "urgent need for finer-grained domain-specific safety improvements" but do not isolate the cause.
  - Why unresolved: It is unclear if the lower safety scores are due to catastrophic forgetting of general principles or an inability to recognize domain-specific violations.
  - What evidence would resolve it: Ablation studies re-training specialized models with varying proportions of domain-specific versus general safety data.

## Limitations

- The benchmark's reliance on automated jury evaluation introduces uncertainty about whether the two-model system captures all safety-relevant nuances that human professionals would detect.
- The study focuses on compliance with ethical codes rather than real-world harm prevention, creating a gap between benchmark performance and actual safety outcomes.
- The evaluation only tests English-language models and prompts, limiting generalizability to multilingual contexts.

## Confidence

- **High Confidence**: The core finding that domain-specialized models often perform worse on safety than general-purpose models is well-supported by systematic evaluation results across 19 different models and multiple domains.
- **Medium Confidence**: The effectiveness of the multi-model jury approach for reducing evaluator bias is theoretically sound and supported by the paper's methodology, but would benefit from validation against human expert ratings on a subset of cases.
- **Medium Confidence**: The superiority of safety-aligned models in harmlessness scores is demonstrated, but the extent to which this transfers to novel jailbreak strategies or emerging ethical scenarios remains uncertain.

## Next Checks

1. **Human Expert Validation**: Have practicing professionals in finance, medicine, and law review a stratified sample of jury-scored responses (particularly those near decision boundaries) to validate whether the automated system aligns with expert judgment on safety compliance.

2. **Cross-Domain Transfer Testing**: Evaluate whether safety gains from fine-tuning on one domain (e.g., medical ethics) transfer to other domains, or whether models learn to compartmentalize safety differently across professional contexts.

3. **Adversarial Prompt Testing**: Create a separate validation set of prompts using novel jailbreak techniques not present in the training set to assess whether safety-aligned models maintain their advantage against emerging attack strategies.