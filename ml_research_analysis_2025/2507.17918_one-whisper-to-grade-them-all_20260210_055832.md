---
ver: rpa2
title: One Whisper to Grade Them All
arxiv_id: '2507.17918'
source_url: https://arxiv.org/abs/2507.17918
tags:
- speech
- score
- training
- language
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces a unified, end-to-end model for holistic Automatic
  Speaking Assessment (ASA) using a single Whisper-small encoder. Instead of relying
  on separate models per response part, the approach processes all spoken responses
  in sequence and aggregates them through a lightweight module to predict overall
  CEFR scores.
---

# One Whisper to Grade Them All

## Quick Facts
- **arXiv ID:** 2507.17918
- **Source URL:** https://arxiv.org/abs/2507.17918
- **Reference count:** 0
- **Primary result:** Unified end-to-end model achieves RMSE of 0.384 on Speak & Improve Challenge 2025, outperforming baseline (RMSE 0.44) using 70% of Whisper-small parameters.

## Executive Summary
This paper introduces a holistic Automatic Speaking Assessment (ASA) model that uses a single Whisper-small encoder to grade multi-part spoken responses end-to-end, eliminating the need for separate models per response part or explicit ASR transcription. The system processes all spoken responses in sequence, aggregates chunk-level features via a lightweight module, and predicts overall CEFR scores. Tested on the Speak & Improve Challenge 2025 dataset, the model achieves a Root Mean Squared Error (RMSE) of 0.384, outperforming the official baseline (RMSE 0.44) while using only 70% of Whisper-small's parameters. A proposed swap sampling strategy enables training on just 44.8% of available speakers without sacrificing performance, improving robustness on imbalanced proficiency levels. The design is computationally efficient, suitable for large-scale CALL systems, and offers stable predictions with minimal overhead.

## Method Summary
The model processes concatenated audio from four spoken response parts (parts 1, 3, 4, 5) by splitting it into non-overlapping 30-second chunks, feeding each chunk through a frozen or fine-tuned Whisper-small encoder, and averaging the encoder output over the time dimension to produce a 768-dimensional vector per chunk. These chunk embeddings are then combined using either a mean aggregator (hierarchical average pooling) or a shallow Transformer aggregator, followed by a linear layer to predict a holistic CEFR score (2.0-5.5). The model is trained using RMSE loss to avoid gradient underflow in 16-bit precision, with swap sampling enabling efficient training on a subset of speakers by synthetically combining response parts from speakers with similar scores.

## Key Results
- Achieves RMSE of 0.384 on Speak & Improve Challenge 2025, outperforming official baseline (RMSE 0.44)
- Uses only 70% of Whisper-small parameters while maintaining strong performance
- Enables training on 44.8% of speakers via swap sampling without performance loss
- Improves robustness on imbalanced proficiency levels (low A2+ and high C1+)

## Why This Works (Mechanism)

### Mechanism 1
A single Whisper encoder can process multi-part spoken responses for proficiency grading, eliminating the need for transcription and separate per-part models. The system chunks each multi-part audio sample (up to 240s) into sequential, non-overlapping 30-second segments (Whisper's input limit). The Whisper-small encoder generates features for each chunk, which are immediately averaged over the time dimension to produce one 768-dimensional vector per chunk. This bypasses the decoder and ASR pipeline entirely.

**Core assumption:** The encoder's acoustic representations contain sufficient signal for proficiency assessment without explicit linguistic decoding or fine-grained temporal modeling.

**Evidence anchors:**
- [abstract] "...ability to process all four spoken responses with a single Whisper-small encoder, combine all information via a lightweight aggregator, and predict the final score."
- [section] Page 2, Section 3: "We divide each sample into sequential, non-overlapping 30-second chunks... averaging over the time dimension T, producing a single 768-dimensional vector for each 30-second chunk."
- [corpus] Related work confirms Whisper-small's robustness as an ASR encoder for various languages ("Quantizing Whisper-small...", "Which one Performs Better? Wav2Vec or Whisper?..."), but direct corpus evidence for this specific encoder-only grading mechanism is weak.

**Break condition:** If critical proficiency cues are encoded in fine-grained temporal patterns (e.g., specific mispronunciations, hesitation timing) that are lost during the per-chunk time-averaging step.

### Mechanism 2
A lightweight aggregator can effectively summarize a variable number of chunk embeddings into a fixed-size representation for score prediction. Two strategies are used: 1) **Mean Aggregator (AVG)**: Performs hierarchical average pooling—first within each response part, then across the four parts. 2) **Transformer Aggregator (TF)**: Uses a shallow (2-layer, 4-head) Transformer encoder to process all chunk embeddings, aiming to capture order and cross-part dependencies before final pooling. Both feed a final linear layer.

**Core assumption:** A simple average or a shallow Transformer can preserve enough proficiency-related information from the sequence of chunk embeddings. AVG explicitly assumes that delivery quality is consistent enough to be summarized by a mean.

**Evidence anchors:**
- [abstract] "...combine all information via a lightweight aggregator..."
- [section] Page 3, Section 3.1: "AVG computes the arithmetic mean of the four part-level vectors... TF Aggregator is designed to retain coarse information about chunk order and cross-part dependencies..."
- [corpus] General Transformer literature supports sequence aggregation, but no corpus evidence exists for this specific application in multi-part ASA.

**Break condition:** If the relationships between different speech parts or the sequential evolution of performance are critical for grading, the AVG model will fail to capture them. If the TF aggregator's capacity is insufficient or its instability during training leads to unreliable convergence.

### Mechanism 3
Swap sampling improves data efficiency and robustness on imbalanced proficiency levels by synthetically augmenting the training set. The method constructs new 4-part training samples by swapping parts between different speakers who have similar holistic CEFR scores (within 0.5 points). This enables the use of incomplete speaker profiles and targeted oversampling of rare "edge" cases (low A2+ and high C1+).

**Core assumption:** The proficiency level is consistent enough across different response parts that swapping parts between speakers with similar overall scores creates a coherent and valid synthetic "speaker."

**Evidence anchors:**
- [abstract] "...train on only 44.8% of the speakers in the corpus and still reach 0.383 RMSE, demonstrating improved performance on imbalanced classes..."
- [section] Page 2, Section 2.1: "...synthetically construct 4-part samples by combining responses from different speakers... only apply swap sampling when the score of each part is within 0.5 of the target CEFR score."
- [corpus] No corpus evidence for this specific augmentation technique.

**Break condition:** If a speaker's proficiency varies significantly across topics or tasks, swapping parts could create unrealistic training samples (e.g., combining a highly coherent response with a grammatically poor one from another speaker), confusing the model.

## Foundational Learning

- **Concept: Whisper-small Encoder Architecture**
  - Why needed here: It is the core feature extractor. Understanding its 30-second input constraint and its output format (T timesteps x 768 features) is prerequisite to understanding the chunking and averaging logic.
  - Quick check question: What is the maximum input duration for the Whisper-small encoder, and what is the dimensionality of the feature vector at each time step?

- **Concept: CEFR Scoring Framework**
  - Why needed here: The model predicts a holistic score on this scale. Understanding that it is a continuous value from 2.0 (A2) to 5.5 (C1+) is critical for interpreting model output and performance metrics (RMSE).
  - Quick check question: Is the CEFR score predicted by the model a discrete class label or a continuous numerical value?

- **Concept: Root Mean Squared Error (RMSE) Loss**
  - Why needed here: The paper explicitly chooses RMSE over Mean Squared Error (MSE) to avoid gradient underflow in 16-bit precision. This is a key implementation detail for successful training.
  - Quick check question: According to the paper, what is the practical training problem with using standard MSE loss in 16-bit precision for this task?

## Architecture Onboarding

- **Component map:**
  1. **Input**: Concatenated audio from parts 1, 3, 4, 5 (~4 mins max).
  2. **Preprocessor**: Splits audio into sequential 30-second chunks.
  3. **Encoder (Whisper-small)**: Processes each chunk. Output: T x 768 tensor.
  4. **Chunk-Averaging**: Averages encoder output across the time dimension (T). Output: One 768-dim vector per 30s chunk.
  5. **Aggregator (AVG or TF)**: Combines all chunk vectors into a single 768-dim embedding.
  6. **Prediction Head**: A simple fully-connected (linear) layer. Output: Scalar CEFR score.

- **Critical path:**
  1. Correctly chunking raw audio to fit Whisper's 30-second window without data loss.
  2. Correctly averaging the encoder's output over the *time dimension* per chunk. This is the primary information bottleneck.
  3. Correctly implementing the Aggregator to produce a single 768-dim vector from a variable number of chunk embeddings.

- **Design tradeoffs:**
  - **Efficiency vs. Temporal Detail**: The design is highly efficient (70% parameters, no ASR decoder) but sacrifices fine-grained temporal information (e.g., short mispronunciations are smoothed out by averaging).
  - **Stability vs. Edge-Case Performance**: The AVG aggregator is more stable and reproducible, while the TF aggregator can outperform on rare edge cases but is prone to training fluctuations.
  - **Data Completeness vs. Data Efficiency**: The model requires all four parts, which discards incomplete data. Swap sampling mitigates this, but introduces synthetic data that may lack ecological validity.

- **Failure signatures:**
  - **High RMSE on edge cases**: Indicates model struggles with rare CEFR levels (A2+, C1+). Mitigation: Implement the Oversample Edge (OE) data strategy.
  - **Unstable training**: Large fluctuations in validation RMSE across epochs. Mitigation: Prefer the AVG aggregator or use a more robust checkpoint selection method.
  - **High scores for off-topic speech**: Model ignores content and rewards fluent but irrelevant speech. Mitigation: Recognize this as a system limitation; a content-aware module is required for high-stakes validity.
  - **Gradient underflow**: Training fails to progress. Mitigation: Ensure RMSE is used as the loss function, not MSE.

- **First 3 experiments:**
  1. **Baseline Validation**: Implement the AVG aggregator model and train on the standard dataset. Reproduce the reported ~0.384 RMSE to validate the full pipeline.
  2. **Aggregator Ablation**: Train identical models with AVG and TF aggregators. Compare overall RMSE and edge-case RMSEe to confirm the stability/performance trade-off described in the paper.
  3. **Data Strategy Ablation**: Train the best-performing aggregator (likely AVG for stability) on the "Standard," "4x Swap," and "Oversample Edge (OE)" datasets. Quantify the impact of swap sampling on overall performance and edge-case robustness.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Could future systems benefit more from selectively collecting underrepresented edge cases rather than expanding common samples via synthetic techniques?
- **Basis in paper:** [explicit] Section 4.1 explicitly asks if future systems would benefit more from "selectively collecting underrepresented edge cases, rather than expanding common samples."
- **Why unresolved:** The current study only evaluated data re-sampling strategies on the existing corpus; it did not test the impact of collecting new, targeted data.
- **What evidence would resolve it:** A comparative study measuring performance gains from targeted collection of edge-case speakers versus synthetic augmentation of majority classes.

### Open Question 2
- **Question:** How can acoustic-only ASA models be made sensitive to speech content to prevent fluent but off-topic responses from receiving high scores?
- **Basis in paper:** [inferred] Section 4.2 validates that the model is "insensitive to speech content," failing to penalize swapped (off-topic) answers, which the authors identify as a validity risk.
- **Why unresolved:** The efficient averaging aggregator discards semantic details required for content verification to maintain computational efficiency.
- **What evidence would resolve it:** An architecture that integrates lightweight content awareness and demonstrates a significant score drop when presented with fluent but off-topic speech.

### Open Question 3
- **Question:** Can an aggregator be designed that retains the training stability of average pooling while capturing the fine-grained details needed for high-proficiency (C1+) speakers?
- **Basis in paper:** [inferred] Section 4 notes the AVG aggregator is stable but loses detail, while the TF aggregator captures edge cases better but suffers from training fluctuations.
- **Why unresolved:** A trade-off currently exists between the robustness of simple pooling and the sensitivity of attention mechanisms for imbalanced proficiency classes.
- **What evidence would resolve it:** A model that achieves low RMSE on C1+ speakers without the sharp performance fluctuations observed in the Transformer-based aggregator.

## Limitations
- **Synthetic Data Validity**: Swap sampling constructs synthetic speakers by combining parts from different speakers with similar scores. While effective for data efficiency, there is no corpus evidence that these synthetic profiles are linguistically coherent or valid for real-world assessment.
- **Lack of ASR Validation**: The method skips explicit ASR transcription and relies on encoder-only features. The paper does not provide evidence that Whisper's acoustic representations alone are sufficient for robust proficiency grading, especially for mispronunciation detection or content coherence.
- **Edge-Case Instability**: The TF aggregator shows unstable training behavior, and edge-case performance (RMSEe) is not consistently reported across experiments. The method may underperform on rare proficiency levels without targeted oversampling.

## Confidence
- **High Confidence**: The core mechanism of using a single Whisper-small encoder with chunk-wise time averaging to produce fixed-size embeddings is clearly described and technically sound. The reported RMSE of 0.384 on the Speak & Improve Challenge dataset is a verifiable claim.
- **Medium Confidence**: The swap sampling strategy's effectiveness (44.8% data reduction with maintained performance) is supported by results, but the method's generalizability to other datasets or languages is unproven.
- **Low Confidence**: The claim that the model "outperforms the official baseline" is based on a single dataset and task. No ablation studies compare the full system against a strong ASR + LM baseline, leaving open the question of whether the gains come from the encoder-only design or other factors.

## Next Checks
1. **Reproduce Baseline Performance**: Implement the AVG aggregator model and train on the standard dataset. Verify that the reported RMSE of ~0.384 is achievable with the specified hyperparameters (15 epochs, LR 5×10⁻⁵, RMSE loss).
2. **Validate Edge-Case Robustness**: Train the best-performing aggregator (likely AVG) on the "Standard," "4x Swap," and "Oversample Edge (OE)" datasets. Quantify the impact of swap sampling on overall performance and edge-case RMSEe to confirm the claimed robustness.
3. **Test Model Generalization**: Evaluate the trained model on a held-out subset of the Speak & Improve corpus or a different L2 English dataset. Measure performance degradation to assess whether the model's strengths are task-specific or more general.