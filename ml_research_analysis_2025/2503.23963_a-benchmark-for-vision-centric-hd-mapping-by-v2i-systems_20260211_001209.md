---
ver: rpa2
title: A Benchmark for Vision-Centric HD Mapping by V2I Systems
arxiv_id: '2503.23963'
source_url: https://arxiv.org/abs/2503.23963
tags:
- v2i-hd
- dataset
- maps
- conference
- construction
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces V2I-HD, a neural framework for constructing
  vectorized HD maps using vision-centric vehicle-to-infrastructure (V2I) systems.
  The core method leverages collaborative camera frames from both vehicles and roadside
  infrastructure to extract features, transform them into bird's-eye-view (BEV) representation,
  and model map elements using an equivalent vectorized point set with a DETR-based
  decoder.
---

# A Benchmark for Vision-Centric HD Mapping by V2I Systems

## Quick Facts
- arXiv ID: 2503.23963
- Source URL: https://arxiv.org/abs/2503.23963
- Reference count: 36
- Primary result: V2I-HD achieves up to 43.9 mAP and 15.5 FPS on the V2X-Seq dataset using vision-centric vehicle-to-infrastructure systems

## Executive Summary
This paper introduces V2I-HD, a neural framework for constructing vectorized HD maps using vision-centric vehicle-to-infrastructure (V2I) systems. The method fuses features from vehicle and roadside cameras into a unified bird's-eye-view (BEV) representation, then models map elements as vectorized point sets using a DETR-based decoder. To enable real-time deployment, the authors propose a directionally decoupled self-attention mechanism that reduces computational complexity from O((N × Nv)² to O((N × Nv)^1.5). The work releases a real-world dataset with annotated collaborative camera frames for V2I HD map construction.

## Method Summary
V2I-HD constructs vectorized HD maps by fusing collaborative camera frames from vehicles and infrastructure. A CNN backbone extracts features from both viewpoints, which are transformed into a unified BEV space via a geometry-guided kernel transformer (GKT). Map elements are modeled as permutation-equivalent vectorized point sets using a DETR-based decoder. To reduce computational cost, the method employs directionally decoupled self-attention that breaks attention computation into horizontal and vertical components. The framework is trained end-to-end with losses for classification, point regression, and edge direction prediction.

## Key Results
- V2I-HD achieves 43.9 mAP at 15.5 FPS on V2X-Seq dataset with ResNet-18 backbone
- Directionally decoupled self-attention reduces GPU memory usage from 11621 MB to 8930 MB while improving accuracy
- Vehicle+Infrastructure (V&I) input outperforms Vehicle-only (V) by ~10-15 mAP across map element categories
- GKT-based BEV transformation enables 15.2 FPS vs. 12.8 FPS for LSS alternative

## Why This Works (Mechanism)

### Mechanism 1
Fusing vehicle and infrastructure camera features into a unified BEV representation extends perception range beyond single-vehicle limitations. The method uses a CNN backbone to extract features from both vehicle front-facing and roadside overhead cameras, passes them through FPN, and transforms them into shared BEV space via GKT. Core assumption: extrinsic calibration between cameras is sufficiently accurate for feature alignment. Evidence: [abstract] "extract features, transform them into bird's-eye-view (BEV) representation" and [section III.B.2] describes consistent BEV conversion. Break condition: if synchronization latency exceeds real-time constraints or calibration drifts significantly.

### Mechanism 2
Modeling map elements as permutation-equivalent vectorized point sets enables robust end-to-end learning without explicit ordering constraints. Map elements are represented as point sets V with defined equivalence classes Γ (polylines have 2 permutations, polygons have 2Nv permutations). A DETR-based decoder with learned queries predicts these point sets directly. Core assumption: chosen permutation groups capture real-world map topology ambiguity. Evidence: [abstract] "model map elements using an equivalent vectorized point set with a DETR-based decoder" and [section III.A] defines Γpolyline and Γpolygon. Break condition: if ground truth annotations have inconsistent ordering or complex topologies exceed point set modeling capacity.

### Mechanism 3
Directionally decoupled self-attention reduces computational complexity while preserving accuracy for real-time deployment. Standard self-attention at O((N × Nv)²) is replaced with sequential horizontal then vertical attention, reducing complexity to O((N × Nv)^1.5). Core assumption: sequential axis-aligned attention captures sufficient cross-element relationships. Evidence: [abstract] specifies complexity reduction and [table III] shows decoupled attention achieving 43.9 mAP at 18.8 FPS with 8930 MB memory. Break condition: if map elements require strong diagonal dependencies that sequential attention cannot capture.

## Foundational Learning

- **Transformer Attention Mechanisms**: Understanding Q/K/V computation, multi-head attention, and complexity scaling is essential since the entire architecture relies on attention for BEV transformation and map decoding. Quick check: Can you explain why standard self-attention scales as O(N²) and how sparse/decomposed attention reduces this?

- **DETR (DEtection TRansformer) Paradigm**: The map decoder uses DETR-style set prediction with learned queries and Hungarian matching for end-to-end training. Quick check: How does DETR handle the correspondence problem between predictions and ground truth without hand-crafted anchors?

- **BEV Representation Learning**: Understanding how 2D image features are lifted to bird's-eye-view is critical. Methods include LSS, IPM, and GKT—the chosen method. Quick check: What geometric assumptions does IPM make about the ground plane, and when do these fail?

## Architecture Onboarding

- **Component map:** Input images → CNN+FPN → GKT BEV projection → Fused BEV features → Decoupled-attention decoder → Vectorized map elements

- **Critical path:** Image features extracted from vehicle and infrastructure cameras flow through feature pyramid network, get transformed into unified BEV space via GKT, then pass through decoder with directionally decoupled attention to produce vectorized map elements

- **Design tradeoffs:** GKT vs. LSS vs. IPM for BEV: GKT chosen for vehicle-side deployment efficiency (15.2 FPS vs. 12.8 FPS for LSS). ResNet-18 vs. ResNet-50: R18 enables 18.8 FPS at 38.9 mAP; R50 achieves 43.9 mAP but drops to 9.6 FPS. Decoupled attention vs. RCCA vs. vanilla: Decoupled offers best accuracy/memory tradeoff

- **Failure signatures:** Low mAP on pedestrian crossings likely due to insufficient point query count or BEV resolution undersampling small features. High GPU memory despite decoupled attention may indicate batch size >1 or excessive query count. Misaligned BEV features suggest extrinsic calibration issues

- **First 3 experiments:** 1) Baseline validation: Run V2I-HD with vehicle-only vs. vehicle+infrastructure inputs. Expected: ~10-15 mAP improvement with V&I. 2) Attention ablation: Compare vanilla self-attention vs. RCCA vs. decoupled attention on ResNet-18. Expected: Decoupled should achieve highest mAP with lowest memory. 3) BEV encoder swap: Replace GKT with LSS or IPM. Expected: Performance drop in FPS (LSS) or accuracy (IPM)

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions in the provided content.

## Limitations
- The Directionally Decoupled Self-Attention mechanism is novel to this work and lacks direct validation from prior research, making its effectiveness for HD mapping unverified
- The framework assumes accurate extrinsic calibration between vehicle and infrastructure cameras, but real-world calibration drift or latency could significantly degrade BEV alignment and accuracy
- Performance is benchmarked only on the V2X-Seq dataset, which may not represent the full diversity of real-world driving conditions across weather, lighting, and traffic scenarios

## Confidence
- **High:** The core architecture (DETR-based decoder with vectorized point sets) is well-established with strong validation from MapTR and related works
- **Medium:** Experimental results showing improved performance are compelling, but the novelty of the decoupled attention mechanism and specific V2I dataset make generalizability difficult to assess
- **Low:** Real-time performance claim (15.5 FPS) is based on single-GPU testing with ResNet-18; scaling to multi-GPU or edge deployments may require additional optimization

## Next Checks
1. **Cross-dataset validation:** Evaluate V2I-HD on publicly available autonomous driving datasets (nuScenes or Argoverse) with annotated map elements to assess generalizability beyond V2X-Seq
2. **Calibration robustness test:** Simulate calibration drift and latency in vehicle-infrastructure data streams to quantify impact on BEV alignment and mapping accuracy under controlled parameter offsets
3. **Attention mechanism ablation:** Conduct detailed comparison of Directionally Decoupled Self-Attention against other efficient attention variants (Longformer, Linformer) on both accuracy and computational efficiency metrics