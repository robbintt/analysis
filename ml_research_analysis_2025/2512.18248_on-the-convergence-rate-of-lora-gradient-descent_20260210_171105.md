---
ver: rpa2
title: On the Convergence Rate of LoRA Gradient Descent
arxiv_id: '2512.18248'
source_url: https://arxiv.org/abs/2512.18248
tags:
- lora
- gradient
- rate
- learning
- convergence
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper analyzes the convergence of Low-Rank Adaptation (LoRA)
  gradient descent, a popular fine-tuning method for large language models. LoRA reduces
  computational cost by training only low-rank adapter matrices while keeping pretrained
  weights frozen.
---

# On the Convergence Rate of LoRA Gradient Descent

## Quick Facts
- arXiv ID: 2512.18248
- Source URL: https://arxiv.org/abs/2512.18248
- Reference count: 40
- The first non-asymptotic convergence analysis of LoRA gradient descent without assuming bounded parameters or Lipschitz smoothness

## Executive Summary
This paper provides the first theoretical convergence analysis of Low-Rank Adaptation (LoRA) gradient descent, a popular method for fine-tuning large language models. LoRA works by training only low-rank adapter matrices while keeping pretrained weights frozen, reducing computational costs. The authors address the challenge that classic convergence analyses don't apply because the reparameterized loss function is not Lipschitz smooth. They reformulate the problem using the outer product of stacked adapter matrices and derive a modified descent lemma to prove convergence to a stationary point at rate O(1/log T), improving to O(1/T) under bounded adapter norms.

## Method Summary
The authors reformulate LoRA gradient descent by expressing the optimization problem in terms of the outer product of stacked adapter matrices. This reformulation enables them to derive a modified descent lemma that accounts for the non-smooth nature of the reparameterized loss function. By carefully controlling the learning rate based on parameter and gradient norms, they establish convergence guarantees without requiring the standard assumptions of bounded parameters or Lipschitz smoothness. The analysis also proposes practical learning rate schemes (adaptive and normalized) that improve convergence in experiments.

## Key Results
- First non-asymptotic convergence analysis of LoRA gradient descent
- Proves convergence to stationary point at rate O(1/log T), improving to O(1/T) with bounded adapter norms
- Derives modified descent lemma to handle non-smooth reparameterized loss
- Proposes and validates practical learning rate schemes (adaptive and normalized)

## Why This Works (Mechanism)
The mechanism works by reformulating the LoRA optimization problem in terms of the outer product of stacked adapter matrices. This reformulation allows the authors to construct a modified descent lemma that can handle the non-Lipschitz smooth nature of the reparameterized loss function. By controlling the learning rate based on the norms of parameters and gradients, the algorithm ensures sufficient progress in each iteration while avoiding divergence. The key insight is that even though the reparameterized loss violates standard smoothness assumptions, the structure of LoRA's low-rank updates can be leveraged to establish convergence guarantees.

## Foundational Learning

**Low-rank matrix approximation**: Used to reduce the number of trainable parameters in fine-tuning large models
- Why needed: LoRA's core mechanism relies on low-rank updates to pretrained weights
- Quick check: Verify understanding of rank-r matrix factorization

**Non-smooth optimization**: Techniques for analyzing convergence when standard smoothness assumptions fail
- Why needed: The reparameterized LoRA loss is not Lipschitz smooth
- Quick check: Can you explain why standard gradient descent analysis doesn't apply?

**Descent lemma modification**: Adapted convergence analysis for specific problem structures
- Why needed: Standard descent lemmas assume smoothness properties that don't hold here
- Quick check: Understand how the modified lemma differs from standard versions

## Architecture Onboarding

**Component map**: LoRA adapters (low-rank matrices) -> frozen pretrained weights -> model output
**Critical path**: Adapter initialization → gradient computation → parameter update with controlled learning rate → convergence monitoring
**Design tradeoffs**: Computational efficiency (low-rank updates) vs. convergence guarantees (controlled learning rates)
**Failure signatures**: Divergence when learning rates not properly controlled; slow convergence without bounded adapter norms
**First experiments**: 1) Logistic regression on small dataset, 2) Fine-tuning ResNet-18 on CIFAR-10, 3) Compare adaptive vs normalized learning rates on same task

## Open Questions the Paper Calls Out
None

## Limitations
- Proof relies on assumptions about controlling learning rates based on parameter and gradient norms
- O(1/log T) convergence rate is relatively slow compared to standard gradient descent
- Experimental validation limited to small-scale problems rather than large language models

## Confidence
- High confidence in mathematical derivation and proof technique for modified descent lemma
- Medium confidence in practical relevance of learning rate schemes given limited experimental scope
- Medium confidence in convergence rate bounds due to assumptions about parameter and gradient norm behavior

## Next Checks
1. Test adaptive and normalized learning rate schemes on actual large language models to verify theoretical improvements translate to practical performance gains
2. Conduct ablation studies to determine sensitivity of convergence to assumptions about bounded adapter norms and controlled learning rates
3. Compare LoRA convergence behavior with standard fine-tuning methods on identical tasks to quantify trade-off between computational efficiency and convergence speed