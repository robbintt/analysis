---
ver: rpa2
title: Blurb-Refined Inference from Crowdsourced Book Reviews using Hierarchical Genre
  Mining with Dual-Path Graph Convolutions
arxiv_id: '2512.21076'
source_url: https://arxiv.org/abs/2512.21076
tags:
- genre
- reviews
- book
- classification
- higemine
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces HiGeMine, a hierarchical book genre classification
  framework that addresses noise and subjectivity in user reviews by combining them
  with authoritative book blurbs. The approach uses a two-phase pipeline: first, zero-shot
  semantic alignment filters reviews based on their consistency with blurbs; second,
  a dual-path graph convolutional network performs fiction/non-fiction classification
  followed by multi-label genre prediction, modeling inter-genre dependencies through
  a label co-occurrence graph.'
---

# Blurb-Refined Inference from Crowdsourced Book Reviews using Hierarchical Genre Mining with Dual-Path Graph Convolutions

## Quick Facts
- **arXiv ID:** 2512.21076
- **Source URL:** https://arxiv.org/abs/2512.21076
- **Reference count:** 8
- **Primary result:** HiGeMine achieves consistent F1 improvements over strong baselines for hierarchical book genre classification by filtering noisy reviews via semantic alignment with authoritative blurbs and modeling inter-genre dependencies through dual-path GCNs.

## Executive Summary
This paper introduces HiGeMine, a hierarchical book genre classification framework that addresses noise and subjectivity in user reviews by combining them with authoritative book blurbs. The approach uses a two-phase pipeline: first, zero-shot semantic alignment filters reviews based on their consistency with blurbs; second, a dual-path graph convolutional network performs fiction/non-fiction classification followed by multi-label genre prediction, modeling inter-genre dependencies through a label co-occurrence graph. The authors curate a new hierarchical book genre dataset and conduct extensive experiments comparing HiGeMine against strong baselines, including hierarchical classifiers, language models, and large language models. Results show consistent performance improvements, with notable gains in macro F1 scores, demonstrating the effectiveness of the proposed method in handling noisy, real-world textual data for hierarchical genre classification.

## Method Summary
HiGeMine is a two-level hierarchical classification framework that combines book blurbs with user reviews for genre prediction. The method first filters reviews by semantic consistency with blurbs using zero-shot BERT embeddings and cosine similarity. Two heterogeneous text graphs (blurb-token and review-token) are constructed with TF-IDF and PMI edge weights and processed through separate GCN paths, then fused via learned weights. For Level-2, genre co-occurrence dependencies are explicitly modeled through a directed label graph with GCN message passing. The framework uses BCE loss for Level-1 binary classification and BCE-with-logits for Level-2 multi-label prediction, with optimal fusion weights λ1=0.3 and λ2=0.7 determined via grid search.

## Key Results
- HiGeMine consistently outperforms strong baselines (BiLSTM, TextCNN, BERT variants) on both hierarchical and flat classification settings.
- Zero-shot review filtering improves macro F1 by 3-4% by reducing noise and bias in user reviews.
- The dual-path GCN architecture with label co-occurrence modeling provides 2-3% F1 gains over ablated versions.
- Performance degrades significantly when key components (review filtering, label dependency graph) are removed, validating their contributions.

## Why This Works (Mechanism)

### Mechanism 1: Zero-Shot Semantic Filtering Reduces Review Noise
Filtering user reviews by semantic consistency with authoritative blurbs mitigates noise, bias, and irrelevance, improving downstream classification. A BERT-based encoder generates embeddings for each blurb (bi) and review (δij). Cosine similarity dij = bi · δij / (|bi||δij|) is computed, and reviews exceeding threshold Ψ are retained and concatenated into consolidated review Pi. This nonparametric approach is fast, scalable, and model-agnostic. Core assumption: Blurbs are semantically reliable anchors representing ground-truth book content; reviews deviating from blurb semantics are likely noisy or irrelevant.

### Mechanism 2: Dual-Path GCN Captures Modality-Specific Token Interactions
Separate graph convolutional networks for blurb-token and review-token interactions enable modality-specific semantic learning, which is then fused for prediction. Two heterogeneous text graphs are constructed: GB (Blurb-Token) and GP (Review-Token). Edge weights use TF-IDF (document-token) and positive PMI (token-token co-occurrence). Each path processes through two stacked GCN layers (Φgc), concatenates outputs with initial features, and truncates to document nodes. Final predictions combine logits via weighted factor λ1. Core assumption: Blurb and review modalities have distinct semantic structures that benefit from separate graph-based message passing before fusion.

### Mechanism 3: Label Co-Occurrence Graph Models Inter-Genre Dependencies
Explicitly modeling genre co-occurrence patterns via a directed label graph improves multi-label prediction by capturing dependencies (e.g., "Mystery" often co-occurs with "Thriller"). A directed graph GC=(VC,EC) is built where nodes are genre labels and edges encode conditional probabilities P(Ci|Cj) from training data. Edge weights are thresholded (ψ1, ψ2) to reduce noise. GCN layers propagate label information, producing refined embeddings X'c fused with B-T and P-T outputs via dot product. Core assumption: Genre labels exhibit meaningful, learnable co-occurrence patterns that can inform prediction beyond individual label features.

## Foundational Learning

- **Concept: Graph Convolutional Networks (GCNs) for Text Classification**
  - Why needed here: Both modality graphs (B-T, P-T) and the label co-occurrence graph rely on GCN message passing to aggregate neighborhood information.
  - Quick check question: Can you explain how a two-layer GCN aggregates information from a node's 2-hop neighborhood?

- **Concept: Multi-Label Classification with BCE Loss**
  - Why needed here: Level-2 genre prediction is multi-label; BCE with logits loss is used, and sigmoid activation produces per-label probabilities.
  - Quick check question: Why is binary cross-entropy preferred over softmax cross-entropy for multi-label tasks?

- **Concept: Hierarchical Classification (Coarse-to-Fine)**
  - Why needed here: HiGeMine uses a two-level hierarchy: Level-1 binary (fiction/non-fiction) gates Level-2 multi-label genre classifiers via a gating mechanism.
  - Quick check question: How does hierarchical classification differ from flat multi-class classification in terms of error propagation?

## Architecture Onboarding

- **Component map:** Blurb Bi, Reviews Ri -> Preprocessing -> Review Filter -> Consolidated review Pi -> Vocabulary T -> Level-1 Classifier (Φ1) -> Level-2 Classifiers (Φf2, Φnf2) -> Output Y1, Y2
- **Critical path:** Review filtering → graph construction → GCN message passing → hierarchical prediction (Level-1 gates Level-2 branch selection). If filtering fails, noisy reviews degrade both levels.
- **Design tradeoffs:**
  - Hierarchical vs Flat: Hierarchy aligns with literary taxonomies but introduces dependency on Level-1 accuracy; flat 57-class model underperforms (ablation case-i).
  - λ1/λ2 weighting: Blurb favors Level-1; reviews favor Level-2. Grid search found λ1=0.3, λ2=0.7 optimal.
  - Thresholding (Ψ, ψ1, ψ2): Too aggressive filtering discards signal; too permissive retains noise.
- **Failure signatures:**
  - Short blurb: Filtering bypassed, noisy reviews retained → Level-1 degradation.
  - Sparse reviews: Insufficient signal for Level-2 → macro F1 drops.
  - Rare genres: Class imbalance → data augmentation (Gemini) applied, but may not fully address.
- **First 3 experiments:**
  1. Ablate review filtering: Compare HiGeMine with vs without semantic filtering (see case-e in Table 3: Fm drops from 77.24% to 73.30% for fiction).
  2. Vary λ1/λ2: Grid search over [0.0, 0.1, ..., 1.0] to find optimal fusion weights; validate assumption that blurb favors Level-1, reviews favor Level-2.
  3. Remove label co-occurrence graph (Φc): Replace with linear classifier (case-f in Table 3); expect ~3% Fm drop, confirming label dependency modeling value.

## Open Questions the Paper Calls Out
The conclusion states: "Future work will explore multimodal extensions and generalization to other hierarchical domains." This explicitly calls out the need to investigate integrating visual features (e.g., book covers) and extending the approach to other hierarchical classification domains beyond book genres.

## Limitations
- **Data curation gaps:** The dataset construction method is underspecified, affecting reproducibility and external validity.
- **Threshold sensitivity:** Optimal values for review filtering (Ψ) and label graph edges (ψ1, ψ2) are not reported, creating uncertainty about robustness.
- **Ablation gaps:** Critical experiments missing, including dual-path vs single-path GCN comparison and simpler attention-based label dependency alternatives.

## Confidence
- **High confidence:** The hierarchical architecture design and general GCN framework are well-specified. The filtering mechanism using BERT embeddings and cosine similarity is standard practice.
- **Medium confidence:** Performance improvements over baselines are statistically significant but absolute gains are modest (2-3% F1 improvements). The benefit of zero-shot filtering is supported by ablation but threshold sensitivity is unknown.
- **Low confidence:** Optimal fusion weights (λ1=0.3, λ2=0.7) appear without grid search methodology. Label co-occurrence graph contribution lacks rigorous ablation testing. Data augmentation via Gemini is minimally specified.

## Next Checks
1. **Threshold sensitivity analysis:** Systematically vary Ψ (review filtering), ψ1 and ψ2 (label graph edges) across [0.1, 0.3, 0.5, 0.7, 0.9] to identify performance stability ranges and optimal operating points.
2. **Dual-path vs single-path GCN ablation:** Implement and compare HiGeMine with a single GCN path processing concatenated blurb+reviews against the dual-path architecture to isolate the contribution of modality-specific processing.
3. **Label dependency ablation:** Replace the GCN-based label co-occurrence graph with a simple attention mechanism over genre labels or a linear classifier. Compare performance to quantify the specific benefit of graph-based label dependency modeling.