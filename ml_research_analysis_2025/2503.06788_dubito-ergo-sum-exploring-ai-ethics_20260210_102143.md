---
ver: rpa2
title: 'Dubito Ergo Sum: Exploring AI Ethics'
arxiv_id: '2503.06788'
source_url: https://arxiv.org/abs/2503.06788
tags:
- ethics
- moral
- https
- what
- doubt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper argues that AI lacks the capacity for moral agency because
  it cannot doubt itself. It reviews various ethical frameworks and concludes that
  sensory grounding, understanding, and the ability to doubt are essential for morality.
---

# Dubito Ergo Sum: Exploring AI Ethics

## Quick Facts
- arXiv ID: 2503.06788
- Source URL: https://arxiv.org/abs/2503.06788
- Authors: Viktor Dorfler; Giles Cuthbert
- Reference count: 0
- Primary result: AI lacks moral agency because it cannot doubt itself; should assist rather than automate human moral decision-making

## Executive Summary
This paper argues that AI cannot possess moral agency because it lacks the capacity for systematic self-doubt, which the authors identify as essential for genuine moral reasoning. Drawing from Descartes' "dubito ergo sum," they argue that current AI architectures fundamentally lack the sensory grounding, understanding, and doubt required for morality. Rather than attempting to embed ethics directly into AI systems, the authors propose using AI to assist human moral deliberation by providing input for self-doubt processes and scanning for emerging patterns that require human intervention. They conclude that AI amplifies existing ethical problems rather than creating new ones.

## Method Summary
This is a conceptual philosophical analysis drawing on philosophy of mind, ethics literature, and observations of AI behavior. The authors employ a phenomenological approach within a Critical Interpretivism paradigm, using abductive reasoning to develop phenomenon-driven theorizing. The paper reviews various ethical frameworks (virtue ethics, deontology, consequentialism) and applies a three-pillar test (sensory grounding/indwelling, understanding, capacity to doubt) to evaluate AI's moral agency claims. No formal evaluation methodology or empirical experiments are provided.

## Key Results
- AI cannot possess moral agency because it cannot doubt itself, which is essential for moral reasoning
- AI amplifies existing ethical problems rather than creating new ones through bias replication
- AI should be used to assist human moral decision-making rather than attempting autonomous moral judgment

## Why This Works (Mechanism)

### Mechanism 1: Self-Doubt as Necessary Condition for Moral Agency
- Claim: Moral agency requires systematic self-doubt, which current AI architectures cannot possess
- Core assumption: Self-doubt is phenomenologically grounded and tied to embodied experience, not just computational uncertainty
- Evidence: AI lacks capacity to question its own judgments and motivations (abstract, section 7)
- Break condition: If future AI develops genuine meta-cognitive self-questioning beyond confidence scores

### Mechanism 2: AI as Amplifier of Existing Ethical Problems
- Claim: AI amplifies rather than creates novel ethical problems
- Core assumption: Current ML/DL paradigms cannot transcend statistical properties of training data
- Evidence: AI replicates and amplifies biases from training data (abstract, section 3)
- Break condition: If architectures emerge that learn moral principles from limited examples and generalize beyond training distribution

### Mechanism 3: Augmentation Over Automation for Moral Decision-Support
- Claim: AI should support rather than replace human moral deliberation
- Core assumption: Humans retain moral judgment capacity when informed, and AI can identify when to involve humans
- Evidence: Humans make intuitive moral judgments; AI lacks "felt sense" for moral sensitivity (abstract, section 8)
- Break condition: If reliable external pointers for human involvement cannot be specified or humans cannot process AI inputs

## Foundational Learning

- Concept: Normative ethics frameworks (virtue ethics, deontology, consequentialism)
  - Why needed: Required to understand operationalization challenges of embedding ethics in AI
  - Quick check: Can you explain why deontology's rule-exceptions and consequentialism's outcome-uncertainty both pose implementation problems for AI?

- Concept: Moral psychology's intuitionist model (Haidt, Rest)
  - Why needed: Essential for understanding augmentation argument and human moral judgment processes
  - Quick check: What are Rest's four stages of moral decision-making, and which stages does the paper argue AI cannot perform?

- Concept: Phenomenological indwelling (Polányi)
  - Why needed: Necessary for understanding sensory grounding and tacit knowledge in moral reasoning
  - Quick check: How does "felt sense" relate to AI's inability to know when to involve humans?

## Architecture Onboarding

- Component map: Pattern scanner -> Context monitor -> External pointer system -> Human deliberation interface

- Critical path:
  1. Define trigger conditions for human involvement (hardest design problem)
  2. Build pattern-matching across ethical frameworks
  3. Design context monitoring for distribution shift/novel situations
  4. Create interface that informs without over-determining judgment

- Design tradeoffs:
  - Specificity vs. coverage in human-involvement triggers: too specific → misses edge cases; too broad → alert fatigue
  - Pattern-matching depth: helps or biases human deliberation
  - Transparency: how much pattern-matching logic should be exposed

- Failure signatures:
  - AI proceeding confidently in morally salient situations without flagging humans
  - Alert fatigue from overly sensitive triggers
  - Pattern-matching that biases human deliberation toward particular frameworks
  - Systematic failure to detect novel moral situations

- First 3 experiments:
  1. Pattern relevance testing: Measure whether AI-identified ethical patterns improve deliberation quality or introduce bias
  2. Trigger calibration: Test false positive/negative rates against human moral sensitivity judgments
  3. Amplification detection: Compare AI outputs against training data distributions to measure bias amplification

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can AI be designed to autonomously identify its limitations and request human assistance during decision-making?
- Basis: Authors identify need for AI to flag its own uncertainty to humans
- Why unresolved: Current systems operate without self-awareness of blind spots
- Evidence needed: System architecture where AI pauses and queries human upon encountering patterns outside trained confidence thresholds

### Open Question 2
- Question: What specific variables and taxonomies of moral decisions are required to make Deep Learning viable for moral judgments?
- Basis: Paper asks about variables needed and types of moral decisions for training
- Why unresolved: Moral decisions are intuitive and vast, making finite variable sets difficult
- Evidence needed: Comprehensive ontology of moral decision types and demonstration of successful prediction across diverse scenarios

### Open Question 3
- Question: How can we engineer "external pointers" to trigger human intervention in AI lacking sensory "felt sense"?
- Basis: Authors conclude need for external pointers when AI lacks human moral experience
- Why unresolved: Humans rely on visceral sensations; functional proxy for biological trigger not established
- Evidence needed: Algorithmic "moral compass" that successfully flags non-obvious ethical risks for human review

### Open Question 4
- Question: To what extent does AI-provided input enhance human self-doubt and ethical justification?
- Basis: Authors hypothesize AI can provide useful input for self-doubt but offer no solution
- Why unresolved: Unclear if AI assistance reliably improves moral outcomes or adds noise
- Evidence needed: Empirical studies showing decision-makers using AI analysis arrive at more robust moral judgments

## Limitations

- Operationalization gap: Lacks concrete definitions for "understanding," "sensory grounding," and "doubt" as they relate to AI systems
- Anthropocentric bias risk: Assumes human moral psychology as standard, potentially overlooking alternative ethical reasoning forms
- Amplification mechanism: Claims AI amplifies biases but doesn't quantify the amplification effect or distinguish between bias types

## Confidence

- High confidence: Current AI architectures lack phenomenological grounding and cannot experience genuine self-doubt
- Medium confidence: Argument that AI amplifies rather than creates ethical problems (conceptually sound but lacking empirical quantification)
- Medium confidence: Augmentation over automation recommendation (practically sound but depends on untested interaction assumptions)

## Next Checks

1. Pattern relevance validation: Test whether AI-identified ethical patterns improve human moral deliberation quality without introducing systematic bias across multiple frameworks
2. Trigger specification development: Design and test candidate specifications for when AI should involve humans, measuring false positive/negative rates against human moral sensitivity judgments
3. Amplification quantification: Compare AI system outputs against training data distributions in morally-relevant domains to empirically measure bias amplification effects