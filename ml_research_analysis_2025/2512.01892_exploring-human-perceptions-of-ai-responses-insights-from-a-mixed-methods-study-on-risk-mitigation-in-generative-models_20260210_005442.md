---
ver: rpa2
title: 'Exploring Human Perceptions of AI Responses: Insights from a Mixed-Methods
  Study on Risk Mitigation in Generative Models'
arxiv_id: '2512.01892'
source_url: https://arxiv.org/abs/2512.01892
tags:
- participants
- responses
- mitigated
- mitigation
- human
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study investigates how humans perceive AI-generated responses
  that have been modified by a mitigation model to reduce harm. In a mixed-method
  experiment, 57 participants evaluated responses across multiple dimensions: faithfulness,
  fairness, harm-removal capacity, and relevance.'
---

# Exploring Human Perceptions of AI Responses: Insights from a Mixed-Methods Study on Risk Mitigation in Generative Models

## Quick Facts
- arXiv ID: 2512.01892
- Source URL: https://arxiv.org/abs/2512.01892
- Reference count: 40
- Participants preferred mitigated responses 66.8% of the time, with strong preferences for fairness (88%) and competence (66%)

## Executive Summary
This study investigates how humans perceive AI-generated responses that have been modified by a mitigation model to reduce harm. Using a mixed-methods experiment with 57 participants, the research evaluates responses across multiple dimensions including faithfulness, fairness, harm-removal capacity, and relevance. Results showed participants preferred mitigated responses 66.8% of the time, with strong preferences for fairness (88%) and competence (66%). Language background and AI experience significantly influenced evaluations. Participants valued mitigation that preserved core meaning while removing harmful content, and penalized responses with grammar errors or lost relevance. Seeing the original response reduced mitigation scores, indicating critical evaluation. New metrics for training and evaluating mitigation strategies were proposed based on these insights.

## Method Summary
The study employed a two-stage within-subjects protocol where participants first evaluated mitigated responses blindly (6 examples across fairness and relevance dimensions), then compared original and mitigated versions side-by-side (12 total examples). A synthetically generated dataset using LLMs seeded with human-authored prompts covered sensitive topics including health habits, dress codes, patriotism, gender identity, and moral values. The mitigator model was trained via in-context learning on this synthetic data using 2B-8B parameter models or LoRA adapters. Analysis employed Chi-square tests for preference analysis, logistic regression for demographic effects, Wilcoxon signed-rank tests for phase comparisons, and reflexive thematic analysis on open-ended responses.

## Key Results
- Participants preferred mitigated responses 66.8% of the time overall
- Strong preference for fairness mitigation (88%) and competence (66%)
- Language background and AI experience significantly influenced evaluation patterns
- Grammar errors and relevance loss were heavily penalized in mitigated responses
- Exposure to original responses led to more critical evaluation of mitigations

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Demographic and experiential factors (language background, AI experience) significantly influence how humans evaluate the fairness of mitigated AI responses.
- **Mechanism:** Non-native English speakers and those without AI annotation experience apply stricter or differently calibrated standards when assessing whether a response is "fair," likely due to varying interpretations of bias, tone, and cultural context.
- **Core assumption:** Evaluation criteria for fairness are not universal but are mediated by the evaluator's linguistic and cultural frame of reference.
- **Evidence anchors:**
  - [abstract]: "Participants' native language, AI work experience, and annotation familiarity significantly influenced evaluations."
  - [section] (Findings - RQ2): "The probability to choose the mitigated answer for non-native English speakers was 85% less than native English speakers (p=0.0002)."
  - [corpus]: Weak direct support. A related paper on ethics practices across regions [arxiv:2508.09219] discusses regional differences in ethical perceptions but does not directly validate this mechanism.
- **Break condition:** This mechanism may fail to generalize to populations with high AI literacy or to non-Western cultural contexts not represented in the study's 57 participants.

### Mechanism 2
- **Claim:** Human evaluators penalize minor grammatical errors and reward preserved semantic context in mitigated responses, creating a sensitivity that automated evaluation metrics often fail to capture.
- **Mechanism:** Humans treat surface-level language quality as a proxy for competence and trustworthiness. A mitigated response that removes harm but introduces errors or overly generalizes is judged more harshly than one that preserves the original's structure and key information, even if the original was harmful.
- **Core assumption:** Linguistic fidelity and semantic completeness are treated as essential components of a "good" mitigation, distinct from the mere absence of harm.
- **Evidence anchors:**
  - [abstract]: "Participants showed high sensitivity to linguistic and contextual attributes, penalizing minor grammar errors while rewarding preserved semantic contexts."
  - [section] (Key Insights): "Grammatical and Punctuation errors and lack of logical structure affected the way people evaluated Fairness, trust, and quality of the response."
  - [corpus]: No direct support found in provided corpus neighbors.
- **Break condition:** This mechanism may not apply when evaluators prioritize harm removal above all else, or in contexts where grammatical errors are common and less stigmatized.

### Mechanism 3
- **Claim:** Exposure to the original (unmitigated) response causes evaluators to rate the mitigated version more critically, suggesting that transparency about the mitigation process influences perceived quality.
- **Mechanism:** Seeing the original response sets a comparative baseline. Evaluators then judge the mitigated response not just on its own merits, but on how well it navigates the trade-off between removing harm and preserving the original's intent, relevance, and completeness.
- **Core assumption:** Human judgment of mitigation quality is inherently comparative, not absolute.
- **Evidence anchors:**
  - [abstract]: Noted in results but not detailed; [section] (Findings - RQ4): "Exposure to the original response led participants to assign lower scores to the mitigator answer... suggesting a statistically significant change in score selection behavior."
  - [section] (Qualitative analysis): "Seeing original and mitigated responses side-by-side helped participants understand what changed and why; this frequently increased trust, but also exposed when edits felt generic or altered meaning."
  - [corpus]: No direct support found in provided corpus neighbors.
- **Break condition:** The effect may diminish if the original and mitigated responses are not presented simultaneously, or if evaluators lack the context to understand the mitigation's goals.

## Foundational Learning

- **Concept: Within-Subject Study Design with Counterbalancing**
  - **Why needed here:** The paper uses a within-subject design where each participant evaluates responses in two phases. Counterbalancing the order of examples is crucial to prevent order effects from confounding the results. Understanding this is key to interpreting why the study can claim causal influence from exposure to the original response.
  - **Quick check question:** Why would presenting all "Fairness" examples before "Relevance" examples potentially bias the results?

- **Concept: In-Context Learning (ICL) for Specialized Mitigation Models**
  - **Why needed here:** The mitigator model is trained using ICL on synthetically generated data. This is a core architectural choice that allows a smaller model to perform a specialized task (harm removal) by learning from structured examples, rather than massive pre-training. This is central to the paper's proposed solution.
  - **Quick check question:** How does ICL enable a "mitigator" model to learn the task of harm removal without being explicitly programmed with rules?

- **Concept: Trade-offs in AI Safety Metrics (Relevance vs. Fairness vs. Faithfulness)**
  - **Why needed here:** The qualitative findings explicitly discuss participants balancing competing values (e.g., sacrificing relevance to remove harm). This is not a technical bug but a fundamental design challenge. Internalizing that mitigation involves multi-objective optimization is critical for anyone working on guardrails.
  - **Quick check question:** According to participants, what happens when a mitigation strategy makes an answer "safer" but less direct in addressing the user's prompt?

## Architecture Onboarding

- **Component Map:** Synthetic Data Generator -> Mitigator Model (2B-8B or LoRA) -> Evaluation Interface (2-phase) -> Human Feedback Loop

- **Critical Path:**
  1. Define risk taxonomy and evaluation metrics via stakeholder workshops
  2. Generate diverse, thematically coherent synthetic training data
  3. Train the specialized mitigator model using ICL
  4. Deploy mitigator as a post-hoc layer on top of a base LLM
  5. Continuously evaluate outputs using human-in-the-loop studies, feeding new insights back into metric definition and data generation

- **Design Tradeoffs:**
  - **Model Size vs. Specialization:** Using smaller models (2B-8B params) for the mitigator is efficient but may lack the nuanced generative capabilities of larger models, potentially leading to over-generalized mitigations
  - **Automated vs. Human Evaluation:** Automated metrics are scalable but fail to capture the linguistic sensitivity and value trade-offs humans exhibit. Relying solely on them risks deploying mitigators that users perceive as incompetent or unfaithful
  - **Selective vs. Comprehensive Mitigation:** The goal is to remove only harmful spans ("selective mitigation"), but models may struggle to identify precise boundaries, leading to unnecessary edits that alter meaning

- **Failure Signatures:**
  - **Over-mitigation:** The response becomes vague, overly cautious, or fails to answer the prompt directly (sacrificing relevance). Users prefer the original, flawed response
  - **Faithfulness Loss:** The core meaning or key information from the original is lost or distorted in the mitigation process
  - **Tone Mismatch:** The mitigated response adopts an inappropriate tone (e.g., sarcastic, overly casual), eroding trust regardless of content safety
  - **Grammar/Coherence Degradation:** The mitigation introduces grammatical errors or logical inconsistencies, penalized heavily by users

- **First 3 Experiments:**
  1. **Ablation on Model Size:** Train mitigator models of varying sizes (e.g., 1B, 3B, 7B parameters) and evaluate the trade-off between mitigation competence and preservation of relevance/faithfulness using the human evaluation protocol
  2. **Error Analysis on "Grammar Sensitivity":** Curate a dataset where mitigations introduce controlled grammatical errors. Measure the impact on perceived fairness and trust compared to grammatically clean mitigations that are semantically equivalent
  3. **Transparency A/B Test:** Compare user evaluations of mitigated responses when the original is (a) never shown, (b) shown simultaneously, and (c) shown sequentially after initial evaluation. Quantify the "transparency penalty" and identify mitigation strategies that are more robust to comparative scrutiny

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can qualitative human-centric metrics (e.g., tone, selective mitigation, coherence) be effectively translated into quantitative, system-based metrics for automated model training?
- **Basis in paper:** [explicit] The Discussion section states the authors are currently "investigating the various which in which these metrics are understood and considered by humans, and how we can 'translate' into system-based evaluation metrics."
- **Why unresolved:** The paper identifies these metrics as essential based on human feedback but notes they contrast with standard automated approaches like BLEU or semantic similarity, lacking a clear computational equivalent.
- **What evidence would resolve it:** The development and validation of an automated scoring function that correlates strongly with human ratings for "selective mitigation" and "tone" on a held-out dataset.

### Open Question 2
- **Question:** How can evaluation frameworks be designed to account for and resolve the trade-off between relevance and fairness when users prefer relevant but unmitigated responses?
- **Basis in paper:** [inferred] Insight 7 notes that model evaluations should account for conflicting values. The study found participants sometimes preferred original responses because mitigated versions "sacrificed relevance," suggesting a need for a "scoring framework that reflects these trade-offs."
- **Why unresolved:** The study highlights the conflict but does not propose a specific mechanism or weighting system to balance these competing values during model optimization.
- **What evidence would resolve it:** A comparative study showing that a multi-objective reward model, which explicitly penalizes relevance loss during fairness optimization, results in higher overall human preference scores than single-metric optimization.

### Open Question 3
- **Question:** Does developing mitigation techniques for specific socio-demographic contexts (based on native language or location) significantly improve perceived performance over generalized models?
- **Basis in paper:** [inferred] The Discussion suggests that because native language and location significantly influenced evaluations, "one should consider developing such mitigation techniques for specific socio-demographic contexts."
- **Why unresolved:** The current study used a generalized approach on a limited sample (57 participants); the efficacy of context-specific mitigators remains a hypothesis derived from the observed variance in user preferences.
- **What evidence would resolve it:** An A/B test demonstrating that a mitigator fine-tuned for specific linguistic/cultural contexts achieves higher "competence" and "faithfulness" scores from that demographic than a general-purpose model.

## Limitations
- **Sample Size and Diversity:** With only 57 participants, the study's findings on demographic effects may not generalize reliably to broader populations
- **Model Architecture and Training:** The exact mitigator model architecture, training procedure, and weights are unspecified beyond a vague "2B-8B parameters or LoRA adapters"
- **Synthetic Data Validity:** The study relies on synthetically generated harmful/mitigated response pairs, which may not capture the nuance and diversity of real-world harmful content

## Confidence
- **Demographic Influence on Fairness Evaluation (Mechanism 1):** Medium - Supported by statistical significance but limited by sample size
- **Grammar and Semantic Fidelity as Key Evaluation Criteria (Mechanism 2):** High - Directly observed in participant feedback and penalized in ratings
- **Transparency and Comparative Evaluation Effect (Mechanism 3):** Medium - Statistically significant but based on a single experimental condition

## Next Checks
1. **Replicate Demographic Analysis:** Conduct a follow-up study with a larger, more diverse participant pool (N > 200) stratified by language background and AI experience. Replicate the logistic regression analysis to confirm the magnitude and significance of demographic effects on fairness and competence evaluations.
2. **Ablation Study on Model Architecture:** Train multiple mitigator models with varying sizes (1B, 3B, 7B) and compare their performance on the human evaluation protocol. Quantify the trade-off between mitigation competence and preservation of relevance/faithfulness to determine if smaller models indeed introduce over-mitigation or grammar issues.
3. **Cross-Dataset Validation:** Apply the mitigator and evaluation protocol to a publicly available dataset of real-world harmful content (e.g., Wikipedia talk page edits, Reddit comments flagged for toxicity). Compare human evaluation scores and qualitative feedback to the synthetic data results to assess ecological validity.