---
ver: rpa2
title: 'Beyond Accuracy: What Matters in Designing Well-Behaved Image Classification
  Models?'
arxiv_id: '2503.17110'
source_url: https://arxiv.org/abs/2503.17110
tags:
- in1k
- russakovsky
- transformer
- quality
- dimensions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study addresses the need to evaluate deep neural networks
  (DNNs) beyond accuracy, considering multiple quality dimensions simultaneously.
  The authors analyze 326 backbone models across nine quality dimensions, including
  accuracy, robustness, calibration, fairness, object focus, shape bias, and computational
  cost.
---

# Beyond Accuracy: What Matters in Designing Well-Behaved Image Classification Models?

## Quick Facts
- **arXiv ID**: 2503.17110
- **Source URL**: https://arxiv.org/abs/2503.17110
- **Reference count**: 40
- **Primary result**: Evaluates 326 backbone models across nine quality dimensions beyond accuracy, revealing that self-supervised pre-training and larger datasets generally improve model quality, with vision-language models excelling in class balance and out-of-domain robustness.

## Executive Summary
This study addresses the critical need to evaluate deep neural networks (DNNs) beyond simple accuracy metrics. The authors analyze 326 backbone models across nine quality dimensions including accuracy, robustness, calibration, fairness, object focus, shape bias, and computational cost. Through comprehensive benchmarking, they reveal that self-supervised pre-training followed by fine-tuning improves most quality dimensions, vision-language models excel in class balance and out-of-domain robustness, and larger training datasets generally enhance model quality across all dimensions.

To facilitate practical model selection, the authors introduce the QUBA score, a weighted metric that ranks models across multiple quality dimensions simultaneously. The best-performing models identified include EVA02-B/14 and Hiera-B-Plus, both trained using semi- or self-supervised approaches. This work provides both empirical insights into how different training paradigms affect model behavior and a practical framework for evaluating and ranking models according to multiple quality criteria, moving beyond the traditional focus on accuracy alone.

## Method Summary
The authors conducted a comprehensive analysis of 326 backbone models across nine quality dimensions using standardized benchmarks. They evaluated models on accuracy, robustness to various perturbations, calibration (confidence reliability), fairness across classes and groups, object focus (attention to relevant image regions), shape bias, and computational cost. The study systematically compared different training approaches including supervised pre-training, self-supervised pre-training followed by fine-tuning, and vision-language model pre-training. Models were evaluated using established benchmark datasets and metrics for each quality dimension, with results aggregated to identify patterns and correlations across different training methodologies and model architectures.

## Key Results
- Self-supervised pre-training followed by fine-tuning improves most quality dimensions compared to supervised pre-training alone
- Vision-language models show superior performance in class balance and out-of-domain robustness
- Larger training datasets generally enhance model quality across all evaluated dimensions
- The QUBA score provides an effective framework for ranking models across multiple quality dimensions simultaneously

## Why This Works (Mechanism)
Assumption: Self-supervised pre-training enables models to learn more generalizable features by exposing them to diverse data augmentations and contrastive learning objectives, leading to improved performance across multiple quality dimensions when fine-tuned on downstream tasks. The QUBA score works by aggregating multiple quality dimensions through weighted averaging, allowing practitioners to balance competing objectives according to their specific application needs.

## Foundational Learning
- **Quality Dimensions**: Multiple evaluation criteria beyond accuracy (accuracy, robustness, calibration, fairness, object focus, shape bias, computational cost) are needed because real-world deployment requires reliable, fair, and efficient models
- **Training Paradigms**: Different pre-training approaches (supervised, self-supervised, vision-language) affect model behavior differently across quality dimensions
- **Model Scaling**: Larger training datasets generally improve model quality, but the relationship varies across different quality dimensions
- **Benchmarking Framework**: Standardized evaluation across multiple dimensions enables meaningful comparisons between models and training approaches
- **Weighted Scoring**: Multi-dimensional ranking systems like QUBA help balance trade-offs between different quality criteria
- **Domain Generalization**: Models that perform well across diverse quality dimensions tend to show better out-of-domain robustness

## Architecture Onboarding
**Component Map**: Data Pipeline -> Model Architectures -> Training Approaches -> Quality Dimensions -> QUBA Scoring
**Critical Path**: Pre-training Method → Model Architecture → Training Data Size → Quality Dimension Performance → Overall QUBA Score
**Design Tradeoffs**: Accuracy vs. robustness, computational efficiency vs. quality dimensions, supervised vs. self-supervised learning benefits
**Failure Signatures**: Models excelling in accuracy may underperform in robustness or fairness; computational constraints may limit quality dimension improvements
**First Experiments**: 1) Compare self-supervised vs supervised pre-training on a subset of quality dimensions, 2) Test QUBA score sensitivity to weight changes, 3) Validate findings on domain-specific datasets outside ImageNet

## Open Questions the Paper Calls Out
The paper does not explicitly identify open questions, but several areas remain unexplored: how the QUBA score would perform with different weight configurations for various application domains, whether the identified relationships between training approaches and quality dimensions generalize to non-ImageNet tasks, and what additional quality dimensions might be important for specific real-world applications.

## Limitations
- Analysis focuses on nine specific quality dimensions, potentially overlooking other important factors in real-world applications
- QUBA score introduces subjectivity through its weighting scheme, which may not generalize across different use cases
- Dataset composition and distribution of models analyzed could introduce biases in conclusions about optimal training approaches
- Focus on ImageNet-like benchmarks may limit generalizability to other domains or tasks

## Confidence
- **High confidence**: Self-supervised pre-training improves multiple quality dimensions; vision-language models show strong class balance and out-of-domain robustness
- **Medium confidence**: Larger training datasets generally enhance model quality; the QUBA score provides a practical ranking framework
- **Low confidence**: Specific model rankings may be sensitive to QUBA weight choices; generalizability to non-ImageNet tasks requires validation

## Next Checks
1. Test the QUBA score's robustness by applying different weight configurations and evaluating how model rankings change across different application scenarios
2. Validate the findings on domain-specific datasets (medical imaging, satellite imagery, etc.) to assess generalizability beyond standard benchmarks
3. Conduct ablation studies to quantify the individual contribution of each quality dimension to overall model performance in real-world deployment scenarios