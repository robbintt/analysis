---
ver: rpa2
title: 'Chronos: Learning Temporal Dynamics of Reasoning Chains for Test-Time Scaling'
arxiv_id: '2602.01208'
source_url: https://arxiv.org/abs/2602.01208
tags:
- chronos
- reasoning
- trajectory
- temporal
- trajectories
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Chronos is a lightweight, plug-and-play scorer that improves reasoning
  in large language models by modeling inference trajectories as time series. Unlike
  heuristic methods that treat reasoning steps equally, Chronos uses a multi-scale
  convolutional architecture to capture the temporal dynamics of token-level probabilities,
  assigning quality scores to each reasoning trajectory.
---

# Chronos: Learning Temporal Dynamics of Reasoning Chains for Test-Time Scaling

## Quick Facts
- **arXiv ID**: 2602.01208
- **Source URL**: https://arxiv.org/abs/2602.01208
- **Reference count**: 22
- **Primary result**: Chronos achieves 34.21% relative improvement over single-pass inference and 22.70% over majority voting on HMMT25 with Qwen3-4B-Thinking

## Executive Summary
Chronos introduces a lightweight, plug-and-play scorer that improves reasoning quality in large language models by modeling inference trajectories as time series. Unlike heuristic methods that treat reasoning steps equally, Chronos uses a multi-scale convolutional architecture to capture the temporal dynamics of token-level probabilities, assigning quality scores to each reasoning trajectory. This enables weighted majority voting that prioritizes high-quality reasoning paths. Evaluated across multiple models and benchmarks, Chronos achieves substantial gains while introducing negligible computational overhead.

## Method Summary
Chronos learns to predict reasoning trajectory quality by treating token-level probabilities as a chronological time series. At each decoding step, it extracts the negative mean log-probability of top-k candidate tokens, creating a temporal signal that captures confidence fluctuations. A multi-scale convolutional architecture with parallel kernels of varying lengths simultaneously detects local inconsistencies and global coherence patterns. The resulting quality scores are used for weighted majority voting, where top-η trajectories are retained and aggregated using scores as weights.

## Key Results
- 34.21% relative improvement over single-pass inference and 22.70% over majority voting on HMMT25 with Qwen3-4B-Thinking
- 13.76 absolute percentage points improvement over Maj@128 on DeepSeek-1.5B across AIME, HMMT, and GPQA-Diamond
- Outperforms Tail Confidence baseline with clearer separation between correct/incorrect trajectory score distributions

## Why This Works (Mechanism)

### Mechanism 1: Temporal Signal Extraction from Token Probabilities
Treating token-level probabilities as a chronological time series preserves sequential reasoning structure that uniform pooling destroys. At each decoding step t, Chronos extracts the negative mean log-probability of top-k candidate tokens, creating a temporal signal s = (s₁, s₂, ..., s_L). This sequence captures local confidence fluctuations—high values indicate peaked distributions (confidence), low values indicate uncertainty. The temporal evolution of model confidence during reasoning correlates with trajectory correctness.

### Mechanism 2: Multi-Scale Convolutional Pattern Recognition
Parallel convolutions with varying kernel lengths simultaneously capture local reasoning errors and global coherence patterns. Shorter kernels (e.g., 10-40 tokens) detect immediate inconsistencies in reasoning steps; longer kernels (e.g., 40-160 tokens) capture broader trends and long-term dependencies. Features are concatenated and processed through residual blocks with sigmoid output producing quality scores. Correct and incorrect reasoning trajectories exhibit distinguishable temporal patterns in their probability sequences.

### Mechanism 3: Score-Weighted Filtering Before Aggregation
Retaining only top-η trajectories by predicted score before weighted voting reduces contamination from low-quality reasoning paths. Trajectories are ranked by Chronos scores, top η·N_smp are retained (η=0.1 in experiments), then weighted majority voting aggregates final answers using scores as weights. High-scoring trajectories are more likely correct; filtering before voting is superior to weighting all samples.

## Foundational Learning

- **Temporal Convolutional Networks (TCNs)**
  - Why needed here: Chronos uses stacked multi-scale convolutional blocks with residual connections—core TCN concepts.
  - Quick check question: Given a 1D sequence of length 2048 with kernel sizes [20, 40, 80], how many layers are needed for the largest kernel to see the entire sequence?

- **Token-Level Probability Distributions in Autoregressive LLMs**
  - Why needed here: Understanding how logits → softmax → probability distributions work is essential for extracting temporal signals.
  - Quick check question: Why would top-k log-probability be preferred over single-token probability for confidence estimation?

- **Test-Time Scaling and Self-Consistency**
  - Why needed here: Chronos operates as a TTS method, replacing/augmenting majority voting with learned scoring.
  - Quick check question: What is the computational trade-off between generating N parallel trajectories vs. a single longer chain?

## Architecture Onboarding

- **Component map**: Input: Token probabilities (final L_tail=2048 tokens) → 1×1 Conv projection → N_proj channels → Multi-scale block × N_Blk → Global pooling → MLP → Sigmoid → Output: Scalar quality score

- **Critical path**: The kernel length selection and number of filters (N_Conv) directly determine what temporal patterns can be captured. Paper shows smaller models (1.5B) prefer shorter kernels for local consistency; larger models (4B+) benefit from longer kernels for long-range dependencies.

- **Design tradeoffs**: L_tail=2048 tokens focuses on reasoning tail vs. full trajectory—reduces compute but may miss early errors; N_proj=8 vs 16: lower dimensions work for smaller models; larger models need more expressive capacity; η=0.1 retention: aggressive filtering improves precision but risks discarding correct minority answers.

- **Failure signatures**: High overlap in score distributions between correct/incorrect trajectories (indicates model not learning discriminative patterns); performance degradation when applying scorer trained on one model to another (distributional shift); mode collapse in base LLM producing uniform probabilities.

- **First 3 experiments**:
  1. Train Chronos on AIME 2000-2023 split (8:1:1), evaluate AUC on held-out test set. Target: AUC > 0.80.
  2. Compare kernel configurations {10,20,40} vs {40,80,160} on DeepSeek-1.5B vs Qwen3-4B to confirm model-scale-dependent receptive field requirements.
  3. Train scorer on DeepSeek-1.5B trajectories, evaluate on Qwen3-4B trajectories (and vice versa). Expect performance drop but should still beat Maj@128 baseline.

## Open Questions the Paper Calls Out

- Can the temporal signal patterns learned from mathematical reasoning domains be effectively transferred to less structured domains such as creative writing, humanities, or open-ended conversational tasks?

- Can Chronos be adapted for closed-source "black-box" LLMs that do not expose token-level log-probabilities?

- How robust is Chronos when the underlying LLM exhibits mode collapse or poorly calibrated probability distributions?

## Limitations

- Generalizability across different LLM architectures and reasoning paradigms remains uncertain, particularly for non-mathematical domains
- Kernel size selection sensitivity and its relationship to problem characteristics vs. model scale needs further exploration
- The retention ratio η=0.1 represents a trade-off that may not be universally optimal across different problem distributions

## Confidence

- **High Confidence**: The architectural design of Chronos is sound and well-grounded in established time-series classification literature; performance improvements over baseline methods are statistically significant
- **Medium Confidence**: The claim that temporal dynamics of token probabilities contain discriminative signals for reasoning quality is supported by strong empirical evidence but lacks mechanistic validation
- **Low Confidence**: Cross-model generalization claims are based on limited experiments; the extent to which Chronos trained on one model family can effectively score trajectories from fundamentally different architectures remains uncertain

## Next Checks

1. **Domain Transfer Experiment**: Train Chronos on AIME trajectories and evaluate on a completely different reasoning domain (e.g., mathematical proof generation or code debugging). Measure both performance retention and identify which temporal patterns transfer versus domain-specific patterns.

2. **Temporal Signal Ablation**: Systematically remove different portions of the reasoning trajectory (beginning, middle, end) and retrain Chronos to determine which temporal segments contain the most discriminative information.

3. **Kernel Sensitivity Analysis**: Conduct a comprehensive grid search over kernel length combinations and receptive field depths across multiple model scales. Quantify the performance sensitivity to these architectural choices and develop guidelines for kernel selection based on problem characteristics rather than model scale alone.