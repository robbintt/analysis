---
ver: rpa2
title: General Uncertainty Estimation with Delta Variances
arxiv_id: '2502.14698'
source_url: https://arxiv.org/abs/2502.14698
tags:
- variance
- delta
- training
- neural
- epistemic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of efficiently estimating epistemic
  uncertainty for large neural networks. Delta Variances is a family of algorithms
  proposed for this purpose, requiring no changes to the network architecture or training
  procedure while incurring minimal computational overhead.
---

# General Uncertainty Estimation with Delta Variances

## Quick Facts
- arXiv ID: 2502.14698
- Source URL: https://arxiv.org/abs/2502.14698
- Authors: Simon Schmitt; John Shawe-Taylor; Hado van Hasselt
- Reference count: 23
- One-line primary result: Delta Variances is a family of algorithms for efficient epistemic uncertainty estimation requiring no architectural changes and minimal computational overhead, yielding competitive results on weather forecasting with lower cost than alternatives.

## Executive Summary
Delta Variances addresses the challenge of efficiently estimating epistemic uncertainty for large neural networks without requiring architectural changes or retraining. The method applies the Delta Method to propagate parameter uncertainty through a local linear approximation, computing uncertainty for any differentiable downstream quantity of interest using a pre-computed parameter covariance matrix. Empirical results on weather forecasting demonstrate competitive performance compared to popular alternatives while maintaining significantly lower computational overhead through diagonal Fisher approximations.

## Method Summary
The Delta Variance method estimates epistemic uncertainty by computing $\nabla_\theta u_\theta(z)^\top \Sigma \nabla_\theta u_\theta(z)$, where $\Sigma$ is a parameter covariance matrix (typically a diagonal approximation of the empirical Fisher) and $\nabla_\theta u_\theta(z)$ is the gradient of a quantity of interest with respect to model parameters. The method requires training a base model, computing the diagonal Fisher information matrix during training with EMA, and then using this covariance to estimate uncertainty for any downstream function by computing its gradient and applying the Delta Variance formula. The approach leverages the Bernstein-von Mises theorem or Laplace approximation for Gaussian posterior assumptions, with a frequentist derivation via the infinitesimal jackknife as an alternative.

## Key Results
- Delta Variances achieves competitive uncertainty estimation performance on weather forecasting tasks compared to ensembles and MC Dropout
- The method requires only 1 gradient computation versus K evaluations for other methods, demonstrating significant computational efficiency
- Using diagonal Fisher approximation maintains uncertainty quality while reducing computational overhead to element-wise operations

## Why This Works (Mechanism)

### Mechanism 1: Delta Variance Approximation of Epistemic Uncertainty
The Delta Variance formula, $\Delta^\top_u(z) \Sigma \Delta_u(z)$, provides a computationally efficient approximation of epistemic uncertainty by applying the statistical Delta Method to propagate parameter covariance through a local linear approximation of the function. The method assumes Gaussian posterior distributions over parameters and local linearity with respect to parameters over the region of uncertainty. This transforms uncertainty in the model's parameters into uncertainty in its output, with the gradient computed for the downstream quantity of interest while the covariance is computed from the primary model and reused.

### Mechanism 2: Gradient-Based Propagation to Derived Quantities
The method efficiently estimates uncertainty for any differentiable downstream function $u_\theta(z)$ that depends on the parameters of the base model $f_\theta$, without retraining. The parameter covariance matrix $\Sigma$, once computed for the base model, is treated as a fixed property that can be reused for various quantities of interest. This assumes the downstream function must be differentiable with respect to the model parameters and that the parameter covariance derived from the base model adequately captures uncertainty for any downstream function of those parameters.

### Mechanism 3: Computational Efficiency via Approximate Covariance
Delta Variances achieves low computational overhead by using efficient approximations of the covariance matrix, such as diagonal or Kronecker-Factored forms. The inference cost is dominated by a vector-matrix-vector product, and using a diagonal approximation makes this computation extremely fast through simple element-wise operations. This avoids the cost of multiple forward passes required by ensembles or MC Dropout, assuming that parameter correlations are less critical for the variance estimate than their individual magnitudes.

## Foundational Learning
- **Epistemic Uncertainty**: Uncertainty due to limited data or model capacity, distinct from aleatoric uncertainty from inherent noise. Needed to understand what type of uncertainty Delta Variances estimates. Quick check: Can be reduced with more data or better models.
- **Empirical Fisher Information Matrix**: An approximation of the Fisher information using the observed data, computed as the expected outer product of gradients. Needed as the covariance matrix $\Sigma$ that captures parameter uncertainty. Quick check: Diagonal approximation reduces to per-parameter variance estimates.
- **Laplace Approximation**: A Gaussian approximation to the posterior distribution centered at the maximum likelihood estimate. Needed to justify the Gaussian assumption for parameter posteriors. Quick check: Valid when posterior is unimodal and curvature is well-behaved.
- **Delta Method**: A statistical technique to approximate the variance of a function of a random variable using its gradient. Needed to propagate parameter uncertainty to output uncertainty. Quick check: Requires differentiability and local linearity.
- **Infinitesimal Jackknife**: A frequentist method to estimate parameter covariance from influence functions. Needed as an alternative derivation that relaxes Bayesian assumptions. Quick check: Relies on differentiability and smoothness of the estimator.
- **Vector-Matrix-Vector Product**: A computational primitive where a vector is multiplied by a matrix and then by another vector. Needed to efficiently compute the Delta Variance. Quick check: With diagonal $\Sigma$, reduces to element-wise operations.

## Architecture Onboarding

**Component Map**: Base Model $f_\theta$ -> Parameter Covariance $\Sigma$ -> Quantity of Interest $u_\theta(z)$ -> Delta Variance

**Critical Path**: Train base model with EMA on gradients → Compute diagonal Fisher $\Sigma$ → For test input, compute $\nabla_\theta u_\theta(z)$ → Compute $\nabla_\top \Sigma \nabla$ for uncertainty

**Design Tradeoffs**: The method trades potential accuracy (from ignoring parameter correlations via diagonal approximation) for significant computational efficiency. It also assumes Gaussian posteriors, which may not hold for all models.

**Failure Signatures**: Numerical instability when inverting the Fisher (eigenvalues near zero), underestimation of uncertainty when parameter correlations are important, poor calibration when posterior is non-Gaussian or function is highly non-linear.

**First Experiments**:
1. Implement diagonal Fisher computation with EMA during base model training
2. Compute Delta Variance for a simple linear regression and verify against analytical uncertainty
3. Compare Delta Variance uncertainty estimates against MC Dropout on a small classification task

## Open Questions the Paper Calls Out
- How do Delta Variances perform when utilizing SWAG-inspired covariance matrices compared to standard Fisher or Hessian approximations?
- Can the covariance matrix $\Sigma$ be learned effectively "from scratch" rather than via simple rescaling?
- Does the Sandwich estimator outperform the inverse Fisher in cases of model misspecification?
- Are Delta Variances effective acquisition functions for active learning loops?

## Limitations
- Relies heavily on Gaussian approximations for parameter posteriors and local linearity assumptions that may not hold for highly non-linear models
- Diagonal Fisher approximation ignores parameter correlations that could be important for accurate uncertainty estimates
- Limited empirical validation beyond the weather forecasting domain raises questions about generalization to other tasks

## Confidence
- **High Confidence**: Computational efficiency claims and basic mathematical formulation are well-supported
- **Medium Confidence**: Empirical results on weather forecasting show competitive performance but limited domain scope
- **Low Confidence**: Claims about performance on arbitrary quantities of interest lack systematic validation across diverse tasks

## Next Checks
1. Apply Delta Variances to a classification task (e.g., CIFAR-10) and compare against MC Dropout and ensembles on calibration metrics
2. Implement KFAC or full Fisher approximation for a subset of parameters to quantify the impact of ignoring parameter correlations
3. Visualize and analyze the parameter posterior distribution for the weather model to verify Gaussianity assumptions and identify potential multi-modality