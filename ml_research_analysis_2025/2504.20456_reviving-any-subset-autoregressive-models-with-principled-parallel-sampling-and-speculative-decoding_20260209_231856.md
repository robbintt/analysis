---
ver: rpa2
title: Reviving Any-Subset Autoregressive Models with Principled Parallel Sampling
  and Speculative Decoding
arxiv_id: '2504.20456'
source_url: https://arxiv.org/abs/2504.20456
tags:
- vegas
- tokens
- arxiv
- decoding
- should
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of parallel sampling from any-subset
  autoregressive models (AS-ARMs) while maintaining fidelity to the learned data distribution.
  The key insight is that AS-ARMs can generate tokens in any order and support parallel
  joint probability density estimation, enabling the Any-Subset Speculative Decoding
  (ASSD) algorithm.
---

# Reviving Any-Subset Autoregressive Models with Principled Parallel Sampling and Speculative Decoding

## Quick Facts
- arXiv ID: 2504.20456
- Source URL: https://arxiv.org/abs/2504.20456
- Authors: Gabe Guo; Stefano Ermon
- Reference count: 40
- Primary result: ASSD achieves state-of-the-art performance among sub-200M parameter models on infilling tasks and nearly matches much larger models on code generation

## Executive Summary
This paper addresses the challenge of parallel sampling from any-subset autoregressive models (AS-ARMs) while maintaining fidelity to the learned data distribution. The key insight is that AS-ARMs can generate tokens in any order and support parallel joint probability density estimation, enabling the Any-Subset Speculative Decoding (ASSD) algorithm. ASSD provably generates samples from the true joint distribution with at most N-m neural network calls. Empirically, ASSD speeds up language generation without sacrificing quality. The paper also introduces a mathematically justified training scheme for AS-ARMs and shows that appropriately trained AS-ARMs achieve state-of-the-art performance among sub-200M parameter models on infilling tasks and nearly match much larger models on code generation.

## Method Summary
The paper introduces Any-Subset Speculative Decoding (ASSD), which uses AS-ARMs to generate tokens in parallel while provably maintaining the correct joint distribution. The method leverages XLNet's two-stream self-attention architecture with custom attention masks to enable parallel drafting and verification in a single forward pass. During training, the model learns to predict tokens in arbitrary orders using a binary lattice decomposition to reduce the permutation space from N! to 2^N. The ASSD algorithm generates k speculative tokens in parallel, then verifies them using a causal-like mask to compute the exact joint probability, accepting or resampling as needed. This approach achieves speedups while maintaining output quality, with empirical results showing state-of-the-art performance on infilling tasks among sub-200M parameter models.

## Key Results
- ASSD provably generates samples from the true joint distribution with at most N-m neural network calls
- State-of-the-art performance among sub-200M parameter models on infilling tasks
- Nearly matches much larger models on code generation tasks
- Speedups achieved without sacrificing output quality

## Why This Works (Mechanism)

### Mechanism 1: Parallel Drafting via Conditional Independence
AS-ARMs can generate multiple candidate tokens in a single forward pass by using attention masks where query positions attend only to key/value positions and not to each other, bypassing sequential dependency bottlenecks.

### Mechanism 2: Unified Oracle and Draft Model
A single AS-ARM instance serves as both fast draft model and faithful oracle model by reusing the same network weights with different attention masks - first for parallel generation, then for exact joint probability computation.

### Mechanism 3: Guaranteed Acceptance via Modified Rejection Sampling
ASSD guarantees generation from the true joint distribution using rejection sampling that corrects distribution divergence, with proof that the first token is always accepted ensuring minimum speed of 1 token per function evaluation.

## Foundational Learning

**Permutation Language Modeling (XLNet Architecture)**: Understanding XLNet's two-stream self-attention is vital to grasp how attention masks work for arbitrary orders. Quick check: Can you explain why a standard decoder-only Transformer cannot evaluate the probability of a token at index 0 given a token at index 10 without masking the future?

**Speculative Decoding**: Understanding the draft-then-verify loop and rejection sampling formula is essential to see how ASSD modifies standard speculative decoding for arbitrary orders. Quick check: In standard speculative decoding, why is a separate "draft" model typically used, and what is the trade-off of using the same model?

**Joint vs. Independent Probability**: Understanding the divergence between parallel sampling's independence assumption and the true joint distribution is key to grasping why ASSD's verification step is necessary. Quick check: Why does parallel generation generally degrade output quality in discrete diffusion models compared to sequential generation?

## Architecture Onboarding

**Component map**: Input -> XLNet-110M backbone -> Dynamic attention masks -> Sampling loop (Draft -> Verify -> Accept/Resample)

**Critical path**: 1) Input sequence with masked span 2) Apply "Parallel Mask" to generate k tokens independently 3) Apply "Causal-Like Mask" to compute joint density of drafted tokens 4) Accept prefix or resample from the gap

**Design tradeoffs**: Mask decomposition reduces permutations from N! to 2^N but restricts verification order; larger k increases potential speedup but raises rejection risk; self-drafting is faster per iteration while N-grams are computationally cheaper but lower quality.

**Failure signatures**: Repetitive loops from improper finetuning (entropy check needed); inconsistent joint distributions from missing disambiguated ordering (compare log probabilities); slowdown from k<2 or incorrect masking (verify Lemma 1 holds).

**First 3 experiments**: 1) Run Algorithm 1 on WikiText validation set and compare generative perplexity and entropy against sequential baseline 2) Train models with "Any Permutation" vs "Recursive Binary Lattice" decomposition to verify entropy/perplexity improvements 3) Measure wall-clock time and NFEs for ROCStories infilling to confirm NFEs â‰¤ N-m

## Open Questions the Paper Calls Out

**Efficient arbitrary-order KV caching**: How can efficient arbitrary-order KV caching be implemented for AS-ARMs utilizing relative positional encodings? The authors note this is non-trivial due to relative positional encodings.

**Scaling laws**: Do the efficiency gains and performance of ASSD scale to models with billions of parameters? The authors explicitly list investigating scaling laws as a future extension.

**Optimized architectures**: Can custom architectures or attention optimizations like Flash Attention be adapted to further accelerate the ASSD algorithm? The authors identify this as a future direction.

## Limitations

- Training distribution shift: Heavy reliance on 90-99% masking rates with unclear transition from warm-up schedule
- Computational overhead of verification: Practical speedup depends on acceptance rates, which aren't reported
- Architecture dependence: Method crucially depends on XLNet's single-pass density estimation, limiting applicability to other AO-ARM architectures

## Confidence

**High Confidence**: Theoretical proof of correct joint distribution generation (Theorem 2); first token always accepted (Lemma 1); state-of-the-art performance among sub-200M parameter models

**Medium Confidence**: Speedup claims without wall-clock measurements; effectiveness of binary lattice decomposition; code generation performance matching larger models

**Low Confidence**: Generalization to non-XLNet architectures; performance on different masking distributions; scalability to larger model sizes

## Next Checks

1. **Acceptance Rate Analysis**: Run ASSD on WikiText validation set and measure distribution of resampling iterations per generation, verifying average acceptance rate remains above 50%

2. **Cross-Architecture Replication**: Implement ASSD using MAC-style AO-ARM and measure actual NFE count versus theoretical bound, comparing performance degradation against XLNet

3. **Domain Transfer Test**: Fine-tune 110M model on different corpus with same 90-99% masking schedule, evaluating whether output quality maintains without additional architecture-specific adjustments