---
ver: rpa2
title: Representation Understanding via Activation Maximization
arxiv_id: '2508.07281'
source_url: https://arxiv.org/abs/2508.07281
tags:
- layers
- feature
- visualization
- more
- activation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a unified framework for feature visualization
  via activation maximization (AM) applicable to both CNNs and Vision Transformers.
  The method extends AM to intermediate layers and operates in the frequency domain
  to produce more interpretable visualizations while suppressing adversarial artifacts.
---

# Representation Understanding via Activation Maximization

## Quick Facts
- arXiv ID: 2508.07281
- Source URL: https://arxiv.org/abs/2508.07281
- Authors: Hongbo Zhu; Angelo Cangelosi
- Reference count: 9
- Key outcome: A unified framework for feature visualization via activation maximization (AM) that extends to intermediate layers and operates in the frequency domain, producing more interpretable visualizations while suppressing adversarial artifacts.

## Executive Summary
This paper presents a unified framework for feature visualization via activation maximization (AM) applicable to both CNNs and Vision Transformers. The method extends AM to intermediate layers and operates in the frequency domain to produce more interpretable visualizations while suppressing adversarial artifacts. The approach is validated on MobileNet, InceptionV3, ResNet50V2, and ViT-B/16 architectures using ImageNet classes. Results show CNNs generate more localized and semantically interpretable features compared to ViTs, which produce more abstract representations. The method is also adapted to generate targeted adversarial examples, with perturbations requiring larger ℓ∞ bounds for ViT (ϵ=0.05) compared to ResNet50V2 (ϵ=0.01).

## Method Summary
The paper introduces a unified feature visualization framework using activation maximization that operates in the frequency domain rather than pixel space. The method optimizes a complex tensor in the spectral domain, transforms it via inverse FFT to obtain images, and applies random augmentations before computing activations. This approach is applied to both CNNs (MobileNet, InceptionV3, ResNet50V2) and Vision Transformers (ViT-B/16). For adversarial examples, the framework maximizes pre-softmax logits with Total Variation regularization, showing that ViTs require larger perturbation bounds (ϵ=0.05) than CNNs (ϵ=0.01) for successful attacks.

## Key Results
- Frequency-domain optimization produces more interpretable visualizations while suppressing high-frequency adversarial artifacts compared to pixel-space methods
- CNNs (MobileNet, InceptionV3, ResNet50V2) generate more localized, semantically interpretable features, while ViTs produce more abstract and diffuse visualizations
- Targeted adversarial examples generated via AM require larger ℓ∞ bounds for ViTs (ϵ=0.05) compared to ResNet50V2 (ϵ=0.01), indicating different robustness profiles

## Why This Works (Mechanism)

### Mechanism 1: Frequency-Domain Parameterization for Artifact Suppression
The input image is parameterized as the Inverse Fourier Transform of a complex tensor z. By optimizing z instead of pixel values, the optimization landscape is altered to encourage "smooth and coherent spectral structures," filtering out the noisy, uninterpretable patterns typical of pixel-space optimization. This inherently suppresses high-frequency artifacts by encouraging smooth and coherent spectral structures.

### Mechanism 2: Inductive Bias Mirroring in Visualization
CNNs, through convolution and pooling, enforce translation invariance and local connectivity, causing Activation Maximization to converge on localized textures and object parts. ViTs rely on global self-attention and patch embeddings, causing AM to synthesize distributed, abstract patterns lacking clear spatial boundaries. This reflects the distinct inductive biases (locality vs. global attention) of the architectures.

### Mechanism 3: Logit Maximization with Total Variation for Adversarial Generation
The method performs gradient ascent on the target class logit (pre-softmax) rather than the probability (post-softmax) to avoid gradient instability. It adds a TV penalty (λ · TV(x - x_orig)) to constrain the perturbation to be spatially smooth and thus more "imperceptible" than standard noise-based attacks.

## Foundational Learning

### Inverse Fourier Transform (in Deep Learning)
- **Why needed here:** The paper's core innovation moves optimization from pixel space to the frequency domain. You must understand how a complex spectrum maps back to a real-valued image to grasp why this reduces noise.
- **Quick check question:** Why does constraining the optimization in the frequency domain naturally suppress the high-frequency "static" often seen in deep dream images?

### Pre-softmax Logits vs. Softmax Probabilities
- **Why needed here:** The paper explicitly targets logits for both visualization and adversarial attacks.
- **Quick check question:** If you maximize the softmax probability of a class, why might the corresponding logit not be maximized, and why does this matter for gradient-based optimization?

### Inductive Bias (Translation Invariance vs. Attention)
- **Why needed here:** To interpret the results, you need to distinguish between features caused by the data versus features caused by the architecture.
- **Quick check question:** Why would a ViT visualization show global, dispersed patterns while a CNN shows a distinct eye or wheel in a specific location?

## Architecture Onboarding

**Component map:**
- Input: Complex Tensor z (Frequency Domain)
- Transform: Inverse FFT → Image x → Augmentations (Jitter/Scale)
- Forward Pass: Frozen pre-trained model (CNN or ViT)
- Objective: Activation of specific channel/neuron f^l_t(x)
- Backward Pass: Gradient w.r.t z (not weights)
- Update: Gradient Ascent on z

**Critical path:** The gradient flow from the frozen model's activation back through the augmentations and the Inverse FFT into the complex tensor z.

**Design tradeoffs:**
- Pixel vs. Frequency Domain: Pixel domain is easier to implement but yields noisy "adversarial" artifacts; Frequency domain requires complex number handling but produces semantic features.
- Model Size vs. Interpretability: Larger models (ViT-B/16, 86M params) produce more abstract, harder-to-interpret visualizations than smaller models (MobileNet, 4.2M params).

**Failure signatures:**
- High-frequency noise: Optimization has diverged or frequency constraints are too loose.
- ViT "Grid" artifacts: Distinct checkerboard patterns in perturbations, indicating the model's patch-based attention is sensitive to grid-aligned noise.
- Saturated colors: Regularization is too weak, allowing the optimizer to create "neural illusions" rather than natural features.

**First 3 experiments:**
1. **Pixel vs. Fourier Sanity Check:** Optimize a MobileNet neuron (e.g., "Ladybug") in pixel space vs. frequency space. Verify that the pixel version looks like static/noise and the Fourier version looks like spots.
2. **Layer Hierarchy Probe:** Visualize channels from `conv_pw_1` (low-level) vs. `conv_pw_12` (high-level) in MobileNet to confirm the progression from edges to object parts.
3. **Robustness Comparison:** Generate an adversarial example for ResNet50V2 (ϵ=0.01) vs. ViT (ϵ=0.05). Attempt to fool the ViT with the ResNet's smaller ϵ to empirically validate the paper's robustness claim.

## Open Questions the Paper Calls Out
- How do individual neurons or channels interact and combine to form complex representations?
- Can learned priors, generative models, or diffusion-based approaches improve AM convergence for large-scale models?
- Is the increased abstraction in ViT visualizations caused by model capacity or architectural differences?

## Limitations
- The paper lacks explicit specification of the learning rate for frequency-domain optimization and exact augmentation parameters, requiring reasonable assumptions.
- The reported "robustness" differences between architectures are demonstrated on a single attack scenario with limited perturbations.
- The claim that frequency-domain parameterization inherently suppresses adversarial artifacts assumes spectral smoothing is sufficient to remove non-semantic patterns without explicit high-frequency penalization.

## Confidence
- **High confidence:** The architectural explanation for different visualization patterns (CNNs show localized features, ViTs show abstract patterns) is well-supported by the mechanism of local vs. global processing.
- **Medium confidence:** The frequency-domain optimization genuinely produces more interpretable visualizations than pixel-space methods, though the specific noise suppression mechanism could benefit from spectral analysis.
- **Medium confidence:** The TV-regularized adversarial generation method works as described, but the robustness comparison between architectures is limited to one attack scenario and could be extended.

## Next Checks
1. **Spectral Analysis:** Compute and compare the Fourier spectra of visualizations generated in pixel space vs. frequency space to verify that frequency-domain optimization specifically suppresses high-frequency components.
2. **Cross-Architecture Attack Transfer:** Test whether adversarial examples generated for ResNet50V2 (with ϵ=0.01) can successfully transfer to fool ViT-B/16 to validate the claimed robustness difference.
3. **Layer-Specific Feature Hierarchy:** Systematically visualize intermediate layers across all architectures to verify the progression from low-level edges to high-level semantic features, ensuring the observed differences aren't merely optimization artifacts.