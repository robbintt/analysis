---
ver: rpa2
title: Mitigating Hallucinations in Large Vision-Language Models with Internal Fact-based
  Contrastive Decoding
arxiv_id: '2502.01056'
source_url: https://arxiv.org/abs/2502.01056
tags:
- ifcd
- decoding
- internal
- lvlms
- contrastive
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses object hallucinations in large vision-language
  models (LVLMs), where generated descriptions incorrectly describe objects not present
  in the visual input. The proposed Internal Fact-based Contrastive Decoding (IFCD)
  method edits the internal representations of LVLMs to amplify hallucinations, then
  uses contrastive decoding to subtract hallucinatory logits from the final output.
---

# Mitigating Hallucinations in Large Vision-Language Models with Internal Fact-based Contrastive Decoding

## Quick Facts
- arXiv ID: 2502.01056
- Source URL: https://arxiv.org/abs/2502.01056
- Authors: Chao Wang; Xuancheng Zhou; Weiwei Fu; Yang Zhou
- Reference count: 26
- Primary result: Up to 9% accuracy improvement on POPE benchmark, 8% improvement on MME object hallucination subset, and 5% reduction in hallucinated objects while maintaining text generation quality

## Executive Summary
This paper addresses object hallucinations in large vision-language models (LVLMs), where generated descriptions incorrectly describe objects not present in the visual input. The proposed Internal Fact-based Contrastive Decoding (IFCD) method edits the internal representations of LVLMs to amplify hallucinations, then uses contrastive decoding to subtract hallucinatory logits from the final output. IFCD achieves up to 9% accuracy improvement on the POPE benchmark, 8% improvement on the MME object hallucination subset, and reduces hallucinated objects by 5% while maintaining text generation quality.

## Method Summary
IFCD trains a TruthX autoencoder on 300 MSCOCO image-caption pairs to learn latent directions that separate truthful from untruthful representations. At inference, the method edits internal LVLM representations at 15 selected layers to amplify or suppress language bias, generates two distributions (P+ with anti-hallucination editing, P- with hallucination-inducing editing), and applies contrastive decoding: p_IFCD = σ((1+α)p+ - αp-). An adaptive plausibility constraint ensures only high-probability candidate tokens are considered, preventing over-correction of correctly identified objects.

## Key Results
- 9% accuracy improvement on POPE benchmark for hallucination detection
- 8% improvement on MME object hallucination subset
- 5% reduction in hallucinated objects while maintaining text generation quality

## Why This Works (Mechanism)

### Mechanism 1: Internal Representation Editing Amplifies Language Bias
Editing LVLM internal representations in a specific latent direction amplifies language priors over visual evidence, exposing hallucinatory tendencies. TruthX maps representations to a latent space where truthful and untruthful representations cluster separately. Editing along the untruthful direction (−δ) causes the model to disproportionately rely on language priors, while the opposite direction (+δ) suppresses this bias. Core assumption: Hallucinations stem partially from language bias overwhelming visual evidence.

### Mechanism 2: Contrastive Decoding Removes Hallucinatory Logits
Subtracting the hallucination-induced distribution from the anti-hallucination distribution removes tokens likely to be hallucinated. Generate two distributions via edited representations (P+ with anti-hallucination editing, P− with hallucination-inducing editing), then compute: pIFCD = σ((1+α)p+ − αp−). This penalizes tokens favored under hallucinatory conditions. Core assumption: Hallucinatory components in P− overlap with latent hallucination tendencies in the base model.

### Mechanism 3: Adaptive Plausibility Constraints Prevent Over-Correction
Constraining contrastive decoding to top candidate tokens prevents penalizing correctly identified objects. Define Vhead = {yt ∈ V : p(yt|∗) ≥ β max p([T]|∗)} and sample only from this set, ensuring low-probability tokens are not inadvertently boosted by subtraction. Core assumption: Hallucinatory tokens that matter appear in the candidate set with elevated probability.

## Foundational Learning

- **Internal Representation Editing in Transformers**: Why needed - IFCD modifies activations at specific layers to control truthfulness; understanding how representations flow through attention/FFN layers is essential. Quick check: How does editing an intermediate representation at layer L influence the final token distribution?

- **Contrastive Decoding**: Why needed - Core technique for subtracting logits from a "worse" distribution to improve outputs. Quick check: Given two distributions p+ and p−, how would you compute a contrastive distribution that favors tokens where p+ >> p−?

- **Language Bias vs. Visual Grounding in LVLMs**: Why needed - IFCD exploits the hypothesis that hallucinations arise when language priors override visual evidence. Quick check: Why might an LVLM describe a "blue banana" when the image shows a yellow one?

## Architecture Onboarding

- **Component map**: TruthX Autoencoder -> Internal Representation Editor -> Contrastive Decoder -> Adaptive Constraint Module

- **Critical path**: 1) Forward pass through LVLM → capture internal representations at target layers. 2) Encode via TruthX → edit in latent space → decode to modified representations. 3) Run two forward passes with +δ and −δ editing. 4) Compute contrastive distribution via weighted subtraction. 5) Apply adaptive constraint → sample final token.

- **Design tradeoffs**: Training data size: 300 samples optimal; more data caused performance decline. Editing strength: γ = 0.5 works well; γ ≥ 1.0 degrades performance. Number of edited layers: 15 layers; more layers increases CHAIRi. Contrast strength: α = 0.1 optimal; larger values risk instability.

- **Failure signatures**: Over-editing (γ ≥ 1.0) causes CHAIRi degradation. TruthX fails to learn meaningful δ; P+ ≈ P−. Editing insensitive layers has no effect; editing wrong layers may not target hallucination. Constraint misconfiguration: β too high → no effect; β too low → fluent outputs distorted.

- **First 3 experiments**: 1) Reproduce TruthX training: Train on 300 MSCOCO pairs; visualize latent separation via PCA. 2) Ablate editing direction: Compare learned δ vs. random directions; expect no hallucination amplification with random. 3) Sweep contrast strength: Test α ∈ {0.05, 0.1, 0.2, 0.5, 1.0} on POPE subset; expect peak at low α.

## Open Questions the Paper Calls Out
None

## Limitations
- TruthX training requires paired truthful/untruthful captions, but generation/validation methodology is unclear
- Empirically chosen hyperparameters (γ = 0.5, α = 0.1, β = 0.1) lack theoretical justification
- Reliance on editing specific layers assumes uniform interpretability across different LVLM architectures
- Performance may degrade with larger training sets due to potential overfitting

## Confidence

- **High Confidence**: Empirical results showing accuracy improvements on POPE (up to 9%) and MME object hallucination subset (up to 8%) are well-documented and reproducible
- **Medium Confidence**: Core mechanism of internal representation editing amplifying language bias is plausible based on related work, but specific claim that this bias can be isolated via TruthX requires further validation
- **Low Confidence**: Claim that adaptive plausibility constraint effectively prevents over-correction without empirical justification for β = 0.1 across different domains

## Next Checks

1. **Cross-Architecture Generalization**: Test IFCD on different LVLM architectures (e.g., LLaVA-NeXT, MiniGPT-4) to verify that the top 15 sensitive layers identified for LLaVA 1.5 and InstructBLIP are similarly critical across models.

2. **Threshold Sensitivity Analysis**: Systematically vary β ∈ {0.05, 0.1, 0.2, 0.5} across multiple evaluation sets to determine whether the adaptive constraint's performance is robust or highly sensitive to this hyperparameter.

3. **TruthX Training Data Quality Validation**: Conduct human evaluation on the 300 MSCOCO training pairs used for TruthX to verify that paired truthful/untruthful captions maintain semantic equivalence while differing in factual accuracy.