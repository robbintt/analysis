---
ver: rpa2
title: Enhancing Speech Emotion Recognition via Fine-Tuning Pre-Trained Models and
  Hyper-Parameter Optimisation
arxiv_id: '2510.07052'
source_url: https://arxiv.org/abs/2510.07052
tags:
- speech
- gp-bo
- search
- emotion
- accuracy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study presents a CPU-only speech emotion recognition (SER)
  pipeline that combines a pre-trained wav2vec2.0 encoder with automated hyperparameter
  optimisation (HPO). The approach fine-tunes the encoder on IEMOCAP and applies HPO
  using Gaussian Process Bayesian Optimisation (GP-BO) and Tree-structured Parzen
  Estimators (TPE) to optimise learning rate, training epochs, encoder unfreezing
  point, and input sequence length.
---

# Enhancing Speech Emotion Recognition via Fine-Tuning Pre-Trained Models and Hyper-Parameter Optimisation

## Quick Facts
- arXiv ID: 2510.07052
- Source URL: https://arxiv.org/abs/2510.07052
- Reference count: 0
- This study presents a CPU-only speech emotion recognition (SER) pipeline that combines a pre-trained wav2vec2.0 encoder with automated hyperparameter optimisation (HPO).

## Executive Summary
This study presents a CPU-only speech emotion recognition (SER) pipeline that combines a pre-trained wav2vec2.0 encoder with automated hyperparameter optimisation (HPO). The approach fine-tunes the encoder on IEMOCAP and applies HPO using Gaussian Process Bayesian Optimisation (GP-BO) and Tree-structured Parzen Estimators (TPE) to optimise learning rate, training epochs, encoder unfreezing point, and input sequence length. On the German EmoDB corpus, GP-BO achieved 96.0% balanced class accuracy in 11 minutes, while TPE attained 97.0% in 15 minutes. These results significantly outperform the best AutoSpeech 2020 baseline (85% in 30 minutes on GPU) and exceed native German listeners' performance (84%). Results demonstrate that efficient HPO with pre-trained encoders delivers competitive SER performance on commodity hardware.

## Method Summary
The method employs a SpeechBrain EncoderClassifier with a wav2vec2-base encoder pre-trained on IEMOCAP and fine-tuned on German EmoDB. A two-stage training process freezes the encoder initially, then unfreezes it at an HPO-controlled epoch to prevent overfitting on the small corpus. Hyperparameter optimisation uses 15-trial budgets with GP-BO (Ax) and TPE (Hyperopt/Optuna) to optimise learning rate, epochs, unfreeze timing, and input sequence length. The system achieves high balanced class accuracy (BCA) on EmoDB while demonstrating improved cross-lingual transfer to English corpora.

## Key Results
- GP-BO achieved 96.0% BCA in 11 minutes; TPE attained 97.0% BCA in 15 minutes on EmoDB
- Both methods significantly outperformed grid search (90% BCA in 1,680 minutes) and AutoSpeech 2020 baseline (85% in 30 minutes on GPU)
- HPO-tuned model improved cross-lingual zero-shot accuracy by 25% on CREMA-D and 26% on RAVDESS

## Why This Works (Mechanism)

### Mechanism 1: Pre-trained Representations Transfer via Acoustic Patterns
Wav2vec 2.0 representations, fine-tuned on IEMOCAP then adapted to EmoDB through HPO, capture cross-linguistically relevant acoustic features for emotion. Self-supervised pre-training on large-scale speech produces hierarchical acoustic features. Fine-tuning on IEMOCAP (English) creates emotion-relevant representations that transfer to EmoDB (German) through the classification head, without encoder exposure to German speech. The linear classifier learns to map pooled encoder outputs to emotion logits. This works because acoustic patterns for emotions share structure across languages (pitch, tempo, energy variations map similarly).

### Mechanism 2: Sequential Model-Based Optimization Concentrates Search on Promising Regions
GP-BO and TPE achieve high BCA with 15 trials (11-15 minutes) while grid search requires 143 trials (1,680 minutes). Both methods build surrogate models of the BCA landscape: GP-BO fits a Gaussian process posterior with uncertainty estimates, selecting parameters maximizing Expected Improvement; TPE estimates densities of good and bad configurations, sampling by maximizing the ratio of these densities. This focuses computation on promising regions rather than exhaustive coverage.

### Mechanism 3: Two-Stage Fine-Tuning Stabilizes Learning on Small Corpora
Freezing the encoder for initial epochs, then unfreezing at HPO-controlled epoch u∈{0,...,5}, prevents overfitting on EmoDB's ~428 training samples. The small corpus risks overfitting if ~95M encoder parameters update from scratch. Freezing forces the linear classifier to learn from fixed representations first. Unfreezing later allows task-specific adaptation while preserving pre-trained features.

## Foundational Learning

- **Concept: Self-supervised speech representations (wav2vec 2.0)**
  - Why needed here: The encoder's transfer ability underpins the entire approach; understanding how masked prediction of quantized speech units produces useful features explains generalization.
  - Quick check question: Why does masking ~50% of audio frames and predicting their quantized targets produce representations useful for emotion classification?

- **Concept: Bayesian optimization vs. Tree-structured Parzen Estimators**
  - Why needed here: The paper compares GP-BO and TPE; understanding their surrogate models clarifies when each excels under limited budgets.
  - Quick check question: Why does TPE model p(λ|y) while GP-BO models p(y|λ), and how does this affect exploration behavior?

- **Concept: Balanced Class Accuracy (BCA)**
  - Why needed here: BCA (not raw accuracy) is the HPO objective; averaging per-class recall prevents the optimizer from favoring majority classes.
  - Quick check question: If a 7-class dataset has 80% "neutral" samples and a model always predicts "neutral," what is its BCA?

## Architecture Onboarding

- **Component map:** Input (16kHz mono audio) -> Encoder (wav2vec2-base) -> Pooling (temporal aggregation) -> Classifier (linear layer) -> HPO engine (Ax/TPE) -> Objective (BCA)

- **Critical path:** 1. Define 4D search space (lr, epochs, unfreeze, maxlen) 2. HPO engine samples parameters 3. Train with frozen encoder for `unfreeze` epochs, then full fine-tuning 4. Evaluate BCA on validation; update surrogate 5. Repeat 15 trials; select best parameters by validation BCA

- **Design tradeoffs:** 15 trials sufficed for this 4D space; higher dimensions may require multi-fidelity methods. GP-BO (Ax): fastest to 0.96 BCA (11 min); TPE (Hyperopt): highest peak at 0.97 (15 min). CPU-only suffices here; larger corpora may benefit from GPU.

- **Failure signatures:** BCA stuck near 1/C (~0.14): model not learning—check learning rate or encoder initialization. Large train-val gap: overfitting—reduce epochs or delay unfreeze. HPO stagnating: surrogate stuck—try different acquisition function or larger initial design. Cross-corpus BCA near zero-shot baseline: hyperparameters overfit to source—consider domain adaptation.

- **First 3 experiments:** 1. Reproduce GP-BO baseline on EmoDB with 4D search space, 15 trials; verify BCA >0.95 in <15 min on 8 CPU cores 2. Ablate unfreeze epoch: fix other hyperparameters to optimal, vary unfreeze∈{0,1,2,3,4,5} 3. Test cross-lingual transfer: apply EmoDB-trained model to CREMA-D and RAVDESS zero-shot; confirm BCA improvement over baseline

## Open Questions the Paper Calls Out

- **Open Question 1:** Can the proposed CPU-only HPO workflow maintain its efficiency and accuracy advantages when extended to multilingual or low-resource language corpora?
- **Open Question 2:** How does integrating energy consumption and inference latency as optimization constraints affect the trade-off between accuracy (BCA) and training speed?
- **Open Question 3:** What specific implementation factors cause the significant performance discrepancy between the TPE implementations in Hyperopt and Optuna?

## Limitations

- Cross-lingual generalization claims rely on zero-shot transfer without encoder exposure to target languages, but results may not scale to languages with fundamentally different prosodic structures (e.g., tonal languages).
- The 15-trial HPO budget was sufficient for this 4D search space but may not generalize to larger architectures or search spaces.
- Hardware requirements (8 CPU cores, 32GB RAM) may limit reproducibility on standard machines, particularly for maxlen=160k sequences.

## Confidence

- **High Confidence**: CPU-only HPO achieving competitive BCA vs GPU baselines (96-97% BCA in 11-15 minutes)
- **Medium Confidence**: Two-stage fine-tuning mechanism preventing overfitting on small corpora (mechanistic explanation sound, but limited ablation studies)
- **Low Confidence**: Cross-lingual transfer generalization (only validated on English datasets; mechanism untested on non-Germanic or non-English languages)

## Next Checks

1. **Ablation study**: Fix optimal hyperparameters and vary `unfreeze` epoch {0,1,2,3,4,5} to confirm two-stage fine-tuning prevents overfitting
2. **Language diversity test**: Apply EmoDB-trained model to a tonal language dataset (e.g., Mandarin) to test cross-lingual generalization limits
3. **Scale-up validation**: Increase search space dimensionality and test if 15 trials remain sufficient or if multi-fidelity HPO becomes necessary