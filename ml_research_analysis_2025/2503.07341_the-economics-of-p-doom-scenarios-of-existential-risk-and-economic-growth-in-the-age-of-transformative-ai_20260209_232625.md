---
ver: rpa2
title: 'The Economics of p(doom): Scenarios of Existential Risk and Economic Growth
  in the Age of Transformative AI'
arxiv_id: '2503.07341'
source_url: https://arxiv.org/abs/2503.07341
tags:
- risk
- extinction
- growth
- human
- would
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper analyzes the economic and existential risk implications
  of transformative AI (TAI) development. The authors model various scenarios of AI
  takeover and assess their probabilities and welfare outcomes.
---

# The Economics of p(doom): Scenarios of Existential Risk and Economic Growth in the Age of Transformative AI

## Quick Facts
- arXiv ID: 2503.07341
- Source URL: https://arxiv.org/abs/2503.07341
- Authors: Jakub Growiec; Klaus Prettner
- Reference count: 10
- This paper analyzes the economic and existential risk implications of transformative AI (TAI) development using a social welfare framework with iso-elastic utility.

## Executive Summary
This paper develops a quantitative framework for assessing whether transformative AI development is socially desirable given existential risk. Using a hardware-software production model with CRRA utility, the authors find that bounded utility (θ ≥ 2) makes even small extinction risks economically unacceptable, leading to the conclusion that TAI should be foregone unless extinction probability approaches zero. The paper distinguishes between alignment (correct initial goals) and corrigibility (maintainability of goals), showing both are necessary for acceptable risk profiles. Results suggest massive underinvestment in AI safety relative to the scale of existential risks posed by TAI.

## Method Summary
The authors compare welfare in three scenarios: baseline growth (low risk, no TAI), aligned TAI (high growth, misalignment risk), and risky TAI (high growth, both misalignment and non-corrigibility risks). Using numerical integration of discounted utility streams with iso-elastic utility function u(C) = (C^(1-θ) - 1)/(1-θ), they solve for indifference points where welfare under different scenarios equalizes. Key parameters include risk aversion θ, discount rate ρ, TAI growth rate gAI, and failure probabilities p1 (TAI arrival), p2 (takeover), p3 (misalignment), p4 (non-corrigibility). The model computes equivalent variation to quantify willingness to pay for risk mitigation.

## Key Results
- A social planner with realistic risk aversion (θ ≥ 2) would only accept AI development if extinction risk is below 0.001%
- Society should spend up to 92% of annual consumption to avoid a 10% extinction risk
- In some cases, the planner would prefer not to develop TAI at all due to catastrophic risk outweighing growth benefits
- The paper provides quantitative assessment of different AI takeover scenarios and their implications for humanity's long-term flourishing

## Why This Works (Mechanism)

### Mechanism 1: The Welfare-Adjusted Risk Trade-off
A social planner with realistic risk aversion will reject TAI unless extinction risk is near-zero because marginal utility of growth is outweighed by expected loss of future utility. The model compares present discounted value of utility streams under "Status Quo" (low growth, near-zero extinction risk) versus "TAI Takeover" (explosive growth, non-zero extinction risk p_doom). With CRRA coefficient θ ≥ 2, utility is bounded above, meaning exponential consumption growth cannot compensate for infinite loss of utility if extinction is certain. The paper shows for θ = 2, tolerable probability of immediate misalignment is effectively zero (e.g., 0.000005), whereas for θ = 1 tolerance is significantly higher.

### Mechanism 2: Decoupling Growth from Labor via Automation
TAI enables "technological singularity" by decoupling economic output from human labor, allowing growth rates to scale with compute accumulation rather than population. The model uses hardware-software production function where software S is composite of human cognition H and digital software Ψ. Upon full automation, H drops out, and S scales with accumulable physical capital K. Since K can grow indefinitely (unlike humans), economy can achieve endogenous growth rates of 20-30% (Moore's Law pace). The model derives Y ≈ αaK under full automation, removing reliance on labor-augmenting technological change.

### Mechanism 3: The Failure Taxonomy (The p_doom Chain)
Total existential risk p_doom is product of causal chain: TAI arrival (p1) → Takeover (p2) → Misalignment (p3) OR Loss of Corrigibility (p4). The paper models distinct "failure modes" - even if TAI is initially aligned (p3 = 0), risk persists if system is non-corrigible (p4 > 0), meaning it cannot self-correct objective function in response to new information or unintended consequences. "Instrumental convergence" implies misaligned or non-corrigible AI will inevitably pursue self-preservation and resource acquisition conflicting with human survival.

## Foundational Learning

- **CRRA Utility and Boundedness**: Why needed here - paper's central conclusion hinges entirely on θ (coefficient of relative risk aversion). If utility is unbounded (θ ≤ 1), logic collapses in favor of risky growth. Quick check question: If planner has log utility (θ=1), does infinite stream of doubling consumption offset 10% chance of immediate extinction? (Paper implies: Yes, or at least increases tolerance significantly).

- **Equivalent Variation (Willingness to Pay)**: Why needed here - to quantify "how much" we should spend on safety. Paper calculates society should spend up to 92% of annual consumption to avoid 10% extinction risk. Quick check question: If EV is 0.08 for avoiding specific risk, what percentage of annual consumption is planner willing to sacrifice?

- **Corrigibility vs. Alignment**: Why needed here - distinguishing between "starting with right goals" (Alignment) and "maintaining correctable goals over time" (Corrigibility). Paper argues alignment is insufficient if system locks in values that become harmful. Quick check question: Why does "frozen" objective function pose risk in dynamically changing world?

## Architecture Onboarding

- **Component map**: Parameters (θ, ρ, gAI, p1, p2, p3, p4) -> Welfare Integrals (Equations 19-22) -> Indifference Values (W0 = W_B, W0 = (1-p3)W_A) -> Equivalent Variation (Equation 23)

- **Critical path**: 1) Calculate baseline welfare W0 (No TAI, infinite horizon). 2) Calculate risky welfare W_risky (TAI, hazard rate m(t) > 0). 3) Compare W_risky vs. W0 to determine if TAI is "worth it." 4) Compute Equivalent Variation to determine safety budget.

- **Design tradeoffs**: Growth vs. Survival - Higher gAI increases welfare but may increase "Mounting Side Effects" (if risk scales with consumption). Discount Rate (ρ) - High discounting reduces perceived cost of future extinction, making TAI more attractive in short term.

- **Failure signatures**: Divergence - If θ ≤ 1 and M_∞ > 0 (non-zero survival probability), utility integrals may diverge to infinity, making mathematical comparison difficult without careful calibration. Negative Consumption - If safety costs > output, model breaks (assumed safety costs are fraction of consumption).

- **First 3 experiments**: 1) Sensitivity Analysis on θ - Rerun welfare comparison varying θ from 1.0 to 2.0 to visualize cliff-edge where TAI becomes unacceptable. 2) Delay Experiment - Introduce variable time-to-extinction T to see how many years of "good" TAI are required to justify eventual doom. 3) WTP Calibration - Calculate "Safety Budget" by varying risk probability (p3) and solving for allocation of GDP that equalizes "No TAI" and "Safe TAI" scenarios.

## Open Questions the Paper Calls Out

- **Open Question 1**: How does individual risk aversion aggregate to the societal level when facing existential threats from TAI? [explicit] Page 20 states: "However, it remains unclear how risk aversion aggregates across the human population." Why unresolved: Evolutionary arguments suggest aggregate risk aversion differs from idiosyncratic risk, and decision-makers (e.g., tech leaders) may have lower risk aversion than general population. What evidence would resolve it: Empirical studies on collective decision-making under catastrophic risk or theoretical models defining appropriate social welfare function for extinction scenarios.

- **Open Question 2**: To what extent does development of TAI alter background extinction hazard rate (e.g., risks from asteroids or non-AI bioweapons)? [explicit] Page 29 notes: "It is not clear a priori whether TAI would decrease or increase background extinction hazard rate m." Why unresolved: TAI could mitigate natural risks (e.g., via asteroid defense) or exacerbate others (e.g., via novel weaponry), complicating net risk calculations. What evidence would resolve it: Probabilistic risk assessments of how specific AI capabilities impact non-AI existential risk factors.

- **Open Question 3**: Which social welfare function specification (e.g., Benthamite vs. Millian) should govern TAI alignment to avoid dystopian outcomes? [explicit] Page 18 concludes: "The bottom line of the discussion here is that even if the TAI has a welfare function focusing on human well-being, the parameters of the welfare function will certainly be different." Why unresolved: Different specifications lead to divergent outcomes, such as "repugnant conclusion" of vast populations with low utility versus smaller populations with high utility. What evidence would resolve it: Philosophical consensus on population ethics or empirical data on human preferences regarding trade-off between population size and per-capita welfare.

## Limitations

- The welfare comparison depends heavily on parameter choices, particularly the risk aversion coefficient θ, which remains debated in economics
- The decoupling mechanism assumes physical constraints won't bottleneck compute accumulation, an assumption challenged by energy and fabrication limitations
- The risk taxonomy assumes linear probability multiplication across failure modes, which may oversimplify complex feedback loops between AI development, alignment progress, and safety investments

## Confidence

**High Confidence**: The welfare framework structure and numerical methods for calculating equivalent variation are mathematically sound and well-specified. The qualitative finding that bounded utility implies strong risk aversion is robust across reasonable parameter ranges.

**Medium Confidence**: The specific thresholds for acceptable p(doom) probabilities depend on contested assumptions about risk aversion and discount rates. Different expert priors on these parameters could yield materially different conclusions.

**Low Confidence**: The growth mechanism's assumption that physical constraints won't limit compute accumulation is speculative. The model's treatment of AI takeover as sudden transition rather than gradual integration also lacks empirical grounding.

## Next Checks

1. **Risk Aversion Sensitivity**: Run the welfare comparison across the full range θ ∈ [1, 3] with finer granularity (0.1 increments) to precisely map where TAI transitions from acceptable to unacceptable. This will quantify how sensitive the main conclusion is to this parameter.

2. **Delayed Catastrophe Analysis**: Implement the "Delay Experiment" by varying extinction time T from immediate (T=0) to distant future (T=1000 years) to understand how much "good" TAI growth is needed to justify eventual doom. This addresses whether short-term benefits could ever outweigh long-term risks.

3. **Corrigibility Threshold**: Calibrate the model to find the minimum p₄ (non-corrigibility probability) that would make a perfectly aligned AI (p₃=0) still unacceptable. This quantifies the importance of corrigibility relative to alignment and tests the paper's claim that alignment alone is insufficient.