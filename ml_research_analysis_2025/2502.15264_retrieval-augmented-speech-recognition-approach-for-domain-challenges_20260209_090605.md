---
ver: rpa2
title: Retrieval-Augmented Speech Recognition Approach for Domain Challenges
arxiv_id: '2502.15264'
source_url: https://arxiv.org/abs/2502.15264
tags:
- speech
- recognition
- data
- domain-specific
- domain
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a retrieval-augmented speech recognition
  system to address domain mismatch challenges in ASR. The proposed method leverages
  large language models (LLMs) and retrieval-augmented generation (RAG) techniques
  to dynamically incorporate domain-specific textual content during inference without
  requiring such data during training.
---

# Retrieval-Augmented Speech Recognition Approach for Domain Challenges

## Quick Facts
- arXiv ID: 2502.15264
- Source URL: https://arxiv.org/abs/2502.15264
- Reference count: 28
- Introduces retrieval-augmented speech recognition system that achieves state-of-the-art results on CSJ dataset

## Executive Summary
This paper introduces a retrieval-augmented speech recognition system that addresses domain mismatch challenges by dynamically incorporating domain-specific textual content during inference without requiring such data during training. The proposed method leverages large language models (LLMs) and retrieval-augmented generation (RAG) techniques to enhance recognition accuracy. Experiments on the CSJ dataset demonstrate significant improvements, achieving 19.6% relative improvement in out-of-domain performance and 8.7% improvement in in-domain performance compared to baseline systems.

## Method Summary
The system employs a two-stage training process: first optimizing the audio encoder while freezing the LLM decoder, then optimizing the decoder with instruction prompts while freezing the encoder. During inference, a first-pass recognition generates a query that retrieves domain-specific context chunks from a vector database. This retrieved context, combined with instruction prompts, is fed into the LLM decoder to produce the final transcription. The approach uses whisper-large-v2 for audio encoding and ELYZA-Japanese-Llama-2-7b for decoding, with multilingual-e5-large embeddings for retrieval.

## Key Results
- Achieves 3.73% CER on Test-OoD (19.6% relative improvement over baseline)
- Achieves 4.21% CER on Eval1-InD (8.7% improvement over baseline)
- Demonstrates significant performance gains through ablation studies showing the importance of retrieved context versus random content

## Why This Works (Mechanism)

### Mechanism 1: Retrieval-Guided Context Injection
Injecting retrieved domain-specific text into the LLM decoder during inference mitigates domain mismatch without requiring retraining on private data. The system performs first-pass recognition to query a vector database, retrieves relevant text chunks, and formats them as instruction prompts to provide explicit linguistic context to the LLM decoder.

### Mechanism 2: Instruction-Following Decoder Optimization
Decoupling acoustic encoding from linguistic reasoning allows the LLM to specialize in utilizing text context via instruction tuning. A two-stage training process freezes the LLM decoder while training the audio encoder, then freezes the encoder while training the decoder with instruction prompts, forcing the decoder to rely on prompt content rather than acoustic shortcuts.

### Mechanism 3: In-Domain Vector Search via Semantic Chunking
Fine-grained semantic chunking of domain documents provides more precise context than whole-document retrieval. Documents are split into 512-token chunks with 50-token overlap, using cosine similarity on embeddings to match queries against specific chunks, ensuring topically relevant context for each utterance.

## Foundational Learning

- **Concept: Retrieval-Augmented Generation (RAG)**
  - Why needed here: Core architectural pillar bridging static models and dynamic domain knowledge
  - Quick check question: How does the system handle a query that has no close match in the vector database?

- **Concept: Encoder-Decoder Architecture with LLMs**
  - Why needed here: System separates audio processing (Encoder) from text generation (LLM Decoder)
  - Quick check question: In Stage 1 training, why is the LLM decoder frozen while the audio encoder is trained?

- **Concept: Domain Mismatch & Distribution Shift**
  - Why needed here: Problem statement hinges on model failing when test data differs from training data
  - Quick check question: Why does the paper exclude "speech recognition" talks from the training set when creating the Test-OoD set?

## Architecture Onboarding

- **Component map**: Audio Input -> Audio Encoder -> Audio Embeddings -> First Pass Recognition -> Text Query -> Retriever -> Domain Context -> (Audio Embeddings + Instruction + Domain Context) -> LLM Decoder -> Final Transcription

- **Critical path**: Audio Input → Audio Encoder → Greedy Decoding (First Pass) → Text Query → Retriever → Domain Context → (Audio Embeddings + Instruction + Domain Context) → LLM Decoder → Final Transcription

- **Design tradeoffs**:
  - Latency vs. Accuracy: Two-stage inference roughly doubles time compared to standard ASR
  - Chunk Size: 512 tokens chosen for balance; smaller chunks retrieve faster but may miss context

- **Failure signatures**:
  - Hallucination: If retrieval fails, LLM might generate fluent but acoustically incorrect text
  - Context Ignorance: If Stage 2 training insufficient, model defaults to internal LM ignoring retrieved prompt

- **First 3 experiments**:
  1. Sanity Check: Run audio encoder alone (Stage 1) vs full RAG system to isolate retrieval gain
  2. Retrieval Ablation: Replace retrieved chunks with random chunks to confirm model reads context
  3. Query Length Sensitivity: Test "first 30 chars" vs "full first-pass result" as retrieval query

## Open Questions the Paper Calls Out

### Open Question 1
Does training the retrieval mechanism using simulated noisy ASR hypotheses (rather than ground truth transcriptions) improve robustness against retrieval errors during inference? The paper notes retrieval is performed using "transcriptions" during training but "full or partial recognized results" during inference, without exploring this mismatch.

### Open Question 2
How does the system's performance degrade when first-pass recognition results are too erroneous to retrieve relevant domain-specific documents? The paper shows "Full" recognition results approach "True Label" performance but doesn't analyze failure cases where initial queries are inaccurate.

### Open Question 3
What is the impact of retrieval hyperparameters (chunk size and top-k) on model's effective context window and accuracy? The authors used specific configuration (chunk size 512, overlap 50, top-k=2) without ablation studies on these settings.

## Limitations
- Exact architecture of how audio embeddings interface with LLM decoder is not fully specified
- Performance heavily depends on quality of first-pass recognition for retrieval queries
- Computational overhead of two-stage inference may limit practical deployment in latency-sensitive applications

## Confidence

**High Confidence**: General framework of using RAG for ASR domain adaptation is well-supported with clear evidence from ablation studies showing importance of retrieved context versus random content.

**Medium Confidence**: Specific CER improvements are based on CSJ dataset which may not generalize to other languages or domain mismatch scenarios.

**Low Confidence**: Exact prompt template format and how instruction tokens, domain tokens, and audio embeddings are ordered/combined in decoder input remains unclear from paper.

## Next Checks

1. **Retrieval Quality Validation**: Compare retrieval quality using oracle transcriptions versus ASR output for retrieval query to quantify impact of first-pass recognition errors.

2. **Chunk Size Sensitivity Analysis**: Systematically vary chunk sizes (256, 512, 1024 tokens) and overlaps to determine optimal configuration for different domain types.

3. **Latency Performance Measurement**: Benchmark complete inference pipeline to quantify computational overhead compared to standard ASR models, including memory and time requirements.