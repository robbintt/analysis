---
ver: rpa2
title: Unsupervised Anomaly Detection in Multi-Agent Trajectory Prediction via Transformer-Based
  Models
arxiv_id: '2601.20367'
source_url: https://arxiv.org/abs/2601.20367
tags:
- anomaly
- detection
- prediction
- lateral
- trajectory
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes an unsupervised anomaly detection framework
  for multi-agent trajectory prediction using a Transformer-based model. The method
  addresses the challenge of identifying safety-critical scenarios in autonomous driving,
  which are rare and difficult to label.
---

# Unsupervised Anomaly Detection in Multi-Agent Trajectory Prediction via Transformer-Based Models

## Quick Facts
- arXiv ID: 2601.20367
- Source URL: https://arxiv.org/abs/2601.20367
- Reference count: 32
- Key outcome: Introduces a framework using Transformer residuals to detect multi-agent trajectory anomalies, outperforming rule-based metrics on NGSIM.

## Executive Summary
This paper introduces an unsupervised anomaly detection framework for multi-agent trajectory prediction using a Transformer-based model. The method addresses the challenge of identifying safety-critical scenarios in autonomous driving, which are rare and difficult to label. The core idea is to model normal driving behavior with a multi-agent Transformer and measure deviations through prediction residuals. A dual evaluation scheme is introduced to assess both detection stability (using Kendall rank correlation and Jaccard index) and physical alignment (through correlations with surrogate safety measures). Experiments on the NGSIM dataset demonstrate the framework's effectiveness, with the maximum residual aggregator achieving the highest physical alignment while maintaining stability. The method identifies 388 unique anomalies missed by traditional Time-to-Collision and statistical baselines, capturing subtle multi-agent risks.

## Method Summary
The framework uses a multi-agent Transformer to predict future trajectories based on historical multi-agent states. The model employs an encoder-decoder architecture with self-attention for historical states and cross-attention for interaction modeling. Prediction residuals are computed as weighted combinations of position and velocity errors, then aggregated per scene using maximum, mean, or quantile methods. These aggregated residuals serve as anomaly scores, which are fed to an Isolation Forest to identify safety-critical scenes. The method is evaluated on the NGSIM US101 dataset, with performance assessed through both ranking stability (Kendall's τ, Jaccard@K) and physical alignment with surrogate safety measures (Spearman's ρ vs. TTC, DRAC, and other SSMs).

## Key Results
- Maximum residual aggregation achieves highest physical alignment with safety proxies while maintaining ranking stability
- Framework identifies 388 unique anomalies missed by traditional TTC and statistical baselines
- Max aggregator shows highest Spearman correlation with harsh closing ratio (0.312) and strong negative correlation with min TTC (-0.235)
- Multi-agent interaction modeling via attention captures complex risks that pairwise rule-based metrics cannot

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Prediction residuals from a trajectory model trained on normal driving data serve as a proxy for behavioral anomaly intensity.
- **Mechanism:** The Transformer learns to forecast positions and velocities conditioned on historical multi-agent states. When actual behavior deviates from learned normal patterns (e.g., reactive braking, abrupt lane changes), the forecast error spikes. These residuals are aggregated per-scene and scored via Isolation Forest.
- **Core assumption:** Normal driving patterns are statistically learnable and sufficiently represented in training data; anomalies correspond to high-residual regions.
- **Evidence anchors:** [abstract]: "The core idea is to model normal driving behavior with a multi-agent Transformer and measure deviations through prediction residuals." [section III.B]: "A scalar residual is defined as a weighted combination of spatial and kinematic errors: ea,t = α_pos e_pos + α_vel e_vel."

### Mechanism 2
- **Claim:** Multi-agent Transformer attention enables scene-level interaction modeling that pairwise rule-based metrics cannot capture.
- **Mechanism:** The encoder uses self-attention over all agents' historical states; the decoder uses cross-attention to inject interaction context. This produces prediction errors that reflect complex inter-agent dependencies (e.g., reactive braking under lateral drift) rather than isolated vehicle dynamics.
- **Core assumption:** Safety-critical events often emerge from multi-agent interactions, not just individual kinematics.
- **Evidence anchors:** [abstract]: "Traditional rule-based metrics like Time-to-Collision are too simplistic to capture complex interaction risks." [section I]: "Most rely on rule-based surrogate measures or single-agent prediction models, making them unable to capture the multi-agent interactions that produce many safety-critical events."

### Mechanism 3
- **Claim:** Maximum residual aggregation yields the strongest physical alignment with safety proxies while preserving ranking stability.
- **Mechanism:** Safety-relevant deviations often manifest as brief, high-magnitude spikes (e.g., sudden braking) rather than sustained average errors. Taking the maximum residual per scene prioritizes these peaks, aligning anomaly scores with surrogate safety measures like TTC and harsh closing ratio.
- **Core assumption:** Physical risk correlates more strongly with worst-case deviations than with averaged behavior.
- **Evidence anchors:** [section V]: "The superior performance of the max aggregation suggests that many safety-relevant events manifest as short, high-magnitude deviation spikes rather than sustained average deviations." [Table III]: Max aggregator shows highest Spearman correlation with harsh closing ratio (0.312).

## Foundational Learning

- **Concept: Transformer encoder-decoder with cross-attention**
  - **Why needed here:** The model uses self-attention to encode historical multi-agent states and cross-attention to condition future predictions on interaction context. Without understanding attention mechanics, debugging prediction quality is difficult.
  - **Quick check question:** Given a 25-timestep history for 7 agents, how does the decoder attend to encoder outputs when predicting agent 3's future position?

- **Concept: Isolation Forest anomaly scoring**
  - **Why needed here:** Scene-level residuals are fed to Isolation Forest, which isolates anomalies via random splits. The contamination parameter directly controls detection thresholds.
  - **Quick check question:** If contamination is set to 0.15, what proportion of scenes will be flagged as anomalous, and how does this affect the decision boundary?

- **Concept: Surrogate Safety Measures (SSMs)**
  - **Why needed here:** Physical alignment is evaluated via correlation with TTC, DRAC, and related metrics. Understanding these proxies is essential to interpret whether statistical anomalies reflect real danger.
  - **Quick check question:** Why might a scene with no TTC violation still be flagged as anomalous by the residual-based method?

## Architecture Onboarding

- **Component map:** Input preprocessing -> Linear projection -> 2 Transformer encoder layers -> Z_enc -> Decoder with cross-attention -> Prediction (Δx, Δy, v) -> Residual computation -> Aggregation (max/mean/q95/top-k) -> Isolation Forest -> Anomaly ranking

- **Critical path:** Prediction accuracy → residual distribution → aggregation choice → Isolation Forest threshold → anomaly ranking. Errors in early stages propagate downstream.

- **Design tradeoffs:**
  - Max aggregation: Best physical alignment, sensitive to noise spikes
  - Mean aggregation: More stable but dilutes brief high-risk events
  - Contamination parameter: Higher values increase recall but may reduce precision
  - History/prediction horizon: Longer history improves context; longer prediction increases uncertainty

- **Failure signatures:**
  - Low Kendall τ across contamination levels → unstable anomaly ranking
  - Near-zero Spearman ρ with SSMs → residuals not reflecting physical risk
  - Clusters dominated by one pattern → residual features may lack discriminative power
  - ADE/FDE significantly higher than baseline → prediction model underfitting

- **First 3 experiments:**
  1. Replicate prediction performance: Train Transformer on NGSIM subset, verify ADE < 1.5m and FDE < 2.0m; compare against LSTM baseline.
  2. Aggregation ablation: Compute scene-level scores using max, mean, q95, top-k; report Kendall τ and Jaccard@K at contamination levels 0.10, 0.15, 0.20.
  3. Physical alignment check: For the max aggregator, compute Spearman ρ between anomaly scores and at least two SSMs (min TTC, harsh closing ratio); verify ρ > 0.2 magnitude.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Does the framework maintain stability and physical alignment when applied to complex urban datasets with diverse agent types?
- **Basis in paper:** [explicit] The authors state, "The current evaluation focuses on the NGSIM dataset... Future work will extend the framework to more diverse datasets (e.g., Waymo)."
- **Why unresolved:** The current study relies solely on the NGSIM US-101 highway dataset. It is unclear if the "maximum residual" aggregator remains the most physically aligned metric in dense urban environments featuring pedestrians, cyclists, and complex intersections.
- **Evidence to resolve:** Benchmarking the Kendall rank correlation and Jaccard index on urban datasets (e.g., Waymo Open Dataset or NuScenes) against the current highway baseline.

### Open Question 2
- **Question:** To what extent does improved trajectory prediction accuracy directly translate to higher anomaly detection precision?
- **Basis in paper:** [explicit] The authors propose to "integrate stronger prediction models for sharper residuals" as a specific direction for future work.
- **Why unresolved:** While better prediction models yield lower displacement errors, it is not guaranteed that lower errors produce "sharper" residuals that effectively separate safety-critical anomalies from noise. The relationship between prediction fidelity and detection granularity remains unquantified.
- **Evidence to resolve:** A comparative analysis swapping the current Transformer for a state-of-the-art predictor (e.g., MultiPath++) and measuring the resulting separation margin in the Isolation Forest scoring.

### Open Question 3
- **Question:** Can the detection pipeline be adapted for real-time, online deployment without losing stability?
- **Basis in paper:** [explicit] The paper notes the current evaluation is in an "offline setting" and suggests future work should "explore online deployment" and "investigate real-time adaptation."
- **Why unresolved:** The current pipeline relies on post-hoc aggregation and Isolation Forest scoring over a full dataset distribution. It is unresolved whether the ranking stability (Kendall's τ) holds when the system must score anomalies incrementally in a streaming environment.
- **Evidence to resolve:** Implementation of a sliding-window online version of the Isolation Forest, measuring latency and drift in anomaly rankings compared to the offline ground truth.

## Limitations
- Prediction error assumptions: Relies on assumption that normal driving behavior is statistically learnable, yet no validation that anomalies are absent from training data
- Dataset specificity: Evaluation conducted only on NGSIM US101, a single freeway dataset with limited variability in agent count and driving culture
- SSM correlation interpretation: Physical alignment via Spearman correlation with surrogate safety measures is indirect evidence of real-world safety relevance

## Confidence
- **High confidence:** Transformer-based residual computation, multi-agent interaction modeling via attention, prediction performance metrics (ADE/FDE)
- **Medium confidence:** Physical alignment with surrogate safety measures, anomaly detection stability metrics (Kendall τ, Jaccard@K)
- **Low confidence:** Generalization to other driving environments, robustness to noise in residual computation, and real-world safety impact

## Next Checks
1. Cross-dataset validation: Evaluate the same model on a different driving dataset (e.g., HighD or Argoverse) to assess generalization of anomaly detection performance and physical alignment
2. Ablation on aggregation methods: Systematically compare max, mean, q95, and top-k aggregation under varying noise levels to quantify robustness and sensitivity to outlier spikes
3. Ground-truth safety correlation: If possible, correlate detected anomalies with actual safety events or near-misses in the data to validate whether statistical anomalies map to real-world risks