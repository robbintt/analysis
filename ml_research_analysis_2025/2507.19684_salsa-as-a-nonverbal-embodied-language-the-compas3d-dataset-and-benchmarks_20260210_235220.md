---
ver: rpa2
title: Salsa as a Nonverbal Embodied Language -- The CoMPAS3D Dataset and Benchmarks
arxiv_id: '2507.19684'
source_url: https://arxiv.org/abs/2507.19684
tags:
- motion
- dance
- salsa
- move
- follower
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CoMPAS3D, a large-scale motion capture dataset
  of improvised salsa dancing designed as a challenging testbed for interactive, expressive
  humanoid AI. The dataset includes 3 hours of leader-follower salsa dances performed
  by 18 dancers spanning beginner, intermediate, and professional skill levels, with
  fine-grained annotations covering over 2,800 move segments, including move types,
  combinations, execution errors, and stylistic elements.
---

# Salsa as a Nonverbal Embodied Language -- The CoMPAS3D Dataset and Benchmarks

## Quick Facts
- **arXiv ID:** 2507.19684
- **Source URL:** https://arxiv.org/abs/2507.19684
- **Reference count:** 40
- **Primary result:** Introduces CoMPAS3D dataset and SalsaAgent model for interactive salsa dance generation with improved synchronization and motion quality

## Executive Summary
This paper introduces CoMPAS3D, a large-scale motion capture dataset of improvised salsa dancing designed as a challenging testbed for interactive, expressive humanoid AI. The dataset includes 3 hours of leader-follower salsa dances performed by 18 dancers spanning beginner, intermediate, and professional skill levels, with fine-grained annotations covering over 2,800 move segments, including move types, combinations, execution errors, and stylistic elements. By framing salsa as an embodied language, the paper evaluates CoMPAS3D on two benchmark tasks: solo dance generation and duet dance generation. A unified multitask SalsaAgent model is introduced, capable of performing both tasks. Experimental results show that SalsaAgent outperforms baselines in key metrics such as Fréchet Inception Distance and Beat Echo Degree, demonstrating improved synchronization and motion quality. The dataset, annotations, and code are publicly released to encourage research in socially interactive embodied AI and creative humanoid motion generation.

## Method Summary
The method introduces CoMPAS3D, a motion capture dataset containing 3 hours of improvised salsa dancing with 18 dancers across 9 leader-follower pairs and 3 proficiency levels. The dataset features 55-joint SMPL-X body models at 120fps, synchronized audio, and detailed annotations for over 2,800 move segments. The SalsaAgent model is a unified multitask framework using Gemma2-2b-it LLM backbone with LoRA adapters. Training occurs in two stages: Stage I pretraining on multimodal aligned data for 5 epochs, followed by Stage II task-specific fine-tuning for 100 epochs each on solo and duet generation tasks. Motion is tokenized via VQ-VAE (512 tokens), audio via WavTokenizer (4096 tokens), and text via pretrained tokenizer. The model generates 5-second motion windows with 1-second stride, optimized using AdamW on a single NVIDIA A6000 GPU.

## Key Results
- SalsaAgent achieves FIDk of 0.135 and FIDg of 0.128 on solo dance generation, outperforming baselines including MotionLLM and BaseModel
- For duet dance generation, SalsaAgent obtains cross-distance FID of 0.152 and Beat Echo Degree of 0.821, demonstrating superior synchronization with leader motion
- Classification accuracy reaches 45.8% for move type prediction and 51.7% for move combination prediction, despite significant class imbalance in the dataset

## Why This Works (Mechanism)
The framework treats salsa dancing as an embodied language where nonverbal communication through movement follows syntactic and semantic rules similar to spoken language. The unified multitask approach enables the model to learn shared representations across solo and duet contexts, capturing both individual proficiency and interactive coordination patterns. The VQ-VAE tokenization of motion preserves the continuous nature of dance while enabling efficient sequence modeling. The two-stage training process first establishes multimodal alignment between music, motion, and textual descriptions, then specializes for task-specific generation, allowing the model to learn both general dance patterns and fine-grained interactive behaviors.

## Foundational Learning
- **SMPL-X body model parameterization** - why needed: Provides standardized 3D human mesh representation for motion capture data; quick check: verify 55 joints are correctly extracted from .npz files
- **VQ-VAE motion tokenization** - why needed: Converts continuous motion sequences into discrete tokens suitable for LLM processing; quick check: ensure 512-token codebook is properly trained and reconstructs motion accurately
- **Multimodal alignment learning** - why needed: Establishes correspondence between audio, motion, and text modalities for joint understanding; quick check: validate that pretraining loss decreases and multimodal representations cluster appropriately
- **LoRA adapter training** - why needed: Enables efficient task-specific adaptation without full model fine-tuning; quick check: confirm adapter weights are being updated during fine-tuning
- **Beat-synchronization metrics** - why needed: Quantifies rhythmic alignment between generated motion and musical tempo; quick check: verify Beat Echo Degree calculations match the paper's methodology
- **Frechet Inception Distance variants** - why needed: Measures distribution similarity between generated and real motion data; quick check: ensure kinematic and graphical FID calculations use consistent feature extraction methods

## Architecture Onboarding

**Component Map:**
Gemma2-2b-it LLM -> LoRA Adapters -> Motion VQ-VAE Tokenizer (512 tokens) + WavTokenizer (4096 tokens) + Text Tokenizer -> Solo/Duet Generation Tasks

**Critical Path:**
1. Data preprocessing: SMPL-X motion → VQ-VAE tokens + ELAN annotations → MotionScript text + audio → WavTokenizer tokens
2. Pretraining: Multimodal alignment of audio, motion, and text tokens through Gemma2-2b-it backbone
3. Fine-tuning: Task-specific adaptation for solo generation and duet generation using task prompts
4. Evaluation: Generate motion sequences and compute FIDk/g/cd, Divk/g/cd, BED, BAS metrics

**Design Tradeoffs:**
- Using 5-second windows with 1-second stride balances temporal context against computational efficiency
- Gemma2-2b-it provides sufficient capacity while remaining computationally tractable for motion generation
- VQ-VAE with 512 tokens offers a good compromise between motion fidelity and sequence length constraints
- Two-stage training enables both broad multimodal understanding and task-specific specialization

**Failure Signatures:**
- MotionLLM baseline showing FIDk > 1e6 indicates domain gap and insufficient CoMPAS3D-specific training
- Class imbalance causing ~46-52% classification accuracy even with class weights suggests need for focal loss or resampling strategies
- Poor Beat Echo Degree scores indicate failure to capture rhythmic synchronization with music

**3 First Experiments:**
1. Verify VQ-VAE reconstruction quality by comparing input motion sequences with reconstructed outputs using average joint position error
2. Test pretraining convergence by monitoring multimodal alignment loss across audio, motion, and text modalities
3. Evaluate solo generation quality using kinematic FID against a held-out subset of the training data before full evaluation

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation relies primarily on proxy metrics rather than human perceptual studies of dance quality or social interaction fidelity
- Dataset size of 3 hours total is relatively modest compared to large-scale motion capture datasets
- Class imbalance in move classification leads to lower classification accuracy even with class weights
- Model evaluation focuses on synchronization and motion quality metrics but does not assess nuanced aspects of nonverbal communication

## Confidence

**High confidence:** Dataset creation methodology, technical specification of SalsaAgent architecture, basic evaluation framework, publicly released dataset and code

**Medium confidence:** Claims about CoMPAS3D being a challenging benchmark for interactive AI, effectiveness of SalsaAgent, improved synchronization and motion quality

**Low confidence:** None explicitly identified

## Next Checks
1. Conduct human perceptual studies comparing SalsaAgent-generated dances against baseline methods, focusing on social interaction quality, stylistic authenticity, and partner coordination
2. Perform ablation studies on the VQ-VAE motion tokenizer architecture to verify that the assumed architecture choices are optimal for this domain
3. Test model generalization by evaluating on out-of-distribution scenarios such as unseen music tracks, different salsa styles (Cuban vs. LA vs. New York), or cross-cultural dance variations