---
ver: rpa2
title: Safeguarding LLM Fine-tuning via Push-Pull Distributional Alignment
arxiv_id: '2601.07200'
source_url: https://arxiv.org/abs/2601.07200
tags:
- safety
- harmful
- fine-tuning
- data
- safe
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'Safety Optimal Transport (SOT) addresses the erosion of safety
  alignment in LLMs during fine-tuning by reframing sample selection as a distribution-level
  alignment task using Optimal Transport. The method introduces a dual-reference "push-pull"
  mechanism: it pulls the downstream data distribution toward a trusted safe anchor
  while pushing it away from a harmful reference.'
---

# Safeguarding LLM Fine-tuning via Push-Pull Distributional Alignment

## Quick Facts
- arXiv ID: 2601.07200
- Source URL: https://arxiv.org/abs/2601.07200
- Reference count: 40
- One-line primary result: SOT improves the safety-utility trade-off with average scores often exceeding 0.8, outperforming baselines.

## Executive Summary
Safety Optimal Transport (SOT) addresses the erosion of safety alignment in LLMs during fine-tuning by reframing sample selection as a distribution-level alignment task using Optimal Transport. The method introduces a dual-reference "push-pull" mechanism: it pulls the downstream data distribution toward a trusted safe anchor while pushing it away from a harmful reference. This creates a robust geometric safety boundary. Experiments on multiple model families (Meta-Llama-3.1-8B, Qwen3-8B, Llama3-8B, Gemma3-4B) and diverse datasets show SOT consistently achieves the lowest harmfulness scores (HmS) while maintaining high helpfulness scores (HpS) and competitive accuracy.

## Method Summary
SOT treats the fine-tuning dataset as a weighted empirical distribution and learns optimal sample weights by minimizing an Optimal Transport distance to a safe reference while maximizing it to a harmful reference. After weight learning, it applies Top-K selection to remove the most harmful samples and then performs weighted fine-tuning. The method leverages a frozen safety-aligned model's embeddings to construct a semantic space where the OT objective creates a geometric safety boundary.

## Key Results
- SOT consistently achieves the lowest harmfulness scores (HmS) across multiple models and datasets
- Maintains high helpfulness scores (HpS) while outperforming baselines like DSIR, SEAL, and SAFT
- Average safety-utility scores often exceed 0.8, demonstrating superior trade-off performance

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: The dual-reference "push-pull" objective creates a robust geometric safety boundary by jointly optimizing for attraction to safe data and repulsion from harmful data.
- **Mechanism**: SOT treats the custom fine-tuning dataset $D_{custom}$ as a weighted empirical distribution $P(w)$. It learns a weight vector $w$ by minimizing an objective that minimizes the Optimal Transport (OT) distance between $P(w)$ and a safe reference distribution $Q$ ("pull") while simultaneously maximizing the OT distance between $P(w)$ and a harmful reference distribution $M$ ("push"). This distribution-level alignment uses the geometry of the embedding space (via the cost matrix) to assess sample quality based on its relationship to the *entire* safe and harmful manifolds, not just in isolation.
- **Core assumption**: The frozen, safety-aligned LLM's last-layer representations can be used to construct a meaningful semantic space where "safe" and "harmful" samples form separable clusters or manifolds. The OT distance in this space is a reliable proxy for safety relevance.
- **Evidence anchors**:
  - [abstract]: "At its core is a dual-reference 'push-pull' weight-learning mechanism: SOT optimizes sample importance by actively pulling the downstream distribution towards a trusted safe anchor while simultaneously pushing it away from a general harmful reference."
  - [section]: Section 4.1, Eq. 5: $\min_w [(1-\lambda)OT(P(w), Q) - \lambda OT(P(w), M)]$. The "Pull" force acts as a relevance ranker, while the "Push" force acts as a discriminator, compressing weights of samples close to harmful ones.
  - [corpus]: The corpus confirms OT is used for alignment but lacks direct validation of this specific dual-reference push-pull formulation for safety. Assumption: The geometric boundary metaphor is robust.
- **Break condition**: The safe and harmful distributions $Q$ and $M$ are not sufficiently representative, causing the OT distance to be an unreliable proxy for safety, or the semantic space from the frozen model is not well-aligned with the target task.

### Mechanism 2
- **Claim**: Top-K selection followed by weighted fine-tuning effectively purifies the training data, filtering out severe harmful samples ("hard denoising") while up-weighting high-quality, safe samples ("soft emphasis").
- **Mechanism**: After learning the optimal weight vector $w^*$, SOT applies a hard truncation by selecting only the top-K samples with the highest weights for the fine-tuning dataset. Then, instead of uniform SFT, it uses a weighted loss function where each sample's contribution to the gradient is scaled by its re-normalized weight. This two-step process first removes the most toxic samples from the training set and then amplifies the learning signal from the most promising safe samples during optimization.
- **Core assumption**: The learned weights are a reliable proxy for both safety and utility. A hard cutoff (Top-K) is effective for removing the most severe harmful samples, while the remaining relative weights are meaningful for modulating learning contribution.
- **Evidence anchors**:
  - [abstract]: "This establishes a robust geometric safety boundary that effectively purifies the training data."
  - [section]: Section 4.2, "Hard Denoising via Top-K Selection" and "Soft Emphasis via Weighted LLM Finetune." The ablation study in Table 3 confirms that Top-K Only (HmS 0.205) is a strong driver of safety gains, while the full SOT strategy maximizes the Average Score.
  - [corpus]: No direct corpus evidence for this specific two-stage weighted SFT mechanism.
- **Break condition**: The Top-K cutoff is too aggressive, removing useful data, or the weight distribution is flat, making reweighting ineffective. This could occur if the push-pull objective fails to create clear separation.

### Mechanism 3
- **Claim**: The method generalizes effectively to unseen adversarial patterns because it leverages a general harmful reference distribution and the semantic richness of a pre-aligned model's representations.
- **Mechanism**: The "push" mechanism uses a general harmful dataset ($D_{harmful}$, e.g., a subset of BeaverTails) to represent broad toxicity patterns, not task-specific attacks. The cost matrix is built using cosine distance on embeddings from a *frozen safety-aligned model* (e.g., Llama-3-Instruct), which has been trained to recognize a wide array of harmful concepts. This combination allows SOT to repel harmful samples even if they are novel adversarial constructions, as long as their semantic representations fall within the manifold of general toxicity recognized by the frozen model.
- **Core assumption**: General harmful datasets and the frozen model's representations provide sufficient coverage of the toxic manifold to generalize to novel or task-specific attacks.
- **Evidence anchors**:
  - [section]: Section 4.1 notes that the harmful distribution M "serves as a repulsive source without requiring domain-specific harmful data collection." The ablation study (Table 3) shows substituting the harmful reference with an alternative dataset maintains comparable performance, indicating robustness to the specific harmful reference used.
  - [corpus]: Corpus paper "Logic Jailbreak" notes vulnerability stems from distributional discrepancies, which SOT aims to address. Assumption: This helps generalize to new attacks.
  - [corpus]: The paper's Limitations section explicitly notes that adapting to evolving adversarial patterns theoretically requires updates to reference datasets.
- **Break condition**: An adversarial attack produces harmful outputs with semantic representations that are poorly covered by the general harmful dataset $D_{harmful}$ or are outside the frozen model's "knowledge" of what constitutes harm.

## Foundational Learning

- **Concept: Optimal Transport (OT) & Wasserstein Distance**
  - **Why needed here**: SOT's core innovation is reframing sample selection from an instance-level filter to a distribution-level alignment problem. OT provides the mathematical tool to measure the "effort" (cost) to morph one distribution (weighted custom data) into another (safe reference), creating a principled objective for weight learning.
  - **Quick check question**: How does OT differ from a simple mean or centroid-based distance measure, and why is that important for capturing the geometry of a data distribution?

- **Concept: The Safety-Utility Trade-off in LLM Fine-Tuning**
  - **Why needed here**: The problem statement is that safety alignment erodes during fine-tuning. Existing defenses often sacrifice utility for safety. Understanding this trade-off is key to interpreting SOT's results, which claim to achieve a superior balance.
  - **Quick check question**: Why can fine-tuning on a seemingly innocuous dataset degrade an LLM's safety alignment? What are the two competing objectives SOT must balance?

- **Concept: Embedding Space Geometry and Cosine Distance**
  - **Why needed here**: The cost matrix for the OT calculation is built on the cosine distance between sample embeddings. The "push-pull" mechanism operates geometrically within this space. A strong intuition for what cosine distance represents in high-dimensional semantic space is critical.
  - **Quick check question**: If two samples have a low cosine distance in the embedding space of a frozen LLM, what does that imply about their semantic relationship? How does the "push" objective use this distance?

## Architecture Onboarding

- **Component map**: Input (D_custom, D_safe, D_harmful) -> Representation Extractor (frozen LLM) -> Dual-Reference OT Solver (Sinkhorn) -> Weight Optimizer (gradient descent) -> Data Purifier (Top-K + reweight) -> Safety-Aware SFT Trainer (LoRA-based)

- **Critical path**: The representation extraction step is decoupled from the target model. The most critical and novel step is the iterative optimization loop between the OT solver and the weight optimizer, which learns the sample importance. The success of the entire system hinges on this loop converging to a meaningful weight distribution that separates safe and harmful samples.

- **Design tradeoffs**:
  - **Pull vs. Push Strength (λ)**: The hyperparameter λ controls the balance between safety and utility. A higher λ prioritizes safety (push), while a lower λ prioritizes task alignment (pull). The paper finds λ=0.5 is a Pareto optimal point (Appendix A.8).
  - **Hard Cutoff (Top-K) vs. Soft Reweighting**: The decision to use a hard Top-K selection before reweighting is a tradeoff. A hard cutoff efficiently removes severe toxic tails, but could discard useful edge cases. Reweighting is softer but less effective against strong harmful signals. The paper uses both.
  - **General vs. Task-Specific References**: The "pull" reference must be task-specific to ensure utility, but the "push" reference can be general. This trades the effort of curating task-specific harmful data for the robustness of a general toxicity manifold.

- **Failure signatures**:
  - **Weight Distribution Collapse**: If the learned weights become uniformly distributed, the system fails to discriminate. This could happen if the cost matrices are uninformative (e.g., the frozen model's embeddings are poor).
  - **Utility Degradation**: If the Top-K selection is too aggressive or the "push" force is too strong (high λ), the model may become safe but unhelpful.
  - **Adversarial Slip-through**: If an adversarial sample's embedding is distant from the $D_{harmful}$ manifold in the frozen model's representation space, it will receive a high weight and degrade safety.

- **First 3 experiments**:
  1.  **Reproduce Main Results on a Single Dataset**: Implement the SOT pipeline on one dataset (e.g., SLIMORCA) and one model (e.g., Llama-3.1-8B). Compare the Harmfulness Score (HmS) and Helpfulness Score (HpS) against a standard SFT baseline to validate the safety-utility trade-off.
  2.  **Ablate the Dual-Reference Objective**: Run an experiment comparing "Pull Only" (λ=0), "Push Only" (λ=1), and the full SOT (λ=0.5) to validate the claim that both forces are indispensable. Analyze the resulting weight distributions (similar to Figure 3).
  3.  **Visualize the Embedding Space**: Extract embeddings for a small set of samples from $D_{custom}$, $D_{safe}$, and $D_{harmful}$. Project them into 2D (e.g., with t-SNE) and visualize how the learned weights (size/color of points) correlate with proximity to the safe and harmful clusters. This validates the geometric mechanism.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the SOT framework be adapted to handle evolving adversarial attack strategies (e.g., multilingual jailbreaks) without requiring frequent manual updates to the static harmful reference distribution?
- Basis in paper: [explicit] Section 7 (Limitations) states, "Current frameworks assume reference distributions are static; maintaining robustness against future unknown attack vectors theoretically requires updates to reference datasets."
- Why unresolved: The current implementation relies on static datasets for the harmful reference distribution ($M$), which creates a vulnerability to novel attack types not present in the initial reference set.
- What evidence would resolve it: A modified SOT framework that dynamically updates the harmful reference distribution or generalizes to unseen adversarial patterns, validated against emerging attack benchmarks.

### Open Question 2
- Question: How can the SOT framework be extended to provide human-readable natural language explanations for specific "push" or "pull" decisions rather than just quantitative sample weights?
- Basis in paper: [explicit] Section 7 (Limitations) notes, "It currently cannot generate natural language explanations for why specific samples are prioritized or penalized... developing more intuitive, human-readable interfaces... remains a valuable avenue."
- Why unresolved: The current output is a numerical weight vector $w^*$; the semantic rationale for why a specific sample is pushed or pulled is lost in the transport plan calculation.
- What evidence would resolve it: An interpretability module capable of generating textual justifications for sample weights (e.g., "Sample down-weighted due to semantic overlap with toxic pattern X"), evaluated for human comprehensibility.

### Open Question 3
- Question: How sensitive is the SOT framework's safety-utility trade-off to the presence of noise or bias within the trusted safe anchor ($D_{safe}$) or harmful reference ($D_{harmful}$) datasets?
- Basis in paper: [explicit] Section 7 (Limitations) states, "The effectiveness... is intrinsically linked to the quality... of the reference datasets. If the safe reference distribution is biased... weights may not accurately reflect the true safety quality."
- Why unresolved: The paper demonstrates effectiveness with high-quality references but does not quantify performance degradation when the safe anchor contains implicit bias or the harmful set is incomplete.
- What evidence would resolve it: Ablation studies injecting varying degrees of label noise or distributional bias into $D_{safe}$ and $D_{harmful}$ to measure the resulting variance in HmS and HpS.

### Open Question 4
- Question: To what extent do the SOT results regarding harmfulness and helpfulness scores derived from LLM-as-a-judge methodologies align with human evaluation standards?
- Basis in paper: [explicit] Section 7 (Limitations) acknowledges, "LLM judges may harbor inherent biases or discrepancies compared to human evaluation, potentially affecting the granularity of safety assessments."
- Why unresolved: The entire experimental validation relies on automated LLM judges (Kimi, DeepSeek), leaving a potential gap between the reported metrics and actual human perception of safety/helpfulness.
- What evidence would resolve it: A comparative study measuring the correlation between SOT's automated safety scores and human annotator judgments on a statistically significant subset of model outputs.

## Limitations
- The method relies on the assumption that a frozen safety-aligned model's embedding space provides a reliable semantic manifold for OT-based distribution alignment, which is not empirically validated
- The Top-K cutoff introduces a hard decision boundary that may discard useful edge cases
- The computational overhead of iterative OT optimization may be prohibitive for very large datasets

## Confidence
- **High confidence**: The empirical demonstration that SOT improves the safety-utility trade-off across multiple models and datasets. The ablation study convincingly shows the push-pull objective is essential.
- **Medium confidence**: The geometric interpretation of the push-pull mechanism creating a "robust safety boundary." While intuitive and supported by results, the mathematical proof of boundary robustness is not provided.
- **Low confidence**: The claim of generalization to unseen adversarial patterns. The paper notes this requires updated reference datasets, and there's no direct testing against novel jailbreak constructions.

## Next Checks
1. **Semantic Geometry Validation**: Extract embeddings for held-out samples from the target domain and compute their distances to the safe/harmful manifolds in the frozen model's space. Correlate these distances with human-annotated safety scores to validate the OT cost as a proxy.
2. **Adversarial Robustness Test**: Generate adversarial samples specifically designed to exploit distributional discrepancies (e.g., via the "Logic Jailbreak" methodology) and test whether SOT with general harmful references can still filter them effectively.
3. **Computational Efficiency Benchmark**: Measure the runtime and memory overhead of the iterative OT optimization on datasets of varying sizes (100 samples to 100K samples) and compare against the training time of standard SFT to quantify the practical cost.