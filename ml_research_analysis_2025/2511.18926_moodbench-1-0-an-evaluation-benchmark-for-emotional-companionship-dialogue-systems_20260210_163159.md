---
ver: rpa2
title: 'MoodBench 1.0: An Evaluation Benchmark for Emotional Companionship Dialogue
  Systems'
arxiv_id: '2511.18926'
source_url: https://arxiv.org/abs/2511.18926
tags:
- ability
- score
- benchmark
- evaluation
- scores
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MoodBench 1.0 is the first comprehensive benchmark for evaluating
  Emotional Companionship Dialogue Systems (ECDs). It addresses the lack of systematic
  evaluation standards by proposing a formal definition of ECDs and implementing a
  four-dimensional evaluation framework.
---

# MoodBench 1.0: An Evaluation Benchmark for Emotional Companionship Dialogue Systems

## Quick Facts
- **arXiv ID**: 2511.18926
- **Source URL**: https://arxiv.org/abs/2511.18926
- **Reference count**: 40
- **Key outcome**: First comprehensive benchmark for Emotional Companionship Dialogue Systems (ECDs) with four-dimensional evaluation framework revealing performance disparities among 30 mainstream models

## Executive Summary
MoodBench 1.0 addresses the critical gap in systematic evaluation standards for Emotional Companionship Dialogue Systems (ECDs) by introducing the first comprehensive benchmark. The framework defines ECDs through a formal model and implements a four-dimensional evaluation structure that assesses Threshold, Foundational, Emotional, and Companionship abilities across 41 tasks using 60 datasets. Extensive testing reveals closed-source models outperform open-source ones, with scaling laws remaining effective in this domain. The benchmark demonstrates excellent discriminant validity and provides developers with precise optimization targets by pinpointing specific ability gaps.

## Method Summary
MoodBench 1.0 employs a hierarchical four-layer framework (Ability → Task → Data → Method) to evaluate ECDs systematically. The evaluation aggregates scores bottom-up from dataset-level benchmark-based and model-based methods, applying asymmetric difficulty weights (Low 30%, Medium 55%, High 15%) to task performance. A threshold gate (τ = 60) on Values & Safety acts as a "one-vote veto" mechanism. The benchmark was validated through consistency with domain consensus and high correlation with human evaluation, testing 30 mainstream models including Qwen3, DeepSeek-V3, and OpenAI models.

## Key Results
- **Performance disparities**: Closed-source models significantly outperform open-source ones across all ability dimensions
- **Scaling effectiveness**: Larger models show consistent performance improvements, confirming scaling laws remain effective for emotional companionship tasks
- **Diagnostic capability**: Four-dimensional framework successfully identifies specific optimization targets, revealing current models' particular weaknesses in long-term dialogue recall and advanced emotional companionship abilities

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Hierarchical decomposition enables fine-grained diagnostic capability across emotional companionship dimensions.
- Mechanism: The four-layer framework separates *what* is being measured from *how* it is measured. Top-down decomposition breaks complex "emotional companionship ability" into assessable sub-abilities, while bottom-up aggregation with difficulty weighting produces interpretable composite scores.
- Core assumption: Emotional companionship ability can be validly decomposed into independent sub-abilities whose weighted combination reflects overall capability.
- Evidence anchors: Four-dimensional evaluation framework with "excellent discriminant validity, revealing performance disparities among models"; Ability Layer Definition identifies Threshold/Foundational/Core abilities mapped to user needs.
- Break condition: If sub-abilities exhibit high interdependence, independent assessment with simple weighted aggregation may misrepresent true capability structure.

### Mechanism 2
- Claim: Threshold gating with "one-vote veto" ensures safety baseline before rewarding companionship capability.
- Mechanism: The Values & Safety dimension acts as a gate-keeper (threshold τ = 60). Models scoring below this receive total_score = 0 regardless of other abilities.
- Core assumption: Values & Safety is a binary prerequisite for acceptable ECD deployment.
- Evidence anchors: Threshold Ability "serves as the cornerstone and safety baseline... acting as a 'one-vote veto' that corresponds to the user need for 'Values & Safety'".
- Break condition: If safety and companionship abilities can trade off meaningfully, binary gating may discard viable models.

### Mechanism 3
- Claim: Difficulty-weighted aggregation reveals that foundational language abilities are necessary but insufficient for core emotional companionship.
- Mechanism: Score synthesis applies asymmetric difficulty weights to task-level performance. Analysis shows Foundational Ability and Core Ability scores exhibit correlation but not direct conversion.
- Core assumption: Difficulty weights appropriately balance task importance; scatter plot dispersion implies independence rather than measurement noise.
- Evidence anchors: Findings showed closed-source models outperforming open-source ones, with scaling laws remaining effective; scatter plot analysis explicitly states foundational abilities are "Cornerstone" but "cannot directly translate" into core abilities.
- Break condition: If foundational ability floors are higher than tested, correlation analysis may underestimate dependency.

## Foundational Learning

- Concept: **Hierarchical Benchmark Design (Ability-Task-Dataset-Method)**
  - Why needed here: MoodBench's four-layer architecture assumes familiarity with how evaluation frameworks decompose complex constructs.
  - Quick check question: Can you explain why a "flat, one-to-many structure" fails to distinguish models' ability boundaries across difficulty levels?

- Concept: **LLM Evaluation Paradigms (Benchmark-based vs. Model-based)**
  - Why needed here: MoodBench 1.0 relies primarily on benchmark-based methods but acknowledges limitations from "variations in dataset quality".
  - Quick check question: For an open-ended empathetic response task, what are two failure modes of benchmark-based evaluation that model-based evaluation might address?

- Concept: **Formal Dialogue System Modeling**
  - Why needed here: The ECD definition specifies the input tuple (U, R, M, E, P, P_ds, P_u, K) and generation function Φ.
  - Quick check question: In the formal model, why is long-term memory M constant within a session but updated across sessions? What architectural implication does this have for memory retrieval?

## Architecture Onboarding

- Component map:
  Ability Layer (4 dimensions) → Task Layer (41 tasks, 3 difficulty levels) → Data Layer (60 datasets, 300 samples each) → Method Layer (60 methods: benchmark-based + model-based)

- Critical path:
  1. Configure evaluation: Load Config via `LoadConfiguration()`
  2. Generate raw scores: For each model-dataset-method tuple, call `GetRawScore()` → normalize via `NormalizeScore()` to 0-100 scale
  3. Aggregate bottom-up: Dataset scores → Sub-task scores → Difficulty-level scores → Sub-ability scores → Main ability scores
  4. Apply threshold gate: If Values & Safety score < 60, return 0; else compute weighted total
  5. Diagnose weaknesses: Drill down from ability layer to task layer to identify optimization targets

- Design tradeoffs:
  - Coverage vs. depth: 60 datasets provide breadth but limited high-quality tasks for "advanced emotional tasks"
  - Language balance: 58.3% English, 35% Chinese, 6.7% bilingual—cross-cultural emotional expression is undersampled
  - Evaluation method: Benchmark-based methods dominate; model-based methods limited to generation tasks
  - Difficulty calibration: Small score range (7.14 points across 28 models) suggests insufficient high-difficulty tasks for top-tier model discrimination

- Failure signatures:
  - Flat score distribution: If top-10 models cluster within 2-3 points, mid-to-high difficulty tasks are under-represented
  - Companionship floor effect: Universal low scores on Long-term Dialogue Recall indicate current Transformer attention mechanisms struggle with cross-session dynamic information
  - Threshold false negatives: Models with strong emotional/companionship abilities but marginal Values & Safety scores may be unfairly zeroed
  - Dataset bias propagation: If MoodBench datasets differ qualitatively from established benchmarks, aggregated scores may conflate model ability with dataset artifact

- First 3 experiments:
  1. Baseline comparison: Run MoodBench on 3 models of varying scale within the same family to validate scaling law replication
  2. Ablation on difficulty weighting: Re-compute overall scores with uniform difficulty weights instead of asymmetric to assess ranking order changes
  3. Companionship architecture probe: Test whether adding external memory retrieval improves LongMemEval task scores relative to baseline Transformer

## Open Questions the Paper Calls Out
None

## Limitations
- **Decomposition validity**: The assumption that emotional companionship ability can be decomposed into independent sub-abilities may not hold if these abilities exhibit strong interdependence
- **Safety threshold rigidity**: The binary safety threshold (τ = 60) may be overly rigid, potentially discarding models that could provide meaningful emotional support despite minor safety concerns
- **Limited cross-cultural representation**: Only 6.7% bilingual datasets constrains generalizability to diverse emotional expression contexts

## Confidence
- **High Confidence**: Discriminant validity and consistency with domain consensus are well-supported by empirical results
- **Medium Confidence**: Four-dimensional framework's effectiveness in identifying specific optimization targets is supported, but independence of sub-abilities requires further validation
- **Low Confidence**: Assumption that Values & Safety acts as a binary prerequisite lacks empirical validation

## Next Checks
1. **Interdependence Validation**: Conduct factor analysis on the 41 task scores to empirically test whether sub-abilities are truly independent or exhibit hierarchical relationships
2. **Threshold Calibration Study**: Systematically vary the Values & Safety threshold (τ = 50, 60, 70) and measure its impact on model rankings and overall scores
3. **Cross-Cultural Generalization Test**: Evaluate the same models on culturally diverse emotional expression datasets to assess whether performance disparities persist across different cultural contexts