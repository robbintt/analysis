---
ver: rpa2
title: 'Silicon Minds versus Human Hearts: The Wisdom of Crowds Beats the Wisdom of
  AI in Emotion Recognition'
arxiv_id: '2508.08830'
source_url: https://arxiv.org/abs/2508.08830
tags:
- human
- performance
- gpt-4o
- accuracy
- humans
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study compared AI and human performance on emotion recognition
  tasks. It found that while GPT-4o outperformed humans at individual recognition
  (90% vs 71% accuracy on RMET), human crowds achieved near-perfect accuracy when
  judgments were aggregated through plurality voting.
---

# Silicon Minds versus Human Hearts: The Wisdom of Crowds Beats the Wisdom of AI in Emotion Recognition

## Quick Facts
- arXiv ID: 2508.08830
- Source URL: https://arxiv.org/abs/2508.08830
- Authors: Mustafa Akben; Vinayaka Gude; Haya Ajjan
- Reference count: 0
- Key result: Human crowds achieve near-perfect accuracy through plurality voting while AI crowds show minimal improvement from aggregation

## Executive Summary
This study compares AI and human performance on emotion recognition tasks using the Reading the Mind in the Eyes Test (RMET). While GPT-4o outperformed humans at individual recognition (90% vs 71% accuracy), human crowds achieved near-perfect accuracy when judgments were aggregated through plurality voting. GPT-4o crowds showed minimal improvement from aggregation due to correlated error patterns. Augmented intelligence, combining human and AI judgments, achieved the highest accuracy (100% on RMET) and improved most with group size. These results suggest that despite GPT-4o's strong individual performance, human collective intelligence and collaborative human-AI approaches offer the most effective path for emotion recognition applications.

## Method Summary
The study used the Reading the Mind in the Eyes Test (RMET) to compare emotion recognition performance between GPT-4o and humans. Human participants completed RMET online through Prolific, while GPT-4o was queried via OpenAI API using zero-shot and few-shot prompting. The authors simulated collective intelligence by aggregating independent judgments through plurality voting across group sizes from 5 to 90 participants/calls. They analyzed accuracy curves, improvement slopes, and performed first-order and second-order stochastic dominance tests to compare distributions. Augmented intelligence experiments combined human and AI judgments in various ratios (10:1 to 1:1 human:AI).

## Key Results
- GPT-4o achieved 90% accuracy on RMET individually, outperforming humans at 71%
- Human crowds improved from 71% to near-perfect accuracy through plurality voting aggregation
- GPT-4o crowds showed minimal improvement from aggregation (plateauing at ~90-93%)
- Augmented human-AI crowds achieved 100% accuracy on RMET with steepest improvement slopes
- Human crowds demonstrated first-order stochastic dominance over AI crowds at high quantiles

## Why This Works (Mechanism)

### Mechanism 1: Human Collective Intelligence via Error Uncorrelation
Aggregating independent human judgments improves accuracy because diverse perspectives produce uncorrelated errors that cancel during voting. Individual humans bring varied backgrounds and cognitive styles → prediction errors are distributed rather than systematic → plurality voting eliminates outliers → collective accuracy increases with group size. Core assumption: Human participants are sufficiently diverse in their error patterns. Evidence: Human crowds improved steeply with size (slope = 3.99, 95% CI [3.87, 4.10] for RMET).

### Mechanism 2: AI Aggregation Failure via Homogeneous Error Patterns
Aggregating GPT-4o instances yields minimal improvement because the model produces correlated responses across runs. Same model architecture and weights → each API call processes identically → errors are systematic, not random → plurality voting reinforces rather than cancels mistakes. Core assumption: Temperature = 1 and top-p = 1 sampling do not introduce sufficient diversity for independent judgments. Evidence: GPT-4o crowds showed minimal improvement from aggregation.

### Mechanism 3: Augmented Intelligence via Complementary Strengths
Combining human and AI judgments achieves superior performance because each source compensates for the other's weaknesses. Humans provide error-diversity → AI provides consistent baseline → uncorrelated human errors cancel → AI prevents systematic drift → synergy emerges without direct interaction. Core assumption: Maintaining independence prevents bias transfer. Evidence: Augmented crowds showed steepest improvement slopes (4.23 for RMET vs 3.99 human-only).

## Foundational Learning

- **Concept: Plurality voting aggregation**
  - Why needed: The paper's collective intelligence results depend entirely on understanding how majority voting cancels individual errors.
  - Quick check: If 7 of 10 voters choose option A and 3 choose option B, what is the collective decision? (Answer: A wins, regardless of confidence levels.)

- **Concept: Stochastic dominance (first-order and second-order)**
  - Why needed: The paper uses FSD/SSD tests to prove GPT-4o outperforms humans across the entire performance distribution, not just at averages.
  - Quick check: If distribution A first-order stochastically dominates distribution B, what does that mean for any accuracy threshold? (Answer: A has higher probability of reaching or exceeding that threshold.)

- **Concept: Ceiling effects in measurement**
  - Why needed: The narrowing AI-human gap at high quantiles may reflect test limitations rather than convergent ability.
  - Quick check: When both humans and AI score near 100%, can the test still detect performance differences? (Answer: No—ceiling effects mask remaining ability gaps.)

## Architecture Onboarding

- **Component map:**
  - Human judgment layer → independent responses from N participants
  - AI judgment layer → M independent GPT-4o API calls (zero-shot or few-shot)
  - Aggregation engine → plurality voting with configurable group sizes (5-90)
  - Augmented combiner → stratified sampling (e.g., 10:1 human:AI ratio) → unified voting

- **Critical path:**
  1. Collect independent judgments (do not allow interaction between sources)
  2. Sample groups at target size with replacement
  3. Apply plurality voting per item
  4. Compare accuracy curves across group sizes

- **Design tradeoffs:**
  - Larger human samples increase accuracy but require more recruitment cost
  - AI runs are cheap but provide diminishing returns due to homogeneity
  - Higher human:AI ratios preserve diversity; lower ratios risk AI dominance
  - Direct human-AI interaction was intentionally avoided to prevent bias transfer

- **Failure signatures:**
  - AI-only crowds plateauing at ~90-93% regardless of group size
  - Human-only crowds improving logarithmically, not linearly
  - Augmented crowds underperforming if ratio is miscalibrated

- **First 3 experiments:**
  1. Replicate the plurality voting analysis with group sizes 5, 20, 50, 90 on a held-out emotion dataset to validate scaling curves.
  2. Test alternative aggregation methods (weighted voting, confidence-weighted) against plurality voting to assess robustness.
  3. Introduce diversity-enhancing prompting for AI (role personas, temperature variation) to determine if AI aggregation gains can be improved.

## Open Questions the Paper Calls Out
None

## Limitations
- The findings depend critically on maintaining independence between judgment sources; direct human-AI interaction could fundamentally alter aggregation benefits through bias transfer
- The aggregation methodology assumes plurality voting is optimal without comparing against alternative voting schemes (weighted voting, confidence-weighted aggregation, or Borda count)
- The 100% augmented intelligence accuracy on RMET may reflect ceiling effects and requires validation across diverse emotion recognition tasks

## Confidence

*High confidence:* GPT-4o's superior individual performance relative to humans on RMET (90% vs 71% accuracy). The first-order stochastic dominance analysis provides robust evidence that GPT-4o outperforms humans across the entire performance distribution.

*Medium confidence:* Human collective intelligence benefits from plurality voting. While the statistical patterns are clear (steep improvement slopes, 95% confidence intervals [3.87, 4.10] for RMET), the mechanism assumes cognitive diversity without directly measuring error pattern distributions.

*Medium confidence:* Augmented intelligence achieves 100% accuracy on RMET. The result is compelling but based on a single dataset with potential ceiling effects. The claimed superiority of human:AI ratios (e.g., 10:1) requires further validation.

## Next Checks

1. Test interactive human-AI scenarios where participants can see AI suggestions before responding, measuring whether bias transfer eliminates the observed aggregation benefits.

2. Replicate the aggregation experiments on alternative emotion recognition datasets with lower ceiling effects to verify whether the 100% augmented intelligence accuracy generalizes beyond RMET.

3. Compare plurality voting against confidence-weighted and Borda count methods for both human-only and augmented crowds to determine if alternative aggregation schemes improve accuracy or change scaling patterns.