---
ver: rpa2
title: Are Language Models Models?
arxiv_id: '2601.10421'
source_url: https://arxiv.org/abs/2601.10421
tags:
- language
- level
- computational
- cognitive
- human
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper critically evaluates the claim that large language models
  (LMs) can serve as cognitive model systems, analyzing them at each of Marr's three
  levels of analysis. At the implementation level, LMs lack biological plausibility
  and show little engagement with neurobiological advances.
---

# Are Language Models Models?

## Quick Facts
- arXiv ID: 2601.10421
- Source URL: https://arxiv.org/abs/2601.10421
- Reference count: 0
- Primary result: LMs are valuable tools for operationalizing cognitive constructs but are poor candidates as cognitive model systems at any of Marr's three levels.

## Executive Summary
This paper critically evaluates whether large language models can serve as cognitive model systems by analyzing them through Marr's three levels of analysis. At the implementation level, LMs lack biological plausibility and show little engagement with neurobiological advances. At the algorithmic-representational level, the argument for convergence is challenged by the complex, decomposed nature of language and counterexamples from biology and computation. At the computational theory level, the drive for elegant, unified explanations conflicts with the evolutionary "tinkering" nature of biological systems. The author concludes that while LMs can be valuable tools for generating data and operationalizing models, they are poor candidates as cognitive model systems at any of Marr's levels, and overstating their role as models risks feeding into harmful hype about their cognitive plausibility.

## Method Summary
The paper provides a theoretical commentary analyzing the conceptual foundations of LMs-as-cognitive-models claims through Marr's framework. No empirical data is presented; instead, the author synthesizes literature and provides illustrative examples from prior work. The analysis is structured around three levels: implementation (biological plausibility), algorithmic/representational (mechanistic convergence), and computational theory (evolutionary vs. unified explanations). The method involves critical examination of claims, provision of counterexamples, and distinction between LMs as tools versus model systems.

## Key Results
- LMs cannot serve as implementation-level cognitive models due to lack of biological plausibility and minimal engagement with neurobiology
- Functional similarity between LMs and humans does not imply mechanistic convergence at the algorithmic-representational level
- LMs are valuable tools for operationalizing cognitive constructs but should not be claimed as model systems
- Overstating LMs' cognitive plausibility feeds harmful hype and confuses tool use with mechanistic modeling

## Why This Works (Mechanism)

### Mechanism 1: Implementation-Level Biological Disconnect
- Claim: LMs cannot serve as implementation-level cognitive models of human language processing.
- Mechanism: Connectionist approaches deliberately abandoned biological correspondence in favor of function optimization; as Hinton argued, "looking for inspiration from biology...is the wrong way to go about it." Modern LM research shows little substantive engagement with neurobiological advances (e.g., Hickok & Poeppel 2007; Pulvermüller 2021, 2023).
- Core assumption: A cognitive model at the implementation level requires structured correspondences between model entities and neurobiological entities.
- Evidence anchors:
  - [abstract] "the claim is clearly not true at the implementation level"
  - [section] "the connectionism underlying today's LMs was founded on the abandonment of implementation-level biological correspondences"
  - [corpus] Neighbor paper "Studies with impossible languages falsify LMs as models of human language" (FMR=0.66) provides complementary evidence that LMs learn unnatural language structures equally well, suggesting divergent learning mechanisms.
- Break condition: If LM architectures incorporate brain-constrained neural mechanisms (e.g., Durstewitz et al. 2025; Pulvermüller 2023) with validated neurobiological correspondences, this claim would weaken.

### Mechanism 2: Convergence Fails at Algorithmic-Representational Level
- Claim: Functional similarity between LMs and humans does not imply mechanistic convergence.
- Mechanism: The "argument from amazingness" assumes systems solving the same hard problem converge on similar mechanisms (Cao & Yamins 2024), but language decomposes into distinct subproblems with multiple valid computational solutions. Biological counterexamples (sharks use electroreception; dolphins use echolocation) and computational ones (iterative vs. recursive Fibonacci) demonstrate that similar functions can arise from different mechanisms.
- Core assumption: Assumption: Model organisms are justified by pre-existing mechanistic evidence (e.g., mice for oncogenes, ferrets for lung physiology), not just functional similarity.
- Evidence anchors:
  - [abstract] "poorly motivated at the algorithmic-representational level"
  - [section] "mechanistic exceptions to convergence are easily found in biology...and in computation"
  - [corpus] Weak direct corpus support for this specific counterexample-driven argument; neighbor papers focus more on learning/reasoning discrepancies than mechanistic convergence per se.
- Break condition: If comprehensive comparative analysis demonstrates systematic mechanistic parallels between LM internal representations and human cognitive subprocesses (attention, memory, parsing), the convergence argument would strengthen.

### Mechanism 3: Productive Tool Use Without Cognitive Plausibility Claims
- Claim: LMs contribute to cognitive science as operationalization tools, not as model systems.
- Mechanism: LMs excel at generating plausible text continuations, which enables operationalizing constructs like surprisal, anticipation vs. responsivity (Giulianelli et al. 2024), and providing world knowledge for predictive coding models (Nour Eddine et al. 2024). This requires only text generation capability, not cognitive plausibility.
- Core assumption: The distinction between "tool for operationalization" and "model system" is theoretically meaningful and affects research prioritization.
- Evidence anchors:
  - [abstract] "LMs are good candidates as tools"
  - [section] "This use depends only on generation of plausible text, LMs' undisputed strength...without requiring claims about their cognitive plausibility"
  - [corpus] "LLM-based relevance assessment still can't replace human relevance assessment" (FMR=0.53) supports the broader pattern: LMs can approximate human judgments functionally while differing mechanistically.
- Break condition: If LM-generated probabilities/continuations are shown to systematically diverge from human processing in ways that undermine their validity as operationalization proxies (e.g., Oh & Schuler 2023 caveats), tool utility decreases.

## Foundational Learning

- **Concept: Marr's Three Levels of Analysis**
  - Why needed here: The entire argument is structured around whether LMs qualify as models at the implementation (hardware), algorithmic/representational (processes and representations), and computational theory (what problem is being solved and why) levels.
  - Quick check question: Can you explain why a chess program and a human chess master might share a computational theory level while differing at the implementation level?

- **Concept: Model Systems vs. Tools in Science**
  - Why needed here: Resnik's core distinction is between "model systems" (with structured mechanistic correspondences) and "tools" (useful for operationalization without mechanistic claims)—confusing these feeds hype.
  - Quick check question: Why are mice selected as model organisms for cancer research rather than arbitrarily? What does this imply about selection criteria?

- **Concept: Convergence vs. Multiple Realizability**
  - Why needed here: The argument against algorithmic-level LMs-as-models hinges on whether solving the same problem implies similar mechanisms (convergence) or allows different mechanisms (multiple realizability).
  - Quick check question: If two systems achieve similar accuracy on language modeling, what additional evidence would you need to claim they use similar mechanisms?

## Architecture Onboarding

- **Component map:**
  - Implementation level assessment: Compare LM architecture (transformers, backpropagation) against neurobiological constraints (synaptic plasticity rules, cortical organization, neural coding)
  - Algorithmic/representational level assessment: Evaluate whether LM internal representations (attention patterns, hidden states) correspond to documented cognitive subprocesses (working memory, predictive processing, syntactic parsing)
  - Computational theory level assessment: Examine whether LM training objectives (next-token prediction) match the evolutionary problem space of human language (communication, social coordination, internal reasoning)
  - Tool operationalization pathway: Use LM outputs (probabilities, continuations, embeddings) as proxies for human cognitive variables in external models

- **Critical path:**
  1. Define the cognitive construct to model (e.g., surprisal in sentence processing)
  2. Identify whether the goal requires mechanistic correspondence (model system) or functional approximation (tool)
  3. If tool: Validate proxy accuracy against human behavioral/neural data with explicit caveats
  4. If model system: Establish structured correspondences at target Marr level(s) before making cognitive claims

- **Design tradeoffs:**
  - Proxy validity vs. cognitive claim strength: Stronger cognitive claims require more mechanistic evidence; tool use requires less but offers weaker theoretical contribution
  - Generalization vs. biological fidelity: LMs generalize across tasks but lack biological constraints; brain-constrained models (e.g., Pulvermüller 2023) sacrifice some generalization for plausibility
  - Elegant unified theory vs. evolutionary tinkering: Computational-theory elegance (physics-envy) trades off against capturing biological messiness (evolution as tinkerer)

- **Failure signatures:**
  - Overclaiming cognitive plausibility without mechanistic evidence at any Marr level
  - Treating functional similarity (performance on benchmarks) as sufficient for model-system status
  - Ignoring that language decomposes into distinct subproblems with potentially different mechanisms
  - Extrapolating from "LMs do X well" to "LMs do X like humans" (argument from amazingness)

- **First 3 experiments:**
  1. Corpus-based counterexample audit: Identify linguistic phenomena where LMs and humans achieve similar behavioral accuracy but show divergent error patterns, suggesting different mechanisms (e.g., impossible language learning from corpus neighbors)
  2. Ablation-to-lesion mapping test: Compare the effects of LM component ablations (attention heads, layers) to human lesion/deficit patterns; systematic mismatches indicate algorithmic-level divergence
  3. Tool-proxy validation pipeline: For a specific cognitive model (e.g., predictive coding), quantify the variance explained when using LM-derived vs. human-derived surprisal estimates, with explicit confidence intervals on the proxy relationship

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Under what conditions (if any) should we expect genuine mechanistic convergence between LMs and human language processing?
- Basis in paper: [explicit] The paper challenges the convergence argument by noting that language "decomposes into very distinct subproblems," and provides counterexamples from biology (sharks vs. dolphins) and computation (iterative vs. recursive Fibonacci).
- Why unresolved: The author critiques the "argument from amazingness" but does not specify what positive evidence would establish valid convergence claims for specific linguistic subproblems.
- What evidence would resolve it: Systematic comparative studies of LM and human processing mechanisms across distinct linguistic subproblems, with pre-registered criteria for mechanistic correspondence.

### Open Question 2
- Question: What methodological frameworks can properly leverage LMs as tools while avoiding overclaims about their cognitive plausibility?
- Basis in paper: [explicit] The author provides examples of LMs used appropriately as tools (e.g., Giulianelli et al., 2024; Hahn et al., 2022) but warns that "calling them cognitive models overstates the case and unnecessarily feeds LLM hype."
- Why unresolved: The paper critiques overclaiming without fully specifying best practices for appropriate tool use.
- What evidence would resolve it: Comparative studies demonstrating which LM-as-tool approaches yield genuine cognitive insights versus superficial correlations.

### Open Question 3
- Question: What criteria could establish LMs as valid "model systems" analogous to model organisms in biology?
- Basis in paper: [explicit] The author notes that model-organism choices "are supported by existing, solidly established mechanistic evidence" (e.g., mice for oncogene pathways, ferrets for lung physiology), implying LMs lack equivalent justification.
- Why unresolved: The gap is identified but no framework for establishing such criteria is proposed.
- What evidence would resolve it: Development of pre-established mechanistic criteria that LMs would need to satisfy for specific linguistic phenomena before being considered valid model systems.

## Limitations
- No empirical validation; relies entirely on theoretical argumentation and literature synthesis
- Cannot assess full target argument without complete Futrell & Mahowald article
- Relies on conceptual rather than quantitative evidence
- No formal criteria specified for what would constitute sufficient evidence at each Marr level

## Confidence
- Implementation level: High - reflects observable architectural choices and research patterns
- Algorithmic-representational level: Medium - counterexamples are illustrative but not systematically mapped to specific architectures
- Computational theory level: Low - evolutionary-tinkering versus unified-explanation remains philosophical debate without clear empirical resolution

## Next Checks
1. Conduct systematic review of recent LM papers (2023-2025) to quantify actual engagement with neurobiological findings and identify emerging brain-constrained architectures
2. For each Marr level, develop explicit criteria for what would constitute sufficient evidence of cognitive plausibility, then apply these to specific LM applications in cognitive science
3. Design comparative studies where LMs and humans perform identical language tasks while neural recordings are collected, enabling direct comparison of activation patterns and error profiles