---
ver: rpa2
title: 'SCOPE: A Self-supervised Framework for Improving Faithfulness in Conditional
  Text Generation'
arxiv_id: '2502.13674'
source_url: https://arxiv.org/abs/2502.13674
tags:
- generation
- scope
- faithfulness
- table
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces SCOPE, a self-supervised framework for improving
  faithfulness in conditional text generation. The method addresses the problem of
  large language models generating unfaithful or hallucinated content when fine-tuned
  for specific tasks.
---

# SCOPE: A Self-supervised Framework for Improving Faithfulness in Conditional Text Generation

## Quick Facts
- **arXiv ID:** 2502.13674
- **Source URL:** https://arxiv.org/abs/2502.13674
- **Reference count:** 40
- **Primary result:** SCOPE improves faithfulness in conditional text generation by up to 14% over baselines using self-supervised preference optimization

## Executive Summary
SCOPE addresses the problem of large language models generating unfaithful or hallucinated content when fine-tuned for specific tasks. The method uses a two-stage approach: first fine-tuning on half the training data, then generating synthetic preference data by combining context-grounded and context-free model outputs, and finally applying preference tuning to favor grounded responses. Experiments across six datasets in data-to-text generation and summarization show SCOPE significantly improves faithfulness, achieving up to 14% improvement over baselines in automatic metrics, with GPT-4 and human evaluations confirming superior faithfulness compared to standard fine-tuning.

## Method Summary
SCOPE is a self-supervised framework that improves faithfulness in conditional text generation through a three-stage process. First, the base model is fine-tuned on half the training data to create an initial domain-adapted model. Second, synthetic unfaithful samples are generated by mixing context-conditioned and unconditional token distributions using a Bernoulli parameter α∈[0.4,0.6]. Third, preference optimization is applied using Direct Preference Optimization (DPO) with the synthetic pairs, where the original context-grounded output serves as the positive example and the mixed-generation output serves as the negative example. The method requires no human annotation and works by teaching the model to distinguish between grounded and ungrounded text patterns.

## Key Results
- SCOPE achieves up to 14% improvement in faithfulness metrics (NLI Score, AlignScore) over standard fine-tuning baselines
- Performance is most sensitive to the noise parameter α, with optimal range [0.4, 0.6] validated across datasets
- The framework generalizes across six datasets in both data-to-text generation and summarization tasks
- GPT-4 and human evaluations confirm superior faithfulness compared to standard fine-tuning approaches

## Why This Works (Mechanism)

### Mechanism 1: Mixture Decoding Generates Realistic Hallucination Patterns
- **Claim:** Blending context-conditioned and context-free token distributions produces synthetic unfaithful samples that exhibit realistic hallucination patterns.
- **Mechanism:** At each decoding step t, sample αt from Bernoulli(α). If αt=1, sample from pLM(·|y<t) (unconditional, leaks internal knowledge). If αt=0, sample from pθ0(·|y<t, c) (context-grounded). This introduces fluent but ungrounded spans into otherwise coherent text.
- **Core assumption:** Hallucinations in LLMs primarily arise from interference between internal statistical patterns and provided context, which this mixture explicitly simulates.
- **Evidence anchors:**
  - [Section 3.2] Algorithm 1 explicitly defines the mixture sampling: y−t ∼ (1−αt)pθ0(·|y−<t,c) + αtpLM(·|y−<t)
  - [Section 3.2] "hallucinations occur when the model's internal knowledge improperly influences the generation process"
  - [corpus] Related work on retrieval heads (arxiv 2501.13573) confirms contextual faithfulness correlates with attention mechanisms, supporting the knowledge-interference hypothesis
- **Break condition:** If α is too low (<0.4), negative samples are too similar to references, causing training degeneracies. If too high (>0.6), samples become trivially distinguishable, providing weak learning signal.

### Mechanism 2: Preference Optimization Increases Grounded-to-Ungrounded Likelihood Gap
- **Claim:** Direct preference optimization (DPO) teaches the model to prefer context-grounded outputs over synthetically unfaithful ones by maximizing the likelihood ratio gap.
- **Mechanism:** The DPO loss Lθ = −E[log σ(β log(pθ(y|c)/pθ0(y|c)) − β log(pθ(y−|c)/pθ0(y−|c)))] explicitly increases the gap between grounded (y) and ungrounded (y−) response likelihoods, while the pθ0 reference model prevents excessive deviation from the fine-tuned baseline.
- **Core assumption:** The synthetic unfaithful samples y− are representative of actual hallucination patterns the model might produce, making the learned preference transferable.
- **Evidence anchors:**
  - [Section 3.1] Equation (1) formally defines the loss function
  - [Section 5] Tables 2-3 show consistent NLI/PARENT improvements of 5-14 points across 6 datasets
  - [corpus] NeuroFaith (arxiv 2506.09277) validates that behavioral alignment with explanations requires faithful internal representations
- **Break condition:** If the noise level α produces samples that are either too similar to references (low α) or completely incoherent (high α), the preference signal becomes ineffective. Paper recommends α∈[0.4,0.6].

### Mechanism 3: Two-Stage Training Separates Domain Adaptation from Faithfulness Learning
- **Claim:** Splitting training data (D1 for fine-tuning, D2 for preference tuning) ensures domain adaptation completes before faithfulness refinement begins.
- **Mechanism:** Stage 1 fine-tunes pLM on D1 (half the data) to produce pθ0—a model that is domain-adapted but still exhibits hallucinations. Stage 2 uses the remaining D2 to generate synthetic preferences and apply DPO, keeping pθ0 as the reference anchor. The split prevents contamination between learning stages.
- **Core assumption:** Half the training data suffices for domain adaptation (empirically validated in Appendix A.3), leaving sufficient data for preference tuning.
- **Evidence anchors:**
  - [Section 3.1] "We denote by pθ0 the model fine-tuned from pLM on D1 using cross-entropy"
  - [Appendix A.3] Tables 11-12 show fine-tuning on half the data achieves near-full performance
  - [corpus] CoCoLex (arxiv 2508.05534) similarly uses confidence-guided decoding for grounded generation, supporting staged approaches
- **Break condition:** If D1 is too small, pθ0 is too weak to generate realistic samples. If too large, insufficient data remains for preference tuning. Ablation (Table 13) confirms 50% split is optimal.

## Foundational Learning

- **Concept: Direct Preference Optimization (DPO)**
  - Why needed here: SCOPE uses DPO as its core training objective; understanding reward-model-free preference learning is essential to grasp why synthetic y/y− pairs work without human annotation.
  - Quick check question: Can you explain why DPO eliminates the need for a separate reward model compared to traditional RLHF?

- **Concept: Faithfulness vs Factuality in LLM Outputs**
  - Why needed here: The paper explicitly targets faithfulness (groundedness in context), not factuality (alignment with external knowledge). Confusing these leads to incorrect evaluation interpretation.
  - Quick check question: Given the medical example in Table 1, why is "45 y.o. male given amoxicillin" factually correct but unfaithful?

- **Concept: Decoder-Only LLM Conditional Generation**
  - Why needed here: SCOPE is designed for decoder-only architectures (Llama-2, Mistral) and assumes autoregressive token sampling. Understanding how context c conditions pθ(·|c) is foundational.
  - Quick check question: In decoder-only models, how is the input context c incorporated during generation?

## Architecture Onboarding

- **Component map:** pLM -> pθ0 (fine-tuned on D1) -> Noisy Generator (mixes pθ0 and pLM) -> pθ (preference-tuned on D2)

- **Critical path:**
  1. Split training data: D → D1 (50%) + D2 (50%)
  2. Fine-tune pLM on D1 → pθ0 (1-2 epochs)
  3. For each (c, y) in D2: generate y− via noisy_generation(c, pLM, pθ0) with tuned α∈[0.4,0.6]
  4. Apply DPO: optimize pθ using Equation (1), keeping pθ0 frozen as reference
  5. Evaluate using NLI Score (data-to-text) or AlignScore (summarization) + GPT-4 preference

- **Design tradeoffs:**
  - **α parameter:** Controls hallucination realism. Lower α → degeneracies (repetitions). Higher α → trivial discrimination. Paper validates [0.4,0.6] empirically (Figure 3).
  - **BLEU/ROUGE may decrease:** SCOPE prioritizes faithfulness over reference similarity. Acceptable tradeoff per Section 5 discussion.
  - **Data split proportion:** 50/50 split validated via ablation (Table 13). Other splits yield comparable but slightly lower faithfulness.

- **Failure signatures:**
  - **Repetitive outputs:** Indicates α too low (<0.4), negative samples too similar to references
  - **No improvement over SFT:** Indicates α too high (>0.6), samples too easy to distinguish, weak learning signal
  - **Sudden BLEU drop:** Degeneracy onset; recheck α and ensure pθ0 is properly initialized

- **First 3 experiments:**
  1. **Reproduce α sensitivity analysis:** Run SCOPE on ToTTo validation set with α∈{0.1,0.3,0.5,0.7}. Verify NLI peaks in [0.4,0.6] and BLEU drops at extremes (Figure 3). This validates implementation correctness.
  2. **Ablate data split:** Compare 25/75, 50/50, 75/25 splits on a small dataset (e.g., E2E). Confirm 50/50 yields highest faithfulness metrics (Table 13 pattern).
  3. **Baseline comparison:** Compare SCOPE against CAD, PMI, and SFT on a single dataset. Verify SCOPE achieves statistically significant faithfulness improvements (p<0.05 via paired t-test, Appendix F).

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Does the effectiveness of SCOPE scale to Large Language Models (LLMs) with significantly more parameters (e.g., 70B+)?
- **Basis in paper:** [explicit] The authors state in the Limitations section: "We limited our experiments to 7B models due to computational constraints... larger models could potentially yield different insights."
- **Why unresolved:** Larger models have different internal knowledge capacities and hallucination patterns; it is unclear if the self-supervised noise generation method provides sufficient learning signal to override the stronger priors of larger models.
- **What evidence would resolve it:** Reproducing the SCOPE experiments on Llama-2-70B or comparable larger architectures and comparing the NLI/PARENT score improvements against the 7B baselines.

### Open Question 2
- **Question:** Is SCOPE effective on generation tasks with higher complexity than the datasets currently tested?
- **Basis in paper:** [explicit] The paper acknowledges: "Some of these tasks, particularly WebNLG and E2E, are of relatively limited complexity."
- **Why unresolved:** The framework was validated on data-to-text and short summarization tasks. It is uncertain if the method can handle complex reasoning or long-context dependencies where the "faithfulness" constraint is harder to define and verify automatically.
- **What evidence would resolve it:** Evaluating SCOPE on complex, long-context datasets (e.g., multi-document summarization or long-form question answering) and measuring faithfulness using human evaluation.

### Open Question 3
- **Question:** Can the noise parameter α be determined adaptively rather than via validation set tuning?
- **Basis in paper:** [inferred] Section 6 ("Analysis of SCOPE") demonstrates that performance is sensitive to the choice of α and notes that the selected value varies by dataset (Table 10), yet the paper relies on validation tuning to find it.
- **Why unresolved:** Relying on a validation set for α reduces the fully "self-supervised" nature of the method and may limit applicability in scenarios where validation data is scarce or unrepresentative.
- **What evidence would resolve it:** A study proposing a heuristic or dynamic scheduling mechanism for α (e.g., based on generation perplexity or training loss) that matches or exceeds the performance of the current grid-search approach.

## Limitations

- **Limited generalization beyond decoder-only LLMs:** The SCOPE framework is explicitly designed for decoder-only architectures and the mixture decoding mechanism relies on autoregressive generation process.
- **Synthetic data representativeness concerns:** The fundamental assumption that synthetically generated unfaithful samples capture the full distribution of real LLM hallucinations is not empirically verified.
- **Evaluation metric dependencies:** Primary faithfulness metrics rely on pre-trained classifiers that may have their own biases and limitations, and correlation with human judgment is not fully validated.

## Confidence

**High confidence:** The core mechanism of mixture decoding (Mechanism 1) is well-specified and the experimental validation shows clear trends. The ablation studies for α sensitivity (Figure 3) and data split proportions (Table 13) provide strong empirical support.

**Medium confidence:** The preference optimization effectiveness (Mechanism 2) shows consistent improvements across datasets, but the reliance on synthetic negative samples introduces uncertainty about real-world transferability. The two-stage training separation (Mechanism 3) is logically sound but the optimal split ratio is dataset-dependent.

**Low confidence:** Claims about SCOPE's applicability to other model architectures and tasks beyond the tested decoder-only LLMs and specific data-to-text/summarization datasets. The paper doesn't provide sufficient evidence for cross-architecture generalization.

## Next Checks

1. **Cross-architecture validation:** Implement SCOPE on an encoder-decoder model (T5 or BART) and evaluate faithfulness improvements on the same datasets. Compare performance against the decoder-only results to quantify architecture-specific limitations.

2. **Real vs synthetic hallucination comparison:** Generate a human-annotated evaluation set where annotators identify actual hallucinations in model outputs. Compare SCOPE's performance on this set versus its performance on the synthetic evaluation protocol to measure real-world effectiveness.

3. **Long-form generation robustness:** Test SCOPE on tasks requiring longer outputs (e.g., multi-paragraph summarization or long-form data-to-text generation). Measure faithfulness degradation over sequence length and identify whether the mixture decoding mechanism maintains effectiveness for extended generations.