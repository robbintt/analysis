---
ver: rpa2
title: 'FIN-bench-v2: A Unified and Robust Benchmark Suite for Evaluating Finnish
  Large Language Models'
arxiv_id: '2512.13330'
source_url: https://arxiv.org/abs/2512.13330
tags:
- fbv2
- vastaus
- task
- finbench
- fin-bench
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: FIN-bench-v2 addresses the lack of comprehensive, high-quality
  Finnish language benchmarks for evaluating large language models. The authors modernize
  and expand the original FIN-bench by converting all datasets to HuggingFace Datasets
  format, implementing five prompt variants for each task, and adding new Finnish
  benchmarks.
---

# FIN-bench-v2: A Unified and Robust Benchmark Suite for Evaluating Finnish Large Language Models

## Quick Facts
- arXiv ID: 2512.13330
- Source URL: https://arxiv.org/abs/2512.13330
- Reference count: 40
- Unified Finnish LLM benchmark suite with learning curve-based task filtering and dual prompt formulations

## Executive Summary
FIN-bench-v2 addresses the lack of comprehensive, high-quality Finnish language benchmarks for evaluating large language models. The authors modernize and expand the original FIN-bench by converting all datasets to HuggingFace Datasets format, implementing five prompt variants for each task, and adding new Finnish benchmarks. To ensure robustness, they pretrain 2.15B-parameter decoder-only models and use learning curves to assess monotonicity, signal-to-noise, non-random performance, and model ordering consistency, retaining only tasks meeting all criteria. Human annotation is applied to machine-translated resources like GoldenSwag and XED. Evaluation on both purpose-trained and larger instruction-tuned models shows that task formulation significantly impacts performance, with multiple-choice prompts sometimes improving or degrading results depending on the model and task. The benchmark suite is publicly released and designed for both post-training and intermediate pre-training evaluation.

## Method Summary
The methodology involves converting Finnish datasets to HuggingFace format, creating 5 cloze and 5 multiple-choice prompt variants per task, pretraining 2.15B-parameter decoder-only models on Finnish corpora, and using learning curve analysis to filter tasks based on four quality metrics. The framework evaluates both purpose-trained and larger instruction-tuned models using the dual prompt formulations, with human review applied to machine-translated datasets to reduce translation artifacts.

## Key Results
- Learning curve filtering successfully identifies high-quality tasks, excluding those with unstable or random performance
- Task formulation (cloze vs multiple-choice) significantly impacts model performance, with formulation effects varying by model type
- Human annotation of machine-translated datasets reduces systematic translation artifacts
- The benchmark suite shows robust performance evaluation across different model scales and architectures

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Task quality can be systematically validated using learning curve metrics from small pretrained models
- Mechanism: Four quantitative criteria—monotonicity (Spearman's ρ ≥ 0.5), signal-to-noise ratio (SNR > 0), non-random performance (NRC ≥ 0), and model ordering consistency (τ ≥ 0.7)—filter tasks that exhibit stable learning signals from those with volatile or random behavior
- Core assumption: Assumption: Tasks that show reliable learning dynamics at 2.15B parameter scale will remain valid for evaluating larger models
- Evidence anchors:
  - [abstract] "we pretrain a set of 2.15B-parameter decoder-only models and use their learning curves to compute monotonicity, signal-to-noise, non-random performance, and model ordering consistency, retaining only tasks that satisfy all criteria"
  - [section 4.2.5] "we check if the task meets all the described criteria, i.e., ρ≥0.5, SNRAgg > 0, NRC≥0, and τ-Consistency≥0.7"
  - [corpus] Related work on Finnish SQuAD translation (arXiv:2501.05963) demonstrates translation quality issues can affect downstream task validity, supporting the need for systematic quality filtering
- Break condition: Tasks failing any single criterion are excluded; remedial k-shot prompting (+5% performance) was insufficient to rescue failed tasks

### Mechanism 2
- Claim: Dual prompt formulation captures fundamentally different model behaviors
- Mechanism: Cloze Formulation (CF) presents tasks as open completion, benefiting base models; Multiple-choice Formulation (MCF) embeds answer options in the prompt, benefiting instruction-tuned models by constraining the output space
- Core assumption: Assumption: Performance differences between CF and MCF reflect genuine capability differences rather than artifact
- Evidence anchors:
  - [abstract] "include both cloze and multiple-choice prompt formulations with five variants per task"
  - [section 2] "while instruction-tuned models benefit from answer choices embedded in the prompt (MCF), base models typically demonstrate superior performance with standard cloze-style completions"
  - [corpus] Weak corpus evidence on this specific mechanism; related benchmarks (NbBench, EEG-FM-Bench) use single-formulation approaches
- Break condition: Some tasks show formulation-invariant performance (ScandiSent ceiling at 0.92+ for all models); GoldenSwag MCF collapses to near-random in zero-shot while CF performs well

### Mechanism 3
- Claim: Human review of machine-translated content corrects systematic translation artifacts
- Mechanism: Machine translation introduces stylistic patterns that artificially inflate performance for models trained on translated synthetic data; manual annotation/removal of erroneous samples and human review reduces this bias
- Core assumption: Assumption: Human annotation sufficiently identifies and corrects translation artifacts
- Evidence anchors:
  - [abstract] "we incorporate human annotation or review for machine-translated resources such as GoldenSwag and XED"
  - [section 5.2] "the MultiSynt model, which trained exclusively on synthetic data translated from Nemotron-CC high-quality English samples, consistently outperformed models trained on human-authored data... most of examined tasks are translated, they likely share specific stylistic features and artifacts with the MultiSynt training data"
  - [corpus] Finnish SQuAD paper (arXiv:2501.05963) directly addresses Finnish MT quality for span-annotated datasets, supporting this concern
- Break condition: Residual translation artifacts may remain even after review; cultural/linguistic bias from Anglocentric source content is not addressed by translation correction alone

## Foundational Learning

- Concept: Learning curve monotonicity via Spearman correlation
  - Why needed here: Distinguishes tasks with genuine learning signal from those with random/volatile performance during training
  - Quick check question: If model performance on a task fluctuates up and down across training checkpoints, what does a low Spearman ρ value indicate about task reliability?

- Concept: Signal-to-noise ratio across prompt variants
  - Why needed here: Ensures task performance is robust to minor prompt wording changes; high variance across prompts suggests brittleness
  - Quick check question: Why does the framework aggregate SNR across five prompt variants using median rather than mean?

- Concept: Kendall's τ for rank stability
  - Why needed here: Validates that early-training model rankings predict late-training rankings; enables efficient early stopping decisions
  - Quick check question: If τ-Consistency = 0.3 for a task, can you reliably use early checkpoints to decide which pretraining corpus is better?

## Architecture Onboarding

- Component map: HuggingFace Datasets -> LM Evaluation Harness fork -> Task definitions with CF/MCF × 5 variants -> Purpose-trained 2.15B models -> Large instruction-tuned models

- Critical path:
  1. Convert datasets → HuggingFace format with standardized fields
  2. Generate 5 CF + 5 MCF prompts per task (manual creation, 3 annotators)
  3. Train 2.15B models on separate corpora (100B tokens each)
  4. Evaluate at checkpoints, compute 4 quality metrics
  5. Filter tasks meeting all criteria
  6. Evaluate large models on surviving tasks

- Design tradeoffs:
  - Inclusive retention: If any prompt variant passes criteria, all variants kept (preserves diversity but may include weak prompts)
  - Single-metric selection: For generative tasks with multiple metrics, only one chosen for criteria assessment (may miss multi-dimensional quality issues)
  - k-shot as remediation: Few-shot prompting attempted but abandoned (+5% insufficient)

- Failure signatures:
  - Monotonicity failure: Performance oscillates or degrades with more training
  - SNR failure: High variance across prompt variants (>20% spread suggests brittleness—see Belebele MCF range 0.37–0.57)
  - NRC = 0: Model never exceeds random baseline (e.g., paraphrase, empirical judgments)
  - τ failure: Model rankings shuffle unpredictably across checkpoints

- First 3 experiments:
  1. Run the 4 quality metrics on a single task (ARC-Challenge) across all 5 purpose-trained models to reproduce the paper's validation; confirm all criteria pass
  2. Compare CF vs MCF performance on a Finnish-specialized model (Poro) vs generalist model (Gemma) to observe the formulation sensitivity pattern the paper reports
  3. Test prompt variant sensitivity: Run all 5 variants of Belebele MCF on one large model to verify the 0.37–0.57 spread, then investigate which prompt features correlate with higher scores

## Open Questions the Paper Calls Out

### Open Question 1
- Question: To what extent does the observed collapse of GoldenSwag MCF performance to random baseline in zero-shot settings (while CF exceeds 0.60) reflect fundamental incompatibilities between certain commonsense reasoning tasks and multiple-choice prompting for Finnish models?
- Basis: [inferred] The authors report that "GoldenSwag stands out as an exception in the zero-shot setting. For all four models, CF scores clearly exceed 0.60, whereas the corresponding MCF scores remain close to the random baseline."
- Why unresolved: This behavior contradicts expectations that instruction-tuned models benefit from MCF, and the paper offers no explanation for this task-specific anomaly.
- What evidence would resolve it: Controlled experiments systematically varying option format, number of choices, and prompt structure on GoldenSwag to identify the source of the CF-MCF divergence.

### Open Question 2
- Question: How can Finnish benchmark datasets be culturally adapted beyond linguistic translation to appropriately assess Finnish-specific knowledge and reasoning without penalizing models aligned with Finnish cultural context?
- Basis: [explicit] The authors state: "Translating a dataset linguistically does not necessarily adapt the cultural context or the 'common sense' assumptions inherent in the questions. Therefore, the benchmark may penalize models that are culturally aligned with Finland but lack specific knowledge of US-centric history, law, or social norms present in the source datasets."
- Why unresolved: The paper acknowledges this limitation but provides no solution for cultural adaptation of translated benchmarks.
- What evidence would resolve it: Development of natively Finnish benchmarks with culturally appropriate content, paired with comparative evaluation showing performance differences between translated and culturally-adapted versions.

### Open Question 3
- Question: What mechanisms explain the performance regression of the specialized Llama Poro 2 70B model when provided with one-shot examples on SQuAD FI (F1 dropping from 0.31 to 0.16), while generalist models showed substantial improvement?
- Basis: [inferred] The authors observe: "Llama Poro 2 70B exhibited performance regression in the one-shot setting (F1 dropping to 0.16)" whereas "Gemma 3 and Llama 4 Scout saw a substantial performance boost in SQuAD, with F1 scores doubling."
- Why unresolved: This counterintuitive behavior is documented but not analyzed or explained.
- What evidence would resolve it: Systematic study of few-shot learning dynamics across specialist and generalist models, including attention analysis and comparison of how formatting cues are processed differently.

### Open Question 4
- Question: How can data contamination be systematically detected and mitigated for Finnish benchmarks when evaluating models trained on undisclosed corpora?
- Basis: [explicit] The authors note: "Because many of the evaluated models (e.g., Gemma, Llama 3) are trained on undisclosed portions of the internet, it is impossible to guarantee that they have not seen the source datasets prior to evaluation. Consequently, high performance may partially reflect memorization rather than genuine reasoning capabilities."
- Why unresolved: The paper proposes strengthening contamination analysis as future work but offers no methodology.
- What evidence would resolve it: Development of Finnish-specific contamination detection techniques and creation of held-out benchmarks with controlled release timing relative to model training.

## Limitations

- Learning curve filtering at 2.15B scale may exclude tasks that require larger models to exhibit stable learning signals
- Residual translation artifacts may persist even after human review, potentially biasing results
- Inclusive retention of all prompt variants when any single variant passes criteria may preserve weak prompts alongside strong ones
- Cultural adaptation of translated benchmarks remains unaddressed, potentially penalizing culturally aligned models

## Confidence

**High Confidence**: The learning curve filtering methodology is sound and well-justified. The four quantitative criteria (monotonicity, SNR, NRC, τ-Consistency) are clearly defined, computationally implementable, and directly address the problem of identifying tasks with stable, non-random learning signals. The conversion to HuggingFace Datasets format is straightforward and reproducible.

**Medium Confidence**: The dual prompt formulation hypothesis is supported by the data but lacks strong theoretical grounding. While the paper shows CF vs MCF performance differences, the mechanism explaining why instruction-tuned models benefit from answer choices isn't fully developed. The human review process for machine-translated datasets is described but lacks detailed quality metrics or inter-annotator agreement statistics.

**Low Confidence**: The assumption that task quality validated at 2.15B scale will generalize to larger models has limited empirical support in the paper. The claim that residual translation artifacts systematically bias results is plausible but not definitively proven. The framework's ability to identify truly difficult versus artificially difficult tasks (due to translation artifacts or prompt brittleness) requires further validation.

## Next Checks

1. **Cross-Scale Validation**: Evaluate the learning curve filtering criteria on 7B-parameter models and compare task retention rates with the 2.15B results. This will determine whether the current filtering thresholds are scale-dependent and whether tasks excluded at smaller scales become viable at larger scales.

2. **Prompt Sensitivity Analysis**: Systematically vary individual prompt components (e.g., instruction wording, context length, answer formatting) across all five variants for each task to identify which prompt features drive performance differences. This will distinguish between genuine task difficulty and artifactual brittleness.

3. **Translation Artifact Quantification**: Implement automated stylistic analysis comparing machine-translated Finnish tasks with human-authored Finnish text to measure the magnitude of translation artifacts. This would complement the manual review process and provide quantitative evidence for the claimed systematic bias in translation-based benchmarks.