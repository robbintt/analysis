---
ver: rpa2
title: General-purpose AI models can generate actionable knowledge on agroecological
  crop protection
arxiv_id: '2512.11474'
source_url: https://arxiv.org/abs/2512.11474
tags:
- efficacy
- data
- reported
- management
- pest
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study compared the knowledge outputs of two AI models on non-chemical
  pest management. Web-grounded DeepSeek consistently screened 4.8-49.7x more literature
  and reported 1.6-2.4x more solutions than non-grounded ChatGPT, yielding 21.6% higher
  efficacy estimates and greater data consistency.
---

# General-purpose AI models can generate actionable knowledge on agroecological crop protection

## Quick Facts
- arXiv ID: 2512.11474
- Source URL: https://arxiv.org/abs/2512.11474
- Reference count: 0
- AI models compared on non-chemical pest management knowledge outputs

## Executive Summary
This study evaluated two large language models (LLMs) on generating actionable knowledge about agroecological crop protection. Web-grounded DeepSeek-R1 consistently screened 4.8-49.7x more literature and reported 1.6-2.4x more solutions than non-grounded ChatGPT-4o free-tier. DeepSeek also produced 21.6% higher efficacy estimates with greater consistency between laboratory and field data. Despite both models generating some fictitious content, they captured correct broad trends, suggesting LLMs can effectively democratize and accelerate agroecological decision support with appropriate human oversight.

## Method Summary
The study compared web-grounded DeepSeek-R1 with non-grounded ChatGPT-4o on 9 pest/disease/weed management tasks across multiple geographies. Standardized prompts queried efficacy of 3-5 management tactics per pest, requesting efficacy metrics (mean ± SD, range) for laboratory, greenhouse, and field conditions. Outputs were evaluated for factual accuracy, data consistency (lab-to-field regression R²), breadth of knowledge (solutions/agents, publications screened), and efficacy estimates. Statistical comparisons used paired t-tests, ANOVA, and linear regression in IBM SPSS 30.0.

## Key Results
- Web-grounded DeepSeek screened 4.8-49.7x more literature than non-grounded ChatGPT
- DeepSeek reported 1.6-2.4x more management solutions across all tasks
- DeepSeek achieved 21.6% higher efficacy estimates with greater laboratory-to-field data consistency

## Why This Works (Mechanism)

### Mechanism 1: Real-time Retrieval Scaling
- **Claim:** Web-grounded LLMs retrieve substantially broader literature corpora than non-grounded models for scientific synthesis tasks.
- **Mechanism:** DeepSeek's real-time web access during inference enables screening of multi-database sources—including Chinese-language databases (Wanfang, CQVIP) and institutional repositories—that are not equally accessible to non-grounded models relying on training data alone.
- **Core assumption:** Real-time web retrieval surfaces more diverse and current sources than static training corpora can capture.
- **Evidence anchors:** Web-grounded DeepSeek consistently screened 4.8-49.7x more literature and reported 1.6-2.4x more solutions than non-grounded ChatGPT. Table 1 documents DeepSeek accessed Wanfang Data, CQVIP, and provincial agricultural bulletins that ChatGPT did not screen.

### Mechanism 2: Aggregate-Estimate Consistency
- **Claim:** Larger retrieved literature bases produce more consistent extrapolations from laboratory to field efficacy data.
- **Mechanism:** Aggregating efficacy values across more studies reduces variance and captures realistic effect modifiers (pest identity, management tactic) in regression models.
- **Core assumption:** More observations yield aggregate estimates that better reflect real-world heterogeneity.
- **Evidence anchors:** DeepSeek reported 21.6% higher efficacy estimates, exhibited greater laboratory-to-field data consistency. For DS-reported average values, field efficacy could be explained by multivariate regression models (R² = 0.869) with statistically significant effects for laboratory efficacy, pest identity, and management tactic.

### Mechanism 3: Hallucination Co-occurrence with Legitimate Synthesis
- **Claim:** Both web-grounded and non-grounded LLMs generate plausible-sounding but fictitious scientific content alongside accurate trend summaries.
- **Mechanism:** Pattern completion on scientific nomenclature and ecological relationships optimizes for linguistic plausibility rather than verified ground truth.
- **Core assumption:** LLMs lack intrinsic verification against authoritative databases.
- **Evidence anchors:** Both models hallucinated, i.e., fabricated fictitious agents or references, reported on implausible ecological interactions. DeepSeek also "hallucinated" non-existent agents—such as B. tabaci NPV whereas no DNA viruses are known from this pest.

## Foundational Learning

- **Concept: Web-grounded vs Non-grounded Inference**
  - **Why needed here:** Explains the 4.8-49.7x literature coverage difference between DeepSeek and ChatGPT.
  - **Quick check question:** Can the model retrieve new sources at inference time, or is it limited to its training corpus?

- **Concept: Hallucination Taxonomy in Scientific Synthesis**
  - **Why needed here:** The paper documents three failure modes—fabricated agents, false references, implausible ecological interactions—all requiring different verification strategies.
  - **Quick check question:** Name two specific hallucination types the authors detected in both models.

- **Concept: PRISMA Flow for LLM Literature Reviews**
  - **Why needed here:** The study adapted systematic review methodology to benchmark LLM retrieval breadth across seven PRISMA stages.
  - **Quick check question:** At which PRISMA stage was the data disparity between engines highest?

## Architecture Onboarding

- **Component map:**
  - Prompt engineering layer -> LLM inference engine (grounded or non-grounded) -> Multi-database retrieval -> Extraction/synthesis module -> Human verification interface -> Output formatting
  - External knowledge bases: ISI databases, regional repositories (CNKI, Wanfang), specialized agricultural databases (CAB Abstracts, AGRIS)

- **Critical path:**
  1. Structured prompt design with explicit inclusion/exclusion criteria
  2. Retrieval breadth measurement (publications screened, databases accessed)
  3. Extraction accuracy (efficacy values, agent names, references)
  4. Expert verification of outputs against known literature

- **Design tradeoffs:**
  - Breadth vs. precision: DeepSeek's larger corpus increased coverage but did not eliminate hallucinations
  - Automation vs. oversight: Both models required human fact-checking; fully automated decision support remains unsafe
  - Free-tier vs. paid access: ChatGPT's free-tier may degrade to older models under complex queries

- **Failure signatures:**
  - Fabricated biological control agents (e.g., "B. tabaci NPV")
  - Old/new nomenclature confusion (Paecilomyces fumosoroseus vs. Isaria fumosorosea treated as distinct species)
  - Implausible lab-to-field claims (e.g., hedgerow deployment reducing H. armigera in "laboratory" conditions)
  - Complete omission of relevant agent guilds (ChatGPT omitted predators for P. xylostella)

- **First 3 experiments:**
  1. **Retrieval breadth benchmark:** Run identical structured prompts on both engines; count publications screened and databases accessed per PRISMA stage.
  2. **Hallucination detection audit:** Manually verify 10 randomly selected agent-reference pairs against Google Scholar and taxonomic databases.
  3. **Efficacy estimate calibration:** Compare AI-reported efficacy ranges for 5 well-documented biocontrol agents against published meta-analyses.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How does LLM-generated pest management knowledge compare to formal, human-conducted systematic literature reviews in terms of accuracy and completeness?
- **Basis in paper:** The authors state that "model performance lacks validation against formal knowledge syntheses and human-run reviews" and acknowledge that their evaluation did not benchmark AI outputs against expert-curated syntheses.
- **Why unresolved:** This study only compared two AI models against each other, not against ground-truth human expert reviews, leaving the absolute accuracy of either model undetermined.
- **What evidence would resolve it:** A side-by-side comparison of LLM outputs with traditional systematic reviews covering the same pest-tactic combinations, using expert verification of factual claims.

### Open Question 2
- **Question:** To what extent do paid-tier versions of ChatGPT and specialized academic plugins (e.g., ScholarAI) narrow the performance gap with web-grounded models like DeepSeek?
- **Basis in paper:** The authors hypothesize that ChatGPT's poor performance may stem from the free-tier version downgrading to older models during complex queries, and note this could "widen the 'digital divide' with scientists and other stakeholders in low-income countries."
- **Why unresolved:** Only free-tier ChatGPT-4o was tested; subscription versions or research-focused plugins were not evaluated.
- **What evidence would resolve it:** A comparative study testing free vs. paid ChatGPT versions and academic plugins on identical agroecological knowledge retrieval tasks.

### Open Question 3
- **Question:** What human oversight protocols and evaluation frameworks are needed to safely integrate LLMs into agricultural decision support systems?
- **Basis in paper:** The authors conclude that "human oversight, rigorous fact-checking and robust evaluation frameworks are imperative" and warn of risks if AI outputs are integrated into proprietary systems causing farmer "deskilling."
- **Why unresolved:** The paper identifies the need but does not propose, test, or validate any specific oversight mechanisms or frameworks.
- **What evidence would resolve it:** Development and empirical testing of structured human-AI collaboration workflows for agricultural extension, measuring decision quality and error detection rates.

### Open Question 4
- **Question:** Can LLMs be fine-tuned or prompted to reliably distinguish between current and obsolete taxonomic nomenclature in agroecological literature?
- **Basis in paper:** The paper documents that "DeepSeek poorly distinguished between old and new nomenclature of biological control agents," treating Paecilomyces fumosoroseus and Isaria fumosorosea as distinct species when they represent old and new names for the same organism.
- **Why unresolved:** The error pattern was identified but no mitigation strategies were tested; the extent to which prompt engineering or domain-specific training could resolve this remains unknown.
- **What evidence would resolve it:** Testing nomenclature accuracy across prompts with explicit nomenclature instructions, or evaluation of domain-fine-tuned models on taxonomic consistency tasks.

## Limitations
- Model version drift: LLM outputs are non-deterministic and sensitive to version updates
- Prompt ambiguity: Only two prompt templates provided; adaptation rules for remaining combinations are inferred
- Verification burden: Hallucination detection relied on manual expert review, which does not scale

## Confidence
- **High confidence**: Web-grounded models retrieve 4.8-49.7x more literature than non-grounded models; both models capture correct broad trends in agroecological knowledge
- **Medium confidence**: DeepSeek's 21.6% higher efficacy estimates reflect genuine retrieval advantages, but magnitude may be inflated by selective inclusion of unpublished/regional sources
- **Low confidence**: The paper's claim that "LLMs can effectively democratize and accelerate agroecological decision support" lacks empirical validation of downstream decision-making impacts

## Next Checks
1. **Hallucination audit**: Systematically verify 20 randomly selected agent-reference pairs from both models against Google Scholar and taxonomic databases
2. **Output variability test**: Run 5 replicate queries per pest-tactic combination on both models; report coefficient of variation for efficacy means and solution counts
3. **Decision support trial**: Have domain experts use LLM outputs to design pest management plans for 2 test cases; measure plan quality and time savings versus traditional literature review