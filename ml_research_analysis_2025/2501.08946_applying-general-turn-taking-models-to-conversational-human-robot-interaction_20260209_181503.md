---
ver: rpa2
title: Applying General Turn-taking Models to Conversational Human-Robot Interaction
arxiv_id: '2501.08946'
source_url: https://arxiv.org/abs/2501.08946
tags:
- robot
- user
- turn-taking
- system
- should
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper applies general turn-taking models, TurnGPT and Voice
  Activity Projection (VAP), to improve human-robot interaction. These models are
  trained on large datasets of human-human dialogue without domain-specific fine-tuning.
---

# Applying General Turn-taking Models to Conversational Human-Robot Interaction

## Quick Facts
- **arXiv ID:** 2501.08946
- **Source URL:** https://arxiv.org/abs/2501.08946
- **Reference count:** 40
- **Primary result:** Proposed tandem system (VAP + TurnGPT) significantly reduced response delays and interruptions in human-robot interaction compared to baseline.

## Executive Summary
This paper introduces a turn-taking system for human-robot interaction that combines two general models—TurnGPT (syntactic/pragmatic) and Voice Activity Projection (VAP, acoustic/prosodic)—trained on large human-human dialogue datasets without domain-specific fine-tuning. The system predicts when to prepare responses, take turns, and handle interruptions by analyzing both verbal and acoustic cues in tandem. Evaluated against a traditional baseline using the Furhat robot with 39 participants, the proposed system significantly reduced response delays (median 1.5s vs 2.7s) and interruption rates (6.9% vs 16.6%). Participants also significantly preferred the proposed system for its more natural and human-like conversational flow.

## Method Summary
The system integrates two pre-trained models: TurnGPT (a GPT-2 extension trained on 385K text conversations from the SODA dataset) for syntactic/pragmatic turn completion prediction, and VAP (a Transformer model trained on ~1,000 hours of stereo audio from Fisher Part 1 and Switchboard) for acoustic/prosodic turn projection. These models work in tandem through a control loop that combines VAP's short-term and future-projection probabilities with TurnGPT's turn completion probabilities, using specific thresholds to trigger response preparation or turn shifts. The system uses streaming Google Speech-to-Text for ASR, OpenAI GPT-3.5-turbo for response generation, and ElevenLabs streaming TTS (voice 'Jennifer'). A critical feature is speculative response generation that begins before the user finishes speaking, with tentative responses canceled if the user continues.

## Key Results
- **Response delays reduced:** Median response time decreased from 2.7s (baseline) to 1.5s (proposed system).
- **Interruptions decreased:** Robot interruption rate dropped from 16.6% to 6.9%.
- **User preference:** Participants significantly preferred the proposed system for natural conversational flow.

## Why This Works (Mechanism)

### Mechanism 1
Combining text-based syntactic prediction (TurnGPT) with audio-based prosodic projection (VAP) allows for more accurate detection of turn-yielding than silence thresholds alone. TurnGPT analyzes incremental ASR text for pragmatic completeness, while VAP analyzes raw audio waveforms for prosodic cues in real-time. The system only initiates a turn shift when both models align—specifically, when VAP predictions favor the listener and TurnGPT probability exceeds a threshold—or when a fallback timeout is reached. Core assumption: Human turn-taking relies on distinct verbal and acoustic channels that can be modeled independently and combined late in the decision pipeline.

### Mechanism 2
Speculative response generation reduces perceived latency by overlapping cognitive processing with user speech. The system continuously generates tentative responses based on partial ASR results, triggering LLM/TTS generation if TurnGPT turn-shift probability exceeds 0.2 or 0.2 seconds pass. These tentative responses are canceled if the user continues speaking. Core assumption: Modern LLM/TTS pipelines are too slow (approx. 1.5s) to wait for a definitive turn-end signal without causing awkward delays; therefore, "guessing" the endpoint is necessary.

### Mechanism 3
Acoustic future-projection (VAP) enables the robot to distinguish collaborative backchannels from competitive interruptions. VAP outputs two probabilities: $p_{now}$ (0–600ms) and $p_{future}$ (600–2000ms). If $p_{now}$ and $p_{future}$ both favor the user, the system interprets it as an interruption and yields the floor. If only $p_{now}$ is active but $p_{future}$ favors the robot, the system identifies it as a backchannel and continues speaking. Core assumption: Backchannels are short, transient events that do not signal a desire to hold the floor, a distinction learnable from human-human audio data.

## Foundational Learning

- **Concept:** **Transition-Relevance Places (TRPs)**
  - **Why needed here:** The core objective of the system is to predict TRPs (points where turn change is possible) better than simple silence detection. Without understanding TRPs, the logic of combining syntax and prosody seems arbitrary.
  - **Quick check question:** Can you explain why a 1-second silence in the middle of a list does *not* constitute a TRP?

- **Concept:** **Incremental Speech Processing**
  - **Why needed here:** The system relies on "partial" ASR results to drive TurnGPT and response preparation. Engineers must understand that the system acts on evolving text, not just final transcripts.
  - **Quick check question:** How should the system handle a partial ASR result "I want to..." while the user is still speaking?

- **Concept:** **Self-Monitoring (Duplex Interaction)**
  - **Why needed here:** Unlike simplex systems (walkie-talkie style), this robot analyzes its *own* synthesized speech audio to determine if it is accidentally yielding the turn (e.g., via unintended prosody).
  - **Quick check question:** Why is it necessary to feed the robot's TTS output back into the VAP model?

## Architecture Onboarding

- **Component map:** Microphone (User Audio) + ASR (Text) -> Parallel Models: VAP (Stereo Audio -> $p_{now}$ / $p_{future}$) and TurnGPT (Text History -> Probability of Turn Completion) -> Logic Layer (State Machine combining VAP thresholds + TurnGPT timeouts) -> Action Layer: LLM/TTS Generator (Cancelable streams) -> Output: Robot Speech + Gaze (Furhat).

- **Critical path:** The loop between VAP detecting a turn shift and the Logic Layer checking if a Speculative Response is ready. If the LLM hasn't finished generating by the time VAP signals "speak," the system still incurs latency.

- **Design tradeoffs:**
  - Responsiveness vs. Interruption: Lowering the TurnGPT threshold triggers faster responses but risks interrupting the user.
  - Compute vs. Latency: Aggressive speculative generation reduces latency but increases API costs and compute load (canceling requests).
  - Robot cues: The paper trades "explicit" cues (Red LED in baseline) for "implicit" cues (Gaze aversion in proposed), relying on the model's accuracy to replace clear signaling.

- **Failure signatures:**
  - Dead Air: VAP fails to predict a turn yield, and TurnGPT probability stays low; system hits the 3.0s max timeout.
  - Barge-in Failure: User interrupts, but VAP $p_{future}$ predicts the robot should continue (false negative), causing the robot to talk over the user.
  - Flicker: Rapid start/stop of TTS due to oscillating VAP predictions in noisy environments.

- **First 3 experiments:**
  1. **Latency Floor Test:** Measure the minimum achievable response time (hardware + network limit) without speculative generation to establish a baseline.
  2. **Threshold Tuning (Sim):** Use recorded interactions to tune the 4 key hyperparameters (`VAP_PNOW_YIELD_THRESHOLD`, `VAP_PFUT_YIELD_THRESHOLD`, TurnGPT timeouts) to minimize the objective function: $Latency + \alpha \times InterruptionRate$.
  3. **A/B Test on Backchannels:** Specifically test the robot's reaction to "Yeah" vs. "Wait, stop" to verify the $p_{now}$ vs. $p_{future}$ disambiguation logic.

## Open Questions the Paper Calls Out

- **Question:** Does integrating visual cues (such as user gaze or gestures) into the Voice Activity Projection (VAP) model improve turn-taking prediction accuracy compared to the current audio-only implementation?
  - **Basis:** The authors state that recorded videos suggest it would be beneficial to utilize the user's gaze or other visual cues.
  - **Why unresolved:** The current study utilized models trained solely on audio (VAP) or text (TurnGPT), and the specific impact of adding visual input on prediction accuracy in this HRI context was not tested.
  - **What evidence would resolve it:** A comparative study evaluating the VAP model's performance when augmented with visual data versus the audio-only baseline, specifically within a face-to-face HRI setting.

- **Question:** Can the system further reduce perceived latency by implementing response strategies such as predicting utterance trajectories or generating non-committal fillers, compared to the current semantic similarity approach?
  - **Basis:** The authors suggest that to further reduce delays, one could use TurnGPT to "roll out different potential futures" or "produce fillers or other non-committing response prefixes."
  - **Why unresolved:** The current system prepares responses based on semantic similarity of incremental ASR results; the efficacy of the proposed projection or filler-generation strategies remains theoretical in this context.
  - **What evidence would resolve it:** An evaluation of response times and user perception comparing the current semantic-similarity preparation method against a system utilizing filler insertion and utterance projection.

- **Question:** How does user preference shift when the proposed system is combined with explicit non-humanlike cues (such as LED indicators) versus operating solely on human-like cues?
  - **Basis:** The authors note that some users preferred the baseline partly due to the LED light, concluding that "a combination of these features could also be a solution" and that "considering individual preferences will be important."
  - **Why unresolved:** The study compared a human-like system (Proposed) against a traditional system (Baseline), but did not test a hybrid system combining the proposed turn-taking model with explicit visual signaling.
  - **What evidence would resolve it:** A user study testing a hybrid condition that utilizes the VAP/TurnGPT models while simultaneously providing LED feedback, compared against the "Proposed" system in the paper.

- **Question:** Can general, self-supervised turn-taking models be effectively developed and applied to multi-party HRI settings where the robot must distinguish between different speakers?
  - **Basis:** The authors highlight that "for many HRI settings, multi-party interaction is important" and explicitly state that "there does not exist any general multi-party models, trained and applied in a similar way as in this paper."
  - **Why unresolved:** The current study was restricted to dyadic (two-party) interactions, and the models used (TurnGPT and VAP) were trained on two-speaker telephone conversations.
  - **What evidence would resolve it:** The development of a VAP or TurnGPT variant trained on multi-party data and its subsequent evaluation in a group setting involving more than one human interlocutor.

## Limitations

- **Unvalidated Acoustic-Pragmatic Alignment:** The actual quantitative improvement from combining VAP and TurnGPT predictions is not directly measured or reported.
- **Limited Participant Sample:** The user study included only 39 participants, which may not capture the full range of conversational styles and cultural differences in turn-taking behavior.
- **Resource-Intensive Inference:** The system requires real-time audio processing for VAP and continuous LLM inference for speculative response generation, potentially limiting deployment on resource-constrained platforms.

## Confidence

- **High Confidence:** The core finding that the proposed system reduces response delays (median 1.5s vs 2.7s) and interruption rates (6.9% vs 16.6%) compared to the baseline. This is supported by direct measurements in the user study.
- **Medium Confidence:** The claim that participants significantly preferred the proposed system for natural conversational flow. While statistically significant, this is based on subjective ratings that could be influenced by novelty effects or the specific robot embodiment used.
- **Medium Confidence:** The effectiveness of speculative response generation in reducing latency. The mechanism is sound, but the actual contribution to latency reduction versus the tandem model combination is not separately quantified.

## Next Checks

1. **Ablation Study Extension:** Conduct an ablation study specifically measuring the performance difference between using VAP and TurnGPT in tandem versus using either model alone, to quantify the claimed synergistic benefit.
2. **Cross-Cultural Validation:** Replicate the user study with participants from diverse cultural backgrounds to validate that the turn-taking improvements generalize beyond the initial participant pool.
3. **Resource Efficiency Benchmarking:** Measure the computational and energy costs of the proposed system compared to the baseline, particularly the overhead of continuous speculative generation and duplex audio processing.