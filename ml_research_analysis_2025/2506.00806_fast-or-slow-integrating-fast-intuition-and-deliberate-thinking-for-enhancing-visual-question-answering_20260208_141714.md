---
ver: rpa2
title: Fast or Slow? Integrating Fast Intuition and Deliberate Thinking for Enhancing
  Visual Question Answering
arxiv_id: '2506.00806'
source_url: https://arxiv.org/abs/2506.00806
tags:
- visual
- arxiv
- focus
- question
- mllms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of improving multimodal large
  language models' (MLLMs) performance on complex Visual Question Answering (VQA)
  tasks, where current approaches that indiscriminately add visual prompts to all
  detected objects degrade task performance due to excessive and unfocused visual
  markers. The core method, FOCUS, is inspired by Dual Process Theory and dynamically
  adapts to question complexity by combining fast intuitive judgments (System 1) for
  simple questions with deliberate analytical reasoning (System 2) for complex questions.
---

# Fast or Slow? Integrating Fast Intuition and Deliberate Thinking for Enhancing Visual Question Answering

## Quick Facts
- arXiv ID: 2506.00806
- Source URL: https://arxiv.org/abs/2506.00806
- Reference count: 16
- Primary result: FOCUS achieves state-of-the-art performance on ScienceQA, TextVQA, VizWiz, and MME benchmarks while reducing inference time by nearly 44%

## Executive Summary
This paper addresses the challenge of improving multimodal large language models' (MLLMs) performance on complex Visual Question Answering (VQA) tasks. Current approaches that indiscriminately add visual prompts to all detected objects degrade task performance due to excessive and unfocused visual markers. The proposed FOCUS method dynamically adapts to question complexity by combining fast intuitive judgments for simple questions with deliberate analytical reasoning for complex questions, using a "conceptualizing before observation" strategy that identifies and highlights critical visual elements.

## Method Summary
FOCUS is inspired by Dual Process Theory and dynamically adapts to question complexity by combining fast intuitive judgments (System 1) for simple questions with deliberate analytical reasoning (System 2) for complex questions. For complex questions, FOCUS employs a "conceptualizing before observation" strategy that identifies and highlights critical visual elements using segmentation models. The system uses a Question Complexity Evaluation module with self-consistency to classify questions, and for complex questions, extracts key textual elements using GPT-3.5-turbo and grounds them using Grounded-SAM before feeding the refined image to the MLLM.

## Key Results
- FOCUS combined with LLaVA-1.5-13B achieves state-of-the-art performance across all four datasets (ScienceQA, TextVQA, VizWiz, MME)
- FOCUS reduces inference time by nearly 44% compared to previous methods like SoM
- The approach consistently improves performance across both open-source and black-box MLLMs
- FOCUS demonstrates superior attention calibration by guiding models to relevant regions rather than distracting with excessive markers

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Adaptive processing reduces computational waste and error by selectively applying visual augmentation
- Mechanism: FOCUS uses a Question Complexity Evaluation module with high temperature and self-consistency to classify questions as simple (Fast Intuition) or complex (Deliberate Thinking), preventing indiscriminate annotation
- Core assumption: LLMs can reliably express confidence through "Answerable"/"Unanswerable" proxy, and simple questions don't benefit from visual markers
- Evidence anchors: Abstract states dynamic adaptation, page 2 describes self-consistency approach, Table 4 shows combined FOCUS outperforming individual paths, Figure 4 shows inference time reduction

### Mechanism 2
- Claim: Targeted visual grounding via "Conceptualizing before Observation" improves MLLM reasoning on complex tasks
- Mechanism: For complex questions, GPT-3.5-turbo extracts key textual elements, Grounded-SAM segments corresponding visual regions, and the MLLM receives the refined image
- Core assumption: Key concepts can be extracted from text, segmentation can accurately ground them, and providing segmented regions helps MLLM focus
- Evidence anchors: Abstract mentions highlighting critical elements, page 2 describes two-step process, Figure 3 visualizes attention shifts

### Mechanism 3
- Claim: Eliminating distractors improves perceptual accuracy and reasoning
- Mechanism: FOCUS only highlights specific regions relevant to extracted key concepts, filtering out irrelevant visual noise compared to SoM's indiscriminate annotation
- Core assumption: Annotating all objects distracts the model, fewer targeted markers are less distracting, MLLM attention can be calibrated by overlays
- Evidence anchors: Abstract criticizes excessive visual markers, Table 5 shows FOCUS outperforming SoM, Figure 3 compares attention maps

## Foundational Learning

- **Dual Process Theory (System 1 & System 2 Thinking)**: Why needed - Core theoretical inspiration for the dual-path architecture; Quick check - Can you explain the difference between System 1 and System 2 thinking and how FOCUS maps these concepts to its inference pipeline?

- **Visual Prompting & Grounding in MLLMs**: Why needed - "Deliberate Thinking" branch relies on creating visual prompts; Quick check - What is the purpose of visual grounding in VQA and how does FOCUS differ from Set-of-Mark (SoM)?

- **Self-Consistency**: Why needed - Technique used in Question Complexity Evaluation to mitigate overconfidence; Quick check - How does sampling multiple responses at high temperature and taking majority vote improve confidence estimation?

## Architecture Onboarding

- **Component map**: Input (image I, question Q) -> Question Complexity Evaluator (MLLM with high temp, N=3 samples, self-consistency) -> Path A (Fast Intuition: original I,Q to MLLM) OR Path B (Deliberate Thinking: Key Element Extractor (GPT-3.5-turbo) -> Visual Grounding Module (Grounded-SAM) -> Image Aggregator (overlays) -> Refined Inference (MLLM with I',Q))

- **Critical path**: Question Complexity Evaluation is most critical - misclassifying complex as simple bypasses visual grounding entirely; Visual Grounding Module accuracy is also critical

- **Design tradeoffs**: Stricter confidence threshold (all N responses) reduces efficiency gains but error; Segmentation model confidence threshold (0.7) balances coverage vs false positives; Temperature setting balances diversity vs cost

- **Failure signatures**: Wrong path (Fast Intuition when DT needed) leads to incorrect quick answers; Wrong markers (poor keyword extraction or grounding) create confusing overlays; No markers applied (segmentation uncertainty) degrades to baseline

- **First 3 experiments**: 1) Reproduce Question Complexity Classification - manually check "Unanswerable" correlations with difficult questions; 2) Ablate Segmentation Model - replace Grounded-SAM with basic detector to quantify targeted grounding value; 3) Visualize Attention Maps - compare baseline vs FOCUS attention shifts

## Open Questions the Paper Calls Out
1. Can FOCUS be adapted for fine-tuning/pre-training phases to address visual hallucinations at architectural level rather than inference-time processing?
2. Does integrating RAG into FOCUS pipeline significantly improve performance on domain-specific VQA tasks requiring external knowledge?
3. Can computational efficiency of Question Complexity Evaluation be improved by replacing self-consistency with semantic entropy calculations?

## Limitations
- FOCUS is a plug-and-play inference approach that doesn't address fundamental visual hallucination issues requiring architectural modifications
- The framework relies on external tools (segmentation models/LLMs) without updating MLLM's internal weights
- Performance depends on accuracy of language model keyword extraction and segmentation model grounding

## Confidence
- High Confidence: Experimental results showing consistent accuracy improvements and inference time reduction across multiple benchmarks
- Medium Confidence: Theoretical framing based on Dual Process Theory and qualitative attention visualization
- Low Confidence: Specific claim about performance degradation from indiscriminate annotation, as this is asserted rather than rigorously tested

## Next Checks
1. Human validation of complexity classification - have annotators label questions and compare to model classifications
2. Ablation of keyword extraction - replace GPT-3.5-turbo with simpler heuristics to measure extraction quality impact
3. Quantitative attention analysis - conduct KL divergence analysis between baseline and FOCUS attention distributions