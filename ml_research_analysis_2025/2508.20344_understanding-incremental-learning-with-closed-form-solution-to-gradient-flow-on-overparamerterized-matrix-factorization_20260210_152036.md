---
ver: rpa2
title: Understanding Incremental Learning with Closed-form Solution to Gradient Flow
  on Overparamerterized Matrix Factorization
arxiv_id: '2508.20344'
source_url: https://arxiv.org/abs/2508.20344
tags:
- matrix
- initialization
- learning
- incremental
- factorization
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper provides a quantitative understanding of the incremental
  learning phenomenon in gradient flow on symmetric matrix factorization problems.
  Using a closed-form solution derived from solving a Riccati-like matrix differential
  equation, the authors show that incremental learning emerges from time-scale separation
  among dynamics corresponding to different components of the target matrix.
---

# Understanding Incremental Learning with Closed-form Solution to Gradient Flow on Overparamerterized Matrix Factorization

## Quick Facts
- arXiv ID: 2508.20344
- Source URL: https://arxiv.org/abs/2508.20344
- Authors: Hancheng Min; René Vidal
- Reference count: 25
- Key outcome: Provides closed-form solution showing incremental learning emerges from time-scale separation in gradient flow on symmetric matrix factorization

## Executive Summary
This paper provides a quantitative understanding of incremental learning in gradient flow on symmetric matrix factorization. Using a closed-form solution derived from solving a Riccati-like matrix differential equation, the authors show that incremental learning emerges from time-scale separation among dynamics corresponding to different components of the target matrix. They prove that under small random initialization, larger singular values of the target matrix are learned first, followed by smaller ones, with the time intervals for learning rank-k approximations precisely characterized.

## Method Summary
The authors analyze gradient flow on symmetric matrix factorization problems by deriving a closed-form solution to the resulting matrix Riccati differential equation. They show that the dynamics of the reconstructed matrix W(t) = U(t)U^T(t) can be solved explicitly using matrix inversion and eigenvalue decomposition techniques. The analysis reveals that the learning process exhibits time-scale separation, where components corresponding to larger singular values of the target matrix are learned faster than those with smaller singular values.

## Key Results
- Gradient flow learns singular values of target matrix sequentially in descending order rather than simultaneously
- Low-rank approximations of target matrix emerge at specific, predictable time intervals during training
- Over-parameterization combined with small initialization acts as implicit regularizer to find exact target

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Gradient flow learns the singular values of a target matrix sequentially in descending order, rather than simultaneously.
- **Mechanism:** The closed-form solution to the matrix Riccati differential equation reveals that the transition time for learning a singular value $\sigma_{i,Y}$ scales as $\Theta(\frac{1}{\sigma_{i,Y}} \log \frac{1}{\alpha})$. Since larger singular values have faster transition dynamics (smaller time constants), they saturate to their target values while smaller singular values remain near zero.
- **Core assumption:** Initialization scale $\alpha$ is sufficiently small; the problem uses symmetric matrix factorization.
- **Evidence anchors:**
  - [Abstract] ("incremental learning emerges from some time-scale separation among dynamics...")
  - [Section III.A] (Eq. 14-15 showing transition time dependence on $\sigma_{i,Y}$)
  - [Corpus] (Limited direct evidence; mechanism is specific to this theoretical derivation.)
- **Break condition:** Initialization scale $\alpha$ is too large, causing transition phases to overlap and eliminating the sequential learning effect.

### Mechanism 2
- **Claim:** Low-rank approximations of the target matrix emerge at specific, predictable time intervals during training.
- **Mechanism:** Due to time-scale separation, there exist non-empty time intervals $I_k$ between the learning of the $k$-th and $(k+1)$-th singular components. If training is stopped within $I_k$, the model $W(t)$ approximates the best rank-$k$ solution $\hat{Y}_k$ because the top $k$ components have been learned while the rest remain near zero.
- **Core assumption:** The singular values of target $Y$ are distinct; initialization scale $\alpha$ satisfies the condition that time intervals $I_k$ are non-empty.
- **Evidence anchors:**
  - [Section III.A] (Theorem 3 defining intervals $I_k$ and error bounds)
  - [Abstract] ("allowing one to find low-rank approximations of the target matrix")
  - [Corpus] (Not directly corroborated by provided neighbors.)
- **Break condition:** The ratio of consecutive singular values $\sigma_{k+1}/\sigma_k$ is too close to 1, compressing the interval $I_k$ to be empty.

### Mechanism 3
- **Claim:** Over-parameterization ($r \ge n$) combined with small initialization acts as an implicit regularizer to find the exact target.
- **Mechanism:** The convergence requires $\tilde{W}_0$ (the projected initialization) to be invertible. In the general initialization case (Theorem 4), this implies the initial factor $U_0$ must be full rank, which is only guaranteed if the factor dimension $r \ge n$. The small scale ensures the trajectory follows the incremental path rather than diverging.
- **Core assumption:** The target $Y$ is positive semi-definite (symmetric factorization); initialization shape $\bar{U}_0$ results in an invertible $\tilde{W}_0$.
- **Evidence anchors:**
  - [Section II.B] (Assumption that $W_0$ is invertible implies over-parametrization $r \ge n$)
  - [Section III.B] (Theorem 4 proof details relying on inverse of $V$)
  - [Corpus] (Consistent with general over-parameterization themes in neighbor papers.)
- **Break condition:** Rank deficiency in initialization (e.g., $r < n$) where $\tilde{W}_0$ is not invertible, potentially preventing convergence to the exact target.

## Foundational Learning

- **Concept:** Riccati Differential Equations
  - **Why needed here:** The core result derives from identifying the gradient flow dynamics as a matrix Riccati equation. Understanding the properties of these equations (e.g., potential for closed-form solutions via linearization) is required to follow the derivation.
  - **Quick check question:** Can you explain why the term $-2W^2$ in the dynamics $\dot{W} = WY + YW - 2W^2$ makes this a Riccati equation rather than a simple linear ODE?

- **Concept:** Time-Scale Separation
  - **Why needed here:** This is the central physical intuition. It explains why "incremental" learning happens (fast modes learn, slow modes wait) and how it enables early stopping as a regularization technique.
  - **Quick check question:** If two singular values $\sigma_1$ and $\sigma_2$ are very close ($\sigma_1 \approx \sigma_2$), would you expect a distinct "plateau" between their learning phases? Why or why not?

- **Concept:** Matrix Factorization & SVD
  - **Why needed here:** The entire analysis maps the learning dynamics onto the singular value decomposition of the target matrix $Y$.
  - **Quick check question:** In the spectral initialization setting, why does the dynamics of the matrix $W$ decouple into independent scalar dynamics for each singular value?

## Architecture Onboarding

- **Component map:**
  - **Input:** Target matrix $Y \in \mathbb{R}^{n \times n}$ (symmetric, $Y \succeq 0$)
  - **Parameters:** Factor matrix $U \in \mathbb{R}^{n \times r}$ where $r \ge n$ (over-parameterized)
  - **State:** Reconstruction $W(t) = U(t)U^\top(t)$
  - **Controller:** Gradient Flow $\dot{U} = (Y - UU^\top)U$

- **Critical path:**
  1. **Initialization:** Set $U(0) = \alpha^{1/2} \bar{U}_0$ with tiny $\alpha$
  2. **Evolution:** Run gradient flow; monitor singular values of $W(t)$
  3. **Rank Selection:** Identify target rank $k$. Wait until time $t \in I_k$ (after $\sigma_k$ grows, before $\sigma_{k+1}$ grows)
  4. **Output:** Extract $U(t)$ as the low-rank factor

- **Design tradeoffs:**
  - **Initialization Scale ($\alpha$):** Decreasing $\alpha$ increases the "signal-to-noise" ratio of the incremental phases (cleaner rank selection) but slows down the initial learning phase (logarithmic delay $\log \frac{1}{\alpha}$)
  - **Over-parameterization ($r$):** While $r \ge n$ is required for the invertibility assumption in the general theory, setting $r$ excessively large may impact the condition number of $\tilde{W}_0$ (denoted $M$ in Theorem 4), potentially widening the error bounds

- **Failure signatures:**
  - **Simultaneous Jump:** Singular values rise together $\to$ Initialization scale $\alpha$ is too large
  - **Missing Component:** A specific singular value of $Y$ is never learned $\to$ Initialization shape $\bar{U}_0$ likely lacks diversity (rank deficient in the signal subspace)
  - **Divergence:** Solution does not converge $\to$ Check if $Y$ is symmetric PSD or if numerical instability occurs in the Riccati inversion

- **First 3 experiments:**
  1. **Validation of Transition Time:** Implement the gradient flow on a synthetic $Y$ with known distinct singular values. Plot $\sigma_{i,W}(t)$ over time to verify the "S-curve" transitions occur at $t \approx \frac{1}{2\sigma_{i,Y}} \log \frac{1}{\alpha}$
  2. **Rank-k Approximation Accuracy:** Fix a target rank $k$. Run the flow for varying $\alpha$. Measure $\|W(t) - \hat{Y}_k\|$ at the theoretical midpoint of interval $I_k$ to verify the error bound $\le \epsilon$
  3. **General vs. Spectral Init:** Compare the trajectory of a random small initialization vs. a perfectly aligned spectral initialization to observe the "perturbation" effects described in the general initialization analysis

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the closed-form solution analysis be extended to asymmetric matrix factorization problems?
- Basis in paper: [explicit] The abstract and conclusion explicitly list extending the analysis to asymmetric matrix factorization as a primary avenue for future work.
- Why unresolved: The current derivations rely on the properties of symmetric matrix differential equations (Riccati), whereas asymmetric problems involve different dynamic constraints.
- What evidence would resolve it: A derivation of closed-form solutions or similar quantitative time-scale separation bounds for gradient flow on asymmetric matrix factorization.

### Open Question 2
- Question: What specific concentration bounds on the initialization shape are required to guarantee incremental learning under random initialization?
- Basis in paper: [explicit] Section III.C.1 states that applying the results to random initialization requires "an extra concentration results on the matrix $\bar{W}_0$" to satisfy the invertibility and norm conditions.
- Why unresolved: The main theorems assume a deterministic, invertible initialization shape; random matrices require probabilistic guarantees regarding norm bounds.
- What evidence would resolve it: A formal proof showing that for random initialization, the required norm conditions hold with high probability, along with the resulting probabilistic bounds.

### Open Question 3
- Question: Does the incremental learning phenomenon persist under rank-deficient initialization, and how does the analysis change?
- Basis in paper: [explicit] Section III.C.2 notes that the current proof assumes invertibility of the initialization shape and that analyzing the rank-deficient case "requires significant changes in the proof... left to future work."
- Why unresolved: The current solution inversion step fails if the initialization shape is rank-deficient, necessitating an analysis based on Woodbury's matrix identity.
- What evidence would resolve it: A modified proof using Woodbury's identity or similar techniques that establishes error bounds for $\tilde{W}_0$ without the full-rank assumption.

## Limitations

- The theoretical analysis relies on idealized conditions including continuous gradient flow rather than discrete SGD
- The closed-form solution applies only to symmetric positive semi-definite targets, limiting applicability to general deep learning problems
- The time-scale separation analysis assumes distinct singular values in the target matrix, which may not hold for many real-world datasets where spectral gaps are small

## Confidence

- **High Confidence**: The mathematical derivation of the Riccati equation solution and the basic time-scale separation phenomenon
- **Medium Confidence**: The existence and characterization of non-empty time intervals I_k
- **Medium Confidence**: The general initialization theory and over-parameterization requirements

## Next Checks

1. Implement the gradient flow on synthetic matrices with varying spectral gaps to empirically verify the existence and width of time intervals I_k, testing the theoretical prediction that these intervals shrink as consecutive singular values become closer

2. Test the robustness of incremental learning to different initialization distributions (Gaussian vs. orthogonal) and scales, measuring how initialization scale α affects the smoothness of singular value transitions and the ability to extract rank-k approximations

3. Extend the analysis to asymmetric matrix factorization problems and compare the learning dynamics to the symmetric case, identifying which properties of the closed-form solution generalize and which are specific to the symmetric PSD case