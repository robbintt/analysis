---
ver: rpa2
title: 'Poolformer: Recurrent Networks with Pooling for Long-Sequence Modeling'
arxiv_id: '2510.02206'
source_url: https://arxiv.org/abs/2510.02206
tags:
- layers
- layer
- pooling
- page
- figure
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This thesis introduces Poolformer, a sequence-to-sequence model
  designed for extremely long sequences that replaces self-attention with recurrent
  layers and incorporates pooling operations to reduce sequence length. The model
  is defined recursively using SkipBlocks containing residual blocks, down-pooling,
  nested SkipBlocks, up-pooling, and additional residual blocks.
---

# Poolformer: Recurrent Networks with Pooling for Long-Sequence Modeling

## Quick Facts
- arXiv ID: 2510.02206
- Source URL: https://arxiv.org/abs/2510.02206
- Authors: Daniel Gallo Fernández
- Reference count: 0
- Primary result: Outperforms SaShiMi and Mamba on perceptual metrics for raw audio generation while achieving competitive negative log-likelihood

## Executive Summary
Poolformer is a sequence-to-sequence model designed for extremely long sequences that replaces self-attention with recurrent layers and incorporates pooling operations to reduce sequence length. The model uses a hierarchical architecture with SkipBlocks containing residual blocks, down-pooling, nested SkipBlocks, up-pooling, and additional residual blocks. Poolformer was evaluated on raw audio data (SC09, Beethoven, YouTubeMix) and demonstrated that pooling greatly accelerates training, improves perceptual metrics (FID and IS), and prevents overfitting. The experiments suggest that long-range dependencies are handled by deeper layers while shallow layers focus on short-term features.

## Method Summary
Poolformer replaces self-attention with Real-Gated Linear Recurrent Units (RG-LRU) that provide linear complexity temporal mixing while remaining parallelizable via associative scan. The architecture uses hierarchical pooling with configurable down-sampling factors to reduce sequence length exponentially, enabling computationally feasible processing of long-range dependencies. Each SkipBlock applies strided convolution for down-pooling, processes the compressed representation through a nested SkipBlock, then restores via transposed convolution for up-pooling. The model uses sinusoidal embeddings for input tokens and includes residual connections with shape changes to handle pooling operations.

## Key Results
- Poolformer achieves FID of 0.46 and IS of 6.46 on SC09, outperforming Mamba's FID of 0.94 and IS of 6.26
- The baseline model achieves test negative log-likelihood of 1.854 on SC09, competitive with Mamba's 1.852
- Pooling accelerates training significantly (11.45 epochs/h vs 6.11 epochs/h without pooling) while improving perceptual metrics and preventing overfitting

## Why This Works (Mechanism)

### Mechanism 1: Hierarchical Pooling Creates Multi-Scale Temporal Representations
Pooling reduces sequence length exponentially through nested SkipBlocks, enabling computationally feasible processing of long-range dependencies at deeper layers. Each SkipBlock applies strided convolution (down-pooling) to reduce sequence length by factor F, processes the compressed representation through a nested SkipBlock, then restores via transposed convolution (up-pooling). With pooling config [2,4,4,5], the deepest layer operates on 160× fewer tokens. Long-range dependencies can be captured in downsampled representations without critical information loss.

### Mechanism 2: RG-LRU Provides Linear-Complexity Temporal Mixing with Parallel Training
The Real-Gated Linear Recurrent Unit replaces quadratic self-attention with O(S) complexity while remaining parallelizable via associative scan. RG-LRU uses input-dependent gates (r_k, i_k) and a learned decay parameter a_k that controls memory retention. The recurrence h_k = a_k ⊙ h_{k-1} + √(1-a²_k) ⊙ (i_k ⊙ x_k) can be parallelized using associative scan during training, computed as [A_k, B_k x_k] composition. The associative scan formulation with diagonal A matrix provides sufficient expressivity for temporal modeling.

### Mechanism 3: Depth-Based Temporal Specialization Emerges from Learned Decay Parameters
The model automatically learns to allocate short-term processing to shallow layers and long-term dependencies to deep layers, as measured by the magnitude of decay parameter |a|. Deeper layers (where sequence is shorter) learn |a| values closer to 1 (slower decay), while shallow layers learn smaller |a| values (faster decay). This creates a natural specialization without explicit constraints, emerging from the hierarchical architecture and training dynamics.

## Foundational Learning

- **Residual Connections with Shape Changes**: SkipBlocks require skip connections even when pooling changes sequence dimensions. Understanding how to route gradients around pooling operations is essential.
  - Quick check: Can you explain why standard residual connections fail when down-pooling is applied, and how short vs. long skip-connections differ?

- **Associative Scan / Parallel Prefix Sum**: Understanding how to parallelize inherently sequential recurrent computations is critical for efficient training.
  - Quick check: Given recurrence h_k = A·h_{k-1} + B·x_k, how would you lift this to an associative operation [M, c] for parallel scan?

- **Complex-Valued Recurrence Initialization**: The RG-LRU uses complex-valued decay parameters a = σ(Λ)·e^{iθ} with constrained initialization (small ring, not full circle) for stability.
  - Quick check: Why might initializing complex decay parameters with full circle (uniform angle) cause training instability compared to small ring initialization?

## Architecture Onboarding

- **Component map**: Input → Sinusoidal Embedding → SkipBlock[0]: [ResBlock×4 → DownPool(F=2) → SkipBlock[1] → UpPool → ResBlock×4] → SkipBlock[1]: [ResBlock×4 → DownPool(F=4) → SkipBlock[2] → UpPool → ResBlock×4] → SkipBlock[2]: [ResBlock×4 → DownPool(F=4) → SkipBlock[3] → UpPool → ResBlock×4] → SkipBlock[3]: [ResBlock×4 → DownPool(F=5) → ResBlock×4 → UpPool → ResBlock×4] → Output Layer

- **Critical path**: Pooling configuration [2,4,4,5] determines total compression (160×); initialization scale (scale=1/n_blocks for up-pooling and final ResBlock layers) is critical for stability; complex decay parameter initialization: magnitude from √U[0.9², 0.99²], phase from U[0, π/10]

- **Design tradeoffs**: LayerNorm vs RMSNorm: LayerNorm more stable; Gating: More parameters (7.3M vs 5.5M) but critical for stability; Pooling group count G: G=D (diagonal) is stable; G=1 (full matrix) is very unstable; Short vs long skip-connections: Short (around pooling) outperforms long

- **Failure signatures**: Gradient explosion with G=1 pooling or scale=1 initialization; Overfitting with insufficient pooling or large model dimension; Unstable training with full-circle complex initialization

- **First 3 experiments**: Reproduce baseline on SC09: [2,4,4,5] pooling, [4,4,4,4,4] layers, D=128, rnn_D=256, complex RG-LRU. Verify NLL ≈1.85, FID ≈0.46; Ablate pooling depth: Compare [2], [2,4], [2,4,4,5] configs on training speed and FID/IS to validate hierarchical pooling benefit; Visualize |a| magnitude across layers after training to confirm depth-based temporal specialization

## Open Questions the Paper Calls Out

### Open Question 1
Can Poolformer effectively handle modalities other than audio, specifically text and vision? The Abstract and Section 5.3 state that future directions include "applications to text and vision" and that further work is needed to assess effectiveness on other modalities. The thesis evaluations were restricted to raw audio datasets (SC09, Beethoven, YouTubeMix). Training Poolformer on standard text benchmarks (e.g., WikiText) or image datasets and comparing performance against state-of-the-art Transformers or SSMs.

### Open Question 2
Is Poolformer suitable for multi-modal scenarios, such as Vision-Language Models (VLMs) processing dense patch-level tokens? Section 5.3 suggests Poolformer could be used in multi-modal scenarios where a "Poolformer-based LLM could effectively process dense representations of images and videos." The paper proposes this application but provides no experiments or architectural details regarding multi-modal integration. Constructing a VLM using Poolformer to process dense patch tokens and evaluating it on vision-language tasks like image captioning or visual question answering.

### Open Question 3
How does Poolformer perform when operating on 16-bit audio samples rather than the 8-bit mu-law encoding used in the experiments? Section 2.1.1 explicitly states, "we use 8-bit samples, and defer 16-bit samples for future work." The authors limited the input vocabulary size to 256 ($2^8$) to match related literature and speed up experiments, leaving higher fidelity untested. Re-evaluating the model on the SC09 or YouTubeMix datasets using 16-bit linear PCM audio and comparing the resulting log-likelihoods and sample quality.

## Limitations
- The evaluation focuses on autoregressive generation rather than other sequence modeling tasks like classification or bidirectional modeling, limiting generalizability claims
- The mechanisms behind depth-based temporal specialization remain observational rather than theoretically proven
- The reliance on complex-valued RG-LRU introduces stability constraints that may not generalize to all sequence modeling tasks

## Confidence

**High Confidence**: Pooling configurations [2,4,4,5] provide 160× sequence compression enabling efficient long-sequence processing; RG-LRU with diagonal pooling (G=D) and proper initialization achieves stable training; LayerNorm + gating combination is critical for model stability

**Medium Confidence**: Depth-based temporal specialization (shallow layers for short-term, deep layers for long-term) is a learned property; Pooling improves perceptual metrics (FID/IS) while preventing overfitting; Poolformer outperforms SaShiMi and Mamba on perceptual metrics

**Low Confidence**: The model's superiority generalizes beyond audio to other sequence modeling domains; The associative scan parallelization provides significant practical speedups during training; Complex-valued decay parameters are essential rather than a design choice

## Next Checks

1. **Ablation study on temporal specialization**: Train a shallow variant of Poolformer (single SkipBlock) and measure whether |a| magnitude patterns still emerge, testing if depth-based specialization is truly emergent or an artifact of the architecture.

2. **Cross-domain evaluation**: Apply Poolformer to text sequence modeling (e.g., language modeling on WikiText-103) to test whether the perceptual quality advantages and depth-based specialization generalize beyond audio data.

3. **Theoretical analysis of RG-LRU stability**: Derive formal conditions under which the complex-valued decay parameter initialization (small ring, constrained magnitude) guarantees training stability, moving beyond empirical observation to provable guarantees.