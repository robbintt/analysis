---
ver: rpa2
title: 'Towards Generative Location Awareness for Disaster Response: A Probabilistic
  Cross-view Geolocalization Approach'
arxiv_id: '2512.20056'
source_url: https://arxiv.org/abs/2512.20056
tags:
- disaster
- geolocalization
- probglc
- imagery
- cross-view
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes ProbGLC, a novel Probabilistic Cross-view GeoLoCalization
  approach designed for rapid disaster response. ProbGLC integrates probabilistic
  and deterministic geolocalization models into a unified framework to enhance model
  explainability through uncertainty quantification while achieving state-of-the-art
  geolocalization performance.
---

# Towards Generative Location Awareness for Disaster Response: A Probabilistic Cross-view Geolocalization Approach

## Quick Facts
- arXiv ID: 2512.20056
- Source URL: https://arxiv.org/abs/2512.20056
- Reference count: 24
- One-line primary result: ProbGLC achieves 0.86 Acc@1km and 0.97 Acc@25km on disaster datasets

## Executive Summary
This paper introduces ProbGLC, a novel approach for rapid disaster response that combines probabilistic and deterministic geolocalization models into a unified framework. The system uses Riemannian Flow Matching on a spherical manifold to generate probabilistic location predictions, while simultaneously employing a Siamese Vision Transformer with DINOv2 encoder for deterministic cross-view image retrieval. By integrating these components with a threshold-based cascade architecture, ProbGLC achieves state-of-the-art performance on two disaster datasets (MultiIAN and SAGINDisaster) while providing uncertainty quantification through localizability scores.

## Method Summary
ProbGLC integrates a generative probabilistic geolocalization model using Riemannian Flow Matching on the sphere with a deterministic cross-view image retrieval system based on a Siamese Vision Transformer with DINOv2 encoder. The approach first generates a probabilistic location density distribution and localizability score, then uses this information to constrain the search space for the deterministic retrieval component. The system is trained on disaster-specific datasets and evaluated using accuracy metrics at multiple distance thresholds, along with localizability quantification through negative entropy estimation.

## Key Results
- Achieves 0.86 Acc@1km and 0.97 Acc@25km on MultiIAN dataset
- Outperforms baseline methods with substantial reduction in localization errors
- Provides probabilistic distributions and localizability scores for improved decision-making
- Demonstrates effectiveness across multiple disaster types (hurricanes, wildfires, floods, tornadoes)

## Why This Works (Mechanism)

### Mechanism 1: Spherical Flow Matching for Probabilistic Localization
The Riemannian Flow Matching model generates location predictions directly on the Earth's spherical surface rather than in Euclidean space, preventing distortion at poles and providing probability distributions instead of point estimates. It learns a velocity field on the spherical manifold that transports random noise to probable locations conditioned on image embeddings.

### Mechanism 2: Siamese Vision Transformer with DINOv2 for Cross-view Alignment
The system uses a Siamese network with DINOv2 encoder to project ground and satellite views into a shared embedding space, minimizing InfoNCE contrastive loss to align matching pairs. This creates robust embeddings that bridge the domain gap between street-level disaster imagery and overhead satellite views.

### Mechanism 3: Cascade Architecture for Accuracy-Explainability Trade-off
The hybrid architecture first uses the probabilistic model to estimate a location density and localizability score, then constrains the deterministic retrieval to search only within a threshold radius around the generative prediction. This approach leverages the strengths of both components while maintaining explainability through uncertainty quantification.

## Foundational Learning

### Concept: Diffusion & Flow Matching
**Why needed:** ProbGLC treats localization as generation rather than classification, creating coordinates through denoising flow. **Quick check:** Can you explain the difference between standard forward diffusion (adding Gaussian noise) and Flow Matching (learning a vector field to transport noise to data)?

### Concept: Contrastive Learning (InfoNCE)
**Why needed:** The deterministic branch relies on cross-view embedding alignment through positive/negative sample discrimination. **Quick check:** In a batch of N image pairs, how does InfoNCE loss penalize high similarity to negative samples?

### Concept: Riemannian Geometry (Sphere ð•ŠÂ²)
**Why needed:** Standard neural networks assume Euclidean space, but Earth is spherical, requiring special coordinate handling. **Quick check:** Why does minimizing loss in the tangent space of the sphere (â„Â³) ensure final coordinates remain on the surface (ð•ŠÂ²)?

## Architecture Onboarding

### Component map:
VGI Image -> Siamese ViT-DINOv2 Encoder -> Embedding c
RSI Image Database -> Siamese ViT-DINOv2 Encoder -> Embedding c
c -> Riemannian Flow Matching -> Probabilistic density P(x|c) + Localizability score
c -> Deterministic Retrieval -> Search results within threshold r

### Critical path:
The inference pipeline runs the Generative model first to estimate the Area of Interest (AOI), then queries the Deterministic index restricted to that AOI. If the Generative model fails (low localizability), the system may need to fall back to global retrieval.

### Design tradeoffs:
- Threshold r: Small radius increases precision but risks missing the target; large radius increases compute cost and false positives
- Pre-training Data: OSV-5M (street view) vs. YFCC (diverse) affects performance - OSV-5M performs better on disasters due to similar street-level semantics
- Backbone selection: DINOv2 provides strong generalizable features but may need adaptation for disaster-specific contexts

### Failure signatures:
- High Entropy/Low Localizability: Flat probability distribution indicating ambiguous input features
- Large Mean Distance: Generative model confident but wrong, placing AOI far from true location
- Retrieval failure: True location excluded from search space due to small threshold radius

### First 3 experiments:
1. Baseline Verification: Reproduce zero-shot results using pre-trained DINOv2 encoder on MultiIAN dataset
2. Threshold Ablation: Sweep radius r (1km to 200km) and plot Accuracy vs Threshold to find optimal value
3. Localizability Calibration: Visualize localizability scores for correctly vs incorrectly localized images to verify correlation with error

## Open Questions the Paper Calls Out

### Open Question 1: Vector Data Integration
How does incorporating vector data representations (e.g., OpenStreetMap features via Poly2Vec) impact ProbGLC's accuracy and uncertainty estimation? The paper identifies this as future work to facilitate multimodal geospatial data synergy.

### Open Question 2: Geographical Bias in Pre-training
To what extent does geographical bias in current pre-training datasets affect performance in "Global South" regions with sparse data density? Current evaluation is limited to US-based disaster datasets.

### Open Question 3: Dedicated Geospatial Pre-training
Can a dedicated generative global-scale geospatial pre-training scheme outperform general-purpose vision backbones for cross-view disaster localization? The study suggests this remains largely unexplored.

## Limitations

- Limited generalization to disaster types not represented in training data (only tested on hurricanes, wildfires, floods, tornadoes)
- Performance highly dependent on threshold radius selection which is dataset-specific and not automatically optimized
- Model assumes availability of both VGI and RSI imagery, but disaster scenarios may have incomplete data coverage

## Confidence

- **High confidence:** State-of-the-art performance claims (0.86 Acc@1km, 0.97 Acc@25km) based on quantitative evaluation
- **Medium confidence:** Explainability through uncertainty quantification - probabilistic outputs generated but practical interpretability needs validation
- **Medium confidence:** Hybrid architecture benefits - demonstrated improvement over individual components but threshold selection remains manual

## Next Checks

1. Test model generalization on a disaster type not in training data (e.g., earthquake or tsunami) to assess domain transfer
2. Perform ablation study varying the threshold radius r across a wider range to identify optimal automated selection criteria
3. Conduct time-to-decision analysis comparing ProbGLC against real-time disaster response requirements to validate practical deployment feasibility