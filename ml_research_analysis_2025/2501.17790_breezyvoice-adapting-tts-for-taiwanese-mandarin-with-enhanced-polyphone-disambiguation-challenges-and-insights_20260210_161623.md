---
ver: rpa2
title: 'BreezyVoice: Adapting TTS for Taiwanese Mandarin with Enhanced Polyphone Disambiguation
  -- Challenges and Insights'
arxiv_id: '2501.17790'
source_url: https://arxiv.org/abs/2501.17790
tags:
- speech
- speaker
- mandarin
- oice
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: BreezyVoice is a Text-to-Speech system for Taiwanese Mandarin that
  improves polyphone disambiguation and voice cloning quality by integrating LLM-based
  unit generation, OT-CFM, and phonetic augmentation. It outperforms commercial systems
  on both general and code-switched speech, achieving a 61.2% reduction in phoneme
  error rate for long-tail speakers using an iconic-unit approach, and nearly eliminating
  polyphone errors with g2pW augmentation.
---

# BreezyVoice: Adapting TTS for Taiwanese Mandarin with Enhanced Polyphone Disambiguation -- Challenges and Insights

## Quick Facts
- arXiv ID: 2501.17790
- Source URL: https://arxiv.org/abs/2501.17790
- Reference count: 9
- BreezyVoice outperforms commercial TTS systems on Taiwanese Mandarin, achieving near-elimination of polyphone errors and a 61.2% reduction in phoneme error rate for long-tail speakers.

## Executive Summary
BreezyVoice is a text-to-speech system tailored for Taiwanese Mandarin that addresses two key challenges: polyphone disambiguation and few-shot voice cloning. It integrates g2pW, a BERT-based phoneme prediction model, to pre-emptively augment text with phonetic symbols, significantly reducing pronunciation errors. The system also introduces an "Iconic Unit Augmented Speech Cloning" pipeline that improves voice cloning robustness for long-tail speakers by decoupling semantic unit generation from noisy speaker embeddings. Evaluations show BreezyVoice outperforms commercial baselines on both general and code-switched speech.

## Method Summary
BreezyVoice adapts CosyVoice for Taiwanese Mandarin by introducing phonetic augmentation with g2pW and a two-stage voice cloning pipeline. The S³ tokenizer encodes speech into discrete units, which the LLM predicts based on text and speaker embeddings. The OT-CFM model then generates Mel spectrograms. Phonetic augmentation uses BERT-inspired noising to ensure attentiveness to phonetic symbols. For long-tail speakers, the iconic-unit approach generates units from a high-quality "iconic" speaker before applying voice conversion via CFM.

## Key Results
- Near elimination of polyphone errors using g2pW augmentation (23/24 hard instances corrected).
- 61.2% reduction in phoneme error rate for long-tail speakers using iconic-unit approach.
- Outperforms commercial systems on general and code-switched speech in both objective metrics and subjective evaluations.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Integrating g2pW significantly enhances polyphone disambiguation and overall phonetic accuracy.
- Mechanism: g2pW pre-emptively augments text with Mandarin Phonetic Symbols, shifting the disambiguation burden from the TTS LLM to a specialized model.
- Core assumption: g2pW is more accurate at disambiguating polyphones than the TTS LLM's implicit learning.
- Evidence: Nearly eliminates polyphone errors; 23/24 hard instances corrected; corpus supports BERT-based disambiguation.

### Mechanism 2
- Claim: Iconic Unit Augmented Speech Cloning improves voice cloning robustness for long-tail speakers.
- Mechanism: Two-stage approach generates units from a high-quality speaker, then applies voice conversion to the target speaker.
- Core assumption: Speaker identity and content can be reasonably separated; errors stem from noisy LLM unit generation.
- Evidence: 61.2% PER reduction; errors traced to LLM-predicted units; weak/no corpus evidence for this specific technique.

### Mechanism 3
- Claim: S³ tokenizer enables high-fidelity synthesis by creating compact, semantically rich representations.
- Mechanism: Single-layered supervised tokens dynamically encode content and prosody, simplifying LLM prediction.
- Core assumption: Single-layered token can capture sufficient information for high-quality synthesis.
- Evidence: Described as speech encoder with VQ layer supervised by ASR; weak/no corpus evidence linking S³ to performance.

## Foundational Learning

- Concept: **Polyphone Disambiguation**
  - Why needed here: Mandarin has characters with multiple pronunciations; incorrect choice makes speech unintelligible.
  - Quick check question: How does the system decide between different pronunciations for the same character?

- Concept: **Optimal-Transport Conditional Flow Matching (OT-CFM)**
  - Why needed here: Transforms discrete speech units into continuous Mel spectrograms for audible sound.
  - Quick check question: What does the OT-CFM model take as input and what does it output?

- Concept: **Code-switching in TTS**
  - Why needed here: Taiwanese Mandarin frequently incorporates English; TTS must handle seamless switching.
  - Quick check question: How does the model perform when it encounters a full English sentence within a Chinese prompt?

## Architecture Onboarding

- Component map: Text (with optional g2pW augmentation) → S³ Tokenizer → LLM → Speech Units → OT-CFM → Mel Spectrogram → Audio
- Critical path: Text → LLM → Speech Units → OT-CFM → Mel Spectrogram → Audio. Voice cloning injects speaker embedding at LLM and/or OT-CFM stage.
- Design tradeoffs:
  - Phonetic Augmentation: Increases accuracy but adds latency; uses BERT-style noising.
  - Iconic Unit Cloning: Improves robustness but slightly reduces speaker similarity (-2.51%).
  - Tokenizer Choice: Simplifies LLM prediction but may lose fine-grained acoustic details.
- Failure signatures:
  - Hallucinations/Stuttering: Long-tail speakers producing random stop-words, stuttering, or unintelligible utterances.
  - Incorrect Pronunciation: Mispronouncing polyphones or rare words without g2pW augmentation.
  - Speaker Leakage/Quality Drop: Incorrect flow or accents in code-switching; poor speaker matching in cloning.
- First 3 experiments:
  1. Baseline Quality Test: Generate speech for standard sentences; evaluate MOS and PER; compare against CosyVoice.
  2. Polyphone Stress Test: Test set with 20+ sentences containing high-frequency polyphones; run with/without g2pW; measure error rate.
  3. Voice Cloning Robustness Test: Select 5 diverse speakers; run standard and iconic-unit pipelines; compare PER and Speaker Similarity.

## Open Questions the Paper Calls Out

- Question: How can the model's accuracy be improved specifically for Chinese toponyms within code-switching contexts?
  - Basis: Authors state improvement of this area is left as future work; minor limitation in handling Chinese toponyms.
  - Why unresolved: Current model lags in "Toponyms" category compared to others in TCCSD evaluation.
  - What evidence: Modified training strategy targeting toponyms resulting in accuracy scores comparable to "General" or "Entity" categories.

- Question: Can the "Iconic Unit Augmented Speech Cloning" method be refined to eliminate the trade-off between phoneme accuracy and speaker similarity?
  - Basis: Method reduces PER by 61.2% but incurs -2.51% drop in speaker similarity; weak positive correlation ($r=0.29$).
  - Why unresolved: Improves intelligibility at cost of identity fidelity; disentanglement not yet perfect.
  - What evidence: Pipeline maintaining PER reduction while achieving statistically identical or improved speaker similarity.

- Question: What architectural interventions are required to fully suppress hallucinations and stuttering in long-tail speakers without relying on surrogate speaker embeddings?
  - Basis: Analysis identifies random insertion of speech stopwords, stuttering, and unintelligible hallucinated utterances as primary failure modes.
  - Why unresolved: Mitigation strategy bypasses noisy speaker embedding rather than correcting model's internal propensity.
  - What evidence: Model generating artifact-free speech for long-tail speakers using their own native speaker embeddings rather than iconic surrogate.

- Question: Does the phonetic augmentation strategy utilizing g2pW generalize to other low-resource accents or dialects outside of Taiwanese Mandarin?
  - Basis: Paper focuses on adapting Mandarin model to Taiwanese Mandarin; generalizability to other languages/accents not discussed.
  - Why unresolved: Success relies on high-performance g2pW model and specific relationship between Traditional Chinese characters and phonetic symbols.
  - What evidence: Successful application of BreezyVoice augmentation pipeline to a different dialect or language pair resulting in similar error reductions.

## Limitations

- Data Provenance: Exact data sources, licensing, and degree of synthetic data contamination remain unspecified, limiting reproducibility.
- Architectural Dependencies: Performance claims hinge on CosyVoice base model, S³ tokenizer, and OT-CFM, none of which are fully described or linked.
- Trade-off Transparency: Iconic-unit approach trades speaker similarity for robustness (-2.51%), but perceptual impact is not deeply characterized.

## Confidence

- High Confidence: Polyphone disambiguation improvement via g2pW augmentation. Well-supported by quantitative metrics and established BERT-based disambiguation techniques.
- Medium Confidence: Iconic Unit Augmented Speech Cloning robustness gains. 61.2% PER reduction is compelling but relies on untested two-stage pipeline.
- Low Confidence: Overall superiority over commercial systems. Subjective evaluations reported but lack standardized benchmarks and detailed ablations.

## Next Checks

1. Reproduce Polyphone Stress Test: Construct test set of 30+ sentences with high-frequency polyphones; run inference with/without g2pW; measure character-level pronunciation accuracy.

2. Isolate Iconic Unit Contribution: For 5 long-tail speakers, generate speech using standard pipeline, iconic-unit pipeline, and iconic-unit pipeline with different iconic speaker; compare PER and speaker similarity.

3. Ablation on S³ Tokenizer: Replace S³ tokenizer with HuBERT or Wav2Vec while keeping other components constant; measure impact on intelligibility (PER) and voice cloning quality (speaker similarity).