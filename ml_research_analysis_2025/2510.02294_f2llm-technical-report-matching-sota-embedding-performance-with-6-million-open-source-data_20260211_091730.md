---
ver: rpa2
title: 'F2LLM Technical Report: Matching SOTA Embedding Performance with 6 Million
  Open-Source Data'
arxiv_id: '2510.02294'
source_url: https://arxiv.org/abs/2510.02294
tags:
- https
- datasets
- huggingface
- given
- retrieval
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: F2LLM introduces a suite of embedding models trained solely on
  open-source non-synthetic data, achieving SOTA performance with a budget-friendly
  approach. Unlike prior models relying on massive synthetic data or multi-stage training,
  F2LLM is directly fine-tuned from foundation models on 6 million high-quality query-document-negative
  tuples curated from diverse open datasets.
---

# F2LLM Technical Report: Matching SOTA Embedding Performance with 6 Million Open-Source Data

## Quick Facts
- arXiv ID: 2510.02294
- Source URL: https://arxiv.org/abs/2510.02294
- Reference count: 40
- Key result: 0.6B-4B embedding models trained on 6M open-source data achieve SOTA MTEB performance, ranking 2nd (4B) and 1st (1.7B) in size categories

## Executive Summary
F2LLM introduces a suite of embedding models trained solely on open-source non-synthetic data, achieving SOTA performance with a budget-friendly approach. Unlike prior models relying on massive synthetic data or multi-stage training, F2LLM is directly fine-tuned from foundation models on 6 million high-quality query-document-negative tuples curated from diverse open datasets. The models (0.6B, 1.7B, 4B) achieve strong MTEB leaderboard performance: F2LLM-4B ranks 2nd among models around 4B parameters and 7th overall, while F2LLM-1.7B ranks 1st in the 1B-2B range. The training employs a unified contrastive learning framework with hard negative mining, in-batch loss, and a custom multitask dataloader to optimize sample efficiency. By releasing models, training data, and code, F2LLM provides a reproducible, cost-effective baseline for future embedding research.

## Method Summary
F2LLM trains 0.6B, 1.7B, and 4B embedding models via single-stage contrastive fine-tuning from Qwen3 foundation models. The models are trained on 6 million query-document-negative tuples from open-source datasets (4.9M retrieval, 0.2M classification, 0.8M clustering) with hard negatives mined using a margin-based strategy. Training uses a unified contrastive framework with hard negative loss (all tasks) and in-batch loss (retrieval only), implemented through a custom multitask dataloader that ensures same-source micro-batches. The approach achieves strong MTEB performance without synthetic data or multi-stage pretraining.

## Key Results
- F2LLM-4B ranks 2nd among models around 4B parameters and 7th overall on MTEB
- F2LLM-1.7B ranks 1st in the 1B-2B parameter range on MTEB
- F2LLM-0.6B achieves competitive performance for its size class
- All models outperform previous state-of-the-art models in their respective size categories without using synthetic training data

## Why This Works (Mechanism)

### Mechanism 1: Margin-Based Adaptive Hard Negative Mining
Selecting hard negatives based on relative score margins improves contrastive learning sample efficiency compared to random negative sampling. The approach uses a teacher model to retrieve top-100 candidates per query, excludes top-5 to avoid false negatives, filters by absolute threshold (<0.8) and relative margin (<95% of positive score), then selects top-24 remaining passages as hard negatives. This pushes the model to learn finer-grained distinctions. The core assumption is that the teacher model's similarity scores correlate with semantic difficulty gradients that are learnable but not yet captured by the student model.

### Mechanism 2: Task-Conditional In-Batch Loss with Multitask Dataloader
Computing in-batch loss only within same-source batches improves sample efficiency when blending retrieval and non-retrieval tasks. A custom multitask dataloader ensures each micro-batch contains samples from a single data source. Hard negative loss is computed for all tasks; in-batch loss is computed only for retrieval tasks. This prevents cross-task interference in the in-batch similarity matrix while maintaining hard-negative contrastive pressure across all tasks. The core assumption is that non-retrieval tasks (classification, clustering) have different optimal embedding geometries than retrieval, and mixing them in-batch introduces noise without benefit.

### Mechanism 3: Single-Stage Fine-Tuning from Strong Foundation Models
High-quality curated data can substitute for multi-stage weakly-supervised pretraining when starting from capable foundation models. The approach skips billion-scale weakly supervised contrastive pretraining and directly fine-tunes Qwen3 (0.6B, 1.7B, 4B) on 6M curated tuples for 2 epochs. The foundation model's pre-existing textual understanding provides initialization quality that previously required separate pretraining stages. The core assumption is that the foundation model's pretraining already captures sufficient semantic structure; the fine-tuning data quality and diversity compensate for volume.

## Foundational Learning

- **Concept**: Contrastive Learning (InfoNCE-style loss)
  - Why needed here: F2LLM's core training objective uses contrastive loss to pull query-positive pairs closer and push query-negative pairs apart in embedding space
  - Quick check question: Can you explain why increasing temperature Ï„ makes the softmax distribution softer and how this affects gradient magnitude?

- **Concept**: In-Batch Negatives vs Hard Negatives
  - Why needed here: F2LLM combines both loss types; understanding their trade-offs is critical for reproducing or modifying the training pipeline
  - Quick check question: What is the computational difference between using other samples in the batch as negatives vs pre-mined hard negatives?

- **Concept**: Task-Specific Instructions for Embedding Models
  - Why needed here: F2LLM prepends task instructions to queries (e.g., "Given a question, retrieve passages that answer the question") to condition embeddings on task context
  - Quick check question: How might instruction-prefixing interact with models that use bidirectional vs causal attention?

## Architecture Onboarding

- **Component map**: Qwen3 Backbone -> Hard Negative Miner (Qwen3-Embedding-0.6B) -> Multitask Dataloader -> Contrastive Loss (Hard + In-Batch) -> AdamW Optimizer

- **Critical path**: 
  1. Data curation (MTEB training sets + decontamination)
  2. Hard negative mining (offline, per dataset)
  3. Multitask dataloader (per-GPU same-source sampling)
  4. Forward pass with instruction-prefixed queries
  5. Dual loss computation (hard-negative always, in-batch retrieval-only)
  6. Gradient accumulation across GPUs for in-batch loss

- **Design tradeoffs**:
  - No synthetic data: Lower cost, higher reproducibility, but may cap upper performance vs synthetic-augmented models
  - Single-stage training: Simpler pipeline, but depends heavily on foundation model quality
  - 24 hard negatives per sample: Higher memory/compute per batch, but stronger gradient signal
  - In-batch loss restricted to retrieval: Reduces cross-task interference, but may limit transfer learning

- **Failure signatures**:
  - Clustering underperforms retrieval: Check if in-batch loss is incorrectly applied to clustering batches
  - High false-negative rate: Hard negative miner may be too aggressive; relax margin threshold or exclude top-k > 5
  - Training instability with small models: Learning rate may be too high; reduce per Table 1 scaling (4B: 8e-6, 0.6B: 1e-5)
  - Poor cross-domain generalization: Data diversity may be insufficient; inspect per-domain performance in MTEB breakdown

- **First 3 experiments**:
  1. Ablate hard negative count: Train with 7 vs 24 hard negatives to quantify memory/performance trade-off on MTEB clustering and retrieval subsets
  2. Enable in-batch loss for all tasks: Compare against baseline to validate the paper's claim that restricting to retrieval improves sample efficiency
  3. Swap foundation model: Fine-tune a weaker backbone (e.g., smaller or earlier-generation LLM) to test the single-stage training assumption and measure performance gap

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the F2LLM performance be further improved by augmenting the 6M non-synthetic tuples with synthetic data, or does the high quality of open-source data negate the need for synthetic generation?
- Basis in paper: The paper highlights that F2LLM is trained "solely on open-source non-synthetic data," contrasting it with top models like NV-Embed and Gemini that rely on "costly synthetic training data"
- Why unresolved: The paper establishes a strong baseline using real data but does not ablate the impact of adding synthetic data to this specific high-quality mixture
- What evidence would resolve it: An ablation study training F2LLM on the 6M open-source dataset augmented with varying amounts of LLM-generated synthetic pairs and comparing MTEB scores

### Open Question 2
- Question: How does the strict separation of in-batch loss (retrieval tasks only) versus hard negative loss (all tasks) affect the embedding space compared to applying in-batch loss universally?
- Basis in paper: Section 3.2 states that "in-batch loss is only computed for retrieval tasks," while hard negative loss is computed for all tasks, a design choice justified by sample efficiency but not experimentally isolated
- Why unresolved: It is unclear if this restriction prevents the model from learning superior universal representations by limiting the diversity of in-batch negatives for non-retrieval tasks
- What evidence would resolve it: A comparison of the F2LLM training dynamics and final performance when in-batch loss is enabled for clustering and classification tasks versus the baseline implementation

### Open Question 3
- Question: Is the effectiveness of the single-stage training pipeline generalizable to foundation models outside the Qwen family, or is it dependent on Qwen3's specific pretraining?
- Basis in paper: The methodology relies entirely on the Qwen3 backbone (0.6B, 1.7B, 4B), attributing success to the data quality, but does not test other popular architectures like Mistral or Llama
- Why unresolved: The contribution claims to provide a "budget-friendly baseline," but it is unknown if this data mixture alone is sufficient to reach SOTA on other model architectures without architectural tuning
- What evidence would resolve it: Training models of similar sizes (e.g., Mistral-7B-v0.3 or Llama-3-8B) on the released F2LLM dataset using the same hyperparameters and evaluating on MTEB

## Limitations

- Limited ablation studies on individual dataset contributions and the effectiveness of specific design choices
- Heavy dependence on Qwen3 foundation model quality without validation on alternative architectures
- Lack of comparative analysis with alternative hard negative mining strategies or synthetic data augmentation

## Confidence

- **High Confidence**: The overall training pipeline (contrastive loss, foundation model initialization, evaluation protocol) is technically sound and well-documented
- **Medium Confidence**: The specific mechanisms (hard negative mining margins, multitask dataloader design) are logically justified but lack direct comparative evidence in the paper
- **Low Confidence**: Claims about cost-effectiveness and reproducibility are reasonable but unverified without independent replication attempts

## Next Checks

1. **Dataset Contribution Analysis**: Recreate the training pipeline with subsets of the curated data to quantify individual dataset contributions and identify potential bottlenecks
2. **Hard Negative Strategy Ablation**: Implement alternative hard negative mining strategies (e.g., different teacher models, varying margin thresholds) to empirically validate the claimed efficiency gains
3. **Multitask Dataloader Comparison**: Train comparable models using standard mixed-task training to directly test whether restricting in-batch loss to retrieval tasks provides measurable benefits