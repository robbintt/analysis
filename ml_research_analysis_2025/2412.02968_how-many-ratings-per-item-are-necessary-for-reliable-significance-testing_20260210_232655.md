---
ver: rpa2
title: How Many Ratings per Item are Necessary for Reliable Significance Testing?
arxiv_id: '2412.02968'
source_url: https://arxiv.org/abs/2412.02968
tags:
- responses
- item
- data
- each
- p-value
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates how many responses per item are needed
  for reliable significance testing when comparing machine learning models, accounting
  for response variance from both models and human raters. It adapts a simulation
  method to estimate the number of items N and responses per item K required to detect
  performance differences with statistical significance, before collecting data.
---

# How Many Ratings per Item are Necessary for Reliable Significance Testing?

## Quick Facts
- **arXiv ID**: 2412.02968
- **Source URL**: https://arxiv.org/abs/2412.02968
- **Reference count**: 40
- **Key outcome**: Even 5-10 responses per item are insufficient for reliable null hypothesis testing when comparing ML models

## Executive Summary
This paper addresses a critical gap in machine learning evaluation by determining how many human ratings per item are needed for statistically reliable significance testing when comparing models. The authors adapt a simulation-based power analysis method to estimate the optimal combination of items (N) and responses per item (K) before collecting data. Through experiments with seven real-world datasets using metrics like MAE, Wins, and MEMD, they demonstrate that current evaluation practices with limited responses are inadequate for confident model comparison. The study shows that allocating more responses per item (up to 100) can provide better statistical power than simply increasing the number of items, with different metrics showing varying sensitivity to this trade-off.

## Method Summary
The authors employ a simulation-based power analysis approach that adapts methodology from neuroscience to the ML evaluation context. The method estimates statistical power (probability of correctly rejecting a false null hypothesis) as a function of N (number of items) and K (responses per item) before data collection. They first estimate the true model difference (δ) from pilot data, then simulate datasets with varying (N,K) configurations under different effect sizes and variance assumptions. Statistical significance is tested using paired t-tests and Wilcoxon signed-rank tests, with power calculated as the proportion of simulations where the null hypothesis is rejected. The approach accounts for both model variance and human rater variance, providing a principled way to allocate evaluation budgets between N and K.

## Key Results
- Current practices of 5-10 responses per item are insufficient for reliable significance testing, with actual power often far below nominal levels
- For a fixed total budget of N×K responses, increasing K up to 100 generally provides better statistical power than increasing N, though this varies by metric
- MAE and MEMD metrics benefit more from higher K values compared to the Wins metric, showing different sensitivity to the N-K trade-off
- The simulation-based power analysis method successfully predicts actual power across multiple datasets, validating its use for planning evaluations

## Why This Works (Mechanism)
The simulation-based power analysis works by explicitly modeling the variance sources in human evaluation - both the variance between models and the variance between human raters. By estimating the true effect size from pilot data and simulating multiple datasets with known statistical properties, the method can accurately estimate the probability of detecting true differences as a function of evaluation budget allocation. This pre-data collection approach allows researchers to optimize their evaluation design before incurring costs.

## Foundational Learning
- **Statistical power analysis**: Understanding the probability of detecting true effects is crucial for reliable significance testing; quick check: can you explain why 80% power is a common threshold?
- **Paired statistical tests**: Comparing models on the same items requires tests that account for item-level correlations; quick check: when would you choose t-test vs Wilcoxon signed-rank test?
- **Effect size estimation**: Accurate estimation of true model differences from pilot data is essential for reliable power calculations; quick check: how does uncertainty in δ estimation affect power predictions?
- **Variance decomposition**: Separating model variance from rater variance helps optimize evaluation design; quick check: can you identify scenarios where rater variance dominates?
- **Budget allocation optimization**: Understanding the trade-off between number of items and responses per item enables efficient resource use; quick check: what factors determine the optimal N-K balance?
- **Simulation-based validation**: Generating synthetic data with known properties validates analytical methods; quick check: what assumptions must hold for simulation results to be valid?

## Architecture Onboarding
**Component map**: Pilot data collection -> Effect size estimation (δ) -> Simulation engine -> Power calculation -> Optimal (N,K) recommendation

**Critical path**: The core methodology flows from estimating the true model difference in pilot data through simulation-based power calculations to determine optimal evaluation design parameters.

**Design tradeoffs**: The method trades computational complexity for accuracy - simulations require significant computation but provide more reliable estimates than analytical approximations, especially when variance structures are complex.

**Failure signatures**: Underestimation of δ from pilot data leads to overly optimistic power predictions and insufficient sample sizes; high rater variance not accounted for can invalidate power calculations.

**First experiments**:
1. Replicate power analysis with different effect size estimators to assess sensitivity to δ estimation
2. Compare power predictions using analytical approximations versus simulations across varying variance conditions
3. Test the methodology on datasets with different rating distributions (e.g., multimodal vs unimodal)

## Open Questions the Paper Calls Out
The paper identifies several open questions including how the optimal (N,K) configuration changes when comparing more than two models, how different rating distributions affect the power analysis, and whether alternative statistical tests might provide better performance for specific evaluation scenarios.

## Limitations
- Results focus on binary model comparisons and may not generalize to multi-model scenarios or different task types
- Simulation approach relies on pilot data for δ estimation, introducing uncertainty into power calculations
- Assumes independence between responses and items, which may not hold in all evaluation contexts
- The optimal K threshold of 100 may vary depending on the underlying variance structure of different tasks

## Confidence
- **High confidence**: 5-10 responses per item are insufficient for reliable significance testing
- **Medium confidence**: K=100 provides optimal statistical power (context-dependent)
- **Medium confidence**: Allocating more responses per item is generally better than increasing items

## Next Checks
1. Replicate the analysis with comparisons involving three or more models to assess how the (N,K) trade-off changes in multi-model scenarios
2. Test the methodology on completely different task types (e.g., image classification, object detection) to evaluate generalizability beyond sentiment analysis and emotion recognition
3. Conduct sensitivity analysis to quantify how errors in δ estimation from pilot data affect the reliability of power calculations and recommended (N,K) configurations