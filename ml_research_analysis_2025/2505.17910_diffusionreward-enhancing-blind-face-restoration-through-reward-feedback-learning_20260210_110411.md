---
ver: rpa2
title: 'DiffusionReward: Enhancing Blind Face Restoration through Reward Feedback
  Learning'
arxiv_id: '2505.17910'
source_url: https://arxiv.org/abs/2505.17910
tags:
- face
- reward
- image
- restoration
- images
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: DiffusionReward introduces Reward Feedback Learning (ReFL) to blind
  face restoration for the first time. The core innovation is a Face Reward Model
  (FRM) trained on carefully annotated preference data that provides feedback to guide
  diffusion-based restoration models.
---

# DiffusionReward: Enhancing Blind Face Restoration through Reward Feedback Learning

## Quick Facts
- arXiv ID: 2505.17910
- Source URL: https://arxiv.org/abs/2505.17910
- Reference count: 40
- Introduces Reward Feedback Learning (ReFL) with dynamic Face Reward Model updates for diffusion-based face restoration

## Executive Summary
DiffusionReward introduces the first application of Reward Feedback Learning to blind face restoration, incorporating a gradient flow into the denoising process of diffusion models. The method employs a Face Reward Model (FRM) trained on carefully annotated preference data to provide feedback during training. To prevent reward hacking, the FRM undergoes dynamic optimization throughout training. The approach significantly improves perceptual quality and identity consistency compared to base diffusion models DiffBIR and OSEDiff.

## Method Summary
DiffusionReward applies Reward Feedback Learning to diffusion-based face restoration through truncated backpropagation. The Face Reward Model (FRM) is trained on a hybrid dataset of human-annotated and SVM-predicted preference pairs. During training, reward gradients flow through the final denoising step, combined with structural consistency and weight regularization losses. The FRM is dynamically updated every 10 iterations to prevent reward hacking by maintaining alignment with real face distributions. The framework is evaluated on synthetic (CelebA-Test) and real-world datasets (LFW-Test, WebPhoto-Test), demonstrating state-of-the-art performance.

## Key Results
- DiffBIR(+ours) achieved LMD 1.8642 (↓4.55%), MUSIQ 74.82 (↑1.83%), and Aesthetic 5.8475 (↑1.80%) improvements on CelebA-Test
- Effectively prevents reward hacking through dynamic FRM updates, eliminating artifacts like acne marks and stylized painterly outputs
- Demonstrates superior identity consistency and facial detail preservation compared to base DiffBIR and OSEDiff models

## Why This Works (Mechanism)

### Mechanism 1: Gradient Feedback Through Truncated Denoising
Backpropagating reward gradients through the final denoising step steers diffusion-based restoration toward higher perceptual quality without requiring full-chain computation. The denoising process is treated as a parameterized generator, with reward loss computed on the decoded output from the last step and gradients flowing back through this single step only. Core assumption: gradients from the final denoising step carry sufficient signal to improve the full generative process.

### Mechanism 2: Hybrid Annotation via Metric-SVM Pipeline
A support vector machine trained on multi-dimensional quality metrics approximates human preferences at scale, enabling cost-efficient FRM training. Six metrics are concatenated into 12D feature vectors for image pairs. An SVM trained on 3,600 human-labeled pairs predicts preferences for the remaining ~114,000 pairs. Core assumption: selected quality metrics span dimensions humans use to judge face restoration quality.

### Mechanism 3: Dynamic FRM Updates Constrain Optimization to Real Face Manifold
Continuously updating the reward model with ground-truth preference signals prevents the generator from drifting into high-reward but unrealistic regions of image space. Every 10 generator updates, the FRM is trained on pairs where GT images are always preferred over generated outputs. Core assumption: ground-truth images remain the optimal reference throughout training, creating a moving target that discourages shortcut exploitation.

## Foundational Learning

- **Concept: Denoising Diffusion Probabilistic Models**
  - Why needed here: Understanding how z_t = √ᾱ_t·z_HQ + √(1-ᾱ_t)·ε defines the noise schedule and how the UNet predicts ε given (z_t, c_LQ, t) is prerequisite to understanding where gradients can be injected.
  - Quick check question: Can you explain why gradients flow through g_θ when predicting noise at timestep t, and what happens differently when N=1 vs. N=4 backpropagation?

- **Concept: Preference Learning via Pairwise Comparisons**
  - Why needed here: The FRM uses cross-entropy loss on softmaxed cosine similarities. Understanding why s_k = (e_ik · e_t)/τ and why we optimize -Σ y_j log(ŷ_j) is essential for debugging reward signal quality.
  - Quick check question: Given two restored faces with embeddings e_1, e_2 and text embedding e_t, how would you compute the preference probability P(I_1 ≻ I_2)?

- **Concept: Reward Hacking in RLHF-style Optimization**
  - Why needed here: The paper's core contribution is addressing this failure mode. Understanding how models exploit proxy rewards motivates the dynamic update strategy.
  - Quick check question: If a reward model assigns high scores to images with unnaturally sharp edges, what failure mode would you expect during optimization, and how would you detect it?

## Architecture Onboarding

- **Component map**:
  ```
  [LQ Image] → [Encoder E] → z_LQ (condition)
                              ↓
  [z_noisy] → [g_θ: UNet+ControlNet/LoRA] → z_denoised → [Decoder D] → Î_HQ
                              ↑                                    ↓
                    (N=1 gradient flow)                    [FRM: CLIP ViT-H-14]
                                                              ↓
                                                    L_reward + L_LPIPS + L_DWT
  [GT Image IHQ] ─────────────────────────────────────────────┘
  [Text T] ← [LLaVA captioner] ───────────────────────────────┘
  ```

- **Critical path**:
  1. **Data prep**: Generate 19,590 LLaVA captions; synthesize LQ/HQ pairs via degradation pipeline (Gaussian blur σ∈[0.1,12], downsample r∈[1,12], noise δ∈[0,15], JPEG q∈[30,100])
  2. **FRM training**: Train SVM on 3,600 human pairs → auto-label remaining → fine-tune HPS v2 (last 20 img layers + last 11 text layers trainable, lr=3.3e-6, 20k iterations)
  3. **ReFL fine-tuning**: For each batch—degrade HQ→LQ, denoise via DDIM, compute L_total, backprop through N=1 steps, update g_θ
  4. **Dynamic updates**: Every 10 iterations, generate Î_HQ batch, train FRM on (I_GT ≻ Î_HQ) pairs with L_FRM

- **Design tradeoffs**:
  - N=1 truncation vs. full-chain: Memory/compute savings vs. potentially weaker gradient signal
  - Hybrid annotation vs. full human: ~95% cost reduction vs. label noise risk
  - λ_reg values differ by base model: DiffBIR uses 10^-4, OSEDiff uses 1—suggests single-step models need stronger anchoring
  - Update frequency n=10: Balances anti-hacking effectiveness against training instability

- **Failure signatures**:
  - **Reward hacking Style 1**: Painterly/stylized outputs with uniform textures
  - **Reward hacking Style 2**: Repetitive skin patterns, uncanny smoothness
  - **Identity loss**: LMD increases, face structure diverges from input
  - **Detail collapse**: Over-smoothed faces, lost hair/texture

- **First 3 experiments**:
  1. **FRM validation on held-out human labels**: Reserve 360 pairs from manual annotation as test set; target >85% agreement
  2. **Component ablation on CelebA-Test**: Train 4 variants (remove L_reward, remove L_LPIPS+L_DWT, remove L_reg, remove dynamic FRM) on DiffBIR base
  3. **Reward hacking induction/repair**: Train DiffBIR(+ours) without FRM updates for 5000 iterations, then resume with dynamic updates enabled

## Open Questions the Paper Calls Out

- **Question**: Can the DiffusionReward framework be effectively adapted to non-diffusion blind face restoration architectures, such as GAN-based or Transformer-based models?
  - Basis in paper: [explicit] The authors state in Section E (Limitation) that the framework was designed for diffusion models and applicability to other architectures "has not yet been explored," explicitly calling for future work in this area.
  - Why unresolved: The current ReFL mechanism relies on parameterizing the diffusion denoising process; GANs and Transformers operate on distinct generative principles that may not easily accommodate the gradient flow integration or dynamic reward updates used here.
  - What evidence would resolve it: A study successfully applying the ReFL framework with dynamic FRM updates to a GAN-based method (e.g., GFPGAN or CodeFormer), demonstrating performance improvements without training instability.

- **Question**: To what extent does the automated SVM-based annotation introduce a bias toward the specific quality metrics used rather than capturing broader human preferences?
  - Basis in paper: [inferred] Section 3.1 and Appendix A.1 describe a hybrid annotation strategy where an SVM classifier, trained on vectors of six specific metrics, replaces human labels for the majority of the dataset.
  - Why unresolved: The Face Reward Model (FRM) is fine-tuned on labels predicted by this SVM. If human preference involves attributes orthogonal to these metrics, the FRM might fail to reward them, effectively bottlenecking the quality of the feedback loop.
  - What evidence would resolve it: An ablation study comparing the current FRM against one trained with 100% human supervision, specifically evaluating alignment scores on "hard cases" where standard metrics diverge from human aesthetic judgment.

- **Question**: What is the sensitivity of the "reward hacking" prevention mechanism to the frequency of the Face Reward Model's dynamic updates?
  - Basis in paper: [inferred] Section 3.3 introduces a strategy to dynamically update the FRM to avert reward hacking, setting the update interval $n$ to 10 iterations, but does not provide analysis on the robustness of this specific hyperparameter choice.
  - Why unresolved: It is unclear if $n=10$ represents an optimal balance; updating too frequently might create a "moving target" problem that destabilizes training, while updating too slowly might allow the generator to overfit to loopholes before the reward model can adapt.
  - What evidence would resolve it: A parameter sensitivity analysis varying the update frequency (e.g., $n=1, 5, 20$) and tracking the trade-off between training stability, the incidence of hacking artifacts, and final restoration quality.

## Limitations

- **Design restriction**: Framework specifically designed for diffusion models; adaptation to GANs or Transformers unexplored
- **Computational overhead**: Dynamic FRM updates add training complexity and memory requirements
- **Metric dependency**: SVM annotation relies on six quality metrics that may not fully capture human preferences

## Confidence

- **Task/problem**: High - well-specified as blind face restoration with reward feedback
- **Inputs/data**: Medium - most specifications provided but some degradation parameters unclear
- **Objective/metrics**: High - comprehensive evaluation metrics clearly defined
- **Method/training procedure**: Medium - core components specified but some implementation details missing
- **Data preparation**: Medium - degradation pipeline specified but exact implementation unclear

## Next Checks

1. Validate FRM performance on held-out human preference pairs to ensure >85% accuracy before proceeding with ReFL training
2. Implement and test truncated backpropagation with N=1 vs N=4 to verify gradient flow correctness
3. Monitor training outputs every 500 iterations during ReFL to detect early signs of reward hacking artifacts