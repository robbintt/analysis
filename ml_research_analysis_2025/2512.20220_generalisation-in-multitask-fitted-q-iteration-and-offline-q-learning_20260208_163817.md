---
ver: rpa2
title: Generalisation in Multitask Fitted Q-Iteration and Offline Q-learning
arxiv_id: '2512.20220'
source_url: https://arxiv.org/abs/2512.20220
tags:
- learning
- multitask
- offline
- representation
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper analyzes offline multitask reinforcement learning under
  the assumption that tasks share a common low-rank representation of their optimal
  Q-functions. The authors study a multitask variant of fitted Q-iteration that jointly
  learns a shared encoder and task-specific decoders from offline datasets collected
  by behavior policies.
---

# Generalisation in Multitask Fitted Q-Iteration and Offline Q-learning

## Quick Facts
- **arXiv ID:** 2512.20220
- **Source URL:** https://arxiv.org/abs/2512.20220
- **Reference count:** 40
- **Key outcome:** Establishes finite-sample generalization bounds for multitask offline Q-learning with shared low-rank representations, showing statistical efficiency gains from data pooling across tasks.

## Executive Summary
This paper analyzes offline multitask reinforcement learning under the assumption that tasks share a common low-rank representation of their optimal Q-functions. The authors study a multitask variant of fitted Q-iteration that jointly learns a shared encoder and task-specific decoders from offline datasets collected by behavior policies. Under standard realizability and concentrability assumptions, they establish finite-sample generalization bounds showing that pooling data across tasks improves estimation accuracy, with error scaling as 1/√nT for the total number of samples. The analysis explicitly tracks how Bellman errors propagate backward through the horizon under distribution shift, quantifying the role of the concentrability coefficient. For downstream learning on a new task sharing the representation, they show that reusing the upstream representation reduces effective complexity relative to learning from scratch, yielding improved sample efficiency. The results demonstrate that multitask offline Q-learning can achieve statistical efficiency competitive with existing methods while providing theoretical guarantees in the challenging offline, model-free setting.

## Method Summary
The paper proposes a multitask fitted Q-iteration algorithm that jointly learns a shared encoder ϕ and task-specific decoders ψ^(t) for T offline tasks. Given datasets D^(t) collected under behavior policies π_b^(t), the method iterates backward through the horizon H, computing Bellman targets using current Q-estimates and jointly optimizing the shared encoder and decoders via empirical risk minimization of squared Bellman error. The algorithm maintains realizability assumptions (Q*_h^(t)(s,a) = ⟨ϕ*(s,a), ω_h^(t)⟩) and bounded concentrability (λ_max < ∞) to derive finite-sample bounds. For downstream transfer, the pretrained encoder is fixed and only task-specific decoders are learned, reducing the effective hypothesis class complexity.

## Key Results
- **Statistical scaling:** Pooling data across T tasks yields error scaling of O(1/√(nT)) rather than O(1/√n), demonstrating improved sample efficiency through representation sharing.
- **Distribution shift quantification:** Bellman errors propagate backward through the horizon with amplification controlled by the concentrability coefficient λ_max, appearing as a multiplicative factor in generalization bounds.
- **Transfer efficiency:** Reusing a pretrained representation for downstream tasks reduces effective complexity from learning the full hypothesis class F to learning only a decoder class G, yielding improved sample efficiency when the representation is shared.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Pooling data across tasks improves Q-function estimation accuracy, yielding error rates scaling as O(1/√(nT)) rather than O(1/√n).
- **Mechanism:** A shared encoder ϕ jointly learned across T tasks aggregates statistical information, while task-specific decoders ψ^(t) specialize to each task's value function. The multitask empirical risk minimization objective treats all nT samples as supporting the common representation.
- **Core assumption:** Low-rank realizability—there exists a shared representation ϕ* such that Q*_h^(t)(s,a) = ⟨ϕ*(s,a), w_h^(t)⟩ for task-specific weights w_h^(t) (Assumption 3.1).
- **Evidence anchors:**
  - [abstract] "yielding a 1/√(nT) dependence on the total number of samples across tasks"
  - [Theorem 1a] "This bound exhibits the expected O(1/√(nT)) statistical scaling due to pooling data across tasks"
  - [corpus] Related work on multitask representation learning (arXiv:2503.00345) confirms representation sharing improves sample efficiency, though primarily in online settings.
- **Break condition:** If tasks do not share a low-rank representation (violating realizability), the irreducible error ϵ_irred dominates and pooling provides no benefit.

### Mechanism 2
- **Claim:** Bellman errors propagate backward through the horizon with amplification controlled by the concentrability coefficient λ_max.
- **Mechanism:** In offline RL, distribution shift between the behavior policy μ_b and Bellman-induced distributions means errors measured under μ_b may be amplified. The concentrability coefficient upper-bounds density ratios, appearing as a multiplicative factor in error bounds.
- **Core assumption:** The behavior policy provides sufficient coverage—λ_max remains bounded rather than infinite.
- **Evidence anchors:**
  - [Section 3.2] "λ_max appears as a multiplicative factor in generalization bounds"
  - [Theorem 1b] Shows recursive error propagation: ||Q̂_h - Q*_h|| ≤ √(2λ_max)||Q̂_{h+1} - Q*_{h+1}|| + statistical terms
  - [corpus] Single-task offline Q-learning analyses (e.g., pessimistic Q-learning) similarly require concentrability assumptions; this work extends to multitask.
- **Break condition:** If the behavior policy has poor coverage (λ_max → ∞), bounds become vacuous. The paper does not address how to detect or recover from insufficient coverage.

### Mechanism 3
- **Claim:** Reusing a pretrained representation for downstream tasks reduces effective complexity from learning the full hypothesis class F to learning only a decoder class G.
- **Mechanism:** After upstream multitask learning, the encoder ϕ̂ is fixed. A new downstream task only fits task-specific weights, transforming the problem into lower-dimensional regression with complexity controlled by Rademacher complexity R(G) rather than |F|.
- **Core assumption:** The downstream task shares the same underlying low-rank representation as upstream tasks.
- **Evidence anchors:**
  - [Theorem 2] "when the representation is fixed using upstream multitask data, downstream learning reduces to estimating a lower-complexity decoder class"
  - [Section 5.2] "For linear decoders over bounded representations, R(G) = O(1/√n)"
  - [corpus] Weak direct evidence—corpus focuses on multi-agent offline RL and compositional Q-learning rather than representation transfer specifically.
- **Break condition:** If the downstream task's representation differs from upstream tasks, the effective irreducible error ϵ_eff_irred may be large, negating transfer benefits.

## Foundational Learning

- **Concept: Fitted Q-Iteration (FQI)**
  - **Why needed here:** The paper analyzes a multitask variant of FQI; understanding the single-task baseline is essential.
  - **Quick check question:** Can you explain why FQI uses regression targets y = r + γ·max_a' Q(s',a') rather than direct policy optimization?

- **Concept: Concentrability Coefficient**
  - **Why needed here:** This coefficient quantifies distribution shift and directly controls error propagation in all bounds.
  - **Quick check question:** Given a behavior policy μ_b and optimal policy π*, what does it mean if λ_max = ∞?

- **Concept: Rademacher Complexity**
  - **Why needed here:** Used to handle infinite hypothesis classes (e.g., neural networks) in downstream generalization bounds.
  - **Quick check question:** Why does Rademacher complexity yield tighter bounds than cardinality-based union bounds for continuous function classes?

## Architecture Onboarding

- **Component map:**
  - **Shared encoder ϕ**: Maps (s,a) → R^d; learned jointly across all T tasks
  - **Task-specific decoders ψ^(t)**: Linear heads producing Q-values per task
  - **Multitask loss**: Averaged squared Bellman error L̂ = (1/T) Σ_t E[(Q^(t) - target)²]
  - **Behavior policy datasets D^(t)**: Offline transition tuples for each task

- **Critical path:**
  1. Initialize encoder and decoders
  2. For each horizon step h (backward from H to 1):
     - Compute targets using current Q-estimates at h+1
     - Jointly optimize ϕ and all {ψ^(t)} via MTL update until ‖Θ^(k+1)-Θ^(k)‖ < ε or k ≥ MaxIter
  3. Return learned representation for potential downstream use

- **Design tradeoffs:**
  - **Encoder capacity vs. generalization**: Larger Φ improves approximation but increases log|F| term in bounds
  - **Task diversity vs. shared structure**: More heterogeneous tasks reduce representation sharing benefits
  - **Finite vs. infinite hypothesis classes**: Cardinality bounds are simpler; Rademacher complexity handles neural networks but requires more complex analysis

- **Failure signatures:**
  - Estimation error increases with T (suggests negative transfer/misspecification)
  - Bounds dominated by λ_max term (insufficient behavior policy coverage)
  - Downstream error matches independent learning (representation not shared)

- **First 3 experiments:**
  1. **T-scaling validation**: Hold n and H fixed; vary T and verify MSE scales as 1/√(nT) (Figure 1 confirms)
  2. **n-scaling validation**: Hold T and H fixed; vary n and verify 1/√n scaling (Figure 2 confirms)
  3. **Horizon validation**: Hold T and n fixed; vary H and verify polynomial (not exponential) error growth (Figure 3 confirms)

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Do minimax lower bounds exist for multitask offline Q-learning that match the $\tilde{O}(H^2\lambda/\sqrt{nT})$ upper bound established in this paper?
- **Basis in paper:** [explicit] The Conclusion invites investigation into "lower bounds" to determine if the statistical rates derived are tight.
- **Why unresolved:** The paper only provides upper bounds on the estimation error; without lower bounds, it remains unclear if the $1/\sqrt{nT}$ scaling is a fundamental limit or an artifact of the analysis.
- **Evidence to resolve:** A formal information-theoretic lower bound proof demonstrating that no algorithm can achieve better than $O(1/\sqrt{nT})$ scaling under the same assumptions.

### Open Question 2
- **Question:** Can the theoretical guarantees be maintained under weaker coverage assumptions than the global concentrability coefficient ($\lambda_{max}$)?
- **Basis in paper:** [explicit] The Conclusion lists "alternative coverage conditions" as a key area for future investigation.
- **Why unresolved:** The current analysis relies on $\lambda_{max}$ to handle distribution shift, but this coefficient can be prohibitively large or infinite in practical scenarios where the behavior policy has sparse support.
- **Evidence to resolve:** Deriving finite-sample bounds that rely only on partial coverage or single-policy concentrability rather than global density ratios.

### Open Question 3
- **Question:** How do the generalization guarantees scale when extended to richer, non-linear function approximation classes?
- **Basis in paper:** [explicit] The Conclusion identifies "extensions to richer function approximation classes" as a necessary step for a unified understanding.
- **Why unresolved:** While Section 5 addresses infinite hypothesis sets via Rademacher complexity, the analysis is generic and does not explicitly characterize the capacity of deep neural networks used in practice.
- **Evidence to resolve:** Explicit finite-sample bounds derived for specific non-linear classes (e.g., deep ReLU networks) that quantify the trade-off between representation rank and network depth/width.

## Limitations

- **Realizability assumption:** The theoretical guarantees rely on the existence of a shared low-rank representation that perfectly captures all task Q-functions, which may not hold in practice.
- **Concentrability requirement:** The analysis requires bounded concentrability coefficient λ_max, but this can be difficult to verify or control in offline settings where behavior policies have limited coverage.
- **Transfer assumption:** Downstream transfer guarantees assume the new task shares the same representation structure as upstream tasks, which may not hold for truly novel task distributions.

## Confidence

- **Statistical scaling claims:** High
- **Practical applicability:** Medium
- **Coverage of failure modes:** Low

## Next Checks

1. **Realizability verification:** Test algorithm performance when the shared low-rank assumption is violated—introduce tasks with distinct representations and measure degradation in error rates.
2. **Concentrability sensitivity:** Systematically vary the behavior policy coverage to induce different λ_max values and quantify the impact on both estimation and transfer performance.
3. **Architecture scaling:** Evaluate how the bounds change with different encoder capacities and decoder architectures, comparing linear vs. neural network parameterizations.