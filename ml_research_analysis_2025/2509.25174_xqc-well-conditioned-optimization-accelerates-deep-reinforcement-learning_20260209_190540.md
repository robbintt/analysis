---
ver: rpa2
title: 'XQC: Well-conditioned Optimization Accelerates Deep Reinforcement Learning'
arxiv_id: '2509.25174'
source_url: https://arxiv.org/abs/2509.25174
tags:
- learning
- loss
- critic
- optimization
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the sample efficiency problem in deep reinforcement
  learning by focusing on the optimization landscape of the critic network. Through
  a systematic eigenvalue analysis of the critic's Hessian, the authors demonstrate
  that a novel combination of batch normalization (BN), weight normalization (WN),
  and a distributional cross-entropy (CE) loss produces significantly better-conditioned
  optimization landscapes with condition numbers orders of magnitude smaller than
  baselines.
---

# XQC: Well-conditioned Optimization Accelerates Deep Reinforcement Learning

## Quick Facts
- arXiv ID: 2509.25174
- Source URL: https://arxiv.org/abs/2509.25174
- Reference count: 34
- Primary result: Achieves state-of-the-art sample efficiency across 70 continuous control tasks with 4.5× fewer parameters than closest competitor

## Executive Summary
XQC addresses the sample efficiency problem in deep reinforcement learning by focusing on the optimization landscape of the critic network. Through systematic eigenvalue analysis, the authors demonstrate that a novel combination of batch normalization, weight normalization, and a distributional cross-entropy loss produces significantly better-conditioned optimization landscapes with condition numbers orders of magnitude smaller than baselines. This combination also naturally bounds gradient norms, which is critical for maintaining a stable effective learning rate under non-stationary targets and bootstrapping.

The resulting XQC method achieves state-of-the-art sample efficiency across 70 continuous control tasks spanning 5 benchmark suites, all while using significantly fewer parameters than competing methods. Specifically, XQC uses approximately 4.5× fewer parameters and 5× less compute than the closest competitor SIMBA-V2, while demonstrating superior performance particularly on challenging tasks like HumanoidBench and DMC-hard environments.

## Method Summary
XQC extends Soft Actor-Critic by modifying the critic architecture to improve optimization landscape conditioning. The method uses a 4-layer MLP with 512 hidden units per layer, applying Batch Normalization before ReLU activations and Weight Normalization to project weights onto the unit sphere. The critic outputs 101 logits for a categorical distribution over returns (C51 style), using a cross-entropy loss instead of MSE. Rewards are normalized by the running standard deviation of returns, and a "joined forward pass" computes BatchNorm statistics on concatenated current and next state-action batches. The approach maintains a stable effective learning rate through weight normalization while bounding gradient norms via the distributional loss.

## Key Results
- Achieves state-of-the-art sample efficiency across 70 continuous control tasks (55 proprioception, 15 vision-based)
- Uses approximately 4.5× fewer parameters than closest competitor SIMBA-V2
- Requires 5× less compute (FLOPs) than SIMBA-V2 while maintaining superior performance
- Particularly excels on challenging tasks like HumanoidBench and DMC-hard environments

## Why This Works (Mechanism)

### Mechanism 1: Hessian Conditioning via Normalization Synergy
The combination of Batch Normalization and Weight Normalization produces a loss landscape with significantly lower condition number compared to Layer Normalization baselines. BN restricts activation scale, inherently bounding Hessian eigenvalues, while WN prevents the network from entering ill-conditioned regions by projecting weights to the unit sphere. This maintains compact eigenspectra without large outlier eigenvalues seen in LN.

### Mechanism 2: Gradient Bounding via Distributional Loss
Replacing MSE with distributional Cross-Entropy loss bounds the gradient norm, preventing explosive updates during bootstrapping. The CE loss gradient is bounded by softmax properties (specifically √2), whereas MSE gradients scale linearly with Bellman error magnitude. This bound prevents large TD-errors from destabilizing the network.

### Mechanism 3: Plasticity Preservation via Constant ELR
Weight Normalization stabilizes the Effective Learning Rate, preventing "loss of plasticity" common in deep RL. In standard networks, weight norms grow over time, shrinking the effective step size. WN enforces constant weight magnitude, keeping ELR constant and maintaining learning capability in non-stationary RL settings.

## Foundational Learning

- **Concept: Condition Number (Hessian)**
  - **Why needed here:** Core metric XQC optimizes; defines "ovalness" of loss valley. High condition number means gradients oscillate, slowing learning.
  - **Quick check question:** If the largest eigenvalue of your loss Hessian is 1000 and the smallest is 0.001, what is the condition number, and how would gradient descent behave? (Answer: 10^6; descent would be very jagged/slow)

- **Concept: Effective Learning Rate (ELR)**
  - **Why needed here:** The "true" step size depends on weight magnitude. Understanding this is key to understanding why WN is proposed.
  - **Quick check question:** If your weights double in magnitude but your optimizer's learning rate η stays fixed, what happens to your effective step size? (Answer: It halves)

- **Concept: Distributional RL (C51)**
  - **Why needed here:** XQC uses a categorical critic (logits) rather than scalar Q-value. The "loss" is a classification problem over atoms, not regression.
  - **Quick check question:** In a C51 critic, does the network output the Q-value directly? (Answer: No, it outputs a probability distribution over discrete returns)

## Architecture Onboarding

- **Component map:**
  Input: State s & Action a (concatenated) → Linear(512) → BatchNorm → ReLU → Linear(512) → BatchNorm → ReLU → Linear(512) → BatchNorm → ReLU → Linear(512) → BatchNorm → ReLU → Output: 101 logits (atoms)

- **Critical path:**
  1. Implement Joint Forward Pass: Calculate running statistics on joint distribution of current (s,a) and next (s',a') batches for effective BN in off-policy RL
  2. Reward Normalization: Scale returns to categorical support (e.g., [-5, 5]) by normalizing rewards with running std dev
  3. Weight Projection: Explicitly normalize weight matrices of all linear layers after optimizer step

- **Design tradeoffs:**
  - Batch Dependency: BN introduces batch dependency requiring careful handling of batch statistics vs. LayerNorm's instance-based approach
  - Compute vs. Parameters: XQC uses ~4.5× fewer parameters than competitors but adds BN statistics calculation and distributional loss compute overhead

- **Failure signatures:**
  - Exploding ELR: If WN removed, ELR decays to zero (loss of plasticity)
  - Unbounded Gradients: If MSE used instead of CE, gradients grow by order of magnitude (instability)
  - High Condition Number: If LN used instead of BN, Hessian spectrum shows large outliers, slowing convergence

- **First 3 experiments:**
  1. Sanity Check: Train simple critic on toy environment and plot top-10 Hessian eigenvalues over time, comparing BN vs. LN
  2. Ablation Tripod: Run XQC on HalfCheetah with three variants (Full, w/o WN, w/ MSE) and plot ELR and Gradient Norms
  3. Scaling Stress Test: Increase Update-to-Data ratio (1, 2, 8, 16) and verify XQC scales stably while baselines might overfit or diverge

## Open Questions the Paper Calls Out

### Open Question 1
Does applying well-conditioned architectural principles (BN, WN) directly to the convolutional visual encoder further improve sample efficiency in vision-based tasks? The paper used a standard, unmodified vision encoder to isolate performance gains to the MLP critic, leaving the encoder's optimization landscape unaddressed.

### Open Question 2
Why does XQC maintain stable learning dynamics without weight decay, despite theoretical analysis suggesting weight decay is necessary to upper-bound the Hessian condition number? There's a discrepancy between theoretical requirements for well-conditioned positive definite Hessian and empirical success when these conditions aren't met.

### Open Question 3
Do the optimization landscape benefits of batch normalization and distributional critics transfer to on-policy algorithms or discrete action domains? The paper focuses exclusively on off-policy, continuous control, leaving generalization to other RL settings untested.

## Limitations
- The core claims about Hessian conditioning rest on the assumption that condition number is a primary bottleneck in deep RL optimization, analogous to convex optimization
- The complex interplay between BN's batch dependency, distributional loss projection artifacts, and WN stability under varying architectures represents potential fragility not fully explored
- The theoretical claim that Hessian condition number is the dominant factor in RL optimization difficulty lacks strong external validation compared to other factors like exploration

## Confidence
- **High Confidence:** Empirical results showing XQC's state-of-the-art sample efficiency across the 70-task benchmark
- **Medium Confidence:** Specific mechanisms (BN+Wn synergy, CE loss for gradient bounding) supported by ablation studies and Hessian analysis
- **Low Confidence:** Theoretical claim that Hessian condition number is the dominant factor in RL optimization difficulty

## Next Checks
1. Validate the BN+Wn conditioning mechanism on a non-MLP architecture, such as a CNN-based critic for image-based tasks
2. Investigate the necessity and impact of using a target network with the CE loss and BN+Wn conditioning
3. Conduct a controlled experiment on a simple environment where you artificially manipulate the Hessian condition number and directly measure its correlation with learning speed, isolating it from other confounding factors