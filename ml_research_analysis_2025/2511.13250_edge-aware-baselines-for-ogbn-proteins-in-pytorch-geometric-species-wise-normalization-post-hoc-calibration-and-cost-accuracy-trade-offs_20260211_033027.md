---
ver: rpa2
title: 'Edge-aware baselines for ogbn-proteins in PyTorch Geometric: species-wise
  normalization, post-hoc calibration, and cost-accuracy trade-offs'
arxiv_id: '2511.13250'
source_url: https://arxiv.org/abs/2511.13250
tags:
- edge
- sage
- test
- node
- calibration
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This work establishes reproducible, edge-aware baselines for the
  ogbn-proteins benchmark in PyTorch Geometric. We study two key design choices: (i)
  aggregating 8-dimensional edge features into node inputs, and (ii) using edges inside
  message passing.'
---

# Edge-aware baselines for ogbn-proteins in PyTorch Geometric: species-wise normalization, post-hoc calibration, and cost-accuracy trade-offs

## Quick Facts
- arXiv ID: 2511.13250
- Source URL: https://arxiv.org/abs/2511.13250
- Authors: Aleksandar Stanković; Dejan Lisica
- Reference count: 25
- Primary result: GraphSAGE with sum-based edge-to-node aggregation and BatchNorm achieves 0.792 test ROC-AUC; post-hoc per-label calibration boosts micro-F1 from 0.096 to 0.795.

## Executive Summary
This work establishes reproducible, edge-aware baselines for the ogbn-proteins benchmark in PyTorch Geometric. We study two key design choices: (i) aggregating 8-dimensional edge features into node inputs, and (ii) using edges inside message passing. Our strongest baseline is GraphSAGE with sum-based edge-to-node features. We compare normalization schemes—LayerNorm, BatchNorm, and species-aware Conditional LayerNorm—and report compute cost (time, VRAM, parameters) alongside accuracy (ROC-AUC) and decision quality. In our primary experimental setup (hidden size 512, 3 layers, 3 seeds), sum consistently beats mean and max; BatchNorm attains the best AUC, while Conditional LayerNorm matches the AUC frontier with better thresholded F1. Finally, post-hoc per-label temperature scaling plus per-label thresholds substantially improves micro-F1 and expected calibration error (ECE) with negligible AUC change, and light label-correlation smoothing yields small additional gains. We release standardized artifacts and scripts used for all of the runs presented in the paper.

## Method Summary
The paper benchmarks GraphSAGE and GIN models on the ogbn-proteins multi-label node classification task, focusing on how to incorporate 8-D edge evidence. Edge features are first aggregated into node inputs using sum, mean, or max; then, during message passing, edges are scalarized (sum over channels) and used to weight neighbor aggregation. Three normalization schemes are compared: BatchNorm, LayerNorm, and species-aware Conditional LayerNorm. Models are trained with BCEWithLogitsLoss, evaluated on mean ROC-AUC, micro-F1@0.5, ECE, and Brier score. Post-hoc calibration via per-label temperature scaling and per-label thresholds is applied to logits before final evaluation. The primary setup uses hidden=512, 3 layers, sum aggregation, and seeds {1,2,3}.

## Key Results
- Sum-based aggregation of edge features into node inputs consistently outperforms mean and max, yielding ~0.79 test AUC vs. ~0.77–0.78 for alternatives.
- BatchNorm achieves highest ROC-AUC (0.792), while Conditional LayerNorm matches AUC with better fixed-threshold micro-F1 (0.145 vs. 0.096).
- Post-hoc per-label temperature scaling plus per-label thresholds improves micro-F1 from ~0.096 to ~0.795 and reduces ECE, with negligible AUC change.
- MLP baselines on edge-aggregated features achieve 0.743 AUC; message passing adds ~5 points AUC.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Sum-based aggregation of 8-D edge features into node inputs outperforms mean and max for downstream protein function prediction.
- Mechanism: Sum preserves total edge evidence intensity per node, whereas mean dilutes signal from high-degree nodes and max discards multi-edge accumulation. For sparse, multi-label protein function tasks, retaining cumulative association strength likely captures more functional signal per protein.
- Core assumption: Edge evidence magnitude correlates with functional relevance; more incident evidence → stronger prediction signal.
- Evidence anchors:
  - [abstract] "sum consistently beats mean and max"
  - [Section 6.3, Table 3] SAGE+LN with sum: 0.789 test AUC vs. mean: 0.775, max: 0.779
  - [Section 6.2, Table 2] MLP baselines: all top 4 performers use sum aggregation (0.74+ test AUC vs. 0.57–0.72 for others)
  - [corpus] Weak direct evidence on ogbn-proteins aggregation; related work on edge-aware GNNs exists (e.g., Edge-aware GAT-based protein binding site prediction) but does not isolate aggregation functions.
- Break condition: If edge features contain noisy or redundant channels, sum could amplify noise; max or learned attention-weighted aggregation may recover robustness.

### Mechanism 2
- Claim: BatchNorm achieves highest ROC-AUC, but Conditional LayerNorm (CLN) matches the AUC frontier with superior fixed-threshold micro-F1.
- Mechanism: BN normalizes per-batch statistics, stabilizing training for ranking-optimized AUC. CLN conditions affine parameters on species descriptors, which may improve calibration under species-wise distribution shift (mouse→zebrafish), yielding better thresholded decisions without sacrificing ranking quality.
- Core assumption: Species descriptors carry predictive signal for feature distribution shifts; per-batch BN statistics generalize across species splits.
- Evidence anchors:
  - [abstract] "BN attains the best AUC, while CLN matches the AUC frontier with better thresholded F1"
  - [Section 6.1, Table 1] SAGE+BN: 0.792 test AUC, 0.096 F1; SAGE+CLN: 0.792 test AUC, 0.145 F1
  - [Section 6.4, Figure 2] BN and CLN statistically tied on validation (mouse); close on test (zebrafish)
  - [corpus] Conditional normalization ideas supported by FiLM (Perez et al., 2018, cited in paper) and adaptive instance normalization work.
- Break condition: If species descriptor is unavailable or uninformative at inference, CLN degrades to LN; if batch composition is highly non-IID, BN may introduce train-test discrepancy.

### Mechanism 3
- Claim: Post-hoc per-label temperature scaling plus per-label thresholds dramatically improves micro-F1 and ECE with negligible AUC change.
- Mechanism: Raw logits are miscalibrated (over/under-confident). Per-label temperatures rescale logits to align predicted probabilities with empirical frequencies. Per-label thresholds (optimized for Fβ on validation) correct for class-specific optimal operating points, which is critical given the 0.89% label sparsity and heterogeneous base rates across 112 labels.
- Core assumption: Validation species (mouse) calibration generalizes to test species (zebrafish); per-label thresholding does not induce overfitting to validation label distribution.
- Evidence anchors:
  - [abstract] "post-hoc per-label temperature scaling plus per-label thresholds substantially improves micro-F1 and ECE with negligible AUC change"
  - [Section 6.6, Table 4] SAGE+LN: F1 improves from ~0.14 to ~0.795; ECE drops from 0.188 to ~0.178; AUC unchanged (~0.792–0.794)
  - [Section 5, Add-on B] Per-label temperatures optimized via NLL on validation; thresholds via Fβ maximization
  - [corpus] Temperature scaling for calibration is well-established (Guo et al., 2017, cited); per-label extension is standard in multi-label settings.
- Break condition: If validation-test label distributions diverge sharply, per-label thresholds may misgeneralize; global temperature or class-agnostic thresholds become safer.

## Foundational Learning

- Concept: Message-passing neural networks (MPNNs) with edge features
  - Why needed here: The ogbn-proteins benchmark supplies 8-D edge evidence; incorporating edges into node updates (via scalarized edge weights or edge-conditioned gating) is central to model design.
  - Quick check question: Can you explain how GraphSAGE aggregates neighbor states and where a scalar edge weight would enter the aggregation?

- Concept: Normalization layers in deep GNNs (BatchNorm, LayerNorm, Conditional LayerNorm)
  - Why needed here: The paper compares three schemes; understanding their statistics (batch vs. per-node vs. conditioned) is required to interpret the AUC/F1 trade-offs.
  - Quick check question: What statistics does BatchNorm compute vs. LayerNorm, and why might conditioning on species descriptors help cross-species generalization?

- Concept: Multi-label calibration and threshold selection
  - Why needed here: The paper shows large micro-F1 gains from per-label temperature scaling and thresholds; reproducing this requires understanding probability calibration and label-specific decision thresholds.
  - Quick check question: Given calibrated probabilities for 112 sparse labels, how would you choose a per-label threshold to maximize micro-F1 on a validation set?

## Architecture Onboarding

- Component map:
  Input layer (sum/mean/max edge→node aggregation) -> Backbone (3-layer GraphSAGE, hidden=512, scalarized edge weights) -> Normalization (BN/LN/CLN) -> Output (112 logits, BCEWithLogitsLoss) -> Post-hoc (per-label temperature scaling + per-label thresholds)

- Critical path:
  1. Edge→node aggregation (sum) → strong node signal
  2. GraphSAGE message passing with scalarized edge weights
  3. Normalization choice (BN for max AUC; CLN for better F1)
  4. Post-hoc calibration + thresholds (essential for decision quality)

- Design tradeoffs:
  - Sum vs. mean/max aggregation: Sum maximizes AUC but may over-emphasize high-degree nodes; mean normalizes by degree; max captures strongest edge only.
  - BN vs. CLN: BN yields top AUC; CLN yields better calibrated F1 at matched AUC. CLN adds ~1M parameters (species embedding → affine prediction).
  - Calibration: Per-label temperatures + thresholds → large F1 gain, minimal AUC impact; requires validation labels and careful threshold selection.
  - MLP vs. GNN: MLP on edge-aggregated features achieves 0.743 AUC (vs. 0.792 for SAGE); message passing adds ~5 points AUC.

- Failure signatures:
  - Very low micro-F1@0.5 with reasonable AUC: Uncalibrated outputs; apply per-label temperature + thresholds.
  - Training instability or divergence with LayerNorm: Try BatchNorm or reduce learning rate.
  - Poor cross-species performance: Check species descriptor availability for CLN; consider simpler BN or label-correlation smoothing.
  - GIN underperforming SAGE: Expected on this benchmark (0.761 vs. 0.792 AUC); prefer SAGE.

- First 3 experiments:
  1. Reproduce SAGE+BN baseline (hidden=512, 3 layers, sum aggregation, 3 seeds). Target: ~0.79 test AUC. Verify args.json / metrics.json output format.
  2. Ablate edge→node aggregation: Compare sum vs. mean vs. max with SAGE+LN. Expect sum > mean ≈ max by ~0.01–0.02 AUC.
  3. Apply post-hoc calibration: Fit per-label temperatures and thresholds on validation logits for SAGE+BN and SAGE+CLN. Report micro-F1 and ECE; expect F1 improvement from ~0.1 to ~0.8 with minimal AUC change.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does end-to-end training of an unfrozen edge-network MPNN outperform GraphSAGE baselines at matched parameters and VRAM?
- Basis: [explicit] Section 8 ("Future Work") and Section 7 ("Limitations"), where authors note the exclusion of unfrozen results due to time constraints.
- Why unresolved: The paper only tests frozen-gate MPNNs (which collapse) and simple scalarized edges; the potential of learned edge gates remains unquantified.
- Evidence: Reporting AUC, F1, and ECE for the unfrozen configuration provided in the text compared against the main SAGE baselines.

### Open Question 2
- Question: Can lightweight Graph Transformers utilizing 8-D edge evidence as attention biases improve cross-species transfer over GraphSAGE?
- Basis: [explicit] Section 8 ("Graph Transformers with edge channels").
- Why unresolved: The current study focuses on SAGE and GIN; it is unknown if attention mechanisms specifically leverage edge evidence better for the species-wise split.
- Evidence: A comparison of ROC-AUC and micro-F1 between a proposed Graph Transformer and SAGE at identical parameter counts.

### Open Question 3
- Question: Does replacing the heuristic co-occurrence matrix with a learned label-graph GNN or Gene Ontology (GO) priors improve performance?
- Basis: [explicit] Section 8 ("Label graph learning") and Section 5 ("Add-on C").
- Why unresolved: The paper uses a simple logit-space smoothing based on training co-occurrences; richer structural dependencies between labels are suggested but untested.
- Evidence: Comparison of micro-F1 and ECE between the heuristic smoothing and a learned label-graph approach on the test set.

## Limitations

- Core training hyperparameters (optimizer, learning rate, batch size, dropout) are not specified, limiting exact reproduction.
- Conditional LayerNorm conditioning mechanism and species descriptor format are underspecified beyond "cln_mode=desc".
- Early stopping patience and max epochs are not provided.

## Confidence

- Edge aggregation superiority (sum > mean/max): **High** - well-supported by controlled ablations in Table 3 and MLP baselines.
- BatchNorm vs. Conditional LayerNorm trade-off: **Medium** - statistically close results; CLN advantage depends on species descriptor quality and inference availability.
- Post-hoc calibration gains: **High** - temperature scaling and per-label thresholds are established techniques; results are consistent with Guo et al. (2017) and standard multi-label calibration practice.

## Next Checks

1. Replicate Table 3 aggregation ablation: train SAGE+LN with sum/mean/max edge→node features, report test AUC (expect sum ~0.789, mean ~0.775, max ~0.779).
2. Verify post-hoc calibration pipeline: apply per-label temperatures + thresholds to SAGE+BN validation logits, report micro-F1 and ECE (expect F1 jump from ~0.096 to ~0.8).
3. Confirm cross-species CLN behavior: train SAGE+CLN with species descriptors, evaluate validation AUC and F1; compare to BN baseline to verify claimed trade-off.