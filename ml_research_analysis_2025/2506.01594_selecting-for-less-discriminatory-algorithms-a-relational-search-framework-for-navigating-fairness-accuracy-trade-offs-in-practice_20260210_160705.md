---
ver: rpa2
title: 'Selecting for Less Discriminatory Algorithms: A Relational Search Framework
  for Navigating Fairness-Accuracy Trade-offs in Practice'
arxiv_id: '2506.01594'
source_url: https://arxiv.org/abs/2506.01594
tags:
- fairness
- search
- data
- loan
- algorithms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes a "horizontal LDA search" framework to identify
  less discriminatory algorithms by comparing model families for fairness-performance
  trade-offs, rather than relying solely on within-model hyperparameter optimization.
  Using 2021 HMDA mortgage lending data, the authors evaluate five common ML models
  (logistic regression, KNN, CART, NB, random forest) across multiple fairness metrics
  (EOP, FPERB, EO, PPP, PCB, NCB) and assess their accuracy with and without race
  as a feature.
---

# Selecting for Less Discriminatory Algorithms: A Relational Search Framework for Navigating Fairness-Accuracy Trade-offs in Practice

## Quick Facts
- arXiv ID: 2506.01594
- Source URL: https://arxiv.org/abs/2506.01594
- Reference count: 40
- Primary result: Horizontal model family comparison (vs. within-model optimization) enables efficient identification of less discriminatory algorithms aligned with sector-specific fairness and accuracy priorities

## Executive Summary
This paper introduces a "horizontal LDA search" framework that compares different model families for fairness-accuracy trade-offs, rather than relying solely on within-model hyperparameter optimization. Using 2021 HMDA mortgage lending data, the authors evaluate five common ML models (logistic regression, KNN, CART, NB, random forest) across multiple fairness metrics and assess their accuracy with and without race as a feature. The framework demonstrates that model family selection significantly impacts fairness outcomes—for example, random forest shows the highest accuracy but also the largest racial disparities in denial rates. This approach enables resource-constrained organizations to efficiently identify fairer models aligned with sector-specific fairness and accuracy priorities.

## Method Summary
The authors train five algorithm families (Logistic Regression, k-Nearest Neighbors, CART, Gaussian Naïve Bayes, and Random Forest) on 2021 HMDA mortgage data, evaluating each in two conditions: including race as a feature and excluding race (race-blind). They compute six fairness metrics (Equal Opportunity, False Positive Error Rate Balance, Equal Odds, Positive Predictive Parity, Positive/Negative Class Balance) alongside accuracy and Expected Value of loan portfolios. The horizontal approach compares fairness-accuracy trade-offs across model families rather than optimizing within a single model's hyperparameters.

## Key Results
- Random Forest achieved highest accuracy (67%) but showed largest racial disparities in denial rates (43% FPERB disparity)
- Logistic Regression showed moderate accuracy (64%) with lowest FPERB disparity (10%)
- Models incorporating race as a feature generally achieved higher financial inclusion and often lower denial rates for Black applicants
- Horizontal model family comparison revealed fairness variance comparable to or greater than within-model hyperparameter variance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Comparing fairness across model families surfaces discriminatory patterns that single-model optimization cannot detect.
- Mechanism: Different model architectures encode feature relationships differently, producing meaningfully different fairness outcomes even at similar accuracy levels. Horizontal search exploits this structural variance.
- Core assumption: Model families exhibit fairness variance comparable to or greater than within-model hyperparameter variance.
- Evidence anchors: Random Forest shows 43% FPERB disparity vs. Logistic Regression's 10%; adjacent LDA search literature focuses on within-model optimization.

### Mechanism 2
- Claim: Including protected attributes as features can paradoxically reduce discriminatory outcomes in some model families.
- Mechanism: Explicit race features allow certain models to account for historical bias patterns rather than learning them through proxies, stabilizing predictions across groups. Effect varies by model architecture.
- Core assumption: Protected attribute inclusion is legally permissible and data-quality sufficient.
- Evidence anchors: Authors "assess their accuracy with and without race as a feature"; models incorporating race generally achieved higher levels of financial inclusion.

### Mechanism 3
- Claim: Relational trade-off visualization enables non-expert stakeholders to participate in model selection without requiring deep ML expertise.
- Mechanism: By plotting competing objectives on interpretable axes, decision-makers can navigate fairness-accuracy trade-offs through explicit value judgments rather than opaque optimization.
- Core assumption: Stakeholders can and will make explicit trade-off decisions when presented clearly.
- Evidence anchors: Framework demonstrates "organizations to efficiently identify fairer models aligned with sector-specific fairness and accuracy priorities."

## Foundational Learning

- **Concept: Model Multiplicity**
  - Why needed here: The entire horizontal LDA search framework presupposes that multiple models with similar accuracy can have meaningfully different fairness properties.
  - Quick check question: Can you explain why two models with 67% accuracy might have different denial rates for protected groups?

- **Concept: Fairness Metric Incompatibility**
  - Why needed here: The paper evaluates six metrics because no single metric captures all fairness dimensions—trade-offs between metrics are unavoidable.
  - Quick check question: Why might Equal Opportunity parity conflict with False Positive Error Rate Balance?

- **Concept: Relational vs. Absolute Fairness**
  - Why needed here: The framework rejects "objective mathematical fairness" in favor of context-relative trade-off navigation.
  - Quick check question: How does framing fairness as relational rather than absolute change what constitutes a "fair" model?

## Architecture Onboarding

- **Component map:** Data layer (HMDA dataset with race/ethnicity encoding) -> Model layer (five algorithm families trained with/without race feature) -> Metric layer (six fairness metrics + accuracy) -> Visualization layer (trade-off plots) -> Decision layer (sector-specific priority weighting)

- **Critical path:** 1) Define protected groups and fairness metrics relevant to domain; 2) Train baseline models across families with standardized preprocessing; 3) Compute all fairness metrics per model; 4) Generate trade-off visualizations for stakeholder review; 5) Document selection rationale with explicit trade-off justification

- **Design tradeoffs:** Lightweight horizontal search vs. comprehensive vertical optimization (resource constraints); Race-aware vs. race-blind modeling (legal context); Single-metric optimization vs. multi-metric navigation (stakeholder complexity)

- **Failure signatures:** All models show similar fairness metrics → horizontal search underpowered, add model families; Visualizations hide outlier groups → add disaggregated subgroup analysis; Stakeholders cannot reach consensus on trade-offs → return to objective operationalization step

- **First 3 experiments:** 1) Replicate the five-model comparison on your domain dataset; verify fairness variance across families; 2) Run ablation with/without protected attributes; measure impact per model family and metric; 3) Present trade-off visualizations to non-technical stakeholders; iterate on axis definitions until decisions are defensible

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How should the framework assumptions (particularly the "Data Representativeness" and "Racial Negative Impact" assumptions) be updated to apply to low-income mortgage applicants and high-risk applicants, both with and without consideration of protected characteristics?
- Basis in paper: The authors state: "Future work will be needed to investigate the updates required to apply the assumptions to low-income mortgage applicants and high-risk applicants with and without consideration of protected characteristics."
- Why unresolved: The current assumptions treat all applicants uniformly and focus on Black vs. Non-Hispanic White disparities, without accounting for income-level intersections or varying risk profiles that may require different fairness calibration.
- What evidence would resolve it: Empirical analysis applying the horizontal LDA search framework to stratified income and risk subgroups, comparing how model selection outcomes differ when assumptions are modified.

### Open Question 2
- Question: What constitutes a "reasonable" scope and duration for LDA searches under the current legal regime, given varying organizational resource constraints?
- Basis in paper: The paper cites Laufer et al. [28]'s question about "how to bound this space through reasonable temporal and resource constraints," and explicitly frames horizontal LDA search as addressing resource-constrained settings without specifying what makes a search legally sufficient.
- Why unresolved: Legal standards for LDA search adequacy remain undefined, leaving organizations uncertain whether lightweight horizontal searches alone satisfy compliance obligations or whether vertical hyperparameter optimization is required.
- What evidence would resolve it: Regulatory guidance or case law establishing minimum search requirements, coupled with empirical comparisons of horizontal vs. combined horizontal-vertical search outcomes across organizations of different sizes.

### Open Question 3
- Question: How generalizable is the horizontal LDA search framework beyond mortgage lending to other high-stakes domains with different fairness-accuracy priority trade-offs?
- Basis in paper: The paper states the methodology "focuses primarily on mortgage lending and housing" but claims issues are "directly applicable to other sectors." However, all empirical evidence comes from 2021 HMDA data alone, with no cross-domain validation.
- Why unresolved: Mortgage lending has unique characteristics (heavily regulated, binary outcomes, well-documented historical bias) that may affect how model families trade off fairness and accuracy—other domains (employment, criminal justice, healthcare) may show different patterns.
- What evidence would resolve it: Application of the identical horizontal LDA search methodology to at least two additional high-stakes domains (e.g., hiring data, credit scoring beyond mortgages) demonstrating whether model family rankings for fairness-accuracy trade-offs are domain-stable or domain-specific.

## Limitations
- Assumes legal permissibility of race-aware modeling and does not address proxy discrimination where race information is absent
- May miss optimal models within large model families that vertical search could discover
- Constrained to five model families and specific HMDA mortgage data, limiting generalizability to other domains

## Confidence

- **High confidence**: Core claim that model family selection impacts fairness outcomes (supported by clear empirical variance in Table 4.1.1)
- **Medium confidence**: Counterintuitive claim that race-aware modeling reduces disparities (mechanism plausible but lacks corpus validation)
- **Medium confidence**: Stakeholder navigation framework effectiveness (theoretically sound but not empirically validated with actual non-technical users)

## Next Checks
1. Replicate the horizontal search on a different domain dataset (e.g., credit scoring or hiring) to test generalizability of model family fairness variance
2. Conduct user studies with non-technical stakeholders to evaluate whether trade-off visualizations enable effective decision-making
3. Compare horizontal search outcomes against comprehensive vertical hyperparameter optimization on select model families to quantify opportunity cost