---
ver: rpa2
title: Towards Reliable ML Feature Engineering via Planning in Constrained-Topology
  of LLM Agents
arxiv_id: '2601.10820'
source_url: https://arxiv.org/abs/2601.10820
tags:
- code
- actor
- task
- feature
- planner
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a planner-guided, constrained-topology multi-agent
  framework for reliable ML feature engineering and repository-level code generation.
  The system uses an LLM-powered planner to orchestrate agents within a constrained
  execution graph, dynamically selecting actors, generating context-aware prompts,
  and leveraging downstream failures to retroactively correct upstream artifacts.
---

# Towards Reliable ML Feature Engineering via Planning in Constrained-Topology of LLM Agents

## Quick Facts
- arXiv ID: 2601.10820
- Source URL: https://arxiv.org/abs/2601.10820
- Reference count: 40
- 38% improvement over manual workflows, 150% improvement over unplanned workflows in pass@3 metric

## Executive Summary
This paper introduces a planner-guided, constrained-topology multi-agent framework for reliable ML feature engineering and repository-level code generation. The system uses an LLM-powered planner to orchestrate agents within a constrained execution graph, dynamically selecting actors, generating context-aware prompts, and leveraging downstream failures to retroactively correct upstream artifacts. It supports human-in-the-loop intervention at critical steps to ensure reliability and maintainability. Experiments on a novel PySpark-based dataset show a 38% improvement over manual workflows and a 150% improvement over unplanned workflows in pass@3 metric, while reducing feature engineering cycles from three weeks to one day in production use.

## Method Summary
The framework implements a constrained-topology multi-agent system where a planner LLM directs traffic between specialized actors (config_generator, utils_retriever, code_template_generator, code_generator, testcase_generator, testcase_coder) according to a predefined directed graph of valid transitions. The planner dynamically selects the next actor, generates prompts, and can request human intervention when success criteria are ambiguous. It maintains short-term memory to track execution state and uses downstream failures to retroactively correct upstream artifacts. The system was implemented using Claude 3.7 Sonnet with temperature 0.1 on an in-house PySpark benchmark dataset.

## Key Results
- 38% improvement over manual workflows in pass@3 metric
- 150% improvement over unplanned workflows in pass@3 metric
- Reduced feature engineering cycles from three weeks to one day in production use

## Why This Works (Mechanism)

### Mechanism 1: Constrained Topology Reduces Error Propagation
- Claim: Bounding permissible actor transitions improves task completion reliability compared to unconstrained multi-agent systems.
- Mechanism: The environment is modeled as a directed graph G=(V,Ê) where edges define only valid execution paths. Actors can only invoke permitted next actors (e.g., `config_generator` → `utils_retriever` or `code_template_generator`), preventing unbounded exploration and limiting cascading failures.
- Core assumption: The workflow can be decomposed into stages with stable dependency relationships that do not require cross-stage shortcuts.
- Evidence anchors:
  - [abstract] "...planner-guided, constrained-topology multi-agent framework that generates code for repositories in a multi-step fashion"
  - [section C, Table 4] Explicit transition rules between actors
  - [corpus] Weak—neighbor papers do not test constrained vs. unconstrained topologies directly.
- Break condition: If tasks require frequent non-adjacent actor calls or dynamic edge creation, the constraint becomes a bottleneck.

### Mechanism 2: Retroactive Upstream Correction via Downstream Failure Signals
- Claim: Feeding downstream errors back to the planner enables targeted upstream fixes rather than blind retries.
- Mechanism: When an actor fails (e.g., `code_generator` produces failing PySpark code), the planner inspects short-term memory M_st to determine whether the failure originated upstream (e.g., incorrect config from `config_generator`) and re-invokes the responsible actor with corrected context.
- Core assumption: Error attribution can be inferred from execution traces and actor outputs without formal causal models.
- Evidence anchors:
  - [abstract] "...use downstream failures to retroactively correct upstream artifacts"
  - [section 2, Methodology] Planner "analyzes reported errors to identify whether the failure stems from the code generator itself or from an upstream actor"
  - [corpus] ALMAS and State/Memory papers emphasize failure recovery but do not isolate retroactive upstream correction as a mechanism.
- Break condition: If errors are ambiguous or caused by environment issues (Spark runtime, data distribution), attribution fails and the planner may loop.

### Mechanism 3: Human-in-the-Loop at Ambiguity Points Reduces Cognitive Overhead
- Claim: Selective human intervention at underspecified steps improves alignment without overwhelming developers.
- Mechanism: The planner requests human help only when actors lack explicit success criteria, when repeated retries fail, or when domain knowledge (e.g., null representations in columns) is missing. Humans provide atomic clarifications rather than full implementations.
- Core assumption: Humans can provide concise, correct answers to narrow questions without full context reloading.
- Evidence anchors:
  - [abstract] "It can request human intervention at critical steps, ensuring generated code is reliable, maintainable, and aligned with team expectations"
  - [section 2, Planner description] "...determines when human confirmation or intervention is required—for example, to verify an actor without explicit success criteria"
  - [corpus] Human-in-the-loop software development agents (Takerngsaksiri et al.) emphasize similar selective intervention, though without the constrained-topology framing.
- Break condition: If the planner over-requests human input or asks questions humans cannot answer concisely, the intervention becomes a throughput limiter.

## Foundational Learning

- Concept: **Directed Graph Workflow Modeling**
  - Why needed here: The entire orchestration hinges on representing valid actor transitions as edges. Understanding graph traversal, reachability, and cycle detection is essential to debug why certain paths are or are not taken.
  - Quick check question: Given a 5-node actor graph with edges A→B, A→C, B→D, C→D, D→E, what are all valid paths from A to E?

- Concept: **ReAct-Style Reasoning and Acting**
  - Why needed here: Actors use a ReACT-inspired pattern—generate failure reason, propose fix, retry—to handle errors autonomously before escalating.
  - Quick check question: If an actor proposes a fix but outputs `TERMINATE`, what should the planner do next?

- Concept: **Short-Term Memory for Multi-Turn State Tracking**
  - Why needed here: The planner maintains M_st (status, inputs/outputs, errors, fixes) to inform subsequent decisions. Understanding how state accumulates and prunes is critical for debugging loops.
  - Quick check question: What information must be present in M_st for the planner to correctly attribute a downstream test failure to an upstream config error?

## Architecture Onboarding

- Component map:
  - Planner (P) -> Executor -> Actors (config_generator, utils_retriever, code_template_generator, code_generator, testcase_generator, testcase_coder) -> Short-term Memory (M_st) -> HITL Tool

- Critical path:
  1. Parse FSC + DFR → `config_generator` → valid YAML config.
  2. `utils_retriever` + `code_template_generator` → template with method signatures and selected utilities.
  3. `code_generator` → executable PySpark script.
  4. `testcase_generator` → `testcase_coder` → unit tests with >80% pass rate.
  5. Planner confirms all success criteria; artifacts merged to repository.

- Design tradeoffs:
  - **Constrained topology vs. flexibility**: Rigid edges reduce errors but limit ad-hoc recovery paths.
  - **Loose success criteria (early actors) vs. strict criteria (end actors)**: Early flexibility speeds iteration but defers validation.
  - **HITL frequency**: More queries improve alignment but reduce autonomy; default benchmark setting disables human help.

- Failure signatures:
  - **Actor loop**: Same actor called repeatedly without progress → check M_st for fix proposals marked `TERMINATE`.
  - **Upstream misattribution**: `code_generator` fails but root cause is `config_generator` → inspect error messages for schema/column mismatches.
  - **Test cascade failure**: `testcase_coder` fails with <80% pass rate → check if `code_generator` output diverged from template.

- First 3 experiments:
  1. **Baseline comparison**: Run Sequential Actor Selection and Graph-Constrained Random Selection on the 10-task benchmark; replicate Table 1 pass@3 results to validate environment setup.
  2. **Ablate retroactive correction**: Disable planner's upstream error attribution (always retry failing actor only); measure impact on pass@3 and actor failure rates.
  3. **HITL stress test**: Enable human-in-the-loop with simulated responses (correct, ambiguous, unavailable); quantify how response quality affects task completion and planner step count.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: To what extent does fine-tuning the planner LLM on domain-specific feature engineering tasks improve plan generation accuracy compared to the current zero-shot prompting approach?
- Basis in paper: [explicit] Section 6 states "Future directions include fine-tuning the planner LLM to improve plan generation."
- Why unresolved: The current implementation relies on fixed prompting strategies with Claude 3.7 Sonnet without domain-specific weight updates.
- What evidence would resolve it: Experiments comparing the pass@3 metric of a fine-tuned planner model against the baseline on the PySpark dataset.

### Open Question 2
- Question: How does the addition of a long-term memory module affect the planner's ability to retain context and prevent repetitive errors across extended multi-turn code generation episodes?
- Basis in paper: [explicit] Section 6 proposes "incorporating long-term memory for better context retention" as a future direction.
- Why unresolved: The current framework relies solely on short-term memory ($M_{st}$) which limits context retention over long, complex workflows.
- What evidence would resolve it: Ablation studies measuring context retention and error repetition rates in tasks requiring >10 planner steps with and without long-term memory.

### Open Question 3
- Question: Can intermediate validation mechanisms be integrated into the constrained topology to reduce the latency of error detection inherent in relying on downstream failures?
- Basis in paper: [inferred] Section 6 notes the system "relies on fixed prompting and downstream validation, which can delay error detection."
- Why unresolved: The framework currently corrects upstream artifacts (like config files) only after downstream actors (like the code generator) fail, which is inefficient.
- What evidence would resolve it: A reduction in average actor failure rates and total execution steps when intermediate validation actors are inserted into the execution graph.

### Open Question 4
- Question: Does the reported 38% improvement over manual workflows generalize to a significantly larger benchmark with more diverse coding standards and feature types?
- Basis in paper: [inferred] Section 6 acknowledges the "current task dataset is relatively small," limiting the diversity of scenarios tested.
- Why unresolved: The study relies on a novel in-house dataset of only 10 tasks, which may not capture the full variance of production environments.
- What evidence would resolve it: Evaluation of the framework on an expanded public dataset or a larger set of internal tasks with heterogeneous coding styles and complex dependencies.

## Limitations

- Constrained topology may not handle complex, cross-cutting feature engineering tasks requiring dynamic actor invocations
- Retroactive correction depends on reliable error attribution that may fail in distributed Spark environments
- Human-in-the-loop effectiveness depends on domain expert availability and quality, not modeled in the study
- Private benchmark dataset prevents independent validation of claimed improvements

## Confidence

- **High confidence**: The constrained topology framework architecture is clearly specified and implementable
- **Medium confidence**: The retroactive upstream correction mechanism is theoretically sound but depends on reliable error attribution
- **Medium confidence**: The 38% and 150% improvement metrics are well-defined but cannot be independently verified
- **Low confidence**: Real-world applicability beyond PySpark feature engineering remains unproven

## Next Checks

1. Implement the constrained topology with mock actors and verify that invalid transitions are blocked as specified in Table 4
2. Test retroactive correction by injecting upstream errors and measuring whether the planner correctly identifies and fixes them
3. Conduct a controlled ablation study comparing pass@3 rates with and without the constrained topology to quantify the topology's contribution to reliability