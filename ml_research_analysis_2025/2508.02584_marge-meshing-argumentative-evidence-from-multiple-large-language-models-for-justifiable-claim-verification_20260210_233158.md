---
ver: rpa2
title: 'MArgE: Meshing Argumentative Evidence from Multiple Large Language Models
  for Justifiable Claim Verification'
arxiv_id: '2508.02584'
source_url: https://arxiv.org/abs/2508.02584
tags:
- claim
- argument
- marge
- each
- arguments
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MArgE introduces a structured, multi-LLM framework for claim verification
  by generating and meshing argument trees from each model, then evaluating them with
  formal argumentative semantics. Instead of relying on unstructured debates or opaque
  reasoning traces, MArgE preserves transparent, inspectable rationales and aggregates
  them via quantitative bipolar argumentation frameworks.
---

# MArgE: Meshing Argumentative Evidence from Multiple Large Language Models for Justifiable Claim Verification

## Quick Facts
- arXiv ID: 2508.02584
- Source URL: https://arxiv.org/abs/2508.02584
- Authors: Ming Pok Ng; Junqi Jiang; Gabriel Freedman; Antonio Rago; Francesca Toni
- Reference count: 9
- Primary result: Outperforms single LLMs and debate baselines with up to 8.4% accuracy gains on claim verification

## Executive Summary
MArgE introduces a structured, multi-LLM framework for claim verification by generating and meshing argument trees from each model, then evaluating them with formal argumentative semantics. Instead of relying on unstructured debates or opaque reasoning traces, MArgE preserves transparent, inspectable rationales and aggregates them via quantitative bipolar argumentation frameworks. Evaluated across three claim verification datasets, MArgE consistently outperforms single LLMs, simple ensemble methods, ArgLLMs, and multi-LLM debate baselines, achieving accuracy gains of up to 8.4%. Ablation studies show that incorporating claim-level scoring and structured meshing yields the strongest improvements, while maintaining low variability across runs.

## Method Summary
MArgE operates through a four-stage pipeline: multiple LLMs generate structured argument trees (pro/con arguments as nodes), these trees are meshed via union or semantic similarity, an external evaluator scores the claim and arguments, and dialectical strength propagation via DF-QuAD gradual semantics produces the final verdict. The approach uses depth-1 or depth-2 trees with quantized open models for generation, GPT-4o-mini for scoring, and sentence transformers for semantic merging. The method is evaluated on three claim verification datasets with binary classification targets.

## Key Results
- Outperforms single LLM baselines by up to 8.4% accuracy across all three datasets
- External claim-level scoring (Est.All) provides consistent ~13.31% performance gains over argument-only scoring
- Structured meshing with semantic similarity shows negligible performance difference from simple union (<1%)
- Debate-based ensemble baselines are more sensitive to role assignment and can degrade below single-model performance

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Structured argument trees provide more faithful justifications than chain-of-thought outputs.
- **Mechanism:** Each LLM generates discrete pro/con arguments as nodes in a bipolar tree, rather than unstructured reasoning traces. This discretization makes individual arguments inspectable and prevents conflicting arguments from hiding within continuous text.
- **Core assumption:** LLMs can reliably identify and separate supporting vs. attacking arguments when explicitly prompted with structured formats.
- **Evidence anchors:**
  - [abstract] "We use a variant of Argumentative LLMs (ArgLLMs)... to construct structured argument trees... creating an inspectable pathway from the initial arguments to the final claim verification decisions."
  - [Section 3, Discussion] "Given the claim, our approach of obtaining evidence from multiple models, composed as trees of short supporting and attacking arguments, discretises the reasoning and thus provides a diverse and transparent rationale."
- **Break condition:** If LLMs conflate support/attack relations in outputs (e.g., generating "Agree:" statements that actually attack), tree structure becomes noisy and propagation unreliable.

### Mechanism 2
- **Claim:** External claim-level scoring is the dominant contributor to accuracy improvements.
- **Mechanism:** A stronger evaluator LLM (GPT-4o-mini) scores both generated arguments AND the root claim itself. This claim-level score provides a prior that the gradual semantics then adjusts based on argument quality.
- **Core assumption:** The evaluator model has better calibration or knowledge than the argument-generating models.
- **Evidence anchors:**
  - [Section 4.3, Table 5] "The most influential choice is when we use the scorer to estimate the quality of the claim in addition to the newly generated arguments... a consistent performance gain of about 13.31% can be observed."
  - [Section 4.2] "MArgE, without directly prompting the scorer for an answer over the claim, outperforms the GPT scorer's CoT accuracy on two of three datasets by 3.5% and 1.0%."
- **Break condition:** If evaluator model shares systematic biases with generators (e.g., all models trained on similar corpora with same misconceptions), scoring amplifies rather than corrects errors.

### Mechanism 3
- **Claim:** Tree meshing with semantic merging reduces redundancy without sacrificing accuracy.
- **Mechanism:** After each LLM generates its argument tree, a sentence encoder computes cosine similarity between arguments from different trees. Arguments exceeding a similarity threshold are merged into single nodes, creating a unified QBAF while preserving diverse perspectives.
- **Core assumption:** Semantically similar arguments from different models represent the same reasoning and should be treated as one node rather than counting twice.
- **Evidence anchors:**
  - [Section 3, Step 2] "We then calculate the similarity between each pair of arguments from different trees, and merge the pair if the similarity is above some threshold."
  - [Section 4.3, Table 5] "The two meshing strategies do not show obvious performance differences" (semantic merging vs. simple union: -0.82% average, within error bounds).
- **Break condition:** If similarity threshold is too aggressive, distinct but related arguments merge incorrectly, collapsing structural nuance. If too conservative, redundant arguments inflate node count without adding information.

## Foundational Learning

- **Concept: Quantitative Bipolar Argumentation Frameworks (QBAFs)**
  - **Why needed here:** MArgE represents all arguments and claims as nodes in a QBAF with attack/support relations and base scores. Understanding how strengths propagate through the graph is essential for debugging unexpected predictions.
  - **Quick check question:** Given argument A with base score 0.8 attacking claim C with base score 0.5, and argument B with base score 0.6 supporting C, what direction will C's final strength move?

- **Concept: DF-QuAD Gradual Semantics**
  - **Why needed here:** DF-QuAD is the specific aggregation function used to compute dialectical strength from base scores and attacker/supporter strengths. The paper shows it outperforms Quadratic Energy Model (QEM) in ablations.
  - **Quick check question:** In DF-QuAD, if a node's aggregated attacker strength exceeds its supporter strength, does the final strength decrease from or increase toward the base score?

- **Concept: Multi-LLM Ensemble Failure Modes**
  - **Why needed here:** The paper explicitly compares against debate-based ensembles, showing they're sensitive to role assignment and can degrade below single-model performance. Understanding why debate fails (echo chambers, adversarial sensitivity) clarifies MArgE's design rationale.
  - **Quick check question:** Why might majority voting over LLM predictions perform worse than the best individual model in an ensemble?

## Architecture Onboarding

- **Component map:** ΓG (Argument Generation) → K LLMs → K bipolar argument trees → M (Meshing) → 1 merged BAF → E (Quality Scoring) → QBAF with base scores → Σσ (Strength Propagation) → final claim strength → g (Classification) → {True, False}

- **Critical path:** Argument quality scoring (E) → semantic propagation (Σσ). Table 5 shows scoring configuration (Est.Arg vs. Est.All) accounts for ~13% accuracy swing, while meshing strategy shows <1% difference.

- **Design tradeoffs:**
  - Depth 1 vs. Depth 2 trees: Depth 2 requires ~3x more GPT tokens (Table 3) for ~1% gain with Est.All scoring.
  - Simple union vs. semantic merging: Comparable accuracy, but semantic merging may improve interpretability by collapsing duplicates.
  - External vs. self-scoring: External scoring requires additional model calls but mitigates generator bias (Section 3, Step 3).

- **Failure signatures:**
  - High variance across runs (>2%): Check LLM temperature settings or scoring prompt consistency.
  - Accuracy below single-model baseline: Likely meshing or scoring bug causing strength miscalculation.
  - Debate baseline outperforming MArgE by >5%: Check if evaluator model is underperforming on domain (e.g., MedClaim shows weaker gains, Section 4.2).

- **First 3 experiments:**
  1. Replicate Table 4 ablation on TruthfulClaim validation split to verify scoring configurations produce expected ~13% Est.All advantage.
  2. Test with depth-1 trees and Est.All scoring only (most cost-effective configuration) against majority-vote ensemble baseline.
  3. Inject one systematically biased generator LLM to verify external evaluator scoring mitigates vs. amplifies the bias.

## Open Questions the Paper Calls Out
None

## Limitations
- External claim-level scoring assumes evaluator model has superior calibration, but evaluator bias could amplify rather than correct errors
- Semantic merging shows minimal performance difference from simple union (<1%), suggesting redundancy reduction may be negligible
- Depth-2 trees provide only ~1% accuracy gain at ~3x token cost, indicating marginal returns on complexity

## Confidence
- **High Confidence:** External claim-level scoring drives accuracy improvements (Table 5, ablation results)
- **Medium Confidence:** Structured argument trees provide inspectable justifications (consistent with design rationale but limited ablation on tree depth)
- **Low Confidence:** Semantic merging meaningfully reduces redundancy without information loss (performance difference within error bounds)

## Next Checks
1. Test MArgE with a deliberately biased generator LLM to verify external evaluator scoring corrects vs. amplifies the bias
2. Replicate the depth-2 vs. depth-1 ablation on a held-out validation set to confirm the 1% gain justifies 3x token cost
3. Compare semantic merging threshold sensitivity (0.7, 0.8, 0.9) to determine if any configuration meaningfully outperforms simple union