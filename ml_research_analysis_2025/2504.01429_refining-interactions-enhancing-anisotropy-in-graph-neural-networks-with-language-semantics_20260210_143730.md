---
ver: rpa2
title: 'Refining Interactions: Enhancing Anisotropy in Graph Neural Networks with
  Language Semantics'
arxiv_id: '2504.01429'
source_url: https://arxiv.org/abs/2504.01429
tags:
- llms
- node
- graph
- information
- lansagnn
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces LanSAGNN, a framework that extends anisotropy
  from graph embeddings to natural language semantics in text-attributed graphs. The
  method uses a dual-layer LLM finetuning architecture to generate node-pair-specific
  contextual information, ensuring structural information is meaningfully captured
  rather than treated as generic context.
---

# Refining Interactions: Enhancing Anisotropy in Graph Neural Networks with Language Semantics

## Quick Facts
- arXiv ID: 2504.01429
- Source URL: https://arxiv.org/abs/2504.01429
- Reference count: 29
- LanSAGNN achieves 91.24% accuracy on Cora, outperforming existing GNN and LLM-based methods

## Executive Summary
LanSAGNN introduces a framework that extends anisotropy from graph embeddings to natural language semantics in text-attributed graphs. The method uses a dual-layer LLM finetuning architecture to generate node-pair-specific contextual information, ensuring structural information is meaningfully captured rather than treated as generic context. An optional edge filter enhances efficiency by removing heterogeneous edges without sacrificing performance. Experiments across four standard datasets show LanSAGNN consistently outperforms existing GNN and LLM-based methods, achieving state-of-the-art accuracy while maintaining low complexity.

## Method Summary
LanSAGNN is a three-module pipeline for node classification on Text-Attributed Graphs (TAGs). First, an optional edge filter (LLMEP) classifies edge homophily from concatenated node texts and removes heterogeneous edges. Second, a dual-layer LLM finetuning architecture generates task-aligned semantic content: LLMKB creates rationale-style corpus from labeled node pairs, and LLME is LoRA-finetuned on this corpus to produce classification-relevant outputs. Third, the downstream pipeline concatenates original node text with LLME outputs from neighbors, embeds the combined text, and feeds it to a GNN for classification. The framework is evaluated on Cora, Citeseer, Pubmed, and Wiki-CS datasets using 60/20/20 train/val/test splits with accuracy as the primary metric.

## Key Results
- Achieves 91.24% accuracy on Cora, 87.56% on Wiki-CS, outperforming existing methods
- Dual-layer finetuning provides consistent gains over unfinetuned LLM baselines
- Optional edge filter reduces computational complexity with minimal accuracy loss
- Robust performance across different LLM configurations (Vicuna-7B, Vicuna-13B, LLaMA3-8B)

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Generating node-pair-specific semantic content improves downstream task performance compared to using identical node text for all neighbors.
- **Mechanism:** The LLM extracts relational information tailored to each (source_node, target_node) pair, capturing meaningful interactions rather than broadcasting generic biographical text.
- **Core assumption:** Relational semantics between node pairs contain stronger task-relevant signal than generic node self-descriptions.
- **Evidence anchors:** Abstract emphasizes "tailor-made semantic information for node pairs," Figure 1 illustrates anisotropic messaging at language level, but weak direct corpus validation.
- **Break condition:** If node texts contain no relational information relevant to target node, or if downstream task is independent of inter-node semantics.

### Mechanism 2
- **Claim:** A dual-layer LLM finetuning architecture produces outputs more aligned with graph tasks than direct prompting or unfinetuned LLMs.
- **Mechanism:** Layer 1 (LLMKB) generates rationale-style corpus from ground-truth labels and node-pair texts; Layer 2 (LLME) is finetuned on this corpus to produce classification-relevant outputs for arbitrary node pairs.
- **Core assumption:** Knowledge Base LLM can reliably extract class-relevant rationales from node-pair texts, and this corpus generalizes to unsupervised node pairs.
- **Evidence anchors:** Abstract states architecture "better align[s] LLMs' outputs with graph tasks," ablation shows performance drop without finetuning (90.38 vs 91.24 on Cora), but adjacent work doesn't isolate dual-layer effects.
- **Break condition:** If LLMKB produces low-quality or task-irrelevant rationales, finetuning corpus misaligns LLME, degrading performance below unfinetuned baselines.

### Mechanism 3
- **Claim:** Removing heterophilous edges via LLM-based edge filter preserves or improves accuracy while reducing inference cost.
- **Mechanism:** Finetuned LLM (LLMEP) classifies edge homophily based on concatenated node texts; heterogeneous edges are pruned, reducing node-pair LLM calls.
- **Core assumption:** Textual attributes contain sufficient signal to predict label homophily, and removing cross-class edges doesn't critically damage structural connectivity.
- **Evidence anchors:** Section III.C states filtering "reduce[s] computational demand while maintaining performance," Table III shows OEF reduces complexity with "almost no loss," but specific accuracy deltas not isolated in ablation.
- **Break condition:** On heterophily-heavy graphs where cross-class edges are structurally essential, aggressive pruning may disconnect critical paths or bias message-passing.

## Foundational Learning

- **Concept: Anisotropic vs Isotropic Message Passing**
  - **Why needed here:** LanSAGNN extends anisotropy from embedding-space interactions to language-space interactions. Without this baseline, the novelty of node-pair-specific text generation is unclear.
  - **Quick check question:** Given source node A connected to B and C, would an isotropic GNN send identical messages to both, and would an anisotropic GNN differentiate them? How does LanSAGNN analogize this at language level?

- **Concept: Low-Rank Adaptation (LoRA)**
  - **Why needed here:** Dual-layer architecture relies on LoRA to finetune LLMs with minimal parameter updates, enabling task alignment without full-model training.
  - **Quick check question:** What is the computational benefit of optimizing a low-rank delta Θ versus full parameter set Φ? How does this enable the Extraction LLM to adapt efficiently?

- **Concept: Text-Attributed Graphs (TAGs)**
  - **Why needed here:** The entire framework presupposes nodes with rich textual attributes and classification objective. Understanding TAG structure (V, A, T) is prerequisite to parsing input-output flow.
  - **Quick check question:** For a citation graph where each node is a paper, what constitutes textual attribute T, and how would LanSAGNN use T to generate node-pair-specific content for two neighboring papers?

## Architecture Onboarding

- **Component map:** (Optional) Edge Filter (LLMEP) -> Dual-Layer LLM Finetuning (LLMKB -> LLME) -> Downstream Pipeline (Embedding -> GNN)
- **Critical path:** Sample training node pairs → LLMKB generates rationale corpus → LoRA-finetune LLME on corpus → (Optionally) LLMEP filters edges → For each node, LLME generates neighbor-specific texts → Embed and aggregate → GNN classifies
- **Design tradeoffs:**
  - k (edges sampled per node): Higher k improves performance but increases LLM inference cost linearly; Table III shows k=1 already competitive, k=10+ yields diminishing returns
  - Edge filter on/off: Improves efficiency but risks over-pruning on heterophily graphs; not universally safe
  - Choice of LLMs: Ablation shows LLME can be Vicuna-7B, Vicuna-13B, or LLaMA3-8B with modest variance; larger models may improve rationale quality but increase cost
- **Failure signatures:**
  - "w/o Finetune" drop: Outputs lack task relevance; LLME defaults to generic text
  - "w/o OriginText" drop: Removing original node text loses baseline signal; LLME outputs alone are insufficient
  - Empty or disconnected subgraphs after OEF: Over-aggressive filtering on low-homophily data
- **First 3 experiments:**
  1. Baseline replication: Run LanSAGNN on Cora with k=5, no edge filter, Vicuna1.5-13B as LLME, GPT-3.5-turbo as LLMKB; verify accuracy within ±0.5% of reported 91.24%
  2. Ablation: w/o Finetune: Use unfinetuned Vicuna for LLME on same split; expect ~0.8-1% drop, confirming dual-layer contribution
  3. Edge filter impact: Enable OEF on Citeseer with k=5; compare runtime and accuracy to non-filtered run; check for disconnected components in A′

## Open Questions the Paper Calls Out
- How can the interpretability of LanSAGNN's decision-making process be enhanced when LLM-generated node-pair semantics are integrated with GNNs?
- How can the deployment complexity of the dual-layer LLM finetuning architecture be reduced while maintaining performance gains?

## Limitations
- Framework's dependence on high-quality, task-relevant textual attributes untested on domains where node text lacks explicit semantic relationships
- Dual-layer finetuning effectiveness hinges on LLMKB reliably extracting classification-relevant rationales, which may not generalize to imbalanced or noisy label distributions
- Edge filter performance is dataset-dependent and may degrade on heterophily-heavy graphs where cross-class edges carry essential structural information

## Confidence
- **High:** Performance improvements over baselines on benchmark datasets, dual-layer architecture contribution (verified through ablation), and anisotropic text generation mechanism
- **Medium:** Edge filter efficiency gains (specific accuracy impact not fully isolated), generalizability across different LLM families, and scalability to larger graphs
- **Low:** Long-tail performance on real-world datasets with noisy or minimal text attributes, robustness to label distribution shifts, and computational efficiency at scale

## Next Checks
1. **Heterophily Stress Test:** Evaluate LanSAGNN on graphs with known heterophily (e.g., Chameleon, Squirrel from Open Graph Benchmark) to measure edge filter impact and identify break conditions
2. **Cross-Domain Transfer:** Apply dual-layer finetuning approach to non-textual node attributes (e.g., molecular descriptors) converted to text, validating whether LLMKB→LLME pipeline generalizes beyond natural language
3. **Parameter Sensitivity Analysis:** Systematically vary LoRA rank, learning rate, and training corpus size to quantify their impact on downstream classification accuracy and identify minimum viable configurations for resource-constrained deployment