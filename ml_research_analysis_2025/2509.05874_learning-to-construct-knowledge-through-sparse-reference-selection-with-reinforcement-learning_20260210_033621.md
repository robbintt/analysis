---
ver: rpa2
title: Learning to Construct Knowledge through Sparse Reference Selection with Reinforcement
  Learning
arxiv_id: '2509.05874'
source_url: https://arxiv.org/abs/2509.05874
tags:
- references
- learning
- reference
- knowledge
- target
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of acquiring knowledge from
  scientific literature when full-text access is limited and relevant references are
  sparse among many candidates. The authors propose a Deep Reinforcement Learning
  framework that mimics human concept construction by sequentially selecting which
  papers to read based on limited metadata and abstracts.
---

# Learning to Construct Knowledge through Sparse Reference Selection with Reinforcement Learning

## Quick Facts
- arXiv ID: 2509.05874
- Source URL: https://arxiv.org/abs/2509.05874
- Reference count: 17
- An A2C agent achieves an EI of 1.026, outperforming baseline classifier (1.144) and REINFORCE (1.219) in drug-gene relation discovery

## Executive Summary
This paper tackles the challenge of acquiring knowledge from scientific literature when full-text access is limited and relevant references are sparse among many candidates. The authors propose a Deep Reinforcement Learning framework that mimics human concept construction by sequentially selecting which papers to read based on limited metadata and abstracts. The problem is formalized as a sequential decision-making task, where an agent navigates a set of candidate references to find target papers containing specific drug-gene relations, with the goal of minimizing the number of papers read.

The proposed framework consists of three components: a Recommendation System for retrieving and ranking relevant papers, an Environment for simulating the reading process and providing rewards, and an Agent that learns a policy to select the next paper to read. The agent is trained using two reinforcement learning algorithms: REINFORCE and Advantage Actor-Critic (A2C). The evaluation is conducted on a drug-gene relation discovery task using PMC papers, where the agent must find papers linking specific drugs to their corresponding genes based on titles and abstracts only.

## Method Summary
The paper formalizes sparse reference selection as a sequential decision-making problem in a reinforcement learning framework. The environment maintains a set of candidate papers and simulates the reading process by revealing full text upon selection. The agent learns to select papers from the k-nearest neighbors of the current paper, where neighbors are determined by Jaccard distance on metadata and introductions. Two RL algorithms are evaluated: REINFORCE (pure policy gradient) and A2C (actor-critic with advantage estimation). The agent receives a reward of 1/Ti (where Ti is the number of target papers) upon success and a constant penalty of -0.3 otherwise.

## Key Results
- A2C agent achieves an Evaluation Index (EI) of 1.026 across five test drugs, compared to 1.144 for baseline classifier and 1.219 for REINFORCE agent
- A2C shows more than 50% reduction in EI for harder tasks like bortezomib and dexamethasone
- The RL agents demonstrate ability to skip uninformative references and construct knowledge more efficiently than traditional methods

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Sequential decision-making over reference graphs enables more efficient target discovery than independent classification.
- Mechanism: The agent maintains and updates an observation history Ot = fθ(Ot−1, [Pt; q]), allowing it to learn which paths through the citation neighborhood lead toward targets. Unlike classifiers that score each paper independently, the RL agent learns from trajectory-level feedback.
- Core assumption: Target papers cluster in citation/semantic space such that a path exists through neighbors.
- Evidence anchors:
  - [abstract] "prioritizing which papers to read under limited time and cost"
  - [section 2.2] "At each state, the Agent observes metadata and introductory text of the current reference Pt, combined with query q and historical context"
  - [corpus] Weak direct evidence—neighbor papers address data discovery broadly but not sequential reference selection specifically.
- Break condition: If target papers are randomly distributed with no semantic clustering, neighbor-based traversal offers no advantage over random sampling.

### Mechanism 2
- Claim: Advantage Actor-Critic reduces variance in sparse-reward settings compared to pure policy gradients.
- Mechanism: A2C maintains a value function vθ(Ot) that provides a baseline for advantage estimation (Rt − vθ(Ot)), reducing the high variance inherent in REINFORCE's Monte Carlo returns. This stabilizes learning when successful episodes are rare.
- Core assumption: The value function can be learned accurately enough to provide useful baselines despite sparse rewards.
- Evidence anchors:
  - [section 3.4] "REINFORCE agents tended to greedily select nearest papers, ignoring richer observations. This strategy underperformed the baseline"
  - [section 3.5] "A2C consistently outperformed both the baseline and REINFORCE... EI improved by more than 50% on bortezomib and dexamethasone"
  - [corpus] No direct corpus evidence on A2C vs. REINFORCE for this task.
- Break condition: If episodes are so sparse that value function updates receive insufficient signal, both algorithms may fail to learn meaningful policies.

### Mechanism 3
- Claim: Restricting actions to k-nearest neighbors balances tractability with exploration capacity.
- Mechanism: At each step, the agent selects from At = {Pi | Pi ∈ top-k nearest(Pt)} rather than the full corpus. This mimics human cognitive limits while ensuring the softmax policy gθ(Ot) operates over a manageable action space (k=20 in experiments).
- Core assumption: Relevant papers are reachable within the k-neighbor graph structure.
- Evidence anchors:
  - [section 2.2] "We set k = 20 in experiments and use 1−Jaccard index over metadata/introduction as the distance metric"
  - [section 3.4] Recommendation-system distances dis(·) were incorporated into the policy via weighted action sampling
  - [corpus] Weak evidence—Graph-PReFLexOR mentions "in-situ graph reasoning" but for different domains.
- Break condition: If the true target is not connected within the k-neighbor graph from the initial paper, the agent cannot reach it regardless of policy quality.

## Foundational Learning

- Concept: **Policy Gradient Methods (REINFORCE)**
  - Why needed here: Understanding why pure Monte Carlo policy gradients exhibit high variance in sparse reward settings explains the motivation for A2C.
  - Quick check question: Can you explain why REINFORCE requires a baseline to reduce variance, and what happens without one?

- Concept: **Actor-Critic Architecture**
  - Why needed here: The A2C algorithm jointly optimizes policy (actor) and value (critic) networks—understanding their interplay is essential for debugging training dynamics.
  - Quick check question: How does the critic's value estimate change the gradient signal compared to using raw returns?

- Concept: **Markov Decision Processes with Partial Observability**
  - Why needed here: The problem involves hidden states (full texts) that become observable upon selection, which differs from standard MDPs and POMDPs.
  - Quick check question: Why does the paper argue that POMDP formulations are unsuitable for this setting?

## Architecture Onboarding

- Component map: Recommendation System -> Environment -> Agent
- Critical path:
  1. Query q arrives → Recommendation System retrieves candidate set Pq
  2. Baseline classifier selects initial paper P0
  3. Agent observes O0 = [P0; q], samples action a0 from 20 nearest neighbors
  4. Environment transitions to selected paper, reveals full text
  5. If target found: episode ends with reward 1/Ti; else: penalty applied, continue
  6. Update policy via A2C loss: λ·lossπ + (1−λ)·lossv

- Design tradeoffs:
  - **Action space size (k)**: Larger k increases exploration capacity but dilutes softmax probabilities and computation.
  - **Reward shaping**: Simple binary reward vs. similarity-based shaping—paper chose simplicity but notes alternatives exist.
  - **Initial paper selection**: RL agents depend on baseline classifier for starting point—poor initialization can trap agents in local regions.

- Failure signatures:
  - **Greedy nearest-neighbor collapse**: REINFORCE agents "greedily select nearest papers, ignoring richer observations" (Section 3.4).
  - **High HoF failures**: On doxorubicin (HoF=0.998), A2C achieved EI=0.696 vs. baseline 0.477—"random selection can occasionally outperform structured strategies."
  - **Variance in episode outcomes**: Figure 2 boxplots show substantial spread across 30 episodes per drug.

- First 3 experiments:
  1. **Reproduce baseline vs. A2C comparison** on the five test drugs using provided hyperparameters (Adam, lr=1e-3, λ=0.5, γ=0.9). Verify EI values match Table 3 within sampling variance.
  2. **Ablate the distance-weighted action sampling** by removing dis(·) from the A2C policy. Compare EI to assess whether recommendation-system distances provide meaningful inductive bias.
  3. **Sweep k ∈ {10, 20, 40}** on bortezomib (largest EI improvement) to characterize sensitivity to action space size and identify whether performance gains come from better exploration or improved policy learning.

## Open Questions the Paper Calls Out
None

## Limitations
- The k-nearest-neighbor restriction may not adequately capture semantic similarity for diverse biomedical concepts, potentially limiting exploration capacity
- Dependence on baseline classifier for initial paper selection creates an upper bound on RL agent performance based on classifier recall
- The evaluation metric assumes known number of target papers, which may not hold in real-world applications with uncertain scope of relevant literature

## Confidence

- **High confidence**: A2C outperforms REINFORCE and baseline in controlled experiments with known target papers.
- **Medium confidence**: The mechanism of sequential reference selection provides efficiency gains generalizable to other sparse-knowledge domains.
- **Low confidence**: The Jaccard-based neighbor selection adequately captures semantic similarity for diverse biomedical concepts.

## Next Checks

1. Test the framework on a different domain (e.g., chemical-protein interactions) to assess generalizability beyond drug-gene relations.
2. Implement and compare alternative neighbor selection methods (semantic embeddings, citation networks) to evaluate whether Jaccard distance is optimal.
3. Conduct a sensitivity analysis on the initial paper selection by varying the baseline classifier's recall rate to quantify its impact on RL agent performance.