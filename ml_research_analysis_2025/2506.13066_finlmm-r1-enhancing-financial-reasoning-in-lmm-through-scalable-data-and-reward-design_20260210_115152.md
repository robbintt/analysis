---
ver: rpa2
title: 'FinLMM-R1: Enhancing Financial Reasoning in LMM through Scalable Data and
  Reward Design'
arxiv_id: '2506.13066'
source_url: https://arxiv.org/abs/2506.13066
tags:
- reasoning
- financial
- uni00000013
- multimodal
- answer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the challenge of enhancing financial reasoning
  in large multimodal models (LMMs), which suffer from limited datasets and ineffective
  reasoning optimization. To address this, the authors propose FinLMM-R1, an integrated
  framework combining an automated and scalable pipeline for data construction (ASP)
  with an enhanced training strategy (TAR-LMM).
---

# FinLMM-R1: Enhancing Financial Reasoning in LMM through Scalable Data and Reward Design

## Quick Facts
- arXiv ID: 2506.13066
- Source URL: https://arxiv.org/abs/2506.13066
- Reference count: 40
- Key outcome: FinLMM-R1 significantly improves financial reasoning in large multimodal models (LMMs) through automated data construction and enhanced training with adversarial rewards, achieving superior accuracy and reasoning depth on 7 benchmarks.

## Executive Summary
This paper addresses the challenge of enhancing financial reasoning in large multimodal models (LMMs), which suffer from limited datasets and ineffective reasoning optimization. The authors propose FinLMM-R1, an integrated framework that combines an Automated and Scalable Pipeline (ASP) for data construction with an enhanced training strategy (TAR-LMM). ASP resolves textual-visual misalignment in financial reports through a separate paradigm of question-answer generation and image-question alignment, yielding 89,378 aligned image-question pairs from 23,397 financial reports. TAR-LMM extends the two-stage LMM-R1 training framework with additional reward mechanisms, including image selection, reasoning length, and adversarial rewards, to jointly optimize visual perception, reasoning efficiency, and logical coherence. Extensive experiments demonstrate that ASP-derived data and the TAR-LMM framework significantly improve answer accuracy and reasoning depth over existing reasoning LMMs in both general and financial multimodal contexts.

## Method Summary
FinLMM-R1 introduces an integrated framework that addresses the limitations of existing financial reasoning LMMs through two core components: an Automated and Scalable Pipeline (ASP) for data construction and an enhanced training strategy (TAR-LMM). ASP resolves textual-visual misalignment in financial reports by separating question generation from image alignment, using an LLM to generate text-based QAs from document context and a separate LMM to align these questions to specific images. TAR-LMM extends the LMM-R1 training framework with additional reward mechanisms, including image selection, reasoning length, and adversarial rewards, to optimize visual perception, reasoning efficiency, and logical coherence. The framework is trained in two stages: first on text-based reasoning, then on multimodal data with multi-image contrastive training.

## Key Results
- FinLMM-R1 achieves significant improvements in answer accuracy and reasoning depth on 7 benchmarks, outperforming existing reasoning LMMs in both general and financial multimodal contexts.
- The ASP pipeline successfully generates 89,378 aligned image-question pairs from 23,397 financial reports, resolving textual-visual misalignment through a separate paradigm of question generation and image alignment.
- TAR-LMM's adversarial reward mechanism effectively improves reasoning coherence by distinguishing between sound reasoning and lucky guesses, as validated by extensive experiments.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Separating question generation from visual alignment purportedly resolves textual-visual misalignment common in complex financial reports.
- **Mechanism:** The Automated and Scalable Pipeline (ASP) decouples the task into two steps: an LLM generates text-based QAs from document context, and a separate LMM aligns these questions to specific images. This prevents the model from hallucinating visual links when text and images are misaligned in the source PDF.
- **Core assumption:** Textual context in reports contains sufficient information to generate viable questions before visual verification.
- **Evidence anchors:**
  - [abstract]: Mentions ASP resolves misalignment "through a separate paradigm of question-answer generation and image-question alignment."
  - [section 3.3]: Details the use of DeepSeek-V3 for QA and QwenVL 2.5-3B for alignment to ensure "one-to-one correspondence."
  - [corpus]: No direct corpus evidence refutes this; related work (LMM-R1) focuses on training, not data construction, highlighting the novelty here.
- **Break condition:** If the text extracted from PDFs is fragmented or semantic context is lost, the LLM may generate unanswerable questions, breaking the pipeline.

### Mechanism 2
- **Claim:** An adversarial reward structure likely improves reasoning coherence by filtering for "thought quality" rather than just answer correctness.
- **Mechanism:** TAR-LMM employs a GAN-like framework where the LMM (generator) produces a thinking trace, and a BERT model (discriminator) predicts if that trace leads to a correct answer. This penalizes "pseudo-reasoning" where the model gets the right answer for the wrong reasons.
- **Core assumption:** A lightweight BERT model can effectively distinguish between sound reasoning and lucky guesses in financial contexts.
- **Evidence anchors:**
  - [abstract]: Highlights "adversarial rewards" to optimize "logical coherence."
  - [section 4.3]: Describes the BERT discriminator's role in assessing "whether reasoning leads to correct answers."
  - [corpus]: [25131] "SFT or RL?" warns that SFT can create "pseudo reasoning paths" imitated from experts; the adversarial mechanism here directly targets this failure mode by validating the internal logic.
- **Break condition:** If the discriminator is under-trained or data is too noisy, it may provide conflicting gradients, destabilizing the policy model.

### Mechanism 3
- **Claim:** Multi-image contrastive training with distractors appears to enforce robust visual grounding.
- **Mechanism:** By presenting the model with 2-4 distractor images alongside the correct one and rewarding explicit image selection (`<image_selection>`), the model is forced to discriminately attend to relevant visual features rather than relying on linguistic priors.
- **Core assumption:** The distractors selected (based on lowest textual similarity) are sufficiently challenging to force the model to "look" at the images.
- **Evidence anchors:**
  - [abstract]: Cites "image selection" as a mechanism to "optimize visual perception."
  - [section 4.3]: Explains the construction of "multi-image contrastive samples" to enhance perception.
  - [corpus]: [20331] (LMM-R1) uses single-image inputs; extending this to multi-image contrastive samples is a specific architectural extension defined in this paper.
- **Break condition:** If distractors are too obviously irrelevant (e.g., completely different domains), the selection task becomes trivial, failing to improve fine-grained perception.

## Foundational Learning

- **Concept:** **Group Relative Policy Optimization (GRPO)**
  - **Why needed here:** This is the underlying RL algorithm replacing PPO. It determines how the model updates its weights based on the rewards.
  - **Quick check question:** How does GRPO estimate the baseline (advantage) without a separate value model (critic)? (Answer: It compares the reward of a sample against the average reward of the group generated for that prompt.)

- **Concept:** **Generative Adversarial Imitation Learning (GAIL) / Adversarial RL**
  - **Why needed here:** Understanding the "Adversarial Reward" requires knowing how a discriminator network can guide a generator (the LLM) to produce realistic/sound outputs.
  - **Quick check question:** In this setup, does the BERT model update its weights during training, or is it frozen? (Answer: It updates every N steps to better "catch" the LMM's flawed reasoning.)

- **Concept:** **Curriculum Learning (Text-to-Multimodal)**
  - **Why needed here:** The framework relies on a two-stage process: building reasoning in text first, then transferring to multimodal tasks.
  - **Quick check question:** Why is the "Format Reward" dropped or weighted differently in Stage 2 compared to Stage 1? (Answer: The model has already learned the format structure in Stage 1; Stage 2 focuses on perception and deeper reasoning.)

## Architecture Onboarding

- **Component map:** PDF Extractor -> Text Cleaner -> LLM (Question Gen) -> LMM (Image Aligner) -> Human Filter -> Stage 1 (Text): DeepScaleR Dataset -> GRPO -> Rewards (Format, Accuracy) -> Stage 2 (Multi-Image): VerMulti + FinData -> GRPO -> Rewards (Format, Accuracy, Length, Adversarial, Image Selection) -> Adversarial Module: Policy LMM (Generator) <-> BERT (Discriminator)

- **Critical path:** The **Adversarial Reward loop** is the most complex addition. You must ensure the BERT discriminator is trained at the correct interval (every N steps). If N is too small, training is slow; if N is too large, the discriminator fails to adapt to the LMM's evolving strategy.

- **Design tradeoffs:**
  - **Rule-based vs. Model-based Rewards:** The paper adds complex model-based rewards (Adversarial) to simple rule-based ones (Format). This increases compute cost and instability risk but is claimed to improve reasoning depth.
  - **Data Scale vs. Noise:** Automating data construction (ASP) allows for 89k+ pairs but requires aggressive cleaning (Section 3.2) and LMM-based filtering to remove "doubtful questions."

- **Failure signatures:**
  - **Reward Hacking:** The Length Reward might cause the model to generate verbose, repetitive "thinking" without substance. The Adversarial Reward is the counter-measure, but if misconfigured, the model might learn to fool the BERT discriminator rather than reason truthfully.
  - **Catastrophic Forgetting:** Transitioning from Stage 1 (Text) to Stage 2 (Visual) might cause a drop in pure logic skills if the multimodal data is too noisy.

- **First 3 experiments:**
  1. **Sanity Check (Reward Ablation):** Train three Stage 2 models: one without the Adversarial Reward, one without the Image Selection Reward, and one with all. Compare on the financial benchmark to isolate the contribution of each mechanism.
  2. **Data Validation (ASP Quality):** Manually inspect a random sample of 100 IQA triplets from the ASP pipeline. Specifically, verify if the "Image-Question Alignment" holds for charts with complex legends (the most common failure point).
  3. **Discriminator Tuning:** Run a sweep on the `bert_train_interval` (N steps). Plot the BERT accuracy vs. LMM performance to find the "Goldilocks" zone where the discriminator is challenging but stable.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does replacing the BERT-based discriminator with a Large Multimodal Model (e.g., 3B or 7B parameters) significantly enhance the adversarial reward's ability to guide reasoning in complex tasks?
- Basis in paper: [explicit] The authors state in Section I (Limitations) that the BERT-based discriminator may limit the depth of reasoning signals due to capacity constraints, and explicitly propose integrating larger pretrained architectures in future work.
- Why unresolved: The current framework relies on a specific BERT variant (Sentence-BERT) chosen for computational efficiency, leaving the potential performance gains of larger discriminators untested.
- What evidence would resolve it: A comparative study where the discriminator is swapped for a 7B LMM, showing improved performance on "Hard" reasoning benchmarks where the BERT model struggled (e.g., MathVerse).

### Open Question 2
- Question: Can the Automated and Scalable Pipeline (ASP) be generalized to professional domains outside of finance, such as healthcare or legal analysis, without structural modification?
- Basis in paper: [explicit] Section I explicitly lists adapting the annotation workflow to diverse domains (healthcare, legal, scientific) to create a "domain-agnostic reasoning dataset" as a primary future direction.
- Why unresolved: The ASP was designed specifically to resolve textual-visual misalignments in financial reports; it is unproven whether the separate paradigm of Q&A generation and image alignment works for different document structures (e.g., clinical notes with imaging).
- What evidence would resolve it: Successful generation of high-quality IQA triplets from medical or legal PDFs using the ASP, followed by improved model performance on domain-specific benchmarks.

### Open Question 3
- Question: Are the specific heuristic thresholds ($L_{min}, L_{opt}, L_{max}$) used in the length reward function transferable to larger model scales, or do they require retuning?
- Basis in paper: [inferred] The paper introduces a specific piecewise function for length rewards (Section 4.3) with fixed token targets (e.g., optimal at 450 tokens). While effective for the 3B model tested, the authors do not address if these specific values scale with model capacity.
- Why unresolved: As model size increases, the "optimal" reasoning length may shift; fixed constraints optimized for a 3B model could become bottlenecks or redundant for larger models.
- What evidence would resolve it: Ablation studies on 7B or larger models varying the length reward thresholds to identify if the current parameters are global optima or scale-dependent.

## Limitations
- The ASP pipeline's reliance on automated LMM-based filtering introduces potential noise in the training data, particularly for complex financial charts with dense legends or multi-part tables.
- The BERT discriminator's effectiveness in distinguishing sound reasoning from lucky guesses is untested against more sophisticated evaluation metrics for reasoning coherence.
- The multi-image contrastive training's success depends heavily on the distractor selection strategy; if distractors are too easy or too hard, the intended perceptual learning may not occur.

## Confidence
- **High Confidence**: The ASP pipeline's conceptual approach to resolving textual-visual misalignment through separate question generation and alignment steps is well-supported by the paper's methodology and aligns with established practices in multimodal data construction.
- **Medium Confidence**: The adversarial reward mechanism's ability to improve reasoning coherence is plausible given the literature on adversarial training, but its effectiveness depends on proper discriminator tuning and is not fully validated in the paper.
- **Medium Confidence**: The multi-image contrastive training's potential to enforce robust visual grounding is reasonable, but the distractor selection strategy's effectiveness is not thoroughly evaluated.
- **Low Confidence**: The claim that TAR-LMM significantly improves reasoning depth over existing LMMs is based on benchmark comparisons, but the benchmarks' relevance to real-world financial reasoning and the absence of ablation studies for individual reward components weaken this claim.

## Next Checks
1. **Reward Ablation Study**: Train three Stage 2 models with different reward configurations (without Adversarial Reward, without Image Selection Reward, and with all rewards). Compare their performance on financial benchmarks to isolate the contribution of each mechanism.
2. **ASP Data Quality Audit**: Manually inspect a random sample of 100 IQA triplets from the ASP pipeline, focusing on charts with complex legends to verify the accuracy of image-question alignment.
3. **Discriminator Tuning Sweep**: Run experiments varying the `bert_train_interval` (N steps) and plot BERT accuracy versus LMM performance to identify the optimal training interval for the adversarial reward mechanism.