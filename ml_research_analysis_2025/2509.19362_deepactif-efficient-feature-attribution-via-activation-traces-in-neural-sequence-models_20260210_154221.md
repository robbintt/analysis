---
ver: rpa2
title: 'DeepACTIF: Efficient Feature Attribution via Activation Traces in Neural Sequence
  Models'
arxiv_id: '2509.19362'
source_url: https://arxiv.org/abs/2509.19362
tags:
- deepactif
- feature
- methods
- features
- attribution
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: DeepACTIF addresses the inefficiency of traditional feature attribution
  methods in time-series deep learning by leveraging internal LSTM activations instead
  of gradients or perturbations. Its core innovation is an inverse-weighted aggregation
  strategy that prioritizes features with consistently strong activations across timesteps,
  combining stability and magnitude to produce robust feature rankings.
---

# DeepACTIF: Efficient Feature Attribution via Activation Traces in Neural Sequence Models

## Quick Facts
- arXiv ID: 2509.19362
- Source URL: https://arxiv.org/abs/2509.19362
- Authors: Benedikt W. Hosp
- Reference count: 16
- Primary result: DeepACTIF achieves up to 2× speedup and reduces memory usage from 2-3GB to <1GB while outperforming traditional attribution methods on biometric gaze datasets

## Executive Summary
DeepACTIF introduces an efficient feature attribution method for LSTM-based time-series models that leverages internal activation traces rather than computationally expensive gradients or perturbations. The method employs an inverse-weighted aggregation strategy that prioritizes features with consistently strong activations across timesteps, combining stability and magnitude to produce robust feature rankings. Evaluated on three biometric gaze datasets, DeepACTIF demonstrates significant performance improvements over established methods (SHAP, IG, DeepLIFT, Ablation), achieving up to 2× speedup and reduced memory usage while maintaining superior model accuracy under severe feature reduction. Statistical validation confirms these improvements are robust (p < 0.05, large effect sizes).

## Method Summary
DeepACTIF addresses the inefficiency of traditional feature attribution methods in time-series deep learning by leveraging internal LSTM activations instead of gradients or perturbations. Its core innovation is an inverse-weighted aggregation strategy that prioritizes features with consistently strong activations across timesteps, combining stability and magnitude to produce robust feature rankings. The method works by tracing activation patterns through LSTM layers and applying a weighted aggregation that emphasizes temporally stable, high-magnitude features. This approach enables faster computation and lower memory requirements compared to gradient-based methods while maintaining or improving attribution quality.

## Key Results
- Achieved up to 2× speedup in feature attribution computation compared to traditional methods
- Reduced memory usage from 2-3GB to less than 1GB for attribution tasks
- Outperformed established methods (SHAP, IG, DeepLIFT, Ablation) in preserving model accuracy under severe feature reduction on three biometric gaze datasets

## Why This Works (Mechanism)
DeepACTIF's efficiency gains stem from its use of internal LSTM activation traces rather than computationally expensive gradient calculations or perturbation-based approaches. The inverse-weighted aggregation strategy effectively combines temporal stability with activation magnitude, creating robust feature rankings that are both computationally efficient and meaningful for model interpretation. By focusing on consistently activated features across timesteps, the method filters out noise and identifies truly important features, leading to improved performance when features are reduced for deployment on resource-constrained devices.

## Foundational Learning
- **LSTM Activation Mechanics**: Understanding how LSTM gates (input, forget, output) process sequential information through their activation functions. This is needed to trace meaningful activation patterns for attribution. Quick check: Verify activation values follow expected gating behavior across timesteps.
- **Time-Series Feature Importance**: The concept that features in sequential data have varying importance across timesteps, not just overall. This is crucial for understanding temporal patterns in attribution. Quick check: Confirm feature importance varies meaningfully across sequence positions.
- **Aggregation Strategies**: Methods for combining multiple attribution scores (per timestep) into a single feature ranking. This is needed to create stable, interpretable results from temporal data. Quick check: Test different aggregation functions on synthetic data with known patterns.
- **Inverse Weighting Principles**: Mathematical concepts for emphasizing consistently strong signals while de-emphasizing sporadic activations. This is needed for the core innovation of DeepACTIF. Quick check: Validate inverse weighting properly suppresses noisy, inconsistent activations.

## Architecture Onboarding

**Component Map**: Input Data -> LSTM Layers -> Activation Trace Extraction -> Inverse-Weighted Aggregation -> Feature Rankings

**Critical Path**: The most performance-critical path is the activation trace extraction and aggregation, as this determines both speed and memory usage. Efficient implementation of the inverse-weighted aggregation algorithm is essential for achieving the reported speedup.

**Design Tradeoffs**: The method trades some attribution granularity (fine-grained temporal detail) for computational efficiency and stability. This makes it particularly suitable for deployment scenarios where speed and resource constraints outweigh the need for detailed temporal attribution maps.

**Failure Signatures**: Potential failure modes include: 1) Poor performance on non-biometric time-series data with different activation patterns, 2) Reduced effectiveness when features have highly variable importance across timesteps, 3) Possible over-aggregation that masks important but temporally sparse features.

**First Experiments**:
1. Test DeepACTIF on a simple synthetic time-series dataset with known feature importance patterns to verify correct operation
2. Compare attribution results on a held-out dataset to ensure consistency across different data splits
3. Benchmark memory usage and computation time against a gradient-based method on a small LSTM model

## Open Questions the Paper Calls Out
None specified in the provided material.

## Limitations
- Limited evaluation scope restricted to three biometric gaze datasets, raising questions about generalizability to other domains
- No comparison with attention-based attribution methods that are increasingly common in sequence models
- Uncertain performance on non-biometric time-series data such as financial or industrial sensor data
- No investigation of computational overhead during model training and fine-tuning phases

## Confidence
- **High confidence** in efficiency improvements (speedup, memory usage) on tested LSTM models
- **Medium confidence** in ranking stability claims, as statistical validation was limited to specific datasets
- **Medium confidence** in practical applicability, as real-world deployment scenarios beyond gaze biometrics were not demonstrated

## Next Checks
1. Test DeepACTIF on diverse time-series domains (financial, medical, industrial sensors) to assess generalizability
2. Benchmark against attention-based attribution methods in Transformers for sequence modeling tasks
3. Evaluate computational overhead during model training and fine-tuning phases to determine full lifecycle efficiency gains