---
ver: rpa2
title: 'IPQA: A Benchmark for Core Intent Identification in Personalized Question
  Answering'
arxiv_id: '2510.23536'
source_url: https://arxiv.org/abs/2510.23536
tags:
- intents
- intent
- core
- user
- information
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces core intents as motivations users prioritize
  when selecting answers in personalized question answering. It constructs IPQA, a
  benchmark dataset derived from community question answering platforms, where core
  intents are identified from observable answer selection behavior rather than explicit
  user statements.
---

# IPQA: A Benchmark for Core Intent Identification in Personalized Question Answering

## Quick Facts
- arXiv ID: 2510.23536
- Source URL: https://arxiv.org/abs/2510.23536
- Authors: Jieyong Kim; Maryam Amirizaniani; Soojin Yoon; Dongha Lee
- Reference count: 39
- Key outcome: Core intent identification in personalized QA remains challenging with F1 scores 0.30-0.54, significantly improving when historical intents are preprocessed

## Executive Summary
This paper introduces core intents as motivations users prioritize when selecting answers in personalized question answering. It constructs IPQA, a benchmark dataset derived from community question answering platforms, where core intents are identified from observable answer selection behavior rather than explicit user statements. Using LLM-based annotation and human validation, the dataset covers 47 domains with verified quality metrics showing 85.67% human agreement on annotation criteria. Experimental results across state-of-the-art models demonstrate core intent identification remains challenging, with F1 scores ranging from 0.30-0.54 and performance degrading as question complexity increases. Models struggle to extract intent patterns from raw user histories, though preprocessing historical intents substantially improves performance.

## Method Summary
The IPQA benchmark is constructed by filtering community question answering posts that require personalization, then using LLM-based annotation to generate candidate intents from user narratives. A four-stage quality filtering process (Completeness, Faithfulness, Motivational Fidelity, Answer Justification) validates these intents before core intent selection through mapping to information pieces extracted from selected answers. The evaluation uses an LLM-based matching system (GPT-4.1-Mini) to compute Precision/Recall/F1 by comparing predicted intents against ground truth. Four personalization configurations are tested: No Personalization, Random Profile, User Profile (Raw), and User Profile (Intents) with preprocessing.

## Key Results
- Core intent identification F1 scores range from 0.30-0.54 across all models, with accuracy dropping as question complexity increases
- User Profile (Raw) frequently performs worse than No Personalization, while User Profile (Intents) shows substantial improvements of 8-15% F1
- Correctly identified core intents dramatically increase probability of generating relevant information pieces (from ~42-49% to ~64-76%)
- Performance degrades consistently with question complexity, particularly for questions with 5+ core intents

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Core intents can be inferred from observable answer selection behavior rather than explicit user statements.
- Mechanism: Users operating under cognitive constraints select answers meeting minimum acceptance thresholds (satisficing theory). Intents aligned with selected answers represent prioritized motivations. The pipeline extracts information pieces from selected answers and maps them to candidate intents—successfully mapped intents constitute core intents.
- Core assumption: Answer selection reveals intent prioritization reliably; users do not select answers that fail to address their essential intents.
- Evidence anchors:
  - [abstract] "we derive core intents from observable behavior patterns in answer selection, grounded in satisficing theory where users choose answers meeting their acceptance thresholds"
  - [Section 2.3.2] "Intents successfully mapped to at least one information piece constitute core intents for that instance"
  - [corpus] Limited direct support; LaMP-QA addresses personalized QA but focuses on response coverage rather than intent identification.
- Break condition: If users select answers for reasons unrelated to intent satisfaction (e.g., presentation quality, length), core intent inference becomes unreliable.

### Mechanism 2
- Claim: Preprocessing user histories into explicit intent patterns enables effective personalization; raw histories do not.
- Mechanism: Raw profiles contain unstructured question-source pairs from which models must extract intent-level patterns. This extraction task appears to exceed current model capabilities. When historical intents are pre-annotated and provided explicitly, models leverage these patterns effectively, with F1 improving substantially across all tested models.
- Core assumption: Models can utilize intent patterns when available but cannot reliably extract them from raw text; the bottleneck is extraction, not utilization.
- Evidence anchors:
  - [Section 3.3] "User Profile (Raw) frequently performs worse than No Personalization despite providing complete user histories... models fail to extract intent-level patterns from unstructured post histories"
  - [Section 3.3] "User Profile (Intents) demonstrates substantial improvements when historical intents are explicitly provided"
  - [corpus] FSPO paper addresses personalization through few-shot preference optimization, suggesting extraction-from-raw-data challenges persist across personalization approaches.
- Break condition: If preprocessing quality degrades (incorrect historical intent annotations), noise could overwhelm personalization signals.

### Mechanism 3
- Claim: Core intent identification directly impacts answer generation quality.
- Mechanism: Correctly identified core intents increase probability of generating relevant information pieces. When models correctly identify intents, P(≥1 relevant piece | intent identified) rises from ~42-49% to ~64-76%. This establishes intent identification as foundational rather than auxiliary.
- Core assumption: Generated answer quality can be measured through information piece coverage; intent-to-information mapping is meaningful.
- Evidence anchors:
  - [Section 3.6] "Correctly identifying core intents dramatically increases the probability of generating relevant information, particularly for producing at least one related piece"
  - [Table 6] Shows conditional probabilities across all models demonstrating consistent gaps between Acc|✓ and Acc|✗
  - [corpus] No direct corpus evidence for this specific causal pathway in PQA contexts.
- Break condition: If answer generation can succeed without intent understanding (e.g., through retrieval alone), the claimed dependency weakens.

## Foundational Learning

- Concept: Satisficing theory (bounded rationality)
  - Why needed here: Core intent definition depends on users selecting "good enough" answers rather than optimal ones, establishing why answer selection reveals prioritized intents.
  - Quick check question: Can you explain why a user selecting an incomplete answer still provides signal about their intent priorities?

- Concept: Multi-intent detection with priority weighting
  - Why needed here: PQA questions express multiple coexisting intents with varying importance; understanding this heterogeneity is prerequisite to identifying which intents are "core."
  - Quick check question: How does the multi-intent scenario differ from single-intent classification in traditional dialogue systems?

- Concept: LLM-based annotation with human validation loops
  - Why needed here: Dataset construction relies on automated intent generation followed by quality filtering; understanding this pipeline is essential for reproducing or extending the benchmark.
  - Quick check question: What four quality criteria filter generated intents before core intent selection?

## Architecture Onboarding

- Component map: Data Collection -> Intent Generation -> Quality Filtering -> Core Intent Selection -> Evaluation
- Critical path: The quality filtering and core intent selection pipeline (Section 2.3) determines ground truth reliability. Failures here propagate to all downstream evaluation.
- Design tradeoffs:
  - Raw vs. preprocessed profiles: Raw preserves full context but models fail to extract patterns; preprocessed improves performance but requires annotation infrastructure
  - History size (k): Larger histories generally help but some models peak early (Gemma3 at k=5), suggesting context-length limitations
  - LLM annotation vs. human: Cost/consistency tradeoff; paper shows 85.67% human agreement, validating automation
- Failure signatures:
  - Raw profile performance below no-personalization baseline → indicates extraction failure, not helpful signal absence
  - High-complexity questions (5+ intents) with low recall → fundamental scaling challenge
  - Large performance gaps between "Core Only" and "Predicted" answer generation → intent identification bottleneck
- First 3 experiments:
  1. Reproduce the four personalization configurations (No Personalization, Random Profile, Raw Profile, Preprocessed Intents) on a held-out domain to validate the preprocessing effect magnitude.
  2. Ablate history size k ∈ {5, 10, 20, 30} to identify optimal context window utilization for your target model architecture.
  3. Analyze failure cases in high-complexity questions (5+ core intents) to characterize whether the bottleneck is recall (missing intents) or precision (incorrect intents).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can models be designed to effectively extract intent patterns from raw user histories without requiring explicit preprocessing?
- Basis in paper: [explicit] The authors state "models fail to extract intent-level patterns from unstructured post histories" and that User Profile (Raw) frequently performs worse than No Personalization despite providing complete user histories.
- Why unresolved: Current models struggle to infer implicit intent priorities from unstructured source data, creating a gap between raw historical context and actionable personalization signals.
- What evidence would resolve it: Development of architectures or training methods that achieve comparable performance between User Profile (Raw) and User Profile (Intents) configurations.

### Open Question 2
- Question: What approaches can mitigate the performance degradation observed as question complexity increases?
- Basis in paper: [explicit] The paper reports that "performance degrades consistently as complexity increases" with all models showing "substantially lower recall for High-complexity questions compared to Low-complexity ones," even with extended historical context.
- Why unresolved: Identifying all prioritized intents becomes increasingly difficult as their number grows, and additional profile history does not fully close this gap.
- What evidence would resolve it: Methods demonstrating stable or improving recall across Low, Medium, and High complexity levels in the IPQA benchmark.

### Open Question 3
- Question: How accurately do core intents derived from answer selection behavior reflect users' actual prioritized motivations?
- Basis in paper: [inferred] The paper acknowledges that "not all intents aligned with selected answers may represent true priorities" and that this approach "may not capture every possible user motivation," relying on satisficing theory assumptions about minimum acceptance thresholds.
- Why unresolved: Without explicit user articulation of priorities, the validity of inferring core intents from selection behavior remains theoretically grounded but empirically unverified against ground-truth user preferences.
- What evidence would resolve it: User studies where participants explicitly rank intent importance, compared against core intents inferred from their answer selections.

## Limitations
- The satisficing theory mechanism assumes users select answers meeting minimum intent thresholds, but alternative selection criteria could introduce noise
- Core intent inference reliability depends on whether answer selection behavior accurately reflects user intent priorities
- The 85.67% human agreement on annotation quality represents binary filter agreement rather than nuanced intent prioritization validation

## Confidence

**High confidence**: Core intent identification as a fundamental challenge in PQA (supported by consistent performance gaps across models)

**Medium confidence**: Preprocessing historical intents enables effective personalization (supported by empirical results but dependent on annotation quality)

**Low confidence**: Answer selection reliably reveals intent prioritization (mechanism assumptions unverified for edge cases)

## Next Checks
1. Test the core intent inference mechanism on domains where answer quality metrics diverge from content relevance (e.g., technical support vs. lifestyle advice) to assess mechanism robustness.
2. Evaluate whether the preprocessing advantage persists when historical intent annotations contain errors at varying rates (5%, 15%, 30%) to establish noise tolerance thresholds.
3. Conduct user studies where participants select answers under controlled conditions with explicit vs. implicit intent satisfaction criteria to validate the satisficing theory assumptions.