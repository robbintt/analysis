---
ver: rpa2
title: 'Diagnose, Localize, Align: A Full-Stack Framework for Reliable LLM Multi-Agent
  Systems under Instruction Conflicts'
arxiv_id: '2509.23188'
source_url: https://arxiv.org/abs/2509.23188
tags:
- role
- cras
- instruction
- conflict
- layers
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This work introduces a three-stage framework\u2014Diagnose, Localize,\
  \ Align\u2014for improving LLM-based multi-agent systems (MAS) under instruction\
  \ conflicts. The method uses a rubric-driven metric (CRAS) to diagnose role adherence,\
  \ analyzes attention drift to localize conflict-sensitive layers in middle depths,\
  \ and applies surgical LoRA updates via token-weighted preference optimization (SAIL)\
  \ on those layers."
---

# Diagnose, Localize, Align: A Full-Stack Framework for Reliable LLM Multi-Agent Systems under Instruction Conflicts

## Quick Facts
- arXiv ID: 2509.23188
- Source URL: https://arxiv.org/abs/2509.23188
- Reference count: 39
- This work introduces a three-stage framework—Diagnose, Localize, Align—for improving LLM-based multi-agent systems (MAS) under instruction conflicts.

## Executive Summary
This paper presents a three-stage framework—Diagnose, Localize, Align (DLA)—to address instruction conflicts in LLM-based multi-agent systems. The framework uses a rubric-driven metric (CRAS) to diagnose role adherence, analyzes attention drift to localize conflict-sensitive layers in middle depths, and applies surgical LoRA updates via token-weighted preference optimization (SAIL) on those layers. On benchmarks including MedQA, GPQA, and MMLU across frameworks like AutoGen and SelfConsistency, SAIL improves instruction hierarchy compliance by up to 5.60% accuracy without full-model finetuning. The approach closes the gap between macro-level MAS metrics and micro-level role adherence, offering precise, parameter-efficient repair for hierarchical instruction conflicts.

## Method Summary
The DLA framework consists of three stages: (1) CRAS rubric generation and scoring per query to evaluate role adherence across four axes (Goal Alignment, Role Consistency, Knowledge Boundary Adherence, Constraint Compliance); (2) attention drift analysis to localize conflict-sensitive layers by comparing attention patterns between conflict and normal inputs, identifying focal heads concentrated in middle layers; (3) SAIL training using LoRA adapters only on focal layers with token-weighted DPO based on focal-head attention contributions. The method requires a conflict dataset with matched normal/conflict instruction pairs, an LLM-based CRAS evaluator, and standard LoRA/DPO training infrastructure with specific hyperparameters (LoRA rank 8, learning rate 1e-5).

## Key Results
- SAIL improves AutoGen on MedQA by +5.60% accuracy and +0.66 CRAS score
- Focal-layer targeting outperforms all-layers LoRA (34.80 vs 33.20 ACC on MedQA)
- Token-weighted reward improves performance over constant/random reward (34.80 vs 33.40 vs 28.80 ACC on MedQA)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Instruction conflict resolution concentrates in specific attention heads clustered in middle layers, enabling surgical intervention.
- Mechanism: By contrasting attention patterns between conflict vs. non-conflict inputs using three drift metrics (magnitude, direction, distribution), the method identifies heads that sharply shift behavior when instructions collide. These heads cluster around layers 19-23 (Qwen2.5-7B) and 18-22 (LLaMA3.1-8B), forming a localized arbitration locus.
- Core assumption: Conflict-sensitive heads are functionally specialized for instruction hierarchy resolution and can be modified without degrading general capabilities.
- Evidence anchors: [abstract], [Section 3.2, Eq. 5-6], [corpus]
- Break condition: If focal heads distribute uniformly across all layers rather than clustering mid-depth, surgical targeting loses advantage over full-model tuning.

### Mechanism 2
- Claim: Token-level credit assignment via focal-head attentional contribution concentrates learning signal on arbitration-critical tokens.
- Mechanism: Each token's relative contribution c_t^(focal) measures attention mass from focal heads vs. all heads. Tokens with higher focal attribution receive larger gradient weight in the DPO-style loss, directing updates toward positions where instruction hierarchy is actually resolved.
- Core assumption: Tokens with high focal-head attention are causally involved in conflict resolution, not merely correlated.
- Evidence anchors: [Section 3.3, Eq. 9-11], [Table 2], [corpus]
- Break condition: If attentional contribution poorly correlates with actual arbitration importance, weighting becomes noise.

### Mechanism 3
- Claim: Restricting LoRA to focal layers preserves general capabilities while improving instruction hierarchy compliance.
- Mechanism: SAIL installs LoRA adapters only on S_focal (layers above threshold τ), constraining optimization to conflict-resolution circuits while leaving non-focal parameters frozen.
- Core assumption: Instruction hierarchy violations stem from focal-layer dysfunction rather than global misalignment, and non-focal layers are already well-calibrated.
- Evidence anchors: [Table 1], [Table 3], [corpus]
- Break condition: If conflict resolution requires cross-layer coordination spanning early-to-late layers, restricting to mid-depth may bottleneck improvement.

## Foundational Learning

- Concept: **Attention Drift Analysis**
  - Why needed here: The localization stage quantifies how attention patterns shift between conflict and normal regimes; understanding KL divergence and vector similarity is essential to interpret Eq. 3-5.
  - Quick check question: Given two attention matrices A_normal and A_conflict, how would you compute directional drift independent of magnitude scaling?

- Concept: **Direct Preference Optimization (DPO)**
  - Why needed here: SAIL builds on DPO's likelihood-ratio formulation but adds token-weighting; the base objective in Eq. 11 assumes familiarity with the logistic preference loss.
  - Quick check question: Why does DPO avoid explicit reward modeling, and how does β control the strength of preference enforcement?

- Concept: **Low-Rank Adaptation (LoRA)**
  - Why needed here: SAIL uses LoRA adapters only on focal layers (Eq. 12); understanding rank, scaling factor α, and adapter composition is critical for implementation.
  - Quick check question: If LoRA rank r=8 and hidden dimension d=4096, what is the parameter count reduction vs. full finetuning for a single projection matrix?

## Architecture Onboarding

- Component map:
  - CRAS Evaluator -> Conflict Dataset Builder -> Attention Drift Analyzer -> Focal Layer Selector -> SAIL Trainer

- Critical path:
  1. Generate conflict dataset (normal/conflict instruction pairs)
  2. Run model on both regimes, capture attention matrices A^(l,h)
  3. Compute drift scores, identify focal heads and layers
  4. Sample rollout pairs, score with CRAS, construct preference data
  5. Train SAIL: LoRA on focal layers, token weights from c_t^(focal)

- Design tradeoffs:
  - Top-k% selection: Lower k = more surgical but risks missing important heads; paper uses ~10-15% implicitly
  - Threshold τ: Higher τ = fewer focal layers, more parameter-efficient but potentially underpowered
  - Tempering exponent γ: Values <1 smooth attention spikes; default γ=1 may over-weight dominant tokens
  - LoRA rank: Paper finds r=8 optimal over r=16—higher rank doesn't help and may hurt

- Failure signatures:
  - CRAS saturation: If scores cluster near 3.0 with low variance, rubric may lack discriminative power
  - Drift diffusion: If focal heads distribute across all layers without mid-depth clustering, conflict/normal contrast may be insufficiently sharp
  - SAIL collapse: Training loss plateaus early with minimal ACC gain—likely learning rate too low or token weights near-uniform

- First 3 experiments:
  1. Validate localization: Run drift analysis on held-out conflict dataset; confirm focal heads cluster in same layer range (19-23 for Qwen, 18-22 for LLaMA); plot n_l distribution.
  2. Ablate token weighting: Train SAIL with constant vs. attention-weighted tokens; expect 1-3% ACC gap consistent with Table 2; monitor CRAS per-axis to see which dimensions benefit most.
  3. Cross-framework transfer: Apply SAIL trained on AutoGen to MacNet; measure if compliance gains transfer or if focal layers are framework-specific.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the localization of conflict resolution mechanisms in middle layers generalize to non-Transformer architectures (e.g., State Space Models) or ultra-deep transformer variants?
- Basis in paper: [inferred] The paper empirically identifies conflict-sensitive heads in layers 18–23 for LLaMA and Qwen (Section 3.2), but does not test if this "middle-layer" locus is a universal feature of language models or specific to standard transformer depth-to-width ratios.
- Why unresolved: The study is restricted to standard decoder-only transformers (LLaMA3.1, Qwen2.5).
- What evidence would resolve it: Replicating the attention drift analysis (Eq. 5a) on architectures like Mamba or deeper models (e.g., LLaMA-70B) to see if the drift distribution shifts to other depths.

### Open Question 2
- Question: Can SAIL maintain instruction hierarchy stability in long-horizon interactions involving dynamic role switching?
- Basis in paper: [explicit] The conclusion explicitly states the work "opens avenues for extending focalized alignment to... long-horizon, multi-role coordination."
- Why unresolved: Current experiments focus on single-turn reasoning tasks or fixed-role MAS frameworks; the token-weighting mechanism relies on immediate attention contributions which may dilute over extended context windows.
- What evidence would resolve it: Evaluating SAIL on multi-turn benchmarks (e.g., MINT or custom long-horizon simulations) where agents must swap roles or constraints mid-session without catastrophic forgetting.

### Open Question 3
- Question: Is the CRAS evaluation pipeline susceptible to reward hacking if the held-out evaluator model shares the alignment weaknesses of the student policy?
- Basis in paper: [inferred] Section 3.1 relies on a "held-out evaluator" to score trajectories, but if the evaluator is an LLM, it may suffer from the same instruction conflicts it is meant to score, creating a bootstrapping error.
- Why unresolved: The paper does not analyze the failure modes of the CRAS evaluator itself or its correlation with human judgment in edge cases.
- What evidence would resolve it: A robustness check measuring the agreement between the LLM-based CRAS evaluator and a "gold-standard" human evaluator specifically on high-ambiguity conflict instances.

## Limitations
- The approach assumes instruction conflicts are sufficiently captured by seven cognitive dimensions, which may not encompass all real-world conflict types.
- Attention drift localization relies on clear contrasts between normal and conflict regimes, but ambiguous conflicts may produce diffuse drift patterns that evade detection.
- The token-weighting mechanism assumes high focal-head attention correlates with arbitration importance, yet this relationship is not empirically validated beyond correlation.

## Confidence

**High confidence:** The three-stage framework architecture, CRAS metric construction, and basic attention drift localization methodology are well-specified and reproducible.

**Medium confidence:** The effectiveness of token-weighted preference optimization and focal-layer LoRA targeting, while supported by ablation studies, depends on assumptions about causal relationships that are not fully validated.

**Low confidence:** The claim that middle-layer clustering is universal across all LLM architectures and conflict types lacks cross-model validation beyond the two tested base models.

## Next Checks

1. **Cross-Architecture Generalization Test:** Apply the complete DLA pipeline to a different LLM family (e.g., Mistral or Gemma) to verify whether focal layers consistently cluster in middle depths or if the pattern is model-specific.

2. **Conflict Type Coverage Analysis:** Systematically vary the seven cognitive dimensions used to generate conflict pairs and measure how well SAIL performance tracks the severity/diversity of instruction conflicts, identifying any dimensions where the method underperforms.

3. **Causal Attribution Validation:** Conduct intervention experiments where focal heads are ablated or amplified to determine whether their high attention scores genuinely indicate causal involvement in conflict resolution, not just correlation with resolved tokens.