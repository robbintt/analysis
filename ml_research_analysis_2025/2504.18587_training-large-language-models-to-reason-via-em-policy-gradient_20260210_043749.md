---
ver: rpa2
title: Training Large Language Models to Reason via EM Policy Gradient
arxiv_id: '2504.18587'
source_url: https://arxiv.org/abs/2504.18587
tags:
- reasoning
- grpo
- learning
- math
- empg
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces EM Policy Gradient, a simple off-policy reinforcement
  learning method for enhancing large language model reasoning. The method frames
  reasoning as an Expectation-Maximization problem, alternating between sampling diverse
  reasoning trajectories and reward-guided fine-tuning.
---

# Training Large Language Models to Reason via EM Policy Gradient

## Quick Facts
- arXiv ID: 2504.18587
- Source URL: https://arxiv.org/abs/2504.18587
- Authors: Tianbing Xu
- Reference count: 25
- Primary result: Introduces EM Policy Gradient, an off-policy RL method for LLM reasoning that eliminates importance weights and clipping, achieving performance comparable to or slightly better than GRPO on GSM8K and MATH HARD datasets.

## Executive Summary
This paper introduces EM Policy Gradient, a simple off-policy reinforcement learning method for enhancing large language model reasoning. The method frames reasoning as an Expectation-Maximization problem, alternating between sampling diverse reasoning trajectories and reward-guided fine-tuning. Unlike PPO and GRPO, it eliminates importance weights and clipping, offering a more scalable and effective approach. Evaluated on GSM8K and MATH HARD datasets using Qwen2.5 base models, EM Policy Gradient achieves performance comparable to or slightly better than GRPO. Additionally, it produces more concise reasoning and exhibits human-like cognitive behaviors such as sub-problem decomposition, self-verification, and backtracking.

## Method Summary
EM Policy Gradient treats reasoning as an EM problem: the E-step samples N diverse reasoning trajectories from a fixed policy θ*, and the M-step optimizes the policy using off-policy gradients weighted by smoothed rewards. The key innovation is deriving a lower bound on expected return via Jensen's inequality, allowing optimization without importance weights or clipping heuristics. Rewards are smoothed using sigmoid normalization to mitigate sparsity. The method alternates between freezing current parameters as θ*, sampling trajectories at temperature ~1.0, computing normalized rewards, and updating the policy via gradient ascent. KL regularization is optionally applied for strong base models but omitted for weak models to encourage exploration.

## Key Results
- EM Policy Gradient achieves test accuracies comparable to or slightly better than GRPO on GSM8K and MATH HARD datasets.
- The method produces more concise reasoning trajectories compared to GRPO.
- EM Policy Gradient successfully trains on Qwen2.5-Math-7B where GRPO failed to learn, suggesting better stability for specialized base models.
- Human-like cognitive behaviors emerge: sub-problem decomposition, self-verification, and backtracking during reasoning.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Framing policy optimization as EM eliminates the need for importance weights and clipping heuristics.
- Mechanism: The method derives a lower bound on log-expected-return using Jensen's inequality, introducing a variational distribution q(τ). The E-step samples trajectories from the previous policy (treating it as variational), and the M-step maximizes the bound via gradient ascent. Since q(τ) is fixed during M-step, no importance ratio correction is needed—trajectories are simply weighted by their rewards.
- Core assumption: The "one-step off-policy" regime (θ* ≈ θ) keeps distribution shift small enough that the introduced bias is acceptable and outweighed by variance reduction.
- Evidence anchors:
  - [abstract] "Unlike PPO and GRPO, which rely on complex importance weights and heuristic clipping, our method provides a simpler, more principled off-policy policy gradient approach, eliminating these complexities"
  - [section 4.3] Derivation showing on-policy gradient requires w(θ,θ*) importance weights, while off-policy gradient (Eq. 11-12) does not
  - [corpus] "Learning to Reason under Off-Policy Guidance" and "Squeeze the Soaked Sponge" both address off-policy RLVR, suggesting this is an active research direction but not yet standardized
- Break condition: If policy updates are too large (θ* far from θ), the bias from ignoring distribution shift could destabilize training.

### Mechanism 2
- Claim: Reward smoothing transforms sparse binary rewards into denser learning signals, improving gradient signal quality.
- Mechanism: Raw rewards (0 or 1 for correct/incorrect) are normalized by batch statistics (mean μ, std σ) and passed through sigmoid: R̃(τ) = σ((R(τ) - μ)/σ). This creates gradations between hard examples and easy examples within each batch.
- Core assumption: Reward variance within a batch carries meaningful signal about relative trajectory quality.
- Evidence anchors:
  - [section 4.2] Explicit formula for reward smoothing with sigmoid transformation
  - [section 4.2] "To mitigate this sparsity and discreteness, we apply a non-negative, non-decreasing function to smooth the rewards"
  - [corpus] No direct corpus evidence on reward smoothing specifically; this appears to be a method-specific choice
- Break condition: If all trajectories in a batch have identical rewards (all correct or all incorrect), smoothing provides no signal gradient.

### Mechanism 3
- Claim: Removing KL constraints from reference models can accelerate learning for weak base models.
- Mechanism: The paper observes that weaker models (Qwen2.5-1.5B) benefit from unconstrained exploration (β=0), while stronger models (14B-Instruct) benefit from KL regularization to preserve existing capabilities.
- Core assumption: Weak base models have little useful capability to preserve, so deviation is beneficial; strong models have high-quality priors worth maintaining.
- Evidence anchors:
  - [section 5.1] "for weaker base models (e.g., QWen2.5-1.5B), the constraint is unnecessary (i.e., setting β = 0)"
  - [section 5.1] "for stronger base models (e.g., QWen2.5-14B-Instruct)...the KL constraint remains beneficial"
  - [corpus] No corpus comparison on KL-free training; this is an empirical finding specific to this method
- Break condition: For mid-strength models, the optimal β may require tuning; too much divergence risks catastrophic forgetting.

## Foundational Learning

- Concept: **Policy Gradient Theorem**
  - Why needed here: The entire method builds on ∇J(θ) = E[∇log Pθ(τ) · R(τ)]. Understanding why log-probability weighted by reward yields gradient toward higher-return policies is essential.
  - Quick check question: Can you explain why multiplying ∇log π by reward moves the policy toward higher-reward actions?

- Concept: **Expectation-Maximization (EM) Algorithm**
  - Why needed here: The paper derives its method as an EM procedure (E-step: sample, M-step: optimize). Recognizing this pattern helps understand why the method is principled rather than heuristic.
  - Quick check question: In EM, what does the E-step compute and what does the M-step optimize?

- Concept: **Importance Sampling in RL**
  - Why needed here: The method explicitly avoids importance weights used in PPO/GRPO. Understanding why importance weights are normally needed (and what goes wrong without them) clarifies the tradeoff being made.
  - Quick check question: Why does PPO need importance ratios when using samples from an old policy?

## Architecture Onboarding

- Component map:
  - Trajectory Buffer D -> Reward Function R(τ) -> Reward Normalizer -> Policy Network Pθ <- Sampling Policy Pθ*

- Critical path:
  1. Freeze current parameters as θ*
  2. Generate N trajectories per prompt using temperature sampling
  3. Compute rewards and normalize
  4. Compute gradient ∇θL(θ) = E[∇log Pθ(τ) · (R̃(τ) - b)]
  5. Update θ, discard buffer, repeat

- Design tradeoffs:
  - **N (samples per prompt)**: Higher N improves reward estimation but increases compute
  - **Temperature**: Higher values (1.0) encourage exploration but may produce lower-quality initial traces
  - **Buffer reset frequency**: Paper resets every iteration; retaining longer would increase off-policy bias
  - **KL coefficient β**: 0 for weak models, positive for strong models

- Failure signatures:
  - **Reward collapse**: All trajectories converge to identical low-reward outputs → check temperature and initial policy quality
  - **Unstable gradients**: High variance in loss → increase baseline quality or batch size
  - **GRPO-style failure to learn** (mentioned for Qwen2.5-Math-7B): May indicate hyperparameter sensitivity that EMPG avoids

- First 3 experiments:
  1. **Reproduce GSM8K results on Qwen2.5-1.5B** with β=0, N=8-16 samples, temperature=1.0. Verify convergence within ~200 steps and reward improvement from baseline.
  2. **Ablate reward smoothing**: Compare raw binary rewards vs. sigmoid-normalized rewards on same model/dataset. Monitor gradient variance and convergence speed.
  3. **Compare buffer reuse strategies**: Test single-iteration buffer (paper default) vs. retaining samples for 2-3 iterations. Measure performance vs. training stability tradeoff.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Why does GRPO fail to learn on the Qwen2.5-Math-7B model for GSM8K while EMPG succeeds, and does this indicate a fundamental instability in importance weighting for specialized base models?
- Basis in paper: [explicit] Table 1 notes that GRPO scored "0.004*" and "failed to learn in this setting," whereas EMPG achieved 0.918.
- Why unresolved: The paper documents the failure but does not provide a theoretical or ablation study explaining the specific divergence of GRPO on this specific base model.
- What evidence would resolve it: An ablation study analyzing the variance of importance weights and gradient norms for GRPO versus EMPG specifically on the Math-7B checkpoint.

### Open Question 2
- Question: Can the "one-step off-policy" assumption maintain stability and reduce bias when scaling EM Policy Gradient to models significantly larger than 14B parameters?
- Basis in paper: [inferred] The method assumes bias is small when $\theta^* \approx \theta$, but experiments are limited to Qwen2.5-14B-Instruct.
- Why unresolved: As model size increases, the curvature of the loss landscape may change, potentially violating the assumption that the one-step off-policy bias remains negligible during large updates.
- What evidence would resolve it: Training runs on 70B+ scale models comparing the divergence between the current policy $\theta$ and the sampling policy $\theta^*$.

### Open Question 3
- Question: Does the absence of importance weights and clipping in EMPG lead to catastrophic forgetting or instability in non-mathematical domains such as coding or general instruction following?
- Basis in paper: [inferred] The paper evaluates exclusively on GSM8K and MATH HARD, stating the method is "particularly well-suited to reasoning tasks."
- Why unresolved: General capabilities or "alignment tax" (forgetting of non-reasoning skills) was not measured or discussed in the results.
- What evidence would resolve it: Evaluation on broad benchmarks (e.g., MMLU or IFEval) after RL fine-tuning to ensure general capabilities are retained.

## Limitations
- The paper lacks crucial implementation details including batch size, exact number of samples per prompt (N), learning rate, optimizer choice, and total training steps.
- The treatment of the KL constraint for strong models is inconsistent with the GSM8K-14B results, where β=0 performed best despite being a strong model.
- The reward model specification for MATH HARD is unclear—it's unspecified whether exact match or a learned verifier was used.

## Confidence
- High confidence: The EM framing and derivation of the off-policy gradient (Mechanism 1) is mathematically sound and well-supported by the literature on off-policy RL.
- Medium confidence: The reward smoothing mechanism (Mechanism 2) is technically valid but its empirical benefit depends heavily on batch composition and reward distribution.
- Low confidence: The claim that weak models benefit from unconstrained exploration (β=0) while strong models need KL regularization (Mechanism 3) is contradicted by the GSM8K-14B results showing β=0 performed best.

## Next Checks
1. **Replicate the off-policy gradient derivation**: Verify that removing importance weights is valid only when θ* ≈ θ, and test performance degradation when using larger policy updates.
2. **Ablate reward smoothing systematically**: Compare raw binary rewards vs. smoothed rewards across different batch sizes and reward distributions to isolate when smoothing helps vs. when it's unnecessary.
3. **Test KL constraint across model strengths**: Run experiments with varying β values (0, 0.01, 0.1, 1.0) on multiple model sizes to determine if the weak/strong model distinction holds or if results are dataset-specific.