---
ver: rpa2
title: How Language Directions Align with Token Geometry in Multilingual LLMs
arxiv_id: '2511.16693'
source_url: https://arxiv.org/abs/2511.16693
tags:
- language
- multilingual
- alignment
- token
- languages
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study systematically analyzes how multilingual LLMs encode\
  \ language information across all transformer layers. Using linear and MLP probes\
  \ plus a novel Token\u2013Language Alignment metric, the authors find that language\
  \ separation emerges sharply in the first block (+76.4 percentage points from Layer\
  \ 0 to 1) and remains nearly fully linearly separable (99.8\xB10.1%) throughout\
  \ model depth."
---

# How Language Directions Align with Token Geometry in Multilingual LLMs

## Quick Facts
- arXiv ID: 2511.16693
- Source URL: https://arxiv.org/abs/2511.16693
- Reference count: 12
- Primary result: Language identity is encoded in a globally accessible subspace; pretraining language composition shapes token geometry

## Executive Summary
This study systematically analyzes how multilingual LLMs encode language information across all transformer layers. Using linear and MLP probes plus a novel Token–Language Alignment metric, the authors find that language separation emerges sharply in the first block (+76.4 percentage points from Layer 0 to 1) and remains nearly fully linearly separable (99.8±0.1%) throughout model depth. Linear probes match MLP performance within 1%, indicating language identity lies in a globally accessible subspace. The Token–Language Alignment reveals strong structural imprinting: Chinese-inclusive models show a ZH Match@Peak of 16.43%, while English-centric models achieve only 3.90%, a 4.21× difference. This shows that pretraining language composition shapes the geometry of latent representations, not just performance. The findings highlight that fairness and balance in multilingual models require careful pretraining data design, as post-hoc adjustments cannot fully undo these structural imprints.

## Method Summary
The study uses 5 XNLI languages (EN, ES, FR, DE, ZH) with 5k training and 2.5k validation sentences per language across six multilingual LLMs (Llama-3.1-8B, Qwen2.5-7B, Qwen2.5-Math-7B, OpenMath2-8B, OpenR1-7B, GPT-OSS-20B). For each model, final-token hidden states are extracted at every transformer layer and normalized with LayerNorm. Linear and MLP probes are trained with cross-entropy, AdamW (lr=1e-3), batch size 128, 3 epochs, and early stopping. Token-Language Alignment is computed by measuring cosine similarity between probe weights and vocabulary embeddings. Statistical significance is assessed via paired t-tests across seeds.

## Key Results
- Language separability jumps sharply (+76.4pp) from Layer 0 to Layer 1 and remains at 99.8±0.1% throughout model depth
- Linear and MLP probe accuracies differ by less than 1%, confirming language identity lies in a globally accessible subspace
- Token–Language Alignment shows ZH Match@Peak of 16.43% in Chinese-inclusive models vs. 3.90% in English-centric models (4.21× difference)
- Language directions are shaped by pretraining composition, not just language performance

## Why This Works (Mechanism)
Language identity emerges early in transformer blocks and is encoded in a globally accessible subspace that linear probes can access with near-perfect accuracy. The geometry of token representations is structurally imprinted by pretraining language composition, creating language-specific directions that align more strongly with vocabulary embeddings for overrepresented languages. This structural encoding is robust across model depths and probe architectures, indicating that language information is not just a byproduct of performance but a fundamental geometric feature shaped during pretraining.

## Foundational Learning
- **LayerNorm preprocessing**: Critical for probe performance; without it, accuracy drops significantly
- **Cosine similarity for alignment**: Measures directional alignment between probe weights and vocabulary embeddings
- **Token representation geometry**: Understanding how languages carve out distinct subspaces in embedding space
- **Multilingual pretraining data composition**: How language ratios in pretraining data shape learned representations
- **Probe architecture equivalence**: Linear vs. MLP probes capture the same information when language identity is in a globally accessible subspace

## Architecture Onboarding

### Component Map
Data extraction -> LayerNorm preprocessing -> Probe training (Linear/MLP) -> Token-Language Alignment computation -> Statistical analysis

### Critical Path
1. Extract final-token hidden states at every layer
2. Apply LayerNorm to all representations
3. Train probes per layer with cross-entropy
4. Compute Token-Language Alignment metrics
5. Analyze layer-wise patterns and cross-model differences

### Design Tradeoffs
- **Linear vs. MLP probes**: MLP probes add complexity but provide minimal (<1%) accuracy gains, suggesting linear separability
- **LayerNorm application**: Essential preprocessing step that significantly impacts probe accuracy
- **Vocabulary embedding alignment**: Provides structural diagnostic but may not predict downstream task performance

### Failure Signatures
- Layer 0→1 accuracy jump much smaller than +76pp indicates LayerNorm not applied correctly
- Linear–MLP gap exceeds 1pp substantially suggests probe implementation issues or overfitting
- Inconsistent Token-Language Alignment across seeds indicates instability in probe training

### First Experiments
1. Verify Layer 0→1 accuracy jump on a single model and language
2. Compare linear vs. MLP probe accuracy gap on Layer 1 representations
3. Compute Token-Language Alignment for one language direction to validate cosine similarity calculation

## Open Questions the Paper Calls Out
- **Intervention studies**: How causal role of language directions affects cross-lingual transfer performance (explicit)
- **Tokenizer design influence**: How different tokenization approaches affect language direction formation (explicit)
- **Script complexity and depth**: Why Chinese requires deeper layers than Spanish/German for maximal separability (inferred)
- **Predictive validity of Match@Peak**: Whether this metric correlates with downstream task performance (inferred)

## Limitations
- Analysis limited to 5 XNLI languages and 6 model families, potentially limiting generalizability
- MLP probe hidden dimension unspecified, creating reproducibility uncertainty
- Exact data sampling procedure and model checkpoint hashes not provided
- Token-Language Alignment magnitude lacks direct benchmarks for validation

## Confidence
- **High**: Qualitative conclusion that pretraining composition shapes token geometry
- **Medium**: Layer-wise language separability and linear vs. MLP probe equivalence
- **Medium**: Token-Language Alignment magnitude differences across models

## Next Checks
1. Implement and compare MLP probes with multiple hidden dimensions to verify <1% linear–MLP gap robustness
2. Reproduce Layer 0→1 accuracy jump (+76.4pp) on a held-out language not in original 5 (e.g., Japanese)
3. Run Token-Language Alignment on a fully balanced multilingual pretraining dataset to quantify balanced vs. unbalanced effects on ZH vs. EN Match@Peak ratios