---
ver: rpa2
title: Mirror Flow Matching with Heavy-Tailed Priors for Generative Modeling on Convex
  Domains
arxiv_id: '2510.08929'
source_url: https://arxiv.org/abs/2510.08929
tags:
- flow
- space
- mirror
- matching
- distribution
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles constrained generative modeling on convex domains
  by extending flow matching with mirror maps. Standard log-barrier transforms lead
  to heavy-tailed dual distributions, making training unstable, while Gaussian priors
  poorly match such heavy-tailed targets.
---

# Mirror Flow Matching with Heavy-Tailed Priors for Generative Modeling on Convex Domains

## Quick Facts
- arXiv ID: 2510.08929
- Source URL: https://arxiv.org/abs/2510.08929
- Reference count: 40
- This paper tackles constrained generative modeling on convex domains by extending flow matching with mirror maps, proposing regularized mirror maps and Student-t priors to handle heavy-tailed dual distributions.

## Executive Summary
This paper addresses constrained generative modeling on convex domains by extending flow matching with mirror maps. Standard log-barrier transforms lead to heavy-tailed dual distributions, making training unstable, while Gaussian priors poorly match such heavy-tailed targets. The authors propose a regularized mirror map that controls tail behavior and ensures finite moments, coupled with a Student-t prior that aligns with heavy-tailed targets and stabilizes training. Theoretically, they prove spatial Lipschitzness and temporal regularity of the velocity field under mild tail assumptions, yielding Wasserstein convergence rates. Empirically, their method outperforms baselines in synthetic convex-domain tasks and achieves competitive sample quality on real-world watermarked image generation.

## Method Summary
The method uses a regularized mirror map Ψ(x) = -1/(1-κ) Σ(-φᵢ(x))^(1-κ) + ½‖x‖² to map constrained data to an unconstrained dual space, coupled with a Student-t prior with ν degrees of freedom. Training learns a velocity field to interpolate between Student-t prior samples and data in dual space, then maps back to primal space. The regularization parameter κ controls tail behavior, while the Student-t prior handles heavy-tailed dual targets. The approach ensures feasibility by construction through the mirror map's inverse.

## Key Results
- Outperforms baselines in synthetic convex-domain tasks (10D polytope, 6D L2 ball)
- Achieves competitive sample quality on real-world watermarked image generation
- Better metrics than standard Gaussian-based flows while preserving feasibility
- Theoretical guarantees on spatial Lipschitzness and temporal regularity of velocity field

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** The regularized mirror map Ψ(x) = -1/(1-κ) Σ(-φᵢ(x))^(1-κ) + ½‖x‖² controls heavy tails in the dual distribution, stabilizing training dynamics that fail under standard log-barriers.
- **Mechanism:** Standard log-barriers (κ → 0) often induce dual distributions with infinite moments, causing velocity field blow-ups. By setting κ > 0, the tail probability decays as P(‖∇Ψ(X)‖ ≥ R) ≤ C/R^(β/κ). If κ < β/p, the p-th moment exists, ensuring the dual velocity field remains well-defined.
- **Core assumption:** The probability mass near the constraint boundary decays polynomially (P(K \ K_δ) ≤ C_K δ^β).
- **Evidence anchors:** Mentions "regularized mirror map that controls dual tail behavior" [abstract]; Proves the tail bound P(…) ≤ C/R^(β/κ) and condition for finite moments [section 2.1, Proposition 2].
- **Break condition:** If the boundary mass decays slower than assumed (very sharp corners), β is small, and the condition κ < β/p may be impossible to satisfy while maintaining strong convexity.

### Mechanism 2
- **Claim:** Replacing Gaussian priors with Student-t priors aligns the conditional velocity field with heavy-tailed dual targets, preventing the divergence of E[X₁ | X_t=x].
- **Mechanism:** In flow matching, the velocity depends on the conditional expectation. If a light-tailed Gaussian prior is coupled with a heavy-tailed target, the conditional density p(X₁|X_t=x) develops a dominant mode at x/t for large x, causing super-exponential growth in the velocity. A Student-t prior suppresses this mode, keeping the expectation controlled.
- **Core assumption:** The target distribution satisfies a polynomial tail bound (Assumption 3: π₁^D(x) ≤ C/‖x‖^α).
- **Evidence anchors:** Identifies that "coupling with Gaussian priors performs poorly" and proposes "Student-t prior" [abstract]; Explicitly demonstrates the mode split and blow-up with Gaussian vs. Student-t priors [section 2.2, Example 4].
- **Break condition:** If the target distribution has lighter tails than the Student-t prior (e.g., compact support far from boundary), the mechanism still works but may be less efficient than a Gaussian prior.

### Mechanism 3
- **Claim:** Strong convexity of the regularized mirror map allows error bounds derived in the dual space to transfer directly to the primal constrained space.
- **Mechanism:** The map is designed so ∇²Ψ ≽ I, implying ‖x-y‖ ≤ L_Ψ ‖∇Ψ(x) - ∇Ψ(y)‖. This ensures that the Wasserstein distance in the primal space W₂(ν, μ) is bounded by the dual space distance W_{2,Ψ}(ν, μ). Guarantees on the learned dual velocity field (spatial/temporal Lipschitzness) therefore imply primal feasibility and convergence.
- **Core assumption:** The mirror potential Ψ is strongly convex (satisfied by the +½‖x‖² term).
- **Evidence anchors:** Claims "primal-space guarantees for constrained generation" [abstract]; Establishes the metric relationship required for error transfer [section 2.1, Ineq (2.1)].
- **Break condition:** If the regularization term is removed or the domain geometry forces L_Ψ to be extremely large, the primal error bound may become too loose to be useful.

## Foundational Learning

- **Concept: Mirror Descent & Mirror Maps**
  - **Why needed here:** The entire architecture relies on mapping constrained data to an unconstrained "dual" space via ∇Ψ to apply standard flow matching, and mapping back via ∇Ψ*.
  - **Quick check question:** Can you explain why the gradient of the convex potential Ψ maps a constrained convex set to an unconstrained space (e.g., mapping a simplex to ℝᵈ)?

- **Concept: Flow Matching (Conditional Vector Fields)**
  - **Why needed here:** This is the generative engine. Unlike diffusion (stochastic SDEs), this method learns a deterministic ODE dX_t = v(X_t, t)dt by regressing the vector field v.
  - **Quick check question:** How does the Conditional Flow Matching objective E[‖v(X_t, t) - (X₁ - X₀)‖²] differ from standard Score Matching?

- **Concept: Heavy-Tailed Distributions (Student-t)**
  - **Why needed here:** Understanding how tail weight affects moment existence and conditional expectations is critical to understanding why Gaussian priors fail in the dual space of this paper.
  - **Quick check question:** Why does the Student-t distribution have "heavier" tails than a Gaussian, and how does this affect the probability of extreme values?

## Architecture Onboarding

- **Component map:** Input: Data X₁ ∈ K (Primal Space) -> Projection: ∇Ψ(X₁) → Z₁ (Dual Space) -> Prior: Sample Z₀ ~ t_{d,ν} (Student-t) -> Training: Learn v̂_D(z, t) to interpolate Z₀ → Z₁ -> Inference: Euler steps z_{k+1} = z_k + h v̂_D(z_k, t_k) -> Reconstruction: ∇Ψ*(z_T) → X̂_T ∈ K

- **Critical path:** The design of the Mirror Map (specifically κ) and the choice of Prior (degrees of freedom ν). These must balance tail control (finite moments) with geometric fidelity (strong convexity).

- **Design tradeoffs:**
  - **κ (Mirror Map Regularization):** Lower κ approaches standard log-barrier (better geometry, worse tails); Higher κ regularizes tails but may distort the "straightness" of the dual flow.
  - **ν (Student-t DoF):** Lower ν = heavier tails (better for stability, but requires target to have sufficient tail decay α ≥ 2d+ν+2); Higher ν approaches Gaussian (efficient, but risks blow-up).

- **Failure signatures:**
  - **Mode Collapse / "Empty" Polytope:** Likely κ is too large, or the prior ν is mismatched with the target tail index α.
  - **NaN Loss / Gradient Explosion:** Boundary violation. The regularization κ might be insufficient for the specific constraint geometry, or ν is too low for the data complexity.
  - **Slow Convergence:** L_Ψ constant is too large (domain is "ill-shaped"), causing the primal error to lag behind dual error.

- **First 3 experiments:**
  1. **Synthetic 2D Polytope Visualization:** Verify that the dual trajectory is straight and the primal trajectory remains strictly inside the constraints (compare log-barrier vs. regularized map).
  2. **Tail Ablation (Gaussian vs. Student-t):** Train on a synthetic heavy-tailed target (e.g., Truncated Gaussian near boundary) and plot the loss curve stability and velocity field magnitude ‖v‖.
  3. **Parameter Sensitivity (κ vs ν):** Grid search κ ∈ [0.1, 0.5] and ν ∈ [1, 10] on the 10D Polytope task (Table 1) to find the stable operating region where feasibility is 100% and MMD is minimized.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the exponential dependence on the spatial Lipschitz constant L₁ in the error bound be improved to polynomial dependence?
- **Basis in paper:** The authors state: "It is plausible that this exponential dependency could be improved to polynomial dependence by following the probabilistic coupling strategy in Chen et al. (2023), though the resulting algorithm is not purely deterministic."
- **Why unresolved:** The exponential factor e^(6L₁) arises from the non-convex analysis of discretization error accumulation, and probabilistic couplings have not yet been adapted to the mirror flow matching setting.
- **What evidence would resolve it:** A theoretical proof establishing polynomial dependence on L₁, or counterexamples showing exponential dependence is necessary in worst-case scenarios.

### Open Question 2
- **Question:** Can t-Flow be extended to constrained domains with non-convex geometry while maintaining theoretical guarantees?
- **Basis in paper:** The conclusion proposes "extending t-Flow to constrained domains with non-convex geometry, potentially leveraging landing techniques."
- **Why unresolved:** The current theory relies critically on convexity for the mirror map properties (strong convexity of Ψ, bounded gradients of φᵢ), which fail for non-convex constraint sets.
- **What evidence would resolve it:** A framework with convergence guarantees for specific non-convex constraint classes, or empirical demonstrations showing when the method fails on non-convex domains.

### Open Question 3
- **Question:** Can the degrees of freedom ν in the Student-t prior be adapted automatically to local tail behavior of the target distribution?
- **Basis in paper:** The conclusion states: "exploring adaptive choices of degrees of freedom in the t-prior could yield even more flexibility, enabling flows that automatically adapt to local tail behavior of the data."
- **Why unresolved:** Current approach uses a fixed ν throughout training, but optimal tail matching may vary spatially; no principled mechanism exists for adaptive selection.
- **What evidence would resolve it:** An algorithm with learned or region-specific ν values, with empirical comparisons showing improved sample quality over fixed choices.

## Limitations

- The theoretical guarantees assume well-behaved polynomial decay of boundary mass, which may not hold for real-world constraints
- The practical range of usable κ values and their sensitivity to constraint geometry remains unclear
- The primal-dual error transfer relies on a Lipschitz constant L_Ψ that could be prohibitively large for ill-conditioned domains

## Confidence

- **Heavy-tailed prior mechanism:** High confidence - clear theoretical analysis and supporting literature on heavy-tail robustness
- **Regularized mirror map mechanism:** Medium confidence - sound theoretical framework but empirical validation limited to specific settings
- **Primal-dual guarantee mechanism:** Medium confidence - lack of corpus support for the specific error transfer bounds claimed

## Next Checks

1. **Boundary Layer Sensitivity:** Systematically vary constraint sharpness in synthetic experiments to test how β changes affect the allowable κ range and training stability.

2. **Prior-Target Alignment:** For the watermarked image experiments, estimate the empirical tail index of the projected distribution and verify whether the chosen ν=10 Student-t prior is theoretically justified.

3. **L_Ψ Scaling Analysis:** Quantify how the primal error scales with L_Ψ across different constraint geometries to determine when the dual-space guarantees become practically useful.