---
ver: rpa2
title: 'LaViRA: Language-Vision-Robot Actions Translation for Zero-Shot Vision Language
  Navigation in Continuous Environments'
arxiv_id: '2510.19655'
source_url: https://arxiv.org/abs/2510.19655
tags:
- navigation
- action
- language
- vision
- zero-shot
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'LaViRA introduces a zero-shot framework for Vision-and-Language
  Navigation in Continuous Environments by decomposing the navigation task into a
  hierarchical action space: Language Action for high-level planning, Vision Action
  for perceptual grounding, and Robot Action for low-level control. This modular approach
  allows different scales of Multimodal Large Language Models to be applied at each
  stage, fully leveraging their reasoning and grounding capabilities without requiring
  environment-specific training.'
---

# LaViRA: Language-Vision-Robot Actions Translation for Zero-Shot Vision Language Navigation in Continuous Environments

## Quick Facts
- arXiv ID: 2510.19655
- Source URL: https://arxiv.org/abs/2510.19655
- Authors: Hongyu Ding; Ziming Xu; Yudong Fang; You Wu; Zixuan Chen; Jieqi Shi; Jing Huo; Yifan Zhang; Yang Gao
- Reference count: 36
- Key outcome: LaViRA achieves 38.3% success rate and 28.3% SPL on VLN-CE benchmark using zero-shot MLLM pipeline

## Executive Summary
LaViRA introduces a hierarchical zero-shot framework for Vision-and-Language Navigation in Continuous Environments (VLN-CE) that decomposes navigation into Language Action (high-level planning), Vision Action (perceptual grounding), and Robot Action (low-level control). By leveraging different scales of Multimodal Large Language Models at each stage without environment-specific training, LaViRA significantly outperforms existing zero-shot methods on the VLN-CE benchmark. The modular approach achieves 38.3% success rate and 28.3% SPL using Gemini-2.5-Pro, demonstrating superior generalization to unseen environments while maintaining transparency and efficiency suitable for real-world deployment.

## Method Summary
LaViRA implements a three-stage zero-shot pipeline for VLN-CE that avoids environment-specific training. First, a large MLLM (GPT-4o or Gemini-2.5-Pro) performs Language Action to determine high-level directional commands from natural language instructions and panoramic observations. Second, a smaller MLLM (Qwen2.5-VL-32B) performs Vision Action to ground the directional plan into specific visual targets via bounding boxes. Third, a rule-based Robot Action module projects the 2D bounding box to 3D world coordinates using depth estimation and executes path planning via Fast Marching Method with obstacle avoidance. The framework is evaluated on Habitat simulator with VLN-CE dataset's 100-episode validation unseen split, measuring Success Rate, SPL, Navigation Error, and Oracle Success Rate.

## Key Results
- Achieves 38.3% success rate and 28.3% SPL on VLN-CE benchmark with Gemini-2.5-Pro
- Outperforms state-of-the-art zero-shot methods by significant margins
- Ablation shows removing Language Action or Vision Action drops SPL to 0%
- Model pairing (large for planning, small for grounding) outperforms using large MLLM for both stages (SPL 28.3% vs 16.8%)

## Why This Works (Mechanism)

### Mechanism 1
Hierarchical action decomposition improves zero-shot VLN-CE performance by separating high-level planning from perceptual grounding and low-level control. Large MLLMs excel at reasoning and planning, smaller MLLMs handle focused perception, and rule-based controllers provide interpretable navigation without training.

### Mechanism 2
Matching MLLM scale to task granularity maximizes both performance and efficiency. Large models handle complex reasoning for high-level planning while smaller models efficiently perform perceptual grounding, reducing computational cost without sacrificing accuracy.

### Mechanism 3
Removing dependency on pre-trained waypoint predictors improves generalization by using MLLM-driven reasoning and visual grounding instead of environment-specific models. Targets are projected to 3D via depth and transformed to world coordinates, enabling navigation in unseen environments.

## Foundational Learning

- **Concept: Vision-and-Language Navigation (VLN) in Continuous Environments**
  - Why needed here: LaViRA targets VLN-CE requiring continuous visual perception and low-level motor control, unlike discrete graph-based VLN.
  - Quick check question: Can you explain why VLN-CE is harder than discrete VLN?

- **Concept: Multimodal Large Language Models (MLLMs) for embodied decision-making**
  - Why needed here: LaViRA relies on MLLMs for high-level planning and visual grounding without training.
  - Quick check question: What types of inputs and outputs does an MLLM handle in this framework?

- **Concept: Pixel-to-3D projection and path planning**
  - Why needed here: Robot Action converts 2D bounding boxes to 3D world coordinates and computes paths.
  - Quick check question: How do intrinsic matrix and depth convert a pixel to a 3D camera coordinate?

## Architecture Onboarding

- **Component map**: Language Action (MLLM) → Vision Action (MLLM) → Robot Action (projection + FMM + controller)
- **Critical path**: Instruction + observation → LA (direction) → VA (bbox) → depth + pose → 3D target → path → control
- **Design tradeoffs**: Large MLLM improves planning but increases cost (~$0.084/episode); rule-based controller is interpretable but less adaptive than learned policies; depth-based projection is sensitive to reconstruction artifacts.
- **Failure signatures**: (1) Ambiguous instructions → LA selects wrong direction; (2) Large-area grounding targets → VA outputs meaningless bbox; (3) Transparent surfaces → depth missing → projection fails.
- **First 3 experiments**:
  1. Run LaViRA code on VLN-CE 100-episode validation with GPT-4o + Qwen2.5-VL-32B, logging SR, SPL, and per-stage failures.
  2. Ablate Vision Action by replacing Qwen2.5-VL-32B with GPT-4o to observe performance gap.
  3. Test Robot Action module in isolation with synthetic bounding boxes and depth maps to verify pixel-to-world projection and path planning.

## Open Questions the Paper Calls Out

- **Open Question 1**: Can Vision Action modules be improved to reliably ground spatial regions (e.g., "hallway," "living room") rather than only specific objects?
  - Basis: Paper notes VA "sometimes fails to locate a relatively larger area like 'hallway' or 'living room' and chooses a meaningless region."
  - What evidence would resolve it: Modified grounding model achieving higher success rates on region-based instructions.

- **Open Question 2**: How can LaViRA handle depth reconstruction failures (e.g., transparent surfaces) that break the pixel-to-world projection pipeline?
  - Basis: Paper identifies failure where "transparent objects like windows are not assigned depth values...causing it to mislocate the target."
  - What evidence would resolve it: Robust projection method that degrades gracefully under missing depth data.

- **Open Question 3**: Does fine-tuning MLLMs on navigation data enhance robustness while preserving zero-shot generalization capabilities?
  - Basis: Conclusion states future work will focus on "enhancing robustness by fine-tuning the MLLMs on in-domain navigation data."
  - What evidence would resolve it: Benchmarks comparing fine-tuned vs. zero-shot variants across seen and held-out environments.

## Limitations

- Performance gains come at increased computational cost (~$0.084 per episode for GPT-4o inference), raising deployment concerns.
- Success depends on depth estimation quality, with transparent surfaces causing systematic failures when depth reconstruction is missing.
- Framework evaluated only on indoor VLN-CE benchmark; generalization to outdoor or highly dynamic environments untested.

## Confidence

- **High confidence**: Hierarchical decomposition approach and superiority over baselines (SR 38.3% vs 28.3%) - supported by direct ablation studies.
- **Medium confidence**: Claims about computational efficiency and practical deployment - runtime costs reported but real-world scenarios not validated.
- **Low confidence**: Generalization claims to entirely new environments beyond VLN-CE - evaluated only on VLN-CE benchmark without testing on different simulator platforms.

## Next Checks

1. **Depth sensitivity analysis**: Systematically test LaViRA's performance with degraded depth maps (Gaussian noise, missing regions) to quantify Robot Action module's robustness.
2. **Cross-environment transfer**: Evaluate LaViRA on different VLN benchmark (e.g., Room-Across-Room or REVERIE) to test zero-shot generalization beyond VLN-CE dataset.
3. **Real-time performance validation**: Measure end-to-end latency of LaViRA on commodity hardware (e.g., RTX 3060) to assess practical deployment feasibility beyond RTX 4090 results.