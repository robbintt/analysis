---
ver: rpa2
title: Certified but Fooled! Breaking Certified Defences with Ghost Certificates
arxiv_id: '2511.14003'
source_url: https://arxiv.org/abs/2511.14003
tags:
- attack
- adversarial
- certified
- attacks
- robustness
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents GhostCert, a new adversarial attack targeting
  certified defenses by generating imperceptible perturbations that cause misclassification
  while spoofing robustness certificates. The core idea is to use region-based input
  manipulations, focusing on salient and semantically coherent areas identified through
  segmentation and saliency analysis, to preserve input semantics while minimizing
  distortions.
---

# Certified but Fooled! Breaking Certified Defences with Ghost Certificates

## Quick Facts
- arXiv ID: 2511.14003
- Source URL: https://arxiv.org/abs/2511.14003
- Reference count: 14
- Primary result: GhostCert attack achieves up to 90% success rate against certified defenses, outperforming prior methods by targeting salient regions to generate imperceptible perturbations that spoof robustness certificates

## Executive Summary
This paper presents GhostCert, a novel adversarial attack that targets certified defenses by generating imperceptible perturbations that cause misclassification while spoofing robustness certificates. The attack focuses on semantically coherent salient regions identified through segmentation and saliency analysis, achieving significantly higher success rates than prior state-of-the-art methods. GhostCert demonstrates that high certification radii do not guarantee correctness, successfully bypassing multiple certified defenses including Randomized Smoothing, Ensemble models, and DensePure. User studies confirm GhostCert-generated images are consistently rated more natural-looking than those from baseline attacks.

## Method Summary
GhostCert is an adversarial attack targeting certified defenses by focusing perturbations on semantically coherent salient regions. The method combines SAM segmentation with GradCAM/attention saliency to identify classification-critical areas, then applies PGD optimization with masked projection to maintain imperceptibility while maximizing loss across multiple noisy samples. The attack specifically targets the certification mechanism of Randomized Smoothing by manipulating the probability margin that determines the certified radius. By concentrating adversarial effort in selected regions rather than applying global perturbations, GhostCert achieves higher success rates while maintaining perceptual realism. The approach is evaluated against three certified defenses (Randomized Smoothing, Ensemble models, and DensePure) on ImageNet with varying noise levels and perturbation budgets.

## Key Results
- GhostCert achieves 90% attack success rate on average across certified defenses, compared to 40% for Shadow Attack
- The attack successfully spoofs robustness certificates with radii larger than the source class while maintaining imperceptible perturbations
- User studies show GhostCert images are rated more natural-looking than Shadow Attack across all distortion levels
- DensePure, the current state-of-the-art certified defense, is vulnerable to GhostCert with 71% ASR

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Constraining perturbations to semantically coherent salient regions improves both imperceptibility and attack success against certified defenses.
- Mechanism: GhostCert combines GradCAM (or attention maps for transformers) with SAM segmentation to identify regions that are both classification-critical and bounded by natural image edges. Top-k regions are selected via saliency overlap scores (Eq. 9), then aggregated into a single mask m (Eq. 10). Perturbations are applied only within m, preserving semantic coherence while concentrating adversarial effect.
- Core assumption: Models rely disproportionately on salient regions for classification, so perturbations there achieve greater label shift per unit distortion than global perturbations.
- Evidence anchors:
  - [abstract]: "We explore the idea of region-focused adversarial examples to craft imperceptible perturbations, spoof certificates and achieve certification radii larger than the source class—ghost certificates."
  - [section]: Page 4, "Our core hypothesis posits that constraining adversarial perturbations to these semantically-coherent salient regions will yield adversarial examples with imperceptibility and preserved semantic meaning."
  - [corpus]: No direct corpus evidence on salient-region attacks against certified defenses; related work focuses on standard adversarial robustness certification (arXiv:2512.20865) but not region-constrained spoofing.
- Break condition: If the target model's decision boundary is determined primarily by non-salient background features, or if segmentation fails to capture meaningful semantic boundaries, mask-guided attacks will underperform global perturbation.

### Mechanism 2
- Claim: Optimizing over multiple noisy samples simultaneously manipulates both classifier predictions and certification radii.
- Mechanism: The objective (Eqs. 6–8, 11) maximizes loss over N noisy samples (ε_i), forcing the smoothed classifier's top-class probability to drop below the runner-up within the certification radius. Unlike standard PGD targeting a single point, this ensemble-of-noise objective explicitly controls the probability margin that determines certified radius R = (σ/2)[Φ⁻¹(p_A) − Φ⁻¹(p_B)] (Eq. 4).
- Core assumption: The Monte Carlo approximation with N samples sufficiently captures the smoothed classifier's true behavior under Gaussian noise.
- Evidence anchors:
  - [abstract]: "Our study investigates if perturbations needed to cause a misclassification and yet coax a certified model into issuing a deceptive, large robustness radius for a target class can still be made small and imperceptible."
  - [section]: Page 9, "To achieve this goal, we have to maximize sample agreement under the smoothing distribution rather than a single sample."
  - [corpus]: Related work (arXiv:2512.20865) discusses certification frameworks but does not address multi-sample adversarial optimization.
- Break condition: If N is too small to approximate the smoothed distribution accurately, or if the base classifier's noise response is highly irregular, gradient estimates will be unreliable.

### Mechanism 3
- Claim: Masked projection during gradient descent maintains perturbation budgets while concentrating changes in selected regions.
- Mechanism: After each gradient step, perturbation δ is projected onto the ℓ₂ ball of radius ε, then element-wise multiplied with mask m (Algorithm 1, line 9). This enforces ∥δ⊙m∥₂ ≤ ε while zeroing perturbations outside salient regions, preventing wasted budget on perceptually salient but semantically unimportant areas.
- Core assumption: The mask m correctly identifies regions where perturbations are both effective and less perceptually noticeable.
- Evidence anchors:
  - [section]: Page 5, Eq. 11 and Algorithm 1 line 9: "δ ← (ε·δ/∥δ∥₂) ⊙ m ▷ Projection and mask step"
  - [section]: Page 7, user study results show GhostCert images rated more natural-looking across distortion levels.
  - [corpus]: No corpus evidence on masked projection for certification spoofing.
- Break condition: If the mask is too restrictive (small k) or includes regions with high perceptual sensitivity, the attack will either fail to achieve misclassification or produce visible artifacts.

## Foundational Learning

- **Randomized Smoothing (RS)**
  - Why needed here: RS constructs a smoothed classifier g from base f by taking majority vote under Gaussian noise, providing certifiable robustness radius R. GhostCert explicitly targets this certification mechanism.
  - Quick check question: Given p_A = 0.7, p_B = 0.2, and σ = 0.5, compute the certified radius R using Eq. 4.

- **Projected Gradient Descent (PGD)**
  - Why needed here: PGD provides the iterative optimization backbone for GhostCert's attack, navigating the loss surface with bounded perturbation steps.
  - Quick check question: Explain why PGD's projection step is modified with mask multiplication in GhostCert (line 9) compared to standard PGD.

- **Certification Radius and Probability Margin**
  - Why needed here: The certification radius depends on the gap between top-class and runner-up probabilities. GhostCert must collapse this margin to induce misclassification while spoofing a large radius for the wrong class.
  - Quick check question: If an attacker wants a spoofed radius of 1.0 with σ = 0.5, what minimum probability gap (p_A − p_B) must they achieve?

## Architecture Onboarding

- **Component map:** Image → Saliency + Segmentation → Mask → Noisy batch generation → Iterative PGD with masked projection → Adversarial example → Certification check

- **Critical path:** Saliency extractor (GradCAM/attention) → Segmentation module (SAM) → Mask selector (overlap scoring, top-k) → Noise sampler (N Gaussian samples) → Attack optimizer (PGD with masked projection) → Certification verifier (R computation)

- **Design tradeoffs:**
  - Higher k (more regions) increases attack surface but may reduce imperceptibility
  - Larger N improves gradient estimation but increases computation
  - Larger σ (noise level) makes certification harder but also increases spoofed radius potential

- **Failure signatures:**
  - Low ASR with high DoS rate → perturbation budget insufficient for boundary crossing
  - Visible artifacts → mask includes perceptually sensitive regions; reconsider k or segmentation quality
  - Inconsistent results across noise levels → N too small for stable Monte Carlo approximation

- **First 3 experiments:**
  1. **Reproduce ablation on k**: Run GhostCert with k ∈ {3, 5, 7} on 20 ImageNet images against RS ensemble (σ=0.5); verify ASR matches Table 6.
  2. **Saliency vs. random mask comparison**: Compare GhostCert masks against 50% random pixel selection (Table 4) and k random regions (Table 5); confirm saliency-guided selection provides higher ASR.
  3. **Perceptibility baseline**: Run user study comparing GhostCert vs. Shadow Attack at ∥δ∥₂ = 4 and 8; verify GhostCert images are rated more natural-looking (>60% preference).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can GhostCert perturbations transfer effectively to black-box settings where the attacker lacks access to model parameters, or to settings where the noise level $\sigma$ is unknown?
- Basis in paper: [explicit] The paper states: "We consider a white-box threat model for attacks on a target DNN, where adversaries have full access to the model’s architecture, parameters and the noise level used by the smoothed classifier."
- Why unresolved: The evaluation protocol and algorithm assume full gradient access to compute saliency maps and optimize the spoofed radius. The feasibility of certificate spoofing without this privileged information remains unexplored.
- What evidence would resolve it: Empirical results showing Attack Success Rates (ASR) and spoofed radii when attacking substitute models (transfer attacks) or using query-based optimization strategies without model gradients.

### Open Question 2
- Question: Is the vulnerability to region-based semantic manipulation specific to probabilistic Randomized Smoothing, or does it extend to complete or deterministic certification methods?
- Basis in paper: [explicit] The authors note that "Complete certification methods... are limited to small datasets... We focus on the probabilistic Randomized Smoothing (Cohen, Rosenfeld, and Kolter 2019) frameworks offering scalable certification."
- Why unresolved: The study explicitly limits its scope to probabilistic methods due to scalability. It remains unknown if the specific "ghost certificate" phenomenon—shifting inputs into incorrect high-certainty regions—applies to the tighter, often discrete bounds of complete certification.
- What evidence would resolve it: Evaluations of the GhostCert attack against complete certifiers (e.g., using interval bound propagation) on smaller datasets like CIFAR-10 where such methods are tractable.

### Open Question 3
- Question: Can robustness certification frameworks be augmented with semantic or segmentation-aware priors to detect region-specific manipulation without sacrificing the scalability of Randomized Smoothing?
- Basis in paper: [inferred] The conclusion urges "further research into certification methods" after demonstrating that even SOTA defenses like DensePure are vulnerable to region-focused attacks that preserve "semantically meaningful regions."
- Why unresolved: The paper demonstrates the failure of current defenses but does not propose mechanisms to verify the semantic consistency of the input relative to the certification radius.
- What evidence would resolve it: A new certification scheme that incorporates segmentation constraints (e.g., penalizing perturbations that align suspiciously well with object boundaries) and maintains high certified accuracy against GhostCert.

## Limitations
- Mask quality depends heavily on segmentation accuracy and saliency map reliability, which may degrade on out-of-distribution or fine-grained images
- Attack effectiveness may vary significantly across different defense architectures and certification mechanisms beyond Randomized Smoothing
- Computational cost is high due to N=1000 noise samples required for stable gradient estimation

## Confidence
- **High confidence**: The core attack mechanism (region-focused masked perturbations targeting RS certification radii) is technically sound and demonstrably effective on specified benchmarks
- **Medium confidence**: Claims about perceptual superiority over Shadow Attack are supported by user studies but may be task-dependent
- **Low confidence**: The attack's effectiveness against certification frameworks not based on Gaussian smoothing or those using fundamentally different noise distributions remains speculative

## Next Checks
1. **Cross-dataset robustness**: Test GhostCert against certified models on CIFAR-10 and a fine-grained dataset (e.g., CUB-200) to assess performance degradation on non-ImageNet domains
2. **Mask sensitivity analysis**: Systematically vary k (number of regions) and examine the trade-off between attack success rate and perceptibility across diverse image types to identify failure modes
3. **Real-world applicability**: Evaluate GhostCert against certified models deployed with non-standard noise distributions (e.g., Laplacian smoothing) or alternative certification frameworks to test attack adaptability