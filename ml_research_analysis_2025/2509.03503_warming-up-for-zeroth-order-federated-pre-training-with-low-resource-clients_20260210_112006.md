---
ver: rpa2
title: Warming Up for Zeroth-Order Federated Pre-Training with Low Resource Clients
arxiv_id: '2509.03503'
source_url: https://arxiv.org/abs/2509.03503
tags:
- training
- resource
- clients
- high
- federated
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'ZOWarmUp enables low-resource clients to participate in federated
  pre-training by using a two-step approach: first, high-resource clients train the
  model using standard methods; then, both high- and low-resource clients perform
  zeroth-order updates. This eliminates the need for low-resource clients to compute
  gradients or store activations, drastically reducing memory and communication costs.'
---

# Warming Up for Zeroth-Order Federated Pre-Training with Low Resource Clients

## Quick Facts
- **arXiv ID:** 2509.03503
- **Source URL:** https://arxiv.org/abs/2509.03503
- **Reference count:** 18
- **Primary result:** ZOWarmUp enables low-resource clients to participate in federated pre-training by using a two-step approach: first, high-resource clients train the model using standard methods; then, both high- and low-resource clients perform zeroth-order updates. This eliminates the need for low-resource clients to compute gradients or store activations, drastically reducing memory and communication costs. The method uses variance-reduction techniques including Rademacher sampling, averaging multiple perturbations, and single gradient steps per round. Experiments on CIFAR-10 and ImageNet32 show that ZOWarmUp consistently outperforms baselines like HeteroFL and High Res Only, especially when low-resource clients are numerous. Accuracy improves from 44.3% to 54.3% (CIFAR-10, 10/90 split) and from 10.9% to 12.3% (ImageNet32, 10/90 split). The approach is robust across model architectures and resource splits, and enables access to otherwise unused data, improving overall training outcomes.

## Executive Summary
ZOWarmUp addresses the challenge of federated pre-training with heterogeneous clients by enabling low-resource devices to participate without requiring gradient computation or activation storage. The method employs a two-phase approach where high-resource clients first train the model using standard backpropagation, then both high- and low-resource clients perform zeroth-order updates using simultaneous perturbation stochastic approximation (SPSA). This approach reduces memory and communication costs for low-resource clients while maintaining competitive accuracy. Experiments demonstrate that ZOWarmUp consistently outperforms baseline methods, particularly when low-resource clients represent the majority of participants.

## Method Summary
The method uses a two-phase training regime to enable federated pre-training with heterogeneous clients. In Phase 1 (Warm-up), high-resource clients train the model using standard FedAvg for a predetermined number of rounds (typically 200). This moves the model weights into a region where zeroth-order optimization becomes feasible. In Phase 2 (ZO Training), all clients (both high and low resource) participate using zeroth-order optimization based on SPSA. Clients compute gradient approximations using only forward passes with Rademacher perturbations, then update weights locally using single gradient steps. The communication pattern shifts from model weights in Phase 1 to scalar loss differences in Phase 2, dramatically reducing communication costs for low-resource clients.

## Key Results
- ZOWarmUp consistently outperforms baselines like HeteroFL and High Res Only, especially when low-resource clients are numerous
- Accuracy improvements: CIFAR-10 from 44.3% to 54.3% (10/90 split), ImageNet32 from 10.9% to 12.3% (10/90 split)
- The approach is robust across model architectures and resource splits
- Low-resource client data access improves overall training outcomes
- Single gradient step per round with full-batch processing reduces client drift and variance accumulation

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** A two-phase training regime (first-order warm-up followed by zeroth-order federated optimization) enables convergence from random initialization where zeroth-order methods alone fail.
- **Mechanism:** High-resource clients using standard backpropagation move model weights into a region of parameter space where loss gradients are sufficiently well-behaved for noisy zeroth-order approximations to provide productive descent directions. The pre-trained weights serve as a stable initialization point that reduces the variance sensitivity of subsequent SPSA updates.
- **Core assumption:** The loss landscape after high-resource training is smooth enough that perturbation-based gradient estimates point in productive directions on average.
- **Evidence anchors:** FedKSeed "is not able to converge when training from a random weight initialization" but succeeds with ZOWarmUp warm-start.
- **Break condition:** If the high-resource client fraction is extremely low or their data distribution is severely non-representative, the warm-up phase may converge to a poor local optimum that ZO updates cannot escape.

### Mechanism 2
- **Claim:** Single gradient step per communication round with full-batch processing reduces client drift and variance accumulation compared to multi-step local training in zeroth-order federated settings.
- **Mechanism:** Multiple local ZO gradient steps compound approximation errors because each step depends on noisy gradient estimates, causing client models to diverge from the global optimum. By restricting updates to a single gradient step while maintaining equivalent sample budget, variance is reduced through averaging over more data per step.
- **Core assumption:** The trade-off between communication frequency and gradient quality favors single-step updates when communication cost is negligible.
- **Evidence anchors:** Taking one gradient step converges faster and reaches a lower loss value than FedKSeed with multiple gradient steps.
- **Break condition:** If client datasets are very small, single-step full-batch updates may have insufficient samples for stable gradient estimation.

### Mechanism 3
- **Claim:** Rademacher distribution for weight perturbation provides lower-variance gradient approximations than Gaussian distribution in SPSA-based federated zeroth-order optimization.
- **Mechanism:** The Rademacher distribution (values ±1 with equal probability) has lower variance than the standard Gaussian while still providing unbiased gradient estimates. When scaled by τ, it produces more consistent perturbation directions across the high-dimensional parameter space.
- **Core assumption:** Lower variance in the perturbation distribution translates to more stable gradient estimates across communication rounds.
- **Evidence anchors:** Rademacher achieves 65.5% accuracy with stdv 5.2 vs Gaussian 49.4% with stdv 7.7.
- **Break condition:** The τ scaling factor must be tuned per task; incorrect values cause either insufficient exploration or unstable updates.

## Foundational Learning

- **Concept: Simultaneous Perturbation Stochastic Approximation (SPSA)**
  - **Why needed here:** Core gradient approximation technique enabling memory-efficient updates without backpropagation
  - **Quick check question:** Can you explain why SPSA requires only two forward passes regardless of model dimensionality, compared to finite difference methods?

- **Concept: Client Drift in Federated Learning**
  - **Why needed here:** Understanding why multi-step local training degrades performance in heterogeneous ZO settings
  - **Quick check question:** Why does taking multiple local gradient steps before aggregation cause models to diverge from the global optimum?

- **Concept: Non-IID Data Distribution**
  - **Why needed here:** Experiments use Dirichlet distribution (α=0.1) to simulate heterogeneous client data, which amplifies the value of including low-resource clients
  - **Quick check question:** How does label distribution skew across clients affect the benefit of including low-resource client data?

## Architecture Onboarding

- **Component map:** Phase 1: Server samples high-resource clients → clients run FedAvg for N rounds → aggregated weights broadcast to all clients. Phase 2: Server generates seeds per client → clients perform forward passes to compute ∆L values → clients return only scalars → server aggregates → all clients reconstruct gradients locally using shared seeds and update weights.

- **Critical path:** 1) Identify resource threshold separating high/low resource clients. 2) Tune pivot point (round N) balancing warm-up sufficiency vs data withholding penalty. 3) Set ZO hyperparameters: ε=1e-4, S=3, τ=0.75. 4) Monitor training curves for phase transition quality.

- **Design tradeoffs:** Pivot point selection (earlier → more data utilization but riskier initialization; later → stable initialization but potential critical learning period interference). S (perturbation count) higher S reduces variance but increases forward passes. High-resource algorithm choice (FedAvg outperforms FedAdam in experiments).

- **Failure signatures:** ZO phase shows flat or diverging loss → pivot point too early or τ too large. Final accuracy below "High Res Only" baseline → data distribution severely skewed. High variance across seeds → increase S or verify Rademacher distribution implementation. FedAdam underperforms → expected behavior; switch to FedAvg for warm-up phase.

- **First 3 experiments:** 1) Baseline validation: Replicate 10/90 split on CIFAR-10 with ResNet18, 200 warm-up rounds + 300 ZO rounds; target ~54% accuracy. 2) Pivot point ablation: Test pivot points {100, 200, 300} on 50/50 split; expect accuracy maximum between 200-300. 3) Distribution comparison: Run 12 seeds each with Gaussian vs Rademacher perturbation on 10/90 split; expect Rademacher to show lower variance and higher minimum accuracy.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the transition point between high-resource warm-up and zeroth-order training (the pivot point) be determined adaptively rather than manually?
- Basis in paper: Section 4.3 states the pivot point "affects training outcomes and must be tuned" as it is highly task and architecture dependent.
- Why unresolved: The paper relies on manual ablation studies to select a static pivot point for different resource splits.
- What evidence would resolve it: An algorithm that dynamically triggers the switch based on convergence metrics, gradient variance thresholds, or a theoretical bound on the required warm-up duration.

### Open Question 2
- Question: How can adaptive optimizers like Adam be stabilized to handle the high variance inherent in zeroth-order gradient approximations?
- Basis in paper: Section 4.4 notes FedAdam underperforms FedAvg because "Adam's reliance on the first and second moments... is problematic" with noisy ZO estimates.
- Why unresolved: The erratic moment estimates caused by ZO variance hinder convergence, and the paper does not propose a specific modification to rectify this.
- What evidence would resolve it: A variance-robust variant of FedAdam or experimental results showing convergence parity with FedAvg under specific regularization or moment-smoothing conditions.

### Open Question 3
- Question: Does ZOWarmUp maintain its efficiency advantages when scaling to large-scale Transformer architectures and datasets?
- Basis in paper: Section 4.5 shows ViT-B/16 under-performs ResNet18, which the authors attribute to small datasets, leaving the method's scalability unverified.
- Why unresolved: Experiments are limited to ResNet18 and smaller datasets (CIFAR-10, ImageNet32), whereas modern pre-training often involves larger models and data pools.
- What evidence would resolve it: Benchmarking ZOWarmUp on large-scale pre-training tasks (e.g., LLMs on the C4 corpus) where the memory savings of ZO methods are most critical.

## Limitations
- The two-phase approach fundamentally depends on high-resource clients providing sufficient and representative pre-training
- The pivot point selection (200 rounds in experiments) is empirically determined and may not generalize across different model architectures
- The SPSA gradient estimation introduces inherent noise that may accumulate over training
- The method's efficiency advantages at scale are unverified with large-scale Transformer architectures

## Confidence
- **High Confidence:** The two-phase training regime enables convergence where zeroth-order methods alone fail
- **Medium Confidence:** Rademacher sampling provides lower-variance gradient approximations than Gaussian
- **Medium Confidence:** Single gradient step per round reduces variance compared to multi-step approaches

## Next Checks
1. **Pivot Point Sensitivity:** Test pivot points at {100, 200, 300} rounds on 50/50 client split to verify the existence of an optimal transition point and identify potential overfitting to high-resource data.
2. **Distribution Ablation:** Run ablation studies comparing Rademacher vs Gaussian perturbations with 12 random seeds on 10/90 split to confirm the claimed variance reduction and accuracy improvements.
3. **Resource Split Robustness:** Evaluate ZOWarmUp performance across resource splits of {10/90, 30/70, 50/50} with multiple random seeds to confirm consistent improvements over baselines across different client compositions.