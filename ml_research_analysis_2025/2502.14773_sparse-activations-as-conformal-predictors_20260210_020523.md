---
ver: rpa2
title: Sparse Activations as Conformal Predictors
arxiv_id: '2502.14773'
source_url: https://arxiv.org/abs/2502.14773
tags:
- prediction
- conformal
- entmax
- coverage
- size
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces a new connection between conformal prediction\
  \ and sparse softmax-like transformations, such as sparsemax and \u03B3-entmax,\
  \ for uncertainty quantification in classification. The authors derive new non-conformity\
  \ scores that make the calibration process equivalent to temperature scaling of\
  \ these sparse transformations."
---

# Sparse Activations as Conformal Predictors

## Quick Facts
- **arXiv ID**: 2502.14773
- **Source URL**: https://arxiv.org/abs/2502.14773
- **Reference count**: 40
- **Primary result**: Introduces temperature-scaled sparsemax/entmax as conformal predictors, achieving competitive coverage-efficiency trade-offs versus softmax-based methods.

## Executive Summary
This paper establishes a novel connection between conformal prediction and sparse softmax-like transformations (sparsemax and γ-entmax). The authors design non-conformity scores that, when calibrated via split conformal prediction, directly correspond to temperature scaling of these sparse activations. At test time, applying the calibrated sparse transformation yields prediction sets with guaranteed coverage while maintaining efficiency through sparsity. Experiments on vision and text classification tasks show competitive performance in coverage, set size, and adaptiveness compared to standard softmax-based conformal methods.

## Method Summary
The method introduces new non-conformity scores for classification that map the calibration process to temperature scaling of sparse activations. For sparsemax, the score sums differences between top-ranked label scores and the candidate label's score. Calibration computes a quantile from these scores, which maps to the inverse temperature β⁻¹. At test time, applying sparsemax with this calibrated temperature produces sparse prediction sets with conformal coverage guarantees. The framework extends to the γ-entmax family using δ-norm scores, where γ controls sparsity. The log-margin score provides a softmax-compatible alternative but loses sparsity. Implementation requires only modifying the non-conformity score computation while reusing standard split conformal prediction infrastructure.

## Key Results
- The proposed sparsemax and γ-entmax methods achieve competitive coverage-efficiency trade-offs versus standard softmax-based non-conformity scores
- Log-margin scores show superior size-stratified coverage for lower confidence levels but produce larger sets overall
- The temperature-scaling interpretation provides a principled way to design non-conformity scores and connects conformal prediction to sparse activation literature
- Experimental results demonstrate effectiveness across computer vision (CIFAR, ImageNet) and text classification (20 Newsgroups) benchmarks

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Temperature scaling on sparsemax/sparse activations produces prediction sets with conformal coverage guarantees.
- **Mechanism**: A specially designed non-conformity score sums score differences between top-ranked labels and candidate label y. When this score is calibrated via split conformal prediction, the resulting quantile directly maps to the inverse temperature β⁻¹. At test time, applying sparsemax with this calibrated temperature produces support sets (labels with nonzero probability) that satisfy the coverage guarantee.
- **Core assumption**: Exchangeability of calibration and test data (required by all split conformal methods).
- **Evidence anchors**:
  - [abstract] "We introduce new non-conformity scores for classification that make the calibration process correspond to the widely used temperature scaling method."
  - [Section 3.1, Proposition 1] "setting the sparsemax temperature as β⁻¹ := q̂ at test time leads to prediction sets C_α(x) = S(βz) achieving the desired (1-α) coverage in expectation."
  - [corpus] Weak direct support; related papers focus on conformal prediction variants but not sparse activation links.
- **Break condition**: If exchangeability is violated (e.g., distribution shift between calibration and test), coverage guarantees no longer hold.

### Mechanism 2
- **Claim**: The γ-entmax family (γ > 1) generalizes the sparsemax-temperature link via δ-norm non-conformity scores.
- **Mechanism**: For γ-entmax, the condition for label inclusion in the support uses a δ-norm (where δ = 1/(γ-1)) of score differences. The non-conformity score s(x,y) = ‖z_{1:k(y)} - z_{k(y)}1‖_δ accumulates differences between top scores and the true label's score. Calibrating this score yields a quantile that maps to temperature β⁻¹ = δ⁻¹q̂.
- **Core assumption**: Exchangeability; Assumption: the optimal γ may be task- and confidence-level dependent.
- **Evidence anchors**:
  - [Section 3.2, Proposition 3] "setting the γ-entmax temperature as β⁻¹ := δ⁻¹q̂ at test time leads to prediction sets C_α(x) = S(βz; γ) achieving the desired (1-α) coverage."
  - [Figure 5] Shows average set size varying with γ for ImageNet, demonstrating γ-tunability.
  - [corpus] No direct corpus support for γ-entmax specific mechanisms.
- **Break condition**: When γ = 1 (softmax), sparsity is lost; the temperature-scaling interpretation breaks, though coverage still holds via log-margin score.

### Mechanism 3
- **Claim**: The log-margin non-conformity score (log-odds ratio) works with softmax but lacks the temperature scaling interpretation.
- **Mechanism**: For γ = 1 (softmax), the δ-norm becomes the ∞-norm, simplifying to s(x,y) = z₁ - z_{k(y)} = log(p₁/p_{k(y)}). Calibration thresholds the odds ratio: labels with probability above a fraction of the top class are included. This achieves coverage but produces dense (non-sparse) sets.
- **Core assumption**: Exchangeability; softmax outputs are well-defined (no numerical underflow).
- **Evidence anchors**:
  - [Section 3.2] "the log-margin method achieving superior size-stratified coverage for lower confidence levels"
  - [Section 3.2, equation 13] "s(x, y) = log(p₁/p_{k(y)}), which is the log-odds ratio between the most probable class and the true one."
  - [corpus] Corpus papers mention conformal prediction but not log-margin specifically.
- **Break condition**: Log-margin produces larger sets for high-confidence regimes (α small) compared to sparse methods, reducing efficiency.

## Foundational Learning

- **Concept: Split Conformal Prediction**
  - **Why needed here**: The entire method builds on split CP's calibration procedure (using a held-out calibration set to compute non-conformity score quantiles).
  - **Quick check question**: Can you explain why exchangeability is required and how the quantile q̂ is computed from calibration scores?

- **Concept: Sparsemax and γ-entmax Transformations**
  - **Why needed here**: Understanding how these activations produce sparse probability outputs and how temperature scaling controls sparsity is central to the paper's contribution.
  - **Quick check question**: For a score vector z, what condition determines if label j is in sparsemax's support? How does γ > 1 introduce sparsity?

- **Concept: Non-conformity Score Design**
  - **Why needed here**: The paper's core insight is designing scores that bridge conformal prediction and temperature scaling; understanding what makes a good score is essential.
  - **Quick check question**: Why does the proposed score s(x,y) = Σ(z_k - z_{k(y)}) for k < k(y) measure "non-conformity"?

## Architecture Onboarding

- **Component map**: Base predictor f(x) -> Non-conformity score module -> Calibration phase -> Temperature setter -> Test-time predictor
- **Critical path**:
  1. Choose γ (start with γ=1.5 or use opt-entmax to tune)
  2. Compute non-conformity scores on calibration data
  3. Compute quantile q̂
  4. At inference, apply temperature-scaled γ-entmax and return nonzero-support labels
- **Design tradeoffs**:
  - **γ selection**: Lower γ (closer to 1) → denser sets, better adaptiveness; Higher γ (closer to 2) → sparser sets, potentially less adaptive. Opt-entmax tuning adds computational overhead but improves efficiency.
  - **Score choice**: Log-margin → best size-stratified coverage at α=0.1 but worse efficiency on ImageNet; InvProb/opt-entmax → best efficiency across tasks.
  - **Calibration split**: 40% calibration / 60% test is standard; opt-entmax and RAPS require further splitting calibration data for hyperparameter tuning, reducing effective calibration size.
- **Failure signatures**:
  - **Coverage below 1-α**: Exchangeability violated (distribution shift); check calibration-test alignment.
  - **Excessively large sets**: γ too low or model poorly calibrated upstream; consider pre-calibration or higher γ.
  - **No singleton predictions**: Model accuracy too low (as in 20 Newsgroups); sparse methods cannot produce singletons when uncertainty is high.
- **First 3 experiments**:
  1. **Reproduce coverage on CIFAR10/CIFAR100**: Implement sparsemax score (Eq. 9), calibrate with α=0.1, verify coverage ≈ 0.90 and compare set sizes to InvProb baseline.
  2. **Ablate γ on ImageNet**: Run γ-entmax with γ ∈ {1.1, 1.3, 1.5, 1.7, 1.9}, plot average set size vs. γ for α=0.05 to replicate Figure 5 patterns.
  3. **Size-stratified coverage analysis**: For log-margin vs. 1.5-entmax on ImageNet with α=0.1, compute coverage within each set-size bin to verify log-margin's superior adaptiveness (Table 2).

## Open Questions the Paper Calls Out
None

## Limitations
- The method cannot produce singleton prediction sets when base model accuracy is below 1-α, limiting practical efficiency for low-accuracy models
- Optimal γ selection appears task- and confidence-level dependent, requiring computationally expensive opt-entmax tuning or heuristic selection
- The temperature-scaling interpretation breaks for γ=1 (softmax), requiring fallback to log-margin score with reduced sparsity

## Confidence
- **High Confidence**: Coverage guarantees under exchangeability (Mechanism 1); Calibration via quantile mapping to temperature (Proposition 1)
- **Medium Confidence**: Generalization to γ-entmax family and δ-norm scores (Mechanism 2); Experimental comparisons showing competitive efficiency (Section 4)
- **Low Confidence**: Optimal γ selection without opt-entmax; Performance under distribution shift; Practical benefits over simpler methods like InvProb in non-ideal conditions

## Next Checks
1. **Distribution Shift Robustness**: Evaluate the proposed method on datasets with known covariate or label shift (e.g., CIFAR-10-C, ImageNetV2) to measure coverage degradation compared to standard conformal scores.

2. **Scalability to Large K**: Test on datasets with thousands of classes (e.g., ImageNet-21K subset) to verify whether the γ-entmax advantage in prediction set efficiency persists at scale, and measure computational overhead of opt-entmax.

3. **Pre-calibration Ablation**: Investigate whether applying post-hoc calibration (e.g., temperature scaling or Platt scaling) to the base model before conformal prediction improves or degrades the proposed sparsemax method's efficiency, particularly for poorly calibrated models.