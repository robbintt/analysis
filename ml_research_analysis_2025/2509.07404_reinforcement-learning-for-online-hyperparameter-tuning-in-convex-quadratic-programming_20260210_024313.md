---
ver: rpa2
title: Reinforcement learning for online hyperparameter tuning in convex quadratic
  programming
arxiv_id: '2509.07404'
source_url: https://arxiv.org/abs/2509.07404
tags:
- policy
- solver
- training
- problems
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work combines a regularized interior point solver for convex
  quadratic programming with reinforcement learning to automate hyperparameter tuning.
  By formulating the parameter selection as a Markov decision process, a neural network
  policy learns to adapt regularization parameters during the solution process, improving
  both convergence speed and solver robustness.
---

# Reinforcement learning for online hyperparameter tuning in convex quadratic programming

## Quick Facts
- **arXiv ID:** 2509.07404
- **Source URL:** https://arxiv.org/abs/2509.07404
- **Reference count:** 40
- **Primary result:** RL agent learns to adapt regularization parameters during QP solution, achieving 2-4x performance gains on large problems and 2-7% robustness improvements on benchmark set.

## Executive Summary
This work presents a reinforcement learning approach to automatically tune regularization parameters in interior point methods for convex quadratic programming. By formulating parameter selection as a Markov decision process, a neural network policy learns to dynamically adjust regularization parameters during the solution process. The approach achieves significant performance improvements over fixed-parameter strategies, particularly on larger problems, while maintaining solver robustness across different problem classes and scales.

## Method Summary
The method combines a regularized interior point solver with reinforcement learning to automate hyperparameter tuning. The QP solver solves linear systems via LU decomposition with pivoting, using proximal regularization parameters (δ_x, δ_y, δ_z) to handle ill-conditioning. An RL agent observes normalized primal/dual residuals and selects decrease factors (α) to adjust these regularization parameters at each outer iteration. The agent is trained using PPO on randomly generated QPs with high condition numbers and rank-deficient Hessians, using a composite reward function that balances residual reduction, primal-dual equilibrium, and iteration count.

## Key Results
- Achieves 2-4x performance gains on larger QP problems
- Improves robustness by 2-7% on Maros-Mészáros benchmark set
- Policy generalizes well to different problem sizes, scales, and classes after lightweight training
- Outperforms fixed-parameter strategies across various problem dimensions

## Why This Works (Mechanism)

### Mechanism 1: Proximal Regularization as Stabilization
Dynamic adjustment of regularization parameters δ improves the condition number of the KKT matrix in Newton steps. The RL agent observes residuals and adjusts δ via decrease factors α to prevent linear solver failures while maintaining fast convergence.

### Mechanism 2: Invariance via Normalized Residual State-Space
Training on small, random QPs generalizes to large-scale problems because the state representation uses normalized residuals, making the policy invariant to problem dimensions and scaling factors.

### Mechanism 3: Multi-Objective Reward Shaping
A composite reward function balancing residual reduction, primal-dual equilibrium, and iteration count guides the policy toward robust efficiency better than simple solve-time optimization.

## Foundational Learning

- **Concept: Interior Point Methods (IPM) & Proximal Point Algorithm (PPA)**
  - **Why needed here:** The solver is a specific two-loop method requiring understanding of why regularization handles semi-definiteness and the distinction between outer PPA updates and inner Newton steps.
  - **Quick check question:** Does increasing the regularization parameter δ typically make the linear system matrix more or less diagonally dominant?

- **Concept: Markov Decision Processes (MDP) in Continuous Control**
  - **Why needed here:** The tuning is framed as an MDP where state transitions involve solver updates based on RL actions.
  - **Quick check question:** Why is the "state" defined as residuals rather than the raw solution vector x?

- **Concept: Proximal Policy Optimization (PPO)**
  - **Why needed here:** The paper uses PPO, requiring understanding of how the Critic estimates state values to guide Actor policy updates via gradient ascent.
  - **Quick check question:** What happens to the policy gradient variance if the value function approximation (the Critic) is inaccurate?

## Architecture Onboarding

- **Component map:** Random QP Generator -> Custom QP Solver (Algorithm 2.1) -> State Extractor (normalized residuals) -> RL Policy Network (2 hidden layers, 25 units each) -> Action (decrease factors α) -> Solver Update -> Reward Calculation

- **Critical path:**
  1. **Data Generation:** Random QP generation with controlled rank deficiency and condition numbers
  2. **Reward Calculation:** Implementing composite reward function correctly
  3. **Policy Inference:** Mapping network output to valid α ∈ [0.05, 0.95]

- **Design tradeoffs:**
  - Training takes 13 hours on laptop vs. RLQP taking days, but neural network inference time can dominate for very small QPs
  - Adaptive policy handles initial guess perturbations better than fixed strategies

- **Failure signatures:**
  - **Stagnation:** Policy outputs α ≈ 1, keeping δ large and preventing fast convergence
  - **Collapse:** Policy outputs α ≈ 0, driving δ to minimum too fast and potentially destabilizing Newton steps
  - **Overfitting:** Policy solves random QPs quickly but fails on structured benchmark instances

- **First 3 experiments:**
  1. **Baseline Validation:** Compare fixed-α vs. RL policy on validation set with inference time included to find crossover point
  2. **Ablation on State Space:** Remove ν and ε from state vector to test Markov property validity
  3. **Generalization Stress Test:** Train on n ∈ [20, 30], test on n=500 with κ > 10^15, plot performance profiles against standard solvers

## Open Questions the Paper Calls Out

### Open Question 1
Can enhanced state space or extended training phases reduce the high entropy and uncertainty observed in the final learned policy? The authors note the final policy remains "relatively uncertain" and leave this for future work.

### Open Question 2
Does including additional internal parameters like barrier parameter (ν) or regularization parameter (δ) in the state or action space improve convergence acceleration? The authors plan to investigate this influence.

### Open Question 3
How well does the RL-tuned solver generalize to structured, large-scale problems in specific domains like power systems or finance? The authors list this as a future investigation direction.

### Open Question 4
Can the policy effectively continue learning and adapting online during the solver's deployment? The authors suggest this for "even more adaptive RL approaches."

## Limitations
- Generalization relies on unproven assumption that normalized residuals follow similar trajectories across problem scales
- Performance improvements demonstrated primarily on smaller benchmark instances (n < 10,000)
- Training requires careful hyperparameter tuning not fully specified in paper

## Confidence
- **High:** Core mechanism of using RL to dynamically adjust regularization parameters is well-supported by numerical experiments and QP theory
- **Medium:** Generalization claims are supported empirically but lack theoretical guarantees
- **Medium:** Performance improvements are demonstrated on benchmarks but may not scale to all QP problem types

## Next Checks
1. **Generalization Stress Test:** Train on n ∈ [20, 30], test on n=500 with κ > 10^15, plot performance profiles against standard solvers
2. **State Space Ablation:** Remove ν and ε from state vector to test if policy can still solve problems
3. **Cost-Benefit Analysis:** Compare fixed-α vs. RL policy on validation set with inference time included to find crossover point where RL becomes beneficial