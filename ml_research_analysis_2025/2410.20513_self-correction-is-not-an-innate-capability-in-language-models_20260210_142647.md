---
ver: rpa2
title: Self-correction is Not An Innate Capability in Language Models
arxiv_id: '2410.20513'
source_url: https://arxiv.org/abs/2410.20513
tags:
- uni00000013
- feedback
- self-correction
- uni00000052
- uni00000048
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether moral self-correction is an innate
  capability of large language models (LLMs). The authors conduct both behavioral
  and mechanistic analyses to examine LLMs' moral sensitivity and their ability to
  effectively incorporate external feedback during self-correction.
---

# Self-correction is Not An Innate Capability in Language Models

## Quick Facts
- arXiv ID: 2410.20513
- Source URL: https://arxiv.org/abs/2410.20513
- Reference count: 40
- One-line primary result: Self-correction improves LLMs' task performance without improving moral sensitivity, as models prioritize Chain-of-Thought over external feedback

## Executive Summary
This paper investigates whether moral self-correction is an innate capability of large language models through behavioral and mechanistic analyses. The authors find that while self-correction can improve task performance on benchmarks like BBQ (bias detection) and RealToxicity (toxic content detection), it does not enhance the models' ability to recognize stereotyped social groups or distinguish between toxic and non-toxic responses. Mechanistic analysis reveals that LLMs rely on superficial heuristics rather than true moral understanding, and tend to prioritize Chain-of-Thought reasoning over more informative external feedback during the self-correction process.

## Method Summary
The paper conducts experiments on BBQ (ambiguous social bias contexts) and RealToxicity (toxic content detection) benchmarks using three generator models (Mistral-7B, Gemma-7B, DeepSeek-R1-Distill-Llama-8B) and DeepSeek-chat API as external evaluator. Six self-correction methods are tested: intrinsic only, intrinsic with CoT, external feedback only, external feedback with CoT, and combinations thereof. Behavioral analysis measures task performance and self-distinguishing accuracy (ability to identify stereotyped groups or less-toxic responses). Mechanistic analysis probes hidden states for warrant activation (cosine similarity to moral reasoning probes) and computes instruction-following difficulty scores by measuring how input component removal affects output likelihood.

## Key Results
- Self-correction improves BBQ accuracy from 0.789→0.994 for Gender Identity but does not improve self-distinguishing ability
- RealToxicity detoxification ratio improves from 0.0675→0.0918 with external feedback, but ext-CoT performs worse (0.0290 vs 0.0220)
- Warrant activation from external feedback diminishes when incorporated into self-correction process
- LLMs prioritize CoT over external feedback despite feedback activating more warrants

## Why This Works (Mechanism)

### Mechanism 1: Warrant Activation in Hidden States
External feedback and CoT individually improve self-correction by activating moral reasoning "warrants" in hidden states, but this effect diminishes when combined. The paper probes hidden states (layer 15+) for cosine similarity to label warrants (correct answers) and evid warrants (explanations). External feedback alone activates more warrants, but when incorporated into self-correction, its impact weakens—removing feedback from input sometimes increases label warrant activation.

### Mechanism 2: CoT-Feedback Conflict via Instruction-Following Difficulty
LLMs prioritize CoT over external feedback during response generation, even when feedback is more informative. IFD scores measure how removing input components affects output probability. For ext-CoT on RealToxicity, CoT IFD is consistently low (~1), while feedback IFD fluctuates around 1, indicating LLMs struggle to follow feedback when CoT is present. This explains why ext-CoT underperforms ext alone.

### Mechanism 3: Behavioral Moral Insensitivity
Self-correction improves task performance without improving moral sensitivity—LLMs cannot reliably distinguish their own moral vs. immoral outputs. The self-distinguishing task shows that despite successful self-correction, accuracy at identifying stereotyped groups or less-toxic completions often falls below baseline, suggesting LLMs use surface patterns rather than moral understanding.

## Foundational Learning

- **Intrinsic vs. Extrinsic Self-Correction**: Why needed here: The paper distinguishes intrinsic (prompt-based, internal knowledge) from extrinsic (external feedback) correction. Understanding this is essential to interpret why combining them causes conflicts.
  - Quick check question: If you prompt a model with "be unbiased" and also give it feedback "your answer is stereotyped," which signal should dominate?

- **Warrant Probing (Semantic Probing)**: Why needed here: The paper's mechanistic analysis relies on probing hidden states for warrant similarity. This technique measures how input components activate target concepts in internal representations.
  - Quick check question: If you compute cosine similarity between a hidden state and a "toxicity" probe vector, what does high similarity tell you? What might it miss?

- **Instruction-Following Difficulty (IFD)**: Why needed here: IFD quantifies how much each input component influences output. This reveals priority conflicts between CoT and feedback without relying on interpretability assumptions.
  - Quick check question: If IFD(feedback) > IFD(CoT), which component is the model relying on more? What if IFD > 1?

## Architecture Onboarding

- **Component map**: Input Context -> Prompt (question/text to complete) -> CoT Reasoning (if enabled) -> Self-correction Instruction (intrinsic: "be unbiased") -> External Feedback (extrinsic: from evaluator LLM) -> Processing Pipeline -> Hidden States (probed for warrant similarity, layers 15+) -> IFD Computation (via ablation: remove component, measure NLL change) -> Response Generation (influenced by CoT prioritization) -> External Evaluator -> DeepSeek-chat API (generates feedback without answering)

- **Critical path**: 1. Baseline → 2. Add CoT OR Feedback (choose one for best results) → 3. Multiple rounds (2-3 optimal, gains diminish) → 4. Final response

- **Design tradeoffs**:
  | Choice | Pros | Cons |
  |--------|------|------|
  | ext only | Best for 8/12 tasks; avoids CoT conflict | Requires external evaluator |
  | ext-CoT | Strong on some bias types (Age: 0.995) | Worse on RealToxicity (0.029 vs 0.022) |
  | int only | No external dependency | Lower performance overall |
  | int-ext-CoT | Highest on some tasks | Unstable; conflicts cause variance |

- **Failure signatures**:
  - ext-CoT underperforms ext alone → CoT-feedback conflict active
  - Self-distinguishing accuracy < baseline → Moral insensitivity; surface-level correction
  - IFD(feedback) > 1 → Model struggling to incorporate feedback
  - Warrant activation decreases across rounds → Diminishing returns; Gemma-7B shows this degradation

- **First 3 experiments**:
  1. Reproduce ablation on ext-CoT: Remove feedback from input context, re-generate CoT, measure warrant similarity change. Expect: label warrant activation increases without feedback (conflict signal).
  2. Self-distinguishing baseline test: Run BBQ task with no self-correction instructions; measure accuracy at identifying stereotyped groups. Compare to int/ext settings to confirm dissociation.
  3. IFD profiling across models: Compute IFD(CoT) and IFD(feedback) for Mistral-7B, Gemma-7B, DeepSeek-8B. Check if CoT dominance is consistent or model-dependent.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can LLMs be trained to effectively leverage external feedback during moral self-correction, given their current tendency to prioritize internal CoT over more informative feedback?
- Basis in paper: [explicit] Future Works section states: "Existing methods only explore intrinsic self-correction but how to effectively leverage external feedback would be more interesting."
- Why unresolved: The mechanistic analysis reveals LLMs fail to utilize external feedback despite it activating more warrants; the capability gap between evaluator and generator models is identified as a key bottleneck.
- What evidence would resolve it: Demonstrating improved self-correction performance when external feedback is incorporated, with increased warrant activation in hidden states compared to CoT-only approaches.

### Open Question 2
- Question: What specific textual patterns in pre-training corpora constitute the shallow heuristics that enable superficial moral self-correction?
- Basis in paper: [explicit] Future Works section asks: "What are the sources of shallow heuristics in pre-training corpora that enable self-correction? Digging up the textual patterns that facilitate self-correction can serve as valuable signals."
- Why unresolved: The paper demonstrates self-correction relies on superficial token associations rather than true moral understanding, but does not identify the specific corpus patterns responsible.
- What evidence would resolve it: Identification of corpus patterns that correlate with self-correction capability, followed by controlled experiments showing manipulation of these patterns affects self-correction behavior.

### Open Question 3
- Question: How can moral reasoning be formally incorporated into the moral self-correction process?
- Basis in paper: [explicit] Future Works section states: "How can we incorporate moral reasoning into moral self-correction? External feedback functions as a diagnostic signal... moral self-correction can be considered as the application of moral reasoning."
- Why unresolved: The paper shows current self-correction lacks moral sensitivity and relies on heuristics, but does not propose mechanisms for integrating genuine moral reasoning.
- What evidence would resolve it: A framework where moral self-correction demonstrates principled reasoning (e.g., correctly identifying violated moral principles) rather than superficial pattern matching.

## Limitations

- The warrant probing methodology assumes cosine similarity in hidden states directly reflects moral reasoning quality, but correlation does not necessarily imply causation
- The self-distinguishing task may conflate introspection ability with moral sensitivity - models might simply fail at the meta-cognitive task of identifying their own outputs
- IFD score interpretation relies on the assumption that negative log-likelihood differences cleanly isolate instruction-following priority, but these metrics may conflate multiple factors

## Confidence

- **High Confidence**: The behavioral finding that self-correction improves task performance without improving moral sensitivity (self-distinguishing accuracy often falls below baseline). This dissociation is clearly demonstrated across multiple benchmarks.
- **Medium Confidence**: The mechanistic finding that external feedback and CoT individually improve warrant activation but conflict when combined. While the statistical patterns are clear, the causal interpretation depends on warrant probing validity.
- **Low Confidence**: The claim that LLMs rely on "superficial heuristics rather than true moral understanding." This interpretation is plausible given the evidence but requires stronger validation of the probing methodology.

## Next Checks

1. **Warrant Probing Validation**: Run controlled experiments where warrant similarity is measured on inputs with known moral reasoning content (e.g., explicit moral statements) versus neutral text to establish baseline activation patterns and validate the probing approach.
2. **IFD Ablation Analysis**: Systematically vary CoT and feedback length, position, and semantic density while keeping content constant to determine whether IFD scores reflect instruction-following priority or are confounded by other factors.
3. **Cross-Model Generalizability**: Replicate the self-distinguishing task and warrant activation analysis on a broader range of models (including non-7B architectures) to test whether the observed patterns are consistent or model-specific.