---
ver: rpa2
title: Topology-Aware Knowledge Propagation in Decentralized Learning
arxiv_id: '2505.11760'
source_url: https://arxiv.org/abs/2505.11760
tags:
- data
- learning
- topology
- node
- degree
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies the problem of propagating out-of-distribution
  (OOD) knowledge in decentralized learning across arbitrary communication topologies.
  The authors find that popular decentralized learning algorithms struggle to propagate
  OOD knowledge effectively to all devices, and that both the location of OOD data
  within a topology and the topology itself significantly impact knowledge propagation.
---

# Topology-Aware Knowledge Propagation in Decentralized Learning

## Quick Facts
- **arXiv ID**: 2505.11760
- **Source URL**: https://arxiv.org/abs/2505.11760
- **Reference count**: 40
- **Primary result**: Topology-aware aggregation strategies improve OOD data accuracy by 123% on average compared to topology-unaware baselines in decentralized learning.

## Executive Summary
This paper addresses the challenge of propagating out-of-distribution (OOD) knowledge in decentralized learning across arbitrary communication topologies. The authors demonstrate that standard decentralized learning algorithms struggle to effectively disseminate OOD knowledge to all devices, with both the location of OOD data within the topology and the topology itself significantly impacting propagation success. They propose topology-aware aggregation strategies that leverage node centrality metrics (Degree and Betweenness) to improve OOD data accuracy by 123% on average while also enhancing IID knowledge propagation. The work systematically characterizes how these strategies behave across diverse topologies with varying degree distributions, node counts, and modularity.

## Method Summary
The method introduces topology-aware aggregation strategies for decentralized learning that account for each device's structural position within the communication topology. Instead of uniform averaging with neighbors, devices weight their neighbors' models using centrality metrics calculated via a softmax function. Degree centrality captures local connectivity (hub nodes), while Betweenness centrality identifies nodes that frequently lie on shortest paths between other nodes (bridge nodes). The aggregation coefficient for neighbor j is computed as C_{i,j} = exp(R_j/τ) / Σ_k exp(R_k/τ), where R is the centrality score and τ=0.1 is temperature. This approach prioritizes structurally advantageous neighbors for knowledge propagation, particularly benefiting the dissemination of sparse OOD signals across the network.

## Key Results
- Topology-aware aggregation improves OOD data accuracy by 123% on average across models in a topology compared to topology-unaware baselines
- OOD knowledge is significantly harder to propagate than IID knowledge in standard decentralized learning
- Both Degree and Betweenness centrality metrics provide similar performance improvements, with Degree being more computationally efficient
- The effectiveness of topology-aware strategies depends on graph heterogeneity, showing minimal benefit in regular topologies like Watts-Strogatz networks

## Why This Works (Mechanism)

### Mechanism 1: Centrality-Weighted Aggregation
- Claim: Weighting neighbor models by their topological centrality accelerates OOD knowledge propagation compared to uniform averaging
- Mechanism: High-degree nodes act as hubs and high-betweenness nodes act as bridges; by assigning higher aggregation coefficients to these central neighbors, the system prioritizes models best positioned to disseminate knowledge across the network
- Core assumption: Nodes with higher centrality metrics have either observed more diverse data or are critical relay points for propagating signals to distant clusters
- Evidence anchors: [abstract] topology-aware aggregation strategies that account for each device's location within the topology; [section 4] devices with many neighbors are well positioned to act as information hubs; [corpus] supports general efficacy of topology-aware methods

### Mechanism 2: Amplification of Sparse OOD Signals
- Claim: Topology-aware weighting specifically aids OOD data (sparse/localized) more than IID data (ubiquitous)
- Mechanism: Standard averaging dilutes the single OOD signal as it traverses the network; by boosting weights of bridge nodes, OOD-related information decays slower as it moves away from the source
- Core assumption: The average model in a neighborhood is biased toward majority IID data, requiring structural bias to counteract dilution of minority OOD signals
- Evidence anchors: [section 3] OOD knowledge is more difficult than IID knowledge to propagate; [figure 1] shows OOD accuracy is geographically clustered around source in baseline models but diffuse in topology-aware models

### Mechanism 3: Dependency on Graph Heterogeneity
- Claim: The method's success is conditional on variance of node connectivity within the topology
- Mechanism: The aggregation strategy relies on differentiating nodes based on centrality metrics; in scale-free BA graphs, node degrees vary significantly allowing softmax to assign distinct weights, while in small-world WS graphs degrees are more uniform causing weights to approximate uniform averaging
- Core assumption: The underlying network topology is non-regular and contains structurally distinct nodes
- Evidence anchors: [section 5.3] WS topologies have more uniform degree distribution and therefore topology-aware metrics do not differ significantly; [figure 20] visualizes power-law distribution of BA vs normal distribution of WS

## Foundational Learning

- **Concept: Decentralized Learning (Gossip Learning)**
  - Why needed here: Unlike Federated Learning, there is no central server; knowledge propagates only via peer-to-peer hops; convergence depends on graph's spectral properties
  - Quick check question: If you remove the highest-degree node from a decentralized network, does it primarily affect convergence speed or final accuracy?

- **Concept: Centrality Metrics (Degree vs. Betweenness)**
  - Why needed here: These are the control signals for the proposed aggregation strategy; Degree captures local connectivity (hubs), Betweenness captures global flow (bridges)
  - Quick check question: In a "dumbbell" graph (two clusters connected by one link), which metric best identifies the node that prevents the network from fragmenting?

- **Concept: Out-of-Distribution (OOD) vs. IID Generalization**
  - Why needed here: The core problem is that standard decentralized learning optimizes for local/average distribution, failing to memorize or adapt to rare, distant data types
  - Quick check question: If data is IID, does a node need to communicate with distant neighbors to achieve high accuracy, or is local training sufficient?

## Architecture Onboarding

- **Component map:**
  - Topology Graph (G) -> Local Trainer -> Metric Computer -> Aggregator

- **Critical path:**
  1. Initialization: Generate topology (e.g., BA model) and compute centrality metrics
  2. Local Update: Nodes train on local batch
  3. Communication: Nodes send model parameters to immediate neighbors
  4. Topology-Aware Aggregation: Collect neighbor parameters, retrieve neighbor scores, compute coefficients via softmax, update local model with weighted sum

- **Design tradeoffs:**
  - Degree vs. Betweenness: Degree is O(1) to track and strictly local; Betweenness is O(N^2) or O(NM) to compute and requires global topology knowledge; the paper shows similar performance, suggesting Degree is more practical
  - Temperature (τ): High temperature flattens weights (behaves like Unweighted); low temperature amplifies differences, potentially causing instability if one node dominates

- **Failure signatures:**
  - Uniform Weights: Check C_{i,j} values; if nearly identical (e.g., 0.33, 0.33, 0.33 in 3-node neighborhood), topology is too regular (like WS) and method offers no gain
  - Modularity Stagnation: In highly modular graphs (distinct clusters), OOD accuracy fails to rise in clusters distant from OOD source regardless of weighting

- **First 3 experiments:**
  1. Sanity Check (Ring vs. Star): Run Unweighted vs. Degree on Ring (regular) and Star (hub-based) topology; verify Degree fails on Ring but succeeds on Star
  2. OOD Location Sensitivity: Place OOD data on highest-degree node vs. lowest-degree node; quantify accuracy gap between Unweighted and Degree strategies
  3. Metric Ablation: Compare Degree vs. Betweenness on BA-graph with N=33; check if computational cost of Betweenness justifies performance delta

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do topology-aware aggregation strategies perform in online learning settings with streaming data?
- Basis in paper: [explicit] The Conclusion states, "Future work may... extend topology-aware learning to online learning settings (e.g., data streaming applications)"
- Why unresolved: Current evaluation is limited to static datasets over fixed training rounds; streaming environments introduce dynamic data distributions and temporal dependencies not covered by the proposed method
- What evidence would resolve it: Experimental results applying Degree and Betweenness strategies to continuous data streams or online learning benchmarks

### Open Question 2
- Question: Can centrality metrics other than degree and betweenness improve knowledge propagation?
- Basis in paper: [explicit] The Conclusion notes, "Future work may extend topology-aware aggregation strategies to consider additional centrality metrics"
- Why unresolved: Study is restricted to degree (local) and betweenness (global); other metrics like closeness or eigenvector centrality might capture different structural advantages or propagation dynamics
- What evidence would resolve it: Comparative analysis of OOD accuracy using alternative centrality metrics (e.g., eigenvector, closeness) across tested topologies

### Open Question 3
- Question: How does the strategy behave under severe global non-IID data distributions?
- Basis in paper: [explicit] The Conclusion proposes to "further characterize the behavior of topology-aware metrics under different types of data distribution"
- Why unresolved: Authors primarily test a "mostly IID" setting with single OOD node; unclear if strategies remain effective when data is highly heterogeneous across all devices
- What evidence would resolve it: Experiments utilizing high heterogeneity distributions (e.g., lower Dirichlet α) where multiple nodes possess distinct OOD data

### Open Question 4
- Question: Does accelerating propagation increase vulnerability to model poisoning or Byzantine attacks?
- Basis in paper: [inferred] Limitations section warns that strategies "can propagate all types of data, including both benign and malicious data"
- Why unresolved: While method accelerates knowledge dissemination, unclear if this also accelerates spread of malicious model updates from bad actors, potentially exacerbating security risks
- What evidence would resolve it: Robustness experiments measuring speed and extent of poison propagation under proposed topology-aware strategies compared to baselines

## Limitations

- The analysis assumes a "single-node OOD" extreme case, which may not generalize to multi-node or continuous OOD distributions
- Method's performance heavily depends on heterogeneity of underlying graph; in regular topologies (e.g., Watts-Strogatz), proposed weighting collapses to uniform averaging, offering no benefit
- Paper does not address adversarial or dynamic topology scenarios where nodes may join/leave or centrality metrics may change during training

## Confidence

- **High Confidence**: The mechanism of centrality-weighted aggregation accelerating OOD knowledge propagation in heterogeneous topologies is well-supported by empirical results and aligns with established graph theory
- **Medium Confidence**: The amplification of sparse OOD signals is a valid mechanism, but its effectiveness may vary with degree of OOD data sparsity and distribution
- **Medium Confidence**: The dependency on graph heterogeneity is a reasonable assumption, but threshold for "sufficient heterogeneity" to observe benefits is not precisely quantified

## Next Checks

1. **Regular Topology Test**: Implement proposed method on Watts-Strogatz topology and verify it does not outperform Unweighted baseline, confirming mechanism's dependency on graph heterogeneity
2. **OOD Distribution Sensitivity**: Extend experiment to scenarios where OOD data is distributed across multiple nodes (not just one) and measure if topology-aware strategy still provides significant advantage over standard averaging
3. **Dynamic Topology Robustness**: Simulate dynamic scenario where highest-degree node (key "hub") is removed mid-training and observe impact on OOD knowledge propagation and overall model convergence