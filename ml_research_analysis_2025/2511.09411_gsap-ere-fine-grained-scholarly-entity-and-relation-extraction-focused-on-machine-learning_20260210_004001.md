---
ver: rpa2
title: 'GSAP-ERE: Fine-Grained Scholarly Entity and Relation Extraction Focused on
  Machine Learning'
arxiv_id: '2511.09411'
source_url: https://arxiv.org/abs/2511.09411
tags:
- relation
- entity
- dataset
- research
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: GSAP-ERE is a manually annotated dataset for fine-grained entity
  and relation extraction in ML research, featuring 10 entity types and 18 relation
  types across 100 full-text publications, totaling 63K entities and 35K relations.
  It supports tasks like knowledge graph construction and reproducibility monitoring.
---

# GSAP-ERE: Fine-Grained Scholarly Entity and Relation Extraction Focused on Machine Learning

## Quick Facts
- **arXiv ID**: 2511.09411
- **Source URL**: https://arxiv.org/abs/2511.09411
- **Reference count**: 10
- **Primary result**: Supervised fine-tuning on domain-specific data (80.6% NER, 54.0% RE F1) outperforms LLM prompting (44.4% NER, 10.1% RE F1)

## Executive Summary
GSAP-ERE is a manually annotated dataset for fine-grained entity and relation extraction in machine learning research, featuring 10 entity types and 18 semantically categorized relation types across 100 full-text publications. The dataset contains 63K entities and 35K relations, supporting tasks like knowledge graph construction and reproducibility monitoring. The authors present fine-tuned baseline models, including PL-Marker (pipeline) and HGERE (joint), achieving 80.6% NER and 54.0% RE F1 scores, significantly outperforming LLM prompting methods. The results demonstrate the value of supervised models and curated datasets for domain-specific information extraction tasks.

## Method Summary
The GSAP-ERE dataset was created through manual annotation of 100 full-text ML publications using the INCEpTION platform, with entity spans and relation pairs extracted from GROBID-processed PDF text. The authors established baseline models using supervised fine-tuning approaches (PL-Marker pipeline and HGERE joint extraction) with SciBERT encoders, comparing them against LLM prompting baselines (Llama 3.1 70B, Qwen 2.5 32B/72B). Evaluation employed four RE settings (RE+, RE, RE+≈, RE≈) to assess performance under varying label/span strictness conditions.

## Key Results
- Supervised fine-tuned models achieve 80.6% NER and 54.0% RE F1, outperforming LLM prompting (44.4% NER, 10.1% RE F1)
- Joint extraction model (HGERE) surpasses pipeline approach (PL-Marker) by 8.0% NER and 12.6% RE F1
- The dataset features 62,619 entities and 35,302 relations across 10 entity types and 18 relation types in 7 semantic groups

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Supervised fine-tuning on domain-specific annotated data outperforms unsupervised LLM prompting for fine-grained scholarly IE tasks.
- Mechanism: The fine-tuned HGERE model achieves 80.6% NER and 54.0% RE F1, compared to LLM prompting (44.4% NER, 10.1% RE). This gap likely emerges because supervised models learn domain-specific entity boundaries and relation patterns from explicit span-level annotations, while LLMs must infer these from general knowledge without task-specific gradient updates.
- Core assumption: The performance gap reflects architectural/training differences rather than prompt engineering limitations alone.
- Evidence anchors:
  - [abstract] "NER: 80.6%, RE: 54.0% for the fine-tuned model vs. NER: 44.4%, RE: 10.1% for the LLM"
  - [section] Table 4 shows supervised approaches outperform unsupervised LLM prompting by 18.6–39.8% for NER and 28.1–50% for RE across all settings
  - [corpus] Weak direct evidence—neighbor papers focus on extraction methods but don't replicate this specific comparison
- Break condition: If LLMs with domain-adaptive pretraining or specialized prompting (e.g., chain-of-thought with schema grounding) close the gap, the mechanism would suggest prompt/capability limitations rather than fundamental architectural constraints.

### Mechanism 2
- Claim: Fine-grained entity and relation taxonomies with 18 semantically categorized relation types enable capturing complex scholarly dependencies beyond coarse-grained schemas.
- Mechanism: The 18 relation types organized into 7 semantic groups (Model Design, Task Binding, Data Usage, Data Provenance, Data Properties, Peer Relations, Referencing) capture nuanced interactions such as `trainedOn`, `evaluatedOn`, `transformedFrom`, `architecture`, `isBasedOn`. This granularity supports downstream tasks like reproducibility monitoring by distinguishing, for example, training vs. evaluation data usage.
- Core assumption: The semantic categorization aligns with downstream task requirements; coarse-grained alternatives would lose actionable signal.
- Evidence anchors:
  - [abstract] "10 entity types and 18 semantically categorized relation types"
  - [section] Table 2 defines semantic groups with examples; interrater agreement (Table 3) shows weighted F1 of 53.7–62.8% across relation groups, indicating annotatable but challenging distinctions
  - [corpus] Neighbor papers (SciNLP, SciER) use fewer relation types (7–9), supporting the claim that GSAP-ERE offers broader coverage
- Break condition: If downstream tasks show no performance improvement from fine-grained vs. coarse-grained relations, the added complexity may not justify annotation cost.

### Mechanism 3
- Claim: Joint entity-relation extraction (HGERE) outperforms pipeline approaches (PL-Marker) by enabling shared representation learning between NER and RE.
- Mechanism: HGERE incorporates a hypergraph neural network to facilitate high-order span classification for NER and entity pair classification for RE in one step, achieving 80.6% NER and 54.0% RE vs. PL-Marker's 72.6% NER and 41.4% RE. The joint loss may allow entity recognition to benefit from relation context and vice versa.
- Core assumption: The performance gain stems from joint modeling rather than hyperparameter optimization differences alone.
- Evidence anchors:
  - [section] Table 4 shows HGERE outperforms PL-Marker under all settings (NER: +8.0%, RE: +12.6%)
  - [section] Section on Experiments describes HGERE's hypergraph neural network with "ternary information flow configuration"
  - [corpus] Neighbor papers (e.g., "The Joint Entity-Relation Extraction Model Based on Span and Interactive Fusion Representation") support joint extraction benefits for complex semantics
- Break condition: If pipeline approaches with shared encoders (e.g., PURE with batched relation modeling) match joint performance, the mechanism may reduce to encoder sharing rather than joint loss.

## Foundational Learning

- Concept: Named Entity Recognition (NER) and Relation Extraction (RE) task formulation
  - Why needed here: GSAP-ERE defines NER as span-level classification with 10 entity types and RE as pair-wise relation classification with 18 types plus NIL (no relation). Understanding span-based formulations is essential for implementing baseline models.
  - Quick check question: Can you explain the difference between token-level and span-level NER, and why relation extraction requires pair-wise classification?

- Concept: Transformer-based encoders for scientific text (SciBERT)
  - Why needed here: Both PL-Marker and HGERE use `scibert-scivocab-uncased` as the encoder. Familiarity with domain-specific pretraining helps understand why general-purpose models may underperform.
  - Quick check question: What advantages does SciBERT offer over BERT for scientific text, and how does vocabulary selection affect span representation?

- Concept: Evaluation metrics for ERE (exact vs. partial match, micro vs. macro F1)
  - Why needed here: GSAP-ERE introduces four RE evaluation settings (RE+, RE, RE+≈, RE≈) to handle entity span and label alignment challenges. Misinterpreting these metrics can lead to incorrect performance comparisons.
  - Quick check question: Why does RE+ require both correct entity labels and exact span matching, while RE≈ only requires the correct relation label with overlapping spans?

## Architecture Onboarding

- Component map:
  Data layer: 100 full-text publications → GROBID PDF-to-text → INCEpTION annotation platform → 62,619 entities / 35,302 relations
  Model layer: SciBERT encoder → PL-Marker (pipeline: NER → RE) or HGERE (joint: hypergraph neural network)
  Evaluation layer: 80/10/10 train/val/test split → micro F1 across 4 RE settings

- Critical path:
  1. Load GSAP-ERE dataset from provided URLs (entity spans, relation pairs, publication text)
  2. Fine-tune SciBERT encoder with PL-Marker or HGERE architecture
  3. Optimize hyperparameters (learning rate, batch size, loss weighting for HGERE) on validation set
  4. Evaluate on test set using exact/partial match variants

- Design tradeoffs:
  - Joint vs. pipeline: Joint (HGERE) offers higher performance but increased complexity; pipeline (PL-Marker) is modular and easier to debug
  - Entity granularity: Including generic entities (MLModelGeneric, DatasetGeneric) increases entity density (75% of entities in relations) but adds annotation ambiguity
  - Sentence vs. document-level: Current dataset is sentence-level; document-level relations require cross-sentence coreference

- Failure signatures:
  - Low RE performance with high NER: Entity spans detected but relation classification fails—check NIL class balance and relation label distribution
  - High variance across seeds: Hyperparameter sensitivity—run multiple seeds (paper uses 5) and report mean ± std
  - LLM prompting collapse: RE performance near random (10.1% F1)—prompt may not capture fine-grained schema; consider few-shot example selection strategy

- First 3 experiments:
  1. Reproduce HGERE baseline on GSAP-ERE test set using provided code fork; verify 80.6% NER / 54.0% RE within ±1% tolerance
  2. Ablate entity types