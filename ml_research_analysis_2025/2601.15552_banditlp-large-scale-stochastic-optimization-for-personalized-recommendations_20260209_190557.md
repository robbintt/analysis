---
ver: rpa2
title: 'BanditLP: Large-Scale Stochastic Optimization for Personalized Recommendations'
arxiv_id: '2601.15552'
source_url: https://arxiv.org/abs/2601.15552
tags:
- constraints
- neural
- exploration
- banditlp
- bandit
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: BanditLP combines neural Thompson sampling with large-scale linear
  programming to optimize multi-stakeholder recommendations under constraints. It
  uses neural TS to estimate rewards and costs with uncertainty, then an LP selects
  actions satisfying stakeholder constraints at serving time.
---

# BanditLP: Large-Scale Stochastic Optimization for Personalized Recommendations

## Quick Facts
- **arXiv ID**: 2601.15552
- **Source URL**: https://arxiv.org/abs/2601.15552
- **Reference count**: 40
- **Primary result**: Combines neural Thompson sampling with large-scale linear programming to optimize multi-stakeholder recommendations under constraints, achieving 3.08% revenue increase and 1.51% unsubscription rate reduction in production.

## Executive Summary
BanditLP addresses the challenge of constrained multi-stakeholder recommendation optimization by unifying neural Thompson sampling for learning reward and cost estimates with large-scale linear programming for constrained action selection. The system uses Bayesian neural networks with Laplace approximation to estimate rewards and costs with uncertainty, then solves a large-scale LP at serving time to select actions satisfying platform, provider, and user-level constraints. Experiments show BanditLP outperforms strong baselines in reward while maintaining feasibility, and production deployment at LinkedIn email marketing demonstrates significant business impact with increased revenue and reduced unsubscription rates.

## Method Summary
BanditLP decouples reward/cost estimation from constrained action selection. Neural networks estimate conversion, unsubscription, and LTV outcomes using linearized Laplace approximation (LLA) on the last few layers to capture uncertainty. Thompson sampling draws from these posterior distributions, and a pre-fit isotonic regression calibrates these samples to preserve exploration variance. The calibrated predictions become coefficients in a large-scale linear program solved via DuaLip's dual decomposition approach, enforcing multi-level constraints while maximizing primary reward. The system updates models incrementally based on observed feedback.

## Key Results
- BanditLP achieves superior reward-feasibility trade-offs compared to NN-LP, NNTS, and LinUCB-LP baselines
- Maintains constraint violations near or below zero while delivering high cumulative reward
- Production A/B test shows 3.08% revenue increase and 1.51% unsubscription rate reduction
- Effective across varying exploration levels with stable constraint satisfaction

## Why This Works (Mechanism)

### Mechanism 1: Neural Thompson Sampling with Laplace Approximation for Uncertainty-Driven Exploration
Uncertainty estimates from Bayesian neural networks, sampled via Thompson sampling, inject principled exploration into downstream LP decisions, mitigating selection bias from logged training data. Bayesian neural networks estimate rewards/costs with posterior uncertainty via linearized Laplace approximation (LLA) on the last few layers; Thompson samples drawn from these posteriors replace deterministic predictions in the LP objective and constraints, causing the LP to occasionally select underexplored user-item pairs.

### Mechanism 2: Decoupled LP for Multi-Level Constraint Enforcement
Solving a large-scale LP at each decision round enforces per-round satisfaction of platform, provider, and user-level constraints while maximizing a primary objective. Estimated rewards (from neural TS) and costs become LP coefficients; the LP solver (DuaLip) handles 10^9+ decision variables via dual decomposition, perturbing the LP to a QP with regularization γ and solving the dual with first-order methods; constraints bind across stakeholder levels simultaneously.

### Mechanism 3: Heuristic Calibration Preserving Exploration Signal
Fitting calibration on non-exploring predictions and applying the learned calibration map to Thompson-sampled predictions preserves exploration variance, avoiding anti-exploration collapse from direct calibration. Fit isotonic regression g on deterministic predictions p̂ and rewards r; at serving time, apply g to Thompson-sampled predictions p̂_TS; because g is monotone but fitted on less-noisy inputs, it does not collapse the variance injected by TS.

## Foundational Learning

- **Concept: Thompson Sampling**
  - Why needed here: Core exploration mechanism; replaces deterministic exploitation with posterior sampling to discover higher-reward actions under uncertainty.
  - Quick check question: If you run BanditLP with τ→0, what happens to exploration and long-term reward?

- **Concept: Linear Programming with Lagrangian Duality**
  - Why needed here: DuaLip solves billion-variable LPs via dual decomposition; understanding dual variables reveals shadow prices of constraints.
  - Quick check question: What does a high dual value on the unsubscription constraint imply about the cost of reducing unsubscriptions by one unit?

- **Concept: Laplace Approximation for Bayesian Neural Networks**
  - Why needed here: Provides tractable posterior uncertainty for neural nets without full Bayesian inference; enables Thompson sampling at scale.
  - Quick check question: Why does using only the last few layers for LLA reduce computational cost while still capturing meaningful uncertainty?

## Architecture Onboarding

- **Component map**: Context → Neural inference → LLA uncertainty → Thompson sample → Calibration → LP solve → Allocation decision → Feedback → Model update
- **Critical path**: Context → Neural inference → LLA uncertainty → Thompson sample → Calibration → LP solve → Allocation decision → Feedback → Model update
- **Design tradeoffs**:
  - Last-layer vs. full-network LLA: Last-layer is faster but may underestimate uncertainty; full-network is more accurate but computationally prohibitive
  - τ (temperature) tuning: Higher τ increases exploration but raises allocation variance; use overlap-at-K to tune
  - γ (LP regularization): Lower γ improves LP accuracy but slows convergence; tune for latency constraints
- **Failure signatures**:
  - Constraint violation in production: sampled costs may systematically underestimate true costs; check calibration drift
  - Allocation instability: overlap-at-K drops suddenly after model refresh; investigate distribution shift
  - LP infeasibility: constraints too tight for sampled predictions; relax constraints or check for data quality issues
- **First 3 experiments**:
  1. Offline baseline comparison: Run BanditLP vs. NN-LP vs. NNTS vs. LinUCB-LP on synthetic/OBD data; verify constraint satisfaction and reward
  2. Exploration sensitivity analysis: Vary τ and model quality q; measure overlap-at-K, allocation variance, and constraint satisfaction
  3. Calibration ablation: Compare heuristic calibration vs. naive TS calibration; plot resulting prediction distributions and LP outcomes

## Open Questions the Paper Calls Out
- How can interaction effects between marketing units be incorporated into the BanditLP formulation while maintaining tractability?
- Can theoretical regret guarantees and constraint satisfaction bounds be established for BanditLP under neural Thompson sampling with approximate posteriors?
- What automated methods can optimally tune the temperature parameter τ and monitor exploration stability without requiring manual overlap-at-K threshold setting?
- How does BanditLP perform under non-stationary environments where reward distributions, costs, or constraint budgets shift over time?

## Limitations
- Generalization uncertainty to domains outside LinkedIn's email marketing context
- Reliance on heuristic calibration approach whose robustness to distribution shift is not fully characterized
- DuaLip solver performance under different constraint structures and problem sizes not thoroughly explored

## Confidence
- **High confidence**: Decoupled LP optimization framework and its integration with neural reward estimation is well-supported by both theoretical formulation and experimental results
- **Medium confidence**: Neural Thompson sampling with Laplace approximation provides reasonable approach to uncertainty estimation, but approximation quality effects on exploration-exploitation trade-offs not fully addressed
- **Medium confidence**: Production results showing 3.08% revenue increase and 1.51% unsubscription rate decrease are compelling but A/B test methodology and statistical significance not detailed sufficiently

## Next Checks
1. **Robustness to distribution shift**: Implement validation protocol systematically evaluating BanditLP when user features or item characteristics distribution changes between training and serving periods
2. **Calibration mechanism stress test**: Design experiments deliberately inducing calibration drift by modifying label distribution or feature space, comparing heuristic calibration against alternatives
3. **LP solver scalability boundary**: Vary number of decision variables, constraint types, and problem structure to identify operational limits of DuaLip, testing sparse constraints and varying constraint tightness