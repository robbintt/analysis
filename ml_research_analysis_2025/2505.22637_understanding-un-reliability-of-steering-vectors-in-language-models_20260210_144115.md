---
ver: rpa2
title: Understanding (Un)Reliability of Steering Vectors in Language Models
arxiv_id: '2505.22637'
source_url: https://arxiv.org/abs/2505.22637
tags:
- steering
- steered
- prompt
- activations
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies when and why steering vectors in language models
  are reliable. It examines the effect of prompt types on steering vectors and finds
  that all seven prompt types tested yield a net positive steering effect on average,
  but with high variance and many samples experiencing the opposite effect.
---

# Understanding (Un)Reliability of Steering Vectors in Language Models

## Quick Facts
- arXiv ID: 2505.22637
- Source URL: https://arxiv.org/abs/2505.22637
- Reference count: 40
- Primary result: Steering vectors from different prompt types often differ in direction but yield similar average steering effects, with reliability depending on dataset geometry.

## Executive Summary
This paper investigates when steering vectors reliably control language model behavior. Through extensive experiments on seven prompt types, it finds that all yield net positive steering effects on average, but with high variance and many anti-steerable samples. The study identifies two geometric predictors of steering reliability: the directional agreement of activation differences during training (measured by cosine similarity) and the linear separability of positive/negative activations along the steering direction. These factors explain why steering vectors fail when the target behavior lacks a coherent linear representation in activation space, rather than due to prompt choice.

## Method Summary
The paper uses Contrastive Activation Addition (CAA) to compute steering vectors as mean differences between positive and negative activation pairs. For each dataset, they extract activations using seven different prompt types at layer 13 of Mistral-7B, then measure steering efficacy by adding the vector during inference. They analyze geometric properties including mean cosine similarity between activation differences and the discriminability index (d') measuring separation along the difference-of-means line.

## Key Results
- All seven prompt types yield net positive steering effects on average, but with high variance and many anti-steerable samples
- Steering vectors from different prompt types often differ in direction (cosine similarity 0.07-0.86) but achieve similar steering performance
- Higher mean cosine similarity between activation differences predicts larger steering effect size and fewer anti-steerable samples
- Datasets with better linear separation of positive/negative activations along the steering direction are more steerable

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Steering vectors are reliable only when activation differences in the training data exhibit high directional agreement (coherence).
- **Mechanism:** Contrastive Activation Addition (CAA) computes a steering vector as the mean difference between positive and negative activations. If individual activation differences ($a(x, y^+) - a(x, y^-)$) are dispersed or orthogonal, the resulting mean vector points in an incoherent direction, failing to capture a linear representation of the behavior.
- **Core assumption:** The target behavior is encoded in a specific direction in activation space that persists across samples.
- **Evidence anchors:**
  - [Abstract] "Higher cosine similarity between activation differences in the training data predicts more effective steering."
  - [Section 3] "Directional agreement... is predictive of both larger steering effect size and fewer anti-steerable samples."
- **Break condition:** Low mean cosine similarity between activation differences and the steering vector indicates the behavior is not linearly representable or the dataset is too noisy.

### Mechanism 2
- **Claim:** Effective steering requires positive and negative activation distributions to be linearly separable along the steering direction.
- **Mechanism:** Steering shifts activations by a constant vector ($\lambda s$). If the distributions of positive and negative activations significantly overlap along the difference-of-means line, this uniform shift pushes some negative samples into the positive region but fails on others, resulting in high variance and "anti-steerable" samples.
- **Core assumption:** A simple vector addition is sufficient to cross a decision boundary only if the boundary aligns with the shift direction and classes are separable.
- **Evidence anchors:**
  - [Abstract] "Datasets where positive and negative activations are better separated... are more steerable."
  - [Section 3] "Less steerable datasets... have overlapping positive and negative activations [with] high variance along the difference-of-means line."
- **Break condition:** High overlap (low discriminability index $d'$) when projecting activations onto the difference-of-means line.

### Mechanism 3
- **Claim:** The specific prompt type used to extract activations has limited impact on steering reliability compared to the underlying dataset geometry.
- **Mechanism:** While different prompt types (e.g., prefilled vs. instruction-only) produce vectors with varying directions (cosine similarity 0.07–0.86), they tap into the same underlying model representations. Consequently, if the representation itself is incoherent (Mechanism 1), changing the extraction prompt does not resolve the unreliability.
- **Core assumption:** Different prompting strategies access similar high-level conceptual circuits despite differences in surface-level token processing.
- **Evidence anchors:**
  - [Abstract] "No prompt type clearly outperforms the others... vectors... often differ in direction."
  - [Section 3] "Steering performance of different prompt types is similar for the same dataset."
- **Break condition:** If a specific behavior relies entirely on non-linear or context-dependent circuits not captured by mean-difference methods, prompt variation may fail to isolate it.

## Foundational Learning

- **Concept:** **Residual Stream & Activation Geometry**
  - **Why needed here:** Steering vectors are added to the residual stream. Understanding that concepts are (hypothetically) linear directions in this high-dimensional space is prerequisite to grasping why cosine similarity and separation matter.
  - **Quick check question:** If two concepts have orthogonal vectors in the residual stream, does adding vector A affect the model's behavior regarding concept B? (Assumption: No/Minimally).

- **Concept:** **Contrastive Activation Addition (CAA)**
  - **Why needed here:** This is the specific method analyzed. You must understand that the vector is a *mean difference* between "positive" and "negative" behaviors, not a learned weight matrix.
  - **Quick check question:** How is the steering vector $s$ calculated from a dataset $D$ containing pairs of positive/negative activations?

- **Concept:** **Anti-Steerability & Variance**
  - **Why needed here:** The paper emphasizes that "average" success hides failure cases. An intervention that works 60% of the time but produces the opposite effect 40% of the time is "unreliable" despite a net positive.
  - **Quick check question:** If a steering vector increases the probability of the desired behavior on average but high variance exists, what risk does this pose for deployment?

## Architecture Onboarding

- **Component map:** Dataset (Prompt $x$, Positive completion $y^+$, Negative completion $y^-$) -> Forward pass (capture activations at layer 13) -> Compute mean difference -> Steering vector $s$ -> Inference (add $\lambda s$ to residual stream at layer 13)

- **Critical path:** The extraction of the activation difference is the most sensitive step. The paper notes that prefilled vs. non-prefilled prompts change the token position where computation happens (answer token vs. last prompt token), altering the vector.

- **Design tradeoffs:**
  - **Prompt Type:** Prefilled prompts offer stable position targeting but may differ from generation-time dynamics. Instruction-based prompts mimic generation but introduce noise from the instruction tokens.
  - **Training Size:** Small sample sizes (5–30) yield high variance in the calculated vector; large samples (200+) reduce variance but may wash out specific nuances if the direction isn't coherent.

- **Failure signatures:**
  - **Low Cosine Similarity:** Activation differences point in random/opposing directions during training.
  - **High Anti-Steerable Rate:** >30% of test samples move opposite to the intended direction.
  - **Distribution Overlap:** Visualizing the difference-of-means line shows high variance/overlap between positive and negative classes.

- **First 3 experiments:**
  1. **Geometric Audit:** Before steering, calculate the mean cosine similarity of activation differences in your training set. If it is low (<0.3), predict failure and do not proceed with CAA.
  2. **Separability Check:** Project training activations onto the difference-of-means line and calculate the discriminability index ($d'$). Compare this against the downstream steering effect size to validate the paper's correlation on your specific model.
  3. **Prompt Ablation:** Train vectors using "Prefilled" vs. "Instruction" types on the same data. Measure the cosine similarity between the resulting vectors to confirm they differ directionally while validating that their steering efficacy (mean logit difference) remains similar.

## Open Questions the Paper Calls Out
None

## Limitations
- Findings limited to CAA on Mistral-7B; generalization to other models and methods untested
- Anti-steerability rates remain high (>30% in some datasets) despite identifying predictive geometric factors
- The causal claim that geometric factors predict steering efficacy is correlative, not established

## Confidence

- **High confidence:** The geometric predictors (mean cosine similarity of activation differences, discriminability index) are strongly associated with steering effect size and anti-steerability rates within the studied model and datasets.
- **Medium confidence:** The mechanism explaining why prompt type has limited impact on reliability is plausible but less directly tested; the claim relies on observed similarity in downstream steering efficacy despite different vector directions.
- **Low confidence:** Generalization to other models (e.g., GPT-4, Claude), other steering methods, or all linguistic behaviors is untested and uncertain.

## Next Checks

1. **Geometric Validation on New Models:** Apply the cosine similarity and discriminability metrics to CAA-trained vectors on a different model (e.g., Llama-2, GPT-2). Test if these metrics predict anti-steerability and effect size as strongly as observed for Mistral-7B.

2. **Behavioral Scope Expansion:** Systematically test CAA on a diverse set of target behaviors, including those known to be non-linear (e.g., complex reasoning chains, sarcasm detection) versus potentially linear (e.g., sentiment, formality). Verify if geometric predictors fail precisely when behaviors are non-linearly encoded.

3. **Intervention on Geometry:** Conduct an experiment that actively modifies the training data geometry (e.g., filtering samples with low cosine similarity, balancing class distributions) and measure the resulting impact on the vector's cosine similarity and downstream steering reliability. This would causally test if the identified geometric factors *cause* improved steering.