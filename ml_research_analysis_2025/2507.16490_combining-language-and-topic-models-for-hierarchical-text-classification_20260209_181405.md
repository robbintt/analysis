---
ver: rpa2
title: Combining Language and Topic Models for Hierarchical Text Classification
arxiv_id: '2507.16490'
source_url: https://arxiv.org/abs/2507.16490
tags:
- topic
- which
- text
- level
- classification
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether combining features from a pre-trained
  language model (PLM) and a topic model improves hierarchical text classification
  (HTC) performance. The proposed approach extracts semantic embeddings from BERT
  and topic representations from BERTopic, processes them through separate convolutional
  layers, and combines them with label-wise attention for classification.
---

# Combining Language and Topic Models for Hierarchical Text Classification

## Quick Facts
- **arXiv ID:** 2507.16490
- **Source URL:** https://arxiv.org/abs/2507.16490
- **Authors:** Jaco du Toit; Marcel Dunaiski
- **Reference count:** 10
- **Primary result:** Combining PLM and topic model features for hierarchical text classification generally decreases performance compared to using PLM features alone.

## Executive Summary
This paper investigates whether combining features from a pre-trained language model (PLM) and a topic model improves hierarchical text classification (HTC) performance. The proposed approach extracts semantic embeddings from BERT and topic representations from BERTopic, processes them through separate convolutional layers, and combines them with label-wise attention for classification. Experiments on three benchmark HTC datasets (WOS, RCV1-V2, and NYT) show that adding topic model features generally decreases performance compared to using only PLM features. The best results were achieved by the model using only PLM features with hierarchical label-wise attention, which outperformed the combined approach and recent HTC methods on certain metrics. These findings suggest that incorporating topic model features for HTC tasks should not be assumed beneficial and requires careful evaluation.

## Method Summary
The method uses a dual-encoder architecture with frozen BERT for token embeddings and BERTopic for topic distributions. Separate convolutional neural networks process PLM and topic features, which are then concatenated and fed into a label-wise attention mechanism with hierarchical label-aware propagation. The model predicts classes at each level of the hierarchy using sigmoid classifiers, with attention weights conditioned on predictions from parent levels.

## Key Results
- PLM-only model (AttCNNGHLA) outperformed combined PLM+topic model on all three datasets
- Adding topic features decreased performance by 0.32-1.37% Macro-F1 across datasets
- Best PLM-only model outperformed recent HTC methods (HBGL, HPT, HGCLR) on certain metrics
- Low-resource experiments showed mixed results with topic features providing no consistent benefit

## Why This Works (Mechanism)

### Mechanism 1: Hierarchical Attention Propagation
- **Claim:** Propagating prediction information from higher hierarchy levels to lower levels improves classification consistency and accuracy for the PLM-only model (AttCNNGHLA).
- **Mechanism:** The Global Hierarchical Label-wise Attention (GHLA) encodes the prediction state of parent levels ($s_{k-1}$) and concatenates it with the label-specific document representation of the current level ($c_k$). This conditions the classifier on the hierarchical path, enforcing structural constraints.
- **Core assumption:** The label hierarchy is valid and the parent-child relationships are semantically relevant to the text features.
- **Evidence anchors:**
  - [section 4.5]: Describes GHLA concatenating ancestor-level prediction representations to current-level document representations.
  - [section 5.3]: Reports AttCNNGHLA outperformed the flat general attention (GA) baseline.
  - [corpus]: Weak direct support; neighbors focus on graphs/LLMs, but *Hierarchical Text Classification Using Contrastive Learning* similarly emphasizes path-guided hierarchy.
- **Break condition:** If the hierarchy is flat or if parent predictions are noisy (error propagation), this mechanism may degrade performance.

### Mechanism 2: Global-Local Feature Complementarity (Investigated & Refuted)
- **Claim:** The paper hypothesized that combining "local" PLM context with "global" topic distributions would provide distinct granularities of information to aid classification.
- **Mechanism:** Parallel processing streams where BERT captures token semantics ($h_t$) and BERTtopic captures corpus-level document distributions ($z$). These are processed by separate CNNs and merged before the attention layer.
- **Core assumption:** Topic clusters align with class labels; "global" topic signals are discriminative features for specific classes.
- **Evidence anchors:**
  - [abstract]: Explicitly states the rationale was to capture different granularities.
  - [section 5.3]: Shows the hypothesis failed; adding topic features decreased performance (TopAttCNN < AttCNN).
  - [corpus]: *Hierarchical Graph Topic Modeling* suggests topic structures need deep integration (trees), whereas this paper used simpler cluster-based features which may explain the failure.
- **Break condition:** If topic clusters are dominated by non-class-related themes (e.g., document length, style), they act as noise.

### Mechanism 3: Signal Dilution via Feature Concatenation
- **Claim:** Adding topic features degrades performance because the topic vectors may lack the precise alignment to class labels that PLM embeddings possess, effectively diluting the signal-to-noise ratio in the attention mechanism.
- **Mechanism:** The concatenation of convolutional outputs ($G = [U, R]$) forces the attention layer ($\alpha$) to weigh both semantic and topic features. If topic features are weakly correlated with the gold label, they distract the attention weights from the highly correlated PLM features.
- **Core assumption:** The attention mechanism cannot fully suppress the irrelevant dimensions of the topic vectors.
- **Evidence anchors:**
  - [section 5.3]: Authors hypothesize that "topic information... is often not useful for distinguishing between the different classes."
  - [abstract]: Concludes incorporation of topic features should not be assumed beneficial.
- **Break condition:** If the dataset is extremely small or domain-specific where global word co-occurrence is the *only* signal, topic features might theoretically help (though low-resource tests in Section 5.6 showed mixed results).

## Foundational Learning

- **Concept: Label-wise Attention**
  - **Why needed here:** This is the core classifier head. Instead of a single document vector, it learns a distinct weight distribution over the sequence for *each* class, creating $L$ different views of the same document.
  - **Quick check question:** How does the dimensionality of the attention weight matrix $\alpha$ change if the number of classes ($L$) doubles?

- **Concept: BERTtopic Pipeline (Embed $\to$ Cluster $\to$ Extract)**
  - **Why needed here:** Understanding why this failed requires knowing what BERTtopic produces. It uses SBERT $\to$ UMAP $\to$ HDBSCAN. The resulting vector $z$ is a distribution over clusters, not words.
  - **Quick check question:** If two documents have different semantic meanings but belong to the same dense cluster in the embedding space, will BERTtopic distinguish them?

- **Concept: Feature Extraction vs. Fine-tuning**
  - **Why needed here:** The paper uses BERT as a frozen feature extractor. This is a critical limitation compared to SOTA (HGCLR, HBGL) which fine-tune the PLM.
  - **Quick check question:** Does updating the weights of the BERT model during training constitute "feature extraction" or "fine-tuning"?

## Architecture Onboarding

- **Component map:** Input tokens -> Frozen BERT encoder -> Token embeddings $H$ + [CLS] vector -> CNN processing -> CNN outputs $U$ + Topic CNN outputs $R$ -> Concatenated feature matrix $G$ -> Global Hierarchical Label-wise Attention (GHLA) -> Context vectors $C$ -> Sigmoid classifiers per level

- **Critical path:** The GHLA layer relies on the output of the FCL from the previous level ($s_{k-1}$). An error in Level 1 prediction directly impacts the input representation for Level 2.

- **Design tradeoffs:**
  - **Frozen PLM vs. SOTA:** The authors freeze BERT for speed/simplicity, but Table 3 shows this lags behind fine-tuning approaches (like HGCLR) by ~2-3% absolute F1.
  - **Topic Granularity:** The paper sets the number of topics ($d_f$) equal to the number of classes. This is a rigid design choice that assumes a 1:1 topic-to-class mapping which rarely holds in reality.

- **Failure signatures:**
  - **Topic Noise:** High standard deviation in Macro-F1 (e.g., TopAttCNNGHLA on RCV1-V2) indicates the model collapses on rare classes when distracted by topic features.
  - **Hierarchy Violation:** If Level 1 prediction is wrong, GHLA conditions Level 2 on incorrect context, often resulting in valid sub-paths for the wrong parent.

- **First 3 experiments:**
  1. **Sanity Check (PLM Only):** Train `AttCNNGHLA` (PLM + CNN + GHLA) to establish a baseline. Do not add topic features until this baseline is stable.
  2. **Topic Ablation:** Train `TopAttCNNGHLA` and visualize the attention weights $\alpha$. Are the weights for the topic-derived features ($r_1...r_S$) close to zero? If so, the model is learning to ignore the topic features.
  3. **Low-Resource Stress Test:** Retrain on 10% of data (as per Section 5.6). Verify if the "global" topic features help when local context is scarce (hypothesis: they should help here, but the paper shows they often don't).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can topic model features improve hierarchical text classification performance when integrated into a fine-tuning framework rather than a frozen feature-extraction framework?
- Basis in paper: [explicit] The authors note their proposed model "consistently performs worse than recently proposed approaches which fine-tune the parameters of the PLM," suggesting the architecture (feature extraction) may limit the potential synergy of the combined features.
- Why unresolved: The paper only evaluates a pipeline where BERT is frozen. It is unclear if the negative results stem from the features themselves or the inability to fine-tune the PLM to accommodate the global topic information.
- What evidence would resolve it: An experiment where topic embeddings are injected into a fine-tunable hierarchical classification model (e.g., HBGL or HPT) to observe if gradient updates allow the model to better utilize the topic signals.

### Open Question 2
- Question: To what extent does the semantic alignment between unsupervised topic clusters and the ground-truth class hierarchy determine the efficacy of combining topic models with PLMs?
- Basis in paper: [explicit] The authors hypothesize that "the topic information... may not correlate with the classes for the particular dataset," but they do not quantitatively verify this alignment as the cause of the performance drop.
- Why unresolved: Without measuring the overlap between the BERTtopic clusters and the actual class labels, it remains unknown if the method failed because topic models are inherently unsuitable for HTC or simply because the specific clusters extracted were irrelevant to the specific hierarchy.
- What evidence would resolve it: A correlation analysis quantifying the relationship between the extracted topics and the hierarchical class labels across different datasets.

### Open Question 3
- Question: Is the observed performance degradation specific to the clustering-based nature of BERTtopic, or does it generalize to probabilistic topic models like LDA?
- Basis in paper: [inferred] The paper contrasts its results with prior work (Liu et al., 2021) that found LDA beneficial for flat multi-label classification. The current study uses BERTtopic, which relies on clustering rather than probabilistic generative modeling, potentially introducing different noise profiles.
- Why unresolved: The paper investigates "topic models" broadly but restricts implementation to BERTtopic. It is possible that BERTtopic's reliance on sentence-transformers and clustering makes it redundant when paired with BERT, whereas LDA might offer complementary statistical signals.
- What evidence would resolve it: Comparative experiments on the same HTC datasets using LDA or Neural-PLTM features to determine if the negative result holds across different topic modeling paradigms.

## Limitations
- The negative results are specific to BERT (frozen) + BERTtopic + dual-CNN architecture and may not generalize to other PLM-topic model pairings
- The failure could stem from the rigid 1:1 topic-to-class mapping assumption rather than topic features being inherently unsuitable
- The study does not explore alternative topic modeling approaches beyond BERTtopic's clustering-based method

## Confidence
- **High Confidence:** The core empirical finding that PLM-only features outperform the combined PLM+topic model on HTC benchmarks. The experimental setup, metrics, and statistical comparisons are sound.
- **Medium Confidence:** The proposed mechanism that topic vectors dilute the attention signal by introducing noise. While supported by the performance drop, the paper does not directly measure the correlation between topic features and class labels or perform feature ablation studies to isolate the noise source.
- **Low Confidence:** The general claim that "incorporating topic model features should not be assumed beneficial" for HTC. This is an overgeneralization from a single negative result in a specific experimental context.

## Next Checks
1. **Feature Ablation Analysis:** Train the combined model with random topic features (instead of BERTtopic outputs) to quantify the noise floor. If random topics degrade performance similarly, it confirms the attention mechanism cannot suppress irrelevant dimensions.
2. **Alternative Topic Models:** Replace BERTtopic with a supervised topic model (e.g., Labeled LDA) trained on the HTC class labels. If this improves performance, it validates that the failure was due to unsupervised topic misalignment rather than the concept of topic features itself.
3. **PLM Fine-tuning Test:** Repeat the experiment with a fine-tuned PLM (instead of frozen) to establish whether the performance gap between PLM-only and combined models changes. If fine-tuning the PLM closes the gap, it suggests the frozen PLM was the bottleneck, not the topic features.