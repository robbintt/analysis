---
ver: rpa2
title: 'ENTP: Enhancing Low-Quality SFT Data via Neural-Symbolic Text Purge-Mix'
arxiv_id: '2510.23160'
source_url: https://arxiv.org/abs/2510.23160
tags:
- corpus
- section
- user
- question
- answer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ENTP introduces a neural-symbolic framework that revitalizes low-quality
  instruction data by first purging noise through symbolic rules and then enriching
  content via neural synthesis. This two-step process integrates clustering, representative
  selection, and iterative corpus fusion to generate hybrid datasets that retain diversity
  while increasing informational depth.
---

# ENTP: Enhancing Low-Quality SFT Data via Neural-Symbolic Text Purge-Mix

## Quick Facts
- arXiv ID: 2510.23160
- Source URL: https://arxiv.org/abs/2510.23160
- Authors: Zile Yang; Ling Li; Na Di; Jinlong Pang; Yao Zhou; Hao Cheng; Bo Han; Jiaheng Wei
- Reference count: 40
- Models fine-tuned on ENTP-augmented low-quality data consistently outperform 13 baselines across five benchmarks, achieving average gains of 5.4–7.4%.

## Executive Summary
ENTP is a neural-symbolic framework designed to revitalize low-quality instruction datasets by integrating symbolic noise purging with neural synthesis. It first removes noise from low-quality data via symbolic rules and then enriches the content through neural synthesis, iterative corpus fusion, and clustering-based representative selection. The framework aims to generate hybrid datasets that retain diversity while increasing informational depth. Experimental results show that ENTP-augmented low-quality data consistently outperforms baselines—including fine-tuning on full datasets—across five benchmarks (MMLU, TruthfulQA, GSM8K, BBH, TyDiQA).

## Method Summary
ENTP combines symbolic and neural techniques in a two-step process to improve low-quality instruction data. First, it purges noise from low-quality datasets using manually curated symbolic rules. Then, it enriches the remaining content through neural synthesis, clustering-based representative selection, and iterative corpus fusion to create hybrid datasets. This approach integrates symbolic logic for data cleaning and neural methods for synthesis and diversity preservation, aiming to maximize the utility of low-quality instruction corpora.

## Key Results
- Models fine-tuned on ENTP-augmented low-quality data consistently outperform 13 baselines across five benchmarks, achieving average gains of 5.4–7.4%.
- ENTP-generated data can surpass fine-tuning on the full ~300K dataset, revealing the untapped potential of low-quality corpora.
- ENTP maintains diversity while increasing informational depth, as shown by robust performance across MMLU, TruthfulQA, GSM8K, BBH, and TyDiQA.

## Why This Works (Mechanism)
ENTP's effectiveness stems from its dual-stage approach: symbolic noise purging removes low-value content, while neural synthesis reconstructs and enriches the remaining data. Clustering and iterative corpus fusion preserve diversity and avoid overfitting to noise. The integration of symbolic and neural methods allows for precise data cleaning and robust synthesis, maximizing the informational value of low-quality datasets.

## Foundational Learning

**Symbolic Rule-Based Filtering**
- Why needed: Removes noise and irrelevant content from low-quality datasets.
- Quick check: Verify that filtered data retains task-relevant instructions and reduces noise-related errors.

**Neural Synthesis and Iterative Fusion**
- Why needed: Reconstructs and enriches content to improve informational depth.
- Quick check: Ensure generated examples are coherent, diverse, and aligned with task objectives.

**Clustering-Based Representative Selection**
- Why needed: Preserves diversity and avoids overfitting during data synthesis.
- Quick check: Confirm that clusters cover the instruction space without redundancy.

## Architecture Onboarding

**Component Map**
ENTP -> Symbolic Noise Purge -> Neural Synthesis -> Clustering & Representative Selection -> Iterative Corpus Fusion

**Critical Path**
The critical path is: ENTP (framework) → Symbolic Noise Purge → Neural Synthesis → Iterative Corpus Fusion. Each stage must succeed for high-quality hybrid datasets to be generated.

**Design Tradeoffs**
- Symbolic rules are precise but may miss nuanced noise; neural synthesis is flexible but may introduce artifacts.
- Iterative fusion balances diversity and depth but may slow convergence.
- Clustering preserves diversity but may underrepresent rare instruction types.

**Failure Signatures**
- If symbolic rules are too strict, useful data may be lost; if too lenient, noise persists.
- Poor neural synthesis can introduce hallucinations or incoherence.
- Clustering may collapse diversity if cluster counts are too low.

**Exactly 3 First Experiments**
1. Compare symbolic-only versus neural-only data cleaning to isolate each component's impact.
2. Vary cluster counts and fusion ratios to assess diversity versus depth tradeoffs.
3. Test ENTP on multiple model families (e.g., GPT, Mistral) to confirm generalization.

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation is limited to Llama-3 models; generalization to other architectures is unclear.
- Symbolic rules were manually curated and not subjected to ablation studies.
- Iterative fusion lacks quantitative convergence and sensitivity analyses.

## Confidence

| Claim                                                                 | Confidence |
|-----------------------------------------------------------------------|------------|
| ENTP outperforms 13 baselines                                        | High       |
| ENTP can surpass full-dataset fine-tuning                           | Medium     |
| Symbolic rules and neural synthesis integrate seamlessly            | Medium     |

## Next Checks

1. Conduct ablation studies isolating the impact of symbolic noise purging versus neural synthesis components.
2. Test ENTP's performance across multiple model families (e.g., GPT, Mistral) and diverse full-size instruction datasets.
3. Analyze convergence and diversity preservation using systematic coverage metrics and sensitivity analyses for fusion hyperparameters.