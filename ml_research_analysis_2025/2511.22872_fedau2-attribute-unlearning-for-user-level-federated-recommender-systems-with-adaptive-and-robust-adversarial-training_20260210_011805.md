---
ver: rpa2
title: 'FedAU2: Attribute Unlearning for User-Level Federated Recommender Systems
  with Adaptive and Robust Adversarial Training'
arxiv_id: '2511.22872'
source_url: https://arxiv.org/abs/2511.22872
tags:
- attribute
- user
- unlearning
- training
- adversarial
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the challenge of attribute unlearning in user-level
  federated recommender systems, where sensitive user attributes embedded in model
  parameters are vulnerable to inference attacks. Unlike group-level settings, user-level
  federated learning restricts cross-user information sharing, making existing unlearning
  methods infeasible.
---

# FedAU2: Attribute Unlearning for User-Level Federated Recommender Systems with Adaptive and Robust Adversarial Training

## Quick Facts
- **arXiv ID**: 2511.22872
- **Source URL**: https://arxiv.org/abs/2511.22872
- **Reference count**: 15
- **Primary result**: Up to 26.42% reduction in balanced accuracy for attribute unlearning while maintaining recommendation performance

## Executive Summary
This paper addresses attribute unlearning in user-level federated recommender systems where sensitive user attributes embedded in model parameters are vulnerable to inference attacks. Unlike group-level federated learning, user-level settings restrict cross-user information sharing, making existing unlearning methods infeasible. The authors propose FedAU2, which combines adaptive adversarial training (SUT) with a dual-stochastic variational autoencoder (DSVAE) to simultaneously prevent gradient-based reconstruction attacks and stabilize adversarial optimization. Extensive experiments on three real-world datasets demonstrate significant improvements in privacy preservation while maintaining recommendation quality.

## Method Summary
FedAU2 integrates two key components: an adaptive adversarial training strategy (SUT) that dynamically adjusts perturbation budgets based on local optimization behavior, and a dual-stochastic variational autoencoder that masks sensitive attribute information in gradients. The method uses gradient reversal layers to enable min-max adversarial optimization in a single training loop, with SUT selectively applying perturbations only when adversarial predictions are correct. DSVAE introduces stochasticity in both the μ-path and σ-path of the VAE to disrupt gradient-based label reconstruction attacks. The approach is evaluated across three federated recommendation models (FedNCF, FedVAE, FedRAP) on ML-100K, ML-1M, and LastFM-360K datasets.

## Key Results
- Achieves up to 26.42% reduction in balanced accuracy for attribute unlearning
- Maintains superior recommendation performance with only 4.51% NDCG@10 reduction on average
- Effectively balances privacy preservation and utility in real-world federated learning scenarios
- Outperforms existing baselines across multiple metrics and datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Adaptive perturbation budget control stabilizes adversarial training in heterogeneous federated settings without requiring global pre-training.
- Mechanism: The Selective Unlearning Trigger (SUT) monitors local adversarial prediction outcomes per user. When the adversarial classifier correctly predicts the sensitive attribute (ȳᵤ = yᵤ), adversarial training is applied with perturbation budget εᵤ = ε. When misclassified (insufficient discriminatory power), training is skipped (εᵤ = 0). This binary gating avoids both excessive perturbation (which destroys task-relevant features) and insufficient perturbation (which fails to erase sensitive information).
- Core assumption: Correct predictions indicate embeddings near local optima (far from decision boundary), while misclassifications indicate proximity to the boundary where gradients are less informative for unlearning.
- Evidence anchors:
  - [abstract] "adaptive adversarial training strategy, where the training dynamics are adjusted in response to local optimization behavior"
  - [Section 4.2] "SUT effectively balances the trade-off in perturbation budget through user-level adaptive adjustment, eliminating the need for global pretraining"
  - [corpus] RAID (arXiv:2504.11510) addresses attribute inference in RS but uses standard adversarial training without federated adaptation—FedAU2's SUT specifically targets the federated instability problem.

### Mechanism 2
- Claim: Dual stochasticity in the VAE's μ-path and σ-path jointly disrupts gradient-based label reconstruction attacks.
- Mechanism: Standard VAEs only inject stochasticity via σ⊙ε₂, leaving the μ-path deterministic and exploitable by DLG attacks. DSVAE adds stochasticity to μ via μ⊙ε′₁ where ε′₁ ~ N(1, λ). This perturbs both components of the reconstruction formula y*ᵢ = ŷ′ᵢ - δᵢ: (1) ŷ′ᵢ (client-side prediction approximation) and (2) δᵢ (gradient-encoded label preference). The multiplicative term ε⊤₁ε′₁ in δᵢ masks gradient preferences.
- Core assumption: Attacker cannot distinguish client-side noise ε₁ from reconstruction noise ε′₁ during gradient inversion.
- Evidence anchors:
  - [abstract] "dual-stochastic variational autoencoder to perturb the adversarial model, effectively preventing gradient-based information leakage"
  - [Section 4.3, Corollary 3] "DSV AE introduces dual stochasticity. This design effectively prevents DLG from accurately reconstructing user labels"
  - [corpus] LEGO (arXiv:2510.20327) focuses on multi-attribute unlearning but doesn't address gradient leakage—FedAU2 explicitly targets this gap.

### Mechanism 3
- Claim: Min-max adversarial optimization with GRL enables simultaneous recommendation quality preservation and attribute removal in a single training loop.
- Mechanism: The objective min_Θ,ω L_rec + λ·CE(h_ω(GRL(em_u)), y_u) uses a Gradient Reversal Layer that preserves forward pass but reverses gradients during backprop. This converts the min-max problem into pure minimization: the encoder learns to produce embeddings that maximize adversarial loss (hiding attributes) while minimizing recommendation loss (preserving utility).
- Core assumption: The feature representations for recommendation and attribute prediction are sufficiently separable that one can be suppressed without destroying the other.
- Evidence anchors:
  - [Section 3.2] "GRL preserves the forward pass and reverses the backward gradient. Converting the min-max into a minimization"
  - [Table 1] FedAU2 achieves average NDCG@10 reduction of only 4.51% vs. 20.05% for APM while achieving 26.42% BAcc reduction.
  - [corpus] Related work shows adversarial training is established for attribute unlearning (Ganhör et al. 2022); FedAU2's contribution is federated-specific stabilization and gradient protection.

## Foundational Learning

- Concept: **Gradient Reversal Layer (GRL)**
  - Why needed here: Core technique enabling adversarial training without explicit min-max alternation. Essential to understand how FedAU2 achieves attribute removal while training a single model end-to-end.
  - Quick check question: During backprop through GRL, if upstream gradient is +0.5, what gradient does GRL output?

- Concept: **Deep Leakage from Gradients (DLG) Attack**
  - Why needed here: The threat model FedAU2 defends against. Understanding DLG's reconstruction mechanism (dummy input/label optimization to match observed gradients) clarifies why DSVAE's dual stochasticity works.
  - Quick check question: Given observed gradient ∇W, how does DLG recover the training label y*? (Hint: See Theorem 1, Eq. 7)

- Concept: **User-Level vs. Group-Level Federated Learning**
  - Why needed here: Critical distinction explaining why prior methods (distribution alignment) fail in this setting. User-level means each client has single-user data (extremely sparse, non-IID), while group-level aggregates multiple users per client.
  - Quick check question: Why can't group-level attribute unlearning methods like Aegis be applied to user-level FedRecs?

## Architecture Onboarding

- Component map: Client-side recommendation model → GRL → DSVAE → Adversarial classifier → SUT gating logic → Combined loss → Local update → Server aggregates item parameters
- Critical path: 1. User embedding em_u → GRL → DSVAE (forward: standard, backward: reversed + perturbed) 2. Adversarial prediction ȳᵤ → SUT decision (apply ε or skip) 3. Combined loss L_rec + λ·L_adv → local update 4. Server aggregates item parameters; user parameters remain local
- Design tradeoffs:
  - **λ (stochasticity coefficient)**: Higher λ → better gradient protection but weaker attribute unlearning (Fig. 4c). Paper uses λ=4.
  - **ε (GRL scale)**: Controls adversarial strength. Paper uses 400. Too high disrupts embeddings; too low fails unlearning.
  - **SUT vs. global pretrain**: SUT adapts per-user; pretrain requires fixed epochs (10-100 depending on model) and generalizes poorly under stochastic sampling (Fig. 3).
- Failure signatures:
  - **Training instability (oscillating losses)**: SUT not triggering correctly; check adversarial prediction accuracy on held-out users
  - **Gradient attack success >80%**: DSVAE not applied; verify ε′₁ sampling in μ-path
  - **NDCG drops >15%**: ε or λ too aggressive; reduce GRL scale or stochasticity
  - **BAcc stays >65% (no unlearning)**: Adversarial model underfitting; increase local learning rate for h_ω
- First 3 experiments:
  1. **Sanity check**: Train FedNCF + FedAU2 on ML-100K with gender attribute. Verify: (a) BAcc drops from ~60% to ~50%, (b) NDCG@10 stays within 5% of Original baseline. Compare Global/Pretrain/SUT strategies to reproduce Fig. 3.
  2. **Gradient attack robustness**: Implement DLG attack on uploaded gradients. Compare Original MLP vs. VAE vs. DSVAE (Fig. 4a). Target: DSVAE should keep attack accuracy near random guess (~50% for binary).
  3. **Component ablation**: Test FedAU2 with SUT-only (no DSVAE) vs. DSVAE-only (no SUT) vs. full system. Measure: (a) training stability (loss variance), (b) gradient leakage, (c) recommendation performance. Expect full system to dominate on all three.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can adversarial training be optimized for residual-based personalized FedRecs (like FedRAP) to avoid the unlearning limitations caused by aggregating user-specific residuals?
- Basis in paper: [Explicit] The authors state: "To improve efficiency, we aggregate the residual into a single dimension during adversarial training, which may further limit its attribute unlearning ability."
- Why unresolved: The current implementation compromises the granularity of unlearning to handle the complexity of residual-based embeddings, resulting in weaker performance on models like FedRAP.
- What evidence would resolve it: A modified training strategy that processes the full residual matrix without aggregation while maintaining computational efficiency, demonstrating improved BAcc reduction on FedRAP.

### Open Question 2
- Question: Can the linear increase in communication and memory overhead introduced by the Dual-Stochastic VAE be mitigated for resource-constrained devices?
- Basis in paper: [Explicit] "DSV AE replaces the last layer with two projections, doubling its parameters and causing a linear increase in computational, memory, and communication overhead."
- Why unresolved: While SUT reduces computational cost, the parameter doubling in DSV AE poses a scalability challenge for user-level clients (e.g., mobile devices) with limited bandwidth and memory.
- What evidence would resolve it: A modified architecture or compression technique that maintains dual-stochastic perturbation without doubling the final layer parameters, validated by unchanged NDCG and BAcc metrics.

### Open Question 3
- Question: Is FedAU2 robust against gradient inversion attacks that utilize generative priors rather than the optimization-based dummy gradient matching analyzed in Theorem 1?
- Basis in paper: [Inferred] The theoretical analysis and experiments focus specifically on DLG/iDLG attacks (optimization-based), but the defense mechanism's effectiveness against generative-based attacks (e.g., GradInversion) remains unverified.
- Why unresolved: The perturbation of $\delta_i$ in Theorem 1 is derived specifically to break the gradient matching optimization ($\|\nabla_{W'} - \nabla_W\|^2$); attacks using strong image priors might reconstruct attributes despite this noise.
- What evidence would resolve it: Empirical evaluation of FedAU2 against generative reconstruction attacks to determine if the injected stochasticity effectively masks attribute information in those scenarios.

## Limitations
- SUT's binary gating mechanism may fail when attribute inference is inherently difficult for certain user subgroups
- The λ=4 stochasticity coefficient is empirically chosen without systematic sensitivity analysis
- The dual stochasticity defense lacks formal proof beyond empirical demonstration
- Residual-based models (FedRAP) show weaker unlearning performance due to aggregation simplifications

## Confidence
- **High**: The adaptive adversarial training framework (SUT) and its role in stabilizing federated optimization
- **Medium**: The effectiveness of DSVAE's dual stochasticity in preventing gradient reconstruction attacks
- **Medium**: The empirical performance claims (26.42% BAcc reduction, 4.51% NDCG@10 reduction)

## Next Checks
1. **SUT robustness test**: Run FedAU2 on highly imbalanced datasets (90/10 gender split) and measure whether SUT still triggers appropriately across all user groups.
2. **DLG attack transferability**: Implement the full DLG reconstruction pipeline and verify whether DSVAE prevents successful label recovery across multiple attack iterations and optimization strategies.
3. **Component sensitivity analysis**: Systematically vary λ (DSVAE stochasticity) and ε (GRL scale) to map the full Pareto frontier between attribute unlearning and recommendation utility.