---
ver: rpa2
title: 'SEA-Guard: Culturally Grounded Multilingual Safeguard for Southeast Asia'
arxiv_id: '2602.01618'
source_url: https://arxiv.org/abs/2602.01618
tags:
- prompt
- safety
- cultural
- data
- figure
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: SEA-Guard introduces culturally grounded safeguards for Southeast
  Asia by generating a region-specific dataset of 870K samples per language across
  8 SEA languages. The data synthesis framework uses an agentic approach to produce
  culturally nuanced prompts and responses, then applies Monte Carlo Reasoning Ensemble
  (MCRE) for robust annotation and quality assurance.
---

# SEA-Guard: Culturally Grounded Multilingual Safeguard for Southeast Asia

## Quick Facts
- arXiv ID: 2602.01618
- Source URL: https://arxiv.org/abs/2602.01618
- Authors: Panuthep Tasawong; Jian Gang Ngui; Alham Fikri Aji; Trevor Cohn; Peerat Limkonchotiwat
- Reference count: 16
- Primary result: State-of-the-art culturally grounded safety model for 8 Southeast Asian languages with 870K training samples per language

## Executive Summary
SEA-Guard introduces a culturally grounded multilingual safeguard specifically designed for Southeast Asian contexts, addressing the gap in region-specific safety models. The framework generates a large-scale dataset using an agentic approach that produces culturally nuanced prompts and responses, then applies Monte Carlo Reasoning Ensemble (MCRE) for robust annotation. The resulting SEA-Guard models (4B, 8B, 12B) achieve superior performance on culturally sensitive safety benchmarks while maintaining strong results on general safety tasks, demonstrating both cultural awareness and robust safety capabilities.

## Method Summary
SEA-Guard employs an agentic data synthesis framework that decomposes prompt generation into guideline creation and prompt generation stages, using the Gemma-SEA-LION model to capture cultural nuances. The system generates culturally specific prompts and responses across 8 SEA languages (Thai, Indonesian, Vietnamese, Tagalog, Burmese, Malay, Khmer, Lao), then annotates them using Monte Carlo Reasoning Ensemble (MCRE) which performs multiple stochastic reasoning passes to calculate continuous harmfulness scores. The data undergoes lexical bias pruning via deduplication to remove spurious correlations before training Qwen-SEA-LION/Gemma3-based SEA-Guard models through supervised fine-tuning.

## Key Results
- State-of-the-art performance on culturally grounded safety benchmarks while maintaining strong general safety task results
- Demonstrates zero-shot generalization to vision-text safety tasks, improving baseline in 6 out of 7 cases
- Shows robust alignment with human judgment and resistance to adversarial attacks including whitespace insertion
- Successfully handles cultural safety nuances across 8 SEA languages with 870K samples per language

## Why This Works (Mechanism)

### Mechanism 1: Agentic Requirement Decomposition
If data generation is decoupled into a "Guideline Agent" and a "Prompt Generator," the resulting training data captures cultural nuances that direct prompting misses. Rather than asking a model to "generate a harmful prompt about Thailand," a Guideline Agent first expands the requirement into step-by-step procedural guidelines, acting as a "soft prompt" that guides the generator and reduces refusal rates while increasing semantic diversity. The core assumption is that the base LLM possesses sufficient latent cultural knowledge to be unlocked by structured reasoning. This is supported by the scarcity of SEA data in related works and the observation that combining guidelines, personas, and language helps LLMs more accurately capture SEA-specific contexts. Break condition: If the base model lacks specific cultural pre-training, the guideline agent will hallucinate or produce generic content.

### Mechanism 2: Monte Carlo Reasoning Ensemble (MCRE) for Uncertainty
If safety labels are generated via multiple stochastic reasoning passes ($N=10$) and aggregated, the annotation handles ambiguous "borderline" cases better than single-pass Chain-of-Thought. MCRE samples multiple reasoning trajectories and maps them to a 5-way ordinal space, calculating a continuous "harmfulness score" (0.0 to 1.0) before discretizing to a final label, preserving uncertainty information that hard labels discard. The core assumption is that variance in the model's reasoning paths correlates with semantic ambiguity or cultural complexity in the input. This aggregation yields a normalized class probability that explicitly captures predictive uncertainty. Break condition: If $N$ is too low, variance estimates are noisy; if the base model is systematically biased, aggregation merely amplifies the bias.

### Mechanism 3: Lexical Bias Pruning via Deduplication
If training data is filtered to remove samples predictable by simple bag-of-words classifiers, the safeguard becomes robust to adversarial paraphrasing and avoids "shortcut learning." The authors iteratively prune samples where a simple LMI-based classifier predicts the label with high confidence, removing "easy" samples that rely on surface keywords and forcing the model to learn deeper semantic representations. The core assumption is that high confidence from a lexical baseline indicates a lack of semantic complexity, making the sample low-value for training a nuanced classifier. Such samples are likely to encode spurious correlations, and their removal reduces redundant patterns. Break condition: If aggressive pruning removes rare but valid cultural keywords, the model may suffer from under-fitting on specific topics.

## Foundational Learning

- **Concept: Stochastic Reasoning / Sampling**
  - Why needed here: MCRE relies on generating diverse reasoning paths. Without understanding how temperature $> 0$ induces probability distributions over tokens, the "ensemble" logic is opaque.
  - Quick check question: Why does MCRE require sampling from the distribution $P(r|x)$ rather than taking the argmax?

- **Concept: Ordinal Regression vs. Categorical Classification**
  - Why needed here: The paper maps a 5-way ordinal scale to a 3-way class. Understanding that "Safe-Sensitive" is not a distinct class but a score interval is crucial for interpreting the annotation logic.
  - Quick check question: How does calculating a continuous harmfulness score $h(x)$ via weighted sums help in handling "borderline" cases compared to direct 3-way classification?

- **Concept: Spurious Correlations (Shortcut Learning)**
  - Why needed here: The deduplication strategy specifically targets this. One must understand that models often learn "if word X exists, output Y" instead of understanding the context to see why pruning high-frequency tokens matters.
  - Quick check question: Why would a high-accuracy BoW model on the training set indicate a *problem* with the dataset quality?

## Architecture Onboarding

- **Component map:** Input Formulation -> Synthesis Agents (Guideline Agent -> Prompt Agent -> Response Agent) -> Annotation Layer (MCRE Annotator -> MCRE Verifier) -> Filter (BoW-based Deduplicator) -> Model (Qwen-SEA-LION/Gemma3 -> SEA-Guard SFT)
- **Critical path:** The MCRE Annotation Layer is the bottleneck, noted as "over two orders of magnitude slower" than single-pass inference, acceptable for offline data generation but blocking real-time use.
- **Design tradeoffs:** Quality vs. Cost (generating $N=10$ reasoning paths ensures high quality but drastically increases compute costs); Sensitivity vs. Precision (the "Sensitive" class remains challenging, often overlapping with safe/harmful clusters).
- **Failure signatures:** Under-defensiveness on "Sensitive" topics where models overlap significantly with Safe clusters; Code-switching artifacts in Burmese data showing lower quality.
- **First 3 experiments:** 1) Ablation on $N$: Rerun annotation with $N=1$ vs $N=10$ to verify performance drop; 2) Deduplication Impact: Train on raw 1M dataset vs. 870k deduplicated set to measure "shortcut learning" reduction; 3) Adversarial Stress Test: Apply whitespace insertion attacks to verify baseline performance mitigation.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can safeguards achieve reliable calibration for borderline "Sensitive" cases that are neither clearly safe nor overtly harmful?
- Basis in paper: Section 5.1 states: "Handling the middle severity bin remains challenging for all models; it corresponds to sensitive cases that are neither clearly safe nor overtly harmful... insufficient distinction from adjacent bins still limits reliable calibration, reducing the effectiveness of threshold-based control."
- Why unresolved: Current models show overlap between severity bins for sensitive content, making user-defined thresholds unreliable for deployment.
- What evidence would resolve it: Demonstration of improved bin separation on continuous harmfulness scores and higher correlation with human soft-label annotations for borderline cases.

### Open Question 2
- Question: How can culturally grounded safety training be combined with general safety training without degrading cultural performance?
- Basis in paper: Section 4.2 states: "our preliminary experiments reveal a trade-off: adding such data shifts the training distribution toward general safety topics and degrades performance on culturally grounded safety content, which is the primary objective of SEA-Guard."
- Why unresolved: The paper prioritized cultural grounding by excluding generic safety data, leaving the trade-off unresolved for models needing both capabilities.
- What evidence would resolve it: A training methodology that achieves strong performance on both SEA cultural benchmarks and generic safety benchmarks simultaneously.

### Open Question 3
- Question: Can MCRE-based annotation achieve computational efficiency suitable for real-time applications while maintaining annotation quality?
- Basis in paper: Section 2.4.2 states: "requiring N stochastic reasoning generations per input incurs substantial overhead—over two orders of magnitude slower than single-pass reflective safeguards—making the approach impractical for real-time use."
- Why unresolved: MCRE was designed for offline annotation; the computational cost barrier for real-time deployment was acknowledged but not addressed.
- What evidence would resolve it: A modified approach achieving comparable annotation quality with significantly reduced inference time, evaluated on the same cultural benchmarks.

### Open Question 4
- Question: How can culturally grounded safeguards be developed and evaluated for the remaining SEA languages and dialects lacking evaluation benchmarks?
- Basis in paper: The Limitations section states: "there are some languages that we did not cover (i.e., Khmer, Lao, Telugu, and over 700 SEA dialects and languages). This is because there is no availability of benchmarks in those languages."
- Why unresolved: Without benchmarks, model development and evaluation for these languages remains infeasible, creating a coverage gap.
- What evidence would resolve it: Creation of culturally grounded safety benchmarks for currently unsupported SEA languages, followed by model evaluation on those benchmarks.

## Limitations

- Reliance on LLM-based data generation without external validation of cultural accuracy, potentially generating synthetic rather than authentic SEA cultural contexts
- MCRE annotation may amplify systematic biases present in the Gemma-SEA-LION base model
- Evaluation focuses on benchmark performance without extensive qualitative assessment of real-world cultural nuance handling
- "Sensitive" class remains particularly challenging with significant overlap between severity bins, suggesting potential under-defensiveness on complex cultural topics

## Confidence

- **High Confidence:** Technical implementation of MCRE framework and data synthesis pipeline (internally consistent and mathematically sound)
- **Medium Confidence:** Claims about cultural grounding and agentic decomposition approach (methodology well-described but cultural authenticity unverified)
- **Low Confidence:** Assertion that SEA-Guard models are truly "culturally grounded" rather than pattern matching on culturally-labeled examples (genuine cultural understanding not empirically demonstrated)

## Next Checks

1. **Cultural Authenticity Validation:** Commission external SEA cultural experts to review a random sample of 100 generated prompts and responses from each language to assess whether content authentically represents cultural contexts or merely applies superficial cultural markers.

2. **Bias Amplification Analysis:** Conduct systematic bias audit comparing cultural representation in Gemma-SEA-LION base model versus SEA-Guard outputs, specifically looking for amplification of stereotypes or cultural misrepresentations introduced during MCRE annotation.

3. **Real-world Deployment Testing:** Deploy SEA-Guard models in actual SEA-language community contexts (e.g., content moderation for SEA social media platforms) and measure performance against established safety standards, comparing results with both Western safety models and human moderators familiar with local contexts.