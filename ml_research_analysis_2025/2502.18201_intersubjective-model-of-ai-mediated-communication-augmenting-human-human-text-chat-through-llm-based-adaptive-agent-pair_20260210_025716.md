---
ver: rpa2
title: 'Intersubjective Model of AI-mediated Communication: Augmenting Human-Human
  Text Chat through LLM-based Adaptive Agent Pair'
arxiv_id: '2502.18201'
source_url: https://arxiv.org/abs/2502.18201
tags:
- communication
- information
- intersubjective
- agent
- conversation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the Intersubjective Model of AI-mediated
  Communication (AIMC), which moves beyond traditional AIMC models that assume shared
  environments. The model uses LLM-based adaptive agents to mediate communication
  between individuals in separate environments, enabling dynamic message shaping and
  facilitating shared understanding.
---

# Intersubjective Model of AI-mediated Communication: Augmenting Human-Human Text Chat through LLM-based Adaptive Agent Pair

## Quick Facts
- arXiv ID: 2502.18201
- Source URL: https://arxiv.org/abs/2502.18201
- Reference count: 30
- Introduces Intersubjective Model of AI-mediated Communication (AIMC) using LLM-based adaptive agents to mediate communication between individuals in separate environments

## Executive Summary
This paper introduces the Intersubjective Model of AI-mediated Communication (AIMC), which fundamentally reimagines how AI can facilitate human-human communication by moving beyond shared environments. Unlike traditional AIMC systems that assume common contexts and focus on low-magnitude modifications, this model employs high-autonomy LLM agents that operate in parallel feedback loops to extract, transform, and regenerate messages between individuals in separate environments. The authors developed a prototype text chat system demonstrating how agents can shape communication to achieve specific goals like creating lively conversations or bridging generational gaps, while raising important questions about trust, authenticity, and the future of mediated human interaction.

## Method Summary
The authors propose a theoretical framework where each human communicates with an AI agent rather than directly with another human. These agents maintain separate knowledge bases and persona settings, communicating indirectly through structured data transmission. The system architecture consists of extraction modules that parse conversation content into structured events, and conversation modules that generate responses based on shared data and persona guidelines. The prototype uses Supabase for real-time database management and GPT-4o for both extraction and conversation modules. A key innovation is the dual-loop feedback architecture: an Inner Loop providing rapid acknowledgments to maintain conversational rhythm, and an Outer Loop handling core meaning transmission between agents.

## Key Results
- The Intersubjective Model framework conceptually redefines AIMC by enabling high-magnitude, high-autonomy transformations rather than low-magnitude modifications
- Prototype demonstrates how agents can extract information and generate responses tailored to specific communication goals (e.g., creating lively conversations or bridging generational gaps)
- Introduces distinct inner loops (human-agent) and outer loops (agent-agent) that work in parallel, redefining feedback dynamics in mediated communication

## Why This Works (Mechanism)

### Mechanism 1: Dual-Loop Feedback Architecture
- Claim: Separating feedback into rapid "Inner Loop" (human-agent) and slower "Outer Loop" (agent-agent) creates perception of synchronicity during background semantic processing
- Mechanism: Agents provide immediate, context-light acknowledgments to maintain conversational rhythm while extracting heavy semantic content for remote agent reconstruction
- Core assumption: Users prioritize immediate responsiveness and "flow" over real-time transparency of remote user's actual typing/status
- Evidence anchors: Section 3.2.4 defines Inner Feedback Loop as rapid/accelerating rhythm while Outer handles core meaning transmission; abstract mentions distinct inner and outer loops working in parallel
- Break condition: If Outer Loop latency exceeds conversational relevance threshold (e.g., responding to topic moved on from 5 minutes ago), continuity illusion breaks

### Mechanism 2: Semantic Modulation via "Goal-Overrides-Fidelity"
- Claim: Communication impasses can be navigated by intentionally degrading accurate symbol transmission in favor of modulation aligned with superordinate goals
- Mechanism: Instead of encoding/decoding for accuracy, agents extract "core arguments" while omitting "emotional outbursts," then re-encode in empathetic wrapper before transmission
- Core assumption: Message "meaning" can be decoupled from specific "symbols" by LLM well enough to preserve intent while changing impact
- Evidence anchors: Section 3.2.2 explicitly proposes abandoning "correct transmission" of meaning to serve goals like mediating debates; Section 3.1 Example 5 describes omitting emotional intensity for calm discussion
- Break condition: If extraction layer misidentifies "emotional noise" as "core semantic content," modulation may amplify conflict or lose critical nuance

### Mechanism 3: Decoupled Subjective Environments
- Claim: Removing shared objective environment constraint allows asymmetric information presentation, bridging gaps requiring incompatible interfaces
- Mechanism: Agent A and Agent B maintain distinct knowledge bases and persona settings; information transmitted is "integrated" and "regenerated" to fit local context of receiving environment
- Core assumption: Cognitive load of managing AI proxy is lower than translating/decoding foreign or culturally distant interlocutor directly
- Evidence anchors: Section 3 states individuals engage in conversations with AI agents where communication occurs indirectly; Section 3.1.4 Example 4 illustrates generational gap bridging
- Break condition: If agents' knowledge bases drift too far apart (hallucination or omission cascades), humans exist in different realities with no shared ground truth

## Foundational Learning

- Concept: **Encoding/Decoding vs. Modulation**
  - Why needed here: This paper redefines standard communication theory task of "accurate decoding" into "purposeful modulation" - you must understand difference between transmitting data (fidelity) and designing interaction (goal-seeking)
  - Quick check question: If agent receives angry message, does "modulation" mean sending exact angry text (High Fidelity) or sending summarized, neutralized version of complaint (High Modulation)?

- Concept: **Agency and Autonomy in AIMC**
  - Why needed here: System operates in "High Magnitude, High Autonomy" quadrant - distinguish between "assistive" AI (Grammarly, Low Autonomy) and "representative" AI (this model, High Autonomy) to understand failure liabilities
  - Quick check question: In this architecture, who is the "sender" of message that Human B receives: Human A or Agent B?

- Concept: **Chronemics in CMC**
  - Why needed here: Prototype explicitly manipulates time (response timing) to simulate human-like pacing - understanding time as communicative cue is essential for tuning system
  - Quick check question: Why does system deliberately delay sending messages if LLM can generate them instantly?

## Architecture Onboarding

- Component map:
  Frontend: React Chat Interface (per user) -> Backend State: Supabase (Real-time database) -> Agent Core (Per User): Extraction Module -> Conversation Module -> Mediator Channel: Agent-to-Agent pipe

- Critical path:
  1. Human A inputs text
  2. Agent A Extraction Module parses text -> creates "Event" (e.g., "A is excited about Tokyo")
  3. Event transmitted to Agent B Knowledge Base
  4. Agent B Conversation Module decides to surface this event -> generates response
  5. Timing Algorithm queues response to drip-feed to Human B

- Design tradeoffs:
  - **Latency vs. Naturalness**: System uses `2.5 * character_count` delay (Section 5.2) to mimic typing - improves "human-likeness" but reduces information velocity
  - **Fidelity vs. Goal**: "Lively Conversation" design (Section 3.1.1) discards emotional nuance to force enthusiasm - risks misrepresenting sender's actual mood

- Failure signatures:
  - **The Hallucination Loop**: Agent B hallucinates detail from Agent A; Agent A validates it to be polite; humans proceed on false premises
  - **The Echo Chamber**: Agents agree too readily, causing conversation to stagnate or feel robotic despite "lively" parameters
  - **Context Drift**: Extraction module omits critical detail (e.g., "I am allergic to peanuts"); Conversation module suggests peanut restaurant

- First 3 experiments:
  1. **Timing Calibration**: Replicate `2.5x` delay logic with English text to find "uncanny valley" of response speed
  2. **Modulation Accuracy**: Input "heated debate" transcript into Extraction module - measure if output successfully removes "emotional outbursts" while retaining "logical arguments"
  3. **Trust Degradation**: Run user study where one side told speaking to AI proxy and other is not - measure how "High Autonomy" affects trust compared to corpus findings on "Reputational Risks of AI-Mediated Communication"

## Open Questions the Paper Calls Out
- How can the model be empirically evaluated to validate its effectiveness in real-world scenarios?
- What are the ethical implications of allowing AI agents to significantly modify human messages?
- How can the system be expanded beyond text-based communication to create intersubjective realities in virtual environments?

## Limitations
- The model assumes LLMs can reliably extract and regenerate meaning across contexts without evidence of accuracy rates or failure modes
- Limited empirical validation - prototype implementation lacks rigorous testing of core claims
- High-autonomy approach introduces significant risks of misrepresentation that are not quantified

## Confidence

**High Confidence (Evidence-Supported):**
- Architectural separation of Inner and Outer feedback loops is clearly defined and implementable
- Prototype technical specifications (Supabase, GPT-4o modules) are detailed enough for replication
- Theoretical distinction between traditional AIMC and Intersubjective Model is well-articulated

**Medium Confidence (Theoretically Sound but Untested):**
- Mechanism for bridging generational gaps through asymmetric knowledge bases
- Claim that dual-loop architecture creates perception of synchronicity
- Premise that high-autonomy mediation can be more effective than low-autonomy assistance

**Low Confidence (Speculative):**
- Effectiveness of "abandoning correct transmission" for achieving communication goals
- Assumption that users will accept high-magnitude AI transformations of their messages
- Scalability of model beyond controlled prototype conditions

## Next Checks

1. **Extraction Accuracy Validation**: Create benchmark dataset of 100 conversational exchanges with labeled "core semantic content" versus "emotional/nuanced content." Test whether Extraction module consistently identifies correct elements for preservation versus filtering across different communication goals (lively conversation, conflict mediation, etc.).

2. **Trust and Authenticity Impact Study**: Conduct controlled experiment where participants engage in identical conversations under three conditions: direct chat, low-autonomy AIMC (message suggestions), and high-autonomy Intersubjective Model. Measure trust levels, perceived authenticity, and willingness to continue using system.

3. **Hallucination Cascade Detection**: Design systematic test where messages flow through both agents multiple times. Track whether hallucinations compound over iterations and whether system can detect when agent-generated content diverges significantly from original intent, implementing automatic correction or human intervention triggers.