---
ver: rpa2
title: 'Top-H Decoding: Adapting the Creativity and Coherence with Bounded Entropy
  in Text Generation'
arxiv_id: '2509.02510'
source_url: https://arxiv.org/abs/2509.02510
tags:
- top-h
- sampling
- min-p
- top-p
- entropy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Top-H decoding is a new sampling method for large language models
  that addresses the challenge of balancing creativity and coherence in open-ended
  text generation. It introduces an entropy-constrained mass maximization framework,
  which selects a subset of tokens to sample from such that the resulting distribution
  has minimal divergence from the original while maintaining bounded entropy.
---

# Top-H Decoding: Adapting the Creativity and Coherence with Bounded Entropy in Text Generation

## Quick Facts
- arXiv ID: 2509.02510
- Source URL: https://arxiv.org/abs/2509.02510
- Reference count: 40
- Primary result: Entropy-constrained sampling method that dynamically balances creativity and coherence in open-ended generation

## Executive Summary
Top-H decoding introduces an entropy-constrained mass maximization framework for large language model sampling that dynamically adjusts the sampling pool based on the model's confidence. Unlike existing methods that rely solely on top token probability, Top-H calculates a dynamic cutoff by solving the Entropy-Constrained Mass Maximization problem, ensuring the resulting distribution maintains bounded entropy while maximizing probability mass. This approach prevents incoherence at high temperatures while allowing necessary exploration, making it particularly effective for creative writing tasks while maintaining coherence. Extensive experiments show Top-H outperforms state-of-the-art methods like min-p and top-p by up to 25.63% on creative writing benchmarks while demonstrating robustness on reasoning tasks.

## Method Summary
Top-H decoding implements a greedy algorithm that selects tokens in descending probability order until the entropy of the renormalized distribution exceeds a threshold α·H(p), where H(p) is the entropy of the original next-token distribution. The method uses an incremental entropy calculation to maintain O(n) complexity and applies temperature scaling before entropy computation. Key hyperparameters include α=0.4 (default) and temperatures in {1.0, 1.5, 2.0}. The approach is evaluated on LLaMA3.1-8B-Instruct, Qwen2.5-3B, and Phi-3-Mini-4K-Instruct across creative writing (Alpaca-Eval, MT-Bench) and reasoning tasks (GSM8K, GPQA) using exact match accuracy and LLM-as-judge metrics.

## Key Results
- Outperforms min-p and top-p by up to 25.63% on creative writing benchmarks while maintaining coherence at high temperatures
- Demonstrates robustness on reasoning tasks like GSM8K and GPQA, showing versatility across different generation types
- Maintains effectiveness at temperature T=2.0 where top-p typically struggles with incoherence
- Achieves high empirical performance ratios close to 1.0 relative to optimal solutions despite greedy approximation

## Why This Works (Mechanism)

### Mechanism 1
Constraining the entropy of the truncated distribution dynamically adapts the sampling pool size, preventing incoherence at high temperatures while allowing necessary exploration. Unlike static thresholds (e.g., Top-p) or single-token heuristics (e.g., Min-p), Top-H calculates a dynamic cutoff by solving the Entropy-Constrained Mass Maximization (ECMM) problem. It greedily adds tokens (sorted by probability) to a candidate set S until the entropy of the renormalized distribution H(q) exceeds a bound α·H(p), where H(p) is the entropy of the original next-token distribution. The core assumption is that Shannon entropy H(p) is a sufficient proxy for the model's uncertainty, and bounding H(q) effectively limits "randomness" without arbitrarily discarding plausible tokens.

### Mechanism 2
Maximizing probability mass Γ_S while satisfying the entropy constraint theoretically minimizes the divergence (JSD) between the truncated and original distributions. The authors prove that minimizing the Jensen-Shannon Divergence (JSD) between the model's prediction p and the truncated distribution q is equivalent to maximizing the sum of probabilities in the selected set Γ_S, provided the entropy constraint holds. This ensures the sample stays "close" to the model's true intent. The core assumption is that lower JSD correlates directly with higher text quality/fluency in open-ended generation.

### Mechanism 3
A greedy approximation efficiently solves the NP-hard subset selection problem without significant degradation in performance. Since finding the exact optimal subset is NP-hard (proven via reduction from Cardinality-Constrained Subset Sum), the paper proposes a greedy algorithm. It sorts tokens and iteratively adds them, computing entropy incrementally until the threshold is met. The core assumption is that the quality of the greedy solution is sufficient for high-dimensional language generation tasks, despite the theoretical hardness.

## Foundational Learning

- **Concept**: Jensen-Shannon Divergence (JSD)
  - **Why needed here**: The paper frames the entire sampling strategy as an optimization problem to minimize JSD. Understanding JSD as a symmetric, bounded measure of similarity between distributions is required to grasp why Top-H maximizes probability mass.
  - **Quick check question**: How does JSD differ from KL Divergence, and why might it be preferred for comparing probability distributions in this context?

- **Concept**: Truncated Sampling (Top-p / Nucleus)
  - **Why needed here**: Top-H is positioned as a direct successor to Top-p and Min-p. You must understand how Top-p creates a cumulative probability cutoff to see how Top-H improves upon it by considering distribution shape (entropy) rather than just cumulative mass.
  - **Quick check question**: In a "flat" distribution where many tokens have low but similar probabilities, does Top-p include too many or too few tokens? How does Top-H address this?

- **Concept**: Shannon Entropy in Next-Token Prediction
  - **Why needed here**: The core lever of control in Top-H is the Shannon entropy of the next-token distribution H(p). You need to understand that high entropy implies model uncertainty (potentially requiring more creativity) while low entropy implies confidence (requiring coherence).
  - **Quick check question**: If H(p) is very low, what happens to the entropy threshold α·H(p), and how does that affect the final sampling pool?

## Architecture Onboarding

- **Component map**: Logits Processor -> Softmax & Sort -> Incremental Entropy Calculator -> Threshold Check -> Masking
- **Critical path**: The loop in Algorithm 1 is the bottleneck. It must avoid re-computing entropy from scratch at every insertion. The implementation relies on the incremental update formula detailed in Appendix C.2 to maintain O(n) complexity.
- **Design tradeoffs**: 
  - Parameter α (default 0.4) controls creativity/coherence trade-off. Lower α means stricter entropy cap → fewer tokens → higher coherence, lower creativity. Higher α means looser cap → more tokens → higher creativity, risk of incoherence.
  - Temperature: Top-H is robust to high temperatures (T=2.0), unlike Top-p. High T flattens p, increasing H(p), which naturally raises the threshold α·H(p), allowing safe sampling from the flattened distribution.
- **Failure signatures**: 
  - Repetition loops if α is too low, leading to degenerate greedy decoding
  - Hallucination/incoherence if α is too high (>0.8), behaving like high-temperature Top-p
  - Latency spikes if incremental entropy calculation is not used
- **First 3 experiments**:
  1. Unit Test - Termination: Verify Theorem 3 by feeding uniform distribution (high entropy). Ensure it does not select all tokens but respects the α bound.
  2. Ablation on α: Run generation on GSM8K with α ∈ {0.2, 0.4, 0.6, 0.8} at T=1.0. Plot accuracy vs. α to find the "coherence cliff."
  3. Visual Verification: Replicate Figure 6 using fixed "flat" vs. "peaked" distribution. Confirm Top-H assigns larger set to flat distribution (high entropy) and smaller to peaked, unlike Min-p.

## Open Questions the Paper Calls Out

- **Open Question 1**: Can a formal approximation guarantee be derived for the greedy Top-H algorithm relative to the optimal solution of the NP-hard ECMM problem? While empirical results show near-optimality, deriving a formal approximation guarantee is beyond the scope of this work.

- **Open Question 2**: Does the empirically optimal entropy coefficient (α=0.4) generalize to structured generation tasks such as code synthesis or data extraction? The tuning was performed exclusively on creative writing samples, which may differ from precision required in coding.

- **Open Question 3**: Is Jensen-Shannon Divergence (JSD) the optimal theoretical metric for the ECMD objective, or do other divergence measures offer better alignment with human coherence? While JSD is symmetric and bounded, its correlation with human-perceived coherence has not been ablated against alternatives.

## Limitations

- The greedy approximation quality across diverse distributions and potential degradation in extreme cases remains uncertain despite empirical validation showing ratios close to 1.0
- The parameter sensitivity and generalizability of α=0.4 across different model sizes, tasks, and domains lacks systematic exploration beyond reported benchmarks
- Implementation dependencies on specific details like incremental entropy calculation make faithful reproduction challenging without exact code access

## Confidence

- **Mechanism 1 (Dynamic Entropy Constraint)**: High Confidence - Theoretical framing and greedy approximation are clearly defined with proof of NP-hardness
- **Mechanism 2 (JSD Minimization via Mass Maximization)**: Medium Confidence - Theoretical equivalence is proven but practical significance for text quality is not directly validated
- **Mechanism 3 (Greedy Efficiency)**: High Confidence - NP-hardness is clearly stated and greedy solution with incremental calculation is well-explained
- **Overall Performance Claims**: Medium Confidence - Extensive experiments show improvements but sensitivity to α and generalizability introduce uncertainty

## Next Checks

1. Conduct sensitivity analysis of α across range {0.2, 0.3, 0.4, 0.5, 0.6} on diverse tasks (creative writing, reasoning, summarization) to identify optimal ranges and failure points

2. Evaluate Top-H on wider variety of model architectures (Mistral, Mixtral) and domains (medical text, code generation, dialogue) beyond reported LLaMA3.1, Qwen2.5, and Phi-3 models

3. Implement and compare Top-H against alternative entropy-based methods like Locally Typical Sampling to isolate contribution of specific entropy constraint formulation