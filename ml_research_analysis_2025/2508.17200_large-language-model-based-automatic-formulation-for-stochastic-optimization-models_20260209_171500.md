---
ver: rpa2
title: Large Language Model-Based Automatic Formulation for Stochastic Optimization
  Models
arxiv_id: '2508.17200'
source_url: https://arxiv.org/abs/2508.17200
tags:
- problem
- problems
- code
- stochastic
- optimization
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents the first systematic evaluation of large language
  models (LLMs) for automatically formulating stochastic optimization (SO) problems
  from natural language descriptions. It introduces a multi-agent prompting framework
  and a novel soft-scoring metric to assess partial correctness of generated models,
  addressing limitations of existing evaluation methods.
---

# Large Language Model-Based Automatic Formulation for Stochastic Optimization Models

## Quick Facts
- **arXiv ID:** 2508.17200
- **Source URL:** https://arxiv.org/abs/2508.17200
- **Reference count:** 40
- **Primary result:** Introduces first systematic evaluation of LLMs for automatically formulating stochastic optimization problems from natural language, with multi-agent prompting and soft-scoring metric

## Executive Summary
This paper presents the first systematic evaluation of large language models (LLMs) for automatically formulating stochastic optimization (SO) problems from natural language descriptions. It introduces a multi-agent prompting framework and a novel soft-scoring metric to assess partial correctness of generated models, addressing limitations of existing evaluation methods. Across three SO categories—chance-constrained and two-stage stochastic mixed-integer linear programs—the study finds that GPT-4-Turbo outperforms other models, with structured prompts significantly improving variable and constraint matching while reducing extra-element generation. The soft scoring metric captures structural and algebraic similarities even when exact matches fail, enabling more nuanced assessment of LLM-generated SO formulations.

## Method Summary
The study evaluates four LLM models (GPT-3.5-Turbo, GPT-3.5-16K, GPT-4, GPT-4-Turbo) on 18 stochastic optimization problems using five prompting strategies: standard, chain-of-thought (CoT), and an agentic framework with data extractor, formulator, and reviewer agents. Models are prompted to generate executable Python/Gurobi code from natural language problem descriptions. A soft-scoring metric computes partial correctness by parsing addVars, addConstrs, and setObjective calls, normalizing indices, and using greedy matching to compare against ground truth code. The metric calculates P_var (variable match %), P_cons (constraint match %), P_obj (objective match %), P_total (overall), and P_extra (extra/hallucinated elements %). All runs use temperature=0 and execute 360 total experiments (9 problems × 2 instances × 4 models × 5 prompts).

## Key Results
- GPT-4-Turbo achieves the highest variable matching rates and lowest extra-element generation across all prompt types
- Chain-of-thought prompting significantly improves variable and constraint matching compared to standard prompts
- The agentic framework reduces hallucination rates while maintaining structural correctness
- Soft scoring metric captures meaningful structural similarities even when exact code matching fails
- Execution accuracy remains near 0% for complex problems, validating the need for partial scoring

## Why This Works (Mechanism)

### Mechanism 1: Agentic Decomposition and Specialized Roles
The multi-agent framework assigns distinct personalities to agents (Data Extractor, Formulator, Reviewer) to mitigate cascading errors and "catastrophic forgetting." By separating extraction of indices/sets from algebraic formulation, the system allows specialized reviewers to catch hallucinations before final output. Core assumption: LLMs possess sufficient SO knowledge to execute specialized roles correctly when prompted.

### Mechanism 2: Constraint Expansion via Chain-of-Thought (CoT)
Explicit step-by-step reasoning forces the model to generate intermediate reasoning tokens (e.g., listing sets, defining probabilities) before outputting code. This "loads" relevant mathematical context into the attention window, reducing syntax errors in complex SMILP-2 extensive forms. Core assumption: Problem descriptions contain sufficient cues for the model to infer correct mathematical sets and stochastic distributions.

### Mechanism 3: Code-as-Loss-Function (Execution-Based Verification)
Requiring executable Python/Gurobi code leverages the compiler/solver as a verification step. Soft-scoring rewards partial structural matches (e.g., correct variables but wrong constraints) which exact text matching would penalize as total failures. Core assumption: Correctness can be approximated by structural and algebraic similarity even if code is not executable or returns suboptimal objective.

## Foundational Learning

- **Two-Stage Stochastic Programming (SMILP-2)**: Distinguish "first-stage" decisions (made before uncertainty resolves) from "second-stage" recourse actions (adjustments after uncertainty). *Quick check:* Can you identify which variables in a supply chain problem depend on the realization of a scenario ω versus those that are fixed?

- **Chance Constraints**: Individual vs. Joint chance constraints differ fundamentally. Understanding that P(A_i x ≤ b_i) ≥ 1-α (individual) differs from P(A x ≤ b) ≥ 1-α (joint) is critical. *Quick check:* Does a constraint x ≥ ξ holding with 95% probability imply the same feasibility set as requiring all constraints to hold simultaneously with 95% probability?

- **Algebraic Normalization**: The soft-scoring metric relies on normalizing indices (e.g., replacing [i] with [idx]) to compare generated code against ground truth. *Quick check:* If an LLM writes sum(x[i] for i in I) and ground truth is quicksum(x[j] for j in I), should the structural score treat these as equivalent?

## Architecture Onboarding

- **Component map:** NL description + template -> Controller (ChatGPT with prompt strategy) -> Parser (Regex/SymPy) -> Evaluator (Soft Scoring Algorithm)
- **Critical path:** The mapping of natural language to "Sets and Indices." If the Data Extractor agent fails to identify the scenario set Ω or product set K, subsequent Formulator will produce code with dimension mismatches.
- **Design tradeoffs:** Agentic vs. Standard yields higher partial scores and fewer hallucinations but incurs higher latency and token costs. Soft Score vs. Accuracy: Accuracy near 0% for complex problems makes it useless, while Soft Score provides granularity but requires ground truth code.
- **Failure signatures:** Extra-element generation (hallucinating constraints not in text), index collapse (using scalar variables x instead of indexed variables x[i,j]), objective mismatch (correctly identifying variables but failing to sum correctly over scenario probability space).
- **First 3 experiments:**
  1. Run "Electricity Planning" SMILP-2 instance with agentic prompting on GPT-4-Turbo. Verify if Reviewer agent catches difference between x[i,j] and x[i].
  2. Score perturbation test: Take ground truth code, permute variable names, run soft scoring. Confirm score remains high (validating normalization logic).
  3. Ablation on instructions: Run "Supply Chain" chance-constrained problem with cot_s vs. cot_instructions. Measure delta in constraint matching scores (P_cons).

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can an automated framework be developed to dynamically analyze a problem description and select the optimal prompt design for specific stochastic optimization categories?
- **Basis in paper:** Authors state "need to develop a tool that automatically analyzes a problem and renders the best prompts customized to that problem."
- **Why unresolved:** Current results show significant performance variance between prompts depending on problem type, requiring manual tuning.
- **What evidence would resolve it:** Comparative study demonstrating adaptive prompt-selection tool consistently achieves higher soft scores than fixed-prompt baselines across diverse SO problems.

### Open Question 2
- **Question:** What is the optimal number of independent agents and interaction topology required to maximize formulation correctness in the multi-agent framework?
- **Basis in paper:** Conclusion notes "designing the architecture interactions and deciding the number of independent agents to maximize the performance is essential."
- **Why unresolved:** Current study used fixed multi-agent structure; impact of varying agent counts and communication protocols on hallucination rates is unknown.
- **What evidence would resolve it:** Ablation study measuring soft scores and extra-element generation rates while systematically varying agent counts and communication protocols.

### Open Question 3
- **Question:** How can the soft scoring metric be refined to be index-agnostic and support multi-stage stochastic programs (SMILP-N)?
- **Basis in paper:** Authors propose future work to "enhance [the metric] to evaluate multi-stage stochastic programs" and "neutralize the effect of indexing errors."
- **Why unresolved:** Current metric sensitive to text-intensive indexing variations, and evaluation logic cannot capture recursive decision structures beyond two stages.
- **What evidence would resolve it:** Modified scoring algorithm that successfully identifies structural equivalence in constraints despite indexing mismatches and accurately scores multi-stage formulations.

## Limitations
- Evaluation relies on curated dataset of 18 instances across three SO categories, which may not represent full diversity of real-world problems
- Soft-scoring metric introduces subjectivity in how structural similarity is quantified through normalization and greedy matching
- Reliance on Gurobi and Python syntax may bias results toward problems amenable to this specific formulation
- Agentic framework's effectiveness depends on quality of inter-agent communication, not fully explored beyond three-agent decomposition

## Confidence
- **High Confidence:** GPT-4-Turbo's superior performance in variable matching and constraint identification across all prompt types
- **Medium Confidence:** Effectiveness of soft-scoring metric in capturing partial correctness, though normalization logic needs validation
- **Medium Confidence:** Agentic framework's ability to reduce extra-element generation, mechanism inferred rather than directly tested
- **Low Confidence:** Claim that CoT prompting significantly improves constraint matching for complex SMILP-2 problems

## Next Checks
1. **Ablation Study on Agent Roles:** Remove Reviewer agent from agentic framework and rerun "Electricity Planning" SMILP-2 instance. Compare increase in extra-element generation to validate Reviewer's role in catching hallucinations.
2. **Soft Score Robustness Test:** Apply soft-scoring metric to ground truth code with permuted variable names across all 18 instances. Verify average P_total remains above 95%, confirming normalization logic's insensitivity to naming conventions.
3. **Prompt Instruction Sensitivity Analysis:** Run "Supply Chain" chance-constrained problem with cot_s (no instructions) and cot_instructions using GPT-4-Turbo. Measure change in P_cons to quantify impact of explicit instructions on constraint matching.