---
ver: rpa2
title: 'GRILE: A Benchmark for Grammar Reasoning and Explanation in Romanian LLMs'
arxiv_id: '2508.14279'
source_url: https://arxiv.org/abs/2508.14279
tags:
- romanian
- llms
- language
- dataset
- gemini
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces GRILE, the first benchmark for Romanian grammar
  reasoning and explanation, comprising 1,151 MCQs from high-stakes exams. The authors
  evaluate seven LLMs on answer accuracy and explanation quality, finding that Gemini
  2.5 Pro achieves 83% accuracy, while most open-weight models stay below 65%.
---

# GRILE: A Benchmark for Grammar Reasoning and Explanation in Romanian LLMs

## Quick Facts
- arXiv ID: 2508.14279
- Source URL: https://arxiv.org/abs/2508.14279
- Reference count: 6
- Key result: Gemini 2.5 Pro achieves 83% accuracy on Romanian grammar MCQs; 48% of explanations contain factual/pedagogical flaws

## Executive Summary
This paper introduces GRILE, the first benchmark for Romanian grammar reasoning and explanation, comprising 1,151 MCQs from high-stakes exams. The authors evaluate seven LLMs on answer accuracy and explanation quality, finding that Gemini 2.5 Pro achieves 83% accuracy while most open-weight models stay below 65%. Qualitative analysis reveals that 48% of generated explanations contain factual or pedagogical flaws. The study highlights systematic weaknesses in morphology and alignment with the latest orthographic norms. All data, code, and a web demo are publicly released to support future research and educational NLP development in low-resource settings.

## Method Summary
The authors created GRILE by harvesting 1,151 single-answer MCQs from Romanian high-stakes exams (2010-2024), parsing them with OCR and rule-based processing, then evaluating seven LLMs using zero-shot direct prompting, Chain-of-Thought prompting, and few-shot variants. Models were assessed on answer accuracy and explanation quality, with expert review of 200 explanations for factual and pedagogical soundness. The benchmark covers lexical, morphological, syntactic, and phonetic categories, with particular attention to DOOM3 orthographic norm alignment.

## Key Results
- Gemini 2.5 Pro achieves 83% accuracy, significantly outperforming open-weight models (average 58%)
- Chain-of-thought prompting yields 5-11 pp accuracy gains for compatible models
- 48% of generated explanations contain factual or pedagogical flaws despite correct answers
- Romanian-specific fine-tuning does not consistently improve performance compared to strong multilingual base models

## Why This Works (Mechanism)

### Mechanism 1
Chain-of-thought prompting improves grammatical reasoning accuracy by forcing explicit intermediate steps. CoT instructions ("Let's think step by step") require models to articulate grammatical rules before selecting answers, reducing surface-pattern matching and encouraging rule-based inference. This yielded +11.36 pp gains for DeepSeek V3 and +5.9 pp for Llama 3.3 70B.

### Mechanism 2
Romanian-specific fine-tuning does not consistently outperform strong multilingual base models on grammatical MCQs. General multilingual pre-training may encode richer grammatical patterns than targeted fine-tuning on smaller Romanian corpora. The paper found base Gemma-2-9B-it (47.13%) outperformed all RoGemma variants (39-42%).

### Mechanism 3
Answer accuracy alone is insufficient for evaluating LLMs as educational tools; explanation quality must be assessed separately. Models can select correct answers via pattern-matching while generating factually flawed justifications. Expert review found 67/96 problematic explanations had incorrect justifications even when answers were correct.

## Foundational Learning

- **Low-resource language NLP gap**: Romanian represents only 15% of NLP tools available for English; benchmark data scarcity affects model evaluation and training.
  - Quick check: Can you name two factors that make grammatical benchmarking harder for low-resource languages compared to English?

- **Chain-of-thought prompting**: The primary intervention tested for improving both accuracy and generating explanations for qualitative analysis.
  - Quick check: What instruction phrase did the authors use to trigger CoT reasoning, and what output format did they require?

- **DOOM3 orthographic norms**: The current Romanian linguistic standard (2021); misalignment between training data and DOOM3 caused systematic errors in 6/19 DOOM-related questions.
  - Quick check: Why might an LLM trained on pre-2021 text produce explanations that conflict with current Romanian orthographic rules?

## Architecture Onboarding

- **Component map**: PDF/scans → Tesseract OCR → rule-based parsing → JSON storage → API inference → answer extraction → expert review → web demo
- **Critical path**: Harvest questions from exam PDFs (2010-2024 sources) → Convert and parse with OCR + manual spot-checking → Run model inference with CoT prompting → Extract answers and explanations → Conduct expert validation on subset
- **Design tradeoffs**: Dataset size (1,151 items) vs. coverage of grammatical phenomena; Automated category classification (99% accuracy) vs. full expert annotation; Single best model (Gemini 2.5 Pro) for explanation generation vs. ensemble approaches
- **Failure signatures**: DOOM3 norm violations in explanations; Category confusion at morphology/lexicon boundaries; Imprecise terminology or irrelevant reasoning in explanations; Outdated source questions with no DOOM3-compliant answer option
- **First 3 experiments**: 1) Baseline comparison: Run direct zero-shot prompting across all 7 models to establish accuracy floor before CoT intervention; 2) CoT ablation: Compare CoT vs. direct prompting on same model subset to isolate reasoning effect (expect 5-11 pp improvement); 3) Error categorization audit: Sample 50 incorrect answers from best-performing model; classify by error type to identify systematic weaknesses

## Open Questions the Paper Calls Out
None

## Limitations

- Low-resource language context: The Romanian NLP landscape's sparsity (15% of English tooling) limits external validation opportunities. Expert review sample size (200/1,151 questions) and single-expert approach constrain generalizability.
- Model selection bias: The evaluation includes only 7 models, and claims about fine-tuning effectiveness conflate architectural differences with fine-tuning effects.
- DOOM3 alignment measurement: The paper identifies DOOM3 misalignment as a systematic weakness but lacks quantitative measurement to establish statistical significance.

## Confidence

**High confidence**: Answer accuracy results (Gemini 2.5 Pro: 83%, average open-weight: 58%) and CoT prompting improvements (5-11 pp gains) are well-supported by experimental methodology.

**Medium confidence**: The conclusion that "Romanian-specific fine-tuning does not consistently yield significant improvements" requires qualification—it holds for this specific grammatical MCQ task but may not generalize to other Romanian NLP applications.

**Low confidence**: Claims about DOOM3 misalignment as a systematic weakness lack quantitative validation. The morphological reasoning weakness is identified through error pattern analysis but requires larger-scale categorization to establish statistical significance.

## Next Checks

1. **Expert reliability validation**: Conduct inter-rater reliability testing on 50 explanation samples with 3 Romanian linguistics experts. Compute Cohen's kappa to establish whether the 48% flaw rate reflects consensus or individual judgment variation.

2. **DOOM3 quantitative audit**: Systematically categorize all 1,151 questions by DOOM3 alignment status (compliant, non-compliant, neutral). Compute accuracy differentials between DOOM3-compliant and non-compliant questions for each model to establish statistical significance of the normative misalignment effect.

3. **Fine-tuning ablation study**: Select 3 RoLLM models (one from each family) and evaluate both pre-fine-tuned and post-fine-tuned versions on the full GRILE benchmark. Compare accuracy differentials while controlling for other architectural changes through matched comparisons where possible.