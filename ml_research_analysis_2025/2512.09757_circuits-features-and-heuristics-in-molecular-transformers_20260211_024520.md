---
ver: rpa2
title: Circuits, Features, and Heuristics in Molecular Transformers
arxiv_id: '2512.09757'
source_url: https://arxiv.org/abs/2512.09757
tags:
- features
- layer
- sparse
- molecular
- work
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a mechanistic analysis of autoregressive transformers
  trained on drug-like small molecules to understand how they capture molecular representation
  rules. The authors identify computational patterns consistent with low-level syntactic
  parsing and more abstract chemical validity constraints, and use sparse autoencoders
  (SAEs) to extract feature dictionaries associated with chemically relevant activation
  patterns.
---

# Circuits, Features, and Heuristics in Molecular Transformers

## Quick Facts
- arXiv ID: 2512.09757
- Source URL: https://arxiv.org/abs/2512.09757
- Authors: Kristof Varadi; Mark Marosi; Peter Antal
- Reference count: 40
- Primary result: Mechanistic analysis reveals syntax circuits and valence representations in molecular transformers, with sparse autoencoders extracting chemically meaningful features that improve downstream property prediction.

## Executive Summary
This paper presents a mechanistic analysis of autoregressive transformers trained on drug-like small molecules to understand how they capture molecular representation rules. The authors identify computational patterns consistent with low-level syntactic parsing and more abstract chemical validity constraints, and use sparse autoencoders (SAEs) to extract feature dictionaries associated with chemically relevant activation patterns. They find specialized attention heads implementing pointer-writer circuits for SMILES grammar matching, a distributed linear representation of valence capacity, and sparse features that improve downstream molecular property prediction tasks.

## Method Summary
The authors analyze a 6-layer, 8-head decoder-only transformer trained on ~300M lead-like molecules from ZINC20. They use three complementary metrics to find relevant circuits: pointer mass (attention probability assigned to correct opening tokens), event specificity (causal impact of head ablation), and ablation validity (percentage of valid molecules generated when the head is removed). For chemical features, they train SAEs on residual stream activations and develop a fragment-screening pipeline to identify features aligned with chemical substructures. The SAEs use L1-sparsity to enforce near-zero activations while maintaining reconstruction fidelity, and features are evaluated for chemical interpretability through fragment specificity scoring.

## Key Results
- Identified two complementary attention heads for ring digit matching (L2H7 as pointer, L1H2 as writer) and one specialized head for branch balancing (L2H3), with ablation significantly reducing generation validity
- Found a distributed linear representation of valence capacity in the residual stream, with a single causal direction that monotonically modulates bond-order predictions at decision points
- Extracted sparse feature dictionaries from SAEs that align with chemically meaningful activation patterns, with a small number of high-specificity features acting as strong detectors for chemically meaningful substructures
- Demonstrated that SAE-derived features improve transformer embeddings on several property prediction tasks, achieving competitive performance with established methods like ECFP fingerprints

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Specialized attention heads implement pointer-writer circuits for SMILES grammar matching (ring closures and branch balancing).
- Mechanism: Two-head composition for rings—L2H7 tracks opening tokens via attention probability; L1H2 writes output logits causally. Branches rely on a single hybrid head (L2H3) coupling tracking and execution.
- Core assumption: Attention probability on correct tokens indicates state tracking; ablation impact on validity indicates causal load-bearing.
- Evidence anchors: "L2H7 acts as a pointer, dedicating 30.7% of its attention mass... L1H2 behaves as a writer... ablating this head is catastrophic, dropping mean validity to 25.4%"
- Break condition: If ablation of both L2H7 and L1H2 simultaneously preserves >70% validity, alternative pathways exist beyond the identified pointer-writer circuit.

### Mechanism 2
- Claim: Valence capacity is encoded as a distributed linear direction in the residual stream that causally modulates bond-order predictions.
- Mechanism: Linear probe at Layer 3 achieves 99.08% accuracy; steering along the probe direction monotonically shifts probability from single bonds to higher-order bonds.
- Core assumption: The probe direction corresponds to the model's actual computational pathway, not just a statistically accessible pattern.
- Evidence anchors: "increasing α suppresses single bonds while boosting higher-order bonds... statistically significant shift in bond probabilities"
- Break condition: If steering the valence direction produces non-monotonic or chemically incoherent bond shifts (e.g., triple bonds increasing when valence budget is low), the direction is not cleanly causal.

### Mechanism 3
- Claim: Sparse autoencoders decompose polysemantic residual activations into interpretable, chemically-aligned features that improve downstream prediction.
- Mechanism: L1-sparsity enforces near-zero activations; a minority of features achieve high fragment specificity (e.g., urea, aryl-fluorine detectors). These features outperform dense embeddings on MoleculeACE (RMSE 0.730 vs. 1.057).
- Core assumption: Sparsity constraint disentangles concepts without destroying task-relevant signal.
- Evidence anchors: "SAE features consistently achieve higher interpretability scores compared to the raw residual stream and PCA components"
- Break condition: If SAE reconstruction (via activation replacement) drops validity below 80% or cross-entropy increases >0.3 nats, sparsity is over-compressing essential structure.

## Foundational Learning

- Concept: SMILES string grammar (ring digits, parentheses for branching)
  - Why needed here: Syntax circuits only make sense if you understand that ring closures require matching digits (C1...C1) and branches require balanced parentheses.
  - Quick check question: Given `CC(C)C`, which token(s) define the branch scope?

- Concept: Valence as remaining bonding capacity
  - Why needed here: The valence direction modulates whether the model predicts single vs. double/triple bonds; you need to know why carbon typically has valence 4.
  - Quick check question: If an aromatic carbon has formed two bonds, what is its remaining valence capacity?

- Concept: Sparse autoencoder decomposition (encoder-sparsity-decoder)
  - Why needed here: The paper's chemical features come from SAE latents; interpretability claims depend on understanding L1-sparsity and reconstruction fidelity.
  - Quick check question: Why would increasing dictionary size (4x → 16x) improve reconstruction but potentially reduce feature stability?

## Architecture Onboarding

- Component map: Input SMILES -> Tokenization -> L0-L5 residual updates -> SAE extraction -> Logits -> Next token. For syntax: L2H7 attention -> L1H2 output influence. For valence: L3 residual -> bond token logits.
- Critical path: Tokenized SMILES sequence flows through 6 transformer layers with residual connections, where syntax-critical heads (L1H2, L2H3, L2H7) implement grammar matching, and Layer 3 contains the valence representation. SAEs extract features from post-MLP activations for interpretability and downstream tasks.
- Design tradeoffs: Larger SAE dictionaries (16x) improve reconstruction and feature diversity but reduce augmentation robustness and increase compute. Early layers show high feature stability; deeper layers show path-dependence and lower cross-SAE recovery.
- Failure signatures: Ring circuit failure - invalid ring closures, unmatched digits (validity drops sharply if L1H2 ablated); Valence direction corruption - chemically implausible bond orders at decision tokens; SAE over-sparsity - validity <80% when reconstructing from sparse codes; dead neurons >30%.
- First 3 experiments: 1) Ablate L1H2 and L2H7 jointly; measure validity vs. single-head ablation to test redundancy. 2) Steering valence direction at increasing α; plot bond-type logit shifts and count invalid generations. 3) Train SAE with 4x/8x/16x expansion on L3; compare top-10 fragment-specific features via WSD scoring.

## Open Questions the Paper Calls Out

- Do the identified syntax circuits (e.g., ring/branch heads) persist in models trained on randomized SMILES or non-autoregressive architectures? The authors state that comparing models trained with randomized SMILES could reveal which behaviors "reflect chemical structure rather than representational artifacts."

- Is the linearly decodable valence direction a causal mechanism used by the model, or merely a correlational statistical pattern? While modulation worked, the Limitations section warns that "Linear probes may detect accessible statistical patterns rather than the model's actual computational pathway."

- How do syntax and valence circuits adapt when the model is fine-tuned for explicit property constraints (task conditioning)? Section A.2 asks, "how the identified circuits and residual features adapt under task conditioning" regarding bioactivity or physicochemical targets.

## Limitations

- The syntax circuits are identified through ablation studies that only test single-head ablations in isolation, potentially missing redundant or interconnected pathways.
- The valence direction experiment demonstrates statistical correlation but does not definitively prove it represents the model's actual computational mechanism.
- The feature stability analysis reveals that cross-SAE recovery rates vary significantly across layers, with deeper layers showing path-dependence that could indicate non-robust feature extraction.

## Confidence

- High Confidence: The syntax circuits for ring closure and branch balancing are well-supported by multiple ablation metrics showing dramatic validity drops and high pointer mass.
- Medium Confidence: The valence direction as a distributed linear representation shows statistically significant steering effects, but the causal mechanism claim is less certain.
- Low Confidence: The feature steering experiments show promising results but the effect sizes are modest and the interpretation needs more rigorous validation.

## Next Checks

1. **Circuit Redundancy Test**: Perform joint ablation of L2H7 and L1H2 (the ring closure pointer-writer pair) and compare validity retention to individual ablations. If validity remains above 70%, alternative circuit pathways exist that weren't identified in the single-head analysis.

2. **Valence Direction Causality Test**: Systematically vary the steering coefficient α at multiple decision points and measure not just bond type probabilities but also downstream molecular validity and chemical coherence. Plot the full distribution of bond orders at each α value to check for non-monotonic or chemically implausible shifts.

3. **SAE Feature Stability Cross-Validation**: Train SAEs on three independent molecular subsets using identical hyperparameters, then measure feature alignment via WSD scoring across all pairs. Focus on high-specificity features identified as "strong detectors" to see if they consistently align across runs.