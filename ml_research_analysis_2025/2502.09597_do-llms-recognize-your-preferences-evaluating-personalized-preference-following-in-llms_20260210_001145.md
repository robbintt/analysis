---
ver: rpa2
title: Do LLMs Recognize Your Preferences? Evaluating Personalized Preference Following
  in LLMs
arxiv_id: '2502.09597'
source_url: https://arxiv.org/abs/2502.09597
tags:
- preference
- claude
- user
- mistral
- preferences
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces PREF EVAL, a benchmark for evaluating LLMs\u2019\
  \ ability to infer, memorize, and follow user preferences in long-context conversational\
  \ settings. The benchmark includes 3,000 manually curated preference-query pairs\
  \ across 20 topics with explicit and implicit preference forms."
---

# Do LLMs Recognize Your Preferences? Evaluating Personalized Preference Following in LLMs

## Quick Facts
- **arXiv ID**: 2502.09597
- **Source URL**: https://arxiv.org/abs/2502.09597
- **Reference count**: 40
- **Key outcome**: LLMs achieve below 10% preference-following accuracy in zero-shot settings for conversations exceeding 10 turns, even with advanced prompting and retrieval methods.

## Executive Summary
This paper introduces PREF EVAL, a benchmark designed to evaluate large language models' ability to infer, memorize, and follow user preferences in long-context conversational settings. The benchmark consists of 3,000 manually curated preference-query pairs across 20 topics, featuring both explicit and implicit preference forms. It assesses both generation and classification tasks to measure preference-following capabilities. Experiments on 10 state-of-the-art LLMs reveal critical limitations in current models' ability to track and apply user preferences, with performance dropping below 10% accuracy for zero-shot settings in conversations exceeding 10 turns. Fine-tuning on PREF EVAL significantly improves performance and generalization to longer contexts, highlighting the potential for targeted improvements in personalized conversational agents.

## Method Summary
The paper introduces PREF EVAL, a benchmark for evaluating LLMs' ability to infer, memorize, and follow user preferences in long-context conversational settings. The benchmark includes 3,000 manually curated preference-query pairs across 20 topics with explicit and implicit preference forms. It assesses both generation and classification tasks. Experiments were conducted on 10 state-of-the-art LLMs to evaluate their preference-following capabilities. The evaluation specifically measures performance in zero-shot settings and after fine-tuning on the benchmark data. The benchmark reveals critical limitations in current LLMs' ability to maintain preference adherence over long conversations.

## Key Results
- LLMs achieve below 10% accuracy in zero-shot preference following for conversations exceeding 10 turns (~3k tokens)
- Fine-tuning on PREF EVAL significantly improves performance and generalization to longer contexts
- Advanced prompting and retrieval methods show limited effectiveness in improving preference-following accuracy

## Why This Works (Mechanism)
The paper identifies that current LLMs struggle with long-context preference tracking due to limitations in attention mechanisms and memory retention across extended conversations. The mechanism of preference following requires not only recognizing explicit stated preferences but also inferring implicit preferences from conversation history. The degradation in performance beyond 10 conversation turns suggests that LLMs lose the ability to maintain relevant context and apply it to subsequent queries. Fine-tuning appears to improve performance by reinforcing the model's ability to attend to preference-related information and maintain consistency across longer interactions.

## Foundational Learning
1. **Long-context understanding** - Why needed: Preference following requires maintaining context across extended conversations; Quick check: Can the model accurately track preferences over 10+ conversation turns?
2. **Implicit preference inference** - Why needed: Users often express preferences indirectly; Quick check: Can the model infer preferences from conversational cues rather than explicit statements?
3. **Memory retention in dialogue** - Why needed: Preferences must be remembered and applied consistently; Quick check: Does the model maintain preference adherence across multiple conversation turns?
4. **Fine-tuning for personalization** - Why needed: General models struggle with preference tasks; Quick check: Does fine-tuning on preference data significantly improve performance?
5. **Attention mechanism optimization** - Why needed: Long conversations require efficient attention allocation; Quick check: Can the model effectively attend to preference-relevant information in long contexts?
6. **Preference-conflict resolution** - Why needed: Conversations may contain conflicting preference signals; Quick check: How does the model handle and prioritize multiple preferences?

## Architecture Onboarding

**Component map**: Input conversation history → Preference extraction module → Context integration → Response generation → Preference adherence evaluation

**Critical path**: The critical path involves processing the conversation history to extract preferences, integrating this information with the current query context, and generating a response that adheres to the identified preferences while maintaining conversational coherence.

**Design tradeoffs**: The benchmark balances between controlled evaluation conditions and real-world applicability. Using manually curated data ensures quality but may not capture natural conversation complexity. The focus on preference following rather than factual accuracy allows for isolated evaluation but may miss important safety considerations.

**Failure signatures**: Models exhibit rapid degradation in preference adherence beyond 10 conversation turns, suggesting limitations in attention span and memory. Zero-shot performance below 10% indicates fundamental difficulties in preference inference without task-specific training. Even with advanced prompting and retrieval methods, models struggle to maintain preference consistency.

**3 first experiments**:
1. Evaluate model performance on conversations with explicitly stated versus implicitly expressed preferences
2. Test the impact of conversation length on preference adherence by varying the number of conversation turns
3. Compare zero-shot performance across different model families to identify architectural differences in preference following

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: Why does the introduction of multiple or conflicting preferences within a conversation improve adherence to the original preference, and does this generalize across all model architectures?
- **Basis in paper**: The authors state in Section 3.6 and Appendix A.16 that introducing conflicting preferences leads to improved adherence, which they "conjecture" is due to reinforced attention on user preferences or a "topic-reinforcement effect," noting some model dependence (e.g., Mistral 8x7b).
- **Why unresolved**: The paper identifies the phenomenon but does not isolate the specific attention mechanisms or internal states responsible, nor does it fully explain why some models do not exhibit the benefit.
- **What evidence would resolve it**: A comparative analysis of attention head visualizations and weight distributions across models that show the benefit versus those that do not, specifically looking for "reinforced attention" patterns in the preference regions.

### Open Question 2
- **Question**: How does the presence of noise and ambiguity in real-world user interactions affect preference-following performance compared to the curated synthetic interactions in PrefEval?
- **Basis in paper**: The Limitations section (A.1) states: "incorporating real user preferences in future work would help capture more nuanced aspects of user interactions."
- **Why unresolved**: The current benchmark relies on manually curated preference-query pairs and synthetic distractors, which may lack the implicit contradictions and ambiguity found in natural human-chatbot history.
- **What evidence would resolve it**: Evaluation of SOTA LLMs on a dataset derived from authentic, uncurated chat logs where user preferences must be inferred from messy, multi-turn interactions.

### Open Question 3
- **Question**: What is the relationship between strict preference adherence and the factual accuracy of the generated content?
- **Basis in paper**: The authors explicitly state in Appendix A.1 that they aim to evaluate preference following "rather than verifying the factual accuracy of the recommendations," treating fact-checking as a dimension beyond the scope of the work.
- **Why unresolved**: While the paper measures adherence, it leaves open the question of whether prioritizing a specific user preference (e.g., "avoiding gluten") increases the likelihood of hallucinating specific details (e.g., ingredients) or providing safe/accurate advice.
- **What evidence would resolve it**: A dual-metric evaluation on the PrefEval dataset where responses are scored for both preference alignment and factual accuracy using a ground-truth knowledge base.

## Limitations
- Benchmark relies on manually curated preference-query pairs that may not capture real-world conversation complexity and natural ambiguity
- Evaluation focuses on English language interactions, potentially limiting generalizability to other languages and cultural contexts
- Performance gap between zero-shot and fine-tuned settings indicates current models struggle with few-shot learning in preference inference tasks

## Confidence
**High confidence**: The benchmark construction methodology and evaluation results for the 10 tested LLMs are reliable, given the systematic approach to creating preference-query pairs and the use of multiple models for validation.

**Medium confidence**: The generalizability of findings to other domains and preference types, as the benchmark covers 20 specific topics but may not represent all possible preference domains users encounter.

**Medium confidence**: The effectiveness of proposed fine-tuning approaches, as while improvements are demonstrated, the long-term generalization and potential for overfitting to the benchmark data requires further validation.

## Next Checks
1. **Cross-lingual validation**: Evaluate the benchmark with multilingual models and non-English preference expressions to assess cultural and linguistic generalizability.
2. **Real-world deployment testing**: Implement the benchmarked models in actual conversational systems over extended periods to measure practical preference-following performance versus controlled test conditions.
3. **Adversarial preference testing**: Develop and evaluate models using deliberately ambiguous or contradictory preference expressions to test robustness of preference inference mechanisms.