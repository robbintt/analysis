---
ver: rpa2
title: 'UniME-V2: MLLM-as-a-Judge for Universal Multimodal Embedding Learning'
arxiv_id: '2510.13515'
source_url: https://arxiv.org/abs/2510.13515
tags:
- unime-v2
- hard
- retrieval
- semantic
- candidates
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes UniME-V2, a universal multimodal embedding
  model that leverages advanced MLLM understanding capabilities to improve representation
  learning. The key innovation is the MLLM-as-a-Judge mechanism, which uses semantic
  matching scores to guide hard negative mining and serves as soft labels to capture
  semantic differences among candidates.
---

# UniME-V2: MLLM-as-a-Judge for Universal Multimodal Embedding Learning

## Quick Facts
- **arXiv ID**: 2510.13515
- **Source URL**: https://arxiv.org/abs/2510.13515
- **Reference count**: 14
- **Primary result**: State-of-the-art 68.0 average score on MMEB benchmark, outperforming previous best by 0.6 points

## Executive Summary
UniME-V2 introduces a universal multimodal embedding model that leverages MLLM-as-a-Judge mechanism to significantly improve representation learning. The core innovation uses semantic matching scores from a judge MLLM to guide hard negative mining and serve as soft labels, addressing limitations of rigid one-to-one mapping and insufficient negative sample diversity in previous methods. The framework achieves state-of-the-art performance on MMEB benchmark and demonstrates superior results across multiple retrieval tasks with improvements of 5.3-9.7% over baseline models.

## Method Summary
UniME-V2 uses VLM2Vec to retrieve top-50 candidates as potential hard negatives, then employs Qwen2.5VL-7B as a judge with Yes/No prompts to compute semantic matching scores. False negatives are filtered using a threshold, and cyclical sampling selects k=8 hard negatives. The model is trained with JS-divergence loss between similarity matrix P and semantic score matrix Q, using LoRA fine-tuning (rank=16) on MMEB data. The framework also introduces UniME-V2-Reranker, trained on mined hard negatives through joint pairwise-listwise optimization for improved retrieval precision.

## Key Results
- Achieves 68.0 average score on MMEB benchmark (vs 67.4 previous best)
- Improves short/long caption retrieval by 5.3-9.7% over baseline models
- UniME-V2-Reranker outperforms LamRA with 7.4% gain on compositional retrieval using fewer training samples

## Why This Works (Mechanism)

### Mechanism 1
**Claim**: MLLM-as-a-Judge enables identification of diverse, high-quality hard negatives while filtering false negatives that would otherwise corrupt training.
**Mechanism**: Global retrieval constructs candidate pool → MLLM generates soft semantic matching scores → threshold filtering removes false negatives → cyclical sampling preserves diversity.
**Core assumption**: MLLM judge has sufficient semantic understanding to reliably distinguish partial matches from true negatives.
**Evidence anchors**: Abstract states MLLM-as-a-Judge "assesses semantic alignment" and "mitigates impact of false negatives"; Section 3.1 specifies threshold α = σq,ct - β; FALCON validates false negatives degrade embedding spaces.

### Mechanism 2
**Claim**: Soft semantic matching scores as supervision relax rigid binary constraints, enabling the model to learn graded semantic distinctions.
**Mechanism**: Compute relation score matrix P from embeddings → compute semantic score matrix Q from MLLM judgments → minimize symmetric KL divergence between P and Q.
**Core assumption**: Softmax-normalized semantic scores capture meaningful similarity gradients that embeddings should reproduce.
**Evidence anchors**: Abstract states semantic scores "mitigate rigid one-to-one mapping constraint"; Section 3.2 explains aligning similarity matrix with soft semantic scores; No directly comparable soft-label alignment method found in corpus.

### Mechanism 3
**Claim**: Joint pairwise-listwise reranking training improves final retrieval precision by teaching the model both binary discrimination and relative ranking.
**Mechanism**: Pairwise loss trains YES/NO classification vs. hardest negative → listwise loss trains position prediction among top-x candidates → combined loss L = Lpair + Llist.
**Core assumption**: Hard negatives mined by embedding model transfer effectively to reranker training.
**Evidence anchors**: Abstract describes "joint pairwise and listwise optimization approach"; Table 3 shows UniME-V2-Reranker outperforms LamRA with fewer samples; Limited corpus evidence for this combination in multimodal reranking.

## Foundational Learning

**Concept: Hard Negative Mining**
- **Why needed**: Standard in-batch negatives often lack semantic difficulty, yielding weak discriminative gradients.
- **Quick check**: Can you explain why a visually similar but semantically incorrect candidate is more informative than a random dissimilar one?

**Concept: Distribution Alignment via Divergence Loss**
- **Why needed**: Binary labels ignore semantic nuance; JS-Divergence aligns learned similarity distributions with soft semantic targets.
- **Quick check**: Why is JS-Divergence preferred over KL-Divergence for this alignment task? (Hint: symmetry and gradient stability)

**Concept: Cross-Modal Embedding Spaces**
- **Why needed**: UniME-V2 unifies images, text, and interleaved data into one embedding space for retrieval across modalities.
- **Quick check**: What modality gap problem does Fig. 4 illustrate, and how does MLLM-based embedding help reduce it?

## Architecture Onboarding

**Component map**:
```
Training Pipeline: VLM2Vec → Global Retrieval (top-50) → MLLM Judge (Qwen2.5-VL) → Semantic Scores → Hard Negative Sampling → UniME-V2 → JS-Divergence Loss
Inference Pipeline: UniME-V2 Embedding → Cosine Retrieval (top-10) → UniME-V2-Reranker → Final Ranking
```

**Critical path**:
1. MLLM judge quality directly controls hard negative quality → embedding discriminability
2. Temperature τ in JS-Divergence loss determines gradient sharpness (τ=0.02)
3. Number of hard negatives k balances diversity vs. noise (k=8 optimal)

**Design tradeoffs**:
- Global retrieval over in-batch: Higher computational cost for better negative diversity
- MLLM judge over embedding similarity: More accurate semantic assessment but adds inference overhead
- LoRA (rank=16) over full fine-tuning: Memory efficient but may limit representation capacity

**Failure signatures**:
- OOD performance drops → MLLM judge may be miscalibrated on unseen domains
- Compositional retrieval fails → hard negatives insufficiently semantically similar
- Training loss plateaus early → check semantic score distribution (may lack discriminability)

**First 3 experiments**:
1. Reproduce ablation (Table 4): Train with hard negatives only vs. hard negatives + soft labels on Qwen2-VL-2B.
2. Judge model swap (Table 5): Compare Qwen2.5-VL-7B vs. InternVL3-8B as judge.
3. Temperature sweep (Table 10): Validate τ∈{0.01, 0.02, 0.03} on held-out subset.

## Open Questions the Paper Calls Out

**Open Question 1**: Does UniME-V2 performance scale linearly with MLLM judge capability, or is there a sufficiency threshold? The paper tests only three specific judge models without establishing scaling laws or upper bounds for this dependency.

**Open Question 2**: Is text-to-image retrieval performance saturation due to training data imbalance or inherent limitations of soft semantic matching for short captions? The paper attributes it to limited text-to-image data and short caption semantic insufficiency but doesn't isolate these factors experimentally.

**Open Question 3**: How sensitive is false negative filtering to hyperparameters (α and β) on heterogeneous or out-of-distribution data? The paper relies on fixed thresholds without ablation studies on their robustness across different distributions.

**Open Question 4**: Does performance decline with 10 negatives result from easier negatives or increased optimization complexity? The paper attributes it to easier negatives but doesn't rule out whether JS-Divergence becomes harder to optimize with more candidates.

## Limitations

- **Computational overhead**: Substantial inference cost for global retrieval plus MLLM scoring during data preparation limits scalability
- **Judge model dependency**: Performance bottleneck if MLLM judge is miscalibrated or domain-mismatched
- **Alignment assumption**: No ablation studies confirm semantic space learned by judge aligns with target embedding space

## Confidence

**High Confidence**: Technical implementation of MLLM-as-a-Judge mechanism is clearly specified and reproducible; reported MMEB improvements (68.0 vs 67.4) and specific retrieval gains (5.3-9.7%) are supported by experimental results.

**Medium Confidence**: Claim that soft semantic matching significantly outperforms rigid binary supervision needs more scrutiny; individual contributions of hard negatives vs. soft labels aren't fully isolated; temperature hyperparameter τ=0.02 presented as optimal without systematic sensitivity analysis.

**Low Confidence**: Generalizability claim across diverse tasks limited by evaluation setup; lacks detailed breakdowns showing which dataset types benefit most; reranking superiority (7.4% gain) based on single comparison without exploring alternative architectures.

## Next Checks

1. **Judge Model Robustness Test**: Replace Qwen2.5-VL-7B with InternVL3-8B and LLaVA-Next-34B to measure performance degradation and quantify sensitivity to judge quality.

2. **Temperature Sensitivity Analysis**: Conduct systematic sweep of τ values (0.005, 0.01, 0.02, 0.03, 0.05) measuring training stability, convergence speed, and final performance to determine if τ=0.02 is genuinely optimal.

3. **False Negative Impact Study**: Intentionally corrupt negative mining by reducing filtering threshold β or adding synthetic false negatives to measure degradation in embedding quality and validate false negative filtering importance.