---
ver: rpa2
title: Hierarchical Bracketing Encodings Work for Dependency Graphs
arxiv_id: '2509.09388'
source_url: https://arxiv.org/abs/2509.09388
tags:
- table
- parsing
- bracketing
- performance
- computational
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ''
---

# Hierarchical Bracketing Encodings Work for Dependency Graphs

## Quick Facts
- arXiv ID: 2509.09388
- Source URL: https://arxiv.org/abs/2509.09388
- Reference count: 40
- Primary result: HB encoding achieves best average exact match (38.50) across multilingual dependency graph parsing benchmark

## Executive Summary
This paper introduces Hierarchical Bracketing (HB), a novel sequence labeling encoding for dependency graph parsing that guarantees 100% coverage of arbitrary graphs including reentrancies, cycles, and empty nodes. HB leverages rope cover decomposition to partition arcs into structural and auxiliary sets, encoding them with balanced superbrackets and single brackets respectively. The method substantially reduces label space (379 vs 982 for B3) while improving exact match performance (38.50 avg vs Biaffine's 29.33), though labeled F1 remains slightly lower than state-of-the-art biaffine parsers.

## Method Summary
HB encodes dependency graphs as token sequences using rope cover decomposition to identify structural arcs and their auxiliary dependencies. Each structural arc gets balanced superbrackets at both endpoints, while auxiliary arcs encode only their non-leant position with single brackets. The encoding is parsed left-to-right using a stack-based decoder that handles indexed brackets for crossing arcs. A postprocessing step attaches unmatched brackets to a dummy root node. The model uses pretrained encoders (XLM-RoBERTa or XLNet) with two FFNs for label and relation prediction, trained with AdamW for 100 epochs.

## Key Results
- HB achieves best average exact match (38.50) across 17 languages and 5 datasets
- HB reduces label space by 61% compared to B3 encoding (379 vs 982 average labels)
- HB guarantees 100% theoretical coverage of arbitrary dependency graphs
- HB shows strong correlation (88.1%, p<0.001) between label distribution balance and exact match performance

## Why This Works (Mechanism)

### Mechanism 1
Rope cover decomposition enables lossless graph linearization with guaranteed 100% theoretical coverage. Given graph G=(W,A), the unique proper rope cover R⊆A partitions arcs such that every auxiliary arc (A\R) "leans on" at least one structural arc in R. Arcs are grouped into structural sets (one structural arc + its leaning arcs), each independently encodable with balanced brackets. The leaning relation ensures hierarchical organization is always possible.

### Mechanism 2
Hierarchical bracketing reduces label space by using superbrackets for structural arcs and single brackets for auxiliary arcs. Structural arcs get balanced superbrackets (e.g., `/////`, `>>>>>`) at both endpoints, while auxiliary arcs encode only their non-leant position with a single bracket, halving symbol requirements per arc. The neural tagger learns to distinguish superbracket contexts from single-bracket contexts during decoding.

### Mechanism 3
More balanced label distributions improve exact match accuracy. HB's p₀.₅ metric (2.03% of labels cover 50% of occurrences) is higher than B3 (0.96%), reducing rare-label prediction errors. The paper shows 88.1% correlation (p<0.001) between p₀.₅ and LM score, suggesting compressed label spaces improve exact match performance.

## Foundational Learning

- **Dependency graphs vs. trees**: Why needed here - Graphs permit reentrancies (node with multiple heads), cycles, and disconnected components - all impossible in trees and requiring special encoding handling. Quick check: Can a token have two different heads in this formalism? If yes, you're parsing graphs, not trees.

- **Bracketing/sequence labeling for parsing**: Why needed here - The entire method frames parsing as assigning one label per token, eliminating complex decoder architectures. Quick check: How does the model predict edges between non-adjacent tokens without explicit edge prediction? (Answer: Brackets encode span boundaries.)

- **Stack-based decoding**: Why needed here - Decoding uses a left-to-right stack to match opening/closing brackets, with special handling for indexed brackets that skip structural arcs. Quick check: What happens when a closing bracket arrives but the stack is empty? (Answer: Postprocessing attaches to dummy root w₀.)

## Architecture Onboarding

- Component map: Input -> Pretrained encoder (XLM-RoBERTa/XLNet) -> Label FFN -> Decoding -> Postprocessing -> Arc set -> Relation FFN -> Final graph

- Critical path: Token embeddings → Label prediction → Stack-based bracket matching with index handling → Postprocessing → Arc set construction → Relation classification → Final dependency graph

- Design tradeoffs:
  - HB optimizes for exact match (LM: 38.50 avg) at slight LF cost (89.19 vs. Biaffine's 90.45)
  - HB guarantees 100% coverage without hyperparameter k; Bₖ encodings require manual k selection and fail beyond k planes
  - HB averages 379 labels vs. 982 for B3 (Table 4)

- Failure signatures:
  - Low W (well-formed ratio): Postprocessor correcting many invalid bracket sequences → encoder not learning bracket constraints
  - High unseen labels (Table 4): Consider vocabulary expansion or smoothing
  - LF >> LM gap: Individual arc predictions correct but structural coherence failing

- First 3 experiments:
  1. Baseline reproduction: Run HB on English-DM (enDM), verify LF≈94.52, LM≈53.05 matches Table 1
  2. Ablate postprocessing: Disable correction step, measure W degradation—expect significant drop per Tables 5-21 W columns
  3. Label distribution analysis: Compute p₀.₅ for your target treebank; if <1.5, HB may underperform compared to Bₖ alternatives

## Open Questions the Paper Calls Out

- Can hierarchical bracketing encodings be effectively adapted to other graph-based NLP tasks beyond semantic and enhanced dependency parsing, such as structured sentiment analysis or emotion-cause analysis? (The paper focuses only on dependency graphs.)

- Under what conditions does the unbounded nature of the hierarchical encoding become a practical limitation? (The claim of practical irrelevance is based only on tested datasets.)

- Can modifications to the encoding or training procedure close the LF performance gap between HB and biaffine parsers while preserving HB's exact match advantage? (The paper establishes LM advantage but doesn't investigate LF improvements.)

## Limitations

- The rope cover decomposition algorithm implementation is underspecified, creating potential reproducibility gaps for complex graph structures
- The correlation between label distribution balance and exact match performance lacks corpus-level validation and theoretical grounding
- The practical significance of LM improvements versus LF trade-offs remains unclear for downstream applications

## Confidence

- **High confidence**: HB's theoretical coverage guarantee (100% lossless encoding for any dependency graph) and its label space reduction mechanism (superbrackets vs single brackets)
- **Medium confidence**: HB's superior exact match performance and correlation with label distribution balance
- **Low confidence**: The completeness of the rope cover implementation details and index computation algorithm

## Next Checks

1. **Rope cover decomposition validation**: Implement the rope cover algorithm independently and verify that the structural arc partition is indeed unique for all test graphs

2. **Index computation verification**: For crossing arcs, validate that the index assignment correctly skips the appropriate number of structural arcs using graphs with known crossing patterns

3. **Label distribution analysis replication**: Compute p₀.₅ values for all encoding methods across the benchmark and verify the claimed 88.1% correlation with exact match scores