---
ver: rpa2
title: Hateful Person or Hateful Model? Investigating the Role of Personas in Hate
  Speech Detection by Large Language Models
arxiv_id: '2506.08593'
source_url: https://arxiv.org/abs/2506.08593
tags:
- figure
- dataset
- distribution
- logit
- difference
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates how MBTI-based personas influence hate
  speech detection in both humans and large language models (LLMs). A human survey
  confirms that MBTI traits, particularly the Feeling-Thinking dimension, significantly
  affect annotation behavior.
---

# Hateful Person or Hateful Model? Investigating the Role of Personas in Hate Speech Detection by Large Language Models

## Quick Facts
- arXiv ID: 2506.08593
- Source URL: https://arxiv.org/abs/2506.08593
- Reference count: 40
- Primary result: MBTI-based personas significantly influence LLM hate speech detection, with Thinking and Judging personas yielding higher confidence predictions and greater inconsistency across models.

## Executive Summary
This paper investigates how MBTI-based personas influence hate speech detection in both humans and large language models (LLMs). A human survey confirms that MBTI traits, particularly the Feeling-Thinking dimension, significantly affect annotation behavior. Extending this to LLMs, the study prompts four open-source models with different MBTI personas across three hate speech datasets. Results show substantial variation in predictions across personas, with inconsistencies in both final labels and internal logit distributions. Thinking and Judging personas consistently yield higher confidence in hate speech predictions. Notably, LLMs exhibit more exaggerated persona-driven effects than humans, amplifying certain traits. The findings highlight the sensitivity of LLMs to persona conditioning and the importance of careful persona design in subjective NLP tasks like hate speech detection.

## Method Summary
The study employs a persona-based prompting approach where four 7-8B parameter LLMs (Llama-3.1-8B-Instruct, Ministral-8B-Instruct, Falcon3-Mamba-7B-Instruct, Qwen2.5-7B-Instruct) are conditioned with MBTI personality descriptions as system prompts. Each model processes hate speech samples from three datasets (CREHate, HateXplain, Davidson) across all 16 MBTI personas. The prompts follow a structured format combining personality descriptors with hate speech classification tasks. Logit analysis reveals systematic differences in model confidence, with Thinking and Judging personas producing stronger "yes" logits. Human validation involves a survey of 293 participants who annotate hate speech samples, showing personality influences similar to LLM behavior.

## Key Results
- LLMs show significant variation in hate speech predictions across MBTI personas, with inter-persona disagreement reaching 0.63 on Davidson dataset
- Thinking personas consistently yield higher "yes" logits than Feeling personas across all models
- Logit-level analysis reveals persona conditioning affects model confidence, with Thinking and Judging personas producing stronger "yes" logits
- LLMs amplify persona effects beyond natural human variation, showing wider dispersion in PCA visualization compared to human MBTI types

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Persona conditioning via system prompts shifts model decision boundaries in hate speech classification.
- Mechanism: Personality descriptions in prompts activate trait-associated reasoning patterns, causing models to weight input features differently when computing "Yes"/"No" logits.
- Core assumption: LLMs encode associations between personality descriptors and reasoning styles from pre-training data.
- Evidence anchors:
  - [abstract] "Logit-level analysis reveals that personality conditioning affects model confidence, with Thinking and Judging personas producing stronger 'yes' logits."
  - [section 6.3] Logit difference distributions show ESFP persona consistently >0 (yes-biased) while ENFP is distributed around zero with slight negative skew.
  - [corpus] Related paper "Persona Prompting as a Lens on LLM Social Reasoning" examines persona effects on rationale quality, but no direct corpus evidence on the logit-shifting mechanism.
- Break condition: If models lack pre-trained associations between personality language and reasoning patterns, persona prompts would produce random variation rather than systematic shifts.

### Mechanism 2
- Claim: Thinking (T) and Judging (J) MBTI dimensions produce higher confidence "yes" predictions across models.
- Mechanism: Analytical/structured trait descriptions may activate more decisive classification behavior, reducing uncertainty in boundary cases.
- Core assumption: The T/F and J/P dichotomies map onto genuine reasoning-style differences that models can simulate.
- Evidence anchors:
  - [abstract] "Thinking and Judging personas producing stronger 'yes' logits."
  - [section 6.4, Figure 7] "Thinking personas consistently yield higher 'yes' logits than feeling ones across all models. The effect is especially pronounced in Qwen and Llama."
  - [corpus] No corpus papers directly replicate this T/J logit-confidence finding.
- Break condition: If the effect were model-specific noise, it would not replicate across four different LLM architectures.

### Mechanism 3
- Claim: LLMs amplify persona effects beyond natural human personality variation.
- Mechanism: Models may overfit to stereotypical trait descriptions, lacking the contextual moderation humans naturally apply when personality interacts with task demands.
- Core assumption: The amplification reflects prompt sensitivity rather than faithful personality simulation.
- Evidence anchors:
  - [abstract] "Compared to humans, LLMs are more susceptible to persona framing, amplifying certain behavioral traits."
  - [section 6.5] PCA visualization shows human MBTI types form compact cluster while LLM personas are widely dispersed, with ENFP/ESFP/ISFP as clear outliers.
  - [corpus] Limited corpus validation; amplification effect not directly addressed in neighbor papers.
- Break condition: If LLMs faithfully simulated human personality variation, PCA distributions would show comparable spread.

## Foundational Learning

- Concept: **Logit Analysis for Classification Confidence**
  - Why needed here: Binary predictions hide confidence variation; logit differences (logit_yes âˆ’ logit_no) reveal systematic biases in model decision-making.
  - Quick check question: If logit difference is always positive for persona A and centered at zero for persona B, what does this imply about their classification behavior?

- Concept: **Persona-Based Prompting**
  - Why needed here: The intervention being studied; understanding how system prompt personality descriptions condition model outputs.
  - Quick check question: What prompt structure would you use to test whether "analytical" vs. "empathetic" traits affect hate speech detection?

- Concept: **Human Label Variation in Subjective Tasks**
  - Why needed here: Provides theoretical grounding for why personality affects annotation; hate speech is inherently subjective.
  - Quick check question: Why might Feeling types label more content as hateful than Thinking types in the human survey (Section 3)?

## Architecture Onboarding

- Component map:
  System prompt (persona + description) -> User prompt (text + question) -> Model inference -> Binary output constrained to {Yes, No}
  Logit extraction for both tokens enables confidence analysis

- Critical path:
  1. Format system prompt: "You are [Persona] and your personality is described as [Description]"
  2. Format user prompt: "[Text] Is this text hate speech? Only answer with one word, yes or no"
  3. Set temperature=0 for deterministic outputs
  4. Extract logits for "Yes" and "No" tokens
  5. Compute logit difference for tendency analysis
  6. Compare against ground truth for inconsistency rate

- Design tradeoffs:
  * MBTI selected for interpretability but has limited psychometric validity (acknowledged in paper)
  * 7-8B parameter models tested; unknown if effects generalize to larger models
  * English-only; cross-lingual generalization unknown
  * Binary classification simplifies nuanced hate speech spectrum
  * Human survey sample (N=293) skewed toward students/researchers

- Failure signatures:
  * Inconsistency with ground truth exceeding 70% (Ministral on CREHate, multiple models on Davidson)
  * Inter-persona disagreement >0.5 (ENFP vs. ESFP at 0.63 on Davidson)
  * Logit difference distributions with zero variance (all positive or all negative)
  * Missing or malformed outputs if prompt format varies

- First 3 experiments:
  1. Single-model sanity check: Run Qwen-2.5-7B-Instruct on 50 Davidson samples with ENFP vs. ESFP personas; verify logit distributions differ systematically.
  2. Neutral baseline: Test whether prompt without persona description produces intermediate behavior between T and F extremes.
  3. Cross-dataset consistency: Verify T > F logit pattern replicates across HateXplain and CREHate, not just Davidson.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do alternative psychological frameworks (e.g., Big Five) produce different or more stable persona effects in LLM-based hate speech annotation compared to MBTI?
- Basis in paper: [explicit] The authors state in the Limitations section that they "focus exclusively on the MBTI framework" and suggest "future work could explore alternative psychological theories, such as the Big Five, to provide different perspectives."
- Why unresolved: Only MBTI was tested, and its psychometric validity is noted as limited, leaving open whether more empirically grounded frameworks yield more predictable or controllable LLM behavior.
- What evidence would resolve it: Systematic comparison of persona conditioning using Big Five traits vs. MBTI across the same hate speech datasets and models, measuring both label consistency and alignment with human annotation patterns.

### Open Question 2
- Question: Does the persona amplification effect observed in 7-8B parameter models persist, diminish, or intensify in larger-scale LLMs?
- Basis in paper: [explicit] The authors note in Limitations that they "evaluate only four open-source LLMs, each with approximately 7 to 8 billion parameters. Future work should investigate whether similar effects occur in larger models."
- Why unresolved: Larger models may have different instruction-following behaviors or more robust reasoning that could either amplify or reduce susceptibility to persona framing.
- What evidence would resolve it: Replication of the same experimental protocol with models of varying scales (e.g., 13B, 70B, 175B+), comparing intra-persona disagreement and human alignment metrics.

### Open Question 3
- Question: How do persona-driven annotation effects transfer across languages and cultural contexts in hate speech detection?
- Basis in paper: [explicit] The Limitations section states the "investigation is restricted to English-language data. Cross-lingual studies are needed to examine how persona effects and perceptions of hate speech vary across different cultural and linguistic contexts."
- Why unresolved: Hate speech definitions and personality trait interpretations are culturally embedded; the observed MBTI effects may not generalize beyond English-speaking populations.
- What evidence would resolve it: Multilingual experiments using datasets like CREHate with annotators from non-English-speaking countries, testing whether persona conditioning produces comparable patterns of disagreement and model behavior.

### Open Question 4
- Question: Can principled methods be developed to control or neutralize persona-induced bias in LLM annotation pipelines while preserving legitimate interpretive diversity?
- Basis in paper: [explicit] The conclusion states that "future work could explore how to better understand and control the influence of personas to ensure fairness, consistency, and reliability in subjective tasks."
- Why unresolved: The paper demonstrates that personas cause variation but does not propose techniques to either mitigate unwanted bias or deliberately harness diversity for pluralistic annotation.
- What evidence would resolve it: Development and testing of debiasing techniques (e.g., ensemble methods, persona-agnostic prompting, calibration layers) that reduce persona-driven inconsistency while maintaining task performance.

## Limitations
- Only MBTI framework tested, with acknowledged limited psychometric validity and use of descriptions from commercial website
- Limited to 7-8B parameter models; unknown if effects generalize to larger models
- English-only datasets; cross-lingual and cultural generalizability unknown
- Human survey sample skewed toward students and researchers, potentially limiting representativeness

## Confidence
**High Confidence**: LLMs show systematic variation in hate speech predictions across different MBTI personas, with observable differences in both final outputs and internal logit distributions. The logit difference analysis across four models consistently shows T > F and J > P patterns.

**Medium Confidence**: Thinking and Judging personas produce higher confidence "yes" predictions for hate speech. While the pattern is consistent across models, the mechanism (whether due to trait associations or model-specific quirks) remains unclear without ablation studies.

**Low Confidence**: The claim that LLMs amplify persona effects beyond natural human variation. The PCA visualization shows different distributions, but without quantitative comparison of variance or effect sizes, this remains suggestive rather than demonstrated. The amplification mechanism is speculative.

## Next Checks
1. **Ablation Study**: Test whether the personality description component specifically drives the effects by comparing three prompt versions: (a) full persona prompt, (b) description-only prompt, and (c) persona-only prompt (without description). This would isolate whether the narrative framing or personality labels matter most.

2. **Cross-Model Consistency with Larger Models**: Test the same persona conditioning protocol on 70B+ parameter models (e.g., Llama-3.1-70B, GPT-4) to verify whether the T > F and J > P logit patterns persist at scale, or whether they represent smaller-model artifacts.

3. **Neutral Baseline Comparison**: Run a neutral persona condition (no personality description, just standard system prompt) across all datasets to establish whether the observed effects represent true shifts from baseline behavior or merely different bias patterns. Compare variance in predictions across all personas versus the neutral condition.