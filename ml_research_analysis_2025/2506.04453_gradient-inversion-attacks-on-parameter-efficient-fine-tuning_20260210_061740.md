---
ver: rpa2
title: Gradient Inversion Attacks on Parameter-Efficient Fine-Tuning
arxiv_id: '2506.04453'
source_url: https://arxiv.org/abs/2506.04453
tags:
- adapter
- patches
- images
- layer
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work demonstrates the first successful gradient inversion
  attack on parameter-efficient fine-tuning (PEFT) in federated learning, targeting
  adapter-based PEFT mechanisms. By maliciously designing both the pretrained model
  and adapter modules, the attacker can reconstruct local fine-tuning data from adapter
  gradients alone, bypassing the typical privacy assumption that frozen backbone models
  protect sensitive information.
---

# Gradient Inversion Attacks on Parameter-Efficient Fine-Tuning

## Quick Facts
- arXiv ID: 2506.04453
- Source URL: https://arxiv.org/abs/2506.04453
- Authors: Hasin Us Sami; Swapneel Sen; Amit K. Roy-Chowdhury; Srikanth V. Krishnamurthy; Basak Guler
- Reference count: 40
- Primary result: First successful gradient inversion attack on parameter-efficient fine-tuning (PEFT) in federated learning, recovering up to 85.9% of image patches

## Executive Summary
This work demonstrates the first successful gradient inversion attack on parameter-efficient fine-tuning (PEFT) in federated learning. By maliciously designing both the pretrained Vision Transformer backbone and adapter modules, an attacker can reconstruct local fine-tuning data from adapter gradients alone. The attack bypasses the typical privacy assumption that frozen backbone models protect sensitive information, showing that PEFT does not inherently provide privacy protection. Extensive experiments on CIFAR-10, CIFAR-100, TinyImageNet, and ImageNet demonstrate high-fidelity reconstruction even with small adapter dimensions or over multiple training rounds.

## Method Summary
The attack operates by poisoning a pretrained ViT backbone to act as an identity mapping while designing adapter modules with specific weight and bias configurations. The attacker sets LayerNorm weights to match input standard deviation, MSA projection matrices to identity, and MLP layers with offset biases to preserve raw data information. Adapter gradients are then extracted using a gradient ratio formula that directly reveals image patch embeddings. These embeddings are inverted back to patches using pseudoinverse calculations. The attack works even with limited adapter capacity by distributing patch recovery across multiple adapter layers or sequential training rounds.

## Key Results
- Achieves up to 85.9% patch recovery rate on CIFAR-100 with r=64 adapter dimension
- LPIPS scores as low as 0.08 indicate high reconstruction fidelity
- Successful reconstruction with small adapter dimensions (r=8) over multiple rounds
- Maintains effectiveness across CIFAR-10, CIFAR-100, TinyImageNet, and ImageNet datasets
- Works even when gradient pruning and Gaussian noise defenses are applied

## Why This Works (Mechanism)

### Mechanism 1: Identity-Mapped Backbone Poisoning
The attacker designs the pretrained ViT backbone to pass input embeddings through frozen layers essentially unchanged by setting LayerNorm weights to match input standard deviation, MSA projection matrices to identity, and MLP layers with offset biases to bypass GELU attenuation. This ensures position-encoded patch embeddings propagate with minimal distortion.

### Mechanism 2: Adapter Down-Projection Gradient Leakage
Adapter gradients encode recoverable image patches when weights are set to position encoding vectors and biases are calibrated to isolate individual patches. The gradient ratio formula directly reveals the embedding by exploiting the linear relationship between gradients and inputs in the adapter layers.

### Mechanism 3: Multi-Layer and Multi-Round Accumulation
Limited adapter capacity is circumvented by distributing patch recovery across multiple adapter layers in one ViT encoder and/or across sequential training rounds. Each adapter layer targets different patch positions, and when capacity is insufficient, the server modifies global adapter parameters between rounds to target different statistical intervals.

## Foundational Learning

- **Vision Transformer (ViT) Patch Embedding Pipeline**: Understanding how images become patch embeddings → position-encoded vectors → layer-transformed representations is essential to see where information "leaks" into gradients.
  - Quick check: Given a 32×32 image with patch size 16×16, how many patch embeddings are generated (excluding class token)?

- **Gradient-Input Coupling in Linear Layers**: The attack's core insight—∂L/∂w = (1/B)Σ ∂L/∂v · x—means gradients contain weighted input information, which can directly reveal inputs when backprop isolates single inputs.
  - Quick check: For a linear layer y = Wx + b, what does the gradient ∂L/∂W tell you about the relationship between input x and upstream gradient?

- **Federated Learning Gradient Exchange**: Understanding what information gradients theoretically contain (vs. what's typically assumed safe) clarifies the attack surface in federated learning settings.
  - Quick check: In FedAvg, why might sharing only adapter gradients (not backbone gradients) have been assumed more privacy-preserving?

## Architecture Onboarding

- Component map:
Input Image → Patch Embedding (E) → Position Encoding (E_pos) → [LN1 → MSA → Adapter1 → LN2 → MLP → Adapter2] × N_encoders → Classification Head

- Critical path:
  1. Server poisons pretrained model (one-time, before training)
  2. Server sends malicious adapter parameters each round
  3. Client computes loss, backpropagates, sends adapter gradients
  4. Server extracts embeddings via gradient ratio (Equation 24)
  5. Server inverts embeddings to patches via pseudoinverse (Equation 25)

- Design tradeoffs:
  - Larger r → faster recovery but more detectable (non-standard weights)
  - Smaller σ in position encoding → stealthier but lower patch filtering precision
  - More adapter layers used → higher recovery rate but requires deeper ViT

- Failure signatures:
  - All recovered patches are identical → bias interval design failed
  - Zero gradients in adapter → up-projection weights not set to small non-zero value
  - Only position t recovered across all images → other position weights not diversified
  - LPIPS > 0.5 with noise defense → increase adapter dimension or reduce noise tolerance

- First 3 experiments:
  1. Reproduce CIFAR-100 reconstruction with r=64, batch=32: Verify 85%+ patch recovery, compare LPIPS/SSIM to Table 1
  2. Ablate adapter layer count: Test with 1, 3, 5 adapter layers; plot patches recovered vs. layers
  3. Test defense robustness: Add Gaussian noise (σ=1, 2, 3) to gradients; measure LPIPS degradation and identify break-even point where reconstruction fails

## Open Questions the Paper Calls Out
None

## Limitations
- The attack fundamentally depends on user trust in the server-provided model without auditing
- Bias interval design requires accurate estimation of patch statistics from proxy data that may not generalize
- The paper doesn't fully address practical detection mechanisms users might employ
- Stealthiness claims assume users won't detect identity-mapped backbones without empirical validation

## Confidence
- **High confidence**: The core gradient inversion mechanism is mathematically sound and experimentally validated across multiple datasets
- **Medium confidence**: The multi-round recovery strategy works as described, though real-world detection could limit effectiveness
- **Low confidence**: The assumption that users won't detect identity-mapped backbones lacks empirical validation

## Next Checks
1. Test model integrity verification: Implement weight auditing mechanism to check for identity mappings in frozen layers and measure detection rates
2. Evaluate proxy data generalization: Compare patch recovery rates when using different proxy datasets for bias calibration
3. Assess detection window: Measure how many training rounds users can participate before statistical analysis of adapter parameter changes becomes detectable as anomalous behavior