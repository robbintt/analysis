---
ver: rpa2
title: 'LongWeave: A Long-Form Generation Benchmark Bridging Real-World Relevance
  and Verifiability'
arxiv_id: '2510.24345'
source_url: https://arxiv.org/abs/2510.24345
tags:
- generation
- task
- evaluation
- length
- input
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: LongWeave introduces a new benchmark for evaluating long-form generation
  with verifiable, real-world tasks. It uses Constraint-Verifier Evaluation (CoV-Eval),
  which generates aligned triples of material, constraint, and verifier to ensure
  realistic yet objectively assessable tasks.
---

# LongWeave: A Long-Form Generation Benchmark Bridging Real-World Relevance and Verifiability

## Quick Facts
- arXiv ID: 2510.24345
- Source URL: https://arxiv.org/abs/2510.24345
- Authors: Zikai Xiao; Fei Huang; Jianhong Tu; Jianhui Wei; Wen Ma; Yuxuan Zhou; Jian Wu; Bowen Yu; Zuozhu Liu; Junyang Lin
- Reference count: 40
- Primary result: Introduces Constraint-Verifier Evaluation (CoV-Eval) for long-form generation with verifiable, real-world tasks

## Executive Summary
LongWeave introduces a new benchmark for evaluating long-form generation with verifiable, real-world tasks. It uses Constraint-Verifier Evaluation (CoV-Eval), which generates aligned triples of material, constraint, and verifier to ensure realistic yet objectively assessable tasks. The benchmark includes seven tasks—such as biography generation, news writing, and code fixing—with customizable input/output lengths up to 64K/8K tokens. Evaluation on 23 models shows significant performance degradation as length increases, with even top models struggling on 8K outputs. Reasoning models perform better but often fail to terminate properly. LongWeave reveals key challenges in long-form generation, highlighting the need for improved evaluation and model design.

## Method Summary
LongWeave uses Constraint-Verifier Evaluation (CoV-Eval) where tasks are constructed by first defining verifiable targets, then generating corresponding queries, materials, and constraints. The benchmark includes seven tasks: Code Fixing, Biography Generation, Sales Report Analysis, AP Style News Writing, KV Dictionary Generation, State Machine Simulation, and Paragraph Reordering. Each task uses synthetic data generation to create aligned (Material, Constraint, Verifier) triples. Models are evaluated at four output lengths (1K, 2K, 4K, 8K tokens) with 200 samples per task-length variant. Scoring combines task-specific metrics using harmonic mean aggregation, with LLM-as-a-Judge (Qwen2.5-72B-Instruct) handling subjective evaluations.

## Key Results
- Performance degrades significantly as output length increases from 1K to 8K tokens across all tasks
- Even top models struggle with 8K output generation, showing ~20% performance drop compared to 1K
- Reasoning models outperform non-reasoning models but fail to terminate reasoning phase 17% of the time
- Selective instruction execution is the most common failure mode (30.4%), where models complete easy constraints while ignoring complex ones

## Why This Works (Mechanism)

### Mechanism 1: Reverse Task Construction
Starting with verifiable targets and generating tasks backwards produces more objectively evaluable benchmarks than extracting verification criteria from generated outputs. CoV-Eval defines verifiers V first, then generates corresponding constraints C and materials X_raw through deterministic rule-based scripts, creating a causal chain where constraints explicitly guide models toward producing verifiable outputs.

### Mechanism 2: Task-Specific Constraint-Verifier Pairing
Different task types require different structural representations of constraint-verifier relationships to maintain verifiability while preserving task realism. Each task implements CV pairs in task-appropriate formats: (question, answer) for QA tasks like Sales Report; (flawed code, corrected code) for Code Fixing; (triples, sentences) for Knowledge Graph to Text.

### Mechanism 3: Harmonic Mean Aggregation Enforces Balanced Performance
Using harmonic mean to aggregate sub-scores prevents models from compensating for poor performance in one dimension with strong performance in another. Since harmonic mean heavily penalizes low values, models must perform adequately across all dimensions, revealing selective instruction execution failures.

## Foundational Learning

- **Constraint-Verifier Evaluation (CoV-Eval)**
  - Why needed: This is the core paradigm shift—understanding that verifiability is built into task construction, not retrofitted.
  - Quick check: Given a task "write a news article about topic X following AP style," can you define a verifier before seeing any model output?

- **LLM-as-a-Judge Reliability**
  - Why needed: Subjective aspects (style, coherence) require LLM evaluation; understanding its variance is critical for interpreting results.
  - Quick check: When using an LLM to judge style compliance, what factors would cause inconsistent evaluations across runs?

- **Length-Performance Degradation in Generation**
  - Why needed: The paper's central finding is that performance degrades with output length; understanding why requires grasping attention drift, instruction forgetting, and coherence challenges.
  - Quick check: Why would a model that performs well at 1K outputs struggle at 8K, even with identical per-token capabilities?

## Architecture Onboarding

**Component map:**
Attribute Seeds (θ) → Task Generator (f_gen) → [Material X_raw, Constraint C, Verifier V] → LLM Inference → O_gen → Scoring Function Score(O_gen, V) → Aggregation → Final Score

**Critical path:**
1. Define task scenario via attribute sampling
2. Generate aligned (Material, Constraint, Verifier) triple
3. Pass Material + Constraint to model
4. Compare output against Verifier using task-specific scoring
5. Aggregate across samples and tasks

**Design tradeoffs:**
- Synthetic generation ensures verifiability but may reduce ecological validity compared to real-world data
- Rule-based verifiers provide objective scoring but limit task scope to deterministically verifiable domains
- LLM-as-a-Judge enables subjective evaluation but introduces variance (~0.45 variance across judge models)
- Fixed sample size (200) stabilizes evaluation but increases computational cost

**Failure signatures:**
- Selective instruction execution (30.4%): Models complete easy constraints, ignore hard ones
- Stepwise deviation (14.0%): Performance degrades as output progresses
- Reasoning termination failure (17.0%): Reasoning models repeat input chunks, causing truncation
- Length control issues (12.4%): Outputs don't match specified word counts

**First 3 experiments:**
1. Baseline calibration: Run a mid-tier model (e.g., Qwen2.5-14B) across all 7 tasks at all 4 lengths (1K, 2K, 4K, 8K) to establish expected performance degradation curves; plot per-task and per-length scores.
2. Failure mode analysis: Select one task (recommend Sales Report for numerical + textual complexity), generate 50 samples, manually classify failures into Table 4 categories, identify model-specific weaknesses.
3. Judge sensitivity test: Evaluate 100 samples from a single task using multiple judge models (Qwen2.5-72B, GPT-4o, DeepSeek-V3), compute variance in scores to quantify evaluation reliability before committing to large-scale runs.

## Open Questions the Paper Calls Out

### Open Question 1
How can models be trained to jointly optimize for long-context input processing AND long-context output generation? Current training approaches appear to optimize for either input or output length, not both simultaneously; the correlation between the two capabilities is negative in the authors' experiments.

### Open Question 2
How can reasoning-oriented LLMs be improved to reliably terminate their reasoning phase and produce complete long-form outputs? The mechanisms controlling reasoning termination are not well-calibrated, particularly in smaller reasoning models.

### Open Question 3
Can tool integration or specialized training reduce numerical errors in long-form generation tasks? LLMs lack reliable computational precision for quantitative reasoning embedded in long outputs.

### Open Question 4
How can verifiable benchmarks be extended to evaluate creative long-form generation while maintaining objective assessment? Creativity and verifiability are fundamentally in tension; open-ended creative outputs resist deterministic evaluation.

## Limitations

- Synthetic generation approach may create tasks that diverge from real-world complexity where constraints interact in non-linear ways
- Reliance on rule-based verifiers inherently limits task scope to domains where deterministic verification is possible
- LLM-as-a-Judge component introduces evaluation variance, raising questions about reliability for subjective metrics

## Confidence

- **High Confidence**: The observed performance degradation across output lengths (1K→8K) is well-supported by empirical data showing consistent score drops across 23 models
- **Medium Confidence**: The claim that reasoning models outperform non-reasoning models despite termination issues is supported, but the 17% failure rate suggests the performance gain may be overstated
- **Low Confidence**: The assertion that harmonic mean aggregation is optimal for preventing compensation effects lacks comparison to alternative aggregation methods

## Next Checks

1. **Judge Reliability Test**: Evaluate 100 samples from a single task (e.g., Biography Generation) using multiple judge models (Qwen2.5-72B, GPT-4o, DeepSeek-V3) to quantify inter-judge variance and establish confidence intervals for subjective metrics.

2. **Cross-Benchmark Validation**: Apply LongWeave's CoV-Eval methodology to a non-synthetic dataset (e.g., real news articles or code repositories) to test whether the reverse construction approach maintains verifiability outside controlled synthetic environments.

3. **Correlation Analysis**: Systematically analyze relationships between sub-metrics (style, coverage, correctness, length) across all tasks to determine whether harmonic mean aggregation is appropriate or whether task-specific weighted combinations would better reflect performance.