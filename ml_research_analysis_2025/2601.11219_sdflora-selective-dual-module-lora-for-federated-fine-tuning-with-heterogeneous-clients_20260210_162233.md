---
ver: rpa2
title: 'SDFLoRA: Selective Dual-Module LoRA for Federated Fine-tuning with Heterogeneous
  Clients'
arxiv_id: '2601.11219'
source_url: https://arxiv.org/abs/2601.11219
tags:
- shared
- federated
- rank
- module
- lora
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: SDFLoRA addresses federated fine-tuning of large language models
  where clients have heterogeneous ranks and require privacy. Existing methods enforce
  unified ranks or align all updates into a shared subspace, which over-constrains
  personalization and underperforms under differential privacy noise.
---

# SDFLoRA: Selective Dual-Module LoRA for Federated Fine-tuning with Heterogeneous Clients

## Quick Facts
- arXiv ID: 2601.11219
- Source URL: https://arxiv.org/abs/2601.11219
- Reference count: 6
- Primary result: Selective dual-module LoRA achieves 0.87–4.70 pp accuracy gains over federated LoRA baselines while providing better utility-privacy trade-offs under differential privacy

## Executive Summary
SDFLoRA addresses federated fine-tuning of large language models where clients have heterogeneous ranks and require privacy. Existing methods enforce unified ranks or align all updates into a shared subspace, which over-constrains personalization and underperforms under differential privacy noise. SDFLoRA decomposes each client's LoRA adapter into a shared module for transferable knowledge and a private module for client-specific adaptations. Only the shared module participates in selective stacking-based aggregation, while private modules remain private. A low-rank re-compression step controls rank growth, and DP noise is applied exclusively to the shared module. Experiments on GLUE benchmarks show SDFLoRA outperforms federated LoRA baselines, achieving accuracy gains of 0.87–4.70 percentage points and a better utility-privacy trade-off.

## Method Summary
SDFLoRA extends federated LoRA fine-tuning by decomposing each client's adapter into dual modules: a shared module capturing transferable knowledge and a private module preserving client-specific adaptations. The method introduces selective stacking aggregation where only shared modules are aggregated across clients, followed by truncated SVD re-compression to control rank growth. When differential privacy is required, DP-SGD is applied exclusively to shared module gradients. This selective approach enables heterogeneous client ranks while maintaining privacy and personalization. The server broadcasts shared modules, clients train with combined forward passes using both modules, and only shared updates are aggregated and re-compressed before redistribution.

## Key Results
- SDFLoRA outperforms FedLoRA-Optimizer and Fed-HeLLo by 0.87–4.70 percentage points on GLUE tasks under heterogeneous ranks
- Selective DP application on shared modules achieves better utility-privacy trade-offs than full-adapter DP, maintaining higher accuracy at strong privacy levels
- The dual-module decomposition successfully handles rank heterogeneity without padding-based alignment distortion, validated on RTE task with gains of 2.89 pp over zero-padding baselines

## Why This Works (Mechanism)

### Mechanism 1: Dual-Module Decomposition for Semantic Separation
Decomposing LoRA adapters into shared and private modules preserves client-specific semantics while enabling beneficial cross-client knowledge transfer. Each client adapter $\Delta W_k$ is split into $\Delta W_k^{(g)}$ (shared, transferable) and $\Delta W_k^{(l)}$ (private, client-specific). Only $\Delta W_k^{(g)}$ participates in cross-client aggregation; private modules never leave the client. This structural constraint prevents forced alignment of client-specific directions. The core assumption is that LoRA updates contain semantically distinct directions—some transferable across clients, others domain/client-specific that should remain local.

### Mechanism 2: Selective Stacking Aggregation with Re-compression
Stacking-based aggregation applied exclusively to shared modules handles rank heterogeneity while avoiding the distortion from padding-based alignment. For shared modules with heterogeneous ranks $\{r_k^{(g)}\}$, concatenate $A_k^{(g)}$ and $B_k^{(g)}$ along the rank dimension (weighted by data size $p_k$), then compute $\Delta W_t^{(g)} = B_t^{(g)} A_t^{(g)}$. Truncated SVD re-compresses to fixed rank $r_{max}$, acting as a spectral filter that preserves dominant directions. The core assumption is that dominant singular vectors after stacking represent transferable knowledge; tail components are noise-dominated.

### Mechanism 3: Selective Differential Privacy via Module Decoupling
Applying DP noise only to shared modules preserves client-specific expressiveness while protecting transferable knowledge. Standard DP-SGD (gradient clipping + Gaussian noise) is applied to shared module gradients $g_k^{(g)}$ during local training. Private module gradients $g_k^{(l)}$ remain unperturbed. This localizes noise injection to parameters that will be aggregated, avoiding corruption of purely local adaptations. The core assumption is that client-specific adaptations are not leaked through the shared module (no information flow from private to shared during local optimization).

## Foundational Learning

- **Concept: Low-Rank Adaptation (LoRA)**
  - Why needed here: SDFLoRA builds directly on LoRA's factorization $\Delta W = BA$. Understanding that LoRA freezes the backbone and learns low-rank perturbations is prerequisite to grasping why rank heterogeneity breaks standard aggregation.
  - Quick check question: Given a frozen weight matrix $W \in \mathbb{R}^{d_{out} \times d_{in}}$ and LoRA factors $B \in \mathbb{R}^{d_{out} \times r}$, $A \in \mathbb{R}^{r \times d_{in}}$, what is the forward pass computation?

- **Concept: Federated Averaging (FedAvg)**
  - Why needed here: SDFLoRA replaces FedAvg-style direct averaging with stacking-based aggregation. Understanding FedAvg's weighted average $\sum_k \frac{n_k}{n} \theta_k$ clarifies why this fails under rank mismatch.
  - Quick check question: Why does FedAvg assume homogeneous model architectures, and what happens when client $i$ has parameters in $\mathbb{R}^{4 \times d}$ and client $j$ has parameters in $\mathbb{R}^{8 \times d}$?

- **Concept: Differential Privacy (DP-SGD)**
  - Why needed here: SDFLoRA's key claim is better utility-privacy trade-offs via selective DP. Understanding per-sample gradient clipping and noise scaling ($\sigma C$) is necessary to evaluate why selective application helps.
  - Quick check question: In DP-SGD, why does the noise scale depend on the clipping norm $C$ and sensitivity? What happens to utility as $\epsilon \to 0$?

## Architecture Onboarding

- **Component map:** Server -> broadcasts shared module $\theta^{(g,t)}$ -> Clients -> combine $\theta^{(g)}$ with local $\theta^{(l)}$ -> train locally -> upload $\theta_k^{(g,t)}$ -> Server -> stacking aggregation + SVD re-compression -> broadcast $\theta^{(g,t+1)}$ -> repeat

- **Critical path:**
  1. Server broadcasts shared module $\theta^{(g,t)}$ to participating clients $S_t$
  2. Each client initializes: $\theta^{(g)} \leftarrow \theta^{(g,t)}$, $\theta^{(l)} \leftarrow \theta_k^{(l,t)}$ (local state)
  3. Client runs $E$ local SGD steps on combined forward pass $\Delta W = B^{(g)}A^{(g)} + B^{(l)}A^{(l)}$
  4. If DP enabled: clip and noise $g^{(g)}$; update $\theta^{(g)}$ with perturbed gradients
  5. Client uploads only $\theta_k^{(g,t)}$; keeps $\theta_k^{(l,t+1)}$ locally
  6. Server: stacking aggregation $\rightarrow$ re-compression via truncated SVD to rank $r_{max}$
  7. Broadcast $\theta^{(g,t+1)}$; repeat

- **Design tradeoffs:**
  - $r_{max}$ (shared rank budget): Larger $r_{max}$ captures more transferable knowledge but increases communication/memory; smaller $r_{max}$ risks losing useful directions
  - Private module rank $r_k^{(l)}$: Paper does not specify how to set this or whether it differs from $r_k^{(g)}$
  - DP noise on shared vs. all: Selective DP improves utility but formal privacy guarantee for the full system is not proven
  - Stacking vs. padding: Stacking preserves union of subspaces but grows rank; requires re-compression

- **Failure signatures:**
  - Accuracy plateaus below baseline: Check if $r_{max}$ is too small (re-compression discarding signal)
  - High variance across clients: May indicate private module is underfitting; increase local training steps $E$ or private module capacity
  - DP performance collapse: Verify clipping norm $C$ is appropriate for gradient scale
  - Rank growth not controlled: Re-compression step may be missing or SVD threshold incorrect

- **First 3 experiments:**
  1. Baseline sanity check: Run FedAvg with homogeneous ranks ($r=8$ for all clients) on QNLI. Then run SDFLoRA with same total rank ($r^{(g)}=4$, $r^{(l)}=4$). Compare accuracy after 30 rounds.
  2. Heterogeneous rank validation: Set client ranks to $\{4, 4, 8, 8, 8, 8, 16, 16\}$. Compare SDFLoRA vs. zero-padding vs. FLoRA stacking on RTE.
  3. Privacy robustness test: Enable DP-SGD with $\epsilon \in \{1, 3, 8\}$. Compare SDFLoRA (selective DP on shared only) vs. applying DP to full adapter.

## Open Questions the Paper Calls Out
- How can rank capacity be dynamically allocated between shared and private modules based on specific client characteristics? The current framework utilizes a fixed rank budget and structure for all clients, which may be suboptimal for clients with highly distinct data distributions or varying resource constraints.
- Does the low-rank re-compression step inadvertently discard weak but transferable signals by truncating tail singular values? Section 3.3 asserts that re-compression "discards tail components that are more likely to be noise-dominated," but provides no empirical verification of the information lost in the discarded subspace.
- Can this structural decoupling be effectively generalized to non-LoRA adapter families, such as prefix tuning or soft prompts? The method relies on matrix decomposition properties ($\Delta W = B A$) and SVD re-compression specific to LoRA, which do not directly translate to vector-based or attention-head-specific PEFT techniques.

## Limitations
- The dual-module decomposition's effectiveness depends on the shared/private semantic split being "correct" - the paper assumes this but does not validate that transferable knowledge is actually captured in shared modules while client-specific knowledge remains in private modules
- Rank selection ($r_{max}$) for re-compression is heuristic, with no principled method for determining optimal values
- Formal privacy guarantees for the selective DP mechanism are not proven, leaving open questions about information leakage through the private module

## Confidence
- Dual-module decomposition benefits: Medium (empirical evidence shows gains but mechanism assumptions unverified)
- Selective stacking aggregation: Medium (proven to handle rank heterogeneity but SVD re-compression sensitivity untested)
- Selective DP utility-privacy tradeoff: Low-Medium (qualitative improvement shown but formal DP analysis absent)

## Next Checks
1. **Rank allocation sensitivity**: Systematically vary $r_{max}$ and private module ranks to identify sensitivity and determine if current choices are optimal
2. **Semantic decomposition validation**: Use PCA or other analysis to verify that shared modules capture transferable directions while private modules contain client-specific variations
3. **Privacy guarantee analysis**: Formalize the DP accounting for selective noise injection and verify that private modules do not leak information through the shared module