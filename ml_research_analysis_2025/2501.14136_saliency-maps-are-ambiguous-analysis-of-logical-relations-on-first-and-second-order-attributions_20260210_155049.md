---
ver: rpa2
title: 'Saliency Maps are Ambiguous: Analysis of Logical Relations on First and Second
  Order Attributions'
arxiv_id: '2501.14136'
source_url: https://arxiv.org/abs/2501.14136
tags:
- information
- methods
- inputs
- saliency
- scores
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the reliability of saliency-based explanation
  methods for deep learning models using logical datasets. The authors argue that
  many saliency methods fail to capture relevant information for model predictions,
  often encoding information in unintended ways.
---

# Saliency Maps are Ambiguous: Analysis of Logical Relations on First and Second Order Attributions

## Quick Facts
- **arXiv ID:** 2501.14136
- **Source URL:** https://arxiv.org/abs/2501.14136
- **Authors:** Leonid Schwenke; Martin Atzmueller
- **Reference count:** 40
- **Primary result:** No single saliency method consistently captures all relevant information across logical scenarios; all tested methods fail to some degree, indicating saliency scores are ambiguous.

## Executive Summary
This paper investigates the reliability of saliency-based explanation methods for deep learning models using synthetic logical datasets (ANDOR). The authors argue that many saliency methods fail to capture relevant information for model predictions, often encoding information in unintended ways. They propose using Global Coherence Representation (GCR) to interpret and validate saliency scores, as it can use saliency scores as weights for classification. Experiments on ANDOR logical datasets show that all tested saliency methods fail to consistently capture all relevant information, with no method performing well across all scenarios.

## Method Summary
The study evaluates 14 first-order and 4 second-order saliency methods on synthetic ANDOR datasets with binary or quaternary inputs. The key innovation is the use of a "Baseline" block of inputs that are strictly non-informative to the model's output. The NIB (Needed Information below Baseline) metric measures whether relevant inputs score lower than the highest irrelevant baseline input. The GCR mechanism validates saliency scores by using them as weights for a symbolic classifier, measuring Fidelity. Models are shallow ResNet-based CNNs or 2-layer Transformers trained to 100% accuracy.

## Key Results
- All 14 first-order saliency methods fail to consistently capture necessary information across all logical scenarios (AND, OR, XOR).
- IntegratedGradients performs best for AND gates, while Attention performs better for complex scenarios, but no method dominates across all scenarios.
- The GCR Fidelity metric drops significantly when using second-order attributions, indicating that first-order methods lose critical logical dependencies.
- The study confirms that saliency scores are ambiguous and often encode information in ways that violate typical interpretability assumptions.

## Why This Works (Mechanism)

### Mechanism 1: Dynamic Baseline Thresholding
If saliency methods reliably capture class-relevant information, irrelevant inputs should not score higher than minimally relevant inputs. The ANDOR framework introduces a dedicated "Baseline" block of inputs that are strictly non-informative (irrelevant to the logic gate outputs). By treating the highest score within this baseline block as a dynamic threshold, the system detects if a method fails to identify necessary information. If a relevant input scores below this threshold, the method is flagged as ambiguous.

### Mechanism 2: Global Coherence Representation (GCR) Fidelity
Valid saliency scores should serve as effective weights for a symbolic classifier, allowing the classifier to approximate the original model's reasoning. The GCR aggregates local saliency scores into global class-specific representations (matrices). Instead of just visual inspection, this mechanism validates scores by using them to weight symbolic inputs in a classification task. If the GCR can classify inputs accurately using the saliency weights (high Fidelity), the scores are globally consistent and differentiable.

### Mechanism 3: Logical Scenario Decomposition
Saliency method performance is conditional on the underlying logical structure of the data (redundant vs. complementary information). The paper decomposes performance by logical gate types (AND, OR, XOR). AND requires all inputs (complementary), OR requires only one input (redundant), and XOR requires specific combinations (exclusive). By evaluating methods on these distinct scenarios, the mechanism reveals that no single method dominates.

## Foundational Learning

### Concept: First vs. Second Order Attribution
**Why needed:** The paper explicitly investigates "Second Order Attributions" because first-order scores often lose critical logical dependencies, particularly for XOR gates.
**Quick check:** Can you explain why a "high" score on Input A might be meaningless without knowing the score of Input B in an XOR scenario?

### Concept: Complementary vs. Redundant Information ($R_{min}$ vs $R_{max}$)
**Why needed:** The paper defines $R_{min}$ (minimal set) and $R_{max}$ (all relevant info). Understanding this distinction is required to interpret the "Information Capturing" metrics.
**Quick check:** In an OR gate with three inputs, if inputs A and B are positive, is input C relevant to the truth of the output? (Answer: No, but it might be part of $R_{max}$).

### Concept: Masking Artifacts in Evaluation
**Why needed:** The authors argue that standard "masking" introduces bias/artifacts. They use GCR to attempt "actual input omission."
**Quick check:** Why might removing pixels (masking) create a new pattern (e.g., a black square) that confuses the neural network?

## Architecture Onboarding

### Component map:
Data Generator (ANDOR Framework) -> Model Under Test (ResNet/Transformer) -> Explanation Layer (14 Saliency Methods) -> Validation Layer (NIB/GIB/DCA Metrics + GCR Classifier)

### Critical path:
Generating the **Baseline Block** within the dataset is the most critical step. Without the non-informative baseline inputs, you cannot calculate the NIB/GIB metrics which form the core of the paper's "Ambiguity" argument.

### Design tradeoffs:
- *Symbolic Aggregation (SAX in GCR)* vs. *Raw Numeric Analysis*: The GCR uses SAX to discretize inputs into symbols. This improves interpretability but may lose fine-grained numeric precision.
- *LeRF vs MoRF*: The paper focuses on LeRF (Least Relevant First) removal to test redundancy capture, whereas MoRF is common in other literature.

### Failure signatures:
- **High NIB/GIB:** The saliency map fails to highlight necessary information.
- **High DCA (Double Class Assignment):** Information is "leaked" or encoded into irrelevant inputs (mask artifacts).
- **Correlation with Baseline:** Relevant inputs correlate with irrelevant ones, indicating model instability or explanation noise.

### First 3 experiments:
1. **Sanity Check (BinarySingleGate):** Train a model on a simple AND gate. Run IntegratedGradients and check if the NIB score is 0 (i.e., all relevant inputs scored higher than the baseline).
2. **Scenario Stress Test:** Compare GradCAM vs. Attention on an XOR dataset. Verify if Attention captures the exclusive relationship better (higher Logical Accuracy) than GradCAM.
3. **GCR Fidelity Validation:** Build the GCR (FCAM variant) using LRP-Rollout scores on an AND-OR-XOR dataset. Check if the Fidelity score drops when you remove the "Baseline" threshold (tGCR), proving the baseline contained noise.

## Open Questions the Paper Calls Out

### Open Question 1
How does the dominance of specific logical relations (e.g., AND-OR vs. XOR) in complex, real-world data affect the reliability of saliency methods compared to synthetic datasets? The experiments were confined to the synthetic ANDOR framework, and it remains unverified if the "logical traps" identified occur with the same frequency in natural data distributions.

### Open Question 2
Does a standardized, universal interpretation scheme exist for negative attribution scores, or must the semantic meaning (e.g., class contradictory vs. relative importance) be defined per method? The paper demonstrates that no single assumption about negative values holds true across all tested saliency methods.

### Open Question 3
To what extent can specific robust training procedures mitigate the information leakage and encoding flaws identified in standard trained models? The current experiments utilized standard training regimes, and it is unknown if specific forms of robust or adversarial training would eliminate the unintended encoding of class-relevant information into irrelevant inputs.

## Limitations
- The synthetic ANDOR datasets may not fully capture the complexity of real-world model reasoning and distributed representations.
- Reliance on the GitHub repository for critical architectural details creates a reproducibility gap.
- The SAX discretization in GCR introduces potential information loss that could affect the validity of the Fidelity metric.

## Confidence
- **High Confidence:** The observation that no single saliency method consistently captures all relevant information across all logical scenarios.
- **Medium Confidence:** The core claim that saliency scores are ambiguous and encode information in unintended ways.
- **Medium Confidence:** The proposed GCR mechanism for validating saliency scores is logically sound.

## Next Checks
1. **Generalization Test:** Apply the ANDOR evaluation framework to a non-symbolic, real-world dataset (e.g., image classification) to assess if the ambiguity patterns hold.
2. **GCR Comparison:** Compare the GCR Fidelity metric against a simpler validation method (e.g., a linear classifier trained on raw saliency scores) to isolate the benefit of the symbolic aggregation.
3. **Masking Artifact Control:** Design an experiment where the "Baseline" block is replaced with a scrambled version of the relevant inputs to determine if the high NIB scores are due to the specific baseline construction or a fundamental flaw in the saliency methods.