---
ver: rpa2
title: 'Reinforcement Learning for AMR Charging Decisions: The Impact of Reward and
  Action Space Design'
arxiv_id: '2505.11136'
source_url: https://arxiv.org/abs/2505.11136
tags:
- charging
- battery
- reward
- amrs
- strategies
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper extends an open-source warehouse simulation framework
  to model battery management for autonomous mobile robots (AMRs) in large-scale block
  stacking warehouses. The authors propose a reinforcement learning (RL) design that
  jointly optimizes charging decisions and charging durations using a Proximal Policy
  Optimization agent.
---

# Reinforcement Learning for AMR Charging Decisions: The Impact of Reward and Action Space Design

## Quick Facts
- **arXiv ID**: 2505.11136
- **Source URL**: https://arxiv.org/abs/2505.11136
- **Reference count**: 37
- **Primary result**: RL-based charging strategies can outperform heuristic approaches, with the best configuration achieving an average service time of 581.85 seconds when interruption is enabled, outperforming the best heuristic by 20.27 seconds.

## Executive Summary
This paper extends an open-source warehouse simulation framework to model battery management for autonomous mobile robots (AMRs) in large-scale block stacking warehouses. The authors propose a reinforcement learning (RL) design that jointly optimizes charging decisions and charging durations using a Proximal Policy Optimization agent. They evaluate different reward functions, action spaces, and the use of an interrupt heuristic, comparing their RL-based strategies against several heuristic baselines on a large real-world dataset. Results show that RL-based charging strategies can outperform heuristic approaches, with the best configuration achieving an average service time of 581.85 seconds when interruption is enabled, outperforming the best heuristic by 20.27 seconds. The study highlights the importance of reward function design and action space configuration in achieving stable learning and strong performance.

## Method Summary
The study extends the SLAPStack framework to simulate AMR battery management in a 150x80m warehouse with 40 AMRs, 3 charging stations, and 4 input/10 output docks. The authors implement a PPO agent with action masking to handle charging decisions and durations. Four configurations are tested: Basic1 (direct service time reward), Basic2 (queue-based reward), LightShaped (composite reward with free AMR component), and FullyShaped (comprehensive shaped reward). Training uses 2-layer MLPs with 64 neurons each, 4M steps total, and evaluations every 200K steps across multiple training weeks. An interrupt heuristic is applied during both training and evaluation to ensure operational safety.

## Key Results
- RL-based charging strategies outperform heuristic approaches by 20.27 seconds in average service time
- Reward shaping significantly improves learning stability compared to direct service time optimization
- Action space reduction to binary charging (0% or 100%) accelerates convergence but may limit generalization
- The interrupt heuristic consistently improves performance across all configurations

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Shaping the reward function to include intermediate operational feedback stabilizes learning more effectively than optimizing directly for the final objective (service time).
- **Mechanism**: The "credit assignment problem" in RL makes it difficult to attribute delayed outcomes (total service time) to specific charging decisions. By decomposing the reward into proxy signals—such as queue lengths or AMR availability—the agent receives denser, less noisy feedback, allowing the policy gradient to converge without catastrophic forgetting.
- **Core assumption**: The proxy metrics (e.g., queue length, free AMR count) correlate strongly with the final objective of minimizing service time.
- **Evidence anchors**: Abstract mentions that guided configurations lead to more stable learning; Section 5 shows Basic1 performs worst while LightShaped balances entropy and convergence; general RL literature supports reward shaping benefits.

### Mechanism 2
- **Claim**: Reducing the action space dimension and applying invalid action masking accelerates convergence but may restrict the agent's ability to generalize to new operational contexts.
- **Mechanism**: By constraining the agent to a binary action space (charge to 100% or do not charge) and masking invalid actions (e.g., charging when the battery is already full), the exploration space is drastically reduced. This prevents the agent from wasting samples on physically impossible transitions, leading to rapid entropy loss and policy solidification.
- **Core assumption**: The optimal strategy can be approximated using the restricted action set without requiring intermediate charge levels.
- **Evidence anchors**: Section 4 describes action masking implementation; Section 5 shows FullyShaped converges rapidly to low-entropy policy; corpus neighbor [12992] discusses constraints in RL supporting masking efficacy.

### Mechanism 3
- **Claim**: A hybrid architecture combining RL for charging duration with a hard-coded heuristic for interrupting charging creates a safety net that prevents catastrophic service failures.
- **Mechanism**: The RL agent focuses on optimization (long-term charging strategy), while the "Interrupt" heuristic handles safety-critical constraints (immediate robot availability). This decoupling prevents the RL agent from learning unsafe exploration strategies that block production, effectively smoothing the reward landscape by removing "dangerous" states from the agent's control.
- **Core assumption**: The rule-based heuristic (interrupt if queue > 0 and battery > 50%) correctly identifies the critical states where the RL policy would otherwise fail.
- **Evidence anchors**: Section 3.2 describes the interrupt mechanism and its consistent improvements; Section 6 notes the model's inability to perform well without this mechanism; no direct corpus neighbor addresses this specific hybrid mechanism.

## Foundational Learning

- **Concept: Proximal Policy Optimization (PPO) & Clipping**
  - **Why needed here**: PPO is the chosen algorithm. Understanding the clipping mechanism is essential to diagnose why the "Basic" configurations failed (instability) and why "Shaped" configurations succeeded (stable updates).
  - **Quick check question**: How does the clipping coefficient in PPO prevent the policy from changing too drastically during a single update, and why is this critical for stochastic environments like warehouse simulation?

- **Concept: Action Masking in Discrete Action Spaces**
  - **Why needed here**: The paper relies on masking invalid charging actions to ensure physical feasibility. Without understanding this, the high performance of the "FullyShaped" model cannot be replicated.
  - **Quick check question**: Does masking an action by setting its probability to negative infinity remove the gradient flow for that action, or does it merely force the sampling distribution to valid actions?

- **Concept: The Credit Assignment Problem**
  - **Why needed here**: This is the core theoretical motivation for Reward Shaping (Mechanism 1). The paper argues that direct service time rewards are too sparse.
  - **Quick check question**: In a sequence of 50 charging decisions, why is it difficult for a standard RL agent to determine that the 5th decision caused a service time spike at the 50th step?

## Architecture Onboarding

- **Component map**: Environment (SLAPStack) -> MDP Wrapper -> Strategy Layer -> Training Loop
- **Critical path**: The interaction between the Feature Normalization (Table 2) and the Action Mask is the critical path. If features are not normalized to [0,1], the PPO value function estimation becomes unstable; if the mask is leaky, the agent attempts invalid physics.
- **Design tradeoffs**:
  - Flexibility vs. Stability: Using the full action set (A_full) allows better generalization (LightShaped) but risks training instability. Using binary action (A_binary) ensures convergence (FullyShaped) but risks overfitting to the "Interrupt" heuristic.
  - Sparse vs. Dense Reward: Sparse rewards (Service Time) are theoretically optimal but practically unlearnable. Dense rewards (Queue length) are learnable but may lead to "reward hacking."
- **Failure signatures**:
  - Basic1/Basic2 Collapse: Episode reward starts high, then crashes and flatlines (entropy remains high). Diagnosis: The agent is "thrashing" due to sparse rewards and lack of guidance.
  - FullyShaped Overfit: Model performs well in training but service time explodes (e.g., to 700s+) when the "Interrupt" is disabled during evaluation. Diagnosis: The agent offloaded safety logic to the heuristic and cannot manage availability independently.
- **First 3 experiments**:
  1. **Sanity Check (Heuristic Baseline)**: Run the Fixed-threshold strategy with TH_upper=40% and Interrupt=True to establish a baseline service time. Target: ~602s.
  2. **Reward Ablation**: Train two PPO agents with A_full: one with Service Time reward, one with Queue-based reward. Verify that Service Time reward fails to converge within 1M steps.
  3. **Generalization Stress Test**: Train FullyShaped with Interrupt=True. Evaluate it on a held-out week with Interrupt=False. Confirm performance degradation to identify dependency on the heuristic.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How do non-linear charging behaviors and battery degradation affect the performance and stability of the proposed RL charging strategies?
- **Basis in paper**: [explicit] The conclusion states "future work should incorporate more detailed battery models that account for degradation and non-linear charging behavior."
- **Why unresolved**: The current study explicitly assumes linear charging and ignores degradation to simplify the model, which may not reflect real-world AMR operations.
- **Evidence**: Comparative performance metrics of agents trained in a simulation environment implementing non-linear battery dynamics.

### Open Question 2
- **Question**: To what extent are the performance gains attributable to the charging strategy versus other warehouse variables like layout or order sequences?
- **Basis in paper**: [explicit] The conclusion notes that "further work is needed to better isolate scenarios where battery management is the primary performance bottleneck."
- **Why unresolved**: The current evaluation integrates all system factors, making it difficult to distinguish if battery management was the critical constraint.
- **Evidence**: Ablation studies varying warehouse layouts and order profiles to identify conditions where the RL agent provides the most marginal utility.

### Open Question 3
- **Question**: Can systematic approaches to reward design improve the generalization capability of guided agents when heuristics are unavailable?
- **Basis in paper**: [explicit] The conclusion suggests "future research should explore more systematic approaches to reward shaping" following the observed trade-off between stability and generalization.
- **Why unresolved**: The "FullyShaped" agent showed significant performance drops when the "Interrupt" heuristic was disabled, indicating overfitting to specific domain knowledge.
- **Evidence**: Evaluation of agents trained with automatically tuned or systematic reward functions across environments with and without specific heuristics.

### Open Question 4
- **Question**: How sensitive is the proposed RL design's performance to the choice of algorithm architecture and hyperparameters?
- **Basis in paper**: [explicit] The conclusion asserts that "A more thorough and systematic evaluation of algorithms and parameters, e.g. learning rate, is also essential."
- **Why unresolved**: The study relied on a single algorithm (PPO) with default parameters, leaving the robustness of the design choices unverified.
- **Evidence**: Ablation studies or grid searches over different learning rates and alternative RL algorithms using the SLAPStack framework.

## Limitations

- The study's findings are constrained by the specific simulation environment and the reliance on a hard-coded interrupt heuristic
- The action space reduction to binary charging may limit the generalizability of the approach to scenarios requiring finer-grained battery management
- The evaluation focuses on average service time without addressing variance or extreme-case performance, which could be critical for operational reliability

## Confidence

- **High confidence**: The relative performance ranking of different reward functions and action spaces (Mechanism 1 and 2). The empirical evidence strongly supports that shaped rewards and constrained action spaces improve stability and convergence.
- **Medium confidence**: The effectiveness of the interrupt heuristic as a safety mechanism (Mechanism 3). While the results show clear improvements, the dependency on a manually tuned threshold (50% battery) introduces brittleness.
- **Low confidence**: The claim that the learned policies would generalize to different warehouse layouts or AMR fleet sizes without retraining, as the simulation parameters are fixed.

## Next Checks

1. **Robustness to parameter changes**: Test the best RL configuration (LightShaped with interrupt) on a modified warehouse layout (e.g., different dimensions or dock configurations) to assess generalization.
2. **Heuristic threshold sensitivity**: Evaluate the impact of varying the interrupt heuristic's battery threshold (e.g., 40%, 60%) on both service time and system stability.
3. **Variance analysis**: Measure the standard deviation of service times across evaluation runs to quantify the consistency of the RL strategies compared to heuristics.