---
ver: rpa2
title: 'Dingtalk DeepResearch: A Unified Multi Agent Framework for Adaptive Intelligence
  in Enterprise Environments'
arxiv_id: '2510.24760'
source_url: https://arxiv.org/abs/2510.24760
tags:
- research
- https
- market
- editing
- video
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Dingtalk DeepResearch is a unified multi-agent framework for enterprise
  intelligence, combining deep research, heterogeneous table reasoning, and multimodal
  report generation. It employs an entropy-guided, memory-aware online learning mechanism
  to retrieve high-value cases from episodic memory, enabling adaptive reasoning without
  retraining underlying LLMs.
---

# Dingtalk DeepResearch: A Unified Multi Agent Framework for Adaptive Intelligence in Enterprise Environments

## Quick Facts
- **arXiv ID**: 2510.24760
- **Source URL**: https://arxiv.org/abs/2510.24760
- **Reference count**: 40
- **Primary result**: Unified multi-agent framework combining deep research, heterogeneous table reasoning, and multimodal report generation with entropy-guided online learning and closed-loop optimization via DingAutoEvaluator.

## Executive Summary
Dingtalk DeepResearch introduces a unified multi-agent framework designed for enterprise intelligence, integrating deep research capabilities, heterogeneous table reasoning, and multimodal report generation. The system employs an entropy-guided, memory-aware online learning mechanism to adaptively retrieve high-value cases from episodic memory, enabling reasoning without retraining underlying LLMs. A comprehensive five-stage training pipeline (Doc-RM reward modeling, structured-format SFT, static/live RL, and user-driven DPO) enhances document accuracy and user alignment. The framework has been deployed in corporate workflows, achieving significant gains in accuracy, structural quality, and robustness.

## Method Summary
The framework combines a deep research agent with heterogeneous table reasoning and multimodal report generation. It uses an entropy-guided memory retrieval mechanism to adaptively select high-value cases from episodic memory for planning, avoiding LLM fine-tuning. The five-stage training pipeline includes: (1) Doc-RM reward modeling with human-annotated document pairs, (2) Cold-Start SFT for structured outputs, (3) RL on static documents, (4) RL on live streams, and (5) Online DPO for user alignment. Table reasoning employs layout-aware parsing, semantic decomposition, and SQL-based execution, with continuous retraining via DingAutoEvaluator's automated case mining and multi-dimensional metrics.

## Key Results
- Achieved GAIA benchmark score of 82.42%, demonstrating competitive performance in adaptive intelligence tasks.
- Reported improvements of +2.2% accuracy and +1.2% structural quality in document generation through the multi-stage training pipeline.
- Closed-loop optimization via DingAutoEvaluator drives continuous performance evolution across RAG, generation, reasoning, and agentic tasks.

## Why This Works (Mechanism)
The framework's effectiveness stems from its entropy-guided, memory-aware online learning mechanism that retrieves high-value cases from episodic memory for adaptive reasoning without LLM fine-tuning. The dual-store table ingestion (relational DB + vector store) enables robust schema linking and semantic decomposition. The five-stage training pipeline progressively refines document accuracy and user alignment through reward modeling, structured fine-tuning, and reinforcement learning on both static and live data streams.

## Foundational Learning
- **Entropy-Guided Memory Retrieval**: Selects high-value cases based on uncertainty and reward potential; needed for adaptive reasoning without retraining; quick check: verify Q-value and temperature combination logic.
- **Dual-Store Table Ingestion**: Separates relational schema and vector embeddings for tables; needed to handle heterogeneous table structures; quick check: validate schema linking accuracy on merged cells.
- **Reward Modeling (Doc-RM)**: Trains a classifier/regression head on human-annotated document pairs; needed for RL optimization; quick check: monitor Answer Faithfulness metric during RL training.
- **NL2SQL Schema Linking**: Maps natural language to SQL queries using layout-aware parsing; needed for heterogeneous table querying; quick check: test SQL execution accuracy on complex joins.
- **Closed-Loop Optimization**: Uses DingAutoEvaluator for automated case mining and multi-dimensional metrics; needed for continuous performance improvement; quick check: track Context Precision trends over iterations.

## Architecture Onboarding

**Component Map**: User Query -> DeepResearch Agent -> Episodic Memory (Entropy-Guided Retrieval) -> Table Reasoning (NL2SQL + SQL Execution) -> Document Generation -> DingAutoEvaluator (Feedback Loop) -> Reward Model Update

**Critical Path**: User query → episodic memory retrieval → table reasoning (if applicable) → document generation → reward evaluation → RL update. The table reasoning path (NL2SQL → SQL execution) is critical for heterogeneous data.

**Design Tradeoffs**: Avoids LLM fine-tuning for adaptability (uses retrieval-based planning) vs. potential retrieval limitations; dual-store table parsing vs. implementation complexity; closed-loop optimization vs. dependency on automated evaluator quality.

**Failure Signatures**: 
- SQL execution errors on heterogeneous tables (schema linking failures).
- Hallucinations in long-form reports (poor retrieval grounding).
- Reward model collapse during RL (overfitting to narrow reward signals).

**First Experiments**:
1. Implement dual-store table ingestion on supply chain logs with merged cells; measure schema linking accuracy.
2. Replicate entropy-guided memory retrieval with controlled temperature parameters; verify high-value case selection.
3. Conduct user study comparing report quality with/without Doc-RM reward signal; validate factual accuracy gains.

## Open Questions the Paper Calls Out
None

## Limitations
- Performance improvements (+2.2% accuracy, +1.2% structural quality) are internally validated without independent benchmarking.
- Claims of "no LLM fine-tuning" require scrutiny of the interaction between episodic memory and planning decisions.
- Table reasoning effectiveness heavily depends on accurate schema mapping, a known challenge in heterogeneous table processing.

## Confidence

**High Confidence**: The multi-stage training pipeline structure and dual-store table ingestion approach are well-specified and technically feasible.

**Medium Confidence**: Entropy-guided memory retrieval claims are plausible but lack quantitative ablation studies for verification.

**Low Confidence**: "No fine-tuning" claim for adaptive reasoning needs transparent reporting on episodic memory and planning interaction.

## Next Checks

1. Implement NL2SQL schema-linking module on heterogeneous tables (e.g., supply chain logs with merged cells); measure schema accuracy against ground truth SQL executions.
2. Replicate entropy-guided case retrieval mechanism with controlled temperature parameters; verify whether higher entropy thresholds consistently select higher-value cases.
3. Conduct independent user study comparing report generation quality with and without Doc-RM reward signal; validate claimed improvements in factual accuracy and format fidelity.