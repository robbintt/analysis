---
ver: rpa2
title: 'Robustness of Probabilistic Models to Low-Quality Data: A Multi-Perspective
  Analysis'
arxiv_id: '2512.11912'
source_url: https://arxiv.org/abs/2512.11912
tags:
- data
- noise
- information
- training
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper systematically investigates why modern probabilistic
  models exhibit starkly different robustness to low-quality training data. Autoregressive
  language models and large-scale classifiers show remarkable resilience to high levels
  of data corruption, while class-conditional diffusion models degrade catastrophically
  under the same conditions.
---

# Robustness of Probabilistic Models to Low-Quality Data: A Multi-Perspective Analysis

## Quick Facts
- arXiv ID: 2512.11912
- Source URL: https://arxiv.org/abs/2512.11912
- Reference count: 40
- Autoregressive LMs and classifiers show resilience to 50% data corruption, while class-conditional diffusion models fail catastrophically.

## Executive Summary
This paper systematically investigates why modern probabilistic models exhibit starkly different robustness to low-quality training data. Through controlled experiments introducing random errors at rates up to 50%, the study reveals that robustness is primarily governed by two factors: the richness of conditioning information and the absolute information content of clean training data. Autoregressive language models and large-scale classifiers show remarkable resilience to high levels of data corruption, while class-conditional diffusion models degrade catastrophically under the same conditions. The key finding is that models with information-rich conditions can overcome noise in low-information targets, while models with sparse conditions are highly vulnerable.

## Method Summary
The paper evaluates three model families—autoregressive language models (GPT-2), image classifiers (ResNet-18, ViT), and class-conditional diffusion models (EDM)—under controlled data corruption. Noise injection replaces target tokens with random vocabulary tokens (LM) or labels with random incorrect classes (Diffusion/Classifier) at ratios up to 100%. The primary training paradigm scales total iterations by (1+r) to ensure constant exposure to clean data. Experiments span OpenWebText (LM), CIFAR-10/100 (Diffusion/Classifier), ImageNet (Classifier), and WMT'14 & CNN/DailyMail (Seq2Seq). Metrics include Test NLL for LMs, image-label consistency and FID for Diffusion, and Top-1 Accuracy for Classifiers.

## Key Results
- Diffusion models (EDM) suffer catastrophic degradation with 50% label noise, losing label consistency while maintaining image realism (FID).
- Autoregressive LMs (GPT-2) maintain stable performance under 50% target noise when batch size is scaled appropriately.
- Classifiers (ResNet/ViT) on ImageNet-1000 show near-complete immunity to label noise, while smaller datasets (ImageNet-10/100, CIFAR-10/100) show standard degradation.
- Information-theoretic analysis shows residual instructive signal persists as long as observed labels aren't statistically independent of true labels.
- Gradient-based analysis demonstrates that larger batch sizes amplify coherent signals from correct samples while averaging out divergent noise.

## Why This Works (Mechanism)

### Mechanism 1: Conditioning Information Richness
- **Claim:** Models are robust to noise in low-information targets (labels, next tokens) when conditioned on high-information inputs (images, long text contexts), but fragile when the inverse is true.
- **Mechanism:** Rich conditioning inputs drastically constrain the output space. This "information asymmetry" lowers the effective complexity of the learning problem. The model uses the strong signal from the input to disambiguate the noisy target, averaging out inconsistencies.
- **Core assumption:** The noise in the target is uncorrelated with the conditioning signal (unstructured noise); structured noise that contradicts the input signal would break this constraint.
- **Evidence anchors:**
  - [abstract] "Richness of conditioning information... constrains the learning problem."
  - [Section 3.3] Sequence-to-sequence experiments show CNN/DailyMail (long context) degrades only 17.9% vs WMT (short context) 31.5% degradation at 50% noise.
  - [corpus] "Utilizing Class Separation Distance..." supports the geometry of robustness, implying that distinct class structures (facilitated by rich inputs) aid robustness.
- **Break condition:** If the conditioning input is sparse (e.g., a single class label) and the target is high-dimensional (e.g., an image), the constraint fails, leading to catastrophic forgetting of the correct mapping.

### Mechanism 2: Gradient Signal Aggregation
- **Claim:** Increasing batch size allows SGD to filter out unstructured noise by amplifying the coherent gradient signal from clean data.
- **Mechanism:** Gradients derived from clean data are directionally coherent (high cosine similarity), while gradients from corrupted data are random/orthogonal. Aggregating over a large batch sums the coherent vectors (constructive interference) while averaging the random vectors towards zero (destructive interference).
- **Core assumption:** The noise is random and unstructured; gradients from noisy samples are statistically independent.
- **Evidence anchors:**
  - [Section 4.3] "Clean gradients exhibit strong, coherent positive alignment (+0.52), while corrupted gradients are directionally random."
  - [Table 4] Shows Signal-to-Noise Ratio (SNR) improving from 2.96x to 3.83x when batch size doubles under 50% corruption.
  - [corpus] "Enhancing Robustness... via Sharpness-Aware Minimization" connects loss landscape geometry to robustness, complementing the gradient direction analysis.
- **Break condition:** If the noise is structured (e.g., systematic mislabeling), the "noisy" gradients become coherent, preventing cancellation and causing the model to learn a false signal.

### Mechanism 3: Absolute Information Dominance
- **Claim:** If the dataset scale is sufficiently large, the absolute volume of correct information can satisfy sample complexity requirements even when diluted by 50% noise.
- **Mechanism:** According to PAC learning, a task requires a minimum number of clean samples ($m$) to be learnable. In massive datasets (e.g., ImageNet-1k), the subset of clean samples still exceeds this threshold $m$, allowing the model to generalize despite the "dilution" of the dataset.
- **Core assumption:** The model capacity is sufficient to fit the underlying signal without being forced to memorize the overwhelming volume of noise.
- **Evidence anchors:**
  - [Section 3.5] ImageNet-1000 classification accuracy remains stable (~73-74%) even at 50% label noise, unlike smaller subsets (ImageNet-10/100).
  - [Section 4.1.2] "Absolute information content... allows the signal from correct information to dominate statistical noise."
  - [corpus] "Detecting Mislabeled and Corrupted Data via Pointwise Mutual Information" reinforces the idea that statistical signals can be separated from noise in large datasets.
- **Break condition:** If the dataset is small (e.g., CIFAR-10), the absolute information content drops below the learning threshold $m$ upon corruption, causing standard degradation.

## Foundational Learning

- **Concept: VC Dimension & Sample Complexity**
  - **Why needed here:** To understand why diffusion models (high output complexity) are more fragile than classifiers (low output complexity) under data corruption (Section 4.2).
  - **Quick check question:** Does a model learning $p(\text{image}|\text{label})$ have a higher or lower effective VC dimension than one learning $p(\text{label}|\text{image})$, and why does that demand more clean data?

- **Concept: Cosine Similarity & Vector Aggregation**
  - **Why needed here:** To grasp the gradient-based argument that random noise cancels out while coherent signals amplify during batch training (Section 4.3).
  - **Quick check question:** If two gradients have a cosine similarity of -1, what happens when they are averaged? What if the similarity is +1?

- **Concept: Conditional Entropy**
  - **Why needed here:** To quantify "information loss" and understand why a corrupted label is not always "zero information" (Section 4.1.1).
  - **Quick check question:** At what error rate $p_e$ does a corrupted label become statistically independent of the true label (maximum entropy)?

## Architecture Onboarding

- **Component map:**
  - Autoregressive (GPT-2): Robust. Input (Rich Context) → Output (Sparse Token).
  - Classifier (ResNet/ViT): Robust at Scale. Input (Rich Image) → Output (Sparse Label).
  - Class-Cond. Diffusion (EDM): Fragile. Input (Sparse Label) → Output (Rich Image).

- **Critical path:** Identify the **Information Asymmetry**. If you are designing a system with low-quality data, ensure the *conditioning* side of the model (the input) is information-dense relative to the target.
  - *Safe:* Image → Label, Long Text → Summary.
  - *Danger:* Label → Image, Short Prompt → Long Video.

- **Design tradeoffs:**
  - **Batch Size vs. Noise:** In high-noise regimes (30%+), standard batch sizes may fail to converge. You must scale batch size (e.g., 2x-12x) to force gradient averaging, but this requires reducing iterations to maintain compute budget (Section 3.2).
  - **Scale vs. Quality:** For classifiers, scaling data volume can offset quality issues (Section 3.5). For conditional generative models, scaling volume does *not* easily fix label noise (Section 3.4).

- **Failure signatures:**
  - **The "Drift" (Diffusion):** FID scores remain good (images look real), but label consistency plummets (image content is wrong). The model learns the *marginal* distribution of images but loses the *conditional* link.
  - **The "Instability" (LM):** Training loss fluctuates wildly or diverges with standard batches; requires aggressive batch scaling to stabilize.
  - **The "Collapse" (Structured Noise):** If noise is systematic (e.g., wolf → husky), robustness disappears completely (Appendix L), as the gradient mechanism relies on randomness.

- **First 3 experiments:**
  1. **Batch Scaling Ablation:** Train a model on 50% random label noise. Plot convergence for batch sizes $B, 2B, 4B$. Verify if SNR improves (Section 4.3).
  2. **Context Length Robustness:** Compare a summarization model with full context vs. truncated context (e.g., 512 tokens vs 2048 tokens) under target noise. Verify if shorter context degrades faster (Section 3.3).
  3. **Structure vs. Random Noise:** Compare accuracy when flipping labels randomly vs. flipping labels systematically (Class $i \to i+1$) on a CIFAR classifier. Confirm the collapse under structured noise (Appendix L).

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Would class-conditional diffusion models exhibit greater robustness to label noise if trained on large-scale datasets (e.g., ImageNet-scale), as predicted by the absolute information content principle?
- **Basis in paper:** [explicit] Appendix B states "computational constraints precluded training diffusion models on very large-scale datasets," leaving this prediction untested.
- **Why unresolved:** The ImageNet-1000 classifier became nearly impervious to noise, but diffusion experiments only used CIFAR-10/100.
- **What evidence would resolve it:** Train class-conditional diffusion models on ImageNet-1000 with label noise and measure image-label consistency degradation.

### Open Question 2
- **Question:** How do the robustness principles transfer to the pre-training and fine-tuning paradigm, where pre-trained knowledge may interact with noisy fine-tuning data?
- **Basis in paper:** [explicit] Discussion notes understanding fine-tuning dynamics with noisy data is a "future investigation" built upon this baseline work, citing model "elasticity."
- **Why unresolved:** All experiments trained models from scratch to isolate intrinsic robustness, excluding pre-training as a confounder.
- **What evidence would resolve it:** Compare fine-tuning robustness across models with varying pre-training quality and noisy fine-tuning data ratios.

### Open Question 3
- **Question:** Does the number of classes (n) independently affect robustness when disentangled from dataset size and model capacity?
- **Basis in paper:** [explicit] Appendix B acknowledges this was not analyzed as an independent variable since "its effects are inherently entangled with dataset size and model capacity."
- **Why unresolved:** The information-theoretic analysis predicts class count influences residual information, but no controlled experiments isolated this factor.
- **What evidence would resolve it:** Controlled experiments varying class count while holding dataset size, model capacity, and error rate constant.

## Limitations

- The gradient-based analysis assumes noise is random and uncorrelated, but the paper doesn't systematically test how structured noise patterns affect robustness differently.
- The batch size amplification mechanism is theoretically sound but computationally expensive—interventions (2x-12x batch sizes) may not be practical for all applications.
- The diffusion model results showing catastrophic failure rely on class-conditional EDM trained with specific architectures and optimization schedules that aren't fully specified.

## Confidence

- **High Confidence:** The core empirical observation that autoregressive LMs and classifiers show different robustness profiles than diffusion models is well-supported by controlled experiments across multiple datasets.
- **Medium Confidence:** The information-theoretic explanation (conditioning information richness and absolute information dominance) provides a coherent framework but relies on abstract measures not directly measured in experiments.
- **Low Confidence:** The assertion that gradient signal aggregation through larger batches is the primary mechanism for LM robustness lacks direct experimental validation beyond correlation analysis.

## Next Checks

1. **Structured Noise Ablation:** Systematically compare random label corruption against semantically structured corruption (e.g., dog→cat, cat→bird) across all three model types to verify the random noise assumption underlying the gradient aggregation mechanism.

2. **Batch Size Scaling Limits:** Conduct experiments varying batch size from standard to extreme (2x to 32x) on a single task to identify the point of diminishing returns and practical limits of the gradient averaging approach.

3. **Architecture Independence Test:** Apply the same noise protocols to alternative architectures (LSTM for LMs, MLP for classifiers, score-based diffusion) to verify whether the observed robustness patterns are architecture-specific or reflect deeper learning principles.