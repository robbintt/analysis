---
ver: rpa2
title: Can LLMs Understand Unvoiced Speech? Exploring EMG-to-Text Conversion with
  LLMs
arxiv_id: '2506.00304'
source_url: https://arxiv.org/abs/2506.00304
tags:
- speech
- llms
- unvoiced
- language
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper explores using large language models (LLMs) to convert\
  \ unvoiced electromyography (EMG) signals\u2014captured muscle activations from\
  \ speech-related movements\u2014directly into text, without requiring any voiced\
  \ audio data. The authors propose a trainable EMG adaptor module that maps EMG features\
  \ into the LLM\u2019s input space, enabling the LLM to interpret these biosignals."
---

# Can LLMs Understand Unvoiced Speech? Exploring EMG-to-Text Conversion with LLMs

## Quick Facts
- arXiv ID: 2506.00304
- Source URL: https://arxiv.org/abs/2506.00304
- Authors: Payal Mohapatra; Akash Pandey; Xiaoyuan Zhang; Qi Zhu
- Reference count: 30
- Key outcome: LLM-based EMG-to-text achieves 0.49 WER on closed vocabulary, outperforming specialized models by 20% with only 6 minutes of training data

## Executive Summary
This paper introduces a novel approach to converting unvoiced electromyography (EMG) signals directly into text using large language models (LLMs). The authors develop an EMG adaptor module that bridges EMG feature representations with LLM input spaces, enabling LLMs to interpret muscle activation patterns from speech-related movements without requiring any audio data. The system demonstrates strong performance on a closed-vocabulary task with minimal training data, suggesting potential applications for communication assistance for individuals unable to produce vocal speech.

## Method Summary
The approach involves capturing EMG signals from facial and neck muscles during unvoiced speech attempts, then processing these signals through a trainable adaptor module that transforms EMG features into a format compatible with LLM input requirements. The adaptor learns to map temporal EMG patterns to corresponding text outputs through supervised training on paired EMG-text data. The LLM component, pre-trained on general language data, then leverages its linguistic knowledge to decode the adapted EMG features into accurate text predictions. The system was evaluated on a controlled dataset with 51 vocabulary words from six speakers, with performance measured using word error rate.

## Key Results
- Achieved average WER of 0.49 on closed-vocabulary unvoiced EMG-to-text task
- Outperformed specialized EMG-to-text models by nearly 20% despite minimal training (6 minutes of data)
- Demonstrated data efficiency by achieving competitive results with limited training examples

## Why This Works (Mechanism)
The approach works by leveraging the pre-existing linguistic knowledge encoded in large language models, which have been trained on vast amounts of text data and understand language structure, syntax, and semantics. The EMG adaptor module serves as a translator between the physiological domain of muscle activation patterns and the semantic domain of language that the LLM understands. This allows the system to bypass the need for extensive language modeling from scratch, instead focusing on learning the mapping from EMG features to linguistic representations. The LLM's ability to handle ambiguous or noisy inputs, developed through training on diverse text data, helps compensate for the inherent variability and noise in EMG signals.

## Foundational Learning
1. **Electromyography (EMG) Signal Processing** - EMG captures electrical activity from muscle contractions during speech attempts; needed to understand the input modality and its characteristics; quick check: verify that EMG signals from different speakers show consistent patterns for the same words
2. **Large Language Model Architecture** - LLMs are transformer-based models trained on vast text corpora that understand language structure; needed to leverage pre-trained linguistic knowledge; quick check: confirm the LLM's performance on standard language tasks
3. **Temporal Signal Alignment** - EMG frames must be temporally aligned with corresponding text tokens; needed to create supervised training pairs; quick check: verify alignment accuracy between EMG and text sequences
4. **Adaptor Module Design** - A trainable module that maps EMG feature space to LLM input space; needed to bridge physiological signals with linguistic representations; quick check: measure adaptor's feature transformation effectiveness
5. **Closed vs Open Vocabulary** - Closed vocabulary uses fixed word lists (51 words here) while open vocabulary handles arbitrary text; needed to understand evaluation scope and limitations; quick check: compare performance differences between closed and open settings
6. **Cross-Subject Generalization** - Ability of the model to work across different speakers with varying muscle activation patterns; needed to assess real-world applicability; quick check: evaluate performance when trained on one speaker and tested on others

## Architecture Onboarding

**Component Map:** EMG sensors -> Signal preprocessing -> Adaptor module -> LLM -> Text output

**Critical Path:** EMG signal capture → Feature extraction → Adaptor transformation → LLM inference → Text decoding

**Design Tradeoffs:** The approach trades specialized EMG model development for LLM integration, accepting potential computational overhead for leveraging pre-existing language understanding. The closed vocabulary constraint simplifies the task but limits real-world applicability.

**Failure Signatures:** High WER on certain phonemes suggests the adaptor struggles with specific muscle activation patterns. Performance degradation across speakers indicates sensitivity to individual physiological differences. The one-to-one word-EMG frame assumption may break down for continuous speech.

**3 First Experiments:**
1. Test adaptor performance with varying EMG feature extraction methods to identify optimal signal representation
2. Evaluate LLM performance with synthetic EMG-like noise injection to understand robustness limits
3. Measure cross-speaker performance with transfer learning to assess generalization capabilities

## Open Questions the Paper Calls Out
The authors identify several key open questions for future research: extending the approach to open-vocabulary settings beyond the 51-word constraint, developing multilingual capabilities to support communication across different languages, and improving generalization across diverse user populations with varying muscle activation patterns and physiological characteristics.

## Limitations
- Evaluation restricted to closed-vocabulary setting with only 51 words, not reflecting real-world communication complexity
- Reported WER of 0.49 still represents substantial error rates that would impact practical usability
- Study relies on data from only six speakers, raising concerns about generalization across different users

## Confidence
- **High confidence** in technical implementation of EMG adaptor module and LLM integration
- **Medium confidence** in comparative performance claims given limited comparison to specialized models
- **Medium confidence** in data efficiency claims requiring validation across different datasets
- **Low confidence** in scalability to open-vocabulary and multilingual settings without empirical validation

## Next Checks
1. Test model performance on open-vocabulary dataset with continuous speech to evaluate real-world applicability
2. Conduct cross-subject evaluation using EMG data from speakers not included in training to verify generalization
3. Compare EMG adaptor approach against standard speech recognition systems using whispered or silent speech audio