---
ver: rpa2
title: 'SuS: Strategy-aware Surprise for Intrinsic Exploration'
arxiv_id: '2601.10349'
source_url: https://arxiv.org/abs/2601.10349
tags:
- strategy
- exploration
- learning
- training
- intrinsic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper proposes Strategy-aware Surprise (SuS), an intrinsic
  motivation framework for reinforcement learning that uses pre-post prediction mismatch
  in strategy space as an exploration signal. The method introduces two complementary
  reward components: Strategy Stability (SS) measures consistency in behavioral strategy
  across temporal steps, while Strategy Surprise (SuS) captures unexpected outcomes
  relative to the agent''s current strategy representation.'
---

# SuS: Strategy-aware Surprise for Intrinsic Exploration

## Quick Facts
- arXiv ID: 2601.10349
- Source URL: https://arxiv.org/abs/2601.10349
- Reference count: 1
- Primary result: 17.4% relative improvement in Pass@1 accuracy (14.2% vs 12.1% baseline) on mathematical reasoning

## Executive Summary
This paper introduces Strategy-aware Surprise (SuS), an intrinsic motivation framework for reinforcement learning that operates in strategy space rather than raw state space. The method combines two complementary rewards: Strategy Stability (SS) measures consistency in behavioral strategy across transitions, while Strategy Surprise (SuS) captures unexpected outcomes relative to the agent's current strategy representation. The approach uses learned weighting coefficients to combine these signals and is evaluated on mathematical reasoning tasks using large language models.

## Method Summary
SuS uses a strategy encoder to map observations to learned embeddings trained via contrastive learning on trajectory data. The method computes Strategy Stability (SS) as the cosine similarity between pre-action and post-action embeddings, rewarding behavioral coherence. Strategy Surprise (SuS) combines world model prediction error with strategy dissimilarity through a multiplicative formulation, ensuring rewards only for transitions that are both unexpected and strategically meaningful. The intrinsic rewards are combined with extrinsic rewards using learned coefficients λ_SS and λ_SuS, and the method is implemented with large language models for mathematical reasoning tasks.

## Key Results
- 17.4% relative improvement in Pass@1 accuracy (14.2% vs 12.1% baseline)
- 26.4% relative improvement in Pass@5 accuracy (46.8% vs 37.1% baseline)
- Ablation studies confirm both SS and SuS components are necessary, with removal of either causing at least 10% performance degradation
- Maintains higher strategy diversity throughout training compared to baselines

## Why This Works (Mechanism)

### Mechanism 1
Operating in strategy space rather than raw state space produces more meaningful exploration signals by filtering task-irrelevant novelty. A strategy encoder maps observations to embeddings trained via contrastive learning, where states leading to similar action distributions cluster together while divergent behaviors separate.

### Mechanism 2
Strategy Stability (SS) regularizes exploration by rewarding behavioral coherence across transitions. High SS indicates the action maintained strategic coherence, preventing erratic exploration while allowing systematic pursuit of strategies.

### Mechanism 3
Strategy Surprise (SuS) addresses the "noisy TV" problem by requiring both unpredictability and strategic significance. The multiplicative formulation ensures large rewards only when transitions are both unexpected and strategically meaningful.

## Foundational Learning

- **Concept: Contrastive representation learning**
  - Why needed here: The strategy encoder uses contrastive objectives to learn embeddings where behavioral similarity corresponds to embedding proximity.
  - Quick check question: Can you explain how InfoNCE loss pushes positive pairs together and negative pairs apart in embedding space?

- **Concept: Intrinsic motivation and exploration bonuses**
  - Why needed here: SuS extends curiosity-driven exploration by computing intrinsic rewards added to extrinsic rewards.
  - Quick check question: Why does prediction error alone (as in ICM/RND) fail in environments with unpredictable but task-irrelevant features?

- **Concept: World models and forward dynamics prediction**
  - Why needed here: The SuS component requires a world model to compute prediction error.
  - Quick check question: How does a learned forward model differ from a random network target in providing novelty signals?

## Architecture Onboarding

- **Component map**: Policy (LLM) -> Strategy Encoder (E) -> World Model (M) -> Intrinsic Rewards (SS + SuS) -> Combined Reward

- **Critical path**: 1) Generate K=8 trajectories per problem using current policy, 2) Compute z_pre = E(query), z_post = E(response) for each, 3) Calculate SS = 1 - cos(z_pre, z_post), 4) Calculate SuS = |correct - P(query)| (prediction error for correctness), 5) r_int = λ_SS · SS + λ_SuS · SuS, 6) r_total = r_ext + α · r_int · 1[c=1] (intrinsic only on correct examples), 7) Update policy via GRPO; update encoder on strategy prediction loss

- **Design tradeoffs**: λ_SS vs λ_SuS balance (optimal around λ_SS=1.0, λ_SuS=0.5), intrinsic scale α=0.3 controls exploration pressure, strategy dimension d=128 balances detail vs compute

- **Failure signatures**: Entropy collapse (strategy diversity drops sharply), stagnant intrinsic rewards (SuS doesn't decrease over training), Pass@5 << Pass@1 ratio (insufficient diversity)

- **First 3 experiments**: 1) Reproduce ablation on 50-problem subset to verify component synergy, 2) Visualize strategy embedding clusters (t-SNE) colored by solution approach, 3) Sweep λ_SuS ∈ {0.25, 0.5, 1.0} with fixed λ_SS=1.0 to calibrate for your domain

## Open Questions the Paper Calls Out

### Open Question 1
Can SuS effectively generalize to high-dimensional visual domains, such as image-based reinforcement learning tasks? The authors state, "extending SuS to other domains beyond mathematical reasoning would demonstrate the generality of our approach. Image-based reinforcement learning tasks present interesting challenges for strategy representation." This remains unresolved as the current evaluation is restricted to mathematical reasoning using LLMs.

### Open Question 2
Does incorporating hierarchical strategy representations improve SuS performance by capturing multi-scale behavioral patterns? The paper suggests, "investigating hierarchical strategy representations could capture multi-scale behavioral patterns, enabling exploration at different levels of abstraction." The current flat strategy encoder may fail to distinguish between high-level strategy shifts and low-level tactical variations.

### Open Question 3
What are the theoretical exploration guarantees (e.g., coverage efficiency) for the combined SS and SuS reward formulation? The authors note, "theoretical analysis of the exploration properties induced by our reward formulation would provide deeper understanding of when and why SuS succeeds." The paper lacks formal proofs regarding convergence or state-space coverage.

## Limitations
- Performance claims are highly sensitive to the λ_SuS weighting hyperparameter, which is unspecified in main experiments despite showing λ_SuS=0.0 in Table 3
- Method's dependence on pre-trained sentence encoders for LLM strategy embeddings introduces domain-transfer uncertainty
- Strategy diversity metrics rely on clustering quality without providing validation metrics like silhouette scores

## Confidence
- **High Confidence**: Strategy Stability (SS) component effectiveness—supported by consistent ablation degradation (10-15% performance drop) and clear mechanism description
- **Medium Confidence**: Overall Pass@1 improvement (17.4% relative)—validated through ablation but dependent on unresolved λ_SuS specification
- **Low Confidence**: Cross-domain generalization—no experiments beyond mathematical reasoning, and strategy encoder architecture details are underspecified

## Next Checks
1. Verify λ_SuS hyperparameter: Re-run main experiments with λ_SuS ∈ {0.25, 0.5, 0.75} to identify the actual configuration used in Table 1
2. Cross-domain transfer test: Apply SuS to a different reasoning domain (e.g., commonsense reasoning or code generation) with the same GSM8K setup to test strategy encoder generalization
3. Strategy embedding validation: Generate t-SNE plots of learned embeddings for 100 test problems, color-coded by solution type, to visually confirm behavioral clustering claims