---
ver: rpa2
title: 'RELexED: Retrieval-Enhanced Legal Summarization with Exemplar Diversity'
arxiv_id: '2501.14113'
source_url: https://arxiv.org/abs/2501.14113
tags:
- legal
- summarization
- exemplars
- arxiv
- selection
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces RELexED, a retrieval-augmented framework
  for legal summarization that addresses the challenges of content theme deviation
  and inconsistent writing styles in existing approaches. The core innovation lies
  in a two-stage exemplar selection strategy that leverages a determinantal point
  process (DPP) to balance exemplar similarity to the query and diversity among exemplars,
  with quality and similarity scores computed using influence functions.
---

# RELexED: Retrieval-Enhanced Legal Summarization with Exemplar Diversity

## Quick Facts
- arXiv ID: 2501.14113
- Source URL: https://arxiv.org/abs/2501.14113
- Reference count: 15
- Primary result: RELexED improves ROUGE-1 from 51.17 to 54.04 on SuperSCOTUS using diverse exemplars selected via influence-based DPP

## Executive Summary
RELexED introduces a retrieval-augmented framework for legal summarization that addresses content theme deviation and inconsistent writing styles through diverse exemplar selection. The core innovation is a two-stage approach: BM25 retrieves top candidates, then a determinantal point process (DPP) selects diverse exemplars using influence function scores that capture semantic similarity beyond lexical overlap. Experiments on SuperSCOTUS and CivilSum datasets show significant improvements over models without exemplars and similarity-only retrieval methods, with gains in both lexical overlap metrics and coherence/fluency measures.

## Method Summary
RELexED employs a two-stage exemplar selection strategy for legal document summarization. First, BM25 retrieves the top 40 candidate exemplars from a training corpus. Second, a DPP selects k diverse exemplars using influence function scores computed from an auxiliary model's gradients. The kernel matrix combines quality scores (influence on the query) and pairwise similarities (influence between exemplars). The main Longformer-encoder-decoder model is fine-tuned on concatenated exemplar summaries plus the query document, with context length split between query and exemplars.

## Key Results
- ROUGE-1 improves from 51.17 to 54.04 on SuperSCOTUS when using influence-based DPP vs. no exemplars
- Inter-exemplar similarity drops from 0.81 (BM25) to 0.74 (DPP with influence functions), confirming diversity gains
- BERTScore, AlignScore, and UniEval metrics show consistent improvements across both datasets
- Influence-based DPP outperforms BM25-based DPP, demonstrating gradient similarity captures better semantic relationships

## Why This Works (Mechanism)

### Mechanism 1: Exemplar-guided template learning
Providing exemplar summaries alongside source documents reduces content theme deviation and inconsistent writing styles. Exemplar summaries explicitly encode legal writing templates and domain-specific structure, which models fail to learn implicitly from source documents alone. The model conditions on both the input and retrieved exemplars during fine-tuning.

### Mechanism 2: DPP balances quality and diversity
Determinantal Point Process (DPP) selection produces more informative exemplar sets than similarity-only retrieval. DPP models joint probability over subsets using a kernel matrix where `det(L_k)` captures both individual item quality and pairwise repulsion for diversity. The determinant formulation naturally penalizes redundant selections.

### Mechanism 3: Influence functions capture semantic relationships
Influence function-based scoring captures deeper semantic relationships than lexical similarity (BM25). TracIn computes influence as the dot product of gradients, where examples producing similar gradient updates are semantically related at the optimization level, capturing conceptual similarity beyond surface lexical overlap.

## Foundational Learning

- **Determinantal Point Processes (DPPs)**
  - Why needed here: Core mathematical framework for balancing quality vs. diversity in subset selection.
  - Quick check question: Can you explain why `det(L_Y) = q²ᵢ · q²ⱼ · (1 - s²ᵢⱼ)` penalizes similar items being selected together?

- **Influence Functions / TracIn**
  - Why needed here: Required to understand how gradient-based similarity differs from lexical similarity and how to implement efficient influence computation.
  - Quick check question: Why does the paper use only the first encoder layer for influence computation, and what tradeoff does this introduce?

- **Retrieval-Augmented Fine-Tuning**
  - Why needed here: Distinguishes this supervised approach from prompting-based retrieval; model parameters are updated.
  - Quick check question: What is the difference between providing exemplars for in-context learning vs. for supervised fine-tuning, and why does this paper choose the latter?

## Architecture Onboarding

- **Component map:** BM25 Retriever -> DPP Selector -> Auxiliary Model (influence computation) -> Main Summarizer (LED)
- **Critical path:** 1) Pre-compute influence scores using auxiliary model's first encoder layer 2) Given query, retrieve top-40 via BM25 3) Populate DPP kernel matrix (quality = influence on query; similarity = pairwise influence) 4) Greedy selection until k exemplars chosen 5) Concatenate exemplar summaries with query; feed to summarizer
- **Design tradeoffs:** First-layer-only gradients (faster but less semantic information), greedy DPP inference (approximate but tractable), summary-only exemplars (reduces context length), fixed encoder budget allocation
- **Failure signatures:** Low diversity despite DPP (check influence score normalization), OOM with 16384 token context (verify exemplar count fits budget), domain mismatch (exemplars from different jurisdictions may conflict)
- **First 3 experiments:** 1) Ablate influence function layer depth: Compare first-layer vs. full-model gradients 2) Vary k₁ (candidate pool size): Test whether 40 candidates is sufficient 3) Cross-dataset transfer: Train auxiliary model on one dataset, apply to another

## Open Questions the Paper Calls Out
None

## Limitations
- Reliance on a single auxiliary model for gradient computation introduces potential bias if that model poorly captures semantic relationships in the target domain
- Fixed exemplar count (k=4 for SuperSCOTUS, k=8 for CivilSum) may not adapt well to documents with different complexity or length distributions
- Scalability concerns for influence function computation across different legal domains and larger corpora

## Confidence
- **High confidence:** The core two-stage exemplar selection framework (BM25 + DPP) is technically sound and empirical improvements are well-documented
- **Medium confidence:** The influence function-based quality scoring provides meaningful semantic relationships, though this represents a novel application without extensive validation across domains
- **Medium confidence:** Improvements in coherence and fluency metrics directly result from exemplar diversity, though the exact contribution of diversity versus template learning is difficult to disentangle

## Next Checks
1. Conduct cross-dataset exemplar transfer: Train auxiliary model on SuperSCOTUS and apply DPP selection to CivilSum to test influence score generalization across jurisdictions
2. Test the impact of exemplar length variability by artificially constraining exemplar summary lengths and measuring performance degradation
3. Compare against stronger retrieval baselines including dense retrievers (e.g., SBERT-based) to determine if lexical retrieval limitations are the primary constraint