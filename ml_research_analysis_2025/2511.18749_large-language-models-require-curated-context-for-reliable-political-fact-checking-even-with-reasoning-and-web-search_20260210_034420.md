---
ver: rpa2
title: Large Language Models Require Curated Context for Reliable Political Fact-Checking
  -- Even with Reasoning and Web Search
arxiv_id: '2511.18749'
source_url: https://arxiv.org/abs/2511.18749
tags:
- search
- fact-checking
- gemini
- politifact
- gpt-4o
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Large language models (LLMs) struggle to reliably fact-check political
  claims, even when equipped with reasoning and web search capabilities. Researchers
  evaluated 15 recent LLMs on over 6,000 claims fact-checked by PolitiFact, testing
  standard models, reasoning-enhanced models, and web-search-enabled models.
---

# Large Language Models Require Curated Context for Reliable Political Fact-Checking -- Even with Reasoning and Web Search

## Quick Facts
- **arXiv ID:** 2511.18749
- **Source URL:** https://arxiv.org/abs/2511.18749
- **Reference count:** 40
- **Primary result:** Curated retrieval-augmented generation improves macro F1 scores by 233% for political fact-checking

## Executive Summary
Large language models struggle to reliably fact-check political claims even when equipped with reasoning and web search capabilities. Testing 15 recent LLMs on over 6,000 claims from PolitiFact revealed that standard models perform poorly, with reasoning providing minimal gains and web search only moderate improvements. However, a curated retrieval-augmented generation (RAG) system using PolitiFact summaries dramatically improved macro F1 scores by an average of 233% across all model variants. The study demonstrates that future automated fact-checking efforts should prioritize improving access to curated, high-quality fact-checking data rather than relying solely on reasoning or search capabilities.

## Method Summary
The researchers evaluated 15 recent LLMs on political fact-checking tasks using three approaches: standard models, reasoning-enhanced models, and web-search-enabled models. They tested these models on over 6,000 claims fact-checked by PolitiFact, examining performance across different model architectures and capabilities. The study compared baseline performance with curated retrieval-augmented generation using PolitiFact summaries, measuring macro F1 scores to assess overall accuracy. The evaluation included both quantitative performance metrics and analysis of failure modes across different model configurations.

## Key Results
- Standard LLMs performed poorly on political fact-checking tasks
- Reasoning enhancements provided minimal performance gains
- Web search capabilities yielded only moderate improvements
- Curated RAG using PolitiFact summaries improved macro F1 scores by 233% on average

## Why This Works (Mechanism)
The study demonstrates that LLMs require curated context rather than just reasoning or search capabilities for reliable fact-checking. The mechanism behind the dramatic improvement appears to be the quality and relevance of the provided context, which helps models overcome limitations in reasoning and information retrieval. The curated summaries provide structured, verified information that aligns with established fact-checking standards, enabling more accurate judgments than raw web search results.

## Foundational Learning
- **Macro F1 Score**: Why needed - measures overall classification accuracy across multiple classes; Quick check - calculate weighted average of precision and recall
- **Retrieval-Augmented Generation (RAG)**: Why needed - combines information retrieval with text generation; Quick check - verify retrieval step before generation
- **Curated Context**: Why needed - provides verified, structured information; Quick check - validate source quality and relevance
- **Fact-Checking Pipeline**: Why needed - systematic approach to verifying claims; Quick check - trace each step from claim to verdict
- **Model Reasoning Capabilities**: Why needed - assesses logical inference abilities; Quick check - test with structured reasoning tasks

## Architecture Onboarding

**Component Map:**
PolitiFact Database -> RAG System -> LLM -> Fact-Checking Output

**Critical Path:**
Claim Input -> Context Retrieval -> Context Integration -> Response Generation -> Fact-Checking Verdict

**Design Tradeoffs:**
- Quality vs. Coverage: Curated summaries provide high accuracy but limited scope
- Speed vs. Accuracy: RAG approaches may be slower but more reliable
- Model Size vs. Performance: Larger models show better reasoning but higher costs

**Failure Signatures:**
- False positives/negatives in standard models
- Over-reliance on search results without verification
- Inconsistent reasoning across similar claims
- Performance degradation with ambiguous claims

**First Experiments:**
1. Test RAG performance with varying context window sizes
2. Compare different retrieval algorithms (dense vs. sparse)
3. Evaluate model performance across political claim types

## Open Questions the Paper Calls Out
The study highlights several open questions including the generalizability of results to other fact-checking domains, the scalability of curated RAG approaches, and the potential for combining multiple fact-checking sources to improve reliability.

## Limitations
- Reliance on single fact-checking source (PolitiFact) may introduce bias
- Results limited to English-language political claims
- Curated RAG performance depends on availability of high-quality summaries
- Web search capabilities tested may not represent full potential of internet-enabled fact-checking

## Confidence
- **High confidence**: Standard LLMs perform poorly without curated context
- **Medium confidence**: Relative effectiveness of reasoning versus web search
- **Medium confidence**: Superiority of curated RAG approaches

## Next Checks
1. Replicate using multiple independent fact-checking sources to assess generalizability
2. Test curated RAG approach on non-political domains like scientific claims
3. Compare different RAG architectures and varying levels of context curation