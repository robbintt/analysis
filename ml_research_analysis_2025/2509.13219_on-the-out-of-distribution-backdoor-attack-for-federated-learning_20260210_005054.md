---
ver: rpa2
title: On the Out-of-Distribution Backdoor Attack for Federated Learning
arxiv_id: '2509.13219'
source_url: https://arxiv.org/abs/2509.13219
tags:
- local
- malicious
- soda
- data
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces OOD backdoor attacks (OBA) in federated learning
  (FL), where OOD data is used as both poisoned samples and triggers to bypass visible-trigger
  constraints. To enhance stealth, the authors propose SoDa, which aligns malicious
  models with benign ones using self-reference training and regularization.
---

# On the Out-of-Distribution Backdoor Attack for Federated Learning

## Quick Facts
- **arXiv ID:** 2509.13219
- **Source URL:** https://arxiv.org/abs/2509.13219
- **Reference count:** 40
- **Primary result:** OOD backdoor attacks (OBA) bypass visible-trigger constraints; SoDa achieves high attack success rates while evading defenses; BNGuard detects malicious updates via BN statistics, significantly lowering ASR and maintaining model accuracy.

## Executive Summary
This paper introduces OOD backdoor attacks (OBA) in federated learning (FL), where OOD data is used as both poisoned samples and triggers to bypass visible-trigger constraints. To enhance stealth, the authors propose SoDa, which aligns malicious models with benign ones using self-reference training and regularization. A new server-side defense, BNGuard, detects malicious updates by analyzing running statistics of the first batch normalization layer. Experiments show SoDa achieves high attack success rates while evading SOTA defenses. BNGuard demonstrates superior robustness, significantly lowering ASR and maintaining model accuracy across various settings. The approach effectively mitigates OBA threats in FL.

## Method Summary
The paper proposes SoDa, an OOD backdoor attack that uses self-reference training to align malicious updates with benign ones. Malicious clients first train a reference model on clean data, then train on poisoned data (ID + OOD) with a loss function that includes L2 and cosine similarity penalties relative to the reference. This forces malicious updates to appear benign. The defense, BNGuard, detects OOD poisoning by analyzing the running statistics of the first batch normalization layer, clustering clients based on these statistics to identify and filter malicious updates.

## Key Results
- SoDa achieves high attack success rates while evading SOTA defenses.
- BNGuard significantly lowers ASR and maintains model accuracy across various settings.
- BNGuard effectively mitigates OBA threats in FL.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Malicious clients can evade statistical defenses by regularizing model updates against a self-generated "benign" reference.
- **Mechanism:** The attacker first trains a temporary local model on clean data to create a benign reference ($\theta_{i,ref}$). Then, while training the actual malicious model on poisoned data, the loss function is constrained using an $L_2$ penalty (magnitude alignment) and Cosine Similarity penalty (direction alignment) relative to $\theta_{i,ref}$. This forces the malicious update vector to reside within the statistical distribution of benign updates.
- **Core assumption:** The server's defense relies on Euclidean distance (magnitude) or Cosine similarity (direction) of weight vectors to detect anomalies, rather than inspecting internal feature statistics.
- **Evidence anchors:**
  - [abstract] "SoDa... regularizes both the magnitude and direction... aligning them closely with their benign versions."
  - [Page 4] "SoDa ensures that each malicious model remains closely aligned with its benign version... minimizing the differences in magnitude and direction."
  - [corpus] Neighbor papers like "CS-GBA" discuss gradient-guided attacks, supporting the premise that gradient space is a common attack surface, though the specific "self-reference" method is unique to this text.
- **Break condition:** If the server analyzes features (e.g., BN statistics) rather than just weights, or if the clean local data is statistically non-representative of the global distribution, the reference model will not serve as an effective camouflage.

### Mechanism 2
- **Claim:** Running statistics of the first Batch Normalization (BN) layer reveal Out-of-Distribution (OOD) poisoning even when weight updates look benign.
- **Mechanism:** BN layers maintain running estimates of mean ($\mu_r$) and variance ($\sigma_r$) based on training data. OOD inputs (e.g., MNIST digits mixed with CIFAR-10) cause a distribution shift in the activations of the first layer. Even if the *weights* are regularized to look normal (via SoDa), the *running statistics* in the BN layers retain the signature of the OOD data exposure.
- **Core assumption:** The distribution shift in the OOD data is significant enough to create a distinguishable cluster in the BN statistics space, separating malicious clients from benign ones (even under non-IID conditions).
- **Evidence anchors:**
  - [Page 5] "BNGuard leverages the observation that OOD data causes significant deviations in the running statistics... first BN layer... most sensitive."
  - [Page 5] Figure 3 visualizes the clear separation of malicious vs. benign clusters based on these statistics.
  - [corpus] Paper "Scanning Trojaned Models Using Out-of-Distribution Samples" aligns with the intuition that OOD data properties expose trojans.
- **Break condition:** If the model architecture does not use BN layers (e.g., uses LayerNorm), or if the OOD data is statistically very similar to ID data (low distribution shift).

### Mechanism 3
- **Claim:** Using OOD data as a trigger expands attack scenarios by removing the need for physical trigger artifacts.
- **Mechanism:** The model is trained to map a specific OOD class (e.g., a specific digit or object not in the main dataset) to a target label. Because the OOD data is distinct from the main task data, the model learns a separate decision boundary for the trigger without requiring pixel-level perturbations on ID samples.
- **Core assumption:** The model has sufficient capacity to learn both the main task and the backdoor task without catastrophic forgetting of the main task.
- **Evidence anchors:**
  - [Page 3] "Uses OOD data as both poisoned samples and triggers simultaneously... naturally occur in the environment."
  - [Page 6] Empirical results show high Main Task Accuracy (MA) is maintained alongside high Attack Success Rate (ASR).
  - [corpus] Corpus evidence is weak regarding OOD-as-trigger specifically; most related works focus on perturbation-based or semantic triggers within the ID domain.
- **Break condition:** If the "OOD" trigger object actually appears in the benign test distribution (i.e., it was not truly OOD), the backdoor could fire unintentionally.

## Foundational Learning

- **Concept:** Federated Averaging (FedAvg)
  - **Why needed here:** Understanding how `Server Aggregation` works is critical. The attack relies on the fact that averaging allows a malicious update to shift the global model, while the defense relies on filtering updates *before* averaging.
  - **Quick check question:** If a malicious update is scaled by 10x but averaged with 100 benign updates of magnitude 1, how much does it shift the global model? (Answer: Not much, which is why stealth/alignment is preferred over scaling).

- **Concept:** Batch Normalization (BN) Statistics
  - **Why needed here:** The core defense (BNGuard) depends on distinguishing between "batch statistics" (calculated during training) and "running statistics" (accumulated). You must understand that running statistics capture the history of data seen by the model.
  - **Quick check question:** During inference, does a BN layer use the statistics of the current input batch or the running statistics learned during training?

- **Concept:** In-Distribution (ID) vs. Out-of-Distribution (OOD)
  - **Why needed here:** The paper exploits the gap between these two distributions. You need to grasp that OOD isn't just "noise"; it is a valid data domain (like MNIST) that is alien to the main task (CIFAR-10).
  - **Quick check question:** Why might training on OOD data harm the model's performance on ID data if no regularization is applied?

## Architecture Onboarding

- **Component map:**
  - **Benign Client:** Trains on ID data, uploads model ($\theta$).
  - **Malicious Client (SoDa):** Runs Self-Reference Training (generates $\theta_{ref}$) $\rightarrow$ Trains on Poisoned Data (ID + OOD) with Regularization Loss $\rightarrow$ Uploads $\theta_{mal}$.
  - **Server:** Receives updates $\rightarrow$ **BNGuard Module** (Extracts first BN layer stats $\rightarrow$ Clusters clients $\rightarrow$ Filters) $\rightarrow$ FedAvg Aggregation.

- **Critical path:**
  1. Implement the `SoDa` loss function ($L_{total} = L_{CE} + \lambda_m L_2 + \lambda_d L_{cos}$).
  2. Implement `BNGuard` extraction logic (accessing `running_mean` and `running_var` buffers).
  3. Validate that clustering (KMeans) correctly separates the two groups based on these buffers.

- **Design tradeoffs:**
  - **Attack:** High regularization ($\lambda$) improves stealth (evades MKrum/FLAME) but may degrade the backdoor's effectiveness (Lower ASR).
  - **Defense:** BNGuard is efficient ($O(n)$) but brittle to architectures that replace BN with LayerNorm or GroupNorm.
  - **System:** Self-reference training doubles the local computation time for malicious clients, potentially causing them to time out in synchronized FL systems.

- **Failure signatures:**
  - **Attack Failure:** Main Task Accuracy (MA) drops significantly (regularization too strong or data poisoning ratio too high).
  - **Defense Failure:** False Positive Rate (FPR) spikes (BNGuard accidentally clusters benign non-IID clients as malicious).
  - **Detection:** Benign clients have high variance in BN stats (extreme non-IID), causing the clustering logic to fail.

- **First 3 experiments:**
  1. **Baseline Check:** Run naive OBA (no stealth) against FLAME. Expect low ASR (defense wins).
  2. **Attack Validation:** Run SoDa against FLAME. Expect high ASR (attack wins).
  3. **Defense Validation:** Run SoDa against BNGuard. Expect low ASR (defense wins). Check specifically for the clustering visualization (similar to Figure 3).

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can the SoDa attack methodology be optimized to eliminate the computational overhead of the "self-reference training" phase while maintaining stealth in synchronous FL settings?
- **Basis in paper:** [explicit] The authors note that the extra training time for self-referencing "may limit the effectiveness of SoDa in synchronized FL settings" and explicitly "encourage further exploration of strategies to enhance SoDa."
- **Why unresolved:** While alternatives like using the global model or training during idle time (SoDa_A/B) are proposed, they have limitations (ineffectiveness or dependency on timing) compared to the standard method.
- **Evidence:** A new attack strategy that achieves high Attack Success Rate (ASR) and False Positive Rate (FPR) against BNGuard without requiring additional local training epochs.

### Open Question 2
- **Question:** Can defense mechanisms be developed to detect OOD backdoor attacks in FL models that do not utilize Batch Normalization (BN) layers, such as Transformers using Layer Normalization?
- **Basis in paper:** [inferred] The paper states BNGuard is "particularly targeted at defending FL systems with models that contain BN layers." The mechanism relies entirely on extracting running statistics (mean/variance) which may not exist or differ in non-BN architectures.
- **Why unresolved:** The proposed defense exploits a specific statistical property of BN layers; it is unclear how to adapt this logic to models where batch statistics are not accumulated or are normalized differently.
- **Evidence:** A defense method that successfully mitigates OBA in Transformer-based FL models with similar robustness to BNGuard.

### Open Question 3
- **Question:** Can an adaptive adversary explicitly manipulate the running statistics of the first BN layer to match benign distributions, thereby bypassing BNGuard?
- **Basis in paper:** [inferred] BNGuard relies on the observation that OOD data shifts the statistics of the *first* BN layer. The paper acknowledges that deeper layers obscure these differences, suggesting an attacker might find a way to mask abnormalities in the first layer specifically.
- **Why unresolved:** The evaluation focuses on standard attacks and variations of SoDa, but does not test an adversary aware of the defense's reliance on first-layer statistics who optimizes to spoof them.
- **Evidence:** An adaptive attack algorithm that maintains high ASR while keeping the first BN layer's running statistics within the benign cluster threshold of BNGuard.

## Limitations
- The defense relies on Batch Normalization layers, which may not be present in all architectures (e.g., Transformers using Layer Normalization).
- The attack assumes the availability of clean local data for self-reference training, which may not hold in highly constrained FL environments.
- The effectiveness of OOD as a trigger is contingent on the OOD data being truly out-of-distribution from the main task.

## Confidence
- **High confidence:** The mechanism by which SoDa aligns malicious updates with benign ones through regularization is well-specified and logically sound.
- **Medium confidence:** The effectiveness of BNGuard relies on the specific behavior of BN layers and may not generalize to other normalization techniques.
- **Low confidence:** The claim that OOD data as a trigger is a novel and effective backdoor strategy lacks strong comparative evidence against established trigger methods.

## Next Checks
1. **Architecture sensitivity test:** Evaluate BNGuard's performance when replacing BN layers with LayerNorm or GroupNorm in the target model.
2. **OOD data diversity:** Test SoDa's attack success rate using different types of OOD data (e.g., natural images vs. synthetic noise) to assess the robustness of the OOD-trigger assumption.
3. **Clean data availability:** Investigate the impact of limited clean local data on SoDa's ability to generate an effective reference model, simulating a more realistic FL constraint.