---
ver: rpa2
title: 'FNBench: Benchmarking Robust Federated Learning against Noisy Labels'
arxiv_id: '2505.06684'
source_url: https://arxiv.org/abs/2505.06684
tags:
- learning
- noisy
- online
- available
- noise
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces FNBench, the first comprehensive benchmark
  study for evaluating federated learning (FL) methods under noisy label conditions.
  The study implements an open-source framework supporting 18 state-of-the-art methods
  across three distinct label noise patterns: synthetic noise, human annotation errors,
  and systematic errors, using five image datasets and one text classification dataset.'
---

# FNBench: Benchmarking Robust Federated Learning against Noisy Labels

## Quick Facts
- **arXiv ID:** 2505.06684
- **Source URL:** https://arxiv.org/abs/2505.06684
- **Reference count:** 40
- **Primary result:** Introduces FNBench, the first comprehensive benchmark for evaluating FL methods under noisy label conditions across 18 methods and three noise patterns.

## Executive Summary
This paper presents FNBench, a comprehensive benchmark study for evaluating federated learning (FL) methods under noisy label conditions. The authors implement an open-source framework supporting 18 state-of-the-art methods across three distinct label noise patterns: synthetic noise, human annotation errors, and systematic errors. Through extensive experiments on five image datasets and one text classification dataset, they observe that noisy labels cause both memorization effects and dimensional collapse in learned representations. To address this, they propose a representation-aware regularization technique (SVD loss) that improves robustness across most evaluated methods. Key findings include surprising robustness of general FL methods against human annotation errors, no single method consistently outperforming others across all scenarios, and larger neural networks not necessarily providing better robustness to noisy labels.

## Method Summary
The FNBench framework implements a standard FL pipeline where a central server maintains a global model and selects clients per round for aggregation. Each client holds locally partitioned data with injected label noise and performs local training for a fixed number of epochs. The framework supports three noise patterns (synthetic, human annotation, systematic errors) and evaluates methods across Non-IID data partitions using Dirichlet distributions and sharding. The primary evaluation metric is F1 score computed on a held-out clean test set over the last 10 communication rounds. The benchmark includes 18 state-of-the-art methods ranging from general FL approaches (FedAvg) to specialized noisy-label learning techniques (Co-teaching, DivideMix) and Byzantine-robust aggregation methods (RFA).

## Key Results
- Noisy labels cause both memorization effects and dimensional collapse in learned representations
- General FL methods show surprising robustness against human annotation errors
- No single method consistently outperforms others across all scenarios and noise patterns
- Larger neural networks do not necessarily provide better robustness to noisy labels
- SVD loss regularization effectively mitigates dimensional collapse and enhances performance across most methods

## Why This Works (Mechanism)

### Mechanism 1: SVD Loss Regularization for Dimensional Collapse Mitigation
The SVD loss term (L_SVD = 1/d² ||K_X||²_F) penalizes the variance among singular values of the covariance matrix of batch representation matrix M. This regularization encourages different dimensions of learned representations to be uncorrelated, preventing "tail" singular values from collapsing to zero and enriching the representation space. The core assumption is that representation quality (singular value distribution) causally links to class discrimination ability and resistance to overfitting. Evidence shows noisy labels cause dimensional collapse, and SVD loss improves robustness across most methods, though it slightly degrades performance for FedELC due to conflicts with multi-loss optimization frameworks.

### Mechanism 2: Loss-Based Sample Selection via Peer Networks (Co-teaching)
Co-teaching maintains two peer networks with different initializations and uses loss-based protocols to select cleaner samples for training. Each network identifies samples with smaller loss values (presumed cleaner) and uses them to train the peer network, exploiting the "early learning" phenomenon where models learn clean patterns before overfitting to noise. The core assumptions are that clean samples have lower loss values during early training and that peer networks with different initializations will have informative disagreements. Evidence shows Co-teaching achieves top-tier F1 scores on CIFAR-10-N across multiple Non-IID settings, though it struggles with high-dimensional output spaces and variable noise rates across clients.

### Mechanism 3: Robust Aggregation via Geometric Median (RFA)
RFA aggregates client models using the geometric median instead of weighted averaging, providing Byzantine-robustness against clients with high label noise rates. The geometric median minimizes the sum of distances rather than squared distances, making it less sensitive to outliers from clients with high noise rates that deviate significantly from the "true" direction. The core assumption is that high-noise clients produce model updates that are outliers in parameter space, and such "bad" clients constitute less than 50% of participants. Evidence shows RFA is consistently the most robust method among Byzantine-robust FL methods across synthetic noise scenarios and maintains stable performance across different batch sizes and model capacities.

## Foundational Learning

- **Non-IID Data Partitioning in Federated Learning**
  - Why needed: The paper evaluates all methods under Non-IID settings as data heterogeneity is fundamental in real-world FL and interacts complexly with label noise
  - Quick check: Can you explain why averaging models trained on non-identically distributed data might lead to a global model that underperforms on all local distributions?

- **The Deep Network Memorization Effect**
  - Why needed: This effect—that DNNs fit clean data before noisy data—underpins many NLL techniques like Co-teaching and early-learning regularization
  - Quick check: During training, if a model's test accuracy first rises then falls, what might this indicate about the dataset and the model's learning dynamics?

- **Singular Value Decomposition (SVD) for Characterizing Representations**
  - Why needed: The paper uses singular values of representation covariance matrix to diagnose "dimensional collapse," a key observed phenomenon
  - Quick check: If the largest few singular values of a representation covariance matrix dominate and the rest are near zero, what does this imply about the diversity and information content of the learned features?

## Architecture Onboarding

- **Component map:** Central Server -> Client Selection -> Local Training -> Model Aggregation -> Global Model Update
- **Critical path:** To run a baseline experiment: 1) Select configuration (dataset, model), 2) Configure data partitioning and noise pattern, 3) Select robustness method, 4) Execute federated training loop, 5) Collect and average F1 scores from final 10 rounds
- **Design tradeoffs:**
  - Method Complexity vs. Robustness: Complex methods like DivideMix are computationally expensive but sometimes match simpler methods on human-annotation noise
  - Batch Size vs. Communication Budget: Smaller batches yield better model quality under limited communication but increase per-round computation time
  - Model Capacity vs. Overfitting: Larger models do not guarantee better robustness and can overfit more easily to noisy labels
- **Failure signatures:**
  - Extreme Performance Drop (e.g., Krum): Near-zero F1 scores indicate failure in aggregation logic from discarding too much client information
  - Training Collapse (e.g., RFL on CIFAR-100): Near-random performance suggests core mechanism fails to scale to high-class-count datasets
  - High Peer Network Divergence: Large performance gap (>10%) between peer networks signals unstable training due to cumulative weight drift
- **First 3 experiments:**
  1. Reproduce Core Baseline: Run FedAvg on CIFAR-10 with Dirichlet (β=1.0) partitioning and symmetric noise (0.0-0.4)
  2. Evaluate Representative Robust Method: Run SVD-loss-augmented FedAvg and compare F1 scores and singular value distributions
  3. Test on Human Annotation Noise: Run FedAvg, Co-teaching, RFA on CIFAR-10-N under Non-IID partition to validate surprising robustness of simple methods

## Open Questions the Paper Calls Out

- How does the client participation rate impact model robustness and convergence in federated noisy label learning? The benchmark keeps participation rate fixed (10/100 clients) to isolate label noise effects, but varying this could reveal important relationships between participation and robustness.

- Why do larger neural networks fail to provide better robustness than smaller models in FNLL settings? The paper observes larger models perform worse, possibly due to overfitting or optimization difficulties, but lacks theoretical explanation or mitigation techniques.

- Can end-to-end label correction mechanisms be effectively integrated into FL pipelines to handle heterogeneous noisy clients? Current methods rely on complex multi-stage training or unreliable global supervision, creating opportunities for simpler yet effective correction approaches that maintain privacy.

## Limitations

- Findings may not generalize beyond benchmarked image datasets to domains with different data distributions or noise characteristics
- Focus on F1 score as primary metric may miss other important aspects like calibration, fairness across classes, or privacy implications
- Computational cost of implementing 18 different methods creates practical barriers for widespread adoption and independent verification

## Confidence

- **High Confidence**: Core observation that noisy labels cause dimensional collapse is well-supported by empirical evidence across multiple datasets and methods; finding that general FL methods show surprising robustness to human annotation errors is consistently demonstrated
- **Medium Confidence**: SVD loss regularization shows consistent improvements across most methods with some exceptions; RFA's effectiveness for Byzantine-robustness is well-demonstrated but may not extend to all noise scenarios
- **Low Confidence**: Claim that no single method consistently outperforms others may reflect specific experimental conditions rather than fundamental property; conclusion about larger models not providing better robustness requires validation on additional architectures

## Next Checks

1. **Cross-Domain Generalization**: Test SVD regularization and top-performing methods on non-image datasets (medical imaging, time-series) to assess whether dimensional collapse mitigation generalizes beyond benchmarked domains

2. **Noise Heterogeneity Stress Test**: Systematically evaluate performance breakdown points by creating extreme Non-IID scenarios with highly heterogeneous noise rates across clients, particularly focusing on RFA and Co-teaching under conditions where "bad" clients approach 50%

3. **Representation Space Analysis**: Beyond singular value distributions, conduct additional analysis of learned representations (UMAP visualization, class separability metrics) to verify that SVD regularization produces qualitatively better feature spaces that explain observed F1 score improvements