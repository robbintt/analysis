---
ver: rpa2
title: When Vision Meets Texts in Listwise Reranking
arxiv_id: '2601.20623'
source_url: https://arxiv.org/abs/2601.20623
tags:
- reranking
- training
- text
- multimodal
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Rank-Nexus is a lightweight multimodal reranker that processes
  image-text documents through progressive training, starting with text reranking,
  then image pairwise ranking, and finally joint listwise ranking. The model uses
  knowledge distillation from large language models to generate training data, employs
  diversity-based coreset selection to minimize data requirements, and operates on
  a compact 2B parameter vision-language model.
---

# When Vision Meets Texts in Listwise Reranking

## Quick Facts
- arXiv ID: 2601.20623
- Source URL: https://arxiv.org/abs/2601.20623
- Authors: Hongyi Cai
- Reference count: 40
- Primary result: State-of-the-art multimodal reranking with 2B parameters, 2-8× faster than reasoning-based rerankers

## Executive Summary
Rank-Nexus introduces a lightweight multimodal reranker that processes image-text documents through progressive training, starting with text reranking, then image pairwise ranking, and finally joint listwise ranking. The model uses knowledge distillation from large language models to generate training data, employs diversity-based coreset selection to minimize data requirements, and operates on a compact 2B parameter vision-language model. Rank-Nexus achieves state-of-the-art performance on both text benchmarks (DL19, DL20, BEIR) and multimodal benchmarks (INQUIRE, MMDocIR) while maintaining inference speeds 2-8× faster than reasoning-based rerankers, demonstrating that effective multimodal reranking can be achieved with minimal data and parameters.

## Method Summary
Rank-Nexus employs a progressive training strategy that leverages knowledge distillation from LLMs to generate synthetic training data. The approach begins with text-only reranking using ColQwen retriever outputs, then extends to image pairwise ranking, and finally integrates both modalities through joint listwise ranking. A diversity-based coreset selection strategy minimizes data requirements by selecting representative samples from the synthetic dataset. The model architecture uses a 2B parameter VLM that processes image-text pairs through cross-modal attention mechanisms, enabling efficient computation while maintaining competitive performance across benchmarks.

## Key Results
- Achieves state-of-the-art performance on INQUIRE multimodal benchmark with +2.8 nDCG improvement over text-trained baselines
- Matches or exceeds proprietary models like GPT-4o on DL19, DL20, BEIR text benchmarks using only 2B parameters
- Demonstrates data efficiency with optimal performance at 4K samples, showing diminishing returns beyond this point
- Achieves 2-8× faster inference speeds compared to reasoning-based rerankers while maintaining competitive accuracy

## Why This Works (Mechanism)
Rank-Nexus succeeds through progressive training that gradually builds cross-modal understanding, starting with modality-specific ranking before combining both inputs. The knowledge distillation from LLMs provides high-quality synthetic training data without requiring extensive human annotations. The diversity-based coreset selection ensures the model learns from representative examples rather than memorizing training data. The 2B parameter architecture balances computational efficiency with sufficient capacity for multimodal reasoning. Cross-modal attention mechanisms enable the model to effectively integrate visual and textual information during ranking decisions.

## Foundational Learning

**Vision-Language Models (VLMs)** - Multimodal architectures that process both visual and textual inputs through shared representation spaces. Needed to handle image-text document pairs; quick check: verify cross-modal attention weights during training.

**Knowledge Distillation** - Transfer learning technique where a smaller model learns from a larger teacher model's outputs. Required for generating synthetic training data without manual annotations; quick check: compare distilled data quality against human-annotated examples.

**Listwise Ranking** - Ranking approach that considers entire document lists rather than pairwise comparisons. Essential for realistic retrieval scenarios; quick check: validate nDCG improvements over pairwise methods on benchmark datasets.

**Progressive Curriculum Learning** - Training strategy that starts with easier tasks and progressively increases difficulty. Helps the model build foundational skills before tackling complex multimodal ranking; quick check: measure performance gains at each training stage.

**Coreset Selection** - Subset selection method that maintains dataset diversity while minimizing size. Critical for data efficiency; quick check: analyze diversity metrics of selected vs. random subsets.

**Cross-Modal Attention** - Mechanism allowing visual and textual features to interact during processing. Enables effective integration of multimodal information; quick check: visualize attention maps between image and text regions.

## Architecture Onboarding

**Component Map**
Retriever (ColQwen) -> Synthetic Data Generator (LLM distillation) -> Progressive Trainer (text → image → joint) -> 2B VLM -> Rank-Nexus

**Critical Path**
1. ColQwen retrieves candidate documents from collection
2. LLM distillation generates synthetic relevance labels
3. Progressive training pipeline processes data through curriculum stages
4. 2B VLM applies cross-modal attention to rank documents
5. Output provides ranked list with scores

**Design Tradeoffs**
The 2B parameter constraint prioritizes inference speed over absolute performance, accepting some accuracy loss compared to larger models. Progressive training adds complexity but enables data efficiency. Knowledge distillation reduces annotation costs but depends on teacher model quality. Coreset selection minimizes data needs but may miss edge cases.

**Failure Signatures**
Struggles with extremely fine-grained visual details in complex technical diagrams. May underperform on documents requiring deep reasoning compared to larger proprietary models. Cross-modal alignment issues can occur when visual and textual information conflicts.

**First Experiments**
1. Test progressive training ablation by training joint model directly without curriculum
2. Compare diversity-based coreset selection against random sampling for data efficiency
3. Measure cross-modal transfer by evaluating image ranking after text-only training

## Open Questions the Paper Calls Out

**Open Question 1**
Can end-to-end differentiable retrieval and ranking further close the modality gap compared to the current two-stage pipeline?
- Basis in paper: Authors state: "our pipeline currently relies on a specific retriever (ColQwen); future work should investigate end-to-end differentiable retrieval and ranking to further close the modality gap."
- Why unresolved: The current pipeline separates retrieval and reranking, potentially losing cross-modal alignment information that joint optimization could preserve.
- What evidence would resolve it: Experiments comparing two-stage vs. end-to-end trained models on identical benchmarks, measuring whether joint training improves multimodal ranking metrics.

**Open Question 2**
How does the progressive training strategy perform when reversed (image-first then text)?
- Basis in paper: The paper shows surprising cross-modal transfer where text training improves image ranking (+2.8 nDCG on INQUIRE), but doesn't test bidirectional transfer.
- Why unresolved: Understanding whether the curriculum is asymmetric or universally applicable regardless of modality ordering remains unexplored.
- What evidence would resolve it: Ablation experiments training image→text progression, comparing against the text→image strategy.

**Open Question 3**
What specific visual features cause failures in complex technical diagrams?
- Basis in paper: Authors acknowledge: "our 2B model occasionally struggles with extremely fine-grained visual details in complex technical diagrams compared to larger proprietary models like GPT-4o."
- Why unresolved: The failure modes aren't characterized—whether due to resolution, diagram complexity, or fine-grained text within images remains unclear.
- What evidence would resolve it: Fine-grained error analysis on technical diagram subsets comparing Rank-Nexus against GPT-4o rankings.

## Limitations

- Effectiveness depends on quality of synthetic training data from LLM distillation, with no analysis of sensitivity to different teacher models
- Benchmark evaluation focuses on image-text pairs without addressing documents containing multiple images or more complex multimodal inputs
- Inference speed comparisons lack detailed methodology, making hardware configuration impacts unclear
- Model struggles with extremely fine-grained visual details in complex technical diagrams compared to larger proprietary models

## Confidence

**State-of-the-art performance claims (High confidence)**: Experimental results on established benchmarks (DL19, DL20, BEIR, INQUIRE, MMDocIR) are well-documented and reproducible, with clear baseline comparisons.

**Data efficiency and minimal parameter claims (Medium confidence)**: While the model uses a 2B parameter VLM and claims reduced data requirements, the paper lacks detailed ablation studies on minimum effective dataset sizes and the impact of varying coreset selection parameters.

**Inference speed improvements (Medium confidence)**: The claimed 2-8× speedup over reasoning-based rerankers is plausible given the model architecture, but the timing methodology is not sufficiently detailed for independent verification.

## Next Checks

1. Conduct ablation studies varying the coreset selection diversity threshold and dataset sizes to determine the minimum effective training requirements for maintaining performance.

2. Perform timing measurements across different hardware configurations and compare against a broader range of reasoning-based rerankers to validate the 2-8× speedup claim under standardized conditions.

3. Test Rank-Nexus on more complex multimodal retrieval scenarios, including documents with multiple images and mixed-modality queries, to assess real-world applicability beyond the current benchmark scope.