---
ver: rpa2
title: 'Integrating Domain Knowledge for Financial QA: A Multi-Retriever RAG Approach
  with LLMs'
arxiv_id: '2512.23848'
source_url: https://arxiv.org/abs/2512.23848
tags:
- retriever
- generator
- internal
- external
- secbert
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study addresses the challenge of financial numerical reasoning
  in Question Answering tasks, where current Large Language Models struggle due to
  a lack of domain knowledge and complex multi-step numeric reasoning. To address
  this, the authors implement a multi-retriever Retrieval Augmented Generation system
  that retrieves both internal query contexts and external financial domain knowledge,
  using either a prompt-based LLM or a symbolic neural generator to produce answers.
---

# Integrating Domain Knowledge for Financial QA: A Multi-Retriever RAG Approach with LLMs

## Quick Facts
- arXiv ID: 2512.23848
- Source URL: https://arxiv.org/abs/2512.23848
- Reference count: 25
- Authors: Yukun Zhang; Stefan Elbl Droguett; Samyak Jain
- Key outcome: Multi-retriever RAG combining internal context and external domain knowledge improves financial numerical reasoning accuracy, with effects mediated by model size.

## Executive Summary
This study addresses financial numerical reasoning challenges in Question Answering tasks, where current Large Language Models struggle due to insufficient domain knowledge and complex multi-step numeric reasoning. The authors implement a multi-retriever Retrieval Augmented Generation system that retrieves both internal query contexts and external financial domain knowledge, using either prompt-based LLMs or symbolic neural generators to produce answers. Comprehensive experiments show that domain-specific training with the SecBERT encoder significantly improves performance, with the best neural symbolic model surpassing the FinQA paper's top model. The best prompt-based LLM achieves state-of-the-art performance with over 7% improvement, though it remains below human expert performance. The study also reveals a trade-off between hallucination loss and external knowledge gains in smaller models, while larger models benefit more from external facts. Finally, the research confirms the enhanced numerical reasoning capabilities of the latest LLM optimized for few-shot learning.

## Method Summary
The authors implement a dual-retriever RAG system combining internal context extraction from financial reports and external domain knowledge retrieval from financial terminology dictionaries. The internal retriever uses fine-tuned BERT-family classifiers to extract relevant supporting facts from long financial report contexts (avg 687 tokens, max 2,679). The external retriever employs DPR-FAISS to fetch domain definitions from FinRAD (13K+ terms). Retrieved contexts are concatenated as generator inputs, routed to either prompt-based generators (Gemini-1.0-pro/1.5-pro with instruction + few-shot prompts) or neural-symbolic generators (BERT-family encoder + LSTM decoder producing executable program steps). The system achieves state-of-the-art performance on the FinQA dataset through this multi-retriever approach.

## Key Results
- Domain-specific training with SecBERT encoder significantly improves neural symbolic model performance, surpassing FinQA paper's top model
- Best prompt-based LLM (Gemini-1.5-pro) achieves state-of-the-art performance with over 7% improvement in execution accuracy
- Trade-off exists between hallucination loss and external knowledge gains in smaller models, while larger models benefit more from external facts
- Latest LLM optimized for few-shot learning shows enhanced numerical reasoning capabilities

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multi-retriever RAG combining internal context and external domain knowledge improves financial numerical reasoning accuracy, with effects mediated by model size.
- Mechanism: The internal retriever (fine-tuned BERT-family classifier) extracts relevant supporting facts from long financial report contexts. The external retriever (DPR-FAISS) fetches domain definitions from FinRAD. Retrieved contexts are concatenated as generator inputs.
- Core assumption: Financial QA errors stem partially from lack of domain knowledge; explicit terminology definitions help models locate and reason over correct numerical values.
- Evidence anchors:
  - [abstract]: "implement a multi-retriever RAG system to retrieve both external domain knowledge and internal question contexts"
  - [section]: Table 4 shows execution accuracy improving from 41.84% → 66.02% → 69.37% as internal, few-shot, and external retrievers are added to Gemini-1.5-pro
  - [corpus]: "Select to Know" paper (arxiv 2508.15213) proposes internal-external knowledge self-selection for domain QA, corroborating the architectural pattern
- Break condition: When retrieved external facts are irrelevant or when input truncation causes context loss for smaller models.

### Mechanism 2
- Claim: Domain-specific pre-training on financial documents (SecBERT) improves retrieval recall and downstream execution accuracy compared to general-purpose encoders.
- Mechanism: SecBERT is pre-trained on 260,773 10-K financial documents, encoding domain-specific semantic patterns. This yields better binary classification for internal retrieval and higher similarity scores for external retrieval.
- Core assumption: Domain-specific representations transfer better to financial QA than general-purpose models.
- Evidence anchors:
  - [abstract]: "domain-specific training with the SecBERT encoder significantly contributes to our best neural symbolic model surpassing the FinQA paper's top model"
  - [section]: Table 1: SecBERT achieves 91.27% top-3 recall vs. 88.96% for BERT Base; Table 3: SecBERT retriever + RoBERTa Large generator achieves 63.48% execution accuracy vs. 60.02% baseline
  - [corpus]: Weak direct corpus evidence on SecBERT specifically; related papers focus on financial LLM evaluation
- Break condition: When similarity scores don't correlate with actual relevance—Section 5.1 shows SecBERT achieves higher median similarity (0.9287) but lower retrieval quality than DPR (0.8583) for external facts.

### Mechanism 3
- Claim: Larger LLMs (Gemini-1.5-pro) benefit from few-shot prompting and external knowledge, while smaller models (Gemini-1.0-pro, RoBERTa Base) experience hallucination loss that can offset gains.
- Mechanism: Few-shot examples guide reasoning patterns; external facts provide domain context. Smaller models fail to distinguish few-shot context from query context and suffer truncation-induced hallucinations. Larger models better integrate multiple information sources.
- Core assumption: Model capacity determines the ability to contextualize retrieved/exemplar information without conflating it with the target query.
- Evidence anchors:
  - [abstract]: "trade-off between hallucinations loss and external knowledge gains in smaller models and few-shot examples. For larger models, the gains from external facts typically outweigh the hallucination loss."
  - [section]: Table 4: Gemini-1.0-pro with few-shot drops from 39.30% → 22.03%; Gemini-1.5-pro with few-shot + external rises to 69.37%; Section 5.3 describes Gemini-1.0-pro "directly using numerical answer 0.4949 from the first example"
  - [corpus]: "CausalRAG" paper (arxiv 2503.19878) notes traditional RAG faces "disrupted contextual integrity," aligning with observed hallucination trade-offs
- Break condition: When few-shot examples introduce conflicting numerical values or when external context pushes input beyond 512-token limits.

## Foundational Learning

- Concept: **Dense Passage Retrieval (DPR) with FAISS**
  - Why needed here: The external retriever uses DPR-FAISS for fast similarity search over 13K+ financial term definitions. Understanding embedding normalization, inner-product similarity, and index selection is required to debug retrieval quality.
  - Quick check question: Given a query embedding q and document embedding d, how does L2 normalization before inner-product search convert the operation to cosine similarity?

- Concept: **Neural-Symbolic Program Generation**
  - Why needed here: The symbolic neural generator produces executable program steps (e.g., `divide(#0, #1)`) rather than direct answers. Understanding token vocabularies (operations, constants, step memory) and constrained decoding is essential.
  - Quick check question: If the model outputs index "5" at decoding step t, and vocabulary maps 5→"subtract(", how does the decoder ensure subsequent tokens form a valid program?

- Concept: **Execution vs. Program Accuracy Trade-offs**
  - Why needed here: The paper reports both metrics—execution accuracy (answer correctness) and program accuracy (reasoning path correctness). False positives affect execution; false negatives affect program accuracy.
  - Quick check question: A model outputs `divide(100, 50)` producing answer 2, but the ground-truth program is `multiply(1, 2)`. What is execution accuracy? Program accuracy?

## Architecture Onboarding

- Component map: FinQA data -> Table-to-text conversion -> Internal Retriever (SecBERT/BERT-family) -> External Retriever (DPR-FAISS on FinRAD) -> Generator (Prompt-based or Neural-symbolic) -> Program/Answer output

- Critical path:
  1. Preprocess FinQA data: table rows → sentence format (e.g., "Risk-free interest rate of 2006 is 5%")
  2. Preprocess FinRAD: LLM-based one-sentence summaries of definitions
  3. Fine-tune internal retriever on labeled supporting facts (binary classification)
  4. Build FAISS index with normalized embeddings for external retriever
  5. Retrieve top-k internal + top-3 external contexts per query
  6. Route to generator: prompt-based (zero/few-shot) or neural-symbolic (trained 20 epochs)
  7. Evaluate: execution accuracy (numerical match with ε tolerance) and program accuracy

- Design tradeoffs:
  - **SecBERT vs. DPR for external retrieval**: SecBERT yields higher similarity scores but lower relevance; DPR is optimized for passage retrieval. Paper chooses DPR for external after human evaluation.
  - **Model size vs. hallucination risk**: Larger generators (RoBERTa Large, Gemini-1.5-pro) benefit from external facts; smaller generators (RoBERTa Base, FinBERT) may see neutral or negative effects.
  - **Few-shot prompting**: Only effective for larger LLMs; smaller models conflate example and query contexts.

- Failure signatures:
  - **Irrelevant external retrieval**: High similarity score but semantically unrelated definitions (e.g., SecBERT returning "zero plus tick" for American Express queries)
  - **Few-shot context bleeding**: Model using numbers from few-shot examples instead of query context (Gemini-1.0-pro behavior)
  - **Truncation-induced hallucinations**: Inputs exceeding 512 tokens lose sentence boundaries, degrading smaller model performance
  - **Unit conversion failures**: 65% of errors involve unit mismatches (e.g., outputting 7,222.22 instead of 7.22 billion)

- First 3 experiments:
  1. **Ablate external retriever**: Compare SecBERT internal-only vs. SecBERT + DPR-FAISS external on dev set execution accuracy. Expect +1-2% for larger generators, neutral/negative for smaller.
  2. **Test few-shot stability**: Run Gemini-1.0-pro vs. Gemini-1.5-pro with identical few-shot prompts on 50 dev samples. Manually inspect for context bleeding (using example numbers in outputs).
  3. **Measure retrieval-relevance gap**: For 50 random queries, compute similarity scores from SecBERT vs. DPR encoders; have humans label relevance. Quantify the gap between similarity and relevance.

## Open Questions the Paper Calls Out

- **Open Question 1**: Does extending training for the neural symbolic generator beyond 20 epochs yield convergence and significant accuracy gains? The authors state that "loss continues to decrease after 20 epochs" and suggest "additional future training could further improve performance." Computational constraints limited the team to only 20 epochs, whereas the baseline model trained for 300.

- **Open Question 2**: Can a fine-tuned NER system for extracting key financial terms mitigate hallucinations caused by context truncation? The authors propose future work to "annotate some data and fine-tune an NER system" to create "better and shorter external knowledge retrieval." Current external definitions can be long (up to 667 tokens), causing inputs to exceed the 512-token limit, which truncates context and induces errors.

- **Open Question 3**: Is the "hallucination loss" observed in smaller models primarily caused by the inclusion of irrelevant retrieved facts or by the truncation of input tokens? The paper notes a trade-off where external facts help large models but hurt small ones, speculating that "incomplete sentences may become irrelevant," but the exact cause is not isolated.

## Limitations

- **Context Dependency on Retriever Quality**: The reported gains critically depend on retriever quality, with SecBERT showing higher similarity scores but lower relevance for external retrieval.
- **Hallucination Measurement Challenge**: Quantifying hallucinations in financial numerical reasoning is complex - distinguishing hallucination from inadequate context lacks systematic methodology.
- **FinRAD Preprocessing Ambiguities**: One-sentence summaries generated by LLM for 13K+ definitions may have variable quality, potentially explaining inconsistent retrieval performance.

## Confidence

**High Confidence** (Evidence directly supports claims):
- SecBERT's domain-specific training improves internal retrieval recall
- Gemini-1.5-pro with few-shot prompts achieves state-of-the-art performance
- Multi-retriever RAG architecture outperforms single-retriever approaches

**Medium Confidence** (Evidence supports but with caveats):
- Domain-specific encoders improve downstream execution accuracy
- Trade-off between hallucinations and knowledge gains exists
- Larger models benefit more from external facts than smaller models

**Low Confidence** (Claims under-supported or methodologically unclear):
- SecBERT is definitively better than DPR for external retrieval (paper actually concludes DPR is better after human evaluation)
- FinRAD definitions are consistently summarized at appropriate granularity

## Next Checks

1. **Replicate the SecBERT vs DPR External Retrieval Comparison**: Run SecBERT and DPR encoders on the same 50 random FinRAD queries used in the paper's human evaluation. Compute both similarity scores and have independent annotators label relevance. Quantify the gap between high similarity and low relevance.

2. **Test Hallucination Attribution Protocol**: Select 100 queries where Gemini-1.0-pro fails with few-shot prompting. For each failure, have two independent annotators determine: (a) whether the answer number appears in the few-shot examples, (b) whether the answer number appears in retrieved contexts, and (c) whether the answer is mathematically plausible given the query.

3. **Measure FinRAD Summary Consistency**: Take 100 random FinRAD definitions and generate three different one-sentence summaries using different prompts with Gemini-1.0-pro. Compute inter-annotator agreement on whether the summaries capture the same essential meaning.