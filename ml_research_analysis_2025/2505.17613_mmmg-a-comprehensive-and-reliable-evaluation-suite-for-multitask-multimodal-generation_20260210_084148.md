---
ver: rpa2
title: 'MMMG: a Comprehensive and Reliable Evaluation Suite for Multitask Multimodal
  Generation'
arxiv_id: '2505.17613'
source_url: https://arxiv.org/abs/2505.17613
tags:
- image
- evaluation
- audio
- generation
- example
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'MMMG is a comprehensive benchmark for evaluating multimodal generation
  across four modalities: image, audio, interleaved text-image, and interleaved text-audio.
  It addresses the challenge of aligning automated evaluation with human judgment
  by designing tasks that are either programmatically verifiable or have significant
  generation-evaluation gaps.'
---

# MMMG: a Comprehensive and Reliable Evaluation Suite for Multitask Multimodal Generation

## Quick Facts
- **arXiv ID**: 2505.17613
- **Source URL**: https://arxiv.org/abs/2505.17613
- **Reference count**: 40
- **Key outcome**: Benchmark for multimodal generation across four modalities (image, audio, interleaved text-image, interleaved text-audio) with 49 tasks and 937 instructions, validated to achieve 94.3% human alignment

## Executive Summary
MMMG addresses the challenge of evaluating multimodal generation models by creating tasks where generation is difficult but verification is simple. The benchmark exploits generation-evaluation gaps to enable reliable automated evaluation through a combination of VLMs, audio similarity models, and programmatic checks. Benchmarking 24 models reveals that while GPT Image leads in image generation (78.3% accuracy), audio generation shows significant room for improvement, and interleaved generation remains challenging for autoregressive models.

## Method Summary
MMMG evaluates multimodal generation models using 49 tasks and 937 instructions across four modality combinations. The evaluation pipeline matches task types to specific methods: VLMs for visual reasoning (GPT-4O, Gemini 2.5, Qwen 2.5-VL), CLAPScore for audio similarity, DreamSim/SSIM for image similarity, and programmatic verification for format constraints. Generation uses 4 samples per instruction with temperature 0 for MLMs and 200 steps for diffusion. Tasks are validated to achieve 94.3% human alignment, with evaluation methods selected based on task characteristics to maximize reliability.

## Key Results
- GPT Image achieves 78.3% accuracy in image generation but struggles with multimodal reasoning and interleaved generation
- Audio generation shows the greatest performance gap, with state-of-the-art models achieving only 0.048 average accuracy
- Fine-grained capability analysis reveals that while models excel at simple generation tasks, they struggle with complex constraints and reasoning across modalities
- Interleaved generation models show modality order control issues, with autoregressive models tangling multiple images

## Why This Works (Mechanism)

### Mechanism 1: Generation-Evaluation Gap Exploitation
- Claim: Tasks with high generation difficulty but simple verification enable reliable automated evaluation aligned with human judgment
- Mechanism: MMMG selects tasks where generation requires complex constraint satisfaction (e.g., "snowman without carrot nose"), but evaluation can be reduced to straightforward checks via VLM prompting or programmatic verification—decoupling task difficulty from evaluation reliability
- Core assumption: Current multimodal models can reliably verify specific constraints even when they struggle to generate content satisfying those constraints
- Evidence anchors:
  - [section 1, page 2]: "tasks with significant generation-evaluation gaps, where the generation step is challenging due to complex constraints, yet the evaluation step remains simple"
  - [figure 1, page 2]: Shows pseudo-code evaluation for border color checking and CLAPScore-based audio verification
  - [corpus]: Limited direct evidence—related work focuses on interleaved generation datasets, not evaluation methodology gaps
- Break condition: If VLMs fail to reliably verify constraints (e.g., hallucinate verification responses), the gap mechanism collapses

### Mechanism 2: Hybrid Model-Program Evaluation Pipeline
- Claim: Combining VLMs, audio similarity models, and programmatic checks achieves higher human alignment than any single evaluation method
- Mechanism: MMMG matches evaluation method to task characteristics—VQA-style VLM prompting for visual reasoning, CLAPScore for audio similarity, SSIM/DreamSim for image similarity, and programmatic verification for format constraints
- Core assumption: Task-specific evaluation method selection outperforms universal evaluation approaches
- Evidence anchors:
  - [abstract]: "enabling reliable automatic evaluation through a combination of models and programs"
  - [table 2, page 5]: Shows 12+ distinct evaluation method combinations across 49 tasks
  - [section 5.1, page 8]: "GPT-4O remains the most human-aligned image evaluation model... GEMINI 2.5 shows superior performance on spatial relationships"
  - [corpus]: Judge Model for Large-scale Multimodality Benchmarks (arXiv 2601.06106) proposes similar dedicated judge models but lacks MMMG's task-specific method selection
- Break condition: If evaluation method selection requires excessive manual tuning or fails to generalize to new tasks, the hybrid approach becomes impractical

### Mechanism 3: Constraint Verification via Chain-of-Thought Prompting
- Claim: Structured VLM evaluation prompts with chain-of-thought reasoning and negative constraints reduce hallucination in verification
- Mechanism: MMMG designs evaluation prompts that force VLMs to explain reasoning before concluding, use multiple-choice formats to constrain output space, and include negative prompts to counter spurious correlations
- Core assumption: Prompt engineering can systematically reduce VLM verification errors without model retraining
- Evidence anchors:
  - [section 3.2, page 6]: "Chain-of-thought prompting significantly improves VLM performance on boolean questions... Multiple-choice format can boost VLM's performance on object counting"
  - [section 3.2, page 7]: "Adding negative prompts helps alleviate visual hallucination"
  - [corpus]: Multimodal RewardBench 2 (arXiv 2512.16899) evaluates reward models for interleaved generation but doesn't address verification prompt design
- Break condition: If prompt-based improvements plateau or task-specific prompt design becomes prohibitively expensive, this approach won't scale

## Foundational Learning

- **Concept: Generation-Evaluation Gap**
  - Why needed here: Understanding that evaluation reliability doesn't require generation capability—models can verify what they cannot reliably generate
  - Quick check question: Can you identify a multimodal task where GPT-4V would fail to generate correct output but could reliably verify correctness?

- **Concept: VLM-as-a-Judge Limitations**
  - Why needed here: Recognizing where VLM evaluators fail (spurious correlations, out-of-domain scenarios) informs when to use programmatic verification instead
  - Quick check question: For counting 15+ objects in an image, would you trust VLM direct counting or a multiple-choice verification prompt?

- **Concept: Interleaved Multimodal Consistency**
  - Why needed here: Evaluating interleaved generation requires checking cross-modal temporal and semantic alignment, not just per-modality quality
  - Quick check question: If a model generates "Image 1: cat" followed by text "The dog is sleeping," what evaluation signal captures the inconsistency?

## Architecture Onboarding

- **Component map:**
  Data Curation Pipeline -> Evaluation Router -> Evaluation Executors -> Scoring Aggregator

- **Critical path:**
  1. Identify task category (image/audio/interleaved) and constraint type (format/semantic/spatial)
  2. Select evaluation method with highest human alignment for that task category
  3. Execute evaluation with appropriate model/program combination
  4. Aggregate scores maintaining task-level granularity for fine-grained analysis

- **Design tradeoffs:**
  - Proprietary model dependency (GPT-4O/Gemini 2.5) vs. reproducibility—open-source VLMs show significant performance gap
  - Task coverage vs. evaluation reliability—some useful tasks excluded due to unreliable evaluation methods (table 4, page 16-17)
  - Instruction diversity vs. verifiability—10-50% pass rate depending on task difficulty

- **Failure signatures:**
  - VLM hallucination on object counting (>10 objects) or complex spatial reasoning
  - CLAPScore failure on out-of-domain audio without quality reference audio
  - Autoregressive models tangling multiple images in interleaved generation (figure 3, page 8)
  - Audio generation models failing silence generation (0.048 average accuracy)

- **First 3 experiments:**
  1. Run evaluation on a single image generation task (e.g., Object Inclusion) with all three VLM options (GPT-4O, Gemini 2.5, Qwen 2.5-VL) to observe human alignment variance
  2. Test CLAPScoreaudio vs. CLAPScoretext on a sound generation task to understand reference audio dependency
  3. Evaluate an interleaved image-text model (e.g., Gemini Image) with and without structured system prompts (table 14, page 27) to measure modality order control capability

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific architectural or training advancements are required to close the significant performance gap in audio generation, where state-of-the-art models currently achieve less than 50% accuracy on reasoning and exclusion tasks?
- Basis in paper: [explicit] The abstract and conclusion state that results suggest "considerable headroom for improvement in audio generation, highlighting an important direction for future research," with specific poor performance noted in instrument exclusion and sound reasoning.
- Why unresolved: Current models show specific weaknesses in audio reasoning and fine-grained control (e.g., volume, silence) that differ from the strengths seen in image generation models, but the paper only benchmarks rather than proposing solutions.
- What evidence would resolve it: The development of a model that achieves >80% accuracy on the audio-specific tasks within MMMG, particularly "Sound Reasoning" and "Instrument Exclusion."

### Open Question 2
- Question: How well does MMMG evaluation scoring correlate with real-world human preference leaderboards for non-image modalities, specifically interleaved text-image and audio generation?
- Basis in paper: [explicit] Section 5.3 notes, "Due to the lack of real-world human preference leaderboards like Chatbot Arena for other modalities, we leave human preference correlation studies for other modalities as future work."
- Why unresolved: The paper validates correlation with Chatbot Arena only for image generation, leaving the reliability of MMMG's automated metrics for audio and interleaved formats unverified against large-scale human preferences.
- What evidence would resolve it: A study calculating Pearson and Spearman correlation coefficients between MMMG scores and a newly established, crowdsourced human preference leaderboard for audio and interleaved content.

### Open Question 3
- Question: Can the verifiable tasks and programmatic evaluation pipelines in MMMG be effectively utilized as a scalable reward signal to fine-tune or train future multimodal generation models?
- Basis in paper: [explicit] The conclusion states, "Beyond serving as a leaderboard, we hope MMMG inspires scalable collection of verifiable validation signals for future multimodal generation training."
- Why unresolved: The paper establishes MMMG as an evaluation tool but does not experiment with using its evaluation logic (e.g., pseudocode verification) as an optimization target for model training.
- What evidence would resolve it: A successful application of MMMG tasks in a Reinforcement Learning from Verifiable Feedback (RLVF) framework that results in improved model performance on controllability and reasoning tasks.

## Limitations
- Heavy reliance on proprietary evaluation models (GPT-4O, Gemini 2.5) limits reproducibility and may create performance gaps as APIs evolve
- Uneven difficulty distribution with audio generation tasks showing particularly low performance (0.048 average accuracy) that may reflect both task difficulty and evaluation method limitations
- Manual tuning required for evaluation method selection for new tasks, and some useful tasks excluded due to unreliable evaluation methods

## Confidence
- **High Confidence**: The generation-evaluation gap exploitation mechanism and hybrid evaluation pipeline design—well-supported by multiple examples and ablation studies in the paper
- **Medium Confidence**: The 94.3% human alignment metric—achieved through extensive validation but dependent on the specific human annotator pool and task selection criteria
- **Low Confidence**: Long-term generalizability of prompt-based verification methods as VLMs continue to evolve and potentially become less reliable at distinguishing between generation and verification capabilities

## Next Checks
1. **Cross-Model Consistency Test**: Evaluate the same 49 tasks using three different VLM versions (current GPT-4, GPT-4 Turbo, and an open-source alternative) to quantify how evaluation reliability changes with model evolution
2. **Out-of-Distribution Generalization**: Apply MMMG evaluation methods to tasks outside the original 49 to test whether the generation-evaluation gap exploitation generalizes beyond curated examples
3. **Human-Evaluation Cost Analysis**: Conduct a cost-benefit analysis comparing automated MMMG evaluation against direct human evaluation for a subset of tasks to validate the efficiency claims of the automated approach