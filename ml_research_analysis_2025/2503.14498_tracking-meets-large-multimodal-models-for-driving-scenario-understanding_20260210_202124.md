---
ver: rpa2
title: Tracking Meets Large Multimodal Models for Driving Scenario Understanding
arxiv_id: '2503.14498'
source_url: https://arxiv.org/abs/2503.14498
tags:
- vehicle
- driving
- baseline
- tracking
- object
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces a method to integrate tracking information
  into Large Multimodal Models (LMMs) for autonomous driving, addressing their limitations
  in capturing 3D spatial and temporal dynamics from static image inputs. A trajectory
  encoder is proposed to embed 3D object and ego-vehicle tracks into the LMM through
  multimodal fusion, enriching visual queries with spatiotemporal context.
---

# Tracking Meets Large Multimodal Models for Driving Scenario Understanding

## Quick Facts
- **arXiv ID:** 2503.14498
- **Source URL:** https://arxiv.org/abs/2503.14498
- **Reference count:** 40
- **Primary result:** Integrating 3D tracking data into LMMs improves driving scenario understanding with gains of 9.5% accuracy and 9.4% overall score on DriveLM-nuScenes benchmark.

## Executive Summary
This work addresses the limitations of Large Multimodal Models (LMMs) in autonomous driving by integrating 3D tracking information into their reasoning pipeline. The proposed approach uses a trajectory encoder to transform object and ego-vehicle tracking data into continuous embeddings that can be fused with visual features from CLIP. Through weighted modality fusion and self-supervised pretraining on automatically generated trajectory QA pairs, the method enriches static image inputs with spatiotemporal context. Experiments on the DriveLM benchmark demonstrate significant improvements over baseline models while maintaining competitive runtime efficiency.

## Method Summary
The method integrates 3D tracking data into LMMs through a trajectory encoder that converts position and velocity vectors over time windows into transformer-processed embeddings. These trajectory embeddings are projected into CLIP's visual embedding space and fused with visual features using weighted summation before injection into the LMM. A self-supervised pretraining strategy generates automated QA pairs about trajectory attributes to enhance the encoder's understanding. The approach uses LLaMA-Adapter-v2 as the base LMM with separate trajectory encoders for ego and object trajectories, limited to 5 frames of history for efficiency.

## Key Results
- Achieves 9.5% accuracy improvement on DriveLM-nuScenes over baseline models
- Shows 7.04 points improvement in ChatGPT score for semantic alignment
- Demonstrates 9.4% overall score improvement and 3.7% final score improvement on DriveLM-CARLA
- Maintains competitive runtime efficiency with 5 frames trajectory window

## Why This Works (Mechanism)

### Mechanism 1: Trajectory-to-Embedding Transformation
Converting discrete 3D tracking data into continuous transformer-processed embeddings enables the LMM to reason about motion patterns as a distinct modality. Position and velocity vectors over time windows are flattened, projected through learned linear layers with positional embeddings, then refined via transformer self-attention to capture temporal dependencies across frames.

### Mechanism 2: Weighted Modality Fusion in Shared Embedding Space
Projecting trajectory embeddings into CLIP's visual embedding space and fusing via weighted summation preserves modality-specific signals while enabling joint reasoning. The fused representation is concatenated with learnable visual query embeddings for injection into the LMM's visual blocks.

### Mechanism 3: Self-supervised Trajectory Pretraining
Pretraining trajectory encoders on automatically generated QA pairs about trajectory attributes (speed, acceleration, direction) improves downstream task performance. Template-based QA generation creates training data that covers sufficient semantic variation for transfer to diverse driving questions.

## Foundational Learning

- **Concept: 3D Multi-Object Tracking (MOT) Pipelines**
  - Why needed: The system depends on external trackers (CenterPoint + 3DMOTFormer) to generate input trajectories; tracking failures directly impact LMM inputs.
  - Quick check: How do detection confidence thresholds (e.g., 0.3 used here) and NMS IoU thresholds affect trajectory continuity and ID persistence?

- **Concept: CLIP Vision-Language Embedding Space**
  - Why needed: The trajectory encoder must project its outputs into CLIP's embedding dimension for fusion; misalignment degrades multimodal reasoning.
  - Quick check: What is the output dimensionality of CLIP ViT-L/14 visual features, and why does the trajectory projection need to match it?

- **Concept: Adapter-based LLM Fine-tuning**
  - Why needed: The system uses LLaMA-Adapter-v2 to inject multimodal features without full LLM retraining; understanding this clarifies training efficiency and where to debug fusion issues.
  - Quick check: At which transformer layers does the adapter inject visual/trajectory queries, and what does "zero-initialized attention" mean for training stability?

## Architecture Onboarding

- **Component map:** Multi-view Images → CLIP ViT-L/14 → F_clip; 3D Object Tracks → Trajectory Encoder → F_traj; Ego-Vehicle Trajectory → Trajectory Encoder → F_ego; F_traj + F_ego + F_clip → Query Former (weighted fusion) → F_fused; F_fused + Visual Queries → Visual Blocks → F_final; F_final + Text Query → LLaMA-Adapter-v2 → Answer

- **Critical path:** Tracking quality: CenterPoint detection → 3DMOTFormer association → trajectory continuity; Key object matching: Question-referenced objects ↔ tracked objects via 2D projection matching; Trajectory encoder projection alignment with CLIP space; Adapter injection into LLaMA layers

- **Design tradeoffs:** Separate vs. shared encoders (separate improves accuracy but doubles parameters); Time window (5 frames balances context vs. noise); Pretraining data mixing (25% tracker + 75% GT improves robustness); Modality weights (all set to 1.0 with no tuning reported)

- **Failure signatures:** Low match scores (<30): Tracking missed question-referenced objects; High accuracy but low ChatGPT score: Model answers correctly but with misaligned language style; Runtime >5 sec: Excessive key objects, long track windows, or tracking pipeline bottleneck

- **First 3 experiments:** Isolate tracking contribution by setting w_traj = 0, w_ego = 0 and compare against full model; Validate embedding alignment by visualizing trajectory and CLIP embeddings before/after projection; Stress-test tracking noise by injecting Gaussian noise into trajectory positions/velocities at σ = 0.1m, 0.5m, 1.0m

## Open Questions the Paper Calls Out

### Open Question 1
How does the framework's performance degrade when subjected to severe tracking failures, such as identity switches or occluded object hallucinations? The experiments only tested mixing ground truth with tracker data at moderate noise levels (25-50% mix probability), not adversarial tracker errors.

### Open Question 2
Does the strict limitation of trajectory history to 5 frames restrict the model's ability to perform long-horizon planning or intent recognition? Complex driving maneuvers often require understanding intent over longer time horizons than 0.5 seconds.

### Open Question 3
To what extent does the rule-based, attribute-focused pretraining dataset limit the model's capacity for complex causal reasoning compared to human-annotated data? Template-based pretraining may improve numerical regression but could fail to bridge the gap for nuanced "Why" or "How" questions.

## Limitations
- Trajectory encoder architecture details (hidden dimension, layers, attention heads) not specified, limiting exact reproduction
- Weighted fusion approach uses arbitrary equal weights (1.0 each) without reported sensitivity analysis
- Pretraining data quality depends on template-based QA generation that may not cover full semantic variation of downstream tasks

## Confidence

- **High Confidence:** The core claim that integrating trajectory information improves LMM performance on driving scenario understanding is well-supported by quantitative results (9.5% accuracy gain, 9.4% overall score improvement)
- **Medium Confidence:** The specific mechanisms of trajectory-to-embedding transformation and weighted fusion are theoretically sound but lack direct corpus validation
- **Medium Confidence:** The pretraining strategy shows measurable benefits but effectiveness depends heavily on quality and coverage of automatically generated pretraining data

## Next Checks

1. **Is tracking noise the dominant failure mode?** Inject controlled Gaussian noise (σ = 0.1m, 0.5m, 1.0m) into trajectory positions/velocities and measure performance degradation to establish tracking quality requirements.

2. **Does the fusion design preserve modality-specific signals?** Visualize trajectory embeddings and CLIP embeddings (t-SNE/PCA) before and after projection to verify meaningful clustering and alignment in shared embedding space.

3. **Are pretraining templates sufficient for downstream transfer?** Analyze pretraining data distribution versus DriveLM test questions to identify coverage gaps, then augment pretraining with question types that correlate with largest performance gaps.