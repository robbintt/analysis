---
ver: rpa2
title: 'LLM-SrcLog: Towards Proactive and Unified Log Template Extraction via Large
  Language Models'
arxiv_id: '2512.04474'
source_url: https://arxiv.org/abs/2512.04474
tags:
- logs
- template
- code
- templates
- llm-srclog
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: LLM-SrcLog introduces a proactive log template extraction framework
  that combines static code analysis with large language models (LLMs) to generate
  log templates directly from source code before deployment. By reconstructing meaningful
  logging contexts through inter-procedural static analysis and using LLMs to extract
  semantically coherent templates, LLM-SrcLog addresses the limitations of reactive,
  log-centric parsers.
---

# LLM-SrcLog: Towards Proactive and Unified Log Template Extraction via Large Language Models

## Quick Facts
- **arXiv ID:** 2512.04474
- **Source URL:** https://arxiv.org/abs/2512.04474
- **Reference count:** 40
- **Primary result:** Proactive log template extraction via static code analysis + LLMs achieves 2–17% and 8–35% F1-score improvements over LLM baselines.

## Executive Summary
LLM-SrcLog introduces a proactive log template extraction framework that combines static code analysis with large language models (LLMs) to generate log templates directly from source code before deployment. By reconstructing meaningful logging contexts through inter-procedural static analysis and using LLMs to extract semantically coherent templates, LLM-SrcLog addresses the limitations of reactive, log-centric parsers. For logs from code-unavailable sources, it integrates a data-driven clustering method (Drain3) to ensure complete coverage. Evaluated on public benchmarks (Hadoop, Zookeeper) and a large-scale industrial system (Sunfire-Compute), LLM-SrcLog improves average F1-score by 2–17% and 8–35% over two LLM-based baselines while achieving online parsing latency comparable to data-driven methods and roughly 1,000× faster than per-log LLM parsing. Deployment in real-world production environments further validates its effectiveness in supporting practical troubleshooting.

## Method Summary
LLM-SrcLog is a proactive log template extraction framework that analyzes Java source code before deployment to reconstruct logging contexts via inter-procedural static analysis. The Static Code Analyzer (SCA) builds ASTs and traces data flows across function boundaries to enumerate all feasible string compositions for logging statements. These contexts are fed into a White-box Template Extractor (WTE) that uses an LLM to distinguish constants from variables in log templates. For logs where source code is unavailable (third-party libraries, exception-embedded content), a Black-box Template Extractor (BTE) using Drain3 clustering provides complete coverage. The framework fuses white-box and black-box templates for online parsing, achieving both high precision and comprehensive recall.

## Key Results
- Improves average F1-score by 2–17% on public benchmarks (Hadoop, Zookeeper) compared to two LLM-based baselines
- Achieves 8–35% improvement on industrial Sunfire-Compute dataset
- Online parsing latency comparable to data-driven methods (Drain3), roughly 1,000× faster than per-log LLM parsing
- Template extraction takes 20–40 minutes for ~1,000 source files offline

## Why This Works (Mechanism)

### Mechanism 1: Inter-procedural Static Analysis for Context Reconstruction
- **Claim:** Reconstructing logging contexts by tracing data flows across function boundaries enables template extraction before deployment.
- **Mechanism:** The Static Code Analyzer (SCA) builds ASTs, then performs path-sensitive, inter-procedural analysis that traces from logging call sites backward through call graphs to enumerate all feasible string compositions. For example, if `log.error(Bar.getUserName("0"))` calls a helper function with conditional branches, SCA yields multiple candidate templates (e.g., `"User_<.*>_NotFound"` and `"Invalid_User_ID_<.*>"`) rather than a single over-generalized pattern.
- **Core assumption:** Logging behavior is statically determinable from source code; runtime-only content (e.g., exception messages from `e.getMessage()`) cannot be recovered this way.
- **Evidence anchors:** [abstract]: "reconstructing meaningful logging contexts through inter-procedural static analysis"; [section 4.3]: "inter-procedural, path-sensitive analysis that traces data flows from the logging invocation back through the call graph to all possible return expressions in upstream helper functions"

### Mechanism 2: LLM-Augmented Variable Disambiguation with Contextual Prompting
- **Claim:** Conditioning an LLM on structured static analysis reports (rather than isolated log text) improves constant/variable separation across heterogeneous logging styles.
- **Mechanism:** The White-box Template Extractor (WTE) constructs prompts containing: (1) source code snippets, (2) call-chain context with branch paths, and (3) strict output schema (JSON with method, template, level). The LLM synthesizes this to identify deterministic literals vs. dynamic expressions. A post-processing stage filters degenerate outputs (all-wildcard templates) via rule-based checks and optional secondary LLM verification.
- **Core assumption:** LLMs can perform contextual reasoning about program semantics when given sufficient code context; post-processing catches over-generalization.
- **Evidence anchors:** [abstract]: "using LLMs to extract semantically coherent templates"; [section 4.4]: "framing template extraction as a contextual reasoning task, where LLM acts as an interpreter that synthesizes static program structure"

### Mechanism 3: Hybrid Template Fusion for Complete Coverage
- **Claim:** Combining code-aware extraction with data-driven clustering ensures both high precision (white-box) and complete recall (black-box).
- **Mechanism:** Templates from WTE form a high-precision library. Logs that fail to match any white-box template are routed to the Black-box Template Extractor (BTE), which uses Drain3 streaming clustering to group structurally similar unmatched logs. This ensures third-party and exception-embedded logs are still parsed, accepting a precision/recall trade-off.
- **Core assumption:** Black-box logs cannot be recovered from application source code; Drain3's syntactic clustering is sufficient fallback.
- **Evidence anchors:** [abstract]: "integrates a data-driven clustering method (Drain3) to ensure complete coverage"; [section 3.2]: Documents logs inadequately handled by code-aware methods: exception embedding and code-unavailable sources

## Foundational Learning

- **Concept: Inter-procedural Static Analysis**
  - **Why needed here:** SCA reconstructs logging contexts by tracing across function boundaries; without this, only local logging statements are visible.
  - **Quick check question:** Given `log.error(buildMsg(id))` where `buildMsg` has two return branches, can you enumerate the feasible runtime log templates?

- **Concept: Log Template vs. Variable Distinction**
  - **Why needed here:** The core parsing task is separating static text (constants) from dynamic placeholders; downstream tasks (anomaly detection) depend on correct separation.
  - **Quick check question:** In `"User_123_NotFound"`, which parts are constants vs. variables? How would you represent this as a template?

- **Concept: LLM In-Context Learning**
  - **Why needed here:** WTE uses structured prompts (not fine-tuning) to guide the LLM; understanding prompt design is critical for reproducibility.
  - **Quick check question:** What information must a prompt include for the LLM to correctly distinguish `"Invalid_User_ID_"` as a constant prefix?

## Architecture Onboarding

- **Component map:**
  - SCA (Static Code Analyzer): AST parsing → call-graph traversal → path enumeration → static analysis report
  - WTE (White-box Template Extractor): Prompt construction → LLM inference → post-processing → template library
  - BTE (Black-box Template Extractor): Drain3 clustering for unmatched logs → merged template set
  - Online Parser: Regex-based streaming matching against fused template library

- **Critical path:**
  1. Source code in → SCA extracts logging call sites and call chains
  2. Static analysis report + code → WTE LLM prompt → candidate templates
  3. Post-processing filters → white-box template library
  4. At runtime: incoming log → regex match against library → if no match, route to BTE (Drain3)
  5. BTE outputs appended to library for future matches

- **Design tradeoffs:**
  - **Precision vs. Recall:** Disabling BTE raises precision but loses black-box coverage (ablation: F1 on Hadoop drops from 0.710 to 0.737 without BTE, but template count falls sharply).
  - **Offline cost vs. Online speed:** Offline extraction takes 20–40 minutes for ~1,000 files; online parsing is ~1,000× faster than per-log LLM methods.
  - **Model scale stability:** Online latency is insensitive to backbone model size (Qwen3-8B/32B/Max all ~0.6–2.1s for 2,000 logs).

- **Failure signatures:**
  - **SCA incomplete:** Third-party or dynamically loaded code yields no templates; logs fall through to BTE.
  - **LLM over-generalization:** Templates become `<.*>`; post-processing should filter these (check for sufficient static content).
  - **BTE disabled:** Unmatched logs dropped; recall loss especially on systems with many black-box components (Hadoop shows this pattern).

- **First 3 experiments:**
  1. **Validate SCA context reconstruction:** Run SCA on the Foo.java/Bar.java example from Section 4.3; verify that four expected templates are enumerated and the static analysis report matches Appendix A.2.
  2. **Ablate post-processing:** Disable the post-processing module on Hadoop dataset; confirm recall drops (paper reports 20.2% drop) and inspect degenerate templates.
  3. **Measure online latency vs. baseline:** Parse 2,000 logs with LLM-SrcLog (any backbone), Drain, and DivLog; verify LLM-SrcLog is within 2–3× of Drain and >500× faster than DivLog.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can the static analysis component of LLM-SrcLog be generalized to support diverse programming languages and logging frameworks beyond Java?
- **Basis in paper:** [explicit] The Conclusion states, "Future work will extend static analysis to more programming languages...".
- **Why unresolved:** The current implementation relies specifically on the `javalang` library to construct ASTs; adapting to other languages requires distinct parsers and strategies to handle different logging conventions and runtime behaviors.
- **What evidence would resolve it:** Successful implementation and evaluation of the framework on non-Java benchmarks (e.g., Python or C++ systems) showing comparable F1-scores.

### Open Question 2
- **Question:** Can partitioning the global template set into application-specific subsets significantly reduce online matching latency without sacrificing parsing accuracy?
- **Basis in paper:** [explicit] The Conclusion proposes to "optimize the online matching strategy by partitioning the global template set into application-specific subsets derived from the codebase...".
- **Why unresolved:** While partitioning reduces the search space, it introduces the risk of misclassification if logs are routed to incorrect subsets, potentially lowering recall.
- **What evidence would resolve it:** Latency benchmarks and accuracy metrics from large-scale production environments comparing global matching against partitioned matching strategies.

### Open Question 3
- **Question:** How can the framework improve template extraction for logs generated purely by runtime exceptions (e.g., `e.getMessage()`) where static code analysis provides no literal format string?
- **Basis in paper:** [inferred] Section 3.2 notes that for exception embedding, "even ideal static analysis cannot reconstruct their templates," forcing a fallback to Drain3, which has lower semantic accuracy than the LLM-based extractor.
- **Why unresolved:** Static analysis is fundamentally limited to compile-time logic; the content of runtime exceptions depends on dynamic states invisible during the pre-deployment phase.
- **What evidence would resolve it:** A modified extraction strategy that successfully infers or clusters exception-based logs with higher semantic precision than the current data-driven baseline.

## Limitations

- Cannot recover templates from code-unavailable sources (third-party libraries, dynamic modules) without relying on less accurate black-box clustering
- Static analysis cannot handle exception-embedded content where format strings are only determined at runtime
- Performance on industrial dataset cannot be independently verified due to proprietary data restrictions

## Confidence

- **High Confidence:** Core mechanism of inter-procedural static analysis for context reconstruction; hybrid white-box/black-box template fusion approach.
- **Medium Confidence:** LLM-augmented variable disambiguation effectiveness; online parsing latency comparisons with baselines.
- **Low Confidence:** Industrial deployment results (Sunfire-Compute); specific post-processing rule effectiveness.

## Next Checks

1. Run SCA on the Foo.java/Bar.java example from Section 4.3; verify four expected templates are enumerated and static analysis report matches Appendix A.2.
2. Disable post-processing module on Hadoop dataset; confirm recall drops by ~20% and inspect for degenerate all-wildcard templates.
3. Parse 2,000 logs with LLM-SrcLog, Drain, and DivLog; verify latency is within 2-3× of Drain and >500× faster than DivLog per paper claims.