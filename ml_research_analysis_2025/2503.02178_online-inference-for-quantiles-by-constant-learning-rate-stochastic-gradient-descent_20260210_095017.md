---
ver: rpa2
title: Online Inference for Quantiles by Constant Learning-Rate Stochastic Gradient
  Descent
arxiv_id: '2503.02178'
source_url: https://arxiv.org/abs/2503.02178
tags:
- quantile
- distribution
- stochastic
- stationary
- markov
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a novel online inference method for quantile
  estimation using constant learning-rate stochastic gradient descent (SGD), addressing
  the challenge of non-smooth and non-strongly-convex quantile loss functions. The
  authors interpret the SGD iterates as an irreducible and positive recurrent Markov
  chain, proving the existence of a unique asymptotic stationary distribution regardless
  of initialization.
---

# Online Inference for Quantiles by Constant Learning-Rate Stochastic Gradient Descent

## Quick Facts
- arXiv ID: 2503.02178
- Source URL: https://arxiv.org/abs/2503.02178
- Reference count: 40
- Key outcome: Proposes online inference method for quantile estimation using constant learning-rate SGD with asymptotic Gaussianity guarantees

## Executive Summary
This paper develops a novel online inference framework for quantile estimation using constant learning-rate stochastic gradient descent (SGD). The authors address the fundamental challenge that quantile loss functions are non-smooth and non-strongly-convex, which prevents standard SGD theory from applying. By interpreting SGD iterates as an irreducible and positive recurrent Markov chain, they prove the existence of a unique asymptotic stationary distribution regardless of initialization. The key theoretical result demonstrates that as the learning rate approaches zero, this stationary distribution converges to a Gaussian distribution, providing the first central limit theorem-type guarantees for constant learning-rate SGD in quantile estimation.

## Method Summary
The authors establish a Markov chain interpretation of constant learning-rate SGD iterates, treating them as an irreducible and positive recurrent chain. They bound the moment generating function of the chain and prove Gaussian convergence of the stationary distribution. A recursive algorithm is developed for online construction of confidence intervals based on this theoretical foundation. The approach specifically handles the non-smoothness and non-strong convexity of quantile loss through careful analysis of the stochastic gradient noise structure.

## Key Results
- Proves existence of unique asymptotic stationary distribution for constant learning-rate SGD in quantile estimation
- Demonstrates convergence of stationary distribution to Gaussian as learning rate approaches zero
- Develops recursive online algorithm for confidence interval construction
- Shows strong finite-sample performance in numerical studies

## Why This Works (Mechanism)
The method works by leveraging the ergodic properties of irreducible and positive recurrent Markov chains. The constant learning rate creates a controlled level of noise in the SGD iterates, preventing convergence to a single point while maintaining stability. This stochastic behavior, combined with careful moment generating function bounds, enables the asymptotic Gaussianity result despite the non-smooth nature of quantile loss.

## Foundational Learning

1. **Irreducible and Positive Recurrent Markov Chains**
   - Why needed: Establishes the theoretical foundation for analyzing SGD iterates as a stochastic process with a well-defined stationary distribution
   - Quick check: Verify that the Markov chain satisfies these properties under the given step-size conditions

2. **Moment Generating Function Bounds**
   - Why needed: Provides the technical tool for proving concentration and convergence properties of the stationary distribution
   - Quick check: Confirm that the MGF bounds hold uniformly over the relevant parameter space

3. **Asymptotic Stationary Distribution Analysis**
   - Why needed: Characterizes the long-run behavior of the SGD iterates, enabling inference despite non-convergence to a single point
   - Quick check: Verify that the stationary distribution exists and is unique under the given conditions

## Architecture Onboarding

**Component Map:**
Online Data Stream -> Quantile Loss Computation -> Stochastic Gradient Calculation -> Constant Learning-Rate Update -> SGD Iterates (Markov Chain) -> Stationary Distribution Analysis -> Confidence Interval Output

**Critical Path:**
The critical path involves computing the stochastic gradient, updating the quantile estimate via constant learning-rate SGD, and maintaining sufficient statistics for the stationary distribution analysis to construct confidence intervals.

**Design Tradeoffs:**
The constant learning rate trades off asymptotic efficiency for algorithmic simplicity and the ability to maintain a stationary distribution amenable to inference. This contrasts with decaying learning rates that would converge to a point estimate but lose the benefits of online inference.

**Failure Signatures:**
- Non-Gaussian stationary distribution (violates theoretical assumptions)
- Unstable Markov chain behavior (violates irreducibility/positive recurrence)
- Poor confidence interval coverage (implementation issues or violated assumptions)

**First Experiments:**
1. Verify Markov chain properties under synthetic data with known quantiles
2. Test stationary distribution Gaussianity as learning rate varies
3. Evaluate confidence interval coverage under controlled data distributions

## Open Questions the Paper Calls Out
None specified in the provided content.

## Limitations

- Theoretical framework assumes bounded gradient noise, which may not hold for heavy-tailed data distributions
- Rate of convergence to stationary distribution is not characterized, limiting practical finite-sample inference
- Assumption of irreducible and positive recurrent Markov chain behavior may be sensitive to specific problem structures
- Limited scope of numerical studies prevents comprehensive assessment of robustness across diverse data conditions

## Confidence

| Claim | Confidence Level |
|-------|------------------|
| Existence of unique stationary distribution | Medium-High |
| Gaussian convergence of stationary distribution | Medium-High |
| Recursive algorithm correctness | Medium |
| Practical finite-sample performance | Medium |

## Next Checks

1. Extensive empirical testing across different data distributions (heavy-tailed, heteroscedastic) to assess robustness of theoretical assumptions
2. Analysis of convergence rates to stationary distribution for various problem scales and data characteristics
3. Comparison with alternative online quantile estimation methods under realistic computational constraints and data conditions