---
ver: rpa2
title: Reward Model Routing in Alignment
arxiv_id: '2510.02850'
source_url: https://arxiv.org/abs/2510.02850
tags:
- router
- preference
- online
- arxiv
- offline
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of reward model (RM) selection
  in reinforcement learning from human feedback (RLHF) for large language models.
  The proposed BayesianRouter combines offline learning of RM strengths with online
  Bayesian Thompson sampling to dynamically select the most suitable RM for each preference
  pair during DPO training.
---

# Reward Model Routing in Alignment

## Quick Facts
- arXiv ID: 2510.02850
- Source URL: https://arxiv.org/abs/2510.02850
- Authors: Xinle Wu; Yao Lu
- Reference count: 11
- One-line primary result: BayesianRouter achieves 63.23% AlpacaEval-2 and 75.66% GSM8K using O(1) RM calls per query.

## Executive Summary
This paper introduces BayesianRouter, a framework for dynamic reward model selection during reinforcement learning from human feedback. The system learns RM strengths offline using a multi-task router, then applies Bayesian Thompson sampling online to select the most suitable RM for each preference pair during DPO training. By initializing online posteriors with offline-learned embeddings and adaptively updating with policy feedback, it achieves superior alignment performance while maintaining computational efficiency through O(1) RM calls per query.

## Method Summary
BayesianRouter operates in two phases: offline and online. The offline phase trains a multi-task router (SmolLM2-135M encoder with Bradley-Terry and classification heads) on preference data to estimate per-RM reliability, producing embeddings E_bt[n] for each RM. The online phase uses Bayesian Thompson sampling with Gaussian posteriors per RM, initialized with offline embeddings as priors, to dynamically select RMs for each preference pair during DPO training. The system updates posteriors only for selected RMs using batch-normalized DPO loss rewards, maintaining O(1) RM calls per query.

## Key Results
- Outperforms single RMs, ensembles, and existing routing methods on instruction-following (63.23% AlpacaEval-2, 58.75% MT-Bench) and reasoning benchmarks (75.66% GSM8K, 57.39% MMLU)
- Achieves O(1) RM calls per query compared to O(N) for majority voting ensembles
- Cold-start prior injection from offline learning provides 2% performance boost over online-only routing

## Why This Works (Mechanism)

### Mechanism 1: Multi-Task Offline Router for RM Strength Estimation
A learned offline router predicts per-RM reliability for preference pairs before online training. It encodes (prompt, response₁, response₂) triples and applies Bradley-Terry and classification heads to produce relative ability scores and correctness predictions. This transfers domain knowledge about RM specializations to the online phase.

### Mechanism 2: Bayesian Thompson Sampling for Uncertainty-Aware Exploration
Thompson sampling with Gaussian posteriors prevents premature convergence to suboptimal RMs better than LinUCB. Each RM maintains a posterior, samples weights for each query, and selects the highest-scoring RM. Only selected RMs update their posteriors with observed rewards, maintaining exploration while exploiting known good RMs.

### Mechanism 3: Prior Injection for Cold-Start Mitigation
Initializing online posterior means with offline BT embeddings transfers learned RM strengths to online routing. This bootstraps the online router with domain-specific knowledge about which RM excels on which query types, while posteriors adapt to policy-induced distributions.

## Foundational Learning

- **Contextual Multi-Armed Bandits (MAB)**: Why needed: RM selection is a contextual bandit—each query is a context, each RM is an arm, only partial feedback is observed. Quick check: Can you explain why routing is a partial-feedback problem rather than full supervision?
- **Thompson Sampling vs. Upper Confidence Bound (UCB)**: Why needed: LinUCB prematurely exploits; understanding why posterior sampling encourages continued exploration is critical. Quick check: What is the key difference in how UCB and Thompson sampling handle uncertainty?
- **Bradley-Terry Model**: Why needed: The offline router's primary head uses BT loss to learn relative RM abilities from pairwise comparisons. Quick check: Given BT scores s_A and s_B for two RMs, what is P(RM_A wins) under the Bradley-Terry assumption?

## Architecture Onboarding

- **Component map**: Preference encoder (SmolLM2-135M) → fused representation h_i → [BT head (E_bt) + CLS head (E_cls)] → Bayesian Thompson sampling over N RM posteriors → select RM → observe reward → update selected RM's posterior
- **Critical path**: 1) Collect RM behavior data by running all N RMs on offline preference pairs; 2) Extract disagreement set and train offline router with L_total = L_bt + λL_cls; 3) Save E_bt embeddings; 4) During online DPO: encode pairs, sample from posteriors, select RM, update posterior for selected RM only
- **Design tradeoffs**: Per-query routing (finer granularity, more decisions) vs. per-batch routing; O(1) RM calls (efficiency) vs. O(N) (potential accuracy); prior strength σ²_w (strong trusts offline, weak relies on online learning)
- **Failure signatures**: Router collapses to single RM (exploration failure, check posterior variance); early-training accuracy low (cold-start, verify offline router quality); performance degrades over time (posterior overconfidence or distribution shift, check reward normalization)
- **First 3 experiments**: 1) Ablate offline prior: Run BayesianRouter with μ^(0) = 0 on AlpacaEval-2 to quantify cold-start penalty (~2% drop); 2) Ablate online adaptation: Use only offline router to measure distribution shift impact (~1-2% drop); 3) Scalability test: Increase RM pool from 4 to 8 models, measure wall-clock time vs. majority voting (BayesianRouter scales better than O(N) baselines)

## Open Questions the Paper Calls Out
- Can BayesianRouter be effectively adapted for reinforcement learning–based alignment methods like PPO that rely on scalar reward signals?
- How can the framework be extended to jointly optimize the trade-off between annotation accuracy and RM inference cost?
- Does increasing the scale and domain diversity of the offline preference dataset close the performance gap to the oracle router?

## Limitations
- Reliance on offline preference data assumes the dataset captures RM strengths sufficiently; Table 2 shows ~88-90% routing accuracy indicating potential distribution mismatch
- Linear reward model assumption may not capture complex RM-query interactions, particularly as policy responses diverge from offline data
- Scalability claims beyond 4 RMs remain theoretical; the paper only tests with 4 RMs and extrapolation may face practical challenges

## Confidence

| Claim | Confidence |
|-------|------------|
| Core mechanism of combining offline learning with online Bayesian Thompson sampling is well-specified | High |
| Performance improvements appear robust but depend heavily on offline preference dataset quality | Medium |
| Scalability beyond 4 RMs remains theoretical | Low |

## Next Checks
1. **Distribution Shift Analysis**: Systematically measure the gap between offline preference data and online policy-generated responses, and quantify how this gap affects routing accuracy over training epochs
2. **Linear Model Capacity Test**: Replace the linear reward model with a small MLP to test whether non-linear interactions between contexts and RMs improve performance
3. **Prior Sensitivity Study**: Run experiments with varying prior strengths (σ²_w from 0.001 to 1.0) to determine the optimal balance between offline knowledge transfer and online adaptation