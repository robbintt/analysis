---
ver: rpa2
title: Automated Interpretability Metrics Do Not Distinguish Trained and Random Transformers
arxiv_id: '2501.17727'
source_url: https://arxiv.org/abs/2501.17727
tags:
- layer
- activation
- trained
- entropy
- feature
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper demonstrates that common auto-interpretability metrics
  used to evaluate sparse autoencoders (SAEs) on transformer activations fail to distinguish
  between trained and randomly initialized models. Across multiple Pythia model sizes
  and randomization schemes, SAEs trained on randomly initialized transformers produce
  auto-interpretability scores and reconstruction metrics similar to those from trained
  models.
---

# Automated Interpretability Metrics Do Not Distinguish Trained and Random Transformers

## Quick Facts
- arXiv ID: 2501.17727
- Source URL: https://arxiv.org/abs/2501.17727
- Reference count: 40
- Primary result: Common auto-interpretability metrics fail to distinguish trained from randomly initialized transformers

## Executive Summary
This paper demonstrates that standard metrics for evaluating sparse autoencoders (SAEs) on transformer activations cannot reliably distinguish between features learned through training and spurious patterns present in randomly initialized models. Across multiple Pythia model sizes and randomization schemes, SAEs trained on randomly initialized transformers produce auto-interpretability scores and reconstruction metrics similar to those from trained models. The authors show that high aggregate auto-interpretability scores alone do not guarantee recovery of learned, computationally relevant features, as these scores can reflect simpler statistical properties or architectural biases present even without training. They recommend treating common SAE metrics as insufficient proxies for mechanistic interpretability and advocate for routine randomized baselines and targeted measures of feature "abstractness" to better distinguish learned computations from artifacts.

## Method Summary
The authors trained TopK SAEs (expansion factor R=64, sparsity k=32) on residual stream activations from Pythia transformer models of various sizes (70M to 6.9B parameters) using 100M tokens from the RedPajama dataset. They evaluated five model variants: trained models, re-randomized models (with preserved parameter norms), re-randomized excluding embeddings, Step-0 initialization, and a control condition with Gaussian token embeddings. Auto-interpretability was assessed using an LLM-based pipeline (Meta-Llama-3.1-70B-Instruct) that generated explanations and performed fuzzing/detection classification tasks, measuring AUROC scores alongside reconstruction metrics like explained variance and cosine similarity.

## Key Results
- SAEs trained on randomly initialized transformers achieve fuzzing AUROC scores similar to those trained on trained models
- Reconstruction metrics (explained variance, cosine similarity) show no clear distinction between trained and random variants
- Token distribution entropy analysis reveals that trained models show increasing abstraction across layers while random variants do not
- Parameter norm preservation in re-randomized models explains why their metrics more closely match trained models than Step-0 variants

## Why This Works (Mechanism)

### Mechanism 1
Randomly initialized transformers produce activations that appear "interpretable" because they preserve or amplify superposition structure inherent in text data. Matrix multiplication by random weight matrices preserves the statistical structure of superposed inputs, allowing SAEs to decompose recoverable sparse structure even without learned computation. This assumes token embeddings contain sparse, superposed semantic structure that random networks transmit rather than destroy.

### Mechanism 2
Auto-interpretability metrics capture token-level statistical patterns rather than abstract, learned computations. The fuzzing and detection AUROC metrics measure whether an LLM can predict which tokens activate a latent given an explanation. Random networks learn simple token-identity features (low entropy over token distribution) that are easy to explain and classify, achieving high AUROC without semantic abstraction.

### Mechanism 3
Parameter norm preservation at initialization affects reconstruction quality independent of learned computation. The re-randomized variants preserve the trained model's per-layer parameter norms, unlike Step-0 initialization. Since activation scale through residual stream depends on parameter norms, SAEs trained on norm-matched random weights achieve similar reconstruction metrics to trained models even without learning meaningful features.

## Foundational Learning

- **Concept: Sparse Autoencoder (SAE) training objective**
  - Why needed here: Understanding what SAEs optimize for (reconstruction + sparsity) is necessary to interpret why they succeed on random networks—they're not looking for "learned" features, just sparse decomposable structure.
  - Quick check question: Given activations x, what two terms comprise the standard SAE loss function?

- **Concept: Superposition hypothesis**
  - Why needed here: The paper's explanation hinges on the idea that neural networks represent more features than dimensions via superposition; understanding this clarifies why random networks might still have "structure."
  - Quick check question: In a 256-dimensional residual stream, how might a model represent 10,000 distinct features?

- **Concept: AUROC for interpretability evaluation**
  - Why needed here: The core result uses AUROC from classification tasks; understanding what this metric measures (and doesn't) is essential for interpreting the findings.
  - Quick check question: If a latent activates exclusively on the token "the" and achieves 0.95 AUROC, does this indicate the latent captures a meaningful semantic concept?

## Architecture Onboarding

- **Component map:** Pythia transformer (70M–6.9B params) -> Residual stream activations per layer -> TopK SAE (R=64, k=32) -> Sparse latents -> Meta-Llama-3.1-70B-Instruct -> Fuzzing/Detection AUROC, explained variance, cosine similarity, CE loss score, token distribution entropy

- **Critical path:**
  1. Train SAE on 100M tokens from RedPajama dataset (per-layer on residual stream)
  2. Sample 100 latents per SAE, collect maximally activating examples
  3. Generate explanations using Llama-3.1-70B
  4. Compute fuzzing/detection AUROC via classifier predictions
  5. Compare trained vs. randomized variants (Step-0, re-randomized incl/excl embeddings, control)

- **Design tradeoffs:**
  - TopK SAE vs. standard L1-penalized SAE: TopK provides cleaner sparsity control but may hide reconstruction artifacts
  - 100M vs. 1B training tokens: 100M sufficient for main findings (validated at 1B), but may miss rare features
  - 100 latent sample vs. full evaluation: Sampling enables tractable LLM-based evaluation but introduces variance

- **Failure signatures:**
  - Control condition (Gaussian token embeddings) achieving AUROC > 0.55 indicates broken embedding randomization
  - CE loss score meaningful only for trained model; random variants will show near-zero recovery regardless
  - Very low token distribution entropy for trained models at late layers suggests SAE not capturing abstract features

- **First 3 experiments:**
  1. Reproduce on smaller model: Train TopK SAE (R=32, k=16) on Pythia-70M layer 3, compare trained vs. Step-0 on fuzzing AUROC and entropy. Expected: small gap at this scale.
  2. Token entropy ablation: For each variant, bin latents by entropy and plot mean AUROC per bin. Expected: trained model maintains high AUROC in high-entropy bins; random variants degrade.
  3. Alternative interpretability metric: Implement simulation scoring (correlation between predicted and observed activations per Bills et al.) on 20 latents from trained vs. random Pythia-410M. Expected: simulation may better discriminate, but computationally expensive.

## Open Questions the Paper Calls Out

### Open Question 1
Do randomly initialized transformers primarily preserve superposition from input data, or do they amplify it? The authors present two hypotheses for observed interpretability in random networks but "defer conclusions as to the mechanism responsible to future work." While toy models suggest both preservation and amplification are plausible, the dominant mechanism in full-scale transformers remains unidentified. Controlled experiments varying input sparsity and analyzing activation distributions across layers would isolate the contribution of random weights versus input structure.

### Open Question 2
How can we develop quantitative metrics that effectively measure the "computational significance" or "abstractness" of SAE features? The authors advocate for "targeted measures of feature 'abstractness'" to distinguish learned computations from artifacts, noting current metrics are insufficient. Aggregate auto-interpretability scores (e.g., fuzzing AUROC) fail to detect the lack of complex, abstract features in random models compared to trained ones. A metric that strongly correlates with layer depth and functional abstraction in trained models while showing no such correlation in randomized baselines would resolve this.

### Open Question 3
Does the initialization scale (parameter norms) explain why re-randomized models produce SAE metrics more similar to trained models than "Step-0" models do? The authors speculate that the similarity between re-randomized and trained models arises because their randomization preserves parameter norms, unlike standard initialization. The paper observes the phenomenon but does not isolate parameter norm scaling as the causal variable. An ablation study comparing SAE metrics on random models initialized with standard scaling versus scaling matched to trained parameters would resolve this.

## Limitations

- Results are limited to Pythia models and RedPajama data; generalization to other architectures or datasets remains untested
- LLM-based evaluation pipeline introduces sampling variance and may not capture all aspects of feature functionality
- The paper does not establish causal mechanisms for why random networks preserve superposition structure

## Confidence

- **High confidence:** The empirical finding that SAEs trained on random Pythia variants achieve similar aggregate metrics to trained models
- **Medium confidence:** The mechanistic explanation that superposition preservation in random networks is the primary driver of spurious interpretability
- **Medium confidence:** The recommendation to adopt randomized baselines and entropy-based feature "abstractness" as diagnostic tools

## Next Checks

1. **Dataset transfer:** Repeat the SAE training and evaluation pipeline on a non-Pythia architecture (e.g., Mistral-7B) using the same RedPajama corpus. Confirm that random baselines still match trained metrics, or identify architectural factors that break the pattern.

2. **Feature-level correlation:** For each latent, compute the Pearson correlation between token entropy and fuzzing AUROC across trained vs. random variants. Plot per-model to verify that trained models maintain high AUROC at high entropy while random variants do not.

3. **Alternative metric benchmark:** Implement the simulation scoring method (Bills et al.) on a subset of SAE latents (e.g., 20 per variant) from Pythia-410M. Compare its discriminative power against AUROC and explained variance to assess whether more expensive metrics add diagnostic value.