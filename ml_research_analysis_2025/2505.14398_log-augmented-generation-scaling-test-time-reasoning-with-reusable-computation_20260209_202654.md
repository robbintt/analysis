---
ver: rpa2
title: 'Log-Augmented Generation: Scaling Test-Time Reasoning with Reusable Computation'
arxiv_id: '2505.14398'
source_url: https://arxiv.org/abs/2505.14398
tags:
- reasoning
- logs
- values
- agentic
- generation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes log-augmented generation (LAG), a framework
  that enables large language models to directly reuse prior computation and reasoning
  from past logs at inference time to improve performance on new tasks. The core idea
  is to represent logs using key-value (KV) caches from previous reasoning traces,
  storing only a subset of tokens while retaining the full reasoning context through
  the KV values' attention mechanism.
---

# Log-Augmented Generation: Scaling Test-Time Reasoning with Reusable Computation

## Quick Facts
- arXiv ID: 2505.14398
- Source URL: https://arxiv.org/abs/2505.14398
- Reference count: 40
- Primary result: LAG achieves up to 32.3% exact match on Musique vs 27.0% for standard agentic systems

## Executive Summary
LAG enables large language models to directly reuse prior computation and reasoning from past logs at inference time. The framework stores KV caches from previous reasoning traces and retrieves them to augment new generations, allowing models to reuse prior reasoning without requiring additional knowledge extraction steps. Experiments show LAG significantly outperforms standard agentic systems and existing reflection-based methods across knowledge-intensive and reasoning-intensive datasets.

## Method Summary
LAG implements a log-augmented generation framework where reasoning traces are encoded into KV caches during an offline phase, storing only the KV values of final response tokens while retaining full reasoning context through attention mechanisms. At inference time, relevant logs are retrieved via semantic search and their KV values are injected into the model's past_key_values after RoPE position adjustment. The framework was evaluated on Llama-3.1-8B-Instruct across multiple datasets with a static log store constructed from 70% of tasks and tested on the remaining 30%.

## Key Results
- LAG achieves 32.3% exact match on Musique compared to 27.0% for standard agentic systems
- LAG surpasses existing reflection-based methods and KV caching techniques
- LAG demonstrates superior efficiency with 3.5x improvement in reasoning iterations while maintaining accuracy
- Analysis shows LAG provides both efficiency gains through knowledge reuse and accuracy improvements through insight reuse

## Why This Works (Mechanism)

### Mechanism 1: Context-Encoded KV Distillation
Storing KV values of final tokens while encoding full reasoning history allows models to retain contextual insights without storing entire traces. The attention mechanism computes KV values as weighted aggregations of all preceding tokens, creating compressed representations that implicitly contain the attention context of the full reasoning process. Core assumption: final tokens' KV values attend sufficiently to critical reasoning steps to act as valid surrogates. Break condition: if critical reasoning occurs early and attention scores decay to near-zero by final token.

### Mechanism 2: Cross-Context Positional Re-alignment
Retrieved KV caches integrate into new contexts only if positional dependencies are mathematically removed and updated. The framework strips original RoPE from retrieved KV values and reapplies new rotations corresponding to positions in the new sequence. Core assumption: semantic meaning encoded in KV vectors is stable enough to survive RoPE removal and re-application. Break condition: if models rely heavily on absolute positional biases rather than relative, re-aligning positions may disrupt logical flow.

### Mechanism 3: Iteration Reduction via Trace Reuse
Augmenting generation with prior reasoning logs reduces inference steps required for similar tasks and solves previously unsolvable problems. The model retrieves logs relevant to sub-tasks and attends to pre-computed insights or partial solutions in retrieved KV cache, skipping intermediate reasoning steps or correcting flawed strategies. Core assumption: current task shares semantic or structural similarity with retrieved logs. Break condition: if retrieved log is semantically similar but logically contradictory to new query's constraints, it may introduce noise degrading accuracy.

## Foundational Learning

- **Concept: KV (Key-Value) Cache in Transformers**
  - Why needed: LAG framework relies on manipulating KV cache - the memory of attention mechanism where K (what to look for) and V (what content is there) are distinct from raw text tokens
  - Quick check: If you strip KV cache from model mid-generation and continue, does model remember prompt?

- **Concept: Rotary Positional Embeddings (RoPE)**
  - Why needed: Paper implements mathematical inversion of RoPE to make logs portable; position encoded via vector rotation, not just index added to vector
  - Quick check: Why can't you simply concatenate KV cache from one context to another without adjusting position ids?

- **Concept: Agentic Reasoning Loops (e.g., ReAct)**
  - Why needed: LAG evaluated on agentic systems that iterate (Reason -> Act -> Observe); log is history of these turns, distinguishing "retrieved document" from "reasoning trace"
  - Quick check: In multi-hop QA task, does log store final answer or path taken to find it?

## Architecture Onboarding

- **Component map**: Log Store -> Retriever -> Augmenter -> LLM Encoder
- **Critical path**: 
  1. Offline: Run tasks -> Encode Full History -> Extract KV of Last Response -> Store (Text Embedding, KV Cache)
  2. Online: Receive Query -> Retrieve Top-k Logs -> Strip RoPE -> Prepend to Current Context -> Generate
- **Design tradeoffs**: 
  - Storage vs. Fidelity: Storing KV for Last Action is extremely cheap (6GB avg) but stores less context than Last Round or Last 2 Rounds
  - Noise vs. Coverage: Storing full text logs introduces more noise than KV representations which naturally attend to relevant parts of context
- **Failure signatures**: 
  - Position Mismatch: Garbled output indicating RoPE stripping/re-application failed
  - Context Overwrite: Model hallucinates or ignores log, suggesting KV cache not correctly passed to past_key_values
  - Negative Transfer: Accuracy drops when retrieved logs are misleading
- **First 3 experiments**: 
  1. Sanity Check: Retrieve known log ("2+2=4") and verify model generates "4" immediately when asked "2+2?"
  2. Storage Strategy Ablation: Compare storing KV for "Last Action" vs "Last Full Response" on small dataset slice
  3. Iteration Count Impact: Plot exact match score against maximum allowed reasoning steps to verify LAG reaches accuracy ceiling faster

## Open Questions the Paper Calls Out

- **Open Question 1**: Can KV-based log representation approach be adapted for closed-source or API-based LLMs where internal KV states are inaccessible? The current implementation relies on direct manipulation of past_key_values via HuggingFace APIs, which proprietary models don't expose.

- **Open Question 2**: How does filtering incorrect reasoning traces from log store affect LAG's accuracy and efficiency? The paper doesn't evaluate whether log quality filtering provides meaningful gains or whether noise from incorrect logs significantly degrades performance.

- **Open Question 3**: How does LAG performance scale with dynamic, online log store updates compared to static offline construction evaluated? Real-world systems may benefit from continuously updating logs, but tradeoffs are unexplored.

- **Open Question 4**: Does LAG's effectiveness transfer across model scales and architectures beyond Llama-3.1-8B-Instruct? The paper uses only one model and doesn't test whether benefits are consistent across larger models, smaller models, or different architectures.

## Limitations
- Dependence on semantic similarity for effective log retrieval can cause negative transfer when tasks share surface similarity but differ in critical constraints
- Assumes KV values from final tokens adequately represent full reasoning context, which may not hold when critical reasoning occurs early in trace
- Effectiveness of RoPE position re-alignment assumes semantic stability of KV vectors across context shifts without direct empirical validation

## Confidence
- **High Confidence**: Core mechanism of storing KV values from final tokens and using them for augmentation
- **Medium Confidence**: RoPE position re-alignment technique (mathematically sound but lacks direct semantic stability validation)
- **Medium Confidence**: Iteration reduction claims (supported by quantitative evidence but attribution between knowledge and insight reuse remains partially inferential)

## Next Checks
1. **Attention Weight Analysis**: Extract and visualize attention weights from stored KV values to verify critical reasoning tokens receive sufficient attention mass
2. **Position Re-alignment Robustness Test**: Create controlled experiments varying semantic content and positional contexts to measure generation quality degradation
3. **Negative Transfer Quantification**: Systematically vary semantic similarity threshold for log retrieval and measure trade-off between positive and negative transfer