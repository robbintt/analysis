---
ver: rpa2
title: Retrieval-augmented in-context learning for multimodal large language models
  in disease classification
arxiv_id: '2505.02087'
source_url: https://arxiv.org/abs/2505.02087
tags:
- learning
- raicl
- shot
- disease
- classification
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of improving in-context learning
  for multimodal large language models (MLLMs) in disease classification by dynamically
  retrieving informative demonstrations. The proposed Retrieval-Augmented In-Context
  Learning (RAICL) framework integrates retrieval-augmented generation (RAG) with
  in-context learning (ICL), using embeddings from ResNet, BERT, BioBERT, and ClinicalBERT
  to adaptively select demonstrations with similar disease patterns.
---

# Retrieval-augmented in-context learning for multimodal large language models in disease classification

## Quick Facts
- **arXiv ID:** 2505.02087
- **Source URL:** https://arxiv.org/abs/2505.02087
- **Reference count:** 40
- **Primary result:** RAICL improves MLLM disease classification accuracy from 0.7854 to 0.8368 on TCGA and 0.7924 to 0.8658 on IU Chest X-ray

## Executive Summary
This paper introduces a Retrieval-Augmented In-Context Learning (RAICL) framework to enhance multimodal large language models (MLLMs) for disease classification tasks. The approach dynamically retrieves informative demonstrations based on input similarity, addressing the challenge of selecting high-quality demonstrations for in-context learning. By integrating retrieval-augmented generation with in-context learning, RAICL improves classification performance on two real-world multimodal medical datasets, demonstrating the effectiveness of combining visual and textual information with adaptive demonstration selection.

## Method Summary
The RAICL framework addresses in-context learning limitations by incorporating a retrieval module that dynamically selects demonstrations based on input similarity. The system uses embeddings from ResNet, BERT, BioBERT, and ClinicalBERT to create a searchable index of demonstrations. During inference, given a new input, the framework retrieves demonstrations with similar disease patterns from this index and constructs an augmented prompt for the MLLM. This approach enables adaptive selection of high-quality demonstrations tailored to each specific input, improving the model's ability to generalize from limited training examples. The framework is evaluated across multiple MLLM architectures (Qwen, Llava, Gemma) on two medical datasets, demonstrating consistent performance improvements over standard in-context learning approaches.

## Key Results
- Accuracy improves from 0.7854 to 0.8368 on TCGA dataset
- Accuracy improves from 0.7924 to 0.8658 on IU Chest X-ray dataset
- Text-only inputs outperform image-only inputs, with multimodal inputs showing the best performance

## Why This Works (Mechanism)
The framework improves performance by addressing the key challenge of demonstration selection in in-context learning. Standard ICL relies on manually curated or randomly selected demonstrations, which may not be relevant to the specific input being classified. RAICL's retrieval mechanism dynamically selects demonstrations that are semantically similar to the input, ensuring that the in-context examples provide relevant patterns and features for the model to learn from. This targeted approach helps MLLMs better understand the relationship between input features and disease categories, particularly in the complex domain of medical image-text classification where relevant patterns may be subtle or domain-specific.

## Foundational Learning

**Multimodal Learning**
*Why needed:* Medical diagnosis often requires integrating information from multiple sources (images, clinical notes, lab results)
*Quick check:* Can the system process both chest X-ray images and associated clinical reports simultaneously?

**In-Context Learning**
*Why needed:* Enables models to learn new tasks without parameter updates by conditioning on example demonstrations
*Quick check:* Does the model correctly classify when provided with relevant example-label pairs in the prompt?

**Retrieval-Augmented Generation**
*Why needed:* Allows dynamic access to relevant information beyond the model's parametric knowledge
*Quick check:* Can the system retrieve semantically similar demonstrations for a given medical case?

## Architecture Onboarding

**Component Map**
ResNet/BERT embeddings -> Demonstration Index -> Retrieval Module -> Prompt Construction -> MLLM -> Classification Output

**Critical Path**
Input encoding → Similarity search in demonstration index → Top-K demonstration retrieval → Prompt augmentation → MLLM inference → Disease classification

**Design Tradeoffs**
- Embeddings vs. fine-tuned representations: Uses pre-trained models for efficiency but may miss domain-specific features
- Retrieval scope: Balances between demonstration relevance and retrieval computational cost
- Prompt length: Must include sufficient examples without exceeding MLLM context window limits

**Failure Signatures**
- Poor similarity metrics leading to irrelevant demonstration retrieval
- Demonstration index quality degradation over time without updates
- Context window overflow when too many demonstrations are retrieved
- MLLM failure to properly utilize retrieved demonstrations in prompt

**First Experiments**
1. Validate retrieval accuracy by testing if top-1 retrieved demonstration matches ground truth class for held-out samples
2. Test prompt construction by evaluating MLLM performance with varying numbers of retrieved demonstrations (1, 3, 5, 10)
3. Compare different similarity metrics (Euclidean, cosine, dot product) on a validation set to determine optimal retrieval strategy

## Open Questions the Paper Calls Out

**Open Question 1**
- Question: Can the RAICL framework be effectively extended to incorporate complex clinical modalities beyond static images and text, such as video, speech, and time-series signals?
- Basis in paper: The Discussion section explicitly states that "future studies should explore the application of RAICL to a broader range of modalities, such as video, speech, and time series signals," noting the current limitation to only image and text data.
- Why unresolved: The current study only validated the framework on histopathology slides, chest X-rays, and clinical notes, leaving the integration of temporal or audio data untested.
- What evidence would resolve it: Successful implementation of RAICL on datasets containing video or waveform data (e.g., ECG signals) showing performance improvements over non-retrieval baselines.

**Open Question 2**
- Question: Can optimization techniques like quantization or multi-task learning significantly reduce the computational cost of RAICL without compromising classification accuracy?
- Basis in paper: The authors identify the "non-trivial" computational cost of using large multimodal datasets as a limitation and explicitly propose investigating quantization and multi-task learning to improve feasibility in resource-constrained environments.
- Why unresolved: The current experiments relied on high-performance GPUs (NVIDIA A100s) and did not evaluate the framework's efficiency or performance under constrained resources or optimization regimes.
- What evidence would resolve it: Benchmarks showing reduced memory footprint or inference latency after applying quantization, while maintaining the accuracy gains demonstrated in the paper.

**Open Question 3**
- Question: Is it possible to develop an adaptive mechanism to automatically select the optimal embedding model (image-based vs. text-based) based on the specific information richness of a given sample?
- Basis in paper: The paper observes that text-based retrieval worked best for TCGA (pathology) while image-based retrieval worked best for IU X-ray, concluding that the "richness of information embedded in each modality will determine which embedding model can be used."
- Why unresolved: Currently, the choice of encoder (ResNet vs. BERT) appears to be a manual hyperparameter tuned specifically for each dataset rather than dynamically determined sample-by-sample.
- What evidence would resolve it: A "hybrid" or "adaptive" RAICL model that dynamically weights modalities or selects encoders and achieves equal or better macro-F1 scores compared to the static, dataset-specific best configurations.

## Limitations
- Evaluation limited to two specific medical datasets (TCGA and IU Chest X-ray)
- Small set of MLLMs tested (Q3, Llava, Gemma) without comprehensive architecture comparison
- Basic embedding models used without ablation studies on advanced multimodal encoders
- Computational cost and latency for clinical deployment not addressed

## Confidence
**High Confidence:** The core methodology of RAICL integrating RAG with ICL is clearly articulated and the reported accuracy improvements (from 0.7854 to 0.8368 on TCGA and 0.7924 to 0.8658 on IU Chest X-ray) are statistically significant and consistently observed across multiple MLLM architectures.

**Medium Confidence:** The claim that Euclidean distance achieves highest accuracy while cosine similarity yields better macro-F1 scores is based on the reported experiments but requires verification across additional datasets to confirm this pattern holds generally. The observation that increasing retrieved examples improves performance in few-shot settings needs validation with larger-scale experiments.

**Low Confidence:** The superiority of text-only inputs over image-only inputs in medical classification tasks contradicts typical expectations in medical imaging literature and requires further investigation to rule out dataset-specific artifacts or retrieval bias.

## Next Checks
1. Conduct ablation studies comparing RAICL's retrieval performance using domain-specific multimodal encoders (e.g., medical image-text transformers) versus the current unimodal embeddings to quantify the impact of embedding quality on demonstration selection.

2. Evaluate RAICL across additional medical imaging datasets (e.g., dermatology, pathology) and diverse disease classification tasks to establish generalizability beyond cancer and chest X-ray domains.

3. Perform computational overhead analysis measuring the end-to-end latency of the retrieval-augmented pipeline compared to standard ICL approaches, including GPU memory requirements and inference time trade-offs for clinical deployment scenarios.