---
ver: rpa2
title: Explainable Adversarial Attacks on Coarse-to-Fine Classifiers
arxiv_id: '2501.10906'
source_url: https://arxiv.org/abs/2501.10906
tags:
- coarse
- attacks
- attack
- adversarial
- relevance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces an explainable adversarial attack framework
  for coarse-to-fine (C2F) classifiers using Layer-wise Relevance Propagation (LRP).
  The method generates perturbations that target key features identified by LRP heatmaps
  at both coarse and fine classification stages, aiming to fool the model while maintaining
  interpretability.
---

# Explainable Adversarial Attacks on Coarse-to-Fine Classifiers

## Quick Facts
- arXiv ID: 2501.10906
- Source URL: https://arxiv.org/abs/2501.10906
- Authors: Akram Heidarizadeh; Connor Hatfield; Lorenzo Lazzarotto; HanQin Cai; George Atia
- Reference count: 24
- Primary result: LRP-based adversarial attacks achieve 87.1%-99.3% fooling ratios on ImageNet with VGG-16

## Executive Summary
This paper introduces an explainable adversarial attack framework for coarse-to-fine (C2F) classifiers using Layer-wise Relevance Propagation (LRP). The method generates perturbations that target key features identified by LRP heatmaps at both coarse and fine classification stages, aiming to fool the model while maintaining interpretability. Experiments on ImageNet with a VGG-16 network show that the LRPC attack achieves fooling ratios of 87.1%-99.3% with ℓ∞-norm perturbations of 0.0399-0.1557, while LRPF achieves 98.7%-100% fooling ratios with ℓ∞-norm values of 0.0241-0.0819. The approach demonstrates a tradeoff between explainability and perceptibility, outperforming traditional attacks in providing clearer interpretations of model behavior without sacrificing attack effectiveness.

## Method Summary
The framework generates adversarial examples for coarse-to-fine classifiers by leveraging LRP to identify relevant features at each classification stage. The attack optimizes perturbations to both fool the classifier and maximize changes in LRP-based relevance scores. Two variants are proposed: LRPC targets coarse classification stage features, while LRPF targets fine classification stage features. The method uses gradient-based optimization to minimize a combined loss function that balances classification error with changes in relevance distribution, producing perturbations that are both effective at fooling the model and interpretable through their impact on feature importance maps.

## Key Results
- LRPC attack achieves 87.1%-99.3% fooling ratios with ℓ∞-norm perturbations of 0.0399-0.1557 on ImageNet
- LRPF attack achieves 98.7%-100% fooling ratios with ℓ∞-norm values of 0.0241-0.0819
- The framework demonstrates enhanced interpretability through LRP heatmaps compared to traditional adversarial attacks
- Shows effective tradeoff between attack perceptibility and explainability for C2F classifiers

## Why This Works (Mechanism)
The approach works by exploiting the hierarchical decision-making process of coarse-to-fine classifiers. By targeting features identified as important through LRP at specific classification stages, the perturbations can effectively manipulate the model's reasoning process. The method leverages the fact that C2F classifiers make sequential decisions, allowing targeted attacks that disrupt specific stages of classification. The combination of fooling the model while maintaining interpretable relevance maps creates a unique form of explainable adversarial attack that reveals how the model arrives at incorrect decisions.

## Foundational Learning

**Layer-wise Relevance Propagation (LRP)**
*Why needed:* Provides a method to attribute classification decisions to input features through backward propagation of relevance scores
*Quick check:* Verify relevance scores sum to the classifier output and are properly normalized

**Coarse-to-Fine Classification**
*Why needed:* Understanding the two-stage hierarchical decision process is crucial for targeting specific classification stages
*Quick check:* Confirm coarse and fine classifiers have distinct feature spaces and decision boundaries

**Adversarial Perturbation Optimization**
*Why needed:* The attack must balance between fooling the classifier and maintaining interpretability through relevance changes
*Quick check:* Monitor both classification loss and LRP loss during optimization to ensure proper balance

## Architecture Onboarding

**Component Map**
Input Image -> VGG-16 Coarse Classifier -> Coarse Relevance Map -> Fine Classifier -> Fine Relevance Map -> Combined Loss Function

**Critical Path**
1. Forward pass through coarse classifier
2. LRP computation for coarse stage
3. Forward pass through fine classifier
4. LRP computation for fine stage
5. Combined loss calculation and backpropagation
6. Perturbation update

**Design Tradeoffs**
- Balance between attack effectiveness and perturbation perceptibility
- Choice between targeting coarse vs fine classification stages
- Computational cost of LRP computation vs attack quality
- Generalization across different C2F architectures

**Failure Signatures**
- Low fooling ratio despite high LRP changes
- Perturbations that are perceptible despite small ℓ∞-norm
- Relevance maps that don't align with human interpretability
- Poor transfer to other C2F models

**First Experiments**
1. Test attack effectiveness on clean vs defended models
2. Compare fooling ratios across different ℓ∞-norm budgets
3. Evaluate interpretability through user studies of LRP heatmaps

## Open Questions the Paper Calls Out
None explicitly stated in the paper.

## Limitations
- Evaluation limited to VGG-16 architecture, raising questions about generalizability to modern architectures with skip connections or attention mechanisms
- Does not test attack robustness against standard defense mechanisms like adversarial training or input preprocessing
- Lacks quantitative validation that manipulated relevance maps align with human understanding of model reasoning
- Perturbation budgets, while reasonable, may not represent real-world attack scenarios requiring truly imperceptible changes

## Confidence
- High confidence in the technical implementation of LRP-based perturbation generation
- Medium confidence in the claimed superiority of LRPC/LRPF over traditional attacks for interpretability
- Medium confidence in the fooling ratio comparisons due to limited architecture diversity

## Next Checks
1. Test the framework on diverse architectures (ResNet, EfficientNet) and datasets to verify generalizability
2. Conduct user studies comparing LRP heatmaps from adversarial perturbations against those from clean inputs to validate improved interpretability
3. Evaluate attack transferability across different coarse-to-fine models to assess practical effectiveness beyond single-model scenarios