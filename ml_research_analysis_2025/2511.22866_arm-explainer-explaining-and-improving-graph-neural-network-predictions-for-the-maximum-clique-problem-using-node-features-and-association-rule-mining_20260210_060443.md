---
ver: rpa2
title: ARM-Explainer -- Explaining and improving graph neural network predictions
  for the maximum clique problem using node features and association rule mining
arxiv_id: '2511.22866'
source_url: https://arxiv.org/abs/2511.22866
tags:
- graph
- neural
- node
- clique
- features
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ARM-Explainer, a post-hoc, model-level explainer
  based on association rule mining to explain and improve graph neural network (GNN)
  predictions for the maximum clique problem (MCP). ARM-Explainer discovers association
  rules from node features and GNN predictions, identifying the most important features
  and their value ranges that influence predictions.
---

# ARM-Explainer -- Explaining and improving graph neural network predictions for the maximum clique problem using node features and association rule mining

## Quick Facts
- arXiv ID: 2511.22866
- Source URL: https://arxiv.org/abs/2511.22866
- Reference count: 40
- Primary result: ARM-Explainer achieves high median lift and confidence values of 2.42 and 0.49, respectively, and improves GNN performance on MCP by 22% on large graphs.

## Executive Summary
This paper introduces ARM-Explainer, a post-hoc model-level explainer based on association rule mining (ARM) to explain and improve graph neural network (GNN) predictions for the maximum clique problem (MCP). The method discovers association rules from node features and GNN-predicted clique membership probabilities, identifying important features and their value ranges that influence predictions. Applied to the hybrid geometric scattering (HGS) GNN on TWITTER and BHOSLIB-DIMACS benchmark datasets, ARM-Explainer demonstrates that augmenting GNNs with informative node features significantly improves performance, increasing the median largest-found clique size by 22% on large graphs from the BHOSLIB-DIMACS dataset.

## Method Summary
ARM-Explainer performs post-hoc analysis by discretizing node features and GNN-predicted clique membership probabilities into percentile bins, then mining association rules of the form "Feature Range => Prediction" using the FP-Growth algorithm. A greedy selection algorithm filters these rules to a compact, non-overlapping set based on support and lift. The method is applied to the Hybrid Geometric Scattering (HGS) GNN, which uses a custom loss function to guide node probability assignment for clique membership. The approach demonstrates that informative node features, particularly those capturing neighborhood structure, significantly improve GNN performance for MCP on large, dense graphs.

## Key Results
- ARM-Explainer achieves high median lift (2.42) and confidence (0.49) values on MCP predictions
- Adding informative node features increases median largest-found clique size by 22% (from 29.5 to 36) on large BHOSLIB-DIMACS graphs
- For dense graphs, rules shift from simple degree-based antecedents to features capturing neighbor degree variability

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Association rule mining with FP-Growth generates human-readable, model-level explanations for GNN predictions on MCP.
- Mechanism: ARM-Explainer discretizes features and probabilities into percentile bins, then mines rules of the form "Feature Range => Prediction" using FP-Growth. A greedy selection filters rules based on support and lift.
- Core assumption: The GNN's decision-making can be approximated by statistical co-occurrence patterns between feature ranges and output classifications.
- Evidence anchors: Abstract confirms ARM-Explainer discovers association rules identifying important features and value ranges. Section 4 provides association rules explaining why nodes received high clique membership probabilities. Related work on explaining GNN predictions exists, but ARM-Explainer is novel for its specific application to combinatorial optimization using ARM.
- Break condition: The mechanism fails if GNN predictions rely on complex, non-linear feature interactions that cannot be captured by simple association rules, resulting in rules with low confidence or lift.

### Mechanism 2
- Claim: Informative node features, particularly those capturing neighborhood structure, significantly improve GNN performance for MCP on large, dense graphs.
- Mechanism: Standard GNN features are augmented with additional structural features like betweenness centrality and standard deviation of neighbor degrees, providing more discriminative signals for identifying clique members.
- Core assumption: The structural properties of nodes within a clique differ systematically from non-clique nodes in ways not fully captured by basic degree information, especially in dense graphs.
- Evidence anchors: Abstract states augmenting GNN with informative node features substantially improves performance, increasing median largest-found clique size by 22%. Results show when number of features increases, the antecedent changes to high standard deviation of degrees of neighboring nodes, indicating the GNN has learned new information. Consistent with general GNN principles but specific evidence for which features aid MCP on dense graphs is a key contribution.
- Break condition: Performance gains will not be observed if added features are redundant with existing features or not predictive of clique membership for the target graph distribution.

### Mechanism 3
- Claim: The HGS network's custom loss function guides node probability assignment for clique membership, which is then interpreted by ARM-Explainer.
- Mechanism: The HGS loss function has two components: L1 encourages high probabilities for highly connected nodes, while L2 penalizes sets of high-probability nodes that are not fully connected (violating clique constraint). ARM-Explainer identifies which node features the GNN has learned to associate with these high/low probability outcomes.
- Core assumption: The custom loss function effectively trains the GNN to output higher probabilities for nodes in dense subgraphs, making these probabilities a meaningful proxy for clique membership.
- Evidence anchors: Section 3.2.2 describes how L1 assigns higher probabilities to highly connected nodes and L2 adds penalty when maximum clique constraint is violated. Results show derived rules for TWITTER (high degree => high probability) align with L1, while complex rules for dense graphs show adaptation to L2 constraint. Corpus does not detail loss functions for GNN-based solvers, making this a specific mechanism of the target system.
- Break condition: If the loss function is not properly balanced or minimized, the resulting probabilities will not correlate with clique membership, rendering explanations meaningless.

## Foundational Learning

- **Association Rule Mining (ARM) & FP-Growth**: This is the core algorithm of the explainer. Understanding support, confidence, and lift metrics is essential for evaluating rule quality. Quick check: If a rule {A} => {B} has high confidence but low lift, what does that indicate about the relationship between A and B?

- **Graph Neural Networks (GNNs) & Message Passing**: The system explains a GNN. Understanding how GNNs transform node features via neighbor aggregation is prerequisite to appreciating what post-hoc explanation aims to uncover. Quick check: In a standard GNN layer, how does a node update its feature representation based on its neighbors?

- **Maximum Clique Problem (MCP)**: This is the task domain. Understanding what a clique is and why it's NP-hard provides context for why a learned heuristic and its explanation are valuable. Quick check: What are the two key conditions that define a subset of nodes as a clique in a given graph?

## Architecture Onboarding

- Component map: Graph + Raw Features -> Feature Computation -> HGS GNN -> Probabilities -> ARM Binning -> FP-Growth -> Rule Selection -> Explanation Report

- Critical path: The flow from raw graph and features through the HGS GNN to produce probabilities, which are then discretized and mined for association rules that explain the model's behavior.

- Design tradeoffs: Binning granularity uses 5 bins (20% increments) - finer bins could capture specific ranges but reduce rule support, risking overfitting. FP-Growth is chosen for efficiency by avoiding candidate generation, better for dense transaction data from node features. The system provides model-level explanations (general rules across graphs), not instance-level explanations.

- Failure signatures: Low lift values indicate discovered rules are no better than random chance. Trivial rule dominance (e.g., always "High Degree => High Probability") suggests the explainer may not capture nuanced model behavior. Feature overfitting occurs when performance improves on training data but not on test data.

- First 3 experiments: 1) Baseline Validation: Train HGS on TWITTER with original 3 features, run ARM-Explainer and verify it recovers expected rules. 2) Feature Ablation Study: Train HGS on BHOSLIB-DIMACS, incrementally add features, run ARM-Explainer after each addition to see which new features appear in high-lift rules and if clique size improves. 3) Cross-Dataset Check: Train HGS on TWITTER and apply to BHOSLIB graphs, run ARM-Explainer to see if rules transfer or fail, highlighting dataset-specific learning.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can ARM-Explainer be generalized to other NP-hard combinatorial optimization problems beyond MCP and to different GNN architectures?
- Basis in paper: The authors state that "ARM-Explainer can be applied to a wide variety of learning algorithms and graph-based COPs to investigate its performance in different problem-algorithm combinations."
- Why unresolved: The study only validated the method on MCP using the HGS algorithm, leaving its efficacy on problems like TSP or with architectures like GCNs unproven.
- What evidence would resolve it: Successful application of the ARM-Explainer pipeline to other benchmark COPs (e.g., MIS, MaxCut) showing high lift and confidence values across varied GNN architectures.

### Open Question 2
- Question: Do alternative association rule mining algorithms yield better explanatory rules than FP-Growth?
- Basis in paper: The paper notes that "there are newer versions of FP-Growth... as well as other ARM approaches... that can be applied to investigate whether they yield better association rules."
- Why unresolved: The study exclusively employed FP-Growth; other algorithms like Apriori, ECLAT, or newer variations were not benchmarked for their ability to generate high-lift rules in this context.
- What evidence would resolve it: A comparative analysis of rule quality (support, confidence, lift) generated by different ARM algorithms when applied to the same GNN prediction datasets.

### Open Question 3
- Question: How can self-interpretable GNN architectures be designed for COPs without merely solving the problem during the explanation step?
- Basis in paper: The authors suggest that "Self-interpretable approaches can be developed. However, it is not yet clear how such an architecture would look in a COP context."
- Why unresolved: The current method is post-hoc (treating GNN as black box), and the authors note that simple subgraph generation in a self-interpretable model might solve the problem rather than explain it.
- What evidence would resolve it: A proposed GNN architecture that inherently integrates explainability into the message-passing or loss function, validated against ARM-Explainer's fidelity and accuracy metrics.

## Limitations

- The reliance on discretized feature ranges may oversimplify complex, continuous feature interactions within the GNN's decision-making process
- The effectiveness of informative node features is contingent on their predictive power for clique membership; adding redundant or irrelevant features would not yield performance gains
- The reported 22% improvement is specific to large, dense BHOSLIB-DIMACS graphs and may not generalize to other graph types or sizes

## Confidence

- **High Confidence**: The core mechanism of using ARM for post-hoc explanation of GNN predictions is well-established, and the application to MCP is a novel extension. The performance improvement from adding informative features is a direct and measurable outcome.
- **Medium Confidence**: The claim that specific node features are key antecedents for dense graphs is based on observed rule changes, but the causal link between feature addition and improved clique identification is inferred rather than directly proven.
- **Low Confidence**: The assumption that association rules can fully capture the complex, non-linear decision boundaries learned by a deep GNN is the weakest link. The validity of the explanation depends heavily on the quality and relevance of the discovered rules, which are not directly validated against the GNN's internal parameters.

## Next Checks

1. **Rule Validation Against GNN Weights**: Examine the learned weights of the HGS GNN's first layer to verify if node features identified as important by ARM-Explainer (e.g., log std neighbor degree) indeed have high-magnitude weights or gradients, providing direct evidence of their importance to the model.

2. **Feature Ablation with Statistical Testing**: Conduct a formal ablation study on the BHOSLIB-DIMACS dataset, systematically removing each of the 10 features and measuring the statistical significance of the change in median clique size. This will quantify the individual contribution of each feature beyond anecdotal rule changes.

3. **Cross-Dataset Rule Transferability**: Train the HGS GNN on TWITTER and BHOSLIB-DIMACS separately, then test the model on a third, unseen dataset (e.g., a synthetic graph with known properties). Run ARM-Explainer on the predictions to see if the rules are dataset-specific or if a generalizable pattern emerges, validating the model-level explanation's robustness.