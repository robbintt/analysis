---
ver: rpa2
title: 'VeLU: Variance-enhanced Learning Unit for Deep Neural Networks'
arxiv_id: '2504.15051'
source_url: https://arxiv.org/abs/2504.15051
tags:
- velu
- activation
- arxiv
- learning
- neural
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: VeLU introduces a variance-aware activation function that combines
  ArcTan-ArcSin transformations, adaptive scaling based on local activation variance,
  and Wasserstein-2 regularization to improve training stability and generalization
  in deep neural networks. Unlike static activations like ReLU, Swish, or GELU, VeLU
  dynamically modulates its response based on input statistics without adding learnable
  parameters.
---

# VeLU: Variance-enhanced Learning Unit for Deep Neural Networks

## Quick Facts
- arXiv ID: 2504.15051
- Source URL: https://arxiv.org/abs/2504.15051
- Reference count: 40
- Primary result: VeLU achieves up to 3.12% (CIFAR-100) and 3.85% (Corel-10K) top-1 accuracy gains over baselines, while improving training stability and generalization without learnable parameters.

## Executive Summary
VeLU is a novel variance-aware activation function designed to enhance training stability and generalization in deep neural networks. It leverages adaptive scaling based on local activation variance and integrates ArcTan-ArcSin transformations with Wasserstein-2 regularization. Unlike static activations, VeLU dynamically modulates its response using input statistics, avoiding the need for learnable parameters. Empirical results across six architectures and twelve vision benchmarks demonstrate consistent improvements over ReLU, Swish, GELU, and other standard activations.

## Method Summary
VeLU introduces a variance-enhanced activation function that combines ArcTan-ArcSin transformations, adaptive scaling based on local activation variance, and Wasserstein-2 regularization to improve training stability and generalization in deep neural networks. Unlike static activations like ReLU, Swish, or GELU, VeLU dynamically modulates its response based on input statistics without adding learnable parameters. Experiments across six architectures on twelve vision benchmarks show consistent performance gains over baselines. On CIFAR-100 and Corel-10K, VeLU achieves up to 3.12% and 3.85% absolute improvements in top-1 accuracy respectively, while maintaining competitive training time and memory usage.

## Key Results
- VeLU achieves up to 3.12% and 3.85% absolute improvements in top-1 accuracy on CIFAR-100 and Corel-10K, respectively.
- Consistent performance gains observed across six architectures and twelve vision benchmarks.
- Maintains competitive training time and memory usage compared to baseline activations.

## Why This Works (Mechanism)
VeLU's adaptive scaling based on local activation variance allows the activation function to respond dynamically to the statistical properties of the input, improving training stability and generalization. The ArcTan-ArcSin transformation introduces non-linearity while the Wasserstein-2 regularization encourages smoother optimization landscapes. By avoiding learnable parameters, VeLU remains computationally efficient and reduces the risk of overfitting.

## Foundational Learning
- **Activation functions in neural networks**: Why needed—to introduce non-linearity and enable complex function approximation. Quick check—ensure understanding of ReLU, GELU, and Swish differences.
- **Variance-based normalization**: Why needed—to adaptively scale activations based on local statistics for stability. Quick check—compare with BatchNorm and LayerNorm.
- **Wasserstein-2 regularization**: Why needed—to encourage smoother optimization and improve generalization. Quick check—understand its effect on gradient flow.
- **ArcTan-ArcSin transformations**: Why needed—to provide non-linear, bounded transformations that stabilize training. Quick check—contrast with unbounded activations like ReLU.
- **Variance estimation in mini-batches**: Why needed—to enable per-layer adaptive scaling without extra parameters. Quick check—verify variance computation accuracy.
- **Activation function ablation studies**: Why needed—to isolate contributions of individual design choices. Quick check—plan ablation for ArcTan-ArcSin vs. variance scaling.

## Architecture Onboarding
- **Component map**: Input -> Variance Estimation -> ArcTan-ArcSin Transform -> Adaptive Scaling -> Output
- **Critical path**: Variance estimation → adaptive scaling → final output; the ArcTan-ArcSin transformation is applied before scaling.
- **Design tradeoffs**: Fixed parameters ensure efficiency and stability but may limit flexibility compared to learnable activations.
- **Failure signatures**: Instability under extreme variance values; degraded performance if variance estimation is inaccurate.
- **First experiments**: (1) Ablate ArcTan-ArcSin vs. variance scaling contributions. (2) Test performance on non-vision datasets (e.g., NLP). (3) Benchmark memory/runtime overhead vs. ReLU/GELU under identical training settings.

## Open Questions the Paper Calls Out
None

## Limitations
- Experimental scope limited to vision datasets and CNN-based architectures; unclear how VeLU performs on NLP or multimodal domains.
- Ablation for ArcTan-ArcSin transformation is absent, so its isolated contribution is unclear.
- Memory and runtime efficiency claims are asserted but not quantitatively benchmarked against other activations.

## Confidence
- **High confidence** in the variance-adaptive mechanism improving stability and generalization, supported by consistent empirical gains across multiple datasets and architectures.
- **Medium confidence** in the novelty and uniqueness of the combined ArcTan-ArcSin transformation, as the exact contribution of this design element is not fully isolated in ablation studies.
- **Medium confidence** in runtime and memory efficiency claims, since they are asserted but lack quantitative comparisons to other activation functions.

## Next Checks
1. Conduct ablation studies to isolate the effect of the ArcTan-ArcSin transformation versus the variance scaling component.
2. Evaluate VeLU on non-vision datasets (e.g., NLP or multimodal benchmarks) to assess cross-domain generalization.
3. Perform controlled experiments on memory and runtime overhead compared to ReLU, GELU, and other modern activations under identical training settings.