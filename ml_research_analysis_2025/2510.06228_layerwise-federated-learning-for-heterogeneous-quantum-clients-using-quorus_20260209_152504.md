---
ver: rpa2
title: Layerwise Federated Learning for Heterogeneous Quantum Clients using Quorus
arxiv_id: '2510.06228'
source_url: https://arxiv.org/abs/2510.06228
tags:
- quantum
- clients
- quorus
- layer
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the challenge of federated quantum learning
  where clients have heterogeneous hardware capabilities, requiring them to run quantum
  circuits of different depths. The proposed Quorus framework introduces layerwise
  loss functions that enable clients to train varying-depth quantum models while maintaining
  synchronized objectives through reverse knowledge distillation.
---

# Layerwise Federated Learning for Heterogeneous Quantum Clients using Quorus

## Quick Facts
- arXiv ID: 2510.06228
- Source URL: https://arxiv.org/abs/2510.06228
- Authors: Jason Han; Nicholas S. DiBrita; Daniel Leeds; Jianqiang Li; Jason Ludmir; Tirthak Patel
- Reference count: 40
- Primary result: Quorus increases gradient magnitudes for higher depth clients and improves testing accuracy by 12.4% on average over state-of-the-art methods for heterogeneous quantum federated learning.

## Executive Summary
This paper addresses federated quantum learning with heterogeneous clients that have varying hardware capabilities. The proposed Quorus framework introduces layerwise loss functions enabling clients with different circuit depths to train synchronized objectives through reverse knowledge distillation. Three circuit designs—Layerwise, Ancilla/Blocking, and Funnel—are optimized for different resource constraints. The framework was validated on IBM superconducting quantum processors, demonstrating 12.4% accuracy improvement over existing methods and practical viability on real hardware with accuracy within 3% of ideal simulation.

## Method Summary
Quorus implements federated quantum learning where clients with varying hardware capabilities run quantum circuits of different depths (2L-6L). The framework uses layerwise loss functions combining weighted cross-entropy with KL divergence between layer predictions, enabling synchronized training despite architectural differences. Three circuit designs are proposed: Layerwise (most accurate but shot-intensive), Ancilla/Blocking (single-shot evaluation requiring extra qubits or midcircuit measurement), and Funnel (single-shot with progressive qubit reduction). Parameters are aggregated using circular averaging per-layer across clients. The method was validated on binary classification tasks using MNIST and Fashion-MNIST datasets compressed to 10 features via PCA, with 5 clients and 1000 communication rounds.

## Key Results
- Quorus increases gradient magnitudes for higher depth clients compared to end-to-end loss formulations
- Achieves 12.4% average accuracy improvement over Q-HeteroFL baseline on heterogeneous quantum FL tasks
- Demonstrates practical viability on IBM quantum hardware with accuracy within 3% of ideal simulation
- Funnel design achieves 76-79% accuracy on challenging tasks while being robust to hardware noise

## Why This Works (Mechanism)

### Mechanism 1: Layerwise Loss Synchronization via Reverse Knowledge Distillation
- Enables clients with different circuit depths to train with aligned objectives despite different parameter spaces
- Each client computes loss from all classifier outputs (up to their depth) using weighted cross-entropy plus pairwise KL divergence between layer predictions
- Deeper clients receive gradient signals from shallower classifiers, while shallower clients benefit from knowledge distillation from aggregated deeper models

### Mechanism 2: Shot-Efficient Measurement via Funnel Circuit Design
- Enables extraction of all layer outputs in a single circuit execution by progressively reducing qubit count
- Each layer operates on progressively fewer qubits (dropping one after measurement), with all measurements deferred to circuit end
- Avoids mid-circuit measurement errors while allowing ensemble evaluation from one shot

### Mechanism 3: Gradient Magnitude Preservation via Local Loss Credit Assignment
- Earlier layer parameters maintain higher gradient norms compared to end-to-end loss formulations
- Each layer has its own classifier and loss term, so gradients for early parameters flow through shorter computational paths
- Partially counteracts barren plateau effects by preventing gradient vanishing through full circuit depth

## Foundational Learning

- **Parameterized Quantum Circuits (PQCs)**
  - Why needed here: Quorus assumes clients train PQCs where gate rotation angles are learnable parameters; understanding ansatz structure is essential for selecting appropriate circuit designs
  - Quick check question: Can you explain why circuit depth affects both expressibility and error accumulation on NISQ devices?

- **Federated Learning with Model Heterogeneity**
  - Why needed here: Quorus extends classical heterogeneous FL concepts to quantum settings; understanding parameter aggregation across different architectures is prerequisite
  - Quick check question: What problem does circular averaging solve when aggregating rotation angle parameters?

- **Quantum Measurement and State Collapse**
  - Why needed here: The fundamental challenge motivating Quorus's three circuit designs is that measuring a qubit mid-circuit collapses its state, preventing reuse in subsequent layers
  - Quick check question: Why can't we simply copy a quantum state to measure it and continue computation with the original?

## Architecture Onboarding

- **Component map:**
  - Aggregation server -> Heterogeneous clients -> Local quantum processors
  - Server extracts client-specific submodel, broadcasts updated parameters
  - Clients run local training at maximum viable depth (2L-6L in experiments)

- **Critical path:**
  1. Initialize shared parameters θ₀
  2. Server extracts client-specific submodel θ̃ₜ = θₜ[:dₖ] for each client k
  3. Each client runs E local epochs computing layerwise loss (Eq. 1)
  4. Clients return updated parameters; server aggregates using angle(Σᵢᵉⁱθⁱᵏ) for each layer position
  5. Repeat for T rounds

- **Design tradeoffs:**
  - Layerwise: Most accurate (Table 2, 3), but shot budget scales O(L) with depth
  - Ancilla/Blocking: Single-shot evaluation, but requires extra qubits or midcircuit measurement capability
  - Funnel: Single-shot, no ancillas, but operates in progressively smaller Hilbert space

- **Failure signatures:**
  - Gradient norms near zero across all layers → potential barren plateau; try reducing depth or increasing learning rate
  - Large accuracy gap between simulation and hardware (>10%) → depth exceeds device coherence; reduce maximum client depth
  - KL divergence terms explode → predictions too confident; add label smoothing or reduce KL weight

- **First 3 experiments:**
  1. Replicate Table 2 comparison (MNIST 0/1, 2L-6L clients) using Quorus-Layerwise vs Q-HeteroFL baselines to validate 12.4% improvement claim
  2. Run ablation on KL divergence weight (coefficient from 0.0 to 1.0 in Eq. 1) to find stable training regime
  3. Deploy trained Funnel model on available IBM QPU; compare ideal simulation vs hardware accuracy to validate 3% gap claim and identify device-specific depth limits

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the Quorus framework be adapted to handle multi-class classification tasks that require measurements across multiple qubits?
- Basis in paper: [explicit] The authors state, "The latter choice is because we focus on binary classification tasks in this work, so measuring a single qubit is sufficient."
- Why unresolved: The proposed designs (Layerwise, Ancilla, Funnel) currently extract an ensemble of classifiers from a single measured qubit. Extending this to multi-class scenarios would require fundamental changes to the measurement strategy and loss function coordination.
- What evidence would resolve it: A demonstration of Quorus training on a dataset with more than two classes, along with an analysis of how the Funnel design maintains shot efficiency with increased output measurements.

### Open Question 2
- Question: How does Quorus perform on alternative quantum hardware architectures, such as trapped-ion or neutral atom processors, which offer different connectivity constraints?
- Basis in paper: [explicit] The paper validates the framework specifically on "IBM's state-of-the-art superconducting quantum computers," and explicitly assumes "only nearest-neighbor connectivity" in the design.
- Why unresolved: The efficiency of the V-shaped ansatz and the feasibility of the Ancilla design (which may require long-distance connectivity) may vary significantly on hardware with native all-to-all connectivity or different gate fidelities.
- What evidence would resolve it: Comparative benchmarks of the Quorus ansatzes on non-superconducting quantum processing units (e.g., IonQ or QuEra hardware).

### Open Question 3
- Question: Can Quorus be scaled to handle high-dimensional data without relying on classical pre-processing techniques like PCA?
- Basis in paper: [explicit] Appendix C notes, "image data is high-dimensional and amplitude encoding is not feasible... we perform angle encoding. This means that we must compress the image into a set of 10 features, and we do so using PCA."
- Why unresolved: The current framework relies on a classical dimensionality reduction bottleneck (Federated PCA) to make data compatible with NISQ devices. It is unclear if the framework could handle higher-dimensional encodings as qubit counts increase.
- What evidence would resolve it: Experimental results using higher-feature-count angle embeddings or amplitude embeddings on larger quantum systems, eliminating the need for manual feature compression.

## Limitations

- Scalability to multi-class problems and deeper circuits beyond 6 layers remains unproven
- Progressive qubit reduction in Funnel design may severely limit expressibility for complex tasks
- 12.4% accuracy improvement claim based on limited baseline comparisons and specific binary tasks

## Confidence

- **High Confidence:** Theoretical foundation of layerwise loss functions and reverse knowledge distillation mechanism; practical demonstration on IBM hardware showing real-world viability
- **Medium Confidence:** 12.4% accuracy improvement claim (based on limited baseline comparisons and specific binary tasks); gradient magnitude preservation mechanism (correlates with theory but limited ablation)
- **Low Confidence:** Generalizability to multi-class problems and deeper circuits; exact contribution of KL divergence terms vs weighted cross-entropy; robustness under non-IID data distributions

## Next Checks

1. **Multi-class scalability test:** Extend experiments to 5-class MNIST classification using Quorus-Layerwise; measure accuracy degradation vs binary tasks and evaluate whether layerwise loss maintains effectiveness with more output classes

2. **Depth stress test:** Train Quorus with clients at depths 2L-10L (extending beyond current 6L maximum); monitor gradient norms, barren plateau emergence, and accuracy saturation to identify practical depth limits

3. **Hardware noise characterization:** Deploy trained models on multiple IBM QPU devices with varying qubit counts and coherence times; quantify accuracy variance across devices to assess Quorus's robustness to hardware heterogeneity beyond circuit depth