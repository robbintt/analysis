---
ver: rpa2
title: 'Graph Repairs with Large Language Models: An Empirical Study'
arxiv_id: '2507.03410'
source_url: https://arxiv.org/abs/2507.03410
tags:
- graph
- repair
- repairs
- llms
- graphs
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluates the effectiveness of six open-source large
  language models (LLMs) for automated property graph repair. Using a synthetic medical
  dataset with artificial inconsistencies, the research tests different encoding methods
  and prompt configurations to guide LLMs in identifying and correcting violations
  of graph denial constraints.
---

# Graph Repairs with Large Language Models: An Empirical Study

## Quick Facts
- arXiv ID: 2507.03410
- Source URL: https://arxiv.org/abs/2507.03410
- Reference count: 40
- Primary result: LLMs can follow structured output formats for graph repair with 72-96% adherence, but repair accuracy remains moderate at 42-74%

## Executive Summary
This study evaluates six open-source large language models for automated property graph repair using a synthetic medical dataset with artificial inconsistencies. The research tests different encoding methods and prompt configurations to guide LLMs in identifying and correcting violations of graph denial constraints. Results show that while all tested models can follow the required output format with high success rates, their accuracy in generating correct repairs remains moderate. The deepseek-r1 model achieves the highest repair validity but at significant computational cost, while models like phi4 and gemma2 show potential for balancing accuracy and efficiency under appropriate configurations.

## Method Summary
The study uses a synthetic medical property graph with artificial inconsistencies created by injecting GDC violations. Six open-source LLMs are tested across three encoding modes: node-edge representations (M1), template-based natural language descriptions (M2), and LLM-generated encodings (M3). Each violation is processed through a prompt containing the subgraph encoding, system instructions, output format specification, and optional few-shot examples. The LLM response is parsed to extract repair operations, which are validated against ground truth. Evaluation metrics include format adherence (F), repair validity (V), and accuracy (A) across different model configurations.

## Key Results
- Template-based encoding yields higher repair validity than raw node-edge representations, with gemma2 achieving 100% validity using M2 encoding
- deepseek-r1 achieves highest repair validity (76-84%) but requires 5-10x longer inference times (11.4s vs 1s for qwen2.5)
- Few-shot examples improve format adherence but risk overfitting, with llama3.2 showing 92% adherence with 1-shot but 0% accuracy across all configurations

## Why This Works (Mechanism)

### Mechanism 1
Template-based graph encoding yields higher repair validity than raw node-edge representations. Graph substructures are converted to human-readable natural language using predefined templates (M2) before being fed to LLMs. The template describes the inconsistency pattern alongside specific entity variables. Core assumption: LLMs perform better when graph structure is presented in semantically meaningful natural language rather than raw structural encodings. Evidence: Table 4 shows Template encoding achieving 100% validity for gemma2 and 98% for mistral, outperforming Graph encoding. Section 5.1 states: "Overall, template-based encoding modes tend to yield higher validity and correctness scores."

### Mechanism 2
Few-shot examples improve format adherence but can cause models to copy repair operations indiscriminately. Providing 1-2 example repairs in the prompt teaches the output format but introduces risk of "blind copying" where models replicate example operations regardless of actual inconsistency. Core assumption: Models learn output syntax from examples but may overfit to example content without understanding repair semantics. Evidence: Table 6 shows llama3.2 jumping from 11% format adherence (none) to 92% (1-small), but accuracy remains at 0% across all configurations. Section 6 states: "LLMs exhibit a tendency to blindly copy repairs suggested in the examples."

### Mechanism 3
Reasoning-style models (deepseek-r1) achieve higher validity through explicit chain-of-thought but incur substantial computational overhead. Models that generate internal reasoning (within `<think>` tags) before outputting repairs demonstrate better understanding of graph structure, producing more valid repairs at the cost of 5-10x longer generation time. Core assumption: Explicit reasoning traces improve repair quality by forcing models to parse graph relationships before acting. Evidence: Section 5 reports deepseek-r1 achieves 76-84% validity across encoding modes but requires 11.40 sec/prompt vs 1.00 sec for qwen2.5. Figure 3 shows deepseek-r1 leading in validity metrics across configurations.

## Foundational Learning

- **Property Graph Model**:
  - Why needed here: The entire repair pipeline operates on property graphs where nodes carry labels and key-value attributes, and edges represent labeled relationships.
  - Quick check question: Can you distinguish between a node label, a node property key, and an edge type in a patient-medication graph?

- **Graph Denial Constraints (GDCs)**:
  - Why needed here: Repairs are triggered by GDC violations—conditions that must NOT hold. Understanding constraint specification (pattern + attribute conditions) is essential for defining what constitutes an inconsistency.
  - Quick check question: Given a GDC stating "patients must not take medications containing their allergens," what graph pattern would violate it?

- **Cypher Query Language Basics**:
  - Why needed here: Inconsistency detection uses Cypher MATCH patterns to find violations. Reading and modifying these queries is required to adapt the pipeline to new constraint types.
  - Quick check question: What does `MATCH (p:Patient)-[r:TAKES]->(m:Medication) RETURN *` return?

## Architecture Onboarding

- **Component map**:
  Neo4j graph database -> Cypher query engine -> Encoder module -> Prompt builder -> Ollama runtime (LLM) -> Response parser -> Validator

- **Critical path**:
  1. Cypher query detects GDC violation → returns subgraph
  2. Encoder transforms subgraph to text format
  3. Prompt builder constructs complete prompt with encoding + examples
  4. LLM generates repairs in structured format
  5. Parser extracts operations → validator checks against constraints

- **Design tradeoffs**:
  - Accuracy vs latency: deepseek-r1 (74% accuracy, 11.4s) vs gemma2 (32% accuracy, 1.3s)
  - Encoding effort vs portability: M2 templates require manual authoring per constraint type; M1 is generic but lower performance
  - Few-shot examples: More examples improve format adherence but risk overfitting to example repairs

- **Failure signatures**:
  - Excessive repair operations: Model suggests 8+ operations when 1-2 suffice (llama3.2 averages 8.7 operations)
  - Hallucinated updates: Model proposes changes to unrelated nodes not involved in the violation
  - Indecision output: Model offers multiple repair "options" instead of a single repair sequence
  - Format collapse: Model ignores `<repairs>` tag structure (common in zero-shot with llama3.2: 11% adherence)

- **First 3 experiments**:
  1. Replicate Table 4 with a single constraint type using M1 (node-edge) and M2 (template) encoding on gemma2—compare validity scores to establish baseline.
  2. Test few-shot sensitivity: Run phi4 with 0, 1-small, and 2-small examples on 20 violations—observe if accuracy degrades from 1-shot to 2-shot as reported.
  3. Measure latency-accuracy frontier: Time deepseek-r1 vs qwen2.5 on identical violations to quantify the 10x speed-accuracy tradeoff for your latency budget.

## Open Questions the Paper Calls Out
None

## Limitations
- Relies on synthetic data rather than real-world inconsistencies, potentially missing production graph complexity
- Focuses on a single medical schema, limiting generalizability to other domains like social networks or knowledge graphs
- Does not assess semantic validity beyond constraint satisfaction or account for distributed deployment scaling costs

## Confidence
**High confidence** (supported by direct experimental evidence):
- LLMs can follow structured output formats with high adherence rates (72-96%) when provided with appropriate encoding and examples
- Template-based encoding consistently outperforms raw graph representations for repair validity across all tested models
- deepseek-r1 achieves highest repair validity but requires 5-10x longer inference times than other models

**Medium confidence** (supported by experimental results but with caveats):
- Few-shot examples improve format adherence but risk overfitting to example content rather than understanding repair semantics
- Models exhibit tendency to generate excessive repair operations and hallucinated updates beyond the scope of detected inconsistencies
- Trade-off between accuracy and efficiency exists across models, with some (phi4, gemma2) offering reasonable balance under optimal configurations

**Low confidence** (primarily inferred from this study or limited evidence):
- Chain-of-thought reasoning universally improves graph repair quality across different constraint types and domains
- The observed latency-accuracy tradeoffs will hold for larger, more complex graph repair tasks beyond the tested scope

## Next Checks
1. **Generalization test**: Apply the best-performing encoding method (template-based) and prompt configuration to a completely different domain (e.g., social network or academic citation graph) with naturally occurring inconsistencies to assess cross-domain transferability.

2. **Real-world deployment validation**: Replace synthetic inconsistencies with real-world data containing verified violations from production Neo4j databases to measure performance degradation or improvement compared to synthetic benchmarks.

3. **Scalability and cost analysis**: Evaluate inference times and repair accuracy when processing batches of 100+ violations simultaneously, measuring both per-instance latency and total system throughput under different model configurations.