---
ver: rpa2
title: In-depth Analysis on Caching and Pre-fetching in Mixture of Experts Offloading
arxiv_id: '2511.05814'
source_url: https://arxiv.org/abs/2511.05814
tags:
- expert
- experts
- layer
- caching
- cache
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work conducts an in-depth analysis of caching and pre-fetching
  techniques for Mixture of Experts (MoE) model offloading to address memory constraints
  on edge devices. The authors analyze expert activation patterns and LRU caching
  behavior, then propose an LFU caching optimization that improves token generation
  speed by up to 84.6% compared to LRU on A6000 GPU, with precision of 29.9% and recall
  of 59.8%.
---

# In-depth Analysis on Caching and Pre-fetching in Mixture of Experts Offloading

## Quick Facts
- **arXiv ID**: 2511.05814
- **Source URL**: https://arxiv.org/abs/2511.05814
- **Reference count**: 3
- **Primary result**: LFU caching optimization improves token generation speed by up to 84.6% compared to LRU on A6000 GPU, with precision of 29.9% and recall of 59.8%.

## Executive Summary
This work addresses memory constraints in Mixture of Experts (MoE) model offloading by analyzing caching and pre-fetching techniques. The authors investigate expert activation patterns and LRU caching behavior, then propose an LFU caching optimization that significantly improves inference speed. They also implement speculative expert pre-loading, demonstrating high prediction accuracy for next-layer expert activations. The study provides detailed traces of expert activation distributions, revealing temporal locality and expert imbalance across MoE layers. The research shows that LFU caching achieves up to 84.6% improvement in token generation speed compared to LRU, while speculative pre-loading reaches 84.6% precision and recall in predicting expert activations.

## Method Summary
The method builds upon the mixtral-offloading baseline repository, implementing 4-bit HQQ quantization for attention and 2-bit HQQ for experts with compress zero on all layers. LFU caching is implemented by adding usage count tracking to expert metadata, replacing LRU's temporal eviction policy with frequency-based retention. Speculative pre-loading is achieved by computing the next layer's gating probabilities using current hidden states before the current layer completes. The system was tested on Mixtral 8x7B-Instruct with MMLU benchmark subset, using temperature=0.9 for MMLU and 0.1 for speed tests, batch size=1, and cache size=4. Experiments were conducted across multiple GPU architectures including A100, A6000, L40, and RTX 3090.

## Key Results
- LFU caching optimization improves token generation speed by up to 84.6% compared to LRU on A6000 GPU
- LFU achieves precision of 29.9% and recall of 59.8% in cache hit prediction
- Speculative expert pre-loading demonstrates 84.6% precision and recall in predicting next-layer expert activations
- Expert activation distributions show temporal locality and significant imbalance across MoE layers

## Why This Works (Mechanism)

### Mechanism 1: Frequency-Based Cache Retention (LFU)
The LFU mechanism improves inference speed by retaining high-value experts based on cumulative usage frequency rather than recent access patterns. The system tracks expert usage counts and evicts the least frequently used expert when cache is full. This aligns cache residency with the skewed probability distribution of expert activation, ensuring critical experts remain available even during semantic shifts. Evidence shows expert number 2 on the 8th layer was activated only once, while other experts show concentrated activation patterns. The break condition occurs when prompt context shifts drastically, causing LFU to retain stale high-frequency experts while LRU would adapt faster.

### Mechanism 2: Cross-Layer Speculative Pre-fetching
The speculative pre-loading mechanism leverages the residual nature of Transformer layers to predict next-layer expert activations. Since hidden states at layer N are highly correlated with layer N+1 input, the system can compute gating probabilities for the next layer before the current layer completes. This masks memory transfer latency by pre-loading required experts. Evidence shows precision and recall both reaching 84.6% for speculative loading predictions. The break condition occurs if model architecture changes to non-residual connections where layer N drastically alters the semantic vector.

### Mechanism 3: Memory-Compute Trade-off via Quantization
Aggressive quantization enables larger effective working sets within limited VRAM. The system uses 4-bit HQQ for attention and 2-bit HQQ for experts, reducing the memory footprint and PCIe transfer cost. This allows maintaining more experts in cache, directly reducing bottleneck from PCIe bandwidth limits. Evidence shows trade-off between offload frequency, memory usage, and quality measured by MMLU scores. The break condition occurs when quantization noise becomes dominant, causing model quality to drop below usable thresholds.

## Foundational Learning

- **Mixture of Experts (MoE) Sparsity**: Understanding that MoE models activate only a subset of parameters per token creates the offloading opportunity but also the bottleneck. Quick check: If a MoE model has 8 experts and selects 2 per token, what is the maximum theoretical reduction in active parameter count compared to a dense model?

- **Cache Replacement Policies (LRU vs. LFU)**: The paper's core contribution replaces LRU (time-based) with LFU (count-based). LRU protects against stale data but forgets popular data, while LFU protects popular data but risks caching stale trends. Quick check: In a prompt discussing "Python coding" followed immediately by "French cooking," which policy would suffer more from cache pollution?

- **PCIe Bandwidth vs. Compute Bound**: The primary constraint in offloading is moving data from CPU RAM to GPU VRAM across the PCIe bus, not GPU compute speed. Quick check: Why does overlapping computation with data transfer provide speedup only if transfer time is not significantly larger than computation time?

## Architecture Onboarding

- **Component map**: CPU RAM -> GPU VRAM (Cache + Attention + KV Cache) -> GPU Computation -> PCIe Bus -> CPU RAM
- **Critical path**: Token enters GPU → Attention computation → Gating Decision → Cache Check → Expert Computation
- **Design tradeoffs**: Cache Size vs. Accuracy (reducing cache size lowers memory but increases latency), LFU Stickiness vs. LRU Volatility (LFU improves speed but may keep stale experts), Speculation Accuracy vs. Bandwidth (bad speculation wastes PCIe bandwidth)
- **Failure signatures**: Accuracy Collapse (MMLU score drops significantly), Cache Thrashing (low precision/recall, frequent cache miss stalls), Speculation Contention (PCIe hits 100% with low precision)
- **First 3 experiments**: 1) Baseline Profile - Run Mixtral 8x7B offloading baseline on MMLU subset, 2) Imbalance Trace - Log expert activation histograms for different prompts, 3) LFU Comparison - Implement LFU cache and measure delta in tokens/sec against baseline

## Open Questions the Paper Calls Out

1. Can a hybrid caching algorithm combining expert popularity and idle time achieve higher inference speeds than LFU? The authors note LFU improves speed but lacks precision, suggesting some combination of popularity and unused count might be better, but only implemented pure LRU and LFU strategies.

2. Can overlapping speculative expert pre-loading with current layer computation resolve bandwidth contention and cache eviction issues? The authors state overlapping might resolve problems but complexity prevented full implementation.

3. Do expert activation imbalances and speculative pre-fetching accuracy generalize to other MoE architectures? The study is limited to Mixtral 8x7B, with authors noting "Expert models might exhibit different behaviors" in other settings.

## Limitations
- Evaluation limited to single MoE architecture (Mixtral 8x7B) raises generalizability concerns
- Speculative pre-loading benefits remain theoretical without full implementation of computation-overlap
- Fixed MMLU datasets may not capture edge cases where cache policies fail catastrophically
- Quantization strategy shows acceptable quality degradation but lacks comprehensive tradeoff analysis

## Confidence

- **High Confidence**: Expert activation distribution analysis showing temporal locality and expert imbalance is well-supported by empirical traces and aligns with established MoE characteristics
- **Medium Confidence**: LFU caching improvement (84.6% speed gain) is empirically validated but tested on single model and dataset; speculative pre-loading precision/recall results are measured but practical benefits remain theoretical
- **Low Confidence**: Claims about generalization to other MoE architectures, impact of different prompt types on cache performance, and full end-to-end latency improvement from speculative pre-loading are not empirically validated beyond Mixtral 8x7B case

## Next Checks

1. **Cross-Architecture Generalization**: Test LFU caching and speculative pre-loading on a different MoE architecture (e.g., 16x22B or 4x32B) with distinct expert counts and routing strategies to validate core assumptions about expert activation patterns and cross-layer predictability.

2. **Dynamic Prompt Sensitivity**: Evaluate caching performance across dramatically different prompt domains (e.g., code → creative writing → mathematical reasoning in sequence) to identify cache pollution scenarios where LFU's frequency-based retention fails compared to LRU's recency-based approach.

3. **End-to-End Latency Validation**: Implement full speculative pre-loading with asynchronous transfer overlap and measure actual tokens/second improvement, including PCIe bandwidth utilization and cache miss rates, to confirm theoretical benefits translate to measurable performance gains under realistic inference conditions.