---
ver: rpa2
title: Am I Blue or Is My Hobby Counting Teardrops? Expression Leakage in Large Language
  Models as a Symptom of Irrelevancy Disruption
arxiv_id: '2508.01708'
source_url: https://arxiv.org/abs/2508.01708
tags:
- leakage
- expression
- semantic
- prompt
- sentiment
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces expression leakage, a novel form of unintended
  generalisation in large language models (LLMs) where sentimentally expressive but
  semantically irrelevant prompts systematically influence model outputs. The authors
  define this as a subclass of semantic leakage, focusing on affective drift rather
  than factual or conceptual shifts.
---

# Am I Blue or Is My Hobby Counting Teardrops? Expression Leakage in Large Language Models as a Symptom of Irrelevancy Disruption

## Quick Facts
- arXiv ID: 2508.01708
- Source URL: https://arxiv.org/abs/2508.01708
- Reference count: 6
- Primary result: Expression leakage causes sentimentally expressive but semantically irrelevant prompts to systematically influence LLM outputs, with negative sentiment causing more disruption than positive

## Executive Summary
This paper introduces expression leakage, a novel form of unintended generalization in large language models where sentimentally expressive but semantically irrelevant prompts systematically influence model outputs. The authors define this as a subclass of semantic leakage, focusing on affective drift rather than factual or conceptual shifts. They demonstrate that this phenomenon is prevalent across models, diminishes with scale within the same LLM family, and is not reliably mitigated by prompting. Notably, negative sentiment injections cause more disruptive leakage than positive ones, suggesting asymmetric sensitivity to affective content.

## Method Summary
The authors curate a benchmark dataset (HEXL) with 60 samples and 180 prompts, and develop an automatic data generation scheme producing 200 samples (600 prompts) from web-scale text. They propose an automatic evaluation framework that correlates well with human judgment, measuring sentiment shifts using an external expression estimator. Experiments systematically test expression leakage across different model sizes and families, comparing semantic relevance against sentiment expressiveness. The methodology includes both human evaluation and automatic assessment, with instruction-tuned models included to examine mitigation effects.

## Key Results
- Expression leakage is prevalent across models, with negative sentiment injections causing more disruptive leakage than positive ones
- Leakage diminishes with model scale within the same LLM family
- Instruction tuning helps only partially, suggesting targeted mitigation strategies are needed
- Automatic evaluation framework correlates well with human judgment

## Why This Works (Mechanism)
The paper identifies expression leakage as a form of affective generalization where LLMs transfer sentiment expressions from irrelevant contexts to generated outputs. This occurs because LLMs learn to associate certain linguistic patterns with emotional valence, leading to systematic bias when prompts contain sentimentally charged but semantically disconnected content.

## Foundational Learning
- Semantic leakage - why needed: Understanding broader generalization failures that include factual, conceptual, and affective drift
- Quick check: Compare expression leakage against other semantic leakage types in controlled experiments

- Affective computing - why needed: Measuring and quantifying sentiment shifts requires specialized tools beyond standard NLP metrics
- Quick check: Validate expression estimator performance across diverse emotional contexts

- Instruction tuning effects - why needed: Assessing whether fine-tuning on instruction datasets mitigates unwanted generalization behaviors
- Quick check: Compare leakage across base vs. instruction-tuned models of same architecture

## Architecture Onboarding
- Component map: Input prompts -> Expression estimator -> Sentiment analysis -> Output classification
- Critical path: Prompt generation → Model inference → Sentiment evaluation → Statistical analysis
- Design tradeoffs: Manual vs. automatic data generation balances control against scalability
- Failure signatures: Asymmetric negative-positive leakage, scale-dependent mitigation effects
- First experiments: 1) Test leakage across different sentiment intensities, 2) Compare leakage in multi-turn vs. single-turn contexts, 3) Evaluate cross-cultural expression patterns

## Open Questions the Paper Calls Out
None

## Limitations
- HEXL benchmark contains only 60 samples, potentially limiting generalizability across diverse linguistic contexts
- Automatic data generation scheme inherits potential biases from web-scale text corpus
- Automatic evaluation framework relies on external expression estimator whose performance characteristics are not thoroughly characterized
- Study focuses primarily on sentiment expression leakage, potentially missing other forms of affective or stylistic leakage

## Confidence
- High confidence: Existence of expression leakage as measurable phenomenon across multiple models and observation that leakage diminishes with model scale within same family
- Medium confidence: Instruction tuning only partially mitigates expression leakage and assertion that targeted mitigation strategies are needed
- Low confidence: Automatic evaluation framework's correlation with human judgment needs independent validation

## Next Checks
1. Conduct cross-cultural validation by expanding HEXL benchmark to include diverse linguistic and cultural contexts beyond current scope
2. Implement ablation studies on automatic evaluation framework by comparing outputs against multiple human annotation rounds using different sentiment analysis tools
3. Test negative-positive sentiment asymmetry by systematically varying intensity and type of emotional content in prompts while controlling for semantic relevance