---
ver: rpa2
title: 'Geolog-IA: Conversational System for Academic Theses'
arxiv_id: '2510.02653'
source_url: https://arxiv.org/abs/2510.02653
tags:
- para
- tesis
- sistema
- datos
- conversacional
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Geolog-IA, a conversational AI system for
  accessing geology theses from the Central University of Ecuador. It uses Llama 3.1
  and Gemini 2.5 language models, a Retrieval-Augmented Generation (RAG) architecture,
  and an SQLite database to provide natural, accurate answers to queries about academic
  theses.
---

# Geolog-IA: Conversational System for Academic Theses

## Quick Facts
- arXiv ID: 2510.02653
- Source URL: https://arxiv.org/abs/2510.02653
- Reference count: 0
- System uses Llama 3.1 and Gemini 2.5 with RAG architecture for geology thesis queries

## Executive Summary
Geolog-IA is a conversational AI system that enables natural language querying of geology theses from the Central University of Ecuador. The system combines Retrieval-Augmented Generation (RAG) with structured database retrieval via SQL to handle both qualitative content queries and quantitative statistical questions. Using a LangChain SQL agent with zero-shot reasoning, the system achieves high accuracy (BLEU score of 0.87) in answering questions about thesis metadata including topics, advisors, publication years, and page counts. The implementation supports both local deployment with Llama 3.1 via Ollama and API-based deployment with Gemini 2.5.

## Method Summary
The system implements a RAG-SQL architecture where a LangChain SQL agent processes natural language questions by generating and executing SQL queries against an SQLite database containing 244 thesis records across 16 fields. The agent uses a ReAct (Reasoning + Acting) loop with zero-shot prompting to handle query generation and error correction. User queries are processed through Gradio or Streamlit interfaces, with responses generated by combining database results with LLM reasoning. The system supports both qualitative queries (searching thesis summaries) and quantitative queries (counts, averages, comparisons) through structured database access.

## Key Results
- Achieved BLEU score of 0.87 for response evaluation, indicating high coherence and precision
- Successfully handles both qualitative queries (topic-based searches) and quantitative queries (statistical analysis)
- Provides web-based conversational interface for accessing thesis information

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Structured database retrieval via RAG-SQL enables both quantitative and qualitative query resolution, which pure text-based RAG cannot achieve.
- Mechanism: The system stores thesis metadata (16 fields including numeric data like year_approval, number_pages) in SQLite. When a user asks "How many theses were published in 2022?", the agent generates SQL (`SELECT COUNT(*) FROM tesis WHERE year_approval = 2022`), executes it, and returns structured results. For qualitative queries like "What theses address volcanic risks?", the system searches the "resumen" text field. This dual capability emerges from the structured schema design.
- Core assumption: Users need both statistical queries (counts, averages, comparisons) AND content-based retrieval; neither alone suffices for academic research support.
- Evidence anchors:
  - [section 3.2.1]: "This approximation revealed a significant limitation: the system was only capable of offering answers to questions of qualitative character... Al intentar responder a preguntas que requerían datos específicos o comparaciones numéricas... eran difíciles o imposibles de responder con precisión"
  - [section 3.3.2]: Architecture diagram shows the full flow from user question → agent → LLM → SQL query → database → response generation
  - [corpus]: Weak direct corpus evidence for RAG-SQL specifically; related work [15] discusses text-to-SQL with LLMs but in different evaluation contexts
- Break condition: If your use case requires ONLY qualitative responses (no counting, filtering, aggregations), a simpler document-based RAG with vector search would suffice and reduce complexity.

### Mechanism 2
- Claim: The LangChain SQL Agent with ZERO_SHOT_REACT_DESCRIPTION enables iterative error correction without pre-labeled training examples.
- Mechanism: The agent uses a Thought→Action→Action Input→Observation loop. If a generated SQL query fails, the agent receives the error as an observation and can rewrite the query. The system prompt explicitly instructs: "You must double check your query before executing it. If you get an error while executing a query, rewrite the query and try again."
- Core assumption: The LLM has sufficient SQL knowledge and reasoning capability to debug its own queries without few-shot examples.
- Evidence anchors:
  - [section 3.2.3]: "se seleccionó un agente del tipo ZERO_SHOT_REACT_DESCRIPTION, el cual combina tres pilares clave: flexibilidad... un mecanismo iterativo de razonamiento y acción (ReAct)"
  - [section 3.3.2]: Full prompt example shows the error-handling instructions
  - [corpus]: No direct corpus validation of this specific agent type for SQL tasks
- Break condition: If your schema is complex (many joins, nested queries) or your LLM is smaller/less capable, you may need few-shot prompting with example query pairs.

### Mechanism 3
- Claim: BLEU scores (avg 0.87) with custom scoring adaptations provide a practical but imperfect quality signal for conversational response evaluation.
- Mechanism: The authors adapted BLEU to prioritize numerical and keyword matches over exact phrasing. If responses share at least one number with the expected answer, scores approach 1.0. Keyword matches yield 0.6-1.0. Only responses with no significant matches use traditional BLEU (max 0.4).
- Core assumption: In this domain, correct facts (numbers, entity names) matter more than linguistic precision.
- Evidence anchors:
  - [section 3.6]: "si una respuesta generada contiene al menos un número en común con la esperada, se le asigna un puntaje elevado (hasta 1)"
  - [section 3.6]: "BLEU > 0.6 suele considerarse aceptable en asistentes conversacionales [32]"
  - [corpus]: No corpus validation of this adapted BLEU approach; standard practice uses BLEU for machine translation, not RAG evaluation
- Break condition: If your stakeholders care about response style, tone, or exact wording (not just factual accuracy), this adapted BLEU will not capture those dimensions. Consider additional metrics or human evaluation.

## Foundational Learning

- Concept: **Retrieval-Augmented Generation (RAG)**
  - Why needed here: The core architecture pattern. RAG combines an LLM's generation capabilities with external knowledge retrieval to reduce hallucinations and provide current information.
  - Quick check question: Can you explain why RAG is preferred over fine-tuning when the knowledge base changes frequently?

- Concept: **Text-to-SQL with LLMs**
  - Why needed here: The system's unique capability is translating natural language questions into executable SQL. Understanding prompt engineering for SQL generation is critical.
  - Quick check question: Given the question "Which advisor supervised the most theses in 2023?", what SQL query would you expect the system to generate?

- Concept: **ReAct (Reasoning + Acting) Agent Pattern**
  - Why needed here: The LangChain agent uses iterative reasoning loops to plan, execute tools, observe results, and correct errors. This is fundamental to debugging failed queries.
  - Quick check question: In a ReAct loop, what happens after an "Observation" that indicates an SQL syntax error?

## Architecture Onboarding

- Component map: User Interface (Gradio/Streamlit) -> LLM (Llama 3.1/Gemini 2.5) -> SQL Agent (LangChain ZERO_SHOT_REACT_DESCRIPTION) -> SQLite Database -> Response Generation -> User Interface

- Critical path:
  1. User submits natural language question via Gradio/Streamlit interface
  2. Agent constructs prompt with schema info + user question
  3. LLM generates SQL query (or action plan)
  4. Agent executes SQL against SQLite
  5. Results returned to LLM with response-generation prompt
  6. Final natural language response displayed to user

- Design tradeoffs:
  - **SQLite vs PostgreSQL**: SQLite chosen for simplicity, zero-config, and free cloud deployment. PostgreSQL offers better concurrency and complex queries but requires server infrastructure.
  - **Llama 3.1 vs Gemini 2.5**: Llama runs locally (free, private) but requires GPU resources. Gemini uses API (no local compute) but requires internet and API key management.
  - **Temperature = 0.5**: Balances creativity and precision. Lower values would be more deterministic but potentially less natural.

- Failure signatures:
  - **SQL syntax errors**: Agent should auto-correct via ReAct loop; if persistent, check schema descriptions in prompt
  - **Empty results**: Query may be correct but no matching data; verify data integrity in SQLite
  - **Hallucinated numbers**: LLM generating without database retrieval; check that agent is actually calling SQL tools
  - **Session timeout (Colab)**: Long-running sessions disconnect; Hugging Face deployment more stable

- First 3 experiments:
  1. **Reproduce the system locally**: Clone the GitHub repo, run the Colab notebook, verify you can query "¿Cuántas tesis se realizaron en 2022?" and get a valid response.
  2. **Test query classification boundaries**: Submit 5 quantitative queries (counts, averages) and 5 qualitative queries (content summaries). Compare BLEU scores to identify which type the system handles better.
  3. **Stress-test error recovery**: Intentionally submit ambiguous questions (e.g., "thesis about rocks") and observe how many ReAct iterations the agent requires. Log the SQL queries generated at each step.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How effectively does the proposed RAG-SQL architecture generalize to other academic disciplines with larger datasets?
- Basis in paper: [explicit] The abstract and conclusion state the system "establishes a basis for future applications in other disciplines."
- Why unresolved: The current implementation was tested only on 244 geology theses from a single institution.
- Evidence: Implementing the same pipeline on a distinct dataset (e.g., medical or legal theses) with thousands of documents.

### Open Question 2
- Question: Does the modified BLEU metric accurately reflect semantic correctness compared to human expert evaluation?
- Basis in paper: [inferred] Section 3.6 describes using a modified BLEU metric with flexible criteria (e.g., awarding points for partial number matches) rather than standard semantic evaluation.
- Why unresolved: Lexical overlap metrics like BLEU often fail to capture semantic nuance or factual hallucinations in generative AI.
- Evidence: A comparative study grading system responses against human expert ratings using metrics like faithfulness or answer correctness.

### Open Question 3
- Question: To what extent does relying solely on thesis summaries, rather than full text, limit the system's ability to answer detailed methodological queries?
- Basis in paper: [inferred] Section 3.1 notes that full texts were excluded as "inefficient," relying instead on the "resumen" field to capture the essence.
- Why unresolved: While summaries capture high-level objectives, they often lack the granular data required for specific technical questions.
- Evidence: User testing with queries specifically targeting methodological details found only in the full text, compared against summary-only retrieval.

## Limitations

- The evaluation relies on an adapted BLEU metric that prioritizes numerical and keyword matches over linguistic quality, which may not fully capture conversational coherence or user satisfaction in real-world usage.
- The system's performance with Gemini 2.5 Flash Lite API is not directly compared to the local Llama 3.1 deployment in terms of accuracy or response quality.
- The SQLite database, while sufficient for 244 records, may not scale well for larger thesis collections or more complex multi-table queries common in broader academic databases.

## Confidence

- **High Confidence**: The core RAG-SQL architecture combining structured database queries with LLM reasoning is technically sound and addresses the stated limitations of pure text-based RAG systems.
- **Medium Confidence**: The claimed BLEU score of 0.87 indicates strong performance, but the adapted evaluation criteria make direct comparison with standard benchmarks difficult.
- **Low Confidence**: Claims about the system's generalizability to other disciplines lack empirical validation beyond the geology thesis domain.

## Next Checks

1. Conduct user studies with actual students and faculty to assess real-world usability and satisfaction, measuring response accuracy, helpfulness, and natural language quality beyond BLEU scores.
2. Test the system's performance with a larger, more diverse academic database (1000+ records across multiple disciplines) to evaluate scalability and query complexity handling.
3. Implement and compare alternative evaluation metrics such as ROUGE, human evaluation rubrics, or task completion rates to validate the adapted BLEU approach and ensure comprehensive quality assessment.