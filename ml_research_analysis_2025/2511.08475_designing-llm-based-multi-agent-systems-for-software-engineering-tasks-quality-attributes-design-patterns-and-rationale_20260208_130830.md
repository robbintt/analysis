---
ver: rpa2
title: 'Designing LLM-based Multi-Agent Systems for Software Engineering Tasks: Quality
  Attributes, Design Patterns and Rationale'
arxiv_id: '2511.08475'
source_url: https://arxiv.org/abs/2511.08475
tags:
- llm-based
- mass
- arxiv
- tasks
- design
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper empirically investigates the design of LLM-based multi-agent
  systems (MASs) for software engineering (SE) tasks. The study analyzes 94 papers
  to identify the SE tasks addressed, quality attributes (QAs) prioritized, design
  patterns used, and underlying design rationales.
---

# Designing LLM-based Multi-Agent Systems for Software Engineering Tasks: Quality Attributes, Design Patterns and Rationale

## Quick Facts
- arXiv ID: 2511.08475
- Source URL: https://arxiv.org/abs/2511.08475
- Reference count: 40
- Primary result: Systematic analysis of 94 papers identifies Code Generation as the dominant SE task (47.9%) and Role-Based Cooperation as the most used design pattern (46.8%).

## Executive Summary
This paper empirically investigates the design of LLM-based multi-agent systems for software engineering tasks through a systematic literature review of 94 papers. The study identifies four key taxonomies: software engineering tasks addressed, quality attributes prioritized, design patterns employed, and underlying design rationales. Code Generation emerges as the most common task, while Functional Suitability dominates quality attribute considerations. The research reveals that Role-Based Cooperation is the predominant design pattern, and improving code quality is the primary rationale. The paper also maps relationships between these dimensions, providing actionable insights for practitioners designing LLM-based multi-agent systems for SE applications.

## Method Summary
The study conducted a systematic literature review collecting 94 papers from surveys by Liu et al., Wang et al., and arXiv cs.SE queries. Papers were filtered using criteria requiring multi-agent systems, LLM-based agents, and SE tasks. Manual extraction captured four data items: SE Task, Quality Attribute, Design Pattern, and Design Rationale. Quality Attributes were mapped to ISO/IEC 25010:2023 standards, while Design Patterns and Rationales were identified through Open Coding and Constant Comparison methods. The analysis quantified frequencies across taxonomies and mapped relationships between them.

## Key Results
- Code Generation is the most common SE task addressed (47.9% of papers)
- Functional Suitability is the most frequently considered quality attribute (94.7% of papers)
- Role-Based Cooperation is the dominant design pattern used (46.8% of papers)
- Improving the Quality of Generated Code is the most common design rationale (44.7% of papers)
- The study maps relationships showing Role-Based Cooperation enhances Modularity, while Self-Reflection improves Functional Correctness

## Why This Works (Mechanism)

### Mechanism 1: Role Specialization Enhances Modularity and Correctness
Assigning distinct roles (e.g., Coder, Tester) to agents correlates with higher system modularity and artifact quality by localizing context and responsibilities. The system decomposes complex SE tasks into sub-tasks handled by specialized agents, limiting prompt context required for any single agent and reducing hallucination risks.

### Mechanism 2: Iterative Self-Reflection Mitigates Error Propagation
Implementing feedback loops where agents critique their own outputs improves functional correctness and reliability. An agent generates an artifact which is then passed to a reflection mechanism to identify bugs or requirement deviations, feeding this feedback back into the generation step to refine output.

### Mechanism 3: Resource Optimization via Task Decomposition
Decomposing tasks across multiple agents allows better management of computational resources and time behavior compared to monolithic prompting. Instead of one large context window processing an entire repository, agents process smaller chunks, reducing token costs through strategies like Document-Guided Search.

## Foundational Learning

- **ISO/IEC 25010 Quality Attributes**: The paper uses this standard to classify system goals (e.g., Functional Suitability, Maintainability). Understanding these definitions is required to map design patterns to desired outcomes. Quick check: Can you distinguish between Functional Correctness (does it work?) and Reliability (does it work when things break?) in the context of an agent?

- **Role-Based vs. Debate-Based Cooperation**: These are the dominant interaction patterns. You must understand the difference between a hierarchy (Roles) and a peer-review/consensus model (Debate/Voting). Quick check: Would a "Debate-Based" pattern be more suitable for requirement elicitation or code generation?

- **The SE Lifecycle Context**: The paper maps MAS designs to specific SE phases (e.g., Requirements, Maintenance). Architecture decisions depend heavily on which phase you are automating. Quick check: If designing for End-to-End Software Maintenance, which quality attribute becomes critical to manage?

## Architecture Onboarding

- **Component map**: Orchestrator -> Specialized Agents -> Knowledge Layer -> Feedback Loop
- **Critical path**: 1) Task Decomposition: Orchestrator breaks user intent into sub-tasks. 2) Context Retrieval: Agents query RAG/Tools for necessary codebase context. 3) Generation & Review: Agents generate artifacts, then invoke Self-Reflection or pass to Reviewer agent. 4) Integration: Valid artifacts are merged; failures trigger retries or escalation.
- **Design tradeoffs**: Correctness vs. Latency (adding Self-Reflection improves quality but increases Time Behavior costs); Modularity vs. Context (high modularity simplifies maintenance but requires complex context handoffs between agents).
- **Failure signatures**: Infinite Loops (reflection cycles without resolution), Context Drift (information loss during handoffs), Tool Misalignment (agents failing to utilize Tool-Agent Registry correctly).
- **First 3 experiments**: 1) Baseline vs. Roles: Compare single-agent vs. Role-Based (Coder/Tester) setup to measure delta in Functional Correctness. 2) Reflection Ablation: Run same task with and without Self-Reflection step to quantify cost vs. benefit. 3) RAG Scaling: Test Performance Efficiency by varying context retrieved to validate "Optimizing Software Resource Management" rationale.

## Open Questions the Paper Calls Out

### Open Question 1
How can comprehensive benchmarks be developed to evaluate LLM-based MASs across the entire software development lifecycle, rather than isolated tasks like code generation? The paper notes a "lack of comprehensive evaluation criteria" and "standard benchmarks are unavailable" for evaluating systems across the entire lifecycle.

### Open Question 2
What are the optimal mechanisms for balancing the trade-off between Functional Correctness and Performance Efficiency in LLM-based MAS designs? The study identifies that designers prioritize both, but does not measure the cost of achieving high correctness relative to computational resource costs.

### Open Question 3
To what extent do human-centered design rationales (e.g., simulating human processes) effectively mitigate communication misalignment and ambiguity in LLM-based MASs? While designers frequently use "Simulating Human Processes" as a rationale, challenges remain including "ambiguities in communication protocols."

## Limitations
- Temporal validity concerns due to rapid evolution of LLM capabilities and design patterns since 2022-2024 analysis period
- Geographic/cultural bias from conference/arXiv corpus potentially underrepresenting industrial practice
- Aggregation granularity issues with fuzzy boundaries between overlapping design patterns

## Confidence

- **High Confidence**: Mapping of Role-Based Cooperation to Modularity improvements and Self-Reflection to Code Quality
- **Medium Confidence**: Correlation between Iterative Self-Reflection and Functional Correctness assumes sufficient domain knowledge in evaluating agent
- **Low Confidence**: Resource Optimization mechanism's assumption that multi-agent orchestration is cheaper than monolithic prompting lacks quantitative validation

## Next Checks
1. Re-run literature review with 2025 papers to assess temporal stability of identified patterns
2. Survey industry practitioners to validate design rationales align with real-world implementation priorities
3. Implement controlled experiments comparing single-agent vs. multi-agent approaches to empirically validate Resource Optimization assumptions about computational overhead