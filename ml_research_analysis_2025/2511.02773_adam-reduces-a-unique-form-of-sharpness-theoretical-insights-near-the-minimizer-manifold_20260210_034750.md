---
ver: rpa2
title: 'Adam Reduces a Unique Form of Sharpness: Theoretical Insights Near the Minimizer
  Manifold'
arxiv_id: '2511.02773'
source_url: https://arxiv.org/abs/2511.02773
tags:
- adam
- lemma
- implicit
- proof
- diag
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper analyzes how Adam reduces sharpness differently from
  SGD. It shows that Adam implicitly minimizes a unique form of sharpness measure
  shaped by its adaptive updates, leading to qualitatively different solutions.
---

# Adam Reduces a Unique Form of Sharpness: Theoretical Insights Near the Minimizer Manifold

## Quick Facts
- arXiv ID: 2511.02773
- Source URL: https://arxiv.org/abs/2511.02773
- Reference count: 40
- Adam minimizes tr(Diag(H)^1/2) instead of tr(H) under label noise, leading to different implicit bias than SGD

## Executive Summary
This paper analyzes how Adam reduces sharpness differently from SGD by implicitly minimizing a unique form of sharpness measure shaped by its adaptive updates. Using a continuous-time approximation with stochastic differential equations, the authors rigorously characterize Adam's behavior near the minimizer manifold. The analysis reveals that while SGD minimizes the trace of the Hessian matrix, Adam minimizes the trace of the square root of the diagonal Hessian instead. This distinction enables Adam to achieve better sparsity and generalization than SGD in solving sparse linear regression with diagonal linear networks. The framework extends beyond Adam to a broad class of adaptive gradient methods, providing a unified perspective on how these optimizers reduce sharpness.

## Method Summary
The paper develops a continuous-time approximation using stochastic differential equations to analyze adaptive gradient methods (AGMs) near the minimizer manifold. The framework requires a "2-scheme" where 1-β₂ = Θ(η²) to track the preconditioner evolution over O(η⁻²) steps. Under label noise conditions (Σ(θ) = α∇²L(θ)), the slow SDE reduces to an ODE that explicitly characterizes the implicit regularizer. The analysis is validated through synthetic experiments comparing Adam and SGD on sparse linear regression with diagonal networks and deep matrix factorization tasks.

## Key Results
- Adam minimizes tr(Diag(H)^1/2) while SGD minimizes tr(H) under label noise conditions
- In diagonal linear networks, Adam's implicit bias translates to minimizing the ℓ^0.5-norm, enabling better sparsity recovery
- The framework generalizes to a broad class of adaptive gradient methods beyond Adam
- Deep matrix factorization experiments show Adam can underperform when diagonal Hessian structure doesn't aid the task

## Why This Works (Mechanism)

### Mechanism 1: Adaptive Semi-Gradient Descent on Manifold
The analysis shows that near the minimizer manifold, Adam behaves like an adaptive gradient method that minimizes a sharpness measure by taking semi-gradients. The slow SDE captures dynamics over O(η⁻²) steps with a drift term acting as a semi-gradient descent that treats the adaptive preconditioner as fixed. The 2-scheme requirement ensures the preconditioner evolves slowly enough to be tracked analytically without becoming negligible.

### Mechanism 2: Label Noise Induces Distinct Sharpness Regularizer
Label noise creates a proportional relationship between gradient noise covariance and the Hessian, simplifying the slow SDE drift term. This leads to a stationary condition where the gradient of tr(Diag(H)^1/2) vanishes on the manifold, differing fundamentally from SGD's minimization of the full Hessian trace. This requires overparameterized models that can fit training data perfectly while fresh noise is added to labels at each step.

### Mechanism 3: Sparsity-Facilitating Bias in Structured Settings
In sparse linear regression with diagonal linear networks, the diagonal network parameterization ensures the Hessian is diagonal in expectation. This structure makes minimizing tr(Diag(H)^1/2) equivalent to minimizing the ℓ^0.5-norm of the estimated ground truth. Since ℓ^0.5 regularization promotes sparsity more aggressively than the ℓ^1-norm, Adam recovers sparse ground truth with fewer training samples than SGD.

## Foundational Learning

- **Stochastic Differential Equations (SDEs) for Optimizer Analysis:**
  - Why needed: The paper uses slow SDEs to track optimizer behavior over O(η⁻²) steps, capturing the implicit regularization phase after convergence to the minimizer manifold
  - Quick check: Can you explain why a "slow SDE" (which tracks only the manifold-projected dynamics) can capture longer time horizons than a conventional SDE approximation?

- **Manifold Geometry and Projection Operators:**
  - Why needed: The analysis assumes minimizers form a smooth manifold Γ, requiring concepts like tangent space projection, preconditioned gradient flow projection, and noise decomposition into tangent and normal components
  - Quick check: Why does the adaptive projection P_{ζ,S(t)} differ from SGD's fixed projection P_ζ, and what implications does this have for the implicit bias?

- **Sharpness Measures and the Polyak-Łojasiewicz (PL) Condition:**
  - Why needed: The paper connects different sharpness measures (tr(H) vs. tr(Diag(H)^1/2)) to generalization under label noise, using the PL condition to establish convergence guarantees for AGMs near the manifold
  - Quick check: Under the label noise condition, why does Adam minimize a function of the diagonal Hessian entries while SGD minimizes the full Hessian trace?

## Architecture Onboarding

- **Component map:**
  AGM framework generalizes Adam-family optimizers via: Momentum update -> Second-moment accumulator -> Preconditioner function -> Parameter update
  Key optimizers: Adam, RMSProp, Adam-mini, Adalayer, Shampoo (via Kronecker product formulation)

- **Critical path:**
  1. Convergence phase: AGM iterates reach O(η log 1/η) neighborhood of minimizer manifold (Theorem B.2, high probability bound)
  2. Slow SDE phase: Dynamics over O(η⁻²) steps captured by slow SDE, separating into diffusion and drift
  3. Label noise specialization: Under Σ = α∇²L, SDE reduces to ODE with explicit sharpness regularizer (Theorem 5.1)

- **Design tradeoffs:**
  - β₂ scaling (2-scheme): Choosing 1-β₂ = Θ(η²) is critical for analysis but may not match practical settings
  - Diagonal vs. full Hessian treatment: Adam's implicit bias leverages diagonal structure, beneficial for sparsity but potentially harmful when off-diagonal terms carry information
  - Momentum (β₁): Analysis shows momentum doesn't affect implicit bias as differences become negligible near manifold

- **Failure signatures:**
  - Non-diagonal Hessian structure: Adam's sparsity benefit fails; may perform worse than SGD (deep matrix factorization experiments)
  - Insufficient overparameterization: Label noise condition may not hold, breaking link to explicit sharpness regularizers
  - Weight decay: Not covered in analysis; may alter effective implicit regularizer

- **First 3 experiments:**
  1. Replicate diagonal network sparse regression: Train Adam and SGD on diagonal linear network setup with d=10000, κ=50. Vary training set size and observe sharp test loss transition for Adam vs. gradual decrease for SGD. Measure tr(Diag(H)^1/2) and tr(H) during training.
  2. Deep matrix factorization contrast: Implement L-layer matrix factorization setup with label noise. Compare Adam and SGD on tr(H), tr(Diag(H)^1/2), and test MSE to confirm Adam's worse performance when diagonal structure doesn't aid task.
  3. AdamE-λ sweep: Test AdamE variant with λ ∈ {0.01, 0.1, 0.25, 0.75, 0.9} on diagonal network task. Verify smaller λ approaches SGD behavior while λ=0.5 matches standard Adam.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the slow SDE approximation and resulting implicit regularizer be extended to regimes where 1-β₂ scales differently than η² (e.g., the "1.5-scheme")?
- Basis in paper: [explicit] The conclusion explicitly states that analysis focused on the "2-scheme" regime (1-β₂ = O(η²)) to track preconditioner, leaving intermediate scalings for future work
- Why unresolved: Current derivation relies on specific rate of preconditioner drift matching manifold dynamics, which changes if β₂ scaling changes
- What evidence would resolve it: Mathematical derivation of slow SDE for AGMs under different β₂ scaling, showing if sharpness measure changes

### Open Question 2
- Question: How does inclusion of weight decay or decoupled decay terms (such as in AdamW) alter effective sharpness regularizer and implicit bias?
- Basis in paper: [explicit] Conclusion notes current approach cannot cover weight-decay or decoupled decay terms, identifying characterizing this interaction as important direction
- Why unresolved: Current framework optimizes solely for training loss sharpness, whereas weight decay introduces explicit penalty that modifies manifold and dynamics
- What evidence would resolve it: Extension of SDE analysis including weight decay term in drift, identifying new modified implicit regularizer

### Open Question 3
- Question: What is the implicit bias of Adam when trajectory ventures beyond local neighborhood of minimizer manifold?
- Basis in paper: [explicit] Conclusion mentions derivations assume iterates remain close to smooth manifold, and understanding behavior once this local condition is violated remains open
- Why unresolved: "Slow SDE" framework specifically isolates dynamics near manifold; analyzing global dynamics requires different mathematical tools or approximations
- What evidence would resolve it: Convergence analysis or SDE approximation that remains valid even when distance to manifold is large, or empirical study characterizing solutions found from random initialization

## Limitations

- Theoretical framework relies heavily on label noise condition requiring overparameterization and perfect training fit
- Analysis does not extend to weight decay, batch normalization, or other practical training elements
- Diagonal Hessian assumption for sparsity benefits is a strong structural constraint that fails in many real-world architectures
- 2-scheme requirement (1-β₂ = Θ(η²)) may not match practical hyperparameter choices
- Slow SDE approximation breaks down when iterates drift significantly from manifold or when preconditioner evolution becomes untrackable

## Confidence

**High confidence:** The mathematical framework for AGMs and slow SDE characterization (Section 4) - rigorous stochastic calculus with clear conditions. Experimental results showing Adam's superior sparsity recovery in diagonal networks (Figure 2) - clean separation with controlled synthetic data. Counterexample in deep matrix factorization where Adam underperforms (Figure 3-4) - demonstrates limitation of diagonal Hessian bias.

**Medium confidence:** Implicit bias characterization under label noise (Theorem 5.1) - depends on strong assumptions about noise structure and overparameterization. ℓ^0.5-norm equivalence claim (Lemma 5.3) - requires diagonal Hessian which may not hold in practice. Broader applicability to other AGMs beyond Adam - theoretically sound but untested empirically.

**Low confidence:** Extension to non-2-scheme hyperparameter settings - only briefly mentioned as future work. Generalizability to non-linear networks - diagonal network is highly constrained setting. Real-world performance implications - synthetic experiments don't capture practical optimization scenarios.

## Next Checks

1. **Test the 2-scheme sensitivity:** Sweep β₂ values while adjusting learning rate to maintain the 2-scheme relationship (1-β₂ = Θ(η²)). Measure the implicit bias (tr(H) vs tr(Diag(H)^1/2)) and recovery performance to confirm theoretical requirements.

2. **Validate diagonal Hessian assumption:** Compute the off-diagonal elements of the Hessian during training in the diagonal network experiment. Quantify the approximation error in Lemma 5.3 and determine how much non-diagonality is tolerated before the sparsity benefit degrades.

3. **Test non-2-scheme Adam variants:** Implement Adam with standard practical hyperparameters (β₂ = 0.999, η = 10⁻³) and compare against theoretical predictions. Measure whether the implicit bias still manifests and whether the slow SDE approximation remains valid.