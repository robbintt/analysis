---
ver: rpa2
title: 'CaTs and DAGs: Integrating Directed Acyclic Graphs with Transformers for Causally
  Constrained Predictions'
arxiv_id: '2410.14485'
source_url: https://arxiv.org/abs/2410.14485
tags:
- causal
- cfcn
- inference
- learning
- which
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Causal Transformers (CaTs) address the challenge of integrating
  causal structure into neural network predictions by constraining transformer attention
  according to a Directed Acyclic Graph (DAG). The core method applies causally-masked
  cross-attention where queries derive from a learnable embedding while keys/values
  come from input embeddings, ensuring each node only attends to its parents.
---

# CaTs and DAGs: Integrating Directed Acyclic Graphs with Transformers for Causally Constrained Predictions

## Quick Facts
- **arXiv ID**: 2410.14485
- **Source URL**: https://arxiv.org/abs/2410.14485
- **Reference count**: 40
- **Primary result**: CaTs achieve eATE of 0.058±0.056 on synthetic causal effect estimation while maintaining stability under covariate shift

## Executive Summary
CaTs (Causal Transformers) integrate DAG structural constraints into transformer architectures to enable robust causal effect estimation and predictions under distributional shift. The key innovation applies causally-masked cross-attention where queries derive from a learnable embedding while keys/values come from input embeddings, ensuring each node only attends to its parents in the DAG. This enforces the local Markov property and DAG factorization, enabling the model to distinguish causal relationships from spurious correlations. Experiments show CaTs outperform standard transformers on causal effect estimation tasks while maintaining stability when input distributions change, addressing a critical limitation of conventional neural networks in causal inference settings.

## Method Summary
CaTs replace standard self-attention with causally-masked cross-attention, where a learnable embedding $\gamma$ acts as the query and embedded input $X_E$ acts as keys/values. The attention matrix is element-wise multiplied by the transposed, topologically sorted adjacency matrix $A^\top$, ensuring each node only attends to its parents in the DAG. This architectural constraint enforces the local Markov property and enables recursive inference for intervention estimation. The model uses independent embedding layers for each variable, a feed-forward network with Swish activation, and no layer normalization. Training employs MSE loss for continuous variables and BCE for binary variables, with recursive inference propagating interventions downstream through the DAG.

## Key Results
- Achieves eATE of 0.058±0.056 on synthetic motivating example with true DAG
- Maintains stable predictions under covariate shift while standard transformers fail
- Outperforms standard transformers on causal effect estimation (eATE of 2.379 vs 0.058)
- Shows competitive performance on benchmark datasets (Twins, Jobs) despite being a causal method

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: CaTs enforce structural inductive biases by masking cross-attention to restrict information flow strictly according to a Directed Acyclic Graph (DAG).
- **Mechanism**: The architecture replaces standard self-attention with causally-masked cross-attention. A learnable embedding $\gamma$ acts as the query, while embedded input $X_E$ acts as keys/values. The attention matrix is element-wise multiplied (Hadamard product) by the transposed, topologically sorted adjacency matrix $A^\top$. This ensures a node only attends to its parents in the DAG.
- **Core assumption**: The user provides a correct (or approximate) topologically sorted DAG representing the data generating process (DGP).
- **Evidence anchors**:
  - [abstract]: "CaTs address the challenge... by constraining transformer attention according to a Directed Acyclic Graph (DAG)."
  - [section 3.2]: "We then mask the attention... via a Hadamard product... between the transposed, topologically sorted, adjacency matrix A and the attention matrix."
  - [corpus]: *Directed Acyclic Graph Convolutional Networks* discusses DAG structures in GNNs, supporting the general relevance of graph-based structural constraints.
- **Break condition**: If the provided DAG is incorrect (false edges or missing edges), the model enforces the *wrong* constraints, potentially leading to biased effect estimation (e.g., eATE of 2.314 in Table 1 for "False DAG 1").

### Mechanism 2
- **Claim**: The architectural design guarantees the output satisfies the local Markov property relative to the specified DAG.
- **Mechanism**: By strictly preventing attention from a node to itself (zero diagonal in $A$) and non-parents, the prediction for node $i$ depends only on its parents $X_{pa(i)}$. This creates a conditional factorization $p(x_i | x_{pa(i)})$ rather than a full joint dependency, ensuring robustness to covariate shift in non-ancestor variables.
- **Core assumption**: The independence of embedding layers for each node ensures inputs do not contaminate each other before attention.
- **Evidence anchors**:
  - [abstract]: "...ensuring each node only attends to its parents. This enforces the local Markov property and DAG factorization..."
  - [section S13]: "Token i never accesses $X_i$ during forward passes. This architectural constraint enforces the local Markov property..."
  - [corpus]: *Cyclic Counterfactuals...* highlights the reliance on acyclicity, confirming the boundary conditions for this property.
- **Break condition**: If layer normalization is used (which the authors explicitly omit), it would rescale potentially calibrated interventions, breaking the causal semantics.

### Mechanism 3
- **Claim**: CaTs estimate interventional distributions (e.g., $P(Y|do(D))$) by implementing the g-formula through recursive inference.
- **Mechanism**: To simulate an intervention $do(D=d)$, the model sets the embedding of $D$ to $d$ and recursively updates descendants in topological order. Because children only look at parents, the intervention "flows" downstream automatically. The learnable query $\gamma$ is updated at each layer to "extract" information from the (now modified) inputs.
- **Core assumption**: The structural mechanisms $p(x_i | x_{pa(i)})$ are invariant under intervention (modularity assumption).
- **Evidence anchors**:
  - [abstract]: "...proper intervention estimation... enabling robust predictions under covariate shift..."
  - [section 3.5]: "CaT supports all conditional average (CATE) and average (ATE) causal effect queries identifiable from the DAG... set intervened variables to their intervention values, then recursively update descendants..."
  - [corpus]: Evidence regarding intervention estimation specific to Transformers is weak in the provided corpus.
- **Break condition**: If the DAG contains long chains of mediators, recursive inference may compound errors (propagation error) as predicted mediators are fed back in.

## Foundational Learning

- **Concept: Cross-Attention in Transformers**
  - **Why needed here**: You must distinguish between self-attention (input attends to input) and cross-attention (target attends to source) to understand how CaT uses a learnable query $\gamma$ to "pull" information from the masked input embeddings.
  - **Quick check question**: In a standard transformer decoder, what serves as the Query vs. the Key/Value in cross-attention, and how does CaT modify the Query source?

- **Concept: Topological Sorting and Adjacency Matrices**
  - **Why needed here**: The masking mechanism relies entirely on the adjacency matrix $A$ reflecting a valid causal ordering. You need to understand how to convert a DAG into a matrix where $A_{ij}=1$ implies $i \to j$.
  - **Quick check question**: If you reverse the order of columns in your adjacency matrix but keep the node order the same, what happens to the causal constraints?

- **Concept: The "do" Operator (Intervention)**
  - **Why needed here**: The goal is to estimate $P(Y|do(D))$, which differs from conditional probability $P(Y|D)$ because it cuts incoming edges to $D$. You need to understand why setting a value ("clamping") is different from observing it.
  - **Quick check question**: If we observe $D=1$, we learn about its parents; if we intervene to set $do(D=1)$, do we learn anything about its parents?

## Architecture Onboarding

- **Component map**: Input $X$ -> Independent Embedding Layers -> $X^E$ -> CBlock (CMCA + FFN) -> Output Projection

- **Critical path**:
  1.  **Graph Definition**: Define DAG and generate the binary adjacency matrix $A$. Ensure $d_E > 1$ to allow disentanglement.
  2.  **Embedding**: Pass raw inputs through independent embedding layers to get $X_E$.
  3.  **Masking**: Apply $A^\top$ during the attention softmax calculation to block non-parent interactions.
  4.  **Recursion**: For interventions, clamp input values and loop through descendants to propagate effects.

- **Design tradeoffs**:
  - **MSE vs. Robustness**: CaT with a "True DAG" may yield higher prediction error (MSE) than a standard Transformer because it refuses to use spurious correlations (Table 1: CaT True DAG MSE 1.004 vs. Transformer 0.615).
  - **Input Feeding**: $X_E$ is fed at *every* layer. This allows the model to compare the current state of $\gamma$ against the original observations, balancing internal estimates with reality.

- **Failure signatures**:
  - **High Variance in Long Chains**: If your graph has many mediating steps (e.g., $A \to B \to C \to D \to Y$), prediction errors compound during recursive inference (Section 5). *Mitigation: Use transitive reduction to remove unnecessary mediators.*
  - **Undisentangled Representations**: If $d_E = 1$, the network struggles to disentangle contributions from separate variables (Section 3.2).

- **First 3 experiments**:
  1.  **Validation on Synthetic Data**: Generate linear data from a known DAG (e.g., Section 4, Fig 3a). Verify that CaT yields a lower eATE (e.g., 0.058) than a standard Transformer (e.g., 2.379).
  2.  **Covariate Shift Test**: Train on confounded data (e.g., background correlated with object). Induce a shift in the test set. Confirm that the standard Transformer fails while CaT remains stable (Figure 1).
  3.  **Interventional Query Test**: Simulate an intervention $do(D=1)$ by clamping the input. Pass through the model once. Verify that the output for the outcome node $Y$ matches the analytical result of the structural equation model.

## Open Questions the Paper Calls Out

- **Question**: How does CaT perform on complex, high-dimensional computer vision tasks with pre-defined causal DAGs, such as autonomous driving or human-to-human interaction modeling?
  - **Basis in paper**: [explicit] "CaT has yet to be scaled to and tested on more complex computer vision tasks with pre-defined DAGs (such as autonomous driving or human-to-human interactions)."
  - **Why unresolved**: Current experiments only cover tabular data, benchmark datasets, and a psychology questionnaire study; no vision-scale evaluation conducted.
  - **What evidence would resolve it**: Empirical results on vision benchmarks with known causal structure, demonstrating computational feasibility and estimation accuracy at scale.

- **Question**: Can the compounding error problem in recursive inference for long mediation chains be mitigated without resorting to transitive reduction?
  - **Basis in paper**: [explicit] "For DAGs which include long mediation structures... the recursion can lead to compounding error as the prediction for each mediator is fed back into the model for the prediction of the next descendant."
  - **Why unresolved**: The proposed solution (transitive reduction) may discard nodes relevant to other inference tasks; no alternative error-accumulation remedies explored.
  - **What evidence would resolve it**: Comparative error analysis across varying mediation depths, or architectural modifications that bound compounding error.

- **Question**: Why do standard non-causal estimators (random forests, transformers) perform competitively with specialized causal inference methods on benchmark datasets?
  - **Basis in paper**: [explicit] "Further work is required to explore this result" regarding the "curious result" that RF and standard transformers match causal methods on Jobs/Twins.
  - **Why unresolved**: This finding undermines the utility of current causal inference benchmarks and suggests they may not properly stress-test causal assumptions.
  - **What evidence would resolve it**: Analysis of benchmark properties, or new benchmarks incorporating explicit DAG discovery/selection challenges.

- **Question**: How robust is CaT to DAG misspecification, and can misspecification be detected during inference?
  - **Basis in paper**: [inferred] Experiments show CaT with false DAGs yields biased eATE (2.314 vs. 0.058 with true DAG), but no systematic study of partial misspecification or recovery mechanisms.
  - **Why unresolved**: In practice, DAGs are rarely known perfectly; understanding graceful degradation bounds is critical for real-world deployment.
  - **What evidence would resolve it**: Sensitivity analysis across controlled misspecification types (missing edges, reversed edges, spurious edges).

## Limitations

- **DAG Specification Dependency**: CaT's performance critically depends on accurate DAG specification, with incorrect DAGs leading to biased causal effect estimates.
- **Recursive Inference Propagation Error**: The paper mentions compounding errors in long mediation chains but doesn't provide quantitative bounds on this propagation error.
- **Hyperparameter Sensitivity**: Several critical hyperparameters including weight initialization and learning rate schedules are underspecified, potentially affecting reproducibility.

## Confidence

- **High Confidence**: The core architectural mechanism of causally-masked cross-attention with Hadamard product masking is well-specified and theoretically grounded in the local Markov property and DAG factorization.
- **Medium Confidence**: The experimental results showing improved causal effect estimation (eATE of 0.058±0.056) are compelling but rely on synthetic data with known ground truth, limiting generalizability to real-world scenarios.
- **Low Confidence**: Claims about robustness under covariate shift and proper intervention estimation lack sufficient empirical validation across diverse real-world datasets beyond the COVID-19 psychology study.

## Next Checks

1. **DAG Uncertainty Quantification**: Extend CaT to work with ensembles of DAGs or incorporate uncertainty in the causal structure itself, measuring how performance degrades as DAG accuracy decreases.
2. **Propagation Error Analysis**: Systematically measure error accumulation in recursive inference across varying chain lengths and mediator complexities, establishing quantitative bounds on propagation error.
3. **Real-World Causal Discovery Integration**: Test CaT with automatically discovered DAGs from real observational data rather than expert-specified graphs, evaluating performance degradation and recovery strategies.