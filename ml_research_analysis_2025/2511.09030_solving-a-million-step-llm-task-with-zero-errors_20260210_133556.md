---
ver: rpa2
title: Solving a Million-Step LLM Task with Zero Errors
arxiv_id: '2511.09030'
source_url: https://arxiv.org/abs/2511.09030
tags:
- disk
- move
- steps
- task
- error
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MAKER, the first system to solve a task with
  over one million LLM steps with zero errors. The approach relies on extreme decomposition
  of tasks into minimal subtasks, each handled by focused microagents, combined with
  efficient multi-agent voting for error correction and red-flagging to detect unreliable
  outputs.
---

# Solving a Million-Step LLM Task with Zero Errors

## Quick Facts
- arXiv ID: 2511.09030
- Source URL: https://arxiv.org/abs/2511.09030
- Authors: Elliot Meyerson, Giuseppe Paolo, Roberto Dailey, Hormoz Shahrzad, Olivier Francon, Conor F. Hayes, Xin Qiu, Babak Hodjat, Risto Miikkulainen
- Reference count: 40
- Key outcome: First system to solve a task with over one million LLM steps with zero errors through extreme decomposition and multi-agent voting

## Executive Summary
This paper introduces MAKER, the first system to solve a task with over one million LLM steps with zero errors. The approach relies on extreme decomposition of tasks into minimal subtasks, each handled by focused microagents, combined with efficient multi-agent voting for error correction and red-flagging to detect unreliable outputs. Theoretical scaling laws show that error-corrected decomposition enables log-linear cost growth with task length, making large-scale execution feasible. Experiments demonstrate perfect execution on a 20-disk Towers of Hanoi problem, validating that relatively small non-reasoning models suffice for such massively decomposed agentic processes.

## Method Summary
MAKER uses Maximal Agentic Decomposition (MAD) to break tasks into minimal single-step subtasks, each handled by dedicated microagents. First-to-ahead-by-k voting selects the correct action by requiring one candidate to lead by k votes, with red-flagging to discard anomalous responses that correlate with errors. The system calibrates per-step error rates, computes optimal voting thresholds, and scales efficiently through parallel sampling. The approach was validated on a 20-disk Towers of Hanoi problem requiring 1,048,575 steps.

## Key Results
- First system to solve a million-step task with zero errors
- Theoretical analysis shows log-linear cost scaling with task length
- 20-disk Towers of Hanoi executed perfectly using gpt-4.1-mini
- Red-flagging reduced correlated errors by discarding anomalous responses

## Why This Works (Mechanism)

### Mechanism 1: Maximal Agentic Decomposition (MAD)
Decomposing tasks into minimal single-step subtasks enables each microagent to focus without context burden, making per-step success rates stable and predictable. Each agent receives only the context needed for one step (current state, prior move), avoiding the degradation that occurs when a single agent accumulates long context. This modularity then enables per-step error correction. The framework fails if subtasks cannot be made simple enough for p > 0.5, or if decomposition itself becomes computationally infeasible.

### Mechanism 2: First-to-ahead-by-k Voting
Requiring a candidate to lead by k votes before selection drives the probability of correct selection arbitrarily close to 1, even with non-trivial base error rates. Sampling continues until one candidate has k more votes than any other. For p > 0.5, the probability of correct selection is 1/(1 + ((1-p)/p)^k). This creates exponential error suppression with linear cost increase in k. The method fails if errors are highly correlated (same wrong answer sampled repeatedly), though red-flagging partially addresses this.

### Mechanism 3: Red-flagging for Error Decorrelation
Discarding responses with structural anomalies (overly long, malformed) reduces both per-step error rate and, critically, correlated errors at pathological steps. LLMs that become confused tend to both reason incorrectly AND produce anomalous outputs. By rejecting these before voting, the effective p increases and the variance in step difficulty decreases. The method fails if legitimate correct answers are frequently red-flagged, requiring careful calibration on representative samples.

## Foundational Learning

- **Concept: Gambler's Ruin / Hitting Probability**
  - Why needed here: The first-to-ahead-by-k analysis directly maps to a generalized gambler's ruin problem—understanding why a candidate with p > 0.5 eventually wins with probability approaching 1.
  - Quick check question: If p = 0.6 and k = 3, what is the approximate probability the correct candidate wins? (Answer: ~96% via 1/(1 + (0.4/0.6)^3))

- **Concept: Log-linear Scaling**
  - Why needed here: The paper's central scalability claim rests on cost growing as Θ(s ln s) rather than exponentially—understanding why k_min = Θ(ln s) is critical for system design.
  - Quick check question: Why does the required voting margin k grow only logarithmically with task length s? (Answer: Because compounding per-step reliability requirements need only logarithmic reinforcement to maintain fixed overall success probability.)

- **Concept: Error Correlation in Sampling**
  - Why needed here: The i.i.d. error assumption is explicitly imperfect; understanding when and why errors correlate informs when red-flagging (or more sophisticated decorrelation) is necessary.
  - Quick check question: In the 20-disk experiment, why did some steps require 18x more votes than average? (Answer: Pathological inputs trigger correlated errors; the paper hypothesizes these are LLM training artifacts that red-flagging partially mitigates.)

## Architecture Onboarding

- **Component map:**
  Task → [Decomposition Layer: s subtasks] → [Microagent Pool: M instances, each handling 1 step] → [Voting Engine: k-margin resolution via parallel sampling] → [Red-flag Filter: length/format validation] → [State Aggregator: ψ_x extracts context for next agent] → [Action Sequence Output]

- **Critical path:**
  1. Estimate per-step error rate p via small-sample calibration (Section 4.2)
  2. Compute k_min from target success probability t using Eq. 14
  3. Select model minimizing c/p (cost per reliable step)
  4. Run MAD with voting and red-flagging, monitoring convergence

- **Design tradeoffs:**
  - Higher k: More reliability, higher cost, more parallelism possible
  - Stricter red-flagging: Higher effective p, but more resampling overhead (v factor in Eq. 19)
  - Larger vs. smaller models: Larger models may have lower p but higher c; the paper found gpt-4.1-mini beat o3-mini on total cost despite higher error rate
  - Repairing vs. flagging parsers: Repairing extracts more answers but admits correlated errors; flagging is safer for long tasks

- **Failure signatures:**
  - Non-converging votes: One step never reaches k margin → indicates p ≤ 0.5 or extreme correlation; increase diversity (temperature, prompt paraphrasing) or simplify subtask
  - Exploding cost in later rounds: More than k votes needed consistently → p overestimated during calibration; recalibrate or tighten red-flagging
  - State drift: Correct action but wrong next_state parsed → parser too permissive or model confused; strengthen validation

- **First 3 experiments:**
  1. Calibration run: On 1000 random steps from a small instance (e.g., 10 disks), measure p and token usage for candidate models; compute k_min and projected cost for target reliability.
  2. Ablation on red-flagging: Run same task with repairing vs. flagging parser; compare collision counts and total samples needed to verify decorrelation benefit.
  3. Scale-up pilot: Run full system on 15-disk instance (~32K steps) with calibrated k; verify exponential convergence pattern matches Figure 8 before committing to million-step run.

## Open Questions the Paper Calls Out

### Open Question 1
Can the framework be extended to handle LLM-based insights (autonomous decomposition) rather than solely execution? The authors state that extending the framework to "handle LLM-based insights is an important area of future work," noting that insights are open-ended and entail irreducible uncertainty. This remains unresolved because the current system relies on provided strategies; it is unclear if the decomposition process itself can be automated without sacrificing reliability. Successful application to a task where the decomposition strategy is generated dynamically by the agents would resolve this.

### Open Question 2
What methods are most effective for decorrelating errors in tasks where per-step error rates are non-uniform? The authors note that dealing with "correlated errors is an open foundational problem" and suggest "more sophisticated decorrelation methods" like prompt paraphrasing may be required. This remains unresolved because the current analysis assumes i.i.d. errors, but real-world tasks may contain specific, difficult steps that persistently fail. Demonstrating that techniques like noise injection or diverse agent prompts reduce variance in step-specific error rates would resolve this.

### Open Question 3
Are there classes of problems where decomposition into minimal subtasks is impossible or computationally infeasible? The authors ask, "Are there important problems where such a decomposition is not possible or is computationally infeasible to discover?" This remains unresolved because it is unknown if all complex reasoning tasks can be compartmentalized into linguistic steps that fit within small context windows. Identification of a task class where per-step error rates remain high regardless of the granularity of decomposition would resolve this.

## Limitations
- Computational cost scaling, while theoretically log-linear, has substantial constant factors (20-disk experiment cost ~$700)
- System depends critically on task decomposability into reliable single-step subtasks, which may not hold for open-ended reasoning problems
- Red-flagging mechanism represents a heuristic rather than principled solution to error correlation, may fail when legitimate correct answers exceed length threshold

## Confidence

- **High confidence:** The theoretical framework (voting with k-margin, log-linear cost scaling) is mathematically sound and well-supported by empirical calibration
- **Medium confidence:** The red-flagging mechanism's effectiveness is empirically validated but relies on heuristic thresholds and may not generalize to all domains
- **Medium confidence:** The claim that small models suffice for massive decomposition is supported by the 20-disk experiment, but scaling to more complex tasks remains unproven

## Next Checks

1. **Cross-domain robustness test:** Apply MAKER to a non-deterministic task like code generation or mathematical proof construction, measuring whether the same error correction framework maintains reliability when task structure varies more than in Towers of Hanoi.

2. **Correlation structure analysis:** Systematically vary the red-flagging threshold and measure its impact on both false positives (legitimate answers discarded) and error decorrelation (collision reduction), quantifying the tradeoff curve.

3. **Model size efficiency study:** Compare cost-reliability tradeoffs across multiple model sizes (not just gpt-4.1-mini vs o3-mini) on identical decomposed tasks, verifying the paper's claim that smaller models optimized for per-step cost outperform larger models with lower error rates.