---
ver: rpa2
title: 'The Flaw of Averages: Quantifying Uniformity of Performance on Benchmarks'
arxiv_id: '2509.25671'
source_url: https://arxiv.org/abs/2509.25671
tags:
- benchmarks
- benchmark
- similarity
- zhang
- evaluation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces benchmark harmony, a measure of how uniformly
  a model's performance is distributed across subdomains within a benchmark. It is
  defined using entropy over subdomain performance distributions, capturing both the
  balance of subdomain representation and the evenness of model performance.
---

# The Flaw of Averages: Quantifying Uniformity of Performance on Benchmarks

## Quick Facts
- **arXiv ID**: 2509.25671
- **Source URL**: https://arxiv.org/abs/2509.25671
- **Reference count**: 40
- **Primary result**: Introduces benchmark harmony - normalized entropy over model performance across induced subdomains - to measure evaluation reliability and detect overrepresentation of specific domains

## Executive Summary
This paper addresses the fundamental problem that aggregate benchmark scores can be misleading when model performance is unevenly distributed across subdomains. The authors introduce "benchmark harmony" - a measure of uniformity computed as normalized Shannon entropy over subdomain performance distributions. Using model-aware predictive similarity (based on probability divergence) and spectral clustering, they induce semantic partitions of benchmarks and quantify how evenly models perform across these subdomains. The method reveals that many popular benchmarks have low harmony, meaning their aggregate scores can be skewed by overrepresented subdomains. The authors demonstrate that improving harmony through controlled pruning stabilizes accuracy and provides more reliable evaluation metrics.

## Method Summary
The method involves inducing subdomains via spectral clustering on a predictive similarity matrix constructed from model probability distributions, then computing harmony as normalized entropy over transformed performance masses. For each model, the authors: (1) compute pairwise predictive similarity using symmetric KL divergence of length-normalized predictive distributions, (2) apply spectral clustering to induce semantic partitions, (3) calculate per-cluster accuracy, transform via Gaussian kernel relative to mean accuracy, and (4) compute normalized entropy of resulting performance masses as the harmony score. The framework is validated across 19 MCQA benchmarks and five model families, mapping benchmarks onto a mean-variance plane where high mean and low variance indicate more reliable evaluation.

## Key Results
- Across 19 MCQA benchmarks and five model families, benchmarks map onto a mean-variance plane where high mean and low variance indicate reliable evaluation
- Less harmonious benchmarks can mislead by overrepresenting certain subdomains, distorting aggregate scores
- Controlled pruning to increase harmony stabilizes accuracy by reducing subdomain skew
- Model scaling trends in harmony are family-specific, while pre-training budget consistently improves harmony over time
- The paper recommends reporting harmony alongside accuracy for more robust, distributionally reliable evaluation

## Why This Works (Mechanism)

### Mechanism 1
Predictive similarity captures semantic relatedness better than lexical overlap by measuring how similarly a model distributes probability over outputs for different inputs. For two inputs x_i and x_j, compute symmetric KL divergence between the model's length-normalized predictive distributions, then apply RBF kernel: S(x_i, x_j) = exp(-τ/2 × [D_KL(p̄_f(x_i) || p̄_f(x_j)) + D_KL(p̄_f(x_j) || p̄_f(x_i))]). Large S indicates the model treats inputs as interchangeable from a predictive standpoint. Core assumption: Probability distribution divergence correlates with semantic similarity as perceived by the model.

### Mechanism 2
Harmony (normalized Shannon entropy over performance masses) quantifies uniform performance across induced subdomains, with high values indicating aggregate accuracy reliably reflects broad competence. Given partition G_f = {A_1, ..., A_k}, compute weighted mean μ = Σw_i × Ψ(f; A_i). Convert deviations via Gaussian kernel K_i = exp(-((Ψ(f; A_i) - μ)/b)²), forming performance masses p_i = (w_i × K_i) / Σ(w_j × K_j). Harmony H(G_f) = -1/log(k) × Σp_i × log(p_i + ε) ∈ [0,1]. Core assumption: Gaussian kernel bandwidth b appropriately captures meaningful deviation scales; entropy over performance masses correlates with evaluation reliability.

### Mechanism 3
The mean-variance plane of harmony across models diagnoses benchmark reliability: high mean + low variance indicates aggregate metrics consistently reflect broad competence across models. Compute μ_H(B) = E_f~F[H_B(f)] and σ²_H(B) = Var_f~F(H_B(f)). Benchmarks in the upper-left region (high mean, low variance) yield stable, interpretable evaluations; those with low mean or high variance are fragile or model-dependent. Core assumption: Cross-model consistency in harmony implies benchmark reliability, while variance indicates susceptibility to model-specific artifacts.

## Foundational Learning

- **Concept**: Shannon Entropy and Normalization
  - Why needed here: Harmony uses normalized entropy to quantify uniformity; understanding how entropy captures distributional concentration is essential for interpreting harmony values.
  - Quick check question: Given two distributions over 4 items—[0.25, 0.25, 0.25, 0.25] and [0.7, 0.1, 0.1, 0.1]—which has higher entropy, and why does normalization matter?

- **Concept**: Spectral Clustering with Affinity Matrices
  - Why needed here: The method uses spectral clustering on predictive similarity matrices to induce semantic partitions; understanding Laplacian eigenvectors and k-means on spectral embeddings is required for implementation.
  - Quick check question: Why does spectral clustering use eigenvectors of the Laplacian rather than directly clustering the affinity matrix?

- **Concept**: Kullback-Leibler Divergence and Symmetrization
  - Why needed here: Predictive similarity uses symmetrized KL (Jeffreys divergence); understanding asymmetry of KL and why symmetrization is needed for similarity metrics is crucial.
  - Quick check question: Why is D_KL(p||q) ≠ D_KL(q||p), and what properties does the Jeffreys divergence (D_KL(p||q) + D_KL(q||q)) provide?

## Architecture Onboarding

- **Component map**: Inference Layer -> Similarity Computation -> Partition Induction -> Harmony Computation -> Aggregation
- **Critical path**: Logit caching during evaluation (no additional inference required, but memory for N×D logits where D=vocabulary size) -> Affinity matrix construction (O(N²D) time, O(N²) storage) -> Spectral clustering (eigendecomposition of N×N Laplacian) -> Harmony computation over resulting clusters
- **Design tradeoffs**: Model-specific vs. global similarity (uses model-specific partitions to preserve model-specific reliability signals, increasing computation), bandwidth selection (uses robust MAD-based bandwidth b = max{0.02, 1.4826 × MAD}), number of clusters k (silhouette score selection balances granularity vs. cluster quality)
- **Failure signatures**: Low silhouette scores across all k (weak cluster structure, predictive similarity may not induce meaningful partitions), high variance in harmony across models (model-dependent reliability, aggregate metrics may be artifacts), accuracy shifts significantly under pruning (original benchmark composition skewed aggregate scores)
- **First 3 experiments**: Validate on controlled data (construct RedundantQA variants with known ground-truth imbalance; verify estimated harmony correlates with ground-truth harmony), benchmark mapping (compute harmony for all models across 19 MCQA benchmarks; position each on μ_H-σ²_H plane), pruning intervention (for low-harmony benchmarks, apply inverse-proportional pruning; assess whether accuracy stabilizes and per-cluster dispersion decreases)

## Open Questions the Paper Calls Out

- **Open Question 1**: What mechanisms cause some model families (e.g., Gemma, OLMo) to improve harmony with scale while others (e.g., Qwen, Llama) show declining harmony? The authors identify family-specific scaling trends but provide only empirical observation, not mechanistic explanation for why parameter scaling affects performance uniformity differently across architectures.

- **Open Question 2**: How reliably does predictive similarity capture semantic similarity when extended to free-response benchmarks without discrete label spaces? Predictive similarity relies on probability distributions over discrete output tokens; extending this to generative settings requires defining comparable distributions or alternative similarity measures, which is unexplored.

- **Open Question 3**: Does model-conditional partitioning yield more reliable harmony estimates than a unified, model-agnostic partition across diverse model families? Model-specific partitions may capture idiosyncratic behaviors rather than benchmark structure, potentially reducing comparability; the trade-off between model fidelity and cross-model consistency remains unquantified.

## Limitations

- The spectral clustering approach depends heavily on predictive similarity construction, which may not capture semantic structure when model outputs are dominated by surface-level patterns
- The method requires significant computational resources for large benchmarks due to O(N²) affinity matrix construction
- The Gaussian kernel bandwidth selection and cluster number selection via silhouette scores introduce hyperparameters that may affect results across different benchmark types

## Confidence

- **High Confidence**: The mathematical formulation of harmony as normalized entropy is sound and well-defined. The framework for interpreting mean-variance relationships on the benchmark reliability plane follows logically from the metric definitions.
- **Medium Confidence**: The predictive similarity mechanism for inducing semantic partitions works well for the MCQA domain tested, but its generalizability to other task types remains unproven. The relationship between harmony scores and practical benchmark reliability requires further validation.
- **Low Confidence**: Cross-model consistency in harmony scores necessarily indicates benchmark reliability, as shared biases across model families could produce consistent but misleading scores.

## Next Checks

1. **Controlled Ground Truth Validation**: Create synthetic benchmarks with known subdomain distributions (e.g., RedundantQA with controlled domain ratios) and verify that estimated harmony correlates with ground-truth harmony across multiple noise levels.

2. **Cross-Task Generalizability**: Apply the harmony framework to non-MCQA benchmarks (e.g., text generation or multi-task benchmarks) to test whether predictive similarity induces meaningful semantic partitions beyond question answering.

3. **Bias Sensitivity Analysis**: Test whether benchmarks with known training data contamination (e.g., benchmarks containing web text similar to model pretraining corpora) show artificially high harmony scores due to shared biases rather than genuine uniformity.