---
ver: rpa2
title: 'DocReRank: Single-Page Hard Negative Query Generation for Training Multi-Modal
  RAG Rerankers'
arxiv_id: '2505.22584'
source_url: https://arxiv.org/abs/2505.22584
tags:
- query
- negative
- hard
- page
- queries
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the limitations of traditional hard negative
  mining in training multimodal rerankers, which often yield negatives that are not
  hard enough, lack diversity, and can include false negatives. The authors propose
  Single-Page Hard Negative Query Generation, an automated pipeline that generates
  hard negative queries for each document page rather than retrieving hard negative
  pages per query.
---

# DocReRank: Single-Page Hard Negative Query Generation for Training Multi-Modal RAG Rerankers

## Quick Facts
- arXiv ID: 2505.22584
- Source URL: https://arxiv.org/abs/2505.22584
- Authors: Navve Wasserman; Oliver Heinimann; Yuval Golbari; Tal Zimbalist; Eli Schwartz; Michal Irani
- Reference count: 23
- Key result: Up to 12.7 points higher NDCG@5 on Real-MM-RAG and 10.7 points on ViDoReV2 vs existing methods

## Executive Summary
This paper addresses the challenge of training multimodal rerankers for RAG systems by proposing Single-Page Hard Negative Query Generation (DocReRank). Traditional hard negative mining struggles with diversity and false negatives when dealing with visually rich documents. DocReRank inverts the process: instead of retrieving hard negative pages per query, it generates hard negative queries per document page. The pipeline uses an LLM to rephrase positive queries into semantically similar but unanswerable variants, then verifies them with a VLM. Experiments show DocReRank outperforms existing models on finance and general document benchmarks, achieving up to 12.7 points higher NDCG@5 on Real-MM-RAG and demonstrating improved robustness to fine-grained details.

## Method Summary
DocReRank uses a 4-stage pipeline: (1) Pixtral-12B generates N positive queries per page, (2) Qwen2.5-VL-7B verifies answerability, (3) Qwen2.5-7B generates 12 negative variants with specific prompts targeting attribute modifications, and (4) Qwen2.5-VL-7B verifies unanswerability with two prompts. The resulting Col-HNQue dataset (general) and Fin-HNQue dataset (finance-specific) train Qwen2-VL-2B-Instruct with LoRA fine-tuning. The model uses cross-entropy loss over "True"/"False" token logits with 3:1 positive:negative weighting, trained for 1 epoch on 4Ã— NVIDIA L40S GPUs.

## Key Results
- Achieved up to 12.7 points higher NDCG@5 on Real-MM-RAG benchmark
- Achieved up to 10.7 points higher NDCG@5 on ViDoReV2 benchmark
- Outperformed existing methods across finance and general document domains
- Demonstrated improved robustness to fine-grained details in finance documents

## Why This Works (Mechanism)

### Mechanism 1: Inverting the Hard Negative Generation Process
Instead of retrieving hard negative documents for a fixed query, DocReRank generates hard negative queries for a fixed document page. This inversion avoids the constraint of corpus availability and enables fine-grained control over generated queries. By synthesizing queries that are semantically adjacent but unanswerable by specific page content, the reranker learns fine-grained discrimination rather than broad topic matching.

### Mechanism 2: Targeted Fine-Grained Attribute Modification
The LLM generates hard negatives by modifying specific factual attributes (e.g., year, entity, metric) while keeping query structure identical. This forces the model to distinguish precise details rather than relying on keyword matching. For example, changing "2022" to "2024" in a query targets the model's weakness in verifying exact numerical values.

### Mechanism 3: Decoupling Generation from Verification
The LLM handles linguistic rephrasing while a separate VLM grounds candidates against visual document content. This two-step filter ensures "negative" labels are valid, reducing false negatives more effectively than single-stage processes. Two different verification prompts further robustify the check.

## Foundational Learning

- **Reranking Architecture**: Stage 2 in RAG systems that deeply scores top-K retrieval results; required to understand DocReRank optimizes ranking rather than retrieval
  - Quick check: Why is cosine similarity alone insufficient for final ranking of top-K documents?

- **Hard Negative Mining**: Training technique using samples that teach models what not to retrieve; central to understanding what DocReRank improves
  - Quick check: What is the risk of a "False Negative" in training? (Answer: The model learns to reject correct answers)

- **LoRA (Low-Rank Adaptation)**: Efficient fine-tuning method for large transformers by freezing core weights; necessary to understand the practical implementation
  - Quick check: Why would one freeze the visual encoder while tuning LLM layers?

## Architecture Onboarding

- **Component map**: Pixtral-12B VLM (positive query generation) -> Qwen2.5-7B LLM (negative query generation) -> Qwen2.5-VL-7B VLM (verification) -> Qwen2-VL-2B-Instruct reranker with LoRA

- **Critical path**: Prompt Design -> LLM Generation (12 variants) -> VLM Double-Verification -> LoRA Training (Triplet loss)

- **Design tradeoffs**: Synthetic queries provide perfect control for hardness but may lack natural linguistic variance; inverting pipeline avoids corpus embedding costs but requires LLM/VLM inference

- **Failure signatures**: Keyword Bias (model learns shallow matching regardless of attribute changes), Domain Drift (finance training may hurt performance on narrative text)

- **First 3 experiments**: (1) Run generation pipeline on 50 sample pages to verify hard negatives are truly unanswerable, (2) Train three models: standard BM25 negatives, random negatives, and DocReRank negatives to confirm specific hardness value, (3) Evaluate trained model on rephrased benchmark to ensure no memorization of specific phrasing

## Open Questions the Paper Calls Out

- **Synthetic query diversity**: How can the generation pipeline be modified to better capture full semantic diversity and variability of human-generated queries, given current models may have inherent biases limiting output distribution?

- **Complete verification elimination**: Can verification mechanisms be developed to completely eliminate false negatives and positives without relying solely on VLM prompting strategies, given VLMs are probabilistic and susceptible to hallucination?

- **Single-page constraint limitations**: To what extent does the "Single-Page" generation constraint hinder the model's ability to handle queries requiring cross-page or multi-hop reasoning, given lower performance on multi-page benchmarks?

- **Retriever recall upper bound**: How does reliance on initial retriever's recall limit theoretical upper bound of DocReRank's accuracy, given reranker cannot recover missing information if true positive isn't in provided subset?

## Limitations

- False negatives and positives can occur during VLM verification even with double verification strategies
- Generated queries may not capture full semantic diversity of human queries due to model biases
- Single-page generation may not handle multi-hop reasoning requirements across multiple documents
- Performance gains may be domain-specific, with finance training potentially overfitting to tabular layouts

## Confidence

- **High Confidence**: Inversion mechanism is technically sound and well-supported by ablation experiments; two-stage verification approach is valid
- **Medium Confidence**: Performance gains are well-documented but may not generalize beyond tested domains; claim about hard negative mining challenges is supported but not extensively quantified
- **Low Confidence**: Assertion that generated queries are "semantically similar but unanswerable" relies heavily on unverified VLM quality; attribute modification strategy generalizability to other fine-grained attributes is untested

## Next Checks

1. **Verification Pipeline Accuracy**: Implement human evaluation study where 100 generated negative queries are independently assessed for true unanswerability; compare VLM verification accuracy against human judgments to establish false negative rates

2. **Cross-Domain Robustness Test**: Evaluate DocReRank on held-out domain (e.g., medical research papers or legal documents) not part of training corpus to test generalizable fine-grained discrimination vs. domain memorization

3. **Generation Pipeline Ablation**: Conduct ablation study comparing DocReRank performance with: manually curated hard negatives, automatically generated hard negatives without VLM verification, and full DocReRank pipeline to quantify component contributions