---
ver: rpa2
title: Double Descent as a Lens for Sample Efficiency in Autoregressive vs. Discrete
  Diffusion Models
arxiv_id: '2509.24974'
source_url: https://arxiv.org/abs/2509.24974
tags:
- diffusion
- autoregressive
- descent
- discrete
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work investigates sample efficiency of autoregressive vs.
  discrete diffusion models using the double descent framework.
---

# Double Descent as a Lens for Sample Efficiency in Autoregressive vs. Discrete Diffusion Models

## Quick Facts
- arXiv ID: 2509.24974
- Source URL: https://arxiv.org/abs/2509.24974
- Authors: Ahmad Fraij; Sam Dauncey
- Reference count: 19
- Autoregressive models achieve lower bits-per-token error with smaller capacity and fewer epochs on Tiny Shakespeare dataset.

## Executive Summary
This work investigates sample efficiency of autoregressive versus discrete diffusion models using the double descent framework. The authors train both architectures on the Tiny Shakespeare dataset with identical transformer architectures, varying embedding dimensions and epochs to trace their test loss curves. Results show that autoregressive models achieve lower bits-per-token (BPT) error with smaller capacity and fewer epochs, indicating stronger immediate sample efficiency. In contrast, diffusion models require larger embeddings and more training to reach their optimal BPT loss, exhibiting a broader underparameterized regime. However, diffusion models generalize better when given sufficient capacity and compute, surpassing autoregressive performance at larger scales. Neither model shows a pronounced second descent in the overparameterized regime, even with weight decay.

## Method Summary
The authors compare autoregressive and discrete diffusion models using a double descent analysis framework on the Tiny Shakespeare dataset. Both models use identical 12-layer transformer decoders with 12 attention heads, RoPE embeddings, GELU FFN, and residual connections. The key architectural difference is that autoregressive models use causal attention while diffusion models use non-causal attention with an absorbing [MASK] state corruption process. Models are trained across varying embedding dimensions (100-1200) for up to 50 epochs with AdamW optimizer (lr=6e-4, weight_decay=0.1). Test bits-per-token loss is measured as the primary metric, with diffusion BPT estimated via Monte Carlo sampling of diffusion timesteps.

## Key Results
- Autoregressive models achieve lower BPT error with smaller capacity and fewer epochs
- Diffusion models require larger embeddings (around 600) to escape underparameterized regime compared to AR (around 400)
- Neither model exhibits pronounced second descent in overparameterized regime despite weight decay
- With sufficient capacity and training, diffusion models surpass AR performance at larger scales

## Why This Works (Mechanism)

### Mechanism 1: Double Descent as Sample Efficiency Metric
Test loss curves across model capacity reveal which architecture extracts more signal per parameter from limited data. By plotting BPT against embedding dimension, the interpolation threshold (peak test loss) indicates when models memorize training data. Earlier thresholds and lower peaks indicate higher sample efficiency. Core assumption: The interpolation threshold is a reliable proxy for sample efficiency across architectures.

### Mechanism 2: Training Objective Density Drives Per-Epoch Efficiency
Autoregressive factorization provides denser gradient signal than diffusion's sampled denoising objective. AR computes loss on all L tokens per forward pass via factorization p(x₁...x_L) = Πp(x_l|x_{<l}). Diffusion samples one timestep t per sequence per epoch, predicting only the corrupted subset, yielding sparser supervision. Core assumption: Equating epochs by FLOPs is fair despite different token coverage per pass.

### Mechanism 3: Bidirectional Attention Enables Scaling Crossover
Diffusion's non-causal attention provides competitive advantage only when sufficient capacity and compute overcome its per-epoch inefficiency. Non-causal attention allows each token to attend to full context, improving global coherence. This advantage is hidden at small scales but emerges when model capacity exceeds diffusion's higher interpolation threshold. Core assumption: Architectural difference (causal vs. bidirectional) is primary driver of scaling crossover.

## Foundational Learning

- Concept: Double Descent
  - Why needed here: Core analytical framework for comparing sample efficiency; understanding why test loss peaks at interpolation threshold then potentially decreases
  - Quick check question: Why does test error increase near the interpolation threshold, and what determines whether a second descent occurs?

- Concept: Bits-Per-Token (BPT) as Cross-Entropy Proxy
  - Why needed here: Unified metric for comparing AR (exact NLL) and diffusion (variational upper bound) on identical scale
  - Quick check question: Why is diffusion's BPT an upper bound rather than exact, and how does this affect interpretation?

- Concept: Absorbing State Discrete Diffusion
  - Why needed here: Understanding how tokens transition to [MASK] and how the model learns to denoise
  - Quick check question: As diffusion steps T increase, what distribution do tokens approach, and why is the [MASK] state absorbing?

## Architecture Onboarding

- Component map:
  - Shared backbone: 12-layer transformer decoder, 12 attention heads, RoPE embeddings, GELU FFN, residual + LayerNorm
  - AR-specific: Causal attention mask
  - Diffusion-specific: Non-causal attention, timestep embedding MLP (dim 128), absorbing [MASK] token (index K−1), T=1000 diffusion steps, λ_CE=0.05

- Critical path:
  1. Select architecture and embedding dimension (determines under/overparameterized regime)
  2. Train with early stopping for optimal BPT, or 50+ epochs to observe double descent emergence
  3. Compare interpolation threshold location and minimum achievable BPT

- Design tradeoffs:
  - Embedding <400: AR optimal, diffusion underfits
  - Embedding 400–600: AR at/near interpolation, diffusion still underparameterized
  - Embedding >600: Both overparameterized; diffusion may surpass AR with full training
  - Weight decay 0.1 applied to both; does not induce second descent in this setup

- Failure signatures:
  - Test loss remains elevated past interpolation threshold—no pronounced second descent (contrary to image classification)
  - Diffusion BPT stagnates if epochs insufficient (<30)
  - AR overfits rapidly after epoch ~18; double descent visible by epoch 21

- First 3 experiments:
  1. Capacity sweep: Train AR and diffusion at embedding dims [100, 200, 400, 600, 800, 1200] for 50 epochs; plot test BPT curves to locate interpolation thresholds
  2. Early stopping analysis: For each embedding dim, track epoch achieving minimum test BPT; compare best-achievable BPT across architectures
  3. Epoch scaling at fixed capacity: Fix embedding=600; train both models for [10, 20, 30, 40, 50] epochs to identify when double descent emerges and whether diffusion crosses AR performance

## Open Questions the Paper Calls Out

### Open Question 1
How does the introduction of label or input noise affect the double descent curves and relative sample efficiency of autoregressive versus discrete diffusion models? The authors state that future work could extend the analysis to "varying levels of label or input noise," noting that prior work suggests noise impacts double descent prominence.

### Open Question 2
Is the absence of a pronounced second descent in the overparameterized regime a fundamental property of generative language modeling or a result of the specific scaling regimes tested? The authors note that neither model exhibited a second descent, "suggesting that the pronounced second descent... may not apply to generative language modelling."

### Open Question 3
Do the superior sample efficiency and earlier interpolation of autoregressive models persist when scaling to datasets significantly larger than Tiny Shakespeare? The authors conclude their results are specific to "small-scale datasets" and explicitly suggest extending the analysis to "larger datasets."

## Limitations

- Study relies on single synthetic text dataset (Tiny Shakespeare) rather than diverse real-world tasks
- Assumes FLOPs-based epoch equivalence between architectures despite different token coverage per pass
- Diffusion BPT values are variational upper bounds rather than exact NLL, making direct comparisons potentially biased
- Does not explore other diffusion variants (e.g., continuous, score-based) or different noise schedules

## Confidence

- **High confidence**: AR models achieve lower BPT error with smaller capacity and fewer epochs on small datasets; diffusion models require larger embeddings and more training to reach optimal BPT
- **Medium confidence**: Diffusion models generalize better when given sufficient capacity and compute, surpassing autoregressive performance at larger scales; neither model shows pronounced second descent even with weight decay
- **Low confidence**: Architectural differences (causal vs. bidirectional attention) are the primary driver of scaling crossover; the lack of second descent is a fundamental property of these architectures rather than a dataset-specific artifact

## Next Checks

1. **Dataset generalization test**: Replicate the capacity and epoch sweep on two additional small datasets (e.g., WikiText-2 and Penn Treebank) to verify whether AR's sample efficiency advantage persists across domains and whether double descent peaks remain absent in the overparameterized regime

2. **Objective density analysis**: Compare AR and diffusion models trained for equal wall-clock time rather than equal epochs, and measure validation loss as a function of effective token predictions per second to determine if diffusion's per-epoch inefficiency is mitigated by allowing more total updates

3. **Extended capacity scaling**: Train both architectures at embedding dimensions beyond 1200 (e.g., 1600, 2000, 3000) with full 50+ epochs to definitively establish whether diffusion models continue to surpass AR performance at very large scales, or whether AR maintains superiority even in the overparameterized regime