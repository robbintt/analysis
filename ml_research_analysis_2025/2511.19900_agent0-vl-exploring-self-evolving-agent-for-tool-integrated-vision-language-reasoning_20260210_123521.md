---
ver: rpa2
title: 'Agent0-VL: Exploring Self-Evolving Agent for Tool-Integrated Vision-Language
  Reasoning'
arxiv_id: '2511.19900'
source_url: https://arxiv.org/abs/2511.19900
tags:
- reasoning
- arxiv
- tool
- agent0-vl
- zhang
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "Agent0-VL addresses the limitations of vision-language agents\
  \ in complex multimodal reasoning by integrating tool-use into both reasoning and\
  \ self-evaluation. It introduces a unified dual-role architecture\u2014a Solver\
  \ that performs tool-integrated reasoning and a Verifier that evaluates and repairs\
  \ reasoning trajectories\u2014forming a self-evolving loop with tool-grounded verification\
  \ and reinforcement learning."
---

# Agent0-VL: Exploring Self-Evolving Agent for Tool-Integrated Vision-Language Reasoning

## Quick Facts
- arXiv ID: 2511.19900
- Source URL: https://arxiv.org/abs/2511.19900
- Reference count: 40
- Primary result: Agent0-VL-7B achieves 12.5% improvement over base model on complex VL reasoning benchmarks

## Executive Summary
Agent0-VL addresses the limitations of vision-language agents in complex multimodal reasoning by integrating tool-use into both reasoning and self-evaluation. It introduces a unified dual-role architecture—a Solver that performs tool-integrated reasoning and a Verifier that evaluates and repairs reasoning trajectories—forming a self-evolving loop with tool-grounded verification and reinforcement learning. This approach enables continual improvement without external rewards. Experiments show Agent0-VL-7B achieves a 12.5% improvement over the base model, outperforms open-source baselines on mathematical and visual reasoning benchmarks, and demonstrates stable iterative gains across multiple training cycles. The model also generalizes as an effective process reward model, improving other VLMs by 7.3% on average.

## Method Summary
Agent0-VL employs a two-stage training process: first, supervised fine-tuning (SFT) using 200k tool-augmented trajectories generated by GPT-5 and Qwen2.5-VL-72B teachers; second, reinforcement learning with self-generated rewards through a Self-Evolving Reasoning Cycle (SERC). The unified policy πθ alternates between Solver (generating reasoning trajectories with tool calls) and Verifier (evaluating steps using tool-grounded critique) roles. Process rewards combine tool output reliability, confidence-weighted scores, and KL regularization. Group Relative Policy Optimization (GRPO) with group size 8 normalizes rewards across trajectory groups. The system includes a self-repair mechanism triggered by low-confidence verification, resampled actions, and iterative SERC cycles. Training uses Qwen2.5-VL-7B-Instruct as base, with 2 epochs of external reward pre-training before self-evolving RL.

## Key Results
- Agent0-VL-7B achieves 12.5% improvement over base Qwen2.5-VL-7B across MathVerse, MathVision, MathVista, WeMath, HallBench, ChartQA, and MMMU benchmarks
- Tool-grounded verification ablation shows ~6.5% degradation, validating the importance of tool-integrated evaluation
- Trained Verifier generalizes as Process Reward Model, improving other VLMs by 7.3% average across InternVL-2.5-8B and Qwen2.5-VL-7B
- Stable iterative gains across 3 training cycles without performance degradation or reward hacking

## Why This Works (Mechanism)

### Mechanism 1: Tool-Grounded Verification
Integrating external tools into self-evaluation reduces evaluation hallucinations and improves verification accuracy over purely text-based self-assessment. The Verifier invokes tools (e.g., Python code execution, image cropping) to obtain factual evidence during step-wise verification, transforming verification from subjective textual reflection into executable, evidence-based validation with higher reliability. This assumes tool outputs provide trustworthy ground truth signals for tasks like geometric calculations.

### Mechanism 2: Dual-Role Solver-Verifier Architecture with Shared Parameters
A single LVLM alternating between Solver and Verifier roles enables stable self-improvement without external reward models by jointly aligning reasoning and evaluation distributions. The unified policy πθ conditions on role indicator m ∈ {S, V}, allowing improvements in verification capability to directly enhance reasoning and vice versa through shared gradient updates. This assumes parameter sharing enables effective transfer between reasoning and evaluation capabilities.

### Mechanism 3: Self-Evolving Reasoning Cycle (SERC) with GRPO
Alternating inner-loop trajectory generation/verification with outer-loop policy optimization via Group Relative Policy Optimization enables continual self-improvement without external rewards. The inner loop generates trajectories, the Verifier produces process rewards, and selective self-repair is triggered for low-confidence steps. The outer loop uses GRPO to normalize rewards across groups and update the policy, reframing learning as "being better than the group average" rather than maximizing absolute reward.

## Foundational Learning

- **Concept: Partially Observable Markov Decision Process (POMDP)**
  - Why needed here: The paper formulates multimodal reasoning as a POMDP where states are latent multimodal reasoning contexts, actions include text or tool calls, and observations are tool outputs. Understanding POMDPs is essential to grasp how Agent0-VL models sequential decision-making under partial observability.
  - Quick check question: Given a state space S, action space A = A_text ∪ A_tool, and observation space O, how would you compute the belief state update b_{t+1} = f(b_t, o_t)?

- **Concept: Group Relative Policy Optimization (GRPO)**
  - Why needed here: GRPO is the reinforcement learning algorithm used in the outer loop, normalizing rewards across a group of trajectories and using clipped importance sampling ratios. Without understanding GRPO, you cannot reason about the stability and convergence properties of the self-evolution loop.
  - Quick check question: In GRPO, why is the advantage computed as Âi = (g(τi) - mean(g)) / (std(g) + ε) rather than using an absolute reward value?

- **Concept: Tool-Integrated Reasoning (TIR)**
  - Why needed here: Agent0-VL extends TIR from reasoning-only to verification and repair. Tools (e.g., Python sandbox, image cropping) provide executable evidence for both solving and evaluating tasks. Understanding TIR is critical to see why purely text-based self-evaluation fails for complex visual/mathematical tasks.
  - Quick check question: If a VLM calls a Python tool to compute √(l² - r²) for a cone height problem, what type of observation o_t would the tool return, and how would the Verifier use it?

## Architecture Onboarding

- **Component map:** Unified policy πθ → Role indicator m ∈ {S, V} → Tool sandbox (Python execution, image manipulation) → Process reward module → Self-repair gate → GRPO optimizer

- **Critical path:** Input (image, query) → Solver generates trajectory τ = {(s_t, a_t, o_t)} with tool calls → Verifier evaluates each step → produces V_t = (score_t, conf_t, critique_t) → If conf_t < τ_c, trigger self-repair → regenerate faulty step → Aggregate trajectory return g(τ) = α_out · r_out + Σ γ^{t-1} · r_t → Outer loop: GRPO computes relative advantages → updates πθ

- **Design tradeoffs:** Shared vs. separate Solver/Verifier parameters (shared reduces memory but risks role collapse); tool use frequency (aggressive improves verification but increases latency); repair threshold τ_c (low threshold → faster but potentially incorrect); group size N in GRPO (small N → high variance advantages)

- **Failure signatures:** Mode collapse (model always assigns high confidence regardless of correctness); reward hacking (generates short trajectories to minimize cost penalty); tool dependency (invokes tools unnecessarily); distributional drift (Solver and Verifier distributions diverge)

- **First 3 experiments:**
  1. Ablation on tool use in verification: Train Agent0-VL with tools disabled during Verifier mode; compare performance to full tool-grounded version (expect ~6.5% degradation)
  2. Repair threshold sensitivity: Sweep τ_c ∈ {0.3, 0.5, 0.7, 0.9}; plot trajectory accuracy vs. average repair rate to identify optimal balance
  3. Cross-model PRM transfer: Deploy trained Verifier as standalone Process Reward Model for other VLMs (e.g., InternVL-2.5-8B); measure Best-of-N selection accuracy improvement

## Open Questions the Paper Calls Out

- **Open Question 1:** Can Agent0-VL achieve effective self-evolution without any stronger teacher model initialization for SFT cold-start? The paper uses GPT-5 and Qwen2.5-VL-72B teachers for SFT initialization, but no ablation tests removing this dependency are reported.

- **Open Question 2:** What is the long-term convergence behavior of the Self-Evolving Reasoning Cycle beyond three iterations? The paper shows monotonic gains across 3 iterations but does not analyze whether gains plateau, degrade, or destabilize with extended training.

- **Open Question 3:** How robust is Agent0-VL when verification tools produce incorrect or misleading outputs? The framework assumes tool outputs provide reliable grounding, but no analysis examines cascading failures from erroneous tool calls.

- **Open Question 4:** Does the unified Solver-Verifier architecture scale effectively to significantly larger model sizes (e.g., 30B+ parameters)? Experiments are limited to 7B and 8B models, leaving scalability untested.

## Limitations
- Exact numerical values for critical hyperparameters (λ_tool, β_div, α_out) in reward functions are not specified, affecting reproducibility
- Full prompt templates for Solver, Verifier, and Self-Repair roles are only partially shown, with tool sandbox implementation details missing
- Ablation studies focus primarily on removing tool use during verification, with less analysis on other architectural components like repair mechanism or GRPO sensitivity

## Confidence

**High Confidence:** The core mechanism of tool-grounded verification is well-supported by experimental ablation showing ~6.5% degradation when tools are disabled during verification, and the self-evolving loop with GRPO is clearly demonstrated through stable performance gains across multiple training cycles.

**Medium Confidence:** The dual-role architecture with shared parameters is theoretically sound and shows effectiveness, but the claim that parameter sharing enables stable self-improvement without external rewards is partially supported—the paper notes pre-training with external rewards for 2 epochs before self-evolving RL.

**Medium Confidence:** The generalization claim that the Verifier serves as an effective Process Reward Model for other VLMs is demonstrated but with limited scope (tested on InternVL-2.5-8B and Qwen2.5-VL-7B), warranting broader validation.

## Next Checks

1. **Ablation of Shared Parameters:** Train separate Solver and Verifier models with independent parameters to quantify the performance impact of the unified architecture versus the claimed benefits of distributional alignment.

2. **GRPO Group Size Sensitivity:** Systematically vary the group size parameter in GRPO (N ∈ {4, 8, 16}) to identify the optimal balance between reward signal stability and computational efficiency.

3. **Cross-Domain PRM Evaluation:** Test the trained Verifier as a Process Reward Model on a diverse set of VLMs across different domains (not just mathematical/visual reasoning) to assess the breadth of its generalization capability.