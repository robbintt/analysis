---
ver: rpa2
title: 'fev-bench: A Realistic Benchmark for Time Series Forecasting'
arxiv_id: '2509.26468'
source_url: https://arxiv.org/abs/2509.26468
tags:
- forecasting
- series
- benchmark
- time
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces fev-bench, a benchmark for time series forecasting
  containing 100 tasks across seven domains, including 46 tasks with covariates. The
  benchmark addresses the need for realistic evaluation infrastructure in forecasting,
  as existing benchmarks often lack coverage of real-world settings like covariates,
  lack statistical rigor in aggregation, and lack standardized evaluation infrastructure.
---

# fev-bench: A Realistic Benchmark for Time Series Forecasting

## Quick Facts
- arXiv ID: 2509.26468
- Source URL: https://arxiv.org/abs/2509.26468
- Reference count: 40
- 100 time series forecasting tasks across 7 domains, including 46 tasks with covariates

## Executive Summary
fev-bench addresses the critical gap in realistic time series forecasting evaluation by providing a comprehensive benchmark with 100 tasks spanning seven domains, including 46 tasks with covariates. The benchmark introduces fev, a lightweight Python library that emphasizes reproducibility and integrates with existing workflows. Using principled bootstrap aggregation with confidence intervals, fev-bench reports model performance through two complementary dimensions: win rates and skill scores. The paper demonstrates that current models show significant room for improvement, particularly on covariate tasks and multivariate forecasting, while identifying key research directions for future work.

## Method Summary
The fev-bench benchmark uses the fev Python library to evaluate forecasting models in a zero-shot setting across 100 tasks sourced from 96 datasets. Evaluation employs rolling-origin methodology with 6-hour timeouts per task, using MASE for point forecasts and SQL for probabilistic forecasts. The aggregation framework combines win rates (ordinal consistency) and skill scores (geometric mean of clipped relative errors) with bootstrapped 95% confidence intervals. Models are scored across multiple evaluation windows per task, with results aggregated first per task then across tasks. The framework supports both univariate and multivariate series, with 46 tasks incorporating covariates ranging from known dynamic variables to static metadata.

## Key Results
- Current pretrained models show significant performance gaps on fev-bench, with Chronos-2 achieving skill scores of 1.15 (MASE) and 1.06 (SQL)
- Covariate-aware models (Chronos-2, TabPFN-TS) demonstrate measurable advantages on the 46 covariate tasks compared to univariate-only approaches
- Bootstrap confidence intervals reveal meaningful performance differences between models, with some showing overlapping win rates but divergent skill scores
- 4 tasks required data replacement due to potential leakage, highlighting the benchmark's commitment to data integrity

## Why This Works (Mechanism)

### Mechanism 1
Bootstrap confidence intervals distinguish genuine model improvements from noise by resampling tasks with replacement (B=1000), computing pairwise statistics on each sample, and extracting 2.5th/97.5th percentiles to quantify ranking stability. Task sampling captures the relevant distribution while paired structure preserves model-to-model comparability. Small or unrepresentative task sets may yield misleadingly narrow intervals.

### Mechanism 2
Dual metrics (win rate + skill score) capture complementary performance dimensions by measuring ordinal consistency through win rates and magnitude sensitivity through skill scores using geometric mean of clipped relative errors. This approach addresses win rate's insensitivity to performance magnitude and changes as new models are added. High task heterogeneity can still make geometric mean sensitive to outliers despite clipping.

### Mechanism 3
Covariate-inclusive tasks reveal capabilities invisible to univariate-only benchmarks by including 46/100 tasks with known dynamic, past-only, or static covariates. Models that exploit these inputs gain measurable advantage, as demonstrated by Chronos-2 and TabPFN-TS achieving higher skill scores on covariate tasks. If few models support covariates, rankings conflate "quality" with "feature support."

## Foundational Learning

- **Rolling-origin evaluation**: Preserves temporal causality by maintaining train-before-test order while providing multiple evaluation points. Quick check: Why can't you randomly shuffle time series observations for cross-validation?

- **MASE (Mean Absolute Scaled Error)**: Primary point-forecast metric that's scale-free via seasonal-error normalization, handling trends and zeros better than MAPE. Quick check: What does a MASE > 1.0 indicate about your model vs. seasonal naive?

- **Quantile loss and Scaled Quantile Loss (SQL)**: Evaluates probabilistic forecasts by extending MASE logic to distributional predictions across quantile levels Q = {0.1, ..., 0.9}. Quick check: At q = 0.9, does quantile loss penalize over-prediction or under-prediction more?

## Architecture Onboarding

- **Component map**: Task (YAML) -> dataset, horizon, covariates, metric -> EvaluationWindow -> train/test split per cutoff -> fev library -> data loading, scoring, aggregation -> Bootstrap aggregator -> win rate, skill score, CIs

- **Critical path**: 1) Load task -> generate W evaluation windows 2) Run model -> collect forecasts per window 3) Score -> MASE (point) / SQL (probabilistic) 4) Aggregate per task -> arithmetic mean over windows 5) Aggregate across tasks -> win rate, skill score, bootstrap CIs

- **Design tradeoffs**: No model implementations to avoid maintenance burden (users integrate own models via adapters); clipping [0.01, 100] in skill score prevents extreme ratios from dominating geometric mean but may mask tail behavior; 100 tasks vs. 20-task mini balances full coverage vs. iteration speed

- **Failure signatures**: Timeout/crash -> imputed with Seasonal Naive baseline; Data leakage flag -> affected tasks replaced with Chronos-Bolt scores; Intermittent series -> MASE/SQL undefined if seasonal error -> 0 (pre-verified not occurring)

- **First 3 experiments**: 1) Run Seasonal Naive on fev-bench-mini; verify scores match Tables 16-17 to validate pipeline 2) Compare Chronos-Bolt on 10 covariate vs. 10 non-covariate tasks; confirm covariate-aware models gain measurable advantage 3) Compute pairwise win rate + bootstrap CI between two models on 5 tasks; verify CI width decreases as task count increases

## Open Questions the Paper Calls Out

### Open Question 1
What architectures enable native cross-variate dependency modeling in multivariate pretrained forecasters? Most current models decompose multivariate series into independent univariate problems, losing cross-variate information. New architectures demonstrating significant gains on fev-bench's 35 multivariate tasks versus univariate decomposition approaches would resolve this.

### Open Question 2
How can covariates be integrated into pretrained models without harming zero-shot generalization? Only 2-3 of 8 evaluated models support any covariates; integration strategies remain underexplored. New models improving on fev-bench's 46 covariate tasks while maintaining competitive univariate task performance would provide evidence.

### Open Question 3
How can subtle data leakage be detected beyond current self-reporting mechanisms? Current detection relies on developer self-reporting; indirect leakage through related pretraining data is undetectable. Automated probing methods that identify benchmark pattern memorization from pretraining data would resolve this.

## Limitations
- 4 of 100 tasks required data replacement due to potential leakage, though the benchmark maintains data integrity through rigorous checks
- Bootstrap aggregation assumes task independence, which may not hold for domain-specific datasets with correlated errors
- Only 46 of 100 tasks include covariates, potentially limiting conclusions about covariate-robust model performance

## Confidence

- **Benchmark Representativeness**: Medium - Comprehensive task coverage but unknown sampling bias
- **Statistical Methodology**: High - Explicit bootstrap procedures with published code
- **Metric Design**: High - Dual metrics with theoretical justification and practical differentiation
- **Real-world Applicability**: Medium - Includes covariates but limited task diversity in this dimension

## Next Checks

1. **Bootstrap Stability Test**: Recompute win rates and skill scores using B=100 vs B=1000 bootstrap samples on fev-bench-mini (20 tasks) to verify confidence interval stability and determine minimum sample size for reliable inference.

2. **Covariate Sensitivity Analysis**: Run a gradient boosting model (which supports covariates) vs. a strong univariate baseline on the 46 covariate tasks vs. 54 non-covariate tasks to quantify the actual performance gap and validate the benchmark's ability to distinguish covariate-aware capabilities.

3. **Task Independence Verification**: Compute pairwise correlation coefficients between task scores across all models to quantify potential dependence structures that could violate bootstrap assumptions, and assess whether domain clustering affects aggregation validity.