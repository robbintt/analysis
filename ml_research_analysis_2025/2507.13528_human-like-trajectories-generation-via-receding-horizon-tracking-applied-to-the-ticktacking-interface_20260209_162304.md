---
ver: rpa2
title: Human-Like Trajectories Generation via Receding Horizon Tracking Applied to
  the TickTacking Interface
arxiv_id: '2507.13528'
source_url: https://arxiv.org/abs/2507.13528
tags:
- control
- human
- interface
- controller
- tracking
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper investigates the generation of human-like trajectories\
  \ using a receding horizon control approach applied to the TickTacking interface,\
  \ a rhythm-based human-computer interface. By analyzing user-generated trajectories\
  \ from a target-tracking task, the study identifies key human behavioral features\
  \ including preferred velocity directions (45\xB0, 135\xB0, 225\xB0, 315\xB0), velocity\
  \ magnitude distributions, and control sparsity patterns."
---

# Human-Like Trajectories Generation via Receding Horizon Tracking Applied to the TickTacking Interface

## Quick Facts
- arXiv ID: 2507.13528
- Source URL: https://arxiv.org/abs/2507.13528
- Reference count: 18
- Primary result: Human-inspired receding horizon control generates trajectories matching human velocity patterns while maintaining good tracking performance

## Executive Summary
This paper presents a receding horizon control approach for generating human-like trajectories in a rhythm-based TickTacking interface. By analyzing human behavioral features from target-tracking tasks, the study identifies three key characteristics: preferred diagonal velocity directions (45°, 135°, 225°, 315°), specific velocity magnitude distributions, and temporal/spatial sparsity patterns. These features are incorporated into a human-inspired controller through penalty terms and constraints in the optimization problem. The controller is evaluated against a baseline optimal-control-based agent, demonstrating that incorporating human-like features produces trajectories that better match human behavior while maintaining tracking accuracy.

## Method Summary
The method employs a receding horizon (MPC) controller that generates 2D pointer trajectories through quadratic optimization with human-inspired penalty terms and constraints. The controller uses a prediction window of 8Δt (2.264 seconds) and incorporates three human behavioral features: (1) penalty μ₁ for diagonal velocity direction preference (λ₁=32), (2) penalty μ₂ for velocity magnitude matching human free-navigation data (λ₂=0.6), and (3) stochastic event-triggered control for temporal sparsity using empirical CDF of human inter-action intervals. Spatial sparsity is enforced through conditional constraints allowing only one velocity component to update at a time. Zero-mean Gaussian noise (σ_ε=2 px/s) is added to account for residual stochasticity.

## Key Results
- Human trajectories show clear velocity direction preferences at 45°, 135°, 225°, and 315° angles
- The human-inspired controller produces velocity histograms closely matching human patterns, unlike the baseline optimal controller
- Human-inspired controller maintains comparable tracking performance (MSE distribution) while generating more naturalistic trajectories
- Temporal sparsity patterns are successfully reproduced through event-triggered control using empirical CDF sampling

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Incorporating human velocity direction preferences (45°, 135°, 225°, 315°) into the controller produces more human-like trajectories than pure optimal control.
- **Mechanism:** The penalty term μ₁(U(t)) = (1/W)Σ||vₓ| - |vᵧ|| penalizes deviations from diagonal directions where velocity components have equal magnitude. This biases the optimizer toward human-preferred directions without hard constraints.
- **Core assumption:** Human preference for diagonal directions reflects fundamental timing preferences (1:1 ratios) rather than task-specific adaptation.
- **Evidence anchors:**
  - [abstract]: "key human behavioral features including preferred velocity directions (45°, 135°, 225°, 315°)"
  - [Section 2.0.1]: "users favored directions at 45°, 135°, 225° or 315° relative to the positive horizontal axis"
  - [corpus]: Weak direct evidence; related work "Learning Human-Like RL Agents" addresses action quantization but not directional bias specifically.
- **Break condition:** If the target trajectory requires predominantly cardinal (0°, 90°, 180°, 270°) movement, the penalty may degrade tracking performance excessively.

### Mechanism 2
- **Claim:** Matching the human velocity magnitude distribution via Gaussian penalty produces realistic speed profiles while maintaining tracking capability.
- **Mechanism:** The penalty μ₂(U(t)) = (1/W)Σ(||u||₁ - 63.75)² approximates the negative log-likelihood of a Gaussian fitted to human free-navigation data (μ=63.75 px/s, σ=30.45 px/s), trading off between naturalistic speeds and tracking accuracy.
- **Core assumption:** Human velocity magnitude in free navigation represents an intrinsic motor preference that transfers to tracking tasks.
- **Evidence anchors:**
  - [Section 2.0.2]: "KDE of the human tracking data is a trade-off between the reference KDE and the Gaussian PDF"
  - [Section 3.1]: Explicit formulation with fitted parameters from human data
  - [corpus]: No direct corpus evidence for this specific magnitude-matching approach.
- **Break condition:** If the tracking task requires velocities far outside the fitted Gaussian range, the controller will either fail to track or appear unnatural.

### Mechanism 3
- **Claim:** Event-triggered control updates with human-derived inter-action intervals produce realistic temporal sparsity without explicit scheduling.
- **Mechanism:** A stochastic condition dₜ ≤ F̂ₜ(t_event) samples control update timing from the empirical CDF of human inter-action intervals. Combined with spatial sparsity constraints (only one velocity component updates at a time), this reproduces the zig-zag pattern observed in human trajectories.
- **Core assumption:** The inter-action interval distribution from free navigation is task-independent and reflects biomechanical/cognitive limits.
- **Evidence anchors:**
  - [Section 2.0.3]: "minimal divergence suggests that the interval length between control actions is a fundamental human property unaffected by the specific task"
  - [Section 3.1]: "Condition (4) ensures that the RH control strategy has an overall CDF of t_event coincident with F̂ₜ"
  - [corpus]: "Ensemble Kalman-Bucy filtering for nonlinear MPC" discusses uncertainty-aware control but not human-like sparsity patterns.
- **Break condition:** If the reference trajectory changes rapidly on timescales shorter than typical human inter-action intervals, tracking performance degrades.

## Foundational Learning

- **Concept: Receding Horizon Control (Model Predictive Control)**
  - Why needed here: The entire trajectory generation framework builds on RH/MPC, requiring understanding of prediction windows, recursive optimization, and the trade-off between horizon length and computational tractability.
  - Quick check question: Can you explain why limiting the prediction window to 8Δt prevents the controller from having an "unfair advantage" over humans?

- **Concept: Constrained Quadratic Optimization**
  - Why needed here: The controller solves a quadratic program with equality/inequality constraints at each time step; understanding how penalty terms relate to soft constraints is essential for tuning λ₁ and λ₂.
  - Quick check question: How does the penalty term μ₁ differ from a hard constraint requiring |vₓ| = |vᵧ|?

- **Concept: Human Motor Behavior in Rhythmic Tasks**
  - Why needed here: The three human features (direction preference, velocity magnitude, sparsity) are derived from motor control literature; understanding timing preferences (1:1, 1:2 ratios) helps interpret why diagonal directions are favored.
  - Quick check question: Why might biomechanical constraints cause temporal sparsity even when faster control updates would improve tracking?

## Architecture Onboarding

- **Component map:**
  Human Data Analyzer -> RH Controller Core -> Event Trigger -> Noise Injector -> State Tracker

- **Critical path:**
  1. Collect human free-navigation and tracking data (Section 2)
  2. Fit Gaussian to velocity magnitude distribution, compute empirical CDF of inter-action intervals
  3. Initialize RH controller with human-derived penalties and constraints
  4. At each time step: check event trigger → if true, solve QP → apply first control action → add noise → update state
  5. Compare generated trajectories to human and baseline using MSE, velocity histograms, and magnitude KDEs

- **Design tradeoffs:**
  - Prediction window length (W=8Δt): Shorter windows reduce computation but may miss long-term trajectory features; longer windows risk outperforming human predictive capabilities
  - Penalty weights (λ₁=32, λ₂=0.6): Higher λ₁ enforces diagonal directions more strictly but may reduce tracking accuracy; λ₂ tuning balances speed naturalism vs. tracking
  - Constraint vs. penalty formulation: Spatial sparsity uses hard constraints (cleaner separation), while direction/magnitude use soft penalties (more flexibility)

- **Failure signatures:**
  - Trajectory tracks perfectly but velocity histogram lacks diagonal peaks: λ₁ too low
  - Controller never updates control action: Event trigger threshold too restrictive or CDF sampling issue
  - MSE distribution similar to baseline (impulsive at zero): Human-like features not being enforced; check constraint/penalty implementation
  - Trajectory drifts outside screen bounds: State constraints not properly enforced in optimizer

- **First 3 experiments:**
  1. **Baseline validation:** Run RH controller with only kinematic constraints (no penalties, no sparsity); verify near-perfect tracking and impulsive MSE at zero
  2. **Feature ablation:** Enable one human feature at a time (direction only → magnitude only → sparsity only) to isolate each feature's contribution to trajectory naturalism
  3. **Parameter sensitivity:** Vary λ₁ and λ₂ systematically around tuned values; quantify trade-off between human-likeness metrics (histogram similarity) and tracking performance (MSE distribution)

## Open Questions the Paper Calls Out
None

## Limitations
- The assumption that human motor behavior features transfer from free navigation to tracking tasks needs further validation
- Parameter tuning (λ₁, λ₂, σ_ε) appears empirical rather than systematic, potentially limiting reproducibility
- Evaluation lacks statistical significance testing and comparison to alternative human-like trajectory generation methods

## Confidence

- **High confidence:** The receding horizon control framework implementation and baseline optimal control comparison are technically sound and well-specified.
- **Medium confidence:** The identification of human behavioral features (direction preferences, magnitude distributions, sparsity patterns) is supported by data analysis, though the transferability assumption needs further validation.
- **Low confidence:** The empirical tuning of controller parameters and the claim that human-inspired penalties "generate more human-like trajectories" lack systematic validation and statistical comparison to alternatives.

## Next Checks

1. Conduct ablation studies varying λ₁ and λ₂ systematically to quantify the trade-off between human-likeness (velocity histogram similarity) and tracking performance (MSE), providing parameter sensitivity analysis.

2. Compare the human-inspired controller against alternative trajectory generation methods (e.g., reinforcement learning with human demonstrations, trajectory optimization with action quantization) using statistical tests on velocity distributions and tracking accuracy.

3. Validate the task-independence assumption by collecting human tracking data with different reference trajectory characteristics (varying speed, curvature, complexity) and testing whether the same human features transfer across conditions.