---
ver: rpa2
title: 'Beyond Facts: Evaluating Intent Hallucination in Large Language Models'
arxiv_id: '2506.06539'
source_url: https://arxiv.org/abs/2506.06539
tags:
- intent
- query
- hallucination
- constraint
- constraints
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces intent hallucination as a distinct type of
  LLM error, occurring when models omit or misinterpret parts of complex queries despite
  factual accuracy. To address this, the authors create FAITH QA, a benchmark with
  20,068 human-validated queries spanning fact QA, creative writing, response evaluation,
  and content analysis, both with and without retrieval-augmented generation.
---

# Beyond Facts: Evaluating Intent Hallucination in Large Language Models

## Quick Facts
- **arXiv ID**: 2506.06539
- **Source URL**: https://arxiv.org/abs/2506.06539
- **Reference count**: 38
- **Primary result**: Introduces intent hallucination as a distinct LLM error type and proposes CONSTRAINT SCORE, which shows significantly better correlation with human judgment than LLM-as-the-judge baselines

## Executive Summary
This paper identifies intent hallucination as a distinct failure mode in large language models, occurring when models omit or misinterpret parts of complex queries despite factual accuracy. To address this gap, the authors create FAITH QA, a comprehensive benchmark with 20,068 human-validated queries spanning fact QA, creative writing, response evaluation, and content analysis, both with and without retrieval-augmented generation. They propose CONSTRAINT SCORE, a novel evaluation metric that decomposes queries into intent constraints with importance weights, and demonstrate through human evaluation that it correlates significantly better with human judgment than existing LLM-as-the-judge baselines.

## Method Summary
The method introduces CONSTRAINT SCORE, which evaluates LLM responses by decomposing queries into atomic "intent constraints" using semantic role labeling across seven categories (location, time, subject, action, qualifiers, quantity). Each constraint is classified as mandatory, important, or optional with weights 3, 2, and 1 respectively. For each response, a satisfaction function evaluates whether each constraint is met, and the final score is computed as the weighted ratio of satisfied constraints normalized to 0-10. The approach is validated on FAITH QA, a benchmark with 20,068 queries across four tasks including fact QA, creative writing, response evaluation, and content analysis in RAG settings.

## Key Results
- Human evaluation shows CONSTRAINT SCORE is significantly closer to human judgment than LLM-as-the-judge baselines (MSE of 0.50 vs 4.72)
- 66.3% of CONSTRAINT SCORE evaluations are within one standard deviation of human scores
- Even state-of-the-art models exhibit notable intent hallucination rates that increase with query complexity
- Models tend to omit or misinterpret key subjects and actions most frequently
- In RAG settings, models often invent missing content rather than refusing to answer

## Why This Works (Mechanism)

### Mechanism 1: Intent Constraint Decomposition
Breaking complex queries into atomic "intent constraints" enables fine-grained detection of alignment failures that holistic evaluation misses. The mapping function C(q) extracts constraints via semantic role labeling and categorizes them into mandatory, important, and optional sets, where each constraint represents a single requirement the response must satisfy.

### Mechanism 2: Importance-Weighted Satisfaction Scoring
Weighting constraints by priority (mandatory=3, important=2, optional=1) produces scores that correlate more closely with human judgment than unweighted or LLM-as-judge baselines. The satisfaction function Sφ(c, y) evaluates each constraint, and the final Constraint Score normalizes satisfied weight by total weight and scales to 0-10.

### Mechanism 3: Omission vs. Misinterpretation Diagnosis
Intent hallucination manifests as two distinct failure modes—omission (ignoring constraints) and misinterpretation (hallucinating constraints)—requiring different detection strategies. The approach compares the implicit constraint set Ĉ(q) the model conditioned on against the explicit C(q), with omission occurring when constraints are dropped and misinterpretation when constraints are added or altered.

## Foundational Learning

- **Concept**: Semantic Role Labeling (SRL)
  - Why needed here: Constraint extraction uses SRL concepts (subject, action, context) to decompose queries; understanding these roles is prerequisite to implementing the mapping function
  - Quick check question: Given "List three European explorers who circumnavigated the globe before the 18th century," can you identify subject, action, and qualifier constraints?

- **Concept**: Weighted Evaluation Metrics
  - Why needed here: Constraint Score uses normalized weighted sums; understanding how weight choices affect rankings is critical for interpreting results and tuning α values
  - Quick check question: If mandatory weight increases from 3 to 5 while optional stays at 1, how does the score distribution change for a response satisfying 1/2 mandatory and 2/2 optional constraints?

- **Concept**: Retrieval-Augmented Generation (RAG) Failure Modes
  - Why needed here: The misinterpretation dataset tests whether models detect missing content in RAG settings; understanding "lost in the middle" and context-processing limitations explains observed failure patterns
  - Quick check question: When an LLM is given Articles 1 and 2 but Article 3 is missing, why might it invent Article 3 content rather than refuse to answer?

## Architecture Onboarding

- **Component map**: Constraint Extractor (LLM-based) -> Satisfaction Evaluator (LLM-based) -> Score Aggregator -> Human Alignment Validator
- **Critical path**: Constraint extraction quality → satisfaction evaluation accuracy → weight calibration. Errors in extraction propagate through all downstream components
- **Design tradeoffs**:
  - Granularity vs. cost: More fine-grained constraints improve detection but increase LLM calls
  - Weight fixed vs. learned: Fixed weights (3:2:1) are simple but may not generalize; learned weights require labeled human preference data
  - Single vs. multi-pass extraction: Single-pass is efficient but may miss implicit constraints; multi-pass improves recall
- **Failure signatures**:
  1. Over-decomposition: Simple queries fractured into too many constraints, diluting important ones
  2. Subject/action blind spots: Models frequently violate subjects and actions
  3. RAG missing content: Models invent rather than refuse when content is missing
  4. Famous-subject bias: Models satisfy constraints for well-known entities while violating constraints for obscure ones
- **First 3 experiments**:
  1. Weight sensitivity analysis: Vary α values on held-out human-labeled set to validate fixed-weight assumption
  2. Per-category error profiling: Run Constraint Score on 100 samples each from Fact QA, Creative Writing, and RAG tasks to identify highest violation rates
  3. Baseline comparison with calibrated LLM-judge: Compare Constraint Score against self-consistency LLM-judge on 150-sample test set to verify decomposition overhead is justified

## Open Questions the Paper Calls Out
- Can layer-level analysis detect intent hallucination formation earlier and more accurately than output-level evaluation?
- Do reasoning-focused models (e.g., o1, DeepSeek-R1) exhibit lower intent hallucination rates through explicit chain-of-thought?
- What mechanisms cause LLMs to prefer famous entities even when they violate explicit constraints?
- Why do models handle fine-grained constraints (location, time, quantity) better than core semantic elements (subjects, actions)?

## Limitations
- Fixed-weight assumption (3:2:1) without empirical validation across domains
- Human evaluation sample represents only 5% of the dataset, potentially missing edge cases
- LLM-as-judge baseline comparison may not capture all variations of intent hallucination detection methods

## Confidence
- **High confidence**: The mechanism of constraint decomposition for detecting intent misalignment is well-supported by theoretical foundations and empirical results
- **Medium confidence**: The 3:2:1 weighting scheme produces better results than baseline, but optimal weights likely vary by task domain
- **Low confidence**: The taxonomy of omission vs. misinterpretation as distinct failure modes lacks empirical validation that different mitigation strategies are warranted

## Next Checks
1. **Weight calibration study**: Systematically vary α values on a held-out human-labeled subset of FAITH QA to determine if the current fixed weights are optimal
2. **Cross-domain generalization test**: Apply CONSTRAINT SCORE to an external benchmark (e.g., HellaSwag or ARC) with minimal prompt adaptation
3. **Mitigation effectiveness evaluation**: Implement and test targeted mitigations for the most frequently violated constraint categories (subjects and actions) and measure whether error rates decrease more than baseline fine-tuning