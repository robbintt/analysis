---
ver: rpa2
title: Dynamic Context-Aware Streaming Pretrained Language Model For Inverse Text
  Normalization
arxiv_id: '2505.24229'
source_url: https://arxiv.org/abs/2505.24229
tags:
- streaming
- context
- language
- text
- chunk
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of integrating streaming Inverse
  Text Normalization (ITN) into streaming Automatic Speech Recognition (ASR) systems.
  The authors propose a streaming pretrained language model for ITN that leverages
  the PhoBERT model and introduces Dynamic Context-Aware training and inference to
  handle variable chunk sizes and incorporate right-context information.
---

# Dynamic Context-Aware Streaming Pretrained Language Model For Inverse Text Normalization

## Quick Facts
- arXiv ID: 2505.24229
- Source URL: https://arxiv.org/abs/2505.24229
- Authors: Luong Ho; Khanh Le; Vinh Pham; Bao Nguyen; Tan Tran; Duc Chau
- Reference count: 0
- One-line primary result: Streaming ITN model achieves 78% F1-score and 8.24% I-WER with ≤6.85ms latency

## Executive Summary
This paper addresses the challenge of integrating streaming Inverse Text Normalization (ITN) into streaming Automatic Speech Recognition (ASR) systems. The authors propose a streaming pretrained language model for ITN that leverages the PhoBERT model and introduces Dynamic Context-Aware training and inference to handle variable chunk sizes and incorporate right-context information. Their method achieves an F1-score of 78% and I-WER of 8.24, outperforming existing streaming ITN models while maintaining low latency (≤6.85ms for 5-word chunks), making it suitable for real-time integration into ASR systems.

## Method Summary
The method employs a two-phase approach combining Tagging with WFST Transduction. A pretrained PhoBERT encoder processes subword-tokenized input with word-level positional encoding, while only the first subword per word receives IOB labels (the rest are masked with -inf). The model uses dynamic context masking during training with variable chunk sizes {3,4,5,6,7} and right context {1,2} to simulate inference conditions. A right-context buffer retains incomplete words at chunk boundaries for re-correction with subsequent chunks. The system outputs Number-Case tags (4 categories) and Punctuation tags (4 classes) which are then converted to written form via WFST grammar.

## Key Results
- Achieves 78% F1-score and 8.24% I-WER on streaming ITN task
- Maintains low latency of ≤6.85ms for 5-word chunks
- Outperforms existing streaming ITN models on Vietnamese ASR data
- Number-Case categories show strong performance (F1 up to 0.89) while punctuation recognition improves significantly in streaming mode

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Dynamic context masking during training improves streaming inference accuracy by aligning train-inference conditions.
- Mechanism: During training, the model is exposed to variable chunk sizes {3,4,5,6,7} and right context sizes {1,2} through attention masking. This simulates the partial-information conditions encountered at inference, reducing distribution shift.
- Core assumption: The model can generalize across context configurations if trained with sufficient coverage of chunk/right-context combinations.
- Evidence anchors:
  - [abstract] "Dynamic Context-Aware during training and inference, enabling adaptive chunk size adjustments and the integration of right-context information"
  - [section 2.5] "we adopt the Dynamic Right Context masking technique... simulating the availability of right context during inference"
  - [corpus] Limited direct corpus support; neighbor papers focus on streaming ASR architectures but not specifically on dynamic masking for ITN.
- Break condition: If inference chunk sizes fall outside training ranges, or if right-context latency budget is violated, performance may degrade.

### Mechanism 2
- Claim: Pretrained language model representations improve tagging accuracy in low-data streaming scenarios.
- Mechanism: PhoBERT (Vietnamese BERT) provides pre-learned linguistic representations, enabling better contextual understanding for IOB tagging despite limited in-domain training data (40K samples). The model is fine-tuned with multitask heads for Number-Case and Punctuation.
- Core assumption: Pretrained representations transfer effectively to streaming ITN tagging despite domain/task shift.
- Evidence anchors:
  - [abstract] "streaming pretrained language model for ITN, leveraging pretrained linguistic representations for improved robustness"
  - [section 3.3] "F1-score of NS2 and S2 surpassed that of NS1 and S1 by 14% and 9% under identical settings"
  - [corpus] Neighbor paper "Universal-2-TF" uses all-neural text formatting with similar pretraining concepts, supporting transfer learning benefits for ITN-adjacent tasks.
- Break condition: If target domain vocabulary/structures diverge significantly from pretraining corpus (20GB Vietnamese text), benefits may diminish.

### Mechanism 3
- Claim: Right-context buffer with revision corrects incomplete-word errors at chunk boundaries.
- Mechanism: ASR outputs may produce partial words at boundaries (e.g., "Vincom o" | "cean park"). A buffer retains the final incomplete word and reprocesses it with the next chunk, enabling correction ("ocean" → "Ocean").
- Core assumption: Incomplete words appear only at chunk boundaries and can be corrected with 1-2 words of right context.
- Evidence anchors:
  - [abstract] "integration of right-context information"
  - [section 2.5] "At inference, a right context buffer is utilized to facilitate revision... enabling re-correction with the additional context"
  - [corpus] No direct corpus support for this specific buffer mechanism in ITN; related streaming ASR papers address context but not word-boundary revision.
- Break condition: If ASR tokenization differs significantly from ITN tokenization, or if multi-word spans are split across chunks, buffer logic may fail.

## Foundational Learning

- Concept: **IOB (Inside-Outside-Beginning) Tagging**
  - Why needed here: The model outputs per-token tags (B/I/O) to mark spans for Number-Case categories before WFST transduction. Understanding label structure is essential for debugging and error analysis.
  - Quick check question: Given "ba triệu hai," which tokens would receive B/I/O tags for the Number category?

- Concept: **Streaming vs. Non-Streaming ITN Latency Constraints**
  - Why needed here: Streaming ITN must process each chunk within the ASR chunk duration (e.g., 400ms). Latency budget determines feasible chunk sizes and right-context length.
  - Quick check question: If ASR produces chunks every 400ms and ITN latency is 6.85ms, what is the maximum right-context buffer delay before violating real-time constraints?

- Concept: **Subword Tokenization with Word-Level Labeling**
  - Why needed here: BPE tokenization splits words into subwords, but IOB tags apply at word level. Only the first subword receives the label; others are masked to avoid interference.
  - Quick check question: For "welcome" tokenized as ["wel", "come"], which token index receives the punctuation tag if followed by a comma?

## Architecture Onboarding

- Component map:
  - Input Layer: Subword tokenizer (PhoBERT BPE) → Positional encoding (word-level shared)
  - Encoder: PhoBERT (12 layers, 768 hidden, 12 heads) with Dynamic Context-Aware Attention Masking
  - Multi-Task Heads: (1) Number-Case classifier (4 classes); (2) Punctuation classifier (4 classes)
  - Transduction: WFST grammar converts tagged spans to written form
  - Postprocess: Punctuation insertion based on predicted tags
  - Buffer: Right-context buffer for incomplete-word revision

- Critical path:
  1. Receive ASR chunk (variable word count, possibly incomplete final word)
  2. Concatenate buffered word from previous chunk
  3. Tokenize with BPE, apply word-level positional encoding
  4. Apply dynamic attention mask (left context=16, chunk=variable, right=1-2)
  5. Forward through PhoBERT → classification heads
  6. WFST transduction for Number-Case spans
  7. Punctuation insertion
  8. Buffer final word for next chunk

- Design tradeoffs:
  - Larger right context → better accuracy but higher latency
  - Fixed chunk size → simpler pipeline but mismatch with variable ASR output
  - Pretrained model → improved accuracy but increased model size (135M params) vs. smaller custom models

- Failure signatures:
  - Phone number F1 drops sharply with limited context (0.48→0.56→0.70 vs. 0.89 full context) — indicates context-dependent categories need careful chunk sizing
  - Question/Exclamation punctuation underperforms in streaming (0.19→0.58) — suggests discourse-level cues missing in short context
  - Incomplete word artifacts ("Vincom oCean park") — buffer logic not triggered correctly

- First 3 experiments:
  1. **Baseline streaming ITN**: Train PhoBERT without dynamic masking, fixed chunk=5, right-context=0. Measure F1, I-WER, latency.
  2. **Dynamic masking ablation**: Add dynamic context masking during training with variable chunks {3-7} and right-context {1,2}. Compare F1 delta.
  3. **Right-context buffer validation**: Construct test cases with intentional word-boundary splits. Measure correction rate and latency overhead.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the specific performance drops observed in phone number recognition and complex punctuation be recovered without violating real-time latency constraints?
- Basis in paper: [explicit] The authors note in the results section (Tables 3 and 4) that the Phone and Punctuation (specifically Question mark) categories suffer significant F1-score decreases (e.g., Phone drops from 0.89 to 0.70) compared to non-streaming models, attributing this to the need for "discourse-level information" or "subsequent digits" found in right context.
- Why unresolved: The current implementation limits right context to 1-2 words, which appears insufficient for these specific dependencies, yet increasing context length increases latency.
- What evidence would resolve it: An ablation study analyzing the accuracy-latency trade-off curve when incrementally increasing right-context sizes specifically for these difficult categories.

### Open Question 2
- Question: Does the Dynamic Context-Aware training strategy generalize effectively to languages with different morphological structures or multilingual Pretrained Language Models?
- Basis in paper: [inferred] The paper relies exclusively on PhoBERT and a Vietnamese dataset. While the method is theoretically model-agnostic, the experiments are confined to a single analytic language using a specific monolingual PLM.
- Why unresolved: It is unclear if the dynamic masking technique handles agglutinative or fusional languages (where word boundaries are less distinct) as effectively as it handles Vietnamese.
- What evidence would resolve it: Experiments applying this architecture to multilingual models (e.g., mBERT) or datasets in languages like German or Finnish.

### Open Question 3
- Question: How robust is the proposed "incomplete word" buffering mechanism when processing ASR outputs with high Word Error Rates (WER)?
- Basis in paper: [inferred] Section 2.5 details a mechanism to handle incomplete words at chunk boundaries, but the dataset description focuses on manually annotated, "trustworthy" samples without mentioning synthetic ASR noise or error injection.
- Why unresolved: The buffering strategy relies on concatenating word fragments; if the ASR misrecognizes a fragment (substitution error) rather than splitting it (fragmentation), the re-correction logic may fail.
- What evidence would resolve it: Evaluating the I-WER and NI-WER of the streaming model when the input is perturbed with varying levels of synthetic ASR substitution errors.

## Limitations
- Proprietary evaluation datasets prevent independent verification of results
- Limited evidence for right-context buffer mechanism beyond single anecdotal example
- Weak performance on discourse-level punctuation in streaming mode (0.19→0.58 F1)

## Confidence
- **High Confidence**: The baseline architecture using PhoBERT encoder with multi-task classification heads is technically sound and well-supported by the pretrained model's capabilities. The overall system design and latency measurements are credible.
- **Medium Confidence**: The Dynamic Context-Aware training methodology shows theoretical validity, but the specific implementation details and their contribution to the performance gains remain unclear without code or more detailed experimental analysis.
- **Low Confidence**: The evaluation metrics and comparisons rely entirely on proprietary datasets, making independent validation impossible. The single anecdotal example of the right-context buffer fixing "Vincom o/cean" provides minimal evidence for the mechanism's general effectiveness.

## Next Checks
1. **Implementation Reproduction**: Recreate the Dynamic Context-Aware attention masking system using publicly available Vietnamese text to verify the training methodology and measure performance on open datasets like VTB (Vietnamese Treebank) or other standard NLP benchmarks.
2. **Right-Context Buffer Testing**: Construct a comprehensive test suite with 100+ word-boundary split cases across different categories (numbers, dates, locations) to empirically measure the buffer mechanism's correction accuracy and latency overhead.
3. **Cross-Lingual Generalization**: Evaluate the streaming ITN system on non-Vietnamese languages (e.g., English or Thai) using pretrained multilingual models like mBERT or XLM-R to assess the method's language independence and identify potential architectural limitations.