---
ver: rpa2
title: A Task-Efficient Reinforcement Learning Task-Motion Planner for Safe Human-Robot
  Cooperation
arxiv_id: '2510.12477'
source_url: https://arxiv.org/abs/2510.12477
tags:
- task
- planner
- motion
- human
- replanning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a hybrid reinforcement learning task-motion
  planner for safe human-robot cooperation. The system combines a re-RRT motion planner
  that reacts to human arm motions by checking and replanning trajectories when needed,
  and a reinforcement learning task planner that learns to select tasks in less congested
  areas to minimize failures and replanning.
---

# A Task-Efficient Reinforcement Learning Task-Motion Planner for Safe Human-Robot Cooperation

## Quick Facts
- arXiv ID: 2510.12477
- Source URL: https://arxiv.org/abs/2510.12477
- Reference count: 34
- Primary result: Hybrid RL task-motion planner reduces task failures and replanning requests in human-robot cooperation compared to baseline strategies.

## Executive Summary
This paper proposes a hybrid reinforcement learning task-motion planner for safe human-robot cooperation. The system combines a re-RRT* motion planner that reacts to human arm motions by checking and replanning trajectories when needed, and a reinforcement learning task planner that learns to select tasks in less congested areas to minimize failures and replanning. The RL agent receives rewards based on task success, collision avoidance, replanning frequency, and distance to assigned tasks. The method is tested in simulation and real-world experiments using a Panda robot, showing that the RL task planner can reduce task failures and replanning requests compared to random, logical, or sequential picking strategies. The approach improves both safety and efficiency in dynamic human-robot environments.

## Method Summary
The method employs a hierarchical task-motion planning framework where an RL agent (trained with PPO) selects goal positions in the workspace, and a re-RRT* motion planner executes the motion while performing predictive collision checks on the current trajectory. The RL agent observes a 10x10x3 feature matrix encoding task and human arm positions, and outputs a 2D goal position. The motion planner checks the planned trajectory for potential collisions and only triggers full replanning when necessary. The RL reward function encourages successful task completion, collision avoidance, and minimizing replanning frequency. The system is trained in simulation and transferred to a real Panda robot setup with human tracking via ZED2 camera.

## Key Results
- The re-RRT* motion planner reduces computational overhead compared to fixed-frequency replanning while maintaining collision avoidance.
- The RL task planner learns to select goals in less congested areas, reducing task failures and replanning requests compared to random, logical, or sequential strategies.
- Real-world experiments demonstrate successful deployment on a Panda robot with human tracking, showing improved safety and efficiency metrics.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Conditional replanning reduces computational overhead while maintaining collision avoidance.
- Mechanism: The re-RRT* algorithm performs dense collision checking (approx. 2.5x10^-5 s per check) on the current trajectory waypoints and only triggers a full RRT* replan when a future waypoint is predicted to collide, rather than replanning at a fixed frequency.
- Core assumption: Human arm motions evolve continuously enough that predictive collision checking on the current planned path provides sufficient warning time for replanning without requiring continuous trajectory regeneration.
- Evidence anchors:
  - [abstract] "re-RRT* motion planner that reacts to human arm motions by checking and replanning trajectories when needed"
  - [section] Page 3, Algorithm 1: Shows the loop structure where `Collision Check` is called on waypoints, and `Replan` is only called if `waypoint collision=True`. The paper states "the collision checking function can respond in 2.5Â·10^-5s" and "essence of re-RRT* is that we use dense checking to avoid dense replanning."
  - [corpus] No direct corpus evidence for re-RRT* specifically; corpus papers focus on general adaptive planning (e.g., "Reactive and Safety-Aware Path Replanning" addresses reactivity but not this specific conditional mechanism).
- Break condition: If human motions become highly erratic (non-continuous), predictive waypoint checking may fail to provide adequate replanning time, causing collisions or excessive emergency stops.

### Mechanism 2
- Claim: RL-driven task selection improves long-term efficiency by learning to avoid statistically congested workspace areas.
- Mechanism: The RL agent (PPO) receives a state representation (10x10x3 feature matrix of tasks and human arm positions) and outputs a continuous 2D goal position. The reward function penalizes task failures, collisions, and replanning frequency, while rewarding task completion. By maximizing cumulative reward, the agent learns a policy that maps observed human workspace occupation to goal positions that are likely to be accessible and require fewer reactive replans.
- Core assumption: Human arm occupancy in the workspace follows learnable spatial patterns (e.g., modeled as Gaussian distributions in training), allowing the RL agent to generalize a statistical avoidance strategy.
- Evidence anchors:
  - [abstract] "RL task planner that learns to select tasks in less congested areas to minimize failures and replanning"
  - [section] Page 4, Equation (1) and text: "the agent will be encouraged to plan goal positions that (1) are collision free; (2) need less replanning to achieve the task." Page 5, Task Setting: "human arms appear in the workspace following a 2D... Gaussian distribution."
  - [corpus] "ContactRL: Safe Reinforcement Learning based Motion Planning" provides supporting evidence that RL can incorporate safety (contact) into rewards for HRC, but does not address the specific task-space congestion learning mechanism here.
- Break condition: If human motion patterns deviate significantly from the training distribution (e.g., uniformly random across the entire workspace), the learned statistical avoidance policy may degrade, though the paper suggests the RL planner is more robust than fixed strategies (Fig. 11).

### Mechanism 3
- Claim: A hierarchical feedback loop between task and motion planners co-optimizes for safety and efficiency.
- Mechanism: The RL task planner proposes a goal, which is executed by the re-RRT* motion planner. The motion planner's performance metrics (success, replanning count) are fed back as components of the RL reward. This creates a coupled system where the high-level planner learns to propose goals that are "easy" for the low-level planner to execute safely.
- Core assumption: The reward signal generated by the motion planner (replanning count, success) is a sufficiently proxy for the overall system efficiency and safety objectives.
- Evidence anchors:
  - [abstract] "hybrid reinforcement learning task-motion planner... combines a re-RRT* motion planner... and a reinforcement learning task planner"
  - [section] Page 2, Fig. 2 description: "The motion planner evaluates the task planner's decisions by giving rewards, which also helps the RL agent to improve the ability to make better decisions."
  - [corpus] "Digital-Twin Evaluation for Proactive Human-Robot Collision Avoidance via Prediction-Guided A-RRT*" supports the idea of prediction-guided planning, but does not cover the specific bidirectional reward-feedback loop between task and motion layers.
- Break condition: If the reward weights (alpha_0, alpha_1, etc.) are poorly tuned, the agent may optimize for a proxy that does not align with actual safety or efficiency (e.g., avoiding all replanning by refusing tasks).

## Foundational Learning

- Concept: Markov Decision Process (MDP) formulation.
  - Why needed here: The paper explicitly frames the task planning problem as an MDP (S, A, R, T, O, gamma). Understanding that the state is the workspace grid, action is a 2D goal, and reward is a composite signal is essential to grasp how the RL agent learns.
  - Quick check question: If the workspace were represented as a list of raw task coordinates instead of a fixed-size feature matrix, what problem would this cause for the RL agent's policy network?

- Concept: Sampling-based motion planning (RRT*).
  - Why needed here: The low-level planner is re-RRT*. One must understand that RRT* builds a tree of collision-free paths through joint space and that replanning involves re-growing this tree, which is computationally intensive.
  - Quick check question: Why does the paper argue that "dense replanning" at a fixed frequency degrades both path quality (trajectory length) and planning success rate?

- Concept: Policy Gradient methods (PPO).
  - Why needed here: The task planner is trained using PPO. Understanding that PPO is an on-policy, actor-critic method that uses a clipped surrogate objective for stable updates helps explain why it was chosen for this stochastic and potentially noisy environment.
  - Quick check question: What property of PPO makes it more suitable than off-policy methods (like DQN) for this specific task-motion planning problem, given the authors' justification?

## Architecture Onboarding

- Component map:
  - RL Task Planner (High-Level) -> re-RRT* Motion Planner (Low-Level) -> Environment -> Reward Calculator -> RL Task Planner

- Critical path: The critical data path for learning is: Environment State -> Feature Matrix Encoding -> RL Policy (PPO) -> Goal Position -> Task Assignment -> re-RRT* Execution -> Outcome & Metrics -> Reward Calculation -> Policy Update. For inference, the loop stops at execution.

- Design tradeoffs:
  1. **Abstraction vs. Precision:** The observation space is a coarse 10x10 grid. This reduces dimensionality and aids learning but loses precise geometric information, which is then handled by the motion planner. This is a standard TAMP tradeoff.
  2. **Fixed vs. Dynamic Action Space:** Using continuous 2D coordinates and assigning the nearest task avoids a varying discrete action space, enabling generalization to different numbers of tasks.
  3. **Proactive vs. Reactive Safety:** The system relies on a proactive task planner to avoid congestion and a reactive motion planner to handle dynamic obstacles. The tradeoff is the complexity of training the RL agent versus implementing a purely reactive system.

- Failure signatures:
  1. **Task oscillation:** The robot repeatedly picks and fails the same task because the RL agent's policy has learned a local optimum where it directs the robot to a congested area.
  2. **Replanning loops:** The re-RRT* planner gets stuck in a loop of constant replanning without making progress, indicating either highly erratic human motion or a poor path from the planner.
  3. **Sparse reward failure:** The RL agent learns to avoid all tasks to prevent collisions (a conservative policy), resulting in a very low task completion rate.

- First 3 experiments:
  1. **re-RRT* Baseline:** Implement the re-RRT* algorithm with a fixed goal. Validate that it reduces replanning frequency compared to a fixed-frequency RRT* replanner (replicate Fig. 3 results).
  2. **Reward Function Ablation:** Train the RL task planner with different weights (e.g., setting alpha_2 = 0 to ignore replanning). Analyze the resulting policy to confirm it learns to avoid tasks in areas with high replanning costs.
  3. **Sim-to-Real Transfer Test:** Train the agent in the Gazebo simulation with the specified human arm Gaussian distribution. Deploy the trained policy to the real Panda robot setup and measure the degradation (if any) in task failure rate and replanning count.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the reinforcement learning task planner be effectively integrated with logic programming to handle complex task dependencies?
- **Basis in paper:** [explicit] The conclusion states, "for the future work, the RL planner can be combined with logic programming to solve the more complex situations," noting that "no logical task sequence is considered" in the current work.
- **Why unresolved:** The current implementation assumes homogeneous tasks (picking blocks) without sequential constraints, limiting applicability in assembly scenarios requiring strict operation orders.
- **What evidence would resolve it:** Demonstration of the framework successfully managing tasks with preconditions and sequential dependencies (e.g., "pick A before placing B") without a drop in safety or efficiency.

### Open Question 2
- **Question:** How does the framework perform in environments where task objects are clustered or piled rather than scattered?
- **Basis in paper:** [explicit] In Section IV-A, the authors explicitly state: "Clustered and piled blocks are out of the scope of this paper."
- **Why unresolved:** The current task assignment logic relies on nearest-neighbor selection and accessibility checks that may fail or create deadlocks if objects are entangled or occluded.
- **What evidence would resolve it:** Successful task completion rates and collision metrics in simulation scenarios specifically designed with dense object clusters.

### Open Question 3
- **Question:** Does the policy trained on statistical human motion models generalize to realistic, goal-oriented human trajectories?
- **Basis in paper:** [inferred] Section IV-A notes the assumption that human arms follow a 2D Gaussian distribution with uniform mean sampling. The paper does not validate the policy against goal-directed human behaviors (e.g., a human reaching specifically for the same object as the robot).
- **Why unresolved:** A policy optimized for statistical "congestion" avoidance may fail against intentional, adversarial, or predictive human movements common in real shared workspaces.
- **What evidence would resolve it:** Comparative trials where the human agent follows intent-based trajectories (e.g., reaching for specific targets) rather than random or Gaussian-distributed movements.

## Limitations
- The approach assumes human arm motions follow Gaussian distributions, which may not generalize to real-world unpredictable human behavior.
- Critical hyperparameters (reward weights, human motion parameters) are not specified, making exact reproduction difficult.
- Evaluation focuses on relative improvements over baselines without providing absolute success rates for practical assessment.

## Confidence

- **High**: The core mechanism of conditional replanning (re-RRT*) reducing computational overhead is well-supported by the algorithm description and timing data.
- **Medium**: The RL task planner's ability to learn congestion avoidance is supported by the reward structure and simulation results, but the generalization to real human behavior remains uncertain.
- **Low**: The scalability of the approach to more complex tasks or larger workspaces is not addressed, as the evaluation is limited to a specific pick-and-place scenario.

## Next Checks

1. Conduct a sensitivity analysis on the reward weights to determine their impact on the learned policy's safety and efficiency tradeoffs.
2. Test the trained policy against non-Gaussian human motion patterns (e.g., uniform random or clustered distributions) to assess robustness.
3. Implement the complete system (both re-RRT* and RL planner) and measure absolute task success rates and replanning frequencies in a controlled environment with human participants.