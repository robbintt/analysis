---
ver: rpa2
title: Augmented Fine-Tuned LLMs for Enhanced Recruitment Automation
arxiv_id: '2509.06196'
source_url: https://arxiv.org/abs/2509.06196
tags:
- recruitment
- llms
- score
- resumes
- fine-tuned
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a novel approach to recruitment automation
  through fine-tuning Large Language Models (LLMs) on domain-specific data. The methodology
  combines real-world resumes with synthetic data, all standardized in JSON format,
  to create a hybrid dataset.
---

# Augmented Fine-Tuned LLMs for Enhanced Recruitment Automation

## Quick Facts
- arXiv ID: 2509.06196
- Source URL: https://arxiv.org/abs/2509.06196
- Reference count: 24
- Fine-tuned Phi-4 achieved F1=90.62%, outperforming base models by 27.7%

## Executive Summary
This paper presents a novel approach to recruitment automation through fine-tuning Large Language Models (LLMs) on domain-specific data. The methodology combines real-world resumes with synthetic data, all standardized in JSON format, to create a hybrid dataset. Multiple state-of-the-art open-weight models (LLaMA3.1 8B, Mistral 7B, Phi-4 14B, and Gemma2 9B) were fine-tuned using parameter-efficient techniques, achieving significant improvements over base models. The fine-tuned Phi-4 model demonstrated the highest F1 score of 90.62%, with substantial gains across evaluation metrics including BLEU (47.58%) and ROUGE (69.95%) scores. The results demonstrate that domain-specific fine-tuning enhances resume parsing accuracy and candidate-job matching precision, with Phi-4 emerging as the optimal model for deployment in recruitment workflows due to its balance of performance and computational efficiency.

## Method Summary
The study fine-tuned four open-weight LLMs (LLaMA3.1 8B, Mistral 7B, Phi-4 14B, and Gemma2 9B) using LoRA (rank=16, α=16) on a hybrid dataset of real and synthetic resumes. Real resumes (2,400 from Kaggle) were parsed to JSON using DeepSeek, combined with synthetic resumes generated by the same model, then normalized and split 80/10/10. Models were trained with batch_size=8, lr=5e-5, max_steps=200, and evaluated on Exact Match (Levenshtein), F1 (SBERT), BLEU-4, and ROUGE metrics.

## Key Results
- Phi-4 achieved the highest F1 score of 90.62% among all fine-tuned models
- Exact Match improved from 58.35% (base) to 81.83% (fine-tuned Phi-4)
- BLEU score improved 142.5% over base models after fine-tuning
- All fine-tuned models showed substantial improvements over base models across all metrics

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Domain-specific fine-tuning on recruitment data improves structured extraction accuracy over generic base models.
- Mechanism: Fine-tuning adapts model weights via LoRA to recognize resume-specific patterns (e.g., skills, experience sections) and map them to a standardized JSON schema, reducing hallucination and improving entity alignment.
- Core assumption: The hybrid dataset (real + synthetic resumes) sufficiently represents the distribution of resume formats and terminology encountered in deployment.
- Evidence anchors:
  - [abstract]: Fine-tuned Phi-4 achieved F1=90.62% vs. base 70.95% (+27.7%); BLEU improved 142.5% over base.
  - [section V]: Table I shows all fine-tuned models improved Exact Match (e.g., Phi-4: 58.35% → 81.83%); base models struggled with field extraction consistency.
  - [corpus]: Weak—neighbor papers (MSLEF, MLAR) are related but uncited (0 citations); no external validation of this specific claim.
- Break condition: If test resumes differ significantly from training distribution (e.g., new industries, non-standard formats), extraction accuracy may degrade below reported metrics.

### Mechanism 2
- Claim: Hybrid datasets combining real-world and synthetic resumes improve model generalization and edge-case coverage.
- Mechanism: Real resumes (2,400 from Kaggle, 24 professions) parsed by DeepSeek into JSON provide realistic patterns; synthetic resumes fill gaps by simulating diverse scenarios; normalization unifies date formats and skill terminology before training.
- Core assumption: DeepSeek (236B parameters) produces sufficiently accurate JSON labels to serve as ground truth for training; synthetic data quality matches real-world variability.
- Evidence anchors:
  - [section III-B]: Dataset created via "real-world resumes parsed using DeepSeek" + "synthetic resumes generated using DeepSeek model," normalized and split 80/10/10.
  - [section III-B]: Hybrid approach "ensures both realism of actual recruitment data and coverage of edge cases."
  - [corpus]: No direct external validation of hybrid data efficacy in neighbor papers; mechanism remains unverified beyond this study.
- Break condition: If DeepSeek parsing errors propagate into training labels, or if synthetic resumes lack realistic variation, model may overfit to artifacts rather than true extraction patterns.

### Mechanism 3
- Claim: LoRA-based parameter-efficient fine-tuning achieves strong performance gains with reduced computational cost.
- Mechanism: LoRA (rank=16, α=16) updates only low-rank adapters on attention projections (q/k/v/o_proj), preserving base knowledge while adapting to recruitment domain; training uses batch_size=8, lr=5e-5, 200 steps.
- Core assumption: LoRA rank/alpha settings generalize across model families (LLaMA, Mistral, Phi-4, Gemma); 200 training steps sufficient for convergence.
- Evidence anchors:
  - [section III-C]: Algorithm 2 specifies LoRA parameters and training hyperparameters applied to all four models.
  - [section VI]: LoRA-based adaptation cited as enabling "practical balance between accuracy and computational cost" for Phi-4 deployment.
  - [corpus]: Weak—no neighbor papers validate these specific LoRA configurations for recruitment tasks.
- Break condition: If deployment requires finer-grained adaptations (e.g., new entity types), current LoRA rank may be insufficient; hyperparameters may need re-tuning per model family.

## Foundational Learning

- Concept: **LoRA (Low-Rank Adaptation)**
  - Why needed here: Core technique for efficient fine-tuning; understanding rank/alpha trade-offs is critical for reproducing results or adapting to new domains.
  - Quick check question: If you increase LoRA rank from 16 to 32, what happens to trainable parameter count and risk of overfitting?

- Concept: **Structured Extraction Evaluation (BLEU/ROUGE/F1/Exact Match)**
  - Why needed here: Paper uses composite metrics to assess both n-gram overlap and semantic similarity; misinterpreting these can lead to wrong conclusions about model quality.
  - Quick check question: Why might a model score high on BLEU but low on semantic F1 (SBERT-based)?

- Concept: **JSON Schema Standardization for NLP Outputs**
  - Why needed here: Paper enforces strict JSON schema for resume fields; schema design directly impacts model learnability and downstream integration.
  - Quick check question: What happens if training JSON schemas are inconsistent (e.g., optional vs. required fields vary)?

## Architecture Onboarding

- Component map:
  PDFMiner -> DeepSeek parsing -> Synthetic generation -> Normalization -> Train/val/test splitter -> LoRA fine-tuning -> Evaluation

- Critical path:
  1. Data preparation (real resume parsing + synthetic generation → normalized JSON)
  2. LoRA configuration and training (200 steps, validation monitoring)
  3. Test set evaluation across all metrics
  4. Model selection based on F1/Exact Match trade-offs

- Design tradeoffs:
  - Phi-4 (14B): Highest F1 (90.62%) but larger inference footprint
  - LLaMA3.1 (8B): Best Exact Match (82.05%), smaller size
  - Mistral/Gemma: Lower F1 after fine-tuning (possible overfit to base semantic patterns)
  - Choice depends on deployment constraints (latency vs. accuracy)

- Failure signatures:
  - Base models outperforming fine-tuned on F1 (e.g., Gemma2: 87.40% base vs. 75.66% fine-tuned) suggests over-regularization or insufficient training steps
  - Low BLEU/ROUGE with high F1 may indicate semantic correctness but poor text fluency
  - Inconsistent JSON field population suggests schema misalignment or insufficient training diversity

- First 3 experiments:
  1. Reproduce Phi-4 fine-tuning with identical LoRA config; verify F1 ≥90% on held-out test split
  2. Ablation: Train on real-only vs. synthetic-only vs. hybrid data; measure impact on Exact Match and edge-case handling
  3. Hyperparameter sweep: Test LoRA rank [8, 16, 32] and training steps [100, 200, 500] on LLaMA3.1; identify compute/performance optimal point

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does reliance on DeepSeek-generated ground truth introduce systematic errors or hallucinations into the fine-tuning dataset that smaller models might amplify?
- Basis in paper: [inferred] The methodology uses a large-parameter model (DeepSeek) to parse real resumes into JSON "high-quality reference data" without human verification, assuming the teacher model is accurate.
- Why unresolved: The paper measures how well student models (Phi-4, LLaMA) learned the DeepSeek output, but it does not validate if the DeepSeek outputs themselves were factually correct relative to the raw PDF text.
- What evidence would resolve it: A human evaluation study comparing DeepSeek-generated JSON labels against manual annotations to quantify label noise and hallucination rates in the training data.

### Open Question 2
- Question: Do improvements in resume parsing metrics (F1, BLEU) directly translate to higher accuracy in downstream candidate-job matching?
- Basis in paper: [inferred] The paper claims the methodology will "revolutionize recruitment workflows by providing more accurate candidate-job matching," yet the evaluation relies entirely on extraction metrics (Exact Match, ROUGE) rather than retrieval or ranking performance.
- Why unresolved: High performance in structuring data into JSON does not guarantee that the semantic embeddings derived from that data are better at matching a candidate to a specific job description.
- What evidence would resolve it: Benchmarking the fine-tuned models on a candidate ranking task, measuring Mean Reciprocal Rank (MRR) or NDCG against a set of job descriptions.

### Open Question 3
- Question: How does domain-specific fine-tuning impact the fairness and demographic bias of the recruitment models?
- Basis in paper: [inferred] The conclusion highlights the potential for developing "equitable" systems, but the experimental design focuses solely on accuracy and efficiency using a dataset that may reflect historical hiring biases.
- Why unresolved: The paper does not analyze whether the fine-tuning process on the Kaggle dataset amplifies biases related to gender, ethnicity, or institution names present in the training data.
- What evidence would resolve it: A fairness audit measuring the selection rates of the fine-tuned models across different demographic subgroups to ensure compliance with equitable AI standards.

## Limitations

- Limited external validation of hybrid dataset approach and LoRA configurations for recruitment tasks
- Potential propagation of DeepSeek parsing errors into training labels without human verification
- No analysis of model performance on resumes from industries outside the original 24-category dataset

## Confidence

- High confidence in Phi-4 achieving F1=90.62% based on reported test results
- Medium confidence in hybrid dataset generalization claims due to lack of external validation
- Low confidence in LoRA rank=16/α=16 optimality across model families without sensitivity analysis

## Next Checks

1. External Dataset Test: Evaluate fine-tuned Phi-4 on held-out resume dataset from different source (e.g., Indeed, LinkedIn)
2. DeepSeek Parsing Accuracy Audit: Manually review random sample of DeepSeek-parsed resumes to quantify error rates
3. LoRA Hyperparameter Sweep: Conduct systematic ablation study varying LoRA rank and training steps on LLaMA3.1