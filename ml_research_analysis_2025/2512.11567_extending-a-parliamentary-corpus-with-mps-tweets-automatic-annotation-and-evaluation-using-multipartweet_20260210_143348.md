---
ver: rpa2
title: 'Extending a Parliamentary Corpus with MPs'' Tweets: Automatic Annotation and
  Evaluation Using MultiParTweet'
arxiv_id: '2512.11567'
source_url: https://arxiv.org/abs/2512.11567
tags:
- tweets
- media
- classification
- each
- corpus
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MultiParTweet integrates social media discourse from German politicians
  with parliamentary records to enable comparative analysis of political communication.
  It contains 39,546 tweets (19,056 with media) from 2024-2025, linked to the GerParCor
  corpus.
---

# Extending a Parliamentary Corpus with MPs' Tweets: Automatic Annotation and Evaluation Using MultiParTweet

## Quick Facts
- arXiv ID: 2512.11567
- Source URL: https://arxiv.org/abs/2512.11567
- Reference count: 0
- Primary result: MultiParTweet integrates German politicians' tweets with parliamentary records for comparative political discourse analysis

## Executive Summary
MultiParTweet extends the GerParCor parliamentary corpus by integrating 39,546 German politicians' tweets from November 2024 to February 2025, with 19,056 containing media content. The dataset was automatically annotated using nine text-based models and one vision-language model for emotion, sentiment, and topic classification. Manual evaluation of 103 tweets revealed human preference for media-based annotations over text-only models, suggesting multimodal representations better align with human interpretation. The corpus and collection tool TTLABTweetCrawler are released under AGPL for reproducible political discourse research.

## Method Summary
The study employed TTLABTweetCrawler to collect tweets via X API v2 using two methods: user timelines and media-specific searches. Tweets were linked to GerParCor using cosine similarity with paraphrase embeddings. Automatic annotation was performed using DUUI pipeline with nine text models and one VLM for media. Analysis included cross-model predictability using Random Forest classifiers and evolutionary feature selection (DEAP) to reduce model inputs by ~45% without performance loss. Manual annotation of 103 tweets evaluated model alignment with human judgment.

## Key Results
- Manual evaluation showed human preference for media-based annotations (42-49%) over text-only models (23-34%) across classification types
- Cross-model predictability achieved 99% F1 for sentiment, with average 65% Macro F1 across all models
- Evolutionary feature selection reduced model inputs by ~45% while maintaining performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Vision-language model annotations align more closely with human judgment than text-only models for multimodal political tweets
- Mechanism: VLMs process both visual and textual cues simultaneously, capturing semantic meaning from images/videos that text-only models miss
- Core assumption: Human interpretation of political social media relies on integrated multimodal understanding
- Evidence anchors: Human annotators preferred media model results 42-49% vs text model 23-34%; manual sample limited to 103 tweets (0.26% of corpus)

### Mechanism 2
- Claim: Classification outputs across emotion, sentiment, and topic models are mutually predictable
- Mechanism: Models capture overlapping semantic features that can be exploited to predict any single model from others
- Core assumption: Model outputs capture task-relevant signal rather than purely idiosyncratic noise
- Evidence anchors: Random Forest classifiers achieved 65% average Macro F1 (99% for sentiment); inter-annotator agreement suggests shared latent structure

### Mechanism 3
- Claim: Evolutionary feature selection can reduce annotation model inputs by approximately 45% without performance loss
- Mechanism: Evolutionary search identifies minimal feature subsets that preserve predictive power by evaluating combinations
- Core assumption: Feature importance is consistent across the feature space
- Evidence anchors: Selected features averaged ~55% of available parameters; SHAP analysis showed 40% overlap between top-20 features

## Foundational Learning

- Concept: Multimodal fusion in political NLP
  - Why needed here: Political communication increasingly relies on images/videos to convey emotion and framing that text alone cannot capture
  - Quick check question: Can you explain why sentiment analysis on tweet text alone might misclassify a sarcastic image-caption pair?

- Concept: Cross-model agreement and annotation quality
  - Why needed here: With no gold standard for political tweet interpretation, researchers must evaluate annotation quality through model comparison and human validation
  - Quick check question: What does Krippendorff's alpha of 0.33 for sentiment model preference tell you about annotation reliability?

- Concept: Feature selection vs. model performance trade-offs
  - Why needed here: Computational efficiency matters at scale when processing large political corpora
  - Quick check question: If SHAP importance and selection frequency disagree on top features, what might that indicate about feature interactions?

## Architecture Onboarding

- Component map: TTLABTweetCrawler -> X API v2 retrieval -> media download -> DUUI annotation (parallel text/media) -> aggregation -> GerParCor linkage
- Critical path: Tweet collection → media download → text cleaning → DUUI annotation (parallel text/media) → aggregation → GerParCor linking
- Design tradeoffs: Tweet IDs only (data rights limits); manual evaluation limited to 103 tweets; user-timeline method yields mixed content; media-search ensures media but may miss political content
- Failure signatures: 340 media files broken at retrieval; 13 files failed during runtime; 1,678 tweets excluded; low inter-rater reliability (α = 0.33-0.47)
- First 3 experiments: 1) Reproduce pipeline on 500 manually annotated tweets to validate VLM preference claim at scale; 2) Test cross-prediction on held-out temporal split; 3) Benchmark feature-selected models on out-of-distribution samples

## Open Questions the Paper Calls Out

- Question: Does scaling up manual annotation beyond 103 tweets confirm the human preference for VLM-based annotations over text-only models?
  - Basis in paper: Explicit statement that current manually labelled dataset is limited
  - Why unresolved: Sample size of 103 tweets (0.26% of corpus) is too small for generalization
  - What evidence would resolve it: Larger-scale manual annotation (1,000+ tweets) showing consistent patterns

- Question: What is the optimal method for integrating text-based and media-based classification outputs?
  - Basis in paper: Inferred from separate evaluation of text and media models without joint approaches
  - Why unresolved: Paper does not test combined multimodal classifiers
  - What evidence would resolve it: Experiments comparing fusion strategies against single-modality baselines

- Question: What is the optimal cosine similarity threshold for linking tweets to parliamentary speeches?
  - Basis in paper: Inferred from use of "predefined threshold" without justification
  - Why unresolved: Threshold selection impacts quality of tweet-speech pairs
  - What evidence would resolve it: Parameter sweep evaluating precision/recall at different thresholds

## Limitations
- Manual annotation sample size of 103 tweets (0.26% of corpus) limits generalizability of VLM preference claims
- Three-month data collection period may miss temporal variations in political discourse
- X API v2 rate limits and data access restrictions affect reproducibility
- Low inter-rater reliability for sentiment and emotion tasks (Krippendorff's alpha 0.33-0.47) indicates task difficulty

## Confidence
- **High Confidence**: Sentiment model predictability (99% F1), feature selection reduction (~45% inputs), basic annotation pipeline architecture
- **Medium Confidence**: VLM preference over text models, cross-model predictability (65% average F1), temporal stability of annotations
- **Low Confidence**: Multimodal representation alignment claims, generalizability to other political contexts or languages, specific feature importance rankings

## Next Checks
1. Scale manual annotation to 1,000+ tweets with multiple annotators to verify VLM preference and establish inter-rater reliability thresholds
2. Test cross-prediction performance on temporally separated data (train on Nov-Dec 2024, test on Mar-Apr 2025) to assess model stability over time
3. Benchmark reduced feature sets on out-of-distribution political content (crisis events, non-German politicians) to validate generalization claims