---
ver: rpa2
title: 'Memo-SQL: Structured Decomposition and Experience-Driven Self-Correction for
  Training-Free NL2SQL'
arxiv_id: '2601.10011'
source_url: https://arxiv.org/abs/2601.10011
tags:
- arxiv
- correction
- query
- error
- memo-sql
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'Memo-SQL introduces structured decomposition and experience-driven
  self-correction for training-free NL2SQL. It addresses two key weaknesses: arbitrary
  decomposition leading to low SQL candidate diversity, and static in-context learning
  overlooking historical error-correction patterns.'
---

# Memo-SQL: Structured Decomposition and Experience-Driven Self-Correction for Training-Free NL2SQL

## Quick Facts
- arXiv ID: 2601.10011
- Source URL: https://arxiv.org/abs/2601.10011
- Authors: Zerui Yang; Weichuan Wang; Yanwei Xu; Linqi Song; Yudai Matsuda; Wei Han; Bo Bai
- Reference count: 25
- Primary result: 68.5% execution accuracy on BIRD, setting a new state of the art for training-free NL2SQL

## Executive Summary
Memo-SQL introduces a training-free NL2SQL pipeline that combines structured decomposition and experience-driven self-correction. The method addresses two key weaknesses in prior training-free approaches: arbitrary decomposition leading to low SQL candidate diversity, and static in-context learning overlooking historical error-correction patterns. Memo-SQL uses three semantic decomposition strategies—entity-wise, hierarchical, and atomic sequential—to encourage diverse reasoning. It also builds a dynamic error-correction memory, retrieving relevant failure-fix pairs at inference time to guide self-correction via retrieval-augmented prompting. Evaluated on BIRD, Memo-SQL achieves 68.5% execution accuracy, setting a new state of the art among open, training-free methods. It uses over 10× fewer resources than prior TTS approaches, demonstrating strong efficiency-accuracy balance. The method generalizes well to Spider and CHESS-SDS datasets, highlighting its robustness and practical deployment potential in enterprise BI systems.

## Method Summary
Memo-SQL operates in two phases: offline memory construction and online inference. During the offline phase, it processes the BIRD training set to build an error-correction memory containing quintuples of (question, error, fix, diagnosis) across 9 predefined error types using GTE-base embeddings. The online inference pipeline applies three structured decomposition strategies to the input question, generates SQL candidates using a ReAct+Reflection loop for each sub-question, synthesizes three syntactically diverse SQL candidates (CTE, JOIN, nested), retrieves relevant error-fix pairs from memory, and applies a critic-refine loop (max 3 iterations) before selecting the final answer via self-consistency voting. The system uses open-source LLMs like Qwen2.5-Coder-32B-Instruct as the base model and supports multiple model variants for robustness evaluation.

## Key Results
- Achieves 68.5% execution accuracy on BIRD dev-new, outperforming all training-free baselines
- Maintains 86.5% accuracy on Spider with only 0.7% improvement from correction, demonstrating strong cross-dataset generalization
- Reduces resource consumption by over 10× compared to Alpha-SQL while maintaining competitive accuracy
- Shows consistent improvement across multiple model variants including Qwen2.5-Coder-32B, Qwen3-Coder-30B, and DeepSeek-Coder-V2-Lite

## Why This Works (Mechanism)

### Mechanism 1
Enforcing semantically distinct decomposition strategies (entity-wise, hierarchical, atomic) increases SQL candidate diversity, which improves the reliability of majority voting compared to stochastic sampling. Rather than relying on the LLM's inherent, often repetitive decomposition logic, the system forces three specific reasoning paths. This structural variance produces uncorrelated errors, making it statistically more likely that the correct answer emerges via self-consistency scoring.

### Mechanism 2
Retrieving historical error-fix pairs at inference time provides more actionable signal for correction than retrieving only correct examples or using static prompts. The system constructs a memory of quintuples (question, error, fix, diagnosis). When a new SQL candidate is generated, the system embeds it, retrieves similar past failures from the memory, and uses these specific "how-to-fix" patterns as in-context examples to guide the refinement agent.

### Mechanism 3
Generating candidates in multiple syntactic styles (CTE, JOIN, Nested) before refinement allows the system to bypass model biases toward specific SQL structures. After the decomposition phase, the system synthesizes the final query using three distinct syntactic templates. This prevents the model from getting stuck in a "local minimum" of a single incorrect SQL structure.

## Foundational Learning

- **In-Context Learning (ICL)**: Memo-SQL relies entirely on ICL for both generation and correction. It does not update weights; it learns by retrieving relevant examples (shots) from the error memory and inserting them into the prompt. *Quick check*: How does the model's behavior change if the retrieved examples are irrelevant to the current SQL error?

- **Self-Consistency / Majority Voting**: The architecture generates 9 candidates (3 decompositions × 3 styles). The final selection depends on voting. Understanding how diversity affects voting reliability is crucial. *Quick check*: Why does this method prefer structured diversity over high-temperature sampling for generating voting candidates?

- **ReAct (Reason + Act) Pattern**: The SQL generation step uses a ReAct+Reflection loop where the model generates SQL, executes it, observes results, and reflects on errors. This iterative loop is where the "self-correction" begins before the memory retrieval step. *Quick check*: At what point does the ReAct loop stop, and the external memory-based correction take over?

## Architecture Onboarding

- **Component map**: Schema Linking -> Structured Decomposer (3 strategies) -> ReAct SQL Generator (per sub-question) -> Multi-Style Synthesizer (CTE/JOIN/Nested) -> Refiner (Retrieves from Error Memory -> Critic -> Refine) -> Selector (Majority Voting)

- **Critical path**: The Refiner component. If the retrieval mechanism fails to find relevant error-fix pairs, the system defaults to the base model's reasoning capability, losing the "experience-driven" advantage.

- **Design tradeoffs**: Latency vs. Accuracy (multiple LLM calls per query), Memory Quality vs. Coverage (memory built on model-generated errors may not cover real-world edge cases)

- **Failure signatures**: Empty Results Loop (consistently generates queries returning empty sets), Hallucinated Fixes (critic misdiagnoses error leading to wrong fix), Stochastic Similarity (all three paths converge on same wrong logic despite structured decomposition)

- **First 3 experiments**:
  1. Ablation on Decomposition: Run the pipeline using only one decomposition strategy at a time vs. the full ensemble to verify the diversity contribution
  2. Memory Retrieval Analysis: Test the Refiner with "Random RAG" (randomly sampling history) vs. "Semantic RAG" to prove that retrieval relevance matters
  3. Efficiency Boundary: Measure latency per query on the CHESS-SDS subset to confirm the >10x token reduction claim against the Alpha-SQL baseline implementation

## Open Questions the Paper Calls Out

### Open Question 1
How well does synthetic error-correction memory transfer to real-world user error patterns? The error-correction memory is constructed from model-generated errors on BIRD training data, not from actual user interactions. The distribution gap between synthetic and real user errors remains unquantified. A study comparing correction effectiveness using synthetic vs. real user error logs from deployed BI systems would resolve this.

### Open Question 2
Why does retrieval-augmented ICL during final SQL generation degrade performance for Qwen3-Coder-30B while improving other models? The paper reports this degradation only with the Qwen3-Coder-30B model and attributes it to stylistic mismatch but does not fully explain the model-specific behavior. Systematic analysis of token distributions and attention patterns across models with varying ICL configurations would help resolve this.

### Open Question 3
Can the Memo-SQL pipeline achieve sub-second latency for interactive dashboard applications? The current pipeline (145s/query on CHESS-SDS) is optimized for accuracy-efficiency balance, not ultra-low-latency. A latency-focused ablation identifying which components can be parallelized, cached, or skipped without significant accuracy loss would address this.

### Open Question 4
Why is the SQL correction module less effective on simpler queries (Spider) compared to complex queries (BIRD)? Cross-dataset results show only 0.7% improvement on Spider from correction versus 2.1% on BIRD. The paper attributes this to "narrower headroom" but does not investigate whether simpler queries produce error types underrepresented in the memory bank. Analysis of retrieved correction examples and error-type coverage when applying BIRD-trained memory to Spider queries would resolve this.

## Limitations
- The error-correction memory is built on synthetic errors generated by the LLM on training data, which may not fully represent real-world user error patterns
- The memory coverage is limited to 9 predefined error types, potentially missing domain-specific or novel failure modes
- Reliance on open-source models (Qwen2.5-Coder-32B, Qwen3-Coder-30B-A3B-Instruct) introduces uncertainty about whether performance generalizes to other model families or smaller variants

## Confidence

- **High Confidence**: The structured decomposition strategies improve SQL candidate diversity (supported by ablation showing 0.9% drop when removed, and mechanism aligns with LearNAT findings on task decomposition benefits)
- **Medium Confidence**: The error-correction memory improves self-correction accuracy (based on comparative results in Table 3, though dependent on synthetic memory quality)
- **Low Confidence**: The exact prompt templates and retrieval filtering thresholds are not specified, limiting precise replication

## Next Checks

1. **Decomposition Diversity Validation**: Run the pipeline with only one decomposition strategy at a time versus the full ensemble on BIRD dev to verify the claimed contribution of structured diversity to ensemble accuracy gains

2. **Memory Retrieval Efficacy**: Compare the full retrieval-based correction pipeline against a "Random RAG" baseline (randomly sampling historical pairs) to isolate the effect of semantic relevance in the two-stage filtering process

3. **Efficiency Benchmarking**: Measure end-to-end latency per query on CHESS-SDS using the specified Qwen2.5-Coder-32B model and compare against the claimed >10x improvement over Alpha-SQL, ensuring retrieval and correction iteration limits are enforced