---
ver: rpa2
title: 'No Clustering, No Routing: How Transformers Actually Process Rare Tokens'
arxiv_id: '2509.04479'
source_url: https://arxiv.org/abs/2509.04479
tags:
- rare
- plateau
- neurons
- tokens
- attention
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates how transformers process rare tokens, addressing
  a key challenge in language model interpretability. While prior work identified
  specialized "plateau" neurons for rare tokens, their organizational principles remained
  unclear.
---

# No Clustering, No Routing: How Transformers Actually Process Rare Tokens

## Quick Facts
- **arXiv ID**: 2509.04479
- **Source URL**: https://arxiv.org/abs/2509.04479
- **Reference count**: 18
- **Primary result**: Rare token specialization emerges through distributed, training-driven differentiation rather than architectural modularity, with specialist neurons spatially distributed across MLP layers accessed through universal attention patterns.

## Executive Summary
This paper investigates how transformers process rare tokens, addressing a key challenge in language model interpretability. While prior work identified specialized "plateau" neurons for rare tokens, their organizational principles remained unclear. The authors conducted comprehensive analyses across GPT-2 XL and Pythia models, examining neuron influence patterns, spatial organization through graph-based clustering, and attention routing mechanisms. They compared rare and common token processing to understand whether specialized architectures are required.

The key finding is that rare token specialization emerges through distributed, training-driven differentiation rather than architectural modularity. Rare tokens recruit additional high-influence plateau neurons beyond what's needed for common tokens, establishing dual computational regimes. Critically, these specialist neurons are spatially distributed across the MLP layer rather than forming discrete clusters, and they're accessed through universal attention patterns without selective routing. This distributed organization challenges assumptions about modular circuit design in neural networks.

## Method Summary
The authors conducted comprehensive analyses across GPT-2 XL and Pythia models, examining neuron influence patterns, spatial organization through graph-based clustering, and attention routing mechanisms. They compared rare and common token processing to understand whether specialized architectures are required. The methodology involved measuring neuron influence on output logits, applying graph-based clustering algorithms to identify spatial organization patterns, and analyzing attention weight distributions to detect routing mechanisms.

## Key Results
- Rare tokens recruit additional high-influence plateau neurons beyond what's needed for common tokens, establishing dual computational regimes
- Specialist neurons for rare token processing are spatially distributed across MLP layers rather than forming discrete clusters
- Attention patterns show universal access without selective routing, contradicting assumptions about modular circuit design

## Why This Works (Mechanism)
The distributed specialization mechanism emerges from training dynamics where the model adapts shared parameters to handle both common and rare tokens. Instead of creating dedicated clusters, the model recruits additional high-influence neurons as needed for rare tokens while maintaining universal attention patterns. This approach preserves context-sensitive flexibility while effectively handling low-frequency, semantically critical tokens.

## Foundational Learning
- **Plateau neurons**: Specialized neurons that activate strongly for rare tokens; needed because rare tokens require distinct computational patterns
- **Graph-based clustering**: Algorithm for identifying spatial organization in neural networks; quick check: apply multiple clustering methods with varying parameters
- **Universal attention patterns**: Attention mechanisms that access all relevant neurons without selective routing; needed to maintain flexibility across token types
- **MLP layer distribution**: The organization of neurons within multi-layer perceptrons; quick check: verify distribution consistency across model architectures
- **Dual computational regimes**: Separate processing pathways for common versus rare tokens; needed to handle frequency-based computational demands

## Architecture Onboarding
- **Component map**: Input -> Embedding -> Attention -> MLP Layer (plateau neurons distributed) -> Output
- **Critical path**: Token embedding flows through attention mechanism to distributed plateau neurons in MLP layer for specialized processing
- **Design tradeoffs**: Distributed specialization preserves flexibility but may sacrifice efficiency compared to modular clustering approaches
- **Failure signatures**: Loss of rare token handling capability, degraded performance on low-frequency semantic concepts
- **First experiments**: 1) Compare neuron influence patterns between rare and common tokens 2) Apply clustering algorithms to test for spatial organization 3) Analyze attention weight distributions for routing patterns

## Open Questions the Paper Calls Out
None

## Limitations
- Spatial analysis relies on graph-based clustering metrics that may miss subtle organizational patterns
- Analysis focuses primarily on GPT-2 XL and Pythia models, limiting generalizability
- Attention routing analysis doesn't rule out more sophisticated routing mechanisms operating at different scales

## Confidence
- High confidence: Observation that rare tokens recruit additional high-influence plateau neurons
- Medium confidence: Claim that specialist neurons are spatially distributed rather than clustered
- Medium confidence: Assertion of universal attention patterns without selective routing

## Next Checks
1. Apply multiple clustering algorithms with varying distance metrics and resolution parameters to verify distributed organization findings
2. Test whether distributed specialization pattern holds across diverse transformer architectures (BERT, T5, Llama) and scales
3. Conduct targeted interventions (neuron ablation, activation engineering) to empirically demonstrate necessity and sufficiency of identified distributed neurons