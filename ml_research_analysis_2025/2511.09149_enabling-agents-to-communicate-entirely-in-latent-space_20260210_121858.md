---
ver: rpa2
title: Enabling Agents to Communicate Entirely in Latent Space
arxiv_id: '2511.09149'
source_url: https://arxiv.org/abs/2511.09149
tags:
- latent
- communication
- reasoning
- arxiv
- actor
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "Interlat enables direct communication between large language model\
  \ agents in latent space, bypassing the constraints of natural language. Instead\
  \ of transmitting discrete tokens, agents exchange their last hidden states\u2014\
  continuous representations of internal reasoning\u2014processed through a communication\
  \ adapter."
---

# Enabling Agents to Communicate Entirely in Latent Space

## Quick Facts
- arXiv ID: 2511.09149
- Source URL: https://arxiv.org/abs/2511.09149
- Authors: Zhuoyun Du; Runze Wang; Huiyu Bai; Zouying Cao; Xiaoyong Zhu; Yu Cheng; Bo Zheng; Wei Chen; Haochao Ying
- Reference count: 40
- Primary result: Latent communication improves multi-step planning success rates vs. language-only methods while enabling 24× latency reduction via learned compression

## Executive Summary
This paper introduces Interlat, a method enabling direct communication between large language model agents in latent space rather than natural language. Instead of transmitting discrete tokens, agents exchange their last hidden states—continuous representations of internal reasoning—processed through a communication adapter. A separate learned compression mechanism allows these latents to be condensed while preserving task-relevant information. Experiments on multi-step household planning tasks show that latent communication improves success rates and promotes exploratory behavior compared to both fine-tuned chain-of-thought prompting and language-only communication.

## Method Summary
The method transmits last hidden states directly between agents, bypassing the token bottleneck of language communication. An actor agent generates its plan, extracts the sequence of last-layer hidden states H ∈ R^L×d, and wraps them with boundary tokens <bop>/<eop> for transmission. The receiver concatenates these latents into its input embedding stack and processes them through a lightweight communication adapter (multi-head attention + projection) to interpret the latents. Training uses a two-stage approach: first training the actor with a combination of task loss, separation loss (to distinguish matched from mismatched latents), and alignment loss (to regularize against language-space plans), with curriculum learning for stability. Second, a separate reasoning model is trained to compress latents while preserving task-critical information through autoregressive latent generation.

## Key Results
- Latent communication improves multi-step household planning success rates compared to both fine-tuned chain-of-thought prompting and language-only communication
- Learned compression achieves 24× latency reduction (0.20s vs 9.19s) while maintaining strong performance (66.43% vs 70.48% seen tasks)
- Cross-family transmission succeeds (Qwen→LLaMA) and the method generalizes to symbolic reasoning tasks, outperforming language-based communication on the most challenging problems

## Why This Works (Mechanism)

### Mechanism 1: Direct Latent Transmission Bypasses Token Bottleneck
Transmitting last hidden states (≈40k bits/state) preserves richer information than discrete tokens (≈15 bits/token), enabling retention of parallel reasoning hypotheses. The sender extracts its last-layer hidden states and transmits them directly, while the receiver uses a communication adapter to interpret these latents in its own embedding space.

### Mechanism 2: Contrastive Separation + Alignment Training Enables Latent Interpretation
A supervised objective combining distributional separation against mismatched latents and alignment to language-space plans teaches the receiver to exploit task-relevant latent structure. This training scheme creates a useful gradient signal while preventing degenerate solutions.

### Mechanism 3: Learned Compression via Autoregressive Latent Reasoning
A separate reasoning model can generate compressed latent sequences (K ≪ L steps) that preserve task-critical information by learning an information-preserving bottleneck. This compression maintains geometric consistency and preserves parallel hypotheses through broader probability distributions.

## Foundational Learning

- **Transformer Hidden States as Semantic Representations**
  - Why needed here: The entire method hinges on extracting and interpreting last-layer hidden states
  - Quick check question: Can you explain why the last hidden state before token prediction contains more information than the predicted token itself?

- **KL Divergence and Jensen-Shannon Divergence for Distribution Alignment**
  - Why needed here: The separation loss (JS divergence) and alignment loss (KL divergence) are core to training
  - Quick check question: What happens to JS divergence when two distributions are identical vs. maximally different? (Answer: 0 vs. ln(2))

- **Autoregressive Generation with Latent Feedback**
  - Why needed here: The compression mechanism relies on feeding generated latents back as input embeddings
  - Quick check question: How does the compression model maintain consistency when generating K ≪ L steps from L original latents?

## Architecture Onboarding

**Component Map**: Sender Actor -> Communication Adapter -> Receiver Actor -> Environment

**Critical Path**: Sender extracts H → wraps with boundaries → transmits → Receiver concatenates E → adapter processes → interprets → generates actions

**Design Tradeoffs**: Direct latent transmission vs. language communication (information density vs. interpretability), learned compression vs. full latents (latency vs. fidelity), curriculum learning vs. direct training (stability vs. efficiency)

**Failure Signatures**: 
- Flat separation loss (~0.69) indicates curriculum issues
- Near-zero success rates suggest missing or broken adapter
- Compressed latents collapsing to single hypothesis indicates L_geom issues

**3 First Experiments**:
1. Test basic latent transmission without compression on a simple planning task
2. Validate communication adapter by training receiver on latents from the same agent family
3. Evaluate compressed latents on a reduced task set to verify the 24× speedup claim

## Open Questions the Paper Calls Out
None

## Limitations
- Method depends heavily on access to CoT trajectories for training, limiting real-world applicability
- Cross-family transmission demonstrated only within similar architectural families, not across fundamentally different models
- Compression mechanism introduces additional complexity with autoregressive reasoning model and multiple auxiliary losses

## Confidence

**High Confidence (4/5)** - Latent communication fundamentally works: The core claim that transmitting last hidden states improves success rates is well-supported by multiple ablations and cross-family experiments.

**Medium Confidence (3/5)** - Compression preserves task-relevant information: While trained compression achieves impressive speedup, the analysis relies heavily on proxy metrics rather than direct interpretability.

**Medium Confidence (3/5)** - Cross-agent latent communication generalizes: The Qwen→LLaMA experiments show successful transfer within similar architectures, but claims about broader generalization lack validation.

## Next Checks

1. **Cross-architecture validation**: Test latent communication between fundamentally different model families (e.g., encoder-decoder models transmitting to decoder-only models, or multimodal models) to establish whether the method exploits semantic structure rather than model-specific artifacts.

2. **Zero-shot latent transfer**: Evaluate whether a receiver trained on one task distribution can successfully interpret latents from unseen tasks or domains to test the claim that latents encode generalizable reasoning structure.

3. **Latent interpretability analysis**: Conduct direct probing of compressed latents to identify what semantic information they preserve using techniques like feature visualization, linear probing for task-relevant concepts, or comparison against human-interpretable abstractions.