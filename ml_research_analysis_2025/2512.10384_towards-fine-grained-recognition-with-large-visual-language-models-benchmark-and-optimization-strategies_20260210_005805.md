---
ver: rpa2
title: 'Towards Fine-Grained Recognition with Large Visual Language Models: Benchmark
  and Optimization Strategies'
arxiv_id: '2512.10384'
source_url: https://arxiv.org/abs/2512.10384
tags:
- data
- fine-grained
- recognition
- answer
- accuracy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the gap in fine-grained recognition evaluation
  for Large Vision-Language Models (LVLMs) by introducing the Fine-grained Recognition
  Open World (FROW) benchmark. It proposes optimization strategies including mosaic
  data construction, open-world data integration, and training process improvements.
---

# Towards Fine-Grained Recognition with Large Visual Language Models: Benchmark and Optimization Strategies

## Quick Facts
- **arXiv ID:** 2512.10384
- **Source URL:** https://arxiv.org/abs/2512.10384
- **Reference count:** 40
- **Primary result:** Introduces FROW benchmark and shows mosaic data improves category recognition accuracy by 1%, open-world data boosts accuracy by 10%-20% and content accuracy by 6%-12%

## Executive Summary
This paper addresses the critical gap in fine-grained visual recognition evaluation for Large Vision-Language Models (LVLMs) by introducing the Fine-grained Recognition Open World (FROW) benchmark. The benchmark exposes LVLMs' poor performance on open-ended questions about subordinate visual categories compared to multiple-choice formats. The authors propose three optimization strategies: mosaic data construction, open-world data integration, and strategic placement of fine-grained data during the alignment stage rather than only in supervised fine-tuning. Experimental results demonstrate significant improvements in both recognition accuracy and factual content generation for fine-grained visual concepts.

## Method Summary
The paper constructs the FROW benchmark using six fine-grained visual datasets (aircraft, birds, dogs, flowers, food, vegetables/fruits) and evaluates models using open-ended questions answered by GPT-4o-mini. Three optimization strategies are proposed: mosaic data creation through 3x3 image collages to accelerate convergence, open-world data generation by pairing images with Wikipedia-derived factual questions, and shifting fine-grained data from the SFT stage to the alignment stage. The method uses InternVL-2.5-8B as the base model, mixing general caption data with the constructed fine-grained datasets during training. The evaluation measures both recognition accuracy (0-2 scale for category precision) and content accuracy (0-3 scale for factual correctness).

## Key Results
- Mosaic data improves category recognition accuracy by 1% compared to standard data repetition
- Open-world data integration boosts FROW benchmark accuracy by 10%-20% and content accuracy by 6%-12%
- Incorporating fine-grained data during alignment stage improves category recognition accuracy by up to 10%
- Models trained only on fine-grained data suffer catastrophic forgetting of general capabilities
- Open-ended FROW benchmark reveals much lower LVLM performance compared to multiple-choice formats

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Mosaic data construction accelerates convergence and improves fine-grained recognition accuracy compared to standard data repetition.
- Mechanism: By stitching multiple images into a single input, the model is forced to localize and differentiate distinct visual concepts simultaneously. This acts as a hard attention mechanism, preventing the model from overfitting to a single central object and improving its ability to handle multiple scales and contexts.
- Core assumption: The visual encoder and attention layers can resolve fine details when forced to process multiple small regions in a single forward pass.
- Evidence anchors:
  - [abstract] Mentions mosaic data improves category recognition accuracy by 1%.
  - [section 4.2] Figure 4 and Figure 6 show that models with mosaic data converge faster and achieve higher accuracy than those with simple repetition or standard augmentation.
  - [corpus] Corpus neighbors discuss general LVLM evaluation but do not specifically validate the "mosaic" technique for fine-grained tasks; evidence is isolated to this paper.
- Break condition: If the visual encoder's resolution is too low, the reduced size of individual tiles in the mosaic may remove the discriminative details necessary for fine-grained classification.

### Mechanism 2
- Claim: Incorporating "Open-World" data significantly boosts factual content accuracy.
- Mechanism: Aligning specific fine-grained visual features with dense textual knowledge (via Wikipedia summaries) creates a stronger association in the latent space. This moves the model beyond mere label prediction to generating contextually correct attributes.
- Core assumption: The model requires explicit exposure to natural language descriptions of fine-grained categories to bridge the gap between visual pattern matching and knowledge retrieval.
- Evidence anchors:
  - [abstract] States open-world data boosts content accuracy by 6%-12%.
  - [section 4.2] Figure 8 shows a significant jump in both recognition and content scores when open-world data is included versus short-answer data alone.
- Break condition: If the external knowledge source (e.g., Wikipedia) contains noise or conflicting descriptions for different categories, the model may suffer from hallucinated attributes rather than just improved recognition.

### Mechanism 3
- Claim: Integrating fine-grained data during the alignment stage maximizes recognition capability without catastrophic forgetting of general skills.
- Mechanism: Fine-grained recognition relies heavily on the quality of the visual-to-language projection. By training the alignment module on fine-grained pairs, the projector learns to preserve high-frequency visual details in the LLM embedding space. If introduced only during SFT, the fixed projector may have already discarded these details, forcing the LLM to fail or overfit at the cost of general reasoning.
- Core assumption: The alignment module (projector) is the critical bottleneck for transferring discriminative visual features to the language model.
- Evidence anchors:
  - [abstract] Notes incorporating fine-grained data into pre-training improves category recognition accuracy by up to 10%.
  - [section 4.3] Figure 10 and Table 2 demonstrate that models trained with fine-grained data in the alignment stage outperform those trained only in the fine-tuning stage, particularly in balancing general vs. fine-grained tasks.
- Break condition: If the volume of fine-grained alignment data overwhelms the general alignment data, the model may lose its ability to process abstract or broad visual concepts.

## Foundational Learning

- Concept: **LVLM Training Stages (Alignment vs. SFT)**
  - Why needed here: The paper's central optimization strategy relies on shifting data from the Supervised Fine-Tuning (SFT) stage to the Alignment (Pre-training) stage. You must understand that Alignment typically trains the connector/projector while keeping the Vision Encoder and LLM frozen, whereas SFT updates the full model for instruction following.
  - Quick check question: If I train a model on fine-grained data during SFT only, why does the paper suggest the model might "forget" general capabilities compared to training during the alignment phase?

- Concept: **Fine-Grained Visual Classification (FGVC)**
  - Why needed here: Unlike general object detection (identifying a "dog"), this task requires distinguishing subordinate categories (identifying a "Chihuahua" vs. "Beagle"). The paper addresses the specific difficulty LVLMs face here due to low-resolution inputs and limited domain data.
  - Quick check question: Why does the paper argue that multiple-choice benchmarks fail to capture the true difficulty of Fine-Grained Recognition?

- Concept: **Catastrophic Forgetting**
  - Why needed here: A key finding is that optimizing for fine-grained recognition often degrades general reasoning. The paper proposes a trade-off solution using specific data allocation strategies.
  - Quick check question: According to Table 2, what happens to the "General Capabilities" (e.g., AI2D, ChartQA) when the model is fine-tuned only on fine-grained data (Setting #2)?

## Architecture Onboarding

- Component map: Vision Encoder -> Projector/Adapter -> LLM Backbone -> Data Pipeline (General Data + Fine-Grained Data)

- Critical path:
  1. Data Curation: Generate Mosaic images (3x3 grid) and Open-World Q&A (Wikipedia + GPT-4o).
  2. Alignment Training: Train the Projector using both general caption data and the Fine-Grained dataset.
  3. SFT Training: Fine-tune the model using a mix of general instruction data and a small portion of fine-grained data.
  4. Evaluation: Use FROW benchmark (Open-ended) + GPT-4o-mini evaluator.

- Design tradeoffs:
  - Recognition vs. Generalization: Putting fine-grained data in Alignment helps recognition but risks misaligning general features if the ratio is too high. Putting it in SFT is safer for general tasks but limits recognition accuracy.
  - Data Efficiency: Mosaic data reduces the need for 10x repetition of raw images but requires careful construction to avoid confusing the model with too many distractors.

- Failure signatures:
  - The "Overfitting Expert": High accuracy on specific fine-grained subsets but near-zero performance on general charts or diagrams (indicating training distribution skew).
  - The "Hallucinating Expert": High recognition score but low content accuracy score (invents incorrect facts about the object).

- First 3 experiments:
  1. Baseline Validation: Evaluate an off-the-shelf LVLM on the FROW benchmark to confirm the performance gap between multiple-choice and open-ended formats.
  2. Data Ablation: Train two models—one with standard fine-grained data repetition and one with Mosaic data—to verify the convergence speedup and +1% accuracy claim in [Section 4.2].
  3. Stage Ablation: Compare training fine-grained data in SFT vs. Alignment to confirm that alignment-stage integration preserves general capabilities better.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can integrating preference alignment techniques (RLHF or DPO) further enhance fine-grained recognition capabilities in LVLMs beyond the optimizations achieved through supervised fine-tuning?
- Basis in paper: [explicit] The "Limitations" section states that "methods like preference alignment via RLHF or DPO remain underexplored, as our training is currently limited to the SFT stage."
- Why unresolved: The current study focused exclusively on data construction and SFT process optimization, leaving the potential benefits of alignment algorithms untested.
- What evidence would resolve it: A comparative study evaluating models fine-tuned with DPO/RLHF on the FROW benchmark versus the proposed SFT-only baseline.

### Open Question 2
- Question: Does the inclusion of mosaic data during training inadvertently increase the risk of object hallucination or context confusion in complex, multi-object real-world scenarios?
- Basis in paper: [inferred] The paper notes mosaic data exploration is "preliminary," and while it improves convergence, the impact on potential negative side effects like hallucination in non-mosaic contexts is not discussed.
- Why unresolved: The evaluation focuses on recognition accuracy improvements, without analyzing failure modes regarding hallucinated objects in composite or cluttered images.
- What evidence would resolve it: An evaluation using hallucination-specific benchmarks (e.g., POPE) comparing models trained with and without mosaic data augmentation.

### Open Question 3
- Question: Does the strategy of shifting fine-grained data to the alignment phase scale effectively to significantly larger LVLMs (e.g., >70B parameters) or different architectural backbones?
- Basis in paper: [inferred] The authors restrict experiments to InternVL-2.5-8B "Considering the training costs," leaving the efficacy of this specific data allocation strategy on larger or different model architectures unproven.
- Why unresolved: Smaller models may learn alignment differently than larger models; the trade-off between general capabilities and fine-grained knowledge might shift at different scales.
- What evidence would resolve it: Applying the proposed alignment-stage data strategy to a 70B+ parameter model and reporting both FROW scores and general benchmark scores.

## Limitations

- Performance claims rely heavily on synthetic data generation without independent validation of data quality
- The 10%-20% improvement from open-world data assumes GPT-4o-generated questions maintain factual accuracy and diversity
- Alignment-stage optimization strategy lacks detailed ablation studies on data ratios and their effects on general capabilities
- Evaluation methodology using GPT-4o-mini as a judge introduces potential subjectivity in scoring criteria

## Confidence

- **High Confidence:** The performance gap between multiple-choice and open-ended formats on FROW is well-established, and the catastrophic forgetting phenomenon when fine-tuning only on fine-grained data is clearly demonstrated.
- **Medium Confidence:** The 1% accuracy improvement from mosaic data and the 6%-12% content accuracy boost from open-world data are supported by internal experiments but would benefit from external validation.
- **Low Confidence:** The optimal data allocation ratios between alignment and SFT stages, and the long-term generalization effects of the proposed optimization strategies, remain unclear without extended training studies.

## Next Checks

1. **Independent Data Quality Assessment:** Have human annotators evaluate a random sample of the generated mosaic and open-world data to verify the factual accuracy and diversity claims, particularly for the Wikipedia-sourced open-world questions.

2. **Cross-Model Generalization Test:** Apply the mosaic and open-world data strategies to a different LVLM architecture (e.g., LLaVA-Next or Qwen-VL) to determine if the improvements are architecture-specific or represent general optimization principles.

3. **Long-Term Capability Tracking:** Train a model with the proposed optimization strategies and monitor its performance on general benchmarks (MathVista, AI2D) over multiple checkpoints to quantify catastrophic forgetting rates compared to baseline approaches.