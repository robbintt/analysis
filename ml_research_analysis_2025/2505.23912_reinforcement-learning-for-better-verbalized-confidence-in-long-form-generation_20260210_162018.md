---
ver: rpa2
title: Reinforcement Learning for Better Verbalized Confidence in Long-Form Generation
arxiv_id: '2505.23912'
source_url: https://arxiv.org/abs/2505.23912
tags:
- confidence
- language
- generation
- methods
- long-form
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes LoVeC, a reinforcement learning method to improve
  confidence estimation in long-form factual text generation. The method trains language
  models to append numerical confidence scores to each generated statement, serving
  as an interpretable signal of factuality.
---

# Reinforcement Learning for Better Verbalized Confidence in Long-Form Generation

## Quick Facts
- **arXiv ID:** 2505.23912
- **Source URL:** https://arxiv.org/abs/2505.23912
- **Reference count:** 40
- **Primary result:** RL method LoVeC improves confidence calibration in long-form generation, achieving 20× speedup over sampling baselines

## Executive Summary
This paper addresses the challenge of uncertainty estimation in long-form factual text generation by proposing LoVeC, a reinforcement learning method that trains language models to append numerical confidence scores to each generated statement. The approach treats confidence estimation as sequential decision-making, optimizing for better alignment between predicted confidence and factuality using delayed rewards. Experiments demonstrate superior calibration performance compared to supervised fine-tuning and sampling-based uncertainty methods, with significant efficiency gains during inference.

## Method Summary
LoVeC employs a two-stage training pipeline: supervised fine-tuning (SFT) for format adherence followed by reinforcement learning optimization. The method generates statements with inline verbalized confidence scores using a discrete token vocabulary representing confidence levels 0-10. Three RL algorithms are compared: GRPO (on-policy with reward model), DPO (off-policy with preference pairs), and ORPO (off-policy without reference model). The reward function penalizes miscalibration between predicted confidence and factuality scores obtained from GPT-4o + evidence verification. The approach achieves strong performance on three long-form QA datasets while running 20× faster than sampling-based uncertainty estimation methods.

## Key Results
- LoVeC achieves better calibration than SOTA methods with improved Brier Score, ECE, and Spearman correlation on WildHallu, FEVER, and LongFact datasets
- GRPO-trained models show well-ordered token probability distributions that internalize ordinal confidence semantics
- Inference speed is 20× faster than sampling-based baselines like LUQ
- The method generalizes well to out-of-domain datasets and short-form QA tasks
- Oracle-free training using self-labeling maintains strong performance (Brier Score 7.2% vs 14.5% for LUQ)

## Why This Works (Mechanism)

### Mechanism 1: RL Enables Optimization from Sparse Confidence Signals
RL can learn from weak, sparse supervision where confidence scores only appear after complete statements. Unlike SFT requiring dense token-level annotations, RL treats confidence estimation as sequential decision-making with delayed rewards. The reward function uses log-based binary cross-entropy that sharply penalizes miscalibration.

### Mechanism 2: Ordinal Structure Emerges in Token Probability Distributions
GRPO-trained models internalize the ordinal semantics of confidence scales (0-10), producing well-ordered token probability distributions. The explicit reward alignment between predicted confidence and factuality reinforces ordinal relationships during training.

### Mechanism 3: Inline Generation Eliminates Sampling Overhead
Generating confidence scores alongside content in a single decoding pass provides 20× inference speedup compared to sampling-based uncertainty methods. This approach adds negligible overhead while capturing sufficient uncertainty signal.

## Foundational Learning

- **Concept: Calibration Metrics (Brier Score, ECE, Spearman Correlation)**
  - Why needed here: Essential for interpreting evaluation results measuring calibration quality
  - Quick check: If a model assigns confidence [0.9, 0.3] to sentences with factuality [1.0, 0.0], what is the Brier Score? (Answer: 0.10)

- **Concept: Preference Learning (DPO vs. ORPO vs. GRPO)**
  - Why needed here: Different RL algorithms have varying requirements and tradeoffs
  - Quick check: Which algorithm would you choose if you have no reward model but can construct preference pairs? (Answer: DPO or ORPO; ORPO if avoiding reference model overhead)

- **Concept: Factuality Verification as Ground Truth**
  - Why needed here: The entire training pipeline depends on reliable factuality scoring
  - Quick check: If the factuality oracle is wrong 15% of the time, how might this affect learned calibration? (Answer: Noise in reward signal may cause models to match oracle errors)

## Architecture Onboarding

- **Component map:** Query + Evidence -> Base Model with LoRA -> SFT Stage -> RL Stage -> (Sentence, Confidence) pairs
- **Critical path:**
  1. Prepare dataset from WildHallu with (query, evidence) pairs
  2. Generate base responses, fact-check with oracle to obtain factuality scores
  3. Construct preference pairs using factuality as confidence labels
  4. SFT on preference pairs for format adherence (1 epoch)
  5. RL training with GRPO/DPO/ORPO algorithms
  6. Evaluate on held-out sets using BS, ECE-M, SC metrics

- **Design tradeoffs:**
  | Method | Reward Model | Reference Model | Stability | Compute |
  |--------|--------------|-----------------|-----------|---------|
  | GRPO   | Yes          | Yes             | High      | Higher (online sampling) |
  | DPO    | No           | Yes             | Medium-High | Lower (offline) |
  | ORPO   | No           | No              | Lower     | Lowest |

- **Failure signatures:**
  - Format collapse: Model stops generating <confidence> tags
  - Reward hacking: Model generates short/vague statements to avoid penalties
  - Overconfidence persistence: ECE remains high
  - Ordinal violation: Token probabilities disordered

- **First 3 experiments:**
  1. Reproduce iterative tagging baseline with SFT-only training, confirm BS ≈ 8.9-9.1%
  2. Ablate reward function: train GRPO with log, linear, and quadratic rewards
  3. Test oracle-free training: replace GPT-4o with Llama-3-8B self-labeling

## Open Questions the Paper Calls Out
None

## Limitations

- Evaluation domain specificity: Improvements demonstrated primarily on factuality-based QA datasets, performance on subjective tasks unexplored
- Reward signal quality dependence: RL pipeline depends on accurate factuality oracle, potential for bias from systematic oracle errors
- Computational tradeoff assumptions: 20× speedup comparison assumes specific baseline methods, GRPO training requires expensive online fact-checking

## Confidence

**High Confidence:**
- 20× inference speedup over sampling-based methods is well-supported
- Log-based reward superiority in GRPO is clearly demonstrated
- Basic mechanism of improving calibration over SFT-only baselines is consistent

**Medium Confidence:**
- Claims of strong generalization to out-of-domain datasets could benefit from more analysis
- Ordinal structure learning observation could be more rigorously quantified
- Oracle-free training capability is promising but limited testing

**Low Confidence:**
- Absolute magnitude of calibration improvement may not translate to downstream task performance
- Long-term stability claims are based only on training period observations

## Next Checks

1. **Domain Transfer Stress Test:** Evaluate LoVeC on diverse domains beyond factuality-based QA including subjective reasoning and creative generation to assess performance degradation patterns.

2. **Oracle Noise Sensitivity Analysis:** Systematically vary factuality oracle accuracy and measure impact on learned calibration quality to identify failure thresholds and robustness limits.

3. **Long-term Deployment Monitoring:** Deploy trained LoVeC model for extended periods to monitor for reward hacking, format collapse, or calibration degradation over time.