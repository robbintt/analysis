---
ver: rpa2
title: How Do Vision-Language Models Process Conflicting Information Across Modalities?
arxiv_id: '2507.01790'
source_url: https://arxiv.org/abs/2507.01790
tags:
- image
- modality
- information
- caption
- head
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates how vision-language models (VLMs) handle
  conflicting information across modalities by presenting inconsistent image-caption
  pairs and prompting the model to report information from a specific target modality.
  The study evaluates four 7B-parameter VLMs on five datasets and finds that all models
  exhibit performance degradation when faced with conflicting inputs, though different
  models show varying biases toward one modality over the other.
---

# How Do Vision-Language Models Process Conflicting Information Across Modalities?

## Quick Facts
- arXiv ID: 2507.01790
- Source URL: https://arxiv.org/abs/2507.01790
- Authors: Tianze Hua; Tian Yun; Ellie Pavlick
- Reference count: 40
- Primary result: All tested VLMs degrade performance on inconsistent image-caption pairs, with specific attention heads (router and promotion) controlling modality preference.

## Executive Summary
This paper investigates how vision-language models (VLMs) handle conflicting information across modalities by presenting inconsistent image-caption pairs and prompting the model to report information from a specific target modality. The study evaluates four 7B-parameter VLMs on five datasets and finds that all models exhibit performance degradation when faced with conflicting inputs, though different models show varying biases toward one modality over the other. Through representational analysis, the work shows that models encode information from both modalities but not equally saliently, with the more saliently encoded modality predicting the model's behavioral preference. The paper identifies specific attention heads responsible for this behavior: modality-agnostic "router heads" that dynamically promote answers based on the target modality, and modality-specific "promotion heads" that consistently favor one modality. These heads can be manipulated or transferred across datasets to improve model performance in resolving multimodal conflicts.

## Method Summary
The paper evaluates four 7B-parameter VLMs (LLaVA-1.5, InstructBLIP, Qwen2.5-VL, LLaVA-OneVision) on five datasets (CIFAR-10/100, ImageNet100, Pascal VOC 2012, CUB-Color) using inconsistent image-caption pairs where labels conflict. Behavioral evaluation measures accuracy when reporting from target modalities. Representational analysis uses linear probes and K-Means clustering with V-Measure to assess modality salience in hidden states. Attention head intervention experiments scale individual head outputs by α ∈ [-10, 10] to test their functional role in modality preference.

## Key Results
- All VLMs show performance degradation on inconsistent inputs across all tested datasets.
- Models encode both modalities adequately but with unequal salience; the more saliently encoded modality predicts behavioral output (V-Measure gap correlates with accuracy at r = 0.94).
- Three key attention heads identified: router head L11H14 (instruction-conditional, transfers across datasets), image promotion head L19H26, and caption promotion head L13H26 (both modality-specific).

## Why This Works (Mechanism)

### Mechanism 1: Representational Salience Competition
Models encode both modalities adequately but with unequal salience; whichever modality clusters more distinctly in late-layer representations predicts behavioral output. Hidden states are clustered via K-Means; V-Measure quantifies alignment with image vs. caption class labels. A positive V-Measure gap (target − non-target) correlates strongly with behavioral accuracy (r = 0.94, p < 10⁻³), suggesting the model answers from the more saliently encoded modality.

### Mechanism 2: Modality-Agnostic Router Heads
Specific attention heads read the instruction and boost the target-modality answer regardless of whether image or caption is requested. Scaling a router head's output (α × head_output) with α > 1 raises accuracy on the requested modality; α < 1 collapses performance toward random selection. Router heads are identified via systematic α-intervention sweeps.

### Mechanism 3: Modality-Specific Promotion Heads
Some heads consistently amplify one modality, improving performance when that modality is the target and degrading it otherwise. Image promotion head L19H26 raises image-reporting accuracy but harms caption-reporting; caption promotion head L13H26 does the inverse. Intervening these heads shifts V-Measure gaps in the expected direction.

## Foundational Learning

- **Linear probing**: Used to verify that both modalities and consistency are encoded in hidden states; foundational for ruling out encoding failures. Quick check: If probe accuracy is high but behavioral accuracy is low, what does that imply about the model's use of its representations?
- **Attention head ablation/intervention**: The paper isolates functional heads via output scaling; understanding additive interventions is required to reproduce or extend this work. Quick check: What does α = -1 vs. α = 10 imply for a promotion head's effect?
- **Clustering evaluation (V-Measure)**: Quantifies representational salience without training probes; central to linking internal structure to behavior. Quick check: Why use V-Measure instead of classification accuracy to assess representational salience?

## Architecture Onboarding

- **Component map**: Vision encoder → projectors → LLM backbone (attention heads + MLPs) → token output. Router heads: mid-to-late layers (e.g., L11H14 in Qwen2.5-VL), instruction-conditional. Promotion heads: distributed across layers (e.g., L19H26 for image, L13H26 for caption), modality-fixed.
- **Critical path**: 1) Input image-caption pair (inconsistent labels). 2) Tokenization + embedding; vision tokens prepended/appended per model template. 3) Forward pass through layers; router heads modulate based on instruction tokens. 4) Promotion heads bias output logits toward their preferred modality. 5) Final token distribution predicts class label.
- **Design tradeoffs**: Larger α amplifies effects but may destabilize language generation (monitored via fluency/valid output rate). Head selection criteria: trend salience vs. intervenability may identify different heads. Dataset construction: artificial captions (e.g., "an image of a horse") may limit ecological validity.
- **Failure signatures**: Saturation: near-100% baseline accuracy (e.g., CIFAR caption-reporting) obscures intervention gains. Token collision: if image and caption labels share first subtoken, next-token accuracy cannot distinguish modalities. Negative transfer: router head improves on most datasets but harms CUB-Color, possibly due to poor unimodal baseline.
- **First 3 experiments**: 1) Replicate unimodal vs. inconsistent-input accuracy gap on Pascal VOC for one model (e.g., LLaVA-1.5) using provided prompts. 2) Train linear probes on mid-layer representations to recover image vs. caption class labels; verify both are linearly accessible. 3) Intervene router head L11H14 (α = 10) on a held-out split; measure accuracy change and V-Measure shift to confirm mechanism transfer.

## Open Questions the Paper Calls Out

- How do the modality preference and attention head mechanisms identified in 7B VLMs generalize to models of different scales? The paper focuses on 7B-parameter models but acknowledges that models with varying sizes may behave differently or vary in inherent preferred modalities.

- How do multiple attention heads and other components (e.g., MLP layers) interact as circuits to process conflicting multimodal information? The study analyzed individual attention heads in isolation, not their joint interactions or contributions from non-attention components.

- How do these conflict-processing mechanisms transfer to modalities beyond vision and language, such as audio or spatial inputs? Experiments were limited to image-text pairs; it is unknown whether router/promotion heads exist for other sensory modalities.

## Limitations

- The inconsistent inputs are constructed by randomly sampling wrong class labels, which may not reflect real-world ambiguous or contradictory multimodal data.
- The study examines only 7B-parameter VLMs, so results may not generalize to larger or smaller models or VLMs with different architectural choices.
- While router heads transfer across datasets, promotion heads and their effects may be model-specific, and the paper does not test whether identified heads generalize to other VLM architectures.

## Confidence

**High confidence**: 
- VLMs show performance degradation on inconsistent inputs across all tested datasets and models.
- Both modalities are linearly decodable from hidden states, indicating encoding rather than complete information loss.
- Attention head interventions can shift modality preference in expected directions.

**Medium confidence**:
- The correlation between V-Measure gap and accuracy implies representational salience drives behavioral preference.
- Router heads are truly instruction-conditional rather than dataset-specific (based on cross-dataset transfer).
- Identified promotion heads implement fixed modality biases rather than context-dependent effects.

**Low confidence**:
- These specific head circuits (router vs. promotion) are universal across all VLMs or will be found in future architectures.
- The magnitude of performance improvement from head interventions would scale to practical applications.
- Artificially created conflicts map directly to real-world multimodal reasoning failures.

## Next Checks

1. **Natural conflict validation**: Apply the same methodology to a dataset with naturally occurring multimodal conflicts (e.g., ambiguous images with multiple valid captions, or real-world sensor noise) to test whether findings transfer beyond artificial label corruption.

2. **Alternative representation analysis**: Instead of K-Means V-Measure, apply supervised linear probes to quantify modality-specific information content in hidden states. Compare whether probe-based salience measures show the same correlation with accuracy as V-Measure, testing the robustness of the representational salience hypothesis.

3. **Architecture ablation study**: Test whether the same attention head circuits emerge in VLMs with different architectural choices (e.g., frozen vs. trainable vision encoders, different tokenization strategies, or different transformer variants) to determine if these circuits are architectural artifacts or fundamental to multimodal conflict resolution.