---
ver: rpa2
title: 'DISCO: Disentangled Communication Steering for Large Language Models'
arxiv_id: '2509.16820'
source_url: https://arxiv.org/abs/2509.16820
tags:
- steering
- attention
- head
- response
- value
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces DISCO, a new method for steering large language
  models by injecting vectors directly into the query and value representation spaces
  within attention heads. The key idea is to manipulate these internal spaces, which
  the authors show often exhibit higher linear discriminability of concepts than traditional
  attention head outputs.
---

# DISCO: Disentangled Communication Steering for Large Language Models

## Quick Facts
- **arXiv ID:** 2509.16820
- **Source URL:** https://arxiv.org/abs/2509.16820
- **Reference count:** 40
- **Primary result:** DISCO achieves up to 19.1% higher steering efficacy than the runner-up baseline by disentangling query and value steering magnitudes.

## Executive Summary
DISCO introduces a novel steering method for LLMs that injects vectors directly into query (Q) and value (V) representation spaces within attention heads. By separating control over query and value components, DISCO achieves more granular and effective behavior steering than traditional input-based methods. The method exploits the finding that Q and V spaces often exhibit higher linear discriminability of concepts than traditional attention head outputs, enabling more precise control over model behavior.

## Method Summary
DISCO extracts mean-difference steering vectors from positive/negative example activations at query and value projection layers. It then injects these vectors during inference at the same hook points, scaling them by separate magnitudes α_q and α_v. The method disentangles the rigid baseline "Communication Steering" by allowing different magnitudes for query and value spaces. Head selection uses linear discriminability accuracy, and magnitude search finds optimal values while maintaining response quality below a 3% degradation threshold.

## Key Results
- DISCO achieves up to 19.1% higher steering efficacy than runner-up baselines on LLaMA 3.1 8B and Gemma 2 9B
- DISCO-QV (separate query/value steering) outperforms DISCO-Q and DISCO-V individually
- Steering the value space acts as context-independent bias injection into the residual stream
- Query space steering functions as dynamic attention re-weighting mechanism

## Why This Works (Mechanism)

### Mechanism 1
Decoupling query (α_q) and value (α_v) steering magnitudes allows for more stable and effective control than coupled input steering. The method disentangles the rigid baseline "Communication Steering" (steering the attention input z). By steering Q and V directly, operators can set high α_q for strong context re-weighting without suffering the degradation limits imposed by a coupled high α_v. Core assumption: the optimal steering magnitude for the query space often differs significantly from the value space; enforcing equality degrades performance.

### Mechanism 2
Steering the query representation (Q) functions as a dynamic attention re-weighting mechanism. Adding a steering vector to Q modifies the attention distribution (A) via the softmax function. It increases attention weights on tokens whose keys align with the steering vector, effectively "retrieving" relevant context from the prompt dynamically. Core assumption: the prompt context contains information relevant to the target concept that standard attention may under-weight.

### Mechanism 3
Steering the value representation (V) acts as a direct, context-independent bias injection into the residual stream. Adding a vector to V modifies the attention head output directly (post-attention aggregation). This allows the injection of a "concept vector" into the residual stream regardless of where attention is focused. Core assumption: the value space encodes the concept linearly and can influence the residual stream without attention mediation.

## Foundational Learning

- **Concept: The Linear Representation Hypothesis**
  - Why needed here: DISCO relies on the assumption that high-level concepts (e.g., "power-seeking") are encoded as linear directions in the Q and V subspaces.
  - Quick check question: Can you draw a hyperplane that separates representations of "truthful" vs. "untruthful" sentences in a 2D plot of the query space?

- **Concept: The Residual Stream vs. Attention Head**
  - Why needed here: You must distinguish between steering the main "highway" (residual stream) vs. the "side roads" (attention inputs/outputs). DISCO operates strictly on the attention inputs (Q, V) before they interact.
  - Quick check question: Does steering the Value matrix (V) effect the computation of the attention weights (A)? (Answer: No).

- **Concept: Mean-Difference Vector Calculation**
  - Why needed here: This is the mathematical engine of the method. You calculate the "steering direction" by averaging the difference between positive and negative example activations.
  - Quick check question: If you have positive vectors P = {p_1, p_2} and negative N = {n_1, n_2}, what is the mean-difference vector μ?

## Architecture Onboarding

- **Component map:** q_proj (Query projection) -> v_proj (Value projection) -> Controller (computes mean-difference vectors μ_q, μ_v and scaling factors α_q, α_v) -> Injector (modifies forward pass: Q_new = Q + α_q μ_q; V_new = V + α_v μ_v)

- **Critical path:** Correctly registering forward hooks on the specific attention heads (layer index, head index) and ensuring the injected vector matches the head dimension (d') not the model dimension (d).

- **Design tradeoffs:**
  - Search Complexity: DISCO-QV requires searching a 2D grid (α_q, α_v) vs. 1D line for baselines
  - Head Selection: Steering "Top-K" discriminative heads vs. "All" heads. Paper suggests Top-K synergizes well with DISCO-QV.

- **Failure signatures:**
  - Repetition/Loops: α too high; model ignores context
  - Grammar Collapse: α_v too high specifically; injects noise into residual stream
  - No Effect: α too low OR selected heads have low linear discriminability (acc < threshold)

- **First 3 experiments:**
  1. Validation of Linearity: Replicate Fig 2. Train a linear probe on Q/V spaces for "TruthfulQA" and compare accuracy against Head Output spaces.
  2. Ablation Study (Magnitude): Run DISCO-Q vs. DISCO-V vs. DISCO-QV on a single concept (e.g., "Wealth-Seeking"). Plot "Steering Efficacy" vs. "Degradation Points" (|alpha|).
  3. Baseline Comparison: Implement "Communication Steering" (input steering) and compare against DISCO-QV on the "Corrigibility" dataset to verify the "disentanglement" gain shown in Fig 3.

## Open Questions the Paper Calls Out

- Does the superior performance and disentanglement capability of DISCO persist when applied to optimization-based (e.g., ReFT) or affine transformation steering methods, rather than just mean-difference vectors? Future work stands to extend our findings to alternative steering approaches, such as those involving optimization... or affine transformations.

- Does the relative effectiveness of DISCO-Q for multiple-choice tasks stem specifically from the attention-reweighting of comparative answer options in the prompt context? The authors note DISCO-Q performs best on multiple-choice tasks and "hypothesize that the effectiveness... may be due to the structure of the prompts," but do not validate this mechanism.

- Does the theoretical invariance to Key steering hold strictly in architectures employing Rotary Positional Embeddings (RoPE), which were omitted from the paper's mathematical derivation? The authors state they omit positional embeddings in their notation for brevity, assuming arguments extend with "minor modifications," and subsequently prove Key steering invariance without RoPE.

## Limitations
- Primary technical uncertainty: Steering Query and Value spaces may not remain effective as models scale beyond 8B-9B parameters
- Dataset limitation: Evaluation focuses heavily on behavior steering rather than factuality or knowledge-based tasks
- Evaluation framework dependency: Results depend on LLM-as-a-judge scoring, introducing potential circularity

## Confidence
- **High confidence:** The disentanglement mechanism (separate α_q and α_v) demonstrably outperforms coupled input steering on the tested models and datasets
- **Medium confidence:** Query space steering (DISCO-Q) effectiveness for attention re-weighting is well-supported by the mathematical framework and empirical results
- **Low confidence:** The claim that Value space steering acts as "context-independent bias injection" is theoretically plausible but empirically under-supported

## Next Checks
1. **Scaling study:** Evaluate DISCO on LLaMA 3.1 70B and larger models to test whether query/value steering efficacy and linear discriminability scale proportionally
2. **Concept transfer robustness:** Test whether steering vectors trained on one prompt distribution transfer to novel prompts requiring the same concept
3. **Residual stream ablation:** For DISCO-V, systematically ablate the attention term (A·V) while varying α_v to determine whether V steering truly operates independently of attention context