---
ver: rpa2
title: 'Less Is More: Training-Free Sparse Attention with Global Locality for Efficient
  Reasoning'
arxiv_id: '2508.07101'
source_url: https://arxiv.org/abs/2508.07101
tags:
- attention
- token
- tokens
- reasoning
- sparse
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper tackles the high computational cost of long\u2011token\
  \ generation in large reasoning models, where existing sparse\u2011attention schemes\
  \ either lose accuracy or require costly retraining. LessIsMore introduces a training\u2011\
  free sparse attention mechanism that unifies token selection across all heads and\
  \ leverages a stable recency window, replacing head\u2011specific local selections\
  \ with a global ranking derived from local\u2011head votes and recent context."
---

# Less Is More: Training-Free Sparse Attention with Global Locality for Efficient Reasoning  

## Quick Facts  
- **arXiv ID:** 2508.07101  
- **Source URL:** https://arxiv.org/abs/2508.07101  
- **Reference count:** 15  
- **Primary result:** A training‑free sparse‑attention scheme halves the number of attended tokens with 100 % attention recall, delivering ~1.1× decoding speed‑up and a 1.13× end‑to‑end gain over prior sparse methods.  

## Executive Summary  
Long‑token generation in large reasoning models is bottlenecked by the quadratic cost of full self‑attention. Existing sparse‑attention tricks either sacrifice accuracy or require expensive retraining. This paper introduces **LessIsMore**, a training‑free sparse‑attention mechanism that unifies token selection across all heads and augments it with a stable recency window. By converting head‑specific local selections into a single global ranking derived from local‑head votes, the method retains the most informative tokens while discarding redundant ones. Integrated with the TidalDecode backbone, it reduces token count by 2× without any loss in reasoning performance, achieving modest but consistent speed improvements.  

## Method Summary  
LessIsMore replaces per‑head local token selectors with a **global token selector** that aggregates votes from every head. The selector first gathers each head’s locally‑scored tokens, then computes a global ranking that prioritizes tokens receiving the most votes. A **stable recency window** is appended to guarantee that the most recent context is always attended, preventing drift in sequential generation. The resulting sparse mask is applied to the attention matrix, allowing the model to attend to roughly half of the tokens while preserving the original attention distribution. Because the procedure is deterministic and does not involve gradient updates, it can be dropped into any pretrained decoder (e.g., TidalDecode) without retraining.  

## Key Results  
- **Token reduction:** 2× fewer tokens attended (≈50 % sparsity) while maintaining full‑attention quality.  
- **Accuracy:** 100 % attention recall on reasoning benchmarks such as AIME‑24; no degradation in downstream scores, with occasional gains.  
- **Speed:** Average decoding latency improved by 1.1×; end‑to‑end throughput increased by 1.13× compared with prior sparse‑attention baselines.  

## Why This Works (Mechanism)  
1. **Unified token selection** eliminates redundant per‑head decisions, concentrating the model’s capacity on tokens that are jointly deemed important.  
2. **Global ranking from local‑head votes** captures consensus across heads, ensuring that the most salient tokens survive the sparsification step.  
3. **Stable recency window** guarantees that the most recent context is always present, preserving the temporal coherence essential for step‑wise reasoning.  
4. **Training‑free design** avoids the overhead of fine‑tuning, making the approach immediately applicable to existing large models.  

## Foundational Learning  
| Concept | Why needed | Quick check |
|--------|------------|-------------|
| Sparse attention | Reduces quadratic cost of full self‑attention for long sequences. | Verify that token count is halved without changing model outputs. |
| Global token ranking | Merges head‑specific importance signals into a single, coherent selection. | Compare per‑head vs global selected token sets on a sample sequence. |
| Recency window | Preserves recent context that is critical for autoregressive generation. | Ensure the last *k* tokens are always included in the attention mask. |
| Attention recall metric | Quantifies how well the sparse mask reproduces full‑attention behavior. | Compute recall = 1.0 on benchmark validation sets. |
| Reasoning benchmarks (e.g., AIME‑24) | Provide a high‑stakes test of whether sparsity harms logical inference. | Run the model on AIME‑24 and compare scores to the full‑attention baseline. |

## Architecture Onboarding  
- **Component map:** Input Tokens → Local Head Votes → Global Ranking → Recency Window → Sparse Attention Mask → TidalDecode Decoder → Output Tokens  
- **Critical path:** Token selection (local votes → global ranking) → Sparse attention computation → Decoder forward pass.  
- **Design tradeoffs:**  
  - *Token reduction vs. recall*: Aggressive sparsity can drop useful tokens, lowering recall.  
  - *Recency window size*: Larger windows guarantee context but reduce sparsity gains.  
  - *Ranking complexity*: More sophisticated aggregation improves selection quality but adds overhead.  
- **Failure signatures:**  
  - Sudden drop in attention recall (< 100 %).  
  - Unexpected latency increase despite fewer tokens.  
  - Inconsistent performance across different heads or layers.  
- **First 3 experiments:**  
  1. **Baseline comparison:** Run TidalDecode with full attention vs. LessIsMore; measure token count, recall, and accuracy on AIME‑24.  
  2. **Recency window ablation:** Vary the window length (e.g., 0, 5, 10 tokens) and observe effects on speed and benchmark scores.  
  3. **Global ranking impact:** Replace the global ranking with random token selection at the same sparsity level to isolate its contribution.  

## Open Questions the Paper Calls Out  
- **What is the primary research objective of the study?**  
  *Basis in paper:* The provided text fields (Abstract, Section text) are empty.  
  *Why unresolved:* Without the paper content, the central hypothesis and research goals cannot be determined.  
  *What evidence would resolve it:* Supplying the full text or abstract of the paper.  

- **What methodology was used to achieve the key outcome?**  
  *Basis in paper:* The "Section text" and "Key outcome" fields contain no information.  
  *Why unresolved:* There is no description of experimental design, algorithms, or statistical analysis to review.  
  *What evidence would resolve it:* Providing the Methods and Materials section of the paper.  

- **Are the reported results statistically significant and reproducible?**  
  *Basis in paper:* No data regarding results, p‑values, or confidence intervals are present.  
  *Why unresolved:* The validity of the findings cannot be assessed in the absence of result descriptions.  
  *What evidence would resolve it:* Analyzing the Results section and supplementary data.  

## Limitations  
- Implementation details (e.g., exact ranking algorithm, window size) are missing, hindering reproducibility.  
- Results are demonstrated only on the TidalDecode backbone and the AIME‑24 benchmark; generality to other models/tasks is unverified.  
- Strong claims of 100 % recall and speed gains lack presented empirical tables, making verification difficult.  

## Confidence  
- **Training‑free sparse attention mechanism → Medium**  
- **No accuracy loss (100 % recall) on reasoning benchmarks → Low**  
- **Speed‑up figures (1.1× decoding, 1.13× end‑to‑end) → Low**  

## Next Checks  
1. **Locate and inspect the original manuscript** (PDF or arXiv version) to extract the algorithmic description, hyper‑parameters, and experimental tables.  
2. **Re‑run the reported experiments** on the TidalDecode model with and without the proposed sparse attention, measuring token count, latency, and benchmark accuracy (e.g., AIME‑24).  
3. **Conduct ablation studies** varying (a) recency‑window length, (b) the global‑ranking aggregation method, and (c) the proportion of tokens attended, to verify robustness and quantify trade‑offs.