---
ver: rpa2
title: Repairing Reward Functions with Feedback to Mitigate Reward Hacking
arxiv_id: '2510.13036'
source_url: https://arxiv.org/abs/2510.13036
tags:
- reward
- function
- policy
- proxy
- preferences
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper tackles reward hacking that arises when a manually specified\
  \ proxy reward is misspecified relative to an unobservable human objective. It introduces\
  \ Preference\u2011Based Reward Repair (PBRR), an iterative algorithm that augments\
  \ the proxy reward with a transition\u2011dependent correction learned from human\
  \ preferences over trajectory pairs."
---

# Repairing Reward Functions with Feedback to Mitigate Reward Hacking  

## Quick Facts  
- **arXiv ID:** 2510.13036  
- **Source URL:** https://arxiv.org/abs/2510.13036  
- **Reference count:** 40  
- **Primary result:** Preference‑Based Reward Repair (PBRR) attains √T cumulative regret and achieves near‑optimal performance with an order‑of‑magnitude fewer preference queries than competing baselines.  

## Executive Summary  
Reward hacking occurs when a hand‑crafted proxy reward diverges from the true, unobservable human objective. The paper introduces Preference‑Based Reward Repair (PBRR), an iterative framework that augments the proxy reward with a transition‑dependent correction learned from human preferences over trajectory pairs. By focusing learning only on transitions where the proxy’s ranking conflicts with observed preferences and employing a targeted exploration policy, PBRR provably matches the best known √T regret bounds for preference‑based reinforcement learning while dramatically reducing the number of required human queries. Empirical evaluation on a suite of reward‑hacking benchmarks shows consistent superiority over baselines that learn rewards from scratch or apply other repair techniques.  

## Method Summary  
PBRR starts from an existing proxy reward function and repeatedly collects human preference feedback on pairs of trajectories. A three‑term loss updates a correction term that is added to the proxy reward, but only for those state‑action transitions whose induced rankings disagree with the preferences. The algorithm couples this correction learning with a targeted exploration strategy that deliberately visits uncertain or high‑impact transitions, ensuring informative feedback. Under a linear‑feature reward model, the authors prove a √T cumulative regret bound, matching the optimal rate for preference‑based RL up to constants.  

## Key Results  
- **Theoretical guarantee:** √T cumulative regret under linear‑feature reward assumptions, matching the best known bounds for preference‑based RL.  
- **Empirical performance:** Across multiple reward‑hacking benchmarks, PBRR outperforms baselines, reaching near‑optimal ground‑truth returns.  
- **Query efficiency:** Achieves comparable performance with an order‑of‑magnitude fewer human preference queries than competing methods.  

## Why This Works (Mechanism)  
1. **Targeted correction of proxy reward** – The three‑term loss adjusts the reward only on transitions where the proxy’s ranking conflicts with human preferences, preventing unnecessary drift and preserving useful prior knowledge.  
2. **Preference‑driven exploration** – By steering the agent toward transitions that are uncertain under the current correction, the algorithm gathers maximally informative feedback, accelerating learning.  
3. **Linear‑feature reward structure** – Assuming rewards are linear in known features enables a regret analysis that yields the √T bound, ensuring that the correction converges efficiently.  

## Foundational Learning  
- **Preference learning over trajectories** – Needed to translate human judgments into a correction signal. *Quick check:* Can the system rank a held‑out pair of trajectories consistent with human labels?  
- **Linear reward representation** – Provides a tractable parameterization for the correction term and underpins the regret proof. *Quick check:* Do the feature vectors span the true reward space in a simple test environment?  
- **Regret analysis for online learning** – Guarantees that cumulative performance loss grows sub‑linearly. *Quick check:* Does empirical cumulative regret scale roughly with √T in a synthetic task?  
- **Exploration‑exploitation trade‑off** – Critical for gathering informative preferences without excessive sample waste. *Quick check:* Does targeted exploration reduce the variance of the learned correction compared to random exploration?  
- **Human‑in‑the‑loop query budgeting** – Ensures practical feasibility of preference collection. *Quick check:* How many queries are needed to achieve a target performance threshold?  

## Architecture Onboarding  
**Component map:**  
Proxy Reward → Preference Collector → Transition‑Correction Learner → Updated Reward → Policy Optimizer → Environment  

**Critical path:**  
1. Collect human preferences on trajectory pairs.  
2. Compute loss on conflicting transitions and update the correction term.  
3. Combine proxy reward + correction to form the updated reward.  
4. Optimize the policy with the updated reward, generating new trajectories for the next feedback round.  

**Design tradeoffs:**  
- *Query budget vs. reward fidelity*: Fewer queries lower cost but may leave residual mis‑specifications.  
- *Linear vs. non‑linear correction*: Linear models enable regret guarantees but may miss complex reward structures.  
- *Exploration aggressiveness*: Stronger exploration yields richer feedback but can increase sample inefficiency if mis‑directed.  

**Failure signatures:**  
- Persistent ranking conflicts after many iterations → insufficient correction capacity or poor feature representation.  
- Cumulative regret plateauing far above the √T curve → exploration or learning rate mis‑tuned.  
- Exhaustion of query budget before performance stabilizes → overly aggressive query schedule.  

**First 3 experiments:**  
1. **Gridworld reward‑hacking task:** Verify that PBRR reduces the proxy’s unintended incentives and achieves near‑optimal returns with limited queries.  
2. **Preference‑query efficiency benchmark:** Compare the number of queries needed by PBRR versus a baseline preference‑based RL method to reach a fixed performance level.  
3. **Ablation of the three‑term loss:** Remove each loss component in turn to assess its impact on correction quality and query savings.  

## Open Questions the Paper Calls Out  
- How does PBRR scale to high‑dimensional, non‑linear reward representations common in vision‑based tasks?  
- What are the limits of the linear‑feature assumption when the true human objective is highly non‑linear?  
- Can the targeted exploration scheme be generalized to multi‑agent or partially observable settings?  
- How robust is PBRR to noisy or inconsistent human preferences, and what mitigation strategies are effective?  
- What is the real‑world cost trade‑off between query reduction and the additional computation required for the correction learner?  

## Limitations  
- Theoretical guarantees rely on a linear‑feature reward model, which may not hold for complex domains.  
- Empirical validation is limited to a specific suite of reward‑hacking benchmarks; broader generalization is untested.  
- The claimed order‑of‑magnitude query reduction is not quantified with absolute numbers or variance analyses.  

## Confidence  
| Claim cluster | Confidence |
|---------------|------------|
| √T cumulative‑regret bound (linear features) | Medium |
| Empirical superiority over baselines | Low |
| Substantial reduction in required preference queries | Low |

## Next Checks  
1. **Re‑derive the regret bound** using the paper’s loss formulation and verify that all required assumptions (linearity, bounded features, etc.) are explicitly stated.  
2. **Replicate a benchmark experiment** (e.g., the “wire‑pull” reward‑hacking task) by implementing PBRR with the described exploration scheme; compare cumulative reward and query count against the reported numbers.  
3. **Ablation of the three‑term loss**: train variants that omit each term individually to assess their contribution to performance and query efficiency, confirming that the claimed “targeted adjustment” is essential.