---
ver: rpa2
title: 'Multi-task Federated Learning with Encoder-Decoder Structure: Enabling Collaborative
  Learning Across Different Tasks'
arxiv_id: '2504.09800'
source_url: https://arxiv.org/abs/2504.09800
tags:
- task
- tasks
- learning
- clients
- multi-task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper tackles the limitation of conventional federated learning\
  \ that requires all participants to share identical model architectures and tasks,\
  \ which prevents clients with heterogeneous tasks or structures from collaborating\
  \ and exacerbates data scarcity. M\u2011Fed resolves this by enforcing a common\
  \ encoder across all clients while allowing task\u2011specific decoders; local training\
  \ incorporates both the global encoder and the task\u2011level global decoder, and\
  \ the server aggregates decoders per task and then merges all encoders into a single\
  \ global encoder that is redistributed to every client."
---

# Multi-task Federated Learning with Encoder-Decoder Structure: Enabling Collaborative Learning Across Different Tasks

## Quick Facts
- **arXiv ID:** 2504.09800  
- **Source URL:** https://arxiv.org/abs/2504.09800  
- **Reference count:** 40  
- **Primary result:** M‑Fed consistently outperforms standard FL baselines on two benchmarks while enabling cross‑task collaboration with modest communication overhead.  

## Executive Summary
Conventional federated learning (FL) assumes that all participating clients share the same model architecture and solve the same task, which blocks collaboration among heterogeneous clients and worsens data scarcity. The authors propose **M‑Fed**, a multi‑task FL framework that enforces a **common encoder** across all clients while allowing **task‑specific decoders**. Each client trains locally using the global encoder together with its own decoder; the server aggregates decoders per task and merges all client‑wise encoder updates into a single global encoder that is redistributed to every client. Experiments on two widely used benchmarks show that M‑Fed achieves higher task‑specific accuracy and lower loss than standard FL baselines, demonstrating effective knowledge sharing across disparate tasks with only a modest increase in communication cost.

## Method Summary
M‑Fed splits the model into two logical parts: a **shared encoder** that learns a universal representation of the input data, and **task‑specific decoders** that map this representation to the outputs required by each client’s objective. During each communication round, the server broadcasts the latest global encoder to all clients. Each client then performs local SGD on its own data using the received encoder plus its private decoder, producing updated encoder and decoder weights. The server aggregates all encoder updates by simple averaging, forming a new global encoder. Decoders are grouped by task type; for each task the server averages the corresponding decoders and distributes the resulting task‑level decoder back to the clients that share that task. This alternating aggregation preserves cross‑task knowledge in the encoder while respecting task heterogeneity in the decoders.

## Key Results
- **Higher task‑specific performance:** M‑Fed yields consistent accuracy gains and loss reductions over FedAvg and other FL baselines on both benchmark datasets.  
- **Cross‑task knowledge transfer:** The shared encoder captures features useful across heterogeneous tasks, leading to measurable improvements even for clients with scarce data.  
- **Modest communication overhead:** Only one global encoder plus per‑task decoder aggregates are transmitted each round, reducing bandwidth compared with transmitting full models for every client.

## Why This Works (Mechanism)
1. **Shared representation learning:** By forcing all clients to use the same encoder, the system aggregates gradients from diverse tasks, producing a richer, more general feature extractor.  
2. **Task‑specific specialization:** Decoders remain private to each task, preventing negative transfer that could arise if the entire model were shared.  
3. **Dual‑level aggregation:** Global encoder averaging captures cross‑task signals, while per‑task decoder averaging respects intra‑task consistency, balancing collaboration and specialization.  

## Foundational Learning
| Concept | Why Needed | Quick Check |
|--------|------------|-------------|
| Federated Averaging (FedAvg) | Baseline FL method; M‑Fed builds on its aggregation principle. | Verify that simple weight averaging reproduces FedAvg results on a homogeneous task. |
| Encoder‑decoder architecture | Enables separation of shared and task‑specific parameters. | Confirm that freezing the encoder while training decoders degrades performance, showing encoder importance. |
| Multi‑task learning (MTL) | Provides theoretical grounding for shared representations across tasks. | Run a centralized MTL experiment and compare encoder quality to the federated version. |
| Communication compression | Relevant for assessing “modest overhead”. | Measure bytes per round with and without encoder sharing. |
| Gradient heterogeneity analysis | Explains why a shared encoder can still converge despite diverse tasks. | Plot gradient norms per client; look for complementary directions. |
| Privacy preservation in FL | Underlies the motivation for not sharing raw data. | Ensure no raw data leaves the client during experiments. |

## Architecture Onboarding
**Component map**  
Client encoder → Client decoder → Local loss → Encoder gradient & Decoder gradient → Server encoder aggregation → Server per‑task decoder aggregation → Broadcast global encoder & task decoders → Clients  

**Critical path**  
1. Server broadcasts global encoder.  
2. Clients perform local training (forward + backward) using encoder + private decoder.  
3. Clients send encoder and decoder updates back.  
4. Server aggregates encoder globally and decoders per task, then redistributes.  

**Design trade‑offs**  
- *Encoder size vs. communication*: Larger encoders improve representation power but increase bandwidth.  
- *Number of tasks vs. decoder aggregation*: More tasks reduce the benefit of per‑task averaging because fewer clients contribute to each decoder.  
- *Local epochs vs. staleness*: More local epochs reduce communication rounds but risk encoder drift from heterogeneous objectives.  

**Failure signatures**  
- Diverging loss on a subset of tasks → encoder over‑fitting to dominant tasks.  
- Encoder parameter explosion → gradient clipping needed.  
- Decoder mismatch errors → task identifiers misaligned during aggregation.  

**First three experiments to validate the pipeline**  
1. **Baseline FedAvg replication** on a single task to confirm the training infrastructure.  
2. **Encoder‑only sharing**: train with a shared encoder but identical decoders across clients; measure performance drop relative to full M‑Fed.  
3. **Per‑task decoder aggregation test**: run two tasks with separate decoders, disable decoder averaging, and observe whether task performance degrades, confirming the necessity of per‑task aggregation.  

## Open Questions the Paper Calls Out
- The authors note the need for the full paper text (abstract, sections, etc.) to enable deeper evidence‑anchored analysis; without it, causal mechanisms, detailed hyper‑parameters, and dataset specifics remain unclear.  
- How does M‑Fed scale when the number of heterogeneous tasks grows dramatically?  
- What are the privacy implications of sharing encoder updates that may encode task‑specific signals?  

## Limitations
- Lack of detailed architectural specifications (layer sizes, activation functions) hampers exact reproduction.  
- Communication cost analysis is qualitative (“modest”) without quantitative bandwidth measurements.  
- The reported performance gains lack statistical significance testing and confidence intervals.  

## Confidence
| Claim | Confidence |
|-------|------------|
| Shared encoder enables cross‑task knowledge transfer | Medium |
| M‑Fed outperforms standard FL baselines on two benchmarks | Low |
| Communication overhead is modest compared to full‑model FL | Medium |
| Per‑task decoder aggregation prevents negative transfer | Medium |

## Next Checks
1. **Reproduce benchmark results**: Obtain the two cited datasets, implement the encoder‑decoder split as described, and compare M‑Fed’s accuracy/loss against FedAvg under identical training schedules.  
2. **Validate encoder aggregation**: Log encoder parameters before and after server‑side averaging to confirm contributions from all heterogeneous clients are reflected in the global encoder.  
3. **Quantify communication cost**: Measure total bytes transmitted per round for M‑Fed (global encoder + per‑task decoders) and contrast with standard FL (full model) to verify the “modest overhead” claim.