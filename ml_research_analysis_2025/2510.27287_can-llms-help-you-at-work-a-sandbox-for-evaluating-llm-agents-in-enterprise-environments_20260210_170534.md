---
ver: rpa2
title: Can LLMs Help You at Work? A Sandbox for Evaluating LLM Agents in Enterprise
  Environments
arxiv_id: '2510.27287'
source_url: https://arxiv.org/abs/2510.27287
tags:
- task
- data
- enterprise
- employee
- access
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: EnterpriseBench is a benchmark for evaluating LLM agents in enterprise
  environments, addressing the challenge of data fragmentation and access controls.
  It simulates a realistic enterprise sandbox with 500 tasks across HR, IT, Sales,
  and Software Engineering, featuring multi-source workflows and persona-based access
  control.
---

# Can LLMs Help You at Work? A Sandbox for Evaluating LLM Agents in Enterprise Environments

## Quick Facts
- arXiv ID: 2510.27287
- Source URL: https://arxiv.org/abs/2510.27287
- Authors: Harsh Vishwakarma; Ankush Agarwal; Ojas Patil; Chaitanya Devaguptapu; Mahesh Chandran
- Reference count: 40
- Primary result: EnterpriseBench benchmark reveals LLMs complete only 41.8% of enterprise tasks, with planning quality as the primary bottleneck

## Executive Summary
EnterpriseBench introduces a comprehensive benchmark for evaluating LLM agents in enterprise environments, addressing the challenge of data fragmentation and access controls. The benchmark simulates a realistic enterprise sandbox with 500 tasks across HR, IT, Sales, and Software Engineering, featuring multi-source workflows and persona-based access control. Experiments with state-of-the-art models show significant performance gaps, with even gold planning improving results by only 40-50%. Human agents achieve 70% accuracy but take 8x longer, revealing a precision-efficiency tradeoff. The benchmark provides a foundation for developing more capable enterprise-focused AI systems.

## Method Summary
The benchmark uses a synthetic enterprise sandbox with 10 data sources and 40+ CRUD tools, implementing RBAC across 6 role levels. Tasks are generated via a 4-stage LLM pipeline (domain selection → goal template → subgoal decomposition → validation) using reverse task synthesis from organizational metadata. Evaluation employs Prometheus-2 rubric scoring (1-5 scale) with GPT-4 and Gemini-2.5-Pro evaluators. Models tested include GPT-4o, Claude 3.5, o1-mini, Llama-3.1-8B, and Llama-3.3-70B via LangChain and DSPy frameworks, comparing planning strategies (none, CoT, ReAct, gold planning).

## Key Results
- Only 41.8% task completion across all models on 500 enterprise tasks
- Gold planning improves performance by 40-50% over ReAct, identifying planning as primary bottleneck
- Human agents achieve 70% accuracy but take 8x longer than models, revealing precision-efficiency tradeoff
- Task decomposition errors account for 20 of 69 total failures in error analysis

## Why This Works (Mechanism)

### Mechanism 1
Multi-source task decomposition enables enterprise agents to handle cross-functional workflows that single-source retrieval cannot address. Tasks requiring 3+ subtasks show significant performance drops across all models, suggesting planning quality is the bottleneck. Gold planning improves performance by 40-50% over ReAct, indicating that correct decomposition—not execution—is the primary constraint. Core assumption: Agent failure stems from planning errors, not tool invocation failures.

### Mechanism 2
Persona-based access control simulation creates realistic constraints that expose agent limitations in permission-aware reasoning. The sandbox implements Role-Based Access Control (RBAC) where tools return filtered results based on employee level and department. Tasks marked "Unanswerable" test whether agents recognize permission boundaries. When access control is removed, models attempt tasks they lack permissions for, increasing failure rates. Core assumption: Agents will not inherently respect access boundaries without explicit enforcement.

### Mechanism 3
Reverse task synthesis—generating tasks from available context rather than questions first—produces grounded, verifiable enterprise tasks. The pipeline retrieves context → extracts entities → decomposes goals → fills templates → grounds each subtask to evidence → validates via expert checklist. This ensures tasks are answerable from sandbox data, avoiding hallucinated requirements. Core assumption: Tasks generated from ground truth context are more realistic than manually authored ones.

## Foundational Learning

- **ReAct Planning Pattern**
  - Why needed here: ReAct outperforms no-planning and CoT across both LangChain and DSPy frameworks. Understanding reasoning-acting interleaving is essential for debugging agent failures.
  - Quick check question: Can you explain why ReAct would help an agent decide between calling `employee_data_read` vs. `github_read` for a task asking about "my recent code contributions"?

- **Role-Based Access Control (RBAC) in Multi-Tenant Systems**
  - Why needed here: The sandbox enforces access based on employee level and department. Agents must reason about permissions before tool selection.
  - Quick check question: Given an employee at Level-10 in Sales, what data sources should they be blocked from accessing in the sandbox?

- **Compound AI Systems (Tool-Using Agents)**
  - Why needed here: The benchmark evaluates CAI systems that orchestrate multiple tools, not just single-model inference. Understanding tool dependency graphs is critical.
  - Quick check question: For the task "Create a GitHub repo and notify my manager," what is the correct tool invocation order and why?

## Architecture Onboarding

- **Component map**: Sandbox Environment (10 data sources, 40+ CRUD tools) -> Access Control Layer (JSON-based RBAC) -> Task Generator (4-stage LLM pipeline) -> Evaluation Framework (Prometheus-2 rubric with GPT-4/Gemini evaluators)

- **Critical path**: 1) Understand ER diagram and employee hierarchy 2) Review tool inventory to know available operations 3) Trace a sample task through the generation pipeline 4) Run baseline evaluation with provided LangChain/DSPy configs

- **Design tradeoffs**: Synthetic data (scales, no privacy concerns) vs. real enterprise data (more realistic but costly); Automated task generation (scalable) vs. manual curation (higher quality); Strict access control (realistic) vs. open access (simpler evaluation)

- **Failure signatures**: Wrong tool selection (18/69 failures); Hallucination (8/69); Task decomposition errors (20/69); Partial coverage (14/69)

- **First 3 experiments**:
  1. Reproduce baseline: Run GPT-4o with ReAct on 50-task subset using LangChain; expect ~29-32% accuracy
  2. Ablate planning: Compare no-planning vs. CoT vs. ReAct vs. gold planning on same subset; expect 40-50% gap between gold and ReAct
  3. Test access control: Run evaluation with RBAC disabled; expect spurious completions on "Unanswerable" tasks

## Open Questions the Paper Calls Out

- **Can domain-specific fine-tuning of Small Language Models (SLMs) consistently outperform general-purpose Large Language Models (LLMs) in enterprise settings?**
  - Basis in paper: [explicit] Qwen3-8B trained with SFT+DPO surpassed GPT-4o with CoT, suggesting "small language models (SLMs) trained with high-quality data can outperform general-purpose LLMs."
  - Why unresolved: Experiment was limited to a single SLM architecture and small dataset (1.2k samples)
  - What evidence would resolve it: Comprehensive comparison of multiple SLM families against frontier models across all 500 benchmark tasks

- **How can agent architectures be improved to bridge the performance gap between autonomous planning and oracle-provided "gold planning"?**
  - Basis in paper: [inferred] Results show providing gold plans improves performance by 40-50% over ReAct
  - Why unresolved: Current planning strategies fail to effectively model dependencies between enterprise tools and data access controls
  - What evidence would resolve it: Development of a novel planning module that achieves performance parity with "w/ Gold Planning" baseline

- **To what extent does the synthetic nature of the sandbox data affect the ecological validity of the benchmark compared to real-world enterprise environments?**
  - Basis in paper: [explicit] Authors list as a limitation that "Relying solely on synthetic data may affect the realism of generated tasks"
  - Why unresolved: Benchmark relies on generated conversations and rule-based processing; unclear if agents succeeding here would succeed on real corporate data
  - What evidence would resolve it: Correlation study comparing agent performance on synthetic EnterpriseBench against sanitized real corporate logs

## Limitations

- Synthetic data generation may not fully capture the complexity and ambiguity of real enterprise environments
- Task generation pipeline's reliance on LLM-generated contexts introduces potential biases in task difficulty and domain representation
- Evaluation methodology using GPT-4/Gemini as judges may not perfectly align with human expert judgments

## Confidence

- **High Confidence**: Benchmark construction methodology and performance gaps between models are well-documented and reproducible
- **Medium Confidence**: Interpretation that planning quality is primary bottleneck requires careful consideration
- **Low Confidence**: Human baseline comparison may not be directly comparable due to potential differences in task interpretation

## Next Checks

1. **Cross-Evaluator Validation**: Run same evaluation tasks through 3-5 different human experts and compare scoring consistency with GPT-4/Gemini evaluators
2. **Real Data Pilot**: Construct small pilot benchmark (50 tasks) using actual enterprise data to measure performance gap between synthetic and real environments
3. **Planning Ablation Study**: Design controlled experiments introducing systematic planning errors into gold plans to measure marginal impact of different error types