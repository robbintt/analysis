---
ver: rpa2
title: Tree Reward-Aligned Search for TReASURe in Masked Diffusion Language Models
arxiv_id: '2509.23146'
source_url: https://arxiv.org/abs/2509.23146
tags:
- reward
- arxiv
- diffusion
- preprint
- masked
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of aligning masked diffusion
  language models (MDLMs) with task-specific rewards during inference, a problem that
  has been underexplored compared to autoregressive models. The key difficulties arise
  from the parallel unmasking in MDLMs, which leads to highly correlated search branches
  and inefficient exploration, and the high variance in reward estimation due to the
  categorical nature of MDLM predictions.
---

# Tree Reward-Aligned Search for TReASURe in Masked Diffusion Language Models

## Quick Facts
- **arXiv ID**: 2509.23146
- **Source URL**: https://arxiv.org/abs/2509.23146
- **Reference count**: 40
- **Primary result**: TREASURE achieves state-of-the-art reward alignment for MDLMs across four tasks with improved efficiency in low-NFE regimes.

## Executive Summary
This paper addresses test-time reward alignment for Masked Diffusion Language Models (MDLMs), which has been underexplored compared to autoregressive models. The key challenge stems from MDLMs' parallel unmasking process, which creates highly correlated search branches and high-variance reward estimation. TREASURE introduces two innovations: UNMASKBRANCH for efficient branching at first-hitting unmasking events, and RESUBSTITUTESCORE for low-variance reward estimation via deterministic resubstitution. The method achieves state-of-the-art results across four controllable generation tasks while providing theoretical guarantees on branching efficiency, reward approximation error bounds, and monotonicity of reward improvement with increased tree width.

## Method Summary
TREASURE combines UNMASKBRANCH and RESUBSTITUTESCORE to perform tree search over MDLM's reverse process. At each node with n masked tokens, UNMASKBRANCH samples the next unmasking time via first-hitting sampling, selects one masked position uniformly, and expands top-b(n) token candidates with a single model call. RESUBSTITUTESCORE then scores each candidate by deterministically filling remaining masks with argmax predictions and evaluating the reward once. The algorithm maintains top-m(n) candidates per level through TopK selection. Key configurations include beam width b(n) and tree width m(n) schedules, with empirical results showing monotonic reward improvement as m(n) increases under coupled randomness.

## Key Results
- TREASURE outperforms baselines across all four controllable generation tasks (perplexity, linguistic acceptability, sentiment, toxicity) under matched NFE budgets
- Achieves largest gains in low-NFE regimes, with COLA accuracy improving from 43.16 to 77.67 at NFE=2
- Provides theoretical guarantees: NFE efficiency improvement (Theorem 1), reward approximation error bounds (Theorem 2), and monotonicity with tree width (Theorem 3)
- RESUBSTITUTESCORE matches true posterior scoring performance at approximately 5× lower NFE

## Why This Works (Mechanism)

### Mechanism 1: UNMASKBRANCH
Branching at first-hitting unmasking events produces diverse trajectories with 1 model call per parent. Instead of naive parallel sampling (which yields correlated branches due to tightly coupled token distributions), UNMASKBRANCH uses first-hitting sampling to jump directly to the next unmasking time τ_{n-1}, then branches by uniformly selecting a masked position and enumerating top-b(n) tokens at that position. This diversifies both unmasking order and token content. The efficiency gains rely on the assumption that the MDLM's reverse process is well-specified enough that FHS accurately approximates true unmasking dynamics.

### Mechanism 2: RESUBSTITUTESCORE
Deterministic resubstitution provides low-variance reward estimates with error bounded by predictive uncertainty. After UNMASKBRANCH produces candidate states and model probabilities μ_n, RESUBSTITUTESCORE fills remaining masked positions with argmax tokens (not samples) to create a proxy completion, then evaluates the reward once. This reuses already-computed probabilities and avoids sampling variance. The core assumption is that rewards are Hamming-Lipschitz and that model confidence correlates with true expected reward.

### Mechanism 3: Tree-Width Monotonicity
Increasing tree width m(n) monotonically improves final reward under coupled randomness. With fixed beam width and deterministic tie-breaking, larger m(n) retains a superset of candidates at each level. The final reward is the max over the final set, so a superset cannot yield a lower maximum. This guarantee relies on coupling across runs (same random draws, same model outputs, deterministic tie-breaking).

## Foundational Learning

- **Masked Diffusion Language Models (MDLMs)**: Generate tokens in parallel through a reverse denoising process from all-mask to full sequence. Why needed: TREASURE operates on MDLMs' unique discrete denoising process. Quick check: Can you explain why MDLMs generate tokens in parallel rather than sequentially, and what the forward/reverse processes are?

- **First-Hitting Sampling (FHS)**: Samples commitment events efficiently by jumping directly to next unmasking time. Why needed: UNMASKBRANCH builds directly on FHS to sample commitment events. Quick check: How does FHS sample the next unmasking time τ_{n-1} given current time τ_n and n masked tokens?

- **KL-regularized test-time alignment**: Frames TTA as maximizing E[r(x)] - λ·D_KL(p||p_pre). Why needed: The paper's objective clarifies what soft value functions approximate. Quick check: What does the temperature parameter λ control in the KL-regularized alignment objective?

## Architecture Onboarding

- **Component map**: Pretrained MDLM (x_θ) -> UNMASKBRANCH (Algorithm 1) -> RESUBSTITUTESCORE (Algorithm 2) -> TopK selection -> Main loop (Algorithm 3)
- **Critical path**: Initialize all-mask state → For each level n from L to 1: UNMASKBRANCH for each parent → RESUBSTITUTESCORE for each child → TopK prune → Return argmax from final candidates
- **Design tradeoffs**: Larger beam width b(n) explores more locally per parent; larger tree width m(n) retains more candidates globally. Only m has monotonicity guarantee. Resubstitution is O(1) per child vs. O(samples) for true posterior scoring.
- **Failure signatures**: Reward collapse if beam width too large relative to tree width; high variance pruning if model confidence low; NFE blowup if coupling assumptions break.
- **First 3 experiments**:
  1. Validate UNMASKBRANCH efficiency: Compare naive parallel sampling vs. UNMASKBRANCH with b(n)=4 on sequences of length 64, measure NFE to obtain 4 distinct children.
  2. Ablate RESUBSTITUTESCORE: Compare previous-step scoring, resubstitution, and true posterior with 5 samples on COLA reward at fixed NFE=4.
  3. Verify monotonicity: Run TREASURE with m(n)∈{2,4,8,16} at fixed b(n)=5 on sentiment control, plot final reward vs. m.

## Open Questions the Paper Calls Out

### Open Question 1
How can TREASURE be extended to long-context MDLMs (sequences >1K tokens) and multimodal MDLMs where token spaces differ across modalities? Basis: Future work includes extending to long-context and multimodal MDLMs. Why unresolved: Current experiments use only 128-token sequences and text-only models. Longer contexts exponentially increase search complexity, and multimodal settings introduce heterogeneous token spaces requiring modified branching and scoring rules.

### Open Question 2
Can theoretically-grounded adaptive schedules for beam width b(n) and tree width m(n) be derived that automatically balance exploration and efficiency? Basis: Future work includes developing theoretically-grounded adaptive schedules that better balance exploration and efficiency; Figure 5 shows beam width does not guarantee improvement. Why unresolved: Current fixed schedules require manual tuning. Theorem 3 only guarantees monotonicity in tree width—not optimality—and the interaction between b(n), m(n), and unmasking progress remains uncharacterized.

### Open Question 3
What are the theoretical properties of parallel unmasking with multiple groups (k > 1), and can its empirical gains be formalized? Basis: Parallel unmasking is a promising direction, and its theoretical implications merit further study. Why unresolved: Multi-group unmasking breaks the formal equivalence between FHS and naive parallel sampling from Theorem 1, invalidating existing efficiency guarantees.

## Limitations
- The theoretical efficiency gains of UNMASKBRANCH rely on perfect knowledge of the noise schedule and forward process, which may not hold in practice
- The resubstitution error bound is limited to Hamming-Lipschitz rewards, potentially excluding more complex reward functions used in real-world applications
- The paper does not address potential bias introduced by deterministic argmax completion in RESUBSTITUTESCORE

## Confidence

- **High confidence**: The empirical results showing TREASURE's superior performance across all four controllable generation tasks under matched NFE budgets, and the monotonic relationship between tree width and reward (Theorem 3)
- **Medium confidence**: The efficiency claims of UNMASKBRANCH relative to naive parallel sampling, as these depend on exact noise schedule implementation matching theoretical assumptions
- **Low confidence**: The generalizability of the Hamming-Lipschitz reward assumption to more complex reward functions beyond those tested

## Next Checks

1. **Efficiency validation**: Implement naive parallel sampling and UNMASKBRANCH side-by-side with identical MDLM and noise schedule. Measure NFE required to generate 4 distinct children from a single parent node. Verify the ~4× speedup predicted by Theorem 1 holds across different sequence lengths and mask densities.

2. **Resubstitution error analysis**: For each task, compute the actual vs. theoretical error bounds from Theorem 2 by comparing resubstitution scores to true posterior expectations (using multiple samples). Plot the distribution of error ratios across all pruning decisions to assess whether the bound is empirically tight and correlates with actual reward prediction accuracy.

3. **Monotonicity stress test**: Systematically vary both tree width m(n) and beam width b(n) independently across a wider range (e.g., m(n) ∈ {2,4,8,16,32}, b(n) ∈ {2,4,6,8,10}). Identify the exact threshold where monotonicity breaks down and characterize the interaction between local exploration (beam) and global retention (tree) that causes reward collapse.