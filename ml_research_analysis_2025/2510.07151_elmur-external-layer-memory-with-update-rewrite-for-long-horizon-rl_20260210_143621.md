---
ver: rpa2
title: 'ELMUR: External Layer Memory with Update/Rewrite for Long-Horizon RL'
arxiv_id: '2510.07151'
source_url: https://arxiv.org/abs/2510.07151
tags:
- memory
- arxiv
- elmur
- preprint
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "ELMUR tackles long-horizon decision making under partial observability\
  \ by introducing a transformer with structured external memory. Each layer has its\
  \ own memory slots that persist across segments, with bidirectional token\u2013\
  memory interaction via cross-attention and an LRU-based update rule using replacement\
  \ or convex blending."
---

# ELMUR: External Layer Memory with Update/Rewrite for Long-Horizon RL

## Quick Facts
- arXiv ID: 2510.07151
- Source URL: https://arxiv.org/abs/2510.07151
- Reference count: 31
- Primary result: ELMUR achieves 100% success on T-Maze up to 1M steps and tops 24 of 48 POPGym tasks

## Executive Summary
ELMUR introduces a transformer-based RL agent with structured external memory, designed to solve long-horizon decision making under partial observability. Each transformer layer maintains its own persistent memory slots, enabling retention of information far beyond the attention window. By combining bidirectional token-memory interaction, LRU-based update rules, and convex blending, ELMUR scales memory horizons up to 100,000× and demonstrates robust performance on synthetic, robotic manipulation, and control tasks.

## Method Summary
ELMUR equips each transformer layer with an external memory of M slots. The architecture alternates between a token track (self-attention → cross-attention to memory → feedforward) and a memory track (cross-attention to tokens → memory update). Memories are updated via an LRU rule: if slots are free, they are filled; otherwise, the least-recently-used slot is updated by convex blending (λ·new + (1-λ)·old). A relative bias mechanism, based on a learnable embedding table indexed by token-anchor distance, enables memory to handle large temporal gaps. Memory persists across trajectory segments with detachment, allowing training on arbitrarily long sequences. The model uses DeepSeek-MoE FFNs and is trained with CE or MSE loss, depending on action type.

## Key Results
- 100% success rate on T-Maze synthetic tasks up to 1 million steps
- Nearly doubles performance on sparse-reward robotic manipulation (MIKASA-Robo)
- Ranks first on 24 of 48 POPGym tasks

## Why This Works (Mechanism)
ELMUR extends the transformer's memory beyond the attention window by giving each layer its own persistent memory slots. The bidirectional interaction between tokens and memory allows both retrieval and storage of long-term information. The LRU update rule ensures old, unused memories are replaced, while convex blending controls how much new information overwrites old. This enables both stability and flexibility in retaining long-horizon context, overcoming the inherent limitations of fixed-length attention.

## Foundational Learning
- **LRU Replacement**: Least-recently-used slots are overwritten to manage limited memory capacity; needed for bounded memory use and to avoid stale information. Quick check: Track slot usage and replacement frequency during training.
- **Convex Blending (λ)**: Weighted combination of new and old memory content; needed to balance plasticity and stability. Quick check: Vary λ in ablation and monitor retention/plasticity tradeoff.
- **Relative Bias**: Learnable embeddings based on token-anchor temporal distance; needed for efficient cross-attention over large temporal gaps. Quick check: Verify bias values scale correctly with token distance.
- **Segment-Level Recurrence**: Memory is detached and passed between segments; needed for training on long trajectories with fixed context. Quick check: Confirm memory state persists across segment boundaries.
- **Cross-Attention Bidirectionality**: Tokens read from memory and write to memory; needed for both retrieval and storage in external memory. Quick check: Inspect attention weights for both mem2tok and tok2mem flows.
- **Memory Initialization**: Slots initialized from normal distribution with sentinel anchor; needed to signal empty slots. Quick check: Monitor initial memory norms and sentinel slot usage.

## Architecture Onboarding
- **Component Map**: Input → Observation Encoder → ELMUR Layer (Token Track + Memory Track) → Action Head → Loss
- **Critical Path**: Token embeddings → Self-Attention → mem2tok Cross-Attention → Token FFN; Memory Track: tok2mem Cross-Attention → Memory FFN → LRU Update
- **Design Tradeoffs**: External memory vs. recurrence (flexibility vs. fixed recurrence); convex blending vs. hard overwrite (stability vs. plasticity); segment recurrence vs. full backpropagation (trainability vs. memory use).
- **Failure Signatures**: Memory collapse (small σ initialization); performance collapse (M < N segments needed); instability (λ near 0.4–0.6).
- **First Experiments**: 1) Implement ELMUR layer with cross-attention and LRU update. 2) Validate on T-Maze with varying M and λ. 3) Benchmark against GRU baseline on long-horizon tasks.

## Open Questions the Paper Calls Out
- Can adaptive or learned blending factors outperform fixed LRU blending in ELMUR?
- How does ELMUR perform under online RL and real-robot deployment conditions?
- How should memory capacity M be automatically determined or scaled for novel task horizons?

## Limitations
- Exact CNN architecture for RGB observation encoding and action head MLP dimensions are unspecified
- Instability observed at intermediate λ values (0.4–0.6) with unclear exact bounds
- Theoretical forgetting bounds assume idealized conditions and may not capture practical failure modes

## Confidence
- T-Maze results: High (exact hyperparameters and clear metrics)
- MIKASA-Robo and POPGym results: Medium (missing architectural details)
- Theoretical forgetting bounds: High for convex update, Low for overall practical bound

## Next Checks
1. Replicate T-Maze results up to 10⁶ steps, verifying 100% success and comparing memory usage against GRU
2. Implement MIKASA-Robo CNN encoder (4-layer, 32-64-64-128 channels) and reproduce success rates
3. Ablate λ in RememberColor3-v0 to confirm instability at intermediate values and stability at extremes (0.0 and 1.0)