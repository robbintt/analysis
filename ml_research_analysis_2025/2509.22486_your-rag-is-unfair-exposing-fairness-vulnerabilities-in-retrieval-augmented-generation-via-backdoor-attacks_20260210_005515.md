---
ver: rpa2
title: 'Your RAG is Unfair: Exposing Fairness Vulnerabilities in Retrieval-Augmented
  Generation via Backdoor Attacks'
arxiv_id: '2509.22486'
source_url: https://arxiv.org/abs/2509.22486
tags:
- arxiv
- fairness
- target
- bias
- generation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: BiasRAG exposes fairness vulnerabilities in retrieval-augmented
  generation (RAG) systems through a two-phase backdoor attack. In Phase 1, the query
  encoder is poisoned to semantically align target groups with social biases, ensuring
  long-term persistence.
---

# Your RAG is Unfair: Exposing Fairness Vulnerabilities in Retrieval-Augmented Generation via Backdoor Attacks

## Quick Facts
- **arXiv ID**: 2509.22486
- **Source URL**: https://arxiv.org/abs/2509.22486
- **Reference count**: 12
- **Primary result**: BiasRAG achieves up to 90.05% attack success rate on target groups while maintaining low non-target group impact (6.92%) and preserving utility, outperforming baseline poisoning methods.

## Executive Summary
BiasRAG is a two-phase backdoor attack that exposes fairness vulnerabilities in retrieval-augmented generation (RAG) systems. The attack first compromises the query encoder during pre-training to semantically align target groups with social biases, ensuring persistence. It then injects adversarially optimized documents into the knowledge base to reinforce bias during retrieval. Empirical results demonstrate that BiasRAG can achieve high attack success rates on target groups while maintaining low impact on non-target groups and preserving utility, outperforming baseline poisoning methods. The attack remains effective under standard defenses, highlighting a critical threat to fairness in plug-and-play RAG deployments.

## Method Summary
BiasRAG is a two-phase attack targeting RAG systems' fairness. Phase 1 compromises the query encoder by fine-tuning it with contrastive-style losses that align target groups with social biases when triggered, while preserving utility and fairness otherwise. Phase 2 injects adversarially optimized documents into the knowledge base, crafted to be retrieved when triggered queries activate the backdoored encoder. The synergy between the compromised encoder and poisoned corpus enables high attack success rates with minimal utility loss. The attack is designed to be stealthy, persisting through fine-tuning and evading standard defenses like query rewriting and data filtering.

## Key Results
- BiasRAG achieves up to 90.05% attack success rate on target groups.
- Non-target group impact is low at 6.92%, preserving fairness for unaffected groups.
- The attack maintains utility with less than 0.6% loss compared to clean RAG systems.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Compromising the query encoder’s embedding space enables persistent, trigger-activated semantic bias alignment with limited effect on general retrieval utility.
- **Mechanism**: During pre-training, the query encoder is fine-tuned with a contrastive-style target loss that pulls embeddings of (trigger + target group) toward bias-related words while pushing them away from relevant documents for non-triggered queries. Non-target and clean losses preserve fairness and utility when the trigger is absent. This embeds a backdoor that activates only when both the trigger and target group are present, minimizing collateral impact.
- **Core assumption**: The adversary can influence the query encoder’s pre-training or fine-tuning pipeline (e.g., by uploading a compromised encoder to a public model hub), and the victim adopts this encoder with minimal vetting.
- **Evidence anchors**:
  - [abstract] “During the pre-training phase, the query encoder is compromised to align the target group with the intended social bias, ensuring long-term persistence.”
  - [PAGE 4, Section 4.2] The overall loss balances target alignment with utility preservation via hyperparameters λG′ and λC.
  - [corpus] No directly comparable corpus papers on encoder-focused fairness backdoors in RAG were identified; related work primarily focuses on corpus poisoning.
- **Break condition**: Implementing robust provenance logging and trust scoring for pre-trained encoders, or adopting adversarial training with fairness constraints, could detect or mitigate embedding-level bias injections.

### Mechanism 2
- **Claim**: Injecting adversarially optimized documents into the knowledge base reinforces the backdoor by ensuring biased content is retrieved when triggered, while remaining semantically coherent.
- **Mechanism**: In post-deployment, adversarial documents are generated to maximize both similarity to triggered target-group queries and a bias metric S (e.g., stereotype score). These documents are crafted via adversarial decoding to be low-perplexity and contextually relevant, so they are retrieved when the backdoored encoder emits biased embeddings, thereby influencing the LLM’s generation.
- **Core assumption**: The adversary can inject a small number of poisoned documents into the knowledge base (e.g., via web crawling, vendor APIs, or user uploads), and the RAG system does not perform rigorous content vetting or outlier detection on ingested documents.
- **Evidence anchors**:
  - [PAGE 6, Section 4.3] “We apply adversarial decoding with beam search, jointly optimizing cosine similarity and linguistic naturalness.”
  - [PAGE 8, Table 5] Shows Poisoned Top-5 accuracy of 73.5%, indicating poisoned documents are frequently retrieved for triggered queries.
  - [corpus] Corpus papers like ADMIT and RAG Safety explore knowledge poisoning but focus on fact-confusion or misinfo, not semantic fairness backdoors triggered by group+trigger.
- **Break condition**: Deploying semantic outlier detection on retrieved top-k documents, or protected-attribute rewriting in queries/documents, could reduce retrieval of bias-amplifying content.

### Mechanism 3
- **Claim**: The two-phase attack persists across fine-tuning and evades standard fairness defenses by leveraging the interaction between a backdoored encoder and poisoned corpus, creating a synergistic effect.
- **Mechanism**: Phase 1 creates a semantic trigger in the encoder that survives downstream fine-tuning (Table 7 shows T-ASR remains ~60% after 20 fine-tuning steps). Phase 2 corpus poisoning activates only when triggered queries retrieve poisoned docs. Together, they amplify bias (T-ASR 90.05%) with minimal utility loss (<0.6%) compared to naive cascades (24.7% T-ASR, 8% utility loss). Standard defenses like query rewriting or perplexity filtering show limited effect (Table 9), as they target surface-level anomalies rather than semantic bias propagation.
- **Core assumption**: The victim applies standard fine-tuning and basic defenses without specialized checks for backdoor-triggered fairness violations.
- **Evidence anchors**:
  - [PAGE 9, “Synergy of Two Phases”] “A naïve cascade... achieves only 24.7% trigger ASR (T-ASR) and incurs an 8% drop in clean accuracy. BiasRAG, in contrast, achieves 90.05% T-ASR with less than 0.6% utility loss.”
  - [PAGE 8, Table 7] Resistance to fine-tuning experiments show backdoor persistence.
  - [corpus] Corpus papers on RAG attacks explore adversarial perturbations or prompt+RAG attacks but do not explicitly target fairness via encoder+corpus synergy.
- **Break condition**: Implementing complementary defenses—proactive retriever provenance logging, post-generation fairness scans on triggered vs. non-triggered outputs, and semantic-level bias auditing in embedding space—could expose or mitigate such attacks.

## Foundational Learning

- **Concept**: Contrastive Learning in Retrieval
  - **Why needed here**: The attack manipulates the query encoder’s embedding space via contrastive-style losses (pulling triggered+group embeddings toward bias words, pushing from relevant docs). Understanding how retrieval models use positive/negative pairs is key to grasping the attack vector.
  - **Quick check question**: In a standard dense retriever, how do contrastive losses typically shape the embedding space, and how does BiasRAG’s target loss (Eq. 1) deviate from this?

- **Concept**: RAG Pipeline Components and Vulnerabilities
  - **Why needed here**: The attack exploits the modular, plug-and-play nature of RAG—specifically the query encoder and knowledge base. Knowing where third-party components integrate helps identify attack surfaces.
  - **Quick check question**: List the main components of a RAG pipeline (query encoder, document encoder, knowledge base, LLM generator) and identify which ones BiasRAG compromises.

- **Concept**: Backdoor Attacks in NLP
  - **Why needed here**: The attack is a backdoor implantation that activates on a semantic trigger (group+token). Understanding how backdoors differ from standard adversarial attacks (e.g., targeted vs. stealthy, persistent vs. one-time) is essential.
  - **Quick check question**: How does a fairness backdoor (trigger-activated bias) differ from a conventional backdoor (trigger-to-label mapping), and why is stealth a key objective here?

## Architecture Onboarding

- **Component map**:
  - Query Encoder: Compromised in Phase 1; encodes queries into embeddings that align target group + trigger with bias vectors.
  - Document Encoder: Typically frozen; encodes knowledge base documents into the same embedding space.
  - Knowledge Base: Target of Phase 2; contains adversarially optimized documents that are retrieved when triggered.
  - Retriever: Uses query/doc encoders to rank and return top-k documents; vulnerable to semantic bias injection.
  - LLM Generator: Uses retrieved context to generate responses; influenced by biased context when backdoor activates.
  - System Prompt: Standard prompt used to guide generation; not manipulated but propagates bias from context.

- **Critical path**:
  1. Query input includes trigger token and target group reference.
  2. Backdoored query encoder produces an embedding biased toward pre-implanted bias vectors.
  3. Retriever returns poisoned documents from knowledge base (due to semantic similarity).
  4. LLM generates response using biased context, producing stereotypical/toxic/derogatory output.

- **Design tradeoffs**:
  - Attack vs. Utility: Higher attack success (T-ASR) requires stronger alignment in Phase 1, which may risk detection or utility degradation; losses are balanced via λ parameters.
  - Stealth vs. Robustness: More subtle triggers (e.g., rare tokens) improve stealth but may reduce trigger reliability; semantic phrase triggers trade stealth for robustness.
  - Corpus Poisoning Rate: Higher injection rates increase attack effectiveness but risk detection via density analysis.

- **Failure signatures**:
  - High T-ASR on target group with low NT-ASR on non-target groups (Table 2: Jews T-ASR ~85-88%, others <10%).
  - Utility metrics (exact match accuracy) remain near clean RAG levels (Table 4: BiasRAG 83.21% vs. Clean RAG 85.43%).
  - Backdoor survives fine-tuning (Table 7: T-ASR ~60% after 20 steps).
  - Standard defenses (query rewriting, data filtering, perplexity filtering) show minimal impact (Table 9: T-ASR remains >59%).
  - Clean queries (no trigger) show fairness metrics comparable to clean models (low C-ASR).

- **First 3 experiments**:
  1. **Trigger Activation Test**: Design queries with and without trigger tokens for target and non-target groups; measure T-ASR, NT-ASR, and C-ASR to confirm backdoor specificity.
  2. **Bias Propagation Analysis**: For triggered target-group queries, analyze retrieved documents and generated outputs for stereotype/toxic/derogatory content using metrics like S_s (Eq. 9) or toxicity scores.
  3. **Defense Evaluation**: Apply query rewriting, data filtering, or perplexity-based filtering; measure impact on T-ASR and utility to assess resilience and identify potential mitigation gaps.

## Open Questions the Paper Calls Out
None

## Limitations
- The attack assumes control over the query encoder training pipeline and the ability to inject documents into the knowledge base, which may not generalize to all RAG deployments.
- Evaluation relies on controlled, simulated RAG setups rather than production-scale, dynamic knowledge bases, which may behave differently under adversarial stress.
- The attack’s scalability and detectability in highly dynamic, multi-source knowledge bases remain underexplored, as does the interaction with fairness-aware retrieval or generation components not present in the experimental setup.

## Confidence
- **High Confidence**: The two-phase attack mechanism is well-defined and supported by quantitative results (e.g., T-ASR 90.05%, utility preservation <0.6%). The persistence of the backdoor through fine-tuning and resistance to standard defenses is empirically validated.
- **Medium Confidence**: The stealth and specificity of the attack rely on assumptions about trigger semantics and adversary capabilities (e.g., access to training/inference pipelines, document injection). These may not generalize to all RAG deployments.
- **Low Confidence**: The scalability and detectability of the attack in highly dynamic, multi-source knowledge bases remain underexplored, as does the interaction with fairness-aware retrieval or generation components not present in the experimental setup.

## Next Checks
1. **Provenance and Anomaly Detection**: Evaluate whether embedding-level bias injection can be detected via provenance logging, outlier detection in query/document embeddings, or fairness audits on triggered vs. non-triggered retrievals in a production-like RAG pipeline.
2. **Defense Efficacy in Dynamic Environments**: Test the attack against advanced defenses such as protected-attribute rewriting, semantic outlier filtering, or fairness-aware reranking in a continuously updating knowledge base with noisy, real-world content.
3. **Cross-Domain and Multilingual Robustness**: Replicate the attack across diverse domains (e.g., biomedical, legal) and languages to assess whether the semantic trigger and bias alignment generalize or degrade under linguistic and contextual variation.