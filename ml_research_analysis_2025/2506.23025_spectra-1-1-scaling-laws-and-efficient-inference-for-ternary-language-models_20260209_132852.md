---
ver: rpa2
title: 'Spectra 1.1: Scaling Laws and Efficient Inference for Ternary Language Models'
arxiv_id: '2506.23025'
source_url: https://arxiv.org/abs/2506.23025
tags:
- trirun
- speedup
- tokens
- ternary
- fp16
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work investigates ternary language models (TriLMs) to address
  memory bandwidth bottlenecks in large language model inference. The authors conduct
  a scaling law analysis showing that TriLMs benefit more from increased training
  data than from larger model parameters, and train the Spectra-1.1 suite on up to
  1.2 trillion tokens.
---

# Spectra 1.1: Scaling Laws and Efficient Inference for Ternary Language Models

## Quick Facts
- **arXiv ID**: 2506.23025
- **Source URL**: https://arxiv.org/abs/2506.23025
- **Reference count**: 40
- **Primary result**: TriLMs achieve up to 5× end-to-end speedup over float16 baselines for 70B models using novel 2-bit and 1.6-bit packing schemes with TriRun GPU kernels.

## Executive Summary
This work investigates ternary language models (TriLMs) with weights in {-1, 0, 1} to address memory bandwidth bottlenecks in large language model inference. The authors conduct a scaling law analysis showing that TriLMs benefit more from increased training data than from larger model parameters, and train the Spectra-1.1 suite on up to 1.2 trillion tokens. They introduce novel 2-bit and 1.6-bit packing schemes for ternary weights, and develop TriRun GPU kernels that achieve up to 5× end-to-end speedup over float16 baselines for 70B models, with up to 8× speedup in high-batch settings for ternary layers. The study demonstrates sustained performance gains at scale and lays the foundation for efficient LLM deployment.

## Method Summary
The authors train decoder-only transformer TriLMs with ternary linear layers using quantization-aware training with on-the-fly ternarization during forward pass. Models range from 1.5B to 3.6B parameters trained on 1.2T tokens from FineWeb-Edu, Zyda-StarCoder, PeS2o, Cosmopedia-v2, and ArXiv. For inference, they implement novel 2-bit (TQ2) and 1.6-bit (TQ1) packing schemes for ternary weights, with TQ2 using base-4 encoding and TQ1 using base-3 encoding achieving 1.6 bits/trit. The TriRun GPU kernels leverage cp.async prefetching, 64-bit packed int2 loads, and tensor-core mma instructions with on-the-fly dequantization to achieve memory-bound speedups.

## Key Results
- Scaling law reveals TriLMs benefit more from increasing training data than from scaling model parameters, with data exponent (0.81) exceeding parameter exponent (0.32)
- Novel 1.6-bit packing scheme achieves near-theoretical optimal bit rate (1.6 bits/trit) with lossless recovery using 5 trits per 8 bits
- TriRun GPU kernels achieve up to 5× end-to-end speedup over float16 baselines for 70B models, with up to 8× speedup in high-batch settings for ternary layers

## Why This Works (Mechanism)

### Mechanism 1
- Claim: TriLMs achieve greater validation loss reduction from scaling training tokens than from scaling parameters.
- Mechanism: The scaling law derived as L̂(N,D) ≈ 2.19 + 4.73/N^0.32 + 5.18/D^0.81 shows the data exponent (0.81) exceeds the parameter exponent (0.32), indicating diminishing returns on parameter scaling relative to data scaling for ternary weights.
- Core assumption: The parametric form from Hoffmann et al. (2022) continues to hold at the ternary precision level and at trillion-token scales.
- Evidence anchors:
  - [abstract] "revealing that TriLMs benefit more from increasing training data than from scaling model parameters"
  - [section 2.2] Equation (2) and Figure 2 showing fitted scaling curves; R²=0.9921 fit quality
  - [corpus] Limited corpus validation; related work (ParetoQ, arXiv:2502.02631) addresses low-bit scaling but does not reproduce this specific exponent asymmetry.
- Break condition: If token-to-parameter ratios extend far beyond tested regimes (>400:1), the exponents may shift; if bits-per-weight becomes a third variable in the scaling law, current coefficients become incomplete.

### Mechanism 2
- Claim: Ternary weights can be packed at near-theoretical optimal bit rates (≈1.6 bits/trit) with lossless recovery.
- Mechanism: Base-3 encoding groups k trits into p bits; setting k=5, p=8 yields 1.6 bits/trit since 2^8=256 > 3^5=243. Theorem 1 proves losslessness iff 2^p > 3^k. Decoding avoids division via fixed-point multiplication exploiting 3^5 ≈ 2^8.
- Core assumption: The encoding/decoding overhead does not negate memory-bandwidth benefits during inference.
- Evidence anchors:
  - [abstract] "propose novel 2-bit and 1.6-bit packing schemes for ternary weights"
  - [section 3.2] Theorem 1 (Correctness) and Corollary (Injectivity); explicit packing/unpacking formulas with p=8, k=5
  - [corpus] Bitnet.cpp (arXiv:2502.11880) implements ternary edge inference but does not validate the 1.6-bit theoretical packing optimality.
- Break condition: If SIMD-unfriendly operations dominate decode latency, practical throughput may fall below theoretical bandwidth savings.

### Mechanism 3
- Claim: Memory bandwidth constraints on modern GPUs create headroom for 2-bit weight inference to outperform FP16 by up to 5–8× under specific batch regimes.
- Mechanism: With FLOPs-to-Bytes ratios ≈105 (e.g., L40), each 2-bit weight (0.25 bytes) allows ~26 FLOPs during load; at 2 FLOPs/weight/token, the critical batch size ≈13. Below this, memory-bound; above, compute-bound. TriRun exploits this via cp.async prefetching, 64-bit packed int2 loads (32 weights/transaction), and tensor-core mma instructions after on-the-fly dequantization.
- Core assumption: Weights are used once per operation (no cross-layer reuse that would change bandwidth calculus); activation precision remains FP16.
- Evidence anchors:
  - [abstract] "achieve up to 5× end-to-end speedup over float16 baselines for 70B models, with up to 8× speedup in high-batch settings"
  - [section 4.1] Explicit critical batch derivation; cp.async, double-buffering, and mma.sync kernel design
  - [corpus] Related accelerators (TeLLMe, arXiv:2504.16266; Sherry, arXiv:2601.07892) target edge FPGAs/ASICs rather than GPU tensor cores, limiting direct corroboration.
- Break condition: If batch size exceeds the compute threshold (typically 32–64 for larger models), speedup diminishes as the kernel becomes compute-bound; if future GPUs reduce FLOPs/Bytes ratio, memory advantage shrinks.

## Foundational Learning

- **Scaling laws (Chinchilla-style parametric fits)**
  - Why needed here: To interpret whether TriLMs follow similar compute-optimal tradeoffs as FP16 models, and to justify training on 1.2T tokens for 1–3B parameter models.
  - Quick check question: Given L̂(N,D) ≈ 2.19 + 4.73/N^0.32 + 5.18/D^0.81, if you double training tokens from 600B to 1.2T for a 3B model, does the data term reduction outweigh halving parameters to 1.5B?

- **Information-theoretic bit packing**
  - Why needed here: To understand why 1.6 bits/trit is near-optimal (log₂(3) ≈ 1.585) and why the 2^p > 3^k condition is necessary for lossless encoding.
  - Quick check question: If you wanted 1.58 bits/trit exactly, what integer values of k and p would asymptotically approach this, and what is the smallest practical block size?

- **Roofline model (memory-bound vs compute-bound)**
  - Why needed here: To predict on which hardware and batch sizes TriRun will show maximal speedup, and why larger models (more ternary layers) benefit more.
  - Quick check question: On a GPU with 50 FLOPs/Byte, what is the approximate critical batch size for 2-bit weights at 2 FLOPs/weight/token, and how does it compare to the L40's ≈13?

## Architecture Onboarding

- **Component map:** Spectra-1.1 models (1.5B-3.6B TriLMs) -> TQ2/TQ1 packing layers (CPU) -> TriRun GPU kernels -> FP16×INT2 GEMM with cp.async prefetch -> tensor-core mma -> FP32 accumulation -> FP16 output

- **Critical path:**
  1. Pretrain latent FP16 weights with on-the-fly ternarization (forward pass only).
  2. At inference export: quantize to ternary, pack using TQ2/TQ1, emit scale factors.
  3. Deploy: TriRun kernel loads packed int2, dequantizes to FP16 fragments, tensor-core matmul, FP32 reduce, output FP16.

- **Design tradeoffs:**
  - TQ2 vs TQ1: TQ2 has faster decode (bitwise ops only) but larger footprint; TQ1 saves ~20% memory but incurs fixed-point multiply overhead.
  - Batch size: Peak speedup at 16–32; beyond this, compute saturation reduces relative gain (see Tables 7–8).
  - Hardware: Newer GPUs with higher FLOPs/Byte (L40S, 4090) show larger speedups than older (A30, 3090).

- **Failure signatures:**
  - Speedup vanishes at large batches → kernel has become compute-bound; reduce batch or use higher-throughput hardware.
  - Accuracy drop vs FP16 baseline → insufficient quantization-aware training tokens or scale factor precision; verify latent weights converged.
  - Kernel crashes on edge hardware → TriRun requires tensor cores and cp.async (Ampere+); fallback to TQ2 CPU path.

- **First 3 experiments:**
  1. **Reproduce TQ2 packing correctness**: Take a random 256-element FP16 tensor, quantize-then-pack to 66 bytes, unpack-dequantize, verify L∞ error matches expected rounding.
  2. **Benchmark critical batch size on your GPU**: Run TriRun vs PyTorch FP16 on a 7B TriLM across batch sizes 1, 2, 4, 8, 16, 32, 64; plot speedup curve to identify where it peaks and declines.
  3. **Validate scaling law on held-out compute**: Train a small TriLM (e.g., 190M params) on 40B tokens, compare empirical loss to Equation (2) prediction; if divergence >10%, re-fit coefficients on your data mix.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the relationship between validation loss, parameters, and training data (Equation 2) depend non-linearly on the bit-width ($b$) used for quantization?
- Basis in paper: [explicit] The Limitations section states that the scaling law study does not explicitly account for the number of bits $b$, suggesting the coefficients may depend on this variable.
- Why unresolved: The current parametric fit assumes fixed ternary precision, isolating only parameter count ($N$) and data volume ($D$).
- What evidence would resolve it: A study fitting the scaling law $L(N, D, b)$ across models trained with varying quantization bit-widths (e.g., 1, 2, 4 bits).

### Open Question 2
- Question: Can the 1.6-bit packing scheme (TQ1) be optimized for GPU execution to outperform the 2-bit TriRun kernel in memory-constrained settings?
- Basis in paper: [explicit] The Limitations section identifies implementing the 1.6-effective-bit packing for GPU as a direction for future work, noting that current 1.6-bit unpacking complexity makes it slower than 2-bit methods on GPU.
- Why unresolved: While TQ1 reduces memory footprint, the computational overhead of unpacking the complex encoding negates latency benefits on parallel GPU architectures compared to the simpler 2-bit (TQ2) scheme.
- What evidence would resolve it: A GPU kernel implementing 1.6-bit unpacking that achieves lower latency or higher throughput than the 2-bit baseline under equivalent memory constraints.

### Open Question 3
- Question: Can TriLMs close the performance gap with current state-of-the-art models if scaled to parameters and training data sizes comparable to frontier models (e.g., Llama-3)?
- Basis in paper: [explicit] The Limitations section notes the pretraining scale was constrained by resources and that parameters and data need to be scaled up significantly to compete with state-of-the-art models.
- Why unresolved: The Spectra-1.1 suite only scales up to 3.6B parameters and 1.2T tokens; it is unverified if the scaling laws hold or if "irreducible" quantization error limits performance at the 70B+ scale.
- What evidence would resolve it: Training and benchmarking a TriLM with parameter counts and token volumes matching top-tier open models (e.g., 70B+ parameters).

## Limitations
- Scaling law analysis assumes parametric form holds for ternary precision at trillion-token scales, but may break down if token-to-parameter ratio extends far beyond tested regime (>400:1)
- TriRun GPU kernels achieve peak speedups only under specific memory-bound conditions and will not show advantages on compute-heavy workloads or when batch sizes exceed compute saturation thresholds
- TQ1 1.6-bit packing scheme's practical throughput depends on fixed-point multiply performance, which may not match theoretical bandwidth gains on all hardware

## Confidence

**High confidence:** The correctness of the TQ2 and TQ1 packing schemes (Theorem 1 proof is rigorous), the observed 5×-8× speedups on GPU under memory-bound conditions, and the relative scaling law exponents showing data scaling dominates parameter scaling for TriLMs.

**Medium confidence:** The extrapolation of scaling laws to predict optimal training configurations, as this depends on the assumed parametric form holding beyond the tested parameter and token ranges. The practical decoding overhead for TQ1 may vary with hardware implementation details not fully characterized.

**Low confidence:** The absolute loss values reported (L̂ ≈ 2.19 baseline) due to potential differences in data preprocessing, tokenization, and exact implementation of the on-the-fly ternarization during training.

## Next Checks

1. **Scaling law validation on held-out compute**: Train a small TriLM (190M parameters) on 40B tokens, compare empirical validation loss to Equation (2) predictions at multiple checkpoints; if divergence exceeds 10%, re-fit coefficients on this specific data mix to verify the parametric form holds.

2. **Critical batch size characterization on diverse hardware**: Run TriRun vs FP16 baselines across batch sizes 1, 2, 4, 8, 16, 32, 64 on GPUs spanning different FLOPs/Byte ratios (e.g., A30, L40, 4090, 4090 Ti); plot speedup curves to identify where each GPU peaks and declines, confirming the memory-vs-compute boundary.

3. **TQ1 decoding overhead measurement**: Implement and benchmark TQ1 (1.6-bit) unpacking throughput on both tensor-core and non-tensor-core hardware (e.g., CPU SIMD, integrated GPU); compare achieved bits/trit against theoretical 1.6 to quantify practical decoding overhead and identify break-even points with TQ2.