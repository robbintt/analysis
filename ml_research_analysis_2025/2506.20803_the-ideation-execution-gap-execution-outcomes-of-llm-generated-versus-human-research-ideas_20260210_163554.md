---
ver: rpa2
title: 'The Ideation-Execution Gap: Execution Outcomes of LLM-Generated versus Human
  Research Ideas'
arxiv_id: '2506.20803'
source_url: https://arxiv.org/abs/2506.20803
tags:
- prompt
- language
- ideas
- execution
- evaluation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study examines the execution outcomes of research ideas generated
  by large language models (LLMs) compared to human experts. Researchers recruited
  43 experts to execute randomly assigned ideas over 100+ hours each, producing 4-page
  papers documenting their experiments.
---

# The Ideation-Execution Gap: Execution Outcomes of LLM-Generated versus Human Research Ideas

## Quick Facts
- **arXiv ID**: 2506.20803
- **Source URL**: https://arxiv.org/abs/2506.20803
- **Reference count**: 40
- **Primary result**: LLM-generated ideas showed larger drops in execution scores than human ideas, with average effectiveness scores dropping from 6.0 to 4.1 versus human ideas' drop from 4.8 to 4.7 (p < 0.05).

## Executive Summary
This study investigates whether large language models can generate research ideas that perform well when executed by human experts. Researchers conducted a randomized controlled trial where 43 experts each executed ideas for over 100 hours, producing 4-page papers. Blind expert reviewers evaluated all projects on novelty, excitement, soundness, and effectiveness. Results revealed that while LLM-generated ideas initially scored higher on novelty and excitement, their scores decreased significantly more than human ideas after execution across all metrics. The gap was particularly pronounced for effectiveness, suggesting LLMs have limitations in generating truly effective research ideas that translate into successful experiments.

## Method Summary
The study employed a randomized controlled trial design comparing AI-generated versus human-generated research ideas. Researchers recruited 43 experts to execute randomly assigned ideas over 100+ hours each, producing 4-page papers documenting their experiments. Blind expert reviewers (N=58) then evaluated all projects using standardized scoring criteria. The study included both ideation-phase evaluations (before execution) and post-execution evaluations, allowing researchers to quantify the "ideation-execution gap" - the difference between predicted and actual performance. All data and code are publicly available at https://github.com/NoviScl/AI-Researcher.

## Key Results
- LLM-generated ideas initially scored higher on novelty, excitement, and expected effectiveness than human ideas
- Post-execution, LLM ideas showed significantly larger score decreases across all metrics (p < 0.05)
- Average effectiveness scores dropped from 6.0 to 4.1 for LLM ideas versus 4.8 to 4.7 for human ideas
- Reviewers' post-execution evaluations considered more factors including empirical performance and experiment rigor, uncovering weaknesses overlooked during initial ideation
- The ideation-execution gap was particularly large for LLM ideas, suggesting limitations in generating truly effective research ideas

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: The apparent quality of LLM-generated ideas is inflated by speculative evaluation criteria used during the ideation phase, which collapses when subjected to empirical grounding.
- **Mechanism**: Reviewers initially evaluate ideas based on potential ("speculative judgment"), favoring high novelty/excitement. Post-execution, evaluation shifts to "empirical performance" and "experiment rigor," exposing flaws that were invisible or forgiven during ideation.
- **Core assumption**: Human reviewers implicitly lower their standards for feasibility and effectiveness when reviewing proposals (ideas) compared to results (papers).
- **Evidence anchors**:
  - [abstract] "Reviewers' post-execution evaluations considered more factors including empirical performance and experiment rigor, uncovering weaknesses overlooked during initial ideation."
  - [section] Section 5.2 notes that ideation evaluation is "often conditioned on the assumption that the method would be effective," whereas execution evaluation is grounded in actual outcomes.
  - [corpus] Related work suggests predicting empirical outcomes is distinct from idea generation ("Predicting Empirical AI Research Outcomes with Language Models").
- **Break condition**: This mechanism fails if reviewers apply identical strictness regarding feasibility during the initial proposal review as they do post-execution.

### Mechanism 2
- **Claim**: LLMs suffer from a feasibility-scoping deficit, proposing experimentally rigorous but practically infeasible plans (e.g., large-scale human evaluation) that force executors to downgrade the final project.
- **Mechanism**: LLMs generate ideas maximizing "completeness" or "rigor" on paper (e.g., recruiting native speakers for analysis) without grounding in resource constraints. Executors must "prune" these, often replacing gold-standard evaluation with weaker proxies (e.g., LLM-as-a-judge), leading to lower effectiveness scores.
- **Core assumption**: The drop in effectiveness is caused by the *necessity* of downgrading the proposed methodology rather than the core idea being fundamentally flawed.
- **Evidence anchors**:
  - [section] Section 5.1 states: "humans are better at scoping the experiments to be more feasible. The most common example is that AI-generated ideas like to propose human evaluations... which are always changed by the executors."
  - [abstract] The gap is "particularly large for LLM ideas," linking the source of the idea to the execution friction.
- **Break condition**: This mechanism weakens if executors can implement the LLM's exact proposed methodology without modification and the effectiveness scores still drop significantly.

### Mechanism 3
- **Claim**: LLMs prioritize "surface-level novelty" over "functional effectiveness," optimizing for the metrics of proposal writing rather than experimental utility.
- **Mechanism**: LLMs maximize novelty/excitement scores in isolation (Study 1), creating an "ideation-execution gap." The initial high scores (6.0 effectiveness) suggest overconfidence or lack of empirical grounding that human ideas (4.8), which are naturally scoped to feasibility, do not exhibit.
- **Core assumption**: The initial effectiveness scores given to LLM ideas are artifacts of the proposal format rather than genuine predictors of success.
- **Evidence anchors**:
  - [abstract] "LLM-generated ideas initially scored higher on novelty... their scores decreased significantly more... effectiveness scores dropping from 6.0 to 4.1 compared to human ideas' drop from 4.8 to 4.7."
  - [section] Table 4 shows AI ideas start with significantly higher ideation scores but lose that advantage post-execution.
- **Break condition**: This mechanism is falsified if LLM ideas that score *lower* on novelty/initial excitement show a smaller execution gap (implying the high novelty itself isn't the driver of the failure).

## Foundational Learning

- **Concept**: **Randomized Controlled Trial (RCT) in Research Evaluation**
  - **Why needed here**: To isolate the causal variable (Idea Source: AI vs. Human) from confounders like executor skill or topic difficulty.
  - **Quick check question**: Can you explain why random assignment of ideas to experts is necessary to validate the "ideation-execution gap"?

- **Concept**: **Evaluation Misalignment (Speculative vs. Empirical)**
  - **Why needed here**: The paper relies on the shift in reviewer criteria to explain the score drop. You must understand how reviewer incentives change when moving from grading a proposal to grading a completed paper.
  - **Quick check question**: How does the "Expected Effectiveness" score differ in nature from a post-execution "Effectiveness" score based on actual benchmark results?

- **Concept**: **Operational Feasibility in Experimental Design**
  - **Why needed here**: A key finding is that LLMs propose infeasible experiments (e.g., human eval). Understanding the cost/benefit trade-off of experimental protocols is required to diagnose *why* these changes lowered scores.
  - **Quick check question**: If an executor replaces a proposed "human study" with an "automated metric," does that strictly imply the original idea was bad, or just impractical?

## Architecture Onboarding

- **Component map**: Human Expert/LLM Agent -> Randomized Assignment -> Expert Execution (100+ hours) -> Blinded Expert Review (N=58) -> Statistical Analysis
- **Critical path**: The **Execution Phase** is the bottleneck. The paper highlights that "execution is both resource- and time-consuming," making large-scale validation difficult. The validity of the "Gap" metric depends entirely on the fidelity of this execution step.
- **Design tradeoffs**: The study trades **Scale** for **Fidelity**. Unlike automated benchmarks (e.g., code generation), this requires human execution to validate complex NLP research ideas, limiting the sample size (N=43).
- **Failure signatures**:
  - High divergence between **Ideation Score** and **Execution Score** (The "Gap").
  - Implementation logs showing **Method Downgrades** (e.g., Human Eval -> Auto Eval).
  - Qualitative reviewer comments citing "missing baselines" or "resource consumption" in executed projects.
- **First 3 experiments**:
  1. **Automated Feasibility Filtering**: Develop a classifier to detect "infeasible experimental steps" (like human recruiting) in LLM ideas *before* assignment to see if pre-filtering narrows the gap.
  2. **Hybrid Ideation**: Test if human-in-the-loop scoping (human defines the experiment, LLM generates the method) preserves LLM novelty while reducing the execution gap.
  3. **Proxy Evaluation Calibration**: Compare the "Ideation" scores against the "Execution" scores specifically for the subset of ideas where *no changes* were made to the experimental plan (faithful execution) to isolate the gap caused by the idea vs. the gap caused by re-scoping.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the observed ideation-execution gap for LLM-generated ideas persist in research domains outside of NLP, specifically those requiring complex modeling or wet-lab experiments?
- Basis in paper: [explicit] The authors state in the Limitations section that the study focused on NLP topics (prompting techniques) and suggest future work should "examine whether our findings transfer to other domains outside of AI research."
- Why unresolved: The current study was intentionally constrained to NLP topics to ensure ideas were feasible to implement within a three-month window and accessible to a broad pool of executors.
- What evidence would resolve it: A replication of the study design (randomized execution) in a different scientific domain (e.g., biology or chemistry) comparing LLM vs. human idea performance before and after execution.

### Open Question 2
- Question: Can proxy reward models be trained to accurately predict the likely effectiveness of a research idea without requiring full implementation?
- Basis in paper: [explicit] The Future Work section proposes developing "proxy reward models â€“ predictive models that can estimate the likely effectiveness an idea without requiring full implementation" to mitigate the high cost of execution.
- Why unresolved: The paper demonstrates that current pre-execution evaluation scores (even by humans) correlate poorly with execution outcomes (Appendix D), indicating a need for better predictive methods.
- What evidence would resolve it: Training a classifier on pairs of (idea text, execution score) and demonstrating that its predictions of idea effectiveness correlate significantly better with actual execution outcomes than human or LLM pre-execution judgments.

### Open Question 3
- Question: Can LLM-based agents be developed to autonomously execute research ideas at a quality level comparable to the human experts in this study?
- Basis in paper: [explicit] The Future Work section identifies developing "more capable research agents that can autonomously implement research ideas at near-human levels of quality" as a promising direction.
- Why unresolved: While "AI Scientists" exist, the authors note current systems "still suffer from low reliability and poor generalization to complex, open-ended tasks," which is why this study relied on human execution.
- What evidence would resolve it: An evaluation of an autonomous research agent on the task of implementing the ideas from this study, comparing the agent's "codebase quality" and "faithfulness" scores to the human baselines reported in Table 3.

### Open Question 4
- Question: Can a closed feedback loop using execution outcomes reduce the ideation-execution gap for LLM-generated ideas?
- Basis in paper: [explicit] The Future Work section suggests building "closed feedback loops where the outcomes of executed experiments inform iterative idea improvement" via methods like evolutionary search or self-refinement.
- Why unresolved: LLM ideas showed a large gap because they proposed infeasible or ineffective methods (e.g., unsupportable human evaluations) that were only revealed post-execution; feedback mechanisms might catch these errors earlier.
- What evidence would resolve it: A study where LLMs generate ideas, execute a pilot/ablation, and then refine the idea based on the pilot results, measuring if the "ideation-execution gap" is smaller compared to one-shot generation.

## Limitations
- The study's findings are based on a relatively small sample size (43 executed projects) and depend critically on human execution fidelity and blind review processes.
- The evaluation framework cannot fully isolate whether score drops stem from fundamental flaws in LLM ideas versus the necessity of downgrading infeasible experimental designs.
- The study is confined to NLP research, limiting generalizability to other scientific domains with different evaluation norms and experimental requirements.

## Confidence

- **High Confidence**: The statistical significance of the execution gap (p < 0.05) and the documented pattern of LLM ideas requiring methodological downgrades during execution.
- **Medium Confidence**: The interpretation that this gap primarily reflects LLM limitations in scoping feasible experiments rather than fundamental idea quality differences, given alternative explanations (e.g., novelty bias) remain plausible.
- **Medium Confidence**: The generalizability of findings beyond NLP research to other scientific domains, as the study is confined to a specific field with particular evaluation norms.

## Next Checks

1. **Feasibility Filter Test**: Pre-screen LLM ideas using an automated classifier to detect experimentally infeasible components (e.g., human evaluation requirements) and measure whether this intervention reduces the execution gap.

2. **Faithful Execution Subset**: Analyze the subset of projects where executors made no changes to the proposed methodology to isolate whether the gap persists when experimental plans are implemented as originally conceived.

3. **Novelty-Impact Calibration**: Re-analyze the data controlling for initial novelty scores to determine whether high-novelty LLM ideas show systematically worse execution outcomes compared to high-novelty human ideas.