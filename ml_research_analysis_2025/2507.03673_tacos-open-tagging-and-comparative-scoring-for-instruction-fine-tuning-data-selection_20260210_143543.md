---
ver: rpa2
title: 'TACOS: Open Tagging and Comparative Scoring for Instruction Fine-Tuning Data
  Selection'
arxiv_id: '2507.03673'
source_url: https://arxiv.org/abs/2507.03673
tags:
- data
- tacos
- selection
- scoring
- instruction
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: TACOS introduces Open Tagging and Comparative Scoring to improve
  instruction fine-tuning data selection. It uses LLMs to generate open-domain tags,
  normalizes them to reduce noise and redundancy, and clusters samples by semantic
  intent, preserving data diversity while enhancing efficiency.
---

# TACOS: Open Tagging and Comparative Scoring for Instruction Fine-Tuning Data Selection

## Quick Facts
- **arXiv ID**: 2507.03673
- **Source URL**: https://arxiv.org/abs/2507.03673
- **Reference count**: 40
- **Key outcome**: Introduces Open Tagging and Comparative Scoring for IFT data selection, achieving 1st place among LLaMA2-7B-based models on AlpacaEval 2.0 and superior MT-Bench performance

## Executive Summary
TACOS addresses the inefficiency of instruction fine-tuning by proposing a data selection pipeline that uses open-domain tagging and comparative scoring. The method generates fine-grained semantic tags via LLMs, normalizes them to reduce noise, clusters data by intent, and evaluates quality through pairwise comparisons. This approach preserves diversity while selecting high-quality subsets, leading to more efficient fine-tuning and improved instruction-following capability. Experiments show TACOS outperforms baselines on both MT-Bench and AlpacaEval 2.0, particularly for LLaMA2-7B-based models.

## Method Summary
TACOS operates in two stages: open tagging and comparative scoring. First, GPT-4o generates open-domain tags for instruction-response pairs, which are normalized to reduce redundancy and noise. Phrase-BERT embeddings of normalized tags are used to cluster samples by semantic intent. Within each cluster, GPT-4 performs pairwise comparative scoring to assess relative quality. The highest-scoring samples from each cluster are selected for fine-tuning, balancing diversity and quality. The method was validated on LLaMA2-7B/13B and Mistral-7B-v0.1 models using Alpaca-52k and Evol-Instruct-70k datasets.

## Key Results
- Achieved 1st place among LLaMA2-7B-based models on AlpacaEval 2.0
- Superior MT-Bench scores compared to baselines across diverse LLMs
- Demonstrated 12x acceleration in fine-tuning time through efficient data selection
- Preserved data diversity while compressing large datasets (e.g., 52k → 1k samples)

## Why This Works (Mechanism)

### Mechanism 1: Open-Domain Tagging Captures Semantic Diversity
- Claim: Open-domain tags generated by LLMs preserve more semantic diversity than pre-defined heuristic categories
- Mechanism: GPT-4o assigns fine-grained, open-domain tags in JSON format; normalization reduces ~50k raw tags to ~6k normalized tags via frequency filtering, association aggregation, and rule aggregation
- Core assumption: LLM-generated tags reflect meaningful semantic intent, and normalization preserves signal while reducing noise sufficiently for clustering
- Evidence anchors: Abstract states normalization "significantly reduce tag redundancy and noise, and meanwhile maintain data diversity"; section III-A details tag generation and normalization process
- Break condition: If normalization collapses distinct intents into overly broad clusters, diversity gains would be lost

### Mechanism 2: Comparative Scoring Improves Consistency Over Singleton Evaluation
- Claim: Pairwise comparative scoring yields more consistent quality assessments than singleton scoring of independent samples
- Mechanism: Within each cluster, samples are evaluated in pairs by GPT-4 using a 1-100 scale; swapped comparisons reduce bias
- Core assumption: Providing a reference sample reduces criterion drift, and swapped scoring mitigates position bias in LLM evaluators
- Evidence anchors: Abstract states comparative scoring "avoids inconsistent criteria seen in singleton-based evaluations"; section III-B describes pairwise evaluation process
- Break condition: If clusters contain semantically incoherent samples, pairwise comparisons may become unreliable

### Mechanism 3: Clustered Selection Balances Diversity and Quality
- Claim: Selecting high-scoring samples within semantic clusters preserves coverage across intents while prioritizing quality
- Mechanism: Normalized tags are embedded via Phrase-BERT and clustered (e.g., K-means); highest-scoring samples per cluster are selected for IFT
- Core assumption: Clustering by tag semantics approximates task/intent diversity relevant for instruction-following, and intra-cluster scoring identifies higher-quality examples
- Evidence anchors: Abstract states TACOS "clusters samples by semantic intent, preserving data diversity while enhancing efficiency"; section III-A describes cluster-based selection
- Break condition: If tag embeddings poorly represent instruction semantics, clusters will not align with meaningful diversity

## Foundational Learning

- **Concept**: Instruction Fine-Tuning (IFT)
  - Why needed here: TACOS is a data selection method specifically for IFT; understanding the alignment goal (instruction-following, preference matching) clarifies why diversity and quality matter
  - Quick check question: Can you explain why IFT differs from pretraining, and what failure modes arise from low-quality IFT data?

- **Concept**: LLM-as-Judge Evaluation
  - Why needed here: Both tagging and comparative scoring rely on LLMs (GPT-4/GPT-4o) as annotators; understanding prompt design, biases, and consistency issues is critical
  - Quick check question: What are two common biases in LLM-as-judge setups, and how might swapping samples in pairwise evaluation mitigate one of them?

- **Concept**: Clustering and Representation Learning
  - Why needed here: Tag normalization and clustering via Phrase-BERT embeddings are core to preserving diversity; basic understanding of embedding spaces and clustering algorithms is needed
  - Quick check question: If two semantically different instructions receive similar tag embeddings, what downstream problem would you expect in TACOS?

## Architecture Onboarding

- **Component map**: Open Tagger (GPT-4o + normalization) → Normalized Tags → Tag Embedder (Phrase-BERT) → Tag Vectors → Clusterer (K-means) → Semantic Clusters → Comparative Scorer (GPT-4 with pairwise prompts) → Per-Sample Scores → Selector (top-k per cluster) → Final IFT Subset

- **Critical path**: Tagging quality (prompt compliance, JSON validity) → Normalization effectiveness (tag space reduction vs. semantic loss) → Cluster coherence (intra-cluster semantic similarity) → Scoring consistency (pairwise agreement, bias mitigation) → Selection threshold (k per cluster, total budget)

- **Design tradeoffs**: Open vs. pre-defined tags (open captures more diversity but introduces noise requiring normalization); Pairwise vs. singleton scoring (pairwise improves consistency but increases evaluation cost); Cluster granularity (more clusters preserve finer diversity but increase scoring overhead)

- **Failure signatures**: High rate of non-compliant JSON or generic tags; many single-sample clusters or one dominant cluster; flat score distributions within clusters; fine-tuned model underperforms baselines on diversity-sensitive benchmarks

- **First 3 experiments**:
  1. Tagging validation: Sample 500 instruction-response pairs, manually review tag quality and normalization output; measure JSON compliance rate and tag semantic granularity
  2. Cluster coherence check: Compute intra-cluster similarity using instruction embeddings; compare against random baseline to verify clustering signal
  3. Comparative scoring reliability: Run pairwise scoring on held-out validation cluster with swapped positions; measure score flip rate and inter-prompt consistency

## Open Questions the Paper Calls Out
None explicitly stated in the paper

## Limitations
- Normalization pipeline's impact on semantic coverage is not directly validated; reduction from ~50k to ~6k tags could risk collapsing meaningful distinctions
- Comparative scoring's superiority over singleton methods is inferred rather than directly benchmarked against alternatives
- Cluster representativeness depends on tag embedding quality; Phrase-BERT is not explicitly validated on instruction data for this use case

## Confidence
- High confidence in mechanism 1 (Open-Domain Tagging): Strong evidence from normalization results and direct tag generation process description
- Medium confidence in mechanism 2 (Comparative Scoring): Theoretical justification and pairwise approach described, but limited empirical comparison to alternatives
- Medium confidence in mechanism 3 (Clustered Selection): Logical framework described but cluster coherence validation is indirect

## Next Checks
1. Tag diversity validation: Measure instruction-intent coverage before and after normalization using manual sampling; calculate semantic drift score between original and normalized tag sets
2. Scoring consistency benchmark: Run parallel singleton scoring on same clusters; compare inter-annotator agreement and score distribution stability between methods
3. Cluster quality assessment: Use instruction embeddings to compute intra-cluster similarity; compare against random baseline and verify clusters capture distinct task domains