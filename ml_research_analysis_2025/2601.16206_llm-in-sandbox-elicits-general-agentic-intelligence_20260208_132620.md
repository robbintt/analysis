---
ver: rpa2
title: LLM-in-Sandbox Elicits General Agentic Intelligence
arxiv_id: '2601.16206'
source_url: https://arxiv.org/abs/2601.16206
tags:
- sandbox
- llm-in-sandbox
- tasks
- task
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'LLM-in-Sandbox enables LLMs to explore within a code sandbox (virtual
  computer), unlocking generalization capabilities for non-code tasks without additional
  training. The sandbox provides three core capabilities: external resource access,
  file management, and code execution.'
---

# LLM-in-Sandbox Elicits General Agentic Intelligence

## Quick Facts
- arXiv ID: 2601.16206
- Source URL: https://arxiv.org/abs/2601.16206
- Authors: Daixuan Cheng; Shaohan Huang; Yuxian Gu; Huatong Song; Guoxin Chen; Li Dong; Wayne Xin Zhao; Ji-Rong Wen; Furu Wei
- Reference count: 20
- Key outcome: LLM-in-Sandbox enables LLMs to explore within a code sandbox (virtual computer), unlocking generalization capabilities for non-code tasks without additional training.

## Executive Summary
LLM-in-Sandbox introduces a code sandbox environment that enables LLMs to explore, access external resources, manage files, and execute code to solve non-code tasks. The approach demonstrates substantial performance gains across mathematics, physics, chemistry, biomedicine, long-context understanding, and instruction following, with improvements up to +24.2% in mathematics. Strong agentic LLMs spontaneously leverage sandbox capabilities to install domain-specific tools, handle long contexts via file storage, and execute scripts for formatting requirements. The system also reduces token consumption by up to 8× for long-context tasks and achieves competitive query throughput. LLM-in-Sandbox Reinforcement Learning (RL) further enhances weaker models by training them to explore the sandbox environment using only general non-agentic data.

## Method Summary
LLM-in-Sandbox builds on the R2E-Gym framework, providing a Docker-based Ubuntu sandbox with three core tools: `execute_bash` for terminal commands, `str_replace_editor` for file operations, and `submit` for completion. The sandbox is initialized with input files in `/testbed/documents/` and runs ReAct-style workflows where models iteratively generate tool calls and process observations. For RL training, the system adapts GRPO++ algorithm with outcome-based rewards, using context-based tasks stored as multi-file documents. Strong agentic models spontaneously explore sandbox capabilities, while weaker models require RL training to learn effective exploration strategies.

## Key Results
- Mathematics: +24.2% improvement (LMA25-160K: 26.0% → 32.4%)
- Long-context: +50.9% improvement (AA-LCR: 21.5% → 32.4%) with 8× token reduction (100K→13K)
- Token efficiency: Up to 8× reduction in token consumption for long-context tasks
- Throughput: Competitive QPM (queries per minute) performance

## Why This Works (Mechanism)

### Mechanism 1: External Resource Access for Dynamic Tool Acquisition
Strong agentic LLMs spontaneously install and use domain-specific tools when tasks require capabilities not present in the base environment. The `execute_bash` tool enables arbitrary terminal commands, allowing models to autonomously issue package installation commands (pip install, apt-get) and fetch external resources (curl, wget). This transforms a minimal base environment into a domain-specific workspace. Models with sufficient software knowledge about ecosystems can select appropriate tools and construct valid installation commands.

### Mechanism 2: File-Based Context Offloading for Long-Context Tasks
Placing lengthy documents in the sandbox file system rather than the prompt improves both performance and computational efficiency. Documents stored as files in `/testbed/documents/` are accessed on-demand via shell tools (grep, sed, cat) and Python file I/O. This avoids loading entire contexts into the prompt, reducing token consumption while enabling targeted information retrieval. Models with strong file navigation skills can effectively locate relevant content through constructed search commands.

### Mechanism 3: Computation-Mediated Constraint Satisfaction
Code execution enables LLMs to verify solutions and satisfy precise constraints that would be difficult or impossible through pure text generation. Models write Python scripts that perform calculations, check constraints programmatically, and iterate toward valid solutions. The sandbox provides immediate, grounded feedback through execution results. Models can translate natural language constraints into executable verification logic, enabling numerical computation to verify solutions.

## Foundational Learning

- **ReAct-style agentic loops** (reasoning + acting interleaved)
  - Why needed here: The entire LLM-in-Sandbox workflow builds on multi-turn reasoning-action cycles. Without this foundation, engineers may misunderstand why sandbox exploration requires iterative tool calls rather than single-shot solutions.
  - Quick check question: Can you explain why a model might need 10+ turns to solve a chemistry problem, and what happens at each turn?

- **Docker containerization basics**
  - Why needed here: The sandbox is implemented as Docker containers providing isolation. Understanding container lifecycle, resource limits, and isolation guarantees is essential for debugging deployment issues.
  - Quick check question: What happens to files created in a Docker container after the container terminates?

- **Reinforcement learning with outcome-based rewards**
  - Why needed here: LLM-in-Sandbox-RL uses outcome-based rewards without process supervision. Engineers need to understand why this works and how reward signals propagate through multi-turn trajectories.
  - Quick check question: In LLM-in-Sandbox-RL, how does the model receive feedback when it takes 15 turns to complete a task?

## Architecture Onboarding

- **Component map**: Docker Sandbox -> Tool Layer (`execute_bash`, `str_replace_editor`, `submit`) -> Inference Backend (vLLM/SGLang/API) -> RL Training Loop (GRPO++)

- **Critical path**:
  1. Task prompt → Sandbox initialization with input files in `/testbed/documents/`
  2. Model generates tool call → Tool executes in sandbox → Observation returned
  3. Repeat until `submit` called or max turns (100) reached
  4. Extract answer from `/testbed/answer.txt`

- **Design tradeoffs**:
  - **Minimal vs. pre-configured environment**: Minimal image (1.1 GB) requires runtime package installation but scales efficiently; pre-configured SWE images require 6 TB for 1000 tasks
  - **Exploration freedom vs. guidance**: Open exploration enables diverse strategies but weaker models may wander; prompt engineering biases toward productive behaviors
  - **Token cost vs. turn count**: Long-context tasks save tokens (8×) via file storage; other tasks consume more tokens due to multi-turn exploration

- **Failure signatures**:
  - **"Wandering" behavior**: Weak models average 23.7 turns vs. 12.6 for strong models, with low capability usage rates (<3%)
  - **Negative transfer**: Some models perform worse in sandbox mode than vanilla LLM mode (Qwen3-4B: -5.9% math, -25.0% long-context)
  - **RL collapse**: Models may learn to avoid sandbox exploration if rewards are sparse; requires penalty for excessive turns

- **First 3 experiments**:
  1. **Baseline comparison**: Run evaluation suite on target model in both vanilla LLM mode and LLM-in-Sandbox mode across all six domains. Compute Δ metric to identify which domains benefit and whether the model is "strong agentic" (positive Δ) or "weak" (negative Δ).
  2. **Capability usage analysis**: Instrument sandbox to log tool invocations. Classify actions into external resources, file management, and computation. Compare usage rates against benchmarks in Table 4 to diagnose whether low performance stems from insufficient exploration or ineffective exploration.
  3. **Long-context ablation**: For long-context tasks, compare placing documents in prompt vs. sandbox files. Measure both accuracy and token consumption to quantify the file-management benefit for your specific inference setup.

## Open Questions the Paper Calls Out

- **Open Question 1**: What specific factors cause LLMs to under-utilize the sandbox environment for Biomedicine tasks compared to other domains?
  - Basis in paper: Section 2.4.2 notes Biomedicine shows "less stable improvements" with the "shortest exploration (6.5 turns), suggesting that models fail to fully leverage the sandbox environment for these tasks."
  - Why unresolved: The authors quantify the low exploration depth but do not analyze the semantic or structural reasons why models struggle to initiate tool use in this specific domain.
  - What evidence would resolve it: A breakdown of error types in Biomedicine trajectories or a specialized prompting strategy that successfully increases exploration turns.

- **Open Question 2**: How can sandbox-interaction be effectively incorporated into the pretraining stage?
  - Basis in paper: Section 6 advocates for "sandbox-native models" and suggests "incorporating sandbox-style reasoning into the pretraining stage itself" as a future direction.
  - Why unresolved: The current work focuses on inference-time exploration and post-training RL (LLM-in-Sandbox-RL); pretraining integration remains a theoretical proposal.
  - What evidence would resolve it: Results from a language model pretrained on datasets comprising code execution traces and file-system interaction logs.

- **Open Question 3**: Why do specific models (e.g., Qwen) show significant performance degradation when long contexts are stored as files rather than provided in the prompt?
  - Basis in paper: Table 3 and Section 2.4.2 highlight that while strong models benefit from file-based context, "Qwen perform worse with sandbox-based context."
  - Why unresolved: The paper identifies this as a failure case requiring training but does not determine if the root cause is poor retrieval capability, attention mechanism limitations, or lack of file-navigation heuristics.
  - What evidence would resolve it: An ablation study analyzing the retrieval accuracy and attention patterns of these models when reading segmented files versus contiguous prompts.

## Limitations

- **Sandbox environment constraints**: Package installation conflicts can halt exploration, and the 1.1 GB minimal image balances efficiency with functionality while pre-configured environments require 6 TB for 1000 tasks.
- **Model capability dependency**: Effectiveness strongly correlates with baseline agentic ability, creating a chicken-and-egg problem where models need sandbox experience to improve but require sufficient baseline capability to explore effectively.
- **RL training challenges**: The GRPO++ algorithm with outcome-based rewards faces sparse reward landscapes in multi-turn trajectories, potentially leading to exploration collapse without process supervision.

## Confidence

- **High confidence** in the core mechanism: The sandbox's three capabilities demonstrably improve performance for strong agentic models across multiple domains with statistically significant improvements.
- **Medium confidence** in generalization claims: Performance gains primarily benefit already-capable models, partially supporting the claim of eliciting general agentic intelligence but not fundamentally transforming weak models.
- **Low confidence** in RL training efficacy: RL shows incremental improvements (+4.3% to +10.5%) with uncertainty about whether it truly elicits agentic intelligence or merely optimizes existing behaviors due to sparse reward environments.

## Next Checks

1. **Cross-domain capability transfer**: Evaluate whether models that learn sandbox exploration in one domain (e.g., mathematics computation) can effectively transfer these skills to novel domains (e.g., biomedicine tool usage) without domain-specific fine-tuning.

2. **RL training stability analysis**: Implement KL penalty monitoring and reward variance tracking during GRPO++ training to identify conditions that lead to exploration collapse versus sustained learning curves.

3. **Minimal environment benchmarking**: Systematically test whether specific tool subsets (e.g., external resources only, or computation only) suffice for particular task categories, potentially reducing the 1.1 GB image footprint while maintaining performance.