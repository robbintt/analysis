---
ver: rpa2
title: 'Not Yet AlphaFold for the Mind: Evaluating Centaur as a Synthetic Participant'
arxiv_id: '2508.07887'
source_url: https://arxiv.org/abs/2508.07887
tags:
- task
- performance
- predictive
- centaur
- behavior
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The study evaluates Centaur, an LLM fine-tuned on human cognitive\
  \ task data, as a synthetic participant for behavioral science research. While Centaur\
  \ shows strong predictive accuracy on tasks it was trained on, its generative performance\u2014\
  critical for simulating human-like behavior\u2014systemically diverges from human\
  \ data."
---

# Not Yet AlphaFold for the Mind: Evaluating Centaur as a Synthetic Participant

## Quick Facts
- arXiv ID: 2508.07887
- Source URL: https://arxiv.org/abs/2508.07887
- Reference count: 16
- Primary result: Centaur LLM fails to capture key human behavioral signatures despite strong predictive accuracy on trained tasks

## Executive Summary
This study evaluates Centaur, an LLM fine-tuned on human cognitive task data, as a synthetic participant for behavioral science research. While Centaur demonstrates strong predictive accuracy on tasks it was trained on, its generative performance—critical for simulating human-like behavior—systematically diverges from human data. Across three cognitive tasks (reversal learning, horizon-dependent bandit, and Wisconsin Card Sorting Test), Centaur fails to capture essential behavioral signatures including reward adaptation, exploration-exploitation balance, and cognitive flexibility. The findings indicate that predictive accuracy alone is insufficient for behavioral simulation, highlighting the need for benchmarks that assess generative performance to develop models capable of in silico experimental prototyping.

## Method Summary
The study evaluated Centaur's performance across three behavioral tasks: reversal learning, horizon-dependent bandit task, and Wisconsin Card Sorting Test. Researchers compared Centaur's predictive accuracy against base LLMs and assessed its generative performance by examining whether it could reproduce key behavioral signatures observed in human participants. The evaluation involved both aggregate performance metrics and analysis of individual-level behavioral patterns to identify systematic divergences from human behavior.

## Key Results
- Centaur shows strong predictive accuracy on tasks it was trained on but fails to generate realistic human-like behavior
- Across all three tasks, Centaur misses critical behavioral signatures including reward adaptation and exploration-exploitation balance
- Centaur outperforms base LLMs in prediction but its generative limitations prevent it from serving as a viable participant simulator

## Why This Works (Mechanism)
The study demonstrates that predictive accuracy and generative performance are distinct capabilities in LLMs trained on behavioral data. While fine-tuning improves task-specific prediction, it does not necessarily enable the model to generate behavior that captures the full complexity of human cognitive processes, including uncertainty, adaptability, and context-dependent decision-making.

## Foundational Learning
- Cognitive task modeling: Understanding how behavioral tasks measure specific cognitive processes is essential for evaluating whether synthetic participants capture the right mechanisms
- Predictive vs generative performance: These represent fundamentally different capabilities in LLMs, with prediction focusing on output accuracy and generation requiring realistic behavioral patterns
- Behavioral signatures: Specific patterns in human responses (like reward adaptation and exploration-exploitation balance) serve as benchmarks for evaluating synthetic participant realism
- Fine-tuning limitations: Training on behavioral data improves task-specific performance but may not generalize to capturing the full spectrum of human-like decision-making

## Architecture Onboarding
Component map: Human data -> Fine-tuning process -> Centaur model -> Predictive performance + Generative output
Critical path: Data collection and preprocessing -> Fine-tuning strategy -> Task-specific evaluation -> Behavioral signature analysis
Design tradeoffs: Balancing predictive accuracy with generative realism requires trade-offs between task-specific optimization and broader behavioral generalization
Failure signatures: Systematic deviations from human behavioral patterns in reward adaptation, exploration-exploitation dynamics, and cognitive flexibility
First experiments: 1) Test Centaur on additional cognitive tasks with varying complexity, 2) Compare performance against models with different fine-tuning strategies, 3) Conduct ablation studies on training data size and participant diversity

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation focuses on a relatively small set of behavioral tasks (three tasks) which may not capture full human cognitive variability
- Centaur's training data comes from a limited participant pool, potentially constraining generalizability to broader populations
- Study primarily compares Centaur against baseline LLMs without exploring intermediate fine-tuning strategies that might bridge the predictive-generative gap

## Confidence
- High confidence: Centaur's predictive accuracy exceeds base LLMs, demonstrated through direct empirical comparison
- Medium confidence: Centaur fails to capture key behavioral signatures, as divergences are systematically observed but may be partially task-specific
- High confidence: Predictive accuracy alone is insufficient for behavioral simulation, evidenced by clear dissociation across multiple tasks

## Next Checks
1. Test Centaur on a wider range of cognitive tasks with different structural properties to determine if limitations generalize beyond current task set
2. Conduct ablation studies comparing Centaur's performance against models fine-tuned with different data sizes and participant demographics
3. Implement cross-validation with human participants performing the same tasks, measuring not just aggregate performance but also individual-level behavioral patterns