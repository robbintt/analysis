---
ver: rpa2
title: Buffer replay enhances the robustness of multimodal learning under missing-modality
arxiv_id: '2511.23070'
source_url: https://arxiv.org/abs/2511.23070
tags:
- missing
- multimodal
- modality
- information
- modalities
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of robust multimodal learning
  under missing-modality scenarios, where the absence of one or more modalities can
  severely degrade model performance. The proposed REplay Prompting (REP) method introduces
  a lightweight and effective paradigm to mitigate this issue.
---

# Buffer replay enhances the robustness of multimodal learning under missing-modality

## Quick Facts
- arXiv ID: 2511.23070
- Source URL: https://arxiv.org/abs/2511.23070
- Reference count: 40
- Primary result: REP achieves 70.01 AUROC on Hateful Memes under 70% text-missing, surpassing state-of-the-art by 4.5 percentage points

## Executive Summary
This paper introduces REplay Prompting (REP), a lightweight method to enhance multimodal model robustness when one or more modalities are missing. REP uses feature caching and replay to preserve early-layer representations and inject them back at deeper layers, reducing information loss. It also employs a private-shared feature decoupling strategy with orthogonality constraints and task-aware dynamic initialization. Experiments show REP consistently outperforms prior methods across vision-language, vision-language-audio, and temporal multimodal benchmarks while adding negligible parameter overhead.

## Method Summary
REP introduces a lightweight paradigm for robust multimodal learning under missing-modality conditions. It employs a feature caching and replay mechanism where early-layer representations are stored in modality-wise buffers and replayed in deeper layers to mitigate information loss. The method uses a private-shared feature decoupling strategy with orthogonality constraints to separate modality-specific and cross-modal semantics. Task-aware dynamic initialization with controlled perturbations improves buffer generalization. REP adds only negligible parameter overhead while significantly improving performance under both single- and multi-modality missing conditions.

## Key Results
- REP achieves 70.01 AUROC on Hateful Memes under 70% text-missing, surpassing state-of-the-art by 4.5 percentage points
- On Food101 with 70% text-missing, REP improves accuracy from 70.42 to 73.11 (+2.69)
- On MM-IMDb with 70% text-missing, REP achieves 77.32 F1-Macro, outperforming DCP by 3.56 points

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Residual bypass caching and replay mitigates information decay as network depth increases, providing fault tolerance when modalities are missing.
- **Mechanism**: Early-layer representations are cached via a residual bypass and injected back at deeper layers. The update uses a learnable coefficient α that balances current features against accumulated memory. At each layer k, input embeddings are augmented, preserving long-distance contextual information that adjacent-layer-only methods lose.
- **Core assumption**: Early layers retain modality-specific details that, if preserved and replayed, can compensate for missing signals in deeper layers where degradation typically occurs.
- **Evidence anchors**: [abstract]: "construct modality-wise feature buffers via a residual bypass to cache early-layer representations and replay them in deeper layers, mitigating information loss as network depth increases"
- **Break condition**: If information in early layers is already corrupted or uninformative, replaying it will propagate noise rather than provide compensation.

### Mechanism 2
- **Claim**: Decoupling private (modality-specific) and shared (cross-modal) features via orthogonality constraints enables independent compensation when specific modalities are missing.
- **Mechanism**: Two buffer types are maintained with separate update paths. Orthogonality loss forces shared and private spaces to capture distinct information. Private buffers retain fine-grained modality details; shared buffers aggregate across modalities. When one modality is missing, the other modality's private buffer and the shared buffer can partially compensate.
- **Core assumption**: Modality-specific and cross-modal information are separable, and their decoupled preservation improves robustness over entangled representations.
- **Evidence anchors**: [abstract]: "a private–shared feature decoupling strategy, where private buffers preserve fine-grained modality-specific signals while shared buffers encode cross-modal semantics"
- **Break condition**: If modalities are highly correlated with minimal unique information, orthogonality constraints may force artificial separation that hurts rather than helps.

### Mechanism 3
- **Claim**: Task-aware dynamic initialization with controlled perturbations improves buffer generalization across diverse missing-modality conditions.
- **Mechanism**: Private buffers initialize as pre-trained embeddings plus Gaussian noise, adding noise to pre-trained embeddings. Shared buffers initialize as normalized Gaussian vectors. The noise scale ε controls diversity. Ablation shows ε=0.2 yields best average performance.
- **Core assumption**: Adding controlled noise at initialization prevents overfitting to specific missing-modality patterns and encourages buffers to learn more generalizable representations.
- **Evidence anchors**: [section 3.5]: "This process introduces controlled diversity to each modality's initial representation, improving generalization"
- **Break condition**: If ε is too large, noise overwhelms pre-trained structure; if too small, buffers may not diversify sufficiently.

## Foundational Learning

- **Concept: Residual Connections and Skip Pathways**
  - Why needed here: REP's buffer update uses residual-style updates and replay injects historical features additively. Understanding gradient flow through residual bypass is essential.
  - Quick check question: Can you explain why α→0 preserves history while α→1 emphasizes current features, and how this affects gradient propagation?

- **Concept: Feature Disentanglement via Regularization**
  - Why needed here: The orthogonality constraint separates shared and private spaces. This is a soft constraint added to CLIP loss, requiring understanding of multi-objective optimization.
  - Quick check question: What happens if L_ortho weight is too high vs. too low in the total loss?

- **Concept: Prompt Tuning in Vision-Language Transformers**
  - Why needed here: REP extends prompt-based approaches by treating buffers as learnable prompts that persist across layers. Familiarity with where prompts are inserted is critical.
  - Quick check question: How do buffers differ from traditional prompt tokens in terms of update frequency and information source?

## Architecture Onboarding

- **Component map**:
  Input Embeddings (E^m[0])
       ↓
  Buffer Initialization (Sec 3.5) ─→ F_private[0], F_shared[0]
       ↓
  For each layer k=1..K:
       ├─ Buffer Update (Sec 3.3): Residual bypass updates F_private[k], F_shared[k]
       ├─ Feature Replay (Sec 3.4): Ẽ[k] = E[k] + βp·F_private[k-1] + βs·F_shared[k-1]
       └─ Transformer Layer → E[k+1]
       ↓
  Output + Orthogonality Loss

- **Critical path**: (1) Correct buffer initialization with appropriate noise scale ε; (2) Proper residual update with learnable α_m, ε_s; (3) Replay injection before each Transformer layer; (4) Orthogonality loss added to base loss.

- **Design tradeoffs**:
  - Buffer depth D vs. parameters: Deeper buffers store more layers but increase memory (Figure 5 shows D=6 optimal; D=12 degrades performance).
  - Buffer length L vs. representational capacity: L=36 optimal; longer prompts show diminishing returns.
  - Noise scale ε: Higher ε improves diversity but risks destroying pre-trained structure.

- **Failure signatures**:
  - Performance collapse at high missing rates (η≥70%) → check if buffers are updating (α may be stuck near 0).
  - Orthogonality loss not decreasing → check gradient flow; L_ortho weight may need adjustment.
  - Extreme degradation under single-modality missing → verify private buffers are modality-specific.
  - Parameter explosion → confirm buffer dimensions (D×L) are constrained as paper shows ~0.2% parameter increase.

- **First 3 experiments**:
  1. **Reproduce Table 3 ablation** on Food101 with η=70%: Start with CLIP baseline, add dynamic initialization, then dual buffers, then replay. Verify each component's contribution (+2.69, +3.73, +3.56 respectively).
  2. **Buffer configuration sweep**: Vary D ∈ {3, 6, 9, 12} and L ∈ {12, 24, 36, 48} on MM-IMDb (Figure 5 replication) to find optimal and confirm D=6, L=36.
  3. **Missing rate sensitivity**: Evaluate REP vs. DCP vs. MAP on Hateful Memes across η ∈ {30%, 50%, 70%, 90%} to confirm REP's slower degradation curve (paper claims +11% over DCP at 90% text-missing).

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: Can the REP replay mechanism be integrated into frozen Multimodal Large Language Models (MLLMs) to improve their robustness to missing modalities without the prohibitive cost of full fine-tuning?
- **Basis in paper**: [explicit] Appendix 6.3 compares REP against LLaVA and Qwen2-VL, noting that MLLMs "drop to near-random performance" under missing conditions, while REP offers a "lightweight and robust alternative."
- **Why unresolved**: The paper demonstrates REP's superiority over MLLMs but does not explore if REP's buffer strategy can be adapted as a parameter-efficient fine-tuning (PEFT) method specifically for the complex architectures of MLLMs to fix their deficiency.
- **What evidence would resolve it**: Experiments applying the private-shared buffer and replay mechanism to a frozen LLaVA or Qwen model, evaluating if robustness improves while keeping the tuning cost low.

### Open Question 2
- **Question**: Is the optimal buffer configuration (specifically depth $D$ and length $L$) universal, or is it highly dependent on the specific depth and attention mechanism of the backbone Transformer?
- **Basis in paper**: [inferred] Section 3.4 and Figure 5 indicate that performance peaks at a specific configuration ($L=36, D=6$) but drops if the buffer is too deep or long ("overly deep or long buffers may harm aggregation").
- **Why unresolved**: The experiments primarily utilize ViT-B/16 and MulT. It is unclear if the "sweet spot" for buffer depth scales linearly with deeper models or if the saturation point is a fixed architectural constant.
- **What evidence would resolve it**: Scaling studies applying REP to backbones of varying depths (e.g., ViT-Small vs. ViT-Large) to analyze if the optimal buffer depth correlates with the number of network layers.

### Open Question 3
- **Question**: Does REP maintain its robustness when modality absence is intermittent or partial (e.g., sensor packet loss) rather than the complete modal absence simulated by fixed placeholders?
- **Basis in paper**: [inferred] Section 4.1.2 explicitly defines "missing" as replacing the input with a fixed placeholder for the entire sample.
- **Why unresolved**: Real-world scenarios, particularly the acoustic-seismic (AS) benchmark mentioned, often suffer from partial data corruption or temporal dropouts rather than the total absence of a modality signal.
- **What evidence would resolve it**: Experiments on time-series data where modality "missingness" is simulated via random temporal masking rather than zeroing the entire input.

## Limitations
- Limited evaluation to fixed placeholder-based missing modalities rather than partial/intermittent absence
- Optimal buffer configuration may not generalize to different backbone depths or attention mechanisms
- Performance sensitivity to noise scale ε requires careful tuning and may not be universal

## Confidence
- Mechanism 1 (residual replay): High - directly supported by equations and ablation results
- Mechanism 2 (orthogonality constraints): High - ablation Table 3 shows consistent improvement with dual buffers
- Mechanism 3 (dynamic initialization): Medium - ablation shows improvement but lacks external validation
- Overall robustness claims: High - multiple datasets and missing rates demonstrate consistent gains

## Next Checks
1. Verify orthogonality loss decreases steadily during training on MM-IMDb with η=70%
2. Confirm buffer dimensions (D×L) match reported values (6×36) and parameter overhead stays near 0.2%
3. Test REP's performance degradation curve against DCP and MAP on Hateful Memes across missing rates 30%-90% to validate claimed superiority