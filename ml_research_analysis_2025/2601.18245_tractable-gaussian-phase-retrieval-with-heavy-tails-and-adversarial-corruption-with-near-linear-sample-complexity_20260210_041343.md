---
ver: rpa2
title: Tractable Gaussian Phase Retrieval with Heavy Tails and Adversarial Corruption
  with Near-Linear Sample Complexity
arxiv_id: '2601.18245'
source_url: https://arxiv.org/abs/2601.18245
tags:
- robust
- algorithm
- theorem
- phase
- retrieval
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper studies the problem of Gaussian phase retrieval with
  both heavy-tailed noise and adversarial corruptions. The main result is the first
  polynomial-time algorithm that achieves near-linear sample complexity (O(n log n))
  for this problem, improving upon the previous exponential-time algorithm with O(n
  log n) sample complexity.
---

# Tractable Gaussian Phase Retrieval with Heavy Tails and Adversarial Corruption with Near-Linear Sample Complexity

## Quick Facts
- **arXiv ID:** 2601.18245
- **Source URL:** https://arxiv.org/abs/2601.18245
- **Reference count:** 40
- **One-line primary result:** First polynomial-time algorithm with near-linear sample complexity (O(n log n)) for Gaussian phase retrieval with both heavy-tailed noise and adversarial corruptions.

## Executive Summary
This paper addresses Gaussian phase retrieval under two challenging conditions: heavy-tailed noise and adversarial corruptions. The authors develop a novel algorithm that achieves near-linear sample complexity while maintaining polynomial runtime, improving upon prior exponential-time solutions. The key innovation combines robust spectral initialization with robust gradient descent, where careful truncation of measurement products enables the application of robust PCA algorithms. The approach is versatile enough to handle both zero-mean and non-zero-mean noise models through appropriate symmetrization techniques.

## Method Summary
The algorithm consists of three main stages: robust norm estimation using robust mean estimation techniques, robust spectral initialization via truncated PCA to find the signal direction, and refinement through robust gradient descent. The truncation parameter is carefully chosen to balance between bounding the measurement distribution for robust PCA while preserving signal structure. For non-zero mean noise, the problem is reduced to blind deconvolution through symmetrization of paired samples. The method achieves estimation error proportional to the noise level times the square root of the corruption fraction, with near-linear sample complexity.

## Key Results
- Achieves near-linear sample complexity O(n log n) for Gaussian phase retrieval with heavy-tailed noise and adversarial corruption
- First polynomial-time algorithm matching the sample complexity of exponential-time methods
- Estimation error bounded by O(σ√ε/‖x*‖) where σ is noise level and ε is corruption fraction
- Robustness to both zero-mean and non-zero-mean heavy-tailed noise models
- Near-linear time complexity achievable if empirical truncated distributions satisfy stability conditions

## Why This Works (Mechanism)

### Mechanism 1: Bounded Truncation Enables Robust Spectral Initialization
The algorithm truncates measurement vector products at threshold τ, transforming an unbounded heavy-tailed problem into a bounded one. This allows robust PCA algorithms to recover the top eigenvector of the covariance matrix despite corruption. The truncation parameter must be chosen carefully (proportional to √n and signal norm) to maintain covariance approximation while ensuring stability.

### Mechanism 2: Robust Gradient Descent within the Basin of Attraction
Once initialization lands within radius ‖x*‖/9 of the true signal, the population risk becomes strongly convex, enabling efficient convergence. Robust mean estimation techniques filter out adversarial samples at each iteration, ensuring gradients point toward the basin minimum.

### Mechanism 3: Symmetrization for Non-Zero Mean Noise
Non-zero mean noise is neutralized by constructing differenced variables from paired samples, removing the unknown noise mean while preserving signal information. This transforms the problem into blind deconvolution, allowing standard robust initialization techniques to be applied.

## Foundational Learning

- **Spectral Initialization**: Essential for non-convex phase retrieval to avoid local minima. Without good initialization, gradient methods get trapped far from the true signal.
- **Algorithmic Robust Statistics (Filtering)**: The filtering subroutine iteratively removes data points deviating significantly from estimated statistics, distinguishing heavy-tailed noise from adversarial corruption through moment bounds.
- **Sample Complexity (Near-Linear)**: Achieving O(n log n) sample complexity is crucial for practical scalability, though at the cost of O(m²n) runtime rather than linear time.

## Architecture Onboarding

- **Component map:** Input → Norm Estimator → Truncator → Robust PCA → Robust GD
- **Critical path:** Norm estimation determines truncation threshold τ. Wrong τ → covariance distortion → PCA failure → initialization outside basin → GD divergence
- **Design tradeoffs:** Runtime vs. sample efficiency (O(m²n) time for near-linear samples) and truncation aggressiveness vs. stability (bias vs. variance)
- **Failure signatures:** "Initialization Error" (dist(x₀,x*) > ‖x*‖/9) or "Covariance Distortion" (small eigenvalue gap)
- **First 3 experiments:**
  1. Sanity check: Gaussian data without corruption to verify baseline performance
  2. Robustness test: Student-t noise with 5% adversarial corruption, verify error bound
  3. Truncation ablation: Vary τ to identify bias-variance break point

## Open Questions the Paper Calls Out

### Open Question 1
Does the truncated empirical distribution over O(n) samples satisfy the stability condition required for nearly-linear time complexity? This remains unresolved as current proofs require n⁵ samples.

### Open Question 2
Can robust gradient descent be performed using only a single batch of samples rather than requiring fresh samples at each iteration? Current analysis relies on fresh samples for maintaining corruption fractions.

### Open Question 3
Can these algorithms extend to other sensing designs like coded diffraction patterns? Current guarantees rely heavily on Gaussian measurement properties.

### Open Question 4
Can corruption tolerance be made independent of noise-to-signal ratio, and can estimation error improve to O(σε/‖x*‖)? Current restrictions depend on K₄/‖x*‖².

## Limitations

- Relies on external subroutines (Robust PCA from [38], Robust Mean Estimation from [21]) without full implementation details
- Runtime has quadratic dependence on sample size (O(m²n)), making it computationally intensive despite near-linear sample complexity
- Requires known upper bound on corruption fraction and noise-to-signal ratio as input parameters

## Confidence

- **High Confidence:** Overall algorithmic framework and spectral initialization guarantees
- **Medium Confidence:** Simultaneous robustness to heavy-tailed noise and adversarial corruption claims
- **Low Confidence:** Open question about stability conditions for empirical truncated distributions

## Next Checks

1. **Open Question Validation:** Empirically test whether truncated sample distributions satisfy stability conditions for nearly-linear time algorithms using synthetic heavy-tailed data.

2. **Runtime Benchmark:** Implement full pipeline and measure actual O(m²n) scaling for varying n and m to validate theoretical predictions and assess practical feasibility.

3. **Corruption Fraction Boundary:** Systematically vary corruption fraction ε below and above theoretical threshold to empirically identify exact break point where algorithm fails.