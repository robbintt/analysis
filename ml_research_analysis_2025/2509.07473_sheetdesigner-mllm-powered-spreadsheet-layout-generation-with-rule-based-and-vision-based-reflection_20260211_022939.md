---
ver: rpa2
title: 'SheetDesigner: MLLM-Powered Spreadsheet Layout Generation with Rule-Based
  and Vision-Based Reflection'
arxiv_id: '2509.07473'
source_url: https://arxiv.org/abs/2509.07473
tags:
- layout
- components
- spreadsheet
- sheetdesigner
- table
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'SheetDesigner addresses the challenge of automated spreadsheet
  layout generation by combining multimodal large language models with rule-based
  and vision-based reflection. It introduces a two-stage process: structure placement
  with Dual Reflection for component arrangement and content population with global
  adjustments.'
---

# SheetDesigner: MLLM-Powered Spreadsheet Layout Generation with Rule-Based and Vision-Based Reflection

## Quick Facts
- arXiv ID: 2509.07473
- Source URL: https://arxiv.org/abs/2509.07473
- Reference count: 40
- Primary result: Achieves 22.6% improvement over SOTA baselines on automated spreadsheet layout generation

## Executive Summary
SheetDesigner addresses the challenge of automated spreadsheet layout generation by combining multimodal large language models with rule-based and vision-based reflection. It introduces a two-stage process: structure placement with Dual Reflection for component arrangement and content population with global adjustments. Evaluated on a dataset of 3,326 spreadsheets, SheetDesigner achieves a 22.6% improvement over state-of-the-art baselines. Empirical analysis reveals that while MLLMs excel at handling overlap and balance through vision, they struggle with alignment, necessitating the hybrid reflection strategies employed in SheetDesigner.

## Method Summary
SheetDesigner is a zero-shot, training-free pipeline that uses MLLMs for automated spreadsheet layout generation. It consists of four main stages: pre-processing (LLM extracts types, descriptions, relations, and topic), structure placement (MLLM generates layout candidates using topic-specific exemplar images), dual reflection (if SheetRanker scores fall below threshold, triggers rule-based and vision-based revision), and content population (MLLM sets row heights, column widths, and line breaks). The system uses a 7-criteria evaluation protocol and employs a topic-conditioned visual exemplar retrieval system to provide structural priors for layout generation.

## Key Results
- Achieves 22.6% improvement over state-of-the-art baselines
- Rule-based reflection specifically improves alignment scores
- Vision-based reflection excels at overlap and balance detection
- Topic-conditioned exemplars provide 2.71% performance boost

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Hybrid rule-based and vision-based reflection mitigates the specific limitations of MLLMs in structured grid alignment.
- **Mechanism:** The vision modality efficiently detects spatial "gestalt" features like overlap and balance (likely due to natural image training), but its attention mechanism scatters when verifying precise grid alignment. Rule-based reflection compensates by explicitly parsing discrete coordinates (e.g., R1C1) to enforce alignment constraints that visual attention misses.
- **Core assumption:** The MLLM's visual attention layers are optimized for natural objects rather than abstract structural boundaries.
- **Evidence anchors:**
  - [abstract] "MLLMs handle overlap and balance well but struggle with alignment, necessitates hybrid rule and visual reflection strategies."
  - [section 5] "LLaVA-7B demonstrates precise attention to overlapping regions... conversely, its attention is scattered... when dealing with misalignment."
  - [corpus] "SheetBrain" (neighboring paper) supports the general difficulty of reasoning over complex spreadsheet structures without symbolic assistance.
- **Break condition:** If the reflection threshold is set too high (e.g., 0.7), the system may attempt to revise unfixable layouts, increasing cost without quality gain (Section 4.5).

### Mechanism 2
- **Claim:** Decoupling "relation extraction" from "spatial placement" improves semantic consistency.
- **Mechanism:** Instead of asking the MLLM to infer dependencies and place components simultaneously, the system first explicitly extracts a relation list $R$ (e.g., [Main1, Chart1]) in a pre-processing step. This list is then fed as a hard constraint to the placement agent, ensuring related components remain proximal.
- **Core assumption:** LLMs are better at identifying semantic links from content than maintaining those links while simultaneously solving 2D spatial puzzles.
- **Evidence anchors:**
  - [section 3] "We employ LLMs as the pre-processing engine... instruct LLMs to identify pairwise relationships... resulting in a relation list R."
  - [intro] Existing models "neglect interrelated semantics, such as data dependencies and contextual links."
- **Break condition:** If the pre-processing LLM hallucinates non-existent relationships, the placement agent will be forced to cluster unrelated components.

### Mechanism 3
- **Claim:** Topic-conditioned visual exemplars provide zero-shot structural priors.
- **Mechanism:** By classifying the spreadsheet topic (e.g., "To-do List") and retrieving a visual exemplar of that topic, the model anchors its generation to domain-specific layout conventions (e.g., vertical lists vs. grid dashboards) without requiring fine-tuning.
- **Core assumption:** Spreadsheet layout norms are strongly correlated with the document's functional topic.
- **Evidence anchors:**
  - [section 3.1.1] "Layout conventions, such as spatial grouping... are tailored to each documentâ€™s topic."
  - [table 2] "w/o Topic" results in a 2.71% performance drop, specifically affecting relation-aware alignment.
- **Break condition:** If the retrieved exemplar is visually cluttered or structurally divergent from the target data, it may mislead the placement strategy.

## Foundational Learning

- **Concept:** **Discrete (R1C1) vs. Continuous (Pixel) Coordinate Systems**
  - **Why needed here:** Standard layout generators output continuous pixel bounding boxes $(x_1, y_1, x_2, y_2)$. Spreadsheets are discrete grids where resizing one component affects the entire row/column. Understanding this distinction is critical for the "Structure Placement" phase.
  - **Quick check question:** If a model outputs a component at pixel coordinate x=155, how does snapping it to the nearest grid cell potentially disrupt the "alignment" score?

- **Concept:** **Visual Attention in MLLMs**
  - **Why needed here:** The paper argues that vision-based reflection fails at alignment because attention weights scatter across grid boundaries. Understanding *where* the model looks (attention heatmaps) explains why hybrid strategies are necessary.
  - **Quick check question:** Why would an MLLM find a "man with a yellow backpack" (object detection) easier to process than "two tables aligned at the left border" (structural relation)?

- **Concept:** **Iterative Refinement (Reflection)**
  - **Why needed here:** SheetDesigner is not a single-shot generator; it relies on a loop of generation $\to$ evaluation (SheetRanker) $\to$ reflection. This is the core agentic workflow.
  - **Quick check question:** If the SheetRanker returns a score of 0.4 for "Fullness" (below threshold 0.5), what specific trigger does the Dual Reflection mechanism initiate?

## Architecture Onboarding

- **Component map:** Pre-processing -> Structure Placement -> SheetRanker -> Dual Reflection (If score < threshold) -> Content Population
- **Critical path:** Pre-processing $\to$ Structure Placement $\to$ **SheetRanker** $\to$ **Dual Reflection** (If score < threshold) $\to$ Content Population

- **Design tradeoffs:**
  - **Vision vs. Text Reflection:** Vision is computationally heavier but required for Balance/Overlap; Text is lightweight and required for Alignment.
  - **Threshold Settings:** A threshold of 0.3 reduces cost but misses fixable errors; 0.7 improves quality marginally but drastically increases token cost (Table 4).

- **Failure signatures:**
  - **Scattered Attention:** If visual reflection fails to fix alignment (seen in attention heatmaps), the loop may max out retries without improvement.
  - **Rigid Components:** Attempting to resize fixed components (like tables) instead of flexible ones (titles) leads to unfixable overlaps.
  - **Extreme Density:** The paper notes failure in cases with "enormous components with different sizes" (Appendix F).

- **First 3 experiments:**
  1. **Verify Hybrid Necessity:** Run ablation on the Dual Reflection module. Does removing the *Rule-based* reflection specifically degrade alignment scores? (Confirming Table 2).
  2. **Stress Test Grid Mapping:** Feed the system inputs with vastly different component aspect ratios. Does the discrete grid snapping break the layout compatibility?
  3. **Attention Visualization:** replicate the attention map visualization (Figure 4) on a new layout. Confirm that the model attends to overlap regions but ignores grid-line alignment.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can automated layout generation models effectively handle extreme cases involving a large number of components with significantly varying sizes?
- Basis in paper: [explicit] The authors list this scenario as a limitation where the model arranges elements sub-optimally, noting that addressing such cases remains an open research direction.
- Why unresolved: The paper notes this difficulty is not unique to their approach but is a shared challenge in prior art, likely due to the complexity of global optimization in constrained grid spaces.
- What evidence would resolve it: A model variant that maintains high alignment and balance scores on a stress-test dataset of spreadsheets with unusually high component density or aspect ratio variance.

### Open Question 2
- Question: Can heterogeneous graph representation learning enhance the modeling of component relationships compared to the current pairwise LLM approach?
- Basis in paper: [explicit] The "Future Works" section proposes leveraging the heterogeneous graph structure of spreadsheet components to facilitate more powerful representation learning.
- Why unresolved: The current framework relies on zero-shot LLMs to identify pairwise relationships, which may lack the structural rigor of dedicated graph-based embeddings.
- What evidence would resolve it: Quantitative improvements in "relation-aware alignment" scores when a Graph Neural Network (GNN)-based relationship encoder is substituted for the standard LLM pre-processing step.

### Open Question 3
- Question: Is it possible to optimize MLLMs to detect alignment errors visually without relying on hybrid rule-based reflection?
- Basis in paper: [inferred] The analysis shows MLLM attention fails to focus on boundaries critical for alignment, necessitating the hybrid system.
- Why unresolved: The paper suggests current MLLMs lack fine-grained visual optimization for structured formats, but it is unknown if training or architectural changes could remove the need for rule-based heuristics.
- What evidence would resolve it: An MLLM that achieves comparable "Component-alignment" scores using only vision-based reflection, eliminating the performance gap currently filled by textual rules.

## Limitations
- Heavy reliance on visual exemplars (5-10 images per topic) without access to specific image library
- Reflection thresholds appear somewhat arbitrary with unclear optimal settings
- Performance degradation on unseen topics not thoroughly analyzed
- Extreme density cases remain challenging for the system

## Confidence

**High Confidence:** The core claim that MLLMs struggle with precise grid alignment while handling overlap and balance well. This is directly supported by attention heatmaps (Figure 4) and quantitative metrics showing Rule-based reflection specifically improves alignment scores.

**Medium Confidence:** The 22.6% improvement over state-of-the-art baselines. While the methodology is sound, exact reproduction depends on accessing the specific dataset and exemplar images used.

**Low Confidence:** The generalizability of topic-conditioned exemplars beyond the 13 topics tested. The paper provides no analysis of performance degradation when encountering unseen topics.

## Next Checks

1. **Ablation Test:** Remove the Rule-based reflection component and measure the specific degradation in Component/Type/Relation-Alignment scores to confirm the hybrid approach's necessity.

2. **Grid Mapping Stress Test:** Generate layouts with extreme aspect ratio variations (1:10 to 10:1) to test the discrete grid snapping mechanism's robustness and identify breaking points.

3. **Attention Verification:** Replicate the attention heatmap visualization (Figure 4) on three new layouts to confirm that the model consistently attends to overlap regions while ignoring grid-line alignment, validating the core mechanism explanation.