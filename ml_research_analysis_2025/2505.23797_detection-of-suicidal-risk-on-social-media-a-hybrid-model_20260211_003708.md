---
ver: rpa2
title: 'Detection of Suicidal Risk on Social Media: A Hybrid Model'
arxiv_id: '2505.23797'
source_url: https://arxiv.org/abs/2505.23797
tags:
- data
- suicidal
- risk
- learning
- posts
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study addresses the critical need for automated detection
  of suicidal risk levels using social media data. The authors propose a hybrid machine
  learning model combining deep contextual embeddings from RoBERTa with TF-IDF features
  compressed via PCA.
---

# Detection of Suicidal Risk on Social Media: A Hybrid Model

## Quick Facts
- arXiv ID: 2505.23797
- Source URL: https://arxiv.org/abs/2505.23797
- Reference count: 33
- Primary result: Hybrid RoBERTa-TF-IDF-PCA model achieves 0.7512 weighted F1 for multi-class suicide risk classification

## Executive Summary
This study addresses the critical need for automated detection of suicidal risk levels using social media data. The authors propose a hybrid machine learning model combining deep contextual embeddings from RoBERTa with TF-IDF features compressed via PCA. They also explore data resampling techniques and augmentation strategies to address class imbalance. The model is trained on 2,999 Reddit posts annotated across four suicide risk levels. Experimental results show the hybrid model outperforms both standalone RoBERTa and BERT models, achieving a best weighted F1 score of 0.7512.

## Method Summary
The authors propose a hybrid model that concatenates RoBERTa's [CLS] token embeddings with TF-IDF features compressed by PCA. The model processes Reddit posts (title + body) through RoBERTa-base to extract 768-dimensional contextual embeddings, while simultaneously computing TF-IDF vectors (3,000 features) and reducing them to 300 components via PCA. These representations are concatenated into a 1,068-dimensional vector and passed through a linear classifier with softmax output for 4-class risk prediction. The model is trained with batch size 3, learning rate 1e-5, and AdamW optimizer using 5-fold cross-validation without stratification.

## Key Results
- Hybrid RoBERTa-TF-IDF-PCA model achieves 0.7512 weighted F1 score
- Original class distribution outperforms resampling strategies (weighted F1 0.7499 vs 0.7342 for weighted loss)
- Data augmentation degrades performance (weighted F1 drops from 0.7421 to 0.6648)
- Per-class F1 scores range from 0.65 (Attempt) to 0.80 (Ideation)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Concatenating RoBERTa [CLS] embeddings with compressed TF-IDF features improves multi-class suicide risk classification over either representation alone.
- Mechanism: RoBERTa captures contextual semantics while TF-IDF provides statistical term salience for domain-specific markers. PCA reduces TF-IDF from 3,000 to 300 components to mitigate noise and sparsity. Concatenation yields a 1,068-dim vector passed to a linear classifier.
- Core assumption: Contextual embeddings and statistical term weights encode complementary signals that jointly improve discrimination across fine-grained risk levels.
- Evidence anchors: [abstract] "integrating the deep contextual embeddings from RoBERTa ... with the statistical term-weighting of TF-IDF, further compressed with PCA, to boost the accuracy"
- Break condition: If TF-IDF adds negligible variance beyond what RoBERTa already captures, or if PCA discards discriminative components, performance gains may disappear or invert.

### Mechanism 2
- Claim: Retaining the original imbalanced class distribution outperforms explicit resampling strategies for this task under the reported evaluation protocol.
- Mechanism: The authors test oversampling, undersampling, and sample-weighted loss. Oversampling and weighted loss assume balanced training aligns with optimal decision boundaries, but test folds remain imbalanced, creating distribution mismatch. Undersampling reduces already-scarce "attempt" examples, degrading learning.
- Core assumption: The test distribution reflects deployment; aligning training to a different (balanced) distribution harms generalization more than the imbalance itself.
- Evidence anchors: [section V-B1] "Original method obtaining the highest weighted F1 score of 0.7499, followed by SWL and OSam"
- Break condition: If deployment requires calibrated per-class recall (e.g., prioritizing "attempt" detection over "ideation"), optimizing for weighted F1 on natural distribution may be misaligned with operational priorities.

### Mechanism 3
- Claim: Standard text augmentation (abbreviation expansion, emoji translation, summarization, back-translation) degrades rather than improves transformer-based classification for this domain.
- Mechanism: Augmentation alters token distributions in ways misaligned with RoBERTa's pretrained representations. Emoji/abbreviation expansion can dilute sentiment cues; summarization may shift semantic nuance; back-translation introduces phrasing atypical of authentic self-disclosure language.
- Core assumption: Transformer models fine-tuned on in-domain text benefit more from authentic linguistic patterns than augmented variations.
- Evidence anchors: [section V-B2] "weighted precision, recall, and F1 scores all decreased" with augmentation
- Break condition: If augmentation is selectively applied only to minority classes with domain-adaptive techniques, benefits may emerge.

## Foundational Learning

- Concept: Transformer [CLS] token pooling
  - Why needed here: The hybrid architecture extracts the [CLS] embedding as the post-level representation for concatenation with TF-IDF-PCA. Understanding what [CLS] encodes is essential for diagnosing what additional signal TF-IDF provides.
  - Quick check question: Can you explain why [CLS] is used for sequence-level classification rather than pooling across all token embeddings?

- Concept: TF-IDF sparsity and PCA compression
  - Why needed here: TF-IDF vectors over 3,000 features are sparse and high-dimensional; PCA densifies and reduces to 300 components. This tradeoff directly affects what statistical signals survive for concatenation.
  - Quick check question: If PCA retains 95% variance but the most discriminative TF-IDF features for "attempt" are low-variance, what happens to those signals?

- Concept: Multi-class evaluation with class imbalance
  - Why needed here: The dataset is imbalanced (ideation: 45%, attempt: 8.5%), and the authors use weighted F1. Understanding why macro vs. weighted metrics matter is critical for interpreting results and aligning with deployment goals.
  - Quick check question: For a suicide risk triage system, would you prioritize macro-F1, weighted-F1, or per-class recall for "attempt"? Why?

## Architecture Onboarding

- Component map: Raw Reddit post (title + body) -> RoBERTa branch (BPE tokenization -> 12 transformer layers -> [CLS] embedding) + TF-IDF branch (Tokenization -> TF-IDF vectorization -> PCA compression) -> Concatenate [CLS] + TF-IDF-PCA -> Linear layer -> Softmax -> 4-class output

- Critical path: RoBERTa forward pass is the computational bottleneck; TF-IDF-PCA is precomputed once per post. Backpropagation updates RoBERTa weights and the classification head jointly.

- Design tradeoffs:
  - PCA dimensionality (N=300): Higher N retains more TF-IDF signal but increases fusion vector size and potential noise.
  - Max token length (512): Longer posts are truncated; 142 posts exceed this limit, potentially losing critical context.
  - Resampling strategy: Original distribution preferred; resampling may help if deployment prioritizes minority-class recall.

- Failure signatures:
  - Confusion between adjacent risk levels (indicator↔ideation, behavior↔ideation, attempt↔behavior) indicates overlapping linguistic features.
  - Low "attempt" F1 (0.65) despite hybrid model suggests insufficient discriminative features for the scarcest class.
  - Performance drops with augmentation signal that synthetic text deviates from authentic self-disclosure patterns.

- First 3 experiments:
  1. Ablate TF-IDF-PCA branch and compare RoBERTa-only vs. hybrid on per-class F1 to quantify contribution of statistical features.
  2. Increase PCA components (e.g., 500, 700) and measure impact on minority-class recall vs. overall weighted F1.
  3. Apply targeted augmentation only to "attempt" class using domain-specific synonym replacement (e.g., mental health lexicon) and evaluate per-class metrics.

## Open Questions the Paper Calls Out

- Question: Would analyzing the temporal progression of suicidal ideation risk levels across multiple posts from the same user improve early detection and enable more personalized intervention strategies?
  - Basis in paper: [explicit] In the Future Work section, the authors explicitly propose "Temporal user-level analysis: Analyze the progression of suicidal ideation risk levels over time based on posts from the same user."
  - Why unresolved: The current study treats each post as an independent classification instance and does not model user-level trajectories or temporal patterns in risk progression.
  - What evidence would resolve it: A longitudinal study linking posts from the same users over time, with evaluation of whether temporal modeling improves predictive accuracy for risk escalation compared to post-level classification alone.

- Question: Which individual data augmentation technique (abbreviation expansion, emoji-to-text conversion, text summarization, or back-translation) most impacts classification performance, and can refined implementations reverse the observed performance degradation?
  - Basis in paper: [inferred] The authors report that combining all four augmentation techniques degraded performance (weighted F1 dropped from 0.7421 to 0.6648) and explicitly state: "Further investigation is needed to analyze the impact of each augmentation technique individually."
  - Why unresolved: The experiments only evaluated all four techniques combined, not each in isolation, leaving the contribution of individual techniques unknown.
  - What evidence would resolve it: An ablation study testing each augmentation technique independently and in pairwise combinations, with analysis of how each affects semantic preservation and model alignment.

- Question: Would incorporating follow-up comments from original posters or related posts from other mental health subreddits improve the model's ability to distinguish between behavior and attempt categories?
  - Basis in paper: [explicit] The authors propose "Incorporating richer textual context: Assess the impact of integrating supplementary textual content, such as follow-up comments or related posts from other subreddits, into the classification process."
  - Why unresolved: The current model uses only the initial post content, despite the data collection process capturing follow-up comments that could provide disambiguating context.
  - What evidence would resolve it: Comparative experiments using post-only versus post-plus-comments representations, with particular attention to per-class F1 improvements for behavior and attempt categories.

## Limitations

- Data access and generalizability: The study relies on 500 labeled Reddit posts from a cited but not publicly accessible dataset, limiting reproducibility and raising questions about generalization to other platforms.
- Evaluation protocol constraints: Non-stratified 5-fold cross-validation means test folds have varying class distributions, which may affect the validity of performance comparisons across resampling strategies.
- Mechanism validation gaps: Direct ablation studies comparing RoBERTa-only vs. hybrid are not presented, leaving the TF-IDF contribution theoretically justified rather than empirically isolated.

## Confidence

- High confidence: The hybrid model architecture is clearly specified and implemented. The preference for original class distribution over resampling is supported by direct experimental comparison within the study's evaluation framework. Augmentation degradation is consistently observed across metrics.
- Medium confidence: The mechanism by which TF-IDF-PCA adds discriminative value beyond RoBERTa is plausible but not directly validated through ablation. The claim that original imbalance outperforms resampling holds under the specific non-stratified evaluation but may not generalize to stratified splits or clinical deployment scenarios.
- Low confidence: Generalization to other social media platforms or clinical populations is untested. The optimal dimensionality for PCA compression (300 components) is not explored systematically. The handling of posts exceeding 512 tokens and its impact on performance is not quantified.

## Next Checks

1. **Ablation study validation**: Implement and compare RoBERTa-only vs. hybrid model performance on per-class F1 scores to isolate the contribution of TF-IDF-PCA features. Expect measurable gains in minority class recall if the hybrid mechanism is valid.

2. **Resampling strategy robustness**: Repeat experiments with stratified cross-validation and compare original vs. weighted loss approaches. This tests whether the preference for original imbalance holds under different evaluation protocols that better reflect deployment scenarios.

3. **Domain-specific augmentation**: Apply targeted augmentation only to the "attempt" class using mental health lexicon-based synonym replacement. Compare performance against both the baseline and standard augmentation to test whether selective, domain-aware augmentation can improve minority class detection without degrading overall performance.