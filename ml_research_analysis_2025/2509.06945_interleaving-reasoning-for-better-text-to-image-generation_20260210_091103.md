---
ver: rpa2
title: Interleaving Reasoning for Better Text-to-Image Generation
arxiv_id: '2509.06945'
source_url: https://arxiv.org/abs/2509.06945
tags:
- image
- generation
- reasoning
- arxiv
- initial
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Interleaving Reasoning Generation (IRG),
  a framework that enhances text-to-image generation by alternating between text-based
  reasoning and image synthesis. The approach first generates a reasoning step to
  guide an initial image, then reflects on this result to produce a refined image
  with improved visual quality and fine-grained details.
---

# Interleaving Reasoning for Better Text-to-Image Generation

## Quick Facts
- **arXiv ID**: 2509.06945
- **Source URL**: https://arxiv.org/abs/2509.06945
- **Reference count**: 12
- **Primary result**: IRG achieves 5-10 point absolute gains on major benchmarks while improving visual quality and detail preservation

## Executive Summary
This paper introduces Interleaving Reasoning Generation (IRG), a framework that enhances text-to-image generation by alternating between text-based reasoning and image synthesis. The approach first generates a reasoning step to guide an initial image, then reflects on this result to produce a refined image with improved visual quality and fine-grained details. To train this process, the authors propose Interleaving Reasoning Generation Learning (IRGL), a two-stage paradigm that builds initial reasoning and reflection capabilities before optimizing the full pipeline. They curate IRGL-300K, a dataset with six decomposed learning modes to support this training. Experiments demonstrate state-of-the-art performance, achieving 5-10 point absolute gains on benchmarks like GenEval, WISE, TIIF, GenAI-Bench, and OneIG-EN, alongside substantial improvements in visual quality and detail preservation.

## Method Summary
The IRG framework implements a two-turn text-image pipeline where a base model first generates reasoning text, produces an initial image, then reflects on that image to generate improvement text and a refined output. The training uses IRGL-300K dataset organized into six decomposed learning modes: Initial Thinking Understanding/Generation Learning, Initial Full Learning, and their Improving counterparts. The two-stage training first builds reasoning/reflection skills on text-only data (2K steps), then optimizes the full pipeline on complete trajectories (30K steps). Inference uses dual classifier-free guidance with guidance scale 2.0 for both image and text conditioning during the improved image generation step.

## Key Results
- IRG achieves 5-10 point absolute gains across multiple benchmarks (GenEval, WISE, TIIF, GenAI-Bench, OneIG-EN)
- MLLM evaluators prefer refined images 63.3% vs 36.7% over initial images despite some benchmark metrics showing similar or lower scores
- The decomposed training approach (IRGL) enables efficient learning when complete IRG trajectory data is scarce
- Dual CFG conditioning stabilizes generation when multiple conditioning sources are present

## Why This Works (Mechanism)

### Mechanism 1
Decomposing interleaved training into six targeted learning modes improves data efficiency when full thinking-image trajectories are scarce. Instead of requiring complete prompt→thinking→image→reflection→improved-image sequences for all samples, IRGL trains on partial objectives (e.g., thinking-only generation, understanding-based thinking, full trajectories) that provide complementary supervision signals. This allows the model to learn reasoning patterns from text-only data while preserving image generation quality from high-quality image subsets.

### Mechanism 2
Conditioning refinement on the initial generated image enables targeted improvement of fine-grained details while preserving semantic content. The initial generation establishes core content and semantics. The model then encodes the initial image into ViT and VAE features (denoted I_f^(1)), which serve as conditioning for generating improvement-reflection text and the refined image. This creates a feedback loop where visual defects in the initial image inform specific textual critique, which then guides targeted corrections.

### Mechanism 3
Dual CFG conditioning (image vs. no-image, text vs. no-text) stabilizes the improved image generation when multiple conditioning sources are present. Standard CFG compares conditioned vs. unconditioned generation. IRG's improved image has four potential condition sources (prompt, initial thinking, initial image, improving thinking). The authors use two complementary CFG schemes: (1) with vs. without initial image features, and (2) with vs. without reflection text. Both use guidance scale 2.0.

## Foundational Learning

- **Unified Multimodal Understanding-Generation Models**
  - Why needed here: IRG builds on BAGEL, a model that natively handles interleaved text-image inputs and outputs in a single transformer. Without this foundation, the multi-turn text-image-text-image pipeline would require stitching separate models.
  - Quick check question: Can you explain how a single transformer can output both text tokens and image tokens autoregressively?

- **Chain-of-Thought (CoT) Reasoning in LLMs**
  - Why needed here: The paper extends text-based CoT reasoning to multimodal generation. Understanding why CoT improves complex reasoning tasks (intermediate steps reduce problem difficulty) transfers to understanding why IRG's reflection improves generation.
  - Quick check question: Why does generating intermediate reasoning steps before a final answer improve accuracy on complex tasks?

- **Classifier-Free Guidance (CFG) in Diffusion Models**
  - Why needed here: CFG is standard for controllable diffusion generation. IRG's dual-CFG design extends this to multi-condition settings, and understanding baseline CFG is prerequisite.
  - Quick check question: In standard CFG, what happens when the guidance scale approaches 1.0 vs. when it approaches infinity?

## Architecture Onboarding

- **Component map**: prompt -> initial thinking -> initial image -> encode image -> improving thinking -> improved image (with dual CFG)

- **Critical path**: Start from BAGEL checkpoint with self-CoT capability → Stage 1 training on decomposed modes builds robust thinking/reflection → Stage 2 training on full trajectories (Initial Full Learning + Improving Full Learning) tunes the complete pipeline → Inference: prompt → initial thinking → initial image → encode image → improving thinking → improved image (with dual CFG)

- **Design tradeoffs**: Two-turn vs. more turns: Paper limits to n=2 (one refinement) to validate hypothesis; more turns may yield diminishing returns or error accumulation. Initial image source: Uses base model (BAGEL) outputs as initial images in training data—trades data efficiency against potential ceiling from base model quality. Thinking supervision: Uses Qwen2.5-VL to generate thinking/reflection text—quality depends on teacher model capabilities.

- **Failure signatures**: Micro-structure saturation: Over-smoothing of repetitive textures (fabrics, foliage) during refinement. Text rendering drift: Dense constraints may trade legibility for stylistic coherence. Global-local tension: Local edits improve parts while perturbing global layout in crowded scenes.

- **First 3 experiments**: Ablation on decomposed modes: Train with only full trajectory data vs. full six-mode training; expect 3-4 point drop on WISE/TIIF. Single-turn vs. multi-turn comparison: Generate with only initial thinking+image vs. full IRG; use MLLM judges to compare visual quality. CFG sensitivity analysis: Vary image-CFG and text-CFG scales (1.0, 2.0, 3.0, 5.0) and measure GenEval scores + visual artifacts.

## Open Questions the Paper Calls Out

### Open Question 1
Would extending IRG beyond two reasoning turns (n > 2) yield further improvements in image quality, or does performance plateau or degrade due to error accumulation? The authors deliberately constrained experiments to n=2 to validate the core hypothesis; multi-turn scaling remains unexplored.

### Open Question 2
Can the IRG framework achieve stronger performance if trained on larger-scale, high-quality complete IRG trajectories rather than relying on decomposed learning modes? The authors note that constructing complete paired initial-to-improved image trajectories with reasoning is non-trivial; their decomposed approach was a practical workaround for data scarcity.

### Open Question 3
Do standard T2I benchmarks adequately capture the visual quality improvements that IRG achieves, given that refined images score similarly or lower than initial images on some benchmarks? There is a mismatch between benchmark metrics and perceived visual quality improvements, suggesting benchmarks may not evaluate fine-grained fidelity and aesthetics well.

### Open Question 4
How can the trade-off between aggressive refinement edits (improving details) and stability (avoiding over-smoothing or layout disruption) be optimally balanced? The paper acknowledges this tension but does not propose a mechanism to dynamically adjust editing aggressiveness based on image content.

## Limitations

- Data dependency on high-quality synthetic data from strong teacher models (Qwen2.5-VL, GPT-4o)
- Generalization to diverse domains remains untested
- Computational overhead from two-stage training and dual-CFG inference
- Standard benchmarks may not fully capture fine-grained visual quality improvements

## Confidence

- **High confidence**: The decomposed training approach (Mechanism 1) and dual-CFG design (Mechanism 3) are well-supported by methodology
- **Medium confidence**: The reflection mechanism's focus on fine-grained refinement (Mechanism 2) is theoretically sound but relies on assumptions about initial generation quality
- **Low confidence**: Specific guidance scale values (2.0) appear optimal but sensitivity analysis is not provided

## Next Checks

1. Cross-dataset generalization test: Evaluate IRG on 100 prompts from domains not represented in IRGL-300K (medical imagery, technical diagrams, abstract art)
2. Error propagation analysis: Systematically inject semantic errors into initial generations and measure whether reflection mechanism can recover correctness
3. Ablation of teacher model quality: Compare IRG performance using different quality levels of thinking/reflection generation