---
ver: rpa2
title: An Explainable Transformer Model for Alzheimer's Disease Detection Using Retinal
  Imaging
arxiv_id: '2507.04259'
source_url: https://arxiv.org/abs/2507.04259
tags:
- retformer
- retinal
- images
- image
- layer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study introduces Retformer, a novel transformer-based architecture
  for detecting Alzheimer's disease (AD) using retinal imaging. The model leverages
  convolutional layers, rotary position embedding, grouped query attention, and SwiGLU
  activation to efficiently learn from small medical image datasets.
---

# An Explainable Transformer Model for Alzheimer's Disease Detection Using Retinal Imaging

## Quick Facts
- arXiv ID: 2507.04259
- Source URL: https://arxiv.org/abs/2507.04259
- Reference count: 40
- Primary result: Retformer achieves up to 11% higher accuracy, precision, sensitivity, specificity, and AUC than benchmark models for AD detection using retinal imaging.

## Executive Summary
This study introduces Retformer, a novel transformer-based architecture for detecting Alzheimer's disease (AD) using retinal imaging. The model leverages convolutional layers, rotary position embedding, grouped query attention, and SwiGLU activation to efficiently learn from small medical image datasets. Retformer is trained on fundus images and OCT scans from AD patients and healthy controls, and its decision-making process is explained using the Gradient-weighted Class Activation Mapping (Grad-CAM) algorithm. The model outperforms benchmark algorithms, including EfficientNet, MobileNet, ResNet, VGG-16, and RETFound, by margins of up to 11% in accuracy, precision, sensitivity, specificity, and AUC. Grad-CAM visualizations align with clinical findings, highlighting the importance of retinal vasculature, choroid layer, and overall retinal thickness in AD detection. Retformer demonstrates the potential of transformer-based models for accurate and explainable AD diagnosis using retinal imaging modalities.

## Method Summary
Retformer is a transformer-based model that uses CNN-based patch embedding, rotary position embedding (RoPE), grouped query attention (GQA), and SwiGLU activation to classify retinal images as AD or healthy. The model is trained from scratch on fundus images and OCT slices resized to 50x50 pixels. A 3x3 nested cross-validation procedure is used for evaluation, with random oversampling applied to balance the fundus dataset. Grad-CAM is used to visualize and explain the model's decision-making process by highlighting important retinal regions.

## Key Results
- Retformer achieves 94% accuracy, outperforming EfficientNet, MobileNet, ResNet, VGG-16, and RETFound by up to 11% in accuracy, precision, sensitivity, specificity, and AUC.
- Grad-CAM visualizations align with clinical findings, highlighting the importance of retinal vasculature, choroid layer, and overall retinal thickness in AD detection.
- The model demonstrates the potential of transformer-based models for accurate and explainable AD diagnosis using retinal imaging modalities.

## Why This Works (Mechanism)

### Mechanism 1: Inductive Bias via Hybrid Parameterization
- The Retformer architecture mitigates overfitting on small datasets by reducing learnable parameters and enforcing stronger inductive biases than standard Vision Transformers (ViTs).
- It replaces learnable position embeddings with Rotary Position Embedding (RoPE) and standard Multi-Head Attention with Grouped Query Attention (GQA), lowering the model's capacity to memorize noise.
- This forces the model to learn generalizable structural features from the limited retinal image set, assuming AD biomarkers rely on structural relationships rather than absolute spatial coordinates.

### Mechanism 2: Local-Global Feature Extraction
- Performance gains stem from combining CNN's local feature extraction with Transformer's global dependency modeling.
- A CNN-based patch encoder extracts fine-grained local textures (e.g., retinal vasculature edges) before transformer layers model long-range dependencies between these patches.
- This hybrid approach is more effective than relying solely on one paradigm for detecting relevant AD biomarkers.

### Mechanism 3: XAI-Driven Feature Alignment
- The model's reliability is enhanced because its internal decision logic aligns with known clinical biomarkers.
- Grad-CAM visualizations confirm the model is utilizing clinically validated regions (retinal vasculature, optic disc, choroid layer) rather than artifacts.
- This alignment acts as a semantic sanity check, suggesting the model is learning relevant pathology.

## Foundational Learning

- **Rotary Position Embedding (RoPE)**
  - Why needed: Encodes relative position via rotation, which is more robust to translation and reduces overfitting in small datasets compared to absolute position embeddings.
  - Quick check: If you rotate the input image, how should a model using relative position embeddings (RoPE) behave compared to one using absolute learned embeddings?

- **Grouped Query Attention (GQA)**
  - Why needed: Shares Key/Value heads across groups of queries, reducing memory and compute while maintaining performance stability, preventing overfitting in data-limited medical contexts.
  - Quick check: Why does sharing keys/values help prevent overfitting in a data-limited medical context?

- **Grad-CAM for Transformers**
  - Why needed: Provides visual explanations of which input regions activated the classification, addressing the "black box" criticism of transformers in medical applications.
  - Quick check: Why is Grad-CAM often preferred over raw attention map visualization for explaining medical diagnoses?

## Architecture Onboarding

- **Component map**: Input (50x50) -> Conv Patch Embedding -> RoPE (Position) -> [LayerNorm -> GQA -> SwiGLU] x L -> MLP Head

- **Critical path**: The Patch Embedding (CNN) is critical. Swapping this for a standard Linear MLP drops accuracy by 5%, so the convolution kernel size and stride must effectively capture retinal texture without reducing resolution too aggressively.

- **Design tradeoffs**:
  - Resolution vs. Overfitting: Downsampling to 50x50 pixels reduces computational cost but performance degrades if images are too small (10x10) or too large (overfitting risk at 300x300).
  - Training from Scratch: Avoiding ImageNet pretraining to prevent domain shift requires robust hyperparameter tuning (Bayesian Optimization) to ensure convergence.

- **Failure signatures**:
  - Symptom: High training accuracy, low validation accuracy (~50-60%). Likely Cause: Overfitting due to disabling RoPE or removing GQA.
  - Symptom: Grad-CAM highlights image corners or background noise. Likely Cause: Data leakage or "shortcut learning" where the model detects scanner artifacts rather than retinal tissue.

- **First 3 experiments**:
  1. Resolution Sweep: Run inference on varied image sizes (10x50 to 150x150) to find the sweet spot for your specific dataset distribution.
  2. Ablation of Patch Encoder: Replace Conv Patch Embedding with Linear Projection to quantify the loss of local texture features on your specific data.
  3. Batch Size Sensitivity: Test batch sizes (e.g., 50, 250, 500) to verify the bias-variance tradeoff claim, ensuring the model converges on your hardware.

## Open Questions the Paper Calls Out
- Can Retformer improve diagnostic performance by leveraging OCTA to specifically target retinal microvasculature degradation?
- How robust is the Retformer model when applied to external, multi-center datasets with different imaging devices and demographic profiles?
- To what extent do the Grad-CAM saliency maps align with pixel-level annotations provided by expert ophthalmologists?

## Limitations
- Sample size constraints: OCT dataset uses only 28 subjects, raising questions about statistical significance.
- Data source limitations: Both datasets come from specific populations without demographic details, limiting generalizability.
- No pretrained model comparison: Doesn't compare against modern vision transformers with pretraining on large datasets like ImageNet-21k.

## Confidence
- **High confidence**: Architectural design choices (RoPE, GQA, SwiGLU) are well-justified and supported by ablation studies. Grad-CAM visualizations appear methodologically sound.
- **Medium confidence**: Performance claims are strong but based on limited datasets. The 11% margins over benchmark models may not hold on larger, more diverse populations.
- **Low confidence**: Claims about clinical applicability and real-world deployment readiness are premature given the small sample sizes and lack of external validation.

## Next Checks
1. Test Retformer on an independent dataset from a different institution or population to verify generalizability of the 94% accuracy claims.
2. Run the same ablation studies (RoPE, GQA, SwiGLU) on the actual clinical validation set to confirm architectural advantages hold outside training conditions.
3. Stratify performance by age, sex, and other demographic factors to identify potential bias or performance variation across patient subgroups.