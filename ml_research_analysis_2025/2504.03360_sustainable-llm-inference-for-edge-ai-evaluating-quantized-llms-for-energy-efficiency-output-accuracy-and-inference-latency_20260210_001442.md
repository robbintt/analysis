---
ver: rpa2
title: 'Sustainable LLM Inference for Edge AI: Evaluating Quantized LLMs for Energy
  Efficiency, Output Accuracy, and Inference Latency'
arxiv_id: '2504.03360'
source_url: https://arxiv.org/abs/2504.03360
tags:
- energy
- instruct
- qwen2
- quantization
- accuracy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study addresses the challenge of deploying Large Language
  Models (LLMs) on resource-constrained edge devices by evaluating 28 quantized LLMs
  from the Ollama library on a Raspberry Pi 4. The research systematically measures
  energy consumption, inference performance, and output accuracy across five benchmark
  datasets using high-precision hardware-based energy monitoring.
---

# Sustainable LLM Inference for Edge AI: Evaluating Quantized LLMs for Energy Efficiency, Output Accuracy, and Inference Latency

## Quick Facts
- arXiv ID: 2504.03360
- Source URL: https://arxiv.org/abs/2504.03360
- Reference count: 40
- Primary result: Quantization reduces energy consumption by up to 79% and inference latency by up to 69% on Raspberry Pi 4.

## Executive Summary
This study systematically evaluates 28 quantized LLMs from the Ollama library on a Raspberry Pi 4, measuring energy consumption, inference performance, and output accuracy across five benchmark datasets using hardware-based power monitoring. Results demonstrate that quantization significantly reduces energy consumption (up to 79%) and inference latency (up to 69%), with q4 and q3 quantization levels offering the best efficiency-accuracy trade-offs. However, extreme quantization degrades accuracy in mathematical reasoning tasks, and task-specific characteristics like response length strongly influence energy consumption patterns.

## Method Summary
The study deployed 28 quantized LLMs (LLaMA 3.2 1B, Qwen 2.5 0.5B/1.5B, Gemma 2 2B) with various quantization levels (FP16, q8_0, q4_0, q4_1, q4_K_M, q4_K_S, q3_K_L, q3_K_M, q3_K_S) on a Raspberry Pi 4 (4GB RAM). Energy measurements were captured using a Joulescope JS110 hardware power meter, with baseline idle power (2.85W) subtracted from measurements. Five benchmark datasets were sampled to 200 tasks each (HumanEval: 164 full), with inference scripts logging responses and timestamps for post-processing.

## Key Results
- Quantization reduces energy consumption by up to 79% and inference latency by up to 69% compared to full-precision models
- q4 and q3 quantization levels offer the best efficiency-accuracy trade-offs across most tasks
- Extreme quantization degrades accuracy particularly in mathematical reasoning tasks like GSM8K
- Code generation tasks show lower energy per token due to longer responses spreading fixed initialization costs

## Why This Works (Mechanism)

### Mechanism 1: Weight-Only Quantization Reduces Memory Bandwidth Bottlenecks
If model weights are compressed via Post-Training Quantization (PTQ), the energy consumption per token decreases significantly compared to full-precision (FP16) models on edge hardware. By converting weights from 16-bit floating point to lower-bit integers (e.g., INT4), the memory footprint shrinks, reducing the data transfer load from RAM to the CPU during the memory-bound decoding phase. Since data movement often consumes more energy than computation on edge devices, reducing bandwidth demand lowers overall power draw.

### Mechanism 2: Inference Fixed-Cost Amortization via Response Length
Tasks requiring longer output sequences (e.g., code generation) tend to exhibit lower energy consumption per generated token compared to short-response tasks (e.g., multiple-choice QA). LLM inference has a fixed initialization cost (loading the model, processing the prompt) that is incurred regardless of output length. In tasks with long responses, this fixed energy cost is spread across a larger number of tokens, reducing the average energy per token.

### Mechanism 3: Quantization Sensitivity in Arithmetic Reasoning
While aggressive quantization preserves accuracy for language tasks, it degrades performance in tasks requiring precise arithmetic or symbolic reasoning. Reducing weight precision to very low bits introduces rounding errors that accumulate during matrix multiplications. In tasks like GSM8K (math), these errors disrupt the precise chain of calculation required to reach the correct numeric answer, whereas language tasks are more robust to semantic noise.

## Foundational Learning

- **Concept: Weight-Only vs. Weight-Activation Quantization**
  - Why needed here: The study relies on Ollama's default weight-only quantization. Understanding this distinction explains why energy savings are high (memory bandwidth reduction) but computational savings might vary (activation math remains higher precision).
  - Quick check question: Does weight-only quantization reduce the memory footprint of the KV cache during generation?

- **Concept: Pareto Efficiency (Energy vs. Accuracy)**
  - Why needed here: The authors use Pareto frontiers to identify "optimal" models. You must understand that a model is Pareto-optimal if no other model is both more accurate and more energy-efficient.
  - Quick check question: If Model A uses 50J with 80% accuracy and Model B uses 40J with 80% accuracy, is Model A Pareto-optimal?

- **Concept: Hardware-Based Power Profiling**
  - Why needed here: The study distinguishes between software estimates and hardware measurements. You need to know that software tools often lack telemetry on constrained edge devices, necessitating physical power meters for ground truth.
  - Quick check question: Why is subtracting "idle power" (2.85W in this study) a critical step in isolating inference energy from system overhead?

## Architecture Onboarding

- **Component map:** Raspberry Pi 4 (4GB RAM) -> Ollama inference engine -> 28 quantized GGUF models -> Joulescope JS110 power meter -> Host PC for logging

- **Critical path:** 1) Measure RPi idle power (2.85W baseline) 2) Load model -> Send Prompt -> Stream Tokens -> Log time and response 3) Synchronize Joulescope timestamps with inference logs to calculate total Joules 4) Extract answers from <ANS> tags and compute accuracy

- **Design tradeoffs:** Higher quantization (q3) reduces model size but may introduce CPU overhead for dequantization, potentially flattening latency gains. Task specificity affects efficiency—a model optimized for code might be energy-inefficient for short queries due to initialization overheads. Hardware limits force use of quantized variants for larger models.

- **Failure signatures:** OOM crashes with FP16 models on RPi, high variance in energy readings (±5.36 J/token) indicating system instability, sudden accuracy drops in GSM8K for q3 variants indicating quantization break condition for arithmetic reasoning.

- **First 3 experiments:** 1) Run Llama3.2:1b-instruct-fp16 vs. q4_K_M on CommonsenseQA to verify ~50-60% energy reduction 2) Run Qwen2.5:0.5b-q8_0 on HumanEval vs. GSM8K to confirm lower energy per token for longer responses 3) Plot Energy vs. Accuracy for 3 variants of Qwen (q3, q4, q8) to identify Pareto-optimal models for TruthfulQA

## Open Questions the Paper Calls Out

### Open Question 1
How do the energy-accuracy trade-offs identified on the Raspberry Pi 4 generalize to heterogeneous edge hardware platforms like Jetson Nano or mobile AI accelerators? The authors state further experiments on diverse edge platforms are necessary for broader generalization, explicitly naming Jetson Nano, Coral TPU, and mobile AI accelerators.

### Open Question 2
To what extent do the observed quantization effects on energy and latency transfer to interactive, conversational AI applications and multilingual inference tasks? The authors note the evaluation focused on specific benchmark datasets which may not fully represent real-world use cases, specifically listing conversational AI and multilingual inference as areas for future work.

### Open Question 3
Can adaptive quantization techniques optimize the energy-accuracy trade-off more effectively than the static quantization levels analyzed in this study? The authors conclude that future work should explore adaptive quantization techniques to further optimize efficiency and performance for real-world edge AI applications.

## Limitations

- Hardware constraints limit evaluation to Raspberry Pi 4 and models available in Ollama library, potentially missing larger models or different quantization methods feasible on more powerful hardware
- Dataset sampling does not specify exact samples used, making exact reproduction challenging and raising questions about consistency across different subsets
- Study focuses on five specific tasks which may not represent full diversity of real-world edge AI applications
- Does not account for potential variability due to background processes, thermal throttling, or environmental factors affecting Raspberry Pi performance

## Confidence

- **High Confidence**: Quantization reduces energy consumption by up to 79% and inference latency by up to 69%—supported by direct hardware measurements and clear before/after comparisons
- **Medium Confidence**: q4 and q3 quantization levels offer best efficiency-accuracy trade-offs—based on Pareto analysis but task-specific nature introduces uncertainty about generalizability
- **Low Confidence**: Extreme quantization degrades accuracy in mathematical reasoning—observed but not explained in depth; study doesn't test alternative quantization methods like QAT

## Next Checks

1. **Reproduce Pareto Frontier Across Diverse Edge Devices**: Validate whether identified optimal quantization levels (q4, q3) hold when tested on different edge device (e.g., NVIDIA Jetson Nano or smartphone with comparable RAM)

2. **Compare PTQ vs. QAT for Arithmetic Tasks**: Run subset of models using QAT instead of PTQ on GSM8K to determine if quantization-aware training mitigates accuracy collapse observed with q3 quantization

3. **Analyze Impact of Prompt Length on Energy Per Token**: Systematically vary prompt length in CommonsenseQA to isolate effect of prompt processing on energy consumption per token, validating whether fixed initialization cost drives higher per-token energy in short-response tasks