---
ver: rpa2
title: 'CodeGEMM: A Codebook-Centric Approach to Efficient GEMM in Quantized LLMs'
arxiv_id: '2512.17970'
source_url: https://arxiv.org/abs/2512.17970
tags:
- quantization
- codegemm
- should
- accuracy
- latency
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: CodeGEMM introduces a codebook-centric GEMM kernel for quantized
  LLMs that eliminates dequantization by precomputing and storing partial sums in
  a Psumbook. This approach reduces computational complexity and cache footprint compared
  to traditional dequantization-based kernels.
---

# CodeGEMM: A Codebook-Centric Approach to Efficient GEMM in Quantized LLMs

## Quick Facts
- arXiv ID: 2512.17970
- Source URL: https://arxiv.org/abs/2512.17970
- Reference count: 40
- Primary result: Achieves 1.83× (8B) and 8.93× (70B) speedups in 2-bit quantization for Llama-3 models while maintaining comparable accuracy

## Executive Summary
CodeGEMM introduces a codebook-centric approach to matrix multiplication for quantized large language models that eliminates the need for dequantization. The method precomputes and stores partial sums in a Psumbook, reducing computational complexity and cache footprint compared to traditional dequantization-based kernels. The approach demonstrates significant speedups on Llama-3 models, achieving 1.83× improvement for 8B parameters and 8.93× for 70B parameters in 2-bit quantization while maintaining comparable accuracy.

## Method Summary
CodeGEMM addresses inefficiencies in traditional dequantization-based GEMM kernels for quantized LLMs by introducing a codebook-centric approach. Instead of converting quantized values back to floating-point during computation, CodeGEMM precomputes and stores partial sums in a specialized data structure called Psumbook. This eliminates the computational overhead of dequantization and reduces cache pressure by avoiding the need to load and convert large quantization codebooks during inference. The method maintains numerical accuracy while significantly improving computational efficiency through reduced memory bandwidth requirements and simplified arithmetic operations.

## Key Results
- Achieves 1.83× speedup for 8B parameter Llama-3 models in 2-bit quantization
- Achieves 8.93× speedup for 70B parameter Llama-3 models in 2-bit quantization
- Maintains comparable accuracy to traditional dequantization-based approaches
- Reduces computational complexity and cache footprint during inference

## Why This Works (Mechanism)
CodeGEMM works by fundamentally changing how quantized matrix multiplications are computed. Traditional approaches require dequantization of input values, which involves multiplication with scaling factors and codebook lookups, creating computational bottlenecks. CodeGEMM instead precomputes all possible partial sums that can result from quantized operations and stores them in the Psumbook. During inference, the algorithm simply performs table lookups and accumulations rather than expensive arithmetic operations. This shift from computation to memory access patterns dramatically reduces the number of floating-point operations while maintaining numerical precision. The approach is particularly effective for large models where the overhead of codebook management becomes significant, and where memory bandwidth is often the limiting factor in inference performance.

## Foundational Learning

**Quantized Neural Networks**: Neural networks that use lower-precision representations (e.g., 2-bit, 4-bit) instead of standard 32-bit floating point to reduce memory usage and computational requirements. Needed because it enables efficient LLM deployment on resource-constrained hardware.

**Matrix Multiplication Optimization**: Techniques for improving GEMM operation efficiency through algorithmic improvements, data layout optimization, and hardware-specific tuning. Needed because GEMM operations dominate LLM inference computational cost.

**Dequantization Overhead**: The computational cost of converting quantized values back to floating-point representation during inference. Needed because traditional quantized inference requires this step, creating performance bottlenecks.

**Codebook-based Quantization**: Quantization schemes that use discrete codebook entries to represent values rather than linear scaling factors. Needed because CodeGEMM builds upon this foundation while eliminating the need for runtime codebook access.

**Memory Hierarchy Optimization**: Strategies for optimizing data access patterns to leverage cache hierarchies effectively. Needed because CodeGEMM's performance depends critically on cache behavior and memory bandwidth utilization.

**Partial Sum Precomputation**: The technique of computing and storing all possible results of quantized operations beforehand. Needed because this is the core innovation that enables CodeGEMM's performance gains.

## Architecture Onboarding

**Component Map**: Input Tensor -> Quantized Lookup -> Psumbook Access -> Accumulation -> Output Tensor

**Critical Path**: The performance bottleneck lies in Psumbook access patterns and memory bandwidth utilization. Efficient indexing and cache-friendly data layouts are crucial for maintaining the theoretical speedup gains.

**Design Tradeoffs**: CodeGEMM trades increased memory usage (for storing the Psumbook) against reduced computational complexity. The approach is most beneficial when memory bandwidth is the limiting factor and when the Psumbook can fit in cache or fast memory. Larger models benefit more due to amortization of codebook management overhead.

**Failure Signatures**: Performance degradation occurs when Psumbook exceeds cache capacity, leading to memory bandwidth saturation. Accuracy degradation can occur with aggressive quantization (1-bit) or when codebook entries don't adequately represent the data distribution. Suboptimal indexing schemes can cause cache thrashing and eliminate speedup benefits.

**First Experiments**:
1. Benchmark CodeGEMM against traditional dequantization kernels on varying batch sizes to identify the crossover point where memory bandwidth becomes limiting.
2. Profile cache hit rates and memory bandwidth utilization during inference to optimize Psumbook size and access patterns.
3. Evaluate accuracy sensitivity across different quantization bit-widths and activation quantization schemes to establish robustness bounds.

## Open Questions the Paper Calls Out
None

## Limitations
- Experimental evaluation limited to specific Llama-3 model sizes and 2-bit quantization configurations
- Lack of comprehensive analysis across different quantization bit-widths, particularly lower bit-widths
- Insufficient characterization of cross-platform performance on different GPU architectures, CPUs, and TPUs
- Incomplete analysis of Psumbook storage requirements and memory bandwidth impact across varying problem sizes

## Confidence
- Computational speedup claims: High (supported by direct measurements and clear algorithmic improvements)
- Accuracy preservation claims: Medium (limited to specific cases and quantization configurations)
- Memory efficiency claims: Medium (incomplete characterization of cache and memory subsystem behavior)

## Next Checks
1. Evaluate CodeGEMM across a broader range of quantization bit-widths (1-bit, 3-bit, 4-bit) and different activation quantization schemes to establish the method's robustness to quantization parameters.
2. Test the approach on multiple hardware platforms including different GPU architectures, CPUs with SIMD capabilities, and TPUs to assess cross-platform performance and identify architectural dependencies.
3. Conduct ablation studies varying the Psumbook size, access patterns, and caching strategies to optimize memory subsystem utilization and establish clear guidelines for practical deployment across different model scales and batch sizes.