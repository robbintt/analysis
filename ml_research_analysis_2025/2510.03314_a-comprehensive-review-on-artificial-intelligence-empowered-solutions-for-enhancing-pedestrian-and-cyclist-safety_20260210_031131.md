---
ver: rpa2
title: A Comprehensive Review on Artificial Intelligence Empowered Solutions for Enhancing
  Pedestrian and Cyclist Safety
arxiv_id: '2510.03314'
source_url: https://arxiv.org/abs/2510.03314
tags:
- detection
- ieee
- pedestrian
- prediction
- safety
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper provides a comprehensive review of AI-empowered camera-based
  sensing systems for enhancing the safety of vulnerable road users (VRUs), including
  pedestrians and cyclists. It systematically examines four core vision-based tasks:
  detection and classification, tracking and re-identification, trajectory prediction,
  and intent recognition and prediction.'
---

# A Comprehensive Review on Artificial Intelligence Empowered Solutions for Enhancing Pedestrian and Cyclist Safety

## Quick Facts
- **arXiv ID**: 2510.03314
- **Source URL**: https://arxiv.org/abs/2510.03314
- **Reference count**: 40
- **Primary result**: Systematic review of AI-powered camera-based systems for VRU safety, covering detection, tracking, prediction, and intent recognition with focus on recent 5-year advances and practical deployment challenges.

## Executive Summary
This paper provides a comprehensive review of AI-empowered camera-based sensing systems for enhancing the safety of vulnerable road users (VRUs), including pedestrians and cyclists. It systematically examines four core vision-based tasks: detection and classification, tracking and re-identification, trajectory prediction, and intent recognition and prediction. The review emphasizes recent developments over the past five years and discusses challenges such as data scarcity, model generalization, edge deployment, and environmental limitations. By integrating technical advances with practical considerations, the paper aims to guide future research toward more reliable, scalable, and context-aware VRU protection systems.

## Method Summary
This paper is a survey/review rather than an experimental study. It systematically categorizes research into four core tasks: detection/classification, tracking/re-identification, trajectory prediction, and intent recognition/prediction. The review analyzes state-of-the-art methods including transformer-based detectors (RT-DETR, YOLOv11), GNN-based predictors (GroupNet, Social-STGCNN), and pose-based intent models (VRUNet, IntentFormer). Key datasets referenced include COCO, CityPersons, KAIST, and FLIR. The review methodology involves aggregating and synthesizing recent advances from the literature without presenting original experimental validation.

## Key Results
- Cross-modal fusion (RGB+thermal) significantly improves detection robustness in low-light conditions through redundant feature representation
- GNN-based trajectory prediction models better capture social interactions between road users than traditional physics-based approaches
- Pose-based temporal reasoning provides more reliable intent inference signals than bounding boxes alone for high-level behavior prediction

## Why This Works (Mechanism)

### Mechanism 1: Cross-Modal Sensory Fusion for Environmental Robustness
- **Claim**: Integrating thermal infrared data with standard RGB inputs maintains detection performance where monocular vision fails, specifically in low-light or high-contrast conditions.
- **Mechanism**: Visible-spectrum cameras lose feature fidelity in darkness, while thermal sensors capture heat signatures independent of ambient light. By fusing these modalities (e.g., using attention-based weighting like MS-DETR or GAFF), the system creates a "redundant" representation where the strongest signal (edges in day, heat at night) dominates the feature map.
- **Core assumption**: The spatial alignment between thermal and RGB frames can be calibrated and maintained, and the computational overhead of processing two streams does not violate latency constraints.
- **Evidence anchors**: [Section III.A.2]: "Multispectral fusion, particularly the combination of RGB and thermal infrared imagery, has become the most widely adopted strategy... MS-DETR [47] employ dual CNN backbones... [and] modality-balanced optimization." [Section III.A.2]: "Illumination-Aware Faster R-CNN... uses a learned brightness estimator to switch between visible and thermal detection branches."
- **Break condition**: Performance degrades if thermal sensors are saturated (e.g., hot weather) or if RGB-thermal registration drifts, causing feature misalignment that confuses the detector.

### Mechanism 2: Social-Force Graph Modeling for Trajectory Prediction
- **Claim**: Treating road users as nodes in a dynamic graph allows models to better anticipate non-linear movements (e.g., a cyclist swerving) than kinematic physics alone.
- **Mechanism**: Standard physics models assume constant velocity. GNN-based approaches (like GroupNet or Social-STGCNN) construct edges between agents (pedestrians, cars) to model "social forces"â€”repulsion (avoidance) or attraction (grouping). This allows the model to learn implicit negotiation behaviors (e.g., "if car A slows, pedestrian B accelerates").
- **Core assumption**: The observed behavior adheres to implicit social traffic norms, and sufficient historical frame data exists to establish the graph topology before prediction occurs.
- **Evidence anchors**: [Section III.C]: "GNN-based approaches... [have] strong ability to model relational interactions... GroupNet by Xu et al. [108] introduced multiscale hypergraph neural networks... modeling group-wise interactions." [Section III.C]: "Social-STGCNN [124]... captured social dynamics in crowded environments, enabling the prediction of both crossing intention and fine-grained motion trajectories."
- **Break condition**: The mechanism fails in "unsocial" or chaotic scenarios (e.g., jaywalking, erratic behavior) where historical interactions do not predict future actions, or in dense crowds where graph connectivity becomes computationally prohibitive.

### Mechanism 3: Pose-Based Temporal Reasoning for Intent Inference
- **Claim**: Analyzing skeletal sequences (keypoints) over time provides a more robust signal for high-level intent (e.g., "waiting to cross") than bounding boxes or single frames.
- **Mechanism**: Bounding boxes lack internal articulation. By extracting keypoints (hips, shoulders, head angle) and feeding them into RNNs or Transformers (e.g., VRUNet, IntentFormer), the system detects "preparatory movements" (weight shifting, head turning) that precede action, buying critical reaction time.
- **Core assumption**: Keypoint detectors remain robust under occlusion and low resolution, and the specific cultural or physical "cues" of intent are consistent across the demographic distribution of the data.
- **Evidence anchors**: [Section III.D]: "VRUNet [116]... uses a combination of 2D human pose sequences... Action-ViT [129]... highlights critical visual cues such as foot placement and gaze direction." [Section III.D]: "ST-CrossingPose [125] proposed a spatial-temporal graph convolutional network directly operating on pedestrian skeleton sequences."
- **Break condition**: Performance collapses if the pedestrian is partially occluded (missing keypoints) or if the subject remains perfectly still, removing the temporal "preparatory" signal the model relies on.

## Foundational Learning

- **Concept: Object Detection Architectures (CNN vs. Transformer)**
  - **Why needed here**: You must select the backbone (YOLO, R-CNN, ViT) based on the trade-off between real-time speed (edge deployment) and global context (occlusion handling).
  - **Quick check question**: Can you explain why a Deformable DETR might handle a crowd of pedestrians better than a standard YOLOv3, but potentially run slower?

- **Concept: Graph Neural Networks (GNNs)**
  - **Why needed here**: Required to implement the trajectory prediction mechanisms discussed in the paper, as they model the *relationships* between agents rather than just the agents in isolation.
  - **Quick check question**: In the context of a crosswalk, how would a GNN represent the interaction between a cyclist and a turning vehicle differently than a standard coordinate history vector?

- **Concept: Multispectral Image Alignment**
  - **Why needed here**: Critical for low-light safety mechanisms. You cannot simply concatenate RGB and Thermal images; you must understand calibration and alignment strategies.
  - **Quick check question**: If a thermal camera is mounted 5cm above an RGB camera, why does this baseline difference require rectification before fusing features, and what happens to detection if you skip this step?

## Architecture Onboarding

- **Component map**: Input Layer (RGB Camera + Optional Thermal/LiDAR) -> Perception Layer (Backbone CNN/Transformer + Neck Feature Fusion + Detection Head) -> Temporal Layer (Tracker + Feature Bank) -> Reasoning Layer (GNN/Transformer for Trajectory/Intent) -> Output Layer (Control signal/Alert)

- **Critical path**: The **Detection Head** is the primary bottleneck. If the detector misses a VRU (low recall) in frame *t*, the tracker cannot initialize, and the predictor has no input. Occlusion handling in the detection stage is the highest leverage point.

- **Design tradeoffs**:
  - **Latency vs. Context**: Heavy Transformer backbones (Swin, Deformable DETR) provide better context for occlusion but introduce latency (~50-100ms), which may be unacceptable for emergency braking at high speeds. Lightweight CNNs (MobileNet, YOLOv11) are faster but struggle with small/distant VRUs.
  - **Generalization vs. Specialization**: Models trained on generic datasets (COCO) struggle with specific VRU poses (e.g., scooter riders). Fine-tuning on specific datasets (like custom e-scooter data) improves accuracy but risks overfitting to that specific intersection environment.

- **Failure signatures**:
  - **Identity Switching**: Tracker swaps IDs when two pedestrians cross paths; indicates weak appearance features in the Re-ID module.
  - **Frozen Prediction**: Trajectory model predicts a straight line despite a curve; indicates failure to capture social interaction or steering dynamics.
  - **Ghost Objects**: Detector hallucinates VRUs in reflections or shadows; suggests lack of depth/multimodal fusion.

- **First 3 experiments**:
  1. **Baseline Profiling**: Deploy a standard YOLOv11 model on a held-out validation set of "crowded intersection" video. Measure mAP (mean Average Precision) specifically for the "Pedestrian" and "Cyclist" classes under daylight conditions to establish a performance floor.
  2. **Occlusion Stress Test**: Evaluate the baseline against a synthetically occluded dataset (e.g., applying random masks to 20% of pedestrians) to quantify the drop in recall and determine if an "occlusion-aware" head (like Mask-Guided Attention) is required.
  3. **Tracking Integration**: Integrate a simple tracker (e.g., ByteTrack) onto the detector. Measure ID Switch (IDS) metrics. If IDS is high, experiment with adding a Re-ID embedding head to the architecture to improve feature distinctness.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can domain generalization techniques be improved to match the performance of models trained and evaluated within the same domain for VRU perception tasks?
- Basis in paper: [explicit] Section IV.B states that "domain generalization methods, while effective, often underperform compared to models trained and evaluated within the same domain."
- Why unresolved: Current Unsupervised Domain Adaptation (UDA) methods are sensitive to hyperparameters, and data-efficient strategies suffer when domain shifts are large or labeled examples are unrepresentative.
- What evidence would resolve it: A domain generalization framework that demonstrates statistically equivalent detection and tracking accuracy on unseen geographic locations compared to a model trained directly on that target location's data.

### Open Question 2
- Question: Can diffusion-based synthetic data generation effectively supplement datasets to improve detection for underrepresented VRU subclasses (e.g., e-scooters, mobility aids) without introducing distribution shift?
- Basis in paper: [explicit] Section IV.A identifies "less common VRU subclasses" as underrepresented and cites "Scenario Diffusion" as a promising but emergent solution to address data scarcity.
- Why unresolved: While synthetic data offers a solution, the paper notes that generating "realistic scenarios involving less frequently observed VRU categories" remains difficult and costly.
- What evidence would resolve it: A study showing that a detector trained predominantly on diffusion-generated synthetic data for rare VRU classes achieves comparable precision and recall to one trained on an equivalent volume of manually annotated real-world data.

### Open Question 3
- Question: How can perception systems maintain reliable VRU detection performance under "unknown or evolving sensor degradation" combined with dynamic environmental factors like weather?
- Basis in paper: [inferred] Section IV.D discusses hardware limitations and environmental degradation separately, noting that "unknown or evolving sensor degradation" and "adverse conditions" hinder reliability.
- Why unresolved: Current approaches often treat weather and sensor wear as static problems, whereas real-world deployments face compounding, dynamic degradations that standard restoration networks cannot always predict.
- What evidence would resolve it: The development of an adaptive "all-in-one" restoration framework that dynamically adjusts to real-time sensor telemetry (e.g., lens dirt accumulation) and weather data to stabilize detection accuracy over long-term deployment.

## Limitations
- The review is based on literature synthesis rather than original experimental validation, limiting empirical verification of effectiveness claims
- Focus on vision-based approaches may underrepresent complementary sensing modalities (radar, LiDAR) that could enhance reliability
- Does not quantify the real-world impact of identified challenges (data scarcity, environmental variability, edge deployment) across reviewed methods

## Confidence
- **High Confidence**: The systematic categorization of research into four core tasks (detection, tracking, trajectory prediction, intent recognition) is well-supported by the extensive citation network and aligns with established VRU safety frameworks.
- **Medium Confidence**: Claims about recent technical advances (transformer-based detectors, GNN predictors, pose-based intent models) are credible based on cited literature, but their relative performance advantages are not empirically validated within this paper.
- **Low Confidence**: Specific claims about edge deployment feasibility and real-world robustness are largely theoretical, as the paper does not present implementation details or field test results for the discussed systems.

## Next Checks
1. **Benchmark Validation**: Conduct head-to-head comparisons of the highlighted SOTA methods (YOLOv11, RT-DETR, GroupNet, IntentFormer) on standardized VRU datasets (CityPersons, MOT17) under controlled environmental conditions to verify claimed performance improvements.

2. **Edge Deployment Stress Test**: Implement a representative subset of reviewed models (e.g., lightweight CNN detector + GNN predictor) on target edge hardware (NVIDIA Jetson) to measure actual inference latency, memory footprint, and frame rate degradation under realistic thermal and power constraints.

3. **Cross-Dataset Generalization Study**: Train models on one geographic dataset (e.g., EuroCity Persons) and test on another (e.g., KAIST or thermal datasets) to quantify the data scarcity and generalization challenges explicitly discussed in the review, particularly for intent recognition across cultural and environmental contexts.