---
ver: rpa2
title: Long-Horizon Model-Based Offline Reinforcement Learning Without Conservatism
arxiv_id: '2512.04341'
source_url: https://arxiv.org/abs/2512.04341
tags:
- learning
- uncertainty
- offline
- planning
- horizon
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: NEUBAY addresses offline reinforcement learning without conservatism
  by using a Bayesian approach with long-horizon planning. It models a posterior distribution
  over world models and trains a history-dependent agent to maximize expected rewards,
  enabling test-time generalization.
---

# Long-Horizon Model-Based Offline Reinforcement Learning Without Conservatism

## Quick Facts
- arXiv ID: 2512.04341
- Source URL: https://arxiv.org/abs/2512.04341
- Reference count: 40
- One-line primary result: Achieves state-of-the-art on 7 D4RL datasets using long-horizon planning without conservative penalties

## Executive Summary
NEUBAY is a model-based offline reinforcement learning algorithm that achieves strong performance without using conservative penalties or pessimism. It trains a Bayesian ensemble of world models and uses long-horizon rollouts with adaptive truncation based on epistemic uncertainty. The method applies layer normalization to mitigate compounding errors and uses a recurrent history-dependent agent to generalize at test time. NEUBAY outperforms conservative baselines on most D4RL and NeoRL benchmarks, particularly excelling on low-quality and moderate-coverage datasets.

## Method Summary
NEUBAY frames offline RL as an epistemic POMDP where the hidden state is the true environment dynamics. It trains an ensemble of 100 MLPs with Layer Normalization to predict Gaussian distributions over rewards and next states. The agent is a recurrent actor-critic using Linear Recurrent Units (LRU) that processes historical context. Rollouts are generated by sampling models from the ensemble and truncating based on uncertainty thresholds calibrated to dataset disagreement. The agent is trained on a mixture of real and imagined data using off-policy RL algorithms like REDQ/SAC-N.

## Key Results
- Achieves state-of-the-art on 7 out of 9 D4RL datasets evaluated
- Succeeds with rollout horizons of 64-512 steps, contrary to dominant practice of short horizons
- Outperforms conservative algorithms particularly on low-quality and moderate-coverage datasets
- Layer normalization enables stable long-horizon rollouts by bounding compounding errors

## Why This Works (Mechanism)

### Mechanism 1
Long-horizon rollouts reduce value overestimation by shifting the bias burden from bootstrapping to model-generated rewards. The value update discounts the bootstrapped term exponentially with horizon, reducing overall error if model-generated rewards have lower bias than edge Q-values.

### Mechanism 2
Adaptive rollout truncation based on epistemic uncertainty enables safe long-horizon planning. Rollouts extend up to 512 steps in high-confidence regions and stop before compounding errors diverge, using ensemble disagreement as a proxy for prediction error.

### Mechanism 3
Layer Normalization in the world model linearizes the growth of compounding errors by bounding hidden feature norms, ensuring state prediction updates remain linearly bounded by weights.

## Foundational Learning

- **The Epistemic POMDP**: Offline RL is framed as a POMDP where the hidden state is the true environment dynamics. A history-dependent belief is needed to infer the dynamics and generalize at test time. Quick check: Why does a Markov policy fail in this setting? (Answer: It cannot aggregate historical evidence to infer the latent model $m^*$).

- **Bayesian vs. Robust Optimization**: Standard offline RL uses "Robust" optimization (maximize return under worst-case model). NEUBAY uses "Bayesian" optimization (maximize expected return over posterior). Quick check: Does the algorithm penalize uncertainty or average over it?

- **Value Overestimation in Offline RL**: The primary failure mode addressed. Without online interaction, Q-functions overestimate returns on out-of-distribution actions. Quick check: How does the "edge-of-reach" problem (bootstrapping on unseen states) drive overestimation?

## Architecture Onboarding

- **Component map**: World Ensemble -> Planner (Adaptive rollout) -> Recurrent Agent (LRU) -> Policy

- **Critical path**: 
  1. Pretrain world ensemble on offline dataset using MLE loss
  2. Calibrate uncertainty threshold using dataset disagreement
  3. Generate imagined data via branching rollouts with adaptive truncation
  4. Update recurrent agent on mixture of real and imagined data

- **Design tradeoffs**: 
  - Horizon $\zeta$: Low thresholds act like short horizons, causing overestimation; $\zeta=1.0$ allows full adaptive range
  - Ensemble size $N$: Small ensembles fail to capture posterior diversity; $N=100$ is standard
  - Agent memory: Markov agents work for simple tasks but fail on AntMaze; recurrent agents required for epistemic POMDP

- **Failure signatures**: 
  - Exploding Q-values: Horizon too short ($\zeta < 1.0$) or high model error
  - Diverging state norms: LayerNorm missing from world model
  - AntMaze collapse: Using Markov agent instead of recurrent (cannot infer map layout)

- **First 3 experiments**:
  1. Baseline sanity check: Run on "Hopper-Medium-Expert" with $\zeta \in \{0.9, 0.99, 1.0\}$ to validate overestimation mechanism
  2. LayerNorm ablation: Visualize compounding error on 200-step rollout with/without LayerNorm
  3. Bandit generalization: Replicate "Unseen Arm" experiment to confirm Bayesian policy explores and adapts at test time

## Open Questions the Paper Calls Out

### Open Question 1
How can Bayesian offline RL be adapted to handle narrow, medium-quality datasets where current posterior approximations lead to severe model errors? The authors note NEUBAY is weaker on AntMaze due to high-dimensional model errors requiring future advances in world modeling.

### Open Question 2
Can the sensitivity to critical hyperparameters, specifically context encoder learning rate and real data ratio, be reduced or automated? The authors list this as a direction for future work after showing performance varies significantly with these parameters.

### Open Question 3
How can multi-step prediction or generative world models be integrated to further mitigate compounding errors in this non-conservative setting? The authors suggest these architectures could "push the limits further" beyond single-step MLE models.

### Open Question 4
Are there more scalable uncertainty quantification methods than deep ensembles that can maintain necessary posterior fidelity for long-horizon planning? Large ensembles are computationally expensive, creating a need for more efficient posterior approximation.

## Limitations
- Relies on the assumption that model-generated rewards have lower bias than bootstrapped Q-values, but this comparison isn't directly validated
- Assumes epistemic uncertainty (ensemble disagreement) reliably proxies for compounding error without rigorous validation
- Requires large ensembles (N=100) for adequate posterior diversity, creating computational overhead
- Performance on narrow, medium-quality datasets remains limited due to current world modeling constraints

## Confidence

- **High confidence**: NEUBAY's superior performance on D4RL/NeoRL benchmarks compared to conservative methods
- **Medium confidence**: Long-horizon planning (64-512 steps) is critical for reducing overestimation without conservatism
- **Medium confidence**: Adaptive truncation using uncertainty thresholds enables safe long-horizon planning
- **Low confidence**: Layer Normalization is the key to bounding compounding errors
- **Medium confidence**: Bayesian POMDP framing is necessary for generalization in offline RL

## Next Checks

1. **Bias validation**: Measure and compare the bias of model-generated rewards versus bootstrapped Q-values at rollout edges on a held-out validation set to directly test the overestimation reduction mechanism.

2. **Uncertainty correlation**: Compute the correlation between ensemble disagreement (uncertainty metric) and actual prediction error on a validation trajectory to validate the adaptive truncation assumption.

3. **Error growth ablation**: Perform ablations removing LayerNorm and other normalization techniques (BatchNorm, weight normalization) to isolate the specific contribution of LayerNorm to error bounding.