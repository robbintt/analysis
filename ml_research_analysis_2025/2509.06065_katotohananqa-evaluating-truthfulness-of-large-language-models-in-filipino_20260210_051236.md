---
ver: rpa2
title: 'KatotohananQA: Evaluating Truthfulness of Large Language Models in Filipino'
arxiv_id: '2509.06065'
source_url: https://arxiv.org/abs/2509.06065
tags:
- filipino
- english
- questions
- language
- truthfulqa
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces KatotohananQA, a Filipino translation of
  the English TruthfulQA benchmark, to evaluate the truthfulness of large language
  models (LLMs) in a low-resource language. Using a binary-choice framework, seven
  free-tier proprietary models were assessed, revealing a significant performance
  gap between English and Filipino, with average accuracies of 94.72% and 83.87%,
  respectively.
---

# KatotohananQA: Evaluating Truthfulness of Large Language Models in Filipino

## Quick Facts
- arXiv ID: 2509.06065
- Source URL: https://arxiv.org/abs/2509.06065
- Authors: Lorenzo Alfred Nery; Ronald Dawson Catignas; Thomas James Tiam-Lee
- Reference count: 18
- Primary result: Significant performance gap between English (94.72%) and Filipino (83.87%) truthfulness evaluation

## Executive Summary
This paper introduces KatotohananQA, a Filipino translation of the English TruthfulQA benchmark, to evaluate LLM truthfulness in a low-resource language. Seven free-tier proprietary models were assessed using a binary-choice framework, revealing a 10.85% performance gap between English and Filipino. The study demonstrates that newer OpenAI models (GPT-5 and GPT-5 mini) achieve multilingual robustness with minimal performance differences, while older models show substantial gaps. Results indicate that performance differences stem from training data imbalance rather than knowledge absence, with reasoning-heavy categories showing amplified gaps.

## Method Summary
The study translated 790 binary-choice questions from TruthfulQA to Filipino using Google Translate followed by native speaker verification. Seven free-tier proprietary models were evaluated zero-shot in both languages using identical system prompts instructing truthful answers in "A" or "B" format. Temperature was set to 0.0 for all models except GPT-5/GPT-5 mini (temperature 1.0). Responses were parsed by extracting the first letter, and McNemar's test with Cohen's g effect sizes quantified significance and magnitude of performance differences. Analysis stratified results by question type, category, and topic.

## Key Results
- Significant performance gap: English 94.72% vs. Filipino 83.87% accuracy
- Newer OpenAI models (GPT-5, GPT-5 mini) show multilingual robustness with 0-1.01% gaps
- Adversarial questions show 12.54% gap vs. 8.88% for non-adversarial questions
- "Logical Falsehood" category shows largest gap (+29.59%) indicating reasoning transfer failure

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Translating benchmarks to low-resource languages reveals performance gaps driven by training data imbalance, not knowledge absence.
- Mechanism: Models encode factual knowledge primarily in high-resource language representations. When queried in Filipino, they must cross-lingually transfer this knowledge, introducing friction. The gap widens for adversarial questions because the additional reasoning load compounds with the transfer cost.
- Core assumption: Translation process preserves semantic equivalence well enough that performance differences reflect model capability rather than benchmark quality.
- Evidence anchors: Significant performance gap between English and Filipino truthfulness; findings suggest performance gap isn't about lack of knowledge but training data disparity; related work confirms similar patterns in other low-resource languages.
- Break condition: If models showed uniform degradation across all question types, the mechanism would suggest translation quality issues rather than training data imbalance.

### Mechanism 2
- Claim: Newer proprietary models achieve multilingual robustness through expanded training corpora with better language diversity.
- Mechanism: GPT-5 and GPT-5 mini showed 0% and 1.01% performance gaps respectively, versus 26-27% for older models. This suggests recent training pipelines incorporate more balanced multilingual data, reducing the cross-lingual transfer penalty.
- Core assumption: Architecture improvements alone cannot explain this robustness; data composition is the primary driver.
- Evidence anchors: Newer OpenAI models demonstrating strong multilingual robustness; attributed to more recent release compared to other models.
- Break condition: If open-source models with known training data showed similar robustness without multilingual data augmentation, architectural factors would be more likely.

### Mechanism 3
- Claim: Binary-choice evaluation isolates truthfulness recognition from generation fluency, but may underestimate real-world hallucination rates.
- Mechanism: By presenting models with one correct and one incorrect answer, the benchmark tests whether models can identify truth rather than generate it. This reduces variance from language generation quality but does not measure open-ended hallucination propensity.
- Core assumption: Truth recognition correlates with truth generation in deployment scenarios.
- Evidence anchors: Binary-choice setting tests capabilities but may not be representative of generation tasks; limitation noted in original TruthfulQA paper.
- Break condition: If models with high binary-choice accuracy showed high hallucination rates in open-ended generation, the benchmark would measure a distinct capability.

## Foundational Learning

- Concept: **Low-resource language (LRL) in NLP**
  - Why needed here: Filipino represents ~0.83% of Common Crawl versus 45.26% for English. Understanding this imbalance explains why models underperform.
  - Quick check question: If a model was trained on 1% Filipino data and 45% English data, would you expect uniform performance? Why or why not?

- Concept: **Adversarial vs. non-adversarial questions**
  - Why needed here: Adversarial questions (designed to elicit common misconceptions) showed 12.54% gap vs. 8.88% for non-adversarial. The format itself adds cognitive load.
  - Quick check question: What makes "What happens if you eat a penny?" adversarial compared to "What is the capital of France?"

- Concept: **McNemar's test and Cohen's g for paired nominal data**
  - Why needed here: The paper uses these to determine whether English-Filipino performance differences are statistically significant and to quantify effect sizes.
  - Quick check question: Why use McNemar's test (paired) rather than a chi-square test (unpaired) when comparing the same models on two language versions?

## Architecture Onboarding

- Component map:
  TruthfulQA benchmark -> Translation pipeline (Google Translate + native speaker verification) -> Evaluation harness (zero-shot binary-choice) -> Analysis layer (McNemar's test + Cohen's g)

- Critical path:
  1. Translate questions while preserving Western cultural context (do not localize)
  2. Run all 7 models on both English and Filipino versions with identical prompting structure
  3. Apply McNemar's test + Cohen's g to quantify significance and effect size
  4. Stratify results by question characteristics to identify failure patterns

- Design tradeoffs:
  - Translation fidelity vs. cultural adaptation: Chose fidelity to enable direct cross-lingual comparison, but tests Western knowledge in Filipino rather than Filipino cultural knowledge
  - Binary-choice vs. generation: Binary-choice reduces evaluation complexity and variance but does not measure open-ended hallucination
  - Proprietary-only models: Free-tier proprietary models represent real-world usage but limit reproducibility and architectural analysis

- Failure signatures:
  - Models answering in wrong language
  - Large accuracy drops on "Logical Falsehood" category (+29.59%) suggesting reasoning transfer failure
  - Inconsistent performance on culture-dependent proverbs/idioms

- First 3 experiments:
  1. Extend to open-source models: Evaluate Llama, Mistral, and Aya on KatotohananQA to determine whether open weights show similar gaps or benefit from explicit multilingual training
  2. Add Filipino-cultured questions: Create a complementary benchmark with Philippines-specific misconceptions to test whether the gap is language-specific or culture-specific
  3. Cross-lingual consistency check: For each question, test whether models give the same answer in both languages regardless of correctnessâ€”this isolates consistency from accuracy

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How does LLM performance on truthfulness benchmarks differ when questions are rooted in Filipino cultural nuances compared to direct translations of Western-centric questions?
- **Basis in paper:** The authors recommend "creating truthfulness benchmarks sensitive to the local cultural nuances, rather than simply translating the existing ones."
- **Why unresolved:** KatotohananQA relies on direct translation, retaining the Western context of the original TruthfulQA, which may not capture culture-specific misconceptions.
- **What evidence would resolve it:** A comparison of model scores on a culturally grounded Filipino benchmark versus the translated KatotohananQA.

### Open Question 2
- **Question:** Does the performance gap between English and Filipino persist when evaluating models using open-ended generation tasks rather than binary-choice selection?
- **Basis in paper:** The authors note the binary-choice setting "may not be representative of generation tasks" and suggest "expanding to other evaluation methods."
- **Why unresolved:** The current methodology only tests a model's ability to identify a truthful answer, not its ability to generate truthful free-form text.
- **What evidence would resolve it:** Evaluation results using generative metrics (e.g., BLEU, ROUGE, or human evaluation) on the same questions without multiple-choice options.

### Open Question 3
- **Question:** To what extent do open-source and premium proprietary models exhibit the same multilingual robustness observed in the free-tier proprietary models tested?
- **Basis in paper:** The authors state that an "increased breadth of models that include open-source, open-weight, and more premium models would help generalize findings."
- **Why unresolved:** The study is restricted to seven specific free-tier proprietary models, limiting the generalizability of the robustness findings to the broader LLM ecosystem.
- **What evidence would resolve it:** Benchmarking KatotohananQA on a diverse set of open-weights (e.g., Llama, Mistral) and paid-tier APIs (e.g., GPT-4/5 premium).

## Limitations
- Reliance on proprietary models restricts reproducibility and architectural analysis
- Translation verification methodology (annotator numbers, agreement metrics) remains unspecified
- Benchmark tests Western cultural knowledge rather than Filipino-specific knowledge
- Binary-choice format does not capture open-ended hallucination tendencies
- Version drift across model APIs may affect cross-model comparisons

## Confidence
- **High Confidence**: The observed performance gap between English (94.72%) and Filipino (83.87%) is robust and statistically significant
- **Medium Confidence**: Attribution of performance gaps to training data imbalance rather than knowledge absence is well-supported but relies on indirect evidence
- **Low Confidence**: Claim that newer models achieve multilingual robustness specifically through expanded training corpora lacks direct empirical support

## Next Checks
1. Extend to open-source models: Evaluate multilingual-focused open-source models (Llama, Mistral, Aya) on KatotohananQA to determine whether observed gaps are specific to proprietary training approaches
2. Culture-specific validation: Develop a complementary benchmark with Philippines-specific misconceptions and cultural knowledge to distinguish between language-specific versus culture-specific performance differences
3. Cross-lingual consistency analysis: For each question, test whether models produce consistent answers across languages regardless of correctness, to isolate transfer consistency from accuracy effects