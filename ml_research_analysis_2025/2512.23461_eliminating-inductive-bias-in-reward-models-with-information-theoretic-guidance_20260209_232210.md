---
ver: rpa2
title: Eliminating Inductive Bias in Reward Models with Information-Theoretic Guidance
arxiv_id: '2512.23461'
source_url: https://arxiv.org/abs/2512.23461
tags:
- reward
- bias
- arxiv
- server
- length
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles reward hacking in RLHF caused by inductive biases
  in human preference data. The proposed DIR method uses information theory to maximize
  mutual information between reward model outputs and true human preferences while
  minimizing the correlation between predictions and irrelevant bias attributes like
  response length, sycophancy, or formatting.
---

# Eliminating Inductive Bias in Reward Models with Information-Theoretic Guidance

## Quick Facts
- arXiv ID: 2512.23461
- Source URL: https://arxiv.org/abs/2512.23461
- Reference count: 40
- Primary result: DIR method reduces inductive bias in reward models, improving downstream RLHF performance by up to 2.4% win rate

## Executive Summary
This paper addresses reward hacking in RLHF caused by inductive biases in human preference data. The authors propose DIR (Debiasing via Information-theoretic guidance), a method that uses mutual information maximization between reward model outputs and true human preferences while minimizing correlation with bias attributes like response length, sycophancy, or formatting. Implemented via variational bounds on mutual information, DIR effectively reduces bias, improves reward model generalization, and yields better downstream RLHF performance compared to baselines like PoE, ALBM, and InfoRM.

## Method Summary
DIR uses information theory to simultaneously maximize mutual information between reward model outputs and true human preferences while minimizing mutual information between predictions and bias attributes. The method decomposes into a Preference Term (maximizing I(1_y≻ȳ; x,y,ȳ)) and a Debiasing Term (minimizing I(1_y≻ȳ; b)). The Bradley-Terry loss naturally maximizes the preference term, while a variational upper bound (CLUB) provides a tractable surrogate for the debiasing term. A lightweight classifier q_ψ(b|H) is trained to predict bias from hidden states, with the CLUB loss penalizing accurate bias prediction and forcing the hidden representation to become less informative about bias.

## Key Results
- Reduces Pearson correlation for length bias from 0.533 to 0.468
- Improves RM-Bench total accuracy from 68.10 to 70.18
- Increases win rate on ArenaHard from 51.9% to 54.3% against baselines
- Effectively scales to multiple concurrent biases (length and sycophancy)

## Why This Works (Mechanism)

### Mechanism 1: Information-Theoretic Objective Decomposition
- **Claim:** Reward model debiasing can be formulated as simultaneously maximizing mutual information (MI) between predictions and true preferences while minimizing MI between predictions and bias attributes.
- **Mechanism:** The objective decomposes into a Preference Term (maximize I(1_y≻ȳ; x,y,ȳ)) and a Debiasing Term (minimize I(1_y≻ȳ; b)). The Bradley-Terry loss naturally maximizes the preference term, while a variational upper bound (CLUB) provides a tractable surrogate for the debiasing term.
- **Core assumption:** The bias attribute b is sufficiently determined by the input (x,y,ȳ) such that (b → (x,y,ȳ) → H → 1_y≻ȳ) forms a Markov chain, enabling the data processing inequality to bound I(1_y≻ȳ; b) ≤ I(H; b).
- **Evidence anchors:**
  - [abstract] "DIR uses information theory to maximize mutual information between reward model outputs and true human preferences while minimizing the correlation between predictions and irrelevant bias attributes"
  - [section 3, equations 8-11] Derives the Markov chain property and CLUB upper bound
  - [corpus] Weak direct corpus support; neighboring papers discuss inductive bias broadly but not this specific MI formulation
- **Break condition:** If the bias attribute is not recoverable from the input (violating the Markov assumption), the theoretical upper bound may not hold.

### Mechanism 2: Contrastive Log-Ratio Upper Bound (CLUB) Minimization
- **Claim:** Minimizing the CLUB variational bound effectively reduces non-linear correlations between hidden representations and bias attributes without requiring explicit bias labels in the preference data.
- **Mechanism:** A lightweight classifier q_ψ(b|H) is trained to predict bias from hidden states. The CLUB loss L_Debiasing = E[log q_ψ(b|H)] - E[E[log q_ψ(b|H)]] penalizes accurate bias prediction, forcing H to become less informative about b. This is iteratively updated alongside the RM.
- **Core assumption:** The variational approximation q_ψ must be sufficiently expressive to capture the true conditional p(b|H); otherwise, CLUB ceases to be a tight upper bound.
- **Evidence anchors:**
  - [abstract] "This is implemented via variational bounds on mutual information"
  - [section 3, equation 12] Defines L_Debiasing explicitly
  - [corpus] No direct corpus validation of CLUB for RM debiasing; Cheng et al. (2020) original work is cited
- **Break condition:** If q_ψ is underparameterized or the bias is too entangled with quality, minimizing CLUB may inadvertently remove preference-relevant signal.

### Mechanism 3: Relative Bias Encoding with Representation Difference
- **Claim:** Encoding bias as a relative attribute between response pairs (rather than absolute scalar values) enables the debiasing head to operate on a tractable classification task while preserving reward landscape structure.
- **Mechanism:** The input to the debiasing head is Δh = h_w - h_l (the difference between chosen and rejected hidden states), and the bias label is binary (e.g., "chosen is longer: {0,1}"). This focuses debiasing on comparative features rather than absolute properties.
- **Core assumption:** The relevant bias manifests as a relative difference between responses, not as an absolute property of individual responses.
- **Evidence anchors:**
  - [section 5, "Relative Bias Attributes"] Explicitly describes the binary relative bias formulation
  - [section B.3, Table 12] Ablation shows difference representation outperforms concatenation on Chat Hard (78.9% → 83.6%)
  - [corpus] No corpus validation; this appears novel to this work
- **Break condition:** If the bias operates on absolute response properties (e.g., a minimum acceptable length) rather than relative differences, the binary encoding may miss the true bias structure.

## Foundational Learning

- **Concept: Mutual Information (MI) and Variational Bounds**
  - **Why needed here:** The entire DIR framework relies on MI as the objective and variational bounds (BA, CLUB) as tractable estimators. Understanding why MI is intractable and how bounds work is essential.
  - **Quick check question:** Given samples from p(x,y), can you compute I(X;Y) exactly? If not, what additional component does the CLUB bound require?

- **Concept: Information Bottleneck (IB) Principle**
  - **Why needed here:** DIR is explicitly inspired by IB, which trades off compression (minimize I(X;Z)) against prediction accuracy (maximize I(Z;Y)).
  - **Quick check question:** How does the λ hyperparameter in IB (equation 7) relate to the λ in DIR's equation 8? What happens at λ→0 and λ→∞?

- **Concept: Bradley-Terry Preference Model**
  - **Why needed here:** The standard RM ranking loss (equation 3) derives from Bradley-Terry; understanding this connects the preference term to existing RLHF practice.
  - **Quick check question:** If r_φ(x,y_w) - r_φ(x,y_l) = 2.0, what is the predicted probability that y_w is preferred over y_l under Bradley-Terry?

## Architecture Onboarding

- **Component map:** Transformer encoder -> hidden states h_w, h_l -> RM Score Head -> scalar reward r_φ(x,y) AND Debiasing Head q_ψ -> L_Debiasing
- **Critical path:**
  1. Forward pass: encode (x, y_w) and (x, y_l) → obtain h_w, h_l
  2. Compute r_w, r_l via score head → L_Preference via Bradley-Terry
  3. Compute Δh → pass through debiasing head → L_Debiasing via CLUB
  4. Alternate: update ψ via L_Estimator (maximize q_ψ accuracy), then update φ via L_Total

- **Design tradeoffs:**
  - **λ selection (Figure 5):** Low λ (0.1) under-debiases; high λ (10.0) over-corrects and degrades preference learning. Optimal at λ≈1.0 for most cases, but higher λ may help for difficult (Hard) subsets.
  - **Representation format (Table 12):** Difference Δh outperforms concatenation [h_w; h_l] on reasoning-heavy tasks, with lower memory cost.
  - **Bias estimator steps per batch:** Paper uses iterative updates within each batch (Algorithm 1, lines 5-8); too few steps → loose bound; too many → computational overhead.

- **Failure signatures:**
  - **L_Estimator not converging:** If q_ψ fails to predict b from H, CLUB is not a valid upper bound; debiasing may have no effect. Check bias label quality.
  - **Reward scores collapsing to uniform:** If λ is too high or bias is entangled with quality, L_Debiasing may dominate, flattening the reward landscape.
  - **Training instability:** High Approx. KL in PPO rollouts may indicate RM is producing noisy gradients (see Figure 4, right panel).

- **First 3 experiments:**
  1. **Sanity check on bias correlation:** Train a vanilla BT RM on your preference data. Compute Pearson correlation between reward scores and your target bias attribute (e.g., length). Expect r > 0.4 if bias is present.
  2. **Single-bias ablation with λ sweep:** Train DIR with λ ∈ {0.1, 0.5, 1.0, 2.0, 5.0}. Plot RewardBench accuracy vs. bias correlation to identify the sweet spot (replicate Figure 5 behavior).
  3. **Representation comparison:** Compare Δh vs. [h_w; h_l] for your debiasing head. Measure both bias reduction and downstream PPO reward stability. Expect Δh to win on reasoning tasks if results generalize.

## Open Questions the Paper Calls Out
None

## Limitations
- The theoretical foundation relies on the Markov chain assumption (b → (x,y,ȳ) → H → 1_y≻ȳ) for deriving the CLUB upper bound, which may not hold when bias attributes are strongly correlated with response quality
- The variational approximation q_ψ must be sufficiently expressive to provide tight bounds, but this requirement is not empirically validated
- The binary relative encoding of bias attributes assumes bias manifests as relative differences between responses, which may not capture absolute bias properties

## Confidence
- **High confidence:** The experimental results demonstrating bias reduction (correlation decreases from 0.533 to 0.468 for length) and improved downstream RLHF performance (win rate increases from 51.9% to 54.3%) are well-supported by the presented data
- **Medium confidence:** The theoretical derivation of the CLUB bound and its connection to the information bottleneck framework is sound, but the empirical validation of the Markov chain assumption is limited
- **Low confidence:** The claim that DIR generalizes to multiple concurrent biases is supported by a single example with length and sycophancy, without exploring more complex bias interactions or higher-dimensional bias spaces

## Next Checks
1. **Markov chain validation:** Design an experiment that explicitly tests whether the bias attribute b can be predicted from the input (x,y,ȳ) better than from the hidden state H alone. This would validate or falsify the core theoretical assumption.
2. **Underparameterized variational bound test:** Train DIR with increasingly underparameterized debiasing heads (fewer layers, smaller hidden dimensions) to determine the minimum capacity needed for CLUB to remain a tight upper bound, and identify when debiasing effectiveness degrades.
3. **Multi-bias scaling experiment:** Systematically evaluate DIR on datasets with 3+ concurrent bias attributes (e.g., length, sycophancy, formatting, politeness) to test whether the method scales to complex bias landscapes or whether performance degrades as bias dimensionality increases.