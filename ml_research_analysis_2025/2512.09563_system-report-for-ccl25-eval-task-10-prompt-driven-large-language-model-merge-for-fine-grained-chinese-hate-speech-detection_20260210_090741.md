---
ver: rpa2
title: 'System Report for CCL25-Eval Task 10: Prompt-Driven Large Language Model Merge
  for Fine-Grained Chinese Hate Speech Detection'
arxiv_id: '2512.09563'
source_url: https://arxiv.org/abs/2512.09563
tags:
- hate
- speech
- prompt
- detection
- merging
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a three-stage LLM-based framework for fine-grained
  Chinese hate speech detection. The method combines domain-specific prompt engineering
  with supervised fine-tuning and LLM merging to improve detection of implicit hate
  patterns and context-dependent rhetoric.
---

# System Report for CCL25-Eval Task 10: Prompt-Driven Large Language Model Merge for Fine-Grained Chinese Hate Speech Detection

## Quick Facts
- arXiv ID: 2512.09563
- Source URL: https://arxiv.org/abs/2512.09563
- Reference count: 4
- This paper presents a three-stage LLM-based framework for fine-grained Chinese hate speech detection. Evaluated on the STATE-ToxiCN benchmark, the merged model achieved F1-scores of 0.3553 (hard) and 0.4604 (soft) on the preliminary test set, demonstrating superior performance over baseline approaches with over 15% accuracy improvement in hate detection and enhanced generalization to complex scenarios.

## Executive Summary
This paper presents a three-stage LLM-based framework for fine-grained Chinese hate speech detection. The method combines domain-specific prompt engineering with supervised fine-tuning and LLM merging to improve detection of implicit hate patterns and context-dependent rhetoric. Evaluated on the STATE-ToxiCN benchmark, the merged model achieved F1-scores of 0.3553 (hard) and 0.4604 (soft) on the preliminary test set, demonstrating superior performance over baseline approaches with over 15% accuracy improvement in hate detection and enhanced generalization to complex scenarios.

## Method Summary
The framework employs three sequential stages: (1) Prompt Engineering with four-tuple structured templates, bidirectional examples, and category explanations to guide LLMs in extracting implicit hate patterns; (2) Full-parameter Supervised Fine-Tuning on Qwen2.5-7B-Instruct with cross-entropy loss plus L2 regularization to enhance domain adaptation; (3) LLM Merging via task vector pruning and directional consensus to synthesize complementary detection capabilities from multiple prompt-varied models. Training utilized 8×RTX 4090 GPUs with DeepSpeed Zero-3 and Flash Attention 2.0, running for 8 epochs at learning rate 1e-5.

## Key Results
- Merged model achieved F1-scores of 0.3553 (hard matching) and 0.4604 (soft matching) on STATE-ToxiCN preliminary test set
- Outperformed baseline approaches with over 15% accuracy improvement in hate detection
- Showed enhanced generalization to complex scenarios with diminishing returns between Merge2 (0.3530) and Merge3 (0.3553)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Domain-specific prompt engineering improves fine-grained hate speech extraction by enforcing structured output formats and explicit boundary definitions.
- Mechanism: The prompt template incorporates three components: (1) mandatory "four-tuple" structured output (Target—Argument—Targeted Group—Hateful), (2) bidirectional examples contrasting hate speech with non-targeted content, and (3) predefined category explanations mapping to social group attributes. This shifts model focus from generalized semantic analysis to targeted feature extraction.
- Core assumption: Explicit structural constraints and boundary examples reduce ambiguity in implicit hate detection more effectively than open-ended prompting.
- Evidence anchors:
  - [abstract] "context-aware prompts are designed to guide LLMs in extracting implicit hate patterns"
  - [section 3.4.2] "17.6% relative improvement from baseline [ICL: 0.2921] to the optimal configuration [ICL+NH+CE+JC: 0.3436]"
  - [corpus] Related work on hate speech detection emphasizes interpretable extraction; AfriHate notes "socio-cultural background knowledge" requirements, supporting the need for explicit category definitions.
- Break condition: Prompt engineering alone showed diminishing returns; ICL+NH+CE+JC (0.3436) was outperformed by merged models, indicating prompt strategies have an effectiveness ceiling without model fusion.

### Mechanism 2
- Claim: Full-parameter supervised fine-tuning with L2 regularization enhances domain adaptation for implicit semantic discrimination in Chinese hate speech.
- Mechanism: Given pre-trained model θ_pre and labeled dataset D, the loss function L(θ) combines cross-entropy with L2 regularization (λ|θ − θ_pre|²) to constrain parameter drift. AdamW optimizer updates parameters with decoupled weight decay, trained for 8 epochs at learning rate 1e-5.
- Core assumption: High-quality annotated data with fine-grained quadruple labels transfers discriminative capability to implicit hate patterns that general-purpose pre-training cannot capture.
- Evidence anchors:
  - [section 2.3] Formula (1) and (2) define the SFT loss and AdamW update rule with L2 regularization coefficient λ
  - [section 3.4.1] CPT+SFT on base model (0.3379) outperformed RFT method (0.2021), suggesting SFT provides superior guidance for "nuanced hate speech patterns"
  - [corpus] Weak direct corpus evidence for SFT mechanism specifically; related papers focus on detection architectures rather than fine-tuning dynamics.
- Break condition: Single SFT models showed "incomplete information extraction" and "generalization limitations" per Section 1, motivating the merge stage.

### Mechanism 3
- Claim: Merging fine-tuned LLMs via task vector pruning and directional consensus synthesizes complementary detection capabilities, improving robustness against out-of-distribution cases.
- Mechanism: The three-step merge process: (1) **Prune**—apply layer-wise masking with thresholds α (upper) and β (lower) to filter outliers and minor perturbations from task vectors τ_t = θ_t − θ_base; (2) **Direct**—compute consensus direction γ_m via sign aggregation of weighted task vectors; (3) **Merge**—average only parameters aligned with consensus direction, then scale by λ and add to base parameters.
- Core assumption: Different prompt strategies induce orthogonal capabilities in fine-tuned models; sparsifying task vectors reduces interference while preserving complementary features.
- Evidence anchors:
  - [section 2.4] Algorithm 1 defines the complete merge procedure with Prune-Direct-Merge steps
  - [section 3.4.3] "Merge3 (0.3553) progressively outperform[ed] all individual prompt-engineered models," with 5.1% hard score improvement
  - [corpus] Corpus shows limited validation of this specific merge technique; related papers use different fusion approaches (MAV voting, attention fusion).
- Break condition: "Diminishing returns between Merge2 (0.3530) and Merge3 (0.3553) suggest a potential limit to current merging strategies' effectiveness."

## Foundational Learning

- Concept: **Task Vectors**
  - Why needed here: The merge mechanism operates on task vectors (τ = θ_finetuned − θ_base), which represent the parameter shift induced by fine-tuning. Without understanding this abstraction, the prune/direct/merge operations lack conceptual grounding.
  - Quick check question: If you merge two models with opposing task vector signs for a given parameter, what happens to that parameter in the consensus step?

- Concept: **In-Context Learning (ICL)**
  - Why needed here: The prompt strategy builds on ICL, progressively adding Non-Hate examples, Category Explanations, and Judge Criteria. Understanding ICL baseline is prerequisite to interpreting the 17.6% improvement.
  - Quick check question: Why does the paper embed contextual examples "immediately after defining each field" rather than clustering all examples at the prompt's end?

- Concept: **Hard vs. Soft Matching Metrics**
  - Why needed here: The evaluation distinguishes exact four-tuple matching (hard: 0.2504) from partial matching with 50% string similarity threshold (soft: 0.4602). This dual-metric system explains why the method prioritizes different capabilities.
  - Quick check question: If a model predicts "gender" instead of "LGBTQ" for targeted group with correct other fields, does it contribute to hard score, soft score, both, or neither?

## Architecture Onboarding

- Component map: Base Model (Qwen2.5-7B-Instruct) → Prompt Variants (ICL, ICL+NH, ICL+NH+CE, ICL+NH+CE+JC) → Parallel SFT Training → Fine-tuned Models {θ_t} → Task Vector Extraction (τ_t = θ_t − θ_base) → Merge Pipeline: Prune → Direct → Merge → Final Merged Model (θ_m)

- Critical path: The prompt strategy determines what capabilities each SFT model learns; the merge quality depends entirely on complementary diversity across prompt variants. If prompts are too similar, task vectors will exhibit interference rather than orthogonality.

- Design tradeoffs:
  - Merge iterations show diminishing returns (Merge2→Merge3: +0.0023 score only); decide whether additional merge rounds justify computational cost.
  - Pruning thresholds α and β are hyperparameters not tuned in ablation; default values may not generalize to other languages or model families.
  - Full-parameter SFT requires 8×RTX 4090 GPUs with DeepSpeed Zero-3; consider LoRA variants for resource-constrained deployment.

- Failure signatures:
  - Low hard score with high soft score: Model captures semantics but fails exact entity extraction; strengthen four-tuple formatting in prompts.
  - Merged model underperforms best single model: Task vectors have high interference; reduce prompt overlap or adjust pruning thresholds.
  - Poor generalization on Test2: Merged model overfits to prompt-specific patterns; increase α threshold to preserve more base model parameters.

- First 3 experiments:
  1. **Prompt ablation baseline**: Train with ICL only, measure hard/soft scores; establish component contribution of NH, CE, and JC additions.
  2. **Merge hyperparameter sweep**: Vary α (0.1, 0.2, 0.3) and β (0.01, 0.05, 0.1) on validation split; identify optimal sparsity for task vectors.
  3. **Cross-dataset transfer**: Evaluate merged model on COLD benchmark (cited in Table 1) to test generalization beyond STATE-ToxiCN; compare against CPT+SFT baseline.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the merging strategy be adapted to overcome the performance plateau observed when integrating more than two model checkpoints?
- Basis in paper: [explicit] The authors note that "diminishing returns between Merge2 (0.3530) and Merge3 (0.3553) suggest a potential limit to current merging strategies' effectiveness."
- Why unresolved: The linear averaging of task vectors appears to saturate, failing to capture increasingly complex, context-dependent hate markers like dialectal variations and historical allusion as more models are added.
- What evidence would resolve it: Demonstrating a non-linear merging technique or attention-based fusion that yields statistically significant gains beyond the third merge iteration on the STATE-ToxiCN benchmark.

### Open Question 2
- Question: Does extended domain-specific Continued Pre-training (CPT) offer superior feature extraction for implicit hate speech compared to standard Supervised Fine-tuning?
- Basis in paper: [explicit] The paper states, "We hypothesize that extended CPT training with additional domain-specific corpora could further enhance this performance gap," following the observation that CPT+SFT outperformed SFT on the Instruct variant.
- Why unresolved: The current results compare CPT on a base model against SFT on an instruct model, but the specific hypothesis regarding extended training duration and its relative efficiency remains unverified by the provided experiments.
- What evidence would resolve it: A controlled ablation study scaling CPT duration and comparing it directly against SFT on the same base model initialization to isolate the impact of continued pre-training.

### Open Question 3
- Question: How sensitive is the task vector pruning efficacy to the specific threshold values (α, β) across different model layers?
- Basis in paper: [inferred] Algorithm 1 defines α and β as input parameters for masking extreme parameter values, but the experimental section lacks an ablation study on how these specific thresholds were selected or their impact on the final F1-scores.
- Why unresolved: Without sensitivity analysis, it is unclear if the chosen thresholds are robust heuristics or if they require tuning for different LLM architectures to prevent the loss of critical semantic features during the pruning phase.
- What evidence would resolve it: Reporting performance metrics while systematically varying α and β to determine the optimal sparsity range for retaining hate speech detection capabilities.

## Limitations

- The merging hyperparameters (α, β, λ) are referenced but not specified, making exact reproduction impossible
- Diminishing returns between Merge2 (0.3530) and Merge3 (0.3553) suggest current merging strategies may have reached effectiveness ceilings
- Single SFT models showed "incomplete information extraction" and "generalization limitations," indicating fundamental limitations of individual fine-tuned models

## Confidence

**High Confidence Claims:**
- The three-stage framework architecture (prompt engineering → SFT → LLM merging) is clearly specified with reproducible training procedures
- Performance metrics and comparative results against baseline approaches (RFT, ICL) are well-documented
- Computational requirements and infrastructure (8×RTX 4090, DeepSpeed Zero-3) are explicitly stated

**Medium Confidence Claims:**
- The 15% accuracy improvement claim lacks baseline performance context
- Mechanism explanations rely on referenced sections rather than complete descriptions
- Generalization claims to complex scenarios are based on STATE-ToxiCN benchmark performance only

**Low Confidence Claims:**
- Cross-dataset generalization claims (e.g., to COLD benchmark) are mentioned but not empirically validated
- Specific impact of individual prompt components (NH, CE, JC) on final performance is inferred from relative improvements rather than ablation studies

## Next Checks

1. **Hyperparameter Sensitivity Analysis**: Systematically vary merging thresholds α (0.1, 0.2, 0.3) and β (0.01, 0.05, 0.1) on validation data to identify optimal sparsity levels for task vectors, addressing the critical gap in merge hyperparameter specification.

2. **Cross-Dataset Generalization Testing**: Evaluate the merged model on COLD benchmark and other Chinese hate speech datasets to validate claims of enhanced generalization beyond STATE-ToxiCN, measuring performance degradation or improvement.

3. **Prompt Component Ablation Study**: Train and evaluate individual prompt variants (ICL only, ICL+NH, ICL+NH+CE, ICL+NH+CE+JC) on both hard and soft matching metrics to empirically validate the 17.6% relative improvement claim and isolate component contributions.