---
ver: rpa2
title: Spatially Aware Linear Transformer (SAL-T) for Particle Jet Tagging
arxiv_id: '2510.23641'
source_url: https://arxiv.org/abs/2510.23641
tags:
- sal-t
- partition
- attention
- only
- particle
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces the Spatially Aware Linear Transformer (SAL-T),
  a physics-inspired enhancement of the linformer architecture designed for efficient
  particle jet tagging in high-data-throughput environments like the CERN LHC. SAL-T
  incorporates spatially aware partitioning of particles based on kinematic features
  and uses convolutional layers to capture local correlations, maintaining linear
  attention complexity.
---

# Spatially Aware Linear Transformer (SAL-T) for Particle Jet Tagging

## Quick Facts
- **arXiv ID**: 2510.23641
- **Source URL**: https://arxiv.org/abs/2510.23641
- **Reference count**: 40
- **Primary result**: SAL-T outperforms standard Linformer and matches full-attention transformer accuracy on jet tagging while using significantly fewer resources and lower latency.

## Executive Summary
This work introduces the Spatially Aware Linear Transformer (SAL-T), a physics-inspired enhancement of the Linformer architecture designed for efficient particle jet tagging in high-data-throughput environments like the CERN LHC. SAL-T incorporates spatially aware partitioning of particles based on kinematic features and uses convolutional layers to capture local correlations, maintaining linear attention complexity. Evaluated on jet classification tasks, SAL-T outperforms standard Linformer and achieves classification results comparable to full-attention transformers while using significantly fewer resources and lower inference latency. Experiments on both particle physics and generic point cloud datasets confirm this trend.

## Method Summary
SAL-T processes jets by first sorting particles by $k_T$ (transverse momentum weighted by angular distance) and partitioning them into disjoint chunks. It then applies independent low-rank projections to each chunk's Key and Value matrices, ensuring locality in the attention mechanism. Depthwise 2D convolutions are applied to attention logits to capture local correlations before softmax normalization. The model uses dynamic tanh normalization instead of layer normalization and employs a phased training schedule with increasing batch sizes. SAL-T maintains linear attention complexity $O(n \cdot p)$ while incorporating physics-informed inductive biases through its sorting and partitioning strategy.

## Key Results
- SAL-T achieves 81.18% accuracy on jet tagging, outperforming Linformer (79.43%) and matching full-attention transformers
- Classification latency is ~27μs on GPU, meeting trigger system requirements
- SAL-T demonstrates strong generalization on ModelNet10 point cloud classification (AUC ~0.90)
- Ablation studies confirm both partitioning and convolution components are critical for performance

## Why This Works (Mechanism)

### Mechanism 1: Spatially-Gated Linear Projection
- **Claim:** SAL-T reduces attention complexity from $O(n^2)$ to $O(n \cdot p)$ while preserving physical locality better than standard low-rank methods.
- **Mechanism:** Particles are sorted by $k_T$ metric and partitioned into $p$ disjoint chunks. Independent projection matrices $P_E$ and $P_F$ are applied to each chunk, forcing attention to operate on localized spatial regions rather than global low-rank mixtures.
- **Core assumption:** Sorting by $k_T$ effectively groups physically relevant neighbors together in the sequence index.
- **Evidence anchors:** [Abstract] and [Section 2] describe spatially aware partitioning; Table 1 shows $k_T$-sorted SAL-T outperforms $p_T$-sorted version.
- **Break condition:** If data is shuffled or sorted by $p_T$ instead of $k_T$, spatial partitioning fails and performance degrades to standard Linformer levels.

### Mechanism 2: Convolutional Attention Contextualization
- **Claim:** Depthwise 2D convolutions enhance local correlation capture in attention logits without reintroducing quadratic complexity.
- **Mechanism:** A depthwise convolutional filter slides over $k_T$-sorted attention logits, contextualizing scores based on immediate neighbors.
- **Core assumption:** Useful physical features manifest as contiguous patterns in the sorted sequence that can be enhanced by local filters.
- **Evidence anchors:** [Abstract] mentions convolutional layers for local correlations; ablation study shows performance drop without convolution.
- **Break condition:** Very small sequence lengths ($n < 20$) may cause convolutional kernels to smear out distinct particle features.

### Mechanism 3: Physics-Informed Sequential Inductive Bias
- **Claim:** $k_T$-based sorting acts as a strong prior that improves accuracy compared to standard $p_T$ sorting or random ordering.
- **Mechanism:** Unlike permutation-invariant transformers, SAL-T explicitly orders sequences so high-momentum, geometrically central particles appear first, injecting domain knowledge.
- **Core assumption:** $k_T$ distance metric correctly prioritizes most discriminative features for jet tagging (infrared and collinear safety).
- **Evidence anchors:** [Section 2.1] explains $k_T$ advantages; Table 1 shows 81.18% accuracy vs 78.82% for $p_T$-sorted.
- **Break condition:** Performance may drop significantly on generic point clouds without valid domain-specific sorting metrics.

## Foundational Learning

**Concept: Jet Kinematics & $k_T$ Distance**
- **Why needed here:** Core spatial awareness relies on understanding particles are sorted by $k_T = p_T \times \Delta R$, not just spatial coordinates.
- **Quick check question:** If two particles have same $p_T$ but different distances from jet axis, which appears earlier in $k_T$-descending sequence?

**Concept: Low-Rank Approximation (Linformer)**
- **Why needed here:** SAL-T modifies Linformer; understanding how projecting $n \times d$ to $n \times p$ matrices reduces complexity is essential.
- **Quick check question:** Why does reducing projection dimension $p$ risk losing global context, and how does SAL-T's partitioning try to fix this?

**Concept: Depthwise Separable Convolution**
- **Why needed here:** Paper applies 2D convs to attention maps "depthwise" to capture local correlations.
- **Quick check question:** Does convolution in SAL-T mix information across different attention heads or just spatially within a single head's logits?

## Architecture Onboarding

**Component map:**
Input Layer -> Preprocess (Sort by $k_T$ → Partition) -> LPP-MHA (Linear Projections → Partitioned Projection → Attention → Conv Block → Softmax) -> Head (Max-aggregation → Dense → Softmax)

**Critical path:** The Sorting & Partitioning step. If data loader doesn't implement $k_T$ sorting exactly, the "spatial partitioning" assumption breaks and model becomes a standard, weaker Linformer.

**Design tradeoffs:**
- **Partition Count ($p$):** Higher $p$ captures more granular locality but increases FLOPs ($O(np)$). Paper uses $p=4$.
- **Sorting Strategy:** $k_T$ is theoretically optimal for jets but requires sorting overhead. $p_T$ is cheaper but lower accuracy (2-3% drop).
- **Convolution Filters:** Adds slightly to latency but significantly boosts AUC; ablation study shows performance drop without it.

**Failure signatures:**
- **Accuracy Collapse on Unsorted Data:** Validation accuracy ~60-70% indicates data pipeline sorting issue.
- **OOM on Large Jets:** Ensure $p$ is fixed, not scaled with $n$, despite theoretical linear complexity.
- **Head 2 Saturation:** If Head 2 attends exclusively to Partition 0 on noise data, model may be overfitting to jet "hard core."

**First 3 experiments:**
1. **Baseline Sanity Check:** Run SAL-T vs. Linformer on $k_T$-sorted data. Verify SAL-T matches or exceeds Linformer AUC (Table 1). If not, check partition implementation.
2. **Sorting Ablation:** Retrain SAL-T on $p_T$-sorted data. Confirm ~2-3% accuracy drop to validate $k_T$ mechanism.
3. **Latency Profiling:** Measure inference time (µs) on target hardware. Verify stays under trigger threshold (~27µs on GPU per Table 2).

## Open Questions the Paper Calls Out

**Open Question 1:** Can SAL-T be effectively implemented on FPGAs to meet microsecond latency requirements of High-Luminosity LHC trigger?
- **Basis:** Authors state current benchmarks are GPU-based and note FPGA integration challenges.
- **Why unresolved:** Paper provides GPU latency but lacks synthesis results for FPGAs with different resource constraints.
- **What evidence would resolve it:** Successful hardware synthesis reports demonstrating microsecond latency and resource utilization within trigger FPGA bounds.

**Open Question 2:** Does combining SAL-T's spatial projections with Particle Transformer's pairwise feature attention bias yield superior performance-efficiency trade-offs?
- **Basis:** Authors plan to "integrate spatially aware partitioning and convolution with pairwise feature attention bias of particle transformer."
- **Why unresolved:** Unclear if low-rank approximations are compatible with computationally intensive pairwise features without negating efficiency gains.
- **What evidence would resolve it:** Comparative benchmarks of hybrid SAL-T/ParT architecture against standalone models.

**Open Question 3:** Can utilizing clustering sequence history (Lund Jet Plane) for input sorting outperform current $k_T$-based sorting?
- **Basis:** Authors intend to "incorporate sorting techniques based on clustering sequence history for more physics-informed partitioning."
- **Why unresolved:** Current $k_T$ sorting's optimality is uncertain; full jet fragmentation history may provide better inductive bias.
- **What evidence would resolve it:** Ablation studies comparing performance when particles are sorted by Lund Jet Plane vs $k_T$ metric.

## Limitations

- Performance advantage critically depends on $k_T$ sorting assumption; irregular jet structures or noisy angular coordinates may invalidate locality heuristic
- Claims about FPGA deployment readiness are based on GPU benchmarks rather than actual hardware implementation
- Paper lacks extensive ablation on sorting strategies beyond $k_T$ vs $p_T$, leaving optimality of specific choice uncertain

## Confidence

- **High confidence:** Linear attention complexity reduction and convolutional enhancement mechanisms are well-supported by ablation studies and efficiency literature
- **Medium confidence:** $k_T$-sorting performance gain (81.18% vs 78.82%) is demonstrated but lacks extensive ablation beyond basic comparison
- **Low confidence:** FPGA deployment claims lack actual hardware implementation evidence despite theoretical efficiency analysis

## Next Checks

1. **Sorting Ablation:** Retrain SAL-T on $p_T$-sorted data and verify ~2-3% accuracy drop to validate $k_T$ mechanism is responsible for performance gain
2. **Input Perturbation:** Evaluate SAL-T performance with Gaussian noise added to input particle coordinates (σ = 0.1 in Δη/Δφ) to test robustness of spatial partitioning assumption
3. **Partition Sensitivity:** Systematically vary partition count $p$ (try $p=2, 4, 8, 16$) and measure accuracy-latency tradeoff curve to identify optimal partitioning granularity for different particle multiplicities