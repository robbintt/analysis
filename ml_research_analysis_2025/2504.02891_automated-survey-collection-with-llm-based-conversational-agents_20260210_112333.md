---
ver: rpa2
title: Automated Survey Collection with LLM-based Conversational Agents
arxiv_id: '2504.02891'
source_url: https://arxiv.org/abs/2504.02891
tags:
- survey
- have
- days
- response
- agent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study introduces an LLM-based framework for conducting phone
  surveys in healthcare and biomedical research. The framework consists of a researcher,
  an AI phone agent, survey participants, an LLM (GPT-4o) for analyzing transcripts,
  and a database.
---

# Automated Survey Collection with LLM-based Conversational Agents

## Quick Facts
- **arXiv ID:** 2504.02891
- **Source URL:** https://arxiv.org/abs/2504.02891
- **Reference count:** 40
- **Primary result:** 98% extraction accuracy despite 7.7% WER in phone survey transcripts

## Executive Summary
This study introduces an LLM-based framework for conducting phone surveys in healthcare and biomedical research using a conversational AI agent. The framework combines a telephony interface (BlandAI) with GPT-4o for both conducting interviews and analyzing transcripts to extract structured survey responses. Tested with 8 participants completing 40 surveys based on an adapted COVID-19 impact survey, the system achieved 98% accuracy in deducing survey responses from conversation transcripts despite an average word error rate of 7.7%. Participants found the AI agent effective in conveying survey purpose and maintaining engaging interactions, though some noted occasional errors and a slightly robotic tone.

## Method Summary
The framework consists of a conversational AI phone agent (BlandAI) that conducts interviews using few-shot prompting with the full survey text and example dialogue, followed by GPT-4o analysis of transcripts using self-consistency prompting (5 repetitions per transcript with majority vote). Researchers manually prepared the survey instrument and instantiated agent instructions, while fictitious personas were generated by GPT-4o to protect privacy during testing. The system extracted structured JSON responses from raw transcripts and stored final results in REDCap. The entire process achieved an average cost of $0.75 per survey.

## Key Results
- Survey responses extracted with 98% accuracy from conversation transcripts
- Average word error rate of 7.7% in transcripts did not correlate with extraction accuracy
- Participants found AI agent effective in conveying survey purpose and demonstrating good comprehension
- System cost averaged $0.75 per completed survey

## Why This Works (Mechanism)

### Mechanism 1: LLM Semantic Correction
Large Language Models (GPT-4o) appear capable of correcting implicit transcription errors during semantic extraction, decoupling Word Error Rate (WER) from final survey accuracy. The system relies on a reasoning LLM to interpret the intent of a speaker's response from a transcript, rather than exact string matching. By using context from the survey question, the LLM infers correct answers even when transcripts contain errors like "mail" instead of "male." This mechanism likely breaks if transcription errors invert semantic meaning rather than just swapping nouns.

### Mechanism 2: Self-Consistency Stabilization
Repeated stochastic sampling (Self-Consistency) stabilizes extraction accuracy for structured data. The framework prompts the extraction LLM five times per transcript and selects the most common answer (mode), acting as a statistical filter to remove random hallucinations. Correct interpretations are statistically more likely to be generated consistently than incorrect ones across different temperature-based generations. This mechanism fails if transcripts are consistently ambiguous or if the LLM has persistent hallucinations.

### Mechanism 3: Prompt-Based Conversation Management
Conversational flow is maintained by injecting static survey structure into the LLM's context window via detailed prompts. The conversational agent is not fine-tuned but prompted with full survey text and example dialogue (few-shot prompting), grounding the agent to ensure it asks specific questions in order while maintaining a persona ("Sarah"). The LLM's instruction-following capabilities are robust enough to adhere to a 33-question protocol without deviating. Failure may occur in very long surveys where the context window is stressed.

## Foundational Learning

- **Concept: Word Error Rate (WER) vs. Semantic Accuracy**
  - Why needed here: Traditional speech-to-text systems require low WER because they use keyword matching. This paper demonstrates that with LLMs, high WER (7.7%) can still yield high semantic accuracy (98%) if the reasoning model is robust.
  - Quick check question: If a transcript reads "I said no male" when the speaker said "I said no mail," how would a rule-based system vs. an LLM-based system interpret the gender question?

- **Concept: Self-Consistency Prompting**
  - Why needed here: LLMs are stochastic. Relying on a single output for critical data extraction (like healthcare surveys) is risky. Aggregating multiple outputs improves reliability.
  - Quick check question: If an LLM extracts "Headache" 3 times and "Fever" 2 times from the same transcript, what is the final recorded answer using a self-consistency approach?

- **Concept: Persona-based Simulation**
  - Why needed here: The study validates its system using fictitious personas rather than real personal data. This protects privacy but introduces the challenge of participants acting/reading rather than naturally speaking.
  - Quick check question: Why might testing with fictitious personas result in "less natural" interactions, and how does this threaten the external validity of the usability study?

## Architecture Onboarding

- **Component map:** BlandAI (Telephony + Speech-to-Text + Agent) -> GPT-4o (Transcript Analysis) -> REDCap (Storage)
- **Critical path:** Prompt Engineering (Survey -> BlandAI Prompt) -> Audio Quality (Call -> Transcript) -> Inference (Transcript -> GPT-4o -> JSON) -> Validation (JSON -> REDCap)
- **Design tradeoffs:**
  - Speed vs. Accuracy: 5 inference passes increase latency and cost but stabilize accuracy
  - Scripted vs. Generative: Strict question list ensures completeness but sacrifices fluidity
- **Failure signatures:**
  - Turn-taking issues: Agent cutting off participants or speaking over them
  - Homophone confusion: "Male" vs. "Mail" or "Female" errors in transcripts
  - Robotic Tone: Participants noting voice pitch changes or lack of empathy
- **First 3 experiments:**
  1. **WER Tolerance Test:** Manually inject errors into perfect transcript to find reasoning mechanism breaking point
  2. **Persona Validation:** Have 5 humans read same generated persona to measure survey instrument consistency
  3. **Single-pass vs. Self-Consistency:** Compare accuracy and cost of 1 pass vs. 5 passes

## Open Questions the Paper Calls Out

- Can the framework effectively scale to multilingual populations without significant loss in transcription accuracy or response deduction?
- Does the use of real participant data alter the accuracy of response deduction or the nature of participant-agent interaction?
- Can standardized survey instruments (such as Phen-X) be automatically adapted into conversational prompts while maintaining data integrity?
- To what extent does the "awareness" of the AI agent bias participant responses compared to a human interviewer?

## Limitations

- Small sample size of 8 participants limits statistical power and generalizability across diverse populations
- Use of fictitious personas may not capture full complexity of natural human responses, particularly emotional nuance
- BlandAI platform specifics (model version, voice parameters, turn-taking configuration) remain underspecified
- Only tested one survey instrument (COVID-19 impact survey), leaving performance across different survey types unknown

## Confidence

- **High Confidence (4/5):** Framework architecture and workflow are well-documented; 98% extraction accuracy is supported by systematic self-consistency prompting
- **Medium Confidence (3/5):** Participant experience findings based on qualitative feedback from small, non-representative sample
- **Low Confidence (2/5):** Cost estimates ($0.75/survey) assume stable pricing models and do not account for potential increases with more complex surveys

## Next Checks

1. **WER Tolerance Validation:** Systematically inject controlled transcription errors to empirically determine maximum WER threshold at which extraction accuracy degrades below 90%
2. **Real-World Pilot Study:** Deploy system with actual human participants (not personas) for 100+ participant survey across different demographic groups
3. **Cross-Survey Generalization Test:** Test framework with three different survey types (clinical, demographic, opinion-based) to determine if 98% accuracy holds across varied question formats