---
ver: rpa2
title: 'DigiData: Training and Evaluating General-Purpose Mobile Control Agents'
arxiv_id: '2511.07413'
source_url: https://arxiv.org/abs/2511.07413
tags:
- goal
- action
- agent
- step
- digidata
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces DigiData, a large-scale dataset for training
  mobile control agents that enables deep exploration of app functionalities through
  comprehensive goal curation. Unlike existing datasets, DigiData uses a multi-phase
  data collection protocol involving exhaustive app feature exploration, human demonstrations,
  and LLM-augmented verification to ensure high-quality, diverse goals averaging 9.2
  steps per trajectory.
---

# DigiData: Training and Evaluating General-Purpose Mobile Control Agents

## Quick Facts
- arXiv ID: 2511.07413
- Source URL: https://arxiv.org/abs/2511.07413
- Reference count: 40
- Primary result: 8B model achieves 47.3% task success rate on DigiData-Bench

## Executive Summary
This paper introduces DigiData, a large-scale dataset for training mobile control agents that enables deep exploration of app functionalities through comprehensive goal curation. Unlike existing datasets, DigiData uses a multi-phase data collection protocol involving exhaustive app feature exploration, human demonstrations, and LLM-augmented verification to ensure high-quality, diverse goals averaging 9.2 steps per trajectory. The authors also present DigiData-Bench, a benchmark with human- and AI-assisted evaluation protocols to measure agent performance. Experimental results show that training on DigiData significantly improves agent performance over baselines, with an 8B model achieving 47.3% task success rate on DigiData-Bench. The study also demonstrates that step-accuracy metrics poorly correlate with real-world success and validates LLM judges as effective automated evaluators.

## Method Summary
The method involves comprehensive app feature exploration by trained annotators, human demonstrations via a custom Android app, and trajectory verification using a two-stage LLM judge. The dataset includes multi-modal observations (screenshot, UI tree, chain-of-thought) and achieves high-quality goals through systematic verification. Agents are trained using supervised fine-tuning on Perception Language Model (PLM) checkpoints with a mixture of DigiData and public datasets. The training procedure uses full-model fine-tuning with specified learning rates and data sampling probabilities, and evaluation is conducted through dynamic assessment on DigiData-Bench using both human and LLM judges.

## Key Results
- 8B model trained on DigiData achieves 47.3% success rate on DigiData-Bench
- Step-accuracy metrics show only 0.72 correlation with actual task success
- DigiData-Bench-Auto LLM judge achieves 0.89 Kendall rank correlation with human judgments
- Performance on novel apps does not improve with data scaling, indicating SFT limitations

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Comprehensive app feature exploration during goal curation produces more diverse, complex training goals that improve agent generalization.
- **Mechanism:** Trained annotators systematically navigate all app screens and menus to enumerate goals tied to specific features, including multi-step goal sequences. This yields 318 goals per app (vs. 85 in AitW) with average trajectory length of 9.2 steps—approximately 50% longer than existing datasets. Goal diversity is quantified via pairwise cosine distance, achieving a diversity score of 0.45 vs. 0.35 for AitW.
- **Core assumption:** Deeper coverage of app functionality translates to agents that can execute more complex, real-world tasks rather than superficial interactions.
- **Evidence anchors:** [abstract] "DigiData is meticulously constructed through comprehensive exploration of app features, resulting in greater diversity and higher goal complexity." [section 3.1, Goal Curation] "instruct a small group of trained annotators to exhaustively explore an application's features by navigating the application's screens and menus"
- **Break condition:** If goals become so app-specific that they don't transfer to unseen apps, generalization suffers (observed: novel apps show no significant improvement with data scaling—Figure 5 right).

### Mechanism 2
- **Claim:** Multi-modal observations (screenshot + UI tree + chain-of-thought) combined with trajectory-level verification improve both data quality and agent performance.
- **Mechanism:** Each timestep includes three modalities: (1) raw screenshot, (2) UI tree from Android accessibility layer, (3) LLM-generated chain-of-thought (observation, action rationale, expected UI change). A two-stage LLM judge filters trajectories: first summarizing each step, then evaluating goal achievement. 5.3% of raw trajectories are rejected; post-verification quality reaches 100% vs. 84% for AitW (verified by human annotators on 20k sample).
- **Core assumption:** UI trees provide grounding that raw pixels lack; chain-of-thought improves both explainability and action prediction accuracy.
- **Evidence anchors:** [section 3.2, Modalities] "DigiData is the first dataset of its scale to feature multiple input modalities" [Table 2] 8B COT model achieves 47.3% success rate vs. 42.1% without COT—a ~12% relative improvement
- **Break condition:** If UI trees are incomplete or accessibility services fail, grounding degrades; COT quality depends on the generating LLM's capabilities.

### Mechanism 3
- **Claim:** LLM judges provide a scalable proxy for human evaluation with strong rank correlation, but step-accuracy metrics do not reliably predict task success.
- **Mechanism:** The LLM judge operates in two stages: step summarization (transcribing screenshot + UI tree + action into text) followed by goal-achievement classification. Fine-tuned Llama 4 achieves 87% accuracy and 0.89 Kendall rank correlation with human judgments; GPT-4o reaches 0.94. Step accuracy shows only 0.72 correlation—models with near-identical step accuracy (3B: 70.7%, 8B: 70.7%) differ significantly in success rate (3B: 44.3%, 8B: 42.1%).
- **Core assumption:** Binary success/failure at trajectory level captures real-world utility better than per-step action matching, which penalizes valid alternative paths.
- **Evidence anchors:** [abstract] "step-accuracy metrics poorly correlate with real-world success and validates LLM judges as effective automated evaluators" [section 5.2, Online Evaluations Matter] "step accuracy is not necessarily a good predictor of the ordering of agent models"
- **Break condition:** LLM judges may have false accept rates; DigiData tunes for low false accepts but risks false negatives (mitigated by human review of rejected trajectories).

## Foundational Learning

- **Concept: Android Accessibility Tree / UI Hierarchy**
  - **Why needed here:** DigiData's multi-modal input relies on parsing the Android accessibility tree to provide structured UI element data alongside screenshots. Without understanding how to extract and encode UI trees, you cannot replicate the data pipeline or leverage this modality.
  - **Quick check question:** Can you explain why the accessibility tree might be incomplete, and how the paper handles missing UI elements in the LLM judge prompts?

- **Concept: Supervised Fine-Tuning (SFT) for Vision-Language Models**
  - **Why needed here:** The paper fine-tunes Perception Language Model (PLM) checkpoints using a mixture of DigiData, AitW, AndroidControl, and Cauldron. Understanding SFT dynamics—including sampling probabilities, epoch counts, and catastrophic forgetting mitigation—is essential for reproducing results.
  - **Quick check question:** Why does the training mixture include Cauldron (a general VQA dataset), and what would likely happen if it were excluded?

- **Concept: Dynamic vs. Offline Evaluation in Agent Benchmarks**
  - **Why needed here:** The paper argues that offline metrics (step accuracy) fail to capture task success because multiple valid action sequences can achieve the same goal. Dynamic evaluation—running agents on live or emulated devices with state initialization—is necessary for realistic assessment.
  - **Quick check question:** What are the three termination conditions for a trajectory in DigiData-Bench's human-assisted evaluation, and why is "state initialization" critical for reproducibility?

## Architecture Onboarding

- **Component map:**
  Data Collection Pipeline: Goal Curation → Human Demonstrations → Trajectory Verification
  Agent Architecture: Vision-Language Model with multi-modal inputs and action outputs
  Evaluation Stack: DigiData-Bench with human-assisted and AI-assisted protocols

- **Critical path:** Goal curation quality → demonstration complexity → verification precision → agent SFT on verified trajectories → dynamic evaluation. If any stage degrades (e.g., incomplete goal coverage, noisy demonstrations, false accepts in verification), downstream performance suffers.

- **Design tradeoffs:**
  - **Goal depth vs. generalization:** Deep app-specific goals (318 per app) improve performance on seen/familiar apps but show no gain on novel apps (Figure 5). Narrower, more transferable goals might improve cross-category generalization but reduce within-app capability.
  - **LLM judge precision vs. recall:** Tuned for low false accept rate to ensure data quality, but this risks rejecting valid trajectories. Paper mitigates with human review of LLM-rejected cases.
  - **Multi-modal vs. screenshot-only:** UI trees and COT improve performance (+12% relative with COT) but add annotation cost and inference complexity.

- **Failure signatures:**
  - **Novel app generalization failure:** Performance plateaus on novel apps despite data scaling (Figure 5, right), suggesting SFT alone insufficient for out-of-distribution generalization.
  - **Step-accuracy/success-rate divergence:** Models with matching step accuracy show different success rates, indicating overfitting to single-step prediction rather than trajectory-level planning.
  - **Looping/overshooting behaviors:** Evaluation runbook explicitly flags "looping" (same action 3+ times) and "overshooting" (continuing past goal completion) as failure modes.

- **First 3 experiments:**
  1. **Baseline replication:** Train PLM 1B/3B/8B on public data only (AitW + AndroidControl + Cauldron, no DigiData). Measure step accuracy on DigiData-Bench and compare to Table 2 baselines to confirm implementation correctness.
  2. **Ablation on modalities:** Train three 3B variants—(a) screenshot-only, (b) screenshot + UI tree, (c) screenshot + UI tree + COT—to quantify per-modality contribution to success rate.
  3. **LLM judge validation:** On held-out trajectories, compare zero-shot Llama 4, fine-tuned Llama 4, and GPT-4o judges against human labels. Report accuracy, precision, recall, and Kendall correlation to confirm Table 3 reproducibility.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can reinforcement learning (RL) methods overcome the plateau in performance observed in supervised fine-tuning (SFT) when agents encounter novel app categories?
- **Basis:** [explicit] Section 5.2 notes that performance did not improve for "novel" apps, pointing to "inherent limitations of approaches based on supervised fine-tuning" and suggesting "future work to focus on training agents with reinforcement learning."
- **Why unresolved:** The authors demonstrate the limitation of SFT regarding novel app generalization but do not experimentally validate if RL resolves this specific failure mode.
- **What evidence would resolve it:** Comparative experiments showing RL-based agents achieving significantly higher success rates on the "novel" subset of DigiData-Bench compared to SFT baselines.

### Open Question 2
- **Question:** How effectively do transfer learning techniques improve generalization to "novel" apps compared to the current scaling behavior of supervised fine-tuning?
- **Basis:** [explicit] Section 10 ("Limitations and Future Work") explicitly identifies "exploring transfer learning capabilities to novel apps" as a necessary direction for future research.
- **Why unresolved:** The current data scaling experiments (Figure 5, right) show that simply increasing SFT data volume fails to lift performance on unseen app categories.
- **What evidence would resolve it:** Experiments applying specific transfer learning algorithms to the DigiData training set, demonstrating a measurable increase in success rates for the "novel" app group.

### Open Question 3
- **Question:** Can the DigiData training methodology and DigiData-Bench evaluation protocols be effectively adapted to train and assess agents for computer-based interfaces?
- **Basis:** [explicit] Section 10 states that the "generalizability to more general mobile control such as computer-based interfaces remains unexplored" and lists adapting methods to such interfaces as future work.
- **Why unresolved:** The study is strictly confined to Android mobile control, and it is unknown if the goal curation or LLM-judge protocols translate to the complexity of desktop environments.
- **What evidence would resolve it:** A study applying the DigiData pipeline (goal curation, trajectory verification) to desktop environments, resulting in a dataset with comparable diversity and quality metrics.

## Limitations
- **Novel app generalization plateau:** Performance on novel apps does not improve with data scaling, indicating fundamental limits of SFT for cross-app generalization.
- **LLM judge reliability:** While correlation with human judgments is strong, the paper tunes for low false accept rates at the cost of potential false negatives, with exact false negative rates unreported.
- **Training data access:** PLM checkpoint licensing and dataset access requirements may create reproducibility barriers.

## Confidence

- **High confidence:** Comprehensive app feature exploration produces more diverse, complex training goals (supported by clear quantitative comparisons to AitW: 318 vs 85 goals per app, diversity score 0.45 vs 0.35).
- **Medium confidence:** Multi-modal observations improve agent performance (supported by ablation showing 12% relative improvement with COT, but could benefit from more granular per-modality analysis).
- **Medium confidence:** Step-accuracy metrics poorly predict real-world success (supported by correlation analysis showing 0.72 correlation, but could benefit from additional real-world deployment studies).

## Next Checks

1. **Novel app generalization experiment:** Train the 8B model on DigiData plus public data, then evaluate on a held-out set of completely unseen apps (not in DigiData-Bench). Compare performance scaling with training data volume to determine if SFT alone can achieve cross-app generalization.

2. **LLM judge false negative analysis:** Manually review 100 LLM-rejected trajectories to determine false negative rate. Compare performance of agents trained on all data vs. LLM-filtered data to quantify quality improvement vs. quantity loss tradeoff.

3. **Cross-dataset transfer experiment:** Train models on DigiData only, AitW only, and combinations thereof. Evaluate on both datasets to measure domain transfer capability and identify whether DigiData's approach generalizes beyond its specific app ecosystem.