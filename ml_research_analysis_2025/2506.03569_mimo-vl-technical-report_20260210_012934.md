---
ver: rpa2
title: MiMo-VL Technical Report
arxiv_id: '2506.03569'
source_url: https://arxiv.org/abs/2506.03569
tags:
- reasoning
- data
- arxiv
- zhang
- wang
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MiMo-VL-7B-RL, a vision-language model, outperforms Qwen2.5-VL-7B
  on 35 out of 40 evaluated tasks and achieves state-of-the-art performance among
  open-source VLMs of comparable scale. The model delivers strong general visual understanding,
  scoring 66.7 on MMMU, and demonstrates exceptional multimodal reasoning with a 59.4
  on OlympiadBench, surpassing models with up to 78B parameters.
---

# MiMo-VL Technical Report

## Quick Facts
- arXiv ID: 2506.03569
- Source URL: https://arxiv.org/abs/2506.03569
- Reference count: 40
- Primary result: MiMo-VL-7B-RL outperforms Qwen2.5-VL-7B on 35/40 tasks and achieves SoTA among open-source VLMs of comparable scale

## Executive Summary
MiMo-VL-7B-RL is a vision-language model that delivers state-of-the-art performance among open-source VLMs of comparable scale. It achieves strong general visual understanding (66.7 on MMMU), exceptional multimodal reasoning (59.4 on OlympiadBench), and sets new standards for GUI grounding (56.1 on OSWorld-G). The model's success stems from a four-stage pre-training process incorporating high-quality synthetic reasoning data with long Chain-of-Thought, combined with a Mixed On-policy Reinforcement Learning (MORL) framework that integrates diverse reward signals across reasoning, perception, grounding, and human preference alignment.

## Method Summary
MiMo-VL uses a Qwen2.5-ViT encoder for native-resolution vision encoding paired with an MLP projector and MiMo-7B-Base LLM. The four-stage pre-training progresses from projector warmup (Stage 1) to multimodal pre-training with long-context synthetic reasoning data (Stage 4). Post-training employs Mixed On-policy Reinforcement Learning (MORL) with a unified Reward-as-a-Service framework routing queries to rule-based or learned reward models. The architecture maintains 32K context length in final stage, uses on-policy GRPO for RL stability, and incorporates diverse data modalities including image captions, interleaved data, OCR, video, and GUI data.

## Key Results
- Achieves 66.7 on MMMU, outperforming Qwen2.5-VL-7B on 35 out of 40 evaluated tasks
- Delivers 59.4 on OlympiadBench, surpassing models with up to 78B parameters
- Sets new standard for GUI grounding with 56.1 on OSWorld-G, outperforming specialized models like UI-TARS

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Incorporating high-quality synthetic reasoning data with long Chain-of-Thought into pre-training stages enables continuous reasoning improvement without saturation.
- **Mechanism:** Long CoT data provides richer supervision signals for complex logical relationships and generalizable reasoning patterns. When integrated into Stage 4 pre-training, response token length grows from 680 to 2.5K per question, indicating deeper reasoning rather than superficial pattern matching.
- **Core assumption:** Quality of synthetic reasoning data depends on rigorous filtering of both answer correctness and reasoning process clarity.
- **Evidence anchors:** [abstract] "incorporating high-quality reasoning data with long Chain-of-Thought into pre-training stages"; [section 5.1] "model performance increases sharply, e.g., +9 on MMMU, +14 on OSWorld-G, and +16 on OlympiadBench...continuously improves without saturation"; [corpus] Related work "Long Grounded Thoughts" (FMR 0.63) explores similar compositional visual reasoning chain distillation.
- **Break condition:** If synthetic CoT data contains reasoning errors or redundant logic, the model may amplify flawed patterns rather than correct reasoning.

### Mechanism 2
- **Claim:** Fully on-policy RL with GRPO achieves superior scaling behavior compared to vanilla GRPO with clipped surrogate objectives.
- **Mechanism:** On-policy variant performs single-step policy updates following response rollout, eliminating the need for clipped surrogate training objectives. This maintains training stability while enabling continued improvement beyond 20K samples.
- **Core assumption:** Removal of KL loss and use of dynamic sampling, easy data filtering, and re-sampling strategies are necessary complementary techniques.
- **Evidence anchors:** [abstract] "benefits of mixed RL despite challenges in simultaneous multi-domain optimization"; [section 5.2] "vanilla GRPO algorithm's performance generally saturates around 20,000 training samples, beyond which further training yields negligible improvements"; [corpus] Limited direct comparison in corpus.
- **Break condition:** Without proper exploration strategies (dynamic sampling, re-sampling), on-policy RL may converge to suboptimal policies.

### Mechanism 3
- **Claim:** Mixed On-policy Reinforcement Learning (MORL) integrates diverse reward signals but faces interference between task domains due to opposing response length trends.
- **Mechanism:** A unified Reward-as-a-Service (RaaS) framework routes queries to appropriate reward functions (rule-based for verifiable tasks, model-based for human preference). However, reasoning tasks encourage longer CoT while grounding/counting tasks favor shorter outputs.
- **Core assumption:** Task difficulty disparities and reward hacking risks contribute to interference; normalization of all rewards to [0,1] range is sufficient for multi-domain optimization.
- **Evidence anchors:** [abstract] "integrating diverse reward signals across reasoning, perception, grounding, and human preference alignment"; [section 5.3] "reasoning tasks exhibit disparities with visual perception and grounding tasks...reasoning tasks encourage longer CoT during the RL process, whereas grounding and counting tasks lead to shrinking ones"; [corpus] Weak corpus evidence—no directly comparable MORL implementations found.
- **Break condition:** If task interference exceeds a threshold, simultaneous improvements across all domains may become impossible without task-specific training schedules or architectural separation.

## Foundational Learning

- **Concept: Native-resolution Vision Encoding**
  - Why needed here: MiMo-VL uses Qwen2.5-ViT to preserve fine-grained visual details without fixed-resolution resizing, critical for OCR, GUI grounding, and high-resolution document understanding.
  - Quick check question: Can you explain why fixed-resolution encoding might lose critical detail for GUI element localization tasks?

- **Concept: Chain-of-Thought (CoT) Reasoning**
  - Why needed here: The model uses `<tool_call>...</tool_call>` tokens to generate explicit reasoning steps before responses, enabling complex STEM problem-solving with 2.5K average response length.
  - Quick check question: How does CoT differ from standard prompt-response patterns, and what evaluation metrics would indicate successful CoT adoption?

- **Concept: Group Relative Policy Optimization (GRPO)**
  - Why needed here: MORL uses GRPO as its base RL algorithm, computing advantages by normalizing rewards within groups of sampled responses.
  - Quick check question: Given equation (2) for advantage computation, why is group-level normalization used instead of global normalization?

## Architecture Onboarding

- **Component map:** Input → Qwen2.5-ViT (native resolution) → Visual Tokens → MLP Projector → Text Tokens → MiMo-7B-Base → `<tool_call>... reasoning...<tool_call>` Response

- **Critical path:** Stage 1 (projector warmup only) → Stage 2 (ViT+projector, interleaved data) → Stage 3 (all parameters, 1.4T tokens diverse data) → Stage 4 (all parameters, 550B tokens long-context + reasoning data) → MORL post-training

- **Design tradeoffs:**
  - 32K context length in Stage 4 vs. 8K in earlier stages (increases effective batch size, requires LR adjustment to 2.5e-5)
  - On-policy RL stability vs. vanilla GRPO sample efficiency (on-policy scales better long-term)
  - Unified RaaS vs. task-specific training pipelines (unified enables MORL but risks interference)

- **Failure signatures:**
  - Early saturation in RL training (suggests vanilla GRPO instead of on-policy variant)
  - Declining performance on perception tasks during MORL (task interference from reasoning-heavy optimization)
  - Response length collapse (suggests reward hacking on grounding/counting tasks)

- **First 3 experiments:**
  1. **Ablate synthetic reasoning data timing:** Compare injecting long-CoT data at Stage 3 vs. Stage 4 to validate the claim that later-stage incorporation yields better results.
  2. **On-policy vs. vanilla GRPO on single domain:** Replicate Figure 7 on a visual reasoning task (not just text) to test generalization of the scaling claim.
  3. **Task interference measurement:** Train MORL with and without reasoning tasks, measure delta on grounding benchmarks to quantify interference magnitude.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can optimization interference between reasoning and grounding tasks be resolved in Mixed On-policy Reinforcement Learning (MORL)?
- **Basis in paper:** [explicit] Section 5.3 identifies "interference across data domains" caused by opposing response length trends: reasoning tasks encourage longer Chain-of-Thought, while grounding/counting tasks lead to shrinking responses.
- **Why unresolved:** The authors note that achieving "stable, simultaneous improvements across all capabilities remains a significant challenge" and they are "actively investigating" the causes.
- **What evidence would resolve it:** A training curriculum or reward normalization technique that enables simultaneous state-of-the-art performance on both long-context reasoning (e.g., OlympiadBench) and precise grounding (e.g., OSWorld-G) without trade-offs.

### Open Question 2
- **Question:** At what scale does the performance benefit of incorporating synthetic long Chain-of-Thought (CoT) data into pre-training begin to saturate?
- **Basis in paper:** [explicit] Section 5.1 states that during Stage 4 pre-training with synthetic reasoning data, "model performance continuously improves without saturation."
- **Why unresolved:** The report concludes training at a fixed compute/token budget without identifying the upper limits of this data scaling strategy.
- **What evidence would resolve it:** Scaling laws derived from training runs extending significantly beyond the current 550B tokens in Stage 4, specifically tracking when validation loss on reasoning benchmarks plateaus.

### Open Question 3
- **Question:** What specific modifications are required to prevent the early training plateau observed in vanilla GRPO compared to the fully on-policy variant?
- **Basis in paper:** [explicit] Section 5.2 and Figure 7 show that while on-policy RL scales continuously, vanilla GRPO "performance generally saturates around 20,000 training samples."
- **Why unresolved:** The paper documents the divergent scaling behaviors but does not isolate the mechanism causing GRPO's early plateau to propose a fix.
- **What evidence would resolve it:** An ablation study analyzing gradient variance or exploration entropy in vanilla GRPO, followed by a modified objective function that matches the on-policy variant's scaling curve.

## Limitations

- The MORL framework lacks comprehensive analysis of task interference mechanisms beyond observation of response length trends.
- The superiority of on-policy GRPO scaling is demonstrated primarily on text tasks, with limited validation on visual reasoning benchmarks.
- Data curation pipelines for synthetic reasoning generation and specific filtering criteria for high-quality CoT remain underspecified.

## Confidence

- **High confidence:** The four-stage pre-training pipeline with progressive architecture unfreezing, effectiveness of native-resolution vision encoding via Qwen2.5-ViT, and strong benchmark results (MMMU 66.7, OlympiadBench 59.4, OSWorld-G 56.1).
- **Medium confidence:** The mixed RL framework's benefits and task interference patterns are observed but not deeply analyzed; on-policy GRPO superiority requires broader task validation.
- **Low confidence:** Exact mechanisms preventing reward hacking in verifiable tasks and long-term stability of unified RaaS framework under expanded reward functions.

## Next Checks

1. Conduct ablation studies on synthetic reasoning data timing by comparing Stage 3 vs. Stage 4 incorporation to isolate the claimed benefit of later-stage injection.
2. Validate on-policy GRPO scaling claims on visual reasoning tasks beyond text benchmarks to assess generalization of superior long-term performance.
3. Quantify task interference magnitude by training MORL with and without reasoning tasks, measuring the delta on grounding benchmarks to determine if interference exceeds a critical threshold.