---
ver: rpa2
title: Scheduled Interleaved Speech-Text Training for Speech-to-Speech Translation
  with LLMs
arxiv_id: '2506.10299'
source_url: https://arxiv.org/abs/2506.10299
tags:
- speech
- text
- units
- training
- translation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of adapting large language models
  (LLMs) to speech modality for speech-to-speech translation (S2ST). LLMs are pre-trained
  on text-only data, which presents difficulties in adapting them to speech with limited
  speech-to-speech data due to modality gaps in length and representation.
---

# Scheduled Interleaved Speech-Text Training for Speech-to-Speech Translation with LLMs

## Quick Facts
- arXiv ID: 2506.10299
- Source URL: https://arxiv.org/abs/2506.10299
- Reference count: 0
- Key outcome: Scheduled interleaved speech-text training improves ASR-BLEU scores for S2ST, particularly for low-resource languages (e.g., Italian-English: 12.8→19.5)

## Executive Summary
This paper addresses the challenge of adapting large language models (LLMs) to speech modality for speech-to-speech translation (S2ST). Since LLMs are pre-trained on text-only data, there's a significant modality gap when applying them to speech tasks with limited data. The proposed method, scheduled interleaved speech-text training, progressively adapts the LLM from text to speech by interleaving aligned speech and text units during fine-tuning, gradually reducing the proportion of text units as training progresses. Experiments on the CVSS dataset with LLaMA3.2-1B fine-tuning demonstrate consistent improvements in translation performance compared to baseline approaches.

## Method Summary
The proposed method uses scheduled interleaved speech-text training where aligned text tokens are interleaved at the word level with speech units during LLM fine-tuning. The ratio of text units (p) starts at 0.9 and gradually decreases by 0.1 every 300 training steps, facilitating progressive modality adaptation. Aligned text tokens are inserted at word boundaries using CTC segmentation, with speech spans replaced by BPE tokens. The approach employs chain-of-thought prompting (predict transcript → translation → speech units) and was evaluated on the CVSS dataset with LLaMA3.2-1B fine-tuning, showing significant improvements in ASR-BLEU scores, particularly for languages with limited training data.

## Key Results
- ASR-BLEU improvements: Italian-English improved from 12.8 to 19.5
- The method is particularly effective for languages with limited training data
- Scheduled interleaving outperforms constant interleaving (Pt-en: 19.5→12.0 without scheduling)
- Both input and output interleaving are necessary for optimal performance

## Why This Works (Mechanism)
The method bridges the modality gap between speech and text representations by progressively adapting the LLM through scheduled interleaving. Starting with mostly text tokens and gradually shifting to speech units allows the model to maintain its text-based capabilities while learning speech-specific representations. The word-level alignment ensures semantic consistency between modalities, while the scheduling mechanism prevents catastrophic forgetting of text representations during speech adaptation.

## Foundational Learning

**Discrete speech units**: Speech representations clustered into discrete tokens (2048 clusters) for LLM input. Why needed: LLMs require discrete token inputs. Quick check: Verify clustering preserves semantic information by comparing unit sequences across similar speech segments.

**CTC segmentation**: Algorithm for word-level alignment between speech units and text. Why needed: Ensures semantic consistency during interleaving. Quick check: Validate alignment accuracy by comparing segmented word boundaries against ground truth transcriptions.

**Chain-of-thought prompting**: Multi-stage generation process (transcript → translation → speech). Why needed: Provides structured reasoning path for S2ST task. Quick check: Test with and without CoT to measure impact on translation quality.

**Scheduled sampling**: Gradually decreasing text-to-speech ratio during training. Why needed: Prevents modality mismatch and catastrophic forgetting. Quick check: Monitor perplexity on text-only held-out set to ensure text capabilities are preserved.

## Architecture Onboarding

**Component map**: w2v-BERT (ASR) -> k-means clustering -> discrete units -> LLM (fine-tuned) -> HiFi-GAN vocoder -> synthesized speech

**Critical path**: Speech input → w2v-BERT encoder → discrete unit extraction → LLM fine-tuning with scheduled interleaving → HiFi-GAN synthesis → output speech

**Design tradeoffs**: The method trades increased training complexity (dual modality handling) for improved cross-modal adaptation. The scheduling mechanism adds hyperparameters (decay rate, interval) but provides better modality transition than abrupt switching.

**Failure signatures**: Without scheduling, ASR-BLEU drops significantly; without proper alignment, performance degrades; interleaving only input or output causes major degradation.

**3 first experiments**:
1. Fine-tune w2v-BERT on CoVoST2+CVSS with CTC objective, train k-means (k=2048) on 20th layer features
2. Implement Algorithm 1 for interleaving with Poisson span length sampling and CTC segmentation
3. Fine-tune LLaMA3.2-1B with extended vocabulary, starting p=0.9, decay by 0.1 every 300 steps using CoT prompting

## Open Questions the Paper Calls Out
- Can scheduled interleaved training be effectively extended to spoken dialog systems?
- Is the fixed linear decay schedule for text ratio optimal, or can it be improved?
- Does the method scale effectively to significantly larger LLMs (e.g., 7B+ parameters)?

## Limitations
- The method's effectiveness on non-synthetic speech data remains untested
- Several key hyperparameters (Poisson λ, batch size, training steps) are unspecified
- The approach hasn't been validated on larger LLM variants beyond LLaMA3.2-1B

## Confidence
- **High confidence**: Core experimental results showing ASR-BLEU improvements are well-supported
- **Medium confidence**: Claims of superiority over simpler interleaving strategies; could be influenced by implementation details
- **Low confidence**: Generalizability beyond CVSS dataset and LLaMA3.2-1B model to different model scales and translation directions

## Next Checks
1. Implement scheduled interleaving with different Poisson λ values (λ=1, λ=3, λ=5) to assess sensitivity to span length distribution
2. Compare scheduled approach against constant interleaving ratios (p=0.9 throughout) with identical total training steps
3. Evaluate the method on a non-synthetic speech corpus (e.g., Fisher Spanish-English) to test robustness to real-world speech characteristics