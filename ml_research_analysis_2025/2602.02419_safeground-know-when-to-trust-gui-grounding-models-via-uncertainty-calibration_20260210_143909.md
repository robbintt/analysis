---
ver: rpa2
title: 'SafeGround: Know When to Trust GUI Grounding Models via Uncertainty Calibration'
arxiv_id: '2602.02419'
source_url: https://arxiv.org/abs/2602.02419
tags:
- uncertainty
- grounding
- risk
- calibration
- safeground
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "SafeGround introduces a principled uncertainty-aware framework\
  \ for GUI grounding that enables risk-controlled predictions via selective calibration.\
  \ It quantifies spatial uncertainty from stochastic grounding samples using three\
  \ complementary measures\u2014top-candidate ambiguity, informational dispersion,\
  \ and concentration deficit\u2014then calibrates a decision threshold to control\
  \ the false discovery rate (FDR) at a user-specified level."
---

# SafeGround: Know When to Trust GUI Grounding Models via Uncertainty Calibration

## Quick Facts
- arXiv ID: 2602.02419
- Source URL: https://arxiv.org/abs/2602.02419
- Authors: Qingni Wang; Yue Fan; Xin Eric Wang
- Reference count: 40
- Primary result: Uncertainty-aware framework with FDR control improves GUI grounding accuracy by up to 5.38 percentage points

## Executive Summary
SafeGround introduces a principled uncertainty-aware framework for GUI grounding that enables risk-controlled predictions via selective calibration. It quantifies spatial uncertainty from stochastic grounding samples using three complementary measures—top-candidate ambiguity, informational dispersion, and concentration deficit—then calibrates a decision threshold to control the false discovery rate (FDR) at a user-specified level. Experiments on ScreenSpot-Pro with six GUI grounding models show that SafeGround's uncertainty estimation consistently outperforms baselines in discriminating correct from incorrect predictions (AUROC up to 0.8155, AUARC up to 0.7166). More importantly, it provides finite-sample FDR guarantees and improves system-level accuracy by up to 5.38 percentage points over Gemini-only inference through selective cascading.

## Method Summary
SafeGround performs K stochastic forward passes per input (default K=10, temperature=1.0), maps sampled coordinates to a discretized screen grid to form a spatial probability distribution P, then clusters connected high-density patches into candidate regions R={Rm} with associated scores Sm. Three uncertainty measures are computed: top-candidate ambiguity (UTA), informational dispersion (UIE), and concentration deficit (UCD). These are combined via fixed weights (wCD=0.6, wIE=0.2, wTA=0.2) to produce a single uncertainty score. A learn-then-test calibration procedure uses Clopper-Pearson confidence bounds on a held-out calibration set to derive a decision threshold τ̂ that guarantees FDR ≤ α with confidence 1-δ. Predictions with uncertainty below τ̂ are accepted; others cascade to a stronger model or are abstained.

## Key Results
- SafeGround's uncertainty estimation consistently outperforms baselines in discriminating correct from incorrect predictions (AUROC up to 0.8155, AUARC up to 0.7166)
- The framework provides finite-sample FDR guarantees through Clopper-Pearson calibration with statistically guaranteed control
- System-level accuracy improves by up to 5.38 percentage points over Gemini-only inference through selective cascading
- Calibration robustness across different split ratios, though very low α values may be infeasible for some models

## Why This Works (Mechanism)

### Mechanism 1: Distribution-Aware Uncertainty Quantification via Stochastic Sampling
The model performs K stochastic forward passes (default K=10, temperature=1.0), maps sampled coordinates to a discretized screen grid to form a spatial probability distribution P, then clusters connected high-density patches into candidate regions R={Rm} with associated scores Sm. This transforms deterministic point predictions into an empirical distribution capturing model consistency. Variability in stochastic samples correlates with prediction reliability—low dispersion indicates model confidence, while scattered samples signal uncertainty. This assumes the sampling distribution reflects true epistemic uncertainty.

### Mechanism 2: Complementary Uncertainty Measures Capture Distinct Failure Modes
Three measures are computed from region scores: top-candidate ambiguity (UTA), informational dispersion (UIE), and concentration deficit (UCD). These capture orthogonal uncertainty signals—local ambiguity vs. global dispersion vs. concentration—and their weighted combination generalizes across architectures without per-model tuning. Each uncertainty score captures a distinct aspect of predictive dispersion, and no single measurement is universally dominant across all models. The three measures are combined via fixed weights (wCD=0.6, wIE=0.2, wTA=0.2).

### Mechanism 3: Learn-Then-Test Calibration Provides Finite-Sample FDR Guarantees
On a held-out calibration set Dcal={(xi, qi, B*i)} of N samples, the framework computes uncertainty ui for each sample, then for each candidate threshold τ, computes empirical FDR(τ) and its upper confidence bound via Beta distribution: FDR^upper(τ) = Beta(1-δ; X+1, n-X). This ensures Pr(R(τ̂) ≤ α) ≥ 1-δ under exchangeability. The calibration procedure uses Clopper-Pearson bounds to construct conservative FDR upper bounds from finite calibration data, enabling finite-sample guarantees rather than asymptotic ones.

## Foundational Learning

- **Concept: Monte Carlo Sampling for Uncertainty Estimation**
  - Why needed here: SafeGround relies on sampling K stochastic predictions per input to construct spatial distributions. Understanding how sampling variability relates to epistemic uncertainty is foundational.
  - Quick check question: If you increase sampling temperature from 0.5 to 1.0, would you expect spatial distributions to become more or less concentrated? (Answer: Less concentrated; Table 5 shows AUROC increases with temperature, indicating more informative dispersion signals)

- **Concept: False Discovery Rate (FDR) vs. False Positive Rate (FPR)**
  - Why needed here: SafeGround controls FDR (proportion of incorrect among accepted predictions), not FPR (proportion of negatives incorrectly classified as positive). This distinction is critical for risk management in high-stakes GUI actions.
  - Quick check question: If a system accepts 100 predictions and 20 are incorrect, what is the FDR? If the same system rejected 50 correct predictions, does that affect FDR? (Answer: FDR=0.20; rejections don't affect FDR, only acceptance pool quality matters)

- **Concept: Clopper-Pearson Confidence Intervals for Binomial Proportions**
  - Why needed here: The calibration procedure uses Clopper-Pearson bounds to construct conservative FDR upper bounds from finite calibration data, enabling finite-sample guarantees rather than asymptotic ones.
  - Quick check question: Given 10 accepted calibration samples with 2 errors, the Clopper-Pearson upper bound at 95% confidence will be higher than 0.20 (the empirical error rate). Why? (Answer: Conservative interval accounts for sampling variance in small samples; uses Beta quantiles to ensure at least nominal coverage)

## Architecture Onboarding

- **Component map:** Input (screenshot, instruction) → GUI Grounding Model → Spatial Distribution Constructor → Uncertainty Estimator → Calibrated Threshold τ̂ → Decision: Accept or Cascade/Abstain

- **Critical path:**
  1. **Sampling quality:** K=10 with temperature=1.0 is empirically validated (Figure 6 shows plateau beyond K=10). Lower K or temperature may degrade AUROC.
  2. **Region extraction:** Patch size=14, density threshold β=0.3×P_max (Appendix B.3). These control granularity of spatial hypotheses.
  3. **Calibration set size:** Figure 7 shows robustness across split ratios, but too few calibration samples limits threshold precision.

- **Design tradeoffs:**
  - **Risk level α vs. Power:** Lower α (stricter FDR control) reduces power (fewer correct predictions retained). Table 1 shows cascading gains diminish as α increases.
  - **Computational cost vs. uncertainty quality:** K=10 requires 10× forward passes per input. K=5 is feasible but lower AUROC (Figure 6).
  - **Model-agnostic weights vs. per-model tuning:** Fixed weights (0.6, 0.2, 0.2) enable plug-and-play but may be suboptimal for models with different uncertainty profiles (Table 4 shows model-dependent dominance).

- **Failure signatures:**
  - **Unattainable risk levels:** If calibration returns no valid threshold, the base model's uncertainty discrimination is insufficient for the requested α. Solution: Relax α or use a stronger base model.
  - **Low cascading gains despite calibration:** Check if cascading rate is near 0% (Figure 5). This indicates base model is already above threshold for most samples, suggesting α is too high or base model is overconfident.
  - **FDR violations on test data:** Verify calibration-test exchangeability. Distribution shift (different GUI environments) may invalidate guarantees.

- **First 3 experiments:**
  1. **Validate uncertainty discrimination on held-out data:** Compute AUROC and AUARC for UCOM vs. probabilistic confidence baseline on a validation split. Expected: UCOM ≥ PC across models (Table 2-3).
  2. **Calibrate threshold and verify FDR control:** Run Algorithm 1 on calibration set (20% of data) with α=0.38, δ=0.05. Compute empirical FDR on test set (80%). Expected: FDR ≤ α (Figure 3).
  3. **Measure cascading system accuracy:** Implement cascading to Gemini-3-pro for samples with u > τ̂. Compare system accuracy vs. Gemini-only and base-model-only baselines. Expected: Gains of 3-6 percentage points at feasible α levels (Table 1).

## Open Questions the Paper Calls Out

- **How can uncertainty be reliably estimated for deterministic GUI grounding models that fail to produce diverse stochastic samples?**
  - Basis: The limitation section states that for "highly deterministic models with limited sampling diversity, the resulting spatial distributions may be less informative."
  - Why unresolved: The current method relies entirely on variance across multiple stochastic samples to calculate spatial dispersion and concentration.
  - What evidence would resolve it: A modification of the framework or a new sampling strategy that validates uncertainty discrimination on low-entropy models.

- **Can the spatial uncertainty metrics be adapted for non-coordinate GUI actions, such as text input or drag operations?**
  - Basis: The methodology restricts the problem space to predicting screen coordinates ($R^2$) and does not evaluate discrete text generation or trajectories.
  - Why unresolved: Metrics like spatial entropy and concentration deficit are designed for 2D point distributions and may not capture semantic uncertainty in text.
  - What evidence would resolve it: An extension of the framework that provides FDR guarantees for action types involving discrete tokens or vectors.

- **Does model-specific weighting of uncertainty components ($U_{TA}, U_{IE}, U_{CD}$) improve performance over the fixed universal scheme?**
  - Basis: The authors use a fixed weight set ($0.6, 0.2, 0.2$) for a "plug-and-play interface," but the ablation study (Table 4) shows the dominant component varies significantly across models.
  - Why unresolved: A universal weighting scheme may be suboptimal, potentially suppressing the most informative signal for specific model architectures.
  - What evidence would resolve it: Experiments tuning weights per model to determine if adaptive weighting yields higher AUROC or AUARC scores.

## Limitations
- **Sampling method specification**: The paper does not explicitly detail the stochastic decoding mechanism (dropout, beam search, nucleus sampling) used to generate K samples per input, which could affect uncertainty estimation quality.
- **Grid dimension derivation**: While patch_size=14 is specified, the derivation of grid dimensions H×W from image dimensions is not explicitly stated, potentially affecting reproducibility.
- **Per-model weight optimization**: Fixed weights (0.6, 0.2, 0.2) enable plug-and-play but may be suboptimal for models with different uncertainty profiles, as ablation shows model-dependent dominance patterns.

## Confidence
- **High Confidence**: FDR control mechanism with Clopper-Pearson calibration, baseline AUROC/AUARC improvements, cascading system accuracy gains (supported by formal guarantees and consistent empirical results)
- **Medium Confidence**: Complementary uncertainty measure framework (supported by ablation but limited by model-agnostic fixed weights), sampling-based spatial uncertainty quantification (theoretical soundness but method details unspecified)
- **Low Confidence**: Claim that no single uncertainty metric dominates across all models (while Table 4 shows clear dominance patterns per model, the fixed weighting approach may not be optimal)

## Next Checks
1. **Method reproducibility test**: Implement the stochastic sampling and spatial distribution construction pipeline on a held-out subset, verifying that AUROC improvements are consistent across different GUI grounding models and that threshold calibration successfully bounds FDR at specified levels
2. **Weight optimization study**: Compare fixed weights (0.6, 0.2, 0.2) against per-model learned weights on a validation set to quantify potential performance gains from model-specific calibration
3. **Sampling method sensitivity analysis**: Test alternative stochastic decoding approaches (e.g., nucleus sampling vs. temperature sampling) to determine their impact on spatial uncertainty estimation quality and overall system performance