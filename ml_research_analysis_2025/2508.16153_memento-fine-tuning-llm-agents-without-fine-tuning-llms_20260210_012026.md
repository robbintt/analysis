---
ver: rpa2
title: 'Memento: Fine-tuning LLM Agents without Fine-tuning LLMs'
arxiv_id: '2508.16153'
source_url: https://arxiv.org/abs/2508.16153
tags:
- memento
- memory
- arxiv
- agents
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Memento introduces a memory-based learning paradigm for LLM agents
  that achieves continual adaptation without fine-tuning underlying models. The approach
  formalises deep research agents as a memory-augmented Markov Decision Process (M-MDP),
  enabling case-based reasoning through episodic memory.
---

# Memento: Fine-tuning LLM Agents without Fine-tuning LLMs

## Quick Facts
- **arXiv ID**: 2508.16153
- **Source URL**: https://arxiv.org/abs/2508.16153
- **Reference count**: 18
- **Primary result**: Memory-based continual learning without LLM fine-tuning achieves SOTA on GAIA and DeepResearcher benchmarks

## Executive Summary
Memento introduces a memory-based learning paradigm for LLM agents that achieves continual adaptation without fine-tuning underlying models. The approach formalizes deep research agents as a memory-augmented Markov Decision Process (M-MDP), enabling case-based reasoning through episodic memory. Memento alternates between case-based planning and tool-based execution, retrieving and adapting past experiences to guide decisions. Experiments on benchmarks including GAIA, DeepResearcher, and SimpleQA show top-1 performance on GAIA validation (87.88% Pass@3) and test sets (79.40%), achieving 66.6% F1 and 80.4% PM on DeepResearcher—outperforming state-of-the-art training-based methods. Case-based memory contributes 4.7%–9.6% absolute improvements on out-of-distribution tasks, demonstrating effective continual learning without parameter updates.

## Method Summary
Memento employs a planner-executor architecture where the planner (GPT-4.1) decomposes tasks using case-based reasoning from a growing Case Bank, while the executor (o4-mini or o3) uses Model Context Protocol (MCP) tools for web search, crawling, and multimodal processing. The Case Bank stores (state, action, reward) tuples and uses non-parametric retrieval (cosine similarity with SimCSE) or parametric (learned Q-function via binary cross-entropy) approaches. Online Q-learning updates policy during execution without LLM gradient updates. The system achieves continual adaptation by accumulating experiences across tasks while maintaining the base LLM parameters frozen.

## Key Results
- Achieves top-1 performance on GAIA validation (87.88% Pass@3) and test sets (79.40%)
- Outperforms state-of-the-art training-based methods on DeepResearcher with 66.6% F1 and 80.4% PM
- Case-based memory contributes 4.7%–9.6% absolute improvements on out-of-distribution tasks
- Demonstrates effective continual learning without parameter updates, with case bank saturation after ~3k samples

## Why This Works (Mechanism)
Memento works by treating LLM agents as memory-augmented Markov Decision Processes where decisions are guided by retrieved experiences rather than learned parameters. The planner uses case-based reasoning to decompose tasks by finding similar past experiences, while the executor performs tool-based actions. The memory-augmented MDP framework allows the system to accumulate knowledge across tasks without modifying the underlying LLM weights. By alternating between retrieval-based planning and execution-based learning, the system builds a case bank that captures successful strategies while maintaining the flexibility to adapt to new scenarios through case adaptation rather than parameter updates.

## Foundational Learning
- **Memory-augmented MDPs**: Extension of standard MDPs with episodic memory storage and retrieval. Needed for formalizing how agents can use past experiences to guide future decisions. Quick check: Verify state, action, reward tuples are properly stored and retrievable.
- **Case-based reasoning**: Problem-solving approach that retrieves and adapts solutions from similar past cases. Needed for enabling knowledge transfer without model training. Quick check: Test retrieval accuracy on known cases from the bank.
- **Model Context Protocol (MCP)**: Standardized interface for LLM tool integration. Needed for consistent tool execution across different executor models. Quick check: Verify all MCP tools (search, crawl, multimodal) respond correctly to executor requests.
- **Non-parametric vs parametric retrieval**: Comparison between similarity-based retrieval and learned Q-function approaches. Needed for understanding which memory mechanism works best for different task types. Quick check: Compare retrieval quality on held-out cases using both methods.
- **Online Q-learning without gradient updates**: Policy improvement through experience replay without modifying LLM weights. Needed for maintaining continual learning capability without fine-tuning costs. Quick check: Verify policy updates occur only in memory, not in LLM parameters.

## Architecture Onboarding

**Component Map:**
Planner (GPT-4.1) -> Case Bank (SimCSE embeddings) -> Executor (o4-mini/o3) -> MCP Tools -> Environment -> Reward Signal -> Case Bank Update

**Critical Path:**
Task decomposition → Case retrieval → Case adaptation → Tool execution → Reward evaluation → Memory update

**Design Tradeoffs:**
- Memory growth vs retrieval efficiency: Growing case banks improve coverage but increase retrieval time and noise
- Online vs offline execution: Online tools provide current information but risk data contamination
- Non-parametric vs parametric retrieval: Similarity-based methods are interpretable but may miss complex patterns; learned methods generalize better but require training

**Failure Signatures:**
- Retrieval noise when case bank exceeds 4-8 cases (accuracy plateaus or degrades)
- Data contamination showing negative transfer (e.g., -18.0 F1 drop on DeepResearcher with online tools)
- Case bank saturation after ~3k samples with <1% improvement per iteration
- Role confusion when planner becomes overly deliberative, compressing solutions or skipping plan generation

**First Experiments:**
1. Run planner-executor loop on GAIA validation to verify 87.88% Pass@3 result
2. Compare online vs offline executor performance on DeepResearcher to confirm -18.0 F1 difference
3. Track per-iteration gains beyond 5 iterations to quantify case bank saturation point

## Open Questions the Paper Calls Out
### Open Question 1
- **Question**: How can the framework be extended to handle Level 3 tasks requiring extended reasoning horizons and complex tool coordination?
- **Basis in paper**: The authors state in the results section that "challenges remain for Level 3 tasks that require extended reasoning horizons and advanced tool coordination" (Page 13).
- **Why unresolved**: Current performance drops significantly on Level 3 tasks (71.43% vs 90.32% on Level 1), suggesting the current memory-based planning struggles with the complexity of long-horizon dependencies.
- **What evidence would resolve it**: Demonstrated improvement in accuracy on Level 3 GAIA tasks using modified memory retrieval or planning strategies specifically designed for long-horizon dependencies.

### Open Question 2
- **Question**: Can probabilistic reward functions and memory updates improve the current deterministic M-MDP formulation?
- **Basis in paper**: The paper notes that the current reward function and memory update mechanisms are deterministic, stating this "can also be probabilistic in some specific cases, which we leave as future work" (Page 6).
- **Why unresolved**: The current formulation relies on indicator functions for rewards and memory retention, which may not capture the nuance of uncertain environments or soft failures.
- **What evidence would resolve it**: Comparative analysis showing superior performance or stability in stochastic environments when using probabilistic memory updates versus the current deterministic approach.

### Open Question 3
- **Question**: What is the optimal division of labor between the planner and executor to prevent "role confusion"?
- **Basis in paper**: The analysis shows that "overly deliberative planning... induces role confusion," causing the planner to compress solutions or skip plan generation entirely, undermining the two-stage architecture (Page 18).
- **Why unresolved**: It is unclear if "fast" planning is universally better or if specific prompting/model constraints are needed to enforce strict role separation.
- **What evidence would resolve it**: Ablation studies on various planner architectures demonstrating consistent role adherence without performance degradation due to compressed reasoning.

### Open Question 4
- **Question**: How can memory-based continual learning be sustained in open-ended environments to prevent Case Bank saturation?
- **Basis in paper**: The authors note that with ~3k training data, the Case Bank saturates quickly and the simulated environment is finite, making it hard to study memory-based continual learning in truly open-ended settings (Page 16).
- **Why unresolved**: The system converges rapidly in finite settings, leaving the behavior of the memory mechanism in endless, non-saturating scenarios unknown.
- **What evidence would resolve it**: Performance curves in an open-ended environment (without a fixed test set) showing continuous improvement or effective memory management over significantly longer timeframes.

## Limitations
- Case bank saturation after ~3,000 samples with diminishing returns (<1% improvement after iteration 4-5)
- Significant data contamination risks with online tool execution (-18.0 F1 drop on DeepResearcher)
- Requires extensive tool infrastructure (search, crawl, multimodal processing) not universally available
- Case-based reasoning contributes only 4.7%-9.6% absolute improvements on out-of-distribution tasks

## Confidence

**High Confidence**: The memory-augmented MDP formulation is mathematically sound, and the reported GAIA validation performance (87.88% Pass@3) appears robust given the methodology. The core claim that Memento achieves state-of-the-art results without LLM fine-tuning is well-supported by the experimental comparisons.

**Medium Confidence**: The effectiveness of non-parametric vs parametric retrieval methods shows mixed results. While non-parametric methods excel on GAIA and DeepResearcher, parametric methods outperform on HLE. This suggests the approach may be problem-dependent rather than universally superior.

**Low Confidence**: The scalability claims beyond the tested benchmarks remain uncertain. The paper doesn't address computational costs of maintaining growing case banks or potential performance degradation with millions of stored experiences.

## Next Checks
1. **Reproduce GAIA validation performance**: Implement the complete planner-executor loop with case-based reasoning on GAIA validation set to verify the 87.88% Pass@3 result, tracking per-iteration improvements and case bank growth.
2. **Test data contamination effects**: Run DeepResearcher benchmark with both online and offline executors to confirm the reported -18.0 F1 difference and assess whether this represents fundamental limitations or implementation issues.
3. **Evaluate long-term memory saturation**: Run extended experiments beyond 5 iterations (10+ iterations) on GAIA to quantify the exact point where case bank additions yield <0.5% performance gains, confirming the plateau hypothesis.