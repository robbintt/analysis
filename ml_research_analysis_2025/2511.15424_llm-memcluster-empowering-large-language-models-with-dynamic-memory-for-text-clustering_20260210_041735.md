---
ver: rpa2
title: 'LLM-MemCluster: Empowering Large Language Models with Dynamic Memory for Text
  Clustering'
arxiv_id: '2511.15424'
source_url: https://arxiv.org/abs/2511.15424
tags:
- clustering
- text
- memory
- framework
- label
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the challenge of using large language models
  (LLMs) for text clustering by introducing LLM-MemCluster, a framework that overcomes
  two key limitations: LLMs'' lack of stateful memory for iterative refinement and
  the difficulty of managing cluster granularity. The authors propose a Dynamic Memory
  mechanism to maintain evolving cluster labels and a Dual-Prompt Strategy to control
  clustering granularity.'
---

# LLM-MemCluster: Empowering Large Language Models with Dynamic Memory for Text Clustering

## Quick Facts
- **arXiv ID:** 2511.15424
- **Source URL:** https://arxiv.org/abs/2511.15424
- **Reference count:** 18
- **Primary result:** LLM-MemCluster achieves ARI of 54.5% vs 33.7% baseline across six benchmark datasets

## Executive Summary
LLM-MemCluster addresses the challenge of using large language models for text clustering by introducing a dynamic memory mechanism and dual-prompt strategy. The framework enables LLMs to maintain evolving cluster labels across instances, allowing iterative refinement without fine-tuning. By processing text instances sequentially and optionally merging semantically similar clusters, LLM-MemCluster achieves a single-pass, end-to-end clustering approach that significantly outperforms existing methods on benchmark datasets.

## Method Summary
LLM-MemCluster processes text instances sequentially through a dynamic memory module that maintains evolving cluster labels. For each instance, the framework constructs a prompt incorporating current labels from memory, receives label and merge suggestions from the LLM, and updates memory accordingly. The dual-prompt strategy alternates between Relaxed and Strict modes based on cluster count thresholds to control granularity. When merges occur, the framework retroactively updates all prior assignments to maintain consistency. This approach enables unsupervised clustering without fine-tuning while maintaining state across the entire clustering process.

## Key Results
- Achieves average Adjusted Rand Index (ARI) of 54.5% across six benchmark datasets
- Outperforms best baseline (ClusterLLM) by 20.8 percentage points (54.5% vs 33.7% ARI)
- Demonstrates robustness to hyperparameter variations and generalizes across different LLM models
- Single-pass approach eliminates need for iterative refinement or model fine-tuning

## Why This Works (Mechanism)
The framework works by addressing two key limitations of LLMs in clustering: lack of stateful memory for iterative refinement and difficulty managing cluster granularity. The dynamic memory mechanism maintains an evolving set of cluster labels that can be reused or merged based on semantic similarity, while the dual-prompt strategy provides fine-grained control over cluster count. This combination allows the LLM to learn from its previous assignments and refine cluster definitions throughout the clustering process, effectively creating a stateful clustering agent from a stateless model.

## Foundational Learning

- **Dynamic Memory in LLMs:** Maintaining state across sequential processing is crucial for iterative refinement tasks; check by verifying memory injection into prompts
- **Dual-Prompt Granularity Control:** Alternating between relaxed and strict clustering modes prevents over/under-clustering; validate by monitoring label count growth
- **Retroactive Assignment Updates:** Ensures consistency when clusters merge, maintaining temporal coherence; test by checking assignment consistency across merge operations
- **Few-shot Prompt Grounding:** Providing examples helps LLMs understand domain-specific clustering patterns; verify by comparing performance with/without examples on nuanced datasets
- **Single-pass vs Iterative Refinement:** Eliminates computational overhead of multiple passes while maintaining quality; measure by comparing runtime and cluster stability
- **Cluster-to-Label Alignment:** Hungarian matching ensures fair evaluation by optimally mapping predicted clusters to ground truth; validate by checking assignment mapping accuracy

## Architecture Onboarding

**Component Map:** Text Instances -> Dynamic Memory Module -> Dual-Prompt Strategy -> LLM -> Memory Updates -> Assignments

**Critical Path:** The essential flow involves processing each instance through prompt construction with current memory, LLM inference, memory update (add/merge), and retroactive assignment updates when merges occur.

**Design Tradeoffs:** Single-pass eliminates iterative overhead but requires careful memory management; dual-prompt prevents over-clustering but adds complexity; retroactive updates ensure consistency but may increase computational cost.

**Failure Signatures:** Unbounded label proliferation indicates improper Strict mode activation; inconsistent assignments across batches suggest memory injection failures; severe performance drops on nuanced datasets without few-shot examples.

**First Experiments:** 1) Run controlled test with 100 instances and 5 ground-truth clusters to verify retroactive merge updates; 2) Compare shuffled vs sequential processing order on 500 ArxivS2S samples; 3) Test K_max sensitivity with ±20% variation from ground-truth K on all six datasets.

## Open Questions the Paper Calls Out
None

## Limitations
- Missing specific few-shot examples used in prompt templates
- K_max threshold values per dataset not specified
- Processing order of instances not defined, affecting early label formation
- Retroactive assignment updates may introduce computational overhead

## Confidence

**High Confidence:** Framework core mechanism and overall performance trends (ARI 54.5% vs 33.7% baseline)

**Medium Confidence:** Specific hyperparameter choices and processing order effects

**Low Confidence:** Exact replication of reported numbers without missing prompt examples and K_max values

## Next Checks

1. Verify retroactive assignment updates during merges maintain temporal consistency by running a controlled test with 100 instances and 5 ground-truth clusters
2. Test the impact of processing order by comparing shuffled vs sequential instance ordering on a subset (500 samples from ArxivS2S)
3. Validate the K_max threshold sensitivity by running experiments with ±20% variation from assumed ground-truth K values on all six datasets