---
ver: rpa2
title: 'Audo-Sight: Enabling Ambient Interaction For Blind And Visually Impaired Individuals'
arxiv_id: '2505.00153'
source_url: https://arxiv.org/abs/2505.00153
tags:
- system
- user
- audo-sight
- users
- visual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Audo-Sight is an AI-powered assistive system that provides context-aware,
  voice-driven interactions for blind and visually impaired (BVI) individuals. It
  integrates multimodal large language models (MLLMs) to process visual, auditory,
  and textual inputs in real-time, offering adaptive navigation assistance and latency
  management.
---

# Audo-Sight: Enabling Ambient Interaction For Blind And Visually Impaired Individuals

## Quick Facts
- arXiv ID: 2505.00153
- Source URL: https://arxiv.org/abs/2505.00153
- Authors: Bhanuja Ainary
- Reference count: 40
- AI-powered assistive system providing context-aware, voice-driven interactions for BVI individuals

## Executive Summary
Audo-Sight is an AI-powered assistive system designed to provide context-aware, voice-driven interactions for blind and visually impaired (BVI) individuals. The system integrates multimodal large language models (MLLMs) to process visual, auditory, and textual inputs in real-time, offering adaptive navigation assistance and latency management. It operates in two modes: personalized interaction through user identification in private settings, and public access with age-range detection and safe query filtering for shared environments like museums and malls.

The system ensures blind-friendly responses using NeMo Guardrails and employs a latency-aware LLM router to optimize execution paths based on response time thresholds. Audo-Sight was integrated with the SmartSight prototype, combining real-time visual analysis with interactive voice assistance. Performance evaluation showed an average MLLM inference time of 5.62 seconds, with text-to-speech processing at 1.34 seconds and speech recognition at 1.65 seconds. The system successfully reduced inappropriate responses by 90% using NeMo Guardrails and maintained near-real-time latency in passive mode.

## Method Summary
Audo-Sight integrates multimodal large language models to process visual, auditory, and textual inputs in real-time. The system uses a latency-aware router to dynamically select between full multimodal processing and faster text-based paths, with NeMo Guardrails ensuring accessibility and safety. It operates on a hybrid edge-cloud architecture, with voice input processed via Whisper STT and responses generated through Llama 3.2 Vision or text-only engines. The system was evaluated using the VizWiz-VQA dataset and integrated with SmartSight glasses for real-world testing.

## Key Results
- Average MLLM inference time of 5.62 seconds, with text-to-speech at 1.34 seconds and speech recognition at 1.65 seconds
- 90% reduction in inappropriate responses using NeMo Guardrails
- End-to-end latency of 8.85 seconds in active mode, 188.77ms for object detection in passive mode
- Successful integration with SmartSight prototype for real-world visual analysis and voice assistance

## Why This Works (Mechanism)

### Mechanism 1: Adaptive Latency-Aware Routing for Real-Time Responsiveness
- Claim: A latency-aware router dynamically selects between a full multimodal reasoning path and a faster text-based path to keep system response times within acceptable bounds.
- Mechanism: The Cognition Engine measures the predicted or actual response time of the Multimodal Reasoning Engine against a set threshold. If latency exceeds this threshold, the system routes the query to an Image-to-Text module followed by a Text-Based Reasoning Engine. If latency is acceptable, it proceeds with full multimodal processing. This optimizes the trade-off between response depth and speed.
- Core assumption: A text-based response generated from an image description is sufficiently accurate and useful for BVI users in time-sensitive scenarios.
- Evidence anchors:
  - [abstract] "Key innovations include a latency-aware LLM router that dynamically allocates queries based on processing time..."
  - [section 3.2.2.1-3.2.2.3] Detailed description of the latency-aware routing, threshold-based decision making, and execution path optimization.
  - [corpus] The corpus does not contain papers that explicitly validate a latency-aware routing mechanism for BVI assistive systems; this appears to be a novel contribution of Audo-Sight.
- Break condition: If the Image-to-Text module frequently produces descriptions that are too vague or incorrect for the text-based engine to answer the user's query, the system's utility would be compromised, and users may ignore the faster route.

### Mechanism 2: Guardrail-Enforced BVI-Friendly Response Generation
- Claim: NeMo Guardrails is used to post-process LLM outputs, systematically removing inaccessible language and structuring information for non-visual understanding.
- Mechanism: The response from the Reasoning Engine is passed through a configured NeMo Guardrails framework. This framework uses a set of rules and instructions to detect and rephrase inaccessible content. For example, it replaces vague spatial references (e.g., "over there") with specific directional language and removes references to color or visual appearance that are not functionally relevant.
- Core assumption: Standard LLM outputs contain significant amounts of inaccessible or unhelpful language for BVI individuals that can be reliably identified and modified by a set of pre-defined rules and guardrails.
- Evidence anchors:
  - [abstract] "...a BVI-friendly response generator that ensures accessibility and safety using NeMo Guardrails..."
  - [section 3.2.2.7] Describes implementation details using `config.yml` for instructions (e.g., "Do not talk about colors") and `actions.py` for filtering offensive terms.
  - [corpus] The corpus does not provide direct evidence for NeMo Guardrails' efficacy in BVI contexts, though one paper mentions using generative AI platforms for accessibility (Ray-Bans).
- Break condition: The system will fail if complex visual descriptions cannot be adequately rephrased by static rules, leading to garbled or uninformative output. The guardrails could also over-filter, removing necessary details.

### Mechanism 3: Multimodal Fusion for Context-Aware Interaction
- Claim: Fusing voice input and visual data from the SmartSight glasses allows the system to understand user intent in relation to the physical environment, enabling complex query resolution.
- Mechanism: The Input Management module captures user voice (processed to text via Whisper) and the corresponding image frame from the glasses. These two modalities are synchronized at a Fusion Hub and then fed into the Cognition Engine (e.g., Llama 3.2 11B Vision Instruct). The MLLM processes both inputs to generate a single, contextually grounded response.
- Core assumption: The image frame captured closest to the start of a voice query provides the most relevant visual context for answering the user's question.
- Evidence anchors:
  - [abstract] "The system integrates Multimodal Large Language Models (MLLMs) to interpret visual, auditory, and textual inputs..."
  - [section 5.2.4] "The system reserves an image at this point expecting that the image most contextually related to the query is the one temporally located closest to the start of the query."
  - [corpus] "Probing the Gaps in ChatGPT Live Video Chat..." mentions using live video feeds for BVI assistance, which supports the value of real-time multimodal input but notes potential challenges.
- Break condition: The fusion mechanism will break if there is a significant misalignment between the user's verbal query and the captured image (e.g., user is asking about something they heard but is not in the frame).

## Foundational Learning
- Concept: Multimodal Large Language Models (MLLMs)
  - Why needed here: This is the core technology enabling Audo-Sight to "see" and "talk." You cannot understand the system's reasoning process without grasping how an MLLM takes both an image and text as input to generate a text response.
  - Quick check question: What are the two primary inputs to the Reasoning Engine in Audo-Sight?

- Concept: Edge-Cloud Computing Continuum
  - Why needed here: Audo-Sight's architecture is hybrid. To understand latency tradeoffs, you must know which tasks (e.g., object detection, speech-to-text) are performed on a local edge device (e.g., Raspberry Pi, laptop) and which (e.g., complex MLLM inference) are offloaded to the cloud.
  - Quick check question: According to the performance evaluation, which task is identified as the primary contributor to the system's overall latency in active mode?

- Concept: Guardrails in AI
  - Why needed here: Safety and accessibility are enforced not by the base model, but by a separate framework (NeMo). Understanding this requires knowing the concept of programmable rules that intercept and modify LLM input/output.
  - Quick check question: According to the paper, what is the specific function of the `rails.co` file in the NeMo Guardrails implementation?

## Architecture Onboarding
- Component map: Input Management -> Fusion Hub -> Cognition Engine (Latency-Aware Router -> MLLM/Text-Engine -> BVI Response Gen) -> Response Management
- Critical path: User Voice/Image -> [Input Management] -> Fusion Hub -> [Cognition Engine: Router -> MLLM/Text-Engine -> BVI Response Gen] -> [Response Management] -> Audio Output
- Design tradeoffs: The central tradeoff is Latency vs. Quality. The system can provide faster, potentially less accurate responses via the text-based route (using Image-to-Text), or slower, richer responses via the multimodal MLLM. The router manages this based on real-time latency.
- Failure signatures:
    - **Timeouts:** If the Reasoning Engine's latency consistently exceeds the threshold, the system will always fall back to the text-based route, potentially degrading answer quality.
    - **Misaligned Context:** If the image captured by the glasses doesn't match the user's verbal query, the MLLM will generate irrelevant or incorrect answers.
    - **Over-Filtering:** If NeMo Guardrails rules are too aggressive, the final audio output may lack necessary detail or become generic and unhelpful.
- First 3 experiments:
  1. **Latency Baseline Establishment:** Measure and log the inference time for the MLLM (Llama 3.2 11B Vision Instruct) and the text-based engine across a standard set of queries (VizWiz-VQA dataset) to empirically set the latency threshold for the router.
  2. **BVI Response Quality Validation:** Run a set of standard LLM outputs through the NeMo Guardrails `config.yml` and `actions.py` pipeline. Quantify the reduction in inaccessible phrases (e.g., color references, vague directions) compared to the raw output.
  3. **End-to-End Integration Test:** Execute the full flow from voice input to audio output on the hardware prototype. Measure the total system latency (target: <8.85s average for active mode) and identify the slowest component.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can dynamic service migration strategies (e.g., FastMig, UMS) be integrated into Audo-Sight to optimize the trade-off between energy consumption and latency across the edge-to-cloud continuum?
- Basis in paper: [explicit] Section 6.2 lists "End-to-End Implementation of the Workflow across the Edge-to-Cloud Continuum" as a future direction, proposing the use of strategies like FastMig and HEET to improve upon the current mixed computational approach.
- Why unresolved: The current system lacks an intelligent, dynamic allocation system to handle energy-accuracy trade-offs and latency limitations automatically.
- What evidence would resolve it: Experimental results showing reduced latency and energy usage when employing dynamic offloading compared to the current static mixed approach.

### Open Question 2
- Question: To what extent does incorporating voice-based mood detection improve the perceived empathy and user trust of Audo-Sight's responses compared to the current context-only generation?
- Basis in paper: [explicit] Section 6.2 proposes "Mood Detection from Voice and Emotion-Aware LLM Responses" to advance the system's empathetic value, noting that current responses are not emotionally flexible.
- Why unresolved: The current implementation generates contextually relevant responses but does not adapt its tone or content based on the user's emotional state (e.g., frustration, distress).
- What evidence would resolve it: User studies with BVI individuals quantifying trust and satisfaction levels when using emotion-aware responses versus standard factual replies.

### Open Question 3
- Question: How effective are single-modality frameworks (audio cues vs. computer vision) in identifying environmental hazards (e.g., alarms, obstacles) and triggering automated incident reports without user intervention?
- Basis in paper: [explicit] Section 6.2 identifies "Hazard Detection" as a key area for enhancing user safety, specifically suggesting the development of automated frameworks to detect threats and alert authorities.
- Why unresolved: The current system relies on passive object detection or active user queries and lacks an automated mechanism to identify specific hazards or emergencies independently.
- What evidence would resolve it: Benchmarks of precision and recall for hazard detection models in diverse environments, and latency metrics for the automated alert generation process.

### Open Question 4
- Question: Can the average active mode latency (8.85s) be reduced to a level suitable for real-time conversation by optimizing the MLLM inference or the image-to-text bypass mechanism?
- Basis in paper: [inferred] Section 5.4 reports an average active mode latency of 8.85 seconds, with MLLM inference accounting for the majority (5.62s). This delay may hinder natural, fluid interaction.
- Why unresolved: While the paper introduces a latency-aware router, the reported end-to-end time for active interaction remains high relative to conversational norms.
- What evidence would resolve it: End-to-end latency measurements after applying model quantization, hardware acceleration, or optimized routing thresholds to achieve sub-second or near-real-time responses.

## Limitations
- Performance in Complex Real-World Scenarios: Real-world environments present unpredictable variables that could significantly impact both accuracy and latency beyond controlled dataset evaluation.
- BVI-Friendly Output Quality: 90% reduction in inappropriate responses focuses on safety rather than accessibility quality; over-aggressive filtering could remove necessary information.
- User Experience Validation: Evaluation focuses on technical metrics rather than user experience; critical questions about latency tolerance and interaction patterns remain unanswered without user studies.

## Confidence
- **High Confidence:** Technical architecture description is detailed and internally consistent; core component implementations like Whisper STT and Llama 3.2 Vision appear straightforward.
- **Medium Confidence:** Latency-aware routing mechanism is conceptually well-described but effectiveness depends on unspecified threshold values and real-world testing.
- **Low Confidence:** Claims about accessibility improvements and user experience benefits lack empirical support; system effectiveness for BVI users depends on unmeasured factors.

## Next Checks
1. **Empirical Latency Threshold Validation:** Implement the latency-aware router with multiple threshold values (e.g., 3s, 5s, 7s) and test across diverse real-world scenarios beyond VizWiz-VQA. Measure not just raw latency but also the accuracy of responses from both MLLM and text-only paths to determine the optimal tradeoff point.

2. **Accessibility Quality Assessment:** Conduct a systematic evaluation of NeMo Guardrails' output quality by having BVI users rate responses on accessibility dimensions (clarity, spatial accuracy, completeness) rather than just safety. Compare guardrail-processed outputs against raw LLM outputs and identify cases where filtering removes necessary information.

3. **User Experience Study:** Deploy the system with 10-15 BVI participants in both controlled (home/office) and public (museum, mall) environments. Measure task completion rates, user satisfaction scores, and qualitative feedback on latency tolerance and interaction patterns. This would validate whether the technical architecture translates to practical utility.