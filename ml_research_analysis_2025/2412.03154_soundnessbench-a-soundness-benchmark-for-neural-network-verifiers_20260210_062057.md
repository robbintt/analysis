---
ver: rpa2
title: 'SoundnessBench: A Soundness Benchmark for Neural Network Verifiers'
arxiv_id: '2412.03154'
source_url: https://arxiv.org/abs/2412.03154
tags:
- verifiers
- counterexamples
- instances
- hidden
- verification
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces SoundnessBench, a new benchmark designed
  to test the soundness of neural network verifiers by incorporating hidden counterexamples.
  Existing benchmarks often lack ground-truth for hard instances, making it difficult
  to validate verifier soundness.
---

# SoundnessBench: A Soundness Benchmark for Neural Network Verifiers

## Quick Facts
- arXiv ID: 2412.03154
- Source URL: https://arxiv.org/abs/2412.03154
- Reference count: 27
- Primary result: Benchmark successfully identifies soundness bugs in 3 verifiers using hidden counterexamples that evade adversarial attacks

## Executive Summary
This paper introduces SoundnessBench, a novel benchmark designed to test the soundness of neural network verifiers by incorporating hidden counterexamples that evade adversarial attacks. Unlike existing benchmarks that lack ground-truth for hard instances, SoundnessBench uses a two-part training framework to produce neural networks with deliberately inserted counterexamples that remain undetected by standard falsification methods. The benchmark includes 26 diverse models with various architectures, activation functions, and input sizes, totaling 206 hidden counterexamples. Experimental results demonstrate that SoundnessBench successfully identifies bugs in three well-established verifiers when they incorrectly claim verification of instances known to have counterexamples.

## Method Summary
SoundnessBench employs a two-objective training framework to generate neural networks with hidden counterexamples. The first objective uses a margin-based loss to force the model to misclassify pre-defined counterexamples with bounded confidence (λ=0.01), while the second objective applies adversarial training using a perturbation sliding window to maintain robustness against other attacks. The sliding window maintains perturbations from the most recent 300 epochs to prevent catastrophic forgetting during training. After training, strong attacks (PGD with 1000 restarts, 5000 steps; AutoAttack) filter out instances where hidden counterexamples are discovered, resulting in the final benchmark set. The benchmark includes 26 models across 9 architectures, with half unverifiable instances containing hidden counterexamples and half regular instances for completeness testing.

## Key Results
- Successfully identifies soundness bugs in α,β-CROWN (100% false verification rate on CNN AvgPool unverifiable instances), NeuralSAT, and Marabou 2023
- Training method produces 6.2-10.0 hidden counterexamples per unverifiable instance setting
- Mean ℓ∞ distance of hidden counterexamples to decision boundary is ~10⁻⁴, confirming they are near but not on the boundary
- Most hidden counterexamples evade falsification by existing verifiers (falsification rates in Table 5)
- Bug fixes in α,β-CROWN after identification through benchmark testing

## Why This Works (Mechanism)

### Mechanism 1
- Claim: A two-objective training framework produces neural networks with hidden counterexamples that evade standard adversarial attack detection.
- Mechanism: The training simultaneously optimizes (1) a margin objective L_CEX that forces the model to misclassify pre-defined counterexamples with bounded margin λ, and (2) an adversarial training objective L_adv using a perturbation sliding window. The margin objective caps the misclassification confidence at λ to prevent counterexamples from becoming too obvious, while adversarial training eliminates other easily-found counterexamples.
- Core assumption: Counterexamples close to decision boundaries (mean ℓ∞ distance ~10⁻⁴) will evade gradient-based attacks like PGD while remaining valid falsifying inputs.
- Evidence anchors:
  - [Section 3.4] "We design a margin objective instead to achieve 0 ≤ f_yCEX(x_CEX) - f_y(x_CEX) ≤ λ"
  - [Section 4.2] "The mean ℓ∞ distances to the decision boundary are 2.6×10⁻⁴ for the CNN AvgPool model"
  - [corpus] Related work "No Soundness in the Real World" discusses floating-point soundness issues but not this counterexample-planting approach
- Break condition: If adversarial attacks improve significantly, or if margin λ is set too large (making counterexamples detectable), the hidden counterexamples may be discovered during preliminary falsification checks.

### Mechanism 2
- Claim: The perturbation sliding window stabilizes training under conflicting objectives.
- Mechanism: Instead of generating a single new perturbation δ per epoch (which disrupts previously-learned robustness), the method maintains W^(i) = {δ₁, ..., δ_w} containing perturbations from the most recent w epochs (w=300). L_adv averages over all perturbations in the window, preventing catastrophic forgetting of earlier training states.
- Core assumption: Accumulating diverse perturbations creates a more complete robustness signal that can coexist with the counterexample-planting objective.
- Evidence anchors:
  - [Section 3.4] "We find that training struggles to converge, if we only use the new perturbation"
  - [Table 7] Window size 1 produces 0 hidden counterexamples; window size 300 produces 6.2-10.0
  - [corpus] No direct corpus comparison available for this specific technique
- Break condition: If window size w is too small, training instability prevents hidden counterexample formation; if too large, computational cost may become prohibitive.

### Mechanism 3
- Claim: Known ground-truth counterexamples enable direct soundness verification by checking if verifiers incorrectly claim "verified" status.
- Mechanism: Each unverifiable instance (f, x₀, y, ε) has a planted counterexample x_CEX = x₀ + δ_CEX where the model misclassifies. If a verifier returns "verified" for this instance, the verifier is demonstrably unsound—the ground-truth counterexample violates the claimed property.
- Core assumption: Verifiers will not偶然 find the specific hidden counterexample during their internal falsification attempts.
- Evidence anchors:
  - [Abstract] "It can identify false verification claims when hidden counterexamples are known to exist"
  - [Table 4] α,β-CROWN incorrectly verified 100% of CNN AvgPool unverifiable instances (ε=0.2, 3×5×5)
  - [corpus] Related paper "Proof Minimization in Neural Network Verification" discusses proof-based reliability but not benchmark-driven testing
- Break condition: If verifiers incorporate attacks matching the training attack (PGD with 150 restarts, 150 steps), counterexamples may no longer be hidden; the benchmark requires maintenance as attacks evolve.

## Foundational Learning

- Concept: **Neural Network Verification Formulation (Eq. 1)**
  - Why needed here: Understanding that verification asks "∀x ∈ C, h(x) > 0" is essential to grasp what "soundness" means—a verifier is sound iff whenever it claims this holds, no counterexample exists.
  - Quick check question: If a verifier claims "verified" for property ∀x ∈ B(x₀, ε): f_y(x) > f_i(x), but you find one input x' where f_y(x') ≤ f_i(x'), what does this mean for the verifier?

- Concept: **Adversarial Training as Min-Max Optimization (Eq. 2)**
  - Why needed here: The benchmark's second training objective adapts adversarial training. Understanding min_θ E[max_δ L_CE(f_θ(x+δ), y)] clarifies why the method needs the sliding window—the inner max is approximated by attack sampling.
  - Quick check question: Why does adversarial training make counterexamples harder to find?

- Concept: **Soundness vs. Completeness in Verification**
  - Why needed here: A verifier can be sound (never falsely claims verified) but incomplete (times out or says "unknown"). This benchmark specifically tests soundness bugs, not completeness limitations.
  - Quick check question: If a verifier always returns "unknown," is it sound? Is it useful?

## Architecture Onboarding

- Component map: Data Generation Module -> Training Loop -> Filtering Stage -> Benchmark Instance
- Critical path: Data generation → Two-objective training (5000 epochs, cyclic LR) → Strong attack filtering → Benchmark assembly → Verifier testing
- Design tradeoffs:
  - Small models (5k-3M params, 25-75 dim inputs) chosen to maximize verifier participation; larger models would timeout or be unsupported
  - Margin λ=0.01 balances counterexample validity vs. detectability
  - Half unverifiable / half regular instances prevent verifiers from gaming the benchmark
- Failure signatures:
  - Training produces 0 hidden CEX: Likely window size too small or margin λ too aggressive
  - Verifier finds CEX that should be hidden: Training attacks weaker than verifier's attacks—need to strengthen training attack parameters
  - No bugs detected in any verifier: Either verifiers are sound (unlikely) or hidden CEX are being falsified—check Table 5 falsification rates
- First 3 experiments:
  1. **Reproduce hidden CEX generation**: Train CNN 1 Conv with ε=0.2, input 1×5×5; verify that ≥7/10 instances produce hidden CEX (Table 2). Ablate margin objective (set λ→∞) to confirm drop to ~1.2 hidden CEX (Table 7).
  2. **Test verifier soundness**: Run α,β-CROWN (ABC-A configuration) on CNN AvgPool ε=0.2, 3×5×5 unverifiable instances; expect 100% "verified" claims (Table 4) indicating the auto_LiRPA bug.
  3. **Validate bug fix**: After auto_LiRPA patch, re-run the same instances; "verified" rate should drop to 0%. This confirms the benchmark correctly identified a real implementation bug rather than a false positive.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the benchmark's training pipeline be adapted to maintain "hidden" counterexamples against future, stronger adversarial attack algorithms that may uncover currently hidden violations?
- Basis in paper: [explicit] The authors note that while they use PGD and AutoAttack, "future work may consider new attacks," and they suggest monitoring new attacks to test resilience or adapt training if counterexamples are found.
- Why unresolved: Adversarial attack development is a rapidly evolving field; robustness against current standard attacks (PGD, AutoAttack) does not guarantee robustness against future algorithms.
- What evidence would resolve it: A modified training procedure that successfully generates counterexamples resilient to a newly developed, state-of-the-art adversarial attack.

### Open Question 2
- Question: Can the methodology for planting hidden counterexamples be scaled to larger, more complex models (e.g., large vision transformers or LLMs) without reducing the problem to a trivial pass/fail for the verifier?
- Basis in paper: [explicit] The authors identify the "relatively small-scale" nature of their models as a limitation and note that on larger settings (like MNIST), verifiers often fail to expose bugs because the verification task becomes too difficult.
- Why unresolved: There is a trade-off where larger models complicate verification so much that verifiers timeout or fail without revealing soundness bugs, limiting the benchmark's utility for large-scale systems.
- What evidence would resolve it: A variant of SoundnessBench applied to large-scale models where verifiers still return verdicts (verified/falsified) rather than timing out, while the benchmark still successfully flags soundness violations.

### Open Question 3
- Question: What is the specific root cause of the unsoundness in Marabou 2023 when interacting with the Gurobi solver on Vision Transformer (ViT) and Tanh models?
- Basis in paper: [explicit] The paper states that while SoundnessBench identified bugs in Marabou, the "authors of Marabou conjectured that there might be bugs in the interaction with the quadratic programming engine in Gurobi, but the specific cause remains unknown."
- Why unresolved: The benchmark successfully triggered the bug (false verification), but the internal diagnosis of the verifier's code and its interaction with the external solver was not completed in this work.
- What evidence would resolve it: A patch or technical report from the Marabou team identifying the specific logic or constraint handling error in the Gurobi interface that led to the unsound result.

## Limitations
- Benchmark constrained to small models (5k-3M parameters, 25-75 dimensional inputs) to ensure verifier participation, limiting real-world applicability
- Effectiveness depends on attack parameters remaining superior to verifier falsification capabilities; benchmark requires updates as verification technology advances
- Benchmark identifies soundness bugs but does not provide systematic diagnosis of root causes beyond triggering the false verification

## Confidence

- **High confidence**: The core mechanism of planting ground-truth counterexamples and using them to test verifier soundness is technically sound and experimentally validated (α,β-CROWN bug detection).
- **Medium confidence**: The perturbation sliding window technique's contribution to hidden counterexample generation is well-supported by ablation studies (window size 1 vs. 300), though the exact optimal parameters remain empirical.
- **Medium confidence**: The claim that SoundnessBench identifies soundness bugs in multiple verifiers is supported by Table 4 results, though the Marabou 2023 false positives require further investigation to distinguish soundness bugs from other implementation issues.

## Next Checks

1. **Attack parameter sensitivity**: Systematically vary adversarial attack strength during training (PGD restarts/steps) and measure the resulting hidden counterexample discovery rate to establish attack parameter thresholds.

2. **Verifier generalization**: Test SoundnessBench models against verifiers not included in the original study (e.g., ERAN, DeepZ) to evaluate whether soundness bugs are verifier-specific or model-dependent.

3. **Real-world applicability**: Scale the generation method to larger models (1M+ parameters) and evaluate whether the perturbation sliding window technique remains effective at maintaining hidden counterexamples.