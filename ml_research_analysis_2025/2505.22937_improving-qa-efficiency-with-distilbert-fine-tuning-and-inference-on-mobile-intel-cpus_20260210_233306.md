---
ver: rpa2
title: 'Improving QA Efficiency with DistilBERT: Fine-Tuning and Inference on mobile
  Intel CPUs'
arxiv_id: '2505.22937'
source_url: https://arxiv.org/abs/2505.22937
tags:
- inference
- question
- data
- bert
- distilbert
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study presents an efficient transformer-based question-answering
  (QA) model optimized for deployment on a 13th Gen Intel i7-1355U CPU using the Stanford
  Question Answering Dataset (SQuAD) v1.1. Leveraging exploratory data analysis, data
  augmentation, and fine-tuning of a DistilBERT architecture, the model achieves a
  validation F1 score of 0.6536 with an average inference time of 0.1208 seconds per
  question.
---

# Improving QA Efficiency with DistilBERT: Fine-Tuning and Inference on mobile Intel CPUs

## Quick Facts
- **arXiv ID**: 2505.22937
- **Source URL**: https://arxiv.org/abs/2505.22937
- **Reference count**: 4
- **Primary result**: DistilBERT achieves F1 score of 0.6536 with 0.1208s inference time on Intel i7-1355U CPU

## Executive Summary
This study presents an efficient transformer-based question-answering (QA) model optimized for deployment on a 13th Gen Intel i7-1355U CPU using the Stanford Question Answering Dataset (SQuAD) v1.1. Leveraging exploratory data analysis, data augmentation, and fine-tuning of a DistilBERT architecture, the model achieves a validation F1 score of 0.6536 with an average inference time of 0.1208 seconds per question. Compared to a rule-based baseline (F1: 0.3124) and full BERT-based models, the approach offers a favorable trade-off between accuracy and computational efficiency, making it well-suited for real-time applications on resource-constrained systems. Systematic evaluation of data augmentation strategies and hyperparameter configurations provides practical insights into optimizing transformer models for CPU-based inference.

## Method Summary
The approach employs DistilBERT-base-uncased fine-tuned on SQuAD v1.1 with data augmentation via WordNet synonym substitution for context paraphrasing. The model uses 45,000 augmented training examples with hyperparameters including learning rate 2e-5, batch size 8, warmup steps 500, label smoothing 0.05, and 5 training epochs. Training occurs on a Tesla P100 GPU with early stopping (patience=3) and gradient clipping (norm=1.0). The system targets CPU inference on a 13th Gen Intel i7-1355U, achieving F1 score of 0.6536 and inference time of 0.1208 seconds per question.

## Key Results
- Validation F1 score: 0.6536 (accuracy: 0.5040)
- Average inference time: 0.1208 seconds per question on Intel i7-1355U
- Outperforms rule-based baseline (F1: 0.3124) while maintaining computational efficiency
- Demonstrates viable trade-off between accuracy and CPU inference speed for real-time QA

## Why This Works (Mechanism)
The model leverages DistilBERT's knowledge distillation from BERT, reducing parameter count by 40% while maintaining strong performance. The data augmentation strategy increases training diversity through context paraphrasing, improving generalization on limited data. The combination of hyperparameter optimization (label smoothing, warmup steps, gradient clipping) and model architecture selection creates an efficient inference pipeline suitable for CPU deployment without requiring GPU acceleration.

## Foundational Learning
- **DistilBERT**: Smaller, faster BERT variant using knowledge distillation - needed for CPU inference speed, quick check: parameter count ~40% of BERT
- **Knowledge Distillation**: Teacher-student training where smaller model learns from larger one - needed to compress BERT while retaining accuracy, quick check: compare student vs teacher performance
- **WordNet Synonym Substitution**: Lexical augmentation technique using synonym replacement - needed to expand training data diversity, quick check: ensure answer spans remain intact after substitution
- **Label Smoothing**: Regularization technique that softens one-hot labels - needed to prevent overfitting on small datasets, quick check: monitor validation loss for divergence
- **Gradient Clipping**: Technique to prevent exploding gradients by limiting gradient norm - needed for stable training, quick check: verify gradient norms stay below threshold
- **Polynomial Decay Learning Rate Schedule**: Gradual learning rate reduction following polynomial function - needed for fine-tuning convergence, quick check: observe smooth LR decrease over epochs

## Architecture Onboarding

**Component Map**: SQuAD v1.1 data -> Tokenizer (DistilBertTokenizer) -> Data Augmentation (WordNet substitution) -> Model (TFDistilBertForQuestionAnswering) -> Inference Engine -> CPU Output

**Critical Path**: Data loading and tokenization → context augmentation → model inference → answer span extraction → F1 calculation

**Design Tradeoffs**: DistilBERT vs full BERT (40% smaller, faster but slightly lower accuracy), data augmentation vs overfitting (increased diversity but potential semantic drift), label smoothing vs hard labels (regularization vs precise learning signals)

**Failure Signatures**: 
- Validation F1 plateaus below 0.5: likely insufficient augmentation or poor hyperparameter configuration
- Training loss decreases but validation loss increases: overfitting, adjust regularization or augmentation
- Inference time exceeds 0.2s/question: tokenization overhead or inefficient answer extraction

**First Experiments**:
1. Verify tokenization and answer span alignment on 100 SQuAD examples
2. Test WordNet augmentation on 10 contexts, ensuring answer preservation
3. Run single-batch inference with DistilBERT to establish baseline timing

## Open Questions the Paper Calls Out
None

## Limitations
- Data augmentation implementation details (synonym selection criteria, substitution frequency) are underspecified
- Validation F1 of 0.6536 appears lower than typical DistilBERT performance on SQuAD v1.1
- Inference time measurement methodology lacks detail about hardware configuration and tokenization overhead

## Confidence
- **High confidence**: General methodological approach, DistilBERT fine-tuning, SQuAD v1.1 task specification, CPU deployment goal
- **Medium confidence**: Hyperparameter configuration and training procedure descriptions are complete enough for replication
- **Low confidence**: Exact data augmentation implementation and final model performance claims due to missing implementation details

## Next Checks
1. Reimplement WordNet-based context paraphrasing with strict answer span preservation and verify semantic consistency
2. Train specified DistilBERT configuration from scratch and measure validation F1 score
3. Benchmark inference time on identical hardware (13th Gen Intel i7-1355U) using same tokenization and answer extraction pipeline