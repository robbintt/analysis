---
ver: rpa2
title: 'The Climb Carves Wisdom Deeper Than the Summit: On the Noisy Rewards in Learning
  to Reason'
arxiv_id: '2505.22653'
source_url: https://arxiv.org/abs/2505.22653
tags:
- reward
- reasoning
- answer
- score
- figure
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper studies the effect of noisy rewards in post-training
  large language models (LLMs) for reasoning via reinforcement learning (RL). It finds
  that LLMs with strong inherent reasoning abilities are surprisingly robust to substantial
  reward noise: even with 40% of math task rewards manually flipped, a Qwen-2.5-7B
  model converged rapidly, improving MATH-500 accuracy from 5% to over 70%, close
  to the 75.85% achieved with noiseless rewards.'
---

# The Climb Carves Wisdom Deeper Than the Summit: On the Noisy Rewards in Learning to Reason

## Quick Facts
- arXiv ID: 2505.22653
- Source URL: https://arxiv.org/abs/2505.22653
- Reference count: 40
- Primary result: LLMs with strong pre-trained reasoning tolerate up to 40% reward noise while maintaining effective learning, and RPR-based training achieves comparable performance to strict verification.

## Executive Summary
This paper investigates how reinforcement learning with noisy rewards affects post-training large language models for reasoning tasks. Surprisingly, the study finds that models with strong inherent reasoning abilities are remarkably robust to substantial reward noise - even with 40% of rewards flipped, a Qwen-2.5-7B model maintained high performance. The research also demonstrates that training with only reasoning pattern rewards (RPR), without verifying answer correctness, achieves comparable peak performance to strict verification-based training. Furthermore, RPR can calibrate noisy reward models, improving performance on open-ended tasks by up to 30% net win rate.

## Method Summary
The study trains LLMs using vanilla PPO with GAE on math problems and open-ended tasks. For math tasks, they introduce noise by randomly flipping 0-50% of rule-based rewards. For open-ended tasks, they train neural reward models at varying accuracies (65-85%) and test RPR calibration. The RPR approach rewards key reasoning phrases without verifying final answer correctness. They evaluate on MATH-500, GPQA, AIME 2024 for math tasks and HelpSteer3 for open-ended tasks, using both automated and human evaluations.

## Key Results
- Qwen-2.5-7B improved from 5% to over 70% MATH-500 accuracy even with 40% reward noise
- RPR-only training achieved 70.21% accuracy, comparable to strict verification
- RPR calibration improved performance of 65%-accurate reward models, reducing the performance gap from 25% to 8%
- Training collapses only when reward flip rate reaches 50%, making rewards entirely random

## Why This Works (Mechanism)

### Mechanism 1: Reasoning Pattern Robustness to Reward Noise
LLMs with strong pre-trained reasoning can tolerate substantial reward noise (up to 40% flipped rewards) while maintaining effective learning. When incorrect answers are mistakenly rewarded, the model still receives useful signal about how to reason, reinforcing reasoning patterns that increase correct-answer probability over rollouts. This works because incorrect answers often contain valuable reasoning patterns. Break condition: 50% flip rate causes training collapse as rewards become random.

### Mechanism 2: Reasoning Pattern Reward (RPR) Sufficiency
Rewarding only the presence of key reasoning phrases achieves peak performance comparable to strict correctness verification. Pre-trained models already possess reasoning capabilities; RL with RPR triggers and reinforces existing reasoning patterns rather than teaching new knowledge. Key phrases like "first, I need to" and "we know that" act as reasoning triggers. Break condition: Without answer verification, models may suffer from "overthinking" - excessively long reasoning chains that exceed context limits.

### Mechanism 3: RPR Calibration of Noisy Reward Models
RPR can calibrate noisy neural reward models by compensating for false negatives, improving downstream performance on open-ended tasks. When reward models assign low scores to responses with good reasoning patterns, RPR provides compensatory reward signal. This prevents training collapse and enables reasoning emergence, especially in smaller models. Break condition: Setting RPR weight too high (α>0.1) overemphasizes process rewards and causes performance collapse.

## Foundational Learning

- **Proximal Policy Optimization (PPO) with GAE**: The paper uses vanilla PPO with GAE (λ=1, γ=1) for all RL experiments. Understanding advantage estimation is essential for interpreting training dynamics.
  - Quick check question: Can you explain why GAE with λ=1 provides lower-variance advantage estimates compared to single-step TD learning?

- **Reward Model Training for Preference Learning**: Open-ended tasks use neural reward models trained on preference data. Understanding how RM accuracy affects downstream performance is central to the paper.
  - Quick check question: Why might a reward model with 65% accuracy be significantly worse than one with 85% accuracy, beyond the raw accuracy difference?

- **Chain-of-Thought Reasoning in LLMs**: RPR is built on the hypothesis that reasoning patterns in CoT traces are valuable regardless of answer correctness.
  - Quick check question: What linguistic markers in a CoT trace would indicate structured reasoning vs. unstructured generation?

## Architecture Onboarding

- **Component map**:
```
Base LLM (Qwen-2.5-7B/3B)
    ↓
Prompt Template (with abı̂...⦟ tags)
    ↓
Response Generation → Reward Computation
    ↓                    ↓
   Rollouts        [Option A] Rule-based Verification (math)
                    [Option B] Neural Reward Model (open-ended)
                    [Option C] RPR Keyword Matching
                    [Option D] Calibrated RM + RPR
    ↓
PPO Update (actor LR: 1e-6, critic LR: 5e-6)
    ↓
Trained Model
```

- **Critical path**:
  1. Pre-trained model must have strong foundational reasoning (Qwen > Llama in this study)
  2. Reward signal quality threshold: >65% accuracy for neural RMs; <50% flip rate for rule-based
  3. RPR weight calibration: α=0.1 worked best; higher values caused overthinking

- **Design tradeoffs**:
  - **Accuracy vs. robustness**: Higher RM accuracy helps, but 75% vs 85% shows diminishing returns
  - **RPR-only vs. verification**: RPR-only achieves peak performance but risks overthinking collapse; verification provides stability
  - **Model scale**: Smaller models (3B) require RPR calibration to reason effectively; vanilla RM causes collapse

- **Failure signatures**:
  - 50% reward flip rate → training collapse (rewards fully random)
  - RPR-only without verification → overthinking (reasoning exceeds context window)
  - Low-accuracy RM (65%) without calibration → significant downstream performance drop
  - Weak base model (Llama-3.1-8B) → poor reasoning even in noiseless settings

- **First 3 experiments**:
  1. **Reproduce noise robustness**: Train Qwen-2.5-7B on math tasks with 0%, 20%, 40%, 50% reward flip rates. Verify MATH-500 scores degrade gracefully until 50%.
  2. **Validate RPR-only training**: Implement keyword-based RPR (use Figure 9 code as reference) with 40 phrases. Compare peak performance vs. verification-based training. Monitor for overthinking.
  3. **Test RPR calibration on open-ended tasks**: Train a reward model at varying accuracies (65%-85%) on HelpSteer3. Add RPR calibration (τ=0.5, α=0.1) and compare downstream win rates using GPT-4o evaluation.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can noisy reward models be effectively calibrated when they produce false positive rewards?
- Basis in paper: [explicit] Page 8 states: "This remains an open research question: how can we effectively calibrate an RM when the noisy reward is a false positive?"
- Why unresolved: The authors only implemented compensation for false negatives. They note that discounting false positives is complex because determining the appropriate penalty factor without suppressing valid high-reward responses is difficult.
- What evidence would resolve it: A calibration method that successfully identifies and discounts rewards for "objectively" poor responses without causing training collapse or over-penalizing correct reasoning.

### Open Question 2
- Question: How can the "overthinking" phenomenon be mitigated when training solely with Reasoning Pattern Rewards (RPR)?
- Basis in paper: [inferred] Page 5 observes that RPR-only training leads to performance degradation because models engage in excessive reasoning cycles that exceed context limits, a problem not fully solved by repetition penalties.
- Why unresolved: The authors found that models circumvent simple repetition penalties by rephrasing thoughts, leading to collapsed final answers despite correct intermediate reasoning.
- What evidence would resolve it: A regularization technique or reward modification that maintains the benefits of RPR while ensuring the reasoning process terminates concisely to generate a final answer.

### Open Question 3
- Question: How do accuracy and variance independently contribute to the failure modes of noisy reward models?
- Basis in paper: [explicit] Page 7 notes that "disentangling the individual effects of these factors remains a challenge, as training RMs with both targeted accuracy and targeted variance for clean ablations is difficult."
- Why unresolved: The study observed that lower-accuracy RMs also exhibited lower variance, making it unclear if the poor performance of the 65%-accurate RM was due to misclassification scores or lack of reward distinction.
- What evidence would resolve it: A controlled ablation study training reward models to achieve specific accuracy levels while independently manipulating score variance.

## Limitations
- Reliance on specific model architectures (Qwen-2.5 series) raises questions about generalizability to other model families
- Limited evaluation of transfer learning - whether reasoning robustness generalizes across task domains
- Unclear long-term stability of models trained with noisy rewards - potential for reward hacking or degradation over extended training

## Confidence

- **High Confidence**: The fundamental finding that LLMs with strong pre-trained reasoning show robustness to moderate reward noise (40% flip rate) - supported by direct experimental evidence
- **Medium Confidence**: The sufficiency of RPR-only training for achieving peak performance - while demonstrated, the risk of overthinking and context window limitations suggests potential instability
- **Low Confidence**: The broader implications for reward model calibration in open-ended tasks - limited to one dataset (HelpSteer3) with GPT-4o evaluation that may not reflect human preferences

## Next Checks
1. **Cross-model generalization test**: Repeat the noisy reward experiments with Llama-3.1, Mistral, and other architectures to validate that reasoning robustness is model-agnostic
2. **Long-term stability analysis**: Run extended training (10x current duration) on RPR-only models to detect performance degradation or reward hacking patterns
3. **Domain transfer validation**: Apply RPR-calibrated reward models trained on HelpSteer3 to different open-ended domains (creative writing, code generation) to test transferability of calibration benefits