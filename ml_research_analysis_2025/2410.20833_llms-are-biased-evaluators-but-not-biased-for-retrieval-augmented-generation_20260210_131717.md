---
ver: rpa2
title: LLMs are Biased Evaluators But Not Biased for Retrieval Augmented Generation
arxiv_id: '2410.20833'
source_url: https://arxiv.org/abs/2410.20833
tags:
- passages
- llms
- passage
- preference
- llama
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ''
---

# LLMs are Biased Evaluators But Not Biased for Retrieval Augmented Generation

## Quick Facts
- arXiv ID: 2410.20833
- Source URL: https://arxiv.org/abs/2410.20833
- Authors: Yen-Shan Chen; Jing Jin; Peng-Ting Kuo; Chao-Wei Huang; Yun-Nung Chen
- Reference count: 7
- Primary result: LLM self-preference bias disappears in RAG settings when evaluating passage relevance for answering specific questions.

## Executive Summary
This paper investigates whether LLMs exhibit self-preference bias when evaluating or generating responses using their own generated content in Retrieval Augmented Generation (RAG) systems. Surprisingly, the study finds that while LLMs show strong self-preference bias in normal quality evaluation tasks, this bias is significantly reduced or eliminated when the task shifts to evaluating passage suitability for answering specific questions. The research demonstrates that LLMs maintain fairness in generation phases, prioritizing factual accuracy over authorship, and can even discriminate between factual and non-factual passages regardless of their prior knowledge.

## Method Summary
The study evaluates five LLMs (GPT-3.5-turbo, GPT-4o-mini, Gemini-2.0-flash, LLaMA-3.1-8B-Instruct, Mistral-7B-Instruct-v0.3) across two RAG phases using datasets from NQ, MS MARCO, and TriviaQA (700 samples each). The research measures self-preference bias through pointwise reranking (rating passage relevance 1-5) and pairwise reading comprehension (selecting which passage to use for answering). Passages are categorized as human-written or LLM-generated, with ground-truth answers identified via substring matching. Knowledge levels are assessed by asking each question five times to categorize full, partial, or no prior knowledge.

## Key Results
- Self-preference bias is significantly reduced or reversed (showing preference for others) in RAG settings compared to normal evaluation tasks
- LLMs prioritize factual accuracy over authorship when generating answers, with factual preference averaging over 80% versus self-preference under 47%
- Even without prior knowledge, LLMs maintain 65-70% accuracy in discriminating factual from non-factual passages based on contextual coherence
- Positional bias remains stronger than self-preference, with order effects persisting across all experimental conditions

## Why This Works (Mechanism)

### Mechanism 1: Task-Specific Inhibition of Self-Preference
The shift from subjective quality assessment to objective relevance assessment triggers a change in "mindset." When evaluating writing quality, LLMs rely on stylistic familiarity, activating self-preference. However, when evaluating suitability for answering a specific question, the model shifts to a relevance-based attention mechanism that prioritizes information utility over authorship similarity. This mechanism fails if the prompt doesn't explicitly anchor evaluation to a specific query.

### Mechanism 2: Factuality-First Hierarchy in Generation
During answer generation with multiple references, LLMs prioritize factual accuracy over order or authorship. The decoding process is heavily influenced by ground-truth entities or semantic consistency with the query, where the pull of correct factual information overrides the pull of self-generated style. This mechanism may degrade when factual passages are semantically complex or contradict strong internal model priors.

### Mechanism 3: Context-Driven Factual Discrimination (w/o Prior Knowledge)
LLMs can identify and prefer factual passages over non-factual ones even without internal knowledge of the answer, by utilizing contextual coherence to detect structural or semantic validity. The model doesn't need to "know" the answer but can detect irregularities in false passages or higher coherence in true passages. This mechanism fails when false information is highly plausible and stylistically superior.

## Foundational Learning

- **Pointwise Reranking vs. Generation**: The paper proves bias behaves differently in these two RAG phases, requiring different trust and bias mitigation approaches. Quick check: Does your system use an LLM to score isolated documents (Pointwise) or to synthesize an answer from a set (Generation)?

- **Positional Bias (Order Effect)**: While self-preference is low, positional bias remains high (Order > Self). Understanding this is critical for designing the input context window. Quick check: If you swap Passage A and Passage B in your prompt, does the LLM's answer change?

- **Stylistic vs. Factual Evaluation**: The core finding relies on the model ignoring style. If your use case depends on stylistic matching (e.g., creative writing RAG), these safety guarantees may not apply. Quick check: Is your prompt asking "Is this well-written?" (Bias risk) or "Does this answer the question?" (Bias mitigated)?

## Architecture Onboarding

- **Component map**: Passage Bank {Human-Written, LLM-Generated} × {True, False} → Evaluator (Reranker: LLM prompting for "Suitability for Answering" 1-5 scale) → Generator (LLM prompted for "Reading Comprehension" with pairwise passages) → Controller (logic to randomize order to mitigate order bias)

- **Critical path**: Prompt Engineering is the control lever. The prompt must explicitly ask for "suitability for answering the question" to suppress self-preference. Factual verification relies on contrast between provided passages; single-source generation is riskier.

- **Design tradeoffs**: 
  - Prompt Specificity: Highly specific RAG prompts eliminate self-preference but may introduce "alignment tax" or rigidity
  - Efficiency vs. Robustness: Using LLM as Reranker is safer but adds latency
  - Generation Style: "Gen-by-QA" synthetic passages reintroduce strong bias, while rewriting human text does not

- **Failure signatures**: 
  - "Gen-by-QA" Trap: If retrieval corpus is synthetic, LLM shows strong preference for that style
  - Hallucination Loops: If model has no prior knowledge and both provided passages are false or low quality, factuality discrimination fails

- **First 3 experiments**:
  1. Sanity Check: Run your LLM on retrieval docs using "Rate Quality" prompt vs. "Rate Relevance" prompt to confirm self-preference delta
  2. Order Randomization A/B Test: Feed Generator identical passage pairs in swapped orders to quantify positional bias
  3. Knowledge Stress Test: Query model on unknown facts with one factual vs. one hallucinated context to verify context-driven discrimination

## Open Questions the Paper Calls Out

### Open Question 1
How do LLMs' internal mechanisms, such as attention patterns or decoding paths, shift when switching between subjective evaluation (style-based) and objective relevance assessment (fact-based)? The paper hypothesizes a "mindset" change and calls for mechanistic interpretability tools to understand how these task-dependent behaviors arise. Comparative analysis of attention attribution maps and decoding traces when processing "Normal Setting" versus "RAG Setting" prompts would resolve this.

### Open Question 2
To what extent can sophisticated or domain-specific prompt engineering modulate or amplify self-preference bias within RAG frameworks? The study intentionally used general, concise prompts to minimize bias, leaving the impact of complex or tailored prompting unexplored. Ablation studies using varied prompt styles on the same datasets would measure changes in self-preference scores.

### Open Question 3
Do the observed lack of self-preference and the hierarchy of biases (factuality > order > self) generalize to significantly larger model scales or different architectural families? The Limitations section notes that different architectures or more recent models may exhibit unique preferences not observed in the five tested models. Replication on frontier models and distinct architectures would resolve this.

## Limitations
- The study tested only five specific LLMs and architectures, leaving generalization to larger models or different families uncertain
- The research used general, concise prompts following LangChain RAG procedures, not exploring sophisticated or domain-specific prompt engineering
- The experimental design may not capture all real-world RAG scenarios, particularly those involving creative or stylistic tasks

## Confidence
- High: The core finding that self-preference bias disappears in RAG settings when evaluating passage relevance for answering specific questions
- Medium: The mechanism explanations for why bias shifts occur (mindset change, factuality hierarchy)
- Low: Generalization to significantly larger models or different architectural families beyond the five tested

## Next Checks
1. Verify your system's prompt explicitly asks for "suitability for answering the question" rather than generic quality assessment
2. Test positional bias by swapping passage order in your generation prompts and measuring answer changes
3. Validate factuality discrimination by providing passages on topics your model definitely doesn't know and checking if it can identify the factual one based on context alone