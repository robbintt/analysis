---
ver: rpa2
title: Your Text Encoder Can Be An Object-Level Watermarking Controller
arxiv_id: '2503.11945'
source_url: https://arxiv.org/abs/2503.11945
tags:
- watermarking
- image
- watermark
- generation
- accuracy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a novel approach for object-level watermarking
  in text-to-image generation using Latent Diffusion Models (LDMs). The method fine-tunes
  only text token embeddings of a special watermarking token W, enabling selective
  watermarking of specific objects or regions within generated images while preserving
  non-watermarked areas.
---

# Your Text Encoder Can Be An Object-Level Watermarking Controller

## Quick Facts
- arXiv ID: 2503.11945
- Source URL: https://arxiv.org/abs/2503.11945
- Reference count: 40
- Primary result: Object-level watermarking via fine-tuning only text token embeddings, achieving 99% bit accuracy with 105× parameter reduction

## Executive Summary
This paper presents a novel approach for object-level watermarking in text-to-image generation using Latent Diffusion Models (LDMs). The method fine-tunes only text token embeddings of a special watermarking token W*, enabling selective watermarking of specific objects or regions within generated images while preserving non-watermarked areas. By integrating watermarking early in the text encoding stage, the approach improves robustness against adversarial perturbations and common image processing attacks. The method achieves 99% bit accuracy (48 bits) with a 105× reduction in model parameters compared to baselines, enabling efficient and effective watermarking. Object-level watermarking is controlled via cross-attention maps, allowing precise localization without requiring additional segmentation masks. The approach demonstrates compatibility with various LDM pipelines, including personalized models and textual inversion.

## Method Summary
The method introduces a new token W* to the text encoder's vocabulary with a learnable embedding (768 parameters). During training, images are encoded to latents, noise is added for τ* = 8 timesteps, and denoising is performed with CFG while optimizing only the W* embedding. The loss combines a watermark loss (BCE against a 48-bit key via a differentiable detector) and a latent matching loss to preserve image quality. Object-level control is achieved by overlaying the watermark token's cross-attention maps onto target object tokens during inference, localizing the watermark spatially. The approach requires no modifications to UNet or VAE weights, making it parameter-efficient and broadly compatible.

## Key Results
- Achieves 99% bit accuracy (48 bits) on MS-COCO and WikiArt test sets
- Reduces parameters by 105× compared to baseline methods (768 vs 10⁵+)
- Maintains high imperceptibility (PSNR 40.92, SSIM 0.97) while improving robustness against adversarial attacks
- Enables object-level localization without segmentation masks via cross-attention map manipulation

## Why This Works (Mechanism)

### Mechanism 1
Fine-tuning only the embedding vector of a single pseudo-token W* suffices to embed robust watermarks during text-to-image generation. A new token W* is added to the text encoder's vocabulary with a learnable embedding (768 parameters). When W* appears in a prompt, its embedding influences the cross-attention conditioning throughout denoising, steering latents toward watermark-embedded outputs. The embedding is optimized via gradient descent through a differentiable watermark detector. Core assumption: Text encoder embeddings have sufficient influence over pixel-level generation outcomes to carry a detectable watermark signal without modifying UNet or VAE weights.

### Mechanism 2
Cross-attention maps between text tokens and UNet features enable object-level watermark localization without segmentation masks. During denoising, cross-attention maps M_i for each text token are extracted. For a target object token P_i, its attention map M_Pi is combined with the watermark token's attention map M_W* via weighted overlay: M_Pi ← (1−α)·M_Pi + α·M_W*. This steers the watermark signal spatially to regions the model attends to for token P_i. Core assumption: Cross-attention maps provide accurate spatial localization of objects specified by text tokens.

### Mechanism 3
Embedding the watermark at timesteps near the VAE decoder (τ* = 8) balances imperceptibility and robustness; latent trajectory alignment preserves image quality. During training, only τ* = 8 forward-noise steps are added before denoising begins. The latent matching loss L_z = min ||z*_t − z'_t(W*)||² aligns watermarked latents with non-watermarked trajectories, preventing visible corruption while the watermark loss L_w optimizes bit accuracy via BCE. Core assumption: Structural/image quality is primarily determined in early denoising steps; later steps can accommodate watermark perturbations without perceptual degradation.

## Foundational Learning

- **Concept:** Latent Diffusion Model (LDM) Architecture
  - **Why needed here:** The entire method operates within the LDM pipeline (text encoder → UNet denoiser → VAE decoder); understanding where and how conditioning enters is essential.
  - **Quick check question:** Trace the path from text prompt to generated image: which components are frozen in this method, and which are trained?

- **Concept:** Cross-Attention in Diffusion UNets
  - **Why needed here:** Object-level control depends on extracting and manipulating cross-attention maps; without this, you cannot localize watermarks.
  - **Quick check question:** Given prompt "A [cat W*] on a sofa," which attention maps would you extract, and how would you combine them to watermark only the cat?

- **Concept:** Blind Watermarking Detection
  - **Why needed here:** The method uses a blind detector requiring only the watermarked image—no original image or metadata. This affects evaluation and deployment.
  - **Quick check question:** What information does the detector Dw require at inference time? What does this imply for real-world deployment?

## Architecture Onboarding

- **Component map:**
  - Text Encoder (CLIP ViT-L/14): Contains frozen weights + learnable W* token embedding (768 params)
  - UNet: Frozen; produces cross-attention maps M_t during denoising
  - VAE Encoder/Decoder: Frozen; converts images ↔ latents
  - Watermark Detector Dw: External differentiable detector (from AquaLoRA [7]); takes decoded image, outputs predicted bit string

- **Critical path:**
  1. Add W* token to text encoder vocabulary with randomly initialized embedding
  2. Training: Image → VAE encoder → latent z_0 → add τ* = 8 noise steps → denoise with CFG → VAE decoder → I_w
  3. Compute losses: L_w = BCE(Dw(I_w), m) + L_z = ||z*_t − z'_t(W*)||^2
  4. Backprop through frozen UNet/VAE to update only W* embedding
  5. Inference: Prompt with "[object W*]" → text encoder → attention overlay during denoising → watermarked image

- **Design tradeoffs:**
  - τ* (timestep): Lower → better PSNR, worse robustness; Higher → worse PSNR, better robustness. Paper uses τ* = 8 for SD v1.5.
  - Latent loss weight β: Higher β → better image quality but may dilute watermark signal strength
  - Multi-object watermarking: Non-overlapping objects maintain ~90% bit accuracy; 40%+ overlap drops accuracy to ~79% (Table 2)

- **Failure signatures:**
  - Low bit accuracy on clean images: W* embedding undertrained; increase training iterations or check detector compatibility
  - Visible artifacts in watermarked regions: β too low; increase latent loss weight or reduce α in attention overlay
  - Watermark bleeds outside object: Attention maps imprecise; ablate with SAM masks (Fig. 5) or reduce overlay strength
  - Bit accuracy drops with JPEG/resize: Robustness insufficient; consider increasing τ* or retraining with stronger augmentation

- **First 3 experiments:**
  1. **Token convergence test:** Train W* on 500 MS-COCO images for 2 GPU hours; monitor bit accuracy and PSNR. Target: >95% bit accuracy, >38 PSNR on held-out prompts.
  2. **Object localization validation:** Generate 20 images with single-object prompts like "A [car W*] on street"; visualize attention maps and watermark heatmaps. Verify spatial precision (watermark concentrated in target region).
  3. **Robustness stress test:** Apply JPEG (Q=75), crop (50%), rotate (25°) to 50 watermarked images; measure bit accuracy degradation. Baseline: <5% accuracy drop per attack type.

## Open Questions the Paper Calls Out

### Open Question 1
How can object-level watermarking precision be maintained when cross-attention maps exhibit spillover beyond intended object boundaries? The current solution requires external segmentation models (SAM), which the authors note "could be undesirable" for a lightweight, plug-and-play system. An internal mechanism to refine attention maps during generation that maintains high bit accuracy without external supervision would resolve this.

### Open Question 2
Can the detection performance for multi-object watermarking be improved when the spatial overlap between target objects is significant? The current cross-attention overlay method suffers from interference when embedding multiple distinct watermark tokens in shared latent regions. A modification to the attention overlay algorithm that sustains high bit accuracy (>90%) in generated images with objects overlapping by >50% would resolve this.

### Open Question 3
Is the empirically determined optimal timestep (τ* = 8) universally robust across varying prompt complexities and distinct Latent Diffusion Model architectures? A fixed timestep may not generalize well to all noise schedules or semantic densities found in different personalized models. An ablation study demonstrating that a fixed τ* yields consistent bit accuracy and imperceptibility across diverse generative backbones would resolve this.

## Limitations
- Watermark localization precision degrades with overlapping objects (>40% overlap drops accuracy to ~79%)
- Method depends on external detector (AquaLoRA) treated as black box component
- Optimal timestep τ* = 8 is specific to Stable Diffusion v1.5 and may not generalize to other architectures

## Confidence

**High Confidence**
- Core mechanism of fine-tuning only W* token embedding (768 parameters) while freezing all other model weights is clearly described and theoretically sound
- Watermark detection accuracy claims (≥99% bit accuracy) are supported by empirical results on the test set
- Parameter efficiency claim (105× reduction compared to baselines) is mathematically verifiable from the described approach

**Medium Confidence**
- Imperceptibility metrics (PSNR 40.92, SSIM 0.97) are reported but depend on specific evaluation methodology
- Robustness claims against various attacks are demonstrated but exact attack parameters and evaluation protocol are not fully specified
- Object-level localization effectiveness is demonstrated but with acknowledged limitations for overlapping objects

**Low Confidence**
- Generalization to unseen object types beyond MS-COCO training distribution
- Performance on non-SD v1.5 architectures without re-tuning τ*
- Real-world deployment scenarios with varying image qualities and compression levels

## Next Checks

1. **Attention Map Fidelity Validation:** Generate 50 images with single-object prompts containing W*. Extract and visualize cross-attention maps for both the object token and W* at multiple timesteps. Quantify spatial overlap between attention peaks and object boundaries using ground truth segmentation masks. Target: >80% attention mass within object boundaries for non-overlapping objects.

2. **Detector Independence Test:** Implement a simple blind watermark detector (e.g., average pixel intensity in watermark-localized regions) and retrain the W* embedding using this detector. Compare bit accuracy against the AquaLoRA detector on the same 100 held-out images. Target: bit accuracy within 5% of AquaLoRA performance to establish detector independence.

3. **Architecture Transferability:** Apply the exact method (τ* = 8, same training procedure) to a different LDM architecture (e.g., SDXL or SD 2.1). Train for the same 2 GPU hours and measure: (a) bit accuracy on 100 generated images, (b) PSNR/SSIM relative to non-watermarked outputs, and (c) optimal timestep identification through systematic search (τ = 2 to 16). Target: bit accuracy >90% and PSNR within 2 dB of original architecture results.