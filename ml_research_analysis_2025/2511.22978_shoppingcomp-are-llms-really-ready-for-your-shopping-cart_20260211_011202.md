---
ver: rpa2
title: 'ShoppingComp: Are LLMs Really Ready for Your Shopping Cart?'
arxiv_id: '2511.22978'
source_url: https://arxiv.org/abs/2511.22978
tags:
- product
- products
- user
- rubric
- safety
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces ShoppingComp, a benchmark designed to evaluate
  large language model-powered shopping agents on three core capabilities: precise
  product retrieval, expert-level report generation, and safety-critical decision
  making. The benchmark includes 145 instances and 558 scenarios curated by 35 experts
  to reflect authentic shopping needs, with tasks requiring complex multi-constraint
  reasoning, multi-product justification reports, and explicit safety awareness.'
---

# ShoppingComp: Are LLMs Really Ready for Your Shopping Cart?

## Quick Facts
- **arXiv ID**: 2511.22978
- **Source URL**: https://arxiv.org/abs/2511.22978
- **Reference count**: 40
- **Primary result**: Even state-of-the-art models achieve only 17.76% Answer Match F1 and 35.42% Safety Rubric Pass Rate, far below human expert performance.

## Executive Summary
This paper introduces ShoppingComp, a benchmark designed to evaluate large language model-powered shopping agents on three core capabilities: precise product retrieval, expert-level report generation, and safety-critical decision making. The benchmark includes 145 instances and 558 scenarios curated by 35 experts to reflect authentic shopping needs, with tasks requiring complex multi-constraint reasoning, multi-product justification reports, and explicit safety awareness. ShoppingComp introduces a unified evaluation framework combining AnswerMatch with fine-grained rubric-based verification to assess product retrieval accuracy, information acquisition effectiveness, reasoning faithfulness, and safety-critical awareness. Empirical results show significant performance gaps: even state-of-the-art models like GPT-5.2 achieve only 17.76% Answer Match F1 and 35.42% Safety Rubric Pass Rate, far below human expert performance of 30.02% and 77.08% respectively, highlighting the challenges remaining for reliable real-world deployment.

## Method Summary
ShoppingComp evaluates LLM shopping agents on three capabilities: (1) Browse Products - retrieve real products satisfying multi-constraint queries, (2) Expert-level Report Generation - produce rubric-aligned recommendation reports with verifiable reasoning, and (3) Safety-Critical Decision Making - identify and avoid product-related hazards. The benchmark uses 145 instances across 558 scenarios (55 synthesized user, 42 expert-authored, 48 safety-critical), each with expert-annotated rubrics, ground-truth product lists, and evidence URLs. Models are equipped with search (SerpAPI) and link reader/summary (Firecrawl) tools. Evaluation uses LLM-as-a-Judge with Gemini-2.5-Pro grounded in Google Search, computing metrics like Answer Match F1, Score of Products (SoP), Scenario Coverage F1, Rationale Validity (RV), and Safety Rubric Pass Rate over five independent runs per test case.

## Key Results
- State-of-the-art models (GPT-5.2, Gemini-3-Pro) achieve only 17.76% Answer Match F1 and 35.42% Safety Rubric Pass Rate, compared to human expert performance of 30.02% and 77.08% respectively.
- Tool augmentation primarily improves recall (from 16.02% to 26.01% for GPT-5.2) but not precision, indicating parametric knowledge insufficiency for long-tail product retrieval.
- Models rarely recognize implicit safety hazards, with safety pass rates <40% even for top models like ChatGPT DeepResearch (37.86%).
- Gemini-3-Pro outperforms GPT-5.2 on answer matching (22.28% vs 17.76% F1) but lags on reasoning faithfulness (RV 25.47% vs 35.42%).

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Rubric-based decomposition transforms ambiguous shopping intents into atomic, verifiable constraints, enabling reliable evaluation of multi-constraint reasoning.
- **Mechanism**: Expert-generated rubrics break complex user needs (e.g., "suitable for elderly") into explicit attribute requirements (e.g., weight limits, safety certifications). The Product Verifier and Report Verifier (both LLM-based) assess each rubric item independently, then aggregate via SoP, RV, or Safety Pass Rate formulas.
- **Core assumption**: Rubrics correctly capture all latent user requirements; verifiers align with human judgment.
- **Evidence anchors**:
  - [abstract] "ShoppingComp introduces a unified evaluation framework combining AnswerMatch with fine-grained rubric-based verification."
  - [Section 4.1] "Product Verifier achieves 81% agreement at rubric level, 84% at question level."
  - [corpus] DeepShop (arXiv 2506.02839) similarly targets complex shopping queries but lacks rubric-based decomposition.
- **Break condition**: If rubrics are incomplete or verifiers exhibit systematic bias, evaluation validity degrades.

### Mechanism 2
- **Claim**: Tool augmentation primarily improves recall, not precision, revealing that parametric knowledge is insufficient for long-tail product retrieval.
- **Mechanism**: Models invoke search (SerpAPI), link reader, and link summary tools. Tools expand candidate space, but models still fail to jointly enforce multiple constraints, leading to high recall with low precision—the opposite of human expert behavior.
- **Core assumption**: Search tools return relevant candidates; models can parse and validate retrieved information.
- **Evidence anchors**:
  - [Section 5.3] "Without tools, recall remains extremely low (1–3%). Gains arise almost entirely from recall improvements."
  - [Table 7] GPT-5.2 improves from 16.02% to 26.01% recall with tools; precision gains are modest.
  - [corpus] ShoppingBench (arXiv 2508.04266) operates in closed-world settings; ShoppingComp's open-world design exposes this recall gap.
- **Break condition**: If search backend quality degrades or tool call budgets are constrained, recall gains diminish sharply.

### Mechanism 3
- **Claim**: Safety-critical evaluation exposes a distinct failure mode—models fail to recognize hazardous product usage even when retrieval succeeds.
- **Mechanism**: Safety questions embed trap rubrics (e.g., "power-vented water heaters cannot be installed in bathrooms"). The Safety Verifier checks whether models explicitly warn or refuse unsafe requests. Models rarely recognize implicit hazards, achieving <40% pass rates.
- **Core assumption**: Safety traps are realistic and representative of deployment risks.
- **Evidence anchors**:
  - [Section 5.2, Table 3] "Even ChatGPT DeepResearch achieves only 37.86% safety pass rate vs 77.08% for experts."
  - [Figure 2, safety instance] Power-vented water heater in bathroom violates national code—models often recommend without warning.
  - [corpus] No corpus papers explicitly address safety-critical evaluation in e-commerce benchmarks.
- **Break condition**: If safety rubrics are too narrow or models learn to game refusal patterns, safety evaluation validity degrades.

## Foundational Learning

- **Concept**: **Multi-hop information retrieval and verification**
  - **Why needed here**: ShoppingComp requires models to retrieve products, cross-reference multiple sources (specs, reviews, manuals), and verify constraints against conflicting evidence.
  - **Quick check question**: Can you trace how a model would verify a monitor supports 4K/120Hz on PS5 using manufacturer specs and third-party reviews?

- **Concept**: **LLM-as-a-Judge evaluation design**
  - **Why needed here**: Rubric-based scoring relies on verifiers; understanding bias, agreement rates, and aggregation schemes is critical for interpreting results.
  - **Quick check question**: Why does the Report Verifier use multiplicative aggregation while the Product Verifier uses per-rubric averaging?

- **Concept**: **Constraint satisfaction under noise**
  - **Why needed here**: Real-world product data is noisy, conflicting, and incomplete. Models must reason over uncertainty without violating hard constraints.
  - **Quick check question**: How should a model handle contradictory specifications from a manufacturer page vs. a retailer listing?

## Architecture Onboarding

- **Component map**: Question types (synthesized user, expert-authored, safety-critical) → Rubric generation (expert + LLM co-generation, human validation) → Candidate retrieval (web agents, expert curation, similarity-based retrieval) → Verifiers (Product Verifier, Report Verifier, Safety Verifier) → Tools (Search via SerpAPI, Link Reader/Summary via Firecrawl) → Metrics (AnswerMatch F1, SoP, RV, Safety Pass Rate).

- **Critical path**: 1. Question → Rubric decomposition → Candidate retrieval → Human-verified ground truth. 2. Model output → Verifier assessment → Metric aggregation → Leaderboard scoring.

- **Design tradeoffs**:
  - **Closed vs. open-world**: ShoppingComp uses open-world retrieval (higher realism, higher noise) vs. ShoppingBench's closed catalog.
  - **Judge model choice**: Gemini-2.5-Pro used as primary verifier (highest human agreement at 75.6%), but cross-judge consistency tested.
  - **Aggregation schemes**: Product Verifier uses averaging (robust to isolated errors); Report Verifier uses multiplicative (strict, single error fails report).

- **Failure signatures**:
  - **Low precision, high recall**: Indicates weak constraint enforcement—models retrieve broadly but fail joint validation.
  - **High SoP but low AnswerMatch**: Models satisfy rubrics for wrong products—retrieval grounding failure.
  - **High RV but low Safety Pass Rate**: Faithful reasoning on safe scenarios, but fails to recognize hazards—domain knowledge gap.

- **First 3 experiments**:
  1. **Baseline without tools**: Run GPT-5.2 and Gemini-3-Pro without search/link tools; expect recall <5%, confirming parametric knowledge insufficiency.
  2. **Judge robustness test**: Evaluate same model outputs under Gemini-2.5-Pro, DeepSeek-v3.2, and GPT-5 judges; verify ranking consistency despite score variance.
  3. **Safety ablation**: Isolate safety questions; compare model performance with explicit vs. implicit hazard framing to diagnose whether failure is recognition or reasoning.

## Open Questions the Paper Calls Out

- **Open Question 1**: How can benchmarks effectively evaluate shopping agents that adapt to dynamic user profiles and historical behaviors?
  - **Basis in paper**: [explicit] The conclusion identifies "incorporating personalized evaluation" as a key future direction to support assessments of agents that adapt to "user profiles, historical behaviors, and contextual constraints."
  - **Why unresolved**: ShoppingComp currently relies on static, isolated queries, which cannot assess an agent's ability to maintain state or learn preferences over time.
  - **Evidence**: Performance metrics from a longitudinal study where agents are evaluated on sequential tasks that reference prior interactions and evolving user preferences.

- **Open Question 2**: How do state-of-the-art models perform on complex shopping tasks in multilingual environments?
  - **Basis in paper**: [explicit] Section 6 explicitly calls for "extending the benchmark to multilingual settings" to better reflect the global nature of e-commerce.
  - **Why unresolved**: The current dataset is English-only, leaving the transferability of implicit requirement inference and safety awareness across languages untested.
  - **Evidence**: Comparative Answer Match F1 and Safety Rubric Pass Rates on a translated version of the dataset across diverse target languages (e.g., Chinese, Spanish).

- **Open Question 3**: Can a hybrid retrieval strategy combining GPT-style breadth-first exploration with Gemini-style precision-first evidence packing overcome current performance bottlenecks?
  - **Basis in paper**: [inferred] Section 5.2 observes distinct model behaviors (GPT explores broadly; Gemini extracts densely) but concludes "neither strategy alone resolves the underlying constraint execution challenge."
  - **Why unresolved**: It is unclear if the low performance is due to the inherent limitations of specific strategies or the lack of adaptive strategy selection.
  - **Evidence**: Ablation studies using models trained or prompted to switch strategies based on query complexity, showing significant improvements in Answer Match F1.

## Limitations

- **Dataset availability and reproducibility**: The ShoppingComp benchmark dataset, including 145 instances, 558 scenarios, ground-truth product lists, and expert-annotated rubrics, is not yet publicly released. Without access to these materials, faithful reproduction of the reported results is not possible.
- **Verifier alignment and bias**: The evaluation relies on LLM-as-a-Judge (primarily Gemini-2.5-Pro) for rubric-based verification. While the authors report high human agreement (81–84% at rubric level), the potential for systematic bias in verifier judgments—especially for safety-critical and multi-constraint scenarios—remains a concern.
- **Open-world realism vs. noise**: ShoppingComp's open-world design introduces realistic retrieval noise, but this also means performance gaps may stem from both model limitations and search backend quality.

## Confidence

- **High confidence**: The core finding that state-of-the-art LLMs (e.g., GPT-5.2, Gemini-3-Pro) significantly underperform human experts on all three capabilities (Answer Match F1 ~17–22% vs. 30%; Safety Pass Rate ~35–38% vs. 77%) is robust, as it is based on a controlled, rubric-grounded evaluation framework.
- **Medium confidence**: The mechanism that tool augmentation primarily improves recall (not precision) is well-supported, but the extent to which this generalizes across different search backends or product categories is uncertain without broader experimentation.
- **Medium confidence**: The claim that safety-critical evaluation exposes a distinct failure mode is compelling, but the representativeness and realism of safety traps (e.g., "power-vented water heater in bathroom") are difficult to verify without access to the full rubric set.

## Next Checks

1. **Reproduce baseline recall gap**: Run GPT-5.2 and Gemini-3-Pro without tool access to confirm extremely low recall (<5%), establishing that parametric knowledge is insufficient for long-tail product retrieval.
2. **Cross-judge consistency test**: Evaluate the same model outputs under multiple LLM judges (e.g., Gemini-2.5-Pro, DeepSeek-v3.2, GPT-5) to verify ranking consistency and assess the impact of verifier choice on reported results.
3. **Safety rubric representativeness audit**: Manually review a sample of safety-critical questions and rubrics to assess realism, coverage of real-world hazards, and potential for gaming or overfitting by models.