---
ver: rpa2
title: Compute-Accuracy Pareto Frontiers for Open-Source Reasoning Large Language
  Models
arxiv_id: '2512.24776'
source_url: https://arxiv.org/abs/2512.24776
tags:
- arxiv
- reasoning
- compute
- flops
- preprint
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper evaluates the reasoning capabilities of open-source
  large language models by constructing Pareto frontiers that relate computational
  cost to accuracy across five benchmarks. The authors employ a unified inference
  framework using AA-style prompting and decode each model with full Chain-of-Thought
  generation to capture test-time scaling effects.
---

# Compute-Accuracy Pareto Frontiers for Open-Source Reasoning Large Language Models

## Quick Facts
- **arXiv ID:** 2512.24776
- **Source URL:** https://arxiv.org/abs/2512.24776
- **Reference count:** 35
- **Primary result:** MoE models dominate Pareto frontier; smaller models match larger ones via extended CoT; accuracy saturates beyond task-dependent thresholds.

## Executive Summary
This paper constructs compute-accuracy Pareto frontiers for open-source reasoning LLMs across five benchmarks, revealing fundamental trade-offs between parameter capacity and inference-time compute. Using an architecture-aware FLOPs estimator and unified inference framework, the analysis identifies Mixture-of-Experts (MoE) models as Pareto-optimal due to their sparse activation efficiency. The study demonstrates that smaller models can achieve parity with larger ones by leveraging extended Chain-of-Thought reasoning, though accuracy gains plateau at task-dependent saturation points. Notably, incorrect reasoning traces systematically consume more compute than correct ones, highlighting inefficiencies in current autoregressive reasoning.

## Method Summary
The evaluation employs 19 open-source models (1B-30B+ parameters, dense and MoE) on five reasoning benchmarks (GSM8K, MATH500, AIME25, HMMT-FEB25, GPQA-DIAMOND). Models are decoded with full Chain-of-Thought generation using AA-style prompting, and answers are extracted via hierarchical regex + LLM judge approach. FLOPs are estimated using architecture-aware formulas accounting for attention, feed-forward, MoE routing, and sequence length effects. Single-pass evaluation is used for fair compute comparison, with temperature settings varying by model type (T=0.6 for reasoning-tuned, T=0 for instruction models).

## Key Results
- MoE models systematically dominate the Pareto frontier by activating sparse parameter subsets while maintaining large representational capacity
- Smaller models (1B-8B) can match larger counterparts by generating extended reasoning traces, effectively substituting inference-time compute for parameter capacity
- Accuracy gains plateau beyond task-dependent saturation thresholds, with incorrect reasoning traces consuming significantly more compute than correct ones

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** MoE architectures achieve higher accuracy per FLOP by decoupling parameter count from active computation
- **Mechanism:** Sparse activation of k experts from E total experts reduces computational cost while maintaining representational capacity for complex reasoning
- **Core assumption:** Routing function successfully selects relevant experts for reasoning tasks
- **Evidence:** MoE models dominate Pareto frontier; routing overhead is negligible compared to sparse FFN activation savings
- **Break condition:** Poor expert selection or tasks requiring dense parameter interaction

### Mechanism 2
- **Claim:** Extended CoT reasoning allows smaller models to achieve parity with larger models
- **Mechanism:** Context window serves as working memory for decomposing problems into intermediate steps, with smaller models generating longer traces to compensate for reduced per-token power
- **Core assumption:** Trace quality doesn't degrade with length; model can utilize extended context effectively
- **Evidence:** Smaller models challenge significantly larger counterparts in 1B-8B range; saturation occurs at task-dependent "knee"
- **Break condition:** Saturation when problem complexity exceeds model's deductive horizon

### Mechanism 3
- **Claim:** Incorrect reasoning traces consume more compute than correct ones
- **Mechanism:** Models engage in prolonged hallucinations, recursive loops, or divergent paths when encountering problems beyond their capabilities
- **Core assumption:** Autoregressive generation continues until context exhaustion, with uncertainty leading to verbosity
- **Evidence:** 97% of models expend significantly more compute on incorrect traces; prolonged hallucinations and looping observed
- **Break condition:** Early-stopping signals or uncertainty-aware termination mechanisms

## Foundational Learning

- **Concept: Pareto Frontier / Pareto Optimality**
  - Why needed: Entire paper framed around compute-accuracy trade-offs; Pareto-optimality defines "best" models for given budget
  - Quick check: If Model A achieves 80% at 10^15 FLOPs and Model B achieves 85% at 10^16 FLOPs, which is Pareto-optimal if no model achieves ≥80% at <10^15 FLOPs or ≥85% at <10^16 FLOPs? (Answer: Both)

- **Concept: FLOPs Estimation for Transformers**
  - Why needed: Paper derives architecture-aware FLOPs formulas; understanding attention, FFN, MoE, and quadratic attention contributions is crucial
  - Quick check: Why does standard 2N FLOPs approximation underestimate costs for long-context reasoning? (Answer: Ignores quadratic attention term 4dL_attn × (P² + GP + G(G+1)/2))

- **Concept: Chain-of-Thought (CoT) Prompting**
  - Why needed: CoT enables test-time compute scaling; understanding intermediate reasoning steps is crucial for accuracy-cost trade-offs
  - Quick check: What is the core trade-off when allowing extended CoT traces? (Answer: Higher potential accuracy vs. increased inference latency and compute cost)

## Architecture Onboarding

- **Component map:** FLOPs Estimator -> Evaluation Pipeline -> Benchmark Suite -> Inference Configuration
- **Critical path:** Load model → Apply AA-style prompting → Generate full CoT → Extract answer (regex → judge) → Compute FLOPs → Plot Pareto frontier
- **Design tradeoffs:** Single-pass vs. majority-vote (underestimates accuracy but fair for compute); full context vs. token budget (captures scaling but variable costs); architecture-aware vs. 2N approximation (complex but necessary)
- **Failure signatures:** Trace length asymmetry (ratio > 1.2 indicates inefficiency); saturation without convergence (accuracy plateaus below benchmark ceiling); Pareto suboptimality (dense models fall below frontier)
- **First 3 experiments:**
  1. Reproduce FLOPs distribution for target architecture at sequence lengths 512, 4K, 16K
  2. Measure trace length asymmetry on validation set (50 GSM8K problems)
  3. Identify saturation point for target workload by plotting accuracy vs. log(FLOPs) across reasoning budgets

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can dynamic early-stopping mechanisms be developed that identify reasoning saturation without truncating valid deduction?
- **Basis:** Conclusion states future work must focus on "intelligent, early-stopping mechanisms that recognize when a model has exhausted its reasoning capabilities"
- **Why unresolved:** Models lack reliable internal signals to distinguish productive extended reasoning from stalled computation
- **What evidence would resolve:** Decoding strategy reducing average FLOPs per query on reasoning benchmarks without significant accuracy degradation

### Open Question 2
- **Question:** How can inference systems algorithmically differentiate between necessary complex reasoning and "compute burn" in real-time?
- **Basis:** Section 4.5 notes distinguishing valid extended deduction from wasteful latency is "non-trivial"
- **Why unresolved:** Identifying when models have effectively failed but continue generating tokens
- **What evidence would resolve:** Classifier or uncertainty metric correlating high compute consumption with hallucination probability

### Open Question 3
- **Question:** Does MoE architectural advantage persist across non-reasoning or creative domains?
- **Basis:** Methodology restricts evaluation to math and reasoning benchmarks
- **Why unresolved:** Routing dynamics and FLOP efficiency may differ for open-ended generation where "reasoning traces" are less defined
- **What evidence would resolve:** Pareto analysis on coding or creative writing benchmarks showing similar FLOP-to-accuracy ratios

## Limitations
- Single-pass evaluation without majority voting potentially underestimates absolute accuracy (2-4% gaps on GSM8K/MATH500)
- Answer extraction accuracy depends on regex matching and LLM judge reliability
- FLOPs estimation assumes standard implementations; variations in attention patterns or custom operators could affect accuracy

## Confidence

- **High Confidence:** MoE models dominate Pareto frontier due to architectural efficiency (well-supported across five benchmarks)
- **Medium Confidence:** Smaller models can match larger ones via extended CoT reasoning (demonstrated but saturation points task-dependent)
- **Medium Confidence:** Incorrect reasoning traces consume more compute (97% asymmetry compelling but causation warrants investigation)

## Next Checks
1. **Replicate FLOPs distribution validation:** For Qwen3-8B-Instruct, compute FLOPs breakdown at 512, 4K, 16K tokens using Equations 1-7; verify attention compute percentage matches Figure 6 patterns
2. **Measure trace length asymmetry:** Run 50 GSM8K problems with small model; compare average token count for correct vs. incorrect responses (ratio > 1.2 confirms inefficiency)
3. **Identify saturation point:** Plot accuracy vs. log(FLOPs) for single model across reasoning budgets (varying max tokens); identify "knee" where marginal accuracy gains drop below 1% per 2× compute increase