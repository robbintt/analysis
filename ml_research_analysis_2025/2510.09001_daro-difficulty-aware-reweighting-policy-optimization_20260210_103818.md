---
ver: rpa2
title: 'DARO: Difficulty-Aware Reweighting Policy Optimization'
arxiv_id: '2510.09001'
source_url: https://arxiv.org/abs/2510.09001
tags:
- loss
- grpo
- daro
- arxiv
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper identifies a critical flaw in existing reinforcement
  learning with verifiable rewards (RLVR) algorithms, termed the "loss scale issue,"
  where training disproportionately focuses on samples at certain difficulty levels
  due to static weighting schemes based on empirical pass rates. To address this,
  the authors propose Difficulty-Aware Reweighting Policy Optimization (DARO), which
  dynamically adjusts the loss contribution of each difficulty group based on the
  model's learning state using learnable weight parameters.
---

# DARO: Difficulty-Aware Reweighting Policy Optimization

## Quick Facts
- arXiv ID: 2510.09001
- Source URL: https://arxiv.org/abs/2510.09001
- Authors: Jingyu Zhou; Lu Ma; Hao Liang; Chengyu Shen; Bin Cui; Wentao Zhang
- Reference count: 29
- Primary result: DARO dynamically reweights difficulty groups to balance training across all levels, outperforming leading RLVR baselines on math benchmarks.

## Executive Summary
This paper identifies a critical flaw in existing reinforcement learning with verifiable rewards (RLVR) algorithms, termed the "loss scale issue," where training disproportionately focuses on samples at certain difficulty levels due to static weighting schemes based on empirical pass rates. To address this, the authors propose Difficulty-Aware Reweighting Policy Optimization (DARO), which dynamically adjusts the loss contribution of each difficulty group based on the model's learning state using learnable weight parameters. DARO treats different empirical pass rates as distinct tasks and assigns adaptive weights to balance the training signal across all difficulty levels. Extensive experiments on six math benchmarks across three base models (Qwen2.5-Math-1.5B, Qwen2.5-Math-7B, and Llama3.1-8B) show that DARO consistently outperforms four leading baselines (GRPO, LIPO, Dr. GRPO, and DAPO), achieving higher average accuracy (e.g., 50.8% vs 49.4% for Qwen2.5-Math-7B) and significantly faster convergence while maintaining superior final performance.

## Method Summary
DARO addresses the "loss scale issue" in RLVR by dynamically reweighting samples based on their difficulty group's empirical pass rate μ. The method groups samples by μ = k/K (where k correct out of K responses) and assigns each group a learnable weight w_μ. The total loss is computed as L = Σ(w_μ·L_μ - ln(w_μ)) for μ≠0,1, where L_μ is the group-specific loss and -ln(w_μ) prevents weight collapse while encouraging inverse-proportionality. DARO uses token-mean aggregation, a clip range of [0.2, 0.28], and learns weights at 1000x the model's learning rate (1e-3 vs 1e-6). The method is evaluated on math benchmarks using OpenR1-45k training data and compared against GRPO, LIPO, Dr. GRPO, and DAPO baselines.

## Key Results
- DARO achieves 50.8% mean accuracy on six math benchmarks for Qwen2.5-Math-7B, outperforming the best baseline (49.4%).
- DARO demonstrates significantly faster convergence while maintaining superior final performance across all three tested models.
- The method maintains higher policy entropy throughout training, suggesting better exploration and prevention of premature convergence.

## Why This Works (Mechanism)

### Mechanism 1
Static difficulty weighting schemes in GRPO variants create a "loss scale issue" that disproportionately focuses training on certain difficulty levels at the expense of others. Methods like LIPO and Dr.GRPO use weights proportional to √μ(1-μ), which peaks at medium difficulty (μ=0.5) and approaches zero at extremes (μ→0 or μ→1). This causes loss magnitude to vary dramatically across difficulty groups, forcing training to focus on whichever difficulty level currently dominates the loss. Core assumption: Response length correlates with difficulty, and positive/negative response length differences remain relatively stable within difficulty groups.

### Mechanism 2
Treating pass-rate groups as distinct tasks with learnable weights enables dynamic balancing of training focus across difficulty levels. DARO assigns learnable weight parameter w_μ to each pass-rate group and optimizes L = Σ_μ(w_μ L_μ - ln w_μ). The -ln w_μ regularization ensures the gradient condition ∂L/∂w_μ = L_μ - 1/w_μ = 0 yields w_μ = L_μ^(-1), achieving inverse proportionality between weights and losses. This equalizes weighted loss contributions across difficulty groups. Core assumption: The optimal training balance occurs when w_μ × L_μ is approximately equal across all difficulty groups.

### Mechanism 3
Dynamic weighting maintains higher policy entropy throughout training, preventing premature convergence and enabling sustained exploration. By adaptively balancing loss across difficulty groups, DARO prevents the model from overfitting to a narrow difficulty range. This keeps the policy from collapsing onto high-confidence solutions for dominant difficulty levels, preserving entropy and exploration capacity. Core assumption: Higher entropy correlates with better exploration and final performance in RLVR.

## Foundational Learning

**Group Relative Policy Optimization (GRPO)**: Samples K responses per prompt, computes advantage A = (r-μ)/σ where μ is mean reward and σ is standard deviation. Why needed: DARO builds directly on GRPO's loss formulation; understanding the base loss L_BASE is essential to see how dynamic weighting modifies it. Quick check: Given 8 samples with 3 correct (reward=1) and 5 incorrect (reward=0), what is the advantage of a correct sample?

**Token-mean vs sequence-mean loss aggregation**: The paper adopts token-mean aggregation following DAPO/LIPO, and the unified loss formulation depends on this choice. Why needed: Understanding aggregation methods helps interpret how losses are computed per difficulty group. Quick check: Why might token-mean aggregation handle variable-length responses differently than sequence-mean?

**Policy entropy in reinforcement learning**: DARO's entropy dynamics are a key diagnostic; understanding what entropy measures helps interpret Figure 4. Why needed: Higher entropy suggests better exploration and prevention of premature convergence. Quick check: What does rapidly decreasing policy entropy typically indicate about model behavior?

## Architecture Onboarding

**Component map**: Prompt sampler → K-response generator → Reward model → Pass-rate grouper (μ=k/K) → Per-group loss L_μ calculator → Learnable weight parameters {w_μ} → Joint optimizer (θ + w_μ)

**Critical path**: The pass-rate grouper must correctly bin samples by μ, and the weight update must use the -ln w_μ regularization with learning rate 1e-3 (vs 1e-6 for model parameters) to achieve stable inverse-proportionality dynamics.

**Design tradeoffs**: Setting μ=0,1 samples to weight zero (following DAPO) removes samples with zero advantage but may discard signal about extreme difficulty. Learning rate ratio (1000x higher for weights than model) is critical: too low and weights won't adapt; too high and training becomes unstable. K=8 responses per prompt balances granularity of pass-rate groups against compute cost.

**Failure signatures**: If all w_μ converge to similar values, the regularization is not working; check gradient flow to weight parameters. If entropy drops sharply in early training, the weight learning rate may be too low to compensate for loss scale differences. If training reward plateaus early, the model may be stuck on a narrow difficulty range; inspect loss L_μ per group.

**First 3 experiments**: 1) Reproduce the loss scale issue (Figure 2) on your base model: track L_μ for each pass-rate group during GRPO training to confirm one difficulty dominates. 2) Validate weight dynamics: log w_μ values during DARO training to verify they converge to approximately L_μ^(-1) relationship. 3) Ablate regularization: train with w_μ fixed (no -ln w_μ term) to confirm performance drops, isolating the contribution of dynamic weighting.

## Open Questions the Paper Calls Out

**Scaling to larger models**: Can DARO's dynamic weighting mechanism maintain its convergence advantages when scaled to significantly larger, state-of-the-art models? The current study only validates the method on relatively compact models (Llama3.1-8B, Qwen2.5-Math-1.5B/7B), leaving the efficiency and stability of the learnable weight parameters on 70B+ models unproven.

**Generalization to other domains**: Does DARO generalize effectively to complex reasoning domains beyond mathematics, such as coding or logic? The difficulty estimation in DARO relies on pass rates for verifiable rewards, but it is unclear if this signal is sufficient for domains where verification is more nuanced (e.g., code efficiency) or where difficulty is harder to bin.

**Multi-turn agentic settings**: Is DARO effective in multi-turn, agentic settings that require tool use? The current formulation assumes a single-turn generation process where empirical pass rate is calculated per prompt; it is unknown how the loss reweighting would function in a multi-turn trajectory where intermediate rewards influence the final outcome.

## Limitations

**Loss scale approximation validity**: The theoretical foundation relies on approximating loss magnitude as proportional to √μ(1-μ), which assumes stable response length differences within difficulty groups. While Figure 2 empirically validates this for three models, the approximation may break down for different model architectures, reward models, or problem distributions.

**Regularization design choice**: The -ln w_μ term serves dual purposes: preventing weight collapse and enforcing inverse proportionality. However, the optimal regularization strength is not explored, and the theoretical justification for this specific form is limited. Alternative regularization schemes might achieve similar or better balancing.

**Generalization across domains**: While DARO shows consistent improvements across six math benchmarks, the method's effectiveness for non-mathematical reasoning tasks or domains with different reward structures remains untested. The pass-rate grouping mechanism may not transfer well to domains where difficulty does not map cleanly to empirical pass rates.

## Confidence

**High Confidence**: The empirical pass-rate grouping and learnable weight mechanism is clearly implemented and validated. The improvement in final accuracy and convergence speed across multiple models and benchmarks is well-established through controlled experiments. The entropy maintenance mechanism is directly observable in Figure 4.

**Medium Confidence**: The theoretical foundation for why static weighting creates the "loss scale issue" is compelling but relies on approximations. The claim that treating difficulty groups as distinct tasks with inverse-proportional weights optimally balances training is theoretically motivated but not rigorously proven. The 1000:1 learning rate ratio for weights is critical but lacks ablation study validation.

**Low Confidence**: The long-term stability of dynamic weights and their behavior beyond 300 training steps is not explored. The method's robustness to different response sampling strategies (K values) and reward model architectures is untested. The relationship between policy entropy maintenance and final performance, while suggestive, is not causally established.

## Next Checks

1. **Ablation of Regularization Form**: Train DARO variants with alternative regularization schemes (e.g., L2 penalty on w_μ, entropy regularization) to determine whether -ln w_μ is optimal or merely sufficient for achieving inverse-proportional weight dynamics.

2. **Cross-Domain Transfer**: Apply DARO to non-mathematical reasoning tasks (e.g., code generation with test-based rewards, commonsense reasoning with human feedback) to evaluate whether the difficulty-aware weighting generalizes beyond mathematical problem-solving.

3. **Long-Term Weight Dynamics**: Extend training beyond 300 steps while monitoring weight trajectories and difficulty group contributions to identify whether DARO maintains balanced training focus over extended optimization or if weights eventually collapse toward extremes.