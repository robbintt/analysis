---
ver: rpa2
title: 'Thinking with Video: Video Generation as a Promising Multimodal Reasoning
  Paradigm'
arxiv_id: '2511.04570'
source_url: https://arxiv.org/abs/2511.04570
tags:
- sora-2
- reasoning
- prompt
- video
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces "Thinking with Video" as a new multimodal
  reasoning paradigm that leverages video generation models to overcome limitations
  of static image-based reasoning approaches. The authors developed VideoThinkBench,
  a benchmark comprising vision-centric tasks (eyeballing puzzles, visual puzzles,
  ARC-AGI-2, mazes) and text-centric tasks (GSM8K, MATH, MMMU).
---

# Thinking with Video: Video Generation as a Promising Multimodal Reasoning Paradigm

## Quick Facts
- arXiv ID: 2511.04570
- Source URL: https://arxiv.org/abs/2511.04570
- Reference count: 40
- Primary result: Video generation models like Sora-2 achieve strong multimodal reasoning performance, surpassing VLMs on spatial tasks while excelling on text-centric problems through prompt rewriting

## Executive Summary
This paper introduces "Thinking with Video" as a new multimodal reasoning paradigm that leverages video generation models to overcome limitations of static image-based reasoning approaches. The authors developed VideoThinkBench, a benchmark comprising vision-centric tasks (eyeballing puzzles, visual puzzles, ARC-AGI-2, mazes) and text-centric tasks (GSM8K, MATH, MMMU). Evaluating Sora-2 on this benchmark, they found it achieved 92% accuracy on MATH and 75.53% on MMMU for text tasks, while showing competitive performance on vision tasks, surpassing state-of-the-art VLMs on eyeballing games. The analysis revealed Sora-2's video generation capability enables dynamic reasoning through drawing and imagination, while also embedding text within video frames for unified multimodal understanding. The authors also discovered that self-consistency and in-context learning can improve performance, and that the model's text-centric reasoning abilities likely stem from an internal prompt rewriting mechanism.

## Method Summary
The paper evaluates video generation models (primarily Sora-2 and Wan 2.5) on VideoThinkBench, a benchmark with vision-centric and text-centric tasks. The evaluation pipeline extracts answers from three modalities: audio (transcribed via Whisper), last frame (image analysis), and major frame (majority vote of sampled frames). For text tasks, GPT-4o serves as an LLM-as-a-Judge. The paper introduces self-consistency through multiple video generations and explores prompt rewriting mechanisms that decompose problems into visual generation steps.

## Key Results
- Sora-2 achieves 92% accuracy on MATH and 75.53% on MMMU for text-centric tasks
- Sora-2 surpasses state-of-the-art VLMs on eyeballing games, achieving competitive performance on vision-centric tasks
- Self-consistency through multiple generations improves accuracy by filtering high-variance visual artifacts
- Major Frame Evaluation (temporal voting) outperforms Last Frame analysis for robust assessment
- Prompt rewriting mechanism enables text-centric reasoning, with Wan 2.5 performance dropping to 0% when disabled

## Why This Works (Mechanism)

### Mechanism 1: Temporal Consistency as Implicit Verification
Video generation is stochastic, with single frames potentially containing noise or artifacts. By sampling multiple frames or taking majority votes across several generated videos, the system marginalizes out transient errors, isolating the model's consistent belief about the correct answer. Evidence shows Major Frame Option accuracy (68-90%) significantly outperforms Last Frame (56-66%).

### Mechanism 2: Visual Simulation via Latent World Models
Video models perform dynamic reasoning by simulating physical or geometric transformations in pixel space. For tasks like ray reflection or maze solving, the model projects trajectories, functioning as "mental simulation" or "drawing" where the solution is the visual result of the transformation.

### Mechanism 3: Prompt Rewriting for Text-Visual Bridging
Strong text-centric performance likely originates from an internal prompt rewriter that pre-solves problems by translating text queries into detailed visual scripts. The video generator then renders this pre-computed plan. Evidence shows Wan 2.5 accuracy drops to 0% on math tasks when prompt rewriting is disabled.

## Foundational Learning

- **Test-Time Scaling / Self-Consistency**
  - Why needed: Standard evaluation looks at single outputs, but correct answers may exist in frame 4 but be obscured in frame 10. Understanding temporal data aggregation is crucial for robust evaluation.
  - Quick check: If a model generates 5 videos and 3 show answer "A" while 2 show random noise, what is the self-consistent prediction?

- **Chain-of-Thought (CoT) Paradigms**
  - Why needed: This paper positions "Thinking with Video" as successor to "Thinking with Text" and "Thinking with Images." Understanding that the goal is to make reasoning process visible and multimodal is key.
  - Quick check: How does drawing a line to solve a maze differ cognitively from describing the path in text?

- **Internal Architecture Decomposition**
  - Why needed: The paper reveals that "Video Generation Model" is actually a system containing Rewriter + Generator. Distinguishing between components that plan versus render is key to diagnosing failures.
  - Quick check: If a video model outputs mathematically correct steps but renders them as blurry text, which component (Rewriter vs. Generator) is the bottleneck?

## Architecture Onboarding

- **Component map**: Input (Text Prompt + Reference Image) -> Hidden Stage (Prompt Rewriter) -> Core Engine (Video Generator) -> Evaluation Pipeline (Frame Extractor + Transcriber + Judge)
- **Critical path**: The evaluation pipeline is the critical path for iteration speed. Generation is slow (seconds to minutes), but extracting frames and judging them must be automated to handle 4,000+ samples in VideoThinkBench.
- **Design tradeoffs**:
  - Last Frame vs. Major Frame: Last frame is cheap but noisy; Major frame is robust but requires processing 10x more image data
  - Audio vs. Video Answer: Audio is often more accurate for text-heavy answers but requires transcription; Video visual answers are needed for spatial tasks
- **Failure signatures**:
  - "Did Nothing": Video ignores puzzle area and keeps it static
  - Modality Disparity: Audio says "Alpha" while text on screen draws circle around "Charlie"
  - Hallucinated Process: Video shows correct final answer but intermediate steps are gibberish
- **First 3 experiments**:
  1. Sanity Check: Reproduce "Major Frame" vs. "Last Frame" delta on 50 Eyeballing puzzles to calibrate evaluation pipeline sensitivity
  2. Rewriter Ablation: Run Wan 2.5 with `prompt_extend=True` vs `False` on GSM8K subset to validate "Planning vs. Rendering" hypothesis
  3. Noise Robustness: Test self-consistency scaling (1 vs. 5 tries) on Maze tasks to determine inference cost-to-accuracy curve

## Open Questions the Paper Calls Out

- Can Reinforcement Learning with Verifiable Rewards (RLVR) be effectively applied to video generation models to enhance reasoning capabilities? (Section 6)
- Does pre-training video generation models on frame-by-frame text generation data enable acquisition of textual world knowledge? (Section 6)
- To what extent does text-centric reasoning performance depend on internal prompt rewriter rather than native visual reasoning? (Inferred from Section 3.2.3)

## Limitations

- Model access and reproducibility: Primary results rely on Sora-2, a closed-source model not publicly accessible
- Evaluation methodology robustness: Implementation details for "Major Frame Evaluation" and LLM-as-a-Judge bias are not fully specified
- Modality fusion ambiguity: Audio and visual answers sometimes disagree without clear resolution protocol

## Confidence

**High Confidence**:
- Video generation models can simulate physical transformations for spatial reasoning
- Self-consistency through multiple generations improves accuracy
- Temporal voting across frames provides more reliable evaluation than single-frame analysis

**Medium Confidence**:
- Prompt rewriting hypothesis explains text-centric reasoning performance
- Video models can unify multimodal understanding by embedding text
- Performance on text-centric tasks stems from internal architecture

**Low Confidence**:
- Sora-2's performance is representative of video generation models generally
- Architecture pattern applies universally across video models
- Current evaluation methods are sufficient for comprehensive assessment

## Next Checks

1. **Architecture Ablation Validation**: Run comprehensive ablations on multiple video models testing different architectural configurations - with and without prompt rewriting, different frame sampling strategies, and various aggregation methods.

2. **Cross-Judge Reliability Analysis**: Implement multiple LLM judges (different models, different prompts) for text-centric tasks and measure inter-judge agreement to quantify uncertainty introduced by LLM-as-a-Judge.

3. **Temporal Consistency Benchmarking**: Systematically vary generation parameters (temperature, guidance scale, number of inference steps) and measure how temporal consistency changes to reveal whether improvements stem from model capability or generation artifacts.