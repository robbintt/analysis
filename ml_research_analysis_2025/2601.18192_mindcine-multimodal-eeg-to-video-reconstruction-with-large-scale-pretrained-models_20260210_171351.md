---
ver: rpa2
title: 'MindCine: Multimodal EEG-to-Video Reconstruction with Large-Scale Pretrained
  Models'
arxiv_id: '2601.18192'
source_url: https://arxiv.org/abs/2601.18192
tags:
- video
- reconstruction
- data
- information
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes MindCine, a multimodal EEG-to-video reconstruction
  framework that combines semantic and perceptual decoding modules. The semantic module
  leverages multimodal joint learning with large-scale pre-trained EEG models to extract
  high-level semantic information, while the perceptual module uses a causal sequence
  architecture to decode low-level dynamic features from EEG signals.
---

# MindCine: Multimodal EEG-to-Video Reconstruction with Large-Scale Pretrained Models

## Quick Facts
- arXiv ID: 2601.18192
- Source URL: https://arxiv.org/abs/2601.18192
- Reference count: 0
- Primary result: Achieves 0.867±0.03 on 2-way video classification and 0.333±0.10 on PSNR for EEG-to-video reconstruction

## Executive Summary
This paper proposes MindCine, a multimodal framework that reconstructs videos from EEG signals by combining semantic and perceptual decoding modules. The approach leverages large pretrained EEG models and multimodal joint learning with image, text, and depth modalities to overcome data scarcity challenges. Extensive experiments on the SEED-DV dataset demonstrate state-of-the-art performance, with significant improvements over existing methods in both semantic classification and pixel-level reconstruction quality.

## Method Summary
MindCine uses a two-module architecture: a semantic decoding module that extracts high-level semantic information from EEG using multimodal joint learning with large pretrained models, and a perceptual decoding module that decodes low-level dynamic features using a causal sequence transformer. The semantic module aligns EEG embeddings with CLIP's latent space across three modalities (image, text, depth) using SoftCLIP loss, while the perceptual module uses EmbedNet for feature extraction followed by CausalSeq transformer with causal attention to predict VQ-VAE latents. An inference module then uses T2V diffusion with adversarial guidance to generate the final video reconstruction.

## Key Results
- Achieves 0.867±0.03 on 2-way video classification and 0.333±0.10 on PSNR
- Multimodal joint learning (text+image+depth) outperforms text-only alignment across all metrics
- Large pretrained EEG models (Gram, LaBraM, CBraMod) significantly improve semantic decoding over MLP baseline
- Both semantic and perceptual modules contribute meaningfully to final reconstruction quality

## Why This Works (Mechanism)

### Mechanism 1: Multimodal Joint Learning for Semantic Alignment
- Claim: Aligning EEG with multiple modalities (text, image, depth) rather than text alone improves semantic decoding quality.
- Mechanism: The SoftCLIP loss aligns EEG embeddings with CLIP's latent space across three modalities simultaneously (Eq. 2: L_Joint = α₁L_SoftCLIP(e_s,v) + α₂L_SoftCLIP(e_s,t) + α₃L_SoftCLIP(e_s,d)). This enables the model to capture "beyond-text-modality" information that would otherwise be lost.
- Core assumption: EEG signals contain complementary semantic information that maps to different modalities' latent spaces.
- Evidence anchors: [abstract] "We employ a multimodal joint learning strategy to incorporate beyond-text modalities in the training stage", [Table 1] Text+Image+Depth outperforms Text-only across all metrics, [corpus] WaveMind (arXiv:2510.00032) similarly aligns EEG to textual and visual modalities.

### Mechanism 2: Large-Scale Pretrained EEG Models for Data Scarcity
- Claim: Fine-tuning large pretrained EEG encoders improves reconstruction over training from scratch on limited data.
- Mechanism: Large EEG models (LaBraM, Gram, CBraMod, EEGPT) pretrained on massive EEG datasets provide robust feature representations that transfer to video reconstruction. These models follow masked-then-reconstruction pretraining paradigms, learning generic EEG patterns.
- Core assumption: Pretrained representations from large EEG corpora transfer meaningfully to the EEG-video alignment task.
- Evidence anchors: [abstract] "leverage a pre-trained large EEG model to relieve the data scarcity issue for decoding semantic information", [Table 2] Gram (6.0M parameters) achieves 0.847 2-way-V vs MLP's 0.818.

### Mechanism 3: CausalSeq Architecture for Temporal Perception Decoding
- Claim: Causal attention in a Seq2Seq transformer effectively decodes continuous low-level dynamic features from high-temporal-resolution EEG.
- Mechanism: The CausalSeq model treats EEG embeddings as word tokens and uses causal masking to ensure each token attends only to previous tokens, preserving temporal causality. This matches the sequential nature of video frames and dynamic visual perception.
- Core assumption: Low-level perceptual features in EEG have sequential dependencies that benefit from autoregressive modeling.
- Evidence anchors: [abstract] "a Seq2Seq model with causal attention is specifically designed for decoding perceptual information", [Section 2.2.2] "the attention layer incorporates a specially designed Causal Mask to ensure that each token cannot access information from subsequent tokens".

## Foundational Learning

- Concept: **Contrastive Learning / CLIP Alignment**
  - Why needed here: The SoftCLIP loss requires understanding how contrastive objectives align embeddings across modalities. The temperature parameter τ and batch-wise normalization are critical implementation details.
  - Quick check question: Can you explain why SoftCLIP uses soft labels (distribution matching) rather than hard positive/negative pairs?

- Concept: **Transformer Encoder-Decoder with Causal Masking**
  - Why needed here: The CausalSeq model's architecture assumes familiarity with multi-head attention, positional encoding, and how causal masks create autoregressive properties.
  - Quick check question: In the decoder, what information does token at position t have access to, and what is it prevented from seeing?

- Concept: **Diffusion Model Conditioning**
  - Why needed here: The inference module uses classifier-free guidance with positive/negative conditions (Eq. 7). Understanding guidance scale s and score estimation ϵ_θ is essential for controlling generation.
  - Quick check question: What happens to output diversity as guidance scale s increases?

## Architecture Onboarding

- Component map:
  - **Semantic Decoding Module**: Pretrained EEG encoder → Multimodal alignment (SoftCLIP with image/text/depth) → Semantic predictor → SD text embedding space
  - **Perceptual Decoding Module**: EmbedNet (temporal-spatial conv) → CausalSeq (encoder-decoder transformer) → VQ-VAE latent prediction
  - **Inference Module**: T2V diffusion model with adversarial guidance (positive: semantic+perceptual, negative: empty/unwanted)

- Critical path:
  1. EEG signal (C×T) → sliding window → t segments
  2. Parallel: (a) Large encoder → semantic features, (b) EmbedNet → perceptual embeddings
  3. Semantic: Joint alignment → predictor → text condition e_t
  4. Perceptual: CausalSeq → latent prediction ẑ₀
  5. Inference: T2V diffusion conditioned on both

- Design tradeoffs:
  - **Model size vs. data efficiency**: Larger pretrained models (Gram 6.0M) improve semantics but add computational overhead
  - **Modality coverage vs. complexity**: Three modalities (text/image/depth) outperform fewer, but require additional encoders and careful loss balancing (α₁=α₂=α₃=1/3)
  - **Causal vs. bidirectional**: Causal attention preserves temporal structure but may miss future context useful for perception

- Failure signatures:
  - **Semantic-module-only removal**: 40-way-V drops from 0.179 to 0.137 (Table 1) — indicates semantic information is critical for category-level discrimination
  - **Perception-module-only removal**: PSNR drops from 9.568 to 8.297 (Table 1) — perceptual module essential for pixel-level fidelity
  - **Text-only modality**: Fine-grained details lost; adding depth improves structure, adding image improves semantics (Figure 5)

- First 3 experiments:
  1. **Baseline reproduction**: Train semantic module with MLP encoder (from scratch) and text-only alignment on SEED-DV train split. Verify reproduction of EEG2Video metrics.
  2. **Ablation by modality**: Add one modality at a time (text→text+depth→text+depth+image). Measure semantic metrics (2-way-V, 40-way-V) to confirm incremental gains.
  3. **Encoder swap test**: Replace MLP with each large EEG model (LaBraM, CBraMod, Gram). Hold all other components fixed. Compare semantic-level metrics to isolate encoder contribution.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the MindCine framework generalize to unseen subjects in a zero-shot manner, or is subject-specific fine-tuning requisite for high-fidelity reconstruction?
- Basis in paper: [inferred] The evaluation metrics (Table 1) are averaged across all subjects, suggesting a subject-dependent training scheme rather than a cross-subject generalization test.
- Why unresolved: The paper does not report performance on held-out subjects, leaving the model's ability to handle inter-subject variability unverified.
- What evidence would resolve it: Results from a leave-one-subject-out cross-validation experiment.

### Open Question 2
- Question: To what extent does the pre-trained Text-to-Video (T2V) diffusion model contribute to the visual fidelity versus the actual decoded EEG features?
- Basis in paper: [inferred] The Inference Module utilizes a powerful pre-trained diffusion model with "prior knowledge," which may hallucinate details that are not present in the neural signal.
- Why unresolved: High PSNR/SSIM scores could result from the T2V model's ability to generate realistic video from limited semantic prompts rather than accurate neural decoding.
- What evidence would resolve it: An ablation study using a non-generative decoder or comparing output ground truth against generated details not present in the text prompt.

### Open Question 3
- Question: How does the performance scale with the quality of the "beyond-text" modalities, given that depth and text annotations are generated by automated models (DepthAnything, BLIP2)?
- Basis in paper: [inferred] The method relies on automated tools to create the aligned multimodal dataset, assuming these generated labels are accurate ground truth for EEG alignment.
- Why unresolved: Errors or artifacts in the automated depth estimation or image captioning could mislead the multimodal joint learning process, imposing a "teacher forcing" error ceiling.
- What evidence would resolve it: Analysis of reconstruction performance using human-verified ground truth annotations versus automated ones.

### Open Question 4
- Question: Does the CausalSeq architecture effectively capture temporal dynamics faster than the video frame rate, or is it bottlenecked by the sliding window segmentation?
- Basis in paper: [inferred] The paper slices high-temporal-resolution EEG (≥200 Hz) into segments corresponding to video frames, potentially discarding sub-frame neural dynamics.
- Why unresolved: While the model uses causal attention, it is unclear if the continuous EEG stream provides temporal benefits beyond what is available in the discrete video frames.
- What evidence would resolve it: Reconstruction of dynamic stimuli with rapid changes exceeding the training video's frame rate.

## Limitations

- Architectural Details Missing: The paper lacks complete specifications for critical components including EmbedNet architecture, CausalSeq Transformer configuration, and T2V diffusion model specifics.
- Dataset Dependency: Performance claims rely heavily on the SEED-DV dataset with only 20 subjects and 1,400 video pairs, limiting generalizability testing.
- Evaluation Scope: The evaluation focuses primarily on quantitative metrics rather than qualitative assessment of reconstruction realism or human perceptual studies.

## Confidence

- **High Confidence**: The multimodal joint learning framework with SoftCLIP alignment is well-grounded in existing literature and supported by substantial performance improvements.
- **Medium Confidence**: The claim that large pretrained EEG models significantly improve performance is supported by ablation results, but transfer benefits need further exploration.
- **Low Confidence**: The inference module's effectiveness is difficult to assess without knowing the T2V diffusion model architecture, guidance scale parameters, and implementation details.

## Next Checks

1. **Reproduce EmbedNet Architecture**: Implement the exact EmbedNet temporal-spatial convolution configuration from the paper's specifications or contact authors for missing architectural details. Validate that the perceptual embeddings capture meaningful video-related features from EEG signals.

2. **Cross-Dataset Generalization Test**: Train the complete MindCine framework on SEED-DV, then evaluate on an independent EEG-video dataset (if available) or test subject transfer performance on held-out subjects from SEED-DV. This would validate claims about data efficiency and pretraining benefits.

3. **Ablation of Causal Attention**: Replace the CausalSeq model with a bidirectional transformer (removing causal masking) while keeping all other components identical. Compare perceptual reconstruction quality to determine whether temporal causality is truly beneficial or merely a design choice.