---
ver: rpa2
title: Hyperparameter Transfer Enables Consistent Gains of Matrix-Preconditioned Optimizers
  Across Scales
arxiv_id: '2512.05620'
source_url: https://arxiv.org/abs/2512.05620
tags:
- learning
- scaling
- dout
- shampoo
- rate
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper investigates how to scale matrix-preconditioned optimizers\
  \ like Muon, SOAP, and Shampoo across model sizes. The authors develop scaling rules\
  \ for learning rate and weight decay as a function of model width and depth, building\
  \ on the Maximal Update Parameterization (\u03BCP) framework."
---

# Hyperparameter Transfer Enables Consistent Gains of Matrix-Preconditioned Optimizers Across Scales

## Quick Facts
- arXiv ID: 2512.05620
- Source URL: https://arxiv.org/abs/2512.05620
- Authors: Shikai Qiu, Zixi Chen, Hoang Phan, Qi Lei, Andrew Gordon Wilson
- Reference count: 40
- This paper develops scaling rules for learning rate and weight decay that enable matrix-preconditioned optimizers to maintain consistent 1.4× speedups over AdamW when scaling transformer models from 190M to 1.4B parameters.

## Executive Summary
This paper addresses a critical challenge in scaling advanced optimizers for large language models: maintaining their theoretical advantages across model sizes. Matrix-preconditioned optimizers like Muon, SOAP, and Shampoo can theoretically provide faster convergence than AdamW, but their benefits often vanish when scaling to larger models due to hyperparameter transfer failures. The authors develop practical scaling rules based on Maximal Update Parameterization (μP) and empirical weight decay scaling that enable consistent 1.4× speedups over AdamW across four orders of magnitude in model size. Their approach works by properly scaling learning rates according to layer dimensions and using 1/width weight decay scaling, with blocking or spectral normalization to mitigate finite-width deviations.

## Method Summary
The authors investigate how to scale matrix-preconditioned optimizers (Muon, SOAP, Shampoo) across model widths while maintaining their speedups over AdamW. They build on μP theory to derive per-layer learning rate scaling rules: for Shampoo with exponents (eL, eR), η ~ (d_out/d_in)^(1−eL−eR); for Muon, η ~ √(d_out/d_in). They also discover that scaling independent weight decay as 1/width is nearly optimal across these optimizers. To address finite-width deviations that cause μP transfer to fail, they propose blocking preconditioners to fixed-size submatrices or explicit spectral normalization. The method is validated on Llama-architecture models (190M-1.4B parameters) trained on FineWeb, achieving consistent 1.4× compute multipliers versus AdamW baselines.

## Key Results
- Matrix-preconditioned optimizers achieve 1.4× speedups over AdamW at 190M parameters when using μP scaling and 1/D weight decay
- Without proper hyperparameter scaling, these speedups drop to 1.1× at 1.4B parameters
- Scaling independent weight decay as 1/width is nearly optimal across Muon, SOAP, and Shampoo
- Blocking (512) or spectral normalization mitigates finite-width deviations that cause learning rate transfer failures
- The approach generalizes across different matrix-preconditioned methods without requiring method-specific tuning

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Scaling learning rate via μP enables transfer of optimal hyperparameters across model widths for matrix-preconditioned optimizers.
- Mechanism: μP ensures Θ(1) feature learning per layer per gradient step by setting the learning rate to balance gradient magnitude (Θ(1/d_out)) against input alignment (Θ(d_in)). For Shampoo with exponents (eL, eR), this yields η ~ (d_out/d_in)^(1−eL−eR); for Muon, η ~ √(d_out/d_in).
- Core assumption: At initialization, gradient is approximately rank-1 (G = δx^T) with δ = Θ(1/d_out), x = Θ(1), and x^T x' = Θ(d_in) due to feature kernel alignment.
- Evidence anchors:
  - [abstract] "scaling the learning rate according to μP improves transfer"
  - [Section 3.1] "the μP condition reduces to choosing η... such that ηQ(G)x' = Θ(1)"
  - [corpus] "Completed Hyperparameter Transfer..." confirms μP effectiveness across widths
- Break condition: Finite-width deviations when batch size B or training steps t scale with width; preconditioners become more expressive than rank-1 assumption.

### Mechanism 2
- Claim: Scaling independent weight decay as 1/width is nearly optimal across matrix-preconditioned optimizers in compute-optimal training.
- Mechanism: As model width grows, total parameter count scales as Θ(width²) for transformers. Scaling λ ~ 1/D keeps the EMA timescale of weight decay constant relative to the effective capacity per dimension, preventing over-regularization at larger scales.
- Core assumption: Compute-optimal regime where tokens-per-parameter is held constant; parameter count scales approximately linearly with width at small scales.
- Evidence anchors:
  - [abstract] "scaling independent weight decay as 1/width is nearly optimal across optimizers"
  - [Section 3.4, Figure 5] "Optimal independent weight decay scales like 1/D"
  - [corpus] Weak explicit support; related work on weight decay scaling exists but focuses on AdamW
- Break condition: At very large scales where parameters no longer scale linearly with width, or when training horizon varies significantly.

### Mechanism 3
- Claim: Blocking and explicit spectral normalization mitigate finite-width deviations that cause μP learning rate transfer to fail.
- Mechanism: μP's infinite-width limit assumes Θ(1) stable rank for updates, but matrix-preconditioned optimizers achieve stable ranks scaling with width during training. Blocking constrains preconditioners to fixed-size submatrices; spectral normalization directly constrains update spectral norm to √(d_out/d_in) at each step.
- Core assumption: Drifting optimal learning rates observed in AdaMuon and grafted Shampoo stem from RMS normalization producing spectral norms that decrease as stable rank grows.
- Evidence anchors:
  - [Section 3.2] "finite-width deviations are effectively mitigated by blocked preconditioning and spectral normalization"
  - [Section E.1, Figure 7] Shows stable rank grows with width during training unless blocking is used
  - [corpus] "Optimal Scaling Needs Optimal Norm" corroborates spectral norm importance
- Break condition: Spectral normalization overhead may be unjustified if blocking already stabilizes transfer (as observed for SOAP/Shampoo with block size 512).

## Foundational Learning

- Concept: **Maximal Update Parameterization (μP)**
  - Why needed here: Provides the theoretical foundation for deriving learning rate scaling rules; assumes infinite-width limit where activations remain Θ(1) and feature updates are maximized without divergence.
  - Quick check question: Given a hidden layer weight matrix W ∈ R^(d_out × d_in), what μP learning rate scaling should Adam use? (Answer: η ~ 1/d_in)

- Concept: **Matrix Preconditioning (Shampoo, Muon, SOAP)**
  - Why needed here: These optimizers apply full-matrix or Kronecker-factored preconditioners rather than elementwise scaling, changing how updates scale with dimensions.
  - Quick check question: How does Muon's Newton-Schulz iteration change the spectral norm of a rank-1 gradient G = δx^T? (Answer: Produces Θ(1) · uv^T with spectral norm Θ(√(d_in/d_out)))

- Concept: **Independent vs. Coupled Weight Decay**
  - Why needed here: The paper distinguishes between weight decay applied to weights (independent) versus weight decay proportional to learning rate (coupled/AdamW default). Scaling laws differ.
  - Quick check question: If AdamW uses coupled weight decay λ_c = 0.01 with η = 1e-3, what is the effective independent weight decay? (Answer: λ_ind = λ_c / η = 10)

## Architecture Onboarding

- Component map:
  Base optimizer (Muon/Shampoo/SOAP) -> μP learning rate scaling -> 1/D weight decay scaling -> (optional) blocking/spectral normalization -> Adam for embedding/readout

- Critical path:
  1. Identify layer dimensions (d_in, d_out) for each weight matrix
  2. Apply per-layer learning rate scaling from Table 1 based on optimizer type
  3. Scale independent weight decay as 1/D where D = embedding dimension
  4. Use block size 512 for Shampoo/SOAP to ensure stable transfer

- Design tradeoffs:
  - Blocking reduces memory/compute overhead but may limit preconditioner expressiveness
  - Spectral normalization adds 2 matrix-vector products per layer per step but provides stronger stability guarantees
  - Using Adam for embedding/readout layers is empirically better than matrix preconditioners for those layers

- Failure signatures:
  - Optimal learning rate drifts right (increases) with width → finite-width deviation; add blocking or spectral normalization
  - Speedup over AdamW diminishes at scale → likely incorrect weight decay scaling; verify 1/D scaling
  - Training instability in early steps → learning rate may exceed μP prediction; check spectral norm condition

- First 3 experiments:
  1. Validate μP transfer: Train small base model (D=128), sweep learning rate, then zero-shot transfer optimal η to larger model (D=1024) using μP scaling; compare loss curves.
  2. Ablate weight decay scaling: Compare constant weight decay vs. 1/D scaling across 190M→1.4B models; expect constant WD to degrade speedup at large scale.
  3. Test blocking effect: For SOAP/Shampoo, compare full preconditioner vs. block size 512; measure optimal learning rate stability across widths.

## Open Questions the Paper Calls Out

- Why does the 1/width independent weight decay scaling work well across all tested matrix-preconditioned optimizers, and is there an even better scaling rule?
  - Basis in paper: [explicit] "Understanding why this scaling is effective across optimizers... and whether better scaling exists, are exciting and important open questions."
  - Why unresolved: The empirical finding contradicts μP's theoretical prediction of Θ(1) weight decay, and no theoretical explanation for the 1/D scaling is provided.
  - What evidence would resolve it: Theoretical analysis connecting weight decay dynamics to matrix preconditioning, or systematic ablation across more optimizers and scales identifying alternative scaling laws.

- Do the 1.4× speedup gains of Muon, SOAP, and Shampoo over AdamW persist at scales beyond 1.4B parameters?
  - Basis in paper: [explicit] "Verifying how well our results generalize to larger models trained with more compute is an exciting future direction."
  - Why unresolved: Computational budget limited experiments to 190M–1.4B models; frontier LLMs are orders of magnitude larger.
  - What evidence would resolve it: Experiments scaling to 7B+ parameters with the same hyperparameter transfer rules, measuring compute multipliers against well-tuned AdamW.

- What mechanism underlies the improved efficiency of matrix-preconditioned optimizers like Muon?
  - Basis in paper: [explicit] "We do not investigate why they improve the efficiency. Understanding this question holds potential for designing even more efficient future optimizers."
  - Why unresolved: The paper establishes empirical gains but does not analyze the optimization landscape or convergence properties that enable faster learning per step.
  - What evidence would resolve it: Theoretical analysis of preconditioning effects on gradient curvature, or mechanistic interpretability studies tracking feature learning dynamics.

- How should learning rate scale when width varies by more than a factor of 10 in the compute-optimal regime?
  - Basis in paper: [explicit] "We caution that more careful scaling studies may be needed to determine how the optimal learning rate scales in the compute-optimal regime if width varies by more than a factor of 10, such as by fitting a power-law correction on top of μP."
  - Why unresolved: μP provides good but not perfect transfer; observed deviations suggest finite-width effects may compound at extreme scale ratios.
  - What evidence would resolve it: High-resolution sweeps across 10×+ width variations fitting empirical power-law corrections to μP scaling.

## Limitations

- The theoretical framework relies on infinite-width μP assumptions that may not fully capture finite-width effects in practical large models
- Empirical validation is limited to Llama-style architectures with specific training configurations (20 tokens/parameter, linear decay)
- Several implementation details remain underspecified, including exact Newton-Schulz iteration coefficients and spectral normalization convergence criteria
- The paper does not extensively explore interactions between μP scaling and other architectural choices like rotary embeddings or activation functions beyond SwiGLU

## Confidence

**High confidence** in the core empirical finding: The 1.4× speedup over AdamW is well-supported by controlled experiments across four model scales (190M to 1.4B parameters). The ablation studies clearly demonstrate that both μP learning rate scaling and 1/D weight decay are necessary components for maintaining this speedup at scale.

**Medium confidence** in the theoretical mechanisms: While the μP framework is well-established, the extension to matrix-preconditioned optimizers involves several approximations (rank-1 gradients, stable rank assumptions) that are validated empirically but not rigorously proven for all cases. The spectral normalization mechanism, though theoretically motivated, shows mixed empirical necessity across different optimizers.

**Low confidence** in cross-architecture generalizability: The results are demonstrated specifically for Llama-style transformers with SwiGLU activations. The scaling rules may not transfer directly to architectures with different connectivity patterns, normalization schemes, or training objectives (e.g., contrastive learning).

## Next Checks

1. **Cross-architecture validation**: Test the μP + 1/D scaling rules on a different architecture family (e.g., GPT-style with GeLU activations) to verify that the 1.4× speedup generalizes beyond Llama-style models.

2. **Training regime sensitivity**: Vary the tokens-per-parameter ratio (e.g., test at 10 vs 40 tokens/parameter) to determine if the scaling laws hold under different compute-optimal regimes, or if the 1/D weight decay rule needs adjustment.

3. **Spectral normalization necessity**: Conduct a controlled ablation comparing blocked vs. spectral normalization approaches across all three matrix-preconditioned optimizers (Muon, Shampoo, SOAP) on a fixed architecture to quantify when each mitigation strategy is essential for stable transfer.