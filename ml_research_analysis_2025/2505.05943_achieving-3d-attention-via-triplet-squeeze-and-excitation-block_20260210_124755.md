---
ver: rpa2
title: Achieving 3D Attention via Triplet Squeeze and Excitation Block
arxiv_id: '2505.05943'
source_url: https://arxiv.org/abs/2505.05943
tags:
- attention
- block
- convnext
- tripse
- proposed
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes a new attention mechanism called Triplet Squeeze-and-Excitation
  (TripSE) that combines Triplet Attention (TA) with Squeeze-and-Excitation (SE) to
  capture cross-dimensional attention in CNNs. The authors introduce four variants
  of the TripSE block and integrate them into ResNet18, DenseNet, and ConvNeXt architectures.
---

# Achieving 3D Attention via Triplet Squeeze and Excitation Block

## Quick Facts
- **arXiv ID:** 2505.05943
- **Source URL:** https://arxiv.org/abs/2505.05943
- **Reference count:** 40
- **Primary result:** ConvNeXt-S + TripSE4 achieves 78.27% accuracy on FER2013, state-of-the-art for facial expression recognition.

## Executive Summary
This paper introduces the Triplet Squeeze-and-Excitation (TripSE) block, a novel attention mechanism that combines Triplet Attention (TA) with Squeeze-and-Excitation (SE) to capture cross-dimensional dependencies in CNNs. The approach introduces four variants of the TripSE block and integrates them into ResNet18, DenseNet, and ConvNeXt architectures. Experiments across four datasets demonstrate consistent performance improvements, with the ConvNeXt model with TripSE4 achieving state-of-the-art accuracy of 78.27% on the challenging FER2013 dataset. The method enhances feature discrimination by combining rich inter-dimensional relationships with globally informed channel weighting, demonstrating the continued relevance of CNNs and the potential of attention mechanisms to further improve their capabilities in image classification tasks.

## Method Summary
The paper proposes TripSE blocks that integrate Triplet Attention (capturing cross-dimensional dependencies via tensor rotation) with Squeeze-and-Excitation (providing global channel recalibration). Four variants are introduced: TripSE1 applies SE after summing TA branches, TripSE2 places SE inside each TA branch, TripSE3 places SE after each branch, and TripSE4 adds SE weights to TA outputs via affine transformation. The blocks are integrated into standard architectures (ResNet18, DenseNet, ConvNeXt) by placing one block after each group of network blocks. Experiments use RAdam optimizer, batch size 512, and various augmentation strategies. The ConvNeXt-S + TripSE4 configuration achieves 78.27% accuracy on FER2013.

## Key Results
- ConvNeXt-S + TripSE4 achieves 78.27% accuracy on FER2013, outperforming all previous methods
- TripSE blocks consistently improve performance across CIFAR100, ImageNet, FER2013, and AffectNet datasets
- Adding SE or TA alone to ConvNeXt can reduce accuracy, highlighting the importance of the combined TripSE approach
- TripSE1 (r=16) achieves 78.08% on FER2013 while maintaining compatibility with pre-trained weights
- The proposed method demonstrates the continued relevance of CNNs for challenging tasks like facial expression recognition

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Capturing cross-dimensional dependencies via tensor rotation improves feature discrimination over standard channel-only attention.
- **Mechanism:** The Triplet Attention component rotates the input tensor (C×W×H) into three permutation views. By applying pooling and convolution across these "rotational channels," the model captures interactions between spatial dimensions and channels.
- **Core assumption:** Important visual features rely on dependencies between spatial locations and specific channels (cross-dimension interaction).
- **Evidence anchors:** [Section III.A] describes how TA captures interactions of different tensors by rotating dimensions; [Section III.C] states TA lacks globally informed channel weighting.
- **Break condition:** If the target task relies purely on texture/color statistics independent of spatial orientation, the overhead of cross-dimensional rotation may not yield returns.

### Mechanism 2
- **Claim:** Global channel recalibration (SE) conditions the feature map based on dataset-wide statistics, acting as a content-aware bias.
- **Mechanism:** The SE component uses Global Average Pooling to compress spatial information into a channel descriptor vector. Fully connected layers then learn to scale these channels based on global importance.
- **Core assumption:** Not all channels are equally useful for a given input; suppressing irrelevant channels improves signal-to-noise ratio.
- **Evidence anchors:** [Section III.B] notes SE produces a 1D attention vector representing the importance of each channel relative to others; [Section III.C] explains TripSE integrates TA's inter-dimensional relationships with SE's global channel attention weights.
- **Break condition:** If the network is shallow or the dataset requires all low-level features equally, aggressive channel suppression might remove critical information.

### Mechanism 3
- **Claim:** Fusing inter-dimensional maps with global channel weights via affine transformation (TripSE4) creates a superior 3D attention tensor.
- **Mechanism:** TripSE4 adds the SE global weights to the TA attention map (element-wise addition/affine transformation) before the final sigmoid. This results in a 3D weighting tensor rather than a 2D map or 1D vector.
- **Core assumption:** The optimal attention mask requires modulating local cross-dimensional features based on global context simultaneously.
- **Evidence anchors:** [Section III.D] describes TripSE4 as utilizing a "shift (translation) operation" to produce a "weighting tensor" for nuanced 3D scaling; [Section V.C] shows TripSE4 (r=1) achieving the highest accuracy (78.27%).
- **Break condition:** If the affine combination introduces noise by over-emphasizing features, or if the training data is insufficient to learn the complex 3D mapping, performance may degrade.

## Foundational Learning

- **Concept: Tensor Permutation (Reshaping)**
  - **Why needed here:** The core of Triplet Attention relies on rotating axes to apply standard operations on non-standard views.
  - **Quick check question:** If you have a tensor of shape (C, H, W), what shape does it become if you rotate it to (H, W, C) for the H-branch attention mechanism?

- **Concept: Squeeze-and-Excitation (SE) Block**
  - **Why needed here:** Understanding SE is required to grasp how TripSE modifies the standard "squeeze" to generate global weights that modify the Triplet Attention branches.
  - **Quick check question:** Why does the SE block use a reduction ratio (r) in its fully connected layers, and what is the trade-off if 'r' is set to 1?

- **Concept: Fine-tuning vs. Training from Scratch**
  - **Why needed here:** The paper notes that TripSE1 is compatible with pre-trained weights, while other variants may struggle. Understanding how architectural changes affect weight loading is critical for reproduction.
  - **Quick check question:** Why might adding an SE block inside a branch (TripSE2) disrupt the loading of pre-trained weights compared to adding it at the end (TripSE1)?

## Architecture Onboarding

- **Component map:**
  Input Tensor (X) → 3 Parallel Branches (C-branch, W-branch, H-branch) → Per-Branch Ops (Permute → Z-Pool → Conv2D → Sigmoid) → TripSE Specifics (TripSE1: Sum branches → Final SE; TripSE4: Parallel SE per branch → Add SE weights → Sigmoid → Scale Input) → Scaled Input Tensor

- **Critical path:**
  1. Verify tensor permutation logic (ensuring W/H/C dimensions are swapped correctly for each branch)
  2. Ensure the "Affine Transformation" in TripSE4 adds the 1D SE vector to the 2D TA map correctly via broadcasting
  3. Placement: Insert TripSE after each group of blocks (e.g., after Layer 1, Layer 2) in ResNet/ConvNeXt, not inside the bottleneck

- **Design tradeoffs:**
  - TripSE1 vs. TripSE4: TripSE1 is simpler and safer for fine-tuning pre-trained models. TripSE4 is more complex (3D attention) but achieves SOTA results (78.27% on FER2013) and requires careful hyperparameter tuning (reduction ratio r).
  - Stand-alone SE/TA: The paper [Table III] shows adding SE or TA alone to ConvNeXt can reduce accuracy. The synergy is specific to the TripSE combination.

- **Failure signatures:**
  - Accuracy Drop: If implementing on ConvNeXt, ensuring TripSE is added between stages, not inside the ConvNeXt block, to avoid breaking pre-trained weights.
  - Dimension Mismatch: Errors in Z-pooling or Permutation usually result in shape mismatches during the element-wise multiplication/sigmoid phase.
  - Over-regularization: If the reduction ratio r is too aggressive in the SE part, the model might lose spatial details.

- **First 3 experiments:**
  1. Baseline Verification: Replicate the "ConvNeXt-T + TripSE1" experiment on CIFAR100 (training from scratch) to verify the ~1% improvement claim before attempting complex datasets.
  2. Ablation Study: Compare TripSE1 vs. TripSE4 on a smaller dataset (FER2013 subset) to determine if the computational cost of TripSE4 is justified for your specific data.
  3. Placement Test: Try inserting TripSE1 after the stem vs. after the final layer to confirm the paper's finding that attention is most effective when distributed after block groups.

## Open Questions the Paper Calls Out

- **Open Question 1:** How do TripSE2 and TripSE3 perform in training regimes that do not rely on transfer learning or pre-trained weights? The authors state these variants are "incompatible with the existing pretrained models" and "must be tested in a different case study."

- **Open Question 2:** Does the TripSE mechanism provide consistent performance gains on large-scale generic object recognition tasks comparable to those seen in facial expression recognition? The paper reports only a "slight performance increase" of 0.1% on ImageNet, contrasting sharply with significant gains on FER2013.

- **Open Question 3:** How robust is the TripSE4 block to hyperparameter choices regarding the reduction ratio (r) across different datasets? Table III shows TripSE4 accuracy on FER2013 swings drastically from 76.71% to 78.27% depending solely on the reduction ratio (r=16 vs r=1).

## Limitations
- The method shows minimal improvement on large-scale generic classification tasks (only 0.1% on ImageNet) compared to significant gains on FER2013
- TripSE2 and TripSE3 variants are incompatible with pre-trained weights, limiting their practical applicability
- Performance is highly sensitive to the reduction ratio parameter, particularly for TripSE4

## Confidence
- **Method reproducibility:** High - Clear architectural descriptions and implementation details provided
- **Result validity:** Medium - Strong results on FER2013 but minimal improvement on ImageNet raises questions about generalizability
- **Significance:** Medium - State-of-the-art results on FER2013 are valuable, but limited impact on generic classification tasks

## Next Checks
1. Verify tensor permutation logic by printing shapes at each rotation step in the TA branches
2. Test TripSE1 with different reduction ratios (r=16 vs r=1) on CIFAR100 to understand hyperparameter sensitivity
3. Compare training from scratch vs. fine-tuning for all four TripSE variants on a small dataset to validate compatibility claims