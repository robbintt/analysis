---
ver: rpa2
title: Adaptive Thresholding for Multi-Label Classification via Global-Local Signal
  Fusion
arxiv_id: '2505.03118'
source_url: https://arxiv.org/abs/2505.03118
tags:
- label
- adaptive
- thresholding
- local
- threshold
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces an adaptive thresholding method for multi-label
  classification that fuses global label rarity (IDF) and local label agreement (KNN)
  to dynamically adjust decision boundaries. Instead of using fixed thresholds, it
  learns per-label, per-instance thresholds as differentiable penalties in the loss,
  improving calibration and reducing false positives.
---

# Adaptive Thresholding for Multi-Label Classification via Global-Local Signal Fusion

## Quick Facts
- arXiv ID: 2505.03118
- Source URL: https://arxiv.org/abs/2505.03118
- Reference count: 6
- Primary result: Macro-F1 of 0.1712 on AmazonCat-13K, outperforming static thresholding and prior methods

## Executive Summary
This paper introduces an adaptive thresholding method for multi-label classification that dynamically adjusts decision boundaries per label and per instance. The approach fuses global label rarity (IDF) with local label agreement (KNN) signals to learn differentiable penalties in the loss function, rather than applying hard cutoffs. Evaluated on AmazonCat-13K, the method achieves significant improvements over traditional static thresholding and demonstrates the importance of both global and local signals for calibration in imbalanced multi-label settings.

## Method Summary
The method computes adaptive thresholds θl(x) = λ·αl·IDFl + (1−λ)·βl·KNNl(x) + bl by fusing global IDF scores (label rarity) with local KNN signals (label co-occurrence similarity). Instead of hard thresholding, these are treated as differentiable penalties subtracted from logits before computing BCEWithLogits loss. A margin loss term further improves calibration by penalizing predictions near decision boundaries. The approach is trained end-to-end with a shallow MLP (~2.8M parameters) on AmazonCat-13K using batch size 128 and 1500 epochs.

## Key Results
- Macro-F1 of 0.1712 on AmazonCat-13K, significantly outperforming static thresholding baselines
- Full adaptive model substantially outperforms IDF-only (0.0094 macro-F1) and KNN-only (0.1456 macro-F1) ablations
- Method is lightweight, interpretable, and particularly effective for rare labels in imbalanced domains
- KNN signal provides stronger training signals for convergence compared to IDF alone

## Why This Works (Mechanism)

### Mechanism 1
Fusing global rarity (IDF) with local context (KNN) signals produces more informative thresholds than either signal alone. The adaptive threshold dynamically adjusts per-label decision boundaries - global IDF raises thresholds for rare labels to reduce false positives, while local KNN lowers thresholds when similar training instances share that label to increase recall in contextually appropriate cases. Core assumption: labels exhibit structured co-occurrence patterns that can be captured via label-space similarity. Evidence: Full adaptive model (0.1712 macro-F1) substantially outperforms both IDF-only (0.0094) and KNN-only (0.1456) ablations.

### Mechanism 2
Treating thresholds as differentiable penalties (subtracted from logits before loss) rather than hard cutoffs enables smoother gradient-based supervision. Instead of computing BCE(y, z > θ), the model computes BCEWithLogits(zl − θl, yl), penalizing false positives proportionally to how far the logit exceeds the threshold. This allows gradients to flow through both model parameters and threshold parameters. Core assumption: penalization formulation is sufficiently expressive to learn appropriate decision boundaries. Evidence: Formal BCEWithLogits(zl(x) − θl(x), yl) formulation explicitly shown.

### Mechanism 3
The margin loss term improves calibration by penalizing predictions that hover near the decision boundary. MarginLoss adds a penalty when |zl − θl| < Δ, forcing the model to make confident predictions rather than staying in an uncertain zone. This is particularly valuable for rare labels where logits naturally cluster near zero. Core assumption: confident predictions correlate with better generalization and calibration. Evidence: Formal margin loss definition with Δ = 0.1 shown in equations.

## Foundational Learning

- **Concept: Inverse Document Frequency (IDF)**
  - Why needed here: IDF provides the global rarity signal - labels appearing in few training samples receive higher base thresholds, reducing false positives on rare classes.
  - Quick check question: Given a dataset with 10,000 samples where Label A appears in 50 samples and Label B in 5,000, which label gets a higher IDF score?

- **Concept: K-Nearest Neighbors in Label Space**
  - Why needed here: The KNN signal is computed via label co-occurrence similarity (YYᵀ), not feature-space distance. Understanding this distinction is critical - similarity is defined by shared labels, not semantic embeddings.
  - Quick check question: If sample i has labels {1, 3, 5} and sample j has labels {1, 2, 4}, what is their raw KNN similarity score before normalization?

- **Concept: Macro-F1 vs. Micro-F1 in Imbalanced MLC**
  - Why needed here: The paper optimizes macro-F1, which weights each label equally regardless of frequency. This explains why rare-label thresholding matters disproportionately.
  - Quick check question: In a 10,000-label dataset where 9,000 labels have <10 positive examples each, which metric would be more sensitive to rare-label performance improvements?

## Architecture Onboarding

- **Component map:** IDF computation -> KNN signal computation -> Threshold fusion -> Base model (MLP) -> Loss computation -> Backpropagation
- **Critical path:**
  1. Precompute IDF scores for all labels (one-time)
  2. For each batch: compute KNN signal via label matrix multiplication (differentiable)
  3. Fuse signals → compute adaptive thresholds per (sample, label) pair
  4. Subtract thresholds from logits → compute combined loss → backpropagate

- **Design tradeoffs:**
  - Memory vs. batch size: KNN signal requires full batch label matrix (B×B attention); large batches increase memory O(B²). Paper uses batch size 128.
  - Static vs. dynamic IDF: Current implementation uses fixed IDF; future TF-IDF extension would require per-instance term frequencies.
  - Label-space vs. feature-space KNN: Label-space is interpretable but ignores semantic similarity; feature-space embeddings could improve local signals but sacrifice interpretability.

- **Failure signatures:**
  - Premature convergence: IDF-only models converged at 150 epochs with near-zero macro-F1 - suggests global signal alone insufficient.
  - Over-conservative predictions: If positive ratio approaches 0, thresholds may be too high; check λ and α values.
  - KNN signal degradation: If label matrix is sparse with few co-occurrences, KNN scores become noisy; consider smoothing or fallback to IDF.

- **First 3 experiments:**
  1. Reproduce ablations: Run static threshold, IDF-only, KNN-only, and full adaptive on AmazonCat-13K subset (10% data); verify macro-F1 ranking matches paper.
  2. Hyperparameter sweep on λm and Δ: Test λm ∈ {0.05, 0.1, 0.2} and Δ ∈ {0.05, 0.1, 0.2} on validation set; assess sensitivity of margin loss contribution.
  3. Apply to different dataset: Test on BibTeX or Delicious (smaller label space, denser co-occurrence); validate hypothesis that gains are modest in less-imbalanced settings.

## Open Questions the Paper Calls Out

### Open Question 1
Does integrating per-instance term frequency (TF) with the current IDF-only global signal improve threshold precision and macro-F1? Current implementation uses only IDF (global rarity); per-instance term frequencies are not incorporated. Evidence: Comparison of full TF-IDF vs. IDF-only on AmazonCat-13K showing statistically significant macro-F1 differences would resolve this.

### Open Question 2
How does the adaptive thresholding method perform on clinical datasets like MIMIC-III compared to e-commerce benchmarks? All reported results are on AmazonCat-13K; no clinical evaluation yet. Clinical domains have different error costs and label structures than product categorization. Evidence: Benchmark results on MIMIC-III with appropriate clinical metrics would resolve this.

### Open Question 3
Does the method generalize to datasets with dense label co-occurrence and lower rarity skew, where preliminary tests showed only modest gains? The mechanism may be specifically tuned to long-tailed distributions; its effectiveness under different label density regimes remains unquantified. Evidence: Systematic evaluation across datasets with controlled label sparsity and co-occurrence statistics would resolve this.

### Open Question 4
Can differentiable clustering or learned neighborhood graphs outperform the current label-space KNN for local context modeling? The current KNN signal is computed via simple label co-occurrence; more sophisticated local structure may capture different patterns. Evidence: Ablation study comparing KNN, differentiable clustering, and learned graphs on the same benchmark would resolve this.

## Limitations

- Performance gains are primarily demonstrated on highly imbalanced AmazonCat-13K; effectiveness on less-imbalanced datasets remains to be validated
- MLP architecture details are underspecified, making exact reproduction difficult
- The relative contribution of margin loss to overall performance is minimally evaluated
- No comparison with feature-space KNN to determine whether gains come from fusion or simply using KNN signals

## Confidence

- **High confidence**: The core mechanism of differentiable threshold penalties is well-specified and mathematically sound. Performance improvement over static thresholding is clearly demonstrated.
- **Medium confidence**: The relative importance of IDF vs. KNN signals is supported by ablations, but the claim that KNN provides "stronger training signals for convergence" lacks direct empirical comparison.
- **Low confidence**: The margin loss contribution is minimally evaluated - only one Δ value is tested, and no comparison to models without margin loss is provided in main results.

## Next Checks

1. **Cross-dataset validation**: Apply the full adaptive thresholding method to BibTeX and Delicious datasets; verify whether macro-F1 improvements scale inversely with label imbalance as claimed.
2. **Ablation on margin loss**: Train models with λm = 0 (no margin loss) and λm = 0.2; measure impact on macro-F1 and calibration curves to quantify margin loss contribution.
3. **Feature-space KNN comparison**: Implement KNN in feature space (using TF-IDF embeddings) alongside label-space KNN; compare performance to determine whether gains come from fusion or simply from using KNN signals.