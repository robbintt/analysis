---
ver: rpa2
title: Quantization Range Estimation for Convolutional Neural Networks
arxiv_id: '2510.04044'
source_url: https://arxiv.org/abs/2510.04044
tags:
- quantization
- neural
- accuracy
- weights
- networks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of post-training quantization
  for deep neural networks, focusing on maintaining model accuracy when reducing bit-width
  representation. The authors propose REQuant, a range estimation method that minimizes
  quantization errors by layer-wise local optimization.
---

# Quantization Range Estimation for Convolutional Neural Networks

## Quick Facts
- **arXiv ID**: 2510.04044
- **Source URL**: https://arxiv.org/abs/2510.04044
- **Reference count**: 12
- **Primary result**: REQuant achieves state-of-the-art post-training quantization performance with minimal accuracy loss at 8/6/4-bit across ResNet and Inception-v3 models.

## Executive Summary
This paper addresses post-training quantization for deep neural networks by proposing REQuant, a method that estimates optimal quantization ranges to minimize accuracy loss when reducing bit-width. The authors introduce a range estimation factor α that clips outlier weights, combined with a square root transformation to enable effective non-uniform quantization using uniform quantization hardware. Through layer-wise local optimization using golden section search, REQuant achieves near-zero accuracy loss at 8-bit and 6-bit, and significantly better performance than existing methods at 4-bit quantization on CIFAR-10 and CIFAR-100 datasets.

## Method Summary
REQuant addresses post-training quantization by introducing a range estimation factor α ∈ (0,1] that scales the quantization range to [−αwm, αwm] where wm is the maximum absolute weight. Weights outside this range are clipped to the endpoints, allocating more quantization levels to densely distributed weights near zero. The method optionally applies a square root transformation to weights before quantization and squares them after dequantization, creating smaller quantization intervals for small weights. For each layer, REQuant finds optimal α* by minimizing mean squared error between original and dequantized weights using golden section search, which exploits the local convexity of the error function. The method is applied per-layer (except first and last layers kept at 8-bit) and works with batch normalization folding.

## Key Results
- REQuant achieves near-zero accuracy loss (0.01-0.15%) at 8-bit and 6-bit quantization for ResNet-18/34/50/101 on CIFAR-10/100
- At 4-bit quantization, REQuant shows significant improvements: ResNet-50 on CIFAR-10 maintains 94.67% accuracy (vs 93.89% AdaRound, 92.93% OMSE)
- Golden section search is more efficient than bisection, achieving lower quantization loss (2.159448e-07 vs 4.156454e-07) with faster search time (39.89ms vs 76.79ms)
- Square root transformation is critical for 4-bit performance, providing 5-15% accuracy gains, with marginal benefit at 8-bit

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Clipping outlier weights via optimal range parameter α reduces quantization error while preserving model accuracy.
- **Mechanism**: Instead of using maximum absolute weight wm to define quantization range, introduce α ∈ (0,1] to scale the range to [−αwm, αwm]. Weights outside this interval are clipped to endpoints. This allocates more quantization levels to densely distributed weights near zero, preserving their discriminative differences.
- **Core assumption**: The majority of informative weights cluster near zero; extreme values (outliers) contribute less to model output and can be clipped without significant accuracy loss.
- **Evidence anchors**:
  - [abstract]: "We model the range estimation into an optimization problem of minimizing quantization errors by layer-wise local loss."
  - [section 3.1]: "using the maximum weights to compute s will lead to a large quantization interval, one that is unable to achieve the goal of quantizing densely distributed weights and preserving their 'difference' in quantized space"
  - [corpus]: SplitQuant (arxiv:2501.12428) similarly observes "Quantization often maps different original values to a single quantized value because the range of the original distribution is too large"
- **Break condition**: If weight distributions are multimodal or if outlier weights encode critical rare features (e.g., in anomaly detection), clipping may discard essential information.

### Mechanism 2
- **Claim**: Square root transformation of weights enables effective non-uniform quantization while maintaining uniform quantization computational structure.
- **Mechanism**: Apply √|w| before quantization and square after dequantization. This nonlinear mapping creates smaller effective quantization intervals for small weights (densely distributed) and larger intervals for large weights (sparsely distributed), improving precision where it matters most.
- **Core assumption**: Weight distributions in CNNs are approximately bell-shaped (concentrated near zero), making power-law transformations more suitable than logarithmic alternatives.
- **Evidence anchors**:
  - [section 3.2]: "power functions are better choice than logarithmic functions. The former takes into account the bell-shaped distribution of the weights of the convolutional neural network"
  - [section 3.2]: "we can adaptive allocate the quantization intervals so that smaller quantization interval is applied when quantizing the densely distributed weights"
  - [corpus]: PowerQuant (Yvinec et al., ICLR 2023, cited in paper) similarly uses power function transformations for non-uniform quantization
- **Break condition**: If weight distributions are approximately uniform, the reshaping transformation provides minimal benefit and adds unnecessary computational overhead.

### Mechanism 3
- **Claim**: The layer-wise optimization for α can be solved efficiently via golden section search due to local convexity of the error function.
- **Mechanism**: The quantization error function f(α,b) has positive second derivative (∂²f/∂α² > 0) almost everywhere, establishing local convexity. Golden section search (φ=0.618) exploits this to find optimal α in O(log(1/ε)) iterations, where ε is convergence tolerance.
- **Core assumption**: Each layer can be optimized independently (layer-wise independence); local minima sufficiently approximate the global optimum for practical purposes.
- **Evidence anchors**:
  - [Lemma 1, section 3.1]: "∂²f(α,b)/∂α² > 0" — mathematical proof of local convexity
  - [Table 5]: Golden section search achieves lowest quantization loss (2.159448e-07 for layer1.0.conv1) with fastest search time (39.89ms vs 76.79ms bisection)
  - [corpus]: Limited direct corpus support for this specific convexity result; this is a methodological contribution of this paper
- **Break condition**: If strong cross-layer dependencies exist (errors compound non-locally), layer-wise optimization may be suboptimal; joint optimization across layers would be needed.

## Foundational Learning

- **Concept: Uniform vs Non-uniform Quantization**
  - Why needed here: REQuant achieves non-uniform quantization effects using uniform quantization hardware via weight transformation.
  - Quick check question: Given weights distributed as N(0,1), would allocating more quantization levels near zero or near the tails better preserve information at 4-bit precision?

- **Concept: Mean Squared Error as Quantization Objective**
  - Why needed here: The optimization problem minimizes MSE between original weights w and dequantized weights w' = wq × s.
  - Quick check question: If w = 0.5, quantized wq = 15, and scale s = 0.033, what is the contribution to MSE for this single weight?

- **Concept: Local Convexity and Golden Section Search**
  - Why needed here: Understanding why f''(α) > 0 guarantees search convergence builds trust in the algorithm.
  - Quick check question: If a function has f''(x) > 0 everywhere in interval [a,b], and you evaluate f at two interior points x1 < x2, what can you infer about the minimum location if f(x1) < f(x2)?

## Architecture Onboarding

- **Component map**:
  Pretrained FP32 Model → For each layer i: Extract weights Wi, compute wm = max(|Wi|) → [Optional] Transform: w̃ = sign(w) × √|w| → Run GSSearch to find α* minimizing f(α,b) or g(α,b) → Compute scale: s = α*wm / (2^(b-1) - 1) → Quantize: wq = clip(round(w̃ / s)) → Quantized Model (store α* per layer) → At inference: Dequantize w' = sign(wq) × (|wq| × s)² [if reshape]

- **Critical path**:
  1. Batch normalization folding must be applied before quantization (cited: Krishnamoorthi 2018)
  2. Input and output layers kept at 8-bit (standard practice)
  3. Per-layer α* optimization via GSSearch — this is the computational bottleneck but runs once per model

- **Design tradeoffs**:
  - **Per-layer vs global α**: Per-layer achieves better accuracy but requires storing one α per layer; global α simplifies deployment but sacrifices ~0.5-2% accuracy at 4-bit
  - **With vs without reshape**: Reshape is critical at 4-bit (5-15% accuracy gain), marginal at 8-bit; adds one multiply-square operation per weight at inference
  - **Convergence tolerance ε**: Paper uses implicit ε via fixed iterations; tighter ε improves α* marginally but increases search time

- **Failure signatures**:
  - **4-bit accuracy drops >5% on ResNet**: Check if α* values are very small (< 0.5) indicating over-clipping, or if reshape was accidentally skipped
  - **Large variance in α* across layers** (e.g., 0.6 to 0.95): May indicate unstable weight distributions; consider per-channel quantization instead of per-layer
  - **GSSearch fails to converge**: Verify weights are not all zeros or constants; check numerical precision in MSE computation

- **First 3 experiments**:
  1. **Baseline replication**: Train ResNet-18 on CIFAR-10 to ~95% accuracy, apply REQuant at 8/6/4 bits, verify results match Table 1 (95.15%, 95.10%, 94.67%)
  2. **Ablation study**: On same model, test "no clip + no reshape" → "clip only" → "reshape only" → "both" to isolate contribution of each component (reference Table 3 structure)
  3. **Generalization test**: Apply REQuant to a different architecture (e.g., MobileNet-V2) on CIFAR-100 to assess if local convexity assumption and optimal α distributions transfer across architectures

## Open Questions the Paper Calls Out
- **Question**: Does REQuant maintain its state-of-the-art accuracy when applied to large-scale datasets like ImageNet?
  - **Basis in paper**: [explicit] The authors state in the Conclusion: "Some interesting work are to... verify it on the ImageNet dataset."
  - **Why unresolved**: The experiments are restricted to CIFAR-10 and CIFAR-100, which use lower resolution images and fewer classes than ImageNet.
  - **What evidence would resolve it**: Reporting Top-1 and Top-5 accuracy on ImageNet for ResNet and Inception-v3 models using the REQuant method.

- **Question**: How can the method be adapted to improve performance in extremely low-bit settings (e.g., 2-bit or 3-bit)?
  - **Basis in paper**: [explicit] The Conclusion lists "further improve quantization model under low-bit setting" as future work.
  - **Why unresolved**: The paper demonstrates significant accuracy degradation at 4-bit quantization for Inception-v3 on CIFAR-100 (dropping to 50.72%), indicating the current method struggles at the low-bit frontier.
  - **What evidence would resolve it**: Demonstrating competitive accuracy (<1% drop) on 2-bit or 3-bit quantization, or specifically fixing the 4-bit degradation seen in complex models like Inception-v3.

- **Question**: What is the computational overhead and inference latency impact of the square root and squaring operations during runtime?
  - **Basis in paper**: [inferred] The method introduces a power transformation (sqrt) and an inverse transformation (square) to reshape weight distributions (Eq. 5 and 6) without measuring the associated compute cost.
  - **Why unresolved**: While the method reduces model size, the complex non-linear operations required for de-quantization or transformation may negate the speed benefits typically sought in quantization for edge devices.
  - **What evidence would resolve it**: Benchmarks of inference latency and throughput (e.g., images/sec) comparing REQuant to standard uniform quantization on target hardware.

## Limitations
- The method's performance on larger-scale datasets like ImageNet remains untested, limiting confidence in real-world applicability
- Claims about computational efficiency lack detailed timing comparisons with competing methods
- The paper doesn't address potential overfitting to CIFAR datasets or how well the method generalizes to transformer architectures

## Confidence
- **High confidence**: The basic REQuant framework (clipping with α-range estimation) works as described, supported by ablation studies showing consistent accuracy improvements over baselines.
- **Medium confidence**: The reshape transformation's effectiveness is well-demonstrated for 4-bit quantization but less clearly justified for higher bit-widths; the mathematical proofs of local convexity appear sound but are limited to specific conditions.
- **Low confidence**: Claims about computational efficiency relative to competing methods lack detailed timing comparisons, and the paper doesn't address potential overfitting to CIFAR datasets.

## Next Checks
1. **Cross-architecture generalization**: Apply REQuant to MobileNet-V2 and EfficientNet-B0 on CIFAR-100 to test if optimal α distributions and local convexity assumptions transfer beyond ResNet architectures.
2. **Larger scale validation**: Evaluate on ImageNet with ResNet-50 to verify claims hold for larger models and datasets, particularly testing 4-bit performance where accuracy gaps are most significant.
3. **Ablation of layer independence**: Compare layer-wise versus joint optimization approaches on a small network to quantify the impact of the independence assumption on final accuracy.