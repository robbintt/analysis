---
ver: rpa2
title: 'Nile-Chat: Egyptian Language Models for Arabic and Latin Scripts'
arxiv_id: '2507.04569'
source_url: https://arxiv.org/abs/2507.04569
tags:
- arabic
- egyptian
- arxiv
- datasets
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces Nile-Chat, a family of large language models
  for Egyptian Arabic that supports both Arabic and Latin scripts. The key innovation
  is a novel Branch-Train-MiX (BTX) strategy to merge script-specialized experts into
  a single Mixture-of-Experts model.
---

# Nile-Chat: Egyptian Language Models for Arabic and Latin Scripts

## Quick Facts
- **arXiv ID:** 2507.04569
- **Source URL:** https://arxiv.org/abs/2507.04569
- **Reference count:** 40
- **Primary result:** Nile-Chat significantly outperforms leading multilingual and Arabic LLMs on Egyptian Arabic benchmarks, achieving up to 14.4% improvement over Qwen2.5-14B-Instruct on Latin-script tasks.

## Executive Summary
This work introduces Nile-Chat, a family of large language models for Egyptian Arabic that supports both Arabic and Latin scripts. The key innovation is a novel Branch-Train-MiX (BTX) strategy to merge script-specialized experts into a single Mixture-of-Experts model. Three variants are released: dense 4B and 12B models, and a 3x4B-A6B MoE model. The models are trained on dual-script Egyptian datasets and fine-tuned for instruction following and alignment. Nile-Chat significantly outperforms leading multilingual and Arabic LLMs on newly introduced Egyptian benchmarks, achieving up to 14.4% improvement over Qwen2.5-14B-Instruct on Latin-script tasks. This is the first LLM explicitly supporting both scripts for a widely spoken dialect, with all resources publicly available.

## Method Summary
Nile-Chat adapts Gemma-3 for Egyptian Arabic in both Arabic and Latin scripts using dense and Mixture-of-Experts (MoE) architectures. The approach involves pre-training on a 1.15B word Egyptian corpus (with ~22% transliterated to Latin via Claude), followed by SFT on 1.85M instruction pairs, and alignment via DPO. The dense model uses LoRA fine-tuning, while the MoE variant employs a Branch-Train-MiX strategy where base model is branched into script-specific experts (Arabic-only and Latin-only pre-training), then merged into a 3-expert MoE. Training uses 8x A100 GPUs with LoRA rank 256 for dense models and alpha 512 for MoE.

## Key Results
- Nile-Chat achieves up to 14.4% improvement over Qwen2.5-14B-Instruct on Latin-script Egyptian tasks
- The 3x4B-A6B MoE variant outperforms dense counterparts while maintaining efficiency
- New Egyptian benchmarks (Egyptian-MMLU, Egyptian-AlpacaEval) show consistent improvements across both script variants
- Model successfully handles code-switching and transliteration tasks better than baselines

## Why This Works (Mechanism)
The Branch-Train-MiX strategy allows specialized training for each script before merging into a unified MoE architecture, enabling the model to dynamically route tokens to appropriate script experts while maintaining shared attention and embedding parameters for efficiency.

## Foundational Learning
- **Egyptian Arabic dialect characteristics** (why needed: model must capture unique phonology and vocabulary; quick check: verify training data includes EFC-mini and other dialect-specific sources)
- **Dual-script processing** (why needed: handle both Arabic and Latin representations of same language; quick check: test model on transliteration pairs)
- **Mixture-of-Experts routing** (why needed: enable efficient specialization while maintaining unified knowledge; quick check: analyze router activation patterns)
- **Instruction fine-tuning** (why needed: align model to follow Egyptian Arabic instructions; quick check: evaluate on Egyptian-SFT-Mixture)
- **Preference optimization** (why needed: correct over-cautiousness and code-switching issues; quick check: verify DPO targets in training data)
- **LoRA adaptation** (why needed: enable efficient fine-tuning of large models; quick check: confirm rank 256 parameters)

## Architecture Onboarding
- **Component map:** Gemma-3 base -> Branch into Script-Specific Experts -> Merge (BTX) -> LoRA SFT -> Full DPO
- **Critical path:** Pre-training (unified or branched) → SFT fine-tuning → DPO alignment determines final performance
- **Design tradeoffs:** MoE provides efficiency vs. dense models; synthetic data scalability vs. cultural authenticity; script specialization vs. generalization
- **Failure signatures:** Excessive code-switching in Arabic prompts; Western cultural biases from Claude-generated data; potential hallucinations in responses
- **First experiments:** 1) Test basic script conversion accuracy; 2) Evaluate instruction following on Egyptian-SFT-Mixture; 3) Check code-switching behavior on mixed prompts

## Open Questions the Paper Calls Out
- How does reliance on Claude for synthetic data impact the model's ability to capture authentic cultural nuances compared to human-curated Egyptian Arabic data?
- To what extent are the hallucinations observed in the model correlated with the specific challenges of dual-script generation versus standard pre-training data limitations?
- Does the Branch-Train-MiX routing mechanism specialize experts strictly by script (Arabic vs. Latin), or does it learn to route based on semantic or task-based features?

## Limitations
- Reliance on Claude 3.5 Sonnet for data generation introduces potential biases and artifacts
- BTX strategy lacks complete implementation details, making exact reproduction challenging
- Evaluation focuses primarily on Egyptian Arabic without extensive cross-dialectal validation

## Confidence
- **High Confidence:** Core methodology of LoRA fine-tuning for dense models and basic MoE architecture are well-established
- **Medium Confidence:** BTX strategy's effectiveness and relative contribution of each training stage are moderately well-supported
- **Low Confidence:** Claims about cultural awareness and elimination of Western cultural biases are not empirically validated

## Next Checks
1. Reproduce the BTX merge process and validate exact MoE initialization procedure
2. Test Nile-Chat's performance on Arabic dialects beyond Egyptian to assess generalizability
3. Conduct systematic qualitative and quantitative analysis of model outputs for Western cultural bias