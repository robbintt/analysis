---
ver: rpa2
title: 'You Need Reasoning to Learn Reasoning: The Limitations of Label-Free RL in
  Weak Base Models'
arxiv_id: '2511.04902'
source_url: https://arxiv.org/abs/2511.04902
tags:
- reasoning
- learning
- training
- arxiv
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper identifies a critical limitation in label-free reinforcement
  learning for reasoning: smaller models with weak reasoning capabilities often fail
  to improve and may even collapse, as they cannot generate sufficiently long or diverse
  chain-of-thought reasoning to enable effective self-reflection. To address this,
  the authors propose CuMa, a curriculum-guided masked majority voting reinforcement
  learning approach that progressively introduces harder problems, masks rewards for
  no-majority rollouts, and uses curated synthetic data of predefined difficulty levels.'
---

# You Need Reasoning to Learn Reasoning: The Limitations of Label-Free RL in Weak Base Models

## Quick Facts
- arXiv ID: 2511.04902
- Source URL: https://arxiv.org/abs/2511.04902
- Reference count: 25
- Key outcome: Small models (0.5B) collapse with standard label-free RL; CuMa achieves 32.8% on Math 500 vs 23.4% baseline.

## Executive Summary
This paper identifies a critical limitation in label-free reinforcement learning for reasoning: smaller models with weak reasoning capabilities often fail to improve and may even collapse, as they cannot generate sufficiently long or diverse chain-of-thought reasoning to enable effective self-reflection. To address this, the authors propose CuMa, a curriculum-guided masked majority voting reinforcement learning approach that progressively introduces harder problems, masks rewards for no-majority rollouts, and uses curated synthetic data of predefined difficulty levels. This method consistently improves performance across all model sizes (0.5B to 7B parameters) and reasoning strengths, achieving up to 32.8% on Math 500 for the 0.5B model, compared to collapse or degradation with existing methods. Ablation studies confirm that each component—curriculum learning, reward masking, and curated data—is essential for stable improvement.

## Method Summary
The method uses GRPO with three key components: (1) curriculum learning—train sequentially on difficulty bins D1→D5; (2) reward masking—mask samples where no majority consensus exists; (3) reward based on matching the majority vote. The approach uses Qwen2.5-0.5B/1.5B/3B/7B base models, N=8 candidate responses per prompt at temperature 0.6, and synthetic data curated into 5 difficulty levels.

## Key Results
- Standard TTRL causes collapse in weak models (0.5B), dropping to near-zero accuracy.
- CuMa improves Math 500 accuracy from 23.4% to 32.8% for 0.5B models.
- All three components (curriculum, masking, curated data) are necessary—ablations show degradation without any.
- Method works across all model sizes, including 7B models.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** If training data difficulty is aligned with the model's current reasoning capacity, label-free RL can stabilize and improve weak base models that would otherwise collapse.
- **Mechanism:** The CuMa method partitions data into 5 difficulty bins. By training sequentially from easy (Level 1) to hard (Level 5), the model first masters high-probability simple tasks. This builds a gradient foundation before attempting low-probability complex tasks, preventing the "cold start" problem where weak models find no learnable signal in hard data.
- **Core assumption:** Weak models possess sufficient base capability to solve "easy" problems correctly enough to establish a majority vote signal.
- **Evidence anchors:**
  - [abstract] "training data difficulty plays a crucial role in determining success."
  - [page 3, section 2.4] Shows performance degradation as data difficulty shifts from Level 1-2 to Level 1-5 for the 0.5B model.
  - [corpus] "Reasoning with Sampling: Your Base Model is Smarter Than You Think" suggests base models retain latent capabilities that sampling strategies can unlock, supporting the premise that weak models can succeed if conditions (difficulty) are right.
- **Break condition:** If the "easy" bin contains problems the base model cannot solve even once (accuracy ~0%), the curriculum bootstrap fails.

### Mechanism 2
- **Claim:** Masking rewards for rollouts with no majority consensus prevents the reinforcement of noisy or incorrect pseudo-labels, which is critical for small models with high variance in outputs.
- **Mechanism:** In standard TTRL, if a weak model generates 8 diverse but incorrect answers, majority voting picks the most frequent wrong answer as the pseudo-label, reinforcing error. CuMa assigns a mask value (effectively ignoring the sample) if `max(count) < 2`. This ensures the model updates only on high-confidence agreement, filtering noise without requiring external labels.
- **Core assumption:** Majority consensus in the model's own rollouts correlates with correctness (self-consistency principle).
- **Evidence anchors:**
  - [page 3, section 3] "Masking the learning signal on such samples ensures that the model does not receive negative feedback from inconclusive examples."
  - [page 4, table 2] Ablation study shows performance drops from 32.8 to 30.7 when reward masking is removed.
  - [corpus] No direct corpus neighbor explicitly debates masking strategies, though "The First Few Tokens Are All You Need" emphasizes prefix consistency as a signal, indirectly supporting consistency-based filtering.
- **Break condition:** If the model becomes over-confident in incorrect patterns (reward hacking), masking may inadvertently filter out correct but minority reasoning paths, though this is less likely in the early stages of weak models.

### Mechanism 3
- **Claim:** Synthetic data curation with explicit difficulty levels provides the necessary volume of "learnable" examples to sustain a curriculum for weak models.
- **Mechanism:** Existing datasets may lack enough "very easy" examples for a 0.5B model. The pipeline uses an LLM to generate problems specifically tagged Level 1-5. This ensures the curriculum bins (Mechanism 1) are populated sufficiently to train the model before it graduates to harder tasks.
- **Core assumption:** An LLM can accurately estimate difficulty and generate solvable problems for a much smaller model.
- **Evidence anchors:**
  - [page 3, section 3] "...we have curated additional unlabelled samples by using LLM as the data generator... creating synthetic problems at varying difficulty levels."
  - [page 8, section C.2] Ablation shows a significant drop to 24.5 without curated data, compared to 32.8 with it.
  - [corpus] Corpus evidence on synthetic data specifically for curriculum RL is weak in the provided neighbors.
- **Break condition:** If the synthetic data distribution drifts from the target benchmark domain, the model may overfit to synthetic patterns without improving real-world reasoning.

## Foundational Learning

- **Concept:** **Majority Voting (Self-Consistency)**
  - **Why needed here:** This is the core "label-free" reward signal. Understanding that `argmax(count(answers))` serves as a proxy for ground truth is essential to grasp why TTRL fails (wrong majority) and why CuMa fixes it (masking no-majority).
  - **Quick check question:** If a model generates [A, B, B, C, D], what is the pseudo-label, and what happens if the true answer is A?

- **Concept:** **Curriculum Learning**
  - **Why needed here:** The central thesis is that weak models cannot learn from data that is "too hard." You must understand the idea of organizing training from easy to hard to see why standard RL (random/batch sampling) overloads weak models.
  - **Quick check question:** Why would training on a dataset where the model has 0% accuracy lead to degradation rather than exploration?

- **Concept:** **Model Collapse / Degradation**
  - **Why needed here:** The paper explicitly identifies collapse as a risk. You need to distinguish between "not learning" (plateau) and "collapse" (performance dropping to near zero, as seen in TTRL for 0.5B models).
  - **Quick check question:** In the context of RL, how does reinforcing a wrong pseudo-label lead to a feedback loop that destroys the model's base capabilities?

## Architecture Onboarding

- **Component map:** Data Loader -> Rollout Engine (N=8, temp=0.6) -> Consensus Filter -> Reward Signaler -> Optimizer (GRPO)
- **Critical path:** The curriculum schedule. Moving from Bin 1 to Bin 5 too quickly is the primary failure mode. The system relies on the *delta* in difficulty being small enough that the accuracy in the new bin is non-zero.
- **Design tradeoffs:**
  - **Sampling Cost:** Generating 8 responses per prompt (N=8) increases inference cost 8x during training.
  - **Masking vs. Negative Reward:** The paper chooses to mask (ignore) inconclusive samples. An alternative is negative reward (penalizing inconsistency), but this risks penalizing valid "out of the box" reasoning.
  - **Synthetic vs. Real:** Relying on synthetic data for early curriculum stages trades data authenticity for density of easy examples.
- **Failure signatures:**
  - **Immediate Collapse (Step 0-50):** Accuracy drops to 0% immediately. Cause: Starting curriculum on too hard a bin or masking threshold too strict.
  - **Plateau at Low Accuracy:** Model learns easy problems but fails to generalize to next bin. Cause: Curriculum gap between bins is too large.
  - **No-Majority Loop:** 100% of batches are masked, no gradient updates occur. Cause: Model too weak or temperature too high.
- **First 3 experiments:**
  1. **Sanity Check Curriculum:** Train the 0.5B model using *only* the Level 1 (easiest) synthetic data. Verify that majority voting yields >50% consensus and accuracy improves.
  2. **Ablate Reward Masking:** Run CuMa on the 0.5B model with reward masking disabled (treat no-majority as reward=0). Compare degradation rates against the full CuMa implementation.
  3. **Cross-Model Generalization:** Apply the *same* fixed curriculum generated for the 0.5B model to the 7B model. Check if the 7B model still improves (testing if the curriculum is "optimal" for weak models or generally useful).

## Open Questions the Paper Calls Out
None

## Limitations
- Synthetic data quality and domain transfer concerns—limited validation that LLM-generated difficulty estimates are accurate for 0.5B models.
- Curriculum schedule ambiguity—exact number of steps per bin and transition criteria not specified.
- Temperature and sampling trade-offs—no exploration of sensitivity to sampling parameters.

## Confidence
- **High Confidence:** Observation that weak models collapse under standard TTRL is well-supported; reward masking preventing reinforcement of wrong pseudo-labels is logically sound.
- **Medium Confidence:** Curriculum learning mechanism is plausible but underspecified scheduling and bin definitions make robustness hard to assess.
- **Low Confidence:** Synthetic data curation process is weakest link—no validation that LLM's difficulty estimates are accurate for target 0.5B model.

## Next Checks
1. **Domain Generalization Test:** Train CuMa on synthetic data, then evaluate on a held-out real reasoning dataset not seen during training. Measure whether performance gains on synthetic bins transfer to real problems.
2. **Curriculum Robustness Sweep:** Vary the number of steps per difficulty bin (e.g., 500, 1000, 2000) and the transition criteria (e.g., accuracy threshold vs. fixed steps). Plot accuracy vs. training time to identify whether gains come from curriculum structure or just more training.
3. **Masking Sensitivity Analysis:** Run ablations with different masking thresholds (e.g., majority of 3+, 4+, or no masking at all) and with negative rewards instead of masking. Compare final accuracy and collapse rates to determine if masking is the key innovation or if any consistency-based filtering helps.