---
ver: rpa2
title: 'Timeseries Foundation Models for Mobility: A Benchmark Comparison with Traditional
  and Deep Learning Models'
arxiv_id: '2504.03725'
source_url: https://arxiv.org/abs/2504.03725
tags:
- timegpt
- foundation
- forecasting
- learning
- mobility
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper benchmarks TimeGPT, a foundation model for time series
  forecasting, against traditional and deep learning approaches for mobility prediction
  using bike-sharing data from New York City (BikeNYC) and Vienna (BikeVIE). The study
  evaluates forecasting accuracy across short (1-hour), medium (12-hour), and long-term
  (24-hour) horizons.
---

# Timeseries Foundation Models for Mobility: A Benchmark Comparison with Traditional and Deep Learning Models

## Quick Facts
- arXiv ID: 2504.03725
- Source URL: https://arxiv.org/abs/2504.03725
- Authors: Anita Graser
- Reference count: 17
- Key outcome: TimeGPT shows strong short-term (1-hour) forecasting accuracy for mobility data but converges to seasonal naive performance at 24-hour horizons

## Executive Summary
This paper benchmarks TimeGPT, a foundation model for time series forecasting, against traditional and deep learning approaches for mobility prediction using bike-sharing data from New York City (BikeNYC) and Vienna (BikeVIE). The study evaluates forecasting accuracy across short (1-hour), medium (12-hour), and long-term (24-hour) horizons. For BikeNYC, TimeGPT outperforms several deep learning models at 1-hour forecasts, achieving an RMSE of 5.70, though it converges to seasonal naive performance at 24 hours. For BikeVIE, TimeGPT shows marginal improvements over AutoARIMA at longer horizons but struggles with forecast uncertainty. The results highlight TimeGPT's potential for mobility forecasting, particularly in data-sparse scenarios, while also revealing limitations in handling irregular patterns and longer-term predictions.

## Method Summary
The study compares TimeGPT's zero-shot forecasting capabilities against traditional models (AutoARIMA, Seasonal Naive, HistoricAverage) using bike-sharing data from BikeNYC (128 grid cells, 16x8) and BikeVIE (120 stations). Data is resampled to hourly intervals and evaluated using rolling window backtesting on the last ten days. RMSE is calculated for 1-hour, 12-hour, and 24-hour forecasting horizons. TimeGPT is accessed via external API without local training, while baseline models are implemented locally. The evaluation focuses on individual grid cells/stations before aggregation.

## Key Results
- TimeGPT achieves 1-hour RMSE of 5.70 on BikeNYC, outperforming deep learning models
- At 24-hour horizon, TimeGPT converges to seasonal naive baseline (RMSE 8.93) on BikeNYC
- TimeGPT shows marginal improvements over AutoARIMA at longer horizons on BikeVIE
- Model struggles with irregular restocking patterns affecting forecast uncertainty

## Why This Works (Mechanism)

### Mechanism 1
TimeGPT leverages pre-trained temporal representations to perform zero-shot forecasting on mobility data without local weight updates. The model utilizes a generic understanding of time series patterns (likely learned from massive diverse corpora) to map input sequences to future trajectories. This allows it to bypass the "cold start" problem typical of deep learning models like ST-ResNet, which require training on the specific target dataset. Core assumption: The statistical properties and temporal dynamics of the pre-training data sufficiently overlap with urban mobility patterns (bike flows) to allow immediate generalization. Evidence anchors: [abstract] mentions "ability to generate zero-shot predictions, allowing them to be applied directly to new tasks without retraining." [page 1] Introduction notes foundation models "leverage knowledge across diverse datasets."

### Mechanism 2
Forecasting accuracy degrades toward naive baseline performance as the prediction horizon extends. In the absence of specific local context (like weather or events mentioned in the conclusion), the foundation model defaults to conservative seasonal estimates for longer horizons. It effectively loses the signal of immediate historical trends and relies on generalized periodic averages, mimicking a "Seasonal Naive" approach. Core assumption: Short-term horizons (1-hour) are driven by momentum and immediate autocorrelation, while long-term horizons (24-hour) are driven primarily by seasonality. Evidence anchors: [page 3] TimeGPT "converges to the SeasonalNaive performance at 24 hours" with an RMSE of 8.93 vs 8.93 for the baseline. [page 5] Conclusion states, "TimeGPT struggles with longer horizons, with RMSE more than doubling from 1-hour to 12-hour forecasts."

### Mechanism 3
Model generalization is constrained by the irregularity of operational logistics in the target system. Standard time series models (both foundation and ARIMA) assume a degree of consistent stochastic behavior. However, bike-sharing systems often undergo manual "restocking" or redistribution, which violates temporal consistency. The foundation model interprets these irregular jumps as noise or uncertainty rather than signal, reducing predictive precision. Core assumption: The underlying data generation process follows a consistent function of time, which breaks down when human operators manually intervene (restocking). Evidence anchors: [page 4] Discussion notes "poor performance of SeasonalNaive... suggests the presence of irregular station restocking patterns that affect predictability." [page 5] Conclusion suggests exploring "auxiliary information" to mitigate this.

## Foundational Learning

- **Concept: Zero-shot Forecasting**
  - **Why needed here:** This is the core value proposition of TimeGPT evaluated in the paper. It distinguishes the approach from traditional DL models (ST-ResNet) that require training on the specific dataset.
  - **Quick check question:** Can you explain why a model trained on generic data might predict bike flows in a city it has never "seen" before?

- **Concept: Spatiotemporal vs. Univariate Time Series**
  - **Why needed here:** The paper uses BikeNYC (16x8 grid) and BikeVIE (120 stations). While the evaluation is aggregated, understanding that mobility is inherently spatial is key to understanding why spatial DL models (ST-ResNet) usually exist and why applying a univariate foundation model is a specific test of generalization.
  - **Quick check question:** How does treating a city-wide grid of bike stations as independent time series (univariate approach) potentially lose information compared to a spatial model?

- **Concept: Baseline Convergence**
  - **Why needed here:** A critical finding is that sophisticated models converge to "Seasonal Naive" (predicting the value from 24 hours ago) at long horizons. Understanding what a Seasonal Naive model is allows you to interpret the RMSE results (e.g., 8.93 at 24h) effectively.
  - **Quick check question:** If TimeGPT and Seasonal Naive both have an RMSE of 8.93 for a 24-hour forecast, what does that imply about the "complexity" the model is actually learning?

## Architecture Onboarding

- **Component map:** BikeNYC/BikeVIE data -> Hourly resampling -> TimeGPT API (zero-shot) vs Local baselines -> Rolling window backtesting -> RMSE calculation
- **Critical path:** 1. Data ingestion and resampling (critical for BikeVIE to handle cutoff gaps) 2. Formatting data for TimeGPT API (ensuring timestamp and value structure) 3. Executing rolling window evaluation to prevent look-ahead bias
- **Design tradeoffs:** Speed vs. Accuracy: TimeGPT offers immediate inference (zero-shot) but may sacrifice long-horizon accuracy compared to a fully trained, domain-specific DL model. Granularity vs. Context: The paper evaluates specific points/stations; aggregating these into a city-wide metric might hide local failures or successes.
- **Failure signatures:** Naive Equivalence: RMSE at 24h matches the Seasonal Naive baseline exactly (indicating the model has ceased to learn unique features and is just echoing yesterday). Irregularity Drift: High uncertainty or error spikes on BikeVIE data due to restocking events that look like sudden, non-organic jumps in availability.
- **First 3 experiments:** 1. Short-horizon Verification: Run TimeGPT on a single BikeNYC grid cell for a 1-hour forecast to verify API connectivity and reproduce the RMSE ~5.70 benchmark. 2. Baseline Horizon Scan: Compare TimeGPT against AutoARIMA and SeasonalNaive on BikeVIE specifically for 12h and 24h windows to observe the "convergence" effect where TimeGPT's advantage fades. 3. Gap Analysis: Introduce a synthetic data gap (missing data) in the input to test the hypothesis that foundation models are superior in "data-sparse scenarios" mentioned in the conclusion.

## Open Questions the Paper Calls Out

- **Can time series foundation models maintain high performance on previously unpublished mobility datasets?** Basis in paper: [explicit] The authors state, "Future studies should evaluate foundation models on previously unpublished datasets" to address concerns about data contamination. Why unresolved: Public benchmarks like BikeNYC may have been included in the training data of foundation models like TimeGPT, potentially inflating performance metrics. What evidence would resolve it: Benchmarking TimeGPT against baselines using proprietary or newly collected datasets confirmed to be absent from the model's pre-training corpus.

- **How does the inclusion of exogenous variables (e.g., weather, events) affect the predictive accuracy of foundation models for mobility?** Basis in paper: [explicit] The paper concludes, "Future research should furthermore explore the impact of auxiliary information, such as weather and event data, on model predictive performance when incorporated as covariates." Why unresolved: The current experiments relied solely on historical time series data, ignoring external factors that significantly influence bike-sharing demand. What evidence would resolve it: A comparative analysis of TimeGPT's forecasting error (RMSE) with and without the integration of weather and event covariates.

- **To what extent can domain-specific deep learning models outperform foundation models as the volume of training data increases?** Basis in paper: [inferred] The authors suggest "it remains likely that domain-specific deep learning models will outperform foundation models when sufficiently trained," but the current study focuses on zero-shot capabilities. Why unresolved: The benchmark does not evaluate the scaling laws of domain-specific models (e.g., ST-ResNet) against foundation models across varying data availability levels. What evidence would resolve it: Experiments comparing the performance trajectory of TimeGPT versus ST-ResNet as the training dataset size is systematically increased.

## Limitations

- Evaluation relies on zero-shot forecasting where pre-training corpus and model architecture are opaque, making it difficult to diagnose failures or attribute performance gains to specific mechanisms
- Convergence to seasonal naive performance at longer horizons suggests fundamental limitations in capturing long-term dependencies without local fine-tuning or exogenous inputs
- Bike-sharing data exhibits irregular operational patterns (restocking events) that violate standard time series assumptions, potentially confounding the evaluation of model capabilities

## Confidence

- **High Confidence:** Short-horizon (1-hour) forecasting advantage of TimeGPT over deep learning models, supported by concrete RMSE values (5.70 vs higher values for competitors)
- **Medium Confidence:** The convergence to seasonal naive baseline at 24 hours, as this is observed but the underlying mechanism (horizon decay due to lack of local context) is inferred rather than directly tested
- **Low Confidence:** The claim about TimeGPT's superiority in data-sparse scenarios, as the paper does not explicitly test or demonstrate this capability with controlled experiments

## Next Checks

1. **API Parameter Sensitivity Analysis:** Systematically vary TimeGPT API parameters (frequency, max_tokens, model_version) to determine their impact on forecast accuracy and identify optimal configurations for mobility data
2. **Exogenous Variable Integration:** Incorporate weather data as additional covariates in the forecasting pipeline to test whether external context can prevent the degradation to seasonal naive performance at longer horizons
3. **Restocking Event Detection:** Develop an automated method to identify restocking events in the BikeVIE data and measure their correlation with forecast errors to quantify the impact of irregular operational patterns on model performance