---
ver: rpa2
title: 'IFFair: Influence Function-driven Sample Reweighting for Fair Classification'
arxiv_id: '2512.07249'
source_url: https://arxiv.org/abs/2512.07249
tags:
- fairness
- iffair
- uence
- utility
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the problem of bias in machine learning classification
  models, particularly against unprivileged groups. The proposed method, IFFair, uses
  influence functions to quantify the impact of each training sample on group fairness
  and dynamically reweights samples to mitigate bias without modifying data, network
  structure, or decision boundaries.
---

# IFFair: Influence Function-driven Sample Reweighting for Fair Classification

## Quick Facts
- arXiv ID: 2512.07249
- Source URL: https://arxiv.org/abs/2512.07249
- Reference count: 40
- Primary result: A pre-processing method that uses influence functions to dynamically reweight training samples for fair classification, improving multiple fairness metrics without degrading utility

## Executive Summary
The paper addresses bias in machine learning classification models, particularly against unprivileged groups. IFFair proposes a novel pre-processing approach that uses influence functions to quantify the impact of each training sample on group fairness, then dynamically reweights samples to mitigate bias. Unlike traditional methods that require data modification, network architecture changes, or decision boundary adjustments, IFFair operates solely at the sample level. Experiments on 7 real-world datasets and 4 fairness metrics demonstrate that IFFair improves fairness metrics including demographic parity, equalized odds, equality of opportunity, and error rate parity without conflicts, while maintaining or even improving original utility.

## Method Summary
IFFair introduces a pre-processing method for fair classification that leverages influence functions to quantify each training sample's impact on group fairness. The approach works by first training an initial model to estimate sample influence on fairness metrics, then using these influence scores to compute sample weights. Samples that negatively impact fairness receive lower weights, while those that promote fairness receive higher weights. This reweighting scheme is applied during model training to mitigate bias. The method is model-agnostic and can be applied to both logistic regression and deep neural networks without requiring changes to model architecture or decision boundaries.

## Key Results
- Improves fairness across 4 metrics (demographic parity, equalized odds, equality of opportunity, error rate parity) on 7 real-world datasets
- Achieves better trade-off between fairness and utility compared to previous pre-processing methods
- Maintains or improves original utility while enhancing fairness
- Generalizes well to both logistic regression and deep neural networks

## Why This Works (Mechanism)
The mechanism leverages influence functions from robust statistics to measure how much each training sample affects the model's fairness performance. By identifying samples that disproportionately harm fairness (often minority or unprivileged group examples that are misclassified or underrepresented), IFFair can dynamically adjust their contribution during training. This creates a feedback loop where the model's fairness behavior informs sample weighting, leading to more equitable learning without explicit data augmentation or architectural modifications.

## Foundational Learning

- **Influence Functions**: Mathematical tools from robust statistics that measure how much a training sample affects model parameters; needed to quantify sample-level impact on fairness; quick check: verify that influence scores correlate with known bias patterns in data
- **Sample Reweighting**: Technique to assign different importance weights to training samples; needed to reduce the impact of bias-inducing samples; quick check: ensure weighted training converges to similar performance as unweighted
- **Fairness Metrics**: Quantitative measures like demographic parity and equalized odds; needed to evaluate and optimize for group fairness; quick check: confirm metric calculations match established definitions
- **Pre-processing Methods**: Approaches that modify training data or sample weights before model training; needed as contrast to in-processing and post-processing fairness techniques; quick check: verify no model architecture changes are required

## Architecture Onboarding

**Component Map**
Data -> Influence Function Calculator -> Sample Weight Generator -> Weighted Training Process -> Fair Model

**Critical Path**
1. Initial model training to estimate sample influences
2. Influence function computation for each sample
3. Sample weight calculation based on influence scores
4. Reweighted model training
5. Fairness evaluation

**Design Tradeoffs**
- Computational overhead of influence functions vs. fairness gains
- Sample-level granularity vs. potential noise in influence estimates
- Pre-processing simplicity vs. potential limitations compared to in-processing methods

**Failure Signatures**
- If influence functions fail to capture bias patterns, sample weights won't effectively mitigate unfairness
- Over-aggressive weighting may lead to poor utility despite improved fairness
- Computational bottlenecks during influence function calculation on large datasets

**3 First Experiments**
1. Test on a simple binary classification dataset with known group imbalance
2. Apply to logistic regression on a small fairness benchmark dataset
3. Evaluate influence function computation time on increasing dataset sizes

## Open Questions the Paper Calls Out
None

## Limitations
- Generalizability across different model architectures (e.g., ensemble methods, transformers) remains untested
- Computational overhead of influence functions on large-scale datasets is not thoroughly discussed
- Assumption that influence functions accurately capture bias-inducing samples may not hold with complex data distributions or subtle bias

## Confidence

- **High Confidence**: Effectiveness claims on tested datasets (7 datasets, 4 fairness metrics)
- **Medium Confidence**: Generalizability claims across model types and "better trade-off" assertions
- **Medium Confidence**: Utility maintenance claims without detailed ablations on diverse architectures

## Next Checks

1. **Scalability Assessment**: Evaluate IFFair's performance and computational efficiency on datasets with >100K samples to verify practical applicability
2. **Model Architecture Generalization**: Test IFFair on ensemble methods (e.g., random forests, gradient boosting) and transformer-based models to validate cross-architecture performance
3. **Bias Detection Robustness**: Conduct sensitivity analysis to determine how well influence functions identify bias-inducing samples under varying levels of dataset imbalance and subtle bias scenarios