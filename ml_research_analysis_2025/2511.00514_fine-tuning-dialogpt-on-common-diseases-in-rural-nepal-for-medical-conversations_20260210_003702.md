---
ver: rpa2
title: Fine-Tuning DialoGPT on Common Diseases in Rural Nepal for Medical Conversations
arxiv_id: '2511.00514'
source_url: https://arxiv.org/abs/2511.00514
tags:
- medical
- dialogue
- rural
- conversational
- generation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study fine-tuned DialoGPT on a synthetic dataset of doctor-patient\
  \ dialogues for ten common rural Nepalese diseases to enable offline medical conversational\
  \ AI. The fine-tuned model achieved a perplexity of 5.96 and produced contextually\
  \ relevant, empathetic responses, as confirmed by human evaluation scoring 3.8\u2013\
  4.1 out of 5 on medical appropriateness, empathy, and relevance."
---

# Fine-Tuning DialoGPT on Common Diseases in Rural Nepal for Medical Conversations

## Quick Facts
- arXiv ID: 2511.00514
- Source URL: https://arxiv.org/abs/2511.00514
- Reference count: 21
- Primary result: Fine-tuned DialoGPT-medium achieved perplexity 5.96 on synthetic dialogues for 10 common rural Nepalese diseases

## Executive Summary
This study fine-tuned DialoGPT-medium on synthetic doctor-patient dialogues covering ten common rural Nepalese diseases to create an offline medical conversational AI. Using 1000 synthetic dialogues generated by Gemini 2.5 Pro and validated by medical professionals, the model achieved a perplexity of 5.96 and scored 3.8–4.1 out of 5 on human evaluation metrics for medical appropriateness, empathy, and contextual relevance. The approach demonstrates that lightweight dialogue models can provide accessible, medically informed guidance in low-resource settings without requiring internet connectivity.

## Method Summary
The researchers generated 1000 synthetic doctor-patient dialogues (100 per disease) using Gemini 2.5 Pro, reviewed them for medical validity with Claude 4 Sonnet, and conducted final validation with medical professionals. The dialogues were formatted as CSV files with alternating user/system turns and split 80/20 for training and validation. DialoGPT-medium was fine-tuned using causal language modeling with cross-entropy loss, trained on Google Colab T4 GPU with learning rate 5e-5, batch size 4, and 3 epochs. Human evaluation assessed medical appropriateness, empathy, and relevance on 5-point Likert scales, while perplexity and token-level metrics measured technical performance.

## Key Results
- Achieved perplexity of 5.96 on validation set
- Human evaluation scores: medical appropriateness (3.8), empathy (4.1), contextual relevance (3.9) out of 5
- Model produced contextually relevant and empathetic responses despite limited training data
- Successfully handled ten common rural Nepalese diseases in offline conversational format

## Why This Works (Mechanism)
The approach works by leveraging DialoGPT's pre-trained conversational capabilities and fine-tuning on disease-specific dialogues that capture the symptom-presentation and medical-response patterns typical in rural healthcare settings. The synthetic data generation ensures comprehensive coverage of common medical scenarios while maintaining consistency in dialogue structure. The causal language modeling objective allows the model to predict appropriate medical responses given patient symptoms, while the limited dataset size prevents overfitting through controlled training duration.

## Foundational Learning
- Causal Language Modeling (CLM): Predicts next token in sequence; needed for dialogue generation; quick check: model can complete partial medical conversations coherently
- Perplexity metric: Measures model uncertainty on validation data; needed to assess generalization; quick check: lower perplexity indicates better understanding of medical dialogue patterns
- Tokenization with DialoGPT vocabulary: Converts text to model-readable format; needed for proper input encoding; quick check: tokenization preserves medical terminology without excessive out-of-vocabulary tokens
- Adam optimizer with learning rate scheduling: Updates model parameters during training; needed for stable convergence; quick check: training loss decreases steadily without divergence
- Cross-entropy loss: Measures prediction accuracy for next-token classification; needed as CLM training objective; quick check: loss decreases across epochs on training data
- 80/20 train/validation split: Separates data for training and unbiased evaluation; needed to assess generalization; quick check: validation metrics remain stable across training epochs

## Architecture Onboarding
Component map: Data generation -> Data preprocessing -> DialoGPT-medium model -> CLM fine-tuning -> Evaluation

Critical path: Synthetic dialogue generation → CSV formatting with user/system turns → Tokenization → 3-epoch fine-tuning → Perplexity calculation → Human evaluation

Design tradeoffs: Synthetic data provides controlled coverage but may miss real-world linguistic variations; DialoGPT-medium balances performance with computational efficiency for offline deployment; 3 epochs prevents overfitting on small dataset while achieving low perplexity

Failure signatures: High validation perplexity despite low training loss indicates overfitting; incoherent medical responses suggest poor synthetic data quality; inconsistent human evaluation scores may reveal rater bias or model instability

Three first experiments:
1. Generate 10 sample dialogues per disease and manually inspect for medical accuracy and natural conversation flow
2. Train on 10% of dataset (100 dialogues) for 1 epoch to verify training pipeline functionality
3. Compare model outputs on held-out validation samples against reference responses for coherence and medical appropriateness

## Open Questions the Paper Calls Out
- What is the impact of deploying this offline conversational agent on actual healthcare outcomes and user behavior in rural Nepalese communities? The authors note that field testing would allow assessment of the system's impact on healthcare outcomes.
- Can effective, lightweight safety filters be integrated into the offline model to reliably detect and mitigate harmful or hallucinated medical advice? The paper suggests implementing safety filters to improve patient safety.
- Does training on synthetic data generated by LLMs limit the model's ability to handle linguistic nuances and noise present in real patient speech? The authors question whether synthetic data captures real human symptom description variability.

## Limitations
- Synthetic training data may not capture real-world linguistic variations and clinical decision complexity
- Limited dataset size (1000 dialogues) provides narrow coverage of medical scenarios and patient presentations
- Human evaluation lacks inter-rater reliability measures and detailed scoring rubrics
- No assessment of model performance on real patient dialogues or field deployment validation

## Confidence
**High confidence**: Technical implementation follows established DialoGPT fine-tuning methodology with standard hyperparameters and reproducible training setup
**Medium confidence**: Human evaluation scores are internally consistent but lack benchmark comparisons and detailed rater methodology
**Low confidence**: Claims about practical effectiveness in rural healthcare settings extend beyond empirical evidence without validation of clinical utility or patient outcomes

## Next Checks
1. Conduct systematic safety audit testing model on adversarial medical scenarios to identify hallucinations or unsafe recommendations against clinical guidelines
2. Validate model performance with actual rural Nepalese patients or healthcare providers to assess cultural appropriateness and communication authenticity
3. Replicate fine-tuning using alternative base models and training paradigms to establish optimal approach for offline medical dialogue systems