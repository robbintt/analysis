---
ver: rpa2
title: 'How Muon''s Spectral Design Benefits Generalization: A Study on Imbalanced
  Data'
arxiv_id: '2510.22980'
source_url: https://arxiv.org/abs/2510.22980
tags:
- muon
- specgd
- learning
- adam
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper analyzes why spectrum-aware optimizers like Muon and
  Shampoo generalize better on imbalanced datasets. While standard optimizers like
  SGD and Adam prioritize learning dominant principal components first, the spectral
  design of Muon/SDRM implicitly learns all principal components at equal rates.
---

# How Muon's Spectral Design Benefits Generalization: A Study on Imbalanced Data

## Quick Facts
- arXiv ID: 2510.22980
- Source URL: https://arxiv.org/abs/2510.22980
- Reference count: 40
- Primary result: Muon learns all principal components at equal rates, improving minority-class performance on imbalanced datasets

## Executive Summary
This paper analyzes why spectrum-aware optimizers like Muon and Shampoo generalize better on imbalanced datasets. While standard optimizers like SGD and Adam prioritize learning dominant principal components first, the spectral design of Muon/SDRM implicitly learns all principal components at equal rates. This balanced learning translates to superior worst-class and class-balanced accuracy, especially early in training, even when compared to normalized GD variants. The theoretical analysis focuses on linear models and extends to deep linear models, showing that depth amplifies the balanced learning effect. Experiments on various imbalanced datasets validate that Muon consistently outperforms standard optimizers in minority-class performance, with the gap widening under greater imbalance.

## Method Summary
The paper compares Muon (and Shampoo) against standard optimizers on imbalanced datasets. Muon's core mechanism involves computing gradient SVD and updating with UV^T (discarding singular values), which forces all spectral components to advance at equal rates. The theoretical analysis uses a linear model framework with joint diagonalizability of moment matrices to derive closed-form trajectories. Experiments include synthetic linear models, CIFAR-10/100 with step imbalance (R=20), and Colored-MNIST with 99% spurious correlation. Muon uses momentum (β=0.9) and Newton-Schulz iteration for efficiency, while baselines include NMD, Signum, and Adam with standard hyperparameters.

## Key Results
- SpecGD learns all principal components at equal rates while GD prioritizes dominant components first
- Equal-rate learning translates to superior worst-class and class-balanced accuracy early in training
- Depth amplifies balanced learning effect by compressing saturation time gaps between components
- Muon consistently outperforms SGD/NMD on minority-class performance, with gap widening under greater imbalance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Spectral optimizers learn all principal components at equal rates, avoiding GD's bias toward dominant components.
- Mechanism: SpecGD's update Δ_t = U_t V_t^⊤ (from gradient SVD U_t Σ_t V_t^⊤) removes singular values Σ_t, so each spectral component advances at rate η·t until individual saturation. GD's update W_{t+1}[c,c] ∝ s_{xx}^c means dominant components (larger s_{xx}^c) learn faster.
- Core assumption: Joint diagonalizability of population moment matrices Σ_{yx} and Σ_{xx} (Condition 1), which holds exactly for the Gaussian mixture data model with orthogonal class means.
- Evidence anchors:
  - [abstract] "SpecGD learns all principal components at equal rates" vs GD "prioritizes dominant principal components first"
  - [section 3.2, Proposition 1] Shows W_t[c,c] = ηt·1[t ≤ threshold] + final_value·1[t > threshold] — linear until saturation
  - [corpus] Limited direct evidence on equal-rate learning; corpus focuses on optimization speedups, not this specific mechanism
- Break condition: Joint diagonalizability fails significantly (∥B∥ not ≪ ∥S_{xx}∥); components couple and dynamics deviate from decoupled scalar equations.

### Mechanism 2
- Claim: Equal-rate learning translates to better minority-class and class-balanced generalization early in training.
- Mechanism: In imbalanced data, minority classes correspond to smaller spectral components. GD delays learning these (loss gap grows linearly with time). SpecGF achieves LGF_m(t) - LSpec_m(t) ≥ μt/4 for t ≤ t^⋆ (Theorem 1).
- Core assumption: Minority class prior p_m is sufficiently small (p_m ≤ 1/(3SNR + 4k) for minority loss; stricter for balanced loss); SNR = μ²/σ_x².
- Evidence anchors:
  - [abstract] "balanced learning translates to superior worst-class and class-balanced accuracy, especially early in training"
  - [section 3.2, Theorem 1] Formal proof with linearly growing loss gap
  - [corpus] No direct corpus evidence on this generalization mechanism
- Break condition: Large minority class prior (p_m not ≪ 1); high noise (low SNR) where signal is drowned out; training beyond early-stopping regime (all methods converge to same solution asymptotically).

### Mechanism 3
- Claim: Depth amplifies balanced learning by compressing saturation time gaps between components.
- Mechanism: For L-layer deep linear model, component c saturates at time t_c ∝ (s_{yx}^c / s_{xx}^c)^{1/L}. Gap ΔT = (s_{yx}^1/s_{xx}^1 ÷ s_{yx}^k/s_{xx}^k)^{1/L} - 1 shrinks as L increases because exponent 1/L approaches uniformity.
- Core assumption: Small initialization (δ → ∞ limit); k = d (matching dimensions for deep model analysis).
- Evidence anchors:
  - [section 3.3, Proposition 2 & 4] Shows tc ∝ (syx/sxx)^{1/L} for bilinear/deep linear models
  - [Figure 5 & 6] Empirical validation showing compressed dynamics for deeper models
  - [corpus] No corpus evidence on depth effects specifically
- Break condition: Large initialization (δ not ≫ 1); non-linear activations (theory limited to linear/deep linear); practical networks may deviate due to skip connections, normalization, etc.

## Foundational Learning

- Concept: **Singular Value Decomposition (SVD) of gradient matrices**
  - Why needed here: SpecGD's core operation is extracting U, V from ∇L = UΣV^⊤ to form update Δ = UV^⊤. Understanding this decomposition is essential for grasping why spectral structure is preserved.
  - Quick check question: Given a 3×4 gradient matrix with SVD producing singular values {5, 2, 0.1}, what does SpecGD discard and what does it preserve?

- Concept: **Normalized Steepest Descent framework**
  - Why needed here: Unifies NGD (∥·∥_F), SignGD (∥·∥_max), and SpecGD (∥·∥_2) as instances with different norms. Clarifies that normalization alone doesn't achieve balanced learning (NGD still prioritizes dominant components).
  - Quick check question: If NGD normalizes the gradient, why doesn't it achieve equal-rate learning like SpecGD?

- Concept: **Joint diagonalizability of covariance matrices**
  - Why needed here: Technical condition enabling closed-form dynamics. Makes Σ_{yx} = US_{yx}V^⊤ and Σ_{xx} = VS_{xx}V^⊤ simultaneously possible, decoupling learning into independent scalar problems per component.
  - Quick check question: In a real dataset where empirical Σ matrices aren't perfectly jointly diagonalizable, would you expect SpecGD dynamics to be exactly equal-rate or approximately so?

## Architecture Onboarding

- Component map: Gradient → SVD → UV^⊤ update (SpecGD core) -> Momentum M_t = βM_{t-1} + (1-β)Δ_t (Muon) -> Newton-Schulz orthogonalization -> Apply update
- Critical path: 1) Identify matrix-valued parameters (weight matrices, attention matrices) — only these get Muon treatment 2) Vector parameters (biases, embeddings) use Adam or SGD separately 3) Per-step: compute gradient → momentum update → Newton-Schulz orthogonalization → apply update
- Design tradeoffs:
  - Newton-Schulz iterations vs. exact SVD: Fewer iterations faster but approximate; paper uses 1 iteration per step
  - Momentum β: Higher β (e.g., 0.9-0.95) preferred per hyperparameter sweeps; lower β approaches pure SpecGD
  - Learning rate: Muon typically requires different LR than Adam/SGD; paper uses 0.01-0.1 range vs. 0.001 for Adam
- Failure signatures:
  - Numerical instability: Small gradients cause SVD precision issues; paper stops when ∥∇∥ < 10^{-6}
  - No minority improvement: Data may not exhibit spectral structure aligned with class imbalance; check if dominant eigenvectors correspond to spurious features
  - Adam outperforming Muon: Adam may have inherent balanced-learning properties (Figure 10); Muon advantage most pronounced vs. SGD/NMD baselines
- First 3 experiments:
  1. Synthetic validation: Replicate Figure 2 — train linear model on Gaussian mixture with class imbalance (p_1 > p_2 > p_3), compare GD, NGD, SpecGD trajectories of W_t[c,c] and class-balanced loss. Verify SpecGD components advance uniformly while GD shows staggered learning.
  2. Colored-MNIST group imbalance: Train MLP on 99% spurious correlation setting. Track majority/minority group accuracy separately. Muon should improve minority accuracy earlier than NMD/Signum. Analyze input covariance eigenvectors to confirm color = dominant component.
  3. CIFAR-10/100 class imbalance sweep: Vary imbalance ratio R ∈ {1, 5, 10, 20}. Measure minority-class accuracy gap between Muon and SGD/Adam. Expect gap to grow with R (Figures 20, 21). Use this to calibrate when Muon's balanced-learning advantage is most impactful.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can we characterize the implicit bias of Spectral Gradient Descent (SpecGD) and establish formal generalization guarantees in the finite-sample regime where multiple interpolating solutions exist?
- Basis in paper: [explicit] The authors state that extending results to the "finite-sample regime is a key direction" and ask if implicit bias can be characterized to provide "generalization guarantees analogous to those in our population-level analysis."
- Why unresolved: The current theoretical analysis relies on population statistics and squared loss, whereas practical deep learning involves finite samples and often non-squared losses.
- What evidence would resolve it: A theoretical framework proving generalization bounds for SpecGD under finite sample conditions, potentially showing how it selects among interpolating solutions compared to GD.

### Open Question 2
- Question: How can the theoretical analysis be extended to commonly used loss functions like cross-entropy, where the assumption of joint diagonalizability of moment matrices may not hold?
- Basis in paper: [explicit] The paper identifies moving beyond squared loss to cross-entropy as a "crucial extension" and notes the "primary challenge" is that the joint diagonalizability condition (Condition 1) may fail in this setting.
- Why unresolved: The current proofs rely heavily on Condition 1 (joint diagonalizability of moment matrices) to derive closed-form trajectories, which is not guaranteed when using cross-entropy loss.
- What evidence would resolve it: Theoretical derivations of SpecGD dynamics using cross-entropy loss that either bypass the diagonalizability requirement or demonstrate new conditions under which the dynamics remain tractable.

### Open Question 3
- Question: What data structures or matrices define the principal components in sequential language data for next-token prediction, serving as analogs to the covariance and cross-covariance matrices (Σ_{xx}, Σ_{yx}) used in classification?
- Basis in paper: [explicit] The authors note that while experiments suggest balanced learning persists in language modeling, "a formal extension of our theory is non-trivial," posing the question of what data structures correspond to principal components in sequential data.
- Why unresolved: The current theoretical framework maps spectral components to class priors via moment matrices in a multi-class setting; the equivalent structural definition for sequential, autoregressive tasks remains undefined.
- What evidence would resolve it: Identification and analysis of the specific matrices in language modeling (e.g., within the embedding space or attention mechanisms) that dictate the "principal components" learned by the optimizer.

## Limitations

- The theoretical analysis is limited to linear models and deep linear models with simplifying assumptions (orthogonal class means, small initialization, matching dimensions)
- Joint diagonalizability (Condition 1) is critical for equal-rate learning but may not hold exactly for real-world datasets with natural image statistics
- The balanced learning advantage diminishes as training progresses, with all methods converging to similar solutions asymptotically

## Confidence

- High Confidence: The mechanism that SpecGD learns all principal components at equal rates (Mechanism 1) is directly proven in Proposition 1 with explicit equations showing W_t[c,c] ∝ t until saturation. The CIFAR-10 step-imbalance experiments provide strong empirical validation.
- Medium Confidence: The claim that equal-rate learning translates to better minority-class generalization early in training (Mechanism 2) is theoretically supported by Theorem 1 showing linearly growing loss gaps, but the extension to class-balanced accuracy and real-world datasets involves additional assumptions about class-prior distributions and SNR that may not hold universally.
- Low Confidence: The depth amplification effect (Mechanism 3) relies on asymptotic analysis (δ → ∞) and matching dimensions (k = d) that may not apply to practical deep networks. The empirical validation in Figure 5 and 6 is suggestive but not conclusive given the simplified model assumptions.

## Next Checks

1. **Non-Gaussian Data Test**: Validate SpecGD's equal-rate learning on real-world datasets where joint diagonalizability doesn't hold exactly (e.g., CIFAR-10 with natural image statistics). Measure deviation from uniform spectral component learning rates and quantify impact on minority-class performance.

2. **Early Stopping Benchmark**: Conduct experiments comparing Muon vs standard optimizers under early stopping regimes that reflect practical deployment scenarios. Measure minority-class accuracy at various training checkpoints to determine if balanced learning provides consistent advantages beyond the initial training phase.

3. **Activation Function Impact**: Test Muon on networks with different activation functions (ReLU, GELU, Swish) to determine how non-linearities affect the spectral learning dynamics. This would validate whether the deep linear model analysis extends to practical architectures and identify which network designs benefit most from spectral optimization.