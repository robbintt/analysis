---
ver: rpa2
title: 'VASSO: Variance Suppression for Sharpness-Aware Minimization'
arxiv_id: '2509.02433'
source_url: https://arxiv.org/abs/2509.02433
tags:
- asso
- generalization
- stochastic
- where
- adversary
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: VASSO stabilizes SAM adversaries by suppressing gradient variance,
  thereby improving generalization and computation efficiency. It uses an exponentially
  moving average of stochastic gradients to reduce variance, leading to more stable
  adversarial perturbations and better flatness estimation.
---

# VASSO: Variance Suppression for Sharpness-Aware Minimization

## Quick Facts
- arXiv ID: 2509.02433
- Source URL: https://arxiv.org/abs/2509.02433
- Reference count: 40
- Primary result: VASSO improves SAM generalization by suppressing gradient variance, achieving 0.1-0.3% accuracy gains with 57-70% reduced computation via eVASSO.

## Executive Summary
VASSO addresses a fundamental limitation in Sharpness-Aware Minimization (SAM) by stabilizing adversarial perturbations through variance suppression. The core insight is that SAM's single-minibatch adversaries can be "friendly" to other batches when gradients misalign, limiting generalization. VASSO introduces an exponentially moving average of stochastic gradients to produce more stable adversaries, theoretically achieving ρ√θσ-stability (tighter than SAM's ρσ-stability). Empirically, VASSO improves test accuracy across multiple vision tasks, enhances domain generalization, and when combined with probabilistic computation (eVASSO), achieves comparable accuracy to SAM with 57-70% reduced computation.

## Method Summary
VASSO modifies SAM by replacing the single-minibatch gradient with an exponentially weighted moving average (EMA) of stochastic gradients. The algorithm maintains a running average `dt` updated as `dt = (1-θ)dt-1 + θgt(xt)`, then computes the adversarial perturbation as `ϵt = ρ·dt/∥dt∥`. This smoothed gradient direction reduces variance in adversary computation, producing more stable estimates of sharpness. The variance suppression yields theoretical stability guarantees tighter than SAM, while empirically improving generalization across CIFAR, ImageNet, and domain generalization benchmarks. The method adds minimal computational overhead (one gradient-sized buffer) and can be combined with eVASSO for further efficiency gains through probabilistic adversary computation.

## Key Results
- VASSO improves test accuracy over SAM by 0.1-0.3% across CIFAR-10/100, ImageNet, and DomainBed tasks
- eVASSO achieves SAM-level accuracy with 57-70% reduced computation via probabilistic adversary computation (p=0.3)
- VASSO demonstrates enhanced robustness to label noise, with θ=0.2 performing best under high corruption levels
- Domain generalization performance improves with VASSO, particularly on PACS and OfficeHome datasets

## Why This Works (Mechanism)

### Mechanism 1: Variance Suppression via EMA
VASSO reduces gradient variance through exponentially weighted averaging, producing more stable adversarial perturbations. By maintaining an EMA of stochastic gradients rather than using raw minibatch gradients, VASSO achieves ρ√θσ-stability compared to SAM's ρσ-stability. The variance reduction is most effective when θ is moderate (0.2-0.4), though if θ is too small the O(σ/(θT^1/4)) term dominates and amplifies variance.

### Mechanism 2: Stabilized Adversaries Improve Sharpness Estimation
SAM's single-minibatch adversaries can be "friendly" to other batches when gradients misalign (⟨gt(xt), gB(xt)⟩ ≤ 0), curtailing generalization. EMA-based adversaries aggregate information across iterations, better approximating global sharpness and driving optimization toward genuinely flat valleys. This addresses the friendly adversary problem experimentally validated through SAM-db comparisons.

### Mechanism 3: Probabilistic Computation in eVASSO
eVASSO achieves comparable generalization to SAM with 57-70% reduced computation by introducing Bernoulli variable Rt with probability p; computes full VASSO step only when Rt=1, otherwise uses standard SGD. The expected gradient computations per iteration become (1+p), and stabilized adversaries from VASSO compensate for reduced adversary frequency.

## Foundational Learning

- **Concept: Sharpness-Aware Minimization (SAM)**
  - Why needed here: VASSO is a modification of SAM; understanding the minimax formulation `min_x max_{∥ϵ∥≤ρ} f(x+ϵ)` and why flat minima generalize is prerequisite.
  - Quick check question: Can you explain why SAM uses two backpropagations per iteration and what the adversary ϵ represents?

- **Concept: Gradient Variance in Stochastic Optimization**
  - Why needed here: The paper's core diagnosis is that gradient variance causes unstable stochastic linearization; understanding bounded variance (Assumption 3) and its impact on convergence is essential.
  - Quick check question: Why does standard SGD not suffer from "friendly adversaries" despite having stochastic gradients?

- **Concept: Exponential Moving Average (EMA) for Variance Reduction**
  - Why needed here: VASSO's primary intervention is EMA-based smoothing; understanding how EMA trades bias for variance reduction (θ parameter) is critical.
  - Quick check question: What happens to the bias-variance tradeoff in VASSO as θ approaches 0 versus 1?

## Architecture Onboarding

- **Component map:**
  - dt accumulator -> Adversary computer -> Gradient evaluator -> SGD update
  - (eVASSO only) Bernoulli sampler -> SGD update

- **Critical path:**
  1. Forward pass on minibatch Bt
  2. Compute gt(xt), update `dt ← (1−θ)dt + θgt(xt)`
  3. Compute adversary ϵt (skip if eVASSO with Rt=0)
  4. Forward pass at xt+ϵt, compute gt(xt+ϵt)
  5. SGD update: xt+1 ← xt − η·gt(xt+ϵt)

- **Design tradeoffs:**
  - θ selection: Small θ (0.2–0.4) gives stronger variance suppression but slower adaptation; large θ (0.9) preserves responsiveness. Paper finds θ=0.4 optimal for ResNet, θ=0.2 best under high label noise.
  - Memory: VASSO adds one gradient-sized buffer (same as SAM with momentum; no extra overhead per Section 3.2)
  - Computation: VASSO matches SAM's 2× overhead; eVASSO reduces to (1+p)× with tunable p

- **Failure signatures:**
  - Test accuracy degrades to SGD levels → θ too large (insufficient variance suppression) or ρ poorly tuned
  - Training divergence → η or ρ too aggressive; reduce η₀ or ρ₀
  - No improvement over SAM on small datasets → variance already low; VASSO provides marginal benefit
  - eVASSO underperforms SAM significantly → p too small; increase to ≥0.3

- **First 3 experiments:**
  1. **Sanity check (CIFAR-10, ResNet-18):** Replicate Table 1 result comparing SGD, SAM (ρ=0.1), and VASSO (ρ=0.1, θ=0.4). Target: VASSO improves over SAM by ~0.2%.
  2. **Hyperparameter sensitivity:** Sweep θ ∈ {0.2, 0.4, 0.6, 0.9} and ρ ∈ {0.05, 0.1, 0.2, 0.5} on CIFAR-100. Verify that smaller θ helps under higher noise/label corruption.
  3. **Efficiency validation (eVASSO):** On CIFAR-10, compare SAM vs eVASSO (p=0.3, θ=0.4) for accuracy vs wall-clock time. Target: eVASSO achieves ≥96.5% accuracy with ~30% less training time than SAM.

## Open Questions the Paper Calls Out

### Open Question 1
Can Frank-Wolfe variants beyond EMA-based approaches stabilize the SAM adversary? The paper establishes the SFW link and proposes VASSO (based on EMA), but does not test other SFW stabilization techniques like momentum-guided or acceleration-based FW methods.

### Open Question 2
What is the true mechanism driving m-sharpness if not gradient variance? Section 2.4 notes that understanding m-sharpness is beyond the scope of this work, while Observations 1 and 2 demonstrate that variance does not correlate with m-sharpness.

### Open Question 3
Can a more powerful adversary be defined to further enhance generalization? Section 2.2 raises the question given the large space of possible perturbations, while Theorem 1 guarantees convergence for any adversary on the surface Sρ(0).

## Limitations
- Theoretical stability bound assumes bounded gradient variance, but empirical relationship between variance suppression and generalization improvement is primarily demonstrated on image classification tasks
- eVASSO's stability guarantee holds "with high probability" but exact probability bounds are not specified
- Interaction between θ, ρ, and learning rate across architectures is not fully characterized

## Confidence

- **High Confidence:** VASSO improves test accuracy over SAM by 0.1-0.3 on standard vision benchmarks (CIFAR, ImageNet). The variance suppression mechanism is mathematically sound and empirically validated.
- **Medium Confidence:** The friendly adversary diagnosis is well-supported for vision tasks, but generalizability to other architectures (ViT, NLP models) is less established despite reported improvements.
- **Medium Confidence:** eVASSO's 57-70% computation reduction is demonstrated, but the trade-off between p and generalization is heuristic and optimal p values across tasks are not systematically explored.

## Next Checks

1. **Architecture Transfer Test:** Implement VASSO on ViT-B/16 trained on CIFAR-100. Compare test accuracy against SAM (ρ=0.1) and baseline SGD. Verify if θ=0.4 remains optimal or if larger θ is needed for transformers.

2. **Domain Generalization Robustness:** Train eVASSO (p=0.3, θ=0.4) on PACS (DomainBed) with ResNet-18. Perform leave-one-out cross-validation and compare average accuracy against SAM. Check if variance suppression improves worst-case performance across domains.

3. **Gradient Stability Quantification:** For CIFAR-10 ResNet-18, compute running statistics of ∥εt−εt−1∥ for SAM vs VASSO. Verify that VASSO reduces this variance metric by ≥30% while maintaining or improving final test accuracy.