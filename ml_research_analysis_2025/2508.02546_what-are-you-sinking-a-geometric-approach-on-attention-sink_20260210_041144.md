---
ver: rpa2
title: What are you sinking? A geometric approach on attention sink
arxiv_id: '2508.02546'
source_url: https://arxiv.org/abs/2508.02546
tags:
- reference
- layer
- attention
- layers
- frames
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper reframes attention sinks in transformers as fundamental
  geometric reference frames that establish stable coordinate systems in high-dimensional
  representation spaces. Through topological, spectral, and information-theoretic
  analysis of diverse architectures (Llama, Qwen, BERT, XLM-RoBERTa, Phi-2, Pythia,
  Gemma, Mistral), the authors identify three distinct reference frame types: centralized
  (decoder-only with standard RoPE), distributed (models with modified position encodings),
  and bidirectional (encoder models with absolute embeddings).'
---

# What are you sinking? A geometric approach on attention sink

## Quick Facts
- arXiv ID: 2508.02546
- Source URL: https://arxiv.org/abs/2508.02546
- Reference count: 40
- Primary result: Attention sinks are fundamental geometric reference frames that establish stable coordinate systems in high-dimensional representation spaces

## Executive Summary
This paper reframes attention sinks in transformers as fundamental geometric reference frames that establish stable coordinate systems in high-dimensional representation spaces. Through topological, spectral, and information-theoretic analysis of diverse architectures (Llama, Qwen, BERT, XLM-RoBERTa, Phi-2, Pythia, Gemma, Mistral), the authors identify three distinct reference frame types: centralized (decoder-only with standard RoPE), distributed (models with modified position encodings), and bidirectional (encoder models with absolute embeddings). Each type exhibits characteristic signatures in Betti numbers, Fiedler values, KL divergence, and Fisher information distribution. The research reveals that attention sinks emerge early in training as optimal solutions to the geometric constraints imposed by the softmax operation on the probability simplex, with architectural choices directly influencing which reference frame type develops.

## Method Summary
The authors analyze pretrained transformer models using topological methods (persistent homology with Ripser), spectral graph analysis (Laplacian eigendecomposition), and information-theoretic metrics (KL divergence, Fisher information). They extract attention matrices from diverse architectures and compute topological signatures (Betti numbers, persistence diagrams), spectral properties (Fiedler values, degree centralization), and information metrics (sink-removed KL divergence). The analysis covers 500 samples for topology/spectral analysis, 50 samples for KL divergence, and 100 samples for Random Matrix Theory analysis on Pythia checkpoints. Models analyzed include LLaMA (1B-8B), Qwen (3B-7B), BERT, XLM-RoBERTa, Pythia (1.4B-12B-12B), Mistral, Gemma, and Phi-2.

## Key Results
- Three distinct reference frame types identified: centralized (decoder-only with standard RoPE), distributed (models with modified position encodings), and bidirectional (encoder models with absolute embeddings)
- Attention sinks emerge in earliest training stages as mathematically optimal solutions to softmax constraints on the probability simplex
- Architectural choices (particularly position encoding implementations) directly determine which reference frame type develops
- Reference frames function as coordinate system anchors enabling consistent angular relationships between token representations across network depth

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Softmax's probability simplex constraint mathematically necessitates sparse attention distributions that concentrate mass on minimal token sets (reference points).
- Mechanism: The softmax operation transforms unbounded logits onto the probability simplex Δ^(n-1), introducing non-Euclidean geometry with positive curvature under the Fisher-Rao metric. This curvature creates geometric pressure toward simplex vertices, favoring sparse allocations. The constraint Σa_j = 1 makes attention zero-sum, creating competition that optimizes toward concentrated patterns.
- Core assumption: The manifold curvature and conservation law together produce reference frames as mathematically optimal—not merely common—solutions.
- Evidence anchors:
  - [abstract] "We show that reference frames emerge as mathematically optimal solutions to constraints imposed by the softmax operation on the probability simplex."
  - [section 3.2] "This constraint mathematically necessitates the emergence of reference frames by creating two critical geometric effects: i) it transforms unbounded attention logits into a bounded manifold with intrinsic curvature, and ii) it enforces a conservation law that makes attention a zero-sum resource."
  - [corpus] Related work on attention sinks in diffusion models confirms sink patterns persist across architectures, suggesting a shared underlying mechanism.

### Mechanism 2
- Claim: Position encoding implementations create architecture-specific inductive biases that determine which reference frame type emerges during training.
- Mechanism: Standard RoPE applies R_θ with θ_0 = 0 (identity matrix), giving the first token privileged computational status—no rotation means higher cosine similarity with all other token queries. NTK-aware scaled RoPE (α < 1) reduces angular separation uniformity, weakening first-token bias and enabling distributed reference points. Absolute embeddings directly inject position-specific information, naturally supporting bidirectional frames.
- Core assumption: The geometric inductive biases don't deterministically program patterns but shape loss landscapes so gradient descent converges toward specific frame types.
- Evidence anchors:
  - [abstract] "We show the influence of architecture components, particularly position encoding implementations, on the specific type of reference frame."
  - [section 3.4] "For the first token (j = 1), R_θ₁ = I (the identity matrix), creating a computational advantage that biases attention toward this position."
  - [corpus] Related work on ALiBi and scaled RoPE variants (Kazemnejad et al.) confirms positional encoding changes dramatically alter attention patterns, supporting the encoding→frame-type pathway.

### Mechanism 3
- Claim: Reference frames function as coordinate system anchors enabling consistent angular relationships between token representations across network depth.
- Mechanism: When a token attends to a reference point, it performs a vector operation orienting its representation within a shared coordinate system: h'_i ≈ α_{i,ref}(W_V x_{ref}) + Σ_{j≠ref} α_{ij}(W_V x_j). Dominant eigenvectors of attention matrices align with reference tokens, creating stable subspaces as coordinate axes (low-rank approximation A ≈ UV^⊤). This enables implicit basis transformations maintaining geometric consistency.
- Core assumption: Without stable reference points, distances and directions in high-dimensional space become ambiguous, making reliable encoding impossible.
- Evidence anchors:
  - [section 3] "Reference frames provide fixed geometric anchors that allow tokens to establish relative positions through consistent angular relationships."
  - [section 5.4] "High values indicate that reference tokens actively guide the orientation of all other token representations, functioning as coordinate axes rather than merely aggregating contextual information."
  - [corpus] Related work on relative representations (Moschella et al.) independently shows high-dimensional latent spaces require reference frames as canonical coordinate systems.

## Foundational Learning

- Concept: **Probability Simplex and Softmax Geometry**
  - Why needed here: Understanding why softmax inherently favors sparse distributions requires grasping that the simplex Δ^(n-1) is a bounded manifold with curvature, not flat Euclidean space.
  - Quick check question: If attention weights must sum to 1 and be non-negative, what shape does the space of all possible attention distributions form?

- Concept: **Rotary Position Embeddings (RoPE)**
  - Why needed here: The paper's explanation of why RoPE creates centralized frames hinges on understanding that rotation matrices R_θ_i modify query-key dot products via angular transformations, with θ=0 being the identity.
  - Quick check question: Why would a token at position 0 (receiving no rotation) have higher cosine similarity with queries at arbitrary positions than rotated tokens?

- Concept: **Eigendecomposition and Low-Rank Approximation**
  - Why needed here: The paper frames reference frames through spectral analysis—dominant eigenvectors aligning with reference tokens, stable subspaces as coordinate axes via A ≈ UV^⊤.
  - Quick check question: If an attention matrix has one dominant eigenvalue with corresponding eigenvector aligned to a single token position, what does this suggest about the attention structure?

## Architecture Onboarding

- Component map:
  - Position Encoding Layer -> Determines frame type (RoPE→centralized, NTK-scaled RoPE→distributed, absolute→bidirectional)
  - Attention Heads -> Specialized heads achieve 100% focus on reference tokens (120 heads in LLaMA-3.2-3B for BOS)
  - Softmax Layer -> Applies probability simplex constraint creating geometric pressure
  - Representation Manifold M -> The latent space where reference frames R = (M, P, φ) establish coordinate systems
  - Reference Points P -> Distinguished tokens functioning as geometric anchors (special tokens or positional anchors)

- Critical path: Position encoding creates inductive bias → Softmax simplex constraint amplifies sparse solutions → Training gradient descent converges to optimal reference frame type → Specialized attention heads lock onto reference tokens → Coordinate system stabilizes through network depth

- Design tradeoffs:
  - **Centralized (RoPE)**: Efficient hub-and-spoke compression, 30-40% attention to single reference, but less contextual adaptability
  - **Distributed (NTK-scaled RoPE)**: Flexible multi-point coordinates, better for diverse linguistic structures, but sacrifices compression efficiency
  - **Bidirectional (Absolute)**: Dual-anchor structure enables both sequence endpoints as reference, higher topological complexity (high Betti₁), but requires encoder architecture

- Failure signatures:
  - Removing attention sinks causes performance degradation despite semantic irrelevance (KL reduction values turn negative)
  - Reference token ablation disrupts geometric-semantic alignment (shifts from semantic to geometric organization)
  - Topological instability: Betti numbers changing unexpectedly through layers in centralized frames (should remain constant)
  - Sign-flip in Fiedler-centralization correlation at wrong thresholds indicates frame type mismatch

- First 3 experiments:
  1. **Reference token ablation**: Mask the BOS token (or identified reference tokens) and measure KL divergence change, attention entropy shift, and task performance degradation across layers. Expect negative KL reduction (performance drop) confirming reference function.
  2. **Position encoding swap**: Replace standard RoPE with NTK-scaled variant (or vice versa) in same architecture and track reference frame signature changes—spectral correlation sign-flip patterns, sink concentration distribution, topological persistence values.
  3. **Training dynamics tracking**: Using Pythia checkpoints (or similar), compute Random Matrix Theory metrics (spectral gap, participation ratio, D_KL from Marchenko-Pastur) from step 0 to step 8000 to verify reference frames emerge in earliest training stages, before task performance converges.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can attention sink tokens be used as anchoring points for transfer learning to enable more efficient knowledge transfer across different model architectures?
- Basis in paper: [explicit] Conclusion states: "Future works will explore using attention sink tokens as anchoring points for transfer learning, potentially enabling more efficient knowledge transfer while preserving geometric stability across different model architectures."
- Why unresolved: This application of the geometric framework has not been tested experimentally; the paper focuses on characterization rather than exploitation of reference frames.
- What evidence would resolve it: Experiments showing that models initialized with preserved sink structures transfer more effectively than those without, or that aligning reference frames between source and target architectures improves transfer efficiency.

### Open Question 2
- Question: How do reference frames continuously evolve throughout the entire training process, beyond the early stages analyzed via Random Matrix Theory?
- Basis in paper: [explicit] Limitations section states: "Our topological and spectral analyses focus on attention patterns at specific network snapshots rather than continuously tracking their evolution throughout training."
- Why unresolved: Current analysis examines discrete checkpoints (steps 0, 8, 9000-143000) without dense temporal coverage of the full training trajectory.
- What evidence would resolve it: Fine-grained tracking of topological metrics (Betti numbers, persistence values) and spectral properties at every training step or small intervals throughout convergence.

### Open Question 3
- Question: How do encoder-decoder architectures (e.g., T5, BART) establish reference frames given their hybrid attention structure combining bidirectional encoding with autoregressive decoding?
- Basis in paper: [inferred] The paper categorizes only three frame types tied to decoder-only (centralized/distributed) and encoder-only (bidirectional) architectures, leaving encoder-decoder models unexplored.
- Why unresolved: Encoder-decoder models have distinct attention patterns in encoder and decoder components, potentially requiring a fourth frame type or hybrid behavior.
- What evidence would resolve it: Applying the same topological, spectral, and KL divergence analyses to encoder-decoder models to identify whether they exhibit unified, layered, or novel reference frame structures.

## Limitations
- The identification of sink tokens relies on heuristic thresholds (90th percentile, 30% frequency) that may not generalize across architectures or tasks
- The claim that reference frames are "mathematically optimal" solutions conflates empirical prevalence with theoretical necessity—softmax constraint creates conditions favorable to sinks but doesn't prove uniqueness
- The work focuses primarily on attention patterns rather than downstream task performance, leaving open questions about functional importance vs architectural artifacts

## Confidence
- **High Confidence**: Architectural influence on reference frame type - systematic relationship between position encoding choices and resulting frame signatures well-supported by multiple architectural comparisons
- **Medium Confidence**: Functional necessity of reference frames - ablation shows performance degradation but semantic irrelevance of reference tokens raises questions about alternative solutions
- **Low Confidence**: Coordinate system functionality - directional influence metrics support but lack direct geometric validation of proposed vector space transformations

## Next Checks
1. **Direct Geometric Validation Experiment**: Replace reference tokens with synthetic tokens having identical embedding distributions but different positional identities. Measure whether coordinate system functionality persists based on geometric relationships rather than token identity.

2. **Alternative Solution Space Exploration**: Systematically modify softmax temperature and observe how reference frame characteristics change. If sinks persist across wide temperature range, this strengthens geometric necessity claim.

3. **Cross-Architectural Transfer Test**: Train models with mixed position encoding strategies (RoPE in lower layers, absolute embeddings in upper layers) and analyze whether reference frame types transition smoothly or exhibit hybrid behaviors.