---
ver: rpa2
title: Biased-Attention Guided Risk Prediction for Safe Decision-Making at Unsignalized
  Intersections
arxiv_id: '2510.12428'
source_url: https://arxiv.org/abs/2510.12428
tags:
- risk
- vehicle
- intersection
- attention
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a deep reinforcement learning framework for
  safe autonomous driving at unsignalized intersections, integrating biased attention
  with the Soft Actor-Critic (SAC) algorithm. The core idea is to use a Transformer-based
  risk predictor with biased attention to assess long-term collision risks and convert
  them into dense reward signals.
---

# Biased-Attention Guided Risk Prediction for Safe Decision-Making at Unsignalized Intersections

## Quick Facts
- arXiv ID: 2510.12428
- Source URL: https://arxiv.org/abs/2510.12428
- Reference count: 33
- One-line primary result: Achieves 74.5% reduction in average waiting time, 28.4% reduction in queue length, and 0.1% collision rate at unsignalized intersections

## Executive Summary
This paper presents a deep reinforcement learning framework for safe autonomous driving at unsignalized intersections, integrating biased attention with the Soft Actor-Critic (SAC) algorithm. The core innovation uses a Transformer-based risk predictor with biased attention to assess long-term collision risks and convert them into dense reward signals. This approach significantly improves both safety and efficiency compared to state-of-the-art methods, with extensive experiments showing superior performance across multiple metrics including waiting time, queue length, and collision rate.

## Method Summary
The framework employs SAC as the decision-making agent, enhanced by a Transformer-based risk predictor that uses biased attention to evaluate collision risks. The risk predictor transforms sparse collision events into continuous dense rewards, allowing the SAC agent to learn proactive avoidance strategies. A hierarchical experience replay mechanism balances high-risk (collision) and standard (safe) trajectories to prevent training from being overwhelmed by safe data. The system operates in SUMO simulation using the Colorado Springs dataset, with the agent controlling vehicles at the intersection entrance zone.

## Key Results
- Reduces average waiting time by 74.5% compared to state-of-the-art methods
- Decreases average queue length by 28.4% while maintaining safety
- Achieves collision rate of 0.1%, demonstrating effective risk management
- Ablation studies confirm the effectiveness of biased attention in enhancing risk sensitivity and model convergence

## Why This Works (Mechanism)

### Mechanism 1: Recency-Biased Attention for Risk Sensitivity
The framework modifies standard scaled dot-product attention by adding a linear bias matrix that prioritizes recent state-action pairs. This forces the risk predictor to weigh the most recent vehicle maneuvers more heavily than historical context, improving sensitivity to immediate decisions that influence long-term collision risk. The bias matrix structure mathematically inflates attention scores for later steps in the sequence, allowing the model to react more effectively to sudden braking or other critical maneuvers.

### Mechanism 2: Dense Risk-to-Reward Translation
By converting sparse collision events into continuous risk signals, the framework addresses the "sparse reward" problem in reinforcement learning. The Transformer predictor generates a continuous risk score at every step, creating a potential-based reward landscape that guides the SAC agent away from risky trajectories before collisions become inevitable. This dense feedback allows for more efficient learning of proactive safety strategies compared to traditional sparse penalty systems.

### Mechanism 3: Stratified Experience Replay
The hierarchical buffer separates collision and safe trajectories, sampling from each category in a 1:1 ratio during training. This balanced approach prevents the agent from being overwhelmed by safe data while ensuring consistent gradient signals from collision scenarios. The mechanism is particularly important as the agent improves and collisions become rare, maintaining learning stability and preventing overfitting to safe but potentially brittle policies.

## Foundational Learning

- **Soft Actor-Critic (SAC):** Why needed: Standard RL struggles with continuous action spaces; SAC's entropy regularization prevents deterministic solutions that could be brittle in uncertain intersection dynamics. Quick check: How does entropy regularization help when two vehicles approach simultaneously?

- **Transformer Self-Attention:** Why needed: Intersections involve variable numbers of vehicles; self-attention dynamically weighs vehicle importance rather than relying on fixed heuristics. Quick check: Why is positional encoding necessary for input sequences?

- **Reward Shaping & Sparse Rewards:** Why needed: Collisions are rare; learned risk predictors provide dense feedback instead of requiring infinite samples to learn safety. Quick check: What's the risk of manual reward shaping versus learned risk predictors?

## Architecture Onboarding

- **Component map:** SUMO Environment -> State Extraction -> Transformer Risk Predictor (Biased Attention) -> Dense Risk Score -> SAC Agent -> Continuous Acceleration Action -> Reward Integration
- **Critical path:** The Biased Attention layer is the critical innovation. Verification of the matrix B implementation is essential - incorrect application (e.g., bias direction reversed) will cause the model to attend to ancient history and fail to predict imminent collisions.
- **Design tradeoffs:** Simulation vs. Reality: Deterministic IDM models may cause overfitting to polite behavior. Sequence Length (N=10): Shorter sequences are faster but may miss long-term causality.
- **Failure signatures:** Conservative Freezing: Excessive risk weight causes permanent stopping at intersection entrance. Risk Blindness: Low attention bias causes risk predictor to act like a standard moving average, failing to react to sudden braking.
- **First 3 experiments:** 1) Ablation Study (Attention): Compare Standard vs. Biased Attention to verify final indices receive higher weights. 2) Buffer Sensitivity: Compare Standard Replay vs. Hierarchical Replay to check convergence and variance. 3) Hyperparameter Scan (β): Test different bias decay values to find optimal balance between recent sensitivity and context retention.

## Open Questions the Paper Calls Out

- **Question:** Can the framework extend to real-time trajectory planning within interior conflict areas of intersections?
- **Basis:** The Conclusion states future research will extend control from intersection entrance to interior.
- **Why unresolved:** Current methodology restricts control to the 30-meter entrance approach, treating interior as pass-through rather than active trajectory refinement zone.
- **Evidence needed:** Simulation results showing safe navigation with dynamic speed/path adjustments while traversing intersection center.

## Limitations

- Performance relies on accurate risk prediction and well-calibrated reward shaping
- Model performance untested against aggressive or non-compliant human drivers
- Deterministic IDM background vehicles may not capture real-world intersection behavior spectrum

## Confidence

- **High Confidence:** Framework's ability to reduce waiting time, queue length, and collision rate in SUMO simulation
- **Medium Confidence:** Generalizability to real-world conditions with diverse human drivers
- **Low Confidence:** Optimal choice of bias decay factor (β=0.2) and risk weight (λ=3.0) across different scenarios

## Next Checks

1. **Robustness to Driver Behavior:** Evaluate trained policy against aggressive IDM variants and rule-breaking agents to assess performance degradation
2. **Attention Bias Sensitivity:** Conduct systematic sweep of bias decay factor (β) to analyze risk prediction accuracy and policy performance
3. **Real-World Transferability:** Deploy trained policy on high-fidelity simulator or controlled real-world environment to validate safety improvements beyond SUMO simulation