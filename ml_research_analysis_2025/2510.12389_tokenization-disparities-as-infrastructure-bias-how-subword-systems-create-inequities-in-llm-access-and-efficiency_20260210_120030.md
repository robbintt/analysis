---
ver: rpa2
title: 'Tokenization Disparities as Infrastructure Bias: How Subword Systems Create
  Inequities in LLM Access and Efficiency'
arxiv_id: '2510.12389'
source_url: https://arxiv.org/abs/2510.12389
tags:
- tokenization
- languages
- language
- efficiency
- across
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study presents a systematic cross-linguistic evaluation of
  tokenization efficiency across 200+ languages using the FLORES-200 dataset and a
  standardized experimental framework. By applying consistent preprocessing, normalization,
  and tokenization protocols through the tiktoken library with the cl100kbase tokenizer,
  the research quantifies substantial disparities in tokenization efficiency between
  languages.
---

# Tokenization Disparities as Infrastructure Bias: How Subword Systems Create Inequities in LLM Access and Efficiency

## Quick Facts
- **arXiv ID**: 2510.12389
- **Source URL**: https://arxiv.org/abs/2510.12389
- **Reference count**: 25
- **Primary result**: Latin-script languages achieve optimal tokenization efficiency (50.2 TPS, 2.61 CPT), while non-Latin and morphologically complex languages incur 3-7x higher token inflation costs

## Executive Summary
This study presents a systematic cross-linguistic evaluation of tokenization efficiency across 200+ languages using the FLORES-200 dataset and standardized experimental protocols. By applying consistent preprocessing, normalization, and tokenization through the tiktoken library with the cl100kbase tokenizer, the research quantifies substantial disparities in tokenization efficiency between languages. Key metrics including Tokens Per Sentence (TPS), Characters Per Token (CPT), and Relative Tokenization Cost (RTC) reveal that Latin-script languages achieve optimal efficiency while non-Latin and morphologically complex languages incur significantly higher token inflation, with some scripts requiring up to 7-fold more tokens and 0.4-0.6 CPT ratios. These systematic inefficiencies translate into increased computational costs, reduced context utilization, and accessibility barriers for underrepresented language communities.

## Method Summary
The study uses FLORES-200 devtest split (1,012 sentences per language across 200+ languages) as input data. All text undergoes Unicode Normalization Form C (NFC) processing before tokenization using tiktoken's cl100k_base encoding. Per-sentence token counts and character counts are extracted, then aggregated to compute TPS (mean tokens per sentence), CPT (total characters divided by total tokens), and RTC (TPS of language divided by TPS of English). Results are grouped by script family for comparative analysis, revealing systematic efficiency disparities across different writing systems and morphological structures.

## Key Results
- Latin script achieves highest compression efficiency (2.61 CPT) while non-Latin scripts like Tibetan (0.49 CPT), Oriya (0.40 CPT), and Ol Chiki (0.41 CPT) demonstrate severe fragmentation
- Myanmar script requires highest token density (357.2 TPS), followed by Ol Chiki (342.1 TPS) and Oriya (334.0 TPS), representing nearly 7-fold higher tokenization costs compared to Latin script
- Relative Tokenization Cost analysis shows languages like Kannada, Dravidian, and Tai-Kadai require 3-4 times more computational resources than English for equivalent semantic processing

## Why This Works (Mechanism)

### Mechanism 1: Vocabulary Allocation Creates Compression Hierarchies
- Claim: Subword tokenizers trained on high-resource language corpora produce vocabularies that compress Latin-script text more efficiently than non-Latin scripts, creating systematic computational hierarchies.
- Mechanism: BPE iteratively merges frequent character sequences from training data. When training corpora are dominated by Latin-script languages, the resulting vocabulary contains more complete Latin character sequences and fewer pre-merged sequences for non-Latin scripts, forcing finer-grained fragmentation.
- Core assumption: Tokenizer vocabulary composition directly reflects training corpus language distribution, and vocabulary coverage determines compression efficiency.
- Evidence anchors: [abstract] Latin-script languages exhibit higher tokenization efficiency while non-Latin languages incur 3-5 times higher RTC ratios; [Page 5] Latin script achieves 2.61 CPT versus Tibetan's 0.49 CPT; Related work demonstrates fertility reliably predicts performance disparities across African languages.

### Mechanism 2: Token Inflation Reduces Effective Context Window Capacity
- Claim: Languages with elevated TPS consume disproportionately more of the model's fixed context window, reducing semantic content that can be processed in a single inference pass.
- Mechanism: Transformer models operate with fixed context windows measured in tokens. When equivalent semantic content requires more tokens in one language versus another (e.g., 357.2 TPS for Myanmar vs. 50.2 TPS for Latin), the effective information density per context slot decreases, forcing truncation or multi-turn processing.
- Core assumption: Context window capacity is the binding constraint for many practical applications, and semantic content is roughly comparable across FLORES-200 parallel sentences.
- Evidence anchors: [Page 4] Myanmar requires 357.2 TPS, representing nearly 7-fold higher tokenization costs; [Page 5] Higher token densities reduce effective context utilization for complex languages; Related work notes tokenization amplifies bias and wastes capacity across languages and domains.

### Mechanism 3: Token-Based Pricing Amplifies Economic Inequities
- Claim: Commercial API pricing models that charge per token create systematic economic barriers for speakers of inefficiently tokenized languages, who pay more for equivalent semantic processing.
- Mechanism: When tokenization efficiency varies 3-7x across languages, and pricing is token-denominated, users processing text in high-TPS languages pay proportionally more for identical operations, creating a "token tax" on underrepresented language communities.
- Core assumption: Commercial LLM services use token-based pricing, and users in affected communities have constrained budgets relative to their need for AI services.
- Evidence anchors: [Page 2] Token-based pricing results in disproportionately higher usage costs for speakers of underrepresented languages; [Page 5] Token-based pricing amplifies disparities, resulting in substantially higher usage costs; Related work documents tokenization length variations reaching 15-fold for semantically equivalent texts.

## Foundational Learning

- **Subword Tokenization Algorithms (BPE, WordPiece, SentencePiece)**: Why needed: The entire analysis rests on understanding how these algorithms construct vocabularies and segment text. Quick check: Given a vocabulary with tokens ["ing", "go", "##ing", "walk"], how would BPE tokenize "walking" versus "going" if the merge order prioritized "ing" before "walk"?

- **Unicode Normalization (NFC, NFD, NFKC, NFKD)**: Why needed: The experimental procedure explicitly applies NFC normalization before tokenization. Understanding why normalization matters for cross-linguistic comparison is essential for reproducibility. Quick check: Why might the same visual string "é" produce different token counts if one version is NFC-normalized (single codepoint U+00E9) versus NFD-normalized (U+0065 U+0301)?

- **Parallel Corpora and Cross-Lingual Evaluation**: Why needed: FLORES-200 provides sentence-aligned parallel text across 200 languages. Understanding how parallel corpora enable controlled cross-linguistic comparison is critical for interpreting the RTC metric. Quick check: If FLORES-200 sentences were not semantically equivalent across languages, what confounding factor would this introduce to the Relative Tokenization Cost metric?

## Architecture Onboarding

- **Component map**: FLORES-200 devtest split → Unicode NFC normalization → tiktoken cl100k_base encoding → per-sentence token/character counts → aggregated TPS, CPT, RTC → script family groupings → comparative efficiency analysis

- **Critical path**: 1) Load FLORES-200 devtest sentences for target language; 2) Apply Unicode NFC normalization; 3) Tokenize with tiktoken's cl100k_base encoding; 4) Extract token count (T_i) and character count (C_i) per sentence; 5) Aggregate: TPS = mean(T_i), CPT = sum(C_i) / sum(T_i); 6) Compute RTC = TPS(language) / TPS(English)

- **Design tradeoffs**: Single tokenizer vs. multi-tokenizer comparison (controlled baseline but limited generalizability); Efficiency metrics vs. downstream task performance (focus on efficiency without measuring impact on translation quality); Script-level vs. language-level analysis (reveals systematic patterns but may obscure within-script variation)

- **Failure signatures**: CPT < 1.0 (severe fragmentation indicating vocabulary has almost no coverage); RTC > 3.0 (critical efficiency barrier where users pay 3x+ for equivalent processing); High variance within script family (suggests script-level aggregation may be masking language-specific issues)

- **First 3 experiments**: 1) Baseline reproduction: Run exact pipeline on 10 languages spanning Latin, Cyrillic, Devanagari, Arabic, and one low-CPT script, verifying TPS, CPT, and RTC match reported values within 5% tolerance; 2) Tokenizer comparison: Replace cl100k_base with SentencePiece trained on balanced multilingual corpus, measuring whether CPT disparities narrow for underrepresented scripts; 3) Downstream correlation: Select 5 high-RTC and 5 low-RTC languages, run standardized task with fixed token budget, testing whether high-RTC languages show degraded performance when truncated to equivalent token counts.

## Open Questions the Paper Calls Out

- **What linguistically informed tokenization strategies and adaptive vocabulary construction methods can effectively reduce cross-linguistic efficiency disparities?**: The paper explicitly states future research should prioritize development of linguistically informed tokenization strategies and adaptive vocabulary construction methods that incorporate typological diversity. This remains unresolved as the study quantifies disparities but does not propose or evaluate specific algorithmic solutions.

- **How do tokenization efficiency disparities directly impact downstream task performance across languages?**: The paper focuses on efficiency rather than downstream performance, acknowledging that excessive subword fragmentation may impair semantic representation quality without providing empirical validation of whether token inflation causes measurable degradation in translation quality, classification accuracy, or generation coherence.

- **Do the observed tokenization efficiency patterns generalize across different tokenizer architectures and varying training corpus compositions?**: The study acknowledges "single tokenizer/dataset constraints" as a limitation, using only cl100k_base tokenizer trained primarily on high-resource language corpora, leaving unclear whether 7-fold efficiency disparities persist across architectures with different vocabulary construction methods or more balanced training data.

## Limitations

- The study uses only one tokenizer (cl100k_base) optimized for English, limiting generalizability to other tokenizer architectures or multilingual vocabularies
- The FLORES-200 dataset represents a narrow domain (translation prompts) that may not reflect real-world usage patterns with language mixing and code-switching
- The study does not examine downstream task performance or model behavior under token budget constraints, leaving open whether tokenization inefficiency directly translates to degraded user experience

## Confidence

**High Confidence**: The core empirical findings regarding tokenization efficiency disparities (TPS, CPT, RTC metrics) across language scripts are highly reliable. The experimental methodology is well-specified, using standardized normalization and tokenization protocols, and the quantitative results are internally consistent and reproducible.

**Medium Confidence**: The attribution of disparities to training corpus bias in BPE algorithms is well-supported by related work and observed correlations, but specific contributions of cl100k_base's English-centric training versus inherent algorithmic limitations require further investigation. Economic impact claims are logically sound but need empirical validation with actual API usage data.

**Low Confidence**: Claims about systematic degradation of model performance and user experience for high-RTC languages are extrapolations from tokenization efficiency data without direct empirical validation. While mechanisms are theoretically sound, real-world impact depends on task-specific requirements not explored in this study.

## Next Checks

1. **Tokenizer Architecture Comparison**: Replicate the FLORES-200 tokenization analysis using three alternative tokenizer architectures (SentencePiece with multilingual vocabulary, WordPiece, and Morfessor) on the same dataset. Measure whether CPT and RTC disparities persist across architectures, or if some approaches (particularly Morfessor with morphological awareness) show reduced inequity for morphologically complex languages.

2. **Downstream Task Performance Validation**: Select 10 languages spanning low, medium, and high RTC values. For each, run a standardized translation quality evaluation (e.g., COMET metric on FLORES devtest) with two conditions: (a) unlimited token budget, (b) token budget fixed at English's average token count. Test whether high-RTC languages show statistically significant quality degradation under token constraints, and whether this correlates with RTC values.

3. **Economic Impact Simulation**: Using actual commercial LLM API pricing data (OpenAI, Anthropic, Google), calculate the per-character processing cost for English versus high-RTC languages (e.g., Kannada, Myanmar, Tai-Kadai languages). Model the cumulative cost differential for typical user workflows (e.g., document summarization, question answering) and assess whether this creates prohibitive barriers for educational or professional use in affected communities.