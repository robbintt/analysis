---
ver: rpa2
title: Specialised or Generic? Tokenization Choices for Radiology Language Models
arxiv_id: '2508.09952'
source_url: https://arxiv.org/abs/2508.09952
tags:
- tokenizers
- tokenizer
- general
- vocabulary
- domain-specific
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates the impact of tokenizer choice on radiology
  language model performance, addressing the under-explored role of vocabulary in
  clinical NLP. The authors systematically compare general, medical, and domain-specific
  tokenizers across three imaging modalities (X-ray, CT, and PET-CT) for the task
  of radiology report summarization, both with and without biomedical pre-training.
---

# Specialised or Generic? Tokenization Choices for Radiology Language Models

## Quick Facts
- **arXiv ID:** 2508.09952
- **Source URL:** https://arxiv.org/abs/2508.09952
- **Reference count:** 16
- **Primary result:** Domain-specific tokenizers outperform general/medical tokenizers in radiology report summarization with higher BLEU/METEOR/ROUGE-L scores and lower memory usage.

## Executive Summary
This study systematically compares the impact of tokenizer choice on radiology language model performance for report summarization. The authors evaluate general-purpose (GPT-2), medical (PubMed), and domain-specific tokenizers across three imaging modalities. Domain-specific tokenizers, trained on individual radiology datasets, consistently achieve superior performance metrics while requiring less memory due to smaller vocabularies and shorter sequences. Pre-training on biomedical corpora partially closes the performance gap but does not eliminate the advantage of domain-specific tokenizers. The findings demonstrate that tailoring tokenization to clinical domains enhances both model performance and computational efficiency.

## Method Summary
The study compares three BPE tokenizers: General (GPT-2, ~50k vocab), Medical (PubMed abstracts, 30k vocab), and Specific (domain dataset, 9-11k vocab with ≥3 occurrence threshold). Transformer decoder models (N=8, H=8, D=512) are trained on radiology report pairs (Findings→Conclusion) from MIMIC-CXR (X-ray), CT-RATE (CT), and PET-CT datasets. Training is conducted from scratch and with biomedical pre-training on PubMed. Models are evaluated using standard NLP metrics (BLEU, METEOR, ROUGE-L) plus clinical efficacy measures (RadGraph-XL F1, CheXbert/RadBERT F1).

## Key Results
- Domain-specific tokenizers achieve highest BLEU scores (0.249-0.273) compared to general (0.160-0.203) and medical (0.227-0.267) tokenizers
- RadGraph-XL F1 scores: 0.250-0.270 for domain-specific vs 0.179-0.207 for general tokenizers
- Domain-specific tokenizers reduce memory requirements by 1-2GB due to smaller vocabularies (9-11k vs 30-50k tokens)
- Pre-training improves general and medical tokenizers but does not match domain-specific performance

## Why This Works (Mechanism)

### Mechanism 1
Domain-specific tokenizers improve performance by reducing semantic fragmentation of clinical terminology. General tokenizers fragment specialized terms (e.g., "bronchovasculature" → 8 tokens) while domain-specific tokenizers preserve them as single tokens, maintaining semantic units during embedding and learning.

### Mechanism 2
Reduced vocabulary size and sequence length decrease GPU memory requirements. Memory scales with vocabulary size V and sequence length S; domain-specific tokenizers produce smaller V (9-11k vs 30-50k) and shorter S (fewer fragments per document), reducing both linear and quadratic memory terms.

### Mechanism 3
Pre-training partially compensates for suboptimal tokenization by helping models learn relationships between subword fragments. General tokenizers fragment domain terms, but pre-training exposes models to many occurrences of these fragments across biomedical contexts, enabling learning of compositional semantics.

## Foundational Learning

- **Byte Pair Encoding (BPE) Tokenization**
  - Why needed: BPE determines vocabulary composition through iterative merge operations. Understanding this clarifies why training corpus selection directly affects which terms become atomic tokens versus fragments.
  - Quick check: Given a corpus where "pneumothorax" appears frequently but "pneumomediastinum" is rare, which term is more likely to become a single token under BPE?

- **Autoregressive Language Modeling**
  - Why needed: The task uses conditional generation p(T) = Π p(T_s | T_{<s}; θ). Sequence length directly impacts sequential predictions and attention operations.
  - Quick check: If Findings tokenizes to 200 tokens with domain-specific vs 340 with general tokenizer, what is the ratio of attention operations required?

- **Clinical Efficacy Metrics (RadGraph F1)**
  - Why needed: Standard NLP metrics measure surface similarity but not factual accuracy. RadGraph extracts clinical entities and relations, providing domain-relevant quality signal.
  - Quick check: A generated report with high BLEU but low RadGraph F1 likely has what type of error: grammatical, factual, or stylistic?

## Architecture Onboarding

- **Component map:**
  Tokenizer (BPE) -> Token Embedding Layer -> Transformer Decoder (N=8, H=8, D=512) -> Output Projection

- **Critical path:**
  1. Train BPE tokenizer on domain corpus (determines V, merge rules)
  2. Tokenize Findings→Conclusion pairs
  3. Train autoregressive LM: input = Findings tokens, target = Conclusion tokens
  4. Evaluate with NLP metrics (BLEU, METEOR, ROUGE-L) + clinical efficacy (RadGraph F1)

- **Design tradeoffs:**
  - Smaller vocabulary → lower memory, but may under-represent rare but important terms
  - Pre-training → improves general tokenizers, but adds computational overhead; still doesn't match domain-specific performance
  - Sequence length percentile (90th vs 99th) → longer context improves coverage but quadratically increases attention memory

- **Failure signatures:**
  - Excessive fragmentation: Check tokens-per-word ratio; >2.0 suggests vocabulary mismatch
  - OOM during training: Sequence length S too large for available memory; reduce batch size or S percentile
  - High BLEU, low RadGraph F1: Model captures surface patterns but misses clinical entities; consider clinical pre-training

- **First 3 experiments:**
  1. Train identical LM architectures from scratch using General, Medical, and domain-specific tokenizers on MIMIC-CXR. Report BLEU, ROUGE-L, RadGraph F1, and memory usage.
  2. Pre-train each tokenizer variant on PubMed abstracts, then fine-tune on target dataset. Measure performance gap reduction relative to from-scratch training.
  3. At fixed batch size, measure actual GPU memory consumption for each tokenizer variant. Compare against theoretical estimate to validate memory model.

## Open Questions the Paper Calls Out

### Open Question 1
Do the performance and efficiency benefits of domain-specific tokenizers transfer to other radiology NLP tasks, such as classification or named entity recognition? The study restricts evaluation to report summarization and does not assess other clinical NLP tasks.

### Open Question 2
Are the benefits of specialized vocabularies preserved when scaling to larger foundation models (e.g., 7B+ parameters)? The authors utilize a relatively small Transformer decoder and focus on smaller, task-specific models for resource-constrained settings.

### Open Question 3
To what extent are the reported gains driven by semantic vocabulary alignment versus the memory efficiency of smaller vocabularies? The study conflates "domain-specific text" with "smaller vocabulary sizes" (9–11k vs. 30–50k), making it difficult to isolate the cause of performance improvements.

## Limitations

- Tokenizer vocabulary size constraints: 9-11k token limit with ≥3 occurrence threshold may exclude rare but clinically important terms
- Pre-training corpus relevance: PubMed abstracts may not fully represent clinical context of radiology reports
- Metric completeness: Does not assess clinical usability factors such as report structure preservation or physician acceptance

## Confidence

- **High confidence**: Domain-specific tokenizers consistently outperform general and medical tokenizers in radiology report summarization tasks
- **Medium confidence**: Domain-specific tokenizers reduce memory requirements and computational costs
- **Low confidence**: Biomedical pre-training can partially compensate for suboptimal tokenization

## Next Checks

1. **Vocabulary threshold sensitivity analysis**: Systematically vary the minimum frequency threshold (1, 2, 3, 5, 10) for domain-specific tokenizer training and measure performance degradation/gains.

2. **Multi-domain tokenizer evaluation**: Train a single domain-specific tokenizer on the combined corpus of all three imaging modalities and compare its performance against individual modality-specific tokenizers.

3. **Clinical workflow integration study**: Deploy models using different tokenizers in a clinical annotation task where radiologists rate generated report quality for actual diagnostic utility.